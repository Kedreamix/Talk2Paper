<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose   Interaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-11a3eaa79b0d03650067200e1b8c02ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Pursuing-Temporal-Consistent-Video-Virtual-Try-On-via-Dynamic-Pose-Interaction"><a href="#Pursuing-Temporal-Consistent-Video-Virtual-Try-On-via-Dynamic-Pose-Interaction" class="headerlink" title="Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose   Interaction"></a>Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose   Interaction</h2><p><strong>Authors:Dong Li, Wenqi Zhong, Wei Yu, Yingwei Pan, Dingwen Zhang, Ting Yao, Junwei Han, Tao Mei</strong></p>
<p>Video virtual try-on aims to seamlessly dress a subject in a video with a specific garment. The primary challenge involves preserving the visual authenticity of the garment while dynamically adapting to the pose and physique of the subject. While existing methods have predominantly focused on image-based virtual try-on, extending these techniques directly to videos often results in temporal inconsistencies. Most current video virtual try-on approaches alleviate this challenge by incorporating temporal modules, yet still overlook the critical spatiotemporal pose interactions between human and garment. Effective pose interactions in videos should not only consider spatial alignment between human and garment poses in each frame but also account for the temporal dynamics of human poses throughout the entire video. With such motivation, we propose a new framework, namely Dynamic Pose Interaction Diffusion Models (DPIDM), to leverage diffusion models to delve into dynamic pose interactions for video virtual try-on. Technically, DPIDM introduces a skeleton-based pose adapter to integrate synchronized human and garment poses into the denoising network. A hierarchical attention module is then exquisitely designed to model intra-frame human-garment pose interactions and long-term human pose dynamics across frames through pose-aware spatial and temporal attention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized attention loss between consecutive frames to enhance temporal consistency. Extensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate the superiority of our DPIDM against the baseline methods. Notably, DPIDM achieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over the state-of-the-art GPD-VVTO approach. </p>
<blockquote>
<p>è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ—¨åœ¨æ— ç¼åœ°å°†ç‰¹å®šæœè£…æ·»åŠ åˆ°è§†é¢‘ä¸­çš„ä¸»ä½“ä¸Šã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨åŠ¨æ€é€‚åº”ä¸»ä½“çš„å§¿åŠ¿å’Œä½“å‹çš„åŒæ—¶ï¼Œä¿æŒæœè£…çš„è§†è§‰çœŸå®æ€§ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•ç©¿ä¸Šï¼Œä½†ç›´æ¥å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°è§†é¢‘é€šå¸¸ä¼šå¯¼è‡´æ—¶é—´ä¸Šçš„ä¸ä¸€è‡´ã€‚å½“å‰å¤§å¤šæ•°è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ–¹æ³•é€šè¿‡å¼•å…¥æ—¶é—´æ¨¡å—æ¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†ä»ç„¶å¿½ç•¥äº†äººä¸æœè£…ä¹‹é—´å…³é”®çš„ç©ºé—´æ—¶é—´å§¿åŠ¿äº¤äº’ã€‚è§†é¢‘ä¸­çš„æœ‰æ•ˆå§¿åŠ¿äº¤äº’ä¸ä»…è¦è€ƒè™‘æ¯ä¸€å¸§ä¸­äººä¸æœè£…å§¿åŠ¿çš„ç©ºé—´å¯¹é½ï¼Œè¿˜è¦è€ƒè™‘æ•´ä¸ªè§†é¢‘ä¸­äººçš„å§¿åŠ¿çš„åŠ¨æ€å˜åŒ–ã€‚åŸºäºè¿™æ ·çš„åŠ¨æœºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³åŠ¨æ€å§¿åŠ¿äº¤äº’æ‰©æ•£æ¨¡å‹ï¼ˆDPIDMï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ·±å…¥ç ”ç©¶è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä¸­çš„åŠ¨æ€å§¿åŠ¿äº¤äº’ã€‚æŠ€æœ¯ä¸Šï¼ŒDPIDMå¼•å…¥äº†ä¸€ä¸ªåŸºäºéª¨æ¶çš„å§¿åŠ¿é€‚é…å™¨ï¼Œå°†åŒæ­¥çš„äººç±»å’Œæœè£…å§¿åŠ¿é›†æˆåˆ°å»å™ªç½‘ç»œä¸­ã€‚ç„¶åç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥é€šè¿‡å§¿åŠ¿æ„ŸçŸ¥çš„ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æœºåˆ¶å¯¹å¸§å†…çš„äººä¸æœè£…å§¿åŠ¿äº¤äº’ä»¥åŠè·¨å¸§çš„é•¿æœŸäººç±»å§¿åŠ¿åŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒDPIDMåˆ©ç”¨ç›¸é‚»å¸§ä¹‹é—´çš„æ—¶é—´æ­£åˆ™åŒ–æ³¨æ„åŠ›æŸå¤±æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§ã€‚åœ¨VITON-HDã€VVTå’ŒViViDæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„DPIDMç›¸å¯¹äºåŸºå‡†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDPIDMåœ¨VVTæ•°æ®é›†ä¸Šçš„VFIDå¾—åˆ†è¾¾åˆ°0.506ï¼Œç›¸è¾ƒäºç›®å‰æœ€å…ˆè¿›çš„GPD-VVTOæ–¹æ³•ï¼Œæå‡äº†60.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16980v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ¡†æ¶â€”â€”åŠ¨æ€å§¿æ€äº¤äº’æ‰©æ•£æ¨¡å‹ï¼ˆDPIDMï¼‰ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ·±å…¥æ¢ç©¶è§†é¢‘ä¸­çš„åŠ¨æ€å§¿æ€äº¤äº’ã€‚DPIDMé€šè¿‡å¼•å…¥åŸºäºéª¨æ¶çš„å§¿æ€é€‚é…å™¨å’Œå±‚æ¬¡æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°äº†å¯¹å¸§å†…äººä¸è¡£ç‰©å§¿æ€çš„ç²¾ç»†å»ºæ¨¡ï¼Œå¹¶è€ƒè™‘äº†è·¨å¸§çš„é•¿æœŸäººä½“å§¿æ€åŠ¨æ€ã€‚æ­¤å¤–ï¼ŒDPIDMé‡‡ç”¨ä¸´æ—¶æ­£åˆ™åŒ–æ³¨æ„åŠ›æŸå¤±æ¥å¢å¼ºæ—¶åºä¸€è‡´æ€§ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ—¨åœ¨æ— ç¼åœ°å°†ç‰¹å®šæœè£…æ·»åŠ åˆ°è§†é¢‘ä¸­çš„ä¸»ä½“ä¸Šã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨åŠ¨æ€é€‚åº”ä¸»ä½“å§¿åŠ¿å’Œä½“å‹çš„åŒæ—¶ä¿æŒæœè£…çš„è§†è§‰çœŸå®æ€§ã€‚</li>
<li>å°½ç®¡å›¾åƒè™šæ‹Ÿè¯•ç©¿æ–¹æ³•å¾ˆå—æ¬¢è¿ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘ä¼šå¯¼è‡´æ—¶åºä¸ä¸€è‡´æ€§ã€‚</li>
<li>å½“å‰è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ–¹æ³•è™½ç„¶åŠ å…¥æ—¶åºæ¨¡å—æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†ä»ç„¶å¿½ç•¥äº†å…³é”®çš„ç©ºé—´æ—¶é—´å§¿æ€äº¤äº’ã€‚</li>
<li>DPIDMæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†åŠ¨æ€å§¿æ€äº¤äº’è¿›è¡Œè§†é¢‘è™šæ‹Ÿè¯•ç©¿ã€‚</li>
<li>DPIDMé€šè¿‡å¼•å…¥éª¨æ¶åŸºäºçš„å§¿æ€é€‚é…å™¨å’Œå±‚æ¬¡æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œç²¾ç»†å»ºæ¨¡ã€‚</li>
<li>DPIDMé‡‡ç”¨ä¸´æ—¶æ­£åˆ™åŒ–æ³¨æ„åŠ›æŸå¤±ä»¥å¢å¼ºæ—¶åºä¸€è‡´æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87f9b7ed6326278cca0db76b862880a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a55243a8ded48ece41b92d31fb861b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48953bcc2e805a7eb1b7ead8db0bf5f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1462633c78a27d60bb79a906ce5a62f6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Incorporating-Visual-Correspondence-into-Diffusion-Model-for-Virtual-Try-On"><a href="#Incorporating-Visual-Correspondence-into-Diffusion-Model-for-Virtual-Try-On" class="headerlink" title="Incorporating Visual Correspondence into Diffusion Model for Virtual   Try-On"></a>Incorporating Visual Correspondence into Diffusion Model for Virtual   Try-On</h2><p><strong>Authors:Siqi Wan, Jingwen Chen, Yingwei Pan, Ting Yao, Tao Mei</strong></p>
<p>Diffusion models have shown preliminary success in virtual try-on (VTON) task. The typical dual-branch architecture comprises two UNets for implicit garment deformation and synthesized image generation respectively, and has emerged as the recipe for VTON task. Nevertheless, the problem remains challenging to preserve the shape and every detail of the given garment due to the intrinsic stochasticity of diffusion model. To alleviate this issue, we novelly propose to explicitly capitalize on visual correspondence as the prior to tame diffusion process instead of simply feeding the whole garment into UNet as the appearance reference. Specifically, we interpret the fine-grained appearance and texture details as a set of structured semantic points, and match the semantic points rooted in garment to the ones over target person through local flow warping. Such 2D points are then augmented into 3D-aware cues with depth&#x2F;normal map of target person. The correspondence mimics the way of putting clothing on human body and the 3D-aware cues act as semantic point matching to supervise diffusion model training. A point-focused diffusion loss is further devised to fully take the advantage of semantic point matching. Extensive experiments demonstrate strong garment detail preservation of our approach, evidenced by state-of-the-art VTON performances on both VITON-HD and DressCode datasets. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/SPM-Diff">https://github.com/HiDream-ai/SPM-Diff</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰ä»»åŠ¡ä¸­åˆæ­¥å–å¾—äº†æˆåŠŸã€‚å…¸å‹çš„åŒåˆ†æ”¯æ¶æ„åŒ…å«ä¸¤ä¸ªUç½‘ï¼Œåˆ†åˆ«ç”¨äºéšå¼æœè£…å˜å½¢å’Œåˆæˆå›¾åƒç”Ÿæˆï¼Œå·²æˆä¸ºVTONä»»åŠ¡çš„é…æ–¹ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹çš„å›ºæœ‰éšæœºæ€§ï¼Œä¿æŒç»™å®šæœè£…çš„å½¢çŠ¶å’Œæ¯ä¸€ä¸ªç»†èŠ‚ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°æå‡ºæ˜ç¡®åˆ©ç”¨è§†è§‰å¯¹åº”ä½œä¸ºå…ˆéªŒæ¥é©¯æœæ‰©æ•£è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å°†æ•´ä¸ªæœè£…è¾“å…¥Uç½‘ä½œä¸ºå¤–è§‚å‚è€ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç»†è‡´çš„å¤–è§‚å’Œçº¹ç†ç»†èŠ‚è§£é‡Šä¸ºä¸€ç»„ç»“æ„åŒ–è¯­ä¹‰ç‚¹ï¼Œå¹¶é€šè¿‡å±€éƒ¨æµæ‰­æ›²å°†æœè£…ä¸­çš„è¯­ä¹‰ç‚¹ä¸ç›®æ ‡äººç‰©ä¸Šçš„è¯­ä¹‰ç‚¹ç›¸åŒ¹é…ã€‚è¿™äº›äºŒç»´ç‚¹ç„¶åé€šè¿‡ç›®æ ‡äººç‰©çš„æ·±åº¦&#x2F;æ³•çº¿å›¾å¢å¼ºä¸ºä¸‰ç»´æ„ŸçŸ¥çº¿ç´¢ã€‚è¿™ç§å¯¹åº”å…³ç³»æ¨¡ä»¿äº†å°†è¡£æœç©¿åœ¨äººä½“ä¸Šçš„æ–¹å¼ï¼Œä¸‰ç»´æ„ŸçŸ¥çº¿ç´¢ä½œä¸ºè¯­ä¹‰ç‚¹åŒ¹é…æ¥ç›‘ç£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒã€‚è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§ç‚¹èšç„¦æ‰©æ•£æŸå¤±ï¼Œä»¥å……åˆ†åˆ©ç”¨è¯­ä¹‰ç‚¹åŒ¹é…çš„ä¼˜åŠ¿ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœè£…ç»†èŠ‚ä¿æŠ¤æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œåœ¨VITON-HDå’ŒDressCodeæ•°æ®é›†ä¸Šçš„VTONæ€§èƒ½å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/SPM-Diff">HiDream-ai&#x2F;SPM-Diff</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16977v1">PDF</a> ICLR 2025. Code is publicly available at:   <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/SPM-Diff">https://github.com/HiDream-ai/SPM-Diff</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰ä»»åŠ¡ä¸­å–å¾—åˆæ­¥æˆåŠŸã€‚æ–‡ç« æå‡ºä¸€ç§æ–°å‹çš„åŒåˆ†æ”¯æ¶æ„ï¼Œä½¿ç”¨ä¸¤ä¸ªUNetåˆ†åˆ«å¤„ç†è¡£ç‰©å˜å½¢å’Œå›¾åƒç”Ÿæˆã€‚ä¸ºè§£å†³æ‰©æ•£æ¨¡å‹å†…åœ¨éšæœºæ€§å¯¼è‡´çš„è¡£ç‰©å½¢çŠ¶å’Œç»†èŠ‚ä¿ç•™é—®é¢˜ï¼Œæ–‡ç« åˆ›æ–°åœ°æå‡ºåˆ©ç”¨è§†è§‰å¯¹åº”æ€§ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„å…ˆéªŒä¿¡æ¯ï¼Œè€Œéç®€å•åœ°å°†æ•´ä¸ªè¡£ç‰©è¾“å…¥UNetä½œä¸ºå¤–è§‚å‚è€ƒã€‚é€šè¿‡åŒ¹é…è¡£ç‰©ä¸­çš„è¯­ä¹‰ç‚¹ä¸ç›®æ ‡äººç‰©ä¸Šçš„ç‚¹ï¼Œå®ç°ç²¾ç»†å¤–è§‚å’Œçº¹ç†ç»†èŠ‚çš„è§£è¯»ã€‚æ­¤å¤–ï¼Œå°†è¿™äº›2Dç‚¹å¢å¼ºä¸ºå…·æœ‰ç›®æ ‡äººç‰©æ·±åº¦&#x2F;æ³•çº¿å›¾çš„3Dæ„ŸçŸ¥çº¿ç´¢ã€‚è§†è§‰å¯¹åº”æ¨¡ä»¿äº†è¡£ç‰©ç©¿åœ¨äººä½“ä¸Šçš„æ–¹å¼ï¼Œè€Œ3Dæ„ŸçŸ¥çº¿ç´¢åˆ™ä½œä¸ºè¯­ä¹‰ç‚¹åŒ¹é…æ¥ç›‘ç£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¡£ç‰©ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨VITON-HDå’ŒDressCodeæ•°æ®é›†ä¸Šçš„è™šæ‹Ÿè¯•ç©¿æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸­å–å¾—åˆæ­¥æˆåŠŸã€‚</li>
<li>ä¼ ç»Ÿçš„åŒåˆ†æ”¯æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªUNetï¼Œåˆ†åˆ«ç”¨äºéšå¼è¡£ç‰©å˜å½¢å’Œåˆæˆå›¾åƒç”Ÿæˆã€‚</li>
<li>æå‡ºçš„æ–°æ–¹æ³•åˆ©ç”¨è§†è§‰å¯¹åº”æ€§ä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„å…ˆéªŒä¿¡æ¯ï¼Œä»¥æé«˜è¡£ç‰©å½¢çŠ¶å’Œç»†èŠ‚çš„ä¿ç•™æ•ˆæœã€‚</li>
<li>é€šè¿‡åŒ¹é…è¡£ç‰©ä¸­çš„è¯­ä¹‰ç‚¹ä¸ç›®æ ‡äººç‰©ä¸Šçš„ç‚¹ï¼Œå®ç°ç²¾ç»†å¤–è§‚å’Œçº¹ç†ç»†èŠ‚çš„è§£è¯»ã€‚</li>
<li>2Dç‚¹è¢«å¢å¼ºä¸ºå…·æœ‰ç›®æ ‡äººç‰©æ·±åº¦&#x2F;æ³•çº¿å›¾çš„3Dæ„ŸçŸ¥çº¿ç´¢ã€‚</li>
<li>è§†è§‰å¯¹åº”æ¨¡ä»¿è¡£ç‰©ç©¿åœ¨äººä½“ä¸Šçš„æ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73514832929c45dcf7ef1b454cd3ec57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee74394f860bc4758c5d65d3923b5099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa66913b8ffb9ad465b38ad47d565d84.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Creatively-Upscaling-Images-with-Global-Regional-Priors"><a href="#Creatively-Upscaling-Images-with-Global-Regional-Priors" class="headerlink" title="Creatively Upscaling Images with Global-Regional Priors"></a>Creatively Upscaling Images with Global-Regional Priors</h2><p><strong>Authors:Yurui Qian, Qi Cai, Yingwei Pan, Ting Yao, Tao Mei</strong></p>
<p>Contemporary diffusion models show remarkable capability in text-to-image generation, while still being limited to restricted resolutions (e.g., 1,024 X 1,024). Recent advances enable tuning-free higher-resolution image generation by recycling pre-trained diffusion models and extending them via regional denoising or dilated sampling&#x2F;convolutions. However, these models struggle to simultaneously preserve global semantic structure and produce creative regional details in higher-resolution images. To address this, we present C-Upscale, a new recipe of tuning-free image upscaling that pivots on global-regional priors derived from given global prompt and estimated regional prompts via Multimodal LLM. Technically, the low-frequency component of low-resolution image is recognized as global structure prior to encourage global semantic consistency in high-resolution generation. Next, we perform regional attention control to screen cross-attention between global prompt and each region during regional denoising, leading to regional attention prior that alleviates object repetition issue. The estimated regional prompts containing rich descriptive details further act as regional semantic prior to fuel the creativity of regional detail generation. Both quantitative and qualitative evaluations demonstrate that our C-Upscale manages to generate ultra-high-resolution images (e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more creative regional details. </p>
<blockquote>
<p>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å—é™äºè¾ƒä½åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ï¼Œ1,024 X 1,024ï¼‰ã€‚æœ€è¿‘çš„è¿›å±•é€šè¿‡å›æ”¶é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¹¶é€šè¿‡åŒºåŸŸå»å™ªæˆ–è†¨èƒ€é‡‡æ ·&#x2F;å·ç§¯å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œå®ç°äº†æ— éœ€è°ƒæ•´çš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ä¿æŒå…¨å±€è¯­ä¹‰ç»“æ„çš„åŒæ—¶ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­äº§ç”Ÿåˆ›é€ æ€§åŒºåŸŸç»†èŠ‚æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†C-Upscaleï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„å›¾åƒè¶…åˆ†è¾¨ç‡æ–°æ–¹æ³•ï¼Œå®ƒä¾èµ–äºåŸºäºç»™å®šå…¨å±€æç¤ºå’Œé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¼°è®¡çš„åŒºåŸŸæç¤ºæ‰€æ´¾ç”Ÿçš„å…¨å±€-åŒºåŸŸå…ˆéªŒçŸ¥è¯†ã€‚æŠ€æœ¯ä¸Šï¼Œæˆ‘ä»¬è¯†åˆ«ä½åˆ†è¾¨ç‡å›¾åƒçš„ä½é¢‘æˆåˆ†ä½œä¸ºå…¨å±€ç»“æ„å…ˆéªŒï¼Œä»¥é¼“åŠ±é«˜åˆ†è¾¨ç‡ç”Ÿæˆä¸­çš„å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹åŒºåŸŸå»å™ªè¿‡ç¨‹ä¸­çš„å…¨å±€æç¤ºå’Œæ¯ä¸ªåŒºåŸŸä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›è¿›è¡Œç­›é€‰ï¼Œå½¢æˆåŒºåŸŸæ³¨æ„åŠ›å…ˆéªŒï¼Œç¼“è§£äº†å¯¹è±¡é‡å¤é—®é¢˜ã€‚ä¼°è®¡çš„åŒºåŸŸæç¤ºå¯Œå«æè¿°æ€§ç»†èŠ‚ï¼Œè¿›ä¸€æ­¥ä½œä¸ºåŒºåŸŸè¯­ä¹‰å…ˆéªŒï¼Œä¸ºåŒºåŸŸç»†èŠ‚ç”Ÿæˆæä¾›åˆ›é€ åŠ›ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°å‡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„C-Upscaleèƒ½å¤Ÿç”Ÿæˆè¶…é«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆä¾‹å¦‚ï¼Œ4,096 X 4,096å’Œ8,192 X 8,192ï¼‰ï¼Œå…·æœ‰æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ›´å¯Œæœ‰åˆ›é€ åŠ›çš„åŒºåŸŸç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16976v1">PDF</a> International Journal of Computer Vision (IJCV) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“ä»£æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä¸­çš„æ˜¾è‘—èƒ½åŠ›ï¼Œä½†ä»å—é™äºä½åˆ†è¾¨ç‡ã€‚è¿‘æœŸå‘å±•é€šè¿‡å†åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å¹¶å€ŸåŠ©åŒºåŸŸå»å™ªæˆ–è†¨èƒ€é‡‡æ ·&#x2F;å·ç§¯è¿›è¡Œæ‰©å±•ï¼Œå®ç°äº†æ— è°ƒæ•´é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ä¿æŒå…¨å±€è¯­ä¹‰ç»“æ„å’Œç”Ÿæˆåˆ›æ„åŒºåŸŸç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†C-Upscaleï¼Œä¸€ç§æ— éœ€è°ƒæ•´çš„æ–°å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ¡ˆï¼ŒåŸºäºå…¨å±€å’ŒåŒºåŸŸå…ˆéªŒè¿›è¡Œå·¥ä½œã€‚è¯¥æ–¹æ¡ˆå¯ä»ç»™å®šçš„å…¨å±€æç¤ºå’Œé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¼°è®¡çš„åŒºåŸŸæç¤ºä¸­æ´¾ç”Ÿå‡ºæ¥ã€‚é€šè¿‡è¯†åˆ«ä½åˆ†è¾¨ç‡å›¾åƒçš„ä½é¢‘æˆåˆ†ä½œä¸ºå…¨å±€ç»“æ„å…ˆéªŒï¼Œé¼“åŠ±åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ä¿æŒå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ¥ç€ï¼Œå¯¹åŒºåŸŸå»å™ªè¿‡ç¨‹ä¸­çš„å…¨å±€æç¤ºå’Œæ¯ä¸ªåŒºåŸŸä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›è¿›è¡Œç­›é€‰ï¼Œå½¢æˆåŒºåŸŸæ³¨æ„åŠ›å…ˆéªŒï¼Œå‡è½»äº†å¯¹è±¡é‡å¤é—®é¢˜ã€‚åŒ…å«ä¸°å¯Œæè¿°æ€§ç»†èŠ‚çš„åŒºåŸŸæç¤ºä¼°è®¡è¿›ä¸€æ­¥ä½œä¸ºåŒºåŸŸè¯­ä¹‰å…ˆéªŒï¼Œä¿ƒè¿›åŒºåŸŸç»†èŠ‚ç”Ÿæˆçš„åˆ›é€ åŠ›ã€‚è¯„ä¼°è¯æ˜ï¼ŒC-UpscaleæˆåŠŸç”Ÿæˆäº†è¶…é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå…·æœ‰æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ›´å¯Œæœ‰åˆ›é€ åŠ›çš„åŒºåŸŸç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“ä»£æ‰©æ•£æ¨¡å‹è™½æ“…é•¿æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆï¼Œä½†åˆ†è¾¨ç‡å—é™ã€‚</li>
<li>è¿‘æœŸå‘å±•é€šè¿‡æ‰©å±•é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å®ç°æ— è°ƒæ•´é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ä¿æŒå…¨å±€è¯­ä¹‰ç»“æ„å’Œç”Ÿæˆåˆ›æ„åŒºåŸŸç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>C-Upscaleæ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„æ–°å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ¡ˆï¼ŒåŸºäºå…¨å±€å’ŒåŒºåŸŸå…ˆéªŒã€‚</li>
<li>C-Upscaleé€šè¿‡è¯†åˆ«ä½åˆ†è¾¨ç‡å›¾åƒçš„ä½é¢‘æˆåˆ†ä½œä¸ºå…¨å±€ç»“æ„å…ˆéªŒï¼Œä¿ƒè¿›å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>C-Upscaleé‡‡ç”¨åŒºåŸŸæ³¨æ„åŠ›æ§åˆ¶ï¼Œå‡è½»å¯¹è±¡é‡å¤é—®é¢˜ï¼Œå¹¶åŒ…å«ä¸°å¯Œç»†èŠ‚çš„åŒºåŸŸè¯­ä¹‰å…ˆéªŒï¼Œä¿ƒè¿›åŒºåŸŸç»†èŠ‚åˆ›é€ åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27820cb26a2b0de7d1539f24c5d1b101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f7724161b87a57d114dbd5e2cfbd0b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b743cde9875070c5ecf90aa1b7fd9d77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-818e30c5801c3edf3104a73baf2a269f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bigger-Isnâ€™t-Always-Memorizing-Early-Stopping-Overparameterized-Diffusion-Models"><a href="#Bigger-Isnâ€™t-Always-Memorizing-Early-Stopping-Overparameterized-Diffusion-Models" class="headerlink" title="Bigger Isnâ€™t Always Memorizing: Early Stopping Overparameterized   Diffusion Models"></a>Bigger Isnâ€™t Always Memorizing: Early Stopping Overparameterized   Diffusion Models</h2><p><strong>Authors:Alessandro Favero, Antonio Sclocchi, Matthieu Wyart</strong></p>
<p>Diffusion probabilistic models have become a cornerstone of modern generative AI, yet the mechanisms underlying their generalization remain poorly understood. In fact, if these models were perfectly minimizing their training loss, they would just generate data belonging to their training set, i.e., memorize, as empirically found in the overparameterized regime. We revisit this view by showing that, in highly overparameterized diffusion models, generalization in natural data domains is progressively achieved during training before the onset of memorization. Our results, ranging from image to language diffusion models, systematically support the empirical law that memorization time is proportional to the dataset size. Generalization vs. memorization is then best understood as a competition between time scales. We show that this phenomenology is recovered in diffusion models learning a simple probabilistic context-free grammar with random rules, where generalization corresponds to the hierarchical acquisition of deeper grammar rules as training time grows, and the generalization cost of early stopping can be characterized. We summarize these results in a phase diagram. Overall, our results support that a principled early-stopping criterion - scaling with dataset size - can effectively optimize generalization while avoiding memorization, with direct implications for hyperparameter transfer and privacy-sensitive applications. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹å·²æˆä¸ºç°ä»£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„åŸºçŸ³ï¼Œä½†å…¶èƒŒåçš„æ³›åŒ–æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘ã€‚äº‹å®ä¸Šï¼Œå¦‚æœè¿™äº›æ¨¡å‹èƒ½å¤Ÿå®Œç¾åœ°æœ€å°åŒ–å…¶è®­ç»ƒæŸå¤±ï¼Œå®ƒä»¬åªä¼šç”Ÿæˆå±äºå…¶è®­ç»ƒé›†çš„æ•°æ®ï¼Œå³è¿›è¡Œè®°å¿†ï¼Œæ­£å¦‚åœ¨è¶…å‚æ•°èŒƒå›´å†…æ‰€å‘ç°çš„é‚£æ ·ã€‚æˆ‘ä»¬é€šè¿‡å±•ç¤ºé«˜åº¦è¶…å‚æ•°çš„æ‰©æ•£æ¨¡å‹åœ¨è®°å¿†å¼€å§‹å‰ï¼Œåœ¨è‡ªç„¶æ•°æ®é¢†åŸŸæ³›åŒ–æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å®ç°çš„ï¼Œé‡æ–°å®¡è§†äº†è¿™ä¸€è§‚ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœæ¶µç›–äº†å›¾åƒåˆ°è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œç³»ç»Ÿåœ°æ”¯æŒäº†ç»éªŒæ³•åˆ™ï¼Œå³è®°å¿†æ—¶é—´ä¸æ•°æ®é›†å¤§å°æˆæ­£æ¯”ã€‚æ³›åŒ–ä¸è®°å¿†ä¹‹é—´çš„æœ€ä½³ç†è§£æ˜¯æ—¶é—´å°ºåº¦çš„ç«äº‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨æ‰©æ•£æ¨¡å‹å­¦ä¹ å…·æœ‰éšæœºè§„åˆ™çš„ç®€å•æ— ä¸Šä¸‹æ–‡æ¦‚ç‡è¯­æ³•æ—¶æ¢å¤äº†è¿™ç§ç°è±¡ï¼Œå…¶ä¸­æ³›åŒ–å¯¹åº”äºéšç€è®­ç»ƒæ—¶é—´çš„å¢é•¿å±‚æ¬¡åœ°è·å–æ›´æ·±çš„è¯­æ³•è§„åˆ™ï¼Œæ—©æœŸåœæ­¢çš„æ³›åŒ–æˆæœ¬å¯ä»¥å¾—åˆ°è¡¨å¾ã€‚æˆ‘ä»¬åœ¨ç›¸å›¾ä¸­æ€»ç»“äº†è¿™äº›ç»“æœã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœæ”¯æŒä¸€ä¸ªåŸåˆ™æ€§çš„æ—©æœŸåœæ­¢å‡†åˆ™â€”â€”ä¸æ•°æ®é›†å¤§å°æˆæ¯”ä¾‹â€”â€”å¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–æ³›åŒ–åŒæ—¶é¿å…è®°å¿†ï¼Œå¯¹è¶…å‚æ•°è½¬ç§»å’Œéšç§æ•æ„Ÿåº”ç”¨æœ‰ç›´æ¥çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹æ˜¯ç°ä»£ç”Ÿæˆäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒï¼Œä½†å…¶æ³›åŒ–æœºåˆ¶å°šä¸æ¸…æ¥šã€‚äº‹å®ä¸Šï¼Œå¦‚æœè¿™äº›æ¨¡å‹å®Œç¾æœ€å°åŒ–å…¶è®­ç»ƒæŸå¤±ï¼Œå®ƒä»¬åªä¼šç”Ÿæˆå±äºå…¶è®­ç»ƒé›†çš„æ•°æ®ï¼Œå³å­˜åœ¨è®°å¿†ç°è±¡ï¼Œè¿™åœ¨è¿‡åº¦å‚æ•°åŒ–çŠ¶æ€ä¸‹å·²è¢«ç»éªŒå‘ç°ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†è¿™ä¸€è§‚ç‚¹ï¼Œå‘ç°é«˜åº¦è¿‡åº¦å‚æ•°åŒ–çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œåœ¨è‡ªç„¶æ•°æ®é¢†åŸŸçš„æ³›åŒ–æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å®ç°çš„ï¼Œä¸”åœ¨è®°å¿†å‡ºç°ä¹‹å‰ã€‚æˆ‘ä»¬çš„ç»“æœä»å›¾åƒåˆ°è¯­è¨€æ‰©æ•£æ¨¡å‹éƒ½æœ‰ç³»ç»Ÿæ”¯æŒï¼Œå®è¯è§„å¾‹æ˜¯è®°å¿†æ—¶é—´ä¸æ•°æ®é›†å¤§å°æˆæ­£æ¯”ã€‚æ³›åŒ–ä¸è®°å¿†ä¹‹é—´çš„æœ€ä½³ç†è§£æ˜¯æ—¶é—´å°ºåº¦çš„ç«äº‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸€ç°è±¡åœ¨æ‰©æ•£æ¨¡å‹å­¦ä¹ ç®€å•çš„æ— ä¸Šä¸‹æ–‡æ¦‚ç‡è¯­æ³•ä¸éšæœºè§„åˆ™æ—¶å¾—ä»¥æ¢å¤ï¼Œå…¶ä¸­æ³›åŒ–å¯¹åº”äºéšç€è®­ç»ƒæ—¶é—´çš„å¢é•¿å±‚æ¬¡åœ°è·å–æ›´æ·±çš„è¯­æ³•è§„åˆ™ï¼Œæ—©æœŸåœæ­¢çš„æ³›åŒ–æˆæœ¬å¯ä»¥å¾—åˆ°ç‰¹å¾æè¿°ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœæ”¯æŒä¸€ä¸ªåŸåˆ™æ€§çš„æ—©æœŸåœæ­¢å‡†åˆ™â€”â€”ä¸æ•°æ®é›†å¤§å°æˆæ¯”ä¾‹â€”â€”å¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–æ³›åŒ–åŒæ—¶é¿å…è®°å¿†ç°è±¡ï¼Œå¯¹è¶…å‚æ•°è½¬ç§»å’Œéšç§æ•æ„Ÿåº”ç”¨æœ‰ç›´æ¥æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹æ˜¯ç°ä»£ç”ŸæˆAIçš„æ ¸å¿ƒï¼Œä½†å…¶æ³›åŒ–æœºåˆ¶å°šæœªå®Œå…¨ç†è§£ã€‚</li>
<li>åœ¨è¿‡åº¦å‚æ•°åŒ–çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ³›åŒ–è¿‡ç¨‹æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å®ç°çš„ï¼Œè€Œéä¸€å¼€å§‹å°±èƒ½è¾¾åˆ°ã€‚</li>
<li>è®°å¿†ç°è±¡åœ¨æ‰©æ•£æ¨¡å‹ä¸­ä¹Ÿå­˜åœ¨ï¼Œå°¤å…¶æ˜¯åœ¨è¿‡åº¦å‚æ•°åŒ–çŠ¶æ€ä¸‹ã€‚</li>
<li>è®°å¿†æ—¶é—´ä¸æ•°æ®é›†å¤§å°æˆæ­£æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªå®è¯è§„å¾‹ã€‚</li>
<li>æ³›åŒ–ä¸è®°å¿†ä¹‹é—´çš„å¹³è¡¡å¯ä»¥ç†è§£ä¸ºæ—¶é—´å°ºåº¦çš„ç«äº‰ã€‚</li>
<li>åœ¨å­¦ä¹ ç®€å•çš„æ— ä¸Šä¸‹æ–‡æ¦‚ç‡è¯­æ³•æ—¶ï¼Œæ‰©æ•£æ¨¡å‹çš„æ³›åŒ–è¿‡ç¨‹ä¸è·å–æ›´æ·±å±‚æ¬¡çš„è¯­æ³•è§„åˆ™ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4d6fbfa6b4bb33b7751c1961485380d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaf72d07e407191573008e2ed9ef3a8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-197aff0591f7a96f35c7f838ebe1b455.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb001027ae3b6b82ab6fbce4ec34bff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9467d7179362564865a235a8163f61b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Forward-only-Diffusion-Probabilistic-Models"><a href="#Forward-only-Diffusion-Probabilistic-Models" class="headerlink" title="Forward-only Diffusion Probabilistic Models"></a>Forward-only Diffusion Probabilistic Models</h2><p><strong>Authors:Ziwei Luo, Fredrik K. Gustafsson, Jens SjÃ¶lund, Thomas B. SchÃ¶n</strong></p>
<p>This work presents a forward-only diffusion (FoD) approach for generative modelling. In contrast to traditional diffusion models that rely on a coupled forward-backward diffusion scheme, FoD directly learns data generation through a single forward diffusion process, yielding a simple yet efficient generative framework. The core of FoD is a state-dependent linear stochastic differential equation that involves a mean-reverting term in both the drift and diffusion functions. This mean-reversion property guarantees the convergence to clean data, naturally simulating a stochastic interpolation between source and target distributions. More importantly, FoD is analytically tractable and is trained using a simple stochastic flow matching objective, enabling a few-step non-Markov chain sampling during inference. The proposed FoD model, despite its simplicity, achieves competitive performance on various image-conditioned (e.g., image restoration) and unconditional generation tasks, demonstrating its effectiveness in generative modelling. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD">https://github.com/Algolzw/FoD</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åªå‰å‘æ‰©æ•£ï¼ˆFoDï¼‰çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå»ºæ¨¡ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºè€¦åˆçš„å‰å‘-åå‘æ‰©æ•£æ–¹æ¡ˆçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒFoDé€šè¿‡å•ä¸€çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ç›´æ¥å­¦ä¹ æ•°æ®ç”Ÿæˆï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„ç”Ÿæˆæ¡†æ¶ã€‚FoDçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¸çŠ¶æ€ç›¸å…³çš„çº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹ï¼Œè¯¥æ–¹ç¨‹åœ¨æ¼‚ç§»å’Œæ‰©æ•£å‡½æ•°ä¸­æ¶‰åŠå‡å€¼å›å¤é¡¹ã€‚è¿™ç§å‡å€¼å›å½’å±æ€§ä¿è¯äº†å‘å¹²å‡€æ•°æ®çš„æ”¶æ•›ï¼Œè‡ªç„¶åœ°æ¨¡æ‹Ÿäº†æºåˆ†å¸ƒå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„éšæœºæ’å€¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒFoDåˆ†æä¸Šæ˜¯å¯è¡Œçš„ï¼Œå¹¶ä¸”ä½¿ç”¨ç®€å•çš„éšæœºæµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ¨ç†æœŸé—´å®ç°å‡ æ­¥éé©¬å°”å¯å¤«é“¾é‡‡æ ·ã€‚å°½ç®¡FoDæ¨¡å‹ç®€å•ï¼Œä½†åœ¨å„ç§å›¾åƒæ¡ä»¶ï¼ˆä¾‹å¦‚å›¾åƒæ¢å¤ï¼‰å’Œæ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD%E3%80%82">https://github.com/Algolzw/FoDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16733v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://algolzw.github.io/fod">https://algolzw.github.io/fod</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»…å‰å‘æ‰©æ•£ï¼ˆFoDï¼‰çš„æ–¹æ³•ç”¨äºç”Ÿæˆå»ºæ¨¡ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºè€¦åˆå‰å‘-åå‘æ‰©æ•£æ–¹æ¡ˆçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒFoDé€šè¿‡å•ä¸€çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ç›´æ¥å­¦ä¹ æ•°æ®ç”Ÿæˆï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´è€Œé«˜æ•ˆçš„ç”Ÿæˆæ¡†æ¶ã€‚FoDçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ¶‰åŠå‡å€¼åè½¬é¡¹çš„ä¾èµ–äºçŠ¶æ€çš„éçº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹ï¼Œè¿™ä¸€ç‰¹æ€§ä¿è¯äº†å‘æ¸…æ´æ•°æ®çš„æ”¶æ•›ï¼Œè‡ªç„¶åœ°æ¨¡æ‹Ÿäº†æºåˆ†å¸ƒå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„éšæœºæ’å€¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒFoDå…·æœ‰åˆ†æä¸Šçš„å¯è¿½è¸ªæ€§ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„éšæœºæµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ¨ç†æœŸé—´å®ç°å°‘æ•°éé©¬å°”å¯å¤«é“¾é‡‡æ ·æ­¥éª¤ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼Œæå‡ºçš„FoDæ¨¡å‹åœ¨å›¾åƒæ¡ä»¶ï¼ˆå¦‚å›¾åƒæ¢å¤ï¼‰å’Œæ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ä»…å‰å‘æ‰©æ•£ï¼ˆFoDï¼‰çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒFoDé€šè¿‡å•ä¸€çš„å‰å‘æ‰©æ•£è¿‡ç¨‹è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>FoDçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ¶‰åŠå‡å€¼åè½¬çš„ä¾èµ–äºçŠ¶æ€çš„éçº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹ã€‚</li>
<li>å‡å€¼åè½¬ç‰¹æ€§ä¿è¯äº†å‘æ¸…æ´æ•°æ®çš„æ”¶æ•›ï¼Œæ¨¡æ‹Ÿäº†æºåˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„éšæœºæ’å€¼ã€‚</li>
<li>FoDå…·æœ‰åˆ†æä¸Šçš„å¯è¿½è¸ªæ€§ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„éšæœºæµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚</li>
<li>FoDæ¨¡å‹åœ¨å›¾åƒæ¡ä»¶å’Œæ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-500104a85ae2895290af5871857a908a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ece37e8b928279052176b7dc1c75ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8295672b87fa1a9e83d8e9da885259d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fb188f309760a1e6248228d5f8f5aaa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM"><a href="#MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM" class="headerlink" title="MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM"></a>MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advances in static 3D generation have intensified the demand for physically consistent dynamic 3D content. However, existing video generation models, including diffusion-based methods, often prioritize visual realism while neglecting physical plausibility, resulting in implausible object dynamics. Prior approaches for physics-aware dynamic generation typically rely on large-scale annotated datasets or extensive model fine-tuning, which imposes significant computational and data collection burdens and limits scalability across scenarios. To address these challenges, we present MAGIC, a training-free framework for single-image physical property inference and dynamic generation, integrating pretrained image-to-video diffusion models with iterative LLM-based reasoning. Our framework generates motion-rich videos from a static image and closes the visual-to-physical gap through a confidence-driven LLM feedback loop that adaptively steers the diffusion model toward physics-relevant motion. To translate visual dynamics into controllable physical behavior, we further introduce a differentiable MPM simulator operating directly on 3D Gaussians reconstructed from the single image, enabling physically grounded, simulation-ready outputs without any supervision or model tuning. Experiments show that MAGIC outperforms existing physics-aware generative methods in inference accuracy and achieves greater temporal coherence than state-of-the-art video diffusion models. </p>
<blockquote>
<p>è¿‘æœŸé™æ€3Dç”Ÿæˆçš„è¿›å±•åŠ å‰§äº†å¯¹ç‰©ç†ä¸€è‡´æ€§åŠ¨æ€3Då†…å®¹çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œé€šå¸¸ä¼˜å…ˆè€ƒè™‘è§†è§‰çœŸå®æ€§è€Œå¿½è§†ç‰©ç†å¯è¡Œæ€§ï¼Œå¯¼è‡´ç‰©ä½“åŠ¨æ€ä¸å¯ä¿¡ã€‚ä¹‹å‰é’ˆå¯¹ç‰©ç†æ„ŸçŸ¥åŠ¨æ€ç”Ÿæˆçš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†æˆ–æ¨¡å‹ç²¾ç»†è°ƒæ•´ï¼Œè¿™å¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å’Œæ•°æ®æ”¶é›†è´Ÿæ‹…ï¼Œå¹¶é™åˆ¶äº†è·¨åœºæ™¯çš„æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MAGICï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•å›¾åƒç‰©ç†å±æ€§æ¨æ–­å’ŒåŠ¨æ€ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ä¸åŸºäºè¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»é™æ€å›¾åƒç”Ÿæˆè¿åŠ¨ä¸°å¯Œçš„è§†é¢‘ï¼Œå¹¶é€šè¿‡ä¿¡å¿ƒé©±åŠ¨çš„LLMåé¦ˆå¾ªç¯ç¼©å°è§†è§‰åˆ°ç‰©ç†çš„å·®è·ï¼Œè¯¥å¾ªç¯è‡ªé€‚åº”åœ°å¼•å¯¼æ‰©æ•£æ¨¡å‹æœå‘ç‰©ç†ç›¸å…³çš„è¿åŠ¨ã€‚ä¸ºäº†å°†è§†è§‰åŠ¨æ€è½¬åŒ–ä¸ºå¯æ§çš„ç‰©ç†è¡Œä¸ºï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªå¯ç›´æ¥åœ¨å•å›¾åƒé‡å»ºçš„3Dé«˜æ–¯ä¸Šè¿è¡Œçš„å¯å¾®åˆ†MPMæ¨¡æ‹Ÿå™¨ï¼Œæ— éœ€ç›‘ç£æˆ–æ¨¡å‹è°ƒæ•´å³å¯å®ç°ç‰©ç†åŸºç¡€ã€æ¨¡æ‹Ÿå°±ç»ªçš„è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒMAGICåœ¨æ¨ç†å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ—¶é—´è¿è´¯æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16456v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé™æ€å›¾åƒç”ŸæˆåŠ¨æ€å†…å®¹çš„ç‰©ç†ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹å¿½è§†ç‰©ç†åˆç†æ€§ï¼Œå¯¼è‡´ç‰©ä½“åŠ¨æ€ä¸åˆç†ã€‚MAGICæ¡†æ¶ç»“åˆé¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ä¸åŸºäºLLMçš„æ¨ç†ï¼Œæ— éœ€è®­ç»ƒå³å¯è¿›è¡Œå•å›¾åƒç‰©ç†å±æ€§æ¨æ–­å’ŒåŠ¨æ€ç”Ÿæˆã€‚é€šè¿‡ç½®ä¿¡åº¦é©±åŠ¨çš„LLMåé¦ˆå¾ªç¯å’Œå¯å¾®åˆ†çš„MPMæ¨¡æ‹Ÿå™¨ï¼ŒMAGICå®ç°äº†ä»é™æ€å›¾åƒç”Ÿæˆè¿åŠ¨ä¸°å¯Œè§†é¢‘ï¼Œå¹¶å…³é—­äº†è§†è§‰åˆ°ç‰©ç†çš„å·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒMAGICåœ¨æ¨ç†å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ—¶é—´è¿è´¯æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”ŸæˆåŠ¨æ€å†…å®¹æ—¶å¿½è§†äº†ç‰©ç†ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç‰©ä½“åŠ¨æ€ä¸åˆç†ã€‚</li>
<li>MAGICæ¡†æ¶æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºå•å›¾åƒç‰©ç†å±æ€§æ¨æ–­å’ŒåŠ¨æ€ç”Ÿæˆã€‚</li>
<li>MAGICç»“åˆäº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºLLMçš„æ¨ç†ã€‚</li>
<li>é€šè¿‡ç½®ä¿¡åº¦é©±åŠ¨çš„LLMåé¦ˆå¾ªç¯ï¼ŒMAGICå®ç°äº†è‡ªé€‚åº”å¼•å¯¼æ‰©æ•£æ¨¡å‹å‘ç‰©ç†ç›¸å…³è¿åŠ¨æ–¹å‘ã€‚</li>
<li>MAGICå¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†çš„MPMæ¨¡æ‹Ÿå™¨ï¼Œç›´æ¥åœ¨3Dé«˜æ–¯ä¸Šæ“ä½œï¼Œç”Ÿæˆç‰©ç†ä¸Šåˆç†ä¸”æ¨¡æ‹Ÿå°±ç»ªçš„è¾“å‡ºã€‚</li>
<li>æ— éœ€ç›‘ç£æˆ–æ¨¡å‹è°ƒæ•´ï¼ŒMAGICèƒ½å¤Ÿä»å•ä¸ªé™æ€å›¾åƒç”Ÿæˆç‰©ç†ä¸Šä¸°å¯Œçš„è§†é¢‘å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dca90b978eb12c54278e945f8392df79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e05670f0400652cee38bd0d733e0648d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20e9e24d2e0a0db7dcb33948ed816fa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-488a64befa1216189a3cae36eb973c80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43eded7d51517ad257d1da2db9033f82.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation"><a href="#Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation" class="headerlink" title="Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation"></a>Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation</h2><p><strong>Authors:Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Thomas Oberlin</strong></p>
<p>Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF). CACTI applies statistical normalization selectively based on semantic classes, while CACTIF further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes&#x2F;ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/echigot/cactif">https://github.com/echigot/cactif</a>. </p>
<blockquote>
<p>åˆ©ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šçš„è¡¨ç°å¾€å¾€è¾ƒå·®ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé¢†åŸŸå·®è·é€ æˆçš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºçš„ä¸åˆ©æ¡ä»¶ä¸‹ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨è¿™ç§æ‰©æ•£æ¨¡å‹æ¥æ”¹å–„åœ¨åˆæˆæ•°æ®ä¸Šå­¦ä¹ çš„è§†è§‰æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä»‹ç»ä¸¤ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¯­ä¹‰ä¸€è‡´é£æ ¼è½¬ç§»çš„æ–°æŠ€æœ¯ï¼šåŸºäºç±»åˆ«çš„è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–å’Œäº¤å‰æ³¨æ„åŠ›ï¼ˆCACTIï¼‰ï¼Œä»¥åŠåŸºäºé€‰æ‹©æ€§æ³¨æ„åŠ›è¿‡æ»¤çš„æ‰©å±•ï¼ˆCACTIFï¼‰ã€‚CACTIæ ¹æ®è¯­ä¹‰ç±»åˆ«æœ‰é€‰æ‹©åœ°åº”ç”¨ç»Ÿè®¡å½’ä¸€åŒ–ï¼Œè€ŒCACTIFåˆ™è¿›ä¸€æ­¥æ ¹æ®ç‰¹å¾ç›¸ä¼¼æ€§è¿‡æ»¤äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œé˜²æ­¢åœ¨äº¤å‰æ³¨æ„åŠ›å¯¹åº”è¾ƒå¼±çš„åŒºåŸŸå‡ºç°ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è½¬ç§»é£æ ¼ç‰¹å¾çš„åŒæ—¶ï¼Œä¿ç•™äº†è¯­ä¹‰è¾¹ç•Œå’Œç»“æ„ä¸€è‡´æ€§ï¼Œä¸åŒäºé‚£äº›åº”ç”¨å…¨å±€å˜æ¢æˆ–æ— çº¦æŸç”Ÿæˆå†…å®¹çš„æ–¹æ³•ã€‚ä½¿ç”¨GTA5ä½œä¸ºæºåŸŸï¼ŒCityscapes&#x2F;ACDCä½œä¸ºç›®æ ‡åŸŸçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†è´¨é‡æ›´é«˜ã€FIDå¾—åˆ†æ›´ä½ã€å†…å®¹ä¿å­˜æ›´å¥½çš„å›¾åƒã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œç±»æ„ŸçŸ¥æ‰©æ•£é£æ ¼è½¬ç§»æœ‰æ•ˆåœ°ç¼©çŸ­äº†åˆæˆåˆ°çœŸå®çš„é¢†åŸŸå·®è·ï¼Œå³ä½¿ç›®æ ‡é¢†åŸŸçš„æ•°æ®æœ€å°‘ï¼Œä¹Ÿä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œåº”ç”¨æä¾›äº†ç¨³å¥çš„æ„ŸçŸ¥ç³»ç»Ÿã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/echigot/cactif%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/echigot/cactifè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16360v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹åˆ©ç”¨ç”Ÿæˆå›¾åƒçš„æ–¹æ³•èƒ½æœ‰æ•ˆç¼©å‡åˆæˆå›¾åƒä¸ç°å®å›¾åƒä¹‹é—´çš„å·®è·ï¼Œæé«˜è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šçš„æ€§èƒ½ã€‚æ–‡ä¸­æå‡ºäº†ä¸¤ç§æ–°é¢–çš„è¯­ä¹‰ä¸€è‡´æ€§é£æ ¼è½¬ç§»æŠ€æœ¯â€”â€”åŸºäºåˆ†ç±»è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆCACTIï¼‰å’Œå…¶é€‰æ‹©æ€§æ³¨æ„åŠ›æ»¤æ³¢æ‰©å±•ï¼ˆCACTIFï¼‰ã€‚CACTIèƒ½å¤Ÿé’ˆå¯¹è¯­ä¹‰ç±»åˆ«é€‰æ‹©æ€§åº”ç”¨ç»Ÿè®¡å½’ä¸€åŒ–ï¼Œè€ŒCACTIFåˆ™è¿›ä¸€æ­¥æ ¹æ®ç‰¹å¾ç›¸ä¼¼æ€§è¿‡æ»¤äº¤å‰æ³¨æ„åŠ›æ˜ å°„ï¼Œé¿å…äº†å¼±äº¤å‰æ³¨æ„åŠ›å¯¹åº”åŒºåŸŸçš„ä¼ªå½±ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨GTA5ä½œä¸ºæºåŸŸå’ŒCityscapes&#x2F;ACDCä½œä¸ºç›®æ ‡åŸŸçš„åœºæ™¯ä¸‹ï¼Œç”Ÿæˆå›¾åƒè´¨é‡æ›´é«˜ã€FIDå¾—åˆ†æ›´ä½ä¸”å†…å®¹ä¿ç•™æ›´å¥½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºç±»åˆ«æ„ŸçŸ¥çš„æ‰©æ•£é£æ ¼è½¬ç§»å³ä½¿åœ¨ç›®æ ‡åŸŸæ•°æ®å¾ˆå°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆåœ°å¼¥åˆåˆæˆåŸŸä¸ç°å®åŸŸä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºåº”å¯¹æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œåº”ç”¨æä¾›äº†å…ˆè¿›çš„æ„ŸçŸ¥ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šçš„æ€§èƒ½å¯ä»¥é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ¥æé«˜ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§æ–°çš„è¯­ä¹‰ä¸€è‡´æ€§é£æ ¼è½¬ç§»æŠ€æœ¯ï¼šCACTIå’ŒCACTIFã€‚</li>
<li>CACTIèƒ½å¤Ÿé’ˆå¯¹è¯­ä¹‰ç±»åˆ«è¿›è¡Œé€‰æ‹©æ€§ç»Ÿè®¡å½’ä¸€åŒ–ã€‚</li>
<li>CACTIFé€šè¿‡è¿‡æ»¤äº¤å‰æ³¨æ„åŠ›æ˜ å°„æ¥é¿å…ä¼ªå½±ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼±äº¤å‰æ³¨æ„åŠ›çš„åŒºåŸŸã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒè´¨é‡ã€FIDå¾—åˆ†å’Œå†…å®¹ä¿ç•™æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ç¼©å°äº†åˆæˆåŸŸä¸ç°å®åŸŸä¹‹é—´çš„å·®è·ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨è¾ƒå°‘çš„çœŸå®ä¸–ç•Œæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4eccc3398cbfdd856b0f999f699f917d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7588621449cab3136b0e2f33ade893db.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FPQVAR-Floating-Point-Quantization-for-Visual-Autoregressive-Model-with-FPGA-Hardware-Co-design"><a href="#FPQVAR-Floating-Point-Quantization-for-Visual-Autoregressive-Model-with-FPGA-Hardware-Co-design" class="headerlink" title="FPQVAR: Floating Point Quantization for Visual Autoregressive Model with   FPGA Hardware Co-design"></a>FPQVAR: Floating Point Quantization for Visual Autoregressive Model with   FPGA Hardware Co-design</h2><p><strong>Authors:Renjie Wei, Songqiang Xu, Qingyu Guo, Meng Li</strong></p>
<p>Visual autoregressive (VAR) modeling has marked a paradigm shift in image generation from next-token prediction to next-scale prediction. VAR predicts a set of tokens at each step from coarse to fine scale, leading to better image quality and faster inference speed compared to existing diffusion models. However, the large parameter size and computation cost hinder its deployment on edge devices. To reduce the memory and computation cost, we propose FPQVAR, an efficient post-training floating-point (FP) quantization framework for VAR featuring algorithm and hardware co-design. At the algorithm level, we first identify the challenges of quantizing VAR. To address them, we propose Dual Format Quantization for the highly imbalanced input activation. We further propose Group-wise Hadamard Transformation and GHT-Aware Learnable Transformation to address the time-varying outlier channels. At the hardware level, we design the first low-bit FP quantizer and multiplier with lookup tables on FPGA and propose the first FPGA-based VAR accelerator featuring low-bit FP computation and an elaborate two-level pipeline. Extensive experiments show that compared to the state-of-the-art quantization method, our proposed FPQVAR significantly improves Fr&#39;echet Inception Distance (FID) from 10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit quantization. FPQVAR also significantly improves the performance of 6-bit quantized VAR, bringing it on par with the FP16 model. Our accelerator on AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image&#x2F;s, which is 3.1x higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x higher energy efficiency compared to the integer-based accelerator and GPU baseline, respectively. </p>
<blockquote>
<p>è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰å»ºæ¨¡æ ‡å¿—ç€å›¾åƒç”Ÿæˆä»ä¸‹ä¸€ä¸ªè¯é¢„æµ‹åˆ°ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹çš„èŒƒå¼è½¬å˜ã€‚VARé€šè¿‡ä»ç²—ç•¥åˆ°ç²¾ç»†å°ºåº¦çš„æ¯ä¸€æ­¥é¢„æµ‹ä¸€ç³»åˆ—æ ‡è®°ï¼ˆtokensï¼‰ï¼Œä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œäº§ç”Ÿäº†æ›´é«˜è´¨é‡çš„å›¾åƒå’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼Œå…¶åºå¤§çš„å‚æ•°å¤§å°å’Œè®¡ç®—æˆæœ¬é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚ä¸ºäº†é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†FPQVARï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºVARçš„åè®­ç»ƒæµ®ç‚¹ï¼ˆFPï¼‰é‡åŒ–æ¡†æ¶ï¼Œèåˆäº†ç®—æ³•å’Œç¡¬ä»¶ååŒè®¾è®¡ã€‚åœ¨ç®—æ³•å±‚é¢ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šäº†é‡åŒ–VARçš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºé«˜åº¦ä¸å¹³è¡¡è¾“å…¥æ¿€æ´»çš„åŒé‡æ ¼å¼é‡åŒ–ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åˆ†ç»„å“ˆè¾¾ç›å˜æ¢å’ŒGHTæ„ŸçŸ¥å¯å­¦ä¹ å˜æ¢ï¼Œä»¥è§£å†³æ—¶å˜å¼‚å¸¸é€šé“çš„é—®é¢˜ã€‚åœ¨ç¡¬ä»¶å±‚é¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºFPGAçš„é¦–ä¸ªä½ä½æµ®ç‚¹é‡åŒ–å’Œä¹˜æ³•å™¨æŸ¥æ‰¾è¡¨ï¼Œå¹¶æå‡ºäº†åŸºäºFPGAçš„VARåŠ é€Ÿå™¨ï¼Œè¯¥åŠ é€Ÿå™¨å…·æœ‰ä½ä½æµ®ç‚¹è®¡ç®—å’Œç²¾å¿ƒè®¾è®¡çš„ä¸¤çº§æµæ°´çº¿ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„FPQVARåœ¨4ä½é‡åŒ–ä¸‹å°†FrÃ©chet Inception Distance (FID)ä»10.83æ˜¾è‘—æ”¹è¿›åˆ°3.58ï¼ŒInception Score (IS)ä»175.9æé«˜åˆ°241.5ã€‚FPQVARè¿˜æ˜¾è‘—æé«˜äº†6ä½é‡åŒ–VARçš„æ€§èƒ½ï¼Œä½¿å…¶ä¸FP16æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬çš„åŠ é€Ÿå™¨åœ¨AMD-Xilinx VCK190 FPGAä¸Šçš„ååé‡è¾¾åˆ°1.1å›¾åƒ&#x2F;ç§’ï¼Œæ¯”æ•´æ•°åŠ é€Ÿå™¨é«˜å‡º3.1å€ã€‚å®ƒè¿˜è¡¨ç°å‡ºæ¯”æ•´æ•°åŠ é€Ÿå™¨å’ŒGPUåŸºå‡†æµ‹è¯•é«˜3.6å€å’Œ2.8å€çš„èƒ½æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16335v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå®ç°äº†ä»ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹åˆ°ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹çš„èŒƒå¼è½¬å˜ã€‚VARé€šè¿‡ä»ç²—åˆ°ç»†çš„å°ºåº¦é¢„æµ‹ä¸€ç³»åˆ—æ ‡è®°ï¼Œæé«˜äº†å›¾åƒè´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚ä¸ºé™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œä¾¿äºåœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼Œæœ¬æ–‡æå‡ºäº†FPQVARï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹VARçš„åè®­ç»ƒæµ®ç‚¹ï¼ˆFPï¼‰é‡åŒ–æ¡†æ¶ï¼Œèåˆäº†ç®—æ³•å’Œç¡¬ä»¶ååŒè®¾è®¡ã€‚åœ¨ç®—æ³•å±‚é¢ï¼Œæå‡ºåŒæ ¼å¼é‡åŒ–è§£å†³è¾“å…¥æ¿€æ´»çš„ä¸å¹³è¡¡é—®é¢˜ï¼Œä»¥åŠåˆ†ç»„å“ˆè¾¾ç›å˜æ¢å’ŒGHTæ„ŸçŸ¥å¯å­¦ä¹ å˜æ¢åº”å¯¹æ—¶å˜å¼‚å¸¸é€šé“ã€‚åœ¨ç¡¬ä»¶å±‚é¢ï¼Œè®¾è®¡äº†ä½æ¯”ç‰¹FPé‡åŒ–å’Œä¹˜æ³•å™¨ï¼Œé‡‡ç”¨FPGAæŸ¥æ‰¾è¡¨ï¼Œå¹¶æ¨å‡ºåŸºäºFPGAçš„VARåŠ é€Ÿå™¨ï¼Œå®ç°ä½æ¯”ç‰¹FPè®¡ç®—å’Œä¸¤çº§ç²¾ç»†ç®¡é“ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒFPQVARåœ¨4ä½é‡åŒ–ä¸‹å°†FrÃ©chet Inception Distance (FID)ä»10.83å¤§å¹…æ”¹å–„è‡³3.58ï¼ŒInception Score (IS)ä»175.9æå‡è‡³241.5ã€‚åœ¨6ä½é‡åŒ–æ–¹é¢ï¼ŒFPQVARçš„è¡¨ç°ä¸FP16æ¨¡å‹ç›¸å½“ã€‚åœ¨AMD-Xilinx VCK190 FPGAä¸Šï¼ŒåŠ é€Ÿå™¨ååé‡è¾¾1.1å›¾åƒ&#x2F;ç§’ï¼Œæ˜¯æ•´æ•°åŠ é€Ÿå™¨çš„3.1å€ã€‚æ­¤å¤–ï¼Œä¸æ•´æ•°åŠ é€Ÿå™¨å’ŒGPUåŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œå…¶èƒ½æ•ˆåˆ†åˆ«æé«˜äº†3.6å€å’Œ2.8å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹å®ç°äº†å›¾åƒç”Ÿæˆä¸­çš„å°ºåº¦é¢„æµ‹è½¬å˜ï¼Œä»ç²—åˆ°ç»†æé«˜å›¾åƒè´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
<li>FPQVARæ˜¯é¦–ä¸ªé’ˆå¯¹VARçš„åè®­ç»ƒæµ®ç‚¹ï¼ˆFPï¼‰é‡åŒ–æ¡†æ¶ï¼ŒåŒ…å«ç®—æ³•å’Œç¡¬ä»¶ååŒè®¾è®¡ã€‚</li>
<li>åœ¨ç®—æ³•å±‚é¢ï¼ŒFPQVARé€šè¿‡åŒæ ¼å¼é‡åŒ–è§£å†³è¾“å…¥æ¿€æ´»ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡åˆ†ç»„å“ˆè¾¾ç›å˜æ¢å’ŒGHTæ„ŸçŸ¥å¯å­¦ä¹ å˜æ¢åº”å¯¹å¼‚å¸¸é€šé“ã€‚</li>
<li>åœ¨ç¡¬ä»¶å±‚é¢ï¼Œè®¾è®¡äº†ä½æ¯”ç‰¹FPé‡åŒ–å’Œä¹˜æ³•å™¨ï¼Œå¹¶æ¨å‡ºåŸºäºFPGAçš„VARåŠ é€Ÿå™¨ã€‚</li>
<li>FPQVARæ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶åœ¨4ä½å’Œ6ä½é‡åŒ–æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFPQVARåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡FrÃ©chet Inception Distance (FID)å’ŒInception Score (IS)ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b2ef185e4d29a1e4925cd1b4ba9a8ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ebc388be20c9c4800a45be8f3e68291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e5a68494d2087937e18b5fac1355b35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01c15aa00c2857b4334e27d666006001.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ff93b107768bc8167668ddd22bf89cd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TRAIL-Transferable-Robust-Adversarial-Images-via-Latent-diffusion"><a href="#TRAIL-Transferable-Robust-Adversarial-Images-via-Latent-diffusion" class="headerlink" title="TRAIL: Transferable Robust Adversarial Images via Latent diffusion"></a>TRAIL: Transferable Robust Adversarial Images via Latent diffusion</h2><p><strong>Authors:Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, Cairong Zhao</strong></p>
<p>Adversarial attacks exploiting unrestricted natural perturbations present severe security risks to deep learning systems, yet their transferability across models remains limited due to distribution mismatches between generated adversarial features and real-world data. While recent works utilize pre-trained diffusion models as adversarial priors, they still encounter challenges due to the distribution shift between the distribution of ideal adversarial samples and the natural image distribution learned by the diffusion model. To address the challenge, we propose Transferable Robust Adversarial Images via Latent Diffusion (TRAIL), a test-time adaptation framework that enables the model to generate images from a distribution of images with adversarial features and closely resembles the target images. To mitigate the distribution shift, during attacks, TRAIL updates the diffusion U-Netâ€™s weights by combining adversarial objectives (to mislead victim models) and perceptual constraints (to preserve image realism). The adapted model then generates adversarial samples through iterative noise injection and denoising guided by these objectives. Experiments demonstrate that TRAIL significantly outperforms state-of-the-art methods in cross-model attack transferability, validating that distribution-aligned adversarial feature synthesis is critical for practical black-box attacks. </p>
<blockquote>
<p>åˆ©ç”¨ä¸å—é™åˆ¶çš„è‡ªç„¶æ‰°åŠ¨è¿›è¡Œçš„å¯¹æŠ—æ€§æ”»å‡»å¯¹æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ„æˆäº†ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆçš„å¯¹æŠ—æ€§ç‰¹å¾ä¸ç°å®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…ï¼Œè¿™äº›æ”»å‡»åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´çš„å¯è½¬ç§»æ€§ä»ç„¶æœ‰é™ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå¯¹æŠ—æ€§å…ˆéªŒï¼Œä½†ç”±äºç†æƒ³å¯¹æŠ—æ ·æœ¬çš„åˆ†å¸ƒä¸æ‰©æ•£æ¨¡å‹å­¦åˆ°çš„è‡ªç„¶å›¾åƒåˆ†å¸ƒä¹‹é—´å­˜åœ¨åˆ†å¸ƒåç§»ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡æ½œåœ¨æ‰©æ•£ç”Ÿæˆå¯è½¬ç§»ç¨³å¥å¯¹æŠ—å›¾åƒï¼ˆTRAILï¼‰çš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æµ‹è¯•æ—¶é—´é€‚åº”æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»å…·æœ‰å¯¹æŠ—ç‰¹å¾çš„å›¾åƒåˆ†å¸ƒä¸­ç”Ÿæˆå›¾åƒï¼Œå¹¶ç´§å¯†æ¨¡æ‹Ÿç›®æ ‡å›¾åƒã€‚ä¸ºäº†ç¼“è§£åˆ†å¸ƒåç§»ï¼Œåœ¨æ”»å‡»è¿‡ç¨‹ä¸­ï¼ŒTRAILé€šè¿‡ç»“åˆå¯¹æŠ—ç›®æ ‡ï¼ˆä»¥è¯¯å¯¼ç›®æ ‡æ¨¡å‹ï¼‰å’Œæ„ŸçŸ¥çº¦æŸï¼ˆä»¥ä¿æŒå›¾åƒçš„çœŸå®æ€§ï¼‰æ¥æ›´æ–°æ‰©æ•£U-Netçš„æƒé‡ã€‚é€‚åº”åçš„æ¨¡å‹ç„¶åé€šè¿‡è¿­ä»£å™ªå£°æ³¨å…¥å’Œå»å™ªï¼Œåœ¨è¿™äº›ç›®æ ‡çš„æŒ‡å¯¼ä¸‹ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒTRAILåœ¨è·¨æ¨¡å‹æ”»å‡»è½¬ç§»æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†ä¸å®é™…é»‘ç®±æ”»å‡»ä¸­å¯¹é½åˆ†å¸ƒçš„å¯¹æŠ—ç‰¹å¾åˆæˆçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16166v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTRAILçš„æµ‹è¯•æ—¶é€‚åº”æ€§æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå¯¹æŠ—æ€§å…ˆéªŒï¼Œç”Ÿæˆå…·æœ‰å¯¹æŠ—æ€§ç‰¹å¾çš„å›¾åƒåˆ†å¸ƒï¼Œå¹¶ç´§å¯†æ¨¡æ‹Ÿç›®æ ‡å›¾åƒã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¯¹æŠ—æ€§ç›®æ ‡å’Œæ„ŸçŸ¥çº¦æŸæ¥å‡è½»åˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œä»è€Œæ›´æ–°æ‰©æ•£U-Netçš„æƒé‡ã€‚å®éªŒè¯æ˜ï¼ŒTRAILåœ¨è·¨æ¨¡å‹æ”»å‡»è½¬ç§»æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†åˆ†å¸ƒå¯¹é½å¯¹æŠ—æ€§ç‰¹å¾åˆæˆå¯¹äºå®é™…é»‘ç›’æ”»å‡»çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ”»å‡»å¯¹æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ„æˆä¸¥é‡å®‰å…¨é£é™©ï¼Œä½†å…¶åœ¨æ¨¡å‹é—´çš„ä¼ è¾“æ€§å› ç”Ÿæˆå¯¹æŠ—ç‰¹å¾ä¸çœŸå®ä¸–ç•Œæ•°æ®åˆ†å¸ƒä¸åŒ¹é…è€Œå—é™ã€‚</li>
<li>è¿‘æœŸå·¥ä½œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå¯¹æŠ—æ€§å…ˆéªŒï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå³ç†æƒ³å¯¹æŠ—æ ·æœ¬åˆ†å¸ƒä¸æ‰©æ•£æ¨¡å‹å­¦åˆ°çš„è‡ªç„¶å›¾åƒåˆ†å¸ƒä¹‹é—´çš„åˆ†å¸ƒè½¬ç§»é—®é¢˜ã€‚</li>
<li>TRAILæ¡†æ¶é€šè¿‡æµ‹è¯•æ—¶é€‚åº”æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»å…·æœ‰å¯¹æŠ—æ€§ç‰¹å¾çš„å›¾åƒåˆ†å¸ƒä¸­ç”Ÿæˆå›¾åƒï¼Œå¹¶ç´§å¯†æ¨¡æ‹Ÿç›®æ ‡å›¾åƒã€‚</li>
<li>TRAILç»“åˆå¯¹æŠ—æ€§ç›®æ ‡å’Œæ„ŸçŸ¥çº¦æŸæ¥æ›´æ–°æ‰©æ•£U-Netçš„æƒé‡ï¼Œä»¥å‡è½»åˆ†å¸ƒè½¬ç§»é—®é¢˜ã€‚</li>
<li>å¯¹æŠ—æ€§ç›®æ ‡æ—¨åœ¨è¯¯å¯¼å—å®³è€…æ¨¡å‹ï¼Œè€Œæ„ŸçŸ¥çº¦æŸåˆ™æ—¨åœ¨ä¿æŒå›¾åƒçš„çœŸå®æ€§ã€‚</li>
<li>é€šè¿‡è¿­ä»£å™ªå£°æ³¨å…¥å’Œå»å™ªè¿‡ç¨‹ï¼Œç”Ÿæˆå—ç›®æ ‡å¼•å¯¼çš„å¯¹æŠ—æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a76653627e13f715c22b2f570a0783b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-288492c75d198da3796dffe18f5d214d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7cd05949d2a122ab3bc3c980323f5a9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OSCAR-One-Step-Diffusion-Codec-Across-Multiple-Bit-rates"><a href="#OSCAR-One-Step-Diffusion-Codec-Across-Multiple-Bit-rates" class="headerlink" title="OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates"></a>OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates</h2><p><strong>Authors:Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR">https://github.com/jp-guo/OSCAR</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æœ‰æŸå›¾åƒå‹ç¼©æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œè¿™å¾—ç›Šäºå…¶å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šè¿‡è¿­ä»£å»å™ªä»éšæœºå™ªå£°ä¸­é‡å»ºå›¾åƒï¼Œç”±å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºå¼•å¯¼ã€‚è™½ç„¶è¿™äº›æ–¹æ³•è¾¾åˆ°äº†è¾ƒé«˜çš„é‡å»ºè´¨é‡ï¼Œä½†å®ƒä»¬çš„å¤šæ­¥é‡‡æ ·è¿‡ç¨‹äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹ä¸åŒçš„å‹ç¼©æ¯”ç‰¹ç‡è®­ç»ƒä¸åŒçš„æ¨¡å‹ï¼Œä»è€Œå¯¼è‡´æ˜¾è‘—çš„è®­ç»ƒå’Œå­˜å‚¨æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨å¤šä¸ªæ¯”ç‰¹ç‡çš„æ‰©æ•£ç¼–è§£ç å™¨ï¼ˆOSCARï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å‹ç¼©æ½œåœ¨è¡¨ç¤ºè§†ä¸ºåŸå§‹æ½œåœ¨è¡¨ç¤ºçš„å™ªå£°ç‰ˆæœ¬ï¼Œå…¶ä¸­å¤±çœŸç¨‹åº¦å–å†³äºæ¯”ç‰¹ç‡ã€‚è¿™ä¸ªè§†è§’å…è®¸å°†å®ƒä»¬å»ºæ¨¡ä¸ºæ²¿æ‰©æ•£è½¨è¿¹çš„ä¸­é—´çŠ¶æ€ã€‚é€šè¿‡å»ºç«‹ä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶é—´æ­¥é•¿çš„æ˜ å°„ï¼Œæˆ‘ä»¬å°†å•ä¸€ç”Ÿæˆæ¨¡å‹è°ƒæ•´ä¸ºæ”¯æŒå¤šä¸ªæ¯”ç‰¹ç‡çš„é‡å»ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºå‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—ä¸€æ­¥å»å™ªæˆä¸ºå¯èƒ½ã€‚å› æ­¤ï¼ŒOSCARç”¨å•ä¸ªå»å™ªè¿‡ç¨‹å–ä»£äº†è¿­ä»£é‡‡æ ·ï¼Œå¤§å¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jp-guo/OSCARä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ— æŸå›¾åƒå‹ç¼©é¢†åŸŸå±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œå…¶ç”Ÿæˆå…ˆéªŒå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ç°æœ‰æ‰©æ•£æ–¹æ³•ä¸»è¦é€šè¿‡è¿­ä»£å»å™ªä»éšæœºå™ªå£°ä¸­é‡å»ºå›¾åƒï¼Œè¿™ä¸€è¿‡ç¨‹ç”±å‹ç¼©æ½œåœ¨è¡¨ç¤ºå¼•å¯¼ã€‚è™½ç„¶è¿™äº›æ–¹æ³•é‡å»ºè´¨é‡é«˜ï¼Œä½†å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å¸¦æ¥è¾ƒå¤§è®¡ç®—å¼€é”€ï¼Œä¸”é€šå¸¸éœ€è¦é’ˆå¯¹ä¸åŒå‹ç¼©æ¯”ç‰¹ç‡è®­ç»ƒä¸åŒæ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒå’Œå­˜å‚¨æˆæœ¬å¢åŠ ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§è·¨å¤šæ¯”ç‰¹ç‡çš„å•æ­¥æ‰©æ•£ç¼–è§£ç å™¨OSCARã€‚æˆ‘ä»¬å°†å‹ç¼©æ½œåœ¨è§†ä½œåŸå§‹æ½œåœ¨çš„å™ªå£°ç‰ˆæœ¬ï¼Œå…¶å¤±çœŸç¨‹åº¦å–å†³äºæ¯”ç‰¹ç‡ï¼Œå°†å…¶ä½œä¸ºæ‰©æ•£è½¨è¿¹çš„ä¸­é—´çŠ¶æ€å»ºæ¨¡ã€‚é€šè¿‡å»ºç«‹ä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶é—´æ­¥é•¿çš„æ˜ å°„ï¼Œæˆ‘ä»¬ä½¿å•ä¸€ç”Ÿæˆæ¨¡å‹æ”¯æŒå¤šæ¯”ç‰¹ç‡çš„é‡å»ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºå‹ç¼©æ½œåœ¨ä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä½¿å¾—ä¸€æ­¥å»å™ªæˆä¸ºå¯èƒ½ã€‚å› æ­¤ï¼ŒOSCARç”¨ä¸€æ­¥å»å™ªè¿‡ç¨‹æ›¿ä»£äº†è¿­ä»£é‡‡æ ·ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒOSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå‡å®ç°ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ— æŸå›¾åƒå‹ç¼©é¢†åŸŸå…·æœ‰å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ–¹æ³•å­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€éœ€è¦é’ˆå¯¹å¤šç§æ¯”ç‰¹ç‡è®­ç»ƒä¸åŒæ¨¡å‹çš„é—®é¢˜ã€‚</li>
<li>OSCARæå‡ºä¸€ç§è·¨å¤šæ¯”ç‰¹ç‡çš„å•æ­¥æ‰©æ•£ç¼–è§£ç å™¨æ–¹æ³•ã€‚</li>
<li>OSCARå°†å‹ç¼©æ½œåœ¨è§†ä½œåŸå§‹æ½œåœ¨çš„å™ªå£°ç‰ˆæœ¬ï¼Œå¹¶å»ºç«‹å…¶ä¸æ‰©æ•£è½¨è¿¹ä¸­é—´çŠ¶æ€çš„æ˜ å°„ã€‚</li>
<li>OSCARåˆ©ç”¨å•ä¸€ç”Ÿæˆæ¨¡å‹æ”¯æŒå¤šæ¯”ç‰¹ç‡çš„å›¾åƒé‡å»ºã€‚</li>
<li>OSCARé€šè¿‡ä¸€æ­¥å»å™ªè¿‡ç¨‹æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27d05c0c3c13fed805a463ff1eba9dfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fced3344ddec95fcdb6f16cc547286.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a6a289238a5a219cc6292763810bc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003e472d813eadd16c17e71dd2e022d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aebc4ce4acc0e0bb70f1e5729ef3800.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding"><a href="#MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding" class="headerlink" title="MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding"></a>MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding</h2><p><strong>Authors:Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun</strong></p>
<p>Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brainâ€™s high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: <a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain">https://github.com/yuxiangwei0808/MoRE-Brain</a>. </p>
<blockquote>
<p>ä»åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ä¸­è§£ç è§†è§‰ç»éªŒæä¾›äº†ä¸€ç§å¼ºå¤§çš„æ–¹æ³•æ¥ç†è§£äººç±»æ„ŸçŸ¥å¹¶å¼€å‘å…ˆè¿›çš„è„‘æœºæ¥å£ã€‚ç„¶è€Œï¼Œç›®å‰çš„è¿›å±•å¾€å¾€ä¼˜å…ˆæœ€å¤§åŒ–é‡å»ºä¿çœŸåº¦ï¼Œè€Œå¿½è§†äº†è§£é‡Šæ€§è¿™ä¸€å¯¹äºè·å–ç¥ç»ç§‘å­¦æ´å¯ŸåŠ›çš„å…³é”®æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MoRE-Brainï¼Œè¿™æ˜¯ä¸€ä¸ªç¥ç»å¯å‘çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸã€å¯é€‚åº”å’Œå¯è§£é‡Šçš„è§†è§‰é‡å»ºã€‚MoRE-Brainç‹¬ç‰¹åœ°é‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–çš„æ··åˆä¸“å®¶æ¶æ„ï¼Œå…¶ä¸­ä¸åŒçš„ä¸“å®¶å¤„ç†æ¥è‡ªåŠŸèƒ½ç›¸å…³ä½“ç´ ç»„çš„fMRIä¿¡å·ï¼Œæ¨¡ä»¿ä¸“é—¨çš„è„‘ç½‘ç»œã€‚ä¸“å®¶é¦–å…ˆè¢«è®­ç»ƒå°†fMRIç¼–ç åˆ°å›ºå®šçš„CLIPç©ºé—´ä¸­ã€‚ç„¶åï¼Œç»è¿‡å¾®è°ƒæ‰©æ•£æ¨¡å‹ç”±ä¸“å®¶è¾“å‡ºé€šè¿‡ä¸€ç§æ–°å‹çš„åŒé˜¶æ®µè·¯ç”±æœºåˆ¶åˆæˆå›¾åƒï¼Œè¯¥æœºåˆ¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€æƒè¡¡ä¸“å®¶çš„è´¡çŒ®ã€‚MoRE-Brainæä¾›äº†ä¸‰ä¸ªä¸»è¦è¿›æ­¥ï¼šé¦–å…ˆï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŸºäºè„‘ç½‘ç»œåŸç†çš„æ–°å‹æ··åˆä¸“å®¶æ¶æ„æ¥è¿›è¡Œç¥ç»è§£ç ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å…±äº«æ ¸å¿ƒä¸“å®¶ç½‘ç»œå¹¶ä»…é€‚åº”ç‰¹å®šäºä¸»é¢˜çš„è·¯ç”±å™¨ï¼Œå®ƒå®ç°äº†è·¨ä¸»é¢˜çš„æœ‰æ•ˆæ³›åŒ–ã€‚ç¬¬ä¸‰ï¼Œå®ƒæä¾›äº†å¢å¼ºçš„æœºæ¢°æ´å¯ŸåŠ›ï¼Œå› ä¸ºæ˜ç¡®çš„è·¯ç”±æœºåˆ¶æ­ç¤ºäº†ä¸åŒå»ºæ¨¡çš„è„‘åŒºåŸŸå¦‚ä½•å¡‘é€ é‡å»ºå›¾åƒçš„è¯­ä¹‰å’Œç©ºé—´å±æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†MoRE-Brainçš„é«˜é‡å»ºä¿çœŸåº¦ï¼Œç“¶é¢ˆåˆ†æè¿›ä¸€æ­¥è¯æ˜äº†å®ƒæœ‰æ•ˆåˆ©ç”¨fMRIä¿¡å·çš„èƒ½åŠ›ï¼ŒåŒºåˆ†äº†çœŸæ­£çš„ç¥ç»è§£ç å’Œè¿‡åº¦ä¾èµ–ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ã€‚å› æ­¤ï¼ŒMoRE-Brainæ ‡å¿—ç€æœç€æ›´å…·é€šç”¨æ€§å’Œå¯è§£é‡Šçš„åŸºäºfMRIçš„è§†è§‰è§£ç è¿ˆå‡ºäº†é‡å¤§çš„ä¸€æ­¥ã€‚ä»£ç å°†äºè¿‘æœŸåœ¨<a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/yuxiangwei0808/MoRE-Brainå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15946v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆ©ç”¨åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒè§£ç è§†è§‰ä½“éªŒæ˜¯ç†è§£äººç±»æ„ŸçŸ¥å’Œå‘å±•å…ˆè¿›è„‘æœºæ¥å£çš„æœ‰åŠ›é€”å¾„ã€‚å½“å‰æ–¹æ³•å¾€å¾€é‡è§†é‡å»ºä¿çœŸåº¦çš„æœ€å¤§åŒ–è€Œå¿½è§†äº†è§£é‡Šæ€§ï¼Œè¿™å¯¹äºè·å–ç¥ç»ç§‘å­¦æ´å¯Ÿè‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºMoRE-Brainæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸã€å¯é€‚åº”å’Œå¯è§£é‡Šçš„è§†è§‰é‡å»ºã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç¥ç»å¯å‘çš„å±‚æ¬¡åŒ–æ··åˆä¸“å®¶æ¶æ„ï¼Œå¤„ç†åŠŸèƒ½ç›¸å…³ä½“ç´ ç»„çš„fMRIä¿¡å·ï¼Œæ¨¡ä»¿ä¸“é—¨åŒ–çš„è„‘ç½‘ç»œã€‚ä¸“å®¶é¦–å…ˆè¢«è®­ç»ƒå°†fMRIç¼–ç åˆ°å›ºå®šçš„CLIPç©ºé—´ã€‚éšåé€šè¿‡å¾®è°ƒæ‰©æ•£æ¨¡å‹å¹¶å€ŸåŠ©æ–°é¢–çš„åŒé‡é˜¶æ®µè·¯ç”±æœºåˆ¶åˆæˆå›¾åƒï¼Œè¯¥æœºåˆ¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€æƒè¡¡ä¸“å®¶çš„è´¡çŒ®ã€‚MoRE-Brainå…·æœ‰ä¸‰é¡¹ä¸»è¦ä¼˜åŠ¿ï¼šä¸€æ˜¯å¼•å…¥åŸºäºè„‘ç½‘ç»œåŸç†çš„æ··åˆä¸“å®¶æ¶æ„ç”¨äºç¥ç»è§£ç ï¼›äºŒæ˜¯é€šè¿‡å…±äº«æ ¸å¿ƒä¸“å®¶ç½‘ç»œå¹¶ä»…é€‚åº”ç‰¹å®šä¸»ä½“è·¯ç”±å™¨æ¥å®ç°è·¨ä¸»ä½“é«˜æ•ˆæ¦‚æ‹¬ï¼›ä¸‰æ˜¯æä¾›å¢å¼ºæœºåˆ¶æ´å¯ŸåŠ›ï¼Œæ˜¾å¼è·¯ç”±æ­ç¤ºäº†ä¸åŒæ¨¡æ‹Ÿè„‘åŒºåŸŸå¦‚ä½•ç²¾ç¡®å¡‘é€ é‡å»ºå›¾åƒçš„è¯­ä¹‰å’Œç©ºé—´å±æ€§ã€‚å®éªŒéªŒè¯MoRE-Brainé«˜é‡å»ºä¿çœŸåº¦ï¼Œç“¶é¢ˆåˆ†æè¿›ä¸€æ­¥è¯æ˜å…¶æœ‰æ•ˆåˆ©ç”¨fMRIä¿¡å·ï¼ŒåŒºåˆ†çœŸæ­£çš„ç¥ç»è§£ç ä¸è¿‡åº¦ä¾èµ–ç”Ÿæˆå…ˆéªŒã€‚å› æ­¤ï¼ŒMoRE-Brainæ ‡å¿—ç€åœ¨æ›´å…·é€šç”¨æ€§å’Œå¯è§£é‡Šçš„fMRIè§†è§‰è§£ç æ–¹é¢å–å¾—é‡å¤§è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨è§£ç è§†è§‰ä½“éªŒçš„fMRIæ˜¯ç†è§£äººç±»æ„ŸçŸ¥å’Œå¼€å‘å…ˆè¿›è„‘æœºæ¥å£çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å½“å‰æ–¹æ³•åé‡äºé‡å»ºä¿çœŸåº¦è€Œå¿½è§†äº†è§£é‡Šæ€§ï¼ŒMoRE-Brainæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MoRE-Brainé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å¤„ç†fMRIä¿¡å·ï¼Œæ¨¡ä»¿è„‘ç½‘ç»œå·¥ä½œæ–¹å¼ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†é«˜ä¿çœŸã€å¯é€‚åº”å’Œå¯è§£é‡Šçš„è§†è§‰é‡å»ºã€‚</li>
<li>MoRE-Brainå…·æœ‰è·¨ä¸»ä½“æ¦‚æ‹¬èƒ½åŠ›ï¼Œé€šè¿‡å…±äº«æ ¸å¿ƒä¸“å®¶ç½‘ç»œå¹¶é€‚åº”ç‰¹å®šä¸»ä½“è·¯ç”±å™¨å®ç°ã€‚</li>
<li>æ¡†æ¶æä¾›äº†å¢å¼ºçš„æœºåˆ¶æ´å¯ŸåŠ›ï¼Œå±•ç¤ºäº†ä¸åŒæ¨¡æ‹Ÿè„‘åŒºåŸŸå¦‚ä½•å½±å“é‡å»ºå›¾åƒçš„è¯­ä¹‰å’Œç©ºé—´å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11a3eaa79b0d03650067200e1b8c02ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc92da07951ccfe9eb89d2a0344d7a5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-645e01c92190ccd6f4f3b22d1c901e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de6a61ae5f5f1b4e04a184b09a34118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669d4398a30ac8aa316d07615f14e37b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-the-Powerful-Attention-of-a-Pre-trained-Diffusion-Model-for-Exemplar-based-Image-Colorization"><a href="#Leveraging-the-Powerful-Attention-of-a-Pre-trained-Diffusion-Model-for-Exemplar-based-Image-Colorization" class="headerlink" title="Leveraging the Powerful Attention of a Pre-trained Diffusion Model for   Exemplar-based Image Colorization"></a>Leveraging the Powerful Attention of a Pre-trained Diffusion Model for   Exemplar-based Image Colorization</h2><p><strong>Authors:Satoshi Kosugi</strong></p>
<p>Exemplar-based image colorization aims to colorize a grayscale image using a reference color image, ensuring that reference colors are applied to corresponding input regions based on their semantic similarity. To achieve accurate semantic matching between regions, we leverage the self-attention module of a pre-trained diffusion model, which is trained on a large dataset and exhibits powerful attention capabilities. To harness this power, we propose a novel, fine-tuning-free approach based on a pre-trained diffusion model, making two key contributions. First, we introduce dual attention-guided color transfer. We utilize the self-attention module to compute an attention map between the input and reference images, effectively capturing semantic correspondences. The color features from the reference image is then transferred to the semantically matching regions of the input image, guided by this attention map, and finally, the grayscale features are replaced with the corresponding color features. Notably, we utilize dual attention to calculate attention maps separately for the grayscale and color images, achieving more precise semantic alignment. Second, we propose classifier-free colorization guidance, which enhances the transferred colors by combining color-transferred and non-color-transferred outputs. This process improves the quality of colorization. Our experimental results demonstrate that our method outperforms existing techniques in terms of image quality and fidelity to the reference. Specifically, we use 335 input-reference pairs from previous research, achieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to the reference). Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/satoshi-kosugi/powerful-attention">https://github.com/satoshi-kosugi/powerful-attention</a>. </p>
<blockquote>
<p>åŸºäºèŒƒä¾‹çš„å›¾åƒå½©è‰²åŒ–æ—¨åœ¨ä½¿ç”¨å‚è€ƒå½©è‰²å›¾åƒå¯¹ç°åº¦å›¾åƒè¿›è¡Œå½©è‰²åŒ–ï¼Œç¡®ä¿å‚è€ƒé¢œè‰²æ ¹æ®è¯­ä¹‰ç›¸ä¼¼æ€§åº”ç”¨äºç›¸åº”çš„è¾“å…¥åŒºåŸŸã€‚ä¸ºäº†å®ç°åŒºåŸŸä¹‹é—´çš„å‡†ç¡®è¯­ä¹‰åŒ¹é…ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³¨æ„åŠ›åŠŸèƒ½ã€‚ä¸ºäº†åˆ©ç”¨è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œåšå‡ºä¸¤ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥åŒé‡æ³¨æ„åŠ›å¼•å¯¼è‰²å½©è½¬ç§»ã€‚æˆ‘ä»¬åˆ©ç”¨è‡ªæ³¨æ„åŠ›æ¨¡å—è®¡ç®—è¾“å…¥å›¾åƒå’Œå‚è€ƒå›¾åƒä¹‹é—´çš„æ³¨æ„åŠ›å›¾ï¼Œæœ‰æ•ˆåœ°æ•æ‰è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚ç„¶åï¼Œæ¥è‡ªå‚è€ƒå›¾åƒçš„é¢œè‰²ç‰¹å¾è¢«è½¬ç§»åˆ°è¾“å…¥å›¾åƒçš„è¯­ä¹‰åŒ¹é…åŒºåŸŸï¼Œç”±æ³¨æ„åŠ›å›¾å¼•å¯¼ï¼Œæœ€åï¼Œç°åº¦ç‰¹å¾è¢«ç›¸åº”çš„é¢œè‰²ç‰¹å¾æ‰€æ›¿ä»£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒé‡æ³¨æ„åŠ›åˆ†åˆ«ä¸ºç°åº¦å›¾åƒå’Œå½©è‰²å›¾åƒè®¡ç®—æ³¨æ„åŠ›å›¾ï¼Œå®ç°æ›´ç²¾ç¡®çš„è¯­ä¹‰å¯¹é½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†æ— åˆ†ç±»è‰²å½©åŒ–æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè‰²å½©è½¬ç§»å’Œéè‰²å½©è½¬ç§»çš„è¾“å‡ºï¼Œå¢å¼ºè½¬ç§»çš„è‰²å½©ã€‚è¿™ä¸€è¿‡ç¨‹æé«˜äº†å½©è‰²åŒ–çš„è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œå‚è€ƒå¿ å®åº¦æ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨335ä¸ªè¾“å…¥-å‚è€ƒå¯¹æ¥è‡ªä¹‹å‰çš„ç ”ç©¶ï¼Œè¾¾åˆ°FIDï¼ˆå›¾åƒè´¨é‡ï¼‰ä¸º95.27ï¼ŒSI-FIDï¼ˆå¯¹å‚è€ƒçš„å¿ å®åº¦ï¼‰ä¸º5.51ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/satoshi-kosugi/powerful-attention%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/satoshi-kosugi/powerful-attentionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15812v1">PDF</a> Accepted to IEEE Transactions on Circuits and Systems for Video   Technology (TCSVT)</p>
<p><strong>Summary</strong><br>å½©è‰²åŒ–å›¾åƒçš„ç›®æ ‡æ˜¯åˆ©ç”¨å‚è€ƒå½©è‰²å›¾åƒå¯¹ç°åº¦å›¾åƒè¿›è¡Œç€è‰²ï¼Œç¡®ä¿å‚è€ƒé¢œè‰²æ ¹æ®è¯­ä¹‰ç›¸ä¼¼æ€§åº”ç”¨äºå¯¹åº”çš„è¾“å…¥åŒºåŸŸã€‚ä¸ºäº†å®ç°åŒºåŸŸä¹‹é—´çš„å‡†ç¡®è¯­ä¹‰åŒ¹é…ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³¨æ„åŠ›åŠŸèƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ— éœ€å¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åšå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒæ³¨æ„åŠ›å¼•å¯¼è‰²å½©è½¬ç§»ã€‚åˆ©ç”¨è‡ªæ³¨æ„åŠ›æ¨¡å—è®¡ç®—è¾“å…¥å›¾åƒå’Œå‚è€ƒå›¾åƒä¹‹é—´çš„æ³¨æ„åŠ›å›¾ï¼Œæœ‰æ•ˆåœ°æ•æ‰è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚å‚è€ƒå›¾åƒçš„é¢œè‰²ç‰¹å¾è¢«è½¬ç§»åˆ°è¾“å…¥å›¾åƒçš„è¯­ä¹‰åŒ¹é…åŒºåŸŸï¼Œç”±æ³¨æ„åŠ›å›¾å¼•å¯¼ï¼Œæœ€åç°åº¦ç‰¹å¾è¢«ç›¸åº”çš„é¢œè‰²ç‰¹å¾æ‰€æ›¿ä»£ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†æ— åˆ†ç±»è‰²å½©åŒ–æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè‰²å½©è½¬ç§»å’Œéè‰²å½©è½¬ç§»çš„è¾“å‡ºï¼Œæé«˜äº†è½¬ç§»é¢œè‰²çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œä¿æŒå¯¹å‚è€ƒå›¾åƒçš„å¿ å®åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬ä»ä¹‹å‰çš„ç ”ç©¶ä¸­ä½¿ç”¨äº†335ä¸ªè¾“å…¥-å‚è€ƒå¯¹ï¼Œå®ç°äº†95.27çš„FIDï¼ˆå›¾åƒè´¨é‡ï¼‰å’Œ5.51çš„SI-FIDï¼ˆå¯¹å‚è€ƒçš„å¿ å®åº¦ï¼‰ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/satoshi-kosugi/powerful-attention%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/satoshi-kosugi/powerful-attentionä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–¹æ³•ä½¿ç”¨å‚è€ƒå½©è‰²å›¾åƒæ¥ä¸ºç°åº¦å›¾åƒç€è‰²ï¼Œå¹¶åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§åŒ¹é…è¾“å…¥åŒºåŸŸå’Œå‚è€ƒé¢œè‰²ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å—å®ç°ç²¾ç¡®è¯­ä¹‰åŒ¹é…ã€‚</li>
<li>æå‡ºæ— éœ€å¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨åŒæ³¨æ„åŠ›å¼•å¯¼è‰²å½©è½¬ç§»ï¼Œé€šè¿‡æ³¨æ„åŠ›å›¾æ•æ‰è¯­ä¹‰å¯¹åº”å…³ç³»å¹¶å®ç°é¢œè‰²è½¬ç§»ã€‚</li>
<li>å¼•å…¥æ— åˆ†ç±»è‰²å½©åŒ–æŒ‡å¯¼æ–¹æ³•ï¼Œç»“åˆè‰²å½©è½¬ç§»å’Œéè‰²å½©è½¬ç§»çš„è¾“å‡ºæé«˜é¢œè‰²è´¨é‡ã€‚</li>
<li>æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œå¿ å®äºå‚è€ƒæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨335ä¸ªè¾“å…¥-å‚è€ƒå¯¹è¿›è¡Œå®éªŒéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-385ccfc26b349eb4a8991786c8e57cd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d58f2391042a0ab431dc65dbbfb8167.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbe000b67859beaf5b789df013601915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4686b4d3a5d66eb28bf880cf8711083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3509b13a9a3602040e8215af07ba2b04.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FragFake-A-Dataset-for-Fine-Grained-Detection-of-Edited-Images-with-Vision-Language-Models"><a href="#FragFake-A-Dataset-for-Fine-Grained-Detection-of-Edited-Images-with-Vision-Language-Models" class="headerlink" title="FragFake: A Dataset for Fine-Grained Detection of Edited Images with   Vision Language Models"></a>FragFake: A Dataset for Fine-Grained Detection of Edited Images with   Vision Language Models</h2><p><strong>Authors:Zhen Sun, Ziyi Zhang, Zeren Luo, Zeyang Sha, Tianshuo Cong, Zheng Li, Shiwen Cui, Weiqiang Wang, Jiaheng Wei, Xinlei He, Qi Li, Qian Wang</strong></p>
<p>Fine-grained edited image detection of localized edits in images is crucial for assessing content authenticity, especially given that modern diffusion models and image editing methods can produce highly realistic manipulations. However, this domain faces three challenges: (1) Binary classifiers yield only a global real-or-fake label without providing localization; (2) Traditional computer vision methods often rely on costly pixel-level annotations; and (3) No large-scale, high-quality dataset exists for modern image-editing detection techniques. To address these gaps, we develop an automated data-generation pipeline to create FragFake, the first dedicated benchmark dataset for edited image detection, which includes high-quality images from diverse editing models and a wide variety of edited objects. Based on FragFake, we utilize Vision Language Models (VLMs) for the first time in the task of edited image classification and edited region localization. Experimental results show that fine-tuned VLMs achieve higher average Object Precision across all datasets, significantly outperforming pretrained models. We further conduct ablation and transferability analyses to evaluate the detectors across various configurations and editing scenarios. To the best of our knowledge, this work is the first to reformulate localized image edit detection as a vision-language understanding task, establishing a new paradigm for the field. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity. </p>
<blockquote>
<p>ç²¾ç»†ç¼–è¾‘å›¾åƒçš„å›¾åƒæ£€æµ‹å¯¹äºè¯„ä¼°å†…å®¹çœŸå®æ€§è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°ç°ä»£æ‰©æ•£æ¨¡å‹å’Œå›¾åƒç¼–è¾‘æ–¹æ³•å¯ä»¥äº§ç”Ÿé«˜åº¦é€¼çœŸçš„æ“çºµã€‚ç„¶è€Œï¼Œæ­¤é¢†åŸŸé¢ä¸´ä¸‰ä¸ªæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰äºŒå…ƒåˆ†ç±»å™¨åªäº§ç”Ÿå…¨å±€çœŸå®æˆ–è™šå‡çš„æ ‡ç­¾ï¼Œæ— æ³•æä¾›å®šä½ï¼›ï¼ˆ2ï¼‰ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„åƒç´ çº§æ³¨é‡Šï¼›ï¼ˆ3ï¼‰å¯¹äºç°ä»£å›¾åƒç¼–è¾‘æ£€æµ‹æŠ€æœ¯ï¼Œå°šä¸å­˜åœ¨å¤§è§„æ¨¡é«˜è´¨é‡çš„æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ•°æ®ç”Ÿæˆç®¡é“æ¥åˆ›å»ºFragFakeï¼Œè¿™æ˜¯ä¸“é—¨ç”¨äºç¼–è¾‘å›¾åƒæ£€æµ‹çš„ç¬¬ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¥è‡ªå„ç§ç¼–è¾‘æ¨¡å‹çš„é«˜è´¨é‡å›¾åƒå’Œå¤šç§ç¼–è¾‘å¯¹è±¡ã€‚åŸºäºFragFakeï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨ç¼–è¾‘å›¾åƒåˆ†ç±»å’Œç¼–è¾‘åŒºåŸŸå®šä½ä»»åŠ¡ä¸­ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„VLMsåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ›´é«˜çš„å¯¹è±¡ç²¾åº¦å¹³å‡å€¼ï¼Œæ˜¾è‘—ä¼˜äºé¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œäº†æ¶ˆèå’Œå¯è½¬ç§»æ€§åˆ†æï¼Œä»¥è¯„ä¼°æ£€æµ‹å™¨åœ¨å„ç§é…ç½®å’Œç¼–è¾‘åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡å°†å±€éƒ¨å›¾åƒç¼–è¾‘æ£€æµ‹é‡æ–°å®šä½ä¸ºè§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ï¼Œä¸ºè¯¥é¢†åŸŸå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬é¢„è®¡è¿™é¡¹å·¥ä½œå°†ä¸ºå¤šæ¨¡å¼å†…å®¹çœŸå®æ€§çš„åç»­ç ”ç©¶å·¥ä½œå’Œæä¾›çµæ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15644v1">PDF</a> 14pages,15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒç¼–è¾‘æ£€æµ‹çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åˆ†ç±»å™¨æ— æ³•æä¾›å±€éƒ¨åŒ–ä¿¡æ¯ã€ä¾èµ–é«˜æˆæœ¬åƒç´ çº§æ ‡æ³¨ä»¥åŠç¼ºä¹é’ˆå¯¹ç°ä»£å›¾åƒç¼–è¾‘æ£€æµ‹æŠ€æœ¯çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ›å»ºäº†FragFakeæ•°æ®é›†ï¼Œç”¨äºç¼–è¾‘å›¾åƒæ£€æµ‹ä»»åŠ¡ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶é¦–æ¬¡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå›¾åƒåˆ†ç±»å’Œç¼–è¾‘åŒºåŸŸå®šä½ä»»åŠ¡ï¼Œå¹¶å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚æ­¤ç ”ç©¶å¼€åˆ›æ€§åœ°å°†å…¶æ”¹é©ä¸ºè§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ï¼Œä¸ºé¢†åŸŸä¸­çš„åç»­ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°æ–¹å‘ã€‚åŒæ—¶å¼ºè°ƒäº†å…¶å¯¹å¤šåª’ä½“å†…å®¹çœŸå®æ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å›¾åƒç¼–è¾‘æ£€æµ‹å¯¹äºè¯„ä¼°å†…å®¹çœŸå®æ€§è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°ç°ä»£æ‰©æ•£æ¨¡å‹å’Œå›¾åƒç¼–è¾‘æ–¹æ³•å¯ä»¥äº§ç”Ÿé«˜åº¦é€¼çœŸçš„æ“ä½œã€‚</li>
<li>æ­¤é¢†åŸŸé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šåˆ†ç±»å™¨æ— æ³•å®šä½ç¼–è¾‘åŒºåŸŸã€ä¾èµ–æ˜‚è´µçš„åƒç´ çº§æ ‡æ³¨ä»¥åŠç¼ºä¹é’ˆå¯¹ç°ä»£å›¾åƒç¼–è¾‘æ£€æµ‹çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ã€‚</li>
<li>æå‡ºå¹¶åˆ›å»ºFragFakeæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç¼–è¾‘å›¾åƒæ£€æµ‹ä»»åŠ¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«ä»ä¸åŒç¼–è¾‘æ¨¡å‹ç”Ÿæˆçš„å„ç§é«˜è´¨é‡å›¾åƒå’Œå¤šç§ç¼–è¾‘å¯¹è±¡ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå›¾åƒåˆ†ç±»å’Œç¼–è¾‘åŒºåŸŸå®šä½ä»»åŠ¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºé¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14c0ddc86670c7fe40557d6bbf4dc343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b49e6db7e3eb8119d7dbc26277c15976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b23a308e67ef6ef1eac44bac15c2e27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5accf227b98694b62e15ab50f2bfeeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c38c19aeab800a32aab717466b7010dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edb6243ab1bfa227035f00c5b33d83e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39956ae9bf413249bcd9d01843bea9b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis"><a href="#Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis" class="headerlink" title="Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis"></a>Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis</h2><p><strong>Authors:Zichuan Liu, Liming Jiang, Qing Yan, Yumin Jia, Hao Kang, Xin Lu</strong></p>
<p>We propose a novel framework for ID-preserving generation using a multi-modal encoding strategy rather than injecting identity features via adapters into pre-trained models. Our method treats identity and text as a unified conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal encoder that learns a joint embedding space for both identity and textual semantics. Given a reference face and a text prompt, FaceCLIP produces a unified representation that encodes both identity and text, which conditions a base diffusion model to generate images that are identity-consistent and text-aligned. We also present a multi-modal alignment algorithm to train FaceCLIP, using a loss that aligns its joint representation with face, text, and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL). Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait generation with better identity preservation and textual relevance. Extensive experiments demonstrate its quantitative and qualitative superiority. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ï¼Œè€Œä¸æ˜¯é€šè¿‡é€‚é…å™¨å‘é¢„è®­ç»ƒæ¨¡å‹æ³¨å…¥èº«ä»½ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†èº«ä»½å’Œæ–‡æœ¬è§†ä¸ºç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†FaceCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå®ƒå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç»™å®šå‚è€ƒäººè„¸å’Œæ–‡æœ¬æç¤ºï¼ŒFaceCLIPç”Ÿæˆä¸€ä¸ªç»Ÿä¸€è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºç¼–ç èº«ä»½å’Œæ–‡æœ¬ï¼Œä½¿åŸºç¡€æ‰©æ•£æ¨¡å‹ç”Ÿæˆèº«ä»½ä¸€è‡´ä¸”æ–‡æœ¬å¯¹é½çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨æŸå¤±å‡½æ•°å¯¹é½å…¶è”åˆè¡¨ç¤ºä¸é¢éƒ¨ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´çš„å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPã€‚ç„¶åï¼Œæˆ‘ä»¬å°†FaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ç›¸ç»“åˆï¼Œæ„å»ºäº†FaceCLIP-SDXLï¼Œä¸€ä¸ªIDä¿ç•™å›¾åƒåˆæˆç®¡é“ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFaceCLIP-SDXLèƒ½å¤Ÿå®ç°å…·æœ‰æ›´å¥½èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§çš„ç…§ç‰‡çº§è‚–åƒç”Ÿæˆã€‚å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨æ•°é‡å’Œè´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14202v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ï¼Œè€Œéé€šè¿‡é€‚é…å™¨æ³¨å…¥èº«ä»½ç‰¹å¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•å°†èº«ä»½å’Œæ–‡æœ¬è§†ä¸ºç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†FaceCLIPå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç»™å®šå‚è€ƒäººè„¸å’Œæ–‡æœ¬æç¤ºï¼ŒFaceCLIPäº§ç”Ÿç»Ÿä¸€è¡¨ç¤ºï¼Œç¼–ç èº«ä»½å’Œæ–‡æœ¬ï¼Œä»¥æ¡ä»¶åŸºç¡€æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´èº«ä»½å’Œæ–‡æœ¬å¯¹é½çš„å›¾åƒã€‚è¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPï¼Œä½¿ç”¨æŸå¤±å‡½æ•°å°†å…¶è”åˆè¡¨ç¤ºä¸é¢éƒ¨ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´å¯¹é½ã€‚é€šè¿‡ä¸Stable Diffusion XLï¼ˆSDXLï¼‰é›†æˆï¼Œæ„å»ºäº†FaceCLIP-SDXLèº«ä»½ä¿ç•™å›¾åƒåˆæˆç®¡é“ï¼Œå®ç°äº†å…·æœ‰æ›´å¥½èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§çš„é€¼çœŸè‚–åƒç”Ÿæˆã€‚å®éªŒè¯æ˜å…¶å®šé‡å’Œå®šæ€§ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ã€‚</li>
<li>æå‡ºäº†FaceCLIPå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå¯ä»¥å­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚</li>
<li>FaceCLIPèƒ½å¤Ÿäº§ç”Ÿç»Ÿä¸€è¡¨ç¤ºï¼Œç¼–ç èº«ä»½å’Œæ–‡æœ¬ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ¡ä»¶ã€‚</li>
<li>ä»‹ç»äº†å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPï¼Œé€šè¿‡æŸå¤±å‡½æ•°å®ç°é¢éƒ¨ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´çš„å¯¹é½ã€‚</li>
<li>æ„å»ºäº†FaceCLIP-SDXLèº«ä»½ä¿ç•™å›¾åƒåˆæˆç®¡é“ï¼Œé›†æˆäº†FaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°é€¼çœŸè‚–åƒç”Ÿæˆï¼Œå…·æœ‰æ›´å¥½çš„èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3020f3125643b578602c03193dac6380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f013279aa724a6863bd25a4f96d2b176.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Split-Gibbs-Discrete-Diffusion-Posterior-Sampling"><a href="#Split-Gibbs-Discrete-Diffusion-Posterior-Sampling" class="headerlink" title="Split Gibbs Discrete Diffusion Posterior Sampling"></a>Split Gibbs Discrete Diffusion Posterior Sampling</h2><p><strong>Authors:Wenda Chu, Zihui Wu, Yifan Chen, Yang Song, Yisong Yue</strong></p>
<p>We study the problem of posterior sampling in discrete-state spaces using discrete diffusion models. While posterior sampling methods for continuous diffusion models have achieved remarkable progress, analogous methods for discrete diffusion models remain challenging. In this work, we introduce a principled plug-and-play discrete diffusion posterior sampling algorithm based on split Gibbs sampling, which we call SGDD. Our algorithm enables reward-guided generation and solving inverse problems in discrete-state spaces. We demonstrate the convergence of SGDD to the target posterior distribution and verify this through controlled experiments on synthetic benchmarks. Our method enjoys state-of-the-art posterior sampling performance on a range of benchmarks for discrete data, including DNA sequence design, discrete image inverse problems, and music infilling, achieving more than 30% improved performance compared to existing baselines. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­åé‡‡æ ·çš„é—®é¢˜ï¼Œä½¿ç”¨äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚è™½ç„¶è¿ç»­æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºåˆ†å‰²Gibbsé‡‡æ ·å¼•å…¥äº†ä¸€ç§åŸåˆ™æ€§çš„å³æ’å³ç”¨ç¦»æ•£æ‰©æ•£åé‡‡æ ·ç®—æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºSGDDã€‚æˆ‘ä»¬çš„ç®—æ³•èƒ½å¤Ÿå®ç°å¥–åŠ±å¼•å¯¼ç”Ÿæˆå’Œç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„åé—®é¢˜æ±‚è§£ã€‚æˆ‘ä»¬è¯æ˜äº†SGDDèƒ½å¤Ÿæ”¶æ•›åˆ°ç›®æ ‡åéªŒåˆ†å¸ƒï¼Œå¹¶é€šè¿‡åˆæˆåŸºå‡†çš„æœ‰æ§åˆ¶å®éªŒéªŒè¯äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¦»æ•£æ•°æ®çš„ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­äº«æœ‰æœ€å…ˆè¿›çš„åé‡‡æ ·æ€§èƒ½ï¼ŒåŒ…æ‹¬DNAåºåˆ—è®¾è®¡ã€ç¦»æ•£å›¾åƒåé—®é¢˜å’ŒéŸ³ä¹å¡«å……ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†30%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01161v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„åé‡‡æ ·é—®é¢˜ï¼Œé‡‡ç”¨ç¦»æ•£æ‰©æ•£æ¨¡å‹è¿›è¡Œç ”ç©¶ã€‚å°½ç®¡è¿ç»­æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†è£‚Gibbsé‡‡æ ·çš„ç¦»æ•£æ‰©æ•£åé‡‡æ ·ç®—æ³•SGDDï¼Œè¯¥ç®—æ³•å¯å®ç°å¥–åŠ±å¼•å¯¼ç”Ÿæˆå’Œè§£å†³ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„é€†é—®é¢˜ã€‚å®éªŒè¯æ˜SGDDèƒ½æ”¶æ•›åˆ°ç›®æ ‡åéªŒåˆ†å¸ƒï¼Œå¹¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ°éªŒè¯ã€‚è¯¥æ–¹æ³•åœ¨ç¦»æ•£æ•°æ®çš„ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„åé‡‡æ ·æ€§èƒ½ï¼ŒåŒ…æ‹¬DNAåºåˆ—è®¾è®¡ã€ç¦»æ•£å›¾åƒé€†é—®é¢˜å’ŒéŸ³ä¹å¡«å……ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ€§èƒ½æå‡è¶…è¿‡30%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„åé‡‡æ ·é—®é¢˜ï¼Œè¿™æ˜¯ç¦»æ•£æ‰©æ•£æ¨¡å‹ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåˆ†è£‚Gibbsé‡‡æ ·çš„ç¦»æ•£æ‰©æ•£åé‡‡æ ·ç®—æ³•SGDDã€‚</li>
<li>SGDDç®—æ³•èƒ½å¤Ÿæ‰§è¡Œå¥–åŠ±å¼•å¯¼ç”Ÿæˆå’Œè§£å†³ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„é€†é—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜äº†SGDDèƒ½æ”¶æ•›åˆ°ç›®æ ‡åéªŒåˆ†å¸ƒã€‚</li>
<li>SGDDåœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSGDDåœ¨DNAåºåˆ—è®¾è®¡ã€ç¦»æ•£å›¾åƒé€†é—®é¢˜å’ŒéŸ³ä¹å¡«å……ç­‰æ–¹é¢è¡¨ç°å‡ºæ¯”ç°æœ‰åŸºçº¿æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c8edd6e0443f8d088d56747bc18d693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858c68b7e99cd15f114a194f226b10c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c834eda0b84748656d489c3d30f17e7a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Remasking-Discrete-Diffusion-Models-with-Inference-Time-Scaling"><a href="#Remasking-Discrete-Diffusion-Models-with-Inference-Time-Scaling" class="headerlink" title="Remasking Discrete Diffusion Models with Inference-Time Scaling"></a>Remasking Discrete Diffusion Models with Inference-Time Scaling</h2><p><strong>Authors:Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, Volodymyr Kuleshov</strong></p>
<p>Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: <a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„éƒ¨åˆ†æˆåŠŸæºäºå…¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–çš„èƒ½åŠ›ï¼Œå³ç”Ÿæˆè¿‡ç¨‹ä¸­åå¤ä¿®æ­£è¾“å‡ºã€‚ç„¶è€Œï¼Œç°ä»£æ©ç ç¦»æ•£æ‰©æ•£ç¼ºä¹è¿™ç§èƒ½åŠ›ï¼šä¸€æ—¦ç”Ÿæˆæ ‡è®°ï¼Œå³ä½¿å‡ºç°é”™è¯¯ï¼Œä¹Ÿæ— æ³•å†æ¬¡æ›´æ–°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥é‡æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆReMDMï¼‰é‡‡æ ·å™¨æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¿™æ˜¯ä¸€ç§å¯ä»¥åŸåˆ™æ€§åœ°åº”ç”¨äºé¢„è®­ç»ƒæ©ç æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå®ƒæ¥æºäºå…·æœ‰è‡ªå®šä¹‰åå‘é‡æ©ç è¿‡ç¨‹çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚æœ€æœ‰è¶£çš„æ˜¯ï¼ŒReMDMä¸ºç¦»æ•£æ‰©æ•£èµ‹äºˆäº†æ¨ç†æ—¶é—´è®¡ç®—ç¼©æ”¾çš„å½¢å¼ã€‚é€šè¿‡å¢åŠ é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼ŒReMDMç”Ÿæˆçš„è‡ªç„¶è¯­è¨€è¾“å‡ºæ¥è¿‘è‡ªå›å½’æ¨¡å‹çš„è´¨é‡ï¼Œè€Œå½“è®¡ç®—é¢„ç®—æœ‰é™æ—¶ï¼ŒReMDMèƒ½æ›´å¥½åœ°ä¿æŒè´¨é‡ã€‚ReMDMè¿˜æé«˜äº†ç¦»æ•£å›¾åƒã€åˆ†å­è®¾è®¡ç­‰ç§‘å­¦é¢†åŸŸçš„æ©ç æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡ï¼Œå¹¶æ¨åŠ¨äº†ç›¸å¯¹äºä¼ ç»Ÿæ©ç å’Œå‡åŒ€å™ªå£°æ‰©æ•£çš„å¯æ§æ€§çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬å·²æä¾›é¡¹ç›®é¡µé¢ä¸Šçš„ä»£ç åŠåšå®¢æ–‡ç« ï¼š<a target="_blank" rel="noopener" href="https://remdm.github.io./">https://remdm.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00307v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„ä¸€éƒ¨åˆ†æˆåŠŸæºäºå…¶èƒ½å¤Ÿè¿›è¡Œè¿­ä»£ä¼˜åŒ–çš„èƒ½åŠ›ï¼Œå³ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸æ–­ä¿®æ­£è¾“å‡ºã€‚ç„¶è€Œï¼Œç°ä»£æ©ç ç¦»æ•£æ‰©æ•£ç¼ºä¹è¿™ç§èƒ½åŠ›ï¼šä¸€æ—¦ç”Ÿæˆä»¤ç‰Œï¼Œå³ä½¿å‡ºç°é”™è¯¯ï¼Œä¹Ÿä¸èƒ½å†æ¬¡æ›´æ–°ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥é‡æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆReMDMï¼‰é‡‡æ ·å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŸåˆ™æ€§åœ°åº”ç”¨äºé¢„è®­ç»ƒçš„æ©ç æ‰©æ•£æ¨¡å‹ï¼Œæºäºç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰è‡ªå®šä¹‰çš„é‡æ©ç é€†å‘è¿‡ç¨‹ã€‚ReMDMä½¿ç¦»æ•£æ‰©æ•£å…·æœ‰ä¸€ç§æ¨ç†æ—¶é—´è®¡ç®—ç¼©æ”¾å½¢å¼ã€‚é€šè¿‡å¢åŠ é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼ŒReMDMç”Ÿæˆçš„è‡ªç„¶è¯­è¨€è¾“å‡ºæ¥è¿‘è‡ªå›å½’æ¨¡å‹çš„è´¨é‡ï¼Œè€Œåœ¨è®¡ç®—é¢„ç®—æœ‰é™æ—¶ï¼ŒReMDMèƒ½æ›´å¥½åœ°ä¿æŒè´¨é‡ã€‚æ­¤å¤–ï¼ŒReMDMè¿˜æé«˜äº†æ©ç æ‰©æ•£æ¨¡å‹åœ¨ç¦»æ•£å›¾åƒä»¥åŠç§‘å­¦é¢†åŸŸï¼ˆå¦‚åˆ†å­è®¾è®¡ï¼‰çš„æ ·æœ¬è´¨é‡ï¼Œæ¨åŠ¨äº†å¯æ§æ€§çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œç›¸å¯¹äºä¼ ç»Ÿçš„æ©ç å’Œå‡åŒ€å™ªå£°æ‰©æ•£æœ‰æ‰€çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è¿­ä»£ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚</li>
<li>ç°ä»£æ©ç ç¦»æ•£æ‰©æ•£æ— æ³•æ›´æ–°å·²ç”Ÿæˆçš„ä»¤ç‰Œã€‚</li>
<li>ReMDMé‡‡æ ·å™¨è§£å†³äº†è¿™ä¸€é™åˆ¶ï¼Œæé«˜äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ReMDMèƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—å†…ç”Ÿæˆé«˜è´¨é‡çš„è‡ªç„¶è¯­è¨€è¾“å‡ºã€‚</li>
<li>ReMDMæé«˜äº†æ©ç æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç§‘å­¦é¢†åŸŸçš„æ ·æœ¬è´¨é‡ã€‚</li>
<li>ReMDMåœ¨åˆ†å­è®¾è®¡ç­‰é¢†åŸŸæ¨åŠ¨äº†æ‰©æ•£æŒ‡å¯¼çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75a85e7090232ae136abaf7738419196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-113cbc60f770cf08f0914f121e086943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f06f006b3bff6a186ba420767a39e76.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Dress-1-to-3-Single-Image-to-Simulation-Ready-3D-Outfit-with-Diffusion-Prior-and-Differentiable-Physics"><a href="#Dress-1-to-3-Single-Image-to-Simulation-Ready-3D-Outfit-with-Diffusion-Prior-and-Differentiable-Physics" class="headerlink" title="Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics"></a>Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics</h2><p><strong>Authors:Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang</strong></p>
<p>Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: <a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a> </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹æ¨¡å‹çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†å›¾åƒåˆ°3Dé‡å»ºçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„æ¨¡å‹é€šå¸¸è¢«èåˆæˆå•ä¸ªéƒ¨åˆ†ï¼Œè¿™åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨å—åˆ°å±€é™ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨3Dæœè£…ç”Ÿæˆï¼Œè¿™æ˜¯è™šæ‹Ÿè¯•ç©¿ç­‰åº”ç”¨çš„å…³é”®é¢†åŸŸï¼Œè¦æ±‚æœè£…å¯åˆ†ç¦»ä¸”å¯æ¨¡æ‹ŸåŠ¨æ€æœè£…åŠ¨ç”»ã€‚æˆ‘ä»¬å¼•å…¥äº†Dress-1-to-3ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹ç®¡é“ï¼Œå¯ä»¥ä»é‡ç”Ÿå›¾åƒä¸­é‡å»ºç‰©ç†å¯è¡Œçš„ã€å¯æ¨¡æ‹Ÿçš„åˆ†ç¦»æœè£…ï¼ŒåŒ…æ‹¬ç¼çº«å›¾æ¡ˆå’Œäººç±»ã€‚ä»å›¾åƒå¼€å§‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆé¢„è®­ç»ƒçš„å›¾åƒåˆ°ç¼çº«å›¾æ¡ˆç”Ÿæˆæ¨¡å‹æ¥åˆ›å»ºç²—ç•¥çš„ç¼çº«å›¾æ¡ˆï¼Œä»¥åŠä¸é¢„è®­ç»ƒçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†è§’å›¾åƒã€‚ç¼çº«å›¾æ¡ˆè¿›ä¸€æ­¥ä½¿ç”¨åŸºäºç”Ÿæˆçš„å¤šè§†è§’å›¾åƒçš„å¯å¾®åˆ†æœè£…æ¨¡æ‹Ÿå™¨è¿›è¡Œç»†åŒ–ã€‚å¤šç§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ³•æå¤§åœ°æé«˜äº†é‡å»ºçš„3Dæœè£…å’Œäººä½“ä¸è¾“å…¥å›¾åƒçš„å‡ ä½•å¯¹é½ã€‚æ­¤å¤–ï¼Œé€šè¿‡é›†æˆçº¹ç†ç”Ÿæˆæ¨¡å—å’Œäººä½“è¿åŠ¨ç”Ÿæˆæ¨¡å—ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å®šåˆ¶çš„ã€ç‰©ç†å¯è¡Œçš„å’Œé€¼çœŸçš„åŠ¨æ€æœè£…æ¼”ç¤ºã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03449v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Dress-1-to-3é¡¹ç›®ï¼Œè¯¥é¡¹ç›®ä¸“æ³¨äºä»å›¾åƒé‡å»ºç‰©ç†ä»¿çœŸå¯ç”¨çš„åˆ†ç¦»è¡£ç‰©æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒåˆ°ç¼çº«å›¾æ¡ˆç”Ÿæˆæ¨¡å‹å’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå¯å¾®åˆ†çš„è¡£ç‰©æ¨¡æ‹Ÿå™¨ï¼Œå®ç°ä»å•å¼ å›¾åƒé‡å»ºå‡ºå…·æœ‰ç‰©ç†ä»¿çœŸæ•ˆæœçš„åˆ†ç¦»è¡£ç‰©ã€‚æ­¤æ–¹æ³•æé«˜äº†é‡å»ºçš„3Dè¡£ç‰©ä¸è¾“å…¥å›¾åƒå‡ ä½•å¯¹é½çš„ç²¾ç¡®åº¦ï¼Œå¹¶å¯é€šè¿‡é›†æˆçº¹ç†ç”Ÿæˆæ¨¡å—å’Œè¿åŠ¨ç”Ÿæˆæ¨¡å—ï¼Œäº§ç”Ÿé€¼çœŸçš„åŠ¨æ€è¡£ç‰©æ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dress-1-to-3é¡¹ç›®å®ç°ä»å•å¼ å›¾åƒé‡å»ºç‰©ç†ä»¿çœŸå¯ç”¨çš„åˆ†ç¦»è¡£ç‰©æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒåˆ°ç¼çº«å›¾æ¡ˆç”Ÿæˆæ¨¡å‹åˆ›å»ºç²—ç¼çº«å›¾æ¡ˆã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šè§†è§’å›¾åƒã€‚</li>
<li>ç¼çº«å›¾æ¡ˆåŸºäºç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä½¿ç”¨å¯å¾®åˆ†çš„è¡£ç‰©æ¨¡æ‹Ÿå™¨è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>ä¼˜åŒ–æ–¹æ³•æé«˜äº†é‡å»ºçš„3Dè¡£ç‰©ä¸è¾“å…¥å›¾åƒçš„å‡ ä½•å¯¹é½ç²¾åº¦ã€‚</li>
<li>é€šè¿‡é›†æˆçº¹ç†ç”Ÿæˆæ¨¡å—ï¼Œäº§ç”Ÿå®šåˆ¶çš„ã€ç‰©ç†ä»¿çœŸçš„åŠ¨æ€è¡£ç‰©æ¼”ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24412ef34860fcd55abb854cee3d2aaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a3d193962d7f79ff5c601cbc5452695.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders"><a href="#SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders" class="headerlink" title="SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders"></a>SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders</h2><p><strong>Authors:Bartosz CywiÅ„ski, Kamil Deja</strong></p>
<p>Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Our evaluation shows that SAeUron outperforms existing approaches on the UnlearnCanvas benchmark for concepts and style unlearning, and effectively eliminates nudity when evaluated with I2P. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content under adversarial attack. Code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron">https://github.com/cywinski/SAeUron</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†å¯èƒ½æ— æ„ä¸­ç”Ÿæˆæœ‰å®³æˆ–ä¸è‰¯å†…å®¹ï¼Œå¼•å‘ä¸¥é‡çš„ä¼¦ç†å’Œå®‰å…¨æ‹…å¿§ã€‚æœ€è¿‘çš„æœºå™¨éå­¦ä¹ æŠ€æœ¯æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¾€å¾€ç¼ºä¹é€æ˜åº¦ï¼Œä½¿å¾—éš¾ä»¥äº†è§£å®ƒä»¬å¯¹åŸºç¡€æ¨¡å‹æ‰€åšçš„æ”¹å˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SAeUronï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å­¦ä¹ åˆ°çš„ç‰¹æ€§æ¥æ¶ˆé™¤æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ä¸éœ€è¦æ¦‚å¿µçš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨æ‰©æ•£æ¨¡å‹çš„å¤šä¸ªå»å™ªæ—¶é—´æ­¥é•¿çš„æ¿€æ´»ä¸Šé‡‡ç”¨æ— ç›‘ç£æ–¹å¼è®­ç»ƒçš„SAEå¯ä»¥æ•è·å¯¹åº”äºç‰¹å®šæ¦‚å¿µçš„ç¨€ç–å’Œå¯è§£é‡Šçš„ç‰¹æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œèƒ½å¤Ÿå¯¹æ¨¡å‹æ¿€æ´»è¿›è¡Œç²¾ç¡®å¹²é¢„ï¼Œä»¥é˜»æ­¢ç›®æ ‡å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨æ¦‚å¿µå’Œé£æ ¼éå­¦ä¹ çš„UnlearnCanvasåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSAeUronä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨è¯„ä¼°è£¸ä½“å†…å®¹æ—¶ä¸I2Pç»“åˆä½¿ç”¨å¯ä»¥æœ‰æ•ˆæ¶ˆé™¤å…¶å­˜åœ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„SAEå¯ä»¥åŒæ—¶æ¶ˆé™¤å¤šä¸ªæ¦‚å¿µï¼Œè€Œä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒSAeUronå‡è½»äº†ç”Ÿæˆå¯¹æŠ—æ”»å‡»ä¸‹å¯èƒ½äº§ç”Ÿçš„ä¸å¿…è¦å†…å®¹çš„å¯èƒ½æ€§ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cywinski/SAeUronæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18052v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusionæ¨¡å‹å¯èƒ½äº§ç”Ÿçš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ï¼Œå³å¯èƒ½ç”Ÿæˆæœ‰å®³æˆ–ä¸å—æ¬¢è¿çš„å†…å®¹ã€‚å°½ç®¡ç°æœ‰çš„æœºå™¨é—å¿˜æ³•æä¾›äº†ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹é€æ˜åº¦ï¼Œéš¾ä»¥äº†è§£å¯¹åŸºç¡€æ¨¡å‹æ‰€åšçš„æ”¹å˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SAeUronï¼Œå®ƒåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰çš„ç‰¹æ€§æ¥æ¶ˆé™¤æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¸éœ€è¦çš„æ¦‚å¿µã€‚å®éªŒè¡¨æ˜ï¼ŒSAeUronåœ¨æ¦‚å¿µä¸é£æ ¼é—å¿˜çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¶ˆé™¤å›¾åƒä¸­çš„è£¸ä½“å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å•ä¸ªSAEå¯ä»¥åŒæ—¶æ¶ˆé™¤å¤šä¸ªæ¦‚å¿µçš„èƒ½åŠ›ï¼Œå¹¶ä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒSAeUronå‡è½»äº†ç”Ÿæˆå¯¹æŠ—æ€§æ”»å‡»ä¸‹äº§ç”Ÿçš„ä¸å¿…è¦å†…å®¹çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusionæ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆæœ‰å®³æˆ–ä¸å—æ¬¢è¿çš„å†…å®¹ï¼Œå¼•å‘ä¼¦ç†å’Œå®‰å…¨æ‹…å¿§ã€‚</li>
<li>ç°æœ‰æœºå™¨é—å¿˜æ³•è§£å†³æ­¤é—®é¢˜ä½†ç¼ºä¹é€æ˜åº¦ã€‚</li>
<li>SAeUronæ–¹æ³•åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰çš„ç‰¹æ€§æ¶ˆé™¤æ‰©æ•£æ¨¡å‹ä¸­çš„ä¸éœ€è¦çš„æ¦‚å¿µã€‚</li>
<li>SAeUronåœ¨æ¦‚å¿µä¸é£æ ¼é—å¿˜çš„æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆæ¶ˆé™¤ç‰¹å®šå†…å®¹ã€‚</li>
<li>SAeUronèƒ½åŒæ—¶æ¶ˆé™¤å¤šä¸ªæ¦‚å¿µã€‚</li>
<li>SAeUroné™ä½äº†ç”Ÿæˆå¯¹æŠ—æ€§æ”»å‡»ä¸‹äº§ç”Ÿä¸å¿…è¦å†…å®¹çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce83f2ad29caa2d78af9fdd0e75db81f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-570edce8b07c3b8ecb334a2dc89391a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093b0ce4bf8810190a0dc9edb12810df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d3835115ae6c44a1080ba3db2c97f5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Make-An-Agent-A-Generalizable-Policy-Network-Generator-with-Behavior-Prompted-Diffusion"><a href="#Make-An-Agent-A-Generalizable-Policy-Network-Generator-with-Behavior-Prompted-Diffusion" class="headerlink" title="Make-An-Agent: A Generalizable Policy Network Generator with   Behavior-Prompted Diffusion"></a>Make-An-Agent: A Generalizable Policy Network Generator with   Behavior-Prompted Diffusion</h2><p><strong>Authors:Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu</strong></p>
<p>Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: <a target="_blank" rel="noopener" href="https://cheryyunl.github.io/make-an-agent/">https://cheryyunl.github.io/make-an-agent/</a> </p>
<blockquote>
<p>æˆ‘ä»¬å¯ä»¥åƒä»æ–‡æœ¬æè¿°ä¸­åˆ›å»ºå›¾åƒä¸€æ ·ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªæœŸæœ›è¡Œä¸ºçš„æ¼”ç¤ºæ¥æç¤ºä¸ºæ™ºèƒ½ä½“ç”Ÿæˆæ§åˆ¶ç­–ç•¥å—ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Make-An-Agentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç­–ç•¥å‚æ•°ç”Ÿæˆå™¨ï¼Œå®ƒåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŠ›é‡è¿›è¡Œè¡Œä¸ºåˆ°ç­–ç•¥çš„ç”Ÿæˆã€‚åœ¨è¡Œä¸ºåµŒå…¥çš„æŒ‡å¯¼ä¸‹ï¼ˆè¿™äº›åµŒå…¥ç¼–ç äº†è½¨è¿¹ä¿¡æ¯ï¼‰ï¼Œæˆ‘ä»¬çš„ç­–ç•¥ç”Ÿæˆå™¨åˆæˆæ½œåœ¨å‚æ•°è¡¨ç¤ºï¼Œç„¶åå¯ä»¥å°†å…¶è§£ç ä¸ºç­–ç•¥ç½‘ç»œã€‚æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œåœ¨æœªè§è¿‡çš„æ–°ä»»åŠ¡ä¸Šå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä»…ä½¿ç”¨å°‘æ•°æ¼”ç¤ºä½œä¸ºè¾“å…¥å³å¯è¾“å‡ºè¡¨ç°è‰¯å¥½çš„ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨å„ç§é¢†åŸŸå’Œä»»åŠ¡ä¸­å±•ç¤ºäº†å®ƒçš„åŠŸæ•ˆå’Œæ•ˆç‡ï¼ŒåŒ…æ‹¬ä¸åŒçš„ç›®æ ‡ã€è¡Œä¸ºå’Œç”šè‡³ä¸åŒçš„æœºå™¨äººæ“çºµå™¨ã€‚é™¤äº†æ¨¡æ‹Ÿä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç›´æ¥å°†Make-An-Agentç”Ÿæˆçš„ç­–ç•¥éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¸­çš„æœºå™¨äººä¸Šè¿›è¡Œè¿åŠ¨ä»»åŠ¡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cheryyunl.github.io/make-an-agent/">https://cheryyunl.github.io/make-an-agent/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10973v4">PDF</a> Annual Conference on Neural Information Processing Systems 38</p>
<p><strong>Summary</strong></p>
<p>ä½¿ç”¨å•ä¸€æ¼”ç¤ºè¡Œä¸ºç”Ÿæˆä»£ç†æ§åˆ¶ç­–ç•¥ï¼Œå¦‚åŒæ ¹æ®æ–‡å­—æè¿°ç”Ÿæˆå›¾åƒä¸€æ ·ç®€å•ä¾¿æ·ã€‚æœ¬ç ”ç©¶æå‡ºäº†Make-An-Agentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç­–ç•¥å‚æ•°ç”Ÿæˆå™¨ï¼Œåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹å®ç°è¡Œä¸ºåˆ°ç­–ç•¥çš„ç”Ÿæˆã€‚è¯¥ç­–ç•¥ç”Ÿæˆå™¨é€šè¿‡è¡Œä¸ºåµŒå…¥ç¼–ç è½¨è¿¹ä¿¡æ¯ï¼Œåˆæˆæ½œåœ¨å‚æ•°è¡¨ç¤ºï¼Œç„¶åè§£ç ä¸ºç­–ç•¥ç½‘ç»œã€‚åœ¨ç­–ç•¥ç½‘ç»œæ£€æŸ¥ç‚¹åŠå…¶å¯¹åº”è½¨è¿¹çš„è®­ç»ƒä¸‹ï¼Œè¯¥ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„å¤šåŠŸèƒ½æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”å¯¹æœªè§ä»»åŠ¡çš„ç­–ç•¥è¾“å‡ºå…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä»…ä½¿ç”¨å°‘é‡æ¼”ç¤ºä½œä¸ºè¾“å…¥ã€‚å®ƒåœ¨å„ç§é¢†åŸŸå’Œä»»åŠ¡ä¸­å±•ç¤ºäº†æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä¸åŒçš„ç›®æ ‡ã€è¡Œä¸ºå’Œæœºå™¨äººæ“ä½œå™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†Make-An-Agentç”Ÿæˆçš„ç­–ç•¥ç›´æ¥éƒ¨ç½²åˆ°çœŸå®ä¸–ç•Œæœºå™¨äººä¸Šæ‰§è¡Œè¿åŠ¨ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Make-An-Agentåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹å®ç°äº†ä»è¡Œä¸ºåˆ°ç­–ç•¥çš„ç”Ÿæˆã€‚</li>
<li>è¯¥ç­–ç•¥ç”Ÿæˆå™¨é€šè¿‡è¡Œä¸ºåµŒå…¥ç¼–ç è½¨è¿¹ä¿¡æ¯ï¼Œè¿›è€Œåˆæˆæ½œåœ¨å‚æ•°è¡¨ç¤ºã€‚</li>
<li>ç­–ç•¥ç”Ÿæˆå™¨å¯ä»¥è§£ç ä¸ºç­–ç•¥ç½‘ç»œï¼Œå…·å¤‡å“è¶Šçš„å¤šåŠŸèƒ½æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹åœ¨å°‘é‡æ¼”ç¤ºä½œä¸ºè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå¯¹æœªè§ä»»åŠ¡å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Make-An-Agentåœ¨å„ç§é¢†åŸŸå’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä¸åŒçš„ç›®æ ‡ã€è¡Œä¸ºå’Œæœºå™¨äººæ“ä½œå™¨ã€‚</li>
<li>Make-An-Agentå¯ä»¥ç›´æ¥å°†ç”Ÿæˆçš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®ä¸–ç•Œçš„æœºå™¨äººä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa45f4f001fb029de675ad9f1ebd0c1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423c4964e23d50f08558fb68b2c1ce0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32bf11efff904191e99cde375638abca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbf5570a59fc0a488dc9783f95ddf5d0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9ea8fda246e471d018ae69deae8b7be8.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Oral Imaging for Malocclusion Issues Assessments OMNI Dataset, Deep   Learning Baselines and Benchmarking
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3126b4d9896e174493289f273f89f37.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Seeing through Satellite Images at Street Views
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
