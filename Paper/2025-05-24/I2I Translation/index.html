<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical   Flow Estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Efficient-Correlation-Volume-Sampling-for-Ultra-High-Resolution-Optical-Flow-Estimation"><a href="#Efficient-Correlation-Volume-Sampling-for-Ultra-High-Resolution-Optical-Flow-Estimation" class="headerlink" title="Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical   Flow Estimation"></a>Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical   Flow Estimation</h2><p><strong>Authors:Karlis Martins Briedis, Markus Gross, Christopher Schroers</strong></p>
<p>Recent optical flow estimation methods often employ local cost sampling from a dense all-pairs correlation volume. This results in quadratic computational and memory complexity in the number of pixels. Although an alternative memory-efficient implementation with on-demand cost computation exists, this is slower in practice and therefore prior methods typically process images at reduced resolutions, missing fine-grained details.   To address this, we propose a more efficient implementation of the all-pairs correlation volume sampling, still matching the exact mathematical operator as defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while maintaining low memory usage, and performs on par with the default implementation with up to 95% lower memory usage. As cost sampling makes up a significant portion of the overall runtime, this can translate to up to 50% savings for the total end-to-end model inference in memory-constrained environments. Our evaluation of existing methods includes an 8K ultra-high-resolution dataset and an additional inference-time modification of the recent SEA-RAFT method. With this, we achieve state-of-the-art results at high resolutions both in accuracy and efficiency. </p>
<blockquote>
<p>è¿‘æœŸå…‰å­¦æµåŠ¨ä¼°è®¡æ–¹æ³•ç»å¸¸é‡‡ç”¨ä»å¯†é›†çš„å…¨å¯¹ç›¸å…³æ€§ä½“ç§¯ä¸­è¿›è¡Œå±€éƒ¨æˆæœ¬é‡‡æ ·ã€‚è¿™å¯¼è‡´äº†åœ¨åƒç´ æ•°é‡ä¸Šçš„äºŒæ¬¡è®¡ç®—å’Œå†…å­˜å¤æ‚æ€§ã€‚å°½ç®¡å­˜åœ¨ä¸€ç§å¸¦æœ‰æŒ‰éœ€æˆæœ¬è®¡ç®—çš„å†…å­˜é«˜æ•ˆå®ç°æ–¹æ³•ï¼Œä½†åœ¨å®è·µä¸­è¿™ç§æ–¹æ³•è¾ƒæ…¢ï¼Œå› æ­¤å…ˆå‰çš„æ–¹æ³•é€šå¸¸åœ¨é™ä½çš„åˆ†è¾¨ç‡ä¸Šå¤„ç†å›¾åƒï¼Œä»è€Œä¸¢å¤±äº†ç²¾ç»†çš„é¢—ç²’ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨å¯¹ç›¸å…³æ€§ä½“ç§¯é‡‡æ ·çš„æ›´é«˜æ•ˆå®ç°æ–¹æ³•ï¼Œä»ç„¶åŒ¹é…RAFTæ‰€å®šä¹‰çš„ç²¾ç¡®æ•°å­¦è¿ç®—ç¬¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å†…å­˜ä½¿ç”¨ä¸Šä¼˜äºæŒ‰éœ€é‡‡æ ·é«˜è¾¾90%ï¼ŒåŒæ—¶ä¿æŒä½å†…å­˜ä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­ä¸é»˜è®¤å®ç°ç›¸æ¯”ï¼Œæ€§èƒ½ç›¸å½“ï¼Œå†…å­˜ä½¿ç”¨å¯é™ä½é«˜è¾¾95%ã€‚ç”±äºæˆæœ¬é‡‡æ ·å æ€»ä½“è¿è¡Œæ—¶é—´çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼Œå› æ­¤è¿™å¯ä»¥è½¬æ¢ä¸ºåœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­ç«¯åˆ°ç«¯æ¨¡å‹æ¨ç†æ€»æ—¶é—´çš„æœ€é«˜è¾¾50%çš„èŠ‚çœã€‚æˆ‘ä»¬å¯¹ç°æœ‰æ–¹æ³•çš„è¯„ä¼°åŒ…æ‹¬ä¸€ä¸ª8Kè¶…é«˜åˆ†è¾¨ç‡æ•°æ®é›†å’Œæœ€è¿‘SEA-RAFTæ–¹æ³•çš„é¢å¤–æ¨ç†æ—¶é—´ä¿®æ”¹ã€‚å€Ÿæ­¤ï¼Œæˆ‘ä»¬åœ¨å‡†ç¡®åº¦å’Œæ•ˆç‡æ–¹é¢éƒ½å®ç°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³çš„é«˜åˆ†è¾¨ç‡ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16942v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§æ›´é«˜æ•ˆçš„æˆå¯¹ç›¸å…³æ€§ä½“ç§¯é‡‡æ ·æ–¹æ³•ï¼Œç”¨äºè§£å†³å…‰å­¦æµåŠ¨ä¼°è®¡ä¸­çš„è®¡ç®—é‡å¤§å’Œå†…å­˜å¤æ‚åº¦é«˜çš„é—®é¢˜ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒä½å†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼ŒåŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ï¼Œæœ€é«˜å¯è¾¾90%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å†…å­˜å—é™ç¯å¢ƒä¸­èŠ‚çœé«˜è¾¾50%çš„æ€»ç«¯åˆ°ç«¯æ¨¡å‹æ¨ç†æ—¶é—´ã€‚å¯¹é«˜æ¸…æ™°åº¦æ•°æ®é›†çš„è¯„ä¼°æ˜¾ç¤ºäº†è¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æˆå¯¹ç›¸å…³æ€§ä½“ç§¯é‡‡æ ·æ–¹æ³•ã€‚</li>
<li>åœ¨ä¿æŒä½å†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—åŠ é€Ÿå…‰å­¦æµåŠ¨ä¼°è®¡çš„è®¡ç®—è¿‡ç¨‹ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„å…‰å­¦æµåŠ¨ä¼°è®¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„è¿è¡Œé€Ÿåº¦æœ€é«˜å¯æé«˜90%ã€‚</li>
<li>åœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥èŠ‚çœé«˜è¾¾50%çš„æ€»ç«¯åˆ°ç«¯æ¨¡å‹æ¨ç†æ—¶é—´ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨é«˜æ¸…æ™°åº¦æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16942v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16942v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16942v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16942v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16942v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM"><a href="#MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM" class="headerlink" title="MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM"></a>MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advances in static 3D generation have intensified the demand for physically consistent dynamic 3D content. However, existing video generation models, including diffusion-based methods, often prioritize visual realism while neglecting physical plausibility, resulting in implausible object dynamics. Prior approaches for physics-aware dynamic generation typically rely on large-scale annotated datasets or extensive model fine-tuning, which imposes significant computational and data collection burdens and limits scalability across scenarios. To address these challenges, we present MAGIC, a training-free framework for single-image physical property inference and dynamic generation, integrating pretrained image-to-video diffusion models with iterative LLM-based reasoning. Our framework generates motion-rich videos from a static image and closes the visual-to-physical gap through a confidence-driven LLM feedback loop that adaptively steers the diffusion model toward physics-relevant motion. To translate visual dynamics into controllable physical behavior, we further introduce a differentiable MPM simulator operating directly on 3D Gaussians reconstructed from the single image, enabling physically grounded, simulation-ready outputs without any supervision or model tuning. Experiments show that MAGIC outperforms existing physics-aware generative methods in inference accuracy and achieves greater temporal coherence than state-of-the-art video diffusion models. </p>
<blockquote>
<p>è¿‘æœŸé™æ€3Dç”ŸæˆæŠ€æœ¯çš„è¿›å±•åŠ å‰§äº†å¯¹ç‰©ç†ä¸€è‡´æ€§åŠ¨æ€3Då†…å®¹çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œé€šå¸¸ä¼˜å…ˆè€ƒè™‘è§†è§‰çœŸå®æ€§ï¼Œè€Œå¿½ç•¥äº†ç‰©ç†å¯è¡Œæ€§ï¼Œå¯¼è‡´ç‰©ä½“åŠ¨æ€ä¸å¯ä¿¡ã€‚ä»¥å‰å¯¹ç‰©ç†æ„ŸçŸ¥åŠ¨æ€ç”Ÿæˆçš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†æˆ–æ¨¡å‹ç²¾ç»†è°ƒæ•´ï¼Œè¿™å¸¦æ¥äº†å¾ˆå¤§çš„è®¡ç®—å’Œæ•°æ®é‡‡é›†è´Ÿæ‹…ï¼Œå¹¶é™åˆ¶äº†è·¨åœºæ™¯çš„æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MAGICï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•å›¾åƒç‰©ç†å±æ€§æ¨æ–­å’ŒåŠ¨æ€ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ä¸åŸºäºè¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»é™æ€å›¾åƒç”Ÿæˆä¸°å¯Œçš„è¿åŠ¨è§†é¢‘ï¼Œå¹¶é€šè¿‡ä¿¡å¿ƒé©±åŠ¨çš„LLMåé¦ˆå¾ªç¯ç¼©å°äº†è§†è§‰åˆ°ç‰©ç†çš„å·®è·ï¼Œè¯¥å¾ªç¯è‡ªé€‚åº”åœ°å¼•å¯¼æ‰©æ•£æ¨¡å‹æœç€ç‰©ç†ç›¸å…³çš„è¿åŠ¨å‘å±•ã€‚ä¸ºäº†å°†è§†è§‰åŠ¨æ€è½¬åŒ–ä¸ºå¯æ§çš„ç‰©ç†è¡Œä¸ºï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†çš„MPMæ¨¡æ‹Ÿå™¨ï¼Œç›´æ¥åœ¨ä»å•å¼ å›¾åƒé‡å»ºçš„3Dé«˜æ–¯ä¸Šè¿è¡Œï¼Œä»è€Œå®ç°æ— éœ€ç›‘ç£æˆ–æ¨¡å‹è°ƒæ•´çš„ç‰©ç†åŸºç¡€ã€æ¨¡æ‹Ÿå°±ç»ªè¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒMAGICåœ¨æ¨ç†å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ—¶é—´è¿è´¯æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16456v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸé™æ€3DæŠ€æœ¯çš„å‘å±•åŠ å‰§äº†å¯¹ç‰©ç†ä¸€è‡´æ€§åŠ¨æ€3Då†…å®¹çš„éœ€æ±‚ã€‚ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾€å¾€æ³¨é‡è§†è§‰çœŸå®æ€§è€Œå¿½è§†ç‰©ç†åˆç†æ€§ï¼Œå¯¼è‡´ç‰©ä½“åŠ¨æ€ä¸åˆç†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºMAGICæ¡†æ¶ï¼Œæ— éœ€è®­ç»ƒå³å¯è¿›è¡Œå•å¼ å›¾ç‰‡ç‰©ç†å±æ€§æ¨æ–­å’ŒåŠ¨æ€ç”Ÿæˆï¼Œæ•´åˆé¢„è®­ç»ƒå›¾åƒè½¬è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸è¿­ä»£LLMæ¨ç†ã€‚æ¡†æ¶é€šè¿‡ç½®ä¿¡é©±åŠ¨LLMåé¦ˆç¯ç”Ÿæˆä¸°å¯ŒåŠ¨æ€è§†é¢‘ï¼Œè‡ªé€‚åº”å¼•å¯¼æ‰©æ•£æ¨¡å‹å‘ç‰©ç†ç›¸å…³è¿åŠ¨å‘å±•ã€‚å¼•å…¥å¯å¾®MPMæ¨¡æ‹Ÿå™¨ï¼Œç›´æ¥åœ¨å•å¼ å›¾ç‰‡é‡å»ºçš„3Dé«˜æ–¯ä¸Šæ“ä½œï¼Œäº§ç”Ÿç‰©ç†åŸºç¡€ã€æ¨¡æ‹Ÿå‡†å¤‡è¾“å‡ºï¼Œæ— éœ€ç›‘ç£æˆ–æ¨¡å‹è°ƒæ•´ã€‚å®éªŒæ˜¾ç¤ºï¼ŒMAGICåœ¨æ¨ç†å‡†ç¡®æ€§ä¸Šä¼˜äºç°æœ‰ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œæ—¶é—´è¿è´¯æ€§é«˜äºæœ€å…ˆè¿›è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸé™æ€3DæŠ€æœ¯å‘å±•ä¿ƒè¿›äº†å¯¹ç‰©ç†ä¸€è‡´æ€§åŠ¨æ€å†…å®¹çš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸»è¦å…³æ³¨è§†è§‰çœŸå®æ€§ï¼Œå¿½ç•¥äº†ç‰©ç†åˆç†æ€§ã€‚</li>
<li>MAGICæ¡†æ¶æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å•å›¾åƒç‰©ç†å±æ€§æ¨æ–­å’ŒåŠ¨æ€ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>MAGICç»“åˆäº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸è¿­ä»£LLMæ¨ç†ã€‚</li>
<li>é€šè¿‡ç½®ä¿¡é©±åŠ¨LLMåé¦ˆç¯å®ç°åŠ¨æ€è§†é¢‘ç”Ÿæˆï¼Œå¼•å¯¼æ‰©æ•£æ¨¡å‹å‘ç‰©ç†ç›¸å…³è¿åŠ¨å‘å±•ã€‚</li>
<li>å¼•å…¥å¯å¾®MPMæ¨¡æ‹Ÿå™¨ï¼Œç›´æ¥æ“ä½œå•å¼ å›¾ç‰‡ä¸Šçš„3Dé«˜æ–¯ï¼Œäº§ç”Ÿç‰©ç†åŸºç¡€è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16456v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16456v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16456v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16456v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16456v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="I-2G-Generating-Instructional-Illustrations-via-Text-Conditioned-Diffusion"><a href="#I-2G-Generating-Instructional-Illustrations-via-Text-Conditioned-Diffusion" class="headerlink" title="$I^2G$: Generating Instructional Illustrations via Text-Conditioned   Diffusion"></a>$I^2G$: Generating Instructional Illustrations via Text-Conditioned   Diffusion</h2><p><strong>Authors:Jing Bi, Pinxin Liu, Ali Vosoughi, Jiarui Wu, Jinxi He, Chenliang Xu</strong></p>
<p>The effective communication of procedural knowledge remains a significant challenge in natural language processing (NLP), as purely textual instructions often fail to convey complex physical actions and spatial relationships. We address this limitation by proposing a language-driven framework that translates procedural text into coherent visual instructions. Our approach models the linguistic structure of instructional content by decomposing it into goal statements and sequential steps, then conditioning visual generation on these linguistic elements. We introduce three key innovations: (1) a constituency parser-based text encoding mechanism that preserves semantic completeness even with lengthy instructions, (2) a pairwise discourse coherence model that maintains consistency across instruction sequences, and (3) a novel evaluation protocol specifically designed for procedural language-to-image alignment. Our experiments across three instructional datasets (HTStep, CaptainCook4D, and WikiAll) demonstrate that our method significantly outperforms existing baselines in generating visuals that accurately reflect the linguistic content and sequential nature of instructions. This work contributes to the growing body of research on grounding procedural language in visual content, with applications spanning education, task guidance, and multimodal language understanding. </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œè¿‡ç¨‹æ€§çŸ¥è¯†çš„æœ‰æ•ˆæ²Ÿé€šä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºçº¯ç²¹çš„æ–‡æœ¬æŒ‡ä»¤å¾€å¾€æ— æ³•ä¼ è¾¾å¤æ‚çš„ç‰©ç†åŠ¨ä½œå’Œç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§è¯­è¨€é©±åŠ¨æ¡†æ¶æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œè¯¥æ¡†æ¶å°†è¿‡ç¨‹æ€§æ–‡æœ¬ç¿»è¯‘æˆè¿è´¯çš„è§†è§‰æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ†è§£æŒ‡ä»¤å†…å®¹ä¸ºç›®æ ‡é™ˆè¿°å’Œé¡ºåºæ­¥éª¤æ¥å»ºæ¨¡æŒ‡ä»¤å†…å®¹çš„è¯­è¨€ç»“æ„ï¼Œç„¶åæ ¹æ®è¿™äº›è¯­è¨€å…ƒç´ è¿›è¡Œè§†è§‰ç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆ1ï¼‰ä¸€ç§åŸºäºæˆåˆ†åˆ†æå™¨çš„æ–‡æœ¬ç¼–ç æœºåˆ¶ï¼Œå³ä½¿åœ¨é•¿ç¯‡æŒ‡ä»¤çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼›ï¼ˆ2ï¼‰ä¸€ç§ä¿æŒæŒ‡ä»¤åºåˆ—ä¸€è‡´æ€§çš„æˆå¯¹è¯è¯­è¿è´¯æ€§æ¨¡å‹ï¼›ï¼ˆ3ï¼‰ä¸€ç§ä¸“ä¸ºç¨‹åºè¯­è¨€ä¸å›¾åƒå¯¹é½è®¾è®¡çš„å…¨æ–°è¯„ä¼°åè®®ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæŒ‡ä»¤æ•°æ®é›†ï¼ˆHTStepã€CaptainCook4Då’ŒWikiAllï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ è¯­è¨€å†…å®¹å’ŒæŒ‡ä»¤é¡ºåºæ€§çš„è§†è§‰å›¾åƒæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¿™é¡¹å·¥ä½œå¯¹æ•™è‚²ã€ä»»åŠ¡æŒ‡å¯¼å’Œè·¨æ¨¡æ€è¯­è¨€ç†è§£ç­‰é¢†åŸŸçš„è¿‡ç¨‹æ€§è¯­è¨€ä¸è§†è§‰å†…å®¹çš„ç»“åˆç ”ç©¶åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16425v1">PDF</a> 13 pages, 5 figures, under review</p>
<p><strong>Summary</strong>: </p>
<p>ç¨‹åºæ€§çŸ¥è¯†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æœ‰æ•ˆä¼ è¾¾ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå•çº¯æ–‡æœ¬æŒ‡ä»¤å¾€å¾€æ— æ³•ä¼ è¾¾å¤æ‚çš„ç‰©ç†åŠ¨ä½œå’Œç©ºé—´å…³ç³»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­è¨€é©±åŠ¨æ¡†æ¶ï¼Œå¯å°†ç¨‹åºæ€§æ–‡æœ¬è½¬åŒ–ä¸ºè¿è´¯çš„è§†è§‰æŒ‡ä»¤ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£ç›®æ ‡é™ˆè¿°å’Œé¡ºåºæ­¥éª¤å¯¹æŒ‡ä»¤å†…å®¹çš„è¯­è¨€ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼Œç„¶ååŸºäºè¿™äº›è¯­è¨€å…ƒç´ è¿›è¡Œè§†è§‰ç”Ÿæˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŸºäºä¾å­˜å¥æ³•åˆ†æå™¨çš„æ–‡æœ¬ç¼–ç æœºåˆ¶ï¼Œå³ä½¿åœ¨é•¿ç¯‡æŒ‡ä»¤ä¸‹ä¹Ÿèƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼›è¯è¯­è¿è´¯æ€§æ¨¡å‹ï¼Œä¿æŒæŒ‡ä»¤åºåˆ—çš„ä¸€è‡´æ€§ï¼›ä¸“ä¸ºç¨‹åºæ€§è¯­è¨€ä¸å›¾åƒå¯¹é½è®¾è®¡çš„æ–°å‹è¯„ä¼°åè®®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæŒ‡ä»¤æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œç”Ÿæˆçš„è§†è§‰å›¾åƒèƒ½å‡†ç¡®åæ˜ è¯­è¨€å†…å®¹å’ŒæŒ‡ä»¤çš„åºåˆ—æ€§ã€‚æœ¬æ–‡ä¸ºå°†ç¨‹åºæ€§è¯­è¨€æ ¹æ¤äºè§†è§‰å†…å®¹çš„ç ”ç©¶åšå‡ºäº†è´¡çŒ®ï¼Œåº”ç”¨èŒƒå›´æ¶µç›–æ•™è‚²ã€ä»»åŠ¡æŒ‡å¯¼å’Œå¤šåª’ä½“è¯­è¨€ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç¨‹åºæ€§çŸ¥è¯†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¼ è¾¾å­˜åœ¨æŒ‘æˆ˜ï¼Œå•çº¯æ–‡æœ¬æŒ‡ä»¤éš¾ä»¥è¡¨è¾¾å¤æ‚ç‰©ç†åŠ¨ä½œå’Œç©ºé—´å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¯­è¨€é©±åŠ¨æ¡†æ¶ï¼Œå°†ç¨‹åºæ€§æ–‡æœ¬è½¬åŒ–ä¸ºè§†è§‰æŒ‡ä»¤ã€‚</li>
<li>é€šè¿‡åˆ†è§£ç›®æ ‡é™ˆè¿°å’Œé¡ºåºæ­¥éª¤å¯¹æŒ‡ä»¤å†…å®¹çš„è¯­è¨€ç»“æ„è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>å¼•å…¥äº†åŸºäºä¾å­˜å¥æ³•åˆ†æå™¨çš„æ–‡æœ¬ç¼–ç æœºåˆ¶ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚</li>
<li>é‡‡ç”¨äº†è¯è¯­è¿è´¯æ€§æ¨¡å‹ï¼Œç¡®ä¿æŒ‡ä»¤åºåˆ—çš„ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†æ–°çš„è¯„ä¼°åè®®ï¼Œä¸“é—¨ç”¨äºç¨‹åºæ€§è¯­è¨€ä¸å›¾åƒçš„å¯¹é½ã€‚</li>
<li>å®éªŒåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç”Ÿæˆçš„è§†è§‰å›¾åƒèƒ½åæ˜ è¯­è¨€å†…å®¹å’ŒæŒ‡ä»¤çš„åºåˆ—æ€§ï¼Œå¯¹æ•™è‚²å’Œå¤šåª’ä½“è¯­è¨€ç†è§£ç­‰é¢†åŸŸæœ‰è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16425v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.16425v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models"><a href="#Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models" class="headerlink" title="Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models"></a>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models</h2><p><strong>Authors:Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang</strong></p>
<p>Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the modelâ€™s discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMsâ€™ performance on complex CR tasks. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL">https://github.com/nynu-BDAI/AHNPL</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºå¤šæ¨¡æ€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯éœ€è¦åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´ç»†å¾®è¯­ä¹‰å·®å¼‚çš„ç»„åˆæ¨ç†ï¼ˆCRï¼‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¿½ç•¥äº†åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬çš„é‡è¦æ€§ï¼Œå¯¼è‡´è§†è§‰ç¼–ç å™¨è®­ç»ƒä¸è¶³ï¼Œæœ€ç»ˆå½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè´Ÿæ ·æœ¬é€šå¸¸è¢«ä¸€è§†åŒä»ï¼Œæ²¡æœ‰è€ƒè™‘å…¶éš¾åº¦æ°´å¹³ï¼Œæ­£æ ·æœ¬çš„å¯¹é½ä¹Ÿä¸è¶³ï¼Œè¿™å¯¼è‡´éš¾ä»¥å¯¹é½å›°éš¾æ ·æœ¬å¯¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ç¡¬è´Ÿæ‰°åŠ¨å­¦ä¹ ï¼ˆAHNPLï¼‰ã€‚AHNPLå°†åŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬è½¬æ¢ä¸ºè§†è§‰åŸŸï¼Œä»¥ç”Ÿæˆç”¨äºè®­ç»ƒæ¨¡å‹çš„åœ¨è¯­ä¹‰ä¸Šå—åˆ°å¹²æ‰°çš„åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚AHNPLè¿˜å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æ¥æé«˜æ¨¡å‹åœ¨æ¯ä¸ªæ¨¡æ€å†…å¯¹ç¡¬è´Ÿæ ·æœ¬çš„è¾¨åˆ«èƒ½åŠ›ï¼Œä»¥åŠä¸€ç§åŠ¨æ€è¾¹è·æŸå¤±ï¼Œè¯¥æŸå¤±ä¼šæ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´å¯¹æ¯”è¾¹è·ï¼Œä»¥æé«˜å¯¹å›°éš¾æ ·æœ¬å¯¹çš„åŒºåˆ†èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†VLMsåœ¨å¤æ‚çš„CRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nynu-BDAI/AHNPLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15576v1">PDF</a> Accepted at the International Joint Conference on Artificial   Intelligence (IJCAI 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºè‡ªé€‚åº”ç¡¬è´Ÿæ‰°åŠ¨å­¦ä¹ ï¼ˆAHNPLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æ–¹æ³•çš„ç¼ºé™·ï¼Œå³å¿½ç•¥å›¾åƒåŸºè´Ÿæ ·æœ¬çš„é‡è¦æ€§ä»¥åŠå¯¹æ­£è´Ÿæ ·æœ¬çš„å‡åŒ€å¤„ç†å’Œä¸è¶³çš„æ’åˆ—é—®é¢˜ã€‚AHNPLé€šè¿‡ç¿»è¯‘æ–‡æœ¬åŸºç¡¬è´Ÿç”Ÿæˆè¯­ä¹‰å¹²æ‰°çš„å›¾åƒåŸºè´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥å¯¹æ¯”å­¦ä¹ æ–¹æ³•å’Œå¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æ¥æ”¹è¿›æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡VLMsåœ¨å¤æ‚CRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsåœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸­é¢ä¸´åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¸­çš„ç»†å¾®è¯­ä¹‰å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¯¹æ–‡æœ¬åŸºç¡¬è´Ÿæ ·æœ¬è¿›è¡Œå¾®è°ƒæ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½†å¿½ç•¥äº†å›¾åƒåŸºè´Ÿæ ·æœ¬çš„é‡è¦æ€§ï¼Œå¯¼è‡´è§†è§‰ç¼–ç å™¨è®­ç»ƒä¸è¶³ã€‚</li>
<li>AHNPLé€šè¿‡å°†æ–‡æœ¬åŸºç¡¬è´Ÿæ ·æœ¬è½¬åŒ–ä¸ºå›¾åƒåŸŸæ¥ç”Ÿæˆå›¾åƒåŸºè´Ÿæ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚</li>
<li>AHNPLå¼•å…¥å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æ”¹è¿›æ¨¡å‹å¯¹æ¯ç§æ¨¡æ€å†…ç¡¬è´Ÿçš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>AHNPLè¿˜é‡‡ç”¨åŠ¨æ€è¾¹é™…æŸå¤±ï¼Œæ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´å¯¹æ¯”è¾¹é™…ï¼Œä»¥æé«˜å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬å¯¹çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAHNPLæ–¹æ³•èƒ½æœ‰æ•ˆæå‡VLMsåœ¨å¤æ‚CRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.15576v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.15576v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.15576v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.15576v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.15576v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Pathobiological-Dictionary-Defining-Pathomics-and-Texture-Features-Addressing-Understandable-AI-Issues-in-Personalized-Liver-Cancer-Dictionary-Version-LCP1-0"><a href="#Pathobiological-Dictionary-Defining-Pathomics-and-Texture-Features-Addressing-Understandable-AI-Issues-in-Personalized-Liver-Cancer-Dictionary-Version-LCP1-0" class="headerlink" title="Pathobiological Dictionary Defining Pathomics and Texture Features:   Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary   Version LCP1.0"></a>Pathobiological Dictionary Defining Pathomics and Texture Features:   Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary   Version LCP1.0</h2><p><strong>Authors:Mohammad R. Salmanpour, Seyed Mohammad Piri, Somayeh Sadat Mehrnia, Ahmad Shariftabrizi, Masume Allahmoradi, Venkata SK. Manem, Arman Rahmim, Ilker Hacihaliloglu</strong></p>
<p>Artificial intelligence (AI) holds strong potential for medical diagnostics, yet its clinical adoption is limited by a lack of interpretability and generalizability. This study introduces the Pathobiological Dictionary for Liver Cancer (LCP1.0), a practical framework designed to translate complex Pathomics and Radiomics Features (PF and RF) into clinically meaningful insights aligned with existing diagnostic workflows. QuPath and PyRadiomics, standardized according to IBSI guidelines, were used to extract 333 imaging features from hepatocellular carcinoma (HCC) tissue samples, including 240 PF-based-cell detection&#x2F;intensity, 74 RF-based texture, and 19 RF-based first-order features. Expert-defined ROIs from the public dataset excluded artifact-prone areas, and features were aggregated at the case level. Their relevance to the WHO grading system was assessed using multiple classifiers linked with feature selectors. The resulting dictionary was validated by 8 experts in oncology and pathology. In collaboration with 10 domain experts, we developed a Pathobiological dictionary of imaging features such as PFs and RF. In our study, the Variable Threshold feature selection algorithm combined with the SVM model achieved the highest accuracy (0.80, P-value less than 0.05), selecting 20 key features, primarily clinical and pathomics traits such as Centroid, Cell Nucleus, and Cytoplasmic characteristics. These features, particularly nuclear and cytoplasmic, were strongly associated with tumor grading and prognosis, reflecting atypia indicators like pleomorphism, hyperchromasia, and cellular orientation.The LCP1.0 provides a clinically validated bridge between AI outputs and expert interpretation, enhancing model transparency and usability. Aligning AI-derived features with clinical semantics supports the development of interpretable, trustworthy diagnostic tools for liver cancer pathology. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»å­¦è¯Šæ–­æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç„¶è€Œå…¶ä¸´åºŠåº”ç”¨å´å—é™äºè§£é‡Šæ€§å’Œé€šç”¨æ€§çš„ç¼ºä¹ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†è‚ç™Œç—…ç†ç”Ÿç‰©è¯å…¸ï¼ˆLCP1.0ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤æ‚çš„ç—…ç†å­¦å’Œæ”¾å°„å­¦ç‰¹å¾ï¼ˆPFå’ŒRFï¼‰è½¬åŒ–ä¸ºä¸ç°æœ‰è¯Šæ–­å·¥ä½œæµç›¸å»åˆçš„ä¸´åºŠæ„ä¹‰æ´å¯Ÿã€‚æ ¹æ®IBSIæŒ‡å—æ ‡å‡†åŒ–äº†çš„QuPathå’ŒPyRadiomicsè¢«ç”¨äºä»è‚ç»†èƒç™Œï¼ˆHCCï¼‰ç»„ç»‡æ ·æœ¬ä¸­æå–333ä¸ªæˆåƒç‰¹å¾ï¼ŒåŒ…æ‹¬240ä¸ªåŸºäºPFçš„ç»†èƒæ£€æµ‹&#x2F;å¼ºåº¦ã€74ä¸ªåŸºäºRFçš„çº¹ç†å’Œ19ä¸ªåŸºäºRFçš„ä¸€é˜¶ç‰¹å¾ã€‚æ¥è‡ªå…¬å…±æ•°æ®é›†çš„ä¸“å®¶å®šä¹‰ROIæ’é™¤äº†æ˜“äº§ç”Ÿä¼ªå½±çš„åŒºåŸŸï¼Œç‰¹å¾è¢«èšåˆåˆ°ç—…ä¾‹å±‚é¢ã€‚å®ƒä»¬ä¸WHOåˆ†çº§ç³»ç»Ÿçš„ç›¸å…³æ€§é€šè¿‡ä½¿ç”¨ä¸ç‰¹å¾é€‰æ‹©å™¨ç›¸å…³è”çš„å¤šé‡åˆ†ç±»å™¨è¿›è¡Œè¯„ä¼°ã€‚è¯¥è¯å…¸å¾—åˆ°äº†8åè‚¿ç˜¤å­¦å’Œç—…ç†å­¦ä¸“å®¶çš„éªŒè¯ã€‚æˆ‘ä»¬ä¸10åé¢†åŸŸä¸“å®¶åˆä½œï¼Œå¼€å‘äº†ç—…ç†ç”Ÿç‰©å­¦è¯å…¸ï¼ŒåŒ…æ‹¬æˆåƒç‰¹å¾ï¼Œä¾‹å¦‚PFså’ŒRFsã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œç»“åˆæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¨¡å‹çš„å¯å˜é˜ˆå€¼ç‰¹å¾é€‰æ‹©ç®—æ³•è¾¾åˆ°äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼ˆ0.80ï¼ŒPå€¼å°äº0.05ï¼‰ï¼Œé€‰æ‹©äº†20ä¸ªå…³é”®ç‰¹å¾ï¼Œä¸»è¦æ˜¯ä¸´åºŠå’Œç—…ç†ç‰¹å¾ï¼Œä¾‹å¦‚è´¨å¿ƒã€ç»†èƒæ ¸å’Œç»†èƒè´¨ç‰¹æ€§ã€‚è¿™äº›ç‰¹å¾ï¼Œå°¤å…¶æ˜¯æ ¸å’Œç»†èƒè´¨ç‰¹å¾ä¸è‚¿ç˜¤åˆ†çº§å’Œé¢„åå¯†åˆ‡ç›¸å…³ï¼Œåæ˜ äº†è¯¸å¦‚å¼‚å‹æ€§ã€é«˜è‰²æ€§å’Œç»†èƒå®šä½ç­‰å¼‚å¸¸æŒ‡æ ‡ã€‚LCP1.0æä¾›äº†ä¸€ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„æ¡¥æ¢ï¼Œè¿æ¥AIè¾“å‡ºå’Œä¸“å®¶è§£é‡Šï¼Œæé«˜äº†æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ç”¨æ€§ã€‚å°†AIè¡ç”Ÿçš„ç‰¹å¾ä¸ä¸´åºŠè¯­ä¹‰ç›¸ç»“åˆï¼Œæ”¯æŒå¼€å‘å¯è§£é‡Šã€å¯ä¿¡èµ–çš„è‚ç™Œç—…ç†è¯Šæ–­å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14926v1">PDF</a> 29 pages, 4 figures and 1 table</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é€šè¿‡æ„å»ºPathobiological Dictionary for Liver Cancerï¼ˆLCP1.0ï¼‰æ¡†æ¶ï¼Œå°†å¤æ‚çš„Pathmicså’ŒRadiomicsç‰¹å¾è½¬åŒ–ä¸ºä¸ç°æœ‰è¯Šæ–­å·¥ä½œæµç›¸åŒ¹é…çš„ã€å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§è§£ï¼Œä»è€Œä¿ƒè¿›äººå·¥æ™ºèƒ½åœ¨è‚è„ç™Œç—‡è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚é€šè¿‡æ ‡å‡†åŒ–æå–å½±åƒç‰¹å¾å¹¶ç»è¿‡ä¸“å®¶éªŒè¯ï¼Œå¼€å‘å‡ºå¯è§£é‡Šæ€§å¼ºã€å¯ä¿¡åº¦é«˜çš„è‚ç™Œç—…ç†è¯Šæ–­å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§é™åˆ¶äº†å…¶ä¸´åºŠåº”ç”¨ã€‚</li>
<li>Pathobiological Dictionary for Liver Cancerï¼ˆLCP 1.0ï¼‰æ¡†æ¶èƒ½å°†å¤æ‚çš„Pathmicså’ŒRadiomicsç‰¹å¾è½¬åŒ–ä¸ºå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§è§£ï¼Œä¸ç°æœ‰è¯Šæ–­æµç¨‹ç›¸ç»“åˆã€‚</li>
<li>ä½¿ç”¨QuPathå’ŒPyRadiomicsç­‰å·¥å…·ï¼Œç»“åˆIBSIæŒ‡å—ï¼Œä»è‚ç»†èƒç™Œç»„ç»‡æ ·æœ¬ä¸­æå–äº†333ä¸ªæˆåƒç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä¸“å®¶å®šä¹‰çš„ROIæ’é™¤æ˜“äº§ç”Ÿä¼ªå½±çš„åŒºåŸŸï¼Œå¹¶å¯¹ç‰¹å¾è¿›è¡Œç—…ä¾‹æ°´å¹³æ±‡æ€»ã€‚</li>
<li>é€šè¿‡å¤šåˆ†ç±»å™¨ä¸ç‰¹å¾é€‰æ‹©å™¨è¯„ä¼°ç‰¹å¾ä¸WHOåˆ†çº§ç³»ç»Ÿçš„ç›¸å…³æ€§ã€‚</li>
<li>Variable Thresholdç‰¹å¾é€‰æ‹©ç®—æ³•ç»“åˆSVMæ¨¡å‹è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ï¼ˆ0.80ï¼‰ï¼Œé€‰å‡ºçš„å…³é”®ç‰¹å¾ä¸è‚¿ç˜¤åˆ†çº§å’Œé¢„åå¯†åˆ‡ç›¸å…³ï¼Œä¸»è¦åŒ…æ‹¬ä¸´åºŠå’Œç—…ç†ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.14926v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.14926v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.14926v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2505.14926v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OCSU-Optical-Chemical-Structure-Understanding-for-Molecule-centric-Scientific-Discovery"><a href="#OCSU-Optical-Chemical-Structure-Understanding-for-Molecule-centric-Scientific-Discovery" class="headerlink" title="OCSU: Optical Chemical Structure Understanding for Molecule-centric   Scientific Discovery"></a>OCSU: Optical Chemical Structure Understanding for Molecule-centric   Scientific Discovery</h2><p><strong>Authors:Siqi Fan, Yuguang Xie, Bowen Cai, Ailin Xie, Gaochao Liu, Mu Qiao, Jie Xing, Zaiqing Nie</strong></p>
<p>Understanding the chemical structure from a graphical representation of a molecule is a challenging image caption task that would greatly benefit molecule-centric scientific discovery. Variations in molecular images and caption subtasks pose a significant challenge in both image representation learning and task modeling. Yet, existing methods only focus on a specific caption task that translates a molecular image into its graph structure, i.e., OCSR. In this paper, we propose the Optical Chemical Structure Understanding (OCSU) task, which extends low-level recognition to multilevel understanding and aims to translate chemical structure diagrams into readable strings for both machine and chemist. To facilitate the development of OCSU technology, we explore both OCSR-based and OCSR-free paradigms. We propose DoubleCheck to enhance OCSR performance via attentive feature enhancement for local ambiguous atoms. It can be cascaded with existing SMILES-based molecule understanding methods to achieve OCSU. Meanwhile, Mol-VL is a vision-language model end-to-end optimized for OCSU. We also construct Vis-CheBI20, the first large-scale OCSU dataset. Through comprehensive experiments, we demonstrate the proposed approaches excel at providing chemist-readable caption for chemical structure diagrams, which provide solid baselines for further research. Our code, model, and data are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/PharMolix/OCSU">https://github.com/PharMolix/OCSU</a>. </p>
<blockquote>
<p>ä»åˆ†å­å›¾å½¢çš„è¡¨ç¤ºç†è§£å…¶åŒ–å­¦ç»“æ„æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒæè¿°ä»»åŠ¡ï¼Œå¯¹ä»¥åˆ†å­ä¸ºä¸­å¿ƒçš„ç§‘ç ”å‘ç°å¤§æœ‰è£¨ç›Šã€‚åˆ†å­å›¾åƒçš„å¤šæ ·æ€§å’Œæè¿°çš„å­ä»»åŠ¡å¯¹å›¾åƒè¡¨ç¤ºå­¦ä¹ å’Œä»»åŠ¡å»ºæ¨¡éƒ½æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åªå…³æ³¨ç‰¹å®šçš„æè¿°ä»»åŠ¡ï¼Œå³å°†åˆ†å­å›¾åƒè½¬æ¢ä¸ºå…¶å›¾å½¢ç»“æ„ï¼Œå³OCSRï¼ˆå…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å…‰å­¦åŒ–å­¦ç»“æ„ç†è§£ï¼ˆOCSUï¼‰ä»»åŠ¡ï¼Œå®ƒæ‰©å±•äº†ä½çº§çš„è¯†åˆ«åŠŸèƒ½åˆ°å¤šçº§ç†è§£ï¼Œæ—¨åœ¨å°†åŒ–å­¦ç»“æ„å›¾è½¬åŒ–ä¸ºæœºå™¨å’ŒåŒ–å­¦å®¶éƒ½å¯è¯»çš„å­—ç¬¦ä¸²ã€‚ä¸ºäº†ä¿ƒè¿›OCSUæŠ€æœ¯çš„å‘å±•ï¼Œæˆ‘ä»¬æ¢è®¨äº†åŸºäºOCSRçš„å’Œä¸ä¾èµ–OCSRçš„ä¸¤ç§èŒƒå¼ã€‚æˆ‘ä»¬æå‡ºDoubleCheckæ–¹æ³•ï¼Œé€šè¿‡æ³¨æ„åŠ›ç‰¹å¾å¢å¼ºå±€éƒ¨æ¨¡ç³ŠåŸå­ï¼Œä»¥æé«˜OCSRæ€§èƒ½ã€‚å®ƒå¯ä»¥ä¸ç°æœ‰çš„åŸºäºSMILESçš„åˆ†å­ç†è§£æ–¹æ³•ç›¸ç»“åˆï¼Œå®ç°OCSUã€‚åŒæ—¶ï¼ŒMol-VLæ˜¯ä¸€ç§é’ˆå¯¹OCSUè¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†Vis-CheBI20ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡OCSUæ•°æ®é›†ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨æä¾›åŒ–å­¦ç»“æ„å›¾çš„åŒ–å­¦å®¶å¯è¯»æè¿°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/PharMolix/OCSU%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/PharMolix/OCSUä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15415v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ–‡ä¸­æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„ä»»åŠ¡ï¼Œåä¸ºå…‰å­¦åŒ–å­¦ç»“æ„ç†è§£ï¼ˆOCSUï¼‰ï¼Œæ—¨åœ¨å°†åŒ–å­¦ç»“æ„å›¾è½¬åŒ–ä¸ºæœºå™¨å’Œäººç±»å¯è¯»çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œä¿ƒè¿›åˆ†å­å›¾åƒçš„ç†è§£ã€‚æ–‡ç« ä»‹ç»äº†ä¸¤ç§å®ç°æ–¹å¼ï¼šåŸºäºOCSRçš„æ–¹æ³•å’ŒOCSRè‡ªç”±èŒƒå¼ã€‚é€šè¿‡åŒæ£€æŸ¥å¢å¼ºå±€éƒ¨æ¨¡ç³ŠåŸå­çš„ç‰¹å¾å¢å¼ºæ€§èƒ½ï¼Œå¹¶é€šè¿‡å…¨é¢çš„å®éªŒéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡OCSUæ•°æ®é›†Vis-CheBI20ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºå‡†ã€‚ä»£ç ã€æ¨¡å‹å’Œå¼€æºæ•°æ®å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå³å…‰å­¦åŒ–å­¦ç»“æ„ç†è§£ï¼ˆOCSUï¼‰ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨å°†åŒ–å­¦ç»“æ„å›¾è½¬åŒ–ä¸ºå¯è¯»çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œä¿ƒè¿›åˆ†å­å›¾åƒçš„ç†è§£ã€‚</li>
<li>OCSUæ‰©å±•äº†åˆ†å­å›¾åƒè¯†åˆ«çš„ä»»åŠ¡èŒƒç•´ï¼Œæ¶‰åŠåˆ°å¤šçº§ç†è§£é—®é¢˜ï¼Œä¸ºåˆ†å­ç§‘å­¦å‘ç°æä¾›äº†é‡è¦æ”¯æŒã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†ä¸¤ç§å®ç°OCSUä»»åŠ¡çš„æ–¹æ³•ï¼šåŸºäºOCSRçš„æ–¹æ³•å’ŒOCSRè‡ªç”±èŒƒå¼ï¼Œå±•ç¤ºäº†å…¶æŠ€æœ¯ç»†èŠ‚å’Œç‰¹ç‚¹ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§åŒæ£€æŸ¥ï¼ˆDoubleCheckï¼‰æŠ€æœ¯æ¥å¢å¼ºå±€éƒ¨æ¨¡ç³ŠåŸå­çš„ç‰¹å¾å¢å¼ºæ€§èƒ½ï¼Œä»è€Œæé«˜åŒ–å­¦ç»“æ„è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Vis-CheBI20ç”¨äºOCSUä»»åŠ¡çš„ç ”ç©¶å’Œå®éªŒéªŒè¯ã€‚è¿™å¯¹äºç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºé«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å…¨é¢çš„å®éªŒéªŒè¯äº†æå‡ºçš„æ–¹æ³•åœ¨åŒ–å­¦ç»“æ„å›¾è§£ææ–¹é¢çš„ä¼˜å¼‚æ€§èƒ½ï¼Œæä¾›äº†å‡†ç¡®å’Œå¯é çš„ç»“æœï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2501.15415v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2501.15415v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2501.15415v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2501.15415v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2501.15415v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FaVoR-Features-via-Voxel-Rendering-for-Camera-Relocalization"><a href="#FaVoR-Features-via-Voxel-Rendering-for-Camera-Relocalization" class="headerlink" title="FaVoR: Features via Voxel Rendering for Camera Relocalization"></a>FaVoR: Features via Voxel Rendering for Camera Relocalization</h2><p><strong>Authors:Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly</strong></p>
<p>Camera relocalization methods range from dense image alignment to direct camera pose regression from a query image. Among these, sparse feature matching stands out as an efficient, versatile, and generally lightweight approach with numerous applications. However, feature-based methods often struggle with significant viewpoint and appearance changes, leading to matching failures and inaccurate pose estimates. To overcome this limitation, we propose a novel approach that leverages a globally sparse yet locally dense 3D representation of 2D features. By tracking and triangulating landmarks over a sequence of frames, we construct a sparse voxel map optimized to render image patch descriptors observed during tracking. Given an initial pose estimate, we first synthesize descriptors from the voxels using volumetric rendering and then perform feature matching to estimate the camera pose. This methodology enables the generation of descriptors for unseen views, enhancing robustness to view changes. We extensively evaluate our method on the 7-Scenes and Cambridge Landmarks datasets. Our results show that our method significantly outperforms existing state-of-the-art feature representation techniques in indoor environments, achieving up to a 39% improvement in median translation error. Additionally, our approach yields comparable results to other methods for outdoor scenarios while maintaining lower memory and computational costs. </p>
<blockquote>
<p>ç›¸æœºé‡å®šä½æ–¹æ³•åŒ…æ‹¬ä»å¯†é›†å›¾åƒå¯¹é½åˆ°ç›´æ¥ä»æŸ¥è¯¢å›¾åƒè¿›è¡Œç›¸æœºå§¿æ€å›å½’ã€‚å…¶ä¸­ï¼Œç¨€ç–ç‰¹å¾åŒ¹é…ä»¥å…¶é«˜æ•ˆã€é€šç”¨å’Œé€šå¸¸è¾ƒè½»é‡çº§çš„æ–¹æ³•ä»¥åŠä¼—å¤šåº”ç”¨è€Œè„±é¢–è€Œå‡ºã€‚ç„¶è€Œï¼ŒåŸºäºç‰¹å¾çš„æ–¹æ³•é€šå¸¸åœ¨è§†è§’å’Œå¤–è§‚å˜åŒ–æ˜¾è‘—æ—¶ä¼šå‡ºç°å›°éš¾ï¼Œå¯¼è‡´åŒ¹é…å¤±è´¥å’Œå§¿æ€ä¼°è®¡ä¸å‡†ç¡®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº†ä¸€ç§å…¨å±€ç¨€ç–ä½†å±€éƒ¨å¯†é›†çš„äºŒç»´ç‰¹å¾çš„ä¸‰ç»´è¡¨ç¤ºã€‚é€šè¿‡è·Ÿè¸ªå’Œä¸‰è§’æµ‹é‡ä¸€ç³»åˆ—å¸§ä¸­çš„åœ°æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¼˜åŒ–çš„ç¨€ç–ä½“ç´ åœ°å›¾ï¼Œä»¥å‘ˆç°è·Ÿè¸ªè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„å›¾åƒå—æè¿°ç¬¦ã€‚ç»™å®šåˆå§‹å§¿æ€ä¼°è®¡ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä½“ç§¯æ¸²æŸ“æŠ€æœ¯ä»ä½“ç´ åˆæˆæè¿°ç¬¦ï¼Œç„¶åè¿›è¡Œç‰¹å¾åŒ¹é…ä»¥ä¼°è®¡ç›¸æœºå§¿æ€ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—èƒ½å¤Ÿä¸ºæœªè§è¿‡çš„è§†å›¾ç”Ÿæˆæè¿°ç¬¦ï¼Œæé«˜äº†å¯¹è§†å›¾å˜åŒ–çš„é²æ£’æ€§ã€‚æˆ‘ä»¬åœ¨7åœºæ™¯å’Œå‰‘æ¡¥åœ°æ ‡æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®¤å†…ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„ç‰¹å¾è¡¨ç¤ºæŠ€æœ¯ï¼Œåœ¨å¹³å‡å¹³ç§»è¯¯å·®ä¸Šæœ€å¤šæé«˜äº†39%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®¤å¤–åœºæ™¯çš„ç»“æœä¸å…¶ä»–æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶é™ä½äº†å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07571v4">PDF</a> In Proceedings of the IEEE&#x2F;CVF Winter Conference on Applications of   Computer Vision (WACV), Tucson, Arizona, US, Feb 28-Mar 4, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–ç‰¹å¾åŒ¹é…ä¸å±€éƒ¨å¯†é›†ä¸‰ç»´è¡¨ç¤ºçš„ç›¸æœºé‡å®šä½æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿½è¸ªå’Œä¸‰è§’æµ‹é‡åœ°æ ‡åºåˆ—å¸§æ„å»ºç¨€ç–ä½“ç´ åœ°å›¾ï¼Œä¼˜åŒ–æ¸²æŸ“å›¾åƒè¡¥ä¸æè¿°ç¬¦ã€‚åˆ©ç”¨åˆå§‹å§¿æ€ä¼°è®¡ï¼Œé€šè¿‡ä½“ç§¯æ¸²æŸ“åˆæˆä½“ç´ æè¿°ç¬¦ï¼Œç„¶åè¿›è¡Œç‰¹å¾åŒ¹é…ä¼°è®¡ç›¸æœºå§¿æ€ã€‚æ­¤æ–¹æ³•èƒ½ç”Ÿæˆæœªè§è§†å›¾çš„æè¿°ç¬¦ï¼Œå¢å¼ºå¯¹è§†è§’å˜åŒ–çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨å®¤å†…ç¯å¢ƒä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰ç‰¹å¾è¡¨ç¤ºæŠ€æœ¯ï¼Œå…¶ä¸­ä½å¹³ç§»è¯¯å·®æ”¹è¿›é«˜è¾¾39%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›¸æœºé‡å®šä½æ–¹æ³•åŒ…æ‹¬ä»å¯†é›†å›¾åƒå¯¹é½åˆ°ç›´æ¥ç›¸æœºå§¿æ€å›å½’æŸ¥è¯¢å›¾åƒç­‰å¤šç§æ–¹æ³•ã€‚</li>
<li>ç¨€ç–ç‰¹å¾åŒ¹é…æ˜¯ä¸€ç§é«˜æ•ˆã€é€šç”¨ã€è½»é‡çº§çš„æ–¹æ³•ï¼Œä½†é¢ä¸´è§†è§’å’Œå¤–è§‚å˜åŒ–å¯¼è‡´çš„åŒ¹é…å¤±è´¥å’Œå§¿æ€ä¼°è®¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–ç‰¹å¾åŒ¹é…å’Œå±€éƒ¨å¯†é›†ä¸‰ç»´è¡¨ç¤ºçš„æ–°æ–¹æ³•ï¼Œæ„å»ºç¨€ç–ä½“ç´ åœ°å›¾ä»¥ä¼˜åŒ–æ¸²æŸ“å›¾åƒè¡¥ä¸æè¿°ç¬¦ã€‚</li>
<li>é€šè¿‡ä½“ç§¯æ¸²æŸ“åˆæˆä½“ç´ æè¿°ç¬¦ï¼Œè¿›è¡Œç‰¹å¾åŒ¹é…ä¼°è®¡ç›¸æœºå§¿æ€ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½ç”Ÿæˆæœªè§è§†å›¾çš„æè¿°ç¬¦ï¼Œå¢å¼ºå¯¹è§†è§’å˜åŒ–çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å®¤å†…ç¯å¢ƒä¸‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰ç‰¹å¾è¡¨ç¤ºæŠ€æœ¯ï¼Œå¹¶åœ¨ä¸­ä½å¹³ç§»è¯¯å·®ä¸Šå®ç°é«˜è¾¾39%çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2409.07571v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2409.07571v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2409.07571v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2409.07571v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks"><a href="#Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks" class="headerlink" title="Generalizing Medical Image Representations via Quaternion Wavelet   Networks"></a>Generalizing Medical Image Representations via Quaternion Wavelet   Networks</h2><p><strong>Authors:Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency&#x2F;approximation bands and high-frequency&#x2F;fine-grained features. Then, it weighs the most representative set of sub-bands to be involved as input to any other neural model for image processing, replacing standard data samples. We conduct an extensive experimental evaluation comprising different datasets, diverse image analysis, and synthesis tasks including reconstruction, segmentation, and modality translation. We also evaluate QUAVE in combination with both real and quaternion-valued models. Results demonstrate the effectiveness and the generalizability of the proposed framework that improves network performance while being flexible to be adopted in manifold scenarios and robust to domain shifts. The full code is available at: <a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a>. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œé€šç”¨æ€§ç”±äºä¸åŒæ¥æºå’Œä»»åŠ¡çš„æ•°æ®é›†å¯ç”¨æ€§å¢åŠ è€Œæˆä¸ºä¸€ä¸ªå¹¿æ³›çš„ç ”ç©¶é¢†åŸŸã€‚åœ¨å¤„ç†åŒ»ç–—æ•°æ®æ—¶ï¼Œè¿™ä¸ªé—®é¢˜æ›´åŠ çªå‡ºï¼Œå› ä¸ºæ²¡æœ‰ç»Ÿä¸€çš„æ–¹æ³•å­¦æ ‡å‡†ï¼Œå¯¼è‡´ä¸åŒæˆåƒä¸­å¿ƒæˆ–é€šè¿‡å„ç§è®¾å¤‡å’Œè¾…åŠ©å› ç´ è·å¾—çš„æ•°æ®å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ã€é€šç”¨ã€ä¸å—æ•°æ®å’Œä»»åŠ¡é™åˆ¶çš„åˆ†ææ¡†æ¶ï¼Œèƒ½å¤Ÿä»åŒ»å­¦å›¾åƒä¸­æå–é‡è¦ç‰¹å¾ã€‚æ‰€æå‡ºçš„å››å…ƒå°æ³¢ç½‘ç»œï¼ˆQUAVEï¼‰å¯ä»¥è½»æ¾åœ°ä¸ä»»ä½•ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†ææˆ–åˆæˆä»»åŠ¡é›†æˆï¼Œå¹¶ä¸”å¯ä»¥ä¸å®æ•°ã€å››å…ƒæ•°æˆ–è¶…å¤æ•°æ¨¡å‹ç›¸ç»“åˆï¼Œå°†å…¶æ¨å¹¿åˆ°å•é€šé“æ•°æ®ã€‚QUAVEé¦–å…ˆé€šè¿‡å››å…ƒå°æ³¢å˜æ¢æå–ä¸åŒçš„å­å¸¦ï¼Œå¾—åˆ°ä½é¢‘&#x2F;è¿‘ä¼¼å¸¦å’Œé«˜é¢‘&#x2F;ç²¾ç»†ç‰¹å¾ã€‚ç„¶åï¼Œå®ƒåŠ æƒæœ€å…·ä»£è¡¨æ€§çš„å­å¸¦é›†ä½œä¸ºè¾“å…¥åˆ°å…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œå›¾åƒå¤„ç†ï¼Œä»£æ›¿æ ‡å‡†æ•°æ®æ ·æœ¬ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸åŒçš„æ•°æ®é›†ã€å¤šæ ·åŒ–çš„å›¾åƒåˆ†æå’Œåˆæˆä»»åŠ¡ï¼Œå¦‚é‡å»ºã€åˆ†å‰²å’Œæ¨¡æ€è½¬æ¢ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†QUAVEä¸å®æ•°å’Œå››å…ƒæ•°å€¼æ¨¡å‹çš„ç»„åˆã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œåœ¨æé«˜ç½‘ç»œæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿçµæ´»åœ°åº”ç”¨äºå¤šç§åœºæ™¯å¹¶å¯¹é¢†åŸŸå˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚å®Œæ•´ä»£ç å¯åœ¨ï¼š&lt;<a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10224v5">PDF</a> Paper accepted to Neurocomputing Journal</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç¥ç»ç½‘ç»œæ³›åŒ–èƒ½åŠ›åœ¨åŒ»å­¦æ•°æ®å¤„ç†ä¸Šçš„ä¸è¶³ï¼Œæå‡ºä¸€ç§æ–°å‹é€šç”¨æ¡†æ¶â€”â€”å››å…ƒå°æ³¢ç½‘ç»œï¼ˆQUAVEï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæå–åŒ»å­¦å›¾åƒçš„é‡è¦ç‰¹å¾ï¼Œé€‚ç”¨äºä¸åŒæ¥æºå’Œä»»åŠ¡çš„å¤šç§æ•°æ®é›†ã€‚QUAVEé€šè¿‡å››å…ƒå°æ³¢å˜æ¢æå–ä¸åŒå­å¸¦ï¼Œå¹¶ç»“åˆä½é¢‘å’Œé«˜é¢‘ç‰¹å¾ï¼Œç„¶ååŠ æƒæœ€å…·ä»£è¡¨æ€§çš„å­å¸¦ä½œä¸ºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹çš„è¾“å…¥è¿›è¡Œå›¾åƒå¤„ç†ã€‚å®éªŒè¯æ˜ï¼ŒQUAVEæ¡†æ¶æœ‰æ•ˆä¸”é€šç”¨ï¼Œèƒ½æé«˜ç½‘ç»œæ€§èƒ½ï¼Œé€‚ç”¨äºå¤šç§åœºæ™¯ï¼Œå¯¹é¢†åŸŸå˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæ³›åŒ–èƒ½åŠ›å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œå°¤å…¶åœ¨å¤„ç†æ¥è‡ªä¸åŒæ¥æºå’Œä»»åŠ¡çš„åŒ»ç–—æ•°æ®é›†æ—¶ã€‚</li>
<li>QUAVEæ¡†æ¶æ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒæ•°æ®çš„æ–°å‹é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸åŒæˆåƒä¸­å¿ƒå’Œè®¾å¤‡è·å–çš„åŒ»ç–—æ•°æ®å·®å¼‚é—®é¢˜ã€‚</li>
<li>QUAVEèƒ½å¤Ÿè½»æ¾é›†æˆåˆ°ä»»ä½•ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†ææˆ–åˆæˆä»»åŠ¡ä¸­ï¼Œå¹¶èƒ½ä¸å®å€¼ã€å››å…ƒæ•°æˆ–è¶…å¤æ•°æ¨¡å‹ç›¸ç»“åˆï¼Œæ¨å¹¿åˆ°å•é€šé“æ•°æ®ã€‚</li>
<li>QUAVEé€šè¿‡å››å…ƒå°æ³¢å˜æ¢æå–ä¸åŒå­å¸¦ï¼ŒåŒ…æ‹¬ä½é¢‘å’Œé«˜é¢‘ç‰¹å¾ã€‚</li>
<li>QUAVEé€šè¿‡åŠ æƒæœ€å…·ä»£è¡¨æ€§çš„å­å¸¦ï¼Œä½œä¸ºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹çš„è¾“å…¥ï¼Œè¿›è¡Œå›¾åƒå¤„ç†ã€‚</li>
<li>å®éªŒè¯æ˜QUAVEæ¡†æ¶æœ‰æ•ˆä¸”é€šç”¨ï¼Œèƒ½å¤Ÿæé«˜ç½‘ç»œæ€§èƒ½ï¼Œå¹¶é€‚åº”å¤šç§å›¾åƒåˆ†æå’Œåˆæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬é‡å»ºã€åˆ†å‰²å’Œæ¨¡æ€è½¬æ¢ã€‚</li>
<li>QUAVEæ¡†æ¶å¯¹é¢†åŸŸå˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œå…¶å®Œæ•´ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.10224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_I2I Translation/2310.10224v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e5f7e0b979aba75fcc5cbfca07feb3df.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  LiveVLM Efficient Online Video Understanding via Streaming-Oriented KV   Cache and Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_Few-Shot/2505.15425v1/page_4_0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Four Eyes Are Better Than Two Harnessing the Collaborative Potential of   Large Models via Differentiated Thinking and Complementary Ensembles
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18863.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
