<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-06  TRAVELER A Benchmark for Evaluating Temporal Reasoning across Vague,   Implicit and Explicit References">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8707e3770fca67020b56ee958d37a14a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-06-更新"><a href="#2025-05-06-更新" class="headerlink" title="2025-05-06 更新"></a>2025-05-06 更新</h1><h2 id="TRAVELER-A-Benchmark-for-Evaluating-Temporal-Reasoning-across-Vague-Implicit-and-Explicit-References"><a href="#TRAVELER-A-Benchmark-for-Evaluating-Temporal-Reasoning-across-Vague-Implicit-and-Explicit-References" class="headerlink" title="TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague,   Implicit and Explicit References"></a>TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague,   Implicit and Explicit References</h2><p><strong>Authors:Svenja Kenneweg, Jörg Deigmöller, Philipp Cimiano, Julian Eggert</strong></p>
<p>Understanding and resolving temporal references is essential in Natural Language Understanding as we often refer to the past or future in daily communication. Although existing benchmarks address a system’s ability to reason about and resolve temporal references, systematic evaluation of specific temporal references remains limited. Towards closing this gap, we introduce TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering paradigm and consists of questions involving temporal references with the corresponding correct answers. TRAVELER assesses models’ abilities to resolve explicit, implicit relative to speech time, and vague temporal references. Beyond investigating the performance of state-of-the-art LLMs depending on the type of temporal reference, our benchmark also allows evaluation of performance in relation to the length of the set of events. For the category of vague temporal references, ground-truth answers were established via human surveys on Prolific, following a procedure similar to the one from Kenneweg et al. To demonstrate the benchmark’s applicability, we evaluate four state-of-the-art LLMs using a question-answering task encompassing 3,300 questions. Our findings show that while the benchmarked LLMs can answer questions over event sets with a handful of events and explicit temporal references successfully, performance clearly deteriorates with larger event set length and when temporal references get less explicit. Notably, the vague question category exhibits the lowest performance across all models.   The benchmark is publicly available at: <a target="_blank" rel="noopener" href="https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER">https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER</a> </p>
<blockquote>
<p>理解和解决时间参照对于自然语言理解至关重要，因为我们在日常交流中经常提及过去或未来。尽管现有的基准测试可以评估系统在推理和解决时间参照方面的能力，但对特定时间参照的评估仍然有限。为了缩小这一差距，我们引入了旅行者（TRAVELER）这一新型合成基准数据集，它遵循问答模式，包含涉及时间参照的问题和相应的正确答案。旅行者评估模型解决明确时间参照、隐含相对语时和模糊时间参照的能力。除了根据时间参照类型调查当前最前沿的大型语言模型性能外，我们的基准测试还允许根据事件集长度评估性能。对于模糊时间参照类别，我们通过Prolific上的人类调查来确定真实答案，采用与Kenneweg等人相似的方法。为了证明基准测试的适用性，我们使用包含3300个问题的问答任务评估了四种最前沿的大型语言模型。我们的研究结果表明，虽然所评估的大型语言模型能够成功回答涉及少量事件和明确时间参照的事件集问题，但随着事件集长度的增加和时间参照的模糊性增加，性能明显下降。值得注意的是，模糊问题类别在所有模型中的表现最差。基准测试公开可用：<a target="_blank" rel="noopener" href="https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER%E3%80%82">https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01325v1">PDF</a> 24 pages, 6 figures, submitted to Springer Nature Computer Science</p>
<p><strong>Summary</strong></p>
<p>本文介绍了理解并解决时间参考在自然语言理解中的重要性。为解决现有时间参考解析的评估缺口，提出一个名为“旅行者”（TRAVELER）的新合成基准数据集。该数据集遵循问答模式，包含涉及不同类型时间引用的相关问题及其正确答案。旅行者不仅评估模型对明确、隐含和模糊时间引用的解析能力，还允许根据事件集的长度评估性能。为模糊时间引用类别，通过类似人类调查的方式建立基准答案。为了证明该基准的适用性，对四种先进的LLMS进行了评估。发现虽然这些模型可以成功回答涉及少量事件和明确时间引用的问绝，但随着事件集长度的增加和时间引用的不明确性，性能明显下降。特别是模糊问题类别的表现尤为不佳。该基准数据集已在指定链接中公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间参考理解和解析在自然语言处理中至关重要。</li>
<li>现有时间参考解析的评估标准存在缺口。</li>
<li>引入新的合成基准数据集“旅行者”（TRAVELER），用于评估模型处理不同类型时间引用的能力。</li>
<li>旅行者评估模型在解决明确、隐含和模糊时间引用方面的能力。</li>
<li>旅行者允许根据事件集的长度评估性能差异。</li>
<li>通过类似的人类调查方式确定模糊时间引用的基准答案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01325">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c7b8543d9f380a7b507fcb53f18b34e7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Llama-Nemotron-Efficient-Reasoning-Models"><a href="#Llama-Nemotron-Efficient-Reasoning-Models" class="headerlink" title="Llama-Nemotron: Efficient Reasoning Models"></a>Llama-Nemotron: Efficient Reasoning Models</h2><p><strong>Authors:Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung</strong></p>
<p>We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes – Nano (8B), Super (49B), and Ultra (253B) – and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models – LN-Nano, LN-Super, and LN-Ultra – under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM. </p>
<blockquote>
<p>我们介绍了Llama-Nemotron系列模型，这是一个开放的异质推理模型家族，具有出色的推理能力、推理效率和开放的企业使用许可。该系列有三种型号：Nano（8B）、Super（49B）和Ultra（253B），与DeepSeek-R1等最先进的推理模型相比具有竞争力，同时提供卓越的推理吞吐量和内存效率。在本报告中，我们讨论了这些模型的训练流程，包括使用Llama 3模型的神经网络架构搜索以加速推理、知识蒸馏和持续预训练，随后是注重推理的后期训练阶段，包括两个主要部分：监督微调和大规模增强学习。Llama-Nemotron模型是第一个支持动态推理切换的开源模型，允许用户在推理过程中切换标准聊天和推理模式。为了进一步支持开放研究和促进模型开发，我们提供了以下资源：1.我们在商业许可的NVIDIA开放模型许可协议下发布了Llama-Nemotron推理模型——LN-Nano、LN-Super和LN-Ultra。2.我们发布了完整的后期训练数据集：Llama-Nemotron-Post-Training-Dataset。3.我们还发布了我们的训练代码库：NeMo、NeMo-Aligner和Megatron-LM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00949v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Llama 3模型的神经网络架构搜索、知识蒸馏和持续预训练等训练程序，我们推出了Llama-Nemotron系列模型，包括Nano、Super和Ultra三种规模。该系列模型具备出色的推理能力、推理效率和开放企业使用许可。它们是第一个支持动态推理切换的开源模型，可在推理过程中在标准聊天和推理模式之间进行切换。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Llama-Nemotron系列模型是一个开放的异质推理模型家族，提供Nano、Super和Ultra三种规模，具有出色的推理能力和效率。</li>
<li>该系列模型采用Llama 3模型的神经网络架构搜索进行加速推理，并结合知识蒸馏和持续预训练。</li>
<li>模型在推理聚焦的后训练阶段包括监督微调和大规模强化学习两个主要部分。</li>
<li>Llama-Nemotron模型是第一个支持动态推理切换的开源模型，用户可在推理过程中在标准聊天和推理模式之间切换。</li>
<li>模型资源包括发布Llama-Nemotron推理模型（LN-Nano、LN-Super、LN-Ultra）、完整的后训练数据集以及训练代码库。</li>
<li>模型采用NVIDIA的开放模型许可协议，允许商业使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00949">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2bbb07396e3034fd7cb8c23da4bb4822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffc8d12a6834b4c9e396a6053dddb5b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8707e3770fca67020b56ee958d37a14a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a475bf79d1318e2ef9543170a5d0b56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-955b8ad12e7bdca49170d8f4949d0583.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Autonomous-Embodied-Agents-When-Robotics-Meets-Deep-Learning-Reasoning"><a href="#Autonomous-Embodied-Agents-When-Robotics-Meets-Deep-Learning-Reasoning" class="headerlink" title="Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning"></a>Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning</h2><p><strong>Authors:Roberto Bigazzi</strong></p>
<p>The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks. </p>
<blockquote>
<p>计算能力的增强和深度学习的革命为人工智能研究带来了新的课题和前沿领域。一个名为“实体人工智能”的新领域在计算机视觉、机器人技术和决策制定的交汇点应运而生，并在过去几年中越来越重要。实体人工智能旨在促进智能自主机器人的发展及其在社会的部署。最近出现的庞大三维模型集合，可用于创建逼真的机器人模拟，这允许基于学习的代理进行更快、更安全的训练，并在实际机器人平台上部署前对行为进行评估。这些智能代理旨在在一个可能未知的环境中执行特定任务。为此，代理在模拟训练期间学习不断与周围环境进行交互，例如从环境中收集信息、编码和提取任务的有用线索以及朝着最终目标采取行动；代理的每个行动都会影响其交互方式。本论文将跟踪实体代理在室内环境中的完整创建过程，从概念到实施和部署。我们旨在为实体人工智能和自主代理的研究做出贡献，以促进该领域的未来发展。我们详细分析了实现智能实体代理的程序背后的步骤，包括对当前文献的彻底描述、所提出方法的技术解释以及关于相关机器人任务的精确实验性研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00935v1">PDF</a> Ph.D. Dissertation</p>
<p><strong>Summary</strong><br>     深度学习革命和计算能力的提升推动了人工智能新领域——具象人工智能的发展。该领域旨在培育智能自主机器人及其在社会的部署。利用大量3D模型进行逼真的机器人模拟，可以更快、更安全地对基于学习的代理进行训练，并对其行为进行仔细评估，再部署到真实的机器人平台上。该领域的目标是贡献于具象人工智能和自主代理的研究，以推动未来在这一领域的工作。本文详细阐述了构建室内环境具象代理的完整过程，从概念到实施和部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习革命和计算能力的提升推动了人工智能新领域的发展，即具象人工智能。</li>
<li>具象人工智能的目标在于培育智能自主机器人及其在社会的部署。</li>
<li>利用大量3D模型进行逼真的机器人模拟可以更快、更安全地训练基于学习的代理。</li>
<li>智能代理在模拟环境中进行连续互动以完成任务，如收集环境信息、编码和提取任务中的有用线索以及实现最终目标的行动。</li>
<li>具象人工智能中的代理实施涉及从概念到实施和部署的完整过程。</li>
<li>文章对当前的最新技术进行了全面的描述和分析，包括对提出的方法的技术解释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00935">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5fa2edf6dfed9e3fdba0a471e223de16.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Thoughts-without-Thinking-Reconsidering-the-Explanatory-Value-of-Chain-of-Thought-Reasoning-in-LLMs-through-Agentic-Pipelines"><a href="#Thoughts-without-Thinking-Reconsidering-the-Explanatory-Value-of-Chain-of-Thought-Reasoning-in-LLMs-through-Agentic-Pipelines" class="headerlink" title="Thoughts without Thinking: Reconsidering the Explanatory Value of   Chain-of-Thought Reasoning in LLMs through Agentic Pipelines"></a>Thoughts without Thinking: Reconsidering the Explanatory Value of   Chain-of-Thought Reasoning in LLMs through Agentic Pipelines</h2><p><strong>Authors:Ramesh Manuvinakurike, Emanuel Moss, Elizabeth Anne Watkins, Saurav Sahay, Giuseppe Raffa, Lama Nachman</strong></p>
<p>Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals. </p>
<blockquote>
<p>代理管道为人类中心的可解释性带来了新的挑战和机会。HCXAI社区仍在努力探索如何以可操作的方式使大型语言模型的内部工作透明化。代理管道由多个大型语言模型组成，这些模型以最小的手动控制进行协同工作。在这篇论文中，我们展示了一个感知任务指导系统的代理管道实现的初步发现。通过定量和定性分析，我们研究了链思维推理（CoT推理）在代理管道内的工作方式，这是大型语言模型中解释性的常见方式。我们证明，单独的CoT推理并不能产生更好的输出，也不能提供解释性，因为它往往产生没有解释性的解释，即它们并不能提高最终用户理解系统或实现目标的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00875v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Agentic pipeline为以人为中心的解释性带来了新兴挑战与机遇。HCXAI社区仍在努力探索如何以最佳方式使大型语言模型的内部工作方式以可操作的方式透明化。Agentic pipeline由多个大型语言模型协同工作，仅需极少的人为控制。本文通过定量与定性分析，研究了思维链（Chain-of-Thought，简称CoT）推理在Agentic pipeline中的应用。我们证明单一的思维链推理并不助于提高输出质量或解释性，反而容易产生不清晰的解释，导致最终用户无法更好地了解系统或实现目标。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agentic pipeline带来以人为中心的解释性挑战与机遇。</li>
<li>HCXAI社区正在探索如何使大型语言模型的内部工作方式透明化。</li>
<li>Agentic pipeline包含多个协同工作的大型语言模型，具备低度人为控制特性。</li>
<li>本研究通过对定量和定性分析发现思维链（Chain-of-Thought，简称CoT）推理在Agentic pipeline中的应用特点。</li>
<li>CoT推理并不助于提高输出质量或解释性。</li>
<li>单一的CoT推理产生的解释可能缺乏清晰度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00875">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46fef90c14628eb317dacb577a420bf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c1730e336f19e5806a6569f4f2809a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8e7119b8f77b9eb74e50c107558b824.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-793e282e3935908c94c49cd8633699d4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-Ethics-Benchmark-A-Three-Dimensional-Assessment-System-for-Evaluating-Moral-Reasoning-in-Large-Language-Models"><a href="#LLM-Ethics-Benchmark-A-Three-Dimensional-Assessment-System-for-Evaluating-Moral-Reasoning-in-Large-Language-Models" class="headerlink" title="LLM Ethics Benchmark: A Three-Dimensional Assessment System for   Evaluating Moral Reasoning in Large Language Models"></a>LLM Ethics Benchmark: A Three-Dimensional Assessment System for   Evaluating Moral Reasoning in Large Language Models</h2><p><strong>Authors:Junfeng Jiao, Saleh Afroogh, Abhejay Murali, Kevin Chen, David Atkinson, Amit Dhurandhar</strong></p>
<p>This study establishes a novel framework for systematically evaluating the moral reasoning capabilities of large language models (LLMs) as they increasingly integrate into critical societal domains. Current assessment methodologies lack the precision needed to evaluate nuanced ethical decision-making in AI systems, creating significant accountability gaps. Our framework addresses this challenge by quantifying alignment with human ethical standards through three dimensions: foundational moral principles, reasoning robustness, and value consistency across diverse scenarios. This approach enables precise identification of ethical strengths and weaknesses in LLMs, facilitating targeted improvements and stronger alignment with societal values. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at <a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> The-Responsible-AI-Initiative&#x2F;LLM_Ethics_Benchmark.git. </p>
<blockquote>
<p>本研究建立了一个新型框架，用于系统地评估大型语言模型（LLM）的道德推理能力，随着它们越来越多地融入关键的社会领域。当前的评价方法缺乏评估AI系统中微妙道德决策所需的精确度，从而产生了重大的问责空白。我们的框架通过三个维度来衡量与人类道德标准的契合度：基础道德原则、推理的稳健性以及在不同场景中的价值一致性。这种方法能够精确地识别LLM中的道德优势和劣势，促进有针对性的改进和更紧密地与社会价值契合。为了促进透明度和协作进步在伦理人工智能的发展中，我们在<a target="_blank" rel="noopener" href="https://github.com/The-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E5%BA%93%E3%80%82">https://github.com/The-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git公开发布我们的基准数据集和评估代码库。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00853v1">PDF</a> </p>
<p><strong>Summary</strong>：该研究建立了一个新颖的框架，用于系统地评估大型语言模型（LLM）的道德推理能力，随着它们越来越多地融入关键的社会领域，现有的评估方法缺乏评估AI系统中微妙道德决策所需的精确性，导致存在重大的问责差距。该研究通过三个维度量化与人类道德标准的对齐程度：基础道德原则、推理的稳健性和在不同场景中的价值一致性。该方法能够精确地识别LLM中的道德优势和弱点，促进有针对性的改进和与社会价值更紧密的对齐。为了促进透明度和伦理人工智能的协作发展，研究团队公开分享了他们的基准数据集和评估代码库。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>该研究提出了一个评估大型语言模型道德推理能力的新框架，填补了现有评估方法的不足。</li>
<li>框架从三个维度量化模型与人类道德标准的对齐程度：基础道德原则、推理的稳健性和价值一致性。</li>
<li>通过精确识别LLM的道德优势和弱点，有助于针对性地进行改进，使之更好地符合社会价值。</li>
<li>公开的基准数据集和评估代码库旨在促进伦理AI发展的透明度和协作进步。</li>
<li>当前评估方法缺乏精确评估AI系统中微妙道德决策的能力，导致问责差距。</li>
<li>新框架有助于解决这一挑战，并促进LLM在关键社会领域中的更广泛应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00853">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8e45955377bed7efd4ab705bcb30c015.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4cd9d966c3bd75945caa1998c6768c2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation"><a href="#SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation" class="headerlink" title="SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation"></a>SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation</h2><p><strong>Authors:Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song</strong></p>
<p>Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan – a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics. </p>
<blockquote>
<p>在机器人技术中，特别是在大规模、动态环境中进行有效的路径规划仍然是一个重大挑战。虽然大型语言模型（LLM）提供了强大的推理能力，但它们的高计算成本和在动态场景中的有限适应性阻碍了它们在边缘设备上的实时部署。我们提出了SmallPlan——一个利用LLM作为教师模型来训练用于高级路径规划任务的小型语言模型（SLM）的新型框架。在SmallPlan中，SLM提供最优动作序列，以在紧凑地代表全尺寸3D场景的场景图中进行导航。SLM以模拟驱动、交替的方式进行训练，采用LLM指导的监督微调（SFT）和强化学习（RL）。这一策略不仅使SLM能够成功完成导航任务，还使它们意识到旅行距离和试验次数等重要因素。通过实验，我们证明经过微调SLM在序列路径规划方面的表现与GPT-4o等大型模型具有竞争力，且不会遭受幻觉和过度拟合的困扰。SmallPlan资源效率高，非常适合在边缘设备部署，推动实用型自主机器人技术的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00831v1">PDF</a> Paper is under review</p>
<p><strong>Summary</strong></p>
<p>在机器人学中，大规模动态环境下的路径规划是一大挑战。大型语言模型（LLMs）具备强大的推理能力，但在动态场景中因其高计算成本和有限的适应性而难以实时部署在边缘设备上。本研究提出了SmallPlan框架，利用LLMs作为教师模型来训练轻量级的小型语言模型（SLMs），用于高级路径规划任务。SmallPlan通过SLM生成最优动作序列，在场景图中导航，紧凑地表示全尺寸3D场景。SLM训练采用仿真驱动、交替进行的方式，结合LLM指导的监督微调（SFT）和强化学习（RL）。此策略不仅使SLM成功完成导航任务，还能使其考虑旅行距离和试验次数等重要因素。实验证明，微调后的SLM在序列路径规划上的表现与GPT-4o等大型模型相当，且不存在幻觉和过度拟合问题。SmallPlan具有资源效率高的特点，非常适合部署在边缘设备上，推动实用型自主机器人技术的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）虽具有强大的推理能力，但在动态环境中的路径规划问题上存在高计算成本和适应性有限的挑战。</li>
<li>SmallPlan框架利用LLMs作为教师模型，训练轻量级的小型语言模型（SLMs）进行高级路径规划任务。</li>
<li>SLMs能够在场景图中生成最优动作序列，以紧凑方式表示全尺寸3D场景。</li>
<li>SLMs采用仿真驱动、结合监督微调（SFT）和强化学习（RL）的方式进行训练，使其能在考虑重要因素的情形下成功完成导航任务。</li>
<li>精细调校的SLM在序列路径规划上的表现与大型模型相当，且不存在幻觉和过度拟合问题。</li>
<li>SmallPlan框架的资源效率高，适合部署在边缘设备上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b2f9db9994180ab9a52ecc6445b8c53d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d4bfd2544253742e0f454092cee4cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99e74c9601a2faff20a2aaa27abb4f69.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SpatialLLM-A-Compound-3D-Informed-Design-towards-Spatially-Intelligent-Large-Multimodal-Models"><a href="#SpatialLLM-A-Compound-3D-Informed-Design-towards-Spatially-Intelligent-Large-Multimodal-Models" class="headerlink" title="SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent   Large Multimodal Models"></a>SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent   Large Multimodal Models</h2><p><strong>Authors:Wufei Ma, Luoxin Ye, Nessa McWeeney, Celso M de Melo, Alan Yuille, Jieneng Chen</strong></p>
<p>Humans naturally understand 3D spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. Current large multimodal models (LMMs), however, lack of this capability of 3D spatial reasoning. This limitation stems from the scarcity of 3D training data and the bias in current model designs toward 2D data. In this paper, we systematically study the impact of 3D-informed data, architecture, and training setups, introducing SpatialLLM, a large multi-modal model with advanced 3D spatial reasoning abilities. To address data limitations, we develop two types of 3D-informed training datasets: (1) 3D-informed probing data focused on object’s 3D location and orientation, and (2) 3D-informed conversation data for complex spatial relationships. Notably, we are the first to curate VQA data that incorporate 3D orientation relationships on real images. Furthermore, we systematically integrate these two types of training data with the architectural and training designs of LMMs, providing a roadmap for optimal design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM advances machines toward highly capable 3D-informed reasoning, surpassing GPT-4o performance by 8.7%. Our systematic empirical design and the resulting findings offer valuable insights for future research in this direction. </p>
<blockquote>
<p>人类自然理解3D空间关系，能够进行复杂的推理，如预测来自不同方向的车辆碰撞。然而，当前的大型多模态模型（LMM）缺乏这种3D空间推理能力。这一局限性源于3D训练数据的稀缺性以及当前模型设计对2D数据的偏向。在本文中，我们系统地研究了3D信息数据、架构和训练设置的影响，并引入了具备先进3D空间推理能力的大型多模态模型SpatialLLM。为了解决数据限制问题，我们开发了两类3D信息训练数据集：（1）专注于物体的3D位置和方向的3D信息探测数据；（2）用于复杂空间关系的3D信息对话数据。值得注意的是，我们是第一个在真实图像上整合3D方位关系的问答（VQA）数据。此外，我们系统地将这两种类型的训练数据与LMM的架构和训练设计相结合，为优化设计提供了路线图，旨在实现卓越的3D推理能力。我们的SpatialLLM推动了机器在3D信息推理方面的能力，超越了GPT-4o的性能，提高了8.7%。我们的系统实证设计和研究结果为未来在这一方向的研究提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00788v1">PDF</a> CVPR 2025 highlight, camera ready version</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了人类自然理解的3D空间关系的能力，并指出当前大型多模态模型（LMMs）缺乏这种3D空间推理能力。文章通过研究和引入SpatialLLM模型，解决了这一问题。该模型具有先进的3D空间推理能力，通过开发两种类型的3D训练数据集——专注于物体的3D位置和方向的3D感知探测数据以及用于复杂空间关系的3D感知对话数据，来克服数据缺乏的局限性。SpatialLLM的设计整合了这两种训练数据，为大型多模态模型的架构和训练设计提供了实现优越3D推理能力的路线图。该模型超越了GPT-4o的性能，为未来的相关研究提供了有价值的见解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>人类自然具备理解复杂3D空间关系的能力，但当前的大型多模态模型（LMMs）缺乏这种能力。</li>
<li>3D空间推理能力的缺乏源于3D训练数据的稀缺以及现有模型设计对2D数据的偏向。</li>
<li>提出了SpatialLLM模型，具备先进的3D空间推理能力。</li>
<li>开发两种类型的3D训练数据集以克服数据缺乏的问题：专注于物体3D位置和方向的3D感知探测数据和用于复杂空间关系的3D感知对话数据。</li>
<li>首次在真实图像上创建包含3D方向关系的视觉问答（VQA）数据集。</li>
<li>系统地将这两种训练数据与LMMs的架构和训练设计相结合，为达到优越的3D推理能力提供了设计路线图。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00788">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-900fafc431de8b555266d7b66fb71fd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b160252dd44be35b39b957cc4cbff7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-478275e05a8fce27b86465d7b041d8e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7da4622be092dd3dc959d3085293d6cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42d939f48bc501c776b3fdde34866e55.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-06/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-06/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a2373489183353348b449a9c9c8ec76.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-06  FlowDubber Movie Dubbing with LLM-based Semantic-aware Learning and   Flow Matching based Voice Enhancing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-464bff018899505520fbbb81f441a0f1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-05  3D-LLaVA Towards Generalist 3D LMMs with Omni Superpoint Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
