<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-07-13  PWD Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7c76dcae2c4db2c5d7a6501e80646170.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-13-更新"><a href="#2025-07-13-更新" class="headerlink" title="2025-07-13 更新"></a>2025-07-13 更新</h1><h2 id="PWD-Prior-Guided-and-Wavelet-Enhanced-Diffusion-Model-for-Limited-Angle-CT"><a href="#PWD-Prior-Guided-and-Wavelet-Enhanced-Diffusion-Model-for-Limited-Angle-CT" class="headerlink" title="PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT"></a>PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT</h2><p><strong>Authors:Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu</strong></p>
<p>Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM. </p>
<blockquote>
<p>在医学成像领域，尤其是有限角度计算机断层扫描（LACT）中，生成式扩散模型已引起越来越多的关注。标准扩散模型虽然能够实现高质量图像重建，但在推理过程中需要大量采样步骤，导致计算开销很大。虽然有人提出了跳过采样策略来提高效率，但它们往往会导致精细结构细节的丢失。针对这一问题，我们提出了一种基于先验信息嵌入和小波特征融合的快速采样扩散模型，用于LACT重建。PWD（Prior Wavelet Diffusion）能够在LACT中实现高效采样，同时保持重建保真度，并有效地减轻了跳过采样通常引起的降解。具体而言，在训练阶段，PWD将LACT图像的分布映射到完全采样目标图像的分布，使模型能够学习两者之间的结构对应关系。在推理阶段，LACT图像作为明确的先验来引导采样轨迹，允许以更少的步骤实现高质量重建。此外，PWD在小波域执行多尺度特征融合，通过利用低频和高频信息有效地提高了精细细节的重建效果。在临床牙弓CBCT和根尖周数据集上的定量和定性评估表明，在相同的采样条件下，PWD的性能优于现有方法。仅使用50个采样步骤，PWD的PSNR提高了至少1.7dB，SSIM提高了10%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05317v2">PDF</a> </p>
<p><strong>Summary</strong><br>    本文提出了一种基于先验信息嵌入和小波特征融合的快速采样扩散模型，用于有限角度计算机断层扫描（LACT）重建。该模型在训练阶段将LACT图像分布映射到完全采样的目标图像分布，学习两者之间的结构对应关系。在推理阶段，利用LACT图像作为显式先验来指导采样轨迹，实现高质量重建并大大减少采样步骤。同时，该模型在小波域进行多尺度特征融合，利用高低频信息有效增强细节重建。实验结果表明，PWD在相同采样条件下优于现有方法，使用仅50个采样步骤时，PWD的峰值信噪比（PSNR）提高了至少1.7 dB，结构相似性（SSIM）提高了10%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成性扩散模型在医学成像中受到关注，特别是在有限角度计算机断层扫描（LACT）领域。</li>
<li>标准扩散模型虽然能实现高质量图像重建，但在推理过程中需要大量采样步骤，导致计算开销大。</li>
<li>提出的PWD模型通过先验信息嵌入和小波特征融合实现快速采样扩散。</li>
<li>PWD在训练阶段学习LACT图像与完全采样目标图像之间的结构对应关系。</li>
<li>推理阶段利用LACT图像作为显式先验，实现高质量重建并减少采样步骤。</li>
<li>PWD在小波域进行多尺度特征融合，利用高低频信息增强细节重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05317">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab3e69ab774fe780bcd12a12de07891e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c77d2f231bcd03bfb13565de40981da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab472ee626d855e79e9d3b582d27684b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ffe0bf2d6eeeb6d75f26e36f95c53a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f7169be35bc1757d66024e170114c28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e3343762ed90af0c65f348b8b99008.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Lightweight-Medical-Image-Restoration-via-Integrating-Reliable-Lesion-Semantic-Driven-Prior"><a href="#Lightweight-Medical-Image-Restoration-via-Integrating-Reliable-Lesion-Semantic-Driven-Prior" class="headerlink" title="Lightweight Medical Image Restoration via Integrating Reliable   Lesion-Semantic Driven Prior"></a>Lightweight Medical Image Restoration via Integrating Reliable   Lesion-Semantic Driven Prior</h2><p><strong>Authors:Pengcheng Zheng, Kecheng Chen, Jiaxin Huang, Bohao Chen, Ju Liu, Yazhou Ren, Xiaorong Pu</strong></p>
<p>Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, which is much more urgent in medical systems. To alleviate these issues, we present LRformer, a Lightweight Transformer-based method via Reliability-guided learning in the frequency domain. Specifically, inspired by the uncertainty quantification in Bayesian neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer (RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling operations to generate sufficiently-reliable priors by performing multiple inferences on the foundational medical image segmentation model, MedSAM. Additionally, instead of directly incorporating the priors in the spatial domain, we decompose the cross-attention (CA) mechanism into real symmetric and imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging the conjugated symmetric property of FFT, GFCA reduces the computational complexity of naive CA by nearly half. Extensive experimental results in various tasks demonstrate the superiority of the proposed LRformer in both effectiveness and efficiency. </p>
<blockquote>
<p>医学图像恢复任务旨在从退化的观察中恢复高质量图像，这在许多临床场景中展现出了迫切的需求，例如低剂量CT图像去噪、MRI超分辨率和MRI伪影去除。尽管现有的基于深度学习的恢复方法使用复杂的模块取得了成功，但它们在呈现计算高效的重建结果方面遇到了困难。此外，它们通常忽略了医疗系统中恢复结果的可靠性，这一点更为紧迫。为了缓解这些问题，我们提出了LRformer，这是一种基于轻量级Transformer的可靠性引导频率域学习方法。具体而言，我们受到贝叶斯神经网络（BNNs）中的不确定性量化的启发，开发了一种可靠的病灶语义先验生产者（RLPP）。RLPP利用蒙特卡洛（MC）估计器和随机采样操作，通过对基础医学图像分割模型MedSAM进行多次推理，生成足够可靠的先验。此外，我们没有直接在空间域中融入这些先验知识，而是利用快速傅里叶变换（FFT）将交叉注意（CA）机制分解为实对称和虚反对称部分，从而设计出引导频率交叉注意（GFCA）求解器。通过利用FFT的共轭对称属性，GFCA将朴素CA的计算复杂度降低了近一半。在各项任务中的广泛实验结果证明了所提出的LRformer在有效性和效率方面的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11286v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对医学图像恢复任务的一种新型方法——LRformer。该方法结合了轻量级Transformer和可靠性引导学习，旨在解决现有深度学习恢复方法在效率和可靠性方面的问题。通过引入可靠的病灶语义先验生成器（RLPP）和导向频率交叉注意（GFCA）求解器，LRformer在各种医学图像恢复任务中展现出优越的效果和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRformer是一种基于轻量级Transformer的医学图像恢复方法，旨在解决现有方法的计算效率与结果可靠性问题。</li>
<li>RLPP利用贝叶斯神经网络（BNNs）的不确定性量化，通过Monte Carlo（MC）估计器和随机采样操作生成可靠的先验信息。</li>
<li>GFCA求解器通过快速傅里叶变换（FFT）将交叉注意（CA）机制分解为实对称和虚反对称两部分，降低了计算复杂度。</li>
<li>LRformer在多种医学图像恢复任务中进行了广泛实验，证明了其优越性和有效性。</li>
<li>LRformer对于低剂量CT图像去噪、MRI超分辨率和MRI伪影去除等临床场景具有潜在应用价值。</li>
<li>RLPP和GFCA的结合使得LRformer能够在保证恢复质量的同时，提高计算效率和结果可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11286">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b523e3e3ad54b8d134e4464ee46f7c95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65aa0106c3d206c1e0f5e5198d4b6fff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d074d62a71206e54bf9bd99f4410f4c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8b3821043a9b3008cc23663ae46a97b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200a8dd959eb4874575df86abae030fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8cd6bc3a87ed1d51ce4ac5f9645a46.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RL4Med-DDPO-Reinforcement-Learning-for-Controlled-Guidance-Towards-Diverse-Medical-Image-Generation-using-Vision-Language-Foundation-Models"><a href="#RL4Med-DDPO-Reinforcement-Learning-for-Controlled-Guidance-Towards-Diverse-Medical-Image-Generation-using-Vision-Language-Foundation-Models" class="headerlink" title="RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards   Diverse Medical Image Generation using Vision-Language Foundation Models"></a>RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards   Diverse Medical Image Generation using Vision-Language Foundation Models</h2><p><strong>Authors:Parham Saremi, Amar Kumar, Mohamed Mohamed, Zahra TehraniNasab, Tal Arbel</strong></p>
<p>Vision-Language Foundation Models (VLFM) have shown a tremendous increase in performance in terms of generating high-resolution, photorealistic natural images. While VLFMs show a rich understanding of semantic content across modalities, they often struggle with fine-grained alignment tasks that require precise correspondence between image regions and textual descriptions, a limitation in medical imaging, where accurate localization and detection of clinical features are essential for diagnosis and analysis. To address this issue, we propose a multi-stage architecture where a pre-trained VLFM (e.g. Stable Diffusion) provides a cursory semantic understanding, while a reinforcement learning (RL) algorithm refines the alignment through an iterative process that optimizes for understanding semantic context. The reward signal is designed to align the semantic information of the text with synthesized images. Experiments on the public ISIC2019 skin lesion dataset demonstrate that the proposed method improves (a) the quality of the generated images, and (b) the alignment with the text prompt over the original fine-tuned Stable Diffusion baseline. We also show that the synthesized samples could be used to improve disease classifier performance for underrepresented subgroups through augmentation. Our code is accessible through the project website: <a target="_blank" rel="noopener" href="https://parhamsaremi.github.io/rl4med-ddpo">https://parhamsaremi.github.io/rl4med-ddpo</a> </p>
<blockquote>
<p>视觉语言基础模型（VLFM）在生成高分辨率、逼真自然图像方面的性能表现显著提高。虽然VLFM在跨模态的语义内容理解方面表现出丰富的能力，但在需要图像区域和文本描述之间精确对应的精细对齐任务上常常遇到困难。在医学成像领域，这是诊断和分析中准确定位和检测临床特征的关键限制。为了解决这个问题，我们提出了一种多阶段架构，其中预训练的VLFM（如Stable Diffusion）提供粗略的语义理解，而强化学习（RL）算法通过优化理解语义上下文来完善对齐。奖励信号被设计为将文本中的语义信息与合成图像对齐。在公开的ISIC2019皮肤病变数据集上的实验表明，所提出的方法改进了（a）生成的图像质量，（b）与文本提示的对齐程度，优于原始微调后的Stable Diffusion基线。我们还证明了合成的样本可以通过数据增强来提高代表性不足的疾病分类器的性能。我们的代码可通过项目网站访问：<a target="_blank" rel="noopener" href="https://parhamsaremi.github.io/rl4med-ddpo">https://parhamsaremi.github.io/rl4med-ddpo</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15784v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像领域，预训练视觉语言融合模型（VLFM）虽能生成高质量的自然图像，但对精细对齐任务理解不足，尤其在医学成像中需准确定位和分析临床特征时受限。提出一种多阶段架构，采用预训练VLFM提供语义理解，并用强化学习算法迭代优化对齐精度。奖励信号用于使文本与合成图像语义信息对齐。在公开皮肤病变数据集上的实验表明，该方法提高了图像质量和文本对齐度，合成的样本可用于增强对代表性不足的疾病分类器的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练视觉语言融合模型（VLFM）在医学图像生成中具有巨大的潜力。</li>
<li>VLFM面临精细对齐任务挑战，即在医学成像中需要准确对应图像区域和文本描述。</li>
<li>提出一种多阶段架构，结合预训练VLFM和强化学习算法解决上述问题。</li>
<li>强化学习通过迭代优化过程提高语义上下文的理解和对齐精度。</li>
<li>奖励信号设计旨在使文本与合成图像的语义信息对齐。</li>
<li>在公开皮肤病变数据集上的实验验证了该方法的有效性，提高了图像质量和文本对齐度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-30b7f3f323e52035e50679e7b08c6af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0d139c364a58464815fbea7ae390756.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e63e87d5f0e3d0ca57e31dad1d36a04d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aeae1981ea809340dd8b82fedcbdde41.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Autoregressive-State-Space-Models-for-Medical-Image-Reconstruction"><a href="#Physics-Driven-Autoregressive-State-Space-Models-for-Medical-Image-Reconstruction" class="headerlink" title="Physics-Driven Autoregressive State Space Models for Medical Image   Reconstruction"></a>Physics-Driven Autoregressive State Space Models for Medical Image   Reconstruction</h2><p><strong>Authors:Bilal Kabas, Fuat Arslan, Valiyeh A. Nezhad, Saban Ozturk, Emine U. Saritas, Tolga Çukur</strong></p>
<p>Medical image reconstruction from undersampled acquisitions is an ill-posed problem involving inversion of the imaging operator linking measurement and image domains. Physics-driven (PD) models have gained prominence in reconstruction tasks due to their desirable performance and generalization. These models jointly promote data fidelity and artifact suppression, typically by combining data-consistency mechanisms with learned network modules. Artifact suppression depends on the network’s ability to disentangle artifacts from true tissue signals, both of which can exhibit contextual structure across diverse spatial scales. Convolutional neural networks (CNNs) are strong in capturing local correlations, albeit relatively insensitive to non-local context. While transformers promise to alleviate this limitation, practical implementations frequently involve design compromises to reduce computational cost by balancing local and non-local sensitivity, occasionally resulting in performance comparable to or trailing that of CNNs. To enhance contextual sensitivity without incurring high complexity, we introduce a novel physics-driven autoregressive state-space model (MambaRoll) for medical image reconstruction. In each cascade of its unrolled architecture, MambaRoll employs a physics-driven state-space module (PD-SSM) to aggregate contextual features efficiently at a given spatial scale, and autoregressively predicts finer-scale feature maps conditioned on coarser-scale features to capture multi-scale context. Learning across scales is further enhanced via a deep multi-scale decoding (DMSD) loss tailored to the autoregressive prediction task. Demonstrations on accelerated MRI and sparse-view CT reconstructions show that MambaRoll consistently outperforms state-of-the-art data-driven and physics-driven methods based on CNN, transformer, and SSM backbones. </p>
<blockquote>
<p>从欠采样采集中进行医学图像重建是一个不适定问题，涉及到连接测量和图像域之间的成像算子的反转。由于其在重建任务中的出色性能和泛化能力，物理驱动（PD）模型已经变得突出。这些模型通过结合数据一致性机制和学习的网络模块，同时促进数据保真和伪影抑制。伪影抑制依赖于网络从真实组织信号中分离伪影的能力，这两者在不同的空间尺度上都可以表现出上下文结构。卷积神经网络（CNN）擅长捕捉局部相关性，尽管对非局部上下文的敏感度相对较低。虽然变压器有望缓解这一局限性，但实际应用中经常需要在平衡局部和非局部敏感度的情况下做出设计上的妥协，以降低计算成本，这有时会使得性能与CNN相当或落后。为了在不增加高复杂度的情况下提高上下文敏感度，我们引入了一种新型物理驱动自回归状态空间模型（MambaRoll）用于医学图像重建。MambaRoll的展开架构中的每个级联都使用一个物理驱动状态空间模块（PD-SSM）来有效地聚合给定空间尺度的上下文特征，并自回归地预测更精细尺度的特征图，这些预测基于较粗糙尺度的特征，以捕获多尺度上下文。通过针对自回归预测任务设计的深度多尺度解码（DMSD）损失，进一步增强了跨尺度的学习。在加速MRI和稀疏视图CT重建上的演示表明，MambaRoll始终优于基于CNN、变压器和SSM的最新数据驱动和物理驱动方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09331v2">PDF</a> 14 pages, 11 figures</p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种新型的医学图像重建方法——基于物理驱动的自回归状态空间模型（MambaRoll）。该方法旨在解决从欠采样采集中进行医学图像重建的逆问题。MambaRoll模型通过引入物理驱动的状态空间模块（PD-SSM），有效聚合了特定空间尺度上的上下文特征，并通过自回归方式预测更精细尺度的特征图，从而捕获多尺度上下文信息。此外，该模型还采用了一种针对自回归预测任务的深度多尺度解码（DMSD）损失，以进一步提高跨尺度学习能力。在加速MRI和稀疏视图CT重建的演示中，MambaRoll表现优异，超越了基于CNN、Transformer和SSM等先进数据驱动和物理驱动的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医学图像重建是从欠采样采集数据中恢复完整图像的过程，是一个不适定问题。</li>
<li>物理驱动（PD）模型在医学图像重建任务中因其性能优异而备受关注，它们通过结合数据一致性和学习网络模块来促进数据保真和伪影抑制。</li>
<li>卷积神经网络（CNNs）擅长捕捉局部相关性，但对非局部上下文相对不敏感。</li>
<li>虽然Transformer有潜力缓解这一局限性，但在实践中，为了降低计算成本而进行的平衡设计往往导致性能与CNN相当或落后。</li>
<li>提出了新型的基于物理驱动的自回归状态空间模型（MambaRoll）用于医学图像重建。</li>
<li>MambaRoll通过物理驱动的状态空间模块（PD-SSM）在给定空间尺度上有效地聚合上下文特征。</li>
<li>MambaRoll在加速MRI和稀疏视图CT重建任务中表现出卓越性能，超过了现有先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2682c5d41c584905639d026126c052ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3d9b350d37c825df02a28902b2c6c76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a509de0d0e2cb2176e4918b2ba73da7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0880f39c97a65e513c45b85088a009fe.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-dynamic-deep-image-prior-for-cardiac-MRI"><a href="#Multi-dynamic-deep-image-prior-for-cardiac-MRI" class="headerlink" title="Multi-dynamic deep image prior for cardiac MRI"></a>Multi-dynamic deep image prior for cardiac MRI</h2><p><strong>Authors:Marc Vornehm, Chong Chen, Muhammad Ahmad Sultan, Syed Murtaza Arshad, Yuchi Han, Florian Knoll, Rizwan Ahmad</strong></p>
<p>Cardiovascular magnetic resonance imaging is a powerful diagnostic tool for assessing cardiac structure and function. However, traditional breath-held imaging protocols pose challenges for patients with arrhythmias or limited breath-holding capacity. This work aims to overcome these limitations by developing a reconstruction framework that enables high-quality imaging in free-breathing conditions for various dynamic cardiac MRI protocols. Multi-Dynamic Deep Image Prior (M-DIP), a novel unsupervised reconstruction framework for accelerated real-time cardiac MRI, is introduced. To capture contrast or content variation, M-DIP first employs a spatial dictionary to synthesize a time-dependent intermediate image. Then, this intermediate image is further refined using time-dependent deformation fields that model cardiac and respiratory motion. Unlike prior DIP-based methods, M-DIP simultaneously captures physiological motion and frame-to-frame content variations, making it applicable to a wide range of dynamic applications. We validate M-DIP using simulated MRXCAT cine phantom data as well as free-breathing real-time cine, single-shot late gadolinium enhancement (LGE), and first-pass perfusion data from clinical patients. Comparative analyses against state-of-the-art supervised and unsupervised approaches demonstrate M-DIP’s performance and versatility. M-DIP achieved better image quality metrics on phantom data, higher reader scores on in-vivo cine and LGE data, and comparable scores on in-vivo perfusion data relative to another DIP-based approach. M-DIP enables high-quality reconstructions of real-time free-breathing cardiac MRI without requiring external training data. Its ability to model physiological motion and content variations makes it a promising approach for various dynamic imaging applications. </p>
<blockquote>
<p>心血管磁共振成像是一种评估心脏结构和功能的强大诊断工具。然而，传统的屏气成像协议对于心律失常或屏气能力有限的患者来说提出了挑战。这项工作旨在通过开发一个重建框架来克服这些限制，该框架能够在各种动态心脏MRI协议中实现自由呼吸条件下的高质量成像。介绍了一种用于加速实时心脏MRI的新型无监督重建框架——多动态深度图像先验（M-DIP）。为了捕捉对比度或内容的变化，M-DIP首先使用空间字典合成时间相关的中间图像。然后，这个中间图像进一步使用时间相关的变形场进行精细化处理，以模拟心脏和呼吸运动。与之前的DIP方法不同，M-DIP可以同时捕捉生理运动和帧间内容变化，使其适用于广泛的动态应用。我们使用模拟的MRXCAT电影幻影数据以及来自临床患者的自由呼吸实时电影、单次拍摄晚期钆增强（LGE）和首过灌注数据验证了M-DIP。与最新先进的有监督和无监督方法的对比分析证明了M-DIP的性能和通用性。M-DIP在幻影数据上实现了更好的图像质量指标，在体内电影和LGE数据上获得了更高的读者评分，与另一种基于DIP的方法相比，在体内灌注数据上的得分相当。M-DIP实现了自由呼吸状态下实时心脏MRI的高质量重建，而无需外部训练数据。其模拟生理运动和内容变化的能力使其成为各种动态成像应用的有前途的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04639v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于这篇文章的摘要指出，传统的心血管磁共振成像面临呼吸限制等问题，不利于心脏疾病的诊断。为应对这一挑战，研究人员提出一种名为M-DIP的新颖无监督重建框架，实现了高质量、实时的无呼吸限制心血管磁共振成像。该方法使用空间词典和变形场模型心脏运动和呼吸运动的时间依赖性，使各种动态心脏MRI成为可能。对比现有的监督和非监督方法显示，M-DIP表现优异，能够重建高质量的无呼吸限制实时心血管MRI图像。它有望在多种动态成像应用中展现出巨大的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>M-DIP是一种新颖的重建框架，用于解决传统心血管磁共振成像在呼吸受限情况下的挑战。</li>
<li>M-DIP结合了空间词典和变形场模型心脏和呼吸运动的时间依赖性。</li>
<li>M-DIP在各种动态心脏MRI协议中实现了高质量成像，包括实时、单镜头延迟增强和首过灌注数据。</li>
<li>对比现有方法，M-DIP在图像质量指标、读者评分方面表现优越。</li>
<li>M-DIP无需外部训练数据即可实现高质量重建。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04639">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d495c4805e2a2ae3abe092b1be92342.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12b307411cfddc772738bc7fe8df062f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind-the-Context-Attention-Guided-Weak-to-Strong-Consistency-for-Enhanced-Semi-Supervised-Medical-Image-Segmentation"><a href="#Mind-the-Context-Attention-Guided-Weak-to-Strong-Consistency-for-Enhanced-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Mind the Context: Attention-Guided Weak-to-Strong Consistency for   Enhanced Semi-Supervised Medical Image Segmentation"></a>Mind the Context: Attention-Guided Weak-to-Strong Consistency for   Enhanced Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Yuxuan Cheng, Chenxi Shao, Jie Ma, Yunfei Xie, Guoliang Li</strong></p>
<p>Medical image segmentation is a pivotal step in diagnostic and therapeutic processes, relying on high-quality annotated data that is often challenging and costly to obtain. Semi-supervised learning offers a promising approach to enhance model performance by leveraging unlabeled data. Although weak-to-strong consistency is a prevalent method in semi-supervised image segmentation, there is a scarcity of research on perturbation strategies specifically tailored for semi-supervised medical image segmentation tasks. To address this challenge, this paper introduces a simple yet efficient semi-supervised learning framework named Attention-Guided weak-to-strong Consistency Match (AIGCMatch). The AIGCMatch framework incorporates attention-guided perturbation strategies at both the image and feature levels to achieve weak-to-strong consistency regularization. This method not only preserves the structural information of medical images but also enhances the model’s ability to process complex semantic information. Extensive experiments conducted on the ACDC and ISIC-2017 datasets have validated the effectiveness of AIGCMatch. Our method achieved a 90.4% Dice score in the 7-case scenario on the ACDC dataset, surpassing the state-of-the-art methods and demonstrating its potential and efficacy in clinical settings. </p>
<blockquote>
<p>医学图像分割是诊断和治疗过程中的关键步骤，依赖于高质量标注数据，而这些数据的获取往往具有挑战性和成本高昂。半监督学习通过利用未标注数据，为提高模型性能提供了一种有前途的方法。尽管弱到强的一致性是半监督图像分割中的流行方法，但针对半监督医学图像分割任务的扰动策略的研究却很匮乏。针对这一挑战，本文提出了一种简单而高效的半监督学习框架，名为注意力引导弱到强一致性匹配（AIGCMatch）。AIGCMatch框架在图像和特征层面结合了注意力引导扰动策略，实现了弱到强的一致性正则化。这种方法不仅保留了医学图像的结构信息，还提高了模型处理复杂语义信息的能力。在ACDC和ISIC-2017数据集上进行的广泛实验验证了AIGCMatch的有效性。我们的方法在ACDC数据集上的7个案例场景中实现了90.4%的Dice得分，超越了最先进的方法，证明了其在临床环境中的潜力和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12419v3">PDF</a> </p>
<p><strong>Summary</strong><br>     医学图像分割在诊断和治疗过程中至关重要，需要大量高质量标注数据，但获取这些数据往往具有挑战性和成本高昂。半监督学习通过利用未标注数据来提高模型性能。尽管弱到强一致性是半监督图像分割中的常见方法，但针对半监督医学图像分割任务的扰动策略研究较少。针对这一挑战，本文提出了一种简单高效的半监督学习框架，名为注意力引导弱到强一致性匹配（AIGCMatch）。AIGCMatch框架在图像和特征层面采用注意力引导扰动策略，实现弱到强一致性正则化。此方法不仅保留了医学图像的结构信息，还提高了模型处理复杂语义信息的能力。在ACDC和ISIC-2017数据集上进行的广泛实验验证了AIGCMatch的有效性。在ACDC数据集上的7例场景中，我们的方法实现了90.4%的Dice得分，超越了最先进的方法，证明了其在临床环境中的潜力和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割是医疗诊断与治疗的关键环节，对数据的标注质量和数量有较高要求。</li>
<li>获取高质量标注数据具有挑战性和高成本。</li>
<li>半监督学习能提高模型性能，利用未标注数据。</li>
<li>现有的半监督医学图像分割中的弱到强一致性方法需要结合更有效的扰动策略。</li>
<li>本文提出一种名为AIGCMatch的半监督学习框架，结合注意力引导的扰动策略实现弱到强一致性正则化。</li>
<li>AIGCMatch能保留医学图像的结构信息，并提高模型处理复杂语义信息的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e76eedbe917ea35e89baf5bf11371ef1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b78e7fef7c191d4d1e61e5911e85f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b788c4644eb3e34c50622287bc37c139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d123668fbfe2d9249c25539f6c5d243.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70f56e2fab0da8d82add0532b18e42ff.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Judging-from-Support-set-A-New-Way-to-Utilize-Few-Shot-Segmentation-for-Segmentation-Refinement-Process"><a href="#Judging-from-Support-set-A-New-Way-to-Utilize-Few-Shot-Segmentation-for-Segmentation-Refinement-Process" class="headerlink" title="Judging from Support-set: A New Way to Utilize Few-Shot Segmentation for   Segmentation Refinement Process"></a>Judging from Support-set: A New Way to Utilize Few-Shot Segmentation for   Segmentation Refinement Process</h2><p><strong>Authors:Seonghyeon Moon,  Qingze,  Liu, Haein Kong, Muhammad Haris Khan</strong></p>
<p>Segmentation refinement aims to enhance the initial coarse masks generated by segmentation algorithms. The refined masks are expected to capture more details and better contours of the target objects. Research on segmentation refinement has developed as a response to the need for high-quality image segmentations. However, to our knowledge, no method has been developed that can determine the success of segmentation refinement. Such a method could ensure the reliability of segmentation in applications where the outcome of the segmentation is important and fosters innovation in image processing technologies. To address this research gap, we propose Judging From Support-set (JFS), a method to judge the success of segmentation refinement leveraging an off-the-shelf few-shot segmentation (FSS) model. The traditional goal of the problem in FSS is to find a target object in a query image utilizing target information given by a support set. However, we propose a novel application of the FSS model in our evaluation pipeline for segmentation refinement methods. Given a coarse mask as input, segmentation refinement methods produce a refined mask; these two masks become new support masks for the FSS model. The existing support mask then serves as the test set for the FSS model to evaluate the quality of the refined segmentation by the segmentation refinement methods. We demonstrate the effectiveness of our proposed JFS framework by evaluating the SAM Enhanced Pseudo-Labels (SEPL) using SegGPT as the choice of FSS model on the PASCAL dataset. The results showed that JFS has the potential to determine whether the segmentation refinement process is successful. </p>
<blockquote>
<p>分割细化旨在提高分割算法生成的初始粗糙掩模的质量。细化后的掩模能够捕捉更多细节和目标对象更好的轮廓。分割细化的研究是为了满足对高质量图像分割的需求而发展起来的。然而，据我们所知，还没有方法可以判断分割细化的成功与否。这样的方法可以保证分割在结果重要的应用中的可靠性，并促进图像处理技术的创新。为了弥补这一研究空白，我们提出了“从支持集判断”（JFS）的方法，这是一种利用现成的少样本分割（FSS）模型来判断分割细化成功与否的方法。FSS问题的传统目标是利用支持集给出的目标信息在查询图像中找到目标对象。然而，我们提出了FSS模型在分割细化方法评估管道中的新颖应用。给定一个粗略的掩模作为输入，分割细化方法会产生一个精细的掩模；这两个掩模成为FSS模型的新支持掩模。现有的支持掩模然后作为FSS模型的测试集，以评估分割细化方法所得到的精细分割的质量。我们通过使用SegGPT作为FSS模型在PASCAL数据集上评估SAM增强伪标签（SEPL）来展示我们所提出的JFS框架的有效性。结果表明，JFS具有判断分割细化过程是否成功的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04519v3">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于支持集（Support-set）的评判方法（JFS），用于评估图像分割精细化的效果。此方法借助现成的少量样本分割（FSS）模型，利用粗分割掩膜和精细化后的掩膜作为新的支持集，通过支持集对精细化分割方法进行测试集评估，从而判断分割精细化的成功与否。实验结果表明，JFS框架具有评估分割精细化过程是否成功的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>分割细化旨在改进由分割算法生成的初始粗略掩膜，期望获得更多细节和更精确的目标物体轮廓。</li>
<li>目前尚未有方法能够确定分割细化的成功与否，这影响了分割结果重要的应用并限制了图像处理技术的创新。</li>
<li>提出了一种基于支持集（Support-set）的评判方法（JFS），用于评估分割细化的效果。</li>
<li>JFS利用现成的少量样本分割（FSS）模型，将粗分割掩膜和精细化后的掩膜作为新的支持集。</li>
<li>通过支持集对精细化分割方法进行测试集评估，以判断分割精细化的成功与否。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-790baed0e4d401c2fa5f052c1eb10756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bc610190d31ef53f904fcd57e04ca71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a6e7389c5f5f29cdc27cfdd982b84b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c76dcae2c4db2c5d7a6501e80646170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475c33dc5db066f197ba342644fbf134.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e1a0ab595a25e11703980e518bc7b74c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-07-14  UWarp A Whole Slide Image Registration Pipeline to Characterize   Scanner-Induced Local Domain Shift
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-13/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a565b3f90516e90a97a2b97a3befb5c7.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-07-13  None of the Others a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
