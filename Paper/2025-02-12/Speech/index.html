<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Speaker Embedding Informed Audiovisual Active Speaker Detection for   Egocentric Recordings">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05236v1/page_4_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="Speaker-Embedding-Informed-Audiovisual-Active-Speaker-Detection-for-Egocentric-Recordings"><a href="#Speaker-Embedding-Informed-Audiovisual-Active-Speaker-Detection-for-Egocentric-Recordings" class="headerlink" title="Speaker Embedding Informed Audiovisual Active Speaker Detection for   Egocentric Recordings"></a>Speaker Embedding Informed Audiovisual Active Speaker Detection for   Egocentric Recordings</h2><p><strong>Authors:Jason Clarke, Yoshihiko Gotoh, Stefan Goetze</strong></p>
<p>Audiovisual active speaker detection (ASD) addresses the task of determining the speech activity of a candidate speaker given acoustic and visual data. Typically, systems model the temporal correspondence of audiovisual cues, such as the synchronisation between speech and lip movement. Recent work has explored extending this paradigm by additionally leveraging speaker embeddings extracted from candidate speaker reference speech. This paper proposes the speaker comparison auxiliary network (SCAN) which uses speaker-specific information from both reference speech and the candidate audio signal to disambiguate challenging scenes when the visual signal is unresolvable. Furthermore, an improved method for enrolling face-speaker libraries is developed, which implements a self-supervised approach to video-based face recognition. Fitting with the recent proliferation of wearable devices, this work focuses on improving speaker-embedding-informed ASD in the context of egocentric recordings, which can be characterised by acoustic noise and highly dynamic scenes. SCAN is implemented with two well-established baselines, namely TalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and 10.3% on the Ego4D benchmark, respectively. </p>
<blockquote>
<p>è§†å¬ä¸»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰çš„ä»»åŠ¡æ˜¯ç»™å®šå£°éŸ³å’Œè§†è§‰æ•°æ®æ¥ç¡®å®šå€™é€‰è¯´è¯äººçš„è¯­éŸ³æ´»åŠ¨ã€‚é€šå¸¸ï¼Œç³»ç»Ÿä¼šå¯¹è§†å¬çº¿ç´¢çš„æ—¶é—´å¯¹åº”è¿›è¡Œå»ºæ¨¡ï¼Œä¾‹å¦‚è¯­éŸ³å’Œå”‡éƒ¨è¿åŠ¨ä¹‹é—´çš„åŒæ­¥ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†é€šè¿‡é¢å¤–åˆ©ç”¨ä»å€™é€‰è¯´è¯äººå‚è€ƒè¯­éŸ³ä¸­æå–çš„è¯´è¯äººåµŒå…¥æ¥æ‰©å±•è¿™ç§èŒƒå¼ã€‚æœ¬æ–‡æå‡ºäº†è¯´è¯äººæ¯”è¾ƒè¾…åŠ©ç½‘ç»œï¼ˆSCANï¼‰ï¼Œè¯¥ç½‘ç»œä½¿ç”¨æ¥è‡ªå‚è€ƒè¯­éŸ³å’Œå€™é€‰éŸ³é¢‘ä¿¡å·çš„è¯´è¯äººç‰¹å®šä¿¡æ¯ï¼Œä»¥è§£å†³è§†è§‰ä¿¡å·æ— æ³•è§£ææ—¶å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§æ”¹è¿›çš„é¢éƒ¨è¯´è¯äººåº“æ³¨å†Œæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®ç°äº†åŸºäºè§†é¢‘çš„é¢éƒ¨è¯†åˆ«çš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•ã€‚éšç€å¯ç©¿æˆ´è®¾å¤‡çš„æ—¥ç›Šæ™®åŠï¼Œè¿™é¡¹å·¥ä½œé›†ä¸­åœ¨æ”¹è¿›ä»¥è¯´è¯äººåµŒå…¥ä¿¡æ¯ä¸ºä¸»çš„ASDåœ¨è‡ªæˆ‘ä¸­å¿ƒè®°å½•èƒŒæ™¯ä¸‹çš„åº”ç”¨ï¼Œè¿™å¯ä»¥ä»¥å£°éŸ³å™ªå£°å’Œé«˜åº¦åŠ¨æ€åœºæ™¯ä¸ºç‰¹å¾ã€‚SCANé€šè¿‡ä½¿ç”¨ä¸¤ä¸ªç»è¿‡è‰¯å¥½éªŒè¯çš„åŸºçº¿ï¼ˆå³TalkNetå’ŒLight-ASDï¼‰å®ç°ï¼Œåœ¨Ego4DåŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«æé«˜äº†14.5%å’Œ10.3%çš„mAPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06012v1">PDF</a> Accepted to ICASSP 2025. 5 pages, 4 figures. To appear in Proceedings   of IEEE International Conference on Acoustics, Speech and Signal Processing   (ICASSP), April 6-11, 2025, Hyderabad, India</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†å¬ä¸»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡å£°éŸ³å’Œè§†è§‰æ•°æ®æ¥ç¡®å®šå€™é€‰äººçš„è¯´è¯æ´»åŠ¨ã€‚æ–‡ç« æå‡ºä½¿ç”¨å‘è¨€äººå¯¹æ¯”è¾…åŠ©ç½‘ç»œï¼ˆSCANï¼‰ï¼Œè¯¥ç½‘ç»œä»å‚è€ƒè¯­éŸ³å’Œå€™é€‰éŸ³é¢‘ä¿¡å·ä¸­æå–å‘è¨€äººç‰¹å®šä¿¡æ¯ï¼Œä»¥è§£å†³è§†è§‰ä¿¡å·ä¸å¯åˆ†è¾¨çš„æŒ‘æˆ˜åœºæ™¯ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§æ”¹è¿›çš„é¢éƒ¨å‘è¨€äººåº“æ³¨å†Œæ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºè§†é¢‘çš„é¢éƒ¨è¯†åˆ«çš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•ã€‚è¯¥ç ”ç©¶å…³æ³¨äºç©¿æˆ´å¼è®¾å¤‡é¢†åŸŸï¼Œè‡´åŠ›äºåœ¨è‡ªæˆ‘ä¸­å¿ƒå½•éŸ³èƒŒæ™¯ä¸‹æ”¹è¿›åŸºäºå‘è¨€äººåµŒå…¥çš„ASDæŠ€æœ¯ï¼Œè¯¥ç¯å¢ƒå…·æœ‰å£°å­¦å™ªå£°å’Œé«˜åº¦åŠ¨æ€åœºæ™¯çš„ç‰¹ç‚¹ã€‚é€šè¿‡TalkNetå’ŒLight-ASDä¸¤ä¸ªåŸºå‡†æµ‹è¯•å®ç°SCANï¼Œåœ¨Ego4DåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æé«˜äº†14.5%å’Œ10.3%çš„mAPã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†å¬ä¸»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰æ˜¯é€šè¿‡å£°éŸ³å’Œè§†è§‰æ•°æ®ç¡®å®šè¯´è¯äººæ´»åŠ¨çš„ä»»åŠ¡ã€‚</li>
<li>SCANç½‘ç»œç»“åˆå‚è€ƒè¯­éŸ³å’Œå€™é€‰éŸ³é¢‘ä¿¡å·ä¸­çš„è¯´è¯äººç‰¹å®šä¿¡æ¯ï¼Œè§£å†³è§†è§‰ä¿¡å·ä¸å¯åˆ†è¾¨æ—¶çš„è¯†åˆ«éš¾é¢˜ã€‚</li>
<li>æ”¹è¿›äº†é¢éƒ¨å‘è¨€äººåº“æ³¨å†Œæ–¹æ³•ï¼Œé‡‡ç”¨è‡ªæˆ‘ç›‘ç£çš„è§†é¢‘é¢éƒ¨è¯†åˆ«æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶å…³æ³¨ç©¿æˆ´å¼è®¾å¤‡é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªæˆ‘ä¸­å¿ƒå½•éŸ³ç¯å¢ƒä¸‹çš„ASDæŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯é¢ä¸´å£°å­¦å™ªå£°å’Œé«˜åº¦åŠ¨æ€åœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>SCANç½‘ç»œé€šè¿‡TalkNetå’ŒLight-ASDåŸºå‡†æµ‹è¯•å®ç°ï¼Œå¹¶åœ¨Ego4DåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„mAPæå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.06012v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.06012v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.06012v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.06012v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.06012v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Speech-to-Speech-Translation-with-Translatotron-A-State-of-the-Art-Review"><a href="#Speech-to-Speech-Translation-with-Translatotron-A-State-of-the-Art-Review" class="headerlink" title="Speech to Speech Translation with Translatotron: A State of the Art   Review"></a>Speech to Speech Translation with Translatotron: A State of the Art   Review</h2><p><strong>Authors:Jules R. Kala, Emmanuel Adetiba, Abdultaofeek Abayom, Oluwatobi E. Dare, Ayodele H. Ifijeh</strong></p>
<p>A cascade-based speech-to-speech translation has been considered a benchmark for a very long time, but it is plagued by many issues, like the time taken to translate a speech from one language to another and compound errors. These issues are because a cascade-based method uses a combination of methods such as speech recognition, speech-to-text translation, and finally, text-to-speech translation. Translatotron, a sequence-to-sequence direct speech-to-speech translation model was designed by Google to address the issues of compound errors associated with cascade model. Today there are 3 versions of the Translatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The first version was designed as a proof of concept to show that a direct speech-to-speech translation was possible, it was found to be less effective than the cascade model but was producing promising results. Translatotron2 was an improved version of Translatotron 1 with results similar to the cascade model. Translatotron 3 the latest version of the model is better than the cascade model at some points. In this paper, a complete review of speech-to-speech translation will be presented, with a particular focus on all the versions of Translatotron models. We will also show that Translatotron is the best model to bridge the language gap between African Languages and other well-formalized languages. </p>
<blockquote>
<p>åŸºäºçº§è”çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘å¾ˆé•¿ä¸€æ®µæ—¶é—´ä»¥æ¥ä¸€ç›´è¢«è§†ä¸ºä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä½†å®ƒå­˜åœ¨è®¸å¤šé—®é¢˜ï¼Œå¦‚å°†è¯­éŸ³ä»ä¸€ç§è¯­è¨€ç¿»è¯‘åˆ°å¦ä¸€ç§è¯­è¨€æ‰€éœ€çš„æ—¶é—´ä»¥åŠå¤åˆé”™è¯¯ã€‚è¿™äº›é—®é¢˜çš„åŸå› æ˜¯ï¼Œçº§è”æ–¹æ³•ç»“åˆäº†è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘ä»¥åŠæœ€åçš„æ–‡æœ¬åˆ°è¯­éŸ³çš„ç¿»è¯‘ç­‰æ–¹æ³•ã€‚è°·æ­Œè®¾è®¡äº†åºåˆ—åˆ°åºåˆ—çš„ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹Translatotronï¼Œä»¥è§£å†³ä¸çº§è”æ¨¡å‹ç›¸å…³çš„å¤åˆé”™è¯¯é—®é¢˜ã€‚å¦‚ä»Šï¼ŒTranslatotronæ¨¡å‹å·²æœ‰ä¸‰ä¸ªç‰ˆæœ¬ï¼šTranslatotron 1ã€Translatotron 2å’ŒTranslatotron3ã€‚ç¬¬ä¸€ä¸ªç‰ˆæœ¬æ˜¯ä¸ºäº†è¯æ˜ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘çš„å¯èƒ½æ€§è€Œè®¾è®¡çš„ï¼Œå®ƒçš„æ•ˆæœä¸å¦‚çº§è”æ¨¡å‹ï¼Œä½†äº§ç”Ÿäº†ä»¤äººé¼“èˆçš„ç»“æœã€‚Translatotron2æ˜¯Translatotron 1çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå…¶ç»“æœä¸çº§è”æ¨¡å‹ç›¸ä¼¼ã€‚æœ€æ–°ç‰ˆæœ¬çš„Translatotron 3åœ¨æŸäº›æ–¹é¢ä¼˜äºçº§è”æ¨¡å‹ã€‚æœ¬æ–‡å°†å…¨é¢å›é¡¾è¯­éŸ³åˆ°è¯­éŸ³çš„ç¿»è¯‘ï¼Œé‡ç‚¹å…³æ³¨Translatotronæ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬ã€‚æˆ‘ä»¬è¿˜å°†å±•ç¤ºTranslatotronæ˜¯å¼¥åˆéæ´²è¯­è¨€å’Œå…¶ä»–è§„èŒƒåŒ–è¯­è¨€ä¹‹é—´è¯­è¨€éšœç¢çš„æœ€ä½³æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05980v1">PDF</a> 12 pages and 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºçº§è”çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘é•¿æœŸè¢«è§†ä¸ºåŸºå‡†æ–¹æ³•ï¼Œä½†ä»å­˜åœ¨ç¿»è¯‘æ—¶é—´é•¿å’Œå¤åˆé”™è¯¯ç­‰é—®é¢˜ã€‚Googleè®¾è®¡çš„Translatotronåºåˆ—åˆ°åºåˆ—ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚æœ¬æ–‡å°†å…¨é¢å›é¡¾è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼Œé‡ç‚¹å…³æ³¨Translatotronæ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬ï¼Œå¹¶å±•ç¤ºTranslatotronæ˜¯å¼¥è¡¥éæ´²è¯­è¨€ä¸å…¶ä»–è‰¯å¥½å½¢å¼åŒ–è¯­è¨€ä¹‹é—´è¯­è¨€é¸¿æ²Ÿçš„æœ€ä½³æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº§è”çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘å­˜åœ¨ç¿»è¯‘æ—¶é—´é•¿å’Œå¤åˆé”™è¯¯ç­‰é—®é¢˜ã€‚</li>
<li>Translatotronæ˜¯ä¸€ç§åºåˆ—åˆ°åºåˆ—çš„ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³çº§è”æ¨¡å‹çš„é—®é¢˜ã€‚</li>
<li>Translatotronæ¨¡å‹æœ‰ä¸‰ä¸ªç‰ˆæœ¬ï¼šTranslatotron 1ã€Translatotron 2å’ŒTranslatotron 3ã€‚</li>
<li>Translatotron 1ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œè¯æ˜äº†ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘çš„å¯èƒ½æ€§ï¼Œä½†æ•ˆæœä¸å¦‚çº§è”æ¨¡å‹ã€‚</li>
<li>Translatotron 2æ˜¯Translatotron 1çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå…¶æ•ˆæœä¸çº§è”æ¨¡å‹ç›¸ä¼¼ã€‚</li>
<li>Translatotron 3åœ¨æŸäº›æ–¹é¢ä¼˜äºçº§è”æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05980v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05980v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05980v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Audio-Visual-Representation-Learning-via-Knowledge-Distillation-from-Speech-Foundation-Models"><a href="#Audio-Visual-Representation-Learning-via-Knowledge-Distillation-from-Speech-Foundation-Models" class="headerlink" title="Audio-Visual Representation Learning via Knowledge Distillation from   Speech Foundation Models"></a>Audio-Visual Representation Learning via Knowledge Distillation from   Speech Foundation Models</h2><p><strong>Authors:Jing-Xuan Zhang, Genshun Wan, Jianqing Gao, Zhen-Hua Ling</strong></p>
<p>Audio-visual representation learning is crucial for advancing multimodal speech processing tasks, such as lipreading and audio-visual speech recognition. Recently, speech foundation models (SFMs) have shown remarkable generalization capabilities across various speech-related tasks. Building on this progress, we propose an audio-visual representation learning model that leverages cross-modal knowledge distillation from SFMs. In our method, SFMs serve as teachers, from which multi-layer hidden representations are extracted using clean audio inputs. We also introduce a multi-teacher ensemble method to distill the student, which receives audio-visual data as inputs. A novel representational knowledge distillation loss is employed to train the student during pretraining, which is also applied during finetuning to further enhance the performance on downstream tasks. Our experiments utilized both a self-supervised SFM, WavLM, and a supervised SFM, iFLYTEK-speech. The results demonstrated that our proposed method achieved superior or at least comparable performance to previous state-of-the-art baselines across automatic speech recognition, visual speech recognition, and audio-visual speech recognition tasks. Additionally, comprehensive ablation studies and the visualization of learned representations were conducted to evaluate the effectiveness of our proposed method. </p>
<blockquote>
<p>è§†å¬è¡¨ç¤ºå­¦ä¹ å¯¹äºæ¨è¿›å¤šæ¨¡æ€è¯­éŸ³å¤„ç†ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å”‡è¯»å’Œè§†å¬è¯­éŸ³è¯†åˆ«ã€‚æœ€è¿‘ï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰åœ¨å„ç§è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†å¬è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ¥è‡ªSFMçš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼ŒSFMä½œä¸ºæ•™å¸ˆï¼Œä½¿ç”¨å¹²å‡€çš„éŸ³é¢‘è¾“å…¥æå–å¤šå±‚éšè—è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¤šæ•™å¸ˆé›†æˆæ–¹æ³•æ¥è’¸é¦å­¦ç”Ÿï¼Œå­¦ç”Ÿæ¥æ”¶è§†å¬æ•°æ®ä½œä¸ºè¾“å…¥ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œé‡‡ç”¨ä¸€ç§æ–°çš„è¡¨å¾çŸ¥è¯†è’¸é¦æŸå¤±æ¥è®­ç»ƒå­¦ç”Ÿï¼Œå¹¶åœ¨å¾®è°ƒæœŸé—´ä¹Ÿåº”ç”¨è¯¥æŸå¤±ï¼Œä»¥è¿›ä¸€æ­¥æé«˜åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒé‡‡ç”¨äº†è‡ªç›‘ç£çš„SFMâ€”â€”WavLMå’Œå—ç›‘ç£çš„SFMâ€”â€”iFLYTEK-speechã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè§†å¬è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†æˆ–è‡³å°‘ä¸æœ€æ–°åŸºçº¿æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œè¡¨ç¤ºå¯è§†åŒ–ï¼Œä»¥è¯„ä¼°æˆ‘ä»¬æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05766v1">PDF</a> accepted to Pattern Recognition</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­éŸ³æ¨¡å‹ï¼ˆSFMï¼‰çš„å¤šæ¨¡æ€è§†å¬è¡¨ç¤ºå­¦ä¹ å¯¹äºæé«˜è¯­éŸ³å¤„ç†ä»»åŠ¡å¦‚å”‡è¯»å’Œè§†å¬è¯­éŸ³è¯†åˆ«è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ©ç”¨æ¥è‡ªSFMçš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦çš„è§†å¬è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚ç ”ç©¶ä¸­ï¼ŒSFMä½œä¸ºæ•™å¸ˆï¼Œä½¿ç”¨çº¯å‡€éŸ³é¢‘è¾“å…¥æå–å¤šå±‚éšè—è¡¨ç¤ºã€‚åŒæ—¶å¼•å…¥å¤šæ•™å¸ˆé›†æˆæ–¹æ³•ä»¥è’¸é¦å­¦ç”Ÿæ¨¡å‹ï¼Œå­¦ç”Ÿæ¨¡å‹æ¥æ”¶è§†å¬æ•°æ®ä½œä¸ºè¾“å…¥ã€‚é‡‡ç”¨æ–°å‹è¡¨å¾çŸ¥è¯†è’¸é¦æŸå¤±æ¥é¢„è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶åœ¨å¾®è°ƒé˜¶æ®µç»§ç»­åº”ç”¨ä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚å®éªŒé‡‡ç”¨è‡ªç›‘ç£SFMâ€”â€”WavLMå’Œç›‘ SFMâ€”â€”iFLYTEK-speechï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè§†å¬è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡å…ˆå‰æœ€å…ˆè¿›çš„åŸºçº¿æ°´å¹³ã€‚åŒæ—¶è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œè¡¨å¾å¯è§†åŒ–ä»¥è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¡¨ç¤ºå­¦ä¹ å¯¹å¤šæ¨¡æ€è¯­éŸ³å¤„ç†ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚å”‡è¯»å’Œè§†å¬è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†åŸºäºè¯­éŸ³æ¨¡å‹ï¼ˆSFMï¼‰çš„è§†å¬è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æŠ€æœ¯ã€‚</li>
<li>SFMä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œä½¿ç”¨çº¯å‡€éŸ³é¢‘è¾“å…¥æå–å¤šå±‚éšè—è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥å¤šæ•™å¸ˆé›†æˆæ–¹æ³•ä»¥è¿›ä¸€æ­¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ¥æ”¶è§†å¬æ•°æ®ã€‚</li>
<li>é‡‡ç”¨æ–°å‹è¡¨å¾çŸ¥è¯†è’¸é¦æŸå¤±è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>å®éªŒé‡‡ç”¨è‡ªç›‘ç£å’Œç›‘ç£å­¦ä¹ SFMï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯­éŸ³ä»»åŠ¡ä¸Šæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>è¿›è¡Œäº†æ¶ˆèç ”ç©¶å’Œè¡¨å¾å¯è§†åŒ–ä»¥è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05766v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05766v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="IndexTTS-An-Industrial-Level-Controllable-and-Efficient-Zero-Shot-Text-To-Speech-System"><a href="#IndexTTS-An-Industrial-Level-Controllable-and-Efficient-Zero-Shot-Text-To-Speech-System" class="headerlink" title="IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot   Text-To-Speech System"></a>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot   Text-To-Speech System</h2><p><strong>Authors:Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang</strong></p>
<p>Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at <a target="_blank" rel="noopener" href="https://index-tts.github.io/">https://index-tts.github.io</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”±äºå…¶é«˜åº¦çš„è‡ªç„¶æ€§å’Œå¼ºå¤§çš„é›¶æ ·æœ¬è¯­éŸ³å…‹éš†èƒ½åŠ›ï¼Œå·²é€æ¸æˆä¸ºè¡Œä¸šçš„ä¸»æµã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†IndexTTSç³»ç»Ÿï¼Œå®ƒä¸»è¦åŸºäºXTTSå’ŒTortoiseæ¨¡å‹ï¼Œå¹¶å¢åŠ äº†ä¸€äº›æ–°é¢–çš„æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ä¸­å›½åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç»“åˆå­—ç¬¦å’Œæ‹¼éŸ³çš„æ··åˆå»ºæ¨¡æ–¹æ³•ï¼Œä½¿å¾—å¤šéŸ³å­—ç¬¦å’Œé•¿å°¾å­—ç¬¦çš„å‘éŸ³å¯æ§ã€‚æˆ‘ä»¬è¿˜å¯¹å‘é‡é‡åŒ–ï¼ˆVQï¼‰å’Œæœ‰é™æ ‡é‡é‡åŒ–ï¼ˆFSQï¼‰è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œç”¨äºå£°å­¦ç”Ÿæˆä»¤ç‰Œçš„å­—å…¸åˆ©ç”¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›è¯­éŸ³å…‹éš†æ•ˆæœå’Œç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºé€‚é…å™¨çš„è¯­éŸ³æ¡ä»¶ç¼–ç å™¨ï¼Œå¹¶ç”¨BigVGAN2æ›¿æ¢è¯­éŸ³ä»£ç è§£ç å™¨ã€‚ä¸XTTSç›¸æ¯”ï¼Œå®ƒåœ¨è‡ªç„¶æ€§ã€å†…å®¹ä¸€è‡´æ€§å’Œé›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è‡³äºå¼€æºä¸­æµè¡Œçš„TTSç³»ç»Ÿï¼Œå¦‚Fish-Speechã€CosyVoice2ã€FireRedTTSå’ŒF5-TTSï¼ŒIndexTTSå…·æœ‰ç›¸å¯¹ç®€å•çš„è®­ç»ƒè¿‡ç¨‹ã€æ›´å¯æ§çš„ä½¿ç”¨æ–¹å¼å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†è¿™äº›ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ¼”ç¤ºä½œå“è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://index-tts.github.io./">https://index-tts.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05512v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå·²æˆä¸ºè¡Œä¸šä¸»æµï¼Œå…·æœ‰é«˜åº¦çš„è‡ªç„¶æ€§å’Œå¼ºå¤§çš„é›¶æ ·æœ¬è¯­éŸ³å…‹éš†èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†IndexTTSç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŸºäºXTTSå’ŒTortoiseæ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆå»ºæ¨¡æ–¹æ³•å¤„ç†ä¸­æ–‡åœºæ™¯ä¸‹çš„å¤šéŸ³å­—å’Œé•¿å°¾å­—çš„å‘éŸ³é—®é¢˜ã€‚åŒæ—¶ï¼Œå¯¹å‘é‡é‡åŒ–ä¸æœ‰é™æ ‡é‡é‡åŒ–è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œä»¥æé«˜è¯­éŸ³å…‹éš†æ•ˆæœå’Œç¨³å®šæ€§ã€‚ä¸XTTSç›¸æ¯”ï¼ŒIndexTTSåœ¨è‡ªç„¶æ€§ã€å†…å®¹ä¸€è‡´æ€§å’Œé›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç›¸è¾ƒäºå¼€æºçš„TTSç³»ç»Ÿï¼Œå¦‚Fish-Speechã€CosyVoice2ç­‰ï¼ŒIndexTTSå…·æœ‰æ›´ç®€å•çš„è®­ç»ƒè¿‡ç¨‹ã€æ›´å¯æ§çš„ä½¿ç”¨ä½“éªŒå’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­è¡¨ç°å‡ºä¸»æµè¶‹åŠ¿ï¼Œå› å…¶é«˜è‡ªç„¶åº¦å’Œé›¶æ ·æœ¬è¯­éŸ³å…‹éš†èƒ½åŠ›è€Œå—åˆ°é‡è§†ã€‚</li>
<li>IndexTTSç³»ç»ŸåŸºäºXTTSå’ŒTortoiseæ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆå»ºæ¨¡æ–¹æ³•å¤„ç†ä¸­æ–‡åœºæ™¯ä¸‹çš„å­—ç¬¦å’Œæ‹¼éŸ³ç»“åˆé—®é¢˜ã€‚</li>
<li>IndexTTSåœ¨è¯­éŸ³å…‹éš†æ•ˆæœå’Œç¨³å®šæ€§æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œå¼•å…¥äº†åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„è¯­éŸ³æ¡ä»¶ç¼–ç å™¨ï¼Œå¹¶æ›¿æ¢äº†è¯­éŸ³ä»£ç è§£ç å™¨ã€‚</li>
<li>IndexTTSä¸XTTSç›¸æ¯”ï¼Œåœ¨è‡ªç„¶æ€§ã€å†…å®¹ä¸€è‡´æ€§å’Œé›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ–¹é¢æœ‰æ‰€æå‡ã€‚</li>
<li>IndexTTSç›¸è¾ƒäºå…¶ä»–å¼€æºTTSç³»ç»Ÿï¼Œå…·æœ‰æ›´ç®€å•çš„è®­ç»ƒè¿‡ç¨‹ã€æ›´ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>IndexTTSç³»ç»Ÿæ”¯æŒå¤šéŸ³å­—å’Œé•¿å°¾å­—çš„å¯æ§å‘éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05512v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05512v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05512v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05512v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Expressive-Voice-Conversion-with-Discrete-Pitch-Conditioned-Flow-Matching-Model"><a href="#Enhancing-Expressive-Voice-Conversion-with-Discrete-Pitch-Conditioned-Flow-Matching-Model" class="headerlink" title="Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned   Flow Matching Model"></a>Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned   Flow Matching Model</h2><p><strong>Authors:Jialong Zuo, Shengpeng Ji, Minghui Fang, Ziyue Jiang, Xize Cheng, Qian Yang, Wenrui Liu, Guangyan Zhang, Zehai Tu, Yiwen Guo, Zhou Zhao</strong></p>
<p>This paper introduces PFlow-VC, a conditional flow matching voice conversion model that leverages fine-grained discrete pitch tokens and target speaker prompt information for expressive voice conversion (VC). Previous VC works primarily focus on speaker conversion, with further exploration needed in enhancing expressiveness (such as prosody and emotion) for timbre conversion. Unlike previous methods, we adopt a simple and efficient approach to enhance the style expressiveness of voice conversion models. Specifically, we pretrain a self-supervised pitch VQVAE model to discretize speaker-irrelevant pitch information and leverage a masked pitch-conditioned flow matching model for Mel-spectrogram synthesis, which provides in-context pitch modeling capabilities for the speaker conversion model, effectively improving the voice style transfer capacity. Additionally, we improve timbre similarity by combining global timbre embeddings with time-varying timbre tokens. Experiments on unseen LibriTTS test-clean and emotional speech dataset ESD show the superiority of the PFlow-VC model in both timbre conversion and style transfer. Audio samples are available on the demo page <a target="_blank" rel="noopener" href="https://speechai-demo.github.io/PFlow-VC/">https://speechai-demo.github.io/PFlow-VC/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†PFlow-VCï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç²¾ç»†ç²’åº¦ç¦»æ•£éŸ³è°ƒæ ‡è®°å’Œç›®æ ‡è¯´è¯äººæç¤ºä¿¡æ¯çš„æ¡ä»¶æµåŒ¹é…è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œç”¨äºå®ç°è¡¨è¾¾æ€§è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ã€‚ä¹‹å‰çš„VCå·¥ä½œä¸»è¦é›†ä¸­åœ¨è¯´è¯äººè½¬æ¢ä¸Šï¼Œè€Œå¯¹äºéŸ³è‰²è½¬æ¢ä¸­å¢å¼ºè¡¨è¾¾æ€§ï¼ˆå¦‚è¯­è°ƒå’Œæƒ…æ„Ÿï¼‰çš„æ¢ç´¢ä»éœ€è¿›ä¸€æ­¥æ·±å…¥ã€‚ä¸åŒäºä»¥å¾€çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€å•é«˜æ•ˆçš„æ–¹æ³•æ¥å¢å¼ºè¯­éŸ³è½¬æ¢æ¨¡å‹çš„é£æ ¼è¡¨ç°åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¢„è®­ç»ƒäº†ä¸€ä¸ªè‡ªç›‘ç£çš„éŸ³è°ƒVQVAEæ¨¡å‹ï¼Œå¯¹ä¸è¯´è¯äººæ— å…³çš„éŸ³è°ƒä¿¡æ¯è¿›è¡Œç¦»æ•£åŒ–ï¼Œå¹¶åˆ©ç”¨æ©ç éŸ³è°ƒæ¡ä»¶ä¸‹çš„æµåŒ¹é…æ¨¡å‹è¿›è¡Œæ¢…å°”é¢‘è°±åˆæˆï¼Œä¸ºè¯´è¯äººè½¬æ¢æ¨¡å‹æä¾›ä¸Šä¸‹æ–‡éŸ³è°ƒå»ºæ¨¡èƒ½åŠ›ï¼Œæœ‰æ•ˆæé«˜è¯­éŸ³é£æ ¼è½¬ç§»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå…¨å±€éŸ³è‰²åµŒå…¥å’Œæ—¶å˜éŸ³è‰²æ ‡è®°æ¥æé«˜éŸ³è‰²ç›¸ä¼¼æ€§ã€‚åœ¨æœªè§è¿‡çš„LibriTTSæµ‹è¯•æ¸…æ´æ•°æ®é›†å’Œæƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ESDä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPFlow-VCæ¨¡å‹åœ¨éŸ³è‰²è½¬æ¢å’Œé£æ ¼è½¬ç§»æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨æ¼”ç¤ºé¡µé¢<a target="_blank" rel="noopener" href="https://speechai-demo.github.io/PFlow-VC/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://speechai-demo.github.io/PFlow-VC/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05471v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†PFlow-VCï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç²¾ç»†ç¦»æ•£éŸ³é«˜æ ‡è®°å’Œç›®æ ‡è¯´è¯äººæç¤ºä¿¡æ¯çš„æ¡ä»¶æµåŒ¹é…è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œç”¨äºå®ç°è¡¨ç°æ€§è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ã€‚ä¸ä¸»è¦å…³æ³¨è¯´è¯äººè½¬æ¢çš„å…ˆå‰VCå·¥ä½œä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€å•é«˜æ•ˆçš„æ–¹æ³•æ¥æé«˜è¯­éŸ³è½¬æ¢æ¨¡å‹çš„é£æ ¼è¡¨ç°åŠ›ï¼Œå¦‚ç¦»æ•£åŒ–éŸ³é«˜ä¿¡æ¯å’Œé‡‡ç”¨æ©ç éŸ³é«˜æ¡ä»¶æµåŒ¹é…æ¨¡å‹è¿›è¡Œæ¢…å°”é¢‘è°±åˆæˆç­‰ã€‚æ­¤å¤–ï¼Œç»“åˆå…¨å±€éŸ³è‰²åµŒå…¥å’Œæ—¶å˜éŸ³è‰²æ ‡è®°ï¼Œæé«˜äº†éŸ³è‰²ç›¸ä¼¼åº¦ã€‚åœ¨LibriTTSæµ‹è¯•æ¸…æ´é›†å’Œæƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ESDä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPFlow-VCæ¨¡å‹åœ¨éŸ³è‰²è½¬æ¢å’Œé£æ ¼è¿ç§»æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PFlow-VCæ˜¯ä¸€ç§æ¡ä»¶æµåŒ¹é…è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°è¡¨ç°æ€§è¯­éŸ³è½¬æ¢ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨ç²¾ç»†ç¦»æ•£éŸ³é«˜æ ‡è®°å’Œç›®æ ‡è¯´è¯äººæç¤ºä¿¡æ¯ã€‚</li>
<li>ä¸åŒäºä¼ ç»Ÿçš„è¯­éŸ³è½¬æ¢æ¨¡å‹ä¸»è¦å…³æ³¨è¯´è¯äººè½¬æ¢ï¼ŒPFlow-VCæé«˜äº†é£æ ¼è¡¨ç°åŠ›ã€‚</li>
<li>PFlow-VCé€šè¿‡é¢„è®­ç»ƒçš„éŸ³é«˜VQVAEæ¨¡å‹è¿›è¡ŒéŸ³é«˜ä¿¡æ¯çš„ç¦»æ•£åŒ–ã€‚</li>
<li>é‡‡ç”¨æ©ç éŸ³é«˜æ¡ä»¶æµåŒ¹é…æ¨¡å‹è¿›è¡Œæ¢…å°”é¢‘è°±åˆæˆï¼Œæé«˜äº†è¯­éŸ³é£æ ¼è½¬ç§»èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»“åˆå…¨å±€éŸ³è‰²åµŒå…¥å’Œæ—¶å˜éŸ³è‰²æ ‡è®°ï¼Œæé«˜äº†éŸ³è‰²è½¬æ¢çš„ç›¸ä¼¼åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05471v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05471v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05471v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05471v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Koel-TTS-Enhancing-LLM-based-Speech-Generation-with-Preference-Alignment-and-Classifier-Free-Guidance"><a href="#Koel-TTS-Enhancing-LLM-based-Speech-Generation-with-Preference-Alignment-and-Classifier-Free-Guidance" class="headerlink" title="Koel-TTS: Enhancing LLM based Speech Generation with Preference   Alignment and Classifier Free Guidance"></a>Koel-TTS: Enhancing LLM based Speech Generation with Preference   Alignment and Classifier Free Guidance</h2><p><strong>Authors:Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Mikyas T. Desta, Roy Fejgin, Rafael Valle, Jason Li</strong></p>
<p>While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website. </p>
<blockquote>
<p>è™½ç„¶è‡ªå›å½’è¯­éŸ³ä»¤ç‰Œç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿå…·æœ‰æ˜¾è‘—å¤šæ ·æ€§å’Œè‡ªç„¶æ€§çš„è¯­éŸ³ï¼Œä½†å®ƒä»¬å›ºæœ‰çš„ä¸å¯æ§æ€§å¸¸å¸¸å¯¼è‡´ä¸€äº›é—®é¢˜ï¼Œå¦‚å¹»è§‰å’Œä¸ç¬¦åˆæ¡ä»¶è¾“å…¥çš„æ„å¤–å‘å£°ã€‚æˆ‘ä»¬ä»‹ç»äº†Koel-TTSï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¢å¼ºçš„ç¼–ç å™¨-è§£ç å™¨Transformer TTSæ¨¡å‹ï¼Œé€šè¿‡èå…¥åå¥½å¯¹é½æŠ€æœ¯ï¼Œç»“åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººéªŒè¯æ¨¡å‹æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜èå…¥äº†æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œä»¥è¿›ä¸€æ­¥æé«˜åˆæˆè¯­éŸ³å¯¹æ–‡æœ¬å’Œå‚è€ƒè¯´è¯äººéŸ³é¢‘çš„éµå¾ªåº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›ä¼˜åŒ–æ˜¾è‘—æé«˜äº†ç›®æ ‡è¯´è¯äººçš„ç›¸ä¼¼æ€§ã€è¯­éŸ³çš„æ¸…æ™°åº¦å’Œåˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒKoel-TTSç›´æ¥å°†æ–‡æœ¬å’Œä¸Šä¸‹æ–‡éŸ³é¢‘æ˜ å°„åˆ°å£°éŸ³ä»¤ç‰Œä¸Šï¼Œå¹¶ä¸”åœ¨ä¸Šè¿°æŒ‡æ ‡ä¸Šï¼Œå³ä½¿åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¹Ÿä¼˜äºæœ€å…ˆè¿›çš„TTSæ¨¡å‹ã€‚éŸ³é¢‘æ ·æœ¬å’Œæ¼”ç¤ºè§†é¢‘å¯åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05236v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Koel-TTSï¼Œä¸€ç§æ”¹è¿›çš„ç¼–ç è§£ç å™¨Transformeræ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹å¥—ä»¶ï¼Œé€šè¿‡èå…¥åå¥½å¯¹é½æŠ€æœ¯è§£å†³äº†ç”Ÿæˆè¯­éŸ³çš„ä¸ä¸€è‡´æ€§å’Œç¼ºä¹æ§åˆ¶æ€§é—®é¢˜ã€‚é€šè¿‡èå…¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³éªŒè¯æ¨¡å‹è¿›è¡Œå¼•å¯¼ï¼ŒåŒæ—¶å¼•å…¥æ— åˆ†ç±»å™¨å¼•å¯¼è¿›ä¸€æ­¥æå‡è¯­éŸ³åˆæˆå‡†ç¡®æ€§å¹¶æ›´æ¥è¿‘åŸå§‹æ¼”è®²ç¨¿å†…å®¹ã€‚å®éªŒè¡¨æ˜è¿™äº›ä¼˜åŒ–æé«˜äº†åˆæˆè¯­éŸ³çš„ç›®æ ‡å‡†ç¡®åº¦ã€å¯è¾¨è¯†åº¦å’Œè‡ªç„¶åº¦ã€‚Koel-TTSç›´æ¥å°†æ–‡æœ¬å’Œä¸Šä¸‹æ–‡éŸ³é¢‘æ˜ å°„åˆ°å£°éŸ³æ ‡è®°ä¸Šï¼Œåœ¨ç›¸å…³æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„TTSæ¨¡å‹ï¼Œå³ä½¿æ˜¯åœ¨è®­ç»ƒæ•°æ®é‡æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æœ‰å…³éŸ³é¢‘æ ·æœ¬å’Œæ¼”ç¤ºå¯è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Koel-TTSæ˜¯ä¸€ç§æ”¹è¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹ç¼ºä¹æ§åˆ¶æ€§å’Œåˆæˆè¯­éŸ³ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡èå…¥åå¥½å¯¹é½æŠ€æœ¯å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»¥åŠè¯­éŸ³éªŒè¯æ¨¡å‹çš„å¼•å¯¼æ¥æå‡æ€§èƒ½ã€‚</li>
<li>Koel-TTSé‡‡ç”¨ç¼–ç è§£ç å™¨Transformerç»“æ„ï¼Œèƒ½å¤Ÿç›´æ¥å°†æ–‡æœ¬å’Œä¸Šä¸‹æ–‡éŸ³é¢‘æ˜ å°„åˆ°å£°éŸ³æ ‡è®°ä¸Šã€‚</li>
<li>æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•è¢«ç”¨æ¥è¿›ä¸€æ­¥æå‡è¯­éŸ³åˆæˆçš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKoel-TTSåœ¨ç›®æ ‡è¯´è¯äººç›¸ä¼¼æ€§ã€å¯è¾¨è¯†åº¦å’Œè‡ªç„¶åº¦æ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
<li>Koel-TTSåœ¨æ€§èƒ½æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰TTSæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ•°æ®é‡è¾ƒå°çš„æƒ…å†µä¸‹ä»è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05236v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05236v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05236v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05236v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05236v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Aligner-Encoders-Self-Attention-Transformers-Can-Be-Self-Transducers"><a href="#Aligner-Encoders-Self-Attention-Transformers-Can-Be-Self-Transducers" class="headerlink" title="Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers"></a>Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers</h2><p><strong>Authors:Adam Stooke, Rohit Prabhavalkar, Khe Chai Sim, Pedro Moreno Mengibar</strong></p>
<p>Modern systems for automatic speech recognition, including the RNN-Transducer and Attention-based Encoder-Decoder (AED), are designed so that the encoder is not required to alter the time-position of information from the audio sequence into the embedding; alignment to the final text output is processed during decoding. We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding. This new phenomenon enables a simpler and more efficient model, the â€œAligner-Encoderâ€. To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention â€“ it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message. We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition. In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly, we find that the audio-text alignment is clearly visible in the self-attention weights of a certain layer, which could be said to perform â€œself-transductionâ€. </p>
<blockquote>
<p>ç°ä»£è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ŒåŒ…æ‹¬RNN-Transducerå’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ï¼ˆAEDï¼‰ï¼Œå…¶è®¾è®¡ä½¿å¾—ç¼–ç å™¨ä¸éœ€è¦æ”¹å˜éŸ³é¢‘åºåˆ—ä¸­çš„ä¿¡æ¯çš„æ—¶é—´ä½ç½®åˆ°åµŒå…¥ä¸­ï¼›æœ€ç»ˆæ–‡æœ¬è¾“å‡ºçš„å¯¹é½è¿‡ç¨‹æ˜¯åœ¨è§£ç æœŸé—´è¿›è¡Œçš„ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿‘å¹´æ¥é‡‡ç”¨çš„åŸºäºå˜æ¢å™¨çš„ç¼–ç å™¨å®é™…ä¸Šèƒ½å¤Ÿåœ¨å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­å†…éƒ¨è¿›è¡Œå¯¹é½ï¼Œè€Œæ— éœ€åœ¨è§£ç ä¹‹å‰è¿›è¡Œå¯¹é½ã€‚è¿™ä¸€æ–°ç°è±¡ä½¿å¾—æ›´ç®€æ´é«˜æ•ˆçš„æ¨¡å‹â€”â€”â€œå¯¹é½å™¨ç¼–ç å™¨â€æˆä¸ºå¯èƒ½ã€‚ä¸ºäº†å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬æ”¾å¼ƒäº†RNN-Tçš„åŠ¨æ€è§„åˆ’ï¼Œé‡‡ç”¨äº†AEDçš„å¸§çº§äº¤å‰ç†µæŸå¤±ï¼Œè€Œè§£ç å™¨åˆ™é‡‡ç”¨äº†RNN-Tçš„è½»é‡çº§çº¯æ–‡æœ¬é€’å½’ï¼Œæ— éœ€å­¦ä¹ äº¤å‰æ³¨æ„åŠ›â€”â€”å®ƒåªéœ€æŒ‰åºæ‰«æåµŒå…¥å¸§ï¼Œä»å¤´åˆ°å°¾ç”Ÿæˆä¸€ä¸ªä»¤ç‰Œï¼Œç›´åˆ°é¢„æµ‹æ¶ˆæ¯ç»“æŸã€‚æˆ‘ä»¬è¿›è¡Œäº†å®éªŒï¼Œè¯æ˜äº†å…¶æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“æ¥è¿‘ï¼ŒåŒ…æ‹¬ä¸€ç§ç‰¹æ®Šçš„æ¨ç†é…ç½®ï¼Œå¯å®ç°é•¿å½¢å¼è¯†åˆ«ã€‚åœ¨ä»£è¡¨æ€§æ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬æµ‹é‡æˆ‘ä»¬çš„æ¨¡å‹çš„æ€»æ¨ç†æ—¶é—´æ˜¯RNN-Tçš„2å€å¿«å’ŒAEDçš„16å€å¿«ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æŸä¸€å±‚çš„è‡ªæˆ‘æ³¨æ„åŠ›æƒé‡ä¸­æ¸…æ™°åœ°æ˜¾ç¤ºäº†éŸ³é¢‘æ–‡æœ¬å¯¹é½ï¼Œå¯ä»¥è¯´æ˜¯å®ç°äº†â€œè‡ªæˆ‘ä¼ å¯¼â€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­çš„æ–°å‘ç°ï¼Œå³åŸºäºè½¬æ¢å™¨çš„ç¼–ç å™¨å¯åœ¨è§£ç å‰å†…éƒ¨è¿›è¡Œå¯¹é½ï¼Œä»è€Œç®€åŒ–æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨AEDçš„å¸§çº§äº¤å‰ç†µæŸå¤±æ›¿ä»£RNN-Tçš„åŠ¨æ€è§„åˆ’è®­ç»ƒï¼Œå¹¶ç®€åŒ–è§£ç å™¨ï¼Œè¯¥æ¨¡å‹å®ç°äº†å¿«é€Ÿæ¨ç†å’Œæ¥è¿‘å‰æ²¿çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æ¨ç†é€Ÿåº¦æ˜¯RNN-Tçš„ä¸¤å€ï¼Œæ˜¯AEDçš„16å€ã€‚æ­¤å¤–ï¼Œè¿˜è§‚å¯Ÿåˆ°æŸä¸€å±‚çš„è‡ªæˆ‘æ³¨æ„åŠ›æƒé‡ä¸­æ˜æ˜¾çš„éŸ³é¢‘æ–‡æœ¬å¯¹é½ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºè½¬æ¢å™¨çš„ç¼–ç å™¨å¯åœ¨è§£ç å‰å†…éƒ¨è¿›è¡Œå¯¹é½ï¼Œç®€åŒ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ¨¡å‹ã€‚</li>
<li>æå‡ºäº†åä¸ºâ€œAligner-Encoderâ€çš„æ–°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸¢å¼ƒRNN-Tçš„åŠ¨æ€è§„åˆ’ï¼Œé‡‡ç”¨AEDçš„å¸§çº§äº¤å‰ç†µæŸå¤±è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è§£ç å™¨é‡‡ç”¨RNN-Tçš„è½»é‡çº§æ–‡æœ¬å¤å‘æœºåˆ¶ï¼Œæ— éœ€å­¦ä¹ äº¤å‰æ³¨æ„åŠ›ï¼ŒæŒ‰åºæ‰«æåµŒå…¥æ¡†æ¶å³å¯ç”Ÿæˆä»¤ç‰Œã€‚</li>
<li>è¯¥æ¨¡å‹æ€§èƒ½æ¥è¿‘å‰æ²¿æ°´å¹³ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æ¨ç†é€Ÿåº¦è¿œè¶…RNN-Tå’ŒAEDã€‚</li>
<li>è¯¥æ¨¡å‹å¯å®ç°é•¿å½¢å¼è¯†åˆ«ï¼Œå…·æœ‰ç‰¹æ®Šçš„æ¨ç†é…ç½®ã€‚</li>
<li>åœ¨æ¨¡å‹çš„æŸä¸€å±‚ä¸­è§‚å¯Ÿåˆ°æ˜æ˜¾çš„éŸ³é¢‘æ–‡æœ¬å¯¹é½ç°è±¡ï¼Œä½“ç°åœ¨è‡ªæˆ‘æ³¨æ„åŠ›æƒé‡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05232v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05232v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05232v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05232v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2502.05232v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unveiling-Interpretability-in-Self-Supervised-Speech-Representations-for-Parkinsonâ€™s-Diagnosis"><a href="#Unveiling-Interpretability-in-Self-Supervised-Speech-Representations-for-Parkinsonâ€™s-Diagnosis" class="headerlink" title="Unveiling Interpretability in Self-Supervised Speech Representations for   Parkinsonâ€™s Diagnosis"></a>Unveiling Interpretability in Self-Supervised Speech Representations for   Parkinsonâ€™s Diagnosis</h2><p><strong>Authors:David Gimeno-GÃ³mez, Catarina Botelho, Anna Pompili, Alberto Abad, Carlos-D. MartÃ­nez-Hinarejos</strong></p>
<p>Recent works in pathological speech analysis have increasingly relied on powerful self-supervised speech representations, leading to promising results. However, the complex, black-box nature of these embeddings and the limited research on their interpretability significantly restrict their adoption for clinical diagnosis. To address this gap, we propose a novel, interpretable framework specifically designed to support Parkinsonâ€™s Disease (PD) diagnosis. Through the design of simple yet effective cross-attention mechanisms for both embedding- and temporal-level analysis, the proposed framework offers interpretability from two distinct but complementary perspectives. Experimental findings across five well-established speech benchmarks for PD detection demonstrate the frameworkâ€™s capability to identify meaningful speech patterns within self-supervised representations for a wide range of assessment tasks. Fine-grained temporal analyses further underscore its potential to enhance the interpretability of deep-learning pathological speech models, paving the way for the development of more transparent, trustworthy, and clinically applicable computer-assisted diagnosis systems in this domain. Moreover, in terms of classification accuracy, our method achieves results competitive with state-of-the-art approaches, while also demonstrating robustness in cross-lingual scenarios when applied to spontaneous speech production. </p>
<blockquote>
<p>è¿‘æœŸç—…ç†æ€§è¯­éŸ³åˆ†æçš„ç ”ç©¶è¶Šæ¥è¶Šä¾èµ–äºå¼ºå¤§çš„è‡ªç›‘ç£è¯­éŸ³è¡¨å¾ï¼Œè¿™å¸¦æ¥äº†å¾ˆæœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›åµŒå…¥çš„å¤æ‚æ€§å’Œé»‘ç®±æ€§è´¨ï¼Œä»¥åŠå¯¹å…¶å¯è§£é‡Šæ€§çš„ç ”ç©¶æœ‰é™ï¼Œæ˜¾è‘—é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§£é‡Šæ¡†æ¶ï¼Œä¸“ä¸ºæ”¯æŒå¸•é‡‘æ£®ç—…çš„è¯Šæ–­è€Œè®¾è®¡ã€‚é€šè¿‡ä¸ºåµŒå…¥çº§å’Œæ—¶é—´çº§åˆ†æè®¾è®¡ç®€å•æœ‰æ•ˆçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æ¡†æ¶ä»ä¸¤ä¸ªä¸åŒä½†äº’è¡¥çš„è§’åº¦æä¾›å¯è§£é‡Šæ€§ã€‚åœ¨äº”ä¸ªæˆç†Ÿçš„å¸•é‡‘æ£®ç—…æ£€æµ‹è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨è‡ªç›‘ç£è¡¨å¾ä¸­è¯†åˆ«å„ç§è¯„ä¼°ä»»åŠ¡ä¸­æœ‰æ„ä¹‰çš„è¯­éŸ³æ¨¡å¼ã€‚ç²¾ç»†çš„æ—¶é—´åˆ†æè¿›ä¸€æ­¥çªå‡ºäº†å…¶åœ¨æé«˜æ·±åº¦å­¦ä¹ ç—…ç†è¯­éŸ³æ¨¡å‹çš„å¯è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å…·é€æ˜åº¦ã€å¯ä¿¡èµ–åº¦å’Œä¸´åºŠé€‚ç”¨æ€§çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†ç±»å‡†ç¡®åº¦æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•ç›¸ç«äº‰çš„ç»“æœï¼Œå¹¶ä¸”åœ¨åº”ç”¨äºè‡ªç„¶è¯­éŸ³ç”Ÿäº§æ—¶è¡¨ç°å‡ºè·¨è¯­è¨€çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02006v2">PDF</a> Accepted in the Special Issue on â€œModelling and Processing Language   and Speech in Neurodegenerative Disordersâ€ published by Journal of Selected   Topics in Signal Processing (JSTSP)</p>
<p><strong>Summary</strong><br>å¸•é‡‘æ£®ç—…çš„ç—…ç†è¯­éŸ³åˆ†ææ˜¯è¿‘æœŸç ”ç©¶çƒ­ç‚¹ï¼Œç°æœ‰çš„è‡ªç›‘ç£è¯­éŸ³è¡¨å¾æŠ€æœ¯å–å¾—äº†è‰¯å¥½æ•ˆæœï¼Œä½†ç¼ºä¹è§£é‡Šæ€§é™åˆ¶äº†å…¶åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§£é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰çš„è¯Šæ–­ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ç®€å•æœ‰æ•ˆçš„è·¨åµŒå…¥å’Œæ—¶åºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»ä¸¤ä¸ªä¸åŒä½†äº’è¡¥çš„è§’åº¦æä¾›è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨è‡ªç›‘ç£è¡¨å¾ä¸­è¯†åˆ«æœ‰æ„ä¹‰çš„è¯­éŸ³æ¨¡å¼ï¼Œç”¨äºå¹¿æ³›çš„è¯„ä¼°ä»»åŠ¡ï¼Œæé«˜äº†æ·±åº¦å­¦ä¹ çš„å¯è§£é‡Šæ€§ï¼Œä¸ºå¼€å‘æ›´é€æ˜ã€å¯é çš„ä¸´åºŠè¾…åŠ©è¯Šæ–­ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•çš„åˆ†ç±»ç²¾åº¦ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå¹¶ä¸”åœ¨è·¨è¯­è¨€åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸•é‡‘æ£®ç—…ç—…ç†è¯­éŸ³åˆ†ææ˜¯å½“å‰çš„çƒ­é—¨ç ”ç©¶é¢†åŸŸã€‚</li>
<li>è‡ªç›‘ç£è¯­éŸ³è¡¨å¾æŠ€æœ¯åœ¨è¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>ç¼ºä¹è§£é‡Šæ€§æ˜¯å½“å‰æŠ€æœ¯åº”ç”¨äºä¸´åºŠè¯Šæ–­çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§£é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¸•é‡‘æ£®ç—…çš„è¯Šæ–­ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è·¨åµŒå…¥å’Œæ—¶åºæ³¨æ„åŠ›æœºåˆ¶æä¾›åŒé‡è§£é‡Šè§’åº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨å¤šç§è¯„ä¼°ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2412.02006v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2412.02006v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2412.02006v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Fast-and-High-Quality-Auto-Regressive-Speech-Synthesis-via-Speculative-Decoding"><a href="#Fast-and-High-Quality-Auto-Regressive-Speech-Synthesis-via-Speculative-Decoding" class="headerlink" title="Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative   Decoding"></a>Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative   Decoding</h2><p><strong>Authors:Bohan Li, Hankun Wang, Situo Zhang, Yiwei Guo, Kai Yu</strong></p>
<p>The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens. </p>
<blockquote>
<p>è‡ªå›å½’æ¶æ„ï¼Œå¦‚GPTï¼Œåœ¨ç°ä»£æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ƒäº§ç”Ÿäº†å¤§é‡çš„æ¨ç†æ—¶é—´ï¼Œå°¤å…¶æ˜¯å› ä¸ºé•¿è¯­éŸ³åºåˆ—ä»¤ç‰Œå¸¦æ¥çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VADUSAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡æŠ•æœºè§£ç åŠ é€Ÿè‡ªå›å½’TTSçš„æ–¹æ³•ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒVADUSAä¸ä»…æ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ï¼Œè€Œä¸”é€šè¿‡èå…¥è‰ç¨¿å¤´è¿›è¡Œæœªæ¥è¯­éŸ³å†…å®¹çš„è‡ªå›å½’é¢„æµ‹æ¥æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé‡‡æ ·è¿‡ç¨‹ä¸­å®¹é”™æœºåˆ¶çš„åŠ å…¥åœ¨ä¸ç‰ºç‰²è´¨é‡çš„æƒ…å†µä¸‹åŠ é€Ÿäº†æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§æ•°æ®é›†å’Œå„ç§è¯­éŸ³ä»¤ç‰Œç±»å‹ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21951v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†VADUSAï¼Œè¿™æ˜¯ä¸€ç§åŠ é€ŸåŸºäºGPTçš„è‡ªå›å½’æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨æŠ•æœºè§£ç æŠ€æœ¯ï¼ŒVADUSAä¸ä»…æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè¿˜é€šè¿‡å¼•å…¥è‰æ¡ˆå¤´è¿›è¡Œæœªæ¥è¯­éŸ³å†…å®¹çš„è‡ªå›å½’é¢„æµ‹æé«˜äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé‡‡æ ·è¿‡ç¨‹ä¸­çš„å®¹å¿æœºåˆ¶è¿›ä¸€æ­¥åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¸æŸå®³è´¨é‡ã€‚æ­¤æ–¹æ³•åœ¨å¤§å‹æ•°æ®é›†å’Œå„ç§ç±»å‹çš„è¯­éŸ³ä»¤ç‰Œä¸Šå…·æœ‰è¾ƒå¼ºçš„é€šç”¨æ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>VADUSAæ˜¯é¦–ä¸ªåŠ é€Ÿè‡ªå›å½’æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„æ–¹æ³•ä¹‹ä¸€ã€‚</li>
<li>é€šè¿‡æŠ•æœºè§£ç æŠ€æœ¯æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
<li>é€šè¿‡å¼•å…¥è‰æ¡ˆå¤´è¿›è¡Œæœªæ¥è¯­éŸ³å†…å®¹çš„è‡ªå›å½’é¢„æµ‹ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>é‡‡æ ·è¿‡ç¨‹ä¸­çš„å®¹å¿æœºåˆ¶è¿›ä¸€æ­¥åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>VADUSAåœ¨ä¸å½±å“è´¨é‡çš„å‰æä¸‹åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰åœ¨å¤§å‹æ•°æ®é›†ä¸Šå®ç°å¼ºé€šç”¨æ€§çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2410.21951v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2410.21951v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2410.21951v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2410.21951v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="High-Resolution-Speech-Restoration-with-Latent-Diffusion-Model"><a href="#High-Resolution-Speech-Restoration-with-Latent-Diffusion-Model" class="headerlink" title="High-Resolution Speech Restoration with Latent Diffusion Model"></a>High-Resolution Speech Restoration with Latent Diffusion Model</h2><p><strong>Authors:Tushar Dhyani, Florian Lux, Michele Mancusi, Giorgio Fabbro, Fritz Hohl, Ngoc Thang Vu</strong></p>
<p>Traditional speech enhancement methods often oversimplify the task of restoration by focusing on a single type of distortion. Generative models that handle multiple distortions frequently struggle with phone reconstruction and high-frequency harmonics, leading to breathing and gasping artifacts that reduce the intelligibility of reconstructed speech. These models are also computationally demanding, and many solutions are restricted to producing outputs in the wide-band frequency range, which limits their suitability for professional applications. To address these challenges, we propose Hi-ResLDM, a novel generative model based on latent diffusion designed to remove multiple distortions and restore speech recordings to studio quality, sampled at 48kHz. We benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and Conditional Flow Matching (CFM) components, demonstrating superior performance in regenerating high-frequency-band details. Hi-ResLDM not only excels in non-instrusive metrics but is also consistently preferred in human evaluation and performs competitively on intrusive evaluations, making it ideal for high-resolution speech restoration. </p>
<blockquote>
<p>ä¼ ç»Ÿè¯­éŸ³å¢å¼ºæ–¹æ³•é€šå¸¸é€šè¿‡ä¸“æ³¨äºä¸€ç§å¤±çœŸæ¥ç®€åŒ–æ¢å¤ä»»åŠ¡ã€‚å¤„ç†å¤šç§å¤±çœŸçš„ç”Ÿæˆæ¨¡å‹åœ¨ç”µè¯é‡å»ºå’Œé«˜é¢‘è°æ³¢æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å‡ºç°å‘¼å¸å’Œå–˜æ¯ä¼ªå½±ï¼Œé™ä½äº†é‡å»ºè¯­éŸ³çš„å¯æ‡‚åº¦ã€‚è¿™äº›æ¨¡å‹è®¡ç®—é‡ä¹Ÿå¾ˆå¤§ï¼Œè®¸å¤šè§£å†³æ–¹æ¡ˆä»…é™äºåœ¨å®½å¸¦é¢‘ç‡èŒƒå›´å†…äº§ç”Ÿè¾“å‡ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸“ä¸šåº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Hi-ResLDMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æ¶ˆé™¤å¤šç§å¤±çœŸï¼Œå¹¶å°†è¯­éŸ³è®°å½•æ¢å¤åˆ°48kHzé‡‡æ ·çš„å½•éŸ³æ£šè´¨é‡ã€‚æˆ‘ä»¬å°†Hi-ResLDMä¸åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰ç»„ä»¶çš„æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œåœ¨é‡æ–°ç”Ÿæˆé«˜é¢‘å¸¦ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚Hi-ResLDMä¸ä»…åœ¨éä¾µå…¥æ€§æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”åœ¨äººç±»è¯„ä¼°ä¸­å§‹ç»ˆæ›´å—æ¬¢è¿ï¼Œåœ¨ä¾µå…¥æ€§è¯„ä¼°ä¸­ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä½¿å…¶æˆä¸ºé«˜åˆ†è¾¨ç‡è¯­éŸ³æ¢å¤çš„ç†æƒ³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11145v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¼ ç»Ÿè¯­éŸ³å¢å¼ºæ–¹æ³•çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆæ¨¡å‹â€”â€”Hi-ResLDMã€‚è¯¥æ¨¡å‹æ—¨åœ¨å»é™¤å¤šç§å¤±çœŸå¹¶æ¢å¤è¯­éŸ³å½•åˆ¶è‡³æ¥è¿‘å½•éŸ³å®¤è´¨é‡ï¼Œé‡‡æ ·ç‡ä¸º48kHzã€‚ç›¸è¾ƒäºåˆ©ç”¨GANå’Œæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰ç»„ä»¶çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒHi-ResLDMåœ¨å†ç”Ÿé«˜é¢‘å¸¦ç»†èŠ‚æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ä»…åœ¨éä¾µå…¥æ€§æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”åœ¨äººç±»è¯„ä¼°å’Œä¾µå…¥æ€§è¯„ä¼°ä¸­ä¹Ÿå…·æœ‰ç«äº‰åŠ›ï¼Œæ˜¯ç†æƒ³çš„é«˜åˆ†è¾¨ç‡è¯­éŸ³æ¢å¤è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿè¯­éŸ³å¢å¼ºæ–¹æ³•é€šå¸¸åªå…³æ³¨å•ä¸€ç±»å‹çš„å¤±çœŸï¼Œè€ŒHi-ResLDMæ˜¯ä¸€ä¸ªåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨å»é™¤å¤šç§å¤±çœŸã€‚</li>
<li>Hi-ResLDMå¯ä»¥æ¢å¤è¯­éŸ³å½•åˆ¶è‡³å½•éŸ³å®¤è´¨é‡ï¼Œé‡‡æ ·ç‡é«˜è¾¾48kHzã€‚</li>
<li>ä¸åˆ©ç”¨GANå’ŒCFMç»„ä»¶çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒHi-ResLDMåœ¨å†ç”Ÿé«˜é¢‘å¸¦ç»†èŠ‚æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>Hi-ResLDMåœ¨éä¾µå…¥æ€§æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>Hi-ResLDMåœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Hi-ResLDMåœ¨ä¾µå…¥æ€§è¯„ä¼°ä¸­ä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2409.11145v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2409.11145v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2409.11145v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Speech/2409.11145v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/GAN/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_GAN/2405.00239v2/page_2_0.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ViSIR Vision Transformer Single Image Reconstruction Method for Earth   System Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å½±åƒ_Breast Ultrasound/2407.02844v6/page_5_0.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Exploiting Precision Mapping and Component-Specific Feature Enhancement   for Breast Cancer Segmentation and Identification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11370.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
