<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Visual Agentic AI for Spatial Reasoning with a Dynamic API">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8a55e07fb790a3e01288324e7e95b5cc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="Visual-Agentic-AI-for-Spatial-Reasoning-with-a-Dynamic-API"><a href="#Visual-Agentic-AI-for-Spatial-Reasoning-with-a-Dynamic-API" class="headerlink" title="Visual Agentic AI for Spatial Reasoning with a Dynamic API"></a>Visual Agentic AI for Spatial Reasoning with a Dynamic API</h2><p><strong>Authors:Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari</strong></p>
<p>Visual reasoning â€“ the ability to interpret the visual world â€“ is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: <a target="_blank" rel="noopener" href="https://glab-caltech.github.io/vadar/">https://glab-caltech.github.io/vadar/</a> </p>
<blockquote>
<p>è§†è§‰æ¨ç†â€”â€”è§£é‡Šè§†è§‰ä¸–ç•Œçš„èƒ½åŠ›â€”â€”å¯¹äºåœ¨ä¸‰ç»´åœºæ™¯ä¸­æ“ä½œçš„å®ä½“ä»£ç†è‡³å…³é‡è¦ã€‚äººå·¥æ™ºèƒ½çš„è¿›æ­¥å¯¼è‡´äº†èƒ½å¤Ÿå›ç­”å›¾åƒé—®é¢˜çš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„å‡ºç°ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹3Dç©ºé—´æ¨ç†ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ç±»æ¨ç†é—®é¢˜çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä»£ç†ç¨‹åºç»¼åˆæ–¹æ³•ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ååŒç”ŸæˆPythonic APIï¼Œå¹¶æ·»åŠ æ–°å‡½æ•°ä»¥è§£å†³å¸¸è§å­é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†ä»¥å‰ä¾èµ–äºé™æ€äººä¸ºå®šä¹‰çš„APIçš„æ–¹æ³•çš„å±€é™æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¹¿æ³›çš„æŸ¥è¯¢ã€‚ä¸ºäº†è¯„ä¼°AIå¯¹ä¸‰ç»´åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æŸ¥è¯¢åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠå¤šä¸ªæ­¥éª¤çš„æ¥åœ°å’Œæ¨ç†ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´è§†è§‰æ¨ç†æ–¹é¢ä¼˜äºä¹‹å‰çš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œå¹¶é€šè¿‡å®è¯éªŒè¯äº†æˆ‘ä»¬çš„ä»£ç†æ¡†æ¶åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://glab-caltech.github.io/vadar/">https://glab-caltech.github.io/vadar/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06787v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://glab-caltech.github.io/vadar/">https://glab-caltech.github.io/vadar/</a></p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æ¨ç†â€”â€”è§£é‡Šè§†è§‰ä¸–ç•Œçš„èƒ½åŠ›å¯¹äºåœ¨ä¸‰ç»´åœºæ™¯ä¸­æ“ä½œçš„å®ä½“ä»£ç†è‡³å…³é‡è¦ã€‚éšç€äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œå‡ºç°äº†èƒ½å¤Ÿå›ç­”å›¾åƒé—®é¢˜çš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹ä¸‰ç»´ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šä¸‹é™ã€‚ä¸ºè§£å†³æ­¤ç±»å¤æ‚æ¨ç†é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä»£ç†ç¨‹åºç»¼åˆæ–¹æ³•ï¼Œå…¶ä¸­LLMä»£ç†ååŒç”ŸæˆPythonic APIçš„æ–°åŠŸèƒ½æ¥è§£å†³å¸¸è§å­é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†ä»¥å‰ä¾èµ–äºé™æ€ã€äººä¸ºå®šä¹‰çš„APIçš„æ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿå¤„ç†æ›´å¹¿æ³›çš„æŸ¥è¯¢ã€‚ä¸ºäº†è¯„ä¼°AIå¯¹ä¸‰ç»´ç†è§£çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŸ¥è¯¢åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠå¤šæ­¥å®šä½å’Œæ¨ç†ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´è§†è§‰æ¨ç†æ–¹é¢ä¼˜äºå…ˆå‰çš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ä»£ç†æ¡†æ¶åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†å¯¹äºåœ¨ä¸‰ç»´åœºæ™¯ä¸­æ“ä½œçš„å®ä½“ä»£ç†è‡³å…³é‡è¦ã€‚</li>
<li>AIçš„è¿›æ­¥å·²ç»æ¨åŠ¨äº†è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œèƒ½å¤Ÿå›ç­”å›¾åƒç›¸å…³çš„é—®é¢˜ã€‚</li>
<li>é¢å¯¹ä¸‰ç»´ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶ï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä»£ç†ç¨‹åºç»¼åˆæ–¹æ³•ï¼ŒLLMä»£ç†èƒ½å¤ŸååŒå·¥ä½œï¼Œç”Ÿæˆæ–°çš„Pythonic APIåŠŸèƒ½æ¥è§£å†³å¸¸è§çš„å­é—®é¢˜ã€‚</li>
<li>ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æ›´å¹¿æ³›çš„æŸ¥è¯¢ï¼Œå› ä¸ºå®ƒä¸ä¾èµ–äºé™æ€ã€äººä¸ºå®šä¹‰çš„APIã€‚</li>
<li>ä¸ºäº†è¯„ä¼°AIåœ¨ä¸‰ç»´ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŸ¥è¯¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šæ­¥å®šä½å’Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9fbb4157028d262848ab83888a76701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abed9557003bfeedcaffdb4de0d86cae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cd65ccc14d4e4696d872b8e49a43584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3337c754d506d03ec03d40bcdbaf766c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2d0c99df379e38355c5e24b313fd45.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AgilePilot-DRL-Based-Drone-Agent-for-Real-Time-Motion-Planning-in-Dynamic-Environments-by-Leveraging-Object-Detection"><a href="#AgilePilot-DRL-Based-Drone-Agent-for-Real-Time-Motion-Planning-in-Dynamic-Environments-by-Leveraging-Object-Detection" class="headerlink" title="AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in   Dynamic Environments by Leveraging Object Detection"></a>AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in   Dynamic Environments by Leveraging Object Detection</h2><p><strong>Authors:Roohan Ahmed Khan, Valerii Serpiva, Demetros Aschalew, Aleksey Fedoseev, Dzmitry Tsetserukou</strong></p>
<p>Autonomous drone navigation in dynamic environments remains a critical challenge, especially when dealing with unpredictable scenarios including fast-moving objects with rapidly changing goal positions. While traditional planners and classical optimisation methods have been extensively used to address this dynamic problem, they often face real-time, unpredictable changes that ultimately leads to sub-optimal performance in terms of adaptiveness and real-time decision making. In this work, we propose a novel motion planner, AgilePilot, based on Deep Reinforcement Learning (DRL) that is trained in dynamic conditions, coupled with real-time Computer Vision (CV) for object detections during flight. The training-to-deployment framework bridges the Sim2Real gap, leveraging sophisticated reward structures that promotes both safety and agility depending upon environment conditions. The system can rapidly adapt to changing environments, while achieving a maximum speed of 3.0 m&#x2F;s in real-world scenarios. In comparison, our approach outperforms classical algorithms such as Artificial Potential Field (APF) based motion planner by 3 times, both in performance and tracking accuracy of dynamic targets by using velocity predictions while exhibiting 90% success rate in 75 conducted experiments. This work highlights the effectiveness of DRL in tackling real-time dynamic navigation challenges, offering intelligent safety and agility. </p>
<blockquote>
<p>åŠ¨æ€ç¯å¢ƒä¸‹è‡ªä¸»æ— äººæœºçš„å¯¼èˆªä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¿«é€Ÿç§»åŠ¨çš„ç‰©ä½“å’Œå¿«é€Ÿå˜åŒ–çš„ç›®æ ‡ä½ç½®ç­‰ä¸å¯é¢„æµ‹çš„åœºæ™¯æ—¶ã€‚è™½ç„¶ä¼ ç»Ÿè§„åˆ’å™¨å’Œç»å…¸ä¼˜åŒ–æ–¹æ³•å·²è¢«å¹¿æ³›ç”¨äºè§£å†³è¿™ä¸€åŠ¨æ€é—®é¢˜ï¼Œä½†å®ƒä»¬ç»å¸¸é¢ä¸´å®æ—¶ã€ä¸å¯é¢„æµ‹çš„å˜æ›´ï¼Œè¿™æœ€ç»ˆå¯¼è‡´åœ¨é€‚åº”æ€§å’Œå®æ—¶å†³ç­–æ–¹é¢çš„æ€§èƒ½è¡¨ç°ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„è¿åŠ¨è§„åˆ’å™¨AgilePilotï¼Œå®ƒåœ¨åŠ¨æ€æ¡ä»¶ä¸‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç»“åˆå®æ—¶è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰è¿›è¡Œé£è¡Œè¿‡ç¨‹ä¸­çš„ç›®æ ‡æ£€æµ‹ã€‚ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„æ¡†æ¶ï¼Œå®ƒç¼©å°äº†æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·ï¼Œåˆ©ç”¨å¤æ‚çš„å¥–åŠ±ç»“æ„ï¼Œæ ¹æ®ç¯å¢ƒæ¡ä»¶ä¿ƒè¿›å®‰å…¨æ€§å’ŒæœºåŠ¨æ€§çš„å¹³è¡¡ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿…é€Ÿé€‚åº”å˜åŒ–çš„ç¯å¢ƒï¼ŒåŒæ—¶åœ¨çœŸå®åœºæ™¯ä¸­å®ç°æœ€é«˜æ—¶é€Ÿ3.0ç±³&#x2F;ç§’ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨é€Ÿåº¦é¢„æµ‹ï¼Œåœ¨æ€§èƒ½å’ŒåŠ¨æ€ç›®æ ‡çš„è·Ÿè¸ªç²¾åº¦æ–¹é¢ï¼Œæ˜¯åŸºäºäººå·¥åŠ¿èƒ½åœºï¼ˆAPFï¼‰çš„è¿åŠ¨è§„åˆ’å™¨çš„ä¸‰å€ä¹‹å¤šï¼Œå¹¶ä¸”åœ¨è¿›è¡Œçš„75æ¬¡å®éªŒä¸­æˆåŠŸç‡è¾¾åˆ°äº†90%ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†å®æ—¶åŠ¨æ€å¯¼èˆªæŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæä¾›äº†æ™ºèƒ½çš„å®‰å…¨æ€§å’ŒæœºåŠ¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06725v1">PDF</a> Manuscript has been submitted to 2025 INTERNATIONAL CONFERENCE ON   UNMANNED AIRCRAFT SYSTEMS (ICUAS)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ— äººæœºåŠ¨æ€å¯¼èˆªæŠ€æœ¯ã€‚ä¼ ç»Ÿè§„åˆ’æ–¹æ³•éš¾ä»¥é€‚åº”åŠ¨æ€ç¯å¢ƒä¸­çš„å˜åŒ–ï¼Œå¯¼è‡´å®æ—¶å†³ç­–æ•ˆæœå·®ã€‚è¯¥ç ”ç©¶æå‡ºä¸€ç§æ–°å‹è¿åŠ¨è§„åˆ’å™¨AgilePilotï¼Œç»“åˆå®æ—¶è®¡ç®—æœºè§†è§‰è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œèƒ½åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¿«é€Ÿé€‚åº”å¹¶åšå‡ºå†³ç­–ã€‚é€šè¿‡å¤æ‚çš„å¥–åŠ±ç»“æ„ä¿ƒè¿›å®‰å…¨æ€§å’Œçµæ´»æ€§ï¼Œç¼©å°ä»¿çœŸä¸ç°å®å·®è·ã€‚ç³»ç»Ÿèƒ½åœ¨çœŸå®åœºæ™¯ä¸­è¾¾åˆ°æœ€é«˜æ—¶é€Ÿ3ç±³æ¯ç§’ï¼Œè¶…è¶Šä¼ ç»Ÿç®—æ³•ä¸‰å€ï¼Œä¸”æˆåŠŸç‡ä¸ºå®éªŒæ•°çš„ç™¾åˆ†ä¹‹ä¹åã€‚è¿™ä¸€ç ”ç©¶å±•ç¤ºäº†DRLè§£å†³å®æ—¶åŠ¨æ€å¯¼èˆªæŒ‘æˆ˜çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–°å‹æ— äººæœºè¿åŠ¨è§„åˆ’å™¨AgilePilotï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªæŒ‘æˆ˜ã€‚</li>
<li>AgilePilotç»“åˆå®æ—¶è®¡ç®—æœºè§†è§‰æŠ€æœ¯ç”¨äºç›®æ ‡æ£€æµ‹ï¼Œä»¥æé«˜æ— äººæœºçš„å†³ç­–æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å¤æ‚çš„å¥–åŠ±ç»“æ„ä¿ƒè¿›å®‰å…¨æ€§å’Œçµæ´»æ€§çš„å¹³è¡¡ï¼Œç¼©å°ä»¿çœŸä¸çœŸå®ç¯å¢ƒä¹‹é—´çš„å·®è·ã€‚</li>
<li>è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨çœŸå®åœºæ™¯ä¸­å®ç°é«˜è¾¾æ¯ç§’3ç±³çš„é€Ÿåº¦è¿è¡Œï¼Œè¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿç®—æ³•ç›¸æ¯”ï¼Œå¦‚äººå·¥åŠ¿èƒ½åœºï¼ˆAPFï¼‰ç®—æ³•ï¼ŒAgilePilotåœ¨æ€§èƒ½å’Œè·Ÿè¸ªåŠ¨æ€ç›®æ ‡æ–¹é¢è¡¨ç°æ›´ä¼˜è¶Šã€‚</li>
<li>åœ¨è¿›è¡Œçš„å®éªŒä¸­ï¼ŒAgilePilotè¾¾åˆ°äº†ç™¾åˆ†ä¹‹ä¹åçš„æˆåŠŸç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5203557f2f802db871bb555f174f2cdf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34a0258d58b9cc9beae586bd912ec911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4229765902734e79a830957ff029ec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f56000f24e5752794f0c63464688f935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fedce9e9021f53b778dae36912324a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-528a22b1af0603a79d060b74940f6cd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41007af9c21009a4437293e94997be73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20a1fdce532035302222503855f2c89a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hephaestus-Improving-Fundamental-Agent-Capabilities-of-Large-Language-Models-through-Continual-Pre-Training"><a href="#Hephaestus-Improving-Fundamental-Agent-Capabilities-of-Large-Language-Models-through-Continual-Pre-Training" class="headerlink" title="Hephaestus: Improving Fundamental Agent Capabilities of Large Language   Models through Continual Pre-Training"></a>Hephaestus: Improving Fundamental Agent Capabilities of Large Language   Models through Continual Pre-Training</h2><p><strong>Authors:Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang</strong></p>
<p>Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments. </p>
<blockquote>
<p>ç”±äºé¢å‘ä»£ç†çš„é¢„è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼ŒåŸºäºLLMçš„è‡ªä¸»ä»£ç†é€šå¸¸ä¾èµ–äºå¤æ‚çš„æç¤ºæˆ–å¤§é‡çš„å¾®è°ƒï¼Œè¿™å¾€å¾€éš¾ä»¥åœ¨ä¿æŒå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å¼•å…¥æ–°çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†èµ«æ·®æ–¯æ‰˜æ–¯é”»é€ ï¼ˆHephaestus-Forgeï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­æ–™åº“ï¼Œæ—¨åœ¨å¢å¼ºLLMä»£ç†åœ¨APIå‡½æ•°è°ƒç”¨ã€å†…åœ¨æ¨ç†å’Œè§„åˆ’ä»¥åŠé€‚åº”ç¯å¢ƒåé¦ˆæ–¹é¢çš„åŸºæœ¬èƒ½åŠ›ã€‚èµ«æ·®æ–¯æ‰˜æ–¯é”»é€ åŒ…å«äº†103Bçš„ä»£ç†ç‰¹å®šæ•°æ®ï¼Œæ¶µç›–76,537ä¸ªAPIï¼Œå…¶ä¸­åŒ…æ‹¬å·¥å…·æ–‡æ¡£ä»¥ä»‹ç»APIå‡½æ•°çš„çŸ¥è¯†å’Œå‡½æ•°è°ƒç”¨è½¨è¿¹ä»¥åŠ å¼ºå†…åœ¨æ¨ç†ã€‚ä¸ºäº†æ¢ç´¢æœ‰æ•ˆçš„è®­ç»ƒåè®®ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ‰©å±•å®šå¾‹ï¼Œä»¥ç¡®å®šæ•°æ®æ··åˆæ¯”ä¾‹ä¸­çš„æœ€ä½³é…æ–¹ã€‚é€šè¿‡ä¸æ–­åœ¨èµ«æ·®æ–¯æ‰˜æ–¯é”»é€ ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œèµ«æ·®æ–¯æ‰˜æ–¯åœ¨ä¸‰ä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¸­å°è§„æ¨¡çš„å¼€æºLLMï¼Œå¹¶ä¸å•†ä¸šLLMç›¸åŒ¹æ•Œï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„é¢„è®­ç»ƒè¯­æ–™åº“åœ¨å¢å¼ºLLMçš„åŸºæœ¬ä»£ç†èƒ½åŠ›å’Œæ³›åŒ–æ–°ä»»åŠ¡æˆ–ç¯å¢ƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06589v1">PDF</a> Accepted to NAACL 2025 main conference</p>
<p><strong>æ€»ç»“</strong></p>
<p>é¢å‘è‡ªä¸»ä»£ç†äººçš„é¢„è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»ä»£ç†äººé€šå¸¸ä¾èµ–å¤æ‚çš„æç¤ºæˆ–å¤§é‡çš„å¾®è°ƒï¼Œè¿™å¾€å¾€éš¾ä»¥åœ¨å¼•å…¥æ–°èƒ½åŠ›çš„åŒæ—¶ä¿æŒå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Hephaestus-Forgeï¼Œè¿™æ˜¯é¦–ä¸ªä¸ºå¢å¼ºLLMä»£ç†äººåœ¨APIå‡½æ•°è°ƒç”¨ã€å†…åœ¨æ¨ç†å’Œè§„åˆ’ä»¥åŠé€‚åº”ç¯å¢ƒåé¦ˆæ–¹é¢çš„åŸºæœ¬èƒ½åŠ›è€Œè®¾è®¡çš„å¤§å‹é¢„è®­ç»ƒè¯­æ–™åº“ã€‚Hephaestus-ForgeåŒ…å«103Bçš„ä»£ç†ç‰¹å®šæ•°æ®ï¼Œæ¶µç›–76,537ä¸ªAPIï¼Œæ—¢åŒ…æ‹¬å·¥å…·æ–‡æ¡£ä»¥ä»‹ç»APIå‡½æ•°çŸ¥è¯†ï¼Œä¹ŸåŒ…æ‹¬å‡½æ•°è°ƒç”¨è½¨è¿¹ä»¥åŠ å¼ºå†…åœ¨æ¨ç†ã€‚ä¸ºäº†æ¢ç´¢æœ‰æ•ˆçš„è®­ç»ƒåè®®ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§„æ¨¡å®šå¾‹ä»¥ç¡®å®šæ•°æ®æ··åˆæ¯”ä¾‹çš„æœ€ä½³é…æ–¹ã€‚é€šè¿‡ä¸æ–­åœ¨Hephaestus-Forgeä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒHephaestusåœ¨ä¸‰ä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¸­å°è§„æ¨¡çš„å¼€æºLLMï¼Œå¹¶ä¸å•†ä¸šLLMç›¸æŠ—è¡¡ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„é¢„è®­ç»ƒè¯­æ–™åº“åœ¨æé«˜LLMçš„åŸºæœ¬ä»£ç†èƒ½åŠ›å’Œæ³›åŒ–æ–°ä»»åŠ¡æˆ–ç¯å¢ƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢å‘è‡ªä¸»ä»£ç†äººçš„é¢„è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼Œå¯¼è‡´LLMä»£ç†äººé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Hephaestus-Forgeæ˜¯é¦–ä¸ªé’ˆå¯¹LLMä»£ç†äººçš„å¤§å‹é¢„è®­ç»ƒè¯­æ–™åº“ï¼Œæ—¨åœ¨å¢å¼ºAPIå‡½æ•°è°ƒç”¨ã€å†…åœ¨æ¨ç†å’Œè§„åˆ’ç­‰èƒ½åŠ›ã€‚</li>
<li>Hephaestus-ForgeåŒ…å«ä¸°å¯Œçš„æ•°æ®èµ„æºï¼Œæ¶µç›–APIçŸ¥è¯†åŠå‡½æ•°è°ƒç”¨è½¨è¿¹ï¼Œä»¥å¼ºåŒ–å†…åœ¨æ¨ç†ã€‚</li>
<li>é€šè¿‡ç ”ç©¶è§„æ¨¡å®šå¾‹ï¼Œç¡®å®šäº†æœ€ä½³çš„æ•°æ®æ··åˆæ¯”ä¾‹ã€‚</li>
<li>é€šè¿‡åœ¨Hephaestus-Forgeä¸ŠæŒç»­é¢„è®­ç»ƒï¼ŒHephaestusåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Hephaestusè¶…è¶Šäº†ä¸­å°è§„æ¨¡çš„å¼€æºLLMï¼Œå¹¶ä¸å•†ä¸šLLMç›¸æŠ—è¡¡ã€‚</li>
<li>é¢„è®­ç»ƒè¯­æ–™åº“å¯¹æé«˜LLMçš„åŸºæœ¬ä»£ç†èƒ½åŠ›å’Œæ³›åŒ–æ–°ä»»åŠ¡æˆ–ç¯å¢ƒå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-225598c3cd1ca7339fa492ab477be7d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c935ac7e8a14a6d0d82ba427e0c74da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-390e6044c4e6dc4ce6645a2b9e5b2bd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fab33a44737cc22e0ed7d03686b5adaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f344dbfe743b138da9f7d19d0d6d635.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SIGMA-Sheaf-Informed-Geometric-Multi-Agent-Pathfinding"><a href="#SIGMA-Sheaf-Informed-Geometric-Multi-Agent-Pathfinding" class="headerlink" title="SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding"></a>SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding</h2><p><strong>Authors:Shuhao Liao, Weihang Xia, Yuhong Cao, Weiheng Dai, Chengyang He, Wenjun Wu, Guillaume Sartoretti</strong></p>
<p>The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“è·¯å¾„å¯»æ‰¾ï¼ˆMAPFï¼‰é—®é¢˜æ—¨åœ¨ç¡®å®šåœ¨å·²çŸ¥ä¸”å¯èƒ½å……æ»¡éšœç¢çš„ç¯å¢ƒä¸­ï¼Œå¤šä¸ªæ™ºèƒ½ä½“çš„æ— ç¢°æ’æœ€çŸ­è·¯å¾„ã€‚å®ƒæ˜¯å¤§è§„æ¨¡ç‰©æµå’Œè¿è¾“ä¸­æœºå™¨äººéƒ¨ç½²çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å»ä¸­å¿ƒåŒ–çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨è§£å†³MAPFé—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œæä¾›äº†æ›´çµæ´»å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„MAPFæ–¹æ³•é€šå¸¸ä¾èµ–äºæ™ºèƒ½ä½“åŸºäºæœ‰é™çš„è§†é‡ï¼ˆFOVï¼‰åšå‡ºå†³ç­–ï¼Œå¯¼è‡´çŸ­è§†ç­–ç•¥å’Œå¤æ‚åœºæ™¯ä¸­çš„åˆä½œæ•ˆç‡ä½ä¸‹ã€‚å› æ­¤ï¼Œä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯åœ¨æœ‰é™çš„è§‚å¯Ÿå’Œé€šä¿¡åŸºç¡€ä¸Šå®ç°æ™ºèƒ½ä½“ä¹‹é—´æ½œåœ¨è¿åŠ¨çš„å…±è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å±‚å ç†è®ºåº”ç”¨äºå»ä¸­å¿ƒåŒ–æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡å±€éƒ¨å…±è¯†å­¦ä¹ å½¼æ­¤ä¹‹é—´çš„å‡ ä½•äº¤å‰ä¾èµ–æ€§ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬è¿›è¡Œç´§å¯†åˆä½œå†³ç­–ã€‚ç‰¹åˆ«æ˜¯ï¼Œå±‚å ç†è®ºæä¾›äº†é€šè¿‡å±€éƒ¨è§‚å¯Ÿå®ç°å…¨å±€å…±è¯†çš„æ¡ä»¶æ•°å­¦è¯æ˜ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬ç»“åˆç¥ç»ç½‘ç»œï¼ŒåŸºäºå±‚å ç†è®ºåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿‘ä¼¼å»ºæ¨¡å…±è¯†ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚åœ¨ä»»åŠ¡ä¸­ï¼Œé™¤äº†MAPFçš„æ­£å¸¸ç‰¹å¾å¤–ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è¿˜åˆ†å¸ƒå¼åœ°è€ƒè™‘å­¦ä¹ åˆ°çš„å…±è¯†ç‰¹å¾ï¼Œä»è€Œåœ¨è·¯å¾„æŸ¥æ‰¾å’Œé¿å…ç¢°æ’æ–¹é¢å®ç°é«˜æ•ˆåˆä½œã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨ç›¸å¯¹è¾ƒå¤§å’Œå¤æ‚çš„åœºæ™¯ä¸­ï¼Œç›¸è¾ƒäºæœ€æ–°çš„åŸºäºå­¦ä¹ çš„MAPFè§„åˆ’å™¨æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œåœ¨å„ç§æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­è¶…è¿‡åŸºçº¿æ ‡å‡†çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06440v1">PDF</a> Accepted for presentation at the 2025 IEEE International Conference   on Robotics and Automation (ICRA)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’é—®é¢˜ï¼ˆMAPFï¼‰ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºæµå½¢ç†è®ºçš„æ–°æ¡†æ¶æ¥è§£å†³åˆ†æ•£å¼æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„å±€éƒ¨å…±è¯†é—®é¢˜ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡å±€éƒ¨è§‚å¯Ÿå­¦ä¹ å‡ ä½•äº¤å‰ä¾èµ–å…³ç³»å¹¶ç´§å¯†åˆä½œè¿›è¡Œå†³ç­–ã€‚æ­¤æ¡†æ¶æä¾›äº†é€šè¿‡å±€éƒ¨è§‚å¯Ÿå®ç°å…¨å±€å…±è¯†çš„æ•°å­¦è¯æ˜æ¡ä»¶ã€‚é€šè¿‡ç¥ç»ç½‘ç»œå¯¹åŸºäºæµå½¢ç†è®ºçš„å…±è¯†è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­åŒæ—¶è€ƒè™‘ä¼ ç»ŸMAPFç‰¹å¾å’Œå­¦ä¹ çš„å…±è¯†ç‰¹å¾ï¼Œæœ‰æ•ˆæå‡äº†è·¯å¾„è§„åˆ’å’Œé¿å…ç¢°æ’çš„æ•ˆç‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­ï¼Œæ–°æ–¹æ³•å‡æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MAPFé—®é¢˜çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºç¡®å®šå¤šä¸ªæ™ºèƒ½ä½“åœ¨å·²çŸ¥ç¯å¢ƒä¸­æ— ç¢°æ’çš„æœ€çŸ­è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ç‰©æµå’Œè¿è¾“é¢†åŸŸã€‚</li>
<li>åˆ†æ•£å¼å­¦ä¹ æ–¹æ³•æ˜¯è§£å†³MAPFé—®é¢˜çš„æ½œåŠ›æ‰€åœ¨ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸åŸºäºæœ‰é™è§†é‡ï¼ˆFOVï¼‰è¿›è¡Œå†³ç­–ï¼Œå¯¼è‡´æ”¿ç­–çŸ­è§†å’Œå¤æ‚åœºæ™¯ä¸‹åˆä½œä½æ•ˆã€‚</li>
<li>æµå½¢ç†è®ºåº”ç”¨äºåˆ†æ•£å¼æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½é€šè¿‡å±€éƒ¨å…±è¯†å­¦ä¹ å‡ ä½•äº¤å‰ä¾èµ–å…³ç³»ã€‚</li>
<li>æµå½¢ç†è®ºæä¾›äº†é€šè¿‡å±€éƒ¨è§‚å¯Ÿå®ç°å…¨å±€å…±è¯†çš„æ•°å­¦è¯æ˜æ¡ä»¶ã€‚</li>
<li>é€šè¿‡ç¥ç»ç½‘ç»œå»ºæ¨¡åŸºäºæµå½¢ç†è®ºçš„å…±è¯†ï¼Œå¹¶é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­åŒæ—¶è€ƒè™‘ä¼ ç»ŸMAPFç‰¹å¾å’Œå­¦ä¹ çš„å…±è¯†ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†è·¯å¾„è§„åˆ’å’Œç¢°æ’é¿å…çš„æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f929ade5f8189e81ffd3e0c5ac18b14d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6e753766252e3d14f78023915168b80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91b677165798ecbbd54a23f2b0d179a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc69c10526de899282c607711e5a4742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3845dc53fea3f922fce06e115299c67c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-037c1ee9e7d7da49f95f1803582d6455.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2a037de8223539a3c9720f4306a63c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2df26328b98cfe693d03db040ab76cb1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Bio-inspired-Heuristically-Accelerated-Reinforcement-Learning-for-Adaptive-Underwater-Multi-Agents-Behaviour"><a href="#Towards-Bio-inspired-Heuristically-Accelerated-Reinforcement-Learning-for-Adaptive-Underwater-Multi-Agents-Behaviour" class="headerlink" title="Towards Bio-inspired Heuristically Accelerated Reinforcement Learning   for Adaptive Underwater Multi-Agents Behaviour"></a>Towards Bio-inspired Heuristically Accelerated Reinforcement Learning   for Adaptive Underwater Multi-Agents Behaviour</h2><p><strong>Authors:Antoine Vivien, Thomas Chaffre, Matthew Stephenson, Eva Artusi, Paulo Santos, Benoit Clement, Karl Sammut</strong></p>
<p>This paper describes the problem of coordination of an autonomous Multi-Agent System which aims to solve the coverage planning problem in a complex environment. The considered applications are the detection and identification of objects of interest while covering an area. These tasks, which are highly relevant for space applications, are also of interest among various domains including the underwater context, which is the focus of this study. In this context, coverage planning is traditionally modelled as a Markov Decision Process where a coordinated MAS, a swarm of heterogeneous autonomous underwater vehicles, is required to survey an area and search for objects. This MDP is associated with several challenges: environment uncertainties, communication constraints, and an ensemble of hazards, including time-varying and unpredictable changes in the underwater environment. MARL algorithms can solve highly non-linear problems using deep neural networks and display great scalability against an increased number of agents. Nevertheless, most of the current results in the underwater domain are limited to simulation due to the high learning time of MARL algorithms. For this reason, a novel strategy is introduced to accelerate this convergence rate by incorporating biologically inspired heuristics to guide the policy during training. The PSO method, which is inspired by the behaviour of a group of animals, is selected as a heuristic. It allows the policy to explore the highest quality regions of the action and state spaces, from the beginning of the training, optimizing the exploration&#x2F;exploitation trade-off. The resulting agent requires fewer interactions to reach optimal performance. The method is applied to the MSAC algorithm and evaluated for a 2D covering area mission in a continuous control environment. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤æ‚ç¯å¢ƒä¸­è¦†ç›–è§„åˆ’é—®é¢˜çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemï¼Œç®€ç§°MASï¼‰çš„åè°ƒé—®é¢˜ã€‚æ‰€è€ƒè™‘çš„åº”ç”¨æ˜¯è¦†ç›–åŒºåŸŸæ—¶çš„ç›®æ ‡æ£€æµ‹å’Œè¯†åˆ«ã€‚è¿™äº›ä»»åŠ¡å¯¹äºç©ºé—´åº”ç”¨é«˜åº¦ç›¸å…³ï¼Œä¹Ÿåœ¨å„ç§é¢†åŸŸï¼ˆåŒ…æ‹¬æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨çš„æ°´ä¸‹ç¯å¢ƒï¼‰ä¸­å¼•èµ·äº†å…´è¶£ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¦†ç›–è§„åˆ’ä¼ ç»Ÿä¸Šè¢«å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Processï¼Œç®€ç§°MDPï¼‰ï¼Œéœ€è¦åè°ƒçš„MASï¼ˆç”±å¤šç§è‡ªä¸»æ°´ä¸‹è½¦è¾†ç»„æˆçš„ç¾¤ä½“ï¼‰æ¥è°ƒæŸ¥åŒºåŸŸå¹¶æœç´¢ç›®æ ‡ã€‚è¿™ä¸ªMDPé¢ä¸´å‡ ä¸ªæŒ‘æˆ˜ï¼šç¯å¢ƒä¸ç¡®å®šæ€§ã€é€šä¿¡çº¦æŸä»¥åŠä¸€ç³»åˆ—å±é™©ï¼ŒåŒ…æ‹¬æ°´ä¸‹ç¯å¢ƒä¸­éšæ—¶é—´å˜åŒ–ä¸”ä¸å¯é¢„æµ‹çš„å˜åŒ–ã€‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-Agent Reinforcement Learningï¼Œç®€ç§°MARLï¼‰ç®—æ³•å¯ä»¥è§£å†³é«˜åº¦éçº¿æ€§é—®é¢˜ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¡¨ç°å‡ºå¾ˆå¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯¹æŠ—å¤§é‡æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç”±äºMARLç®—æ³•çš„å­¦ä¹ æ—¶é—´è¾ƒé•¿ï¼Œç›®å‰åœ¨æ°´ä¸‹é¢†åŸŸçš„å¤§éƒ¨åˆ†ç»“æœä»…é™äºæ¨¡æ‹Ÿã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€ç§æ–°ç­–ç•¥æ¥é€šè¿‡èå…¥ç”Ÿç‰©å¯å‘å¼ç®—æ³•åŠ é€Ÿæ”¶æ•›é€Ÿç‡ä»¥å¼•å¯¼è®­ç»ƒæœŸé—´çš„ç­–ç•¥ã€‚ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆParticle Swarm Optimizationï¼Œç®€ç§°PSOï¼‰æ–¹æ³•å—åˆ°åŠ¨ç‰©ç¾¤ä½“è¡Œä¸ºçš„å¯å‘ï¼Œè¢«é€‰æ‹©ä¸ºä¸€ç§å¯å‘å¼æ–¹æ³•ã€‚å®ƒå…è®¸ç­–ç•¥ä»è®­ç»ƒå¼€å§‹å°±æ¢ç´¢è¡Œä¸ºå’ŒçŠ¶æ€ç©ºé—´ä¸­è´¨é‡æœ€é«˜çš„åŒºåŸŸï¼Œä¼˜åŒ–æ¢ç´¢ä¸å¼€å‘çš„æƒè¡¡ã€‚æœ€ç»ˆæ™ºèƒ½ä½“è¾¾åˆ°æœ€ä¼˜æ€§èƒ½æ‰€éœ€çš„äº¤äº’æ¬¡æ•°å‡å°‘ã€‚è¯¥æ–¹æ³•åº”ç”¨äºMSACç®—æ³•ï¼Œå¹¶åœ¨è¿ç»­æ§åˆ¶ç¯å¢ƒä¸­é’ˆå¯¹äºŒç»´è¦†ç›–åŒºåŸŸä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06113v1">PDF</a> i-SAIRAS 2024 Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤æ‚ç¯å¢ƒä¸‹ï¼Œè‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„åè°ƒé—®é¢˜ï¼Œæ—¨åœ¨è§£å†³è¦†ç›–è§„åˆ’é—®é¢˜ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†æ°´ä¸‹ç¯å¢ƒä¸­çš„è¦†ç›–è§„åˆ’ï¼Œå°†å…¶ä¼ ç»Ÿåœ°å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚ç”±äºç¯å¢ƒä¸ç¡®å®šæ€§å’Œé€šä¿¡çº¦æŸç­‰æŒ‘æˆ˜ï¼Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç®—æ³•ç»“åˆæ·±åº¦ç¥ç»ç½‘ç»œè§£å†³äº†é«˜åº¦éçº¿æ€§é—®é¢˜ã€‚ä¸ºæé«˜æ”¶æ•›é€Ÿç‡ï¼Œæ–‡ç« æå‡ºä¸€ç§æ–°å‹ç­–ç•¥ï¼Œç»“åˆç”Ÿç‰©å¯å‘å¼æ–¹æ³•æ¥å¼•å¯¼è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥ã€‚æœ€åé€šè¿‡å®éªŒéªŒè¯ï¼Œåº”ç”¨ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰æ–¹æ³•çš„MSACç®—æ³•åœ¨è¿ç»­æ§åˆ¶ç¯å¢ƒä¸‹çš„äºŒç»´è¦†ç›–åŒºåŸŸä»»åŠ¡è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è§£å†³è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¦†ç›–è§„åˆ’é—®é¢˜ã€‚</li>
<li>æ°´ä¸‹ç¯å¢ƒæ˜¯è¯¥ç ”ç©¶çš„é‡ç‚¹åº”ç”¨é¢†åŸŸï¼Œæ¶‰åŠé«˜åº¦ç›¸å…³çš„ä»»åŠ¡ï¼Œå¦‚æ£€æµ‹å’Œè¯†åˆ«æ„Ÿå…´è¶£çš„å¯¹è±¡ã€‚</li>
<li>å°†è¦†ç›–è§„åˆ’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œé¢ä¸´ç¯å¢ƒä¸ç¡®å®šæ€§ã€é€šä¿¡çº¦æŸå’Œå¤šç§å±é™©ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç®—æ³•è§£å†³é«˜åº¦éçº¿æ€§é—®é¢˜ï¼Œå¹¶ç»“åˆæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚</li>
<li>ä¸ºæé«˜æ”¶æ•›é€Ÿç‡ï¼Œç»“åˆç”Ÿç‰©å¯å‘å¼æ–¹æ³•å¼•å¯¼è®­ç»ƒç­–ç•¥ã€‚</li>
<li>é‡‡ç”¨ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰æ–¹æ³•ä½œä¸ºå¯å‘å¼æ–¹æ³•ï¼Œä¼˜åŒ–æ¢ç´¢ä¸å¼€å‘ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31674a6f460a3162e2bca935e290bfe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ff72cf964b259c6a7e87425f9c7c7d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Training-Language-Models-for-Social-Deduction-with-Multi-Agent-Reinforcement-Learning"><a href="#Training-Language-Models-for-Social-Deduction-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Training Language Models for Social Deduction with Multi-Agent   Reinforcement Learning"></a>Training Language Models for Social Deduction with Multi-Agent   Reinforcement Learning</h2><p><strong>Authors:Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh</strong></p>
<p>Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agentâ€™s goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a modelâ€™s listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a modelâ€™s speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at <a target="_blank" rel="noopener" href="https://socialdeductionllm.github.io/">https://socialdeductionllm.github.io/</a> </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€äº¤æµåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ²Ÿé€šæ˜¯ä¸€é¡¹å¼ºå¤§çš„å·¥å…·ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­ä½¿ç‹¬ç«‹æ™ºèƒ½ä½“å…±äº«ä¿¡æ¯ï¼Œå¹¶ä¸äººç±»å®ç°é›¶å°„å‡»ååŒå·¥ä½œã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œéƒ½æœ‰å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬è¦ä¹ˆä¾èµ–äºå¤§é‡çš„äººç±»æ¼”ç¤ºè¿›è¡Œè®­ç»ƒï¼Œè¦ä¹ˆç¼ºä¹ç”Ÿæˆè‡ªç„¶å’Œæœ‰ç”¨çš„æ²Ÿé€šç­–ç•¥çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œæœ‰å…³å…¶ç¯å¢ƒçš„æœ‰è§åœ°çš„è®¨è®ºï¼Œè€Œæ— éœ€ä»»ä½•äººç±»æ¼”ç¤ºã€‚æˆ‘ä»¬å°†æ²Ÿé€šé—®é¢˜åˆ†è§£ä¸ºå¬å’Œè¯´ã€‚æˆ‘ä»¬çš„å…³é”®æƒ³æ³•æ˜¯åˆ©ç”¨æ™ºèƒ½ä½“çš„ç›®æ ‡æ¥é¢„æµ‹æœ‰å…³ä¸–ç•Œçš„æœ‰ç”¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä½œä¸ºå¯†é›†å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ²Ÿé€šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒæ¨¡å‹æ ¹æ®è®¨è®ºæ¥é¢„æµ‹æœ‰å…³ç¯å¢ƒçš„ä¿¡æ¯æ¥æé«˜å…¶å¬åŠ›æŠ€èƒ½ï¼ŒåŒæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºä¿¡æ¯å¯¹å…¶ä»–æ™ºèƒ½ä½“çš„å½±å“æ¥å¥–åŠ±ä¿¡æ¯æ¥æé«˜æ¨¡å‹çš„è¯´è¯æŠ€èƒ½ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼ã€‚ä¸ºäº†ç ”ç©¶æ²Ÿé€šåœ¨å¤æ‚ç¤¾ä¼šç¯å¢ƒä¸­çš„ä½œç”¨å’Œå¿…è¦æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºã€ŠAmong Usã€‹çš„å…·èº«ç¤¾ä¼šæ¨ç†æ¸¸æˆï¼Œå…¶ä¸­çš„å…³é”®é—®é¢˜æ˜¯ç¡®å®šå¯¹æŠ—æ€§çš„éª—å­èº«ä»½ã€‚æˆ‘ä»¬åˆ†æäº†ç”±äºæˆ‘ä»¬çš„æŠ€æœ¯è€Œäº§ç”Ÿçš„çªå‘è¡Œä¸ºï¼Œä¾‹å¦‚æŒ‡è´£å«Œç–‘äººå’Œæä¾›è¯æ®ï¼Œå¹¶å‘ç°å®ƒèƒ½å¤Ÿä¿ƒè¿›æ¿€çƒˆçš„è®¨è®ºï¼Œä¸æ ‡å‡†å¼ºåŒ–å­¦ä¹ ç›¸æ¯”ï¼Œå°†èƒœç‡æé«˜äº†ä¸€å€ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://socialdeductionllm.github.io/">https://socialdeductionllm.github.io/</a>å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06060v1">PDF</a> 14 pages, 5 figures, 24th International Conference on Autonomous   Agents and Multiagent Systems (AAMAS 2025)</p>
<p><strong>Summary</strong></p>
<p>è‡ªç„¶è¯­è¨€äº¤æµåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­æ˜¯å¼ºå¤§çš„å·¥å…·ï¼Œå¯å®ç°ç‹¬ç«‹æ™ºèƒ½ä½“åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸‹çš„ä¿¡æ¯å…±äº«ä»¥åŠä¸äººç±»çš„é›¶æ‹æ‘„åè°ƒã€‚ä½†å…ˆå‰çš„ç ”ç©¶å¤šä¾èµ–äºå¤§é‡çš„äººç±»æ¼”ç¤ºæ•°æ®æˆ–ç¼ºä¹ç”Ÿæˆè‡ªç„¶ã€æœ‰æ•ˆçš„æ²Ÿé€šç­–ç•¥çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥è‡ªç„¶è¯­è¨€å½¢å¼è®¨è®ºå…¶ç¯å¢ƒï¼Œæ— éœ€ä»»ä½•äººç±»æ¼”ç¤ºã€‚é€šè¿‡åˆ†è§£æ²Ÿé€šé—®é¢˜ä¸ºå¬ä¸è¯´ä¸¤éƒ¨åˆ†ï¼Œåˆ©ç”¨æ™ºèƒ½ä½“çš„ç›®æ ‡é¢„æµ‹ä¸–ç•Œçš„æœ‰ç”¨ä¿¡æ¯ä½œä¸ºä¸°å¯Œçš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ²Ÿé€šã€‚æœ¬ç ”ç©¶é€šè¿‡ç¤¾äº¤æ¨ç†æ¸¸æˆæ¢ç©¶æ²Ÿé€šåœ¨å¤æ‚ç¤¾äº¤ç¯å¢ƒä¸­çš„è§’è‰²å’Œå¿…è¦æ€§ï¼ŒæŠ€æœ¯ä¿ƒè¿›ç´§æ€¥è¡Œä¸ºçš„å‡ºç°ï¼Œå¦‚æŒ‡æ§å«Œç–‘äººå’Œæä¾›è¯æ®ç­‰ï¼Œåˆ†æå‘ç°è¯¥æŠ€æœ¯èƒ½å¼•å‘æ¿€çƒˆè®¨è®ºï¼Œç›¸è¾ƒäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼Œèƒœç‡ç¿»å€ã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://socialdeductionllm.github.io/%E3%80%82">https://socialdeductionllm.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€äº¤æµåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­éå¸¸é‡è¦ï¼Œå¯ä»¥å®ç°ä¿¡æ¯å…±äº«å’Œä¸äººç±»åè°ƒã€‚</li>
<li>å…ˆå‰ç ”ç©¶ä¾èµ–å¤§é‡äººç±»æ¼”ç¤ºæ•°æ®æˆ–ç¼ºä¹ç”Ÿæˆè‡ªç„¶æ²Ÿé€šç­–ç•¥çš„èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥è®¨è®ºç¯å¢ƒï¼Œæ— éœ€äººç±»æ¼”ç¤ºã€‚</li>
<li>é€šè¿‡åˆ†è§£æ²Ÿé€šé—®é¢˜ä¸ºå¬ä¸è¯´ä¸¤éƒ¨åˆ†ï¼Œåˆ©ç”¨æ™ºèƒ½ä½“çš„ç›®æ ‡ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ²Ÿé€šã€‚</li>
<li>åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­æ¢ç©¶æ²Ÿé€šè§’è‰²å’Œå¿…è¦æ€§ã€‚</li>
<li>æŠ€æœ¯ä¿ƒè¿›ç´§æ€¥è¡Œä¸ºå‡ºç°ï¼Œå¦‚æŒ‡æ§å’Œæä¾›è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-391543c42adfab72f8c584b7ee896604.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9798ad7d596185c277eaeb71540672b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb51b247e151a47cd70596815c566689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58766e1a28f337ae440f8e69073bbc6b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Preventing-Rogue-Agents-Improves-Multi-Agent-Collaboration"><a href="#Preventing-Rogue-Agents-Improves-Multi-Agent-Collaboration" class="headerlink" title="Preventing Rogue Agents Improves Multi-Agent Collaboration"></a>Preventing Rogue Agents Improves Multi-Agent Collaboration</h2><p><strong>Authors:Ohav Barbi, Ori Yoran, Mor Geva</strong></p>
<p>Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat â€“ a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents $\textit{before they act}$ may prevent the systemâ€™s failure. In this work, we propose to $\textit{monitor}$ agents during action prediction and $\textit{intervene}$ when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on two variants of WhoDunitEnv and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4% and 20%, respectively. Moreover, a thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå…¶ä¸­ä¸“èŒæ™ºèƒ½ä½“åä½œå®Œæˆå…±åŒä»»åŠ¡ï¼Œä»å¢åŠ çš„æ¨¡å—åŒ–åˆ°æ¨¡æ‹Ÿå¤æ‚ç¯å¢ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹Ÿå­˜åœ¨ä¸€ä¸ªä¸»è¦çš„ç¼ºé™·ï¼Œå³å•ä¸ªæ™ºèƒ½ä½“å¯èƒ½å¯¼è‡´æ•´ä¸ªç³»ç»Ÿå¤±è´¥ã€‚è€ƒè™‘ä¸€ä¸ªç®€å•çš„æ¸¸æˆï¼Œå…¶ä¸­è§£å†³é—®é¢˜çš„çŸ¥è¯†åœ¨æ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œåˆ†å¸ƒï¼Œå®ƒä»¬åœ¨é€šä¿¡é€šé“ä¸­å…±äº«ä¿¡æ¯ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œä»»ä½•æ™ºèƒ½ä½“éƒ½å¯ä»¥ç»ˆæ­¢æ¸¸æˆå¹¶è¿›è¡Œæœ€ç»ˆé¢„æµ‹ï¼Œå³ä½¿ä»–ä»¬å¯¹è¡ŒåŠ¨çš„ç»“æœä¸ç¡®å®šã€‚åœ¨è¡ŒåŠ¨é¢„æµ‹æœŸé—´ç›‘æµ‹æ­¤ç±»æµæ°“æ™ºèƒ½ä½“ï¼Œå¹¶åœ¨å¯èƒ½å‘ç”Ÿæœªæ¥é”™è¯¯æ—¶å¹²é¢„ï¼Œå¯ä»¥é˜²æ­¢ç³»ç»Ÿå¤±è´¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåœ¨è¡ŒåŠ¨é¢„æµ‹æœŸé—´è¿›è¡Œæ™ºèƒ½ä½“ç›‘æ§ï¼Œå¹¶åœ¨å¯èƒ½å‘ç”Ÿæœªæ¥é”™è¯¯æ—¶è¿›è¡Œå¹²é¢„ã€‚ä¸ºäº†æµ‹è¯•æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†WhoDunitEnvå¤šæ™ºèƒ½ä½“åä½œç¯å¢ƒï¼Œå®ƒå…è®¸å¯¹ä»»åŠ¡å¤æ‚æ€§å’Œé€šä¿¡ç»“æ„è¿›è¡Œæ¨¡å—åŒ–æ§åˆ¶ã€‚åœ¨WhoDunitEnvçš„ä¸¤ä¸ªå˜ä½“å’ŒGovSimç¯å¢ƒè¿›è¡Œçš„èµ„æºå¯æŒç»­æ€§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜è¾¾17.4%å’Œ20%çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæ·±å…¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç›‘æ§å™¨æˆåŠŸåœ°è¯†åˆ«å‡ºäº†æ™ºèƒ½ä½“æ··æ·†çš„å…³é”®ç‚¹ï¼Œæˆ‘ä»¬çš„å¹²é¢„æªæ–½æœ‰æ•ˆåœ°é˜»æ­¢äº†æ™ºèƒ½ä½“é”™è¯¯çš„ä¼ æ’­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05986v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡ä¸“ä¸šæ™ºèƒ½ä½“åä½œå®Œæˆå…±åŒä»»åŠ¡ï¼Œå…·æœ‰æ¨¡å—åŒ–æ¨¡æ‹Ÿå¤æ‚ç¯å¢ƒç­‰ä¼˜ç‚¹ï¼Œä½†å•ä¸ªæ™ºèƒ½ä½“çš„å¤±è¯¯å¯èƒ½å¯¼è‡´æ•´ä¸ªç³»ç»Ÿå¤±æ•ˆã€‚æ–‡ç« æå‡ºäº†ç›‘æ§æ™ºèƒ½ä½“è¿›è¡Œè¡ŒåŠ¨é¢„æµ‹çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¯èƒ½å‘ç”Ÿæœªæ¥é”™è¯¯æ—¶å¹²é¢„ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹¶åœ¨è¯†åˆ«æ™ºèƒ½ä½“æ··æ·†çš„å…³é”®ç‚¹å’Œé˜»æ­¢æ™ºèƒ½ä½“é”™è¯¯ä¼ æ’­æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…·æœ‰æ¨¡å—åŒ–ä¸æ¨¡æ‹Ÿå¤æ‚ç¯å¢ƒçš„æ½œåŠ›ï¼Œä½†å•ä¸€æ™ºèƒ½ä½“çš„å¤±è´¥å¯èƒ½å¯¼è‡´æ•´ä¸ªç³»ç»Ÿå¤±æ•ˆã€‚</li>
<li>ç›‘æ§æ™ºèƒ½ä½“åœ¨è¡ŒåŠ¨é¢„æµ‹ä¸­çš„è¡¨ç°å¯é¢„é˜²ç³»ç»Ÿå¤±è´¥ã€‚</li>
<li>ä»‹ç»äº†WhoDunitEnvå¤šæ™ºèƒ½ä½“åä½œç¯å¢ƒï¼Œå…è®¸å¯¹ä»»åŠ¡å¤æ‚æ€§å’Œé€šä¿¡ç»“æ„è¿›è¡Œæ¨¡å—åŒ–æ§åˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜ç›‘æ§æ–¹æ³•å¯æœ‰æ•ˆæé«˜æ€§èƒ½ï¼Œå¹¶åœ¨è¯†åˆ«æ™ºèƒ½ä½“æ··æ·†çš„å…³é”®ç‚¹å’Œé˜»æ­¢é”™è¯¯ä¼ æ’­æ–¹é¢æœ‰æ•ˆã€‚</li>
<li>ç›‘æ§æ–¹æ³•é€šè¿‡è¯†åˆ«æœªæ¥å¯èƒ½å‡ºç°çš„é”™è¯¯æ¥å¹²é¢„æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨èµ„æºå¯æŒç»­æ€§çš„GovSimç¯å¢ƒä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3b733938d819bcf066ab91db1e412a4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c3ec422f5554a5034597485143e76c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95841c7b40c9c19054317faa19b34ef1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HamRaz-A-Culture-Based-Persian-Conversation-Dataset-for-Person-Centered-Therapy-Using-LLM-Agents"><a href="#HamRaz-A-Culture-Based-Persian-Conversation-Dataset-for-Person-Centered-Therapy-Using-LLM-Agents" class="headerlink" title="HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered   Therapy Using LLM Agents"></a>HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered   Therapy Using LLM Agents</h2><p><strong>Authors:Mohammad Amin Abbasi, Farnaz Sadat Mirnezami, Hassan Naderi</strong></p>
<p>This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†HamRazï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºä»¥äººä¸ºä¸­å¿ƒçš„æ²»ç–—ï¼ˆPCTï¼‰è€Œè®¾è®¡çš„æ–°å‹æ³¢æ–¯è¯­å¿ƒç†å¥åº·æ•°æ®é›†ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨AIé©±åŠ¨çš„å¿ƒç†å’¨è¯¢ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ç°æœ‰æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨è¥¿æ–¹å’Œä¸œäºšèƒŒæ™¯ä¸Šï¼Œå¿½è§†äº†æœ‰æ•ˆæ³¢æ–¯è¯­æ²»ç–—æ‰€éœ€çš„æ–‡åŒ–å’Œè¯­è¨€ç»†å¾®å·®åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼ŒHamRazç»“åˆäº†åŸºäºè„šæœ¬çš„å¯¹è¯å’Œè‡ªé€‚åº”LLMè§’è‰²æ‰®æ¼”ï¼Œç¡®ä¿è¿è´¯ä¸”åŠ¨æ€çš„æ²»ç–—äº’åŠ¨ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†HamRazEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒé‡è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨é€šç”¨å¯¹è¯æŒ‡æ ‡å’Œå·´é›·ç‰¹-ä¼¦çº³å¾·å…³ç³»é‡è¡¨ï¼ˆBLRIï¼‰æ¥è¡¡é‡å¯¹è¯è´¨é‡å’Œæ²»ç–—çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHamRazåœ¨å¸¸è§„è„šæœ¬æ¨¡å¼å’ŒåŒä»£ç†æ¨¡å¼æ–¹é¢è¡¨ç°æ›´ä¼˜è¶Šï¼Œäº§ç”Ÿæ›´å¯Œæœ‰åŒæƒ…å¿ƒã€æ³¨é‡ä¸Šä¸‹æ–‡å’Œç°å®çš„æ²»ç–—ä¼šè¯ã€‚é€šè¿‡å‘å¸ƒHamRazï¼Œæˆ‘ä»¬ä¸ºä¸åŒç¤¾åŒºçš„AIé©±åŠ¨çš„å¿ƒç†æ²»ç–—ç ”ç©¶æä¾›äº†æ–‡åŒ–é€‚åº”çš„å¤§å‹è¯­è¨€æ¨¡å‹èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05982v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>HamRazæ˜¯ä¸€æ¬¾åŸºäºæ³¢æ–¯è¯­çš„å¿ƒç†å¥åº·æ•°æ®é›†ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä»¥ç—…äººä¸ºä¸­å¿ƒçš„æ²»ç–—ï¼ˆPCTï¼‰ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨è¥¿æ–¹å’Œä¸œäºšæƒ…å¢ƒè€Œå¿½è§†æ³¢æ–¯è¯­æ²»ç–—çš„æ–‡åŒ–å’Œè¯­è¨€ç»†å¾®å·®åˆ«çš„é—®é¢˜ï¼ŒHamRazç»“åˆäº†è„šæœ¬å¯¹è¯å’Œè‡ªé€‚åº”LLMè§’è‰²æ‰®æ¼”æŠ€æœ¯ï¼Œç¡®ä¿è¿è´¯å’ŒåŠ¨æ€çš„ç–—æ³•äº’åŠ¨ã€‚æ­¤å¤–ï¼Œå¼•å…¥HamRazEvalåŒè¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨é€šç”¨å¯¹è¯æŒ‡æ ‡å’Œå·´é›·ç‰¹-ä¼¦çº³å¾·å…³ç³»é‡è¡¨æ¥è¯„ä¼°å¯¹è¯è´¨é‡å’Œæ²»ç–—æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHamRazä¼˜äºä¼ ç»Ÿçš„è„šæœ¬æ¨¡å¼å’ŒåŒä»£ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å…·åŒæƒ…å¿ƒã€æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†å’Œæ›´çœŸå®çš„æ²»ç–—ä¼šè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HamRazæ˜¯ä¸€ä¸ªé’ˆå¯¹æ³¢æ–¯è¯­çš„å¿ƒç†å¥åº·æ•°æ®é›†ï¼Œç”¨äºä»¥ç—…äººä¸ºä¸­å¿ƒçš„æ²»ç–—ï¼ˆPCTï¼‰ã€‚</li>
<li>HamRazç»“åˆäº†è„šæœ¬å¯¹è¯å’Œè‡ªé€‚åº”çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§’è‰²æ‰®æ¼”æŠ€æœ¯ã€‚</li>
<li>HamRazè®¾è®¡è€ƒè™‘äº†æ–‡åŒ–å’Œè¯­è¨€çš„ç»†å¾®å·®åˆ«ï¼Œé€‚ç”¨äºæ³¢æ–¯è¯­ç–—æ³•ã€‚</li>
<li>HamRazEvalæ˜¯ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¡¡é‡å¯¹è¯è´¨é‡å’Œæ²»ç–—æ•ˆæœã€‚</li>
<li>HamRazä¼˜äºä¼ ç»Ÿçš„æ²»ç–—æ¨¡å¼ï¼Œèƒ½äº§ç”Ÿæ›´çœŸå®çš„ã€å¯Œæœ‰åŒæƒ…å¿ƒçš„æ²»ç–—ä¼šè¯ã€‚</li>
<li>HamRazçš„å‘å¸ƒä¸ºå¤šå…ƒæ–‡åŒ–èƒŒæ™¯çš„AIå¿ƒç†ç–—æ³•ç ”ç©¶æä¾›äº†èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a3b1b32daa9479d111c54e956bc5e15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45cb94c6e28d5e2b79419afd9a305701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e420a770c39287ca89073abaeaf62889.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MetaChain-A-Fully-Automated-and-Zero-Code-Framework-for-LLM-Agents"><a href="#MetaChain-A-Fully-Automated-and-Zero-Code-Framework-for-LLM-Agents" class="headerlink" title="MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents"></a>MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents</h2><p><strong>Authors:Jiabin Tang, Tianyu Fan, Chao Huang</strong></p>
<p>Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChainâ€™s effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChainâ€™s Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½å†³ç­–æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œæ¨åŠ¨äº†è¯¸å¦‚LangChainå’ŒAutoGenç­‰ä»£ç†å¼€å‘æ¡†æ¶çš„å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¡†æ¶ä¸»è¦æœåŠ¡äºå…·æœ‰ä¸°å¯ŒæŠ€æœ¯ä¸“é•¿çš„å¼€å‘è€…â€”â€”è€ƒè™‘åˆ°å…¨çƒåªæœ‰0.03%çš„äººå£å…·å¤‡å¿…è¦çš„ç¼–ç¨‹æŠ€èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é™åˆ¶ã€‚è¿™ä¸€ä¸¥å³»çš„å¯è¾¾æ€§å·®è·æå‡ºäº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æœ¬èº«ï¼Œè®©æ¯ä¸ªäººæ— è®ºå…¶æŠ€æœ¯èƒŒæ™¯å¦‚ä½•ï¼Œéƒ½èƒ½æ„å»ºè‡ªå·±çš„LLMä»£ç†ï¼Ÿä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MetaChainâ€”â€”ä¸€ä¸ªå…¨è‡ªåŠ¨ä¸”é«˜åº¦è‡ªæˆ‘å‘å±•çš„æ¡†æ¶ï¼Œå®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€åˆ›å»ºå’Œéƒ¨ç½²LLMä»£ç†ã€‚ä½œä¸ºè‡ªä¸»ä»£ç†æ“ä½œç³»ç»Ÿï¼ŒMetaChainåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼ši)ä»£ç†ç³»ç»Ÿå®ç”¨ç¨‹åºï¼Œii)LLMé©±åŠ¨çš„å¯è¡Œå¼•æ“ï¼Œiii)è‡ªæˆ‘ç®¡ç†æ–‡ä»¶ç³»ç»Ÿï¼Œä»¥åŠiv)è‡ªæˆ‘ç©è€ä»£ç†è‡ªå®šä¹‰æ¨¡å—ã€‚è¿™ä¸ªè½»ä¾¿è€Œå¼ºå¤§çš„ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆã€åŠ¨æ€åœ°åˆ›å»ºå’Œä¿®æ”¹å·¥å…·ã€ä»£ç†å’Œå·¥ä½œæµç¨‹ï¼Œæ— éœ€ç¼–ç è¦æ±‚æˆ–äººå·¥å¹²é¢„ã€‚é™¤äº†æ— éœ€ç¼–ç çš„ä»£ç†å¼€å‘èƒ½åŠ›å¤–ï¼ŒMetaChainè¿˜ä½œä¸ºé€šç”¨äººå·¥æ™ºèƒ½åŠ©ç†çš„å¤šä»£ç†ç³»ç»Ÿã€‚åœ¨GAIAåŸºå‡†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒMetaChainåœ¨é€šç”¨å¤šä»£ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMetaChainçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸å…³åŠŸèƒ½åœ¨è®¸å¤šå…¶ä»–LLMè§£å†³æ–¹æ¡ˆä¸­è¡¨ç°å‡ºäº†ä¸€è‡´çš„ä¼˜åŠ¿æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05957v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/MetaChain">https://github.com/HKUDS/MetaChain</a></p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨äº†è¯¸å¦‚LangChainå’ŒAutoGenç­‰æ™ºèƒ½ä»£ç†å¼€å‘æ¡†æ¶çš„å¹¿æ³›åº”ç”¨ï¼Œä½†ä¸»è¦æœåŠ¡äºæŠ€æœ¯ä¸“å®¶ã€‚é’ˆå¯¹å…¨çƒåªæœ‰0.03%çš„äººå…·å¤‡ç¼–ç¨‹æŠ€èƒ½çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºMetaChainæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–ä¸”é«˜åº¦è‡ªæˆ‘å‘å±•çš„ç³»ç»Ÿï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å³å¯åˆ›å»ºå’Œéƒ¨ç½²LLMä»£ç†ã€‚MetaChainåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼Œå¹¶èƒ½æœ‰æ•ˆã€åŠ¨æ€åœ°åˆ›å»ºå’Œä¿®æ”¹å·¥å…·ã€ä»£ç†å’Œå·¥ä½œæµç¨‹ï¼Œæ— éœ€ç¼–ç å’Œäººå·¥å¹²é¢„ã€‚å®ƒåŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªé€šç”¨çš„äººå·¥æ™ºèƒ½åŠ©ç†çš„å¤šä»£ç†ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMä»£ç†å…·å¤‡è‡ªåŠ¨åŒ–ä»»åŠ¡å’Œæ™ºèƒ½å†³ç­–çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ¡†æ¶ä¸»è¦æœåŠ¡äºæŠ€æœ¯ä¸“å®¶ï¼Œå­˜åœ¨æ™®åŠæ€§é™åˆ¶ã€‚</li>
<li>MetaChainæ¡†æ¶æ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€è®©éæŠ€æœ¯èƒŒæ™¯çš„ç”¨æˆ·ä¹Ÿèƒ½åˆ›å»ºå’Œéƒ¨ç½²LLMä»£ç†ã€‚</li>
<li>MetaChainæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–å’Œé«˜åº¦è‡ªæˆ‘å‘å±•çš„ç³»ç»Ÿï¼ŒåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>MetaChainèƒ½æœ‰æ•ˆã€åŠ¨æ€åœ°åˆ›å»ºå’Œä¿®æ”¹å·¥å…·ã€ä»£ç†å’Œå·¥ä½œæµç¨‹ï¼Œæ— éœ€ç¼–ç å’Œäººå·¥å¹²é¢„ã€‚</li>
<li>MetaChainæ˜¯ä¸€ä¸ªé€šç”¨çš„äººå·¥æ™ºèƒ½åŠ©ç†çš„å¤šä»£ç†ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2502.05957v1/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a25c44ca4d7d3adc3d4ee5d4e6e78238.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Acquisition-through-My-Eyes-and-Steps-A-Joint-Predictive-Agent-Model-in-Egocentric-Worlds"><a href="#Acquisition-through-My-Eyes-and-Steps-A-Joint-Predictive-Agent-Model-in-Egocentric-Worlds" class="headerlink" title="Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in   Egocentric Worlds"></a>Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in   Egocentric Worlds</h2><p><strong>Authors:Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng</strong></p>
<p>This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, leading to information silos among them, which prevents these abilities from learning from each other and collaborating effectively. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions with a single transformer. EgoAgent unifies the representational spaces of the three abilities by mapping them all into a sequence of continuous tokens. Learnable query tokens are appended to obtain current states, future states, and next actions. With joint supervision, our agent model establishes the internal relationship among these three abilities and effectively mimics the human inference and learning processes. Comprehensive evaluations of EgoAgent covering image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³å­¦ä¹ ä¸€ç§åƒäººç±»ä¸€æ ·è¡Œä¸ºçš„ä»£ç†æ¨¡å‹çš„ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¸–ç•Œä¸­å…±åŒæ„ŸçŸ¥ã€é¢„æµ‹å’Œè¡ŒåŠ¨ã€‚ä»¥å‰çš„æ–¹æ³•é€šå¸¸é’ˆå¯¹è¿™ä¸‰ç§èƒ½åŠ›åˆ†åˆ«è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´å®ƒä»¬ä¹‹é—´å­˜åœ¨ä¿¡æ¯å­¤å²›ï¼Œè¿™é˜»ç¢äº†è¿™äº›èƒ½åŠ›ç›¸äº’å­¦ä¹ å’Œæœ‰æ•ˆåä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆé¢„æµ‹ä»£ç†æ¨¡å‹ï¼Œåä¸ºEgoAgentã€‚EgoAgentä½¿ç”¨å•ä¸ªå˜å‹å™¨åŒæ—¶å­¦ä¹ è¡¨ç¤ºä¸–ç•Œã€é¢„æµ‹æœªæ¥çŠ¶æ€å’Œé‡‡å–åˆç†è¡ŒåŠ¨ã€‚EgoAgenté€šè¿‡å°†æ‰€æœ‰ä¸‰ç§èƒ½åŠ›æ˜ å°„åˆ°ä¸€ç³»åˆ—è¿ç»­ä»¤ç‰Œåºåˆ—æ¥ç»Ÿä¸€å®ƒä»¬çš„è¡¨ç¤ºç©ºé—´ã€‚é€šè¿‡æ·»åŠ å¯å­¦ä¹ çš„æŸ¥è¯¢ä»¤ç‰Œæ¥è·å¾—å½“å‰çŠ¶æ€ã€æœªæ¥çŠ¶æ€å’Œä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚é€šè¿‡è”åˆç›‘ç£ï¼Œæˆ‘ä»¬çš„ä»£ç†æ¨¡å‹å»ºç«‹äº†è¿™ä¸‰ç§èƒ½åŠ›ä¹‹é—´çš„å†…éƒ¨å…³ç³»ï¼Œå¹¶æœ‰æ•ˆåœ°æ¨¡ä»¿äº†äººç±»çš„æ¨ç†å’Œå­¦ä¹ è¿‡ç¨‹ã€‚å¯¹EgoAgentåœ¨å›¾åƒåˆ†ç±»ã€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æœªæ¥çŠ¶æ€é¢„æµ‹å’Œ3Däººç±»è¿åŠ¨é¢„æµ‹ä»»åŠ¡ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬å°†å‘å¸ƒä»£ç å’Œè®­ç»ƒæ¨¡å‹ä»¥ä¾›å¤åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05857v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºEgoAgentçš„è”åˆé¢„æµ‹ä»£ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¸–ç•Œä¸­è”åˆæ„ŸçŸ¥ã€é¢„æµ‹å’Œè¡ŒåŠ¨ï¼Œè¡¨ç°å‡ºç±»ä¼¼äººç±»çš„è¡Œä¸ºã€‚é€šè¿‡é‡‡ç”¨å•ä¸€å˜å‹å™¨è¿›è¡Œå­¦ä¹ ï¼ŒEgoAgentå°†ä¸–ç•Œçš„è¡¨ç¤ºã€æœªæ¥çŠ¶æ€çš„é¢„æµ‹å’Œåˆç†è¡ŒåŠ¨çš„é‡‡å–èä¸ºä¸€ä½“ï¼Œé€šè¿‡æ˜ å°„ä¸ºä¸€ç³»åˆ—è¿ç»­çš„ä»¤ç‰Œåºåˆ—ï¼Œç»Ÿä¸€äº†ä¸‰ç§èƒ½åŠ›çš„è¡¨ç¤ºç©ºé—´ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒEgoAgentåœ¨å›¾åƒåˆ†ç±»ã€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æœªæ¥çŠ¶æ€é¢„æµ‹å’Œ3Däººç±»è¿åŠ¨é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»£ç†æ¨¡å‹EgoAgentèƒ½å¤Ÿè”åˆæ„ŸçŸ¥ã€é¢„æµ‹å’Œè¡ŒåŠ¨åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¸–ç•Œä¸­ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚</li>
<li>EgoAgenté‡‡ç”¨å•ä¸€å˜å‹å™¨è¿›è¡Œå­¦ä¹ ï¼Œå°†ä¸–ç•Œçš„è¡¨ç¤ºã€æœªæ¥çŠ¶æ€çš„é¢„æµ‹å’Œè¡ŒåŠ¨çš„é‡‡å–æ•´åˆåœ¨ä¸€èµ·ã€‚</li>
<li>é€šè¿‡å°†ä¸åŒèƒ½åŠ›æ˜ å°„ä¸ºè¿ç»­çš„ä»¤ç‰Œåºåˆ—ï¼ŒEgoAgentç»Ÿä¸€äº†ä¸‰ç§èƒ½åŠ›çš„è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>å­¦ä¹ æŸ¥è¯¢ä»¤ç‰Œç”¨äºè·å–å½“å‰çŠ¶æ€ã€æœªæ¥çŠ¶æ€å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚</li>
<li>EgoAgenté€šè¿‡è”åˆç›‘ç£å»ºç«‹è¿™ä¸‰ç§èƒ½åŠ›ä¹‹é—´çš„å†…éƒ¨å…³ç³»ï¼Œå¹¶æœ‰æ•ˆæ¨¡ä»¿äººç±»æ¨ç†å’Œå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒEgoAgentåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æœªæ¥çŠ¶æ€é¢„æµ‹å’Œ3Däººç±»è¿åŠ¨é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bc6846310dde496062491db627c65fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-022a34fcd4ad6045b067f27d34ad68fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea887b007a45744020de7ccaee44879.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7865f90962ed929abd6cb5f4bb03da9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-the-use-of-Performer-and-Agent-Attention-for-Spoken-Language-Identification"><a href="#On-the-use-of-Performer-and-Agent-Attention-for-Spoken-Language-Identification" class="headerlink" title="On the use of Performer and Agent Attention for Spoken Language   Identification"></a>On the use of Performer and Agent Attention for Spoken Language   Identification</h2><p><strong>Authors:Jitendra Kumar dhiman, Jainag Ambati</strong></p>
<p>One of the methods for language Identification (LID) involves deriving speech representation from pre-trained models using self-supervised learning, followed by fine-tuning the model for the LID task. State-of-the-art approaches for LID use an attention-based statistical pooling layer to facilitate the aggregation of contextual information across time frames of the embedding vectors extracted from the pre-trained model. In this paper, we delve into exploring recently proposed attention mechanisms, namely performer and agent-attention, in conjunction with the statistical pooling layer. The LID experiments are performed on three datasets: VoxPopuli, FLEURS, and VoxLingua. We compare their performance against vanilla self-attention. Our findings suggest that performer-attention outperforms self-attention and agent-attention exhibits comparable or occasionally superior performance to self-attention, while also being computationally less expensive. </p>
<blockquote>
<p>è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰çš„æ–¹æ³•ä¹‹ä¸€æ¶‰åŠä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æ¨å¯¼è¯­éŸ³è¡¨ç¤ºï¼Œéšåå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è¿›è¡ŒLIDä»»åŠ¡ã€‚LIDçš„æœ€å…ˆè¿›æ–¹æ³•ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç»Ÿè®¡æ± å±‚ï¼Œä»¥ä¿ƒè¿›ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–çš„åµŒå…¥å‘é‡çš„æ—¶é—´å¸§ä¸Šä¸Šä¸‹æ–‡ä¿¡æ¯çš„èšåˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†æœ€è¿‘æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå³è¡¨æ¼”è€…å’Œagent-attentionï¼Œä»¥åŠä¸ç»Ÿè®¡æ± å±‚çš„ç»“åˆã€‚LIDå®éªŒæ˜¯åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼šVoxPopuliã€FLEURSå’ŒVoxLinguaã€‚æˆ‘ä»¬å°†å®ƒä»¬çš„æ€§èƒ½ä¸åŸºæœ¬çš„è‡ªæ³¨æ„åŠ›è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¡¨æ¼”è€…æ³¨æ„åŠ›ä¼˜äºè‡ªæ³¨æ„åŠ›ï¼Œagent-attentionè¡¨ç°å‡ºä¸è‡ªæ³¨æ„åŠ›ç›¸å½“æˆ–å¶å°”æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨è®¡ç®—ä¸Šæ›´åŠ ç»æµã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05841v1">PDF</a> 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œè¯­è¨€è¡¨å¾çš„æ¨å¯¼ï¼Œéšåå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œè¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰ã€‚å½“å‰é«˜çº§LIDæ–¹æ³•ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç»Ÿè®¡æ± åŒ–å±‚æ¥ä¿ƒè¿›ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–çš„åµŒå…¥å‘é‡æ—¶é—´å¸§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èšåˆã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†æœ€è¿‘æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå³Performerå’ŒAgent-Attentionï¼Œä»¥åŠä¸ç»Ÿè®¡æ± åŒ–å±‚çš„ç»“åˆã€‚åœ¨VoxPopuliã€FLEURSå’ŒVoxLinguaä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†LIDå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒPerformer-Attentionè¡¨ç°ä¼˜äºè‡ªæ³¨æ„åŠ›ï¼Œè€ŒAgent-Attentionä¸è‡ªæ³¨æ„åŠ›è¡¨ç°ç›¸å½“ï¼Œæœ‰æ—¶æ›´èƒœä¸€ç­¹ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LIDçš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œè¯­è¨€è¡¨å¾æ¨å¯¼ã€‚</li>
<li>å½“å‰é«˜çº§LIDæ–¹æ³•ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç»Ÿè®¡æ± åŒ–å±‚æ¥ä¿ƒè¿›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èšåˆã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†Performerå’ŒAgent-Attentionè¿™ä¸¤ç§æ–°æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>LIDå®éªŒåœ¨VoxPopuliã€FLEURSå’ŒVoxLinguaä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œã€‚</li>
<li>Performer-Attentionè¡¨ç°ä¼˜äºè‡ªæ³¨æ„åŠ›ã€‚</li>
<li>Agent-Attentionä¸è‡ªæ³¨æ„åŠ›ç›¸å½“æˆ–æœ‰æ—¶è¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a55e07fb790a3e01288324e7e95b5cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71ed8ee4283ff3ee950ffadf8cf0ea54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db97ff5c86f049eb09664d6bd2499e49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6209a7b2998b4d8fbf0a99e8d23035.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CowPilot-A-Framework-for-Autonomous-and-Human-Agent-Collaborative-Web-Navigation"><a href="#CowPilot-A-Framework-for-Autonomous-and-Human-Agent-Collaborative-Web-Navigation" class="headerlink" title="CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web   Navigation"></a>CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web   Navigation</h2><p><strong>Authors:Faria Huq, Zora Zhiruo Wang, Frank F. Xu, Tianyue Ou, Shuyan Zhou, Jeffrey P. Bigham, Graham Neubig</strong></p>
<p>While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agentâ€™s capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at <a target="_blank" rel="noopener" href="https://oaishi.github.io/cowpilot.html">https://oaishi.github.io/cowpilot.html</a> </p>
<blockquote>
<p>å…³äºç½‘ç»œä»£ç†çš„ç ”ç©¶è™½ç„¶å¼ºè°ƒäº†å…¶è‡ªä¸»æ‰§è¡Œç”¨æˆ·ä»»åŠ¡çš„æ½œåŠ›ï¼Œä½†åœ¨ç°å®ä¸­ï¼Œä»£ç†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡å’Œæ¨¡æ‹Ÿç”¨æˆ·åå¥½æ–¹é¢å¾€å¾€è¡¨ç°ä¸è¶³ã€‚è¿™ä¸ºäººç±»ä¸ä»£ç†åä½œå¹¶æœ‰æ•ˆåˆ©ç”¨ä»£ç†çš„èƒ½åŠ›æä¾›äº†æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº†CowPilotæ¡†æ¶ï¼Œå®ƒæ”¯æŒè‡ªä¸»å’Œç½‘ç»œç”¨æˆ·ä¸ä»£ç†çš„åä½œå¯¼èˆªï¼Œå¹¶é€šè¿‡å¯¹ä»»åŠ¡æˆåŠŸå’Œæ•ˆç‡çš„è¯„ä»·è¿›è¡Œè¯„ä¼°ã€‚CowPiloté€šè¿‡å…è®¸ä»£ç†æå‡ºä¸‹ä¸€æ­¥çš„å»ºè®®ï¼Œå‡å°‘äº†äººç±»æ‰§è¡Œä»»åŠ¡çš„æ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶ç”¨æˆ·èƒ½å¤Ÿæš‚åœã€æ‹’ç»æˆ–é‡‡å–æ›¿ä»£è¡ŒåŠ¨ã€‚åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¦†ç›–å»ºè®®æˆ–åœ¨éœ€è¦æ—¶æ¢å¤ä»£ç†æ§åˆ¶æ¥äº¤æ›¿æ‰§è¡Œä»–ä»¬çš„è¡ŒåŠ¨ä¸ä»£ç†çš„è¡ŒåŠ¨ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¸¸è§ç½‘ç«™ä¸Šè¿›è¡Œäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œå‘ç°äººæœºåä½œæ¨¡å¼å–å¾—äº†æœ€é«˜çš„æˆåŠŸç‡ï¼Œè¾¾åˆ°95%ï¼Œè€Œäººç±»ä»…éœ€è¦æ‰§è¡Œæ€»æ­¥éª¤çš„15.2%ã€‚å³ä½¿åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­æœ‰äººç±»å¹²é¢„ï¼Œä»£ç†ä¹Ÿèƒ½ç‹¬ç«‹å®Œæˆé«˜è¾¾ä¸€åŠçš„ä»»åŠ¡ã€‚CowPilotå¯ä»¥ä½œä¸ºè·¨ç½‘ç«™çš„æ•°æ®æ”¶é›†å’Œä»£ç†è¯„ä¼°çš„æœ‰ç”¨å·¥å…·ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™å°†æœ‰åŠ©äºç ”ç©¶ç”¨æˆ·å’Œä»£ç†å¦‚ä½•ååŒå·¥ä½œã€‚è§†é¢‘æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://oaishi.github.io/cowpilot.html%E8%A7%82%E7%9C%8B%E3%80%82">https://oaishi.github.io/cowpilot.htmlè§‚çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16609v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†webä»£ç†åœ¨å¤æ‚ä»»åŠ¡å’Œæ¨¡æ‹Ÿç”¨æˆ·åå¥½æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºCowPilotçš„æ¡†æ¶ï¼Œæ”¯æŒè‡ªä¸»å’Œäººç±»ä»£ç†ååŒçš„ç½‘é¡µæµè§ˆã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œå‘ç°äººç±»ä»£ç†ååŒæ¨¡å¼åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°95%ï¼ŒåŒæ—¶äººç±»åªéœ€æ‰§è¡Œæ€»æ­¥éª¤çš„15.2%ã€‚CowPilotè¿˜å¯ä½œä¸ºè·¨ç½‘ç«™çš„æ•°æ®æ”¶é›†å’Œåˆ†æå·¥å…·ï¼Œä¿ƒè¿›å¯¹äººæœºååŒå·¥ä½œçš„ç ”ç©¶ã€‚è§†é¢‘æ¼”ç¤ºè¯·å‚è§é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Webä»£ç†åœ¨å¤æ‚ä»»åŠ¡å’Œæ¨¡æ‹Ÿç”¨æˆ·åå¥½æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦äººç±»åä½œä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>CowPilotæ¡†æ¶æ”¯æŒè‡ªä¸»å’Œäººç±»ä»£ç†ååŒçš„ç½‘é¡µæµè§ˆï¼Œæå‡ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚</li>
<li>äººç±»ä»£ç†ååŒæ¨¡å¼åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°95%ã€‚</li>
<li>åœ¨äººç±»å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œä»£ç†ä»èƒ½ç‹¬ç«‹å®Œæˆä¸€åŠä»¥ä¸Šçš„ä»»åŠ¡ã€‚</li>
<li>CowPilotæ¡†æ¶å¯ç”¨äºæ•°æ®æ”¶é›†å’Œåˆ†æï¼Œä¿ƒè¿›äººæœºååŒå·¥ä½œçš„ç ”ç©¶ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­éšæ—¶æ¥ç®¡æˆ–æ¢å¤ä»£ç†æ§åˆ¶ï¼Œå®ç°çµæ´»çš„äººæœºäº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c802b7939a641ee3e36362fcde8c5fae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d06b9381a962fa08ea65d39705691cc8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc12a87c7f695729bf6891329bd45ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c5139dc405a2af66452cb20dcf4cd9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca76caa376a8ca1fff888f6c3816e94e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d83a0dc0d150fdb1fd3009897eef937.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2d088a4fd34c0da68a0eb8b926e6ef.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AgentMove-A-Large-Language-Model-based-Agentic-Framework-for-Zero-shot-Next-Location-Prediction"><a href="#AgentMove-A-Large-Language-Model-based-Agentic-Framework-for-Zero-shot-Next-Location-Prediction" class="headerlink" title="AgentMove: A Large Language Model based Agentic Framework for Zero-shot   Next Location Prediction"></a>AgentMove: A Large Language Model based Agentic Framework for Zero-shot   Next Location Prediction</h2><p><strong>Authors:Jie Feng, Yuwei Du, Jie Zhao, Yong Li</strong></p>
<p>Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design specific modules to complete them, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments utilizing mobility data from two distinct sources reveal that AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of 12 metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Our codes are available via <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/AgentMove">https://github.com/tsinghua-fib-lab/AgentMove</a>. </p>
<blockquote>
<p>ä¸‹ä¸€ä¸ªä½ç½®é¢„æµ‹åœ¨å„ç§å®é™…åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ€è¿‘ï¼Œç”±äºç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼Œäººä»¬è¯•å›¾å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºé›¶æ ·æœ¬ä¸‹ä¸€ä¸ªä½ç½®é¢„æµ‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç›´æ¥ä½¿ç”¨LLMsç”Ÿæˆæœ€ç»ˆè¾“å‡ºï¼Œè€Œæ²¡æœ‰è¿›è¡Œç³»ç»Ÿæ€§çš„è®¾è®¡ï¼Œè¿™é™åˆ¶äº†LLMsæ­ç¤ºå¤æ‚ç§»åŠ¨æ¨¡å¼çš„èƒ½åŠ›ï¼Œå¹¶ä½ä¼°äº†å®ƒä»¬ä¸°å¯Œçš„å…¨çƒåœ°ç†ç©ºé—´çŸ¥è¯†å‚¨å¤‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†AgentMoveï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§çš„æ™ºèƒ½é¢„æµ‹æ¡†æ¶ï¼Œä»¥å®ç°é€šç”¨çš„ä¸‹ä¸€ä¸ªä½ç½®é¢„æµ‹ã€‚åœ¨AgentMoveä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ç§»åŠ¨æ€§é¢„æµ‹ä»»åŠ¡è¿›è¡Œåˆ†è§£ï¼Œå¹¶è®¾è®¡ç‰¹å®šçš„æ¨¡å—æ¥å®Œæˆå®ƒä»¬ï¼ŒåŒ…æ‹¬ç”¨äºä¸ªäººç§»åŠ¨æ¨¡å¼æŒ–æ˜çš„ç©ºé—´æ—¶é—´è®°å¿†ã€ç”¨äºå»ºæ¨¡åŸå¸‚ç»“æ„å½±å“çš„å…¨çƒçŸ¥è¯†ç”Ÿæˆå™¨ä»¥åŠç”¨äºæ•æ‰äººç¾¤å…±äº«æ¨¡å¼çš„é›†ä½“çŸ¥è¯†æå–å™¨ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆä¸‰ä¸ªæ¨¡å—çš„ç»“æœè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ã€‚åˆ©ç”¨æ¥è‡ªä¸¤ä¸ªä¸åŒæ¥æºçš„å‡ºè¡Œæ•°æ®è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAgentMoveåœ¨12ä¸ªæŒ‡æ ‡ä¸­çš„8ä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†é¢†å…ˆçš„åŸºçº¿æ¨¡å‹3.33%è‡³8.57%ï¼Œå®ƒè¡¨ç°å‡ºå¯¹å„ç§LLMsåŸºç¡€çš„ç¨³å¥é¢„æµ‹ï¼Œå¹¶ä¸”åœ¨ä¸åŒåŸå¸‚ä¹‹é—´çš„åœ°ç†åè§ä¹Ÿè¾ƒå°ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/AgentMove%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tsinghua-fib-lab/AgentMoveè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13986v2">PDF</a> Accepted by NAACL 2025 as main conference paper,   <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/AgentMove">https://github.com/tsinghua-fib-lab/AgentMove</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼Œç°æœ‰ç ”ç©¶å°è¯•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºé›¶èµ·ç‚¹ä½ç½®é¢„æµ‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç³»ç»Ÿæ€§è®¾è®¡ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºé™åˆ¶äº†LLMsæŒ–æ˜å¤æ‚ç§»åŠ¨æ¨¡å¼çš„èƒ½åŠ›ï¼Œå¹¶ä½ä¼°äº†å…¶ä¸°å¯Œçš„å…¨çƒåœ°ç†ç©ºé—´çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºAgentMoveï¼Œä¸€ä¸ªç³»ç»Ÿæ€§çš„ä»£ç†é¢„æµ‹æ¡†æ¶ï¼Œå®ç°é€šç”¨ä½ç½®é¢„æµ‹ã€‚AgentMoveé€šè¿‡åˆ†è§£ç§»åŠ¨é¢„æµ‹ä»»åŠ¡å¹¶è®¾è®¡ç‰¹å®šæ¨¡å—æ¥å®Œæˆï¼ŒåŒ…æ‹¬ä¸ªä½“ç§»åŠ¨æ¨¡å¼æŒ–æ˜çš„æ—¶ç©ºè®°å¿†ã€å»ºæ¨¡åŸå¸‚ç»“æ„å½±å“çš„ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆå™¨ä»¥åŠæ•æ‰äººå£é—´å…±äº«æ¨¡å¼çš„é›†ä½“çŸ¥è¯†æå–å™¨ã€‚æœ€åï¼Œç»“åˆä¸‰ä¸ªæ¨¡å—çš„ç»“æœè¿›è¡Œæ¨ç†ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAgentMoveåœ¨8ä¸ªæŒ‡æ ‡ä¸­çš„é¢„æµ‹æ•ˆæœè¶…è¿‡é¢†å…ˆåŸºçº¿æ¨¡å‹3.33%~8.57%ï¼Œè¡¨ç°å‡ºç¨³å¥çš„é¢„æµ‹æ€§èƒ½ä¸”åœ¨ä¸åŒLLMsåŸºç¡€ä¸Šå…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ï¼ŒåŒæ—¶å‡å°‘äº†åŸå¸‚é—´çš„åœ°ç†åè§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«å°è¯•åº”ç”¨äºé›¶èµ·ç‚¹ä½ç½®é¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨LLMsç”Ÿæˆæœ€ç»ˆè¾“å‡ºå­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æŒ–æ˜å¤æ‚ç§»åŠ¨æ¨¡å¼å¹¶ä½ä¼°å…¶å…¨çƒåœ°ç†ç©ºé—´çŸ¥è¯†ã€‚</li>
<li>AgentMoveæ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§çš„é¢„æµ‹æ¡†æ¶ï¼ŒåŒ…æ‹¬æ—¶ç©ºè®°å¿†ã€ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆå™¨å’Œé›†ä½“çŸ¥è¯†æå–å™¨ä¸‰ä¸ªæ¨¡å—ã€‚</li>
<li>AgentMoveé€šè¿‡åˆ†è§£ç§»åŠ¨é¢„æµ‹ä»»åŠ¡å¹¶ä½¿ç”¨ç‰¹å®šæ¨¡å—å®Œæˆï¼Œä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>AgentMoveåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šé¢†å…ˆåŸºçº¿æ¨¡å‹ï¼Œè¡¨ç°å‡ºç¨³å¥çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>AgentMoveå…·æœ‰é²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„LLMsåŸºç¡€ä¸Šè¿›è¡Œé¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25a5264fa1cf172c30e5de4693e12d28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49b341ebf5f1bd2b6bd5367fa925790d.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2408.13986v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2408.13986v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Visual-Agents-as-Fast-and-Slow-Thinkers"><a href="#Visual-Agents-as-Fast-and-Slow-Thinkers" class="headerlink" title="Visual Agents as Fast and Slow Thinkers"></a>Visual Agents as Fast and Slow Thinkers</h2><p><strong>Authors:Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, Dongfang Liu</strong></p>
<p>Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1&#x2F;2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaSTâ€™s superior performance. Extensive testing validates the efficacy and robustness of FaSTâ€™s core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps:&#x2F;&#x2F;github.com&#x2F;GuangyanS&#x2F;Sys2-LLaVA. </p>
<blockquote>
<p>å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½éœ€è¦ç²¾è¿›ç³»ç»Ÿ1å’Œç³»ç»Ÿ2æ€ç»´ä¹‹é—´çš„è®¤çŸ¥åŒºåˆ†ã€‚è™½ç„¶å½“ä»£äººå·¥æ™ºèƒ½ï¼Œç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨ï¼Œå±•ç°å‡ºäººç±»ç‰¹å¾ï¼Œä½†ä»æœªè¾¾åˆ°çœŸæ­£çš„è®¤çŸ¥æ°´å¹³ã€‚ä»ç»“æ„åŒ–åŸºå‡†æµ‹è¯•è¿‡æ¸¡åˆ°ç°å®ä¸–ç•Œåœºæ™¯ï¼Œå¯¹è§†è§‰æ™ºèƒ½ä½“æ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå¸¸å¸¸å¯¼è‡´ä¸å‡†ç¡®ä¸”è¿‡äºè‡ªä¿¡çš„å›åº”ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FaSTï¼Œå®ƒå°†å¿«é€Ÿå’Œæ…¢é€Ÿæ€è€ƒæœºåˆ¶èå…¥è§†è§‰æ™ºèƒ½ä½“ã€‚FaSTé‡‡ç”¨å¼€å…³é€‚é…å™¨æ¥åŠ¨æ€é€‰æ‹©ç³»ç»Ÿ1&#x2F;2æ¨¡å¼ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å®šåˆ¶é—®é¢˜è§£å†³æ–¹å¼ã€‚å®ƒé€šè¿‡è°ƒæ•´æ¨¡å‹ä¿¡å¿ƒå¹¶æ•´åˆæ–°çš„ä¸Šä¸‹æ–‡æ•°æ®æ¥åº”å¯¹ä¸ç¡®å®šå’Œæœªè§è¿‡çš„ç‰©ä½“ã€‚å‡­å€Ÿè¿™ä¸€æ–°é¢–è®¾è®¡ï¼Œæˆ‘ä»¬æå€¡ä¸€ä¸ªçµæ´»çš„ç³»ç»Ÿã€åˆ†å±‚æ¨ç†èƒ½åŠ›å’Œé€æ˜çš„å†³ç­–åˆ¶å®šæµç¨‹ï¼Œæ‰€æœ‰è¿™äº›éƒ½æœ‰åŠ©äºå…¶åœ¨è§†è§‰æ™ºèƒ½ä¸­æ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒFaSTåœ¨å„ç§çŸ¥ååŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨è§†è§‰é—®ç­”çš„VQA^{v2}ä¸Šè¾¾åˆ°80.8%çš„å‡†ç¡®ç‡ï¼Œåœ¨æ¨ç†åˆ†å‰²çš„ReasonSegä¸Šè¾¾åˆ°48.7%çš„GIoUåˆ†æ•°ï¼Œè¯æ˜äº†FaSTçš„ä¼˜è¶Šæ€§ã€‚å¤§é‡æµ‹è¯•éªŒè¯äº†FaSTæ ¸å¿ƒç»„ä»¶çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­è®¤çŸ¥è§†è§‰æ™ºèƒ½ä½“å‘å±•çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GuangyanS/Sys2-LLaVA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/GuangyanS/Sys2-LLaVAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08862v4">PDF</a> International Conference on Learning Representations (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½éœ€è¦ä¼˜åŒ–ç³»ç»Ÿ1å’Œç³»ç»Ÿ2æ€ç»´ä¹‹é—´çš„è®¤çŸ¥åŒºåˆ†ã€‚å½“ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„AIè¡¨ç°å‡ºäººç±»ç‰¹è´¨ï¼Œä½†å°šæœªè¾¾åˆ°çœŸæ­£çš„è®¤çŸ¥æ°´å¹³ã€‚ä»ç»“æ„åŒ–åŸºå‡†æµ‹è¯•è¿‡æ¸¡åˆ°ç°å®ä¸–ç•Œåœºæ™¯ï¼Œè§†è§‰ä»£ç†é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ååº”ä¸å‡†ç¡®ä¸”è¿‡äºè‡ªä¿¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FaSTï¼Œå®ƒå°†å¿«é€Ÿå’Œæ…¢é€Ÿæ€è€ƒæœºåˆ¶èå…¥è§†è§‰ä»£ç†ä¸­ã€‚FaSTé€šè¿‡å¼€å…³é€‚é…å™¨åŠ¨æ€é€‰æ‹©ç³»ç»Ÿ1&#x2F;2æ¨¡å¼ï¼Œé’ˆå¯¹ä»»åŠ¡å¤æ‚æ€§è°ƒæ•´è§£å†³æ–¹æ³•ã€‚å®ƒå¤„ç†ä¸ç¡®å®šå’Œæœªè§ç‰©ä½“é€šè¿‡è°ƒæ•´æ¨¡å‹ä¿¡å¿ƒå¹¶é›†æˆæ–°çš„æƒ…å¢ƒæ•°æ®ã€‚å‡­å€Ÿæ–°é¢–çš„è®¾è®¡ï¼Œæˆ‘ä»¬æå€¡çµæ´»çš„ç³»ç»Ÿã€åˆ†å±‚æ¨ç†èƒ½åŠ›å’Œé€æ˜çš„å†³ç­–åˆ¶å®šæµç¨‹ï¼Œæ‰€æœ‰è¿™äº›éƒ½æœ‰åŠ©äºæ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹åœ¨è§†è§‰æ™ºèƒ½ä¸­çš„è¡¨ç°ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒFaSTåœ¨å„ç§çŸ¥ååŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨è§†è§‰é—®ç­”å’Œæ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­åˆ†åˆ«è¾¾åˆ°äº†80.8%çš„å‡†ç¡®ç‡å’Œ48.7%çš„GIoUåˆ†æ•°ã€‚å¹¿æ³›çš„æµ‹è¯•éªŒè¯äº†FaSTæ ¸å¿ƒç»„ä»¶çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨åŠ¨AIç³»ç»Ÿä¸­è®¤çŸ¥è§†è§‰ä»£ç†å‘å±•çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ç°äººç±»æ°´å¹³æ™ºèƒ½éœ€ä¼˜åŒ–ç³»ç»Ÿ1å’Œç³»ç»Ÿ2æ€ç»´çš„è®¤çŸ¥åŒºåˆ†ã€‚</li>
<li>å½“ä»£AIè™½å…·äººç±»ç‰¹è´¨ï¼Œä½†ä»æœªè¾¾åˆ°çœŸæ­£çš„è®¤çŸ¥æ°´å¹³ã€‚</li>
<li>è§†è§‰ä»£ç†åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸‹é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€æé«˜å‡†ç¡®æ€§å’Œåº”å¯¹ä¸ç¡®å®šæ€§ã€‚</li>
<li>FaSTé€šè¿‡ç»“åˆå¿«é€Ÿå’Œæ…¢é€Ÿæ€è€ƒæœºåˆ¶è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>FaSTé€šè¿‡å¼€å…³é€‚é…å™¨åŠ¨æ€è°ƒæ•´ç³»ç»Ÿ1å’Œç³»ç»Ÿ2çš„æ¨¡å¼ä»¥é€‚åº”ä»»åŠ¡å¤æ‚æ€§ã€‚</li>
<li>FaSTåœ¨å¤„ç†ä¸ç¡®å®šå’Œæœªè§ç‰©ä½“æ—¶ï¼Œèƒ½è°ƒæ•´æ¨¡å‹ä¿¡å¿ƒå¹¶é›†æˆæ–°æƒ…å¢ƒæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2408.08862v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2408.08862v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2408.08862v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2408.08862v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Learning-to-Steer-Markovian-Agents-under-Model-Uncertainty"><a href="#Learning-to-Steer-Markovian-Agents-under-Model-Uncertainty" class="headerlink" title="Learning to Steer Markovian Agents under Model Uncertainty"></a>Learning to Steer Markovian Agents under Model Uncertainty</h2><p><strong>Authors:Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, Niao He</strong></p>
<p>Designing incentives for an adapting population is a ubiquitous problem in a wide array of economic applications and beyond. In this work, we study how to design additional rewards to steer multi-agent systems towards desired policies \emph{without} prior knowledge of the agentsâ€™ underlying learning dynamics. Motivated by the limitation of existing works, we consider a new and general category of learning dynamics called \emph{Markovian agents}. We introduce a model-based non-episodic Reinforcement Learning (RL) formulation for our steering problem. Importantly, we focus on learning a \emph{history-dependent} steering strategy to handle the inherent model uncertainty about the agentsâ€™ learning dynamics. We introduce a novel objective function to encode the desiderata of achieving a good steering outcome with reasonable cost. Theoretically, we identify conditions for the existence of steering strategies to guide agents to the desired policies. Complementing our theoretical contributions, we provide empirical algorithms to approximately solve our objective, which effectively tackles the challenge in learning history-dependent strategies. We demonstrate the efficacy of our algorithms through empirical evaluations. </p>
<blockquote>
<p>é’ˆå¯¹é€‚åº”äººç¾¤è®¾è®¡æ¿€åŠ±æœºåˆ¶æ˜¯ä¼—å¤šç»æµåº”ç”¨ä»¥åŠå…¶ä»–é¢†åŸŸä¸­çš„æ™®éé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•è®¾è®¡é™„åŠ å¥–åŠ±æ¥å¼•å¯¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®ç°æœŸæœ›çš„ç­–ç•¥ï¼Œè€Œæ— éœ€äº‹å…ˆäº†è§£æ™ºèƒ½ä½“çš„åŸºæœ¬å­¦ä¹ åŠ¨æ€ã€‚å—ç°æœ‰å·¥ä½œçš„å±€é™æ€§çš„å¯å‘ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§æ–°çš„é€šç”¨å­¦ä¹ åŠ¨æ€ç±»åˆ«ï¼Œç§°ä¸ºâ€œé©¬å°”å¯å¤«æ™ºèƒ½ä½“â€ã€‚æˆ‘ä»¬å¯¹å¼•å¯¼é—®é¢˜é‡‡ç”¨äº†åŸºäºæ¨¡å‹çš„éç‰‡æ®µæ€§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å…¬å¼ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå­¦ä¹ ä¸€ç§ä¾èµ–äºå†å²çš„å¼•å¯¼ç­–ç•¥ï¼Œä»¥åº”å¯¹å…³äºæ™ºèƒ½ä½“å­¦ä¹ åŠ¨æ€å›ºæœ‰çš„æ¨¡å‹ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç›®æ ‡å‡½æ•°ï¼Œä»¥ç¼–ç åœ¨åˆç†æˆæœ¬ä¸‹å®ç°è‰¯å¥½å¼•å¯¼ç»“æœçš„æ„¿æœ›ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬ç¡®å®šäº†å­˜åœ¨å¼•å¯¼ç­–ç•¥çš„æ¡ä»¶ï¼Œä»¥æŒ‡å¯¼æ™ºèƒ½ä½“è¾¾åˆ°æœŸæœ›çš„ç­–ç•¥ã€‚é™¤äº†ç†è®ºè´¡çŒ®ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ç»éªŒç®—æ³•æ¥è¿‘ä¼¼è§£å†³æˆ‘ä»¬çš„ç›®æ ‡ï¼Œè¿™æœ‰æ•ˆåœ°è§£å†³äº†å­¦ä¹ å†å²ä¾èµ–ç­–ç•¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡ç»éªŒè¯„ä¼°è¯æ˜äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10207v3">PDF</a> 35 Pages; ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•è®¾è®¡é¢å¤–å¥–åŠ±æ¥å¼•å¯¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæœå‘æœŸæœ›çš„ç­–ç•¥ï¼Œä¸”æ— éœ€äº‹å…ˆäº†è§£æ™ºèƒ½ä½“çš„å­¦ä¹ åŠ¨æ€ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„éç‰‡æ®µå¼å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å…¬å¼æ¥è§£å†³å¼•å¯¼é—®é¢˜ï¼Œå¹¶å…³æ³¨å­¦ä¹ ä¸€ä¸ªä¾èµ–å†å²çš„å¼•å¯¼ç­–ç•¥æ¥å¤„ç†å¯¹æ™ºèƒ½ä½“å­¦ä¹ åŠ¨æ€çš„å›ºæœ‰æ¨¡å‹ä¸ç¡®å®šæ€§ã€‚å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ç›®æ ‡å‡½æ•°ï¼Œä»¥ç¼–ç åœ¨åˆç†æˆæœ¬ä¸‹å®ç°è‰¯å¥½å¼•å¯¼ç»“æœçš„è¦æ±‚ã€‚ç†è®ºä¸Šï¼Œæ–‡ç« ç¡®å®šäº†å­˜åœ¨å¼•å¯¼ç­–ç•¥çš„æ¡ä»¶ï¼Œä»¥æŒ‡å¯¼æ™ºèƒ½ä½“è¾¾åˆ°æœŸæœ›çš„ç­–ç•¥ã€‚åŒæ—¶ï¼Œæä¾›äº†å®è¯ç®—æ³•æ¥è¿‘ä¼¼è§£å†³ç›®æ ‡é—®é¢˜ï¼Œå¹¶é€šè¿‡å®è¯è¯„ä¼°éªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶å¦‚ä½•è®¾è®¡å¥–åŠ±ä»¥å¼•å¯¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¾¾åˆ°æœŸæœ›ç­–ç•¥ï¼Œä¸”æ— éœ€é¢„å…ˆäº†è§£æ™ºèƒ½ä½“çš„å­¦ä¹ åŠ¨æ€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„éç‰‡æ®µå¼å¼ºåŒ–å­¦ä¹ å…¬å¼æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>å…³æ³¨å­¦ä¹ ä¾èµ–å†å²çš„å¼•å¯¼ç­–ç•¥ï¼Œå¤„ç†å¯¹æ™ºèƒ½ä½“å­¦ä¹ åŠ¨æ€çš„æ¨¡å‹ä¸ç¡®å®šæ€§ã€‚</li>
<li>å¼•å…¥æ–°é¢–ç›®æ ‡å‡½æ•°ï¼Œä»¥ç¼–ç åœ¨åˆç†æˆæœ¬ä¸‹å®ç°è‰¯å¥½å¼•å¯¼ç»“æœçš„è¦æ±‚ã€‚</li>
<li>ç¡®å®šäº†å­˜åœ¨å¼•å¯¼ç­–ç•¥çš„ç†è®ºæ¡ä»¶ã€‚</li>
<li>æä¾›äº†å®è¯ç®—æ³•æ¥è§£å†³ç›®æ ‡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2407.10207v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2407.10207v3/page_1_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="How-Far-Are-We-on-the-Decision-Making-of-LLMs-Evaluating-LLMsâ€™-Gaming-Ability-in-Multi-Agent-Environments"><a href="#How-Far-Are-We-on-the-Decision-Making-of-LLMs-Evaluating-LLMsâ€™-Gaming-Ability-in-Multi-Agent-Environments" class="headerlink" title="How Far Are We on the Decision-Making of LLMs? Evaluating LLMsâ€™ Gaming   Ability in Multi-Agent Environments"></a>How Far Are We on the Decision-Making of LLMs? Evaluating LLMsâ€™ Gaming   Ability in Multi-Agent Environments</h2><p><strong>Authors:Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu</strong></p>
<p>Decision-making is a complex process requiring diverse abilities, making it an excellent framework for evaluating Large Language Models (LLMs). Researchers have examined LLMsâ€™ decision-making through the lens of Game Theory. However, existing evaluation mainly focus on two-player scenarios where an LLM competes against another. Additionally, previous benchmarks suffer from test set leakage due to their static design. We introduce GAMA($\gamma$)-Bench, a new framework for evaluating LLMsâ€™ Gaming Ability in Multi-Agent environments. It includes eight classical game theory scenarios and a dynamic scoring scheme specially designed to quantitatively assess LLMsâ€™ performance. $\gamma$-Bench allows flexible game settings and adapts the scoring system to different game parameters, enabling comprehensive evaluation of robustness, generalizability, and strategies for improvement. Our results indicate that GPT-3.5 demonstrates strong robustness but limited generalizability, which can be enhanced using methods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2. Gemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by LLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental results are publicly available at <a target="_blank" rel="noopener" href="https://github.com/CUHK-ARISE/GAMABench">https://github.com/CUHK-ARISE/GAMABench</a>. </p>
<blockquote>
<p>å†³ç­–æ˜¯ä¸€ä¸ªéœ€è¦å¤šç§èƒ½åŠ›çš„å¤æ‚è¿‡ç¨‹ï¼Œå› æ­¤å®ƒæ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç†æƒ³æ¡†æ¶ã€‚ç ”ç©¶è€…å·²ç»é€šè¿‡åšå¼ˆè®ºçš„è§’åº¦ç ”ç©¶äº†LLMçš„å†³ç­–åˆ¶å®šã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨ä¸¤äººåœºæ™¯ä¸­ï¼Œå³LLMä¸å…¶ä»–LLMä¹‹é—´çš„ç«äº‰ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„åŸºå‡†æµ‹è¯•ç”±äºå…¶é™æ€è®¾è®¡è€Œé­å—æµ‹è¯•é›†æ³„éœ²çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†GAMA($\gamma$)-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°LLMåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ¸¸æˆèƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒåŒ…æ‹¬å…«ä¸ªç»å…¸çš„æ¸¸æˆç†è®ºåœºæ™¯å’Œä¸€ç§åŠ¨æ€è¯„åˆ†æ–¹æ¡ˆï¼Œä¸“é—¨ç”¨äºå®šé‡è¯„ä¼°LLMçš„æ€§èƒ½ã€‚$\gamma$-Benchå…è®¸çµæ´»çš„æ¸¸æˆè®¾ç½®ï¼Œå¹¶é€‚åº”ä¸åŒçš„æ¸¸æˆå‚æ•°æ¥è°ƒæ•´è¯„åˆ†ç³»ç»Ÿï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°LLMçš„ç¨³å¥æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œæ”¹è¿›ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-3.5è¡¨ç°å‡ºå¾ˆå¼ºçš„ç¨³å¥æ€§ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå¯ä»¥é€šè¿‡å¦‚â€œæ€ç»´é“¾â€ç­‰æ–¹æ³•è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æ¥è‡ªå…­ä¸ªæ¨¡å‹å®¶æ—çš„13ä¸ªLLMï¼ŒåŒ…æ‹¬GPT-3.5ã€GPT-4ã€åŒå­åº§ã€LLaMA-3.1ã€Mixtralå’ŒQwen-2ã€‚å…¶ä¸­ï¼ŒåŒå­åº§-1.5-Proè¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†ä¸º69.8ï¼ˆæ»¡åˆ†100ï¼‰ï¼Œå…¶æ¬¡æ˜¯LLaMA-3.1-70Bï¼ˆå¾—åˆ†65.9ï¼‰å’ŒMixtral-8x22Bï¼ˆå¾—åˆ†62.4ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å’Œå®éªŒç»“æœå·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/CUHK-ARISE/GAMABench%E4%B8%8A%E3%80%82">https://github.com/CUHK-ARISE/GAMABenchä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11807v5">PDF</a> Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;   Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,   Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,   Qwen-2-72B</p>
<p><strong>Summary</strong><br>å†³ç­–åˆ¶å®šæ˜¯ä¸€ä¸ªéœ€è¦å¤šç§èƒ½åŠ›çš„å¤æ‚è¿‡ç¨‹ï¼Œå› æ­¤æˆä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç†æƒ³æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡åšå¼ˆè®ºçš„è§’åº¦ç ”ç©¶LLMçš„å†³ç­–åˆ¶å®šã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨ä¸¤ç©å®¶æƒ…å¢ƒä¸‹LLMçš„ç«äº‰è¡¨ç°ã€‚æ­¤å¤–ï¼Œå…ˆå‰çš„åŸºå‡†æµ‹è¯•å› é™æ€è®¾è®¡è€Œé¢ä¸´æµ‹è¯•é›†æ³„éœ²é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GAMA($\gamma$)-Benchæ–°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ¸¸æˆèƒ½åŠ›ã€‚å®ƒåŒ…æ‹¬å…«ä¸ªç»å…¸åšå¼ˆè®ºåœºæ™¯å’Œä¸“é—¨è®¾è®¡çš„åŠ¨æ€è¯„åˆ†æ–¹æ¡ˆï¼Œä»¥å®šé‡è¯„ä¼°LLMçš„è¡¨ç°ã€‚$\gamma$-Benchå¯ä»¥çµæ´»è®¾ç½®æ¸¸æˆç¯å¢ƒå¹¶é€‚åº”ä¸åŒçš„æ¸¸æˆå‚æ•°è¯„åˆ†ç³»ç»Ÿï¼Œå…¨é¢è¯„ä¼°LLMçš„ç¨³å¥æ€§ã€é€šç”¨æ€§å’Œæ”¹è¿›ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºGPT-3.5è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ä½†æœ‰é™çš„é€šç”¨æ€§ï¼Œå¯ä»¥é€šè¿‡é“¾å¼æ€ç»´ç­‰æ–¹æ³•åŠ ä»¥æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æ¥è‡ªå…­ä¸ªæ¨¡å‹å®¶æ—çš„13ä¸ªLLMï¼ŒåŒ…æ‹¬GPT-3.5ã€GPT-4ã€åŒå­åº§ã€LLaMA-3.1ã€Mixtralå’ŒQwen-2ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†³ç­–åˆ¶å®šèƒ½åŠ›é€šè¿‡åšå¼ˆè®ºè§’åº¦è¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨ä¸¤ç©å®¶æƒ…å¢ƒï¼Œä¸”å­˜åœ¨æµ‹è¯•é›†æ³„éœ²é—®é¢˜ã€‚</li>
<li>æ¨å‡ºäº†æ–°çš„è¯„ä¼°æ¡†æ¶GAMA($\gamma$)-Benchï¼Œç”¨äºè¯„ä¼°LLMåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ¸¸æˆèƒ½åŠ›ã€‚</li>
<li>GAMA($\gamma$)-BenchåŒ…å«å…«ä¸ªç»å…¸åšå¼ˆè®ºåœºæ™¯å’ŒåŠ¨æ€è¯„åˆ†æ–¹æ¡ˆã€‚</li>
<li>GAMA($\gamma$)-Benchèƒ½çµæ´»è®¾ç½®æ¸¸æˆç¯å¢ƒï¼Œè¯„ä¼°LLMçš„ç¨³å¥æ€§ã€é€šç”¨æ€§å’Œæ”¹è¿›ç­–ç•¥ã€‚</li>
<li>GPT-3.5å±•ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œä½†é€šç”¨æ€§æœ‰é™ï¼Œå¯é€šè¿‡ç‰¹å®šæ–¹æ³•å¦‚Chain-of-Thoughtè¿›è¡Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2403.11807v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2403.11807v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Agent/2403.11807v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0acdfd199619e2f420eed838a2c3cc62.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  IceBerg Debiased Self-Training for Class-Imbalanced Node Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3f2dbb7b9fc4ae0a1234f5ee88840144.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ReasonFlux Hierarchical LLM Reasoning via Scaling Thought Templates
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
