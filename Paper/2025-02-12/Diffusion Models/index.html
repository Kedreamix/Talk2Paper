<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  A Large-scale AI-generated Image Inpainting Benchmark">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="A-Large-scale-AI-generated-Image-Inpainting-Benchmark"><a href="#A-Large-scale-AI-generated-Image-Inpainting-Benchmark" class="headerlink" title="A Large-scale AI-generated Image Inpainting Benchmark"></a>A Large-scale AI-generated Image Inpainting Benchmark</h2><p><strong>Authors:Paschalis Giakoumoglou, Dimitrios Karageorgiou, Symeon Papadopoulos, Panagiotis C. Petrantonakis</strong></p>
<p>Recent advances in generative models enable highly realistic image manipulations, creating an urgent need for robust forgery detection methods. Current datasets for training and evaluating these methods are limited in scale and diversity. To address this, we propose a methodology for creating high-quality inpainting datasets and apply it to create DiQuID, comprising over 95,000 inpainted images generated from 78,000 original images sourced from MS-COCO, RAISE, and OpenImages. Our methodology consists of three components: (1) Semantically Aligned Object Replacement (SAOR) that identifies suitable objects through instance segmentation and generates contextually appropriate prompts, (2) Multiple Model Image Inpainting (MMII) that employs various state-of-the-art inpainting pipelines primarily based on diffusion models to create diverse manipulations, and (3) Uncertainty-Guided Deceptiveness Assessment (UGDA) that evaluates image realism through comparative analysis with originals. The resulting dataset surpasses existing ones in diversity, aesthetic quality, and technical quality. We provide comprehensive benchmarking results using state-of-the-art forgery detection methods, demonstrating the datasetâ€™s effectiveness in evaluating and improving detection algorithms. Through a human study with 42 participants on 1,000 images, we show that while humans struggle with images classified as deceiving by our methodology, models trained on our dataset maintain high performance on these challenging cases. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/mever-team/DiQuID">https://github.com/mever-team/DiQuID</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—é«˜åº¦é€¼çœŸçš„å›¾åƒæ“ä½œæˆä¸ºå¯èƒ½ï¼Œä»è€Œäº§ç”Ÿäº†å¯¹ç¨³å¥çš„ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ã€‚å½“å‰ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è¿™äº›æ–¹æ³•çš„æ•°æ®é›†åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›å»ºé«˜è´¨é‡å¡«å……æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶åº”ç”¨è¯¥æ–¹æ³•åˆ›å»ºäº†DiQuIDæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡95000å¼ é€šè¿‡78000å¼ åŸå§‹å›¾åƒç”Ÿæˆçš„å¡«å……å›¾åƒï¼Œè¿™äº›åŸå§‹å›¾åƒæ¥æºäºMS-COCOã€RAISEå’ŒOpenImagesã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰è¯­ä¹‰å¯¹é½å¯¹è±¡æ›¿æ¢ï¼ˆSAORï¼‰ï¼Œå®ƒé€šè¿‡å®ä¾‹åˆ†å‰²æ¥è¯†åˆ«åˆé€‚çš„å¯¹è±¡å¹¶ç”Ÿæˆä¸Šä¸‹æ–‡é€‚å½“çš„æç¤ºï¼›ï¼ˆ2ï¼‰å¤šæ¨¡å‹å›¾åƒå¡«å……ï¼ˆMMIIï¼‰ï¼Œé‡‡ç”¨å¤šç§å…ˆè¿›çš„å¡«å……ç®¡é“ï¼Œä¸»è¦åŸºäºæ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ ·åŒ–çš„æ“ä½œï¼›ï¼ˆ3ï¼‰åŸºäºä¸ç¡®å®šæ€§çš„æ¬ºéª—æ€§è¯„ä¼°ï¼ˆUGDAï¼‰ï¼Œé€šè¿‡ä¸åŸå›¾çš„å¯¹æ¯”åˆ†ææ¥è¯„ä¼°å›¾åƒçš„çœŸå®æ€§ã€‚æ‰€å¾—åˆ°çš„æ•°æ®é›†åœ¨å¤šæ ·æ€§ã€ç¾å­¦è´¨é‡å’ŒæŠ€æœ¯è´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„ä¼ªé€ æ£€æµ‹æ–¹æ³•æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨è¯„ä¼°å’Œæ”¹è¿›æ£€æµ‹ç®—æ³•æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹42åå‚ä¸è€…è¿›è¡Œçš„æ¶‰åŠ1000å¼ å›¾åƒçš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œè™½ç„¶äººç±»éš¾ä»¥è¯†åˆ«å‡ºæˆ‘ä»¬æ–¹æ³•åˆ†ç±»ä¸ºæ¬ºéª—çš„å›¾åƒï¼Œä½†åœ¨è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œç»è¿‡æˆ‘ä»¬æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mever-team/DiQuID%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mever-team/DiQuIDè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å›¾åƒæ“ä½œä¼ªé€ è¡Œä¸ºçš„æ£€æµ‹éœ€æ±‚ï¼Œå› å½“å‰è®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†å­˜åœ¨è§„æ¨¡ä¸å¤šæ ·æ€§ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åˆ›å»ºé«˜è´¨é‡å¡«å……æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶æ®æ­¤æ„å»ºäº†DiQuIDæ•°æ®é›†ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šè¯­ä¹‰å¯¹é½å¯¹è±¡æ›¿æ¢ã€å¤šæ¨¡å‹å›¾åƒå¡«å……ä»¥åŠä¸ç¡®å®šæ€§å¼•å¯¼æ¬ºéª—æ€§è¯„ä¼°ã€‚æ‰€åˆ›å»ºçš„DiQuIDæ•°æ®é›†åœ¨å¤šæ ·æ€§ã€ç¾å­¦è´¨é‡å’ŒæŠ€æœ¯è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ•°æ®é›†ã€‚é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•ä¸åŒ…å«42å‚ä¸è€…çš„å®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†æ•°æ®é›†åœ¨è¯„ä¼°å’Œæ”¹è¿›æ£€æµ‹ç®—æ³•ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸç”Ÿæˆæ¨¡å‹è¿›æ­¥æ¨åŠ¨äº†é«˜åº¦é€¼çœŸçš„å›¾åƒæ“ä½œï¼Œå‚¬ç”Ÿäº†è¿«åˆ‡çš„ä¼ªé€ æ£€æµ‹éœ€æ±‚ã€‚</li>
<li>å½“å‰ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„æ•°æ®é›†å­˜åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§ä¸Šçš„å±€é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ›å»ºé«˜è´¨é‡å¡«å……æ•°æ®é›†çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è¯­ä¹‰å¯¹é½å¯¹è±¡æ›¿æ¢ã€å¤šæ¨¡å‹å›¾åƒå¡«å……å’Œä¸ç¡®å®šæ€§å¼•å¯¼æ¬ºéª—æ€§è¯„ä¼°ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ã€‚</li>
<li>åˆ›å»ºçš„DiQuIDæ•°æ®é›†åœ¨å¤šæ ·æ€§ã€ç¾å­¦è´¨é‡å’ŒæŠ€æœ¯è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†DiQuIDæ•°æ®é›†åœ¨è¯„ä¼°å’Œæ”¹è¿›æ£€æµ‹ç®—æ³•ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œäººç±»åœ¨è¯†åˆ«ç”±æ–°æ–¹æ³•åˆ†ç±»ä¸ºæ¬ºéª—æ€§çš„å›¾åƒæ—¶å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06593v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06593v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06593v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06593v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06593v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06593v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Computational-Neuroimaging-A-Survey"><a href="#Diffusion-Models-for-Computational-Neuroimaging-A-Survey" class="headerlink" title="Diffusion Models for Computational Neuroimaging: A Survey"></a>Diffusion Models for Computational Neuroimaging: A Survey</h2><p><strong>Authors:Haokai Zhao, Haowei Lou, Lina Yao, Wei Peng, Ehsan Adeli, Kilian M Pohl, Yu Zhang</strong></p>
<p>Computational neuroimaging involves analyzing brain images or signals to provide mechanistic insights and predictive tools for human cognition and behavior. While diffusion models have shown stability and high-quality generation in natural images, there is increasing interest in adapting them to analyze brain data for various neurological tasks such as data enhancement, disease diagnosis and brain decoding. This survey provides an overview of recent efforts to integrate diffusion models into computational neuroimaging. We begin by introducing the common neuroimaging data modalities, follow with the diffusion formulations and conditioning mechanisms. Then we discuss how the variations of the denoising starting point, condition input and generation target of diffusion models are developed and enhance specific neuroimaging tasks. For a comprehensive overview of the ongoing research, we provide a publicly available repository at <a target="_blank" rel="noopener" href="https://github.com/JoeZhao527/dm4neuro">https://github.com/JoeZhao527/dm4neuro</a>. </p>
<blockquote>
<p>è®¡ç®—ç¥ç»æˆåƒæ¶‰åŠåˆ†æè„‘å›¾åƒæˆ–ä¿¡å·ï¼Œä»¥æä¾›å¯¹äººç±»è®¤çŸ¥å’Œè¡Œä¸ºæœºåˆ¶çš„è§è§£å’Œé¢„æµ‹å·¥å…·ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸­æ˜¾ç¤ºå‡ºç¨³å®šæ€§å’Œé«˜è´¨é‡ç”Ÿæˆï¼Œä½†äººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£å°†å®ƒä»¬é€‚åº”äºåˆ†æå„ç§ç¥ç»ä»»åŠ¡çš„å¤§è„‘æ•°æ®ï¼Œå¦‚æ•°æ®å¢å¼ºã€ç–¾ç—…è¯Šæ–­å’Œå¤§è„‘è§£ç ã€‚æœ¬æ–‡æ¦‚è¿°äº†æœ€è¿‘å°†æ‰©æ•£æ¨¡å‹æ•´åˆåˆ°è®¡ç®—ç¥ç»æˆåƒä¸­çš„åŠªåŠ›ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»å¸¸è§çš„ç¥ç»æˆåƒæ•°æ®æ¨¡å¼ï¼Œç„¶åä»‹ç»æ‰©æ•£å…¬å¼å’Œè°ƒèŠ‚æœºåˆ¶ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¨è®ºå»å™ªèµ·ç‚¹ã€æ¡ä»¶è¾“å…¥å’Œç”Ÿæˆç›®æ ‡çš„æ‰©æ•£æ¨¡å‹çš„å˜åŒ–æ˜¯å¦‚ä½•å‘å±•å’Œå¢å¼ºç‰¹å®šç¥ç»æˆåƒä»»åŠ¡çš„ã€‚ä¸ºäº†å…¨é¢æ¦‚è¿°ç›®å‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/JoeZhao527/dm4neuro%E3%80%82">https://github.com/JoeZhao527/dm4neuroã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06552v1">PDF</a> 9 pages, 1 figure</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®¡ç®—ç¥ç»æˆåƒé€šè¿‡åˆ†æè„‘å›¾åƒæˆ–ä¿¡å·ï¼Œä¸ºäººç±»è®¤çŸ¥å’Œè¡Œä¸ºçš„æœºåˆ¶æä¾›è§è§£å’Œé¢„æµ‹å·¥å…·ã€‚æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸè¡¨ç°å‡ºç¨³å®šæ€§å’Œé«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›ï¼Œå…¶åœ¨è„‘æ•°æ®åˆ†ææ–¹é¢çš„åº”ç”¨ä¹Ÿæ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œå¦‚æ•°æ®å¢å¼ºã€ç–¾ç—…è¯Šæ–­å’Œè„‘è§£ç ç­‰ç¥ç»ä»»åŠ¡ã€‚æœ¬æ–‡æ¦‚è¿°äº†è¿‘æœŸå°†æ‰©æ•£æ¨¡å‹æ•´åˆåˆ°è®¡ç®—ç¥ç»æˆåƒä¸­çš„ç ”ç©¶åŠªåŠ›ã€‚æ–‡ç« ä»‹ç»äº†å¸¸è§çš„ç¥ç»æˆåƒæ•°æ®æ¨¡æ€ã€æ‰©æ•£å…¬å¼å’Œè°ƒèŠ‚æœºåˆ¶ã€‚æ¥ç€è®¨è®ºäº†æ‰©æ•£æ¨¡å‹çš„å»å™ªèµ·å§‹ç‚¹ã€æ¡ä»¶è¾“å…¥å’Œç”Ÿæˆç›®æ ‡çš„å˜ä½“æ˜¯å¦‚ä½•å‘å±•å’Œå¢å¼ºç‰¹å®šç¥ç»æˆåƒä»»åŠ¡çš„ã€‚æ¬²äº†è§£æ›´å¤šç›¸å…³ç ”ç©¶æ¦‚å†µï¼Œè¯·è®¿é—®å…¬å¼€ä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/JoeZhao527/dm4neuro%E3%80%82">https://github.com/JoeZhao527/dm4neuroã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è®¡ç®—ç¥ç»æˆåƒé€šè¿‡åˆ†æè„‘å›¾åƒå’Œä¿¡å·æ¥æ´å¯Ÿäººç±»è®¤çŸ¥å’Œè¡Œä¸ºçš„æœºåˆ¶ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç¥ç»æˆåƒä¸­çš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ï¼Œæ¶‰åŠæ•°æ®å¢å¼ºã€ç–¾ç—…è¯Šæ–­å’Œè„‘è§£ç ç­‰ä»»åŠ¡ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†è¿‘æœŸå°†æ‰©æ•£æ¨¡å‹æ•´åˆåˆ°è®¡ç®—ç¥ç»æˆåƒä¸­çš„ç ”ç©¶æ¦‚è§ˆã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†ç¥ç»æˆåƒæ•°æ®æ¨¡æ€ã€æ‰©æ•£å…¬å¼åŠè°ƒèŠ‚æœºåˆ¶ç­‰åŸºç¡€å†…å®¹ã€‚</li>
<li>æ–‡ä¸­è®¨è®ºäº†å¦‚ä½•é€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„å»å™ªèµ·å§‹ç‚¹ã€æ¡ä»¶è¾“å…¥å’Œç”Ÿæˆç›®æ ‡æ¥ä¼˜åŒ–å…¶åœ¨ç¥ç»æˆåƒä¸­çš„ç‰¹å®šä»»åŠ¡è¡¨ç°ã€‚</li>
<li>æ–‡ç« æä¾›äº†ä¸€ä¸ªå…¬å¼€ä»“åº“ä»¥æä¾›æ›´å…¨é¢çš„ç ”ç©¶æ¦‚å†µã€‚</li>
<li>æ­¤é¢†åŸŸä»æœ‰è®¸å¤šæŒç»­çš„ç ”ç©¶å’Œå‘å±•ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06552v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06552v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.06552v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Inverse-Problems-in-the-Exponential-Family"><a href="#Diffusion-Models-for-Inverse-Problems-in-the-Exponential-Family" class="headerlink" title="Diffusion Models for Inverse Problems in the Exponential Family"></a>Diffusion Models for Inverse Problems in the Exponential Family</h2><p><strong>Authors:Alessandro Micheli, MÃ©lodie Monod, Samir Bhatt</strong></p>
<p>Diffusion models have emerged as powerful tools for solving inverse problems, yet prior work has primarily focused on observations with Gaussian measurement noise, restricting their use in real-world scenarios. This limitation persists due to the intractability of the likelihood score, which until now has only been approximated in the simpler case of Gaussian likelihoods. In this work, we extend diffusion models to handle inverse problems where the observations follow a distribution from the exponential family, such as a Poisson or a Binomial distribution. By leveraging the conjugacy properties of exponential family distributions, we introduce the evidence trick, a method that provides a tractable approximation to the likelihood score. In our experiments, we demonstrate that our methodology effectively performs Bayesian inference on spatially inhomogeneous Poisson processes with intensities as intricate as ImageNet images. Furthermore, we demonstrate the real-world impact of our methodology by showing that it performs competitively with the current state-of-the-art in predicting malaria prevalence estimates in Sub-Saharan Africa. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºè§£å†³åé—®é¢˜çš„å¼ºå¤§å·¥å…·ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å…·æœ‰é«˜æ–¯æµ‹é‡å™ªå£°çš„è§‚å¯Ÿä¸Šï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚è¿™ä¸€é™åˆ¶æŒç»­å­˜åœ¨æ˜¯ç”±äºä¼¼ç„¶å¾—åˆ†éš¾ä»¥å¤„ç†ï¼Œç›´åˆ°ç°åœ¨åªåœ¨é«˜æ–¯ä¼¼çš„æƒ…å†µä¸‹è¿›è¡Œäº†è¿‘ä¼¼å¤„ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°å¤„ç†åé—®é¢˜ï¼Œå…¶ä¸­è§‚å¯Ÿç»“æœéµå¾ªæŒ‡æ•°æ—åˆ†å¸ƒï¼Œå¦‚æ³Šæ¾åˆ†å¸ƒæˆ–äºŒé¡¹åˆ†å¸ƒã€‚é€šè¿‡åˆ©ç”¨æŒ‡æ•°æ—åˆ†å¸ƒçš„å…±è½­æ€§è´¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯æ®æŠ€å·§ï¼Œè¿™æ˜¯ä¸€ç§æä¾›ä¼¼ç„¶å¾—åˆ†çš„å¯è¡Œè¿‘ä¼¼çš„æ–¹æ³•ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç©ºé—´éå‡åŒ€æ³Šæ¾è¿‡ç¨‹ä¸Šè¿›è¡Œè´å¶æ–¯æ¨æ–­çš„æœ‰æ•ˆæ€§ï¼Œå…¶å¼ºåº¦ä¸ImageNetå›¾åƒä¸€æ ·å¤æ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å±•ç¤ºå…¶ä¸å½“å‰é¢„æµ‹æ’’å“ˆæ‹‰ä»¥å—éæ´²ç–Ÿç–¾å‘ç—…ç‡ä¼°è®¡çš„æœ€å…ˆè¿›æ°´å¹³çš„ç«äº‰åŠ›ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°å®ä¸–ç•Œä¸­çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05994v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§£å†³åé—®é¢˜çš„é‡è¦å·¥å…·ï¼Œä½†å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å…·æœ‰é«˜æ–¯æµ‹é‡å™ªå£°çš„è§‚å¯Ÿä¸Šï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ç”±äºä¼¼ç„¶å€¼çš„ä¸å¯é¢„æµ‹æ€§ï¼Œè¿™ä¸€å±€é™æ€§ä¸€ç›´å­˜åœ¨ã€‚ç›´åˆ°ç°åœ¨ï¼Œä¼¼ç„¶å€¼ä»…åœ¨æ›´ç®€å•çš„é«˜æ–¯å¯èƒ½æ€§æƒ…å†µä¸‹è¿›è¡Œè¿‘ä¼¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°å¤„ç†åé—®é¢˜ï¼Œå…¶ä¸­è§‚å¯Ÿç»“æœéµå¾ªæŒ‡æ•°æ—åˆ†å¸ƒï¼Œå¦‚æ³Šæ¾åˆ†å¸ƒæˆ–äºŒé¡¹åˆ†å¸ƒã€‚é€šè¿‡åˆ©ç”¨æŒ‡æ•°æ—åˆ†å¸ƒçš„å…±è½­å±æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯æ®æŠ€å·§ï¼Œè¿™æ˜¯ä¸€ç§æä¾›ä¼¼ç„¶å€¼å¯è¡Œè¿‘ä¼¼çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰ä¸ImageNetå›¾åƒä¸€æ ·å¤æ‚çš„å¼ºåº¦çš„ç©ºé—´éå‡åŒ€æ³Šæ¾è¿‡ç¨‹ä¸Šæœ‰æ•ˆåœ°æ‰§è¡Œè´å¶æ–¯æ¨æ–­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å±•ç¤ºè¯¥æ–¹æ³•åœ¨é¢„æµ‹æ’’å“ˆæ‹‰ä»¥å—éæ´²ç–Ÿç–¾å‘ç—…ç‡æ–¹é¢çš„ç«äº‰åŠ›æ¥è¯æ˜å…¶ç°å®å½±å“åŠ›ï¼Œè¿™ä¸€ç«äº‰åŠ›å ªæ¯”ç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<p>ä»¥ä¸‹æ˜¯è¿™ç¯‡æ–‡ç« çš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>æ‰©æ•£æ¨¡å‹æ˜¯è§£å†³åé—®é¢˜çš„é‡è¦å·¥å…·ï¼Œä½†å…¶åœ¨ç°å®ä¸–ç•Œçš„è¿ç”¨å—åˆ°äº†ä¸€å®šé™åˆ¶ã€‚å…ˆå‰çš„ç›¸å…³ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤„ç†é«˜æ–¯æµ‹é‡å™ªå£°çš„æƒ…å†µä¸Šã€‚è¿™é¡¹å·¥ä½œæ‰“ç ´äº†è¿™ç§å±€é™ã€‚</li>
<li>ä¹‹å‰ç ”ç©¶çš„å±€é™æ€§æ˜¯ç”±äºä¼¼ç„¶å€¼çš„ä¸å¯é¢„æµ‹æ€§å¯¼è‡´çš„ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥è¯æ®æŠ€å·§è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œæä¾›äº†ä¸€ç§å¯è¡Œçš„æ–¹æ³•æ¥è¿‘ä¼¼ä¼¼ç„¶å€¼ã€‚</li>
<li>ç ”ç©¶äººå‘˜åˆ©ç”¨äº†æŒ‡æ•°æ—åˆ†å¸ƒçš„å…±è½­å±æ€§æ¥æ‰©å±•æ‰©æ•£æ¨¡å‹çš„åº”ç”¨èŒƒå›´ã€‚æŒ‡æ•°æ—åˆ†å¸ƒåŒ…æ‹¬æ³Šæ¾åˆ†å¸ƒå’ŒäºŒé¡¹åˆ†å¸ƒç­‰ç±»å‹ã€‚è¿™ä¸€åº”ç”¨å¯ä»¥æ‰©å±•åˆ°å…¶ä»–åˆ†å¸ƒå½¢å¼æ›´ä¸ºå¹¿æ³›çš„åé—®é¢˜é¢†åŸŸã€‚</li>
<li>ç ”ç©¶ä¸­çš„å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å¤æ‚å¼ºåº¦çš„ç©ºé—´éå‡åŒ€æ³Šæ¾è¿‡ç¨‹ä¸Šèƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰§è¡Œè´å¶æ–¯æ¨æ–­ã€‚è¿™æ„å‘³ç€æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤æ‚æ•°æ®å¹¶è¿›è¡Œå‡†ç¡®é¢„æµ‹ã€‚è¿™ç§æœ‰æ•ˆæ€§åœ¨è®¸å¤šé¢†åŸŸä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨è§£å†³éœ€è¦ç²¾ç¡®æ¨æ–­çš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ä»·å€¼ã€‚ä¾‹å¦‚å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ã€è¿›è¡Œé¢„æµ‹åˆ†æç­‰é¢†åŸŸçš„åº”ç”¨å°†ä»ä¸­å—ç›Šã€‚ç„¶è€Œå®éªŒä¹Ÿæš—ç¤ºäº†ä¸€äº›å±€é™æ€§å¯èƒ½å­˜åœ¨åº”ç”¨åœºæ™¯å…·ä½“æƒ…å½¢ä¼šæŒ‘æˆ˜è¯¥æ–¹æ³•çš„ç¨³å®šæ€§åœ¨åº”ç”¨åˆ°æ›´å¤§è§„æ¨¡å’Œæ›´å¤æ‚çš„åœºæ™¯ä¸­è¿˜éœ€è¦æ›´å¤šçš„å·¥ä½œæ¥æå‡æ¨¡å‹æ€§èƒ½å’Œé²æ£’æ€§å…·ä½“æ¥è¯´éœ€è¦åŠ å¼ºå»ºæ¨¡èƒ½åŠ›çš„æ¨å¹¿é€‚åº”æ€§å¯¹äºæ¯ä¸ªæ–°åº”ç”¨é¢†åŸŸéœ€è¦æ ¹æ®ç‰¹å®šæ•°æ®ç¯å¢ƒå’Œè¦æ±‚è¿›ä¸€æ­¥è°ƒæ•´æ¨¡å‹å’Œä¼˜åŒ–å‚æ•°é¿å…é€šç”¨çš„åº”ç”¨æ–¹æ¡ˆçš„ä¸è¶³é™åˆ¶ä»¥ä¾¿æé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ä»¥ä¾¿åœ¨æ›´å¤šçš„ç°å®åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨å’Œåˆ©ç”¨è¯¥æ–¹æ³•åœ¨ç°å®ä¸–ç•Œä¸­å®é™…åº”ç”¨è¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä»·å€¼å°½ç®¡å…¶å…·å¤‡è¯¸å¤šä¼˜ç‚¹ä½†æ˜¯ä»æœ‰å¾…äºè¿›ä¸€æ­¥çš„å®Œå–„å’Œæ”¹è¿›ä»¥åº”å¯¹æœªæ¥çš„æŒ‘æˆ˜å’Œé—®é¢˜</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05994v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05994v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Show-o-Turbo-Towards-Accelerated-Unified-Multimodal-Understanding-and-Generation"><a href="#Show-o-Turbo-Towards-Accelerated-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and   Generation"></a>Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Chenkai Xu, Xu Wang, Zhenyi Liao, Yishun Li, Tianqi Hou, Zhijie Deng</strong></p>
<p>There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zhijie-group/Show-o-Turbo">https://github.com/zhijie-group/Show-o-Turbo</a>. </p>
<blockquote>
<p>å…³äºæ„å»ºç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶å…´è¶£æ—¥ç›Šæµ“åšï¼Œå…¶ä¸­Show-oæ˜¯ä¸€ä¸ªå¼•äººæ³¨ç›®çš„ä»£è¡¨ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢éƒ½æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚Show-oçš„æ¨ç†è¿‡ç¨‹åŒ…æ‹¬é€æ­¥å»å™ªå›¾åƒæ ‡è®°å’Œè‡ªå›å½’è§£ç æ–‡æœ¬æ ‡è®°ï¼Œå› æ­¤ï¼Œä¸å¹¸çš„é­é‡æ˜¯ä»ä¸¤æ–¹é¢å¸¦æ¥çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»Show-o Turboæ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬é¦–å…ˆåŸºäºæ–‡æœ¬æ ‡è®°çš„å¹¶è¡Œè§£ç ï¼Œè¯†åˆ«å‡ºShow-oä¸­å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆçš„ç»Ÿä¸€å»å™ªè§†è§’ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºå°†ä¸€è‡´æ€§è’¸é¦ï¼ˆCDï¼‰è¿™ä¸€ç¼©çŸ­æ‰©æ•£æ¨¡å‹å»å™ªè¿‡ç¨‹çš„åˆæ ¼æ–¹æ³•ï¼Œæ‰©å±•åˆ°Show-oçš„å¤šæ¨¡æ€å»å™ªè½¨è¿¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½¨è¿¹åˆ†å‰²ç­–ç•¥å’Œä¸€ä¸ªè¯¾ç¨‹å­¦ä¹ ç¨‹åºæ¥æ”¹å–„è®­ç»ƒæ”¶æ•›æ€§ã€‚ç»éªŒä¸Šï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒShow-o Turboåœ¨ä¸ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰çš„æƒ…å†µä¸‹ï¼Œä»¥4ä¸ªé‡‡æ ·æ­¥éª¤è¾¾åˆ°GenEvalåˆ†æ•°0.625ï¼Œä¼˜äºåŸå§‹Show-oçš„8ä¸ªæ­¥éª¤å’ŒCFGï¼›åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢ï¼ŒShow-o Turboå®ç°äº†1.5å€çš„é€Ÿåº¦æå‡ï¼Œè€Œä¸ä¼šæ˜¾è‘—ç‰ºç‰²æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhijie-group/Show-o-Turbo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhijie-group/Show-o-Turboæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05415v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Show-o Turboé€šè¿‡å¼•å…¥ç»Ÿä¸€å»å™ªè§†è§’å’Œä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œæé«˜äº†Show-oæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„æ•ˆç‡ã€‚Show-o Turboé€šè¿‡è½¨è¿¹åˆ†å‰²ç­–ç•¥å’Œè¯¾ç¨‹å­¦ä¹ ç¨‹åºè¿›è¡Œæ”¹è¿›è®­ç»ƒæ”¶æ•›æ€§ï¼Œæœ€ç»ˆåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ä¾¿å®ç°äº†è¾ƒé«˜çš„ç”Ÿæˆè¯„ä»·åˆ†æ•°ï¼ŒåŒæ—¶åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†é€Ÿåº¦æå‡ï¼Œæ€§èƒ½æŸå¤±è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Show-o Turboæ˜¯Show-oæ¨¡å‹çš„ä¸€ä¸ªæ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>å¼•å…¥äº†ç»Ÿä¸€å»å™ªè§†è§’ï¼Œä¸ºå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæä¾›äº†ä¸€ä¸ªåŸºäºæ–‡æœ¬ä»¤ç‰Œå¹¶è¡Œè§£ç çš„åŸºç¡€ã€‚</li>
<li>é‡‡ç”¨ä¸€è‡´æ€§è’¸é¦ï¼ˆCDï¼‰æŠ€æœ¯æ¥ç¼©çŸ­æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°Show-oçš„å¤šæ¨¡æ€å»å™ªè½¨è¿¹ã€‚</li>
<li>å®ç°äº†è½¨è¿¹åˆ†å‰²ç­–ç•¥å’Œè¯¾ç¨‹å­¦ä¹ ç¨‹åºæ¥æé«˜è®­ç»ƒæ”¶æ•›æ€§ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒShow-o Turboåœ¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ä¸‹å®ç°äº†è¾ƒé«˜çš„ç”Ÿæˆè¯„ä»·åˆ†æ•°ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>Show-o Turboåœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05415v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05415v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05415v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05415v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05415v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beautiful-Images-Toxic-Words-Understanding-and-Addressing-Offensive-Text-in-Generated-Images"><a href="#Beautiful-Images-Toxic-Words-Understanding-and-Addressing-Offensive-Text-in-Generated-Images" class="headerlink" title="Beautiful Images, Toxic Words: Understanding and Addressing Offensive   Text in Generated Images"></a>Beautiful Images, Toxic Words: Understanding and Addressing Offensive   Text in Generated Images</h2><p><strong>Authors:Aditya Kumar, Tom Blanchard, Adam Dziedzic, Franziska Boenisch</strong></p>
<p>State-of-the-art visual generation models, such as Diffusion Models (DMs) and Vision Auto-Regressive Models (VARs), produce highly realistic images. While prior work has successfully mitigated Not Safe For Work (NSFW) content in the visual domain, we identify a novel threat: the generation of NSFW text embedded within images. This includes offensive language, such as insults, racial slurs, and sexually explicit terms, posing significant risks to users. We show that all state-of-the-art DMs (e.g., SD3, Flux, DeepFloyd IF) and VARs (e.g., Infinity) are vulnerable to this issue. Through extensive experiments, we demonstrate that existing mitigation techniques, effective for visual content, fail to prevent harmful text generation while substantially degrading benign text generation. As an initial step toward addressing this threat, we explore safety fine-tuning of the text encoder underlying major DM architectures using a customized dataset. Thereby, we suppress NSFW generation while preserving overall image and text generation quality. Finally, to advance research in this area, we introduce ToxicBench, an open-source benchmark for evaluating NSFW text generation in images. ToxicBench provides a curated dataset of harmful prompts, new metrics, and an evaluation pipeline assessing both NSFW-ness and generation quality. Our benchmark aims to guide future efforts in mitigating NSFW text generation in text-to-image models. </p>
<blockquote>
<p>æœ€å…ˆè¿›çš„è§†è§‰ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’Œè§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARsï¼‰ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜åº¦é€¼çœŸçš„å›¾åƒã€‚è™½ç„¶ä¹‹å‰çš„å·¥ä½œæˆåŠŸå‡è½»äº†å·¥ä½œåœºæ‰€ä¸å®œï¼ˆNSFWï¼‰å†…å®¹åœ¨è§†è§‰é¢†åŸŸçš„å¨èƒï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæ–°å‹å¨èƒï¼šåœ¨å›¾åƒä¸­åµŒå…¥NSFWæ–‡æœ¬çš„ç”Ÿæˆã€‚è¿™åŒ…æ‹¬å†’çŠ¯æ€§çš„è¯­è¨€ï¼Œå¦‚ä¾®è¾±ã€ç§æ—æ­§è§†å’Œæ€§æ˜ç¡®æœ¯è¯­ï¼Œå¯¹ç”¨æˆ·æ„æˆé‡å¤§é£é™©ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ‰€æœ‰æœ€å…ˆè¿›çš„DMsï¼ˆä¾‹å¦‚SD3ã€Fluxã€DeepFloyd IFï¼‰å’ŒVARsï¼ˆä¾‹å¦‚Infinityï¼‰éƒ½é¢ä¸´è¿™ä¸€é—®é¢˜çš„å¨èƒã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ç°æœ‰çš„å‡è½»æŠ€æœ¯ï¼Œå¯¹äºè§†è§‰å†…å®¹æ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨é˜²æ­¢æœ‰å®³æ–‡æœ¬ç”Ÿæˆæ—¶å´å¤±è´¥äº†ï¼ŒåŒæ—¶æå¤§åœ°æŸå®³äº†è‰¯æ€§æ–‡æœ¬ç”Ÿæˆã€‚ä½œä¸ºè§£å†³è¿™ä¸€å¨èƒçš„åˆæ­¥æ­¥éª¤ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¯¹ä¸»è¦DMæ¶æ„çš„åº•å±‚æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œå®‰å…¨å¾®è°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ä¿æŒæ•´ä½“å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼ŒæŠ‘åˆ¶äº†NSFWçš„ç”Ÿæˆã€‚æœ€åï¼Œä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ToxicBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å›¾åƒä¸­NSFWæ–‡æœ¬ç”Ÿæˆçš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚ToxicBenchæä¾›äº†ä¸€ä¸ªæœ‰å®³æç¤ºçš„ç²¾é€‰æ•°æ®é›†ã€æ–°æŒ‡æ ‡å’Œè¯„ä¼°æµç¨‹ï¼Œè¯„ä¼°NSFWç¨‹åº¦å’Œç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ—¨åœ¨æŒ‡å¯¼æœªæ¥åœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­ç¼“è§£NSFWæ–‡æœ¬ç”Ÿæˆçš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05066v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’Œè§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARsï¼‰åœ¨ç”Ÿæˆå›¾åƒæ—¶å­˜åœ¨çš„æ–°å‹å®‰å…¨éšæ‚£ï¼Œå³ç”Ÿæˆçš„å›¾åƒä¸­åŒ…å«ä¸å®‰å…¨çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚ä¾®è¾±æ€§è¯­è¨€ã€ç§æ—æ­§è§†å’Œè‰²æƒ…è¯æ±‡ç­‰ã€‚ç°æœ‰é’ˆå¯¹è§†è§‰å†…å®¹çš„ç¼“è§£æŠ€æœ¯æ— æ³•æœ‰æ•ˆé˜²æ­¢è¿™ç§æœ‰å®³æ–‡æœ¬ç”Ÿæˆï¼ŒåŒæ—¶è¿˜ä¼šå¯¹è‰¯æ€§æ–‡æœ¬ç”Ÿæˆäº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« åˆæ­¥æ¢è®¨äº†é’ˆå¯¹ä¸»è¦DMæ¶æ„æ–‡æœ¬ç¼–ç å™¨çš„å®‰å…¨å¾®è°ƒæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶æ•°æ®é›†è¿›è¡ŒæŠ‘åˆ¶æœ‰å®³æ–‡æœ¬ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆçš„æ€»ä½“è´¨é‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†é¢å‘è¯„ä¼°å›¾åƒä¸­NSFWæ–‡æœ¬ç”Ÿæˆçš„å¼€æºåŸºå‡†æµ‹è¯•å¹³å°â€”â€”ToxicBenchï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å·¥ä½œæä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’Œè§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARsï¼‰åœ¨ç”Ÿæˆå›¾åƒæ—¶å­˜åœ¨ç”Ÿæˆä¸å®‰å…¨æ–‡æœ¬å†…å®¹ï¼ˆNSFWï¼‰çš„éšæ‚£ã€‚</li>
<li>ç°è¡Œçš„å›¾åƒå†…å®¹è¿‡æ»¤æŠ€æœ¯æ— æ³•æœ‰æ•ˆé˜²æ­¢æœ‰å®³æ–‡æœ¬ç”Ÿæˆï¼Œä¸”å¯èƒ½å¯¹è‰¯æ€§æ–‡æœ¬ç”Ÿæˆäº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>é€šè¿‡å®‰å…¨å¾®è°ƒä¸»è¦DMæ¶æ„çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå¯ä»¥æŠ‘åˆ¶NSFWç”Ÿæˆå¹¶ä¿æŒå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆçš„æ€»ä½“è´¨é‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå®šåˆ¶æ•°æ®é›†è¿›è¡Œå®‰å…¨å¾®è°ƒã€‚</li>
<li>ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°å›¾åƒä¸­NSFWæ–‡æœ¬ç”Ÿæˆçš„å¼€æºåŸºå‡†æµ‹è¯•å¹³å°â€”â€”ToxicBenchã€‚</li>
<li>ToxicBenchæä¾›äº†æœ‰å®³æç¤ºçš„æ•°æ®åº“ã€æ–°æŒ‡æ ‡å’Œè¯„ä¼°ç®¡é“ï¼Œç”¨äºè¯„ä¼°NSFWç¨‹åº¦å’Œç”Ÿæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05066v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05066v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05066v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05066v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.05066v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-Consistent-and-Controllable-Image-Synthesis-for-Face-Editing"><a href="#Towards-Consistent-and-Controllable-Image-Synthesis-for-Face-Editing" class="headerlink" title="Towards Consistent and Controllable Image Synthesis for Face Editing"></a>Towards Consistent and Controllable Image Synthesis for Face Editing</h2><p><strong>Authors:Mengting Wei, Tuomas Varanka, Yante Li, Xingxun Jiang, Huai-Qian Khor, Guoying Zhao</strong></p>
<p>Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/weimengting/RigFace">https://github.com/weimengting/RigFace</a>. </p>
<blockquote>
<p>é¢éƒ¨ç¼–è¾‘æ–¹æ³•å¯¹äºè™šæ‹ŸåŒ–èº«ã€æ•°å­—äººç±»åˆæˆå’Œèº«ä»½ä¿ç•™ç­‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿä¸ŠåŸºäºGANæŠ€æœ¯æ„å»ºï¼Œè€Œæœ€è¿‘çš„å…³æ³¨ç„¦ç‚¹å·²è½¬å‘æ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åœ¨å›¾åƒé‡å»ºæ–¹é¢çš„æˆåŠŸã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹åœ¨æ§åˆ¶ç‰¹å®šå±æ€§å’Œä¿æŒå…¶ä»–æœªæ”¹å˜å±æ€§çš„ä¸€è‡´æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯èº«ä»½ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶æ–¹ä¾¿é¢éƒ¨å›¾åƒçš„ç¼–è¾‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨Stable-Diffusionï¼ˆSDï¼‰æ¨¡å‹å’Œç²—ç•¥çš„3Dé¢éƒ¨æ¨¡å‹çš„åŠ›é‡æ¥æ§åˆ¶è‚–åƒç…§ç‰‡çš„å…‰çº¿ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿ã€‚æˆ‘ä»¬å‘ç°æ­¤ä»»åŠ¡ä¸»è¦æ¶‰åŠèƒŒæ™¯ã€èº«ä»½å’Œé¢éƒ¨å±æ€§çš„ç»„åˆï¼Œç›®çš„åœ¨äºè¿›è¡Œç¼–è¾‘ã€‚æˆ‘ä»¬åŠªåŠ›å……åˆ†è§£å¼€è¿™äº›å› ç´ çš„æ§åˆ¶ï¼Œä»¥å®ç°é¢éƒ¨ç¼–è¾‘çš„ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºRigFaceï¼ŒåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š1ï¼‰ç©ºé—´å±æ€§ç¼–ç å™¨æä¾›ç²¾ç¡®ä¸”è§£è€¦çš„èƒŒæ™¯ã€å§¿åŠ¿ã€è¡¨æƒ…å’Œå…‰çº¿æ¡ä»¶ï¼›2ï¼‰é«˜ä¸€è‡´æ€§FaceFusionæ–¹æ³•å°†ä»èº«ä»½ç¼–ç å™¨æå–çš„èº«ä»½ç‰¹å¾è½¬ç§»åˆ°é¢„è®­ç»ƒSDæ¨¡å‹çš„å»å™ªUNetä¸­ï¼›3ï¼‰å±æ€§è§¦å‘å™¨å°†è¿™äº›æ¡ä»¶æ³¨å…¥å»å™ªUNetä¸­ã€‚ä¸ç°æœ‰çš„é¢éƒ¨ç¼–è¾‘æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨èº«ä»½ä¿ç•™å’Œé€¼çœŸåº¦æ–¹é¢è¾¾åˆ°äº†ç›¸å½“æˆ–æ›´é«˜çš„æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/weimengting/RigFace%E3%80%82">https://github.com/weimengting/RigFaceã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02465v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºStable-Diffusionæ¨¡å‹å’Œç²—ç•¥3Däººè„¸æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶è‚–åƒç…§ç‰‡çš„å…‰çº¿ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿çš„ç¼–è¾‘ã€‚è¯¥æ–¹æ³•ç»“åˆç›®æ ‡èƒŒæ™¯ã€èº«ä»½å’Œäººè„¸å±æ€§è¿›è¡Œç¼–è¾‘ï¼Œé€šè¿‡ç©ºé—´å±æ€§ç¼–ç å™¨æä¾›ç²¾ç¡®ä¸”è§£è€¦çš„æ¡ä»¶ï¼Œé«˜ä¸€è‡´æ€§FaceFusionæ–¹æ³•å°†èº«ä»½ç‰¹å¾ä»èº«ä»½ç¼–ç å™¨è½¬ç§»åˆ°é¢„è®­ç»ƒçš„SDæ¨¡å‹çš„å»å™ªUNetä¸­ï¼Œå¹¶å®ç°æ¡ä»¶æ³¨å…¥ã€‚è¯¥æ¨¡å‹åœ¨èº«ä»½ä¿ç•™å’Œé€¼çœŸåº¦æ–¹é¢ä¸ç°æœ‰é¢éƒ¨ç¼–è¾‘æ¨¡å‹ç›¸æ¯”å…·æœ‰ç›¸å½“æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ä»‹ç»äº†åŸºäºStable-Diffusionæ¨¡å‹å’Œç²—ç•¥3Däººè„¸æ¨¡å‹çš„é¢éƒ¨ç¼–è¾‘æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç›®æ ‡èƒŒæ™¯ã€èº«ä»½å’Œé¢éƒ¨å±æ€§è¿›è¡Œç¼–è¾‘ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æ§åˆ¶ç‰¹å®šå±æ€§å’Œä¿æŒèº«ä»½ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç©ºé—´å±æ€§ç¼–ç å™¨æä¾›ç²¾ç¡®ä¸”è§£è€¦çš„æ¡ä»¶ï¼ŒåŒ…æ‹¬èƒŒæ™¯ã€å§¿åŠ¿ã€è¡¨è¾¾å’Œå…‰çº¿ã€‚</li>
<li>é‡‡ç”¨é«˜ä¸€è‡´æ€§FaceFusionæ–¹æ³•ï¼Œå°†èº«ä»½ç‰¹å¾ä»èº«ä»½ç¼–ç å™¨è½¬ç§»åˆ°é¢„è®­ç»ƒSDæ¨¡å‹çš„å»å™ªUNetä¸­ã€‚</li>
<li>å®ç°äº†æ¡ä»¶æ³¨å…¥çš„Attribute Riggerï¼Œå°†å„ç§æ¡ä»¶æ³¨å…¥åˆ°å»å™ªUNetä¸­ã€‚</li>
<li>æ¨¡å‹åœ¨èº«ä»½ä¿ç•™å’Œé€¼çœŸåº¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰é¢éƒ¨ç¼–è¾‘æ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.02465v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.02465v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.02465v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.02465v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.02465v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.02465v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Compressed-Image-Generation-with-Denoising-Diffusion-Codebook-Models"><a href="#Compressed-Image-Generation-with-Denoising-Diffusion-Codebook-Models" class="headerlink" title="Compressed Image Generation with Denoising Diffusion Codebook Models"></a>Compressed Image Generation with Denoising Diffusion Codebook Models</h2><p><strong>Authors:Guy Ohayon, Hila Manor, Tomer Michaeli, Michael Elad</strong></p>
<p>We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰çš„æ–°å‹ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬åŠå…¶æ— æŸå‹ç¼©æ¯”ç‰¹æµè¡¨ç¤ºã€‚è¿™æ˜¯é€šè¿‡ç”¨é¢„å®šä¹‰çš„å›ºå®šç‹¬ç«‹åŒåˆ†å¸ƒé«˜æ–¯å‘é‡çš„ä»£ç æœ¬ä¸­çš„å™ªå£°æ ·æœ¬æ›¿æ¢åå‘æ‰©æ•£ä¸­çš„æ ‡å‡†é«˜æ–¯å™ªå£°é‡‡æ ·æ¥å®ç°çš„ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå³å»å™ªæ‰©æ•£ä»£ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ï¼Œå³ä½¿åœ¨æå°çš„ä»£ç æœ¬ä¸­ï¼Œä¹Ÿèƒ½ä¿æŒæ ‡å‡†DDMçš„æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨DDCMï¼Œä»ä»£ç æœ¬ä¸­é€‰æ‹©ä¸ç»™å®šå›¾åƒæœ€åŒ¹é…çš„å™ªå£°ï¼Œå°†æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºä¸€ç§é«˜æ•ˆçš„æœ‰æŸå›¾åƒç¼–ç ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ„ŸçŸ¥å›¾åƒå‹ç¼©ç»“æœã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œé€šè¿‡è®¾ç½®å…¶ä»–å™ªå£°é€‰æ‹©è§„åˆ™ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„å‹ç¼©æ–¹æ³•æ‰©å±•åˆ°ä»»ä½•æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒæ¢å¤ï¼‰ï¼Œå…¶ä¸­ç”Ÿæˆçš„å›¾åƒä¸å…¶æµ“ç¼©çš„æ¯”ç‰¹æµè¡¨ç¤ºè”åˆç”Ÿæˆã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜æä¾›äº†æ‰€æå‡ºçš„æœ‰æ¡ä»¶å‹ç¼©ç”Ÿæˆæ–¹æ¡ˆçš„æ•°å­¦è§£é‡Šï¼Œä¸æ‰€è€ƒè™‘ä»»åŠ¡çš„åŸºäºåˆ†æ•°çš„åéªŒé‡‡æ ·å™¨å»ºç«‹äº†è”ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01189v3">PDF</a> Code and demo are available at <a target="_blank" rel="noopener" href="https://ddcm-2025.github.io/">https://ddcm-2025.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬åŠå…¶æ— æŸå‹ç¼©æ¯”ç‰¹æµè¡¨ç¤ºã€‚é€šè¿‡ç”¨é¢„å®šä¹‰çš„ç‹¬ç«‹åŒåˆ†å¸ƒé«˜æ–¯å‘é‡ç æœ¬ä¸­çš„å™ªå£°æ ·æœ¬æ›¿æ¢åå‘æ‰©æ•£ä¸­çš„æ ‡å‡†é«˜æ–¯å™ªå£°æ ·æœ¬ï¼Œæˆ‘ä»¬å®ç°äº†è¢«ç§°ä¸ºå»å™ªæ‰©æ•£ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰çš„æ–¹æ³•ã€‚å³ä½¿å¯¹äºæå°çš„ç æœ¬ï¼ŒDDCMä¹Ÿèƒ½ä¿æŒä¸æ ‡å‡†DDMç›¸å½“çš„æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨DDCMï¼ŒæŒ‘é€‰ä¸ç»™å®šå›¾åƒæœ€åŒ¹é…çš„å™ªå£°ï¼Œå°†ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºé«˜æ•ˆçš„æŸå¤±å›¾åƒå‹ç¼©ç¼–ç å™¨ï¼Œå®ç°äº†å…ˆè¿›çš„æ„ŸçŸ¥å›¾åƒå‹ç¼©ç»“æœã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾ç½®å…¶ä»–å™ªå£°é€‰æ‹©è§„åˆ™ï¼Œæˆ‘ä»¬å°†å‹ç¼©æ–¹æ³•æ‰©å±•åˆ°ä»»ä½•æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒæ¢å¤ï¼‰ï¼Œç”Ÿæˆçš„å›¾åƒä¸å…¶æµ“ç¼©çš„æ¯”ç‰¹æµè¡¨ç¤ºè”åˆäº§ç”Ÿã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜æä¾›äº†å¯¹æ‰€æå‡ºçš„å‹ç¼©æ¡ä»¶ç”Ÿæˆæ–¹æ¡ˆè¿›è¡Œæ•°å­¦è§£é‡Šï¼Œå»ºç«‹äº†æ‰€è€ƒè™‘ä»»åŠ¡çš„å¾—åˆ†è¿‘ä¼¼åéªŒé‡‡æ ·å™¨çš„è”ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰çš„æ–°ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬åŠå…¶æ— æŸå‹ç¼©è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é¢„å®šä¹‰ç æœ¬ä¸­çš„å™ªå£°æ ·æœ¬æ›¿æ¢æ ‡å‡†é«˜æ–¯å™ªå£°é‡‡æ ·ï¼Œå®ç°äº†å»å™ªæ‰©æ•£ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ã€‚</li>
<li>DDCMåœ¨æå°ç æœ¬ä¸‹ä»èƒ½ä¿æŒæ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>DDCMå¯è½¬åŒ–ä¸ºé«˜æ•ˆçš„æŸå¤±å›¾åƒå‹ç¼©ç¼–ç å™¨ï¼Œå®ç°å…ˆè¿›æ„ŸçŸ¥å›¾åƒå‹ç¼©ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•å¯æ‰©å±•è‡³ä»»ä½•æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¦‚å›¾åƒæ¢å¤ã€‚</li>
<li>æå‡ºçš„å‹ç¼©æ¡ä»¶ç”Ÿæˆæ–¹æ¡ˆæœ‰æ•°å­¦è§£é‡Šï¼Œä¸å¾—åˆ†è¿‘ä¼¼åéªŒé‡‡æ ·å™¨å»ºç«‹è”ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.01189v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.01189v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.01189v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.01189v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.01189v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2502.01189v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Grounding-Text-to-Image-Diffusion-Models-for-Controlled-High-Quality-Image-Generation"><a href="#Grounding-Text-to-Image-Diffusion-Models-for-Controlled-High-Quality-Image-Generation" class="headerlink" title="Grounding Text-to-Image Diffusion Models for Controlled High-Quality   Image Generation"></a>Grounding Text-to-Image Diffusion Models for Controlled High-Quality   Image Generation</h2><p><strong>Authors:Ahmad SÃ¼leyman, GÃ¶ksel Biricik</strong></p>
<p>Text-to-image (T2I) generative diffusion models have demonstrated outstanding performance in synthesizing diverse, high-quality visuals from text captions. Several layout-to-image models have been developed to control the generation process by utilizing a wide range of layouts, such as segmentation maps, edges, and human keypoints. In this work, we propose ObjectDiffusion, a model that conditions T2I diffusion models on semantic and spatial grounding information, enabling the precise rendering and placement of desired objects in specific locations defined by bounding boxes. To achieve this, we make substantial modifications to the network architecture introduced in ControlNet to integrate it with the grounding method proposed in GLIGEN. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model improves the precision and quality of controllable image generation, achieving an AP$_{\text{50}}$ of 46.6, an AR of 44.5, and an FID of 19.8, outperforming the current SOTA model trained on open-source datasets across all three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding capabilities in closed-set and open-set vocabulary settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple detailed objects in varying sizes, forms, and locations. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºä»æ–‡æœ¬æè¿°åˆæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒçš„å‡ºè‰²æ€§èƒ½ã€‚ä¸ºäº†æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œå·²ç»å¼€å‘äº†å‡ ç§å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨å„ç§å¸ƒå±€ï¼Œå¦‚åˆ†å‰²å›¾ã€è¾¹ç¼˜å’Œäººç±»å…³é”®ç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ObjectDiffusionæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†T2Iæ‰©æ•£æ¨¡å‹å»ºç«‹åœ¨è¯­ä¹‰å’Œç©ºé—´å®šä½ä¿¡æ¯ä¹‹ä¸Šï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šä½ç½®ç²¾ç¡®å‘ˆç°å’Œæ”¾ç½®æ‰€éœ€å¯¹è±¡ï¼Œè¿™äº›ä½ç½®ç”±è¾¹ç•Œæ¡†å®šä¹‰ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯¹ControlNetä¸­å¼•å…¥çš„ç½‘ç»œæ¶æ„è¿›è¡Œäº†é‡å¤§ä¿®æ”¹ï¼Œå°†å…¶ä¸GLIGENä¸­æå‡ºçš„å®šä½æ–¹æ³•ç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨COCO2017è®­ç»ƒæ•°æ®é›†ä¸Šå¯¹ObjectDiffusionè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨COCO2017éªŒè¯æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ¨¡å‹æé«˜äº†å¯æ§å›¾åƒç”Ÿæˆçš„ç²¾åº¦å’Œè´¨é‡ï¼Œåœ¨ä¸‰ä¸ªæŒ‡æ ‡ä¸Šå‡è¶…è¿‡äº†å½“å‰åœ¨å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹ï¼šä»¥46.6çš„AP50ï¼Œ44.5çš„ARå’Œè¾ƒä½çš„FIDã€‚ObjectDiffusionè¡¨ç°å‡ºç‹¬ç‰¹çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåˆæˆå¤šæ ·ã€é«˜è´¨é‡ã€é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œæ— ç¼ç¬¦åˆè¯­ä¹‰å’Œç©ºé—´æ§åˆ¶å¸ƒå±€ã€‚åœ¨å®šæ€§å’Œå®šé‡æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒObjectDiffusionåœ¨å°é—­å’Œå¼€æ”¾è¯æ±‡è®¾ç½®çš„å„ç§ä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºå“è¶Šçš„å®šä½èƒ½åŠ›ã€‚å®šæ€§è¯„ä¼°éªŒè¯äº†ObjectDiffusionç”Ÿæˆå¤šç§ä¸åŒå¤§å°ã€å½¢çŠ¶å’Œä½ç½®çš„è¯¦ç»†å¯¹è±¡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09194v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºObjectDiffusionçš„æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è¯­ä¹‰å’Œç©ºé—´å®šä½ä¿¡æ¯ï¼Œèƒ½åœ¨ç‰¹å®šè¾¹ç•Œæ¡†å†…ç²¾ç¡®æ¸²æŸ“å’Œæ”¾ç½®æ‰€éœ€å¯¹è±¡ã€‚æ¨¡å‹åœ¨COCO2017è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨éªŒè¯æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†é«˜ç²¾ç¡®åº¦ã€é«˜è´¨é‡çš„å¯æ§å›¾åƒç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ObjectDiffusionæ˜¯ä¸€ä¸ªç»“åˆè¯­ä¹‰å’Œç©ºé—´å®šä½ä¿¡æ¯çš„æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹èƒ½åœ¨ç‰¹å®šè¾¹ç•Œæ¡†å†…ç²¾ç¡®æ¸²æŸ“å’Œæ”¾ç½®æ‰€éœ€å¯¹è±¡ã€‚</li>
<li>ObjectDiffusionåœ¨COCO2017è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨éªŒè¯æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¡¨ç°ã€‚</li>
<li>ç›¸æ¯”å½“å‰åœ¨å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒçš„å…¶ä»–æ¨¡å‹ï¼ŒObjectDiffusionåœ¨ä¸‰ä¸ªæŒ‡æ ‡ä¸Šéƒ½è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>ObjectDiffusionèƒ½åˆæˆå¤šæ ·ã€é«˜è´¨é‡ã€é«˜ä¿çœŸä¸”ç¬¦åˆè¯­ä¹‰å’Œç©ºé—´æ§åˆ¶å¸ƒå±€çš„å›¾åƒã€‚</li>
<li>åœ¨å°é—­è¯æ±‡é›†å’Œå¼€æ”¾è¯æ±‡é›†ç¯å¢ƒä¸‹ï¼ŒObjectDiffusionåœ¨å¹¿æ³›ä¸Šä¸‹æ–‡ä¸­çš„å®šä½èƒ½åŠ›æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2501.09194v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2501.09194v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2501.09194v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2501.09194v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Guided-and-Variance-Corrected-Fusion-with-One-shot-Style-Alignment-for-Large-Content-Image-Generation"><a href="#Guided-and-Variance-Corrected-Fusion-with-One-shot-Style-Alignment-for-Large-Content-Image-Generation" class="headerlink" title="Guided and Variance-Corrected Fusion with One-shot Style Alignment for   Large-Content Image Generation"></a>Guided and Variance-Corrected Fusion with One-shot Style Alignment for   Large-Content Image Generation</h2><p><strong>Authors:Shoukun Sun, Min Xian, Tiankai Yao, Fei Xu, Luca Capriotti</strong></p>
<p>Producing large images using small diffusion models is gaining increasing popularity, as the cost of training large models could be prohibitive. A common approach involves jointly generating a series of overlapped image patches and obtaining large images by merging adjacent patches. However, results from existing methods often exhibit noticeable artifacts, e.g., seams and inconsistent objects and styles. To address the issues, we proposed Guided Fusion (GF), which mitigates the negative impact from distant image regions by applying a weighted average to the overlapping regions. Moreover, we proposed Variance-Corrected Fusion (VCF), which corrects data variance at post-averaging, generating more accurate fusion for the Denoising Diffusion Probabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA), which generates a coherent style for large images by adjusting the initial input noise without adding extra computational burden. Extensive experiments demonstrated that the proposed fusion methods improved the quality of the generated image significantly. The proposed method can be widely applied as a plug-and-play module to enhance other fusion-based methods for large image generation. Code: <a target="_blank" rel="noopener" href="https://github.com/TitorX/GVCFDiffusion">https://github.com/TitorX/GVCFDiffusion</a> </p>
<blockquote>
<p>ä½¿ç”¨å°å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤§å‹å›¾åƒæ­£å˜å¾—è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› ä¸ºè®­ç»ƒå¤§å‹æ¨¡å‹çš„æˆæœ¬å¯èƒ½å¾ˆé«˜ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡è”åˆç”Ÿæˆä¸€ç³»åˆ—é‡å çš„å›¾åƒå—ï¼Œå¹¶é€šè¿‡åˆå¹¶ç›¸é‚»çš„å›¾åƒå—æ¥è·å¾—å¤§å‹å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„ç»“æœå¾€å¾€ä¼šå‡ºç°æ˜æ˜¾çš„ä¼ªå½±ï¼Œä¾‹å¦‚æ¥ç¼å’Œä¸ä¸€è‡´çš„ç‰©ä½“å’Œé£æ ¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼•å¯¼èåˆï¼ˆGFï¼‰ï¼Œé€šè¿‡åº”ç”¨åŠ æƒå¹³å‡æ¥å‡è½»è¿œè·ç¦»å›¾åƒåŒºåŸŸçš„ä¸åˆ©å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ–¹å·®æ ¡æ­£èåˆï¼ˆVCFï¼‰ï¼Œåœ¨å¹³å‡åæ ¡æ­£æ•°æ®æ–¹å·®ï¼Œä¸ºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®çš„èåˆã€‚å¦å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€æ¬¡æ€§é£æ ¼å¯¹é½ï¼ˆSAï¼‰ï¼Œé€šè¿‡è°ƒæ•´åˆå§‹è¾“å…¥å™ªå£°ä¸ºå¤§å‹å›¾åƒç”Ÿæˆè¿è´¯çš„é£æ ¼ï¼Œä¸”ä¸å¢åŠ é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„èåˆæ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå³æ’å³ç”¨æ¨¡å—ï¼Œä»¥å¢å¼ºå…¶ä»–åŸºäºèåˆçš„å¤§å‹å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/TitorX/GVCFDiffusion">https://github.com/TitorX/GVCFDiffusion</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12771v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨å°å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤§å‹å›¾åƒçš„æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åŒ…æ‹¬ Guided Fusionï¼ˆGFï¼‰ã€Variance-Corrected Fusionï¼ˆVCFï¼‰å’Œ one-shot Style Alignmentï¼ˆSAï¼‰ç­‰æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å¯æ˜¾è‘—æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ï¼Œå¹¶å¯å¹¿æ³›åº”ç”¨äºå…¶ä»–èåˆæ–¹æ³•ä»¥å¢å¼ºå¤§å‹å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å°å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤§å‹å›¾åƒè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› ä¸ºè®­ç»ƒå¤§å‹æ¨¡å‹çš„æˆæœ¬å¯èƒ½å¾ˆé«˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç”Ÿæˆå›¾åƒæ—¶é€šå¸¸ä¼šå‡ºç°æ˜æ˜¾çš„ç‘•ç–µï¼Œä¾‹å¦‚æ¥ç¼å’Œä¸è¿è´¯çš„å¯¹è±¡å’Œé£æ ¼ã€‚</li>
<li>Guided Fusionï¼ˆGFï¼‰æ–¹æ³•é€šè¿‡åº”ç”¨åŠ æƒå¹³å‡æ¥å‡è½»è¿œè·ç¦»å›¾åƒåŒºåŸŸçš„ä¸åˆ©å½±å“ã€‚</li>
<li>Variance-Corrected Fusionï¼ˆVCFï¼‰æ–¹æ³•åœ¨å¹³å‡åè¿›è¡Œæ•°æ®æ–¹å·®æ ¡æ­£ï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„èåˆç»“æœã€‚</li>
<li>one-shot Style Alignmentï¼ˆSAï¼‰æ–¹æ³•é€šè¿‡è°ƒæ•´åˆå§‹è¾“å…¥å™ªå£°ä¸ºå¤§å‹å›¾åƒç”Ÿæˆä¸€è‡´çš„é£æ ¼ï¼Œä¸”ä¸å¢åŠ é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>æå‡ºçš„èåˆæ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºå³æ’å³ç”¨æ¨¡å—å¹¿æ³›åº”ç”¨äºå…¶ä»–èåˆæ–¹æ³•ï¼Œä»¥å¢å¼ºå¤§å‹å›¾åƒçš„ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_4_2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Analyzing-and-Mitigating-Model-Collapse-in-Rectified-Flow-Models"><a href="#Analyzing-and-Mitigating-Model-Collapse-in-Rectified-Flow-Models" class="headerlink" title="Analyzing and Mitigating Model Collapse in Rectified Flow Models"></a>Analyzing and Mitigating Model Collapse in Rectified Flow Models</h2><p><strong>Authors:Huminhao Zhu, Fangyikang Wang, Tianyu Ding, Qing Qu, Zhihui Zhu</strong></p>
<p>Training with synthetic data is becoming increasingly inevitable as synthetic content proliferates across the web, driven by the remarkable performance of recent deep generative models. This reliance on synthetic data can also be intentional, as seen in Rectified Flow models, whose Reflow method iteratively uses self-generated data to straighten the flow and improve sampling efficiency. However, recent studies have shown that repeatedly training on self-generated samples can lead to model collapse (MC), where performance degrades over time. Despite this, most recent work on MC either focuses on empirical observations or analyzes regression problems and maximum likelihood objectives, leaving a rigorous theoretical analysis of reflow methods unexplored. In this paper, we aim to fill this gap by providing both theoretical analysis and practical solutions for addressing MC in diffusion&#x2F;flow models. We begin by studying Denoising Autoencoders and prove performance degradation when DAEs are iteratively trained on their own outputs. To the best of our knowledge, we are the first to rigorously analyze model collapse in DAEs and, by extension, in diffusion models and Rectified Flow. Our analysis and experiments demonstrate that rectified flow also suffers from MC, leading to potential performance degradation in each reflow step. Additionally, we prove that incorporating real data can prevent MC during recursive DAE training, supporting the recent trend of using real data as an effective approach for mitigating MC. Building on these insights, we propose a novel Real-data Augmented Reflow and a series of improved variants, which seamlessly integrate real data into Reflow training by leveraging reverse flow. Empirical evaluations on standard image benchmarks confirm that RA Reflow effectively mitigates model collapse, preserving high-quality sample generation even with fewer sampling steps. </p>
<blockquote>
<p>éšç€åˆæˆå†…å®¹åœ¨ç½‘ä¸Šä¸æ–­å¢å¤šï¼Œè®­ç»ƒåˆæˆæ•°æ®å˜å¾—ä¸å¯é¿å…ï¼Œè¿™å¾—ç›Šäºæœ€è¿‘çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å‡ºè‰²æ€§èƒ½ã€‚è¿™ç§å¯¹åˆæˆæ•°æ®çš„ä¾èµ–ä¹Ÿå¯èƒ½æ˜¯æœ‰æ„ä¸ºä¹‹ï¼Œå¦‚åœ¨Rectified Flowæ¨¡å‹ä¸­ï¼Œå…¶Reflowæ–¹æ³•è¿­ä»£ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®æ¥çº æ­£æµå¹¶æ”¹å–„é‡‡æ ·æ•ˆç‡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è‡ªæˆ‘ç”Ÿæˆçš„æ ·æœ¬ä¸Šåå¤è®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹å´©æºƒï¼ˆMCï¼‰ï¼Œéšç€æ—¶é—´æ¨ç§»ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå…³äºMCçš„å¤§å¤šæ•°æœ€æ–°å·¥ä½œè¦ä¹ˆé›†ä¸­åœ¨ç»éªŒè§‚å¯Ÿä¸Šï¼Œè¦ä¹ˆåˆ†æå›å½’é—®é¢˜å’Œæœ€å¤§ä¼¼ç„¶ç›®æ ‡ï¼Œç•™ä¸‹å¯¹Reflowæ–¹æ³•çš„ä¸¥æ ¼ç†è®ºåˆ†æå°šæœªæ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä¸ºæ‰©æ•£&#x2F;æµæ¨¡å‹ä¸­çš„MCæä¾›ç†è®ºåˆ†æå’Œå®é™…è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶å»å™ªè‡ªåŠ¨ç¼–ç å™¨ï¼ˆDAEsï¼‰ï¼Œå¹¶è¯æ˜åœ¨å…¶è‡ªèº«è¾“å‡ºä¸Šè¿­ä»£è®­ç»ƒæ—¶æ€§èƒ½ä¼šä¸‹é™ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡ä¸¥æ ¼åˆ†æDAEsä¸­çš„æ¨¡å‹å´©æºƒï¼Œå¹¶æ‰©å±•è‡³æ‰©æ•£æ¨¡å‹å’ŒRectified Flowã€‚æˆ‘ä»¬çš„åˆ†æå’Œå®éªŒè¡¨æ˜ï¼Œç»è¿‡ä¿®æ­£çš„æµä¹Ÿä¼šå—åˆ°MCçš„å½±å“ï¼Œå¯èƒ½å¯¼è‡´æ¯ä¸ªåæµæ­¥éª¤ä¸­çš„æ½œåœ¨æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜å¼•å…¥çœŸå®æ•°æ®å¯ä»¥é¢„é˜²é€’å½’DAEè®­ç»ƒæœŸé—´çš„MCï¼Œè¿™æ”¯æŒäº†æœ€è¿‘ä½¿ç”¨çœŸå®æ•°æ®ä½œä¸ºç¼“è§£MCçš„æœ‰æ•ˆæ–¹æ³•çš„è¶‹åŠ¿ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„çœŸå®æ•°æ®å¢å¼ºåæµï¼ˆRA Reflowï¼‰å’Œä¸€ç³»åˆ—æ”¹è¿›å˜ä½“ï¼Œé€šè¿‡åˆ©ç”¨åå‘æµæ— ç¼åœ°å°†çœŸå®æ•°æ®é›†æˆåˆ°Reflowè®­ç»ƒä¸­ã€‚åœ¨æ ‡å‡†å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒè¯„ä¼°è¯å®ï¼ŒRA Reflowæœ‰æ•ˆç¼“è§£äº†æ¨¡å‹å´©æºƒï¼Œå³ä½¿åœ¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ä¸‹ä¹Ÿèƒ½ä¿æŒé«˜è´¨é‡çš„æ ·æœ¬ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08175v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆæˆæ•°æ®åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­çš„å¹¿æ³›åº”ç”¨åŠå…¶å¸¦æ¥çš„é—®é¢˜ã€‚è®­ç»ƒæ—¶ä½¿ç”¨åˆæˆæ•°æ®æ˜“å¯¼è‡´æ¨¡å‹å´©æºƒï¼ˆMCï¼‰ã€‚æ–‡ç« æ—¨åœ¨è§£å†³MCé—®é¢˜ï¼ŒåŒ…æ‹¬å¯¹å»å™ªè‡ªç¼–ç å™¨ï¼ˆDAEsï¼‰çš„æ·±å…¥åˆ†æä»¥åŠç›¸åº”çš„å®è¯å®éªŒã€‚é€šè¿‡èå…¥çœŸå®æ•°æ®èƒ½æœ‰æ•ˆç¼“è§£MCé—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥Real-data Augmented Reflowï¼Œè¯¥ç­–ç•¥åˆ©ç”¨åå‘æµæ— ç¼é›†æˆçœŸå®æ•°æ®ï¼Œæœ‰æ•ˆç¼“è§£æ¨¡å‹å´©æºƒé—®é¢˜ï¼Œæé«˜äº†é‡‡æ ·è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆæˆæ•°æ®åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­çš„ä½¿ç”¨æ—¥ç›Šæ™®éï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ¨¡å‹å´©æºƒï¼ˆMCï¼‰çš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡å¯¹å»å™ªè‡ªç¼–ç å™¨ï¼ˆDAEsï¼‰ä¸­çš„æ¨¡å‹å´©æºƒè¿›è¡Œäº†ä¸¥è°¨çš„ç†è®ºåˆ†æï¼Œå¹¶æ‰©å±•åˆ°æ‰©æ•£æ¨¡å‹å’ŒRectified Flowã€‚</li>
<li>é€šè¿‡åˆ†æè¯æ˜èå…¥çœŸå®æ•°æ®èƒ½æœ‰æ•ˆé¢„é˜²MCé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥Real-data Augmented Reflowï¼Œé€šè¿‡åˆ©ç”¨åå‘æµæ— ç¼é›†æˆçœŸå®æ•°æ®ï¼Œå‡å°‘äº†æ¨¡å‹å´©æºƒé—®é¢˜å¹¶æé«˜äº†é‡‡æ ·è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.08175v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.08175v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.08175v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.08175v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Cross-Attention-Head-Position-Patterns-Can-Align-with-Human-Visual-Concepts-in-Text-to-Image-Generative-Models"><a href="#Cross-Attention-Head-Position-Patterns-Can-Align-with-Human-Visual-Concepts-in-Text-to-Image-Generative-Models" class="headerlink" title="Cross-Attention Head Position Patterns Can Align with Human Visual   Concepts in Text-to-Image Generative Models"></a>Cross-Attention Head Position Patterns Can Align with Human Visual   Concepts in Text-to-Image Generative Models</h2><p><strong>Authors:Jungwon Park, Jungmin Ko, Dongnam Byun, Jangwon Suh, Wonjong Rhee</strong></p>
<p>Recent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we introduce a mechanistic interpretability approach for diffusion models by constructing Head Relevance Vectors (HRVs) that align with human-specified visual concepts. An HRV for a given visual concept has a length equal to the total number of cross-attention heads, with each element representing the importance of the corresponding head for the given visual concept. To validate HRVs as interpretable features, we develop an ordered weakening analysis that demonstrates their effectiveness. Furthermore, we propose concept strengthening and concept adjusting methods and apply them to enhance three visual generative tasks. Our results show that HRVs can reduce misinterpretations of polysemous words in image generation, successfully modify five challenging attributes in image editing, and mitigate catastrophic neglect in multi-concept generation. Overall, our work provides an advancement in understanding cross-attention layers and introduces new approaches for fine-controlling these layers at the head level. </p>
<blockquote>
<p>æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åˆ©ç”¨äº†äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¿™äº›å±‚å·²ç»è¢«æœ‰æ•ˆåœ°ç”¨äºå¢å¼ºå„ç§è§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹äº¤å‰æ³¨æ„åŠ›å±‚çš„ç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ„å»ºä¸äººç±»æŒ‡å®šçš„è§†è§‰æ¦‚å¿µç›¸å¯¹åº”çš„Head Relevance Vectorsï¼ˆHRVsï¼‰å¤´ç›¸å…³å‘é‡ï¼‰ï¼Œå¼•å…¥äº†ä¸€ç§æœºæ¢°å¯è§£é‡Šæ€§çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ã€‚ç»™å®šè§†è§‰æ¦‚å¿µçš„HRVçš„é•¿åº¦ç­‰äºäº¤å‰æ³¨æ„åŠ›å¤´çš„æ€»æ•°ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ç›¸åº”å¤´å¯¹äºç»™å®šè§†è§‰æ¦‚å¿µçš„é‡è¦æ€§ã€‚ä¸ºäº†éªŒè¯HRVä½œä¸ºå¯è§£é‡Šç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰åºçš„å‡å¼±åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µå¼ºåŒ–å’Œæ¦‚å¿µè°ƒæ•´æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºå¢å¼ºä¸‰ç§è§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒHRVå¯ä»¥å‡å°‘å›¾åƒç”Ÿæˆä¸­å¯¹å¤šä¹‰è¯çš„è¯¯è§£ï¼ŒæˆåŠŸä¿®æ”¹å›¾åƒç¼–è¾‘ä¸­çš„äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å±æ€§ï¼Œå¹¶å‡è½»å¤šæ¦‚å¿µç”Ÿæˆä¸­çš„ç¾éš¾æ€§å¿½è§†ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œæœ‰åŠ©äºæ›´å¥½åœ°äº†è§£äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå¹¶å¼•å…¥äº†åœ¨å¤´éƒ¨å±‚é¢ç²¾ç»†æ§åˆ¶è¿™äº›å±‚çš„æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02237v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è·¨æ³¨æ„åŠ›å±‚æœºåˆ¶ã€‚ç ”ç©¶é€šè¿‡æ„å»ºä¸äººç±»æŒ‡å®šè§†è§‰æ¦‚å¿µå¯¹é½çš„å¤´é‡è¦æ€§å‘é‡ï¼ˆHRVsï¼‰æ¥æé«˜æ‰©æ•£æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶éªŒè¯äº†HRVsä½œä¸ºå¯è§£é‡Šç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†æ¦‚å¿µå¼ºåŒ–å’Œæ¦‚å¿µè°ƒæ•´æ–¹æ³•ï¼Œç”¨äºå¢å¼ºè§†è§‰ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºå‡å°‘å¤šä¹‰è¯çš„è¯¯è§£ã€æˆåŠŸä¿®æ”¹å›¾åƒç¼–è¾‘ä¸­çš„äº”ä¸ªæŒ‘æˆ˜å±æ€§ï¼Œå¹¶ç¼“è§£å¤šæ¦‚å¿µç”Ÿæˆä¸­çš„ç¾éš¾æ€§å¿½è§†é—®é¢˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶æ¨åŠ¨äº†è·¨æ³¨æ„åŠ›å±‚çš„ç†è§£ï¼Œå¹¶å¼•å…¥äº†å¤´çº§åˆ«çš„ç²¾ç»†æ§åˆ¶æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨æ³¨æ„åŠ›å±‚åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å¤´é‡è¦æ€§å‘é‡ï¼ˆHRVsï¼‰è¢«å¼•å…¥ä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸äººç±»æŒ‡å®šçš„è§†è§‰æ¦‚å¿µå¯¹é½ã€‚</li>
<li>æœ‰åºå‡å¼±åˆ†æéªŒè¯äº†HRVsä½œä¸ºå¯è§£é‡Šç‰¹å¾çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¦‚å¿µå¼ºåŒ–å’Œæ¦‚å¿µè°ƒæ•´æ–¹æ³•è¢«æå‡ºå¹¶åº”ç”¨äºå¢å¼ºè§†è§‰ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>HRVsæœ‰åŠ©äºå‡å°‘å›¾åƒç”Ÿæˆä¸­å¤šä¹‰è¯çš„è¯¯è§£ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹å›¾åƒç¼–è¾‘ä¸­çš„å±æ€§ï¼ŒHRVså–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.02237v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.02237v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.02237v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.02237v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mining-Your-Own-Secrets-Diffusion-Classifier-Scores-for-Continual-Personalization-of-Text-to-Image-Diffusion-Models"><a href="#Mining-Your-Own-Secrets-Diffusion-Classifier-Scores-for-Continual-Personalization-of-Text-to-Image-Diffusion-Models" class="headerlink" title="Mining Your Own Secrets: Diffusion Classifier Scores for Continual   Personalization of Text-to-Image Diffusion Models"></a>Mining Your Own Secrets: Diffusion Classifier Scores for Continual   Personalization of Text-to-Image Diffusion Models</h2><p><strong>Authors:Saurav Jha, Shiqi Yang, Masato Ishii, Mengjie Zhao, Christian Simon, Muhammad Jehanzeb Mirza, Dong Gong, Lina Yao, Shusuke Takahashi, Yuki Mitsufuji</strong></p>
<p>Personalized text-to-image diffusion models have grown popular for their ability to efficiently acquire a new concept from user-defined text descriptions and a few images. However, in the real world, a user may wish to personalize a model on multiple concepts but one at a time, with no access to the data from previous concepts due to storage&#x2F;privacy concerns. When faced with this continual learning (CL) setup, most personalization methods fail to find a balance between acquiring new concepts and retaining previous ones â€“ a challenge that continual personalization (CP) aims to solve. Inspired by the successful CL methods that rely on class-specific information for regularization, we resort to the inherent class-conditioned density estimates, also known as diffusion classifier (DC) scores, for continual personalization of text-to-image diffusion models. Namely, we propose using DC scores for regularizing the parameter-space and function-space of text-to-image diffusion models, to achieve continual personalization. Using several diverse evaluation setups, datasets, and metrics, we show that our proposed regularization-based CP methods outperform the state-of-the-art C-LoRA, and other baselines. Finally, by operating in the replay-free CL setup and on low-rank adapters, our method incurs zero storage and parameter overhead, respectively, over the state-of-the-art. Our project page: <a target="_blank" rel="noopener" href="https://srvcodes.github.io/continual_personalization/">https://srvcodes.github.io/continual_personalization/</a> </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿä»ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æè¿°å’Œå°‘é‡å›¾åƒä¸­æœ‰æ•ˆåœ°è·å–æ–°æ¦‚å¿µè€Œå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›åœ¨å¤šä¸ªæ¦‚å¿µä¸Šè¿›è¡Œæ¨¡å‹ä¸ªæ€§åŒ–ï¼Œä½†ä¸€æ¬¡åªèƒ½è¿›è¡Œä¸€ä¸ªæ¦‚å¿µï¼Œç”±äºå­˜å‚¨&#x2F;éšç§æ‹…å¿§è€Œæ— æ³•è®¿é—®ä»¥å‰æ¦‚å¿µçš„æ•°æ®ã€‚é¢å¯¹è¿™ç§æŒç»­å­¦ä¹ ï¼ˆCLï¼‰è®¾ç½®æ—¶ï¼Œå¤§å¤šæ•°ä¸ªæ€§åŒ–æ–¹æ³•æ— æ³•åœ¨æ–°æ¦‚å¿µçš„è·å–å’Œä¿ç•™æ—§æ¦‚å¿µä¹‹é—´æ‰¾åˆ°å¹³è¡¡â€”â€”è¿™æ˜¯æŒç»­ä¸ªæ€§åŒ–ï¼ˆCPï¼‰æ—¨åœ¨è§£å†³çš„é—®é¢˜ã€‚æˆ‘ä»¬å—åˆ°ä¾èµ–ç‰¹å®šç±»åˆ«ä¿¡æ¯è¿›è¡Œæ­£åˆ™åŒ–çš„æˆåŠŸCLæ–¹æ³•çš„å¯å‘ï¼Œè¯‰è¯¸äºå›ºæœ‰çš„ç±»åˆ«æ¡ä»¶å¯†åº¦ä¼°è®¡ï¼ˆä¹Ÿç§°ä¸ºæ‰©æ•£åˆ†ç±»å™¨ï¼ˆDCï¼‰åˆ†æ•°ï¼‰ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒç»­ä¸ªæ€§åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨DCåˆ†æ•°å¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‚æ•°ç©ºé—´å’ŒåŠŸèƒ½ç©ºé—´è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥å®ç°æŒç»­ä¸ªæ€§åŒ–ã€‚é€šè¿‡å‡ ç§ä¸åŒçš„è¯„ä¼°è®¾ç½®ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºæ­£åˆ™åŒ–çš„CPæ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„C-LoRAå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æ— éœ€å¤ä¹ çš„CLè®¾ç½®å’Œä½é˜¶é€‚é…å™¨ä¸Šè¿è¡Œæˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”åˆ†åˆ«å®ç°äº†é›¶å­˜å‚¨å’Œé›¶å‚æ•°å¼€é”€ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://srvcodes.github.io/continual_personalization/">https://srvcodes.github.io/continual_personalization/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00700v3">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–æŒç»­å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç”¨æˆ·å¸Œæœ›åœ¨ä¸æ–­å­¦ä¹ çš„è¿‡ç¨‹ä¸­æŒæ¡æ–°æ¦‚å¿µï¼ŒåŒæ—¶ä¿ç•™å¯¹æ—§æ¦‚å¿µçš„è®°å¿†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºç±»æ¡ä»¶å¯†åº¦ä¼°è®¡çš„æ‰©æ•£åˆ†ç±»å™¨ï¼ˆDCï¼‰åˆ†æ•°çš„æ–¹æ³•ï¼Œç”¨äºå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡ŒæŒç»­ä¸ªæ€§åŒ–ã€‚é€šè¿‡å‚æ•°ç©ºé—´å’ŒåŠŸèƒ½ç©ºé—´çš„è°ƒèŠ‚ï¼Œè¯¥æ–¹æ³•å®ç°äº†æŒç»­ä¸ªæ€§åŒ–ï¼Œå¹¶åœ¨å¤šç§è¯„ä¼°è®¾ç½®å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯ï¼ˆå¦‚C-LoRAç­‰ï¼‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ— éœ€å›æ”¾ï¼Œåœ¨ä½ç§©é€‚é…å™¨ä¸Šè¿è¡Œï¼Œå®ç°äº†é›¶å­˜å‚¨å’Œé›¶å‚æ•°å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æè¿°å’Œå°‘é‡å›¾åƒä¸­æœ‰æ•ˆåœ°è·å–æ–°æ¦‚å¿µã€‚</li>
<li>åœ¨æŒç»­å­¦ä¹ ï¼ˆCLï¼‰è®¾ç½®ä¸­ï¼Œå¤§å¤šæ•°ä¸ªæ€§åŒ–æ–¹æ³•éš¾ä»¥å¹³è¡¡è·å–æ–°æ¦‚å¿µå’Œä¿ç•™æ—§æ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨ï¼ˆDCï¼‰åˆ†æ•°è¢«ç”¨äºè§£å†³æŒç»­ä¸ªæ€§åŒ–ï¼ˆCPï¼‰çš„é—®é¢˜ï¼Œé€šè¿‡è°ƒèŠ‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‚æ•°ç©ºé—´å’ŒåŠŸèƒ½ç©ºé—´å®ç°ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šç§è¯„ä¼°è®¾ç½®å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸ä½¿ç”¨å›æ”¾çš„æƒ…å†µä¸‹è¿è¡Œï¼Œå‡å°‘äº†å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä½ç§©é€‚é…å™¨ä¸Šè¿è¡Œï¼Œå®ç°äº†é›¶å‚æ•°å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2410.00700v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2410.00700v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2410.00700v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Scalable-Autoregressive-Image-Generation-with-Mamba"><a href="#Scalable-Autoregressive-Image-Generation-with-Mamba" class="headerlink" title="Scalable Autoregressive Image Generation with Mamba"></a>Scalable Autoregressive Image Generation with Mamba</h2><p><strong>Authors:Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li</strong></p>
<p>We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mambaâ€™s core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM">https://github.com/hp-l33/AiM</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åŸºäºMambaæ¶æ„çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹AiMã€‚AiMé‡‡ç”¨Mambaè¿™ä¸€æ–°å‹çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œä»¥å…¶å¯¹é•¿åºåˆ—å»ºæ¨¡çš„å‡ºè‰²æ€§èƒ½ï¼ˆå…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼‰æ¥æ›¿ä»£è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å¸¸ç”¨çš„Transformerï¼Œæ—¨åœ¨å®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•é€šè¿‡å¤šæ–¹å‘æ‰«æä½¿Mambaé€‚åº”å¤„ç†äºŒç»´ä¿¡å·çš„æ–¹å¼ï¼ŒAiMç›´æ¥ä½¿ç”¨åŸºäºä»¤ç‰Œé¢„æµ‹çš„èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†éœ€è¦å¤§é‡ä¿®æ”¹ä»¥ä½¿Mambaèƒ½å¤Ÿå­¦ä¹ äºŒç»´ç©ºé—´è¡¨ç¤ºçš„éœ€æ±‚ã€‚é€šè¿‡å¯¹è§†è§‰ç”Ÿæˆä»»åŠ¡è¿›è¡Œç®€å•è€Œæœ‰é’ˆå¯¹æ€§çš„ä¿®æ”¹ï¼Œæˆ‘ä»¬ä¿ç•™äº†Mambaçš„æ ¸å¿ƒç»“æ„ï¼Œå……åˆ†åˆ©ç”¨å…¶é«˜æ•ˆçš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸åŒè§„æ¨¡çš„AiMæ¨¡å‹ï¼Œå‚æ•°æ•°é‡ä»148Måˆ°1.3Bä¸ç­‰ã€‚åœ¨ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æœ€å¥½çš„AiMæ¨¡å‹å®ç°äº†FIDä¸º2.21ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰å‚æ•°ç›¸ä¼¼çš„ARæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œæ˜¯æ‰©æ•£æ¨¡å‹çš„2åˆ°10å€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hp-l33/AiMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12245v4">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬ä»‹ç»äº†åŸºäºMambaæ¶æ„çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹AiMã€‚AiMåˆ©ç”¨å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚æ€§çš„æ–°å‹çŠ¶æ€ç©ºé—´æ¨¡å‹Mambaï¼Œæ—¨åœ¨å®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚å®ƒé€šè¿‡ç›´æ¥åˆ©ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œé¿å…äº†å¤§é‡ä¿®æ”¹çš„éœ€è¦ã€‚æˆ‘ä»¬æä¾›äº†ä¸åŒè§„æ¨¡çš„AiMæ¨¡å‹ï¼Œå‚æ•°æ•°é‡ä»148Måˆ°1.3Bä¸ç­‰ã€‚åœ¨ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æœ€å¥½çš„AiMæ¨¡å‹FIDè¾¾åˆ°2.21ï¼Œè¶…è¿‡äº†æ‰€æœ‰ç°æœ‰å‚æ•°æ•°é‡ç›¸è¿‘çš„ARæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”æ‰©æ•£æ¨¡å‹å¿«2åˆ°10å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AiMæ˜¯ä¸€ä¸ªåŸºäºMambaæ¶æ„çš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>Mambaæ˜¯ä¸€ä¸ªå…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚æ€§çš„æ–°å‹çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>AiMæ—¨åœ¨å®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>AiMé€šè¿‡ç›´æ¥åˆ©ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚</li>
<li>AiMæ¨¡å‹å‚æ•°æ•°é‡èŒƒå›´ä»148Måˆ°1.3Bä¸ç­‰ã€‚</li>
<li>åœ¨ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAiMæ¨¡å‹FIDè¾¾åˆ°2.21ã€‚</li>
<li>AiMæ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”æ‰©æ•£æ¨¡å‹å¿«2åˆ°10å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2408.12245v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2408.12245v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2408.12245v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2408.12245v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2408.12245v4/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2408.12245v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Iterative-Ensemble-Training-with-Anti-Gradient-Control-for-Mitigating-Memorization-in-Diffusion-Models"><a href="#Iterative-Ensemble-Training-with-Anti-Gradient-Control-for-Mitigating-Memorization-in-Diffusion-Models" class="headerlink" title="Iterative Ensemble Training with Anti-Gradient Control for Mitigating   Memorization in Diffusion Models"></a>Iterative Ensemble Training with Anti-Gradient Control for Mitigating   Memorization in Diffusion Models</h2><p><strong>Authors:Xiao Liu, Xiaoliu Guan, Yu Wu, Jiaxu Miao</strong></p>
<p>Diffusion models, known for their tremendous ability to generate novel and high-quality samples, have recently raised concerns due to their data memorization behavior, which poses privacy risks. Recent approaches for memory mitigation either only focused on the text modality problem in cross-modal generation tasks or utilized data augmentation strategies. In this paper, we propose a novel training framework for diffusion models from the perspective of visual modality, which is more generic and fundamental for mitigating memorization. To facilitate forgetting of stored information in diffusion model parameters, we propose an iterative ensemble training strategy by splitting the data into multiple shards for training multiple models and intermittently aggregating these model parameters. Moreover, practical analysis of losses illustrates that the training loss for easily memorable images tends to be obviously lower. Thus, we propose an anti-gradient control method to exclude the sample with a lower loss value from the current mini-batch to avoid memorizing. Extensive experiments and analysis on four datasets are conducted to illustrate the effectiveness of our method, and results show that our method successfully reduces memory capacity while even improving the performance slightly. Moreover, to save the computing cost, we successfully apply our method to fine-tune the well-trained diffusion models by limited epochs, demonstrating the applicability of our method. Code is available in <a target="_blank" rel="noopener" href="https://github.com/liuxiao-guan/IET_AGC">https://github.com/liuxiao-guan/IET_AGC</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆæ–°é¢–ã€é«˜è´¨é‡æ ·æœ¬çš„å‡ºè‰²èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ï¼Œä½†æœ€è¿‘å…¶æ•°æ®è®°å¿†è¡Œä¸ºå¼•å‘äº†å…³äºéšç§é£é™©çš„æ‹…å¿§ã€‚æœ€è¿‘çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­çš„æ–‡æœ¬æ¨¡æ€é—®é¢˜æˆ–é‡‡ç”¨æ•°æ®å¢å¼ºç­–ç•¥æ¥ç¼“è§£è®°å¿†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»è§†è§‰æ¨¡æ€çš„è§’åº¦ä¸ºæ‰©æ•£æ¨¡å‹æå‡ºäº†ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ç¼“è§£è®°å¿†æ–¹é¢æ›´åŠ é€šç”¨å’Œæ ¹æœ¬ã€‚ä¸ºäº†ä¿ƒè¿›å­˜å‚¨åœ¨æ‰©æ•£æ¨¡å‹å‚æ•°ä¸­çš„ä¿¡æ¯çš„é—å¿˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£é›†æˆè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å°†æ•°æ®åˆ†æˆå¤šä¸ªåˆ†ç‰‡æ¥è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œå¹¶å®šæœŸèšåˆè¿™äº›æ¨¡å‹å‚æ•°ã€‚æ­¤å¤–ï¼Œå¯¹æŸå¤±çš„å®é™…åˆ†æè¡¨æ˜ï¼Œæ˜“äºè®°å¿†çš„å›¾åƒçš„è®­ç»ƒæŸå¤±å¾€å¾€æ˜æ˜¾è¾ƒä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åæ¢¯åº¦æ§åˆ¶æ–¹æ³•ï¼Œå°†å½“å‰å°æ‰¹é‡ä¸­å…·æœ‰è¾ƒä½æŸå¤±å€¼çš„æ ·æœ¬æ’é™¤åœ¨å¤–ï¼Œä»¥é¿å…è®°å¿†ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œåˆ†æï¼Œä»¥è¯´æ˜æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘å†…å­˜å®¹é‡çš„åŒæ—¶ï¼Œç”šè‡³ç•¥å¾®æé«˜äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†èŠ‚çœè®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†è¯¥æ–¹æ³•åº”ç”¨äºå¯¹ç»è¿‡è‰¯å¥½è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæœ‰é™è½®æ¬¡çš„å¾®è°ƒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/liuxiao-guan/IET_AGC">https://github.com/liuxiao-guan/IET_AGC</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15328v3">PDF</a> Accepted in ECCV 2024, 20 pages with 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä»è§†è§‰æ¨¡æ€è§’åº¦æ›´é€šç”¨ã€æ›´åŸºç¡€åœ°ç¼“è§£è®°å¿†é—®é¢˜ã€‚é€šè¿‡åˆ†è£‚æ•°æ®ä¸ºå¤šä¸ªç‰‡æ®µè¿›è¡Œå¤šæ¬¡æ¨¡å‹è®­ç»ƒå¹¶é—´æ­‡æ€§åœ°æ•´åˆæ¨¡å‹å‚æ•°ï¼Œæå‡ºä¸€ç§è¿­ä»£é›†æˆè®­ç»ƒç­–ç•¥ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åæ¢¯åº¦æ§åˆ¶æ–¹æ³•é¿å…è®°å¿†æŸå¤±è¾ƒä½çš„æ ·æœ¬ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘è®°å¿†å®¹é‡ï¼Œå¹¶ç•¥å¾®æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é€‚ç”¨äºå¾®è°ƒå·²è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€‚ç”¨äºèŠ‚çº¦è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆæ–°é¢–ã€é«˜è´¨é‡æ ·æœ¬çš„èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ï¼Œä½†æœ€è¿‘å› æ•°æ®è®°å¿†åŒ–è¡Œä¸ºå¼•å‘éšç§æ‹…å¿§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„æ–‡æœ¬æ¨¡æ€é—®é¢˜æˆ–æ•°æ®å¢å¼ºç­–ç•¥ä¸Šã€‚</li>
<li>æœ¬æ–‡ä»è§†è§‰æ¨¡æ€è§’åº¦æå‡ºä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ›´é€šç”¨ã€æ›´åŸºç¡€åœ°è§£å†³è®°å¿†é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°æ®åˆ†ç‰‡å’Œè¿­ä»£é›†æˆè®­ç»ƒç­–ç•¥ï¼Œä¿ƒè¿›æ‰©æ•£æ¨¡å‹å‚æ•°ä¸­çš„ä¿¡æ¯é—å¿˜ã€‚</li>
<li>å‘ç°æ˜“è®°å¿†å›¾åƒçš„æŸå¤±è¾ƒä½ï¼Œå› æ­¤æå‡ºåæ¢¯åº¦æ§åˆ¶æ–¹æ³•æ’é™¤è¿™äº›æ ·æœ¬ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆå‡å°‘è®°å¿†å®¹é‡ï¼Œå¹¶ç•¥æœ‰æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2407.15328v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2407.15328v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Advancing-Fine-Grained-Classification-by-Structure-and-Subject-Preserving-Augmentation"><a href="#Advancing-Fine-Grained-Classification-by-Structure-and-Subject-Preserving-Augmentation" class="headerlink" title="Advancing Fine-Grained Classification by Structure and Subject   Preserving Augmentation"></a>Advancing Fine-Grained Classification by Structure and Subject   Preserving Augmentation</h2><p><strong>Authors:Eyal Michaeli, Ohad Fried</strong></p>
<p>Fine-grained visual classification (FGVC) involves classifying closely related sub-classes. This task is difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation. Recent advancements in text-to-image diffusion models offer new possibilities for augmenting classification datasets. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on Text2Image generation or Img2Img methods, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the datasetâ€™s diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation. We conduct extensive experiments and benchmark SaSPA against both traditional and recent generative data augmentation methods. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training, contextual bias, and few-shot classification. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal proportion of synthetic data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/EyalMichaeli/SaSPA-Aug">https://github.com/EyalMichaeli/SaSPA-Aug</a>. </p>
<blockquote>
<p>ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰æ¶‰åŠå¯¹å¯†åˆ‡ç›¸å…³çš„å­ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚ç”±äºç±»åˆ«ä¹‹é—´çš„ç»†å¾®å·®å¼‚å’Œé«˜åº¦çš„ç±»å†…å˜åŒ–ï¼Œè¿™é¡¹ä»»åŠ¡å¾ˆå›°éš¾ã€‚æ­¤å¤–ï¼ŒFGVCæ•°æ®é›†é€šå¸¸å¾ˆå°ä¸”éš¾ä»¥æ”¶é›†ï¼Œä»è€Œå‡¸æ˜¾å‡ºå¯¹æœ‰æ•ˆæ•°æ®å¢å¼ºçš„è¿«åˆ‡éœ€æ±‚ã€‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºå¢å¼ºåˆ†ç±»æ•°æ®é›†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚è™½ç„¶è¿™äº›æ¨¡å‹å·²è¢«ç”¨äºç”Ÿæˆåˆ†ç±»ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼Œä½†å®ƒä»¬åœ¨å…¨æ•°æ®é›†è®­ç»ƒFGVCæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚æœ€è¿‘ä¾èµ–äºText2Imageç”Ÿæˆæˆ–Img2Imgæ–¹æ³•çš„æŠ€å·§ï¼Œå¾€å¾€éš¾ä»¥ç”Ÿæˆå‡†ç¡®ä»£è¡¨ç±»çš„å›¾åƒï¼ŒåŒæ—¶åœ¨ä¿®æ”¹å®ƒä»¬æ—¶æ˜¾è‘—å¢åŠ æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SaSPAï¼šç»“æ„å’Œä¸»é¢˜ä¿ç•™å¢å¼ºæ³•ã€‚ä¸æœ€è¿‘çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä½¿ç”¨çœŸå®å›¾åƒä½œä¸ºæŒ‡å¯¼ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆçš„çµæ´»æ€§å¹¶ä¿ƒè¿›äº†æ›´å¤§çš„å¤šæ ·æ€§ã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„ç±»åˆ«è¡¨ç¤ºï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ¡ä»¶æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ä»¥å›¾åƒè¾¹ç¼˜å’Œä¸»é¢˜è¡¨ç¤ºä¸ºæ¡ä»¶ã€‚æˆ‘ä»¬å¯¹SaSPAè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶å°†å…¶ä¸ä¼ ç»Ÿå’Œæœ€æ–°çš„ç”Ÿæˆæ•°æ®å¢å¼ºæ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚SaSPAåœ¨å¤šç§è®¾ç½®ä¸‹å§‹ç»ˆè¶…è¶Šæ‰€æœ‰æ—¢å®šçš„åŸºçº¿ï¼ŒåŒ…æ‹¬å…¨æ•°æ®é›†è®­ç»ƒã€ä¸Šä¸‹æ–‡åå·®å’Œå°‘æ ·æœ¬åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†åœ¨ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡ŒFGVCæ¨¡å‹æ—¶çš„æœ‰è¶£æ¨¡å¼ï¼›ä¾‹å¦‚ï¼Œæˆ‘ä»¬å‘ç°çœŸå®æ•°æ®çš„ä½¿ç”¨é‡ä¸åˆæˆæ•°æ®çš„æœ€ä½³æ¯”ä¾‹ä¹‹é—´å­˜åœ¨å…³ç³»ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EyalMichaeli/SaSPA-Aug">https://github.com/EyalMichaeli/SaSPA-Aug</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14551v3">PDF</a> Accepted to NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Fine-grained Visual Classificationï¼ˆFGVCï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å­ç±»åˆ«é—´çš„ç»†å¾®å·®å¼‚ã€é«˜ç±»å†…æ–¹å·®ä»¥åŠæ•°æ®é›†è·å–å›°éš¾ç­‰é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•SaSPAï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»¥åº”å¯¹FGVCæ¨¡å‹çš„è®­ç»ƒéœ€æ±‚ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–çœŸå®å›¾åƒä½œä¸ºæŒ‡å¯¼ï¼Œé€šè¿‡æ¡ä»¶æœºåˆ¶ç¡®ä¿å‡†ç¡®ç±»è¡¨ç¤ºï¼ŒåŒæ—¶åœ¨å›¾åƒè¾¹ç¼˜å’Œä¸»é¢˜è¡¨ç¤ºä¸Šè¿›è¡Œæ¡ä»¶è®¾å®šï¼Œä»¥å¢åŠ ç”Ÿæˆå¤šæ ·æ€§å’Œçµæ´»æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSaSPAåœ¨ä¸åŒè®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å…¨æ•°æ®é›†è®­ç»ƒã€ä¸Šä¸‹æ–‡åå·®å’Œå°‘æ ·æœ¬åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†ä½¿ç”¨åˆæˆæ•°æ®å¯¹FGVCæ¨¡å‹çš„æœ‰è¶£ç°è±¡ï¼Œå¹¶æŒ‡å‡ºçœŸå®æ•°æ®ä¸åˆæˆæ•°æ®çš„æœ€ä½³æ¯”ä¾‹å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fine-grained Visual Classification (FGVC) é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç±»é—´ç»†å¾®å·®å¼‚ã€é«˜ç±»å†…æ–¹å·®åŠæ•°æ®é›†è·å–å›°éš¾ã€‚</li>
<li>æ•°æ®å¢å¼ºåœ¨FGVCä¸­è‡³å…³é‡è¦ï¼Œè¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸ºæ­¤æä¾›æ–°å¯èƒ½ã€‚</li>
<li>SaSPAæ–¹æ³•é€šè¿‡ä¸ä¾èµ–çœŸå®å›¾åƒæŒ‡å¯¼æ¥å¢åŠ ç”Ÿæˆå¤šæ ·æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>SaSPAé‡‡ç”¨æ¡ä»¶æœºåˆ¶ç¡®ä¿å‡†ç¡®ç±»è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒè¾¹ç¼˜å’Œä¸»é¢˜è¡¨ç¤ºä¸Šè®¾å®šæ¡ä»¶ã€‚</li>
<li>å®éªŒè¯æ˜SaSPAåœ¨å…¨æ•°æ®é›†è®­ç»ƒã€ä¸Šä¸‹æ–‡åå·®å’Œå°‘æ ·æœ¬åˆ†ç±»ç­‰è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä½¿ç”¨åˆæˆæ•°æ®å¯¹FGVCæ¨¡å‹æœ‰è¶£ç°è±¡è¢«æ¢è®¨ï¼ŒçœŸå®æ•°æ®ä¸åˆæˆæ•°æ®ä¹‹é—´çš„æ¯”ä¾‹å…³ç³»è¢«æ­ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.14551v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.14551v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.14551v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.14551v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Not-All-Prompts-Are-Made-Equal-Prompt-based-Pruning-of-Text-to-Image-Diffusion-Models"><a href="#Not-All-Prompts-Are-Made-Equal-Prompt-based-Pruning-of-Text-to-Image-Diffusion-Models" class="headerlink" title="Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image   Diffusion Models"></a>Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image   Diffusion Models</h2><p><strong>Authors:Alireza Ganjdanesh, Reza Shirkavand, Shangqian Gao, Heng Huang</strong></p>
<p>Text-to-image (T2I) diffusion models have demonstrated impressive image generation capabilities. Still, their computational intensity prohibits resource-constrained organizations from deploying T2I models after fine-tuning them on their internal target data. While pruning techniques offer a potential solution to reduce the computational burden of T2I models, static pruning methods use the same pruned model for all input prompts, overlooking the varying capacity requirements of different prompts. Dynamic pruning addresses this issue by utilizing a separate sub-network for each prompt, but it prevents batch parallelism on GPUs. To overcome these limitations, we introduce Adaptive Prompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed for T2I diffusion models. Central to our approach is a prompt router model, which learns to determine the required capacity for an input text prompt and routes it to an architecture code, given a total desired compute budget for prompts. Each architecture code represents a specialized model tailored to the prompts assigned to it, and the number of codes is a hyperparameter. We train the prompt router and architecture codes using contrastive learning, ensuring that similar prompts are mapped to nearby codes. Further, we employ optimal transport to prevent the codes from collapsing into a single one. We demonstrate APTPâ€™s effectiveness by pruning Stable Diffusion (SD) V2.1 using CC3M and COCO as target datasets. APTP outperforms the single-model pruning baselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters learned by APTP reveals they are semantically meaningful. We also show that APTP can automatically discover previously empirically found challenging prompts for SD, e.g. prompts for generating text images, assigning them to higher capacity codes. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®¡ç®—å¼ºåº¦ä½¿å¾—èµ„æºå—é™çš„ç»„ç»‡æ— æ³•åœ¨å¯¹å…¶å†…éƒ¨ç›®æ ‡æ•°æ®è¿›è¡Œå¾®è°ƒåéƒ¨ç½²T2Iæ¨¡å‹ã€‚è™½ç„¶å‰ªææŠ€æœ¯æä¾›äº†å‡å°‘T2Iæ¨¡å‹è®¡ç®—è´Ÿæ‹…çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†é™æ€å‰ªææ–¹æ³•ä½¿ç”¨ç›¸åŒçš„å‰ªææ¨¡å‹æ¥å¤„ç†æ‰€æœ‰è¾“å…¥æç¤ºï¼Œå¿½è§†äº†ä¸åŒæç¤ºæ‰€éœ€çš„èƒ½åŠ›å·®å¼‚ã€‚åŠ¨æ€å‰ªæé€šè¿‡ä¸ºæ¯ä¸ªæç¤ºä½¿ç”¨å•ç‹¬çš„å­ç½‘ç»œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒé˜»æ­¢äº†GPUä¸Šçš„æ‰¹é‡å¹¶è¡Œå¤„ç†ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”æç¤ºå®šåˆ¶å‰ªæï¼ˆAPTPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„æ–°å‹æç¤ºå‰ªææ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæç¤ºè·¯ç”±å™¨æ¨¡å‹ï¼Œå®ƒå­¦ä¹ ç¡®å®šè¾“å…¥æ–‡æœ¬æç¤ºæ‰€éœ€çš„å®¹é‡ï¼Œå¹¶æ ¹æ®ç»™å®šçš„æ€»è®¡ç®—é¢„ç®—å°†æç¤ºè·¯ç”±åˆ°æ¶æ„ä»£ç ã€‚æ¯ä¸ªæ¶æ„ä»£ç ä»£è¡¨ä¸€ä¸ªé’ˆå¯¹åˆ†é…ç»™å®ƒçš„æç¤ºè€Œå®šåˆ¶çš„ç‰¹æ®Šæ¨¡å‹ï¼Œä»£ç çš„æ•°é‡æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒæç¤ºè·¯ç”±å™¨å’Œæ¶æ„ä»£ç ï¼Œç¡®ä¿ç›¸ä¼¼çš„æç¤ºè¢«æ˜ å°„åˆ°é™„è¿‘çš„ä»£ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æœ€ä¼˜ä¼ è¾“æ¥é˜²æ­¢ä»£ç åˆå¹¶ä¸ºå•ä¸€ä»£ç ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨CC3Må’ŒCOCOä½œä¸ºç›®æ ‡æ•°æ®é›†æ¥å‰ªæç¨³å®šçš„æ‰©æ•£ï¼ˆSDï¼‰V2.1ï¼Œè¯æ˜äº†APTPçš„æœ‰æ•ˆæ€§ã€‚APTPåœ¨FIDã€CLIPå’ŒCMMDåˆ†æ•°æ–¹é¢ä¼˜äºå•æ¨¡å‹å‰ªæåŸºçº¿ã€‚æˆ‘ä»¬å¯¹APTPå­¦ä¹ çš„é›†ç¾¤çš„åˆ†æè¡¨æ˜å®ƒä»¬æ˜¯è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†APTPå¯ä»¥è‡ªåŠ¨å‘ç°ä¹‹å‰ç»éªŒä¸Šå‘ç°çš„SDå…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºï¼ˆä¾‹å¦‚ç”¨äºç”Ÿæˆæ–‡æœ¬å›¾åƒçš„æç¤ºï¼‰ï¼Œå¹¶å°†å®ƒä»¬åˆ†é…ç»™æ›´é«˜å®¹é‡çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12042v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>T2Iæ‰©æ•£æ¨¡å‹å…·æœ‰å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶è®¡ç®—å¼ºåº¦é™åˆ¶äº†èµ„æºå—é™ç»„ç»‡åœ¨å†…éƒ¨ç›®æ ‡æ•°æ®ä¸Šå¾®è°ƒåçš„éƒ¨ç½²ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæç¤ºçš„ä¿®å‰ªæ–¹æ³•â€”â€”è‡ªé€‚åº”æç¤ºå®šåˆ¶ä¿®å‰ªï¼ˆAPTPï¼‰ã€‚æ ¸å¿ƒåœ¨äºæç¤ºè·¯ç”±å™¨æ¨¡å‹ï¼Œå…¶å¯å­¦ä¹ ç¡®å®šè¾“å…¥æ–‡æœ¬æç¤ºæ‰€éœ€çš„è®¡ç®—èƒ½åŠ›å¹¶æ ¹æ®æ€»è®¡ç®—é¢„ç®—è¿›è¡Œè·¯ç”±ã€‚æ­¤æ–¹æ³•é’ˆå¯¹ç›¸ä¼¼æç¤ºæ˜ å°„ç›¸è¿‘ä»£ç ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“é˜²æ­¢ä»£ç å´©æºƒã€‚åœ¨SD V2.1ä¸Šé€šè¿‡CC3Må’ŒCOCOç›®æ ‡æ•°æ®é›†è¿›è¡Œä¿®å‰ªçš„å®éªŒæ˜¾ç¤ºï¼ŒAPTPåœ¨FIDã€CLIPå’ŒCMMDåˆ†æ•°ä¸Šä¼˜äºå•æ¨¡å‹ä¿®å‰ªåŸºçº¿ã€‚åˆ†æè¡¨æ˜ï¼ŒAPTPå­¦ä¹ çš„èšç±»åœ¨è¯­ä¹‰ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ï¼Œå¹¶ä¸”APTPèƒ½å¤Ÿè‡ªåŠ¨å‘ç°ä¹‹å‰å®è¯ä¸­éš¾ä»¥å‘ç°çš„æç¤ºå¹¶å°†å…¶åˆ†é…ç»™æ›´é«˜å®¹é‡çš„ä»£ç ã€‚æ€»ä¹‹ï¼ŒAPTPæ–¹æ³•ä¸ºèµ„æºå—é™çš„ç¯å¢ƒæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„T2Iæ‰©æ•£æ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å…¶è®¡ç®—å¼ºåº¦é«˜éš¾ä»¥éƒ¨ç½²äºèµ„æºå—é™ç¯å¢ƒã€‚</li>
<li>ç°æœ‰é™æ€ä¿®å‰ªæ–¹æ³•å¿½ç•¥ä¸åŒæç¤ºå¯¹è®¡ç®—èƒ½åŠ›çš„ä¸åŒéœ€æ±‚ã€‚</li>
<li>åŠ¨æ€ä¿®å‰ªä½¿ç”¨å•ç‹¬çš„å­ç½‘ç»œå¤„ç†æ¯ä¸ªæç¤ºï¼Œä½†æ— æ³•åˆ©ç”¨GPUå¹¶è¡Œå¤„ç†ä¼˜åŠ¿ã€‚</li>
<li>APTPæ˜¯ä¸€ç§é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„åŸºäºæç¤ºçš„ä¿®å‰ªæ–¹æ³•ï¼ŒåŒ…å«æç¤ºè·¯ç”±å™¨æ¨¡å‹å’Œæ¶æ„ä»£ç å­¦ä¹ ã€‚</li>
<li>æç¤ºè·¯ç”±å™¨æ¨¡å‹æ ¹æ®è®¡ç®—é¢„ç®—ä¸ºè¾“å…¥æ–‡æœ¬æç¤ºç¡®å®šæ‰€éœ€è®¡ç®—èƒ½åŠ›ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç”¨äºè®­ç»ƒæç¤ºè·¯ç”±å™¨å’Œæ¶æ„ä»£ç ï¼Œç¡®ä¿ç›¸ä¼¼æç¤ºæ˜ å°„åˆ°ç›¸è¿‘ä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.12042v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.12042v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.12042v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Guided-Score-identity-Distillation-for-Data-Free-One-Step-Text-to-Image-Generation"><a href="#Guided-Score-identity-Distillation-for-Data-Free-One-Step-Text-to-Image-Generation" class="headerlink" title="Guided Score identity Distillation for Data-Free One-Step Text-to-Image   Generation"></a>Guided Score identity Distillation for Data-Free One-Step Text-to-Image   Generation</h2><p><strong>Authors:Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang</strong></p>
<p>Diffusion-based text-to-image generation models trained on extensive text-image pairs have demonstrated the ability to produce photorealistic images aligned with textual descriptions. However, a significant limitation of these models is their slow sample generation process, which requires iterative refinement through the same network. To overcome this, we introduce a data-free guided distillation method that enables the efficient distillation of pretrained Stable Diffusion models without access to the real training data, often restricted due to legal, privacy, or cost concerns. This method enhances Score identity Distillation (SiD) with Long and Short Classifier-Free Guidance (LSG), an innovative strategy that applies Classifier-Free Guidance (CFG) not only to the evaluation of the pretrained diffusion model but also to the training and evaluation of the fake score network. We optimize a model-based explicit score matching loss using a score-identity-based approximation alongside our proposed guidance strategies for practical computation. By exclusively training with synthetic images generated by its one-step generator, our data-free distillation method rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Notably, the one-step distillation of Stable Diffusion 1.5 achieves an FID of 8.15 on the COCO-2014 validation set, a record low value under the data-free setting. Our code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG">https://github.com/mingyuanzhou/SiD-LSG</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤§é‡çš„æ–‡æœ¬å›¾åƒå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å·²è¢«è¯æ˜èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸ç¬¦çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„ä¸€ä¸ªé‡å¤§å±€é™æ€§æ˜¯å®ƒä»¬çš„æ ·æœ¬ç”Ÿæˆè¿‡ç¨‹ç¼“æ…¢ï¼Œéœ€è¦é€šè¿‡åŒä¸€ç½‘ç»œè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— éœ€æ•°æ®å¼•å¯¼è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ— æ³•è®¿é—®çœŸå®è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°è’¸é¦é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ï¼Œç”±äºæ³•å¾‹ã€éšç§æˆ–æˆæœ¬ç­‰æ–¹é¢çš„æ‹…å¿§ï¼ŒçœŸå®è®­ç»ƒæ•°æ®é€šå¸¸å—åˆ°é™åˆ¶ã€‚è¯¥æ–¹æ³•å¢å¼ºäº†åŸºäºåˆ†æ•°èº«ä»½çš„è’¸é¦ï¼ˆSiDï¼‰ä¸é•¿çŸ­æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆLSGï¼‰ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°æ€§çš„ç­–ç•¥ï¼Œä¸ä»…å°†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰åº”ç”¨äºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„è¯„ä¼°ï¼Œè¿˜åº”ç”¨äºå‡åˆ†æ•°ç½‘ç»œçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºåˆ†æ•°èº«ä»½çš„è¿‘ä¼¼æ–¹æ³•ä¼˜åŒ–åŸºäºæ¨¡å‹çš„æ˜¾å¼åˆ†æ•°åŒ¹é…æŸå¤±ï¼Œå¹¶ç»“åˆæˆ‘ä»¬æå‡ºçš„å¼•å¯¼ç­–ç•¥è¿›è¡Œå®é™…è®¡ç®—ã€‚é€šè¿‡ä»…ä½¿ç”¨å…¶ä¸€æ­¥ç”Ÿæˆå™¨ç”Ÿæˆçš„åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ— éœ€æ•°æ®è’¸é¦æ–¹æ³•è¿…é€Ÿæé«˜äº†FIDå’ŒCLIPæŒ‡æ ‡ï¼Œåœ¨ä¿æŒç«äº‰åŠ›çš„CLIPå¾—åˆ†çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„FIDæ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ä¸€æ­¥è’¸é¦çš„Stable Diffusion 1.5åœ¨COCO-2014éªŒè¯é›†ä¸Šè¾¾åˆ°äº†8.15çš„FIDï¼Œè¿™æ˜¯æ— æ•°æ®è®¾ç½®ä¸‹çš„æœ€ä½è®°å½•å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ£€æŸ¥ç‚¹å¯ç”¨äºè®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG%E3%80%82">https://github.com/mingyuanzhou/SiD-LSGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01561v4">PDF</a> ICLR 2025; fixed typos in Table 1; Code and model checkpoints   available at <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD-LSG">https://github.com/mingyuanzhou/SiD-LSG</a>; More efficient code   using AMP is coming soon</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€çœŸå®è®­ç»ƒæ•°æ®çš„æ‰©æ•£æ¨¡å‹è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¾—åˆ†èº«ä»½è’¸é¦ï¼ˆSiDï¼‰ä¸é•¿çŸ­åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆLSGï¼‰ç­–ç•¥ï¼Œä¼˜åŒ–äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯„ä¼°åŠå‡åˆ†æ•°ç½‘ç»œçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚é€šè¿‡åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨æå‡FIDå’ŒCLIPå¾—åˆ†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå®ç°äº†æ•°æ®æ— å…³è®¾ç½®ä¸‹çš„æœ€ä½³FIDè¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨æ–‡æœ¬æè¿°çš„åŸºç¡€ä¸Šç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨æ ·æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­å­˜åœ¨é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€çœŸå®è®­ç»ƒæ•°æ®çš„æ•°æ®å…è´¹è’¸é¦æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¸Šè¿°é€Ÿåº¦é—®é¢˜ã€‚</li>
<li>ç»“åˆäº†SiDä¸LSGç­–ç•¥ï¼Œä¼˜åŒ–äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯„ä¼°åŠå‡åˆ†æ•°ç½‘ç»œçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>é€šè¿‡åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæå‡äº†FIDå’ŒCLIPå¾—åˆ†ã€‚</li>
<li>å®ç°äº†ä¸€æµFIDæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæœ‰ç«äº‰åŠ›çš„CLIPå¾—åˆ†ã€‚</li>
<li>æˆåŠŸè’¸é¦Stable Diffusion 1.5æ¨¡å‹ï¼Œåœ¨COCO-2014éªŒè¯é›†ä¸Šè¾¾åˆ°8.15çš„FIDã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.01561v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.01561v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2406.01561v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study"><a href="#Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study" class="headerlink" title="Weakly-Supervised PET Anomaly Detection using Implicitly-Guided   Attention-Conditional Counterfactual Diffusion Modeling: a Multi-Center,   Multi-Cancer, and Multi-Tracer Study"></a>Weakly-Supervised PET Anomaly Detection using Implicitly-Guided   Attention-Conditional Counterfactual Diffusion Modeling: a Multi-Center,   Multi-Cancer, and Multi-Tracer Study</h2><p><strong>Authors:Shadab Ahamed, Arman Rahmim</strong></p>
<p>Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current un-&#x2F;weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks trained only on healthy data; however GAN-based networks are more challenging to train due to issues with simultaneous optimization of two competing networks, mode collapse, etc. In this paper, we present the weakly-supervised Implicitly guided COuNterfactual diffusion model for Detecting Anomalies in PET images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases containing both local and public datasets. The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates â€œunhealthy-to-healthyâ€ domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUVmax thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a>. </p>
<blockquote>
<p>æœ€å°åŒ–å¯¹åƒç´ çº§æ ‡æ³¨æ•°æ®çš„éœ€è¦ï¼Œä»¥è®­ç»ƒPETå›¾åƒç—…å˜æ£€æµ‹ä¸åˆ†å‰²ç½‘ç»œæ˜¯éå¸¸ç†æƒ³çš„ï¼Œå¹¶è€ƒè™‘åˆ°ä¸ä¸“å®¶æ ‡æ³¨ç›¸å…³çš„æ—¶é—´å’Œæˆæœ¬çº¦æŸï¼Œè¿™å¯èƒ½ä¼šå¸¦æ¥å˜é©ã€‚å½“å‰çš„é&#x2F;å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–äºè‡ªç¼–ç å™¨æˆ–ä»…åœ¨å¥åº·æ•°æ®ä¸Šè®­ç»ƒçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼›ç„¶è€Œï¼ŒåŸºäºGANçš„ç½‘ç»œç”±äºåŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªç«äº‰ç½‘ç»œã€æ¨¡å¼å´©æºƒç­‰é—®é¢˜è€Œæ›´å…·æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼±ç›‘ç£çš„éšå¼å¼•å¯¼PETå›¾åƒå¼‚å¸¸æ£€æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ã€‚è¯¥è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æ¥è‡ªå…­ä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„PETæ‰«æè¿›è¡Œå¼€å‘å’ŒéªŒè¯çš„ï¼Œå…¶ä¸­åŒ…æ‹¬æœ¬åœ°å’Œå…¬å…±æ•°æ®é›†çš„æ€»è®¡2652ä¸ªç—…ä¾‹ã€‚è®­ç»ƒæ˜¯é€šè¿‡æ³¨æ„åŠ›æ¨¡å—ä»¥å›¾åƒç±»åˆ«æ ‡ç­¾ï¼ˆå¥åº·ä¸éå¥åº·ï¼‰ä¸ºæ¡ä»¶è¿›è¡Œçš„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†éšå¼æ‰©æ•£å¼•å¯¼ã€‚æˆ‘ä»¬æ‰§è¡Œåäº‹å®ç”Ÿæˆï¼Œé€šè¿‡ç”Ÿæˆä¸å¥åº·è¾“å…¥å›¾åƒçš„åˆæˆå¥åº·ç‰ˆæœ¬ï¼Œä¿ƒè¿›â€œä¸å¥åº·åˆ°å¥åº·â€çš„é¢†åŸŸè½¬æ¢ï¼Œé€šè¿‡è®¡ç®—å·®å¼‚æ¥å®ç°å¼‚å¸¸æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„å¦‚SUVmaxé˜ˆå€¼æ³•ç­‰æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†åœ¨ç½‘ç»œä¸­èå…¥æ³¨æ„åŠ›æ¨¡å—æ£€æµ‹å°å¼‚å¸¸çš„é‡è¦æ€§ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00239v2">PDF</a> 32 pages, 6 figures, 4 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„éšå¼å¼•å¯¼è®¡æ•°æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ï¼Œç”¨äºPETå›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å‡å°‘åƒç´ çº§æ³¨é‡Šæ•°æ®çš„éœ€æ±‚ï¼Œé€šè¿‡æ¡ä»¶è®­ç»ƒå’Œå¥åº·ä¸ä¸å¥åº·å›¾åƒçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œé‡‡ç”¨éšå¼æ‰©æ•£å¼•å¯¼å’Œåäº‹å®ç”ŸæˆæŠ€æœ¯æ¥æ£€æµ‹å¼‚å¸¸ã€‚è¯¥ç ”ç©¶åœ¨åŒ…å«æœ¬åœ°å’Œå…¬å¼€æ•°æ®é›†çš„å…­ä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„PETæ‰«æä¸Šè¿›è¡Œå¼€å‘å’ŒéªŒè¯ï¼Œå¹¶ä¸å…¶ä»–æ·±åº¦å­¦ä¹ æ–¹æ³•åŠä¼ ç»Ÿæ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•IgCONDA-PETï¼Œç”¨äºPETå›¾åƒä¸­çš„ç—…å˜æ£€æµ‹å’Œåˆ†å‰²ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨å‡å°‘åƒç´ çº§æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œé™ä½æ—¶é—´å’Œæˆæœ¬ã€‚</li>
<li>IgCONDA-PETåˆ©ç”¨æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œè®­ç»ƒï¼ŒåŸºäºå›¾åƒç±»åˆ«æ ‡ç­¾ï¼ˆå¥åº·ä¸ä¸å¥åº·ï¼‰ã€‚</li>
<li>é‡‡ç”¨éšå¼æ‰©æ•£å¼•å¯¼å’Œåäº‹å®ç”ŸæˆæŠ€æœ¯ï¼Œå®ç°â€œä¸å¥åº·åˆ°å¥åº·â€çš„é¢†åŸŸè½¬æ¢ã€‚</li>
<li>æ–¹æ³•åœ¨åŒ…å«æœ¬åœ°å’Œå…¬å¼€æ•°æ®é›†çš„å¤šä¸ªå›é¡¾æ€§é˜Ÿåˆ—ä¸­è¿›è¡ŒéªŒè¯ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>å…¬å¼€å¯ç”¨ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git%E3%80%82">https://github.com/ahxmeds/IgCONDA-PET.gitã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.00239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2405.00239v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2405.00239v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Controllable-Text-to-3D-Generation-via-Surface-Aligned-Gaussian-Splatting"><a href="#Controllable-Text-to-3D-Generation-via-Surface-Aligned-Gaussian-Splatting" class="headerlink" title="Controllable Text-to-3D Generation via Surface-Aligned Gaussian   Splatting"></a>Controllable Text-to-3D Generation via Surface-Aligned Gaussian   Splatting</h2><p><strong>Authors:Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu</strong></p>
<p>While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view diffusion models by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base diffusion model using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score distillation algorithm. Building upon our MVControl architecture, we employ a unique hybrid diffusion guidance method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained geometry on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content. Project page: <a target="_blank" rel="noopener" href="https://lizhiqi49.github.io/MVControl/">https://lizhiqi49.github.io/MVControl/</a>. </p>
<blockquote>
<p>è™½ç„¶æ–‡æœ¬åˆ°3Då’Œå›¾åƒåˆ°3Dç”Ÿæˆä»»åŠ¡å·²ç»å¾—åˆ°äº†ç›¸å½“å¤šçš„å…³æ³¨ï¼Œä½†å®ƒä»¬ä¹‹é—´ä¸€ä¸ªé‡è¦çš„ä½†å°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸæ˜¯å¯æ§æ–‡æœ¬åˆ°3Dç”Ÿæˆï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨è¿™é¡¹å·¥ä½œä¸­çš„ä¸»è¦å…³æ³¨ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œ1ï¼‰æˆ‘ä»¬å¼•å…¥äº†å¤šè§†å›¾ControlNetï¼ˆMVControlï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆè¾¹ç¼˜ã€æ·±åº¦ã€æ³•çº¿å’Œæ¶‚é¸¦å›¾ç­‰é¢å¤–è¾“å…¥æ¡ä»¶ï¼Œå¢å¼ºç°æœ‰çš„é¢„è®­ç»ƒå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå¼•å…¥äº†ä¸€ä¸ªæ§åˆ¶æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨ä»è¾“å…¥æ¡ä»¶å›¾åƒå’Œç›¸æœºå§¿æ€è®¡ç®—å‡ºçš„å±€éƒ¨å’Œå…¨å±€åµŒå…¥æ¥æ§åˆ¶åŸºç¡€æ‰©æ•£æ¨¡å‹ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒMVControlèƒ½å¤Ÿä¸ºåŸºäºä¼˜åŒ–çš„3Dç”Ÿæˆæä¾›3Dæ‰©æ•£æŒ‡å¯¼ã€‚2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„å¤šé˜¶æ®µ3Dç”Ÿæˆç®¡é“ï¼Œå®ƒåˆ©ç”¨æœ€è¿‘çš„é‡å»ºæ¨¡å‹å’Œåˆ†æ•°è’¸é¦ç®—æ³•çš„ä¼˜ç‚¹ã€‚åŸºäºæˆ‘ä»¬çš„MVControlæ¶æ„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„æ··åˆæ‰©æ•£å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºæŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨3Dé«˜æ–¯ä½œä¸ºè¡¨ç¤ºå½¢å¼ï¼Œè€Œä¸æ˜¯å¸¸ç”¨çš„éšå¼è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜é¦–åˆ›äº†SuGaRçš„ä½¿ç”¨ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆè¡¨ç¤ºï¼Œå°†é«˜æ–¯ç»‘å®šåˆ°ç½‘æ ¼ä¸‰è§’å½¢é¢ä¸Šã€‚è¿™ç§æ–¹æ³•ç¼“è§£äº†3Dé«˜æ–¯ä¸­çš„å‡ ä½•é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨ç½‘æ ¼ä¸Šç›´æ¥å¡‘é€ ç²¾ç»†çš„å‡ ä½•å½¢çŠ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç¨³å¥çš„æ³›åŒ–ï¼Œå¹¶èƒ½ç”Ÿæˆå¯æ§çš„é«˜è´¨é‡3Då†…å®¹ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lizhiqi49.github.io/MVControl/%E3%80%82">https://lizhiqi49.github.io/MVControl/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09981v3">PDF</a> 3DV-2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡å…³æ³¨å¯æ§æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ï¼Œé’ˆå¯¹è¿™ä¸€ä»»åŠ¡æå‡ºäº†Multi-view ControlNetï¼ˆMVControlï¼‰ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥æ¶æ„ç»“åˆäº†é¢å¤–çš„è¾“å…¥æ¡ä»¶ï¼Œå¦‚è¾¹ç¼˜ã€æ·±åº¦ã€æ³•çº¿å’Œè‰å›¾å›¾ç­‰ï¼Œæå‡äº†ç°æœ‰é¢„è®­ç»ƒçš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ã€‚æå‡ºäº†ä¸€å¥—é«˜æ•ˆçš„åŸºäºMVControlçš„å¤šé˜¶æ®µä¸‰ç»´ç”Ÿæˆæµç¨‹ï¼Œç»“åˆäº†æœ€è¿‘çš„å¤§å‹é‡å»ºæ¨¡å‹å’Œåˆ†æ•°è’¸é¦ç®—æ³•çš„ä¼˜ç‚¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§ä¸‰ç»´å†…å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„Multi-view ControlNetï¼ˆMVControlï¼‰ï¼Œæ—¨åœ¨æé«˜ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>MVControlæ¶æ„é€šè¿‡æ•´åˆé¢å¤–çš„è¾“å…¥æ¡ä»¶ï¼ˆå¦‚è¾¹ç¼˜ã€æ·±åº¦ã€æ³•çº¿å’Œè‰å›¾å›¾ç­‰ï¼‰æ¥å®ç°å¯æ§æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆçš„ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ¡ä»¶æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å±€éƒ¨å’Œå…¨å±€åµŒå…¥æ¥æ§åˆ¶åŸºç¡€æ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›åµŒå…¥æ˜¯ä»è¾“å…¥æ¡ä»¶å›¾åƒå’Œç›¸æœºå§¿æ€è®¡ç®—çš„ã€‚</li>
<li>MVControlèƒ½å¤Ÿåœ¨è¿›è¡Œä¼˜åŒ–æ—¶æä¾›ä¸‰ç»´æ‰©æ•£æŒ‡å¯¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šé˜¶æ®µä¸‰ç»´ç”Ÿæˆæµç¨‹ï¼Œç»“åˆäº†å¤§å‹é‡å»ºæ¨¡å‹å’Œåˆ†æ•°è’¸é¦ç®—æ³•çš„ä¼˜ç‚¹ã€‚</li>
<li>é‡‡ç”¨ä¸‰ç»´é«˜æ–¯ä½œä¸ºè¡¨ç¤ºå½¢å¼ä»¥æé«˜æ•ˆç‡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ··åˆè¡¨ç¤ºæ–¹æ³•SuGaRï¼Œä»¥è§£å†³ä¸‰ç»´é«˜æ–¯ä¸­çš„å‡ ä½•é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2403.09981v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2403.09981v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2403.09981v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2403.09981v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2403.09981v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_0_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ViSIR Vision Transformer Single Image Reconstruction Method for Earth   System Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_NeRF/2502.05708v1/page_4_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  GWRF A Generalizable Wireless Radiance Field for Wireless Signal   Propagation Modeling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
