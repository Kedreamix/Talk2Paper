<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-02-12  ViSIR Vision Transformer Single Image Reconstruction Method for Earth   System Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-12-更新"><a href="#2025-02-12-更新" class="headerlink" title="2025-02-12 更新"></a>2025-02-12 更新</h1><h2 id="ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models"><a href="#ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models" class="headerlink" title="ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models"></a>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models</h2><p><strong>Authors:Ehsan Zeraatkar, Salah Faroughi, Jelena Tesic</strong></p>
<p>Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM). </p>
<blockquote>
<p>目的：地球系统模型（ESM）整合了大气、海洋、陆地、冰川和生物圈的相互作用，以在多种条件下估计区域和全球气候的状态。由于ESM高度复杂，因此使用深度神经网络架构来建模其复杂性并存储降采样数据。在本文中，我们提出Vision Transformer正弦表示网络（ViSIR）来改进ESM数据的单图像超分辨率（SR）重建任务。方法：ViSIR结合了Vision Transformer（ViT）的超分辨率能力和正弦表示网络（SIREN）的高频细节保留功能，以解决SR任务中观察到的光谱偏置问题。结果：ViSIR在PSNR方面平均比ViT高出4.1 dB，比SIREN高出7.5 dB，比SR生成对抗网络（SR-GANs）高出7.1 dB。结论：经过评估并与最新方法进行比较，结果表明，所提出的ViSIR在均方误差（MSE）、峰值信噪比（PSNR）和结构相似性指数度量（SSIM）等方面均优于其他方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06741v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了地球系统模型（ESMs）的复杂性及其利用深度神经网络架构进行建模的方法。针对ESM数据的单图像超分辨率（SR）重建任务，提出了结合Vision Transformer与Sinusoidal Representation Network优势的Vision Transformer Sinusoidal Representation（ViSIR）网络。ViSIR在SR任务中解决了光谱偏差问题，并在三个不同测量指标上平均比ViT高出4.1dB，比SIREN高出7.5dB，比SR-GANs高出7.1dB的峰值信号噪声比（PSNR）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>地球系统模型（ESMs）整合多个系统（如大气、海洋、陆地、冰和生物圈）的交互作用，用于估算各种条件下的区域和全球气候状态。</li>
<li>ESMs的复杂性需要使用深度神经网络进行建模，并处理下采样数据。</li>
<li>Vision Transformer Sinusoidal Representation（ViSIR）网络结合了Vision Transformer（ViT）的超分辨率能力和Sinusoidal Representation Network（SIREN）的高频细节保留功能。</li>
<li>ViSIR解决了SR任务中的光谱偏差问题。</li>
<li>ViSIR在SR任务上的性能超过了其他方法，包括ViT、SIREN和SR-GANs，平均PSNR有所提高。</li>
<li>本文通过对比实验验证了ViSIR的性能，使用了包括MSE、PSNR和SSIM在内的多个评估指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fully-Exploiting-Vision-Foundation-Model’s-Profound-Prior-Knowledge-for-Generalizable-RGB-Depth-Driving-Scene-Parsing"><a href="#Fully-Exploiting-Vision-Foundation-Model’s-Profound-Prior-Knowledge-for-Generalizable-RGB-Depth-Driving-Scene-Parsing" class="headerlink" title="Fully Exploiting Vision Foundation Model’s Profound Prior Knowledge for   Generalizable RGB-Depth Driving Scene Parsing"></a>Fully Exploiting Vision Foundation Model’s Profound Prior Knowledge for   Generalizable RGB-Depth Driving Scene Parsing</h2><p><strong>Authors:Sicen Guo, Tianyou Wen, Chuang-Wei Liu, Qijun Chen, Rui Fan</strong></p>
<p>Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at <a target="_blank" rel="noopener" href="https://mias.group/HFIT">https://mias.group/HFIT</a>. </p>
<blockquote>
<p>最近的视觉基础模型（VFMs），通常基于视觉转换器（ViT），已经大大推进了众多计算机视觉任务的发展。尽管它们在仅专注于RGB图像的任务中取得了成功，但VFM在RGB-深度驾驶场景解析中的潜力仍大多未被探索。在本文中，我们朝着这一新兴研究领域迈出了一步，通过研究一种可行的技术，以充分利用VFM进行可推广的RGB-深度驾驶场景解析。具体来说，我们探索了RGB和深度数据的内在特性，从而提出了异质特征集成转换器（HFIT）。该网络能够在不需要重新训练ViT的情况下，有效地提取和集成全面的异质特征。来自VFM的相对深度预测结果用作HFIT侧适配器的输入，克服了对深度图的依赖所带来的限制。我们提出的HFIT在Cityscapes和KITTI语义数据集上，相较于其他传统的单模态和数据融合场景解析网络、预训练的VFM和ViT适配器，表现出卓越的性能。我们相信，这一新颖的策略为基于VFM的数据融合技术用于驾驶场景解析的未来创新铺平了道路。我们的源代码已公开在<a target="_blank" rel="noopener" href="https://mias.group/HFIT%E3%80%82">https://mias.group/HFIT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06219v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>基于Vision Transformer（ViT）的最新视觉基础模型（VFMs）已在众多计算机视觉任务中取得显著进展。本文主要探讨了如何将VFMs的潜力充分发挥在RGB-深度驾驶场景解析中。通过探索RGB和深度数据的内在特性，提出了一种异质特征集成变压器（HFIT）网络，该网络能够在无需重新训练ViT的情况下，有效地提取和整合全面的异质特征。在Cityscapes和KITTI Semantics数据集上，与其他的传统单模态和数据融合场景解析网络、预训练的VFMs和ViT适配器相比，所提出的HFIT表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer（ViT）为基础的视觉基础模型（VFMs）在RGB-深度驾驶场景解析中具有潜在价值。</li>
<li>文章探索了RGB和深度数据的内在特性。</li>
<li>提出了Heterogeneous Feature Integration Transformer（HFIT）网络，能高效提取和整合全面的异质特征。</li>
<li>HFIT网络克服了依赖深度图的限制，使用来自VFMs的相对深度预测结果作为输入。</li>
<li>HFIT在Cityscapes和KITTI Semantics数据集上的性能优于其他传统单模态和数据融合场景解析网络、预训练的VFMs和ViT适配器。</li>
<li>文章提供了一种新型策略，为基于VFM的数据融合技术在驾驶场景解析中的应用开辟了道路。</li>
<li>源代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Uni-Retrieval-A-Multi-Style-Retrieval-Framework-for-STEM’s-Education"><a href="#Uni-Retrieval-A-Multi-Style-Retrieval-Framework-for-STEM’s-Education" class="headerlink" title="Uni-Retrieval: A Multi-Style Retrieval Framework for STEM’s Education"></a>Uni-Retrieval: A Multi-Style Retrieval Framework for STEM’s Education</h2><p><strong>Authors:Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan</strong></p>
<p>In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks. This advancement provides a scalable and precise solution for diverse educational needs. </p>
<blockquote>
<p>在人工智能辅助教学中，利用多种查询风格来解读抽象文本描述对于确保高质量教学至关重要。然而，当前的检索模型主要关注自然文本图像检索，由于检索过程中的模糊性，它们对教育场景的适应性不足。在本文中，我们针对教育场景提出了一项多样化的表达检索任务，支持基于多种查询风格和表达方式的检索。我们介绍了STEM教育检索数据集（SER），其中包含超过24,000种不同风格的查询对，以及基于提示调整的Uni-Retrieval高效且风格多样的视觉语言检索模型。Uni-Retrieval提取查询风格特征作为原型，并建立一个不断更新的提示库，其中包含用于不同查询的提示标记。这个库可以在测试期间进行更新，以表示不同主题检索场景的领域特定知识。我们的框架通过基于原型相似性的动态检索提示标记，展示了可扩展性和稳健性，有效地促进了未知查询的学习。实验结果表明，Uni-Retrieval在大多数检索任务中的性能优于现有检索模型。这一进展为多样化的教育需求提供了可扩展且精确的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在人工智能辅助教学领域，解读抽象文本描述时利用多种查询风格至关重要。当前大多数检索模型主要关注自然语言文本图像检索，但由于检索过程中的模糊性，这些模型在教育场景下表现不足。针对这一问题，本文提出了针对教育场景的多样化表达检索任务，支持基于多种查询风格和表达方式的检索。介绍了STEM教育检索数据集（SER），包含超过24,000种不同风格的查询对，以及基于提示调节的高效、多样化的Uni-Retrieval视觉语言模型。该模型通过提取查询风格特征作为原型并建立包含各种查询提示符号的提示库来支持动态更新。实验结果表明，Uni-Retrieval在大多数检索任务中优于现有检索模型，为多样化的教育需求提供了可扩展和精确的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前AI辅助教学领域的检索模型主要关注自然文本图像检索，但其在教育场景中的应用存在局限性。</li>
<li>教育场景下的检索需求多样化，需要支持多种查询风格和表达方式。</li>
<li>STEM教育检索数据集（SER）包含多样化的查询对，有助于满足教育场景下的检索需求。</li>
<li>Uni-Retrieval模型通过提取查询风格特征并建立提示库来支持动态更新和多样化的查询。</li>
<li>Uni-Retrieval模型具有良好的可扩展性和稳健性，能够处理未知查询。</li>
<li>实验结果表明，Uni-Retrieval在大多数检索任务中的性能优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-Visual-Embedding-Spaces-Induced-by-Vision-Transformers-for-Online-Auto-Parts-Marketplaces"><a href="#Exploring-Visual-Embedding-Spaces-Induced-by-Vision-Transformers-for-Online-Auto-Parts-Marketplaces" class="headerlink" title="Exploring Visual Embedding Spaces Induced by Vision Transformers for   Online Auto Parts Marketplaces"></a>Exploring Visual Embedding Spaces Induced by Vision Transformers for   Online Auto Parts Marketplaces</h2><p><strong>Authors:Cameron Armijo, Pablo Rivas</strong></p>
<p>This study examines the capabilities of the Vision Transformer (ViT) model in generating visual embeddings for images of auto parts sourced from online marketplaces, such as Craigslist and OfferUp. By focusing exclusively on single-modality data, the analysis evaluates ViT’s potential for detecting patterns indicative of illicit activities. The workflow involves extracting high-dimensional embeddings from images, applying dimensionality reduction techniques like Uniform Manifold Approximation and Projection (UMAP) to visualize the embedding space, and using K-Means clustering to categorize similar items. Representative posts nearest to each cluster centroid provide insights into the composition and characteristics of the clusters. While the results highlight the strengths of ViT in isolating visual patterns, challenges such as overlapping clusters and outliers underscore the limitations of single-modal approaches in this domain. This work contributes to understanding the role of Vision Transformers in analyzing online marketplaces and offers a foundation for future advancements in detecting fraudulent or illegal activities. </p>
<blockquote>
<p>本研究探讨了Vision Transformer（ViT）模型在生成源自在线市场平台（如Craigslist和OfferUp）的汽车零部件图像视觉嵌入方面的能力。通过专注于单一模态数据，分析评估了ViT检测非法活动迹象模式潜力。工作流程包括从图像中提取高维嵌入，应用统一流形逼近和投影（UMAP）等降维技术来可视化嵌入空间，并使用K-Means聚类算法对类似项目进行分类。每个聚类中心的代表性帖子提供了有关聚类组成和特征的信息。虽然结果突出了ViT在隔离视觉模式方面的优势，但聚类重叠和异常值等挑战也突显了在此领域中单模态方法的局限性。这项工作有助于了解Vision Transformers在分析在线市场平台中的作用，并为未来检测欺诈或非法活动的技术进步奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05756v1">PDF</a> AAAI 2025 Workshop on AI for Social Impact: Bridging Innovations in   Finance, Social Media, and Crime Prevention</p>
<p><strong>Summary</strong></p>
<p>本研究探讨了Vision Transformer（ViT）模型在生成来自在线市场如Craigslist和OfferUp的汽车零部件图像视觉嵌入方面的能力。研究通过单模态数据评估ViT在检测非法活动迹象方面的潜力，通过提取图像的高维嵌入、应用降维技术如Uniform Manifold Approximation和Projection（UMAP）进行可视化嵌入空间，并使用K-Means聚类对类似项目进行分类。靠近每个聚类中心的代表性帖子揭示了聚类的组成和特点。虽然结果突出了ViT在隔离视觉模式方面的优势，但重叠的聚类和异常值等问题凸显了单模态方法在此领域的局限性。本研究为理解Vision Transformers在分析在线市场中的作用以及未来检测欺诈或非法活动的进步提供了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer（ViT）模型被用于生成在线市场汽车零件图像视觉嵌入。</li>
<li>研究通过单模态数据评估ViT在检测非法活动迹象方面的能力。</li>
<li>高维嵌入通过降维技术可视化，并应用K-Means聚类分析类似项目。</li>
<li>聚类分析揭示了不同项目之间的特征和关联。</li>
<li>ViT模型在隔离视觉模式方面表现出优势。</li>
<li>单模态方法在分析在线市场时存在局限性，如重叠的聚类和异常值问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05756">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Kronecker-Mask-and-Interpretive-Prompts-are-Language-Action-Video-Learners"><a href="#Kronecker-Mask-and-Interpretive-Prompts-are-Language-Action-Video-Learners" class="headerlink" title="Kronecker Mask and Interpretive Prompts are Language-Action Video   Learners"></a>Kronecker Mask and Interpretive Prompts are Language-Action Video   Learners</h2><p><strong>Authors:Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li</strong></p>
<p>Contrastive language-image pretraining (CLIP) has significantly advanced image-based vision learning. A pressing topic subsequently arises: how can we effectively adapt CLIP to the video domain? Recent studies have focused on adjusting either the textual or visual branch of CLIP for action recognition. However, we argue that adaptations of both branches are crucial. In this paper, we propose \textbf{CLAVER}: a \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er}, designed to shift CLIP’s focus from the alignment of static visual objects and concrete nouns to the alignment of dynamic action behaviors and abstract verbs. Specifically, we introduce a novel Kronecker mask attention for temporal modeling. Our tailored Kronecker mask offers three benefits 1) it expands the temporal receptive field for each token, 2) it serves as an effective spatiotemporal heterogeneity inductive bias, mitigating the issue of spatiotemporal homogenization, and 3) it can be seamlessly plugged into transformer-based models. Regarding the textual branch, we leverage large language models to generate diverse, sentence-level and semantically rich interpretive prompts of actions, which shift the model’s focus towards the verb comprehension. Extensive experiments on various benchmarks and learning scenarios demonstrate the superiority and generality of our approach. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）在基于图像的视觉学习方面取得了重大进展。随之而来出现了一个紧迫的问题：我们如何有效地将CLIP适应到视频领域？近期的研究主要关注调整CLIP的文本或视觉分支来进行动作识别。然而，我们认为两个分支的适应都是至关重要的。在本文中，我们提出了CLAVER：一种对比语言动作视频学习器（Contrastive Language-Action Video Learner），旨在将CLIP的重点从静态视觉对象和具体名词的对齐转移到动态行为动作和抽象动词的对齐上。具体来说，我们引入了一种新型克罗内克掩膜注意力来实现时序建模。我们定制的克罗内克掩膜具有三个优点：1）它扩大了每个标记的时序感受野；2）它作为有效的时空异质性归纳偏置，缓解了时空同质化的问题；3）它可以无缝地插入到基于变压器模型。对于文本分支，我们利用大型语言模型生成多样、句子级别且语义丰富的动作解释性提示，使模型关注动词的理解。在各种基准测试和学习场景的大量实验证明了我们的方法的优越性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03549v3">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了如何将Contrastive language-image pretraining (CLIP)技术应用于视频领域的问题，并指出了对CLIP的文本和视觉分支进行双重调整的重要性。为此，作者提出了CLAVER模型，该模型通过引入Kronecker mask注意力机制实现了对动态行为动作与抽象动词的对齐。实验表明，该模型在多种基准测试和学习场景中表现出优越的性能和泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>CLAVER模型旨在将CLIP的焦点从静态视觉对象和具体名词的对齐转移到动态行为动作和抽象动词的对齐。</li>
<li>提出了新型的Kronecker mask注意力机制，用于实现时间建模。这种机制提供了三个优点：扩大每个令牌的临时接收场，作为有效的时空异质性归纳偏置，并可以无缝地插入基于transformer的模型中。</li>
<li>在文本分支方面，作者利用大型语言模型生成了多样、句子级别且语义丰富的动作解释性提示，使模型更加关注动词的理解。</li>
<li>CLAVER模型在多种基准测试和学习场景中表现出优越的性能和泛化能力。</li>
<li>该研究强调了仅调整CLIP的文本或视觉分支是不够的，两者的调整都是至关重要的。</li>
<li>Kronecker mask注意力机制可以作为一种有效的工具，帮助模型理解动态行为和抽象动词的对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03549">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ranking-aware-adapter-for-text-driven-image-ordering-with-CLIP"><a href="#Ranking-aware-adapter-for-text-driven-image-ordering-with-CLIP" class="headerlink" title="Ranking-aware adapter for text-driven image ordering with CLIP"></a>Ranking-aware adapter for text-driven image ordering with CLIP</h2><p><strong>Authors:Wei-Hsiang Yu, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</strong></p>
<p>Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies typically focus on the reasoning based on a single image and heavily depend on text prompting, limiting their ability to learn comprehensive understanding from multiple images. To address this, we propose an effective yet efficient approach that reframes the CLIP model into a learning-to-rank task and introduces a lightweight adapter to augment CLIP for text-guided image ranking. Specifically, our approach incorporates learnable prompts to adapt to new instructions for ranking purposes and an auxiliary branch with ranking-aware attention, leveraging text-conditioned visual differences for additional supervision in image ranking. Our ranking-aware adapter consistently outperforms fine-tuned CLIPs on various tasks and achieves competitive results compared to state-of-the-art models designed for specific tasks like facial age estimation and image quality assessment. Overall, our approach primarily focuses on ranking images with a single instruction, which provides a natural and generalized way of learning from visual differences across images, bypassing the need for extensive text prompts tailored to individual tasks. Code is available: github.com&#x2F;uynaes&#x2F;RankingAwareCLIP. </p>
<blockquote>
<p>近期视觉语言模型（VLMs）的进步在需要定量概念的下游任务中取得了显著进展，例如面部年龄估计和图像质量评估，使得VLMs能够探索图像排名和检索等应用。然而，现有研究通常基于单张图像进行推理，并严重依赖于文本提示，这限制了它们从多张图像中学习全面理解的能力。为了解决这个问题，我们提出了一种有效且高效的方法，将CLIP模型重构为学习排名任务，并引入了一个轻量级适配器来增强CLIP进行文本引导的图像排名。具体来说，我们的方法采用可学习的提示来适应新的排名指令，并使用带有排名感知注意力的辅助分支，利用文本条件下的视觉差异进行图像排名的额外监督。我们的排名感知适配器在各种任务上始终优于微调过的CLIP，并在面部年龄估计和图像质量评估等特定任务上取得了具有竞争力的结果。总的来说，我们的方法主要侧重于使用单个指令对图像进行排名，这提供了一种从图像之间视觉差异学习的自然且通用的方式，避免了需要大量针对个别任务定制的文本提示的需求。代码可用：github.com&#x2F;uynaes&#x2F;RankingAwareCLIP。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06760v3">PDF</a> Accepted by ICLR2025. Github link: github.com&#x2F;uynaes&#x2F;RankingAwareCLIP</p>
<p><strong>Summary</strong></p>
<p>本文提出一种有效且高效的针对CLIP模型的改进方法，用于文本指导的图像排名任务。该方法通过引入学习排名任务的提示和辅助分支，实现对CLIP模型的重新构建和增强。新模型能够在多个图像上学习全面的理解，超越基于单一图像的任务依赖文本提示的限制。实验结果显示，该方法在各种任务上的性能优于微调后的CLIP模型，并在面部年龄估计和图像质量评估等特定任务上达到与最先进的模型相当的结果。总的来说，该方法主要通过单一的指令对图像进行排名，提供一种从图像间视觉差异学习的一般化方式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期视觉语言模型（VLMs）在需要定量概念的下游任务（如面部年龄估计和图像质量评估）中取得了显著进展。</li>
<li>VLMs现在可用于图像排名和检索等应用。</li>
<li>现有研究通常依赖于基于单图像的推理和文本提示，限制了它们从多个图像中学习全面理解的能力。</li>
<li>提出了一种改进CLIP模型的有效方法，用于文本指导的图像排名任务，通过引入学习排名任务的提示和辅助分支实现。</li>
<li>该方法能够超越基于单一图像的任务依赖文本提示的限制，在多个图像上学习全面的理解。</li>
<li>该方法在多种任务上的性能优于微调后的CLIP模型，并在特定任务上达到与最先进模型相当的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Magnetic-Resonance-Image-Processing-Transformer-for-General-Accelerated-Image-Reconstruction"><a href="#Magnetic-Resonance-Image-Processing-Transformer-for-General-Accelerated-Image-Reconstruction" class="headerlink" title="Magnetic Resonance Image Processing Transformer for General Accelerated   Image Reconstruction"></a>Magnetic Resonance Image Processing Transformer for General Accelerated   Image Reconstruction</h2><p><strong>Authors:Guoyao Shen, Mengyu Li, Stephan Anderson, Chad W. Farris, Xin Zhang</strong></p>
<p>Recent advancements in deep learning have enabled the development of generalizable models that achieve state-of-the-art performance across various imaging tasks. Vision Transformer (ViT)-based architectures, in particular, have demonstrated strong feature extraction capabilities when pre-trained on large-scale datasets. In this work, we introduce the Magnetic Resonance Image Processing Transformer (MR-IPT), a ViT-based framework designed to enhance the generalizability and robustness of accelerated MRI reconstruction. Unlike conventional deep learning models that require separate training for different acceleration factors, MR-IPT is pre-trained on a large-scale dataset encompassing multiple undersampling patterns and acceleration settings, enabling a unified reconstruction framework. By leveraging a shared transformer backbone, MR-IPT effectively learns universal feature representations, allowing it to generalize across diverse reconstruction tasks. Extensive experiments demonstrate that MR-IPT outperforms both CNN-based and existing transformer-based methods, achieving superior reconstruction quality across varying acceleration factors and sampling masks. Moreover, MR-IPT exhibits strong robustness, maintaining high performance even under unseen acquisition setups, highlighting its potential as a scalable and efficient solution for accelerated MRI. Our findings suggest that transformer-based general models can significantly advance MRI reconstruction, offering improved adaptability and stability compared to traditional deep learning approaches. </p>
<blockquote>
<p>最近深度学习的发展促进了通用模型的开发，这些模型在各种成像任务中实现了最先进的性能。特别是基于视觉转换器（ViT）的架构，在大型数据集上进行预训练后，表现出了强大的特征提取能力。在这项工作中，我们介绍了磁共振图像处理器转换器（MR-IPT），这是一个基于ViT的框架，旨在提高加速磁共振成像重建的通用性和稳健性。不同于需要针对不同加速因子进行单独训练的传统深度学习模型，MR-IPT是在大规模数据集上进行预训练的，涵盖多种欠采样模式和加速设置，从而实现统一的重建框架。通过利用共享转换器主干，MR-IPT有效地学习通用特征表示，从而能够在各种重建任务中通用化。大量实验表明，MR-IPT在基于CNN的和现有的基于转换器的方法中表现优于前者，在各种加速因子和采样掩膜下实现更高的重建质量。此外，MR-IPT表现出强大的稳健性，即使在未见过的采集设置下也能保持高性能，这突显了其作为加速磁共振成像的可扩展和高效解决方案的潜力。我们的研究结果表明，基于转换器的通用模型可以极大地推动磁共振成像重建的发展，与传统深度学习方法相比，提供了更好的适应性和稳定性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15098v2">PDF</a> 28 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>该摘要介绍了基于深度学习的最新进展在视觉转化器模型（如MR-IPT模型）中展现出优秀的表现能力，特别是对大型图像数据处理任务有很好的表现。MR-IPT模型通过预训练学习多种下采样模式和加速设置的大规模数据集，实现了统一的重建框架，能够跨多种重建任务进行通用特征表示的学习，具有良好的泛化能力和鲁棒性。通过一系列实验表明，MR-IPT在CNN和传统转换器方法中表现更出色，可以维持高质量的重建结果并且具有较强的适应性，因此可为MRI快速成像提供良好的应用前景。综上所述，使用视觉转换器架构可为医学图像处理（特别是MRI图像重建）开辟新的应用道路。 </p>
<p><strong>Key Takeaways</strong><br>以下是对此论文最为关键的几个见解：</p>
<ul>
<li>MR-IPT模型基于视觉转换器架构，旨在提高MRI重建的通用性和鲁棒性。 </li>
<li>MR-IPT通过预训练学习涵盖多种下采样模式和加速设置的大规模数据集来实现统一的重建框架。 </li>
<li>MR-IPT具有优秀的泛化能力，可跨不同的重建任务进行特征表示学习。 </li>
<li>MR-IPT的性能表现优于传统的CNN和基于转换器的方法，在不同加速因子和采样掩膜下实现高质量的重建结果。 </li>
<li>MR-IPT在不同采集设置下展现出强大的鲁棒性，即使面对未见过的采集设置也能保持高性能表现。 </li>
<li>MR-IPT模型具备可扩展性和高效性，为解决MRI的快速成像问题提供了潜在解决方案。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15098">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2405.15098v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2405.15098v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_检测_分割_跟踪/2411.18409v2/page_5_0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-02-12  Enhancing Ground-to-Aerial Image Matching for Visual Misinformation   Detection Using Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_视频理解/2502.06428v1/page_0_0.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-02-12  CoS Chain-of-Shot Prompting for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
