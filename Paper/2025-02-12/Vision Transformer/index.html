<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ViSIR Vision Transformer Single Image Reconstruction Method for Earth   System Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    28 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models"><a href="#ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models" class="headerlink" title="ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models"></a>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models</h2><p><strong>Authors:Ehsan Zeraatkar, Salah Faroughi, Jelena Tesic</strong></p>
<p>Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM). </p>
<blockquote>
<p>ç›®çš„ï¼šåœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰æ•´åˆäº†å¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å·å’Œç”Ÿç‰©åœˆçš„ç›¸äº’ä½œç”¨ï¼Œä»¥åœ¨å¤šç§æ¡ä»¶ä¸‹ä¼°è®¡åŒºåŸŸå’Œå…¨çƒæ°”å€™çš„çŠ¶æ€ã€‚ç”±äºESMé«˜åº¦å¤æ‚ï¼Œå› æ­¤ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„æ¥å»ºæ¨¡å…¶å¤æ‚æ€§å¹¶å­˜å‚¨é™é‡‡æ ·æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºVision Transformeræ­£å¼¦è¡¨ç¤ºç½‘ç»œï¼ˆViSIRï¼‰æ¥æ”¹è¿›ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ã€‚æ–¹æ³•ï¼šViSIRç»“åˆäº†Vision Transformerï¼ˆViTï¼‰çš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›å’Œæ­£å¼¦è¡¨ç¤ºç½‘ç»œï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™åŠŸèƒ½ï¼Œä»¥è§£å†³SRä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°çš„å…‰è°±åç½®é—®é¢˜ã€‚ç»“æœï¼šViSIRåœ¨PSNRæ–¹é¢å¹³å‡æ¯”ViTé«˜å‡º4.1 dBï¼Œæ¯”SIRENé«˜å‡º7.5 dBï¼Œæ¯”SRç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆSR-GANsï¼‰é«˜å‡º7.1 dBã€‚ç»“è®ºï¼šç»è¿‡è¯„ä¼°å¹¶ä¸æœ€æ–°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ViSIRåœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ç­‰æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06741v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMsï¼‰çš„å¤æ‚æ€§åŠå…¶åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ã€‚é’ˆå¯¹ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ï¼Œæå‡ºäº†ç»“åˆVision Transformerä¸Sinusoidal Representation Networkä¼˜åŠ¿çš„Vision Transformer Sinusoidal Representationï¼ˆViSIRï¼‰ç½‘ç»œã€‚ViSIRåœ¨SRä»»åŠ¡ä¸­è§£å†³äº†å…‰è°±åå·®é—®é¢˜ï¼Œå¹¶åœ¨ä¸‰ä¸ªä¸åŒæµ‹é‡æŒ‡æ ‡ä¸Šå¹³å‡æ¯”ViTé«˜å‡º4.1dBï¼Œæ¯”SIRENé«˜å‡º7.5dBï¼Œæ¯”SR-GANsé«˜å‡º7.1dBçš„å³°å€¼ä¿¡å·å™ªå£°æ¯”ï¼ˆPSNRï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMsï¼‰æ•´åˆå¤šä¸ªç³»ç»Ÿï¼ˆå¦‚å¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å’Œç”Ÿç‰©åœˆï¼‰çš„äº¤äº’ä½œç”¨ï¼Œç”¨äºä¼°ç®—å„ç§æ¡ä»¶ä¸‹çš„åŒºåŸŸå’Œå…¨çƒæ°”å€™çŠ¶æ€ã€‚</li>
<li>ESMsçš„å¤æ‚æ€§éœ€è¦ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å¤„ç†ä¸‹é‡‡æ ·æ•°æ®ã€‚</li>
<li>Vision Transformer Sinusoidal Representationï¼ˆViSIRï¼‰ç½‘ç»œç»“åˆäº†Vision Transformerï¼ˆViTï¼‰çš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›å’ŒSinusoidal Representation Networkï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™åŠŸèƒ½ã€‚</li>
<li>ViSIRè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚</li>
<li>ViSIRåœ¨SRä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬ViTã€SIRENå’ŒSR-GANsï¼Œå¹³å‡PSNRæœ‰æ‰€æé«˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†ViSIRçš„æ€§èƒ½ï¼Œä½¿ç”¨äº†åŒ…æ‹¬MSEã€PSNRå’ŒSSIMåœ¨å†…çš„å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06741v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fully-Exploiting-Vision-Foundation-Modelâ€™s-Profound-Prior-Knowledge-for-Generalizable-RGB-Depth-Driving-Scene-Parsing"><a href="#Fully-Exploiting-Vision-Foundation-Modelâ€™s-Profound-Prior-Knowledge-for-Generalizable-RGB-Depth-Driving-Scene-Parsing" class="headerlink" title="Fully Exploiting Vision Foundation Modelâ€™s Profound Prior Knowledge for   Generalizable RGB-Depth Driving Scene Parsing"></a>Fully Exploiting Vision Foundation Modelâ€™s Profound Prior Knowledge for   Generalizable RGB-Depth Driving Scene Parsing</h2><p><strong>Authors:Sicen Guo, Tianyou Wen, Chuang-Wei Liu, Qijun Chen, Rui Fan</strong></p>
<p>Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at <a target="_blank" rel="noopener" href="https://mias.group/HFIT">https://mias.group/HFIT</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œé€šå¸¸åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œå·²ç»å¤§å¤§æ¨è¿›äº†ä¼—å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å‘å±•ã€‚å°½ç®¡å®ƒä»¬åœ¨ä»…ä¸“æ³¨äºRGBå›¾åƒçš„ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†VFMåœ¨RGB-æ·±åº¦é©¾é©¶åœºæ™¯è§£æä¸­çš„æ½œåŠ›ä»å¤§å¤šæœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æœç€è¿™ä¸€æ–°å…´ç ”ç©¶é¢†åŸŸè¿ˆå‡ºäº†ä¸€æ­¥ï¼Œé€šè¿‡ç ”ç©¶ä¸€ç§å¯è¡Œçš„æŠ€æœ¯ï¼Œä»¥å……åˆ†åˆ©ç”¨VFMè¿›è¡Œå¯æ¨å¹¿çš„RGB-æ·±åº¦é©¾é©¶åœºæ™¯è§£æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†RGBå’Œæ·±åº¦æ•°æ®çš„å†…åœ¨ç‰¹æ€§ï¼Œä»è€Œæå‡ºäº†å¼‚è´¨ç‰¹å¾é›†æˆè½¬æ¢å™¨ï¼ˆHFITï¼‰ã€‚è¯¥ç½‘ç»œèƒ½å¤Ÿåœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒViTçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æå–å’Œé›†æˆå…¨é¢çš„å¼‚è´¨ç‰¹å¾ã€‚æ¥è‡ªVFMçš„ç›¸å¯¹æ·±åº¦é¢„æµ‹ç»“æœç”¨ä½œHFITä¾§é€‚é…å™¨çš„è¾“å…¥ï¼Œå…‹æœäº†å¯¹æ·±åº¦å›¾çš„ä¾èµ–æ‰€å¸¦æ¥çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºçš„HFITåœ¨Cityscapeså’ŒKITTIè¯­ä¹‰æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºå…¶ä»–ä¼ ç»Ÿçš„å•æ¨¡æ€å’Œæ•°æ®èåˆåœºæ™¯è§£æç½‘ç»œã€é¢„è®­ç»ƒçš„VFMå’ŒViTé€‚é…å™¨ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ä¸€æ–°é¢–çš„ç­–ç•¥ä¸ºåŸºäºVFMçš„æ•°æ®èåˆæŠ€æœ¯ç”¨äºé©¾é©¶åœºæ™¯è§£æçš„æœªæ¥åˆ›æ–°é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„æºä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://mias.group/HFIT%E3%80%82">https://mias.group/HFITã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06219v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºVision Transformerï¼ˆViTï¼‰çš„æœ€æ–°è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å·²åœ¨ä¼—å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¦‚ä½•å°†VFMsçš„æ½œåŠ›å……åˆ†å‘æŒ¥åœ¨RGB-æ·±åº¦é©¾é©¶åœºæ™¯è§£æä¸­ã€‚é€šè¿‡æ¢ç´¢RGBå’Œæ·±åº¦æ•°æ®çš„å†…åœ¨ç‰¹æ€§ï¼Œæå‡ºäº†ä¸€ç§å¼‚è´¨ç‰¹å¾é›†æˆå˜å‹å™¨ï¼ˆHFITï¼‰ç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒViTçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æå–å’Œæ•´åˆå…¨é¢çš„å¼‚è´¨ç‰¹å¾ã€‚åœ¨Cityscapeså’ŒKITTI Semanticsæ•°æ®é›†ä¸Šï¼Œä¸å…¶ä»–çš„ä¼ ç»Ÿå•æ¨¡æ€å’Œæ•°æ®èåˆåœºæ™¯è§£æç½‘ç»œã€é¢„è®­ç»ƒçš„VFMså’ŒViTé€‚é…å™¨ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„HFITè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerï¼ˆViTï¼‰ä¸ºåŸºç¡€çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰åœ¨RGB-æ·±åº¦é©¾é©¶åœºæ™¯è§£æä¸­å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚</li>
<li>æ–‡ç« æ¢ç´¢äº†RGBå’Œæ·±åº¦æ•°æ®çš„å†…åœ¨ç‰¹æ€§ã€‚</li>
<li>æå‡ºäº†Heterogeneous Feature Integration Transformerï¼ˆHFITï¼‰ç½‘ç»œï¼Œèƒ½é«˜æ•ˆæå–å’Œæ•´åˆå…¨é¢çš„å¼‚è´¨ç‰¹å¾ã€‚</li>
<li>HFITç½‘ç»œå…‹æœäº†ä¾èµ–æ·±åº¦å›¾çš„é™åˆ¶ï¼Œä½¿ç”¨æ¥è‡ªVFMsçš„ç›¸å¯¹æ·±åº¦é¢„æµ‹ç»“æœä½œä¸ºè¾“å…¥ã€‚</li>
<li>HFITåœ¨Cityscapeså’ŒKITTI Semanticsæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–ä¼ ç»Ÿå•æ¨¡æ€å’Œæ•°æ®èåˆåœºæ™¯è§£æç½‘ç»œã€é¢„è®­ç»ƒçš„VFMså’ŒViTé€‚é…å™¨ã€‚</li>
<li>æ–‡ç« æä¾›äº†ä¸€ç§æ–°å‹ç­–ç•¥ï¼Œä¸ºåŸºäºVFMçš„æ•°æ®èåˆæŠ€æœ¯åœ¨é©¾é©¶åœºæ™¯è§£æä¸­çš„åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</li>
<li>æºä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.06219v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Uni-Retrieval-A-Multi-Style-Retrieval-Framework-for-STEMâ€™s-Education"><a href="#Uni-Retrieval-A-Multi-Style-Retrieval-Framework-for-STEMâ€™s-Education" class="headerlink" title="Uni-Retrieval: A Multi-Style Retrieval Framework for STEMâ€™s Education"></a>Uni-Retrieval: A Multi-Style Retrieval Framework for STEMâ€™s Education</h2><p><strong>Authors:Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan</strong></p>
<p>In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks. This advancement provides a scalable and precise solution for diverse educational needs. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©æ•™å­¦ä¸­ï¼Œåˆ©ç”¨å¤šç§æŸ¥è¯¢é£æ ¼æ¥è§£è¯»æŠ½è±¡æ–‡æœ¬æè¿°å¯¹äºç¡®ä¿é«˜è´¨é‡æ•™å­¦è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ£€ç´¢æ¨¡å‹ä¸»è¦å…³æ³¨è‡ªç„¶æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼Œç”±äºæ£€ç´¢è¿‡ç¨‹ä¸­çš„æ¨¡ç³Šæ€§ï¼Œå®ƒä»¬å¯¹æ•™è‚²åœºæ™¯çš„é€‚åº”æ€§ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ•™è‚²åœºæ™¯æå‡ºäº†ä¸€é¡¹å¤šæ ·åŒ–çš„è¡¨è¾¾æ£€ç´¢ä»»åŠ¡ï¼Œæ”¯æŒåŸºäºå¤šç§æŸ¥è¯¢é£æ ¼å’Œè¡¨è¾¾æ–¹å¼çš„æ£€ç´¢ã€‚æˆ‘ä»¬ä»‹ç»äº†STEMæ•™è‚²æ£€ç´¢æ•°æ®é›†ï¼ˆSERï¼‰ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡24,000ç§ä¸åŒé£æ ¼çš„æŸ¥è¯¢å¯¹ï¼Œä»¥åŠåŸºäºæç¤ºè°ƒæ•´çš„Uni-Retrievalé«˜æ•ˆä¸”é£æ ¼å¤šæ ·çš„è§†è§‰è¯­è¨€æ£€ç´¢æ¨¡å‹ã€‚Uni-Retrievalæå–æŸ¥è¯¢é£æ ¼ç‰¹å¾ä½œä¸ºåŸå‹ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªä¸æ–­æ›´æ–°çš„æç¤ºåº“ï¼Œå…¶ä¸­åŒ…å«ç”¨äºä¸åŒæŸ¥è¯¢çš„æç¤ºæ ‡è®°ã€‚è¿™ä¸ªåº“å¯ä»¥åœ¨æµ‹è¯•æœŸé—´è¿›è¡Œæ›´æ–°ï¼Œä»¥è¡¨ç¤ºä¸åŒä¸»é¢˜æ£€ç´¢åœºæ™¯çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡åŸºäºåŸå‹ç›¸ä¼¼æ€§çš„åŠ¨æ€æ£€ç´¢æç¤ºæ ‡è®°ï¼Œå±•ç¤ºäº†å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§ï¼Œæœ‰æ•ˆåœ°ä¿ƒè¿›äº†æœªçŸ¥æŸ¥è¯¢çš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-Retrievalåœ¨å¤§å¤šæ•°æ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æ£€ç´¢æ¨¡å‹ã€‚è¿™ä¸€è¿›å±•ä¸ºå¤šæ ·åŒ–çš„æ•™è‚²éœ€æ±‚æä¾›äº†å¯æ‰©å±•ä¸”ç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©æ•™å­¦é¢†åŸŸï¼Œè§£è¯»æŠ½è±¡æ–‡æœ¬æè¿°æ—¶åˆ©ç”¨å¤šç§æŸ¥è¯¢é£æ ¼è‡³å…³é‡è¦ã€‚å½“å‰å¤§å¤šæ•°æ£€ç´¢æ¨¡å‹ä¸»è¦å…³æ³¨è‡ªç„¶è¯­è¨€æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼Œä½†ç”±äºæ£€ç´¢è¿‡ç¨‹ä¸­çš„æ¨¡ç³Šæ€§ï¼Œè¿™äº›æ¨¡å‹åœ¨æ•™è‚²åœºæ™¯ä¸‹è¡¨ç°ä¸è¶³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é’ˆå¯¹æ•™è‚²åœºæ™¯çš„å¤šæ ·åŒ–è¡¨è¾¾æ£€ç´¢ä»»åŠ¡ï¼Œæ”¯æŒåŸºäºå¤šç§æŸ¥è¯¢é£æ ¼å’Œè¡¨è¾¾æ–¹å¼çš„æ£€ç´¢ã€‚ä»‹ç»äº†STEMæ•™è‚²æ£€ç´¢æ•°æ®é›†ï¼ˆSERï¼‰ï¼ŒåŒ…å«è¶…è¿‡24,000ç§ä¸åŒé£æ ¼çš„æŸ¥è¯¢å¯¹ï¼Œä»¥åŠåŸºäºæç¤ºè°ƒèŠ‚çš„é«˜æ•ˆã€å¤šæ ·åŒ–çš„Uni-Retrievalè§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æå–æŸ¥è¯¢é£æ ¼ç‰¹å¾ä½œä¸ºåŸå‹å¹¶å»ºç«‹åŒ…å«å„ç§æŸ¥è¯¢æç¤ºç¬¦å·çš„æç¤ºåº“æ¥æ”¯æŒåŠ¨æ€æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-Retrievalåœ¨å¤§å¤šæ•°æ£€ç´¢ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ£€ç´¢æ¨¡å‹ï¼Œä¸ºå¤šæ ·åŒ–çš„æ•™è‚²éœ€æ±‚æä¾›äº†å¯æ‰©å±•å’Œç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰AIè¾…åŠ©æ•™å­¦é¢†åŸŸçš„æ£€ç´¢æ¨¡å‹ä¸»è¦å…³æ³¨è‡ªç„¶æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼Œä½†å…¶åœ¨æ•™è‚²åœºæ™¯ä¸­çš„åº”ç”¨å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ•™è‚²åœºæ™¯ä¸‹çš„æ£€ç´¢éœ€æ±‚å¤šæ ·åŒ–ï¼Œéœ€è¦æ”¯æŒå¤šç§æŸ¥è¯¢é£æ ¼å’Œè¡¨è¾¾æ–¹å¼ã€‚</li>
<li>STEMæ•™è‚²æ£€ç´¢æ•°æ®é›†ï¼ˆSERï¼‰åŒ…å«å¤šæ ·åŒ–çš„æŸ¥è¯¢å¯¹ï¼Œæœ‰åŠ©äºæ»¡è¶³æ•™è‚²åœºæ™¯ä¸‹çš„æ£€ç´¢éœ€æ±‚ã€‚</li>
<li>Uni-Retrievalæ¨¡å‹é€šè¿‡æå–æŸ¥è¯¢é£æ ¼ç‰¹å¾å¹¶å»ºç«‹æç¤ºåº“æ¥æ”¯æŒåŠ¨æ€æ›´æ–°å’Œå¤šæ ·åŒ–çš„æŸ¥è¯¢ã€‚</li>
<li>Uni-Retrievalæ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§ï¼Œèƒ½å¤Ÿå¤„ç†æœªçŸ¥æŸ¥è¯¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-Retrievalåœ¨å¤§å¤šæ•°æ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05863v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-Visual-Embedding-Spaces-Induced-by-Vision-Transformers-for-Online-Auto-Parts-Marketplaces"><a href="#Exploring-Visual-Embedding-Spaces-Induced-by-Vision-Transformers-for-Online-Auto-Parts-Marketplaces" class="headerlink" title="Exploring Visual Embedding Spaces Induced by Vision Transformers for   Online Auto Parts Marketplaces"></a>Exploring Visual Embedding Spaces Induced by Vision Transformers for   Online Auto Parts Marketplaces</h2><p><strong>Authors:Cameron Armijo, Pablo Rivas</strong></p>
<p>This study examines the capabilities of the Vision Transformer (ViT) model in generating visual embeddings for images of auto parts sourced from online marketplaces, such as Craigslist and OfferUp. By focusing exclusively on single-modality data, the analysis evaluates ViTâ€™s potential for detecting patterns indicative of illicit activities. The workflow involves extracting high-dimensional embeddings from images, applying dimensionality reduction techniques like Uniform Manifold Approximation and Projection (UMAP) to visualize the embedding space, and using K-Means clustering to categorize similar items. Representative posts nearest to each cluster centroid provide insights into the composition and characteristics of the clusters. While the results highlight the strengths of ViT in isolating visual patterns, challenges such as overlapping clusters and outliers underscore the limitations of single-modal approaches in this domain. This work contributes to understanding the role of Vision Transformers in analyzing online marketplaces and offers a foundation for future advancements in detecting fraudulent or illegal activities. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åœ¨ç”Ÿæˆæºè‡ªåœ¨çº¿å¸‚åœºå¹³å°ï¼ˆå¦‚Craigslistå’ŒOfferUpï¼‰çš„æ±½è½¦é›¶éƒ¨ä»¶å›¾åƒè§†è§‰åµŒå…¥æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ä¸“æ³¨äºå•ä¸€æ¨¡æ€æ•°æ®ï¼Œåˆ†æè¯„ä¼°äº†ViTæ£€æµ‹éæ³•æ´»åŠ¨è¿¹è±¡æ¨¡å¼æ½œåŠ›ã€‚å·¥ä½œæµç¨‹åŒ…æ‹¬ä»å›¾åƒä¸­æå–é«˜ç»´åµŒå…¥ï¼Œåº”ç”¨ç»Ÿä¸€æµå½¢é€¼è¿‘å’ŒæŠ•å½±ï¼ˆUMAPï¼‰ç­‰é™ç»´æŠ€æœ¯æ¥å¯è§†åŒ–åµŒå…¥ç©ºé—´ï¼Œå¹¶ä½¿ç”¨K-Meansèšç±»ç®—æ³•å¯¹ç±»ä¼¼é¡¹ç›®è¿›è¡Œåˆ†ç±»ã€‚æ¯ä¸ªèšç±»ä¸­å¿ƒçš„ä»£è¡¨æ€§å¸–å­æä¾›äº†æœ‰å…³èšç±»ç»„æˆå’Œç‰¹å¾çš„ä¿¡æ¯ã€‚è™½ç„¶ç»“æœçªå‡ºäº†ViTåœ¨éš”ç¦»è§†è§‰æ¨¡å¼æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½†èšç±»é‡å å’Œå¼‚å¸¸å€¼ç­‰æŒ‘æˆ˜ä¹Ÿçªæ˜¾äº†åœ¨æ­¤é¢†åŸŸä¸­å•æ¨¡æ€æ–¹æ³•çš„å±€é™æ€§ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºäº†è§£Vision Transformersåœ¨åˆ†æåœ¨çº¿å¸‚åœºå¹³å°ä¸­çš„ä½œç”¨ï¼Œå¹¶ä¸ºæœªæ¥æ£€æµ‹æ¬ºè¯ˆæˆ–éæ³•æ´»åŠ¨çš„æŠ€æœ¯è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05756v1">PDF</a> AAAI 2025 Workshop on AI for Social Impact: Bridging Innovations in   Finance, Social Media, and Crime Prevention</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åœ¨ç”Ÿæˆæ¥è‡ªåœ¨çº¿å¸‚åœºå¦‚Craigslistå’ŒOfferUpçš„æ±½è½¦é›¶éƒ¨ä»¶å›¾åƒè§†è§‰åµŒå…¥æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡å•æ¨¡æ€æ•°æ®è¯„ä¼°ViTåœ¨æ£€æµ‹éæ³•æ´»åŠ¨è¿¹è±¡æ–¹é¢çš„æ½œåŠ›ï¼Œé€šè¿‡æå–å›¾åƒçš„é«˜ç»´åµŒå…¥ã€åº”ç”¨é™ç»´æŠ€æœ¯å¦‚Uniform Manifold Approximationå’ŒProjectionï¼ˆUMAPï¼‰è¿›è¡Œå¯è§†åŒ–åµŒå…¥ç©ºé—´ï¼Œå¹¶ä½¿ç”¨K-Meansèšç±»å¯¹ç±»ä¼¼é¡¹ç›®è¿›è¡Œåˆ†ç±»ã€‚é è¿‘æ¯ä¸ªèšç±»ä¸­å¿ƒçš„ä»£è¡¨æ€§å¸–å­æ­ç¤ºäº†èšç±»çš„ç»„æˆå’Œç‰¹ç‚¹ã€‚è™½ç„¶ç»“æœçªå‡ºäº†ViTåœ¨éš”ç¦»è§†è§‰æ¨¡å¼æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½†é‡å çš„èšç±»å’Œå¼‚å¸¸å€¼ç­‰é—®é¢˜å‡¸æ˜¾äº†å•æ¨¡æ€æ–¹æ³•åœ¨æ­¤é¢†åŸŸçš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶ä¸ºç†è§£Vision Transformersåœ¨åˆ†æåœ¨çº¿å¸‚åœºä¸­çš„ä½œç”¨ä»¥åŠæœªæ¥æ£€æµ‹æ¬ºè¯ˆæˆ–éæ³•æ´»åŠ¨çš„è¿›æ­¥æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹è¢«ç”¨äºç”Ÿæˆåœ¨çº¿å¸‚åœºæ±½è½¦é›¶ä»¶å›¾åƒè§†è§‰åµŒå…¥ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å•æ¨¡æ€æ•°æ®è¯„ä¼°ViTåœ¨æ£€æµ‹éæ³•æ´»åŠ¨è¿¹è±¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>é«˜ç»´åµŒå…¥é€šè¿‡é™ç»´æŠ€æœ¯å¯è§†åŒ–ï¼Œå¹¶åº”ç”¨K-Meansèšç±»åˆ†æç±»ä¼¼é¡¹ç›®ã€‚</li>
<li>èšç±»åˆ†ææ­ç¤ºäº†ä¸åŒé¡¹ç›®ä¹‹é—´çš„ç‰¹å¾å’Œå…³è”ã€‚</li>
<li>ViTæ¨¡å‹åœ¨éš”ç¦»è§†è§‰æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>å•æ¨¡æ€æ–¹æ³•åœ¨åˆ†æåœ¨çº¿å¸‚åœºæ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚é‡å çš„èšç±»å’Œå¼‚å¸¸å€¼é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.05756v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Kronecker-Mask-and-Interpretive-Prompts-are-Language-Action-Video-Learners"><a href="#Kronecker-Mask-and-Interpretive-Prompts-are-Language-Action-Video-Learners" class="headerlink" title="Kronecker Mask and Interpretive Prompts are Language-Action Video   Learners"></a>Kronecker Mask and Interpretive Prompts are Language-Action Video   Learners</h2><p><strong>Authors:Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li</strong></p>
<p>Contrastive language-image pretraining (CLIP) has significantly advanced image-based vision learning. A pressing topic subsequently arises: how can we effectively adapt CLIP to the video domain? Recent studies have focused on adjusting either the textual or visual branch of CLIP for action recognition. However, we argue that adaptations of both branches are crucial. In this paper, we propose \textbf{CLAVER}: a \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er}, designed to shift CLIPâ€™s focus from the alignment of static visual objects and concrete nouns to the alignment of dynamic action behaviors and abstract verbs. Specifically, we introduce a novel Kronecker mask attention for temporal modeling. Our tailored Kronecker mask offers three benefits 1) it expands the temporal receptive field for each token, 2) it serves as an effective spatiotemporal heterogeneity inductive bias, mitigating the issue of spatiotemporal homogenization, and 3) it can be seamlessly plugged into transformer-based models. Regarding the textual branch, we leverage large language models to generate diverse, sentence-level and semantically rich interpretive prompts of actions, which shift the modelâ€™s focus towards the verb comprehension. Extensive experiments on various benchmarks and learning scenarios demonstrate the superiority and generality of our approach. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨åŸºäºå›¾åƒçš„è§†è§‰å­¦ä¹ æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚éšä¹‹è€Œæ¥å‡ºç°äº†ä¸€ä¸ªç´§è¿«çš„é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•æœ‰æ•ˆåœ°å°†CLIPé€‚åº”åˆ°è§†é¢‘é¢†åŸŸï¼Ÿè¿‘æœŸçš„ç ”ç©¶ä¸»è¦å…³æ³¨è°ƒæ•´CLIPçš„æ–‡æœ¬æˆ–è§†è§‰åˆ†æ”¯æ¥è¿›è¡ŒåŠ¨ä½œè¯†åˆ«ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºä¸¤ä¸ªåˆ†æ”¯çš„é€‚åº”éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CLAVERï¼šä¸€ç§å¯¹æ¯”è¯­è¨€åŠ¨ä½œè§†é¢‘å­¦ä¹ å™¨ï¼ˆContrastive Language-Action Video Learnerï¼‰ï¼Œæ—¨åœ¨å°†CLIPçš„é‡ç‚¹ä»é™æ€è§†è§‰å¯¹è±¡å’Œå…·ä½“åè¯çš„å¯¹é½è½¬ç§»åˆ°åŠ¨æ€è¡Œä¸ºåŠ¨ä½œå’ŒæŠ½è±¡åŠ¨è¯çš„å¯¹é½ä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å…‹ç½—å†…å…‹æ©è†œæ³¨æ„åŠ›æ¥å®ç°æ—¶åºå»ºæ¨¡ã€‚æˆ‘ä»¬å®šåˆ¶çš„å…‹ç½—å†…å…‹æ©è†œå…·æœ‰ä¸‰ä¸ªä¼˜ç‚¹ï¼š1ï¼‰å®ƒæ‰©å¤§äº†æ¯ä¸ªæ ‡è®°çš„æ—¶åºæ„Ÿå—é‡ï¼›2ï¼‰å®ƒä½œä¸ºæœ‰æ•ˆçš„æ—¶ç©ºå¼‚è´¨æ€§å½’çº³åç½®ï¼Œç¼“è§£äº†æ—¶ç©ºåŒè´¨åŒ–çš„é—®é¢˜ï¼›3ï¼‰å®ƒå¯ä»¥æ— ç¼åœ°æ’å…¥åˆ°åŸºäºå˜å‹å™¨æ¨¡å‹ã€‚å¯¹äºæ–‡æœ¬åˆ†æ”¯ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·ã€å¥å­çº§åˆ«ä¸”è¯­ä¹‰ä¸°å¯Œçš„åŠ¨ä½œè§£é‡Šæ€§æç¤ºï¼Œä½¿æ¨¡å‹å…³æ³¨åŠ¨è¯çš„ç†è§£ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•å’Œå­¦ä¹ åœºæ™¯çš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03549v3">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†Contrastive language-image pretraining (CLIP)æŠ€æœ¯åº”ç”¨äºè§†é¢‘é¢†åŸŸçš„é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºäº†å¯¹CLIPçš„æ–‡æœ¬å’Œè§†è§‰åˆ†æ”¯è¿›è¡ŒåŒé‡è°ƒæ•´çš„é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†CLAVERæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼•å…¥Kronecker maskæ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å¯¹åŠ¨æ€è¡Œä¸ºåŠ¨ä½œä¸æŠ½è±¡åŠ¨è¯çš„å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œå­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CLAVERæ¨¡å‹æ—¨åœ¨å°†CLIPçš„ç„¦ç‚¹ä»é™æ€è§†è§‰å¯¹è±¡å’Œå…·ä½“åè¯çš„å¯¹é½è½¬ç§»åˆ°åŠ¨æ€è¡Œä¸ºåŠ¨ä½œå’ŒæŠ½è±¡åŠ¨è¯çš„å¯¹é½ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„Kronecker maskæ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå®ç°æ—¶é—´å»ºæ¨¡ã€‚è¿™ç§æœºåˆ¶æä¾›äº†ä¸‰ä¸ªä¼˜ç‚¹ï¼šæ‰©å¤§æ¯ä¸ªä»¤ç‰Œçš„ä¸´æ—¶æ¥æ”¶åœºï¼Œä½œä¸ºæœ‰æ•ˆçš„æ—¶ç©ºå¼‚è´¨æ€§å½’çº³åç½®ï¼Œå¹¶å¯ä»¥æ— ç¼åœ°æ’å…¥åŸºäºtransformerçš„æ¨¡å‹ä¸­ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ†æ”¯æ–¹é¢ï¼Œä½œè€…åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆäº†å¤šæ ·ã€å¥å­çº§åˆ«ä¸”è¯­ä¹‰ä¸°å¯Œçš„åŠ¨ä½œè§£é‡Šæ€§æç¤ºï¼Œä½¿æ¨¡å‹æ›´åŠ å…³æ³¨åŠ¨è¯çš„ç†è§£ã€‚</li>
<li>CLAVERæ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œå­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†ä»…è°ƒæ•´CLIPçš„æ–‡æœ¬æˆ–è§†è§‰åˆ†æ”¯æ˜¯ä¸å¤Ÿçš„ï¼Œä¸¤è€…çš„è°ƒæ•´éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚</li>
<li>Kronecker maskæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„å·¥å…·ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£åŠ¨æ€è¡Œä¸ºå’ŒæŠ½è±¡åŠ¨è¯çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2502.03549v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ranking-aware-adapter-for-text-driven-image-ordering-with-CLIP"><a href="#Ranking-aware-adapter-for-text-driven-image-ordering-with-CLIP" class="headerlink" title="Ranking-aware adapter for text-driven image ordering with CLIP"></a>Ranking-aware adapter for text-driven image ordering with CLIP</h2><p><strong>Authors:Wei-Hsiang Yu, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</strong></p>
<p>Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies typically focus on the reasoning based on a single image and heavily depend on text prompting, limiting their ability to learn comprehensive understanding from multiple images. To address this, we propose an effective yet efficient approach that reframes the CLIP model into a learning-to-rank task and introduces a lightweight adapter to augment CLIP for text-guided image ranking. Specifically, our approach incorporates learnable prompts to adapt to new instructions for ranking purposes and an auxiliary branch with ranking-aware attention, leveraging text-conditioned visual differences for additional supervision in image ranking. Our ranking-aware adapter consistently outperforms fine-tuned CLIPs on various tasks and achieves competitive results compared to state-of-the-art models designed for specific tasks like facial age estimation and image quality assessment. Overall, our approach primarily focuses on ranking images with a single instruction, which provides a natural and generalized way of learning from visual differences across images, bypassing the need for extensive text prompts tailored to individual tasks. Code is available: github.com&#x2F;uynaes&#x2F;RankingAwareCLIP. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥åœ¨éœ€è¦å®šé‡æ¦‚å¿µçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¾‹å¦‚é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ï¼Œä½¿å¾—VLMsèƒ½å¤Ÿæ¢ç´¢å›¾åƒæ’åå’Œæ£€ç´¢ç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶é€šå¸¸åŸºäºå•å¼ å›¾åƒè¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºæ–‡æœ¬æç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ä»å¤šå¼ å›¾åƒä¸­å­¦ä¹ å…¨é¢ç†è§£çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œå°†CLIPæ¨¡å‹é‡æ„ä¸ºå­¦ä¹ æ’åä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§é€‚é…å™¨æ¥å¢å¼ºCLIPè¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ’åã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¯å­¦ä¹ çš„æç¤ºæ¥é€‚åº”æ–°çš„æ’åæŒ‡ä»¤ï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰æ’åæ„ŸçŸ¥æ³¨æ„åŠ›çš„è¾…åŠ©åˆ†æ”¯ï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶ä¸‹çš„è§†è§‰å·®å¼‚è¿›è¡Œå›¾åƒæ’åçš„é¢å¤–ç›‘ç£ã€‚æˆ‘ä»¬çš„æ’åæ„ŸçŸ¥é€‚é…å™¨åœ¨å„ç§ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºå¾®è°ƒè¿‡çš„CLIPï¼Œå¹¶åœ¨é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ç­‰ç‰¹å®šä»»åŠ¡ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºä½¿ç”¨å•ä¸ªæŒ‡ä»¤å¯¹å›¾åƒè¿›è¡Œæ’åï¼Œè¿™æä¾›äº†ä¸€ç§ä»å›¾åƒä¹‹é—´è§†è§‰å·®å¼‚å­¦ä¹ çš„è‡ªç„¶ä¸”é€šç”¨çš„æ–¹å¼ï¼Œé¿å…äº†éœ€è¦å¤§é‡é’ˆå¯¹ä¸ªåˆ«ä»»åŠ¡å®šåˆ¶çš„æ–‡æœ¬æç¤ºçš„éœ€æ±‚ã€‚ä»£ç å¯ç”¨ï¼šgithub.com&#x2F;uynaes&#x2F;RankingAwareCLIPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06760v3">PDF</a> Accepted by ICLR2025. Github link: github.com&#x2F;uynaes&#x2F;RankingAwareCLIP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„é’ˆå¯¹CLIPæ¨¡å‹çš„æ”¹è¿›æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒæ’åä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å­¦ä¹ æ’åä»»åŠ¡çš„æç¤ºå’Œè¾…åŠ©åˆ†æ”¯ï¼Œå®ç°å¯¹CLIPæ¨¡å‹çš„é‡æ–°æ„å»ºå’Œå¢å¼ºã€‚æ–°æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªå›¾åƒä¸Šå­¦ä¹ å…¨é¢çš„ç†è§£ï¼Œè¶…è¶ŠåŸºäºå•ä¸€å›¾åƒçš„ä»»åŠ¡ä¾èµ–æ–‡æœ¬æç¤ºçš„é™åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå¾®è°ƒåçš„CLIPæ¨¡å‹ï¼Œå¹¶åœ¨é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ç­‰ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ–¹æ³•ä¸»è¦é€šè¿‡å•ä¸€çš„æŒ‡ä»¤å¯¹å›¾åƒè¿›è¡Œæ’åï¼Œæä¾›ä¸€ç§ä»å›¾åƒé—´è§†è§‰å·®å¼‚å­¦ä¹ çš„ä¸€èˆ¬åŒ–æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨éœ€è¦å®šé‡æ¦‚å¿µçš„ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ï¼‰ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>VLMsç°åœ¨å¯ç”¨äºå›¾åƒæ’åå’Œæ£€ç´¢ç­‰åº”ç”¨ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šå¸¸ä¾èµ–äºåŸºäºå•å›¾åƒçš„æ¨ç†å’Œæ–‡æœ¬æç¤ºï¼Œé™åˆ¶äº†å®ƒä»¬ä»å¤šä¸ªå›¾åƒä¸­å­¦ä¹ å…¨é¢ç†è§£çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ”¹è¿›CLIPæ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒæ’åä»»åŠ¡ï¼Œé€šè¿‡å¼•å…¥å­¦ä¹ æ’åä»»åŠ¡çš„æç¤ºå’Œè¾…åŠ©åˆ†æ”¯å®ç°ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè¶…è¶ŠåŸºäºå•ä¸€å›¾åƒçš„ä»»åŠ¡ä¾èµ–æ–‡æœ¬æç¤ºçš„é™åˆ¶ï¼Œåœ¨å¤šä¸ªå›¾åƒä¸Šå­¦ä¹ å…¨é¢çš„ç†è§£ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå¾®è°ƒåçš„CLIPæ¨¡å‹ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2412.06760v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Magnetic-Resonance-Image-Processing-Transformer-for-General-Accelerated-Image-Reconstruction"><a href="#Magnetic-Resonance-Image-Processing-Transformer-for-General-Accelerated-Image-Reconstruction" class="headerlink" title="Magnetic Resonance Image Processing Transformer for General Accelerated   Image Reconstruction"></a>Magnetic Resonance Image Processing Transformer for General Accelerated   Image Reconstruction</h2><p><strong>Authors:Guoyao Shen, Mengyu Li, Stephan Anderson, Chad W. Farris, Xin Zhang</strong></p>
<p>Recent advancements in deep learning have enabled the development of generalizable models that achieve state-of-the-art performance across various imaging tasks. Vision Transformer (ViT)-based architectures, in particular, have demonstrated strong feature extraction capabilities when pre-trained on large-scale datasets. In this work, we introduce the Magnetic Resonance Image Processing Transformer (MR-IPT), a ViT-based framework designed to enhance the generalizability and robustness of accelerated MRI reconstruction. Unlike conventional deep learning models that require separate training for different acceleration factors, MR-IPT is pre-trained on a large-scale dataset encompassing multiple undersampling patterns and acceleration settings, enabling a unified reconstruction framework. By leveraging a shared transformer backbone, MR-IPT effectively learns universal feature representations, allowing it to generalize across diverse reconstruction tasks. Extensive experiments demonstrate that MR-IPT outperforms both CNN-based and existing transformer-based methods, achieving superior reconstruction quality across varying acceleration factors and sampling masks. Moreover, MR-IPT exhibits strong robustness, maintaining high performance even under unseen acquisition setups, highlighting its potential as a scalable and efficient solution for accelerated MRI. Our findings suggest that transformer-based general models can significantly advance MRI reconstruction, offering improved adaptability and stability compared to traditional deep learning approaches. </p>
<blockquote>
<p>æœ€è¿‘æ·±åº¦å­¦ä¹ çš„å‘å±•ä¿ƒè¿›äº†é€šç”¨æ¨¡å‹çš„å¼€å‘ï¼Œè¿™äº›æ¨¡å‹åœ¨å„ç§æˆåƒä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„æ¶æ„ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼Œè¡¨ç°å‡ºäº†å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç£å…±æŒ¯å›¾åƒå¤„ç†å™¨è½¬æ¢å™¨ï¼ˆMR-IPTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºViTçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŠ é€Ÿç£å…±æŒ¯æˆåƒé‡å»ºçš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚ä¸åŒäºéœ€è¦é’ˆå¯¹ä¸åŒåŠ é€Ÿå› å­è¿›è¡Œå•ç‹¬è®­ç»ƒçš„ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒMR-IPTæ˜¯åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ¶µç›–å¤šç§æ¬ é‡‡æ ·æ¨¡å¼å’ŒåŠ é€Ÿè®¾ç½®ï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„é‡å»ºæ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨å…±äº«è½¬æ¢å™¨ä¸»å¹²ï¼ŒMR-IPTæœ‰æ•ˆåœ°å­¦ä¹ é€šç”¨ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œèƒ½å¤Ÿåœ¨å„ç§é‡å»ºä»»åŠ¡ä¸­é€šç”¨åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMR-IPTåœ¨åŸºäºCNNçš„å’Œç°æœ‰çš„åŸºäºè½¬æ¢å™¨çš„æ–¹æ³•ä¸­è¡¨ç°ä¼˜äºå‰è€…ï¼Œåœ¨å„ç§åŠ é€Ÿå› å­å’Œé‡‡æ ·æ©è†œä¸‹å®ç°æ›´é«˜çš„é‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼ŒMR-IPTè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå³ä½¿åœ¨æœªè§è¿‡çš„é‡‡é›†è®¾ç½®ä¸‹ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†å…¶ä½œä¸ºåŠ é€Ÿç£å…±æŒ¯æˆåƒçš„å¯æ‰©å±•å’Œé«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºè½¬æ¢å™¨çš„é€šç”¨æ¨¡å‹å¯ä»¥æå¤§åœ°æ¨åŠ¨ç£å…±æŒ¯æˆåƒé‡å»ºçš„å‘å±•ï¼Œä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å¥½çš„é€‚åº”æ€§å’Œç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15098v2">PDF</a> 28 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ‘˜è¦ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•åœ¨è§†è§‰è½¬åŒ–å™¨æ¨¡å‹ï¼ˆå¦‚MR-IPTæ¨¡å‹ï¼‰ä¸­å±•ç°å‡ºä¼˜ç§€çš„è¡¨ç°èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹å¤§å‹å›¾åƒæ•°æ®å¤„ç†ä»»åŠ¡æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚MR-IPTæ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ å¤šç§ä¸‹é‡‡æ ·æ¨¡å¼å’ŒåŠ é€Ÿè®¾ç½®çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå®ç°äº†ç»Ÿä¸€çš„é‡å»ºæ¡†æ¶ï¼Œèƒ½å¤Ÿè·¨å¤šç§é‡å»ºä»»åŠ¡è¿›è¡Œé€šç”¨ç‰¹å¾è¡¨ç¤ºçš„å­¦ä¹ ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒè¡¨æ˜ï¼ŒMR-IPTåœ¨CNNå’Œä¼ ç»Ÿè½¬æ¢å™¨æ–¹æ³•ä¸­è¡¨ç°æ›´å‡ºè‰²ï¼Œå¯ä»¥ç»´æŒé«˜è´¨é‡çš„é‡å»ºç»“æœå¹¶ä¸”å…·æœ‰è¾ƒå¼ºçš„é€‚åº”æ€§ï¼Œå› æ­¤å¯ä¸ºMRIå¿«é€Ÿæˆåƒæä¾›è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œä½¿ç”¨è§†è§‰è½¬æ¢å™¨æ¶æ„å¯ä¸ºåŒ»å­¦å›¾åƒå¤„ç†ï¼ˆç‰¹åˆ«æ˜¯MRIå›¾åƒé‡å»ºï¼‰å¼€è¾Ÿæ–°çš„åº”ç”¨é“è·¯ã€‚ </p>
<p><strong>Key Takeaways</strong><br>ä»¥ä¸‹æ˜¯å¯¹æ­¤è®ºæ–‡æœ€ä¸ºå…³é”®çš„å‡ ä¸ªè§è§£ï¼š</p>
<ul>
<li>MR-IPTæ¨¡å‹åŸºäºè§†è§‰è½¬æ¢å™¨æ¶æ„ï¼Œæ—¨åœ¨æé«˜MRIé‡å»ºçš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚ </li>
<li>MR-IPTé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ æ¶µç›–å¤šç§ä¸‹é‡‡æ ·æ¨¡å¼å’ŒåŠ é€Ÿè®¾ç½®çš„å¤§è§„æ¨¡æ•°æ®é›†æ¥å®ç°ç»Ÿä¸€çš„é‡å»ºæ¡†æ¶ã€‚ </li>
<li>MR-IPTå…·æœ‰ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯è·¨ä¸åŒçš„é‡å»ºä»»åŠ¡è¿›è¡Œç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚ </li>
<li>MR-IPTçš„æ€§èƒ½è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„CNNå’ŒåŸºäºè½¬æ¢å™¨çš„æ–¹æ³•ï¼Œåœ¨ä¸åŒåŠ é€Ÿå› å­å’Œé‡‡æ ·æ©è†œä¸‹å®ç°é«˜è´¨é‡çš„é‡å»ºç»“æœã€‚ </li>
<li>MR-IPTåœ¨ä¸åŒé‡‡é›†è®¾ç½®ä¸‹å±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œå³ä½¿é¢å¯¹æœªè§è¿‡çš„é‡‡é›†è®¾ç½®ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½è¡¨ç°ã€‚ </li>
<li>MR-IPTæ¨¡å‹å…·å¤‡å¯æ‰©å±•æ€§å’Œé«˜æ•ˆæ€§ï¼Œä¸ºè§£å†³MRIçš„å¿«é€Ÿæˆåƒé—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2405.15098v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Vision Transformer/2405.15098v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.18409v2/page_5_0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Enhancing Ground-to-Aerial Image Matching for Visual Misinformation   Detection Using Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_è§†é¢‘ç†è§£/2502.06428v1/page_0_0.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  CoS Chain-of-Shot Prompting for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
