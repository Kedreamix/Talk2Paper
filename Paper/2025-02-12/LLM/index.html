<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ReasonFlux Hierarchical LLM Reasoning via Scaling Thought Templates">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3f2dbb7b9fc4ae0a1234f5ee88840144.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="ReasonFlux-Hierarchical-LLM-Reasoning-via-Scaling-Thought-Templates"><a href="#ReasonFlux-Hierarchical-LLM-Reasoning-via-Scaling-Thought-Templates" class="headerlink" title="ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates"></a>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</h2><p><strong>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</strong></p>
<p>We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºé€šè¿‡æ‰©å±•æ€ç»´æ¨¡æ¿è¿›è¡Œåˆ†å±‚LLMæ¨ç†çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–æ¨ç†æœç´¢ç©ºé—´ï¼Œå¹¶è¶…è¶Šå¼ºå¤§LLMå¦‚OpenAI o1-previewå’ŒDeepSeek V3çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ä»…8ä¸ªGPUå¯¹ReasonFlux-32Bæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°ï¼šï¼ˆiï¼‰ä¸€ä¸ªç»“æ„å’Œé€šç”¨çš„æ€ç»´æ¨¡æ¿åº“ï¼ŒåŒ…å«å¤§çº¦500ä¸ªé«˜çº§æ€ç»´æ¨¡æ¿ï¼Œèƒ½å¤Ÿæ¨å¹¿è‡³ç±»ä¼¼æˆ–ç›¸å…³çš„æ¨ç†é—®é¢˜ï¼›ï¼ˆiiï¼‰åœ¨ä¸€ç³»åˆ—æ€ç»´æ¨¡æ¿ä¸Šæ‰§è¡Œåˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼Œè€Œä¸æ˜¯é•¿CoTsï¼Œä¼˜åŒ–åŸºç¡€LLMä»¥è§„åˆ’å‡ºå¤„ç†å¤æ‚é—®é¢˜çš„æœ€ä½³æ¨¡æ¿è½¨è¿¹ï¼›ï¼ˆiiiï¼‰ä¸€ä¸ªå…¨æ–°çš„æ¨ç†æ‰©å±•ç³»ç»Ÿï¼Œé€šè¿‡è‡ªé€‚åº”æ‰©å±•æ€ç»´æ¨¡æ¿ï¼Œå®ç°åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„åˆ†å±‚LLMæ¨ç†ã€‚é€šè¿‡åŒ…å«è¿ç»­æ€ç»´æ¨¡æ¿çš„è½¨è¿¹ï¼Œæˆ‘ä»¬çš„ReasonFlux-32Bæå¤§åœ°æé«˜äº†æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å‡†ç¡®ç‡è¾¾åˆ°äº†91.2%ï¼Œæ¯”o1-previewé«˜å‡º6.7%ã€‚åœ¨ç¾å›½æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆAIMEï¼‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-32Bå¹³å‡è§£å†³äº†56.7%çš„é—®é¢˜ï¼Œåˆ†åˆ«æ¯”o1-previewå’ŒDeepSeek-V3é«˜å‡º27%å’Œ45%ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06772v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">https://github.com/Gen-Verse/ReasonFlux</a></p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æœç´¢ç©ºé—´ä¼˜åŒ–æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºå±‚çº§åŒ–æ€æƒ³æ¨¡æ¿åº“å¹¶å¯¹å…¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæœ¬ç ”ç©¶æå‡ºReasonFlux-32Bæ¨¡å‹åœ¨å°‘é‡GPUä¸Šè¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œå…¶æ•°å­¦æ¨ç†èƒ½åŠ›å·²è¾¾åˆ°æˆ–è¶…è¶ŠOpenAI o1-previewå’ŒDeepSeek V3ç­‰é«˜çº§LLMæ¨¡å‹ã€‚åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-32Bå‡†ç¡®ç‡è¾¾åˆ°91.2%ï¼Œè¶…è¶Šo1-preview 6.7%ã€‚åœ¨USA Math Olympiadï¼ˆAIMEï¼‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶è§£å†³é—®é¢˜æ•°é‡å¹³å‡é«˜å‡ºDeepSeek-V3æ¨¡å‹è¿‘ä¸€åŠã€‚è¯¥ç ”ç©¶å±•ç°äº†å¼ºå¤§çš„è‡ªé€‚åº”æ€æƒ³æ¨¡æ¿ç¼©æ”¾ç³»ç»Ÿã€‚æ›´å¤šè¯¦æƒ…å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ReasonFlux">GitHubé“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>åˆ©ç”¨å±‚æ¬¡åŒ–çš„æ€æƒ³æ¨¡æ¿åº“ä¼˜åŒ–äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨ç†æœç´¢ç©ºé—´ã€‚</li>
<li>ReasonFlux-32Bæ¨¡å‹é€šè¿‡ç»“æ„åŒ–æ€æƒ³æ¨¡æ¿åº“ä»¥åŠå±‚æ¬¡å¼ºåŒ–å­¦ä¹ å®ç°äº†é«˜æ•ˆçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä»…ä½¿ç”¨å°‘é‡GPUèµ„æºè®­ç»ƒçš„ReasonFlux-32Bæ¨¡å‹è¡¨ç°ä¼˜è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„é«˜çº§LLMæ¨¡å‹å¦‚OpenAI o1-previewå’ŒDeepSeek V3ç­‰ã€‚</li>
<li>åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-32Bæ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°91.2%ï¼Œè¾ƒé¢†å…ˆç«äº‰å¯¹æ‰‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>åœ¨USA Math OlympiadåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasonFlux-32Bè§£å†³äº†è¿œè¶…DeepSeek V3çš„é—®é¢˜æ•°é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ce202dd4a3acdd4d2b9a10dc8db70d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b05dd2556c3fd4db3cbc2c65f469e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3668602eb7cfebcd8812d4e24b9422af.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dynamic-Loss-Based-Sample-Reweighting-for-Improved-Large-Language-Model-Pretraining"><a href="#Dynamic-Loss-Based-Sample-Reweighting-for-Improved-Large-Language-Model-Pretraining" class="headerlink" title="Dynamic Loss-Based Sample Reweighting for Improved Large Language Model   Pretraining"></a>Dynamic Loss-Based Sample Reweighting for Improved Large Language Model   Pretraining</h2><p><strong>Authors:Daouda Sow, Herbert WoisetschlÃ¤ger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang</strong></p>
<p>Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance. </p>
<blockquote>
<p>å¯¹å¤§è§„æ¨¡å’Œå¼‚æ„æ•°æ®é›†è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒå¯¹äºåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°æœ€æ–°æŠ€æœ¯æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰è®­ç»ƒæ¨¡å¼å¹³ç­‰å¯¹å¾…æ‰€æœ‰æ ·æœ¬ï¼Œå¿½è§†äº†è®­ç»ƒè¿‡ç¨‹ä¸­å„ä¸ªæ ·æœ¬çš„é‡è¦æ€§å’Œç›¸å…³æ€§ã€‚ç°æœ‰çš„åŠ æƒç­–ç•¥ä¸»è¦å…³æ³¨äºç¾¤ä½“çº§åˆ«çš„æ•°æ®é‡è¦æ€§ï¼Œæœªèƒ½åˆ©ç”¨ç²¾ç»†çš„å®ä¾‹çº§åˆ«ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸èƒ½éšç€è®­ç»ƒçš„è¿›è¡Œè€ŒåŠ¨æ€é€‚åº”å•ä¸ªæ ·æœ¬çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹åŠ¨æ€å®ä¾‹çº§åˆ«æ•°æ®åŠ æƒçš„æ–°ç®—æ³•ï¼Œæ—¨åœ¨æé«˜LLMé¢„è®­ç»ƒçš„æ•ˆç‡ä¸æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æŸå¤±å€¼åœ¨çº¿è°ƒæ•´å…¶æƒé‡ï¼Œå…è®¸æ¨¡å‹ä»¥åŠ¨æ€æ–¹å¼å…³æ³¨å½“å‰è®­ç»ƒé˜¶æ®µæ›´å…·ä¿¡æ¯æ€§æˆ–é‡è¦æ€§çš„æ ·æœ¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåœ°è®¾è®¡åŠ æƒç­–ç•¥ï¼Œä¼˜å…ˆå¤„ç†å†—ä½™æˆ–ä¿¡æ¯é‡è¾ƒå°‘çš„æ•°æ®ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§åšæ³•å¾€å¾€æ•ˆæœæœ€å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç”¨äºåˆ†æåŸºäºæŸå¤±çš„åŠ æƒå¯¹åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–çš„æ”¶æ•›å½±å“çš„æ–°ç†è®ºæ¡†æ¶ï¼Œé¦–æ¬¡æ­£å¼æè¿°äº†è¿™äº›ç­–ç•¥å¦‚ä½•å½±å“æ”¶æ•›è¾¹ç•Œã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ï¼Œä»é¢„è®­ç»ƒ7Bå’Œ1.4Bå‚æ•°çš„LLMåˆ°è¾ƒå°è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œçº¿æ€§å›å½’é—®é¢˜ï¼Œè¡¨æ˜æˆ‘ä»¬çš„åŸºäºæŸå¤±çš„åŠ æƒæ–¹æ³•å¯ä»¥å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06733v1">PDF</a> Accepted for publication at ICLR 2025. Code base available:   <a target="_blank" rel="noopener" href="https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025">https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒåœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†å…¶ç°æœ‰è®­ç»ƒæ¨¡å¼å¿½è§†äº†å•ä¸ªæ ·æœ¬çš„é‡è¦æ€§ã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å®ä¾‹çº§åˆ«çš„åŠ¨æ€æ•°æ®é‡æ–°åŠ æƒç®—æ³•ï¼Œæ—¨åœ¨æé«˜LLMé¢„è®­ç»ƒçš„æ•ˆç‡ä¸æ•ˆæœã€‚æ ¹æ®æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æŸå¤±å€¼åœ¨çº¿è°ƒæ•´æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½åŠ¨æ€å…³æ³¨å½“å‰é˜¶æ®µæ›´æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚åŒæ—¶ï¼Œæœ¬æ–‡å»ºç«‹äº†åˆ†ææŸå¤±åŠ æƒå¯¹åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ”¶æ•›å½±å“çš„æ–°ç†è®ºæ¡†æ¶ï¼Œå¹¶å®è¯éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢„è®­ç»ƒå¯¹å®ç°å¤šæ ·ä¸‹æ¸¸ä»»åŠ¡çš„é«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è®­ç»ƒæ¨¡å¼å¹³ç­‰å¯¹å¾…æ‰€æœ‰æ ·æœ¬ï¼Œå¿½ç•¥äº†å•ä¸ªæ ·æœ¬çš„é‡è¦æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŠ¨æ€ã€å®ä¾‹çº§åˆ«çš„æ•°æ®é‡æ–°åŠ æƒç®—æ³•ï¼Œæ—¨åœ¨æé«˜LLMé¢„è®­ç»ƒçš„æ•ˆæœã€‚</li>
<li>æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼åœ¨çº¿è°ƒæ•´æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å…³æ³¨æ›´æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚</li>
<li>å»ºç«‹äº†åˆ†ææŸå¤±åŠ æƒå¯¹æ¢¯åº¦ä¼˜åŒ–æ”¶æ•›å½±å“çš„æ–°ç†è®ºæ¡†æ¶ã€‚</li>
<li>å®è¯è¡¨æ˜ï¼ŒæŸå¤±åŠ æƒæ–¹æ³•èƒ½åŠ å¿«æ”¶æ•›é€Ÿåº¦å¹¶æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1252a531481bcd32afb653379c33be6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44d77da764ec9572b31ed2bc86d90984.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Can-1B-LLM-Surpass-405B-LLM-Rethinking-Compute-Optimal-Test-Time-Scaling"><a href="#Can-1B-LLM-Surpass-405B-LLM-Rethinking-Compute-Optimal-Test-Time-Scaling" class="headerlink" title="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time   Scaling"></a>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time   Scaling</h2><p><strong>Authors:Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou</strong></p>
<p>Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯ä¸€ç§åˆ©ç”¨æ¨ç†é˜¶æ®µçš„é¢å¤–è®¡ç®—æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„é‡è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶å¹¶æ²¡æœ‰ç³»ç»Ÿåœ°åˆ†æç­–ç•¥æ¨¡å‹ã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å’Œé—®é¢˜éš¾åº¦å¦‚ä½•å½±å“TTSã€‚è¿™ç§åˆ†æä¸Šçš„ç¼ºå¤±é™åˆ¶äº†TTSæ–¹æ³•çš„ç†è§£å’Œå®è·µåº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹è§£å†³ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šï¼ˆ1ï¼‰åœ¨ä¸åŒçš„ç­–ç•¥æ¨¡å‹ã€PRMå’Œé—®é¢˜éš¾åº¦çº§åˆ«ä¸Šï¼Œå®ç°æµ‹è¯•æ—¶ç¼©æ”¾çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆ2ï¼‰é¢å¤–çš„è®¡ç®—èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæé«˜LLMåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯å¦å¯ä»¥é€šè¿‡è¿™ç§æ–¹æ³•ä½¿è¾ƒå°çš„è¯­è¨€æ¨¡å‹è¡¨ç°ä¼˜äºè¾ƒå¤§çš„æ¨¡å‹ï¼Ÿé€šè¿‡å¯¹MATH-500å’ŒæŒ‘æˆ˜æ€§çš„AIME24ä»»åŠ¡çš„å…¨é¢å®éªŒï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹è§‚å¯Ÿï¼šï¼ˆ1ï¼‰è®¡ç®—æœ€ä¼˜çš„TTSç­–ç•¥é«˜åº¦ä¾èµ–äºç­–ç•¥æ¨¡å‹ã€PRMå’Œé—®é¢˜éš¾åº¦çš„é€‰æ‹©ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨æˆ‘ä»¬çš„è®¡ç®—æœ€ä¼˜TTSç­–ç•¥ï¼Œæå°çš„ç­–ç•¥æ¨¡å‹ç”šè‡³å¯ä»¥è¶…è¶Šè¾ƒå¤§çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨MATH-500ä¸Šï¼Œ1Bçš„LLMå¯ä»¥è¶…è¿‡405Bçš„LLMã€‚æ­¤å¤–ï¼Œåœ¨MATH-500å’ŒAIME24ä¸Šï¼Œ0.5Bçš„LLMè¶…è¶Šäº†GPT-4oï¼Œ3Bçš„LLMè¶…è¶Šäº†405Bçš„LLMï¼Œè€Œ7Bçš„LLMåˆ™å‡»è´¥äº†o1å’ŒDeepSeek-R1ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†æ•ˆç‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé€‚åº”TTSç­–ç•¥åˆ°æ¯ä¸ªä»»åŠ¡å’Œæ¨¡å‹çš„å…·ä½“ç‰¹å¾æ˜¯éå¸¸é‡è¦çš„ï¼Œå¹¶ä¸”è¡¨æ˜TTSæ˜¯ä¸€ç§æœ‰æœ›å¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06703v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ–¹æ³•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½æå‡çš„é‡è¦æ€§ã€‚æ–‡ç« é€šè¿‡å®éªŒç ”ç©¶äº†ç­–ç•¥æ¨¡å‹ã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å’Œé—®é¢˜éš¾åº¦å¯¹TTSçš„å½±å“ï¼Œå¹¶å‘ç°æœ€ä¼˜çš„TTSç­–ç•¥ä¾èµ–äºç­–ç•¥æ¨¡å‹ã€PRMå’Œé—®é¢˜éš¾åº¦çš„é€‰æ‹©ã€‚åŒæ—¶ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¾ƒå°çš„è¯­è¨€æ¨¡å‹é€šè¿‡é€‚å½“çš„TTSç­–ç•¥ç”šè‡³å¯ä»¥åœ¨æ€§èƒ½ä¸Šè¶…è¶Šå¤§å‹æ¨¡å‹ï¼Œè¡¨æ˜TTSç­–ç•¥åœ¨å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯ä¸€ç§é€šè¿‡åœ¨æ¨ç†é˜¶æ®µå¢åŠ è®¡ç®—æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„é‡è¦æ–¹æ³•ã€‚</li>
<li>æœ€ä¼˜çš„TTSç­–ç•¥å–å†³äºç­–ç•¥æ¨¡å‹ã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å’Œé—®é¢˜éš¾åº¦çš„é€‰æ‹©ã€‚</li>
<li>é€šè¿‡é€‚å½“çš„TTSç­–ç•¥ï¼Œè¾ƒå°çš„è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨æŸäº›æƒ…å†µä¸‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šå¤§å‹æ¨¡å‹ã€‚</li>
<li>TTSç­–ç•¥å¯¹äºå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92a9856f23ca9ca44e6272c292a19c6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5feafacc3672efc9ff5425767728f88b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b57a3af607d8e54de2f2beaecd7988f8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Boosting-Self-Efficacy-and-Performance-of-Large-Language-Models-via-Verbal-Efficacy-Stimulations"><a href="#Boosting-Self-Efficacy-and-Performance-of-Large-Language-Models-via-Verbal-Efficacy-Stimulations" class="headerlink" title="Boosting Self-Efficacy and Performance of Large Language Models via   Verbal Efficacy Stimulations"></a>Boosting Self-Efficacy and Performance of Large Language Models via   Verbal Efficacy Stimulations</h2><p><strong>Authors:Rui Chen, Tailai Peng, Xinran Xie, Dekun Lin, Zhe Cui, Zheng Chen</strong></p>
<p>Significant improvements have been observed in the zero-shot capabilities of the Large Language Models (LLMs). Due to their high sensitivity to input, research has increasingly focused on enhancing LLMsâ€™ performance via direct and simple prompt engineering rather than intricate domain adaptation. Studies suggest that LLMs exhibit emotional intelligence, and both positive and negative emotions can potentially enhance task performances. However, prior interaction prompts have predominantly concentrated on a single stimulus type, neglecting to compare different stimulus effects, examine the influence of varying task difficulties, or explore underlying mechanisms. This paper, inspired by the positive correlation between self-efficacy and task performance within the social cognitive theory, introduces Verbal Efficacy Stimulations (VES). Our VES comprises three types of verbal prompts: encouraging, provocative, and critical, addressing six aspects such as helpfulness and competence. And we further categorize task difficulty, aiming to extensively investigate how distinct VES influence the self-efficacy and task achievements of language models at varied levels of difficulty. The experimental results show that the three types of VES improve the performance of LLMs on most tasks, and the most effective VES varies for different models. In extensive experiments, we have obtained some findings consistent with psychological theories, providing novel insights for future research. </p>
<blockquote>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬èƒ½åŠ›å·²ç»å¾—åˆ°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç”±äºå®ƒä»¬å¯¹è¾“å…¥çš„æ•æ„Ÿæ€§æé«˜ï¼Œç ”ç©¶è¶Šæ¥è¶Šé›†ä¸­åœ¨é€šè¿‡ç›´æ¥å’Œç®€å•çš„æç¤ºå·¥ç¨‹æ¥æé«˜LLMçš„æ€§èƒ½ï¼Œè€Œä¸æ˜¯å¤æ‚çš„åŸŸé€‚åº”ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMè¡¨ç°å‡ºæƒ…ç»ªæ™ºèƒ½ï¼Œç§¯æå’Œæ¶ˆæçš„æƒ…ç»ªéƒ½æœ‰å¯èƒ½æé«˜ä»»åŠ¡æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„äº¤äº’æç¤ºä¸»è¦é›†ä¸­åœ¨å•ä¸€åˆºæ¿€ç±»å‹ä¸Šï¼Œå¿½è§†äº†æ¯”è¾ƒä¸åŒåˆºæ¿€æ•ˆæœã€ç ”ç©¶ä¸åŒä»»åŠ¡éš¾åº¦çš„å½±å“æˆ–æ¢ç´¢æ½œåœ¨æœºåˆ¶ã€‚æœ¬æ–‡åœ¨ç¤¾ä¼šè®¤çŸ¥ç†è®ºä¸­çš„è‡ªæˆ‘æ•ˆèƒ½æ„Ÿä¸ä»»åŠ¡è¡¨ç°ä¹‹é—´çš„æ­£ç›¸å…³å…³ç³»å¯å‘ä¸‹ï¼Œå¼•å…¥äº†è¨€è¯­æ•ˆèƒ½åˆºæ¿€ï¼ˆVESï¼‰ã€‚æˆ‘ä»¬çš„VESåŒ…æ‹¬ä¸‰ç§è¨€è¯­æç¤ºï¼šé¼“åŠ±æ€§ã€æŒ‘è¡…æ€§å’Œæ‰¹åˆ¤æ€§ï¼Œæ¶‰åŠæœ‰åŠ©ç›Šæ€§å’Œèƒ½åŠ›ç­‰æ–¹é¢ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å¯¹ä»»åŠ¡éš¾åº¦è¿›è¡Œåˆ†ç±»ï¼Œæ—¨åœ¨å¹¿æ³›ç ”ç©¶ä¸åŒçš„VESå¦‚ä½•åœ¨ä¸åŒéš¾åº¦å±‚æ¬¡ä¸Šå½±å“è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ•ˆèƒ½æ„Ÿå’Œä»»åŠ¡å®Œæˆæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸‰ç§ç±»å‹çš„VESåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­éƒ½æé«˜äº†LLMçš„æ€§èƒ½ï¼Œè€Œä¸”æœ€æœ‰æ•ˆçš„VESå› æ¨¡å‹è€Œå¼‚ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€äº›ä¸å¿ƒç†ç†è®ºä¸€è‡´çš„ç ”ç©¶ç»“æœï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06669v1">PDF</a> to be published in ICONIP 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬èƒ½åŠ›æ˜¾è‘—æ”¹è¿›ï¼Œç ”ç©¶è¶Šæ¥è¶Šå…³æ³¨é€šè¿‡ç®€å•ç›´æ¥çš„æç¤ºå·¥ç¨‹æå‡æ€§èƒ½ï¼Œè€Œéå¤æ‚çš„åŸŸé€‚åº”ã€‚LLMå±•ç°å‡ºæƒ…ç»ªæ™ºèƒ½ï¼Œç§¯æä¸æ¶ˆææƒ…ç»ªå¯æå‡ä»»åŠ¡è¡¨ç°ã€‚æœ¬æ–‡å—ç¤¾ä¼šè®¤çŸ¥ç†è®ºä¸­çš„è‡ªæˆ‘æ•ˆèƒ½æ„Ÿä¸ä»»åŠ¡è¡¨ç°æ­£ç›¸å…³å…³ç³»å¯å‘ï¼Œå¼•å…¥è¨€è¯­æ•ˆèƒ½åˆºæ¿€ï¼ˆVESï¼‰ï¼ŒåŒ…æ‹¬é¼“åŠ±ã€æŒ‘è¡…å’Œæ‰¹è¯„ä¸‰ç§è¨€è¯­æç¤ºï¼Œé’ˆå¯¹æœ‰åŠ©æ€§ã€èƒ½åŠ›ç­‰å…­æ–¹é¢ï¼Œæ¢è®¨ä¸åŒVESå¯¹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ•ˆèƒ½æ„ŸåŠä»»åŠ¡å®Œæˆåº¦çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸‰ç§ç±»å‹çš„VESåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šæå‡äº†LLMçš„è¡¨ç°ï¼Œä¸”æœ€æœ‰æ•ˆçš„VESå› æ¨¡å‹è€Œå¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„é›¶æ ·æœ¬èƒ½åŠ›å·²æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>ç›¸æ¯”å¤æ‚çš„åŸŸé€‚åº”ï¼Œç®€å•ç›´æ¥çš„æç¤ºå·¥ç¨‹æ›´èƒ½æå‡LLMçš„æ€§èƒ½ã€‚</li>
<li>LLMå±•ç°å‡ºæƒ…ç»ªæ™ºèƒ½ï¼Œæƒ…ç»ªå¯å½±å“ä»»åŠ¡è¡¨ç°ã€‚</li>
<li>è¨€è¯­æ•ˆèƒ½åˆºæ¿€ï¼ˆVESï¼‰åŒ…æ‹¬é¼“åŠ±ã€æŒ‘è¡…å’Œæ‰¹è¯„ä¸‰ç§è¨€è¯­æç¤ºã€‚</li>
<li>VESå½±å“è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ•ˆèƒ½æ„ŸåŠä»»åŠ¡å®Œæˆåº¦ã€‚</li>
<li>ä¸åŒç±»å‹çš„VESåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šæå‡äº†LLMçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06bb7610bd2b5afe0b280819cc806196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bfda1a033f3a074d0f27b3e06a9daeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9fcdea29e1d15de5045ef17d9114a28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e244a897d8946998a1fdc96cf71aa3e1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EfficientLLM-Scalable-Pruning-Aware-Pretraining-for-Architecture-Agnostic-Edge-Language-Models"><a href="#EfficientLLM-Scalable-Pruning-Aware-Pretraining-for-Architecture-Agnostic-Edge-Language-Models" class="headerlink" title="EfficientLLM: Scalable Pruning-Aware Pretraining for   Architecture-Agnostic Edge Language Models"></a>EfficientLLM: Scalable Pruning-Aware Pretraining for   Architecture-Agnostic Edge Language Models</h2><p><strong>Authors:Xingrun Xing, Zheng Liu, Shitao Xiao, Boyan Gao, Yiming Liang, Wanpeng Zhang, Haokun Lin, Guoqi Li, Jiajun Zhang</strong></p>
<p>Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge language models. Distinguished from direct pretraining that bounded by the scaling law, this work proposes the pruning-aware pretraining, focusing on retaining performance of much larger optimized models. It features following characteristics: 1) Data-scalable: we introduce minimal parameter groups in LLM and continuously optimize structural pruning, extending post-training pruning methods like LLM-Pruner and SparseGPT into the pretraining phase. 2) Architecture-agnostic: the LLM architecture is auto-designed using saliency-driven pruning, which is the first time to exceed SoTA human-designed LLMs in modern pretraining. We reveal that it achieves top-quality edge language models, termed EfficientLLM, by scaling up LLM compression and extending its boundary. EfficientLLM significantly outperforms SoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM, Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first attempt, EfficientLLM bridges the performance gap between traditional LLM compression and direct pretraining methods, and we will fully open source at <a target="_blank" rel="noopener" href="https://github.com/Xingrun-Xing2/EfficientLLM">https://github.com/Xingrun-Xing2/EfficientLLM</a>. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éµå¾ªè§„æ¨¡å®šå¾‹ï¼Œåœ¨å¤§å‹æ¨¡å‹å°ºå¯¸ä¸Šå®ç°äº†æ™ºèƒ½çš„ç´§æ€¥éœ€æ±‚ã€‚æœ€è¿‘ï¼Œäººä»¬å¯¹äº‘æˆæœ¬ã€å»¶è¿Ÿå’Œéšç§çš„æ‹…å¿§ä¸æ–­å¢åŠ ï¼Œè¿«åˆ‡éœ€è¦å¼€å‘ç´§å‡‘çš„è¾¹ç¼˜è¯­è¨€æ¨¡å‹ã€‚æœ‰åˆ«äºå—è§„æ¨¡å®šå¾‹é™åˆ¶çš„ç›´æ¥é¢„è®­ç»ƒï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†æ³¨é‡ä¿ç•™æ€§èƒ½çš„ä¼˜åŒ–æ¨¡å‹é¢„è®­ç»ƒã€‚å®ƒå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š1ï¼‰æ•°æ®å¯æ‰©å±•æ€§ï¼šæˆ‘ä»¬åœ¨LLMä¸­å¼•å…¥äº†æœ€å°å‚æ•°ç»„ï¼Œå¹¶ä¸æ–­ä¼˜åŒ–ç»“æ„å‰ªæï¼Œå°†LLM-Prunerå’ŒSparseGPTç­‰åè®­ç»ƒå‰ªææ–¹æ³•æ‰©å±•åˆ°é¢„è®­ç»ƒé˜¶æ®µã€‚2ï¼‰æ¶æ„æ— å…³æ€§ï¼šæˆ‘ä»¬ä½¿ç”¨æ˜¾è‘—æ€§é©±åŠ¨çš„å‰ªæè‡ªåŠ¨è®¾è®¡LLMæ¶æ„ï¼Œè¿™æ˜¯ç°ä»£é¢„è®­ç»ƒä¸­é¦–æ¬¡è¶…è¿‡ç°æœ‰æŠ€æœ¯ä¸­äººå·¥è®¾è®¡çš„LLMã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡æ‰©å¤§LLMå‹ç¼©å¹¶æ‰©å±•å…¶è¾¹ç•Œï¼Œå¯ä»¥å®ç°é«˜æ€§èƒ½çš„è¾¹ç¼˜è¯­è¨€æ¨¡å‹ï¼Œç§°ä¸ºEfficientLLMã€‚EfficientLLMåœ¨å¸¸è¯†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºSoTAåŸºçº¿ï¼Œå‚æ•°èŒƒå›´ä¸º$ 1äº¿è‡³ 1åƒäº¿ $ï¼Œå¦‚MobileLLMã€SmolLMã€Qwen2.5-0.5Bã€OLMo-1Bã€Llama3.2-1Bç­‰ã€‚ä½œä¸ºé¦–æ¬¡å°è¯•ï¼ŒEfficientLLMç¼©å°äº†ä¼ ç»ŸLLMå‹ç¼©æ–¹æ³•å’Œç›´æ¥é¢„è®­ç»ƒæ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œæˆ‘ä»¬ä¼šåœ¨<a target="_blank" rel="noopener" href="https://github.com/Xingrun-Xing2/EfficientLLM%E4%B8%8A%E5%AE%8C%E5%85%A8%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Xingrun-Xing2/EfficientLLMä¸Šå®Œå…¨å¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06663v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è§„æ¨¡å®šå¾‹å®ç°æ™ºèƒ½çš„çªç ´ï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œäº‘æˆæœ¬ã€å»¶è¿Ÿå’Œéšç§ç­‰é—®é¢˜æ—¥ç›Šçªå‡ºï¼Œè¿«åˆ‡éœ€è¦å¼€å‘ç´§å‡‘çš„è¾¹ç¼˜è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‰ªææ„ŸçŸ¥é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ç•™ä¼˜åŒ–åå¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å…·æœ‰æ•°æ®å¯æ‰©å±•æ€§å’Œæ¶æ„æ— å…³æ€§ï¼Œé€šè¿‡æ˜¾è‘—æ€§é©±åŠ¨çš„å‰ªæè‡ªåŠ¨è®¾è®¡LLMæ¶æ„ï¼Œé¦–æ¬¡åœ¨ç°ä»£é¢„è®­ç»ƒä¸­è¶…è¶Šäº†ç°æœ‰çš„äººé€ LLMã€‚å®éªŒè¡¨æ˜ï¼ŒEfficientLLMåœ¨å¸¸è¯†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯åŸºå‡†ï¼Œå¦‚MobileLLMã€SmolLMç­‰ã€‚EfficientLLMç¼©å°äº†ä¼ ç»ŸLLMå‹ç¼©æ–¹æ³•ä¸ç›´æ¥é¢„è®­ç»ƒæ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå¹¶å…¬å¼€æºç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´äº‘æˆæœ¬ã€å»¶è¿Ÿå’Œéšç§ç­‰æŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘ç´§å‡‘çš„è¾¹ç¼˜è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æå‡ºåŸºäºå‰ªææ„ŸçŸ¥é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ç•™ä¼˜åŒ–åå¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ•°æ®å¯æ‰©å±•æ€§å’Œæ¶æ„æ— å…³æ€§ï¼Œå¯è‡ªåŠ¨è®¾è®¡LLMæ¶æ„ã€‚</li>
<li>EfficientLLMåœ¨å¸¸è¯†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯åŸºå‡†ã€‚</li>
<li>EfficientLLMç¼©å°äº†ä¼ ç»ŸLLMå‹ç¼©ä¸ç›´æ¥é¢„è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½å·®è·ã€‚</li>
<li>EfficientLLMå°†å…¬å¼€æºç ï¼Œä¾¿äºå…¶ä»–äººä½¿ç”¨å’Œè¿›ä¸€æ­¥æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d93e93085fc77733249f73bd8fa8df48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f025aad58b2410eb5cbfce8e0f9e119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a22aac2719e959ec9d0c1fc6223cc90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e0de30ecf7993f8a9ede19481ca29bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-575ab93fca2c1ad6a064fb09bab9faea.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Transparent-NLP-Using-RAG-and-LLM-Alignment-for-Privacy-Q-A"><a href="#Transparent-NLP-Using-RAG-and-LLM-Alignment-for-Privacy-Q-A" class="headerlink" title="Transparent NLP: Using RAG and LLM Alignment for Privacy Q&amp;A"></a>Transparent NLP: Using RAG and LLM Alignment for Privacy Q&amp;A</h2><p><strong>Authors:Anna Leschanowsky, Zahra Kolagar, Erion Ã‡ano, Ivan Habernal, Dara Hallinan, EmanuÃ«l A. P. Habets, Birgit Popp</strong></p>
<p>The transparency principle of the General Data Protection Regulation (GDPR) requires data processing information to be clear, precise, and accessible. While language models show promise in this context, their probabilistic nature complicates truthfulness and comprehensibility.   This paper examines state-of-the-art Retrieval Augmented Generation (RAG) systems enhanced with alignment techniques to fulfill GDPR obligations. We evaluate RAG systems incorporating an alignment module like Rewindable Auto-regressive Inference (RAIN) and our proposed multidimensional extension, MultiRAIN, using a Privacy Q&amp;A dataset. Responses are optimized for preciseness and comprehensibility and are assessed through 21 metrics, including deterministic and large language model-based evaluations.   Our results show that RAG systems with an alignment module outperform baseline RAG systems on most metrics, though none fully match human answers. Principal component analysis of the results reveals complex interactions between metrics, highlighting the need to refine metrics. This study provides a foundation for integrating advanced natural language processing systems into legal compliance frameworks. </p>
<blockquote>
<p>ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ï¼ˆGDPRï¼‰çš„é€æ˜åŸåˆ™è¦æ±‚æ•°æ®å¤„ç†ä¿¡æ¯æ¸…æ™°ã€ç²¾ç¡®ã€å¯è®¿é—®ã€‚è™½ç„¶è¯­è¨€æ¨¡å‹åœ¨è¿™æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶æ¦‚ç‡æ€§è´¨ä½¿å¾—çœŸå®æ€§å’Œå¯ç†è§£æ€§å˜å¾—å¤æ‚ã€‚æœ¬æ–‡ç ”ç©¶äº†é‡‡ç”¨å¯¹é½æŠ€æœ¯å¢å¼ºã€ç¬¦åˆGDPRè¦æ±‚çš„æœ€æ–°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚æˆ‘ä»¬è¯„ä¼°äº†é‡‡ç”¨å¦‚Rewindable Auto-regressive Inference (RAIN)çš„å¯¹é½æ¨¡å—ä»¥åŠæˆ‘ä»¬æå‡ºçš„å¤šç»´æ‰©å±•MultiRAINçš„RAGç³»ç»Ÿï¼Œä½¿ç”¨éšç§é—®ç­”æ•°æ®é›†ã€‚å“åº”è¢«ä¼˜åŒ–ä¸ºç²¾ç¡®æ€§å’Œå¯ç†è§£æ€§ï¼Œå¹¶é€šè¿‡åŒ…æ‹¬ç¡®å®šæ€§è¯„ä»·å’Œå¤§è¯­è¨€æ¨¡å‹è¯„ä»·åœ¨å†…çš„21é¡¹æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¸¦æœ‰å¯¹é½æ¨¡å—çš„RAGç³»ç»Ÿåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šä¼˜äºåŸºçº¿RAGç³»ç»Ÿï¼Œå°½ç®¡éƒ½æ— æ³•å®Œå…¨åŒ¹é…äººç±»ç­”æ¡ˆã€‚å¯¹ç»“æœçš„ä¸»æˆåˆ†åˆ†ææ­ç¤ºäº†æŒ‡æ ‡ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œçªæ˜¾äº†å®Œå–„æŒ‡æ ‡çš„å¿…è¦æ€§ã€‚æœ¬ç ”ç©¶ä¸ºå°†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæ•´åˆåˆ°æ³•å¾‹åˆè§„æ¡†æ¶ä¸­å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06652v1">PDF</a> Submitted to ARR</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæ¥æ»¡è¶³ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ï¼ˆGDPRï¼‰çš„é€æ˜åº¦åŸåˆ™çš„è¦æ±‚ã€‚æ–‡ç« ç ”ç©¶äº†å¢å¼ºå‹æ£€ç´¢è¾…åŠ©ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†æ ¡å‡†æŠ€æœ¯ï¼Œæ—¨åœ¨ç¡®ä¿æ•°æ®å¤„ç†ä¿¡æ¯çš„æ¸…æ™°ã€ç²¾ç¡®å’Œå¯è®¿é—®æ€§ã€‚æ–‡ç« è¯„ä¼°äº†ä½¿ç”¨å¯å›æ”¾çš„è‡ªåŠ¨å›å½’æ¨ç†ï¼ˆRAINï¼‰å’Œå¤šç»´æ‰©å±•ç‰ˆMultiRAINç­‰æ ¡å‡†æ¨¡å—çš„RAGç³»ç»Ÿçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¸¦æœ‰æ ¡å‡†æ¨¡å—çš„RAGç³»ç»Ÿåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šä¼˜äºåŸºçº¿RAGç³»ç»Ÿï¼Œä½†ä»æ— æ³•å®Œå…¨åŒ¹é…äººç±»çš„ç­”æ¡ˆã€‚è¯¥ç ”ç©¶ä¸ºå°†é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿèå…¥æ³•å¾‹åˆè§„æ¡†æ¶æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ï¼ˆGDPRï¼‰è¦æ±‚æ•°æ®å¤„ç†ä¿¡æ¯å¿…é¡»æ¸…æ™°ã€ç²¾ç¡®å’Œå¯è®¿é—®ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨æ­¤èƒŒæ™¯ä¸‹çš„åº”ç”¨å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶æ¦‚ç‡æ€§ç‰¹ç‚¹å½±å“äº†å…¶çœŸå®æ€§å’Œå¯ç†è§£æ€§ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†å¢å¼ºå‹æ£€ç´¢è¾…åŠ©ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†æ ¡å‡†æŠ€æœ¯ä»¥æ»¡è¶³GDPRçš„è¦æ±‚ã€‚</li>
<li>RAGç³»ç»Ÿä½¿ç”¨å¯å›æ”¾çš„è‡ªåŠ¨å›å½’æ¨ç†ï¼ˆRAINï¼‰å’Œå¤šç»´æ‰©å±•ç‰ˆMultiRAINè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¸¦æœ‰æ ¡å‡†æ¨¡å—çš„RAGç³»ç»Ÿåœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºåŸºçº¿RAGç³»ç»Ÿã€‚</li>
<li>æ²¡æœ‰ç³»ç»Ÿèƒ½å¤Ÿå®Œå…¨åŒ¹é…äººç±»çš„ç­”æ¡ˆï¼Œè¯´æ˜ä»å­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-71d5e4fadca349b48ca1e3a862b15523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f4f8222fc59771381656d958e37c302.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Steel-LLM-From-Scratch-to-Open-Source-â€“-A-Personal-Journey-in-Building-a-Chinese-Centric-LLM"><a href="#Steel-LLM-From-Scratch-to-Open-Source-â€“-A-Personal-Journey-in-Building-a-Chinese-Centric-LLM" class="headerlink" title="Steel-LLM:From Scratch to Open Source â€“ A Personal Journey in Building   a Chinese-Centric LLM"></a>Steel-LLM:From Scratch to Open Source â€“ A Personal Journey in Building   a Chinese-Centric LLM</h2><p><strong>Authors:Qingshui Gu, Shu Li, Tianyu Zheng, Zhaoxiang Zhang</strong></p>
<p>Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the projectâ€™s key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at <a target="_blank" rel="noopener" href="https://github.com/zhanshijinwat/Steel-LLM">https://github.com/zhanshijinwat/Steel-LLM</a>. </p>
<blockquote>
<p>Steel-LLMæ˜¯ä¸€ä¸ªä»¥ä¸­æ–‡ä¸ºä¸­å¿ƒçš„ä»å¤´å¼€å§‹å¼€å‘çš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åˆ›å»ºé«˜è´¨é‡çš„å¼€æºæ¨¡å‹ï¼Œå°½ç®¡è®¡ç®—èµ„æºæœ‰é™ã€‚è¯¥é¡¹ç›®äº2024å¹´3æœˆå¯åŠ¨ï¼Œç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ‹¥æœ‰1äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚é¡¹ç›®æ³¨é‡é€æ˜åº¦å’Œå®ç”¨è§è§£çš„åˆ†äº«ï¼Œä»¥å¸®åŠ©ç¤¾åŒºä¸­çš„å…¶ä»–äººã€‚è®­ç»ƒè¿‡ç¨‹ä¸»è¦ä»¥ä¸­æ–‡æ•°æ®ä¸ºä¸»ï¼Œå¹¶åŒ…å«ä¸€å°éƒ¨åˆ†è‹±æ–‡æ•°æ®ï¼Œé€šè¿‡æä¾›æ›´è¯¦ç»†å’Œå®é™…çš„å»ºæ¨¡æ—…ç¨‹è®°å½•æ¥å¡«è¡¥ç°æœ‰å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºç™½ã€‚Steel-LLMåœ¨CEVALå’ŒCMMLUç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè¶…è¶Šäº†æ¥è‡ªæ›´å¤§æœºæ„çš„æ—©æœŸæ¨¡å‹ã€‚æœ¬æ–‡å…¨é¢æ€»ç»“äº†é¡¹ç›®çš„ä¸»è¦è´¡çŒ®ï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ¨¡å‹è®¾è®¡ã€è®­ç»ƒæ–¹æ³•ä»¥åŠé‡åˆ°çš„æŒ‘æˆ˜ç­‰ï¼Œä¸ºç ”ç©¶å’Œå¸Œæœ›å¼€å‘è‡ªå·±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†å®è´µçš„èµ„æºã€‚è¯¥æ¨¡å‹çš„æ£€æŸ¥ç‚¹å’Œè®­ç»ƒè„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhanshijinwat/Steel-LLM">https://github.com/zhanshijinwat/Steel-LLM</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06635v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>é’¢é“LLMæ˜¯ä¸€ä¸ªä»¥ä¸­æ–‡ä¸ºä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ç”¨æœ‰é™çš„è®¡ç®—èµ„æºåˆ›å»ºé«˜è´¨é‡ã€å¼€æºçš„æ¨¡å‹ã€‚è¯¥é¡¹ç›®äº2024å¹´3æœˆå¯åŠ¨ï¼Œç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ‹¥æœ‰1äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥é¡¹ç›®é‡è§†é€æ˜åº¦å’Œå®ç”¨è§è§£çš„åˆ†äº«ï¼Œä»¥å¸®åŠ©ç¤¾åŒºä¸­çš„å…¶ä»–äººã€‚è¯¥æ¨¡å‹ä¸»è¦å…³æ³¨ä¸­æ–‡æ•°æ®ï¼Œå¹¶åŒ…å«ä¸€å°éƒ¨åˆ†è‹±æ–‡æ•°æ®ã€‚é’¢é“LLMå¡«è¡¥äº†ç°æœ‰å¼€æºLLMçš„ç©ºç™½ï¼Œæä¾›äº†æ›´è¯¦ç»†å’Œå®ç”¨çš„æ¨¡å‹æ„å»ºç»éªŒã€‚å®ƒåœ¨CEVALå’ŒCMMLUç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œç”šè‡³è¶…è¶Šäº†æ¥è‡ªæ›´å¤§æœºæ„çš„æ—©æœŸæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é’¢é“LLMæ˜¯ä¸€ä¸ªé¢å‘ä¸­æ–‡çš„è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨æœ‰é™çš„è®¡ç®—èµ„æºåˆ›å»ºé«˜è´¨é‡çš„å¼€æºæ¨¡å‹ã€‚</li>
<li>é¡¹ç›®ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ‹¥æœ‰1äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>è¯¥é¡¹ç›®æ³¨é‡é€æ˜åº¦å’Œå®ç”¨æ´å¯Ÿçš„åˆ†äº«ï¼Œä»¥å¸®åŠ©è¯­è¨€æ¨¡å‹ç¤¾åŒºä¸­çš„å…¶ä»–æˆå‘˜ã€‚</li>
<li>æ¨¡å‹ä¸»è¦å…³æ³¨ä¸­æ–‡æ•°æ®ï¼Œå¹¶èå…¥äº†éƒ¨åˆ†è‹±æ–‡æ•°æ®ã€‚</li>
<li>é’¢é“LLMå¡«è¡¥äº†ç°æœ‰å¼€æºLLMçš„ç©ºç™½ï¼Œæä¾›äº†è¯¦ç»†çš„æ¨¡å‹æ„å»ºç»éªŒã€‚</li>
<li>åœ¨CEVALå’ŒCMMLUç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œé’¢é“LLMè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>è¯¥é¡¹ç›®ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å®è´µçš„èµ„æºï¼Œç‰¹åˆ«æ˜¯é‚£äº›å¸Œæœ›å¼€å‘è‡ªå·±çš„LLMçš„äººã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b91771398bd90a45600e0efe366d6393.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1300183c0cd8e073cb1aa00c691b78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640b51d92f1e7abd315241d866924f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-841a661b6e044b5e539b59c19bf2d7b3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Automatic-Annotation-Augmentation-Boosts-Translation-between-Molecules-and-Natural-Language"><a href="#Automatic-Annotation-Augmentation-Boosts-Translation-between-Molecules-and-Natural-Language" class="headerlink" title="Automatic Annotation Augmentation Boosts Translation between Molecules   and Natural Language"></a>Automatic Annotation Augmentation Boosts Translation between Molecules   and Natural Language</h2><p><strong>Authors:Zhiqiang Zhong, Simon Sataa-Yu Larsen, Haoyu Guo, Tao Tang, Kuangyu Zhou, Davide Mottin</strong></p>
<p>Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA$^3$, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA$^3$ by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.   Experimental results on text-based <em>de novo</em> molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models. Notably, incorporating LA$^3$ leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA$^3$ notable applications in <em>image</em>, <em>text</em> and <em>graph</em> tasks, affirming its versatility and utility. </p>
<blockquote>
<p>è¿‘æœŸäººå·¥æ™ºèƒ½åœ¨ç”Ÿç‰©ç ”ç©¶é¢†åŸŸçš„å‘å±•é‡ç‚¹åœ¨äºæ•´åˆåˆ†å­æ•°æ®ä¸è‡ªç„¶è¯­è¨€ï¼Œä»¥åŠ é€Ÿè¯ç‰©å‘ç°è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡æ ‡æ³¨çš„ç¨€ç¼ºæ€§é™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚æœ¬æ–‡ä»‹ç»äº†LA$^3$ï¼Œä¸€ä¸ªåŸºäºè¯­è¨€çš„è‡ªåŠ¨æ ‡æ³¨å¢å¼ºæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥å¢å¼ºç°æœ‰æ•°æ®é›†ï¼Œä»è€Œæ”¹è¿›äººå·¥æ™ºèƒ½çš„è®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºå¢å¼ºæ•°æ®é›†LaChEBI-20æ¥å±•ç¤ºLA$^3$çš„æœ‰æ•ˆæ€§ï¼Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°é‡å†™äº†ç°æœ‰æ•°æ®é›†çš„åˆ†å­æ ‡æ³¨ã€‚è¿™äº›é‡å†™çš„æ ‡æ³¨ä¿ç•™äº†å…³é”®çš„åˆ†å­ä¿¡æ¯ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤šæ ·åŒ–çš„å¥å­ç»“æ„å’Œè¯æ±‡ã€‚ä½¿ç”¨LaChEBI-20ï¼Œæˆ‘ä»¬åŸºäºåŸºå‡†æ¶æ„è®­ç»ƒäº†LaMolT5ï¼Œå­¦ä¹ åˆ†å­è¡¨ç¤ºå’Œå¢å¼ºæ ‡æ³¨ä¹‹é—´çš„æ˜ å°„ã€‚åœ¨åŸºäºæ–‡æœ¬çš„æ–°åˆ†å­ç”Ÿæˆå’Œåˆ†å­æè¿°æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMolT5çš„æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸åŸºå‡†æ¶æ„ç›¸æ¯”ï¼Œèå…¥LA$^3$çš„æ”¹è¿›å¹…åº¦é«˜è¾¾301%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å›¾åƒã€æ–‡æœ¬å’Œå›¾ä»»åŠ¡ä¸­éªŒè¯äº†LA$^3$çš„æœ‰æ•ˆæ€§ï¼Œè¿™è¯å®äº†å®ƒçš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06634v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AIåœ¨ç”Ÿç‰©å­¦ç ”ç©¶ä¸­çš„æœ€æ–°è¿›å±•èšç„¦äºæ•´åˆåˆ†å­æ•°æ®ä¸è‡ªç„¶è¯­è¨€ä»¥åŠ é€Ÿè¯ç‰©å‘ç°ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡æ ‡æ³¨çš„ç¨€ç¼ºé™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚æœ¬æ–‡å¼•å…¥äº†LA$^3$ï¼Œä¸€ä¸ªåŸºäºè¯­è¨€çš„è‡ªåŠ¨æ ‡æ³¨å¢å¼ºæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºç°æœ‰æ•°æ®é›†ï¼Œä»è€Œæé«˜AIè®­ç»ƒæ•ˆæœã€‚é€šè¿‡åˆ›å»ºå¢å¼ºæ•°æ®é›†LaChEBI-20ï¼Œå±•ç¤ºäº†LA$^3$çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªå·²æœ‰æ•°æ®é›†çš„åˆ†å­æ ‡æ³¨è¿›è¡Œäº†ç³»ç»Ÿé‡å†™ã€‚è¿™äº›é‡å†™çš„æ ‡æ³¨ä¿ç•™äº†å…³é”®çš„åˆ†å­ä¿¡æ¯ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤šæ ·åŒ–çš„å¥å­ç»“æ„å’Œè¯æ±‡ã€‚ä½¿ç”¨LaChEBI-20ï¼Œæˆ‘ä»¬åŸºäºåŸºå‡†æ¶æ„è®­ç»ƒäº†LaMolT5ï¼Œå­¦ä¹ åˆ†å­è¡¨ç¤ºå’Œå¢å¼ºæ ‡æ³¨ä¹‹é—´çš„æ˜ å°„ã€‚åœ¨åŸºäºæ–‡æœ¬çš„æ–°åˆ†å­ç”Ÿæˆå’Œåˆ†å­æè¿°æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMolT5ä¼˜äºæœ€æ–°æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨LA$^3$çš„æ¨¡å‹åœ¨åŸºå‡†æ¶æ„ä¸Šå®ç°äº†é«˜è¾¾301%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å›¾åƒã€æ–‡æœ¬å’Œå›¾ä»»åŠ¡ä¸­éªŒè¯äº†LA$^3$çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨ç”Ÿç‰©å­¦ç ”ç©¶ä¸­çš„åº”ç”¨æ­£é€æ­¥å‘å±•ï¼Œç‰¹åˆ«æ˜¯æ•´åˆåˆ†å­æ•°æ®ä¸è‡ªç„¶è¯­è¨€ä»¥åŠ é€Ÿè¯ç‰©å‘ç°ã€‚</li>
<li>é«˜è´¨é‡æ ‡æ³¨çš„ç¨€ç¼ºæ€§æ˜¯é™åˆ¶è¯¥é¢†åŸŸè¿›å±•çš„ä¸€ä¸ªå…³é”®å› ç´ ã€‚</li>
<li>LA$^3$æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºç°æœ‰æ•°æ®é›†ï¼Œä»è€Œæé«˜AIè®­ç»ƒæ•ˆæœã€‚</li>
<li>LaChEBI-20æ•°æ®é›†é€šè¿‡ç³»ç»Ÿé‡å†™åˆ†å­æ ‡æ³¨è¢«åˆ›å»ºï¼Œä»¥ä¿ç•™å…³é”®åˆ†å­ä¿¡æ¯å¹¶å¢åŠ å¥å­ç»“æ„å’Œè¯æ±‡çš„å¤šæ ·æ€§ã€‚</li>
<li>LaMolT5æ¨¡å‹åœ¨æ–‡æœ¬åŸºç¡€ä¸Šçš„æ–°åˆ†å­ç”Ÿæˆå’Œåˆ†å­æè¿°æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>å¼•å…¥LA$^3$çš„æ¨¡å‹åœ¨åŸºå‡†æ¶æ„ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-69f68444ec1f3135c40fa255e8960c74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adf1a92faad5228668e9d751bf1d5711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c49667467d55139f8430a4e60e7ee19.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LawGPT-Knowledge-Guided-Data-Generation-and-Its-Application-to-Legal-LLM"><a href="#LawGPT-Knowledge-Guided-Data-Generation-and-Its-Application-to-Legal-LLM" class="headerlink" title="LawGPT: Knowledge-Guided Data Generation and Its Application to Legal   LLM"></a>LawGPT: Knowledge-Guided Data Generation and Its Application to Legal   LLM</h2><p><strong>Authors:Zhi Zhou, Kun-Yang Yu, Shi-Yu Tian, Jiang-Xin Shi, Xiao-Wen Yang, Pengxiao Song, Yi-Xuan Jin, Lan-Zhe Guo, Yu-Feng Li</strong></p>
<p>Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks. However, they face significant limitations in legal reasoning tasks. Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data. To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs. This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data. We propose KgDG, a knowledge-guided data generation framework for legal reasoning. Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data. Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities. Using KgDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples. Our trained model LawGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KgDG and LawGPT. Our code and resources is publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/KgDG-45F5">https://anonymous.4open.science/r/KgDG-45F5</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ— è®ºä¸“æœ‰è¿˜æ˜¯å¼€æºï¼Œåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ³•å¾‹æ¨ç†ä»»åŠ¡æ–¹é¢å­˜åœ¨é‡å¤§å±€é™æ€§ã€‚ä¸“æœ‰æ¨¡å‹å¸¦æ¥æ•°æ®éšç§é£é™©å’Œé«˜æ˜‚çš„æ¨ç†æˆæœ¬ï¼Œè€Œå¼€æºæ¨¡å‹ç”±äºæ³•å¾‹é¢†åŸŸè®­ç»ƒæ•°æ®ä¸è¶³è€Œè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ³•å¾‹æ¨ç†çš„æ•°æ®ç”Ÿæˆï¼Œå€ŸåŠ©ä¸“æœ‰LLMçš„å¸®åŠ©ï¼Œæé«˜å¼€æºLLMçš„æ³•å¾‹æ¨ç†æ€§èƒ½ã€‚ç”±äºä¸“æœ‰LLMä¸­ç¼ºä¹æ³•å¾‹çŸ¥è¯†ï¼Œä»¥åŠéªŒè¯ç”Ÿæˆæ•°æ®çš„å›°éš¾ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†KgDGï¼Œä¸€ä¸ªç”¨äºæ³•å¾‹æ¨ç†çš„çŸ¥è¯†å¼•å¯¼æ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨æ³•å¾‹çŸ¥è¯†æ¥æé«˜ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶å¼•å…¥ç²¾ç‚¼å’ŒéªŒè¯è¿‡ç¨‹ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰©å¤§äº†ç”Ÿæˆçš„æ•°æ®é›†ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨KgDGï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«5ä¸‡é«˜è´¨é‡ç¤ºä¾‹çš„åˆæˆæ³•å¾‹æ¨ç†æ•°æ®é›†ã€‚æˆ‘ä»¬è®­ç»ƒçš„LawGPTæ¨¡å‹è¶…è¶Šäº†ç°æœ‰çš„ç‰¹å®šæ³•å¾‹LLMï¼Œå¹¶å–å¾—äº†ä¸ä¸“æœ‰LLMç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†KgDGå’ŒLawGPTçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/KgDG-45F5%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/KgDG-45F5æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06572v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨ç§æœ‰LLMæ”¹è¿›å¼€æºLLMçš„æ³•å¾‹æ¨ç†æ€§èƒ½ï¼Œæå‡ºçŸ¥è¯†å¼•å¯¼çš„æ•°æ®ç”Ÿæˆæ¡†æ¶KgDGã€‚è¯¥æ¡†æ¶èƒ½æé«˜ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶å¼•å…¥ç²¾ç‚¼å’ŒéªŒè¯æµç¨‹ç¡®ä¿æ•°æ®è´¨é‡ã€‚ä½¿ç”¨KgDGåˆ›å»ºçš„æ³•å¾‹æ¨ç†æ•°æ®é›†ï¼Œè®­ç»ƒå‡ºçš„LawGPTæ¨¡å‹è¶…è¶Šç°æœ‰æ³•å¾‹ç‰¹å®šLLMï¼Œæ€§èƒ½ä¸ç§æœ‰LLMç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç§æœ‰LLMå­˜åœ¨æ•°æ®éšç§é£é™©å’Œé«˜æ˜‚çš„æ¨ç†æˆæœ¬ã€‚</li>
<li>å¼€æºLLMå› ç¼ºä¹æ³•å¾‹é¢†åŸŸçš„è®­ç»ƒæ•°æ®è€Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºçŸ¥è¯†å¼•å¯¼çš„æ•°æ®ç”Ÿæˆæ¡†æ¶KgDGï¼Œç”¨äºæ”¹è¿›æ³•å¾‹æ¨ç†æ€§èƒ½ã€‚</li>
<li>KgDGæ¡†æ¶åˆ©ç”¨æ³•å¾‹çŸ¥è¯†æé«˜æ•°æ®ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚</li>
<li>KgDGå¼•å…¥ç²¾ç‚¼å’ŒéªŒè¯æµç¨‹ç¡®ä¿ç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚</li>
<li>ä½¿ç”¨KgDGåˆ›å»ºçš„æ³•å¾‹æ¨ç†æ•°æ®é›†è®­ç»ƒçš„LawGPTæ¨¡å‹æ€§èƒ½å“è¶Šï¼Œè¶…è¶Šç°æœ‰æ³•å¾‹ç‰¹å®šLLMï¼Œå¹¶ä¸ç§æœ‰LLMç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13e6ca1948e75a7f3238aff0a69fbe1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fb879536ad6d08047c571b5807df1e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7207e4d7925f5b9f6e78820bbf9c713a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43eb6b9212fdf6289a96efe6a0ecbbea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cf01f1e2d89ac042d1ebd97b3ffd80a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-Meet-Symbolic-Provers-for-Logical-Reasoning-Evaluation"><a href="#Large-Language-Models-Meet-Symbolic-Provers-for-Logical-Reasoning-Evaluation" class="headerlink" title="Large Language Models Meet Symbolic Provers for Logical Reasoning   Evaluation"></a>Large Language Models Meet Symbolic Provers for Logical Reasoning   Evaluation</h2><p><strong>Authors:Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He</strong></p>
<p>First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the datasetâ€™s challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework. The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: <a target="_blank" rel="noopener" href="https://github.com/opendatalab/ProverGen">https://github.com/opendatalab/ProverGen</a> </p>
<blockquote>
<p>ä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰æ¨ç†æ¶‰åŠåºè´¯æ¼”ç»ï¼Œå¯¹æ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œå¹¶ä¸”æ˜¯è¯„ä¼°æ¨ç†èƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯åœ¨æ€ç»´é“¾ï¼ˆCoTï¼‰æƒ…å¢ƒä¸­ï¼‰çš„å®è´µä»»åŠ¡ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºå¤§é‡çš„äººå·¥æ³¨é‡Šæˆ–æ‰‹å·¥åˆ¶ä½œçš„æ¨¡æ¿ï¼Œè¿™ä½¿å¾—å®ç°å¿…è¦çš„å¤æ‚æ€§ã€å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§æ¥è¿›è¡Œç¨³å¥è¯„ä¼°å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºProverGençš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ä¸ç¬¦å·è¯æ˜è€…çš„ä¸¥è°¨æ€§å’Œç²¾ç¡®æ€§ç›¸ç»“åˆï¼Œä»è€Œåˆ›å»ºäº†å¯æ‰©å±•ã€å¤šæ ·ä¸”é«˜è´¨é‡çš„ä¸€é˜¶é€»è¾‘æ¨ç†æ•°æ®é›†ProverQAã€‚ProverQAçš„ç‰¹ç‚¹è¿˜åŒ…æ‹¬åŒ…å«æ¯ä¸ªé—®é¢˜çš„æ˜“äºç†è§£å’Œé€»è¾‘è¿è´¯çš„ä¸­é—´æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿å€ŸåŠ©æ€ç»´é“¾æç¤ºï¼Œæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿå¾ˆéš¾è§£å†³ProverQAé—®é¢˜ï¼Œè¿™å‡¸æ˜¾äº†æ•°æ®é›†çš„æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶ç”Ÿæˆçš„å•ç‹¬è®­ç»ƒé›†å¯¹Llama3.1-8B-Instructè¿›è¡Œäº†å¾®è°ƒã€‚ç»è¿‡å¾®è°ƒåçš„æ¨¡å‹åœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•é›†ä¸Šéƒ½è¡¨ç°å‡ºäº†ä¸€è‡´æ€§çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„æ•°æ®ç”Ÿæˆæ¡†æ¶çš„ä»·å€¼ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/opendatalab/ProverGen">https://github.com/opendatalab/ProverGen</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06563v1">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>ä¸€é˜¶é€»è¾‘æ¨ç†æ˜¯æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ï¼Œå¯¹äºè¯„ä¼°ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸€é˜¶é€»è¾‘æ•°æ®é›†å­˜åœ¨å¤æ‚æ€§ã€å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ProverGenæ¡†æ¶ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ä¸ç¬¦å·è¯æ˜å™¨çš„ä¸¥è°¨æ€§å’Œç²¾ç¡®æ€§ï¼Œç”Ÿæˆäº†å¯æ‰©å±•ã€å¤šæ ·ä¸”é«˜è´¨é‡çš„ä¸€é˜¶é€»è¾‘æ¨ç†æ•°æ®é›†ProverQAã€‚ProverQAçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºåŒ…å«äº†é€»è¾‘è¿è´¯çš„ä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ä½¿ç”¨æ€ç»´é“¾æç¤ºï¼Œæœ€å…ˆè¿›çš„LLMåœ¨ProverQAé—®é¢˜ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç”±æˆ‘ä»¬çš„æ¡†æ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸Šå¾®è°ƒäº†Llama3.1-8B-Instructæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•é›†ä¸Šéƒ½è¡¨ç°å‡ºäº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¬¬ä¸€é€»è¾‘æ¨ç†æ€§å¯¹æ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å®ƒä¸ºè¯„ä¼°ç³»ç»Ÿæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå®è´µçš„ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯å¯¹äºå¤æ‚çš„æ€æƒ³æµç¨‹ï¼Œå…·æœ‰æ›´å¤§çš„æŒ‘æˆ˜æ€§ã€‚ä½†ç°æœ‰çš„ä¸€é˜¶é€»è¾‘æ•°æ®é›†å­˜åœ¨å¤æ‚æ€§ã€å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„é™åˆ¶ã€‚ä¸ºæ­¤æˆ‘ä»¬æå‡ºäº†ProverGenæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ProverGenç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸ç¬¦å·è¯æ˜å™¨çš„ä¸¥è°¨æ€§å’Œç²¾ç¡®æ€§æ¥åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹ä¸€é˜¶é€»è¾‘æ¨ç†æ•°æ®é›†ProverQAï¼Œè¿™åœ¨æŸç§ç¨‹åº¦ä¸Šé¢ è¦†äº†ä¼ ç»Ÿçš„æ¨ç†æ•°æ®é›†æ¨¡å¼ã€‚è¯¥æ•°æ®é›†ä¸ä»…è§„æ¨¡åºå¤§è€Œä¸”å…·æœ‰å¤šæ ·æ€§ï¼Œæ›´é‡è¦çš„æ˜¯åŒ…å«äº†é€»è¾‘è¿è´¯çš„ä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£é—®é¢˜çš„å¤æ‚æ€§å¹¶ç»™å‡ºæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1426ab8b8f9876d47ca6d3e54532b8b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59a87548efe11fdc3abab3f33040423b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-507169838eea9aae5eb99294987e85ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b26f48278d6285324175ef85a1d0db3e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UniMoD-Efficient-Unified-Multimodal-Transformers-with-Mixture-of-Depths"><a href="#UniMoD-Efficient-Unified-Multimodal-Transformers-with-Mixture-of-Depths" class="headerlink" title="UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths"></a>UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths</h2><p><strong>Authors:Weijia Mao, Zhenheng Yang, Mike Zheng Shou</strong></p>
<p>Unified multimodal transformers, which handle both generation and understanding tasks within a shared parameter space, have received increasing attention in recent research. Although various unified transformers have been proposed, training these models is costly due to redundant tokens and heavy attention computation. In the past, studies on large language models have demonstrated that token pruning methods, such as Mixture of Depths (MoD), can significantly improve computational efficiency. MoD employs a router to select the most important ones for processing within a transformer layer. However, directly applying MoD-based token pruning to unified transformers will result in suboptimal performance because different tasks exhibit varying levels of token redundancy. In our work, we analyze the unified transformers by (1) examining attention weight patterns, (2) evaluating the layer importance and token redundancy, and (3) analyzing task interactions. Our findings reveal that token redundancy is primarily influenced by different tasks and layers. Building on these findings, we introduce UniMoD, a task-aware token pruning method that employs a separate router for each task to determine which tokens should be pruned. We apply our method to Show-o and Emu3, reducing training FLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or improving performance on several benchmarks. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/showlab/UniMoD">https://github.com/showlab/UniMoD</a>. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€è½¬æ¢å™¨åœ¨å¤„ç†ç”Ÿæˆå’Œç†è§£ä»»åŠ¡æ—¶éƒ½åœ¨åŒä¸€ä¸ªå…±äº«å‚æ•°ç©ºé—´å†…ï¼Œåœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è™½ç„¶å·²ç»æå‡ºäº†å„ç§ç»Ÿä¸€è½¬æ¢å™¨ï¼Œä½†ç”±äºå†—ä½™çš„ä»¤ç‰Œå’Œç¹é‡çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œè¿™äº›æ¨¡å‹çš„è®­ç»ƒæˆæœ¬å¾ˆé«˜ã€‚ä»¥å‰å…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å·²ç»è¯æ˜ï¼Œä»¤ç‰Œä¿®å‰ªæ–¹æ³•ï¼ˆå¦‚æ·±åº¦æ··åˆï¼ˆMoDï¼‰ï¼‰å¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚MoDä½¿ç”¨è·¯ç”±å™¨åœ¨è½¬æ¢å™¨å±‚ä¸­é€‰æ‹©æœ€é‡è¦çš„ä»¤ç‰Œè¿›è¡Œå¤„ç†ã€‚ç„¶è€Œï¼Œç›´æ¥å°†MoDåŸºäºä»¤ç‰Œçš„ä¿®å‰ªåº”ç”¨äºç»Ÿä¸€è½¬æ¢å™¨å°†å¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œå› ä¸ºä¸åŒçš„ä»»åŠ¡è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„ä»¤ç‰Œå†—ä½™ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ï¼ˆ1ï¼‰æ£€æŸ¥æ³¨æ„åŠ›æƒé‡æ¨¡å¼ï¼Œï¼ˆ2ï¼‰è¯„ä¼°å±‚çš„é‡è¦æ€§å’Œä»¤ç‰Œå†—ä½™ï¼Œï¼ˆ3ï¼‰åˆ†æä»»åŠ¡äº¤äº’æ¥åˆ†æç»Ÿä¸€è½¬æ¢å™¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä»¤ç‰Œå†—ä½™ä¸»è¦å—åˆ°ä¸åŒä»»åŠ¡å’Œå±‚çš„å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniMoDï¼Œè¿™æ˜¯ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„ä»¤ç‰Œä¿®å‰ªæ–¹æ³•ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ä½¿ç”¨å•ç‹¬çš„è·¯ç”±å™¨æ¥ç¡®å®šåº”è¯¥ä¿®å‰ªå“ªäº›ä»¤ç‰Œã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºShow-oå’ŒEmu3ï¼Œåœ¨Show-oä¸­å°†è®­ç»ƒFLOPså‡å°‘äº†å¤§çº¦15%ï¼Œåœ¨Emu3ä¸­å‡å°‘äº†40%ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¿æŒæˆ–æé«˜äº†æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/UniMoD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/showlab/UniMoDå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç»Ÿä¸€å¤šæ¨¡æ€Transformerçš„å†—ä½™æ ‡è®°é—®é¢˜ï¼Œå¹¶æå‡ºäº†UniMoDæ–¹æ³•æ¥è§£å†³ä¸åŒä»»åŠ¡ä¸­çš„æ ‡è®°å†—ä½™é—®é¢˜ã€‚é€šè¿‡åˆ†ææ³¨æ„åŠ›æƒé‡æ¨¡å¼ã€å±‚çš„é‡è¦æ€§å’Œæ ‡è®°å†—ä½™ä»¥åŠä»»åŠ¡äº¤äº’ï¼Œå‘ç°æ ‡è®°å†—ä½™ä¸»è¦å—ä¸åŒä»»åŠ¡å’Œå±‚çš„å½±å“ã€‚UniMoDé‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®å•ç‹¬çš„è·¯ç”±å™¨æ¥ç¡®å®šåº”ä¿®å‰ªå“ªäº›æ ‡è®°ã€‚è¯¥æ–¹æ³•åœ¨Show-oå’ŒEmu3ä¸Šçš„è®­ç»ƒFLOPsåˆ†åˆ«å‡å°‘äº†çº¦15%å’Œ40%ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¿æŒæˆ–æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»Ÿä¸€å¤šæ¨¡æ€Transformeråœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¹‹é—´å…±äº«å‚æ•°ç©ºé—´ï¼Œä½†è®­ç»ƒè¿™äº›æ¨¡å‹æˆæœ¬é«˜æ˜‚ï¼Œå­˜åœ¨å†—ä½™æ ‡è®°å’Œç¹é‡çš„æ³¨æ„åŠ›è®¡ç®—é—®é¢˜ã€‚</li>
<li>ä¹‹å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶è¡¨æ˜ï¼Œæ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼ˆå¦‚æ··åˆæ·±åº¦ï¼ˆMoDï¼‰ï¼‰å¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>ç›´æ¥å°†MoDåŸºäºçš„æ ‡è®°ä¿®å‰ªåº”ç”¨äºç»Ÿä¸€å˜å‹å™¨ä¼šäº§ç”Ÿæ¬¡ä¼˜æ€§èƒ½ï¼Œå› ä¸ºä¸åŒä»»åŠ¡çš„æ ‡è®°å†—ä½™ç¨‹åº¦ä¸åŒã€‚</li>
<li>æœ¬æ–‡é€šè¿‡åˆ†ææ³¨æ„åŠ›æƒé‡æ¨¡å¼ã€å±‚çš„é‡è¦æ€§å’Œæ ‡è®°å†—ä½™ä»¥åŠä»»åŠ¡äº¤äº’ï¼Œå‘ç°æ ‡è®°å†—ä½™ä¸ä¸åŒä»»åŠ¡å’Œå±‚å¯†åˆ‡ç›¸å…³ã€‚</li>
<li>åŸºäºè¿™äº›å‘ç°ï¼Œå¼•å…¥äº†UniMoDæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®å•ç‹¬çš„è·¯ç”±å™¨æ¥ç¡®å®šåº”ä¿®å‰ªå“ªäº›æ ‡è®°ã€‚</li>
<li>UniMoDæ–¹æ³•åœ¨Show-oå’ŒEmu3ä¸Šçš„è®­ç»ƒFLOPsåˆ†åˆ«å‡å°‘äº†çº¦15%å’Œ40%ï¼Œæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df94f2629cb015f006c62873cc7d42a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f2dbb7b9fc4ae0a1234f5ee88840144.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd17d49c0cd6005f2bb08d89a4537af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d32b02742a99992df23071cc0952e0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e135b701efb8fa4ea3976644041873.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Prompt-Engineering-Techniques-for-Secure-Code-Generation-with-GPT-Models"><a href="#Benchmarking-Prompt-Engineering-Techniques-for-Secure-Code-Generation-with-GPT-Models" class="headerlink" title="Benchmarking Prompt Engineering Techniques for Secure Code Generation   with GPT Models"></a>Benchmarking Prompt Engineering Techniques for Secure Code Generation   with GPT Models</h2><p><strong>Authors:Marc Bruni, Fabio Gabrielli, Mohammad Ghafari, Martin Kropp</strong></p>
<p>Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a â€œprompt agentâ€ that demonstrates how the most effective techniques can be applied in real-world development workflows. </p>
<blockquote>
<p>æç¤ºå·¥ç¨‹å¯ä»¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†é”™è¯¯ã€‚ç„¶è€Œï¼Œå…¶åœ¨ç¼“è§£LLMç”Ÿæˆä»£ç ä¸­çš„æ¼æ´æ–¹é¢çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€é¡¹åŸºå‡†æµ‹è¯•ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°å„ç§æç¤ºå·¥ç¨‹ç­–ç•¥å¯¹ä»£ç å®‰å…¨æ€§çš„å½±å“ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åˆ©ç”¨äº†ä¸¤ä¸ªç»è¿‡åŒè¡Œè¯„å®¡çš„æç¤ºæ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨é™æ€æ‰«æå™¨å¤§è§„æ¨¡è¯„ä¼°ä»£ç å®‰å…¨æ€§ã€‚æˆ‘ä»¬åœ¨GPT-3.5-turboã€GPT-4oå’ŒGPT-4o-miniä¸Šæµ‹è¯•äº†å¤šç§æç¤ºå·¥ç¨‹æŠ€æœ¯ã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹äºGPT-4oå’ŒGPT-4o-miniï¼Œä»¥å®‰å…¨ä¸ºä¸­å¿ƒçš„æç¤ºå‰ç¼€å¯ä»¥å°†å®‰å…¨æ¼æ´çš„å‡ºç°ç‡é™ä½é«˜è¾¾56%ã€‚æ­¤å¤–ï¼Œåœ¨ä½¿ç”¨è¿­ä»£æç¤ºæŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æµ‹è¯•æ¨¡å‹éƒ½æ˜¾ç¤ºå‡ºæ£€æµ‹å’Œä¿®å¤ä¹‹å‰ç”Ÿæˆä»£ç ä¸­41.9%è‡³68.7%çš„æ¼æ´çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªâ€œæç¤ºä»£ç†â€ï¼Œå±•ç¤ºäº†æœ€æœ‰æ•ˆçš„æŠ€æœ¯å¦‚ä½•åº”ç”¨äºå®é™…å¼€å‘æµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06039v1">PDF</a> Accepted at the 2025 IEEE&#x2F;ACM Second International Conference on AI   Foundation Models and Software Engineering (Forge 2025). 10 pages, 7 figures,   5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¦‚ä½•é€šè¿‡prompt engineeringæ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆä»£ç æ—¶å­˜åœ¨çš„æ¼æ´ã€‚ç ”ç©¶è€…å®ç°äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å¹³å°æ¥è¯„ä¼°ä¸åŒçš„prompt engineeringç­–ç•¥å¯¹ä»£ç å®‰å…¨çš„å½±å“ï¼Œå‘ç°é€šè¿‡ç‰¹å®šçš„å®‰å…¨å¯¼å‘æç¤ºå‰ç¼€ï¼ŒGPT-4oå’ŒGPT-4o-miniæ¨¡å‹å¯ä»¥å‡å°‘é«˜è¾¾56%çš„å®‰å…¨æ¼æ´ã€‚åŒæ—¶ï¼Œæ‰€æœ‰æµ‹è¯•æ¨¡å‹åœ¨é‡‡ç”¨è¿­ä»£æç¤ºæŠ€æœ¯åï¼Œèƒ½å¤Ÿæ£€æµ‹å’Œä¿®å¤å…ˆå‰ç”Ÿæˆä»£ç ä¸­é«˜è¾¾68.7%çš„æ¼æ´ã€‚æœ€åï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œæç¤ºä»£ç†â€çš„å·¥å…·ï¼Œå±•ç¤ºäº†æœ€æœ‰æ•ˆçš„æŠ€æœ¯å¦‚ä½•åº”ç”¨äºå®é™…å¼€å‘æµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prompt engineeringæœ‰åŠ©äºå‡å°‘LLMåœ¨ç”Ÿæˆä»£ç æ—¶çš„é”™è¯¯å’Œæ¼æ´ã€‚</li>
<li>ç ”ç©¶è€…å®ç°äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å¹³å°æ¥è¯„ä¼°prompt engineeringç­–ç•¥å¯¹ä»£ç å®‰å…¨çš„å½±å“ã€‚</li>
<li>å®‰å…¨å¯¼å‘çš„æç¤ºå‰ç¼€å¯ä»¥æ˜¾è‘—å‡å°‘GPT-4oå’ŒGPT-4o-miniæ¨¡å‹ç”Ÿæˆä»£ç æ—¶çš„å®‰å…¨æ¼æ´ï¼Œå‡å°‘ç‡é«˜è¾¾56%ã€‚</li>
<li>é‡‡ç”¨è¿­ä»£æç¤ºæŠ€æœ¯åï¼Œæ‰€æœ‰æµ‹è¯•æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹å’Œä¿®å¤å…ˆå‰ç”Ÿæˆä»£ç ä¸­è¾ƒé«˜æ¯”ä¾‹çš„æ¼æ´ã€‚</li>
<li>æµ‹è¯•äº†å¤šç§prompt engineeringæŠ€æœ¯åœ¨GPT-3.5-turboã€GPT-4oå’ŒGPT-4o-miniä¸Šçš„æ•ˆæœã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œæç¤ºä»£ç†â€çš„å·¥å…·ï¼Œå±•ç¤ºå¦‚ä½•åœ¨å®é™…å¼€å‘æµç¨‹ä¸­åº”ç”¨æœ€æœ‰æ•ˆçš„prompt engineeringæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-55d461a8fcb2912a791930bc919b1d35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be2278dc9953e2b0be33e8901bd136cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783476f603748bf7e54a17a2e86df31f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0759bd23748fc2c55bc1e55a9b530ef2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6efda39c7b4f88f476cda03d5af49f3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c709b777189df95143010a0efa0102e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GRAIT-Gradient-Driven-Refusal-Aware-Instruction-Tuning-for-Effective-Hallucination-Mitigation"><a href="#GRAIT-Gradient-Driven-Refusal-Aware-Instruction-Tuning-for-Effective-Hallucination-Mitigation" class="headerlink" title="GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective   Hallucination Mitigation"></a>GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective   Hallucination Mitigation</h2><p><strong>Authors:Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, Conghui He</strong></p>
<p>Refusal-Aware Instruction Tuning (RAIT) aims to enhance Large Language Models (LLMs) by improving their ability to refuse responses to questions beyond their knowledge, thereby reducing hallucinations and improving reliability. Effective RAIT must address two key challenges: firstly, effectively reject unknown questions to minimize hallucinations; secondly, avoid over-refusal to ensure questions that can be correctly answered are not rejected, thereby maintain the helpfulness of LLM outputs. In this paper, we address the two challenges by deriving insightful observations from the gradient-based perspective, and proposing the Gradient-driven Refusal Aware Instruction Tuning Framework GRAIT: (1) employs gradient-driven sample selection to effectively minimize hallucinations and (2) introduces an adaptive weighting mechanism during fine-tuning to reduce the risk of over-refusal, achieving the balance between accurate refusals and maintaining useful responses. Experimental evaluations on open-ended and multiple-choice question answering tasks demonstrate that GRAIT significantly outperforms existing RAIT methods in the overall performance. The source code and data will be available at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/GRAIT">https://github.com/opendatalab/GRAIT</a> . </p>
<blockquote>
<p>æ‹’ç»æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰æ—¨åœ¨é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹’ç»å›ç­”è¶…å‡ºå…¶çŸ¥è¯†èŒƒå›´é—®é¢˜çš„èƒ½åŠ›ï¼Œä»è€Œå‡å°‘å¹»è§‰å¹¶æé«˜å…¶å¯é æ€§ï¼Œä»è€Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ã€‚æœ‰æ•ˆçš„RAITå¿…é¡»è§£å†³ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šé¦–å…ˆï¼Œæœ‰æ•ˆåœ°æ‹’ç»æœªçŸ¥é—®é¢˜ï¼Œä»¥å°½é‡å‡å°‘å¹»è§‰ï¼›å…¶æ¬¡ï¼Œé¿å…è¿‡åº¦æ‹’ç»ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæ­£ç¡®å›ç­”çš„é—®é¢˜ä¸ä¼šè¢«æ‹’ç»ï¼Œä»è€Œä¿æŒLLMè¾“å‡ºçš„æœ‰ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»åŸºäºæ¢¯åº¦çš„è§’åº¦è¿›è¡Œæ·±åˆ»è§‚å¯Ÿæ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œå¹¶æå‡ºæ¢¯åº¦é©±åŠ¨æ‹’ç»æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´æ¡†æ¶GRAITï¼šï¼ˆ1ï¼‰é‡‡ç”¨æ¢¯åº¦é©±åŠ¨æ ·æœ¬é€‰æ‹©æ¥æœ‰æ•ˆå‡å°‘å¹»è§‰ï¼›ï¼ˆ2ï¼‰åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥è‡ªé€‚åº”åŠ æƒæœºåˆ¶ï¼Œä»¥é™ä½è¿‡åº¦æ‹’ç»çš„é£é™©ï¼Œå®ç°å‡†ç¡®æ‹’ç»ä¸ä¿æŒæœ‰ç”¨å›åº”ä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨å¼€æ”¾å¼å’Œå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGRAITåœ¨æ•´ä½“æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„RAITæ–¹æ³•ã€‚æºä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/GRAIT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/opendatalab/GRAITä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05911v1">PDF</a> Equal contribution: Runchuan Zhu, Zinco Jiang, Jiang Wu;   Corresponding author: Conghui He</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹’ç»æ„è¯†æŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰æ—¨åœ¨é€šè¿‡æé«˜æ¨¡å‹æ‹’ç»å›ç­”è¶…å‡ºå…¶çŸ¥è¯†èŒƒå›´çš„é—®é¢˜çš„èƒ½åŠ›ï¼Œä»è€Œå‡å°‘å¹»è§‰å¹¶å¢å¼ºå¯é æ€§æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºäº†æ¢¯åº¦é©±åŠ¨çš„æ‹’ç»æ„è¯†æŒ‡ä»¤è°ƒæ•´æ¡†æ¶GRAITæ¥è§£å†³ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œæœ‰æ•ˆæ‹’ç»æœªçŸ¥é—®é¢˜å¹¶æœ€å°åŒ–å¹»è§‰é£é™©ï¼Œé¿å…è¿‡åº¦æ‹’ç»ï¼Œç¡®ä¿èƒ½å¤Ÿæ­£ç¡®å›ç­”çš„é—®é¢˜ä¸ä¼šè¢«æ‹’ç»ï¼Œä»è€Œä¿æŒLLMè¾“å‡ºçš„å®ç”¨æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGRAITåœ¨æ•´ä½“æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰RAITæ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨å…¬å¼€ä»“åº“å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAITæ—¨åœ¨é€šè¿‡æé«˜LLMæ‹’ç»å›ç­”è¶…å‡ºå…¶çŸ¥è¯†èŒƒå›´é—®é¢˜çš„èƒ½åŠ›æ¥å¢å¼ºå…¶æ€§èƒ½ã€‚</li>
<li>GRAITæ¡†æ¶é€šè¿‡æ¢¯åº¦é©±åŠ¨çš„æ ·æœ¬é€‰æ‹©å’Œè‡ªé€‚åº”æƒé‡æœºåˆ¶æ¥è§£å†³æœ‰æ•ˆæ‹’ç»æœªçŸ¥é—®é¢˜å’Œé¿å…è¿‡åº¦æ‹’ç»çš„æŒ‘æˆ˜ã€‚</li>
<li>GRAITæ—¨åœ¨å¹³è¡¡å‡†ç¡®æ‹’ç»å’Œä¿æŒæœ‰ç”¨å›åº”ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºGRAITåœ¨æ•´ä½“æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰RAITæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a04b34558301a3f143087c2f6dfb6ab4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3865dd5b4d3ef24863456327555f6e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a46b812326ee758a802be9c2e11c40ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc503b59ac08f4951d3183dff92cdb49.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LegalSeg-Unlocking-the-Structure-of-Indian-Legal-Judgments-Through-Rhetorical-Role-Classification"><a href="#LegalSeg-Unlocking-the-Structure-of-Indian-Legal-Judgments-Through-Rhetorical-Role-Classification" class="headerlink" title="LegalSeg: Unlocking the Structure of Indian Legal Judgments Through   Rhetorical Role Classification"></a>LegalSeg: Unlocking the Structure of Indian Legal Judgments Through   Rhetorical Role Classification</h2><p><strong>Authors:Shubham Kumar Nigam, Tanmay Dubey, Govind Sharma, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</strong></p>
<p>In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce LegalSeg, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory RhetoricLLaMA, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP. </p>
<blockquote>
<p>æœ¬æ–‡æˆ‘ä»¬é€šè¿‡ä¿®è¾è§’è‰²åˆ†ç±»è§£å†³äº†æ³•å¾‹æ–‡ä¹¦è¯­ä¹‰åˆ†å‰²çš„ä»»åŠ¡ï¼Œé‡ç‚¹èšç„¦å°åº¦æ³•å¾‹åˆ¤å†³ã€‚æˆ‘ä»¬ä»‹ç»äº†LegalSegï¼Œè¿™æ˜¯è¯¥ä»»åŠ¡æœ€å¤§çš„æ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡7000ä»½æ–‡æ¡£å’Œ140ä¸‡å¥è¯ï¼Œç”¨7ç§ä¿®è¾è§’è‰²è¿›è¡Œæ ‡æ³¨ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªæœ€æ–°æ¨¡å‹ï¼ŒåŒ…æ‹¬åˆ†å±‚BiLSTM-CRFã€TransformerOverInLegalBERTï¼ˆToInLegalBERTï¼‰ã€å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œè§’è‰²æ„ŸçŸ¥è½¬æ¢å™¨ï¼Œä»¥åŠæ¢ç´¢æ€§çš„ä¿®è¾LLaMAå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€ç»“æ„å…³ç³»å’Œè¿ç»­å¥å­ä¿¡æ¯çš„æ¨¡å‹ä¼˜äºä»…ä¾èµ–å¥å­çº§åˆ«ç‰¹å¾çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å®éªŒï¼Œä½¿ç”¨å‘¨å›´çš„ä¸Šä¸‹æ–‡å’Œç›¸é‚»å¥å­çš„é¢„æµ‹æˆ–å®é™…æ ‡ç­¾æ¥è¯„ä¼°å®ƒä»¬å¯¹åˆ†ç±»å‡†ç¡®æ€§çš„å½±å“ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨åŒºåˆ†ç›¸å…³è§’è‰²å’Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å…ˆè¿›æŠ€æœ¯å¯¹æé«˜æ³•å¾‹æ–‡ä¹¦ç†è§£æ½œåŠ›çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ³•å¾‹NLPçš„æœªæ¥å‘å±•å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05836v1">PDF</a> Accepted on NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºä¿®è¾è§’è‰²åˆ†ç±»çš„è¯­ä¹‰åˆ†å‰²æŠ€æœ¯ï¼Œä¸“æ³¨äºå°åº¦æ³•å¾‹æ–‡ä¹¦çš„åˆ†æã€‚ç ”ç©¶è€…æ¨å‡ºäº†LegalSegæ•°æ®é›†ï¼Œè¿™æ˜¯è¯¥ä»»åŠ¡æœ€å¤§çš„æ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡7000ä»½æ–‡æ¡£å’Œ140ä¸‡å¥è¯ï¼Œè¢«æ ‡æ³¨äº†7ç§ä¿®è¾è§’è‰²ã€‚æ–‡ç« è¯„ä¼°äº†å¤šç§å…ˆè¿›æŠ€æœ¯æ¨¡å‹ï¼ŒåŒ…æ‹¬å±‚æ¬¡åŒ–BiLSTM-CRFã€TransformerOverInLegalBERTï¼ˆToInLegalBERTï¼‰ã€å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œè§’è‰²æ„ŸçŸ¥è½¬æ¢å™¨ï¼Œå¹¶æ¢ç´¢æ€§åœ°ä½¿ç”¨äº†ä¿®è¾LLaMAå¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œèå…¥æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€ç»“æ„å…³ç³»å’Œè¿ç»­å¥å­ä¿¡æ¯çš„æ¨¡å‹ï¼Œè¡¨ç°ä¼˜äºä»…ä¾èµ–å¥å­çº§åˆ«ç‰¹å¾çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‘¨å›´ä¸Šä¸‹æ–‡å’Œç›¸é‚»å¥å­çš„é¢„æµ‹æˆ–å®é™…æ ‡ç­¾è¿›è¡Œå®éªŒï¼Œä»¥è¯„ä¼°å®ƒä»¬å¯¹åˆ†ç±»å‡†ç¡®æ€§çš„å½±å“ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨åŒºåˆ†ç›¸å…³è§’è‰²å’Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†å…ˆè¿›æŠ€æœ¯å¯¹æ”¹å–„æ³•å¾‹æ–‡ä¹¦ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ³•å¾‹NLPçš„æœªæ¥å‘å±•å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶ä¸“æ³¨äºé€šè¿‡ä¿®è¾è§’è‰²åˆ†ç±»è¿›è¡Œæ³•å¾‹æ–‡æ¡£çš„è¯­ä¹‰åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯å°åº¦æ³•å¾‹åˆ¤å†³ã€‚</li>
<li>å¼•å…¥äº†Largest annotated dataset for this task - LegalSegï¼ŒåŒ…å«7,000å¤šä¸ªæ–‡æ¡£å’Œ140ä¸‡å¥æ ‡æ³¨å¥å­ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§å…ˆè¿›æŠ€æœ¯æ¨¡å‹ï¼ŒåŒ…æ‹¬BiLSTM-CRFã€ToInLegalBERTã€GNNså’ŒRole-Aware Transformersç­‰ã€‚</li>
<li>ç ”ç©¶å‘ç°èå…¥æ›´å¤šä¸Šä¸‹æ–‡ã€ç»“æ„å…³ç³»å’Œè¿ç»­å¥å­ä¿¡æ¯çš„æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>é€šè¿‡å‘¨å›´ä¸Šä¸‹æ–‡å’Œç›¸é‚»å¥å­çš„æ ‡ç­¾è¯„ä¼°äº†å…¶å¯¹åˆ†ç±»å‡†ç¡®æ€§çš„å½±å“ã€‚</li>
<li>å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†ä»é¢ä¸´åŒºåˆ†ç›¸å…³è§’è‰²å’Œåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48cddfddae37c8ec2800a170e7848192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a0ff7803c819a736cf69521b737309.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-959c2c291ce5fa10d1d5e62001eb8fc6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platforms"><a href="#Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platforms" class="headerlink" title="Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platforms"></a>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platforms</h2><p><strong>Authors:Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</strong></p>
<p>Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3&#x2F;R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a buildingâ€™s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a buildingâ€™s address, postal code, or geographic coordinates. </p>
<blockquote>
<p>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šçš„åŸå¸‚è™šæ‹Ÿå‰¯æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥å•æ ‹å»ºç­‘è§„æ¨¡ä¸ºé‡ç‚¹çš„æ¡†æ¶ã€‚é€šè¿‡è¿æ¥åˆ°è°·æ­Œåœ°å›¾å¹³å°APIç­‰äº‘åœ°å›¾å¹³å°ï¼Œåˆ©ç”¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹æ•°æ®åˆ†æChatGPTï¼ˆç¬¬4ç‰ˆï¼‰å’ŒDeepseek-V3&#x2F;R1æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨åŸºäºé«˜æ–¯åˆ†å‰²æŠ€æœ¯çš„ç½‘æ ¼æå–ç®¡é“ï¼Œæˆ‘ä»¬çš„æ•°å­—åŒèƒèƒå»ºç­‘æ¡†æ¶å¯ä»¥æ£€ç´¢å»ºç­‘çš„3Dæ¨¡å‹ã€è§†è§‰æè¿°ä¿¡æ¯ï¼Œå®ç°åŸºäºäº‘çš„åœ°å›¾é›†æˆå’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å»ºç­‘æ•°æ®åˆ†ææ•´åˆã€‚è¿™äº›å¯ä»¥é€šè¿‡ä½¿ç”¨å»ºç­‘ç‰©çš„åœ°å€ã€é‚®æ”¿ç¼–ç æˆ–åœ°ç†åæ ‡æ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åŸå¸‚çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œé€šè¿‡åˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªä»¥å•æ ‹å»ºç­‘ä¸ºç„¦ç‚¹çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸è°·æ­Œåœ°å›¾å¹³å°APIç­‰äº‘åœ°å›¾å¹³å°çš„è¿æ¥ï¼Œè¿ç”¨å…ˆè¿›çš„åŸºäºå¤šæ™ºèƒ½ä½“çš„è¯­è¨€æ¨¡å‹æ•°æ®åˆ†æå’Œå»ºç­‘æ¨¡å‹ï¼Œèƒ½å¤Ÿæ£€ç´¢å»ºç­‘ç‰©çš„ä¸‰ç»´æ¨¡å‹ã€è§†è§‰æè¿°ï¼Œå¹¶å®ç°åŸºäºäº‘çš„åœ°å›¾é›†æˆä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®åˆ†ææ•´åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åŸå¸‚çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œåˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’å’Œç®¡ç†ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ä»¥å•æ ‹å»ºç­‘ä¸ºç„¦ç‚¹ã€‚</li>
<li>é€šè¿‡äº‘åœ°å›¾å¹³å°è¿æ¥ï¼Œå¦‚Google Map Platforms APIsã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„å¤šæ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹æ•°æ®åˆ†æï¼Œå¦‚ChatGPTå’ŒDeepseek-V3&#x2F;R1ã€‚</li>
<li>æ¡†æ¶å¯æ£€ç´¢å»ºç­‘ç‰©çš„ä¸‰ç»´æ¨¡å‹å’Œè§†è§‰æè¿°ã€‚</li>
<li>é€šè¿‡é«˜æ–¯ç‚¹äº‘æ•°æ®çš„åœ°å›¾èåˆæŠ€æœ¯å®ç°äº‘åœ°å›¾é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f982260658a363a0126e1d93c0a1b2a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab7856b3c2d207d8e26d17ab282007ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-511989bdfc99fa44c3216465d43b8839.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f687c699bb65931360517387d7fae53f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093fb16992977cba99aedd67067aeafb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee475c836077e72d45aa97380cb700a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd69bacfb0b9c79536b9551b77ae9acc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3ea5af80ebe75832fea69e3b3e9326b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d83d478cf20703a2b1160e7153b264f1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-End-to-End-Relation-Extraction-in-Chinese-A-Comparative-Study-of-Gemini-LLaMA-and-ChatGPT"><a href="#Zero-Shot-End-to-End-Relation-Extraction-in-Chinese-A-Comparative-Study-of-Gemini-LLaMA-and-ChatGPT" class="headerlink" title="Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study   of Gemini, LLaMA and ChatGPT"></a>Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study   of Gemini, LLaMA and ChatGPT</h2><p><strong>Authors:Shaoshuai Du, Yiyi Tao, Yixian Shen, Hang Zhang, Yanxin Shen, Xinyu Qiu, Chuanqi Shi</strong></p>
<p>This study investigates the performance of various large language models (LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that integrates entity recognition and relation extraction without requiring annotated data. While LLMs show promise for RE, most prior work focuses on English or assumes pre-annotated entities, leaving their effectiveness in Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini, and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates the highest overall performance, balancing precision and recall, while Gemini achieves the fastest inference speed, making it suitable for real-time applications. LLaMA underperforms in both accuracy and latency, highlighting the need for further adaptation. Our findings provide insights into the strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on trade-offs between accuracy and efficiency. This study serves as a foundation for future research aimed at improving LLM adaptability to complex linguistic tasks in Chinese NLP. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬ç«¯åˆ°ç«¯ä¸­æ–‡å…³ç³»æŠ½å–ï¼ˆREï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¿™æ˜¯ä¸€ä¸ªé›†æˆäº†å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–çš„ä»»åŠ¡ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚è™½ç„¶LLMåœ¨REæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶é›†ä¸­åœ¨è‹±è¯­æˆ–å‡è®¾é¢„å…ˆæ ‡æ³¨çš„å®ä½“ä¸Šï¼Œåœ¨ä¸­æ–‡REä¸­çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬åŸºäºå‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§å¯¹ChatGPTã€Geminiå’ŒLLaMAè¿›è¡Œäº†è¯„ä¼°ã€‚ChatGPTå±•ç°å‡ºæœ€é«˜çš„æ€»ä½“æ€§èƒ½ï¼Œåœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œè€ŒGeminiçš„æ¨ç†é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨ã€‚LLaMAåœ¨å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢éƒ½è¡¨ç°ä¸ä½³ï¼Œè¿™çªæ˜¾äº†éœ€è¦è¿›ä¸€æ­¥é€‚åº”çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºLLMåœ¨é›¶æ ·æœ¬ä¸­æ–‡REæ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§æä¾›äº†è§è§£ï¼Œä¸ºå‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡æä¾›äº†å¯ç¤ºã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥æ—¨åœ¨æé«˜LLMé€‚åº”ä¸­æ–‡NLPå¤æ‚è¯­è¨€ä»»åŠ¡çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05694v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬ç«¯åˆ°ç«¯ä¸­æ–‡å…³ç³»æŠ½å–ï¼ˆREï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥ä»»åŠ¡èåˆäº†å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚è™½ç„¶LLMåœ¨REä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å¤šé›†ä¸­åœ¨è‹±è¯­æˆ–å‡è®¾å·²é¢„æ ‡æ³¨å®ä½“ï¼Œå¯¹ä¸­æ–‡REä»»åŠ¡çš„åº”ç”¨æ¢ç´¢è¾ƒå°‘ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ChatGPTã€Geminiå’ŒLLaMAåœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§æ–¹é¢çš„è¡¨ç°ã€‚ChatGPTæ€»ä½“æ€§èƒ½æœ€ä½³ï¼Œå¹³è¡¡äº†ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼›Geminiæ¨ç†é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨ï¼›LLaMAåœ¨å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œéœ€è¿›ä¸€æ­¥è°ƒæ•´ã€‚æœ¬ç ”ç©¶ä¸ºLLMåœ¨é›¶æ ·æœ¬ä¸­æ–‡REä»»åŠ¡ä¸­çš„ä¼˜ç¼ºç‚¹æä¾›äº†è§è§£ï¼Œä¸ºåœ¨ä¸­æ–‡NLPçš„å¤æ‚ä»»åŠ¡ä¸­æé«˜LLMé€‚åº”æ€§æœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬ç«¯åˆ°ç«¯ä¸­æ–‡å…³ç³»æŠ½å–ï¼ˆREï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šé›†ä¸­åœ¨è‹±è¯­æˆ–é¢„æ ‡æ³¨å®ä½“å‡è®¾ä¸‹ï¼Œå¯¹ä¸­æ–‡REä»»åŠ¡æ¢ç´¢ä¸è¶³ã€‚</li>
<li>ChatGPTåœ¨REä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹³è¡¡äº†ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚</li>
<li>Geminiæ¨ç†é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨ã€‚</li>
<li>LLaMAåœ¨å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ã€‚</li>
<li>LLMåœ¨é›¶æ ·æœ¬ä¸­æ–‡REä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†å¯¹å…¶ä¼˜ç¼ºç‚¹çš„é‡è¦è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60cabcb6770f3138582f32cc7d1a7fca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b73380cd3c7a69c47dc66710524c33ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f65a7be748c2759a0f0ea490bb565642.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b99a7bafcb26e631666ab4cffde2ee4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a78b0c3f50012cb98a4e1a5b08b93fb7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Be-Query-Optimizer-for-Relational-Databases"><a href="#Can-Large-Language-Models-Be-Query-Optimizer-for-Relational-Databases" class="headerlink" title="Can Large Language Models Be Query Optimizer for Relational Databases?"></a>Can Large Language Models Be Query Optimizer for Relational Databases?</h2><p><strong>Authors:Jie Tan, Kangfei Zhao, Rui Li, Jeffrey Xu Yu, Chengzhi Piao, Hong Cheng, Helen Meng, Deli Zhao, Yu Rong</strong></p>
<p>Query optimization, which finds the optimized execution plan for a given query, is a complex planning and decision-making problem within the exponentially growing plan space in database management systems (DBMS). Traditional optimizers heavily rely on a certain cost model constructed by various heuristics and empirical tuning, probably leading to generating suboptimal plans. Recent developments of Large Language Models (LLMs) have demonstrated their potential in solving complex planning and decision-making problems, such as arithmetic and programmatic tasks. In this paper, we try to explore the potential of LLMs in handling query optimization and propose a tentative LLM-based query optimizer dubbed LLM-QO, established on PostgreSQLâ€™s execution engine. In LLM-QO, we formulate query optimization in an autoregressive fashion which directly generates the execution plan without explicit plan enumeration. To investigate the essential input of LLM-QO, we design a customized data recipe named QInstruct to collect the training data from various optimizers and serialize the databaseâ€™s meta data, queries and corresponding plans into a textual format. Based on QInstruct, we implement a two-stage fine-tuning pipeline, Query Instruction Tuning (QIT) and Query Direct Preference Optimization (QDPO), to empower the capability of general-purpose LLMs in handling query optimization. In our experiments, LLM-QO can generate valid and high-quality plans and consistently outperforms both traditional and learned optimizers on three query workloads. Our findings verify that LLMs can be derived as query optimizers where generalization, efficiency and adaptivity deserve further research efforts. </p>
<blockquote>
<p>æŸ¥è¯¢ä¼˜åŒ–æ˜¯åœ¨æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼ˆDBMSï¼‰ä¸­ä¸ºä¸€ä¸ªç»™å®šæŸ¥è¯¢æ‰¾åˆ°ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’çš„è¿‡ç¨‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„è§„åˆ’å’Œå†³ç­–é—®é¢˜ï¼Œéšç€è®¡åˆ’ç©ºé—´çš„æŒ‡æ•°å¢é•¿è€Œå˜å¾—æ›´åŠ å›°éš¾ã€‚ä¼ ç»Ÿä¼˜åŒ–å™¨ä¸¥é‡ä¾èµ–äºç”±å„ç§å¯å‘å¼æ–¹æ³•å’Œç»éªŒè°ƒæ•´æ„å»ºçš„æˆæœ¬æ¨¡å‹ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆæ¬¡ä¼˜è®¡åˆ’ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•è¡¨æ˜å…¶åœ¨è§£å†³å¤æ‚è§„åˆ’å’Œå†³ç­–é—®é¢˜ä¸Šçš„æ½œåŠ›ï¼Œå¦‚ç®—æœ¯å’Œç¼–ç¨‹ä»»åŠ¡ã€‚æœ¬æ–‡å°è¯•æ¢ç´¢LLMåœ¨å¤„ç†æŸ¥è¯¢ä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æå‡ºä¸€ä¸ªåŸºäºLLMçš„æŸ¥è¯¢ä¼˜åŒ–å™¨LLM-QOï¼Œå®ƒå»ºç«‹åœ¨PostgreSQLçš„æ‰§è¡Œå¼•æ“ä¸Šã€‚åœ¨LLM-QOä¸­ï¼Œæˆ‘ä»¬ä»¥è‡ªå›å½’çš„æ–¹å¼åˆ¶å®šæŸ¥è¯¢ä¼˜åŒ–ï¼Œç›´æ¥ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œæ— éœ€æ˜ç¡®çš„è®¡åˆ’æšä¸¾ã€‚ä¸ºäº†ç ”ç©¶LLM-QOçš„åŸºæœ¬è¾“å…¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å®šåˆ¶çš„æ•°æ®é…æ–¹QInstructï¼Œä»å„ç§ä¼˜åŒ–å™¨ä¸­æ”¶é›†è®­ç»ƒæ•°æ®ï¼Œå¹¶å°†æ•°æ®åº“çš„å…ƒæ•°æ®ã€æŸ¥è¯¢å’Œç›¸åº”è®¡åˆ’åºåˆ—åŒ–ä¸ºæ–‡æœ¬æ ¼å¼ã€‚åŸºäºQInstructï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤é˜¶æ®µå¾®è°ƒç®¡é“ï¼Œå³æŸ¥è¯¢æŒ‡ä»¤è°ƒæ•´ï¼ˆQITï¼‰å’ŒæŸ¥è¯¢ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆQDPOï¼‰ï¼Œä»¥æå‡é€šç”¨LLMåœ¨å¤„ç†æŸ¥è¯¢ä¼˜åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒLLM-QOèƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆä¸”é«˜è´¨é‡çš„è®¡åˆ’ï¼Œå¹¶åœ¨ä¸‰ç§æŸ¥è¯¢å·¥ä½œè´Ÿè½½ä¸Šå§‹ç»ˆä¼˜äºä¼ ç»Ÿå’Œä¼˜åŒ–è¿‡çš„ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥ä½œä¸ºæŸ¥è¯¢ä¼˜åŒ–å™¨ï¼Œå…¶ä¸­æ³›åŒ–ã€æ•ˆç‡å’Œé€‚åº”æ€§éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶åŠªåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05562v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong><br>åœ¨æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼ˆDBMSï¼‰ä¸­ï¼ŒæŸ¥è¯¢ä¼˜åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„è§„åˆ’å’Œå†³ç­–é—®é¢˜ã€‚ä¼ ç»Ÿçš„ä¼˜åŒ–å™¨ä¾èµ–äºç”±å„ç§å¯å‘å¼æ–¹æ³•å’Œç»éªŒè°ƒæ•´æ„å»ºçš„æˆæœ¬æ¨¡å‹ï¼Œå¯èƒ½ç”Ÿæˆéæœ€ä¼˜è®¡åˆ’ã€‚æœ¬æ–‡æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æŸ¥è¯¢ä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œæå‡ºä¸€ç§åŸºäºPostgreSQLæ‰§è¡Œå¼•æ“çš„LLM-QOæŸ¥è¯¢ä¼˜åŒ–å™¨ã€‚LLM-QOé‡‡ç”¨è‡ªå›å½’æ–¹å¼åˆ¶å®šæŸ¥è¯¢ä¼˜åŒ–è®¡åˆ’ï¼Œæ— éœ€æ˜ç¡®æšä¸¾è®¡åˆ’å³å¯ç›´æ¥ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ã€‚è®¾è®¡å®šåˆ¶åŒ–æ•°æ®é…æ–¹QInstructæ”¶é›†è®­ç»ƒæ•°æ®ï¼Œå¹¶å°†æ•°æ®åº“å…ƒæ•°æ®ã€æŸ¥è¯¢å’Œç›¸åº”è®¡åˆ’åºåˆ—åŒ–ä¸ºæ–‡æœ¬æ ¼å¼ã€‚å®ç°ä¸¤ä¸ªé˜¶æ®µå¾®è°ƒç®¡é“Query Instruction Tuningï¼ˆQITï¼‰å’ŒQuery Direct Preference Optimizationï¼ˆQDPOï¼‰ï¼Œå¢å¼ºé€šç”¨LLMå¤„ç†æŸ¥è¯¢ä¼˜åŒ–çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒLLM-QOå¯ç”Ÿæˆæœ‰æ•ˆçš„é«˜è´¨é‡è®¡åˆ’ï¼Œåœ¨ä¸‰é‡æŸ¥è¯¢å·¥ä½œé‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿå’Œå­¦ä¹ çš„ä¼˜åŒ–å™¨ã€‚è¿™è¡¨æ˜LLMå¯ä»¥ä½œä¸ºæŸ¥è¯¢ä¼˜åŒ–å™¨ï¼Œå…¶ä¸­æ³›åŒ–ã€æ•ˆç‡å’Œé€‚åº”æ€§éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢ä¼˜åŒ–æ˜¯æ•°æ®åº“ç®¡ç†ç³»ç»Ÿä¸­çš„å¤æ‚é—®é¢˜ï¼Œä¼ ç»Ÿä¼˜åŒ–å™¨å¯èƒ½ç”Ÿæˆéæœ€ä¼˜è®¡åˆ’ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚è§„åˆ’å’Œå†³ç­–é—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>LLM-QOæ˜¯ä¸€ç§åŸºäºPostgreSQLçš„æŸ¥è¯¢ä¼˜åŒ–å™¨ï¼Œé‡‡ç”¨è‡ªå›å½’æ–¹å¼ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ã€‚</li>
<li>QInstructæ˜¯è®¾è®¡ç”¨äºæ”¶é›†è®­ç»ƒæ•°æ®çš„å®šåˆ¶åŒ–æ•°æ®é…æ–¹ã€‚</li>
<li>LLM-QOé€šè¿‡ä¸¤ä¸ªé˜¶æ®µå¾®è°ƒç®¡é“ï¼ˆQITå’ŒQDPOï¼‰å¢å¼ºå¤„ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜LLM-QOä¼˜äºä¼ ç»Ÿå’Œä¼˜åŒ–å™¨ï¼Œèƒ½ç”Ÿæˆæœ‰æ•ˆçš„é«˜è´¨é‡è®¡åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf3de1efbdd66a6e677ed69eee055935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20fc001ad2d20b51effd0c519c54f566.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42c4355cb56cdd5df6b57aa4bfac04f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6796c79054bba686ab42f3f34784e0a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SSH-Sparse-Spectrum-Adaptation-via-Discrete-Hartley-Transformation"><a href="#SSH-Sparse-Spectrum-Adaptation-via-Discrete-Hartley-Transformation" class="headerlink" title="SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation"></a>SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation</h2><p><strong>Authors:Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania</strong></p>
<p>Low-rank adaptation (LoRA) has been demonstrated effective in reducing the trainable parameter number when fine-tuning a large foundation model (LLM). However, it still encounters computational and memory challenges when scaling to larger models or addressing more complex task adaptation.   In this work, we introduce Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH), a novel approach that significantly reduces the number of trainable parameters while enhancing model performance. It selects the most informative spectral components across all layers, under the guidance of the initial weights after a discrete Hartley transformation (DHT). The lightweight inverse DHT then projects the spectrum back into the spatial domain for updates.   Extensive experiments across both single-modality tasks such as language understanding and generation and multi-modality tasks such as video-text understanding demonstrate that SSH outperforms existing parameter-efficient fine-tuning (PEFT) methods while achieving substantial reductions in computational cost and memory requirements. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨å¾®è°ƒå¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œå·²è¢«è¯æ˜åœ¨å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œå½“å®ƒæ‰©å±•åˆ°æ›´å¤§æ¨¡å‹æˆ–è§£å†³æ›´å¤æ‚çš„ä»»åŠ¡é€‚åº”æ—¶ï¼Œä»ç„¶é¢ä¸´è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šè¿‡ç¦»æ•£å“ˆç‰¹åˆ©å˜æ¢ï¼ˆDHTï¼‰çš„ç¨€ç–è°±é€‚åº”ï¼ˆSSHï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¤§å¤§å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡åŒæ—¶æé«˜æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚å®ƒé€‰æ‹©æ‰€æœ‰å±‚ä¸­æœ€å…·ä¿¡æ¯é‡çš„è°±æˆåˆ†ï¼Œå¹¶åœ¨ç¦»æ•£å“ˆç‰¹åˆ©å˜æ¢åçš„åˆå§‹æƒé‡çš„æŒ‡å¯¼ä¸‹è¿›è¡Œã€‚ç„¶åï¼Œè½»é‡çº§çš„é€†DHTå°†é¢‘è°±æŠ•å½±å›ç©ºé—´åŸŸè¿›è¡Œæ›´æ–°ã€‚åœ¨å•æ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚è§†é¢‘æ–‡æœ¬ç†è§£ï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSSHåœ¨å‡å°‘è®¡ç®—æˆæœ¬å’Œå†…å­˜è¦æ±‚æ–¹é¢ä¼˜äºç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05539v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹é€‚åº”æ–¹æ³•â€”â€”Sparse Spectrum Adaptation via Discrete Hartley Transformationï¼ˆSSHï¼‰ã€‚SSHæ–¹æ³•åœ¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ç¦»æ•£å“ˆç‰¹åˆ©å˜æ¢ï¼ˆDHTï¼‰é€‰æ‹©æ‰€æœ‰å±‚ä¸­æœ€å…·ä¿¡æ¯é‡çš„è°±æˆåˆ†ï¼Œç„¶ååœ¨é€†DHTçš„å¼•å¯¼ä¸‹å°†è°±æ˜ å°„å›ç©ºé—´åŸŸè¿›è¡Œæ›´æ–°ã€‚è¯¥æ–¹æ³•åœ¨å‡å°‘è®­ç»ƒå‚æ•°æ•°é‡çš„åŒæ—¶ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨å•æ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚è§†é¢‘æ–‡æœ¬ç†è§£ï¼‰çš„å®éªŒä¸­ï¼ŒSSHéƒ½è¡¨ç°å‡ºä¼˜äºç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•çš„èƒ½åŠ›ï¼Œå¹¶å®ç°äº†è®¡ç®—æˆæœ¬å’Œå†…å­˜è¦æ±‚çš„å®è´¨æ€§é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSHæ˜¯ä¸€ç§æ–°å‹çš„æ¨¡å‹é€‚åº”æ–¹æ³•ï¼ŒåŸºäºç¦»æ•£å“ˆç‰¹åˆ©å˜æ¢ï¼ˆDHTï¼‰ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå‚æ•°æ•°é‡ã€‚</li>
<li>SSHé€šè¿‡é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è°±æˆåˆ†æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SSHæ–¹æ³•å¯ä»¥åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­æé«˜æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£å’Œç”Ÿæˆã€è§†é¢‘æ–‡æœ¬ç†è§£ç­‰ã€‚</li>
<li>SSHç›¸æ¯”ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>SSHæ–¹æ³•èƒ½å¤Ÿé™ä½æ¨¡å‹è®­ç»ƒçš„è®¡ç®—æˆæœ¬å’Œå†…å­˜è¦æ±‚ã€‚</li>
<li>SSHæ–¹æ³•çš„æ€§èƒ½æå‡å¾—ç›Šäºç¦»æ•£å“ˆç‰¹åˆ©å˜æ¢å’Œé€†DHTçš„è¿ç”¨ï¼Œèƒ½å¤Ÿå®ç°è°±æˆåˆ†çš„é€‰æ‹©å’Œæ˜ å°„ã€‚</li>
<li>SSHä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é€‚åº”æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e401392aedafe3172c561dc180b4c454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6b526fe92f7b92f2189e05e28c2342.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c0119a1589ca1032eb96efb6d945d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-632890b40c084382e7be7433823bf8ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a1347f89432868851992ef0fa938c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e46410daf04fcd0ed348a40fcefba895.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-Prompt-Content-Enhancing-LLM-Performance-via-Content-Format-Integrated-Prompt-Optimization"><a href="#Beyond-Prompt-Content-Enhancing-LLM-Performance-via-Content-Format-Integrated-Prompt-Optimization" class="headerlink" title="Beyond Prompt Content: Enhancing LLM Performance via Content-Format   Integrated Prompt Optimization"></a>Beyond Prompt Content: Enhancing LLM Performance via Content-Format   Integrated Prompt Optimization</h2><p><strong>Authors:Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Peng Cheng</strong></p>
<p>Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HenryLau7/CFPO">https://github.com/HenryLau7/CFPO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œå…¶åœ¨å®é™…ä¸–ç•Œä¸­çš„æ•ˆæœå¾€å¾€ç”±æç¤ºè®¾è®¡é©±åŠ¨ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨ä¼˜åŒ–æç¤ºå†…å®¹ä¸Šï¼Œä½†æç¤ºæ ¼å¼çš„ä½œç”¨ä½œä¸ºä¸€ä¸ªå…³é”®ä½†å¸¸è¢«å¿½è§†çš„ç»´åº¦ï¼Œåªå¾—åˆ°äº†æœ‰é™çš„ç³»ç»Ÿç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å†…å®¹æ ¼å¼é›†æˆæç¤ºä¼˜åŒ–ï¼ˆCFPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è¿­ä»£ç»†åŒ–è¿‡ç¨‹è”åˆä¼˜åŒ–æç¤ºå†…å®¹å’Œæ ¼å¼çš„åˆ›æ–°æ–¹æ³•ã€‚CFPOåˆ©ç”¨è‡ªç„¶è¯­è¨€å˜å¼‚æ¥æ¢ç´¢å†…å®¹å˜åŒ–ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€æ ¼å¼æ¢ç´¢ç­–ç•¥æ¥ç³»ç»Ÿè¯„ä¼°å„ç§æ ¼å¼é€‰é¡¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡å’Œå¼€æºLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸ä»…ä¼˜åŒ–å†…å®¹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCFPOåœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºå¯è¡¡é‡çš„æ”¹è¿›ã€‚è¿™å¼ºè°ƒäº†é›†æˆå†…å®¹æ ¼å¼ä¼˜åŒ–çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§æé«˜LLMæ€§èƒ½çš„å®ç”¨ã€æ¨¡å‹æ— å…³çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HenryLau7/CFPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HenryLau7/CFPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04295v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå…¶å®æ•ˆæ€§å¸¸å–å†³äºæç¤ºè®¾è®¡ã€‚ç°æœ‰ç ”ç©¶å¤šå…³æ³¨æç¤ºå†…å®¹çš„ä¼˜åŒ–ï¼Œè€Œæç¤ºæ ¼å¼è¿™ä¸€é‡è¦ä½†å¸¸è¢«å¿½è§†çš„æ–¹é¢å°šæœªå¾—åˆ°ç³»ç»Ÿçš„ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„æ–¹æ³•â€”â€”å†…å®¹ä¸æ ¼å¼ä¸€ä½“åŒ–æç¤ºä¼˜åŒ–ï¼ˆCFPOï¼‰ï¼Œé€šè¿‡è¿­ä»£è¿‡ç¨‹è”åˆä¼˜åŒ–æç¤ºå†…å®¹å’Œæ ¼å¼ã€‚CFPOåˆ©ç”¨è‡ªç„¶è¯­è¨€å˜å¼‚æ¢ç´¢å†…å®¹å˜åŒ–ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€æ ¼å¼æ¢ç´¢ç­–ç•¥ç³»ç»Ÿè¯„ä¼°ä¸åŒçš„æ ¼å¼é€‰é¡¹ã€‚åœ¨å¤šä¸ªä»»åŠ¡å’Œå¼€æºLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCFPOç›¸è¾ƒäºä»…ä¼˜åŒ–å†…å®¹çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå¯è¡¡é‡çš„æ€§èƒ½æå‡ã€‚è¿™å¼ºè°ƒäº†é›†æˆå†…å®¹å’Œæ ¼å¼ä¼˜åŒ–çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§å®ç”¨çš„ã€æ¨¡å‹æ— å…³çš„æå‡LLMæ€§èƒ½çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œæç¤ºè®¾è®¡å¯¹å…¶å®é™…æ•ˆæœè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æç¤ºå†…å®¹çš„ä¼˜åŒ–ï¼Œè€Œæç¤ºæ ¼å¼çš„ systematic ç ”ç©¶å°šä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æå‡º CFPO æ–¹æ³•ï¼Œè”åˆä¼˜åŒ–æç¤ºå†…å®¹å’Œæ ¼å¼ã€‚</li>
<li>CFPO åˆ©ç”¨è‡ªç„¶è¯­è¨€å˜å¼‚å’ŒåŠ¨æ€æ ¼å¼æ¢ç´¢ç­–ç•¥è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>CFPO åœ¨å¤šä¸ªä»»åŠ¡å’Œå¼€æºLLMä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œç›¸è¾ƒäºä»…ä¼˜åŒ–å†…å®¹çš„æ–¹æ³•ï¼Œå…¶æ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>å¼ºè°ƒé›†æˆå†…å®¹å’Œæ ¼å¼ä¼˜åŒ–çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb770f5c76fbbeeae2d7c20bb37178c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8fdf20bfac1f66eab4f59854b9dd8de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fc0df7338e6ad1f2c57e7caf0d0403d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd97884f536adbb4ef63efd7914a3e52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b66b967f8d4775dcaf0da3f83bfcf65.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Exploring-Audio-Editing-Features-as-User-Centric-Privacy-Defenses-Against-Large-Language-Model-LLM-Based-Emotion-Inference-Attacks"><a href="#Exploring-Audio-Editing-Features-as-User-Centric-Privacy-Defenses-Against-Large-Language-Model-LLM-Based-Emotion-Inference-Attacks" class="headerlink" title="Exploring Audio Editing Features as User-Centric Privacy Defenses   Against Large Language Model(LLM) Based Emotion Inference Attacks"></a>Exploring Audio Editing Features as User-Centric Privacy Defenses   Against Large Language Model(LLM) Based Emotion Inference Attacks</h2><p><strong>Authors:Mohd. Farhan Israk Soumik, W. K. M. Mithsara, Abdur R. Shahid, Ahmed Imteaj</strong></p>
<p>The rapid proliferation of speech-enabled technologies, including virtual assistants, video conferencing platforms, and wearable devices, has raised significant privacy concerns, particularly regarding the inference of sensitive emotional information from audio data. Existing privacy-preserving methods often compromise usability and security, limiting their adoption in practical scenarios. This paper introduces a novel, user-centric approach that leverages familiar audio editing techniques, specifically pitch and tempo manipulation, to protect emotional privacy without sacrificing usability. By analyzing popular audio editing applications on Android and iOS platforms, we identified these features as both widely available and usable. We rigorously evaluated their effectiveness against a threat model, considering adversarial attacks from diverse sources, including Deep Neural Networks (DNNs), Large Language Models (LLMs), and and reversibility testing. Our experiments, conducted on three distinct datasets, demonstrate that pitch and tempo manipulation effectively obfuscates emotional data. Additionally, we explore the design principles for lightweight, on-device implementation to ensure broad applicability across various devices and platforms. </p>
<blockquote>
<p>éšç€è¯­éŸ³æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬è™šæ‹ŸåŠ©æ‰‹ã€è§†é¢‘ä¼šè®®å¹³å°å’Œå¯ç©¿æˆ´è®¾å¤‡åœ¨å†…çš„è¯­éŸ³æŠ€æœ¯å¼•å‘äº†é‡å¤§éšç§æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»éŸ³é¢‘æ•°æ®ä¸­æ¨æ–­æ•æ„Ÿæƒ…æ„Ÿä¿¡æ¯æ–¹é¢ã€‚ç°æœ‰çš„éšç§ä¿æŠ¤æ–¹æ³•å¾€å¾€ç‰ºç‰²äº†å¯ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å®é™…æƒ…å†µä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç†Ÿæ‚‰çš„éŸ³é¢‘ç¼–è¾‘æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯éŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²å¯ç”¨æ€§çš„æƒ…å†µä¸‹ä¿æŠ¤æƒ…æ„Ÿéšç§ã€‚é€šè¿‡åˆ†æAndroidå’ŒiOSå¹³å°ä¸Šæµè¡Œçš„éŸ³é¢‘ç¼–è¾‘åº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å‘ç°è¿™äº›åŠŸèƒ½æ—¢å¹¿æ³›ä½¿ç”¨åˆæ˜“äºä½¿ç”¨ã€‚æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œäº†ä¸¥æ ¼çš„è¯„ä¼°ï¼Œä»¥åº”å¯¹å¨èƒæ¨¡å‹ï¼Œè€ƒè™‘äº†æ¥è‡ªä¸åŒæ¥æºçš„å¯¹æŠ—æ€§æ”»å‡»ï¼ŒåŒ…æ‹¬æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¯é€†æ€§æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒéŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´å¯ä»¥æœ‰æ•ˆåœ°æ¨¡ç³Šæƒ…æ„Ÿæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†è½»é‡çº§ã€è®¾å¤‡ç«¯å®ç°çš„è®¾è®¡åŸåˆ™ï¼Œä»¥ç¡®ä¿åœ¨å„ç§è®¾å¤‡å’Œå¹³å°ä¸Šçš„å¹¿æ³›åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18727v2">PDF</a> Accepted for presentation(Poster) at PPAI-25: The 6th AAAI Workshop   on Privacy-Preserving Artificial Intelligence</p>
<p><strong>Summary</strong></p>
<p>éšç€è¯­éŸ³æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒéŸ³é¢‘æ•°æ®çš„éšç§ä¿æŠ¤é—®é¢˜æ—¥ç›Šçªå‡ºï¼Œå°¤å…¶æ˜¯æƒ…æ„Ÿéšç§çš„ä¿æŠ¤ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç‰ºç‰²äº†å¯ç”¨æ€§å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ç”¨æˆ·ä¸­å¿ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç†Ÿæ‚‰çš„éŸ³é¢‘ç¼–è¾‘æŠ€æœ¯ï¼ˆå¦‚éŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´ï¼‰æ¥ä¿æŠ¤æƒ…æ„Ÿéšç§ï¼Œæ—¢ä¿æŠ¤äº†éšç§åˆä¸ç‰ºç‰²å¯ç”¨æ€§ã€‚é€šè¿‡å¯¹å®‰å“å’ŒiOSå¹³å°ä¸Šæµè¡Œçš„éŸ³é¢‘ç¼–è¾‘åº”ç”¨çš„åˆ†æï¼Œå‘ç°è¿™äº›åŠŸèƒ½å¹¿æ³›ä¸”æ˜“äºä½¿ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒéŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´å¯ä»¥æœ‰æ•ˆåœ°æ©ç›–æƒ…æ„Ÿæ•°æ®ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†è½»é‡çº§è®¾å¤‡å®æ–½çš„è®¾è®¡åŸåˆ™ï¼Œç¡®ä¿å…¶åœ¨å„ç§è®¾å¤‡å’Œå¹³å°ä¸Šçš„å¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†éŸ³é¢‘æ•°æ®éšç§ä¿æŠ¤çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯æƒ…æ„Ÿéšç§çš„ä¿æŠ¤éœ€æ±‚è¿«åˆ‡ã€‚</li>
<li>ç°æœ‰éšç§ä¿æŠ¤æ–¹æ³•å¾€å¾€ç‰ºç‰²äº†å¯ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ç”¨æˆ·ä¸­å¿ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨éŸ³é¢‘ç¼–è¾‘æŠ€æœ¯ï¼ˆå¦‚éŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´ï¼‰ä¿æŠ¤æƒ…æ„Ÿéšç§ã€‚</li>
<li>éŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´æŠ€æœ¯å¹¿æ³›å­˜åœ¨äºå®‰å“å’ŒiOSå¹³å°çš„éŸ³é¢‘ç¼–è¾‘åº”ç”¨ä¸­ï¼Œæ˜“äºä½¿ç”¨ã€‚</li>
<li>å®éªŒè¯æ˜éŸ³é«˜å’ŒèŠ‚å¥è°ƒæ•´å¯ä»¥æœ‰æ•ˆåœ°æ©ç›–æƒ…æ„Ÿæ•°æ®ï¼Œå¯¹æŠ—æ¥è‡ªä¸åŒæºçš„æ”»å‡»ï¼ŒåŒ…æ‹¬æ·±åº¦ç¥ç»ç½‘ç»œå’Œå¤§è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡è¿˜æ¢è®¨äº†è½»é‡çº§è®¾å¤‡å®æ–½çš„è®¾è®¡åŸåˆ™ï¼Œä»¥ç¡®ä¿å…¶åœ¨å„ç§è®¾å¤‡å’Œå¹³å°ä¸Šçš„å¹¿æ³›åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3082c1dfbd08a1a5b0d15c3693a9904b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-038db2f906b4ab124590bfd81ea2338e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-567649f93554fe18119dfdf005493a9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca3e8b1fcd9de4fe567a54db2014244.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e12bdb263f0bb8c82bce6d6919a5a2a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8a55e07fb790a3e01288324e7e95b5cc.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Visual Agentic AI for Spatial Reasoning with a Dynamic API
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-41c9ab818dd1c1e9027abe02b39211c8.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-11  Chest X-ray Foundation Model with Global and Local Representations   Integration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
