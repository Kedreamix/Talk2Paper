<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ViSIR Vision Transformer Single Image Reconstruction Method for Earth   System Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models"><a href="#ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models" class="headerlink" title="ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models"></a>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models</h2><p><strong>Authors:Ehsan Zeraatkar, Salah Faroughi, Jelena Tesic</strong></p>
<p>Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM). </p>
<blockquote>
<p>ç›®çš„ï¼šåœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰æ•´åˆäº†å¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å±‚å’Œç”Ÿç‰©åœˆçš„ç›¸äº’ä½œç”¨ï¼Œä»¥åœ¨å¤šç§æ¡ä»¶ä¸‹ä¼°è®¡åŒºåŸŸå’Œå…¨çƒæ°”å€™çš„çŠ¶æ€ã€‚ç”±äºESMé«˜åº¦å¤æ‚ï¼Œå› æ­¤ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„æ¥å¯¹å…¶å¤æ‚æ€§è¿›è¡Œå»ºæ¨¡å¹¶å­˜å‚¨é™é‡‡æ ·æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Vision Transformer Sinusoidal Representation Networksï¼ˆViSIRï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šViSIRç»“åˆäº†Vision Transformerï¼ˆViTï¼‰çš„SRèƒ½åŠ›ä¸Sinusoidal Representation Networkï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™èƒ½åŠ›ï¼Œä»¥è§£å†³SRä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°çš„é¢‘è°±åè§ã€‚</p>
<p>ç»“æœï¼šViSIRåœ¨ä¸‰ç§ä¸åŒæµ‹é‡ä¸Šçš„å¹³å‡PSNRæŒ‡æ ‡ä¼˜äºViT 4.1 dBï¼Œä¼˜äºSIREN 7.5 dBï¼Œä»¥åŠä¼˜äºSR-ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆSR-GANsï¼‰7.1 dBã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06741v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMsï¼‰çš„å¤æ‚æ€§åŠå…¶æ•°æ®å¤„ç†çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Vision Transformer Sinusoidal Representation Networksï¼ˆViSIRï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å•ä¸€å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡ç»“åˆVision Transformerï¼ˆViTï¼‰çš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›å’ŒSinusoidal Representation Networkï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™èƒ½åŠ›ï¼Œè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViSIRåœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESMsé›†æˆå¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å·å’Œç”Ÿç‰©åœˆäº¤äº’ä½œç”¨ï¼Œç”¨äºä¼°ç®—å„ç§æ¡ä»¶ä¸‹çš„åŒºåŸŸå’Œå…¨çƒæ°”å€™çŠ¶æ€ã€‚</li>
<li>ESMæ•°æ®å¤„ç†çš„å¤æ‚æ€§éœ€è¦ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>æå‡ºäº†Vision Transformer Sinusoidal Representation Networks (ViSIR) æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å•ä¸€å›¾åƒSRé‡å»ºä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>ViSIRç»“åˆäº†Vision Transformerï¼ˆViTï¼‰å’ŒSinusoidal Representation Networkï¼ˆSIRENï¼‰çš„ä¼˜ç‚¹ï¼Œè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚</li>
<li>ViSIRåœ¨æ€§èƒ½ä¸Šä¼˜äºViTã€SIRENå’ŒSR-GANsç­‰æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†4.1 dBã€7.5 dBå’Œ7.1 dBçš„PSNRã€‚</li>
<li>å®éªŒç»“æœä»¥Mean Square Errorï¼ˆMSEï¼‰ã€Peak-Signal-to-Noise-Ratioï¼ˆPSNRï¼‰å’ŒStructural Similarity Index Measureï¼ˆSSIMï¼‰ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06741v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06741v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06741v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06741v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prototype-Contrastive-Consistency-Learning-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Prototype-Contrastive-Consistency-Learning-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Prototype Contrastive Consistency Learning for Semi-Supervised Medical   Image Segmentation"></a>Prototype Contrastive Consistency Learning for Semi-Supervised Medical   Image Segmentation</h2><p><strong>Authors:Shihuan He, Zhihui Lai, Ruxin Wang, Heng Kong</strong></p>
<p>Medical image segmentation is a crucial task in medical image analysis, but it can be very challenging especially when there are less labeled data but with large unlabeled data. Contrastive learning has proven to be effective for medical image segmentation in semi-supervised learning by constructing contrastive samples from partial pixels. However, although previous contrastive learning methods can mine semantic information from partial pixels within images, they ignore the whole context information of unlabeled images, which is very important to precise segmentation. In order to solve this problem, we propose a novel prototype contrastive learning method called Prototype Contrastive Consistency Segmentation (PCCS) for semi-supervised medical image segmentation. The core idea is to enforce the prototypes of the same semantic class to be closer and push the prototypes in different semantic classes far away from each other. Specifically, we construct a signed distance map and an uncertainty map from unlabeled images. The signed distance map is used to construct prototypes for contrastive learning, and then we estimate the prototype uncertainty from the uncertainty map as trade-off among prototypes. In order to obtain better prototypes, based on the student-teacher architecture, a new mechanism named prototype updating prototype is designed to assist in updating the prototypes for contrastive learning. In addition, we propose an uncertainty-consistency loss to mine more reliable information from unlabeled data. Extensive experiments on medical image segmentation demonstrate that PCCS achieves better segmentation performance than the state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/comphsh/PCCS">https://github.com/comphsh/PCCS</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œä½†å½“æ ‡æ³¨æ•°æ®è¾ƒå°‘è€Œæ— æ ‡ç­¾æ•°æ®è¾ƒå¤šæ—¶ï¼Œè¿™ä¼šå˜å¾—éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¯¹æ¯”å­¦ä¹ é€šè¿‡ä»éƒ¨åˆ†åƒç´ æ„å»ºå¯¹æ¯”æ ·æœ¬ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ ä¸­è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡å…ˆå‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿä»å›¾åƒå†…çš„éƒ¨åˆ†åƒç´ ä¸­æŒ–æ˜è¯­ä¹‰ä¿¡æ¯ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ— æ ‡ç­¾å›¾åƒçš„æ•´ä½“ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™å¯¹äºç²¾ç¡®åˆ†å‰²éå¸¸é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŸå‹å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºåŸå‹å¯¹æ¯”ä¸€è‡´æ€§åˆ†å‰²ï¼ˆPCCSï¼‰ï¼Œç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿åŒä¸€è¯­ä¹‰ç±»åˆ«çš„åŸå‹å½¼æ­¤æ¥è¿‘ï¼Œå¹¶å°†ä¸åŒè¯­ä¹‰ç±»åˆ«çš„åŸå‹ç›¸äº’æ¨å¼€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»æ— æ ‡ç­¾å›¾åƒä¸­æ„å»ºå¸¦ç¬¦å·è·ç¦»å›¾å’Œä¸ç¡®å®šæ€§å›¾ã€‚å¸¦ç¬¦å·è·ç¦»å›¾ç”¨äºæ„å»ºå¯¹æ¯”å­¦ä¹ çš„åŸå‹ï¼Œç„¶åæˆ‘ä»¬æ ¹æ®ä¸ç¡®å®šæ€§å›¾ä¼°è®¡åŸå‹çš„ä¸ç¡®å®šæ€§ä½œä¸ºåŸå‹ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„åŸå‹ï¼Œæˆ‘ä»¬åŸºäºå­¦ç”Ÿ-æ•™å¸ˆæ¶æ„ï¼Œè®¾è®¡äº†ä¸€ç§åä¸ºåŸå‹æ›´æ–°åŸå‹çš„æ–°æœºåˆ¶ï¼Œä»¥å¸®åŠ©æ›´æ–°å¯¹æ¯”å­¦ä¹ çš„åŸå‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä»æ— æ ‡ç­¾æ•°æ®ä¸­æŒ–æ˜æ›´å¯é çš„ä¿¡æ¯ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPCCSåœ¨æœ€æ–°æ–¹æ³•çš„åŸºç¡€ä¸Šå®ç°äº†æ›´å¥½çš„åˆ†å‰²æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/comphsh/PCCS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/comphsh/PCCSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06650v1">PDF</a> 17 pages, 10 figures, 7 tables</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å…³é”®ä»»åŠ¡ï¼Œä½†åœ¨åŠç›‘ç£å­¦ä¹ åœºæ™¯ä¸‹ï¼Œç”±äºæ ‡æ³¨æ•°æ®å°‘ã€æœªæ ‡æ³¨æ•°æ®é‡å¤§ï¼Œä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³ç°æœ‰å¯¹æ¯”å­¦ä¹ æ³•å¿½ç•¥æœªæ ‡æ³¨å›¾åƒæ•´ä½“ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºåŸå‹å¯¹æ¯”ä¸€è‡´æ€§åˆ†å‰²ï¼ˆPCCSï¼‰çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåŒè¯­ä¹‰ç±»åŸå‹æ‹‰è¿‘ã€ä¸åŒè¯­ä¹‰ç±»åŸå‹æ‹‰è¿œçš„æ ¸å¿ƒæœºåˆ¶ï¼Œåˆ©ç”¨æœªæ ‡æ³¨å›¾åƒç”Ÿæˆå¸¦ç¬¦å·è·ç¦»å›¾å’Œä¸ç¡®å®šæ€§å›¾ã€‚ä¸ºè·å–æ›´å¥½çš„åŸå‹ï¼Œè®¾è®¡åŸºäºå­¦ç”Ÿ-æ•™å¸ˆæ¶æ„çš„åŸå‹æ›´æ–°æœºåˆ¶ã€‚åŒæ—¶ï¼Œæå‡ºä¸ç¡®å®šæ€§ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä»æœªæ ‡æ³¨æ•°æ®ä¸­æŒ–æ˜æ›´å¯é ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼ŒPCCSåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå®ç°äº†ä¼˜äºæœ€æ–°æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´åŠç›‘ç£å­¦ä¹ æŒ‘æˆ˜ï¼Œæ ‡æ³¨æ•°æ®å°‘ã€æœªæ ‡æ³¨æ•°æ®é‡å¤§ã€‚</li>
<li>ç°æœ‰å¯¹æ¯”å­¦ä¹ æ³•å¿½ç•¥æœªæ ‡æ³¨å›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºåŸå‹å¯¹æ¯”ä¸€è‡´æ€§åˆ†å‰²ï¼ˆPCCSï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ„å»ºå¸¦ç¬¦å·è·ç¦»å›¾å’Œä¸ç¡®å®šæ€§å›¾ã€‚</li>
<li>PCCSé€šè¿‡æ‹‰è¿‘åŒè¯­ä¹‰ç±»åŸå‹ã€æ¨è¿œä¸åŒè¯­ä¹‰ç±»åŸå‹ï¼Œå¼ºåŒ–å­¦ä¹ æ•ˆæœã€‚</li>
<li>å¼•å…¥å­¦ç”Ÿ-æ•™å¸ˆæ¶æ„ï¼Œè®¾è®¡åŸå‹æ›´æ–°æœºåˆ¶ä»¥ä¼˜åŒ–åŸå‹ã€‚</li>
<li>æå‡ºä¸ç¡®å®šæ€§ä¸€è‡´æ€§æŸå¤±ï¼Œæé«˜ä»æœªæ ‡æ³¨æ•°æ®ä¸­è·å–ä¿¡æ¯çš„å¯é æ€§ã€‚</li>
<li>å®éªŒè¯æ˜PCCSåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06650v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06650v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06650v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06650v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CT-UIO-Continuous-Time-UWB-Inertial-Odometer-Localization-Using-Non-Uniform-B-spline-with-Fewer-Anchors"><a href="#CT-UIO-Continuous-Time-UWB-Inertial-Odometer-Localization-Using-Non-Uniform-B-spline-with-Fewer-Anchors" class="headerlink" title="CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using   Non-Uniform B-spline with Fewer Anchors"></a>CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using   Non-Uniform B-spline with Fewer Anchors</h2><p><strong>Authors:Jian Sun, Wei Sun, Genwei Zhang, Kailun Yang, Song Li, Xiangqi Meng, Na Deng, Chongbin Tan</strong></p>
<p>Ultra-wideband (UWB) based positioning with fewer anchors has attracted significant research interest in recent years, especially under energy-constrained conditions. However, most existing methods rely on discrete-time representations and smoothness priors to infer a robotâ€™s motion states, which often struggle with ensuring multi-sensor data synchronization. In this paper, we present an efficient UWB-Inertial-odometer localization system, utilizing a non-uniform B-spline framework with fewer anchors. Unlike traditional uniform B-spline-based continuous-time methods, we introduce an adaptive knot-span adjustment strategy for non-uniform continuous-time trajectory representation. This is accomplished by adjusting control points dynamically based on movement speed. To enable efficient fusion of IMU and odometer data, we propose an improved Extended Kalman Filter (EKF) with innovation-based adaptive estimation to provide short-term accurate motion prior. Furthermore, to address the challenge of achieving a fully observable UWB localization system under few-anchor conditions, the Virtual Anchor (VA) generation method based on multiple hypotheses is proposed. At the backend, we propose a CT-UIO factor graph with an adaptive sliding window for global trajectory estimation. Comprehensive experiments conducted on corridor and exhibition hall datasets validate the proposed systemâ€™s high precision and robust performance. The codebase and datasets of this work will be open-sourced at <a target="_blank" rel="noopener" href="https://github.com/JasonSun623/CT-UIO">https://github.com/JasonSun623/CT-UIO</a>. </p>
<blockquote>
<p>åŸºäºè¶…å®½å¸¦ï¼ˆUWBï¼‰çš„é”šç‚¹æ›´å°‘çš„ä½ç½®å®šä½æŠ€æœ¯åœ¨è¿‘å¹´æ¥å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶å…´è¶£ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ½æºå—é™çš„æ¡ä»¶ä¸‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºç¦»æ•£æ—¶é—´è¡¨ç¤ºå’Œå…‰æ»‘å…ˆéªŒæ¥æ¨æ–­æœºå™¨äººçš„è¿åŠ¨çŠ¶æ€ï¼Œè¿™å¸¸å¸¸é¢ä¸´å¤šä¼ æ„Ÿå™¨æ•°æ®åŒæ­¥çš„å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„UWBæƒ¯æ€§é‡Œç¨‹è®¡å®šä½ç³»ç»ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨éå‡åŒ€Bæ ·æ¡æ¡†æ¶å¹¶å‡å°‘é”šç‚¹çš„ä½¿ç”¨ã€‚ä¸åŒäºä¼ ç»Ÿçš„åŸºäºå‡åŒ€Bæ ·æ¡çš„è¿ç»­æ—¶é—´æ–¹æ³•ï¼Œæˆ‘ä»¬ä¸ºä¸å‡åŒ€è¿ç»­æ—¶é—´è½¨è¿¹è¡¨ç¤ºå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”èŠ‚ç‚¹è·¨åº¦è°ƒæ•´ç­–ç•¥ã€‚è¿™æ˜¯é€šè¿‡æ ¹æ®ç§»åŠ¨é€Ÿåº¦åŠ¨æ€è°ƒæ•´æ§åˆ¶ç‚¹æ¥å®ç°çš„ã€‚ä¸ºäº†å®ç°IMUå’Œé‡Œç¨‹è®¡æ•°æ®çš„æœ‰æ•ˆèåˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›çš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ï¼ˆEKFï¼‰çš„åˆ›æ–°è‡ªé€‚åº”ä¼°è®¡æ–¹æ³•ï¼Œä»¥æä¾›çŸ­æœŸç²¾ç¡®è¿åŠ¨å…ˆéªŒã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³åœ¨å°‘é‡é”šç‚¹æ¡ä»¶ä¸‹å®ç°å®Œå…¨å¯è§‚æµ‹çš„UWBå®šä½ç³»ç»Ÿçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤šç§å‡è®¾çš„è™šæ‹Ÿé”šç‚¹ï¼ˆVAï¼‰ç”Ÿæˆæ–¹æ³•ã€‚åœ¨åç«¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰è‡ªé€‚åº”æ»‘åŠ¨çª—å£çš„CT-UIOå› å­å›¾ç”¨äºå…¨å±€è½¨è¿¹ä¼°è®¡ã€‚åœ¨èµ°å»Šå’Œå±•å…æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒéªŒè¯äº†æ‰€æå‡ºç³»ç»Ÿçš„é«˜ç²¾åº¦å’Œç¨³å¥æ€§èƒ½ã€‚è¯¥å·¥ä½œçš„ä»£ç åº“å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JasonSun623/CT-UIO">https://github.com/JasonSun623/CT-UIO</a>ä¸Šå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06287v1">PDF</a> The codebase and datasets will be open-sourced at   <a target="_blank" rel="noopener" href="https://github.com/JasonSun623/CT-UIO">https://github.com/JasonSun623/CT-UIO</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¶…å®½å¸¦ï¼ˆUWBï¼‰çš„é”šç‚¹è¾ƒå°‘çš„å®šä½æ–¹æ³•è¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ½æºå—é™æ¡ä»¶ä¸‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„UWB-æƒ¯æ€§-é‡Œç¨‹è®¡å®šä½ç³»ç»Ÿï¼Œé‡‡ç”¨éå‡åŒ€Bæ ·æ¡æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”ç»“è·¨è°ƒæ•´ç­–ç•¥å®ç°è¿ç»­æ—¶é—´è½¨è¿¹çš„éå‡åŒ€è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä¸ºäº†èåˆIMUå’Œé‡Œç¨‹è®¡æ•°æ®ï¼Œæ”¹è¿›äº†æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ï¼ˆEKFï¼‰ï¼Œå¹¶æä¾›çŸ­æœŸè¿åŠ¨å…ˆéªŒã€‚åŒæ—¶ï¼Œä¸ºè§£å†³å°‘é”šç‚¹æ¡ä»¶ä¸‹UWBå®šä½ç³»ç»Ÿçš„å…¨è§‚æµ‹æŒ‘æˆ˜ï¼Œæå‡ºåŸºäºå¤šé‡å‡è®¾çš„è™šæ‹Ÿé”šç‚¹ç”Ÿæˆæ–¹æ³•ã€‚åç«¯é‡‡ç”¨å¸¦æœ‰è‡ªé€‚åº”æ»‘åŠ¨çª—å£çš„CT-UIOå› å­å›¾è¿›è¡Œå…¨å±€è½¨è¿¹ä¼°è®¡ã€‚å®éªŒéªŒè¯ï¼Œè¯¥ç³»ç»Ÿåœ¨èµ°å»Šå’Œå±•å…æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé«˜ç²¾åº¦å’Œç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥ç ”ç©¶å…³æ³¨åŸºäºè¶…å®½å¸¦ï¼ˆUWBï¼‰çš„é”šç‚¹è¾ƒå°‘çš„å®šä½æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ½æºå—é™æ¡ä»¶ä¸‹çš„åº”ç”¨ã€‚</li>
<li>é‡‡ç”¨éå‡åŒ€Bæ ·æ¡æ¡†æ¶å®ç°è¿ç»­æ—¶é—´è½¨è¿¹çš„éå‡åŒ€è¡¨ç¤ºï¼Œæé«˜äº†å®šä½ç²¾åº¦ã€‚</li>
<li>é€šè¿‡æ”¹è¿›æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ï¼ˆEKFï¼‰å®ç°IMUå’Œé‡Œç¨‹è®¡æ•°æ®çš„èåˆï¼Œæä¾›çŸ­æœŸè¿åŠ¨å…ˆéªŒã€‚</li>
<li>æå‡ºåŸºäºå¤šé‡å‡è®¾çš„è™šæ‹Ÿé”šç‚¹ç”Ÿæˆæ–¹æ³•ï¼Œè§£å†³äº†å°‘é”šç‚¹æ¡ä»¶ä¸‹UWBå®šä½ç³»ç»Ÿçš„å…¨è§‚æµ‹æŒ‘æˆ˜ã€‚</li>
<li>åç«¯é‡‡ç”¨å¸¦æœ‰è‡ªé€‚åº”æ»‘åŠ¨çª—å£çš„CT-UIOå› å­å›¾è¿›è¡Œå…¨å±€è½¨è¿¹ä¼°è®¡ï¼Œæé«˜äº†ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œç²¾åº¦ã€‚</li>
<li>å®éªŒéªŒè¯ï¼Œè¯¥ç³»ç»Ÿåœ¨èµ°å»Šå’Œå±•å…æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06287v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06287v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06287v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06287v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Data-Efficient-Pan-Tumor-Foundation-Model-for-Oncology-CT-Interpretation"><a href="#A-Data-Efficient-Pan-Tumor-Foundation-Model-for-Oncology-CT-Interpretation" class="headerlink" title="A Data-Efficient Pan-Tumor Foundation Model for Oncology CT   Interpretation"></a>A Data-Efficient Pan-Tumor Foundation Model for Oncology CT   Interpretation</h2><p><strong>Authors:Wenhui Lei, Hanyu Chen, Zitian Zhang, Luyang Luo, Qiong Xiao, Yannian Gu, Peng Gao, Yankai Jiang, Ci Wang, Guangtao Wu, Tongjia Xu, Yingjie Zhang, Xiaofan Zhang, Pranav Rajpurkar, Shaoting Zhang, Zhenning Wang</strong></p>
<p>Artificial intelligence-assisted imaging analysis has made substantial strides in tumor diagnosis and management. Here we present PASTA, a pan-tumor CT foundation model that achieves state-of-the-art performance on 45 of 46 representative oncology tasks â€“ including lesion segmentation, tumor detection in plain CT, tumor staging, survival prediction, structured report generation, and cross-modality transfer learning, significantly outperforming the second-best models on 35 tasks. This remarkable advancement is driven by our development of PASTA-Gen, an innovative synthetic tumor generation framework that produces a comprehensive dataset of 30,000 CT scans with pixel-level annotated lesions and paired structured reports, encompassing malignancies across ten organs and five benign lesion types. By leveraging this rich, high-quality synthetic data, we overcome a longstanding bottleneck in the development of CT foundation models â€“ specifically, the scarcity of publicly available, high-quality annotated datasets due to privacy constraints and the substantial labor required for scaling precise data annotation. Encouragingly, PASTA demonstrates exceptional data efficiency with promising practical value, markedly improving performance on various tasks with only a small amount of real-world data. The open release of both the synthetic dataset and PASTA foundation model effectively addresses the challenge of data scarcity, thereby advancing oncological research and clinical translation. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½è¾…åŠ©æˆåƒåˆ†æåœ¨è‚¿ç˜¤è¯Šæ–­å’Œæ²»ç–—æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PANCTAæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å…¨é¢çš„è‚¿ç˜¤CTåŸºç¡€æ¨¡å‹ï¼Œåœ¨ä»£è¡¨è‚¿ç˜¤çš„ä¼—å¤šä»»åŠ¡ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå…¶ä¸­åŒ…æ‹¬ç—…å˜åˆ†å‰²ã€æ™®é€šCTä¸­çš„è‚¿ç˜¤æ£€æµ‹ã€è‚¿ç˜¤åˆ†æœŸã€ç”Ÿå­˜é¢„æµ‹ã€ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆå’Œè·¨æ¨¡æ€è¿ç§»å­¦ä¹ ç­‰ä»»åŠ¡ä¸­çš„45é¡¹ä»»åŠ¡ä¸­çš„å‰4é¡¹ä»»åŠ¡ã€‚åœ¨å…¶ä¸­çš„35é¡¹ä»»åŠ¡ä¸­ï¼Œå®ƒæ˜¾è‘—ä¼˜äºæ’åç¬¬äºŒçš„æœ€ä½³æ¨¡å‹ã€‚è¿™ä¸€ä»¤äººç©ç›®çš„è¿›æ­¥å¾—ç›Šäºæˆ‘ä»¬å¼€å‘çš„PANCTA-Genè¿™ä¸€åˆ›æ–°æ€§çš„åˆæˆè‚¿ç˜¤ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡ä¸‰ä¸‡ä»½CTæ‰«æçš„ç»¼åˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åƒç´ çº§æ³¨é‡Šçš„ç—…å˜å’Œé…å¯¹ç»“æ„åŒ–æŠ¥å‘Šï¼Œæ¶µç›–äº†åä¸ªå™¨å®˜çš„æ¶æ€§è‚¿ç˜¤å’Œäº”ç§è‰¯æ€§ç—…å˜ç±»å‹ã€‚é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„é«˜è´¨é‡åˆæˆæ•°æ®ï¼Œæˆ‘ä»¬å…‹æœäº†é•¿æœŸä»¥æ¥é˜»ç¢CTåŸºç¡€æ¨¡å‹å‘å±•çš„ç“¶é¢ˆé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºéšç§çº¦æŸå’Œå¤§è§„æ¨¡ç²¾ç¡®æ•°æ®æ ‡æ³¨æ‰€éœ€çš„å¤§é‡åŠ³åŠ¨åŠ›å¯¼è‡´çš„å…¬å¼€å¯ç”¨é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ã€‚ä»¤äººé¼“èˆçš„æ˜¯ï¼ŒPANCTAåœ¨å®ç”¨æ–¹é¢è¡¨ç°å‡ºä»¤äººæŒ¯å¥‹çš„æ•°æ®æ•ˆç‡ä¼˜åŠ¿ï¼Œåœ¨ä»…æœ‰å°‘é‡çœŸå®ä¸–ç•Œæ•°æ®çš„æƒ…å†µä¸‹ä¾¿å¯ä»¥åœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜å…¶æ€§èƒ½è¡¨ç°ã€‚åˆæˆæ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹çš„æˆåŠŸå¼€æºæœ‰åŠ©äºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿›è€Œæ¨åŠ¨è‚¿ç˜¤å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06171v1">PDF</a> 57 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½è¾…åŠ©æˆåƒåˆ†æåœ¨è‚¿ç˜¤è¯Šæ–­å’Œæ²»ç–—ä¸­å–å¾—äº†é‡å¤§è¿›å±•ã€‚æœ¬æ–‡æå‡ºäº†PASTAæ³›è‚¿ç˜¤CTåŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä»£è¡¨è‚¿ç˜¤çš„å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬ç—…ç¶åˆ†å‰²ã€æ™®é€šCTä¸­çš„è‚¿ç˜¤æ£€æµ‹ã€è‚¿ç˜¤åˆ†æœŸã€ç”Ÿå­˜é¢„æµ‹ã€ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆå’Œè·¨æ¨¡æ€è¿ç§»å­¦ä¹ ç­‰ã€‚è¿™ä¸€æ˜¾è‘—è¿›å±•å¾—ç›ŠäºPASTA-Gençš„åˆæˆè‚¿ç˜¤ç”Ÿæˆæ¡†æ¶çš„å¼€å‘ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†åŒ…å«åä¸‡ä¸ªCTæ‰«æå›¾åƒçš„å…¨é¢æ•°æ®é›†ï¼Œå…·æœ‰åƒç´ çº§æ³¨é‡Šçš„ç—…ç¶å’Œé…å¯¹çš„ç»“æ„åŒ–æŠ¥å‘Šï¼Œæ¶µç›–äº†åå¤§å™¨å®˜çš„æ¶æ€§è‚¿ç˜¤å’Œäº”ç§è‰¯æ€§ç—…ç¶ç±»å‹ã€‚é€šè¿‡åˆ©ç”¨è¿™äº›ä¸°å¯Œçš„é«˜è´¨é‡åˆæˆæ•°æ®ï¼Œæˆ‘ä»¬å…‹æœäº†CTåŸºç¡€æ¨¡å‹å‘å±•ä¸­çš„é•¿æœŸç“¶é¢ˆã€‚å…¶ä¸­å­˜åœ¨çš„å…¬å…±é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜å› éšç§çº¦æŸå’Œå¤§è§„æ¨¡ç²¾ç¡®æ•°æ®æ ‡æ³¨æ‰€éœ€çš„å¤§é‡åŠ³åŠ¨è€ŒåŠ å‰§ã€‚ä»¤äººé¼“èˆçš„æ˜¯ï¼ŒPASTAåœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚æœ¬æ–‡æ‰€å¼€æ”¾åˆæˆçš„æ•°æ®é›†å’ŒPASTAåŸºç¡€æ¨¡å‹çš„æ¨å‡ºæœ‰æ•ˆè§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸ºè‚¿ç˜¤å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ç¿»è¯‘å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIè¾…åŠ©æˆåƒåˆ†æåœ¨è‚¿ç˜¤è¯Šæ–­æ²»ç–—ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>PASTAæ¨¡å‹åœ¨å¤šç§è‚¿ç˜¤ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PASTA-Genåˆæˆè‚¿ç˜¤ç”Ÿæˆæ¡†æ¶çš„å¼€å‘ä¸ºè‚¿ç˜¤å½±åƒåˆ†ææä¾›äº†å¤§é‡é«˜è´¨é‡æ•°æ®ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨åˆæˆæ•°æ®ï¼Œè§£å†³äº†CTåŸºç¡€æ¨¡å‹å‘å±•ä¸­å…¬å¼€å¯ç”¨é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>PASTAåœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚</li>
<li>å¼€æ”¾åˆæˆæ•°æ®é›†å’ŒPASTAåŸºç¡€æ¨¡å‹çš„æ¨å‡ºè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06171v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06171v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-coupled-planar-transmit-RF-array-for-ultrahigh-field-spine-MR-imaging"><a href="#A-coupled-planar-transmit-RF-array-for-ultrahigh-field-spine-MR-imaging" class="headerlink" title="A coupled planar transmit RF array for ultrahigh field spine MR imaging"></a>A coupled planar transmit RF array for ultrahigh field spine MR imaging</h2><p><strong>Authors:Yunkun Zhao, Komlan Payne, Leslie Ying, Xiaoliang Zhang</strong></p>
<p>Ultrahigh-field MRI, such as those operating at 7 Tesla, enhances diagnostic capabilities but also presents unique challenges, including the need for advanced RF coil designs to achieve an optimal signal-to-noise ratio and transmit efficiency, particularly when imaging large samples. In this work, we introduce the coupled planar array, a novel technique for high-frequency, large-size RF coil design with enhanced the RF magnetic field (B1) efficiency and transmit performance for ultrahigh-field spine imaging applications. This array comprises multiple resonators that are electromagnetically coupled to function as a single multimodal resonator. The field distribution of its highest frequency mode is suitable for spine imaging applications. Based on the numerical modeling and calculation, a prototype of the coupled planar array was constructed and its performance was evaluated through comprehensive numerical simulations, rigorous RF measurements, empirical tests, and a comparison against a conventional surface coil with the same size and geometry. The results of this study demonstrate that the proposed coupled planar array exhibits superior performance compared to conventional surface coils in terms of B1 efficiency for both transmit (B1+) and receive (B1-) fields, specific absorption rate (SAR), and the ability to operate at high frequencies. This study suggests a promising and efficient approach to the design of high-frequency, large-size RF coils for spine MR imaging at ultrahigh magnetic fields. </p>
<blockquote>
<p>è¶…é«˜åœºMRIï¼Œå¦‚7ç‰¹æ–¯æ‹‰è¿è¡Œçš„MRIï¼Œè™½ç„¶å¢å¼ºäº†è¯Šæ–­èƒ½åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦å…ˆè¿›çš„å°„é¢‘çº¿åœˆè®¾è®¡ï¼Œä»¥å®ç°æœ€ä½³çš„ä¿¡å™ªæ¯”å’Œä¼ è¾“æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æˆåƒå¤§æ ·æœ¬æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è€¦åˆå¹³é¢é˜µåˆ—ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé«˜é¢‘å¤§å°ºå¯¸å°„é¢‘çº¿åœˆè®¾è®¡çš„æ–°æŠ€æœ¯ï¼Œæé«˜äº†å°„é¢‘ç£åœºï¼ˆB1ï¼‰æ•ˆç‡å’Œä¼ è¾“æ€§èƒ½ï¼Œé€‚ç”¨äºè¶…é«˜åœºè„ŠæŸ±æˆåƒåº”ç”¨ã€‚è¯¥é˜µåˆ—ç”±å¤šä¸ªè°æŒ¯å™¨ç»„æˆï¼Œé€šè¿‡ç”µç£è€¦åˆåŠŸèƒ½ä½œä¸ºå•ä¸ªå¤šæ¨¡å¼è°æŒ¯å™¨ã€‚å…¶æœ€é«˜é¢‘ç‡æ¨¡å¼çš„åœºåˆ†å¸ƒé€‚ç”¨äºè„ŠæŸ±æˆåƒåº”ç”¨ã€‚åŸºäºæ•°å€¼å»ºæ¨¡å’Œè®¡ç®—ï¼Œæ„å»ºäº†è€¦åˆå¹³é¢é˜µåˆ—çš„åŸå‹ï¼Œå¹¶é€šè¿‡å…¨é¢çš„æ•°å€¼æ¨¡æ‹Ÿã€ä¸¥æ ¼çš„å°„é¢‘æµ‹é‡ã€å®è¯æµ‹è¯•ä»¥åŠä¸ç›¸åŒå°ºå¯¸å’Œå‡ ä½•å½¢çŠ¶çš„å¸¸è§„è¡¨é¢çº¿åœˆçš„æ¯”è¾ƒï¼Œå¯¹å…¶æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„è¡¨é¢çº¿åœˆç›¸æ¯”ï¼Œæ‰€æå‡ºçš„è€¦åˆå¹³é¢é˜µåˆ—åœ¨B1æ•ˆç‡å’Œä¼ è¾“ï¼ˆB1+ï¼‰å’Œæ¥æ”¶ï¼ˆB-ï¼‰åœºæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå…·æœ‰ç‰¹å®šçš„å¸æ”¶ç‡ï¼ˆSARï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨é«˜é¢‘ä¸‹è¿è¡Œã€‚æœ¬ç ”ç©¶ä¸ºé«˜é¢‘å¤§å°ºå¯¸å°„é¢‘çº¿åœˆçš„è®¾è®¡æä¾›äº†ä¸€ç§æœ‰å‰é€”ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ç”¨äºè¶…é«˜ç£åœºä¸‹çš„è„ŠæŸ±MRæˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06041v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é«˜é¢‘å¤§å°ºå¯¸å°„é¢‘çº¿åœˆè®¾è®¡â€”â€”è€¦åˆå¹³é¢é˜µåˆ—ï¼Œè¯¥è®¾è®¡æé«˜äº†å°„é¢‘ç£åœºï¼ˆB1ï¼‰æ•ˆç‡å’Œä¼ è¾“æ€§èƒ½ï¼Œé€‚ç”¨äºè¶…é«˜åœºè„ŠæŸ±æˆåƒåº”ç”¨ã€‚é€šè¿‡æ•°å€¼å»ºæ¨¡å’Œè®¡ç®—ï¼Œæ„å»ºäº†è¯¥é˜µåˆ—çš„åŸå‹ï¼Œå¹¶é€šè¿‡ç»¼åˆæ•°å€¼æ¨¡æ‹Ÿã€ä¸¥è°¨çš„å°„é¢‘æµ‹é‡ã€ç»éªŒæµ‹è¯•ä»¥åŠä¸å¸¸è§„è¡¨é¢çº¿åœˆçš„å¯¹æ¯”ï¼Œè¯„ä¼°äº†å…¶æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿè¡¨é¢çº¿åœˆç›¸æ¯”ï¼Œè€¦åˆå¹³é¢é˜µåˆ—åœ¨B1æ•ˆç‡å’Œç‰¹å®šå¸æ”¶ç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨é«˜é¢‘ä¸‹è¿è¡Œã€‚è¿™ä¸ºé«˜é¢‘å¤§å°ºå¯¸å°„é¢‘çº¿åœˆçš„è®¾è®¡æä¾›äº†æœ‰å‰æ™¯ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€‚ç”¨äºè¶…é«˜ç£åœºä¸‹çš„è„ŠæŸ±MRæˆåƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç”¨äºè¶…é«˜åœºMRIï¼ˆå¦‚7ç‰¹æ–¯æ‹‰ï¼‰çš„æ–°å‹å°„é¢‘çº¿åœˆè®¾è®¡â€”â€”è€¦åˆå¹³é¢é˜µåˆ—ã€‚</li>
<li>è€¦åˆå¹³é¢é˜µåˆ—ç”±å¤šä¸ªè°æŒ¯å™¨ç»„æˆï¼Œé€šè¿‡ç”µç£è€¦åˆåŠŸèƒ½ä½œä¸ºå•ä¸€çš„å¤šæ¨¡å¼è°æŒ¯å™¨ã€‚</li>
<li>è¯¥é˜µåˆ—çš„æœ€é«˜é¢‘ç‡æ¨¡å¼çš„åœºåˆ†å¸ƒé€‚åˆè„ŠæŸ±æˆåƒåº”ç”¨ã€‚</li>
<li>é€šè¿‡æ•°å€¼å»ºæ¨¡å’Œè®¡ç®—ï¼Œæ„å»ºäº†è€¦åˆå¹³é¢é˜µåˆ—çš„åŸå‹ã€‚</li>
<li>ä¸å¸¸è§„è¡¨é¢çº¿åœˆç›¸æ¯”ï¼Œè€¦åˆå¹³é¢é˜µåˆ—åœ¨B1æ•ˆç‡å’Œç‰¹å®šå¸æ”¶ç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è€¦åˆå¹³é¢é˜µåˆ—èƒ½åœ¨é«˜é¢‘ä¸‹è¿è¡Œï¼Œå…·æœ‰æ›´å¥½çš„ä¼ è¾“å’Œæ¥æ”¶æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.06041v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ClinKD-Cross-Modal-Clinic-Knowledge-Distiller-For-Multi-Task-Medical-Images"><a href="#ClinKD-Cross-Modal-Clinic-Knowledge-Distiller-For-Multi-Task-Medical-Images" class="headerlink" title="ClinKD: Cross-Modal Clinic Knowledge Distiller For Multi-Task Medical   Images"></a>ClinKD: Cross-Modal Clinic Knowledge Distiller For Multi-Task Medical   Images</h2><p><strong>Authors:Hongyu Ge, Longkun Hao, Zihui Xu, Zhenxin Lin, Bin Li, Shoujun Zhou, Hongjin Zhao, Yihang Liu</strong></p>
<p>Med-VQA (Medical Visual Question Answering) is a crucial subtask within the broader VQA (Visual Question Answering) domain. This task requires a visual question answering system to analyze the provided image and corresponding question,offering reasonable analysis and suggestions to assist medical professionals in making pathological diagnoses, or ideally, enabling the system to independently provide correct diagnoses. Furthermore, more advanced Med-VQA tasks involve Referring and Grounding, which not only require the system to accurately comprehend medical images but also to pinpoint specific biological locations within those images. While many large pre-trained models have demonstrated substantial VQA capabilities,challenges persist in the medical imaging domain. The intricacy of biological features in medical images and the scarcity of high-quality medical image datasets, combined with the fact that current models are not tailored for the medical field in terms of architecture and training paradigms, hinder the full exploitation of model generalization. This results in issues such as hallucination in Visual Grounding. In this paper, we introduce the ClinKD model, which incorporates modifications to model position encoding and a diversified training process. Initially, we enhance the modelâ€™s ability to perceive image and modality variations by using Med-CLIP Guided Rotary Position Embedding. Subsequently, we leverage distillation to provide prior knowledge to the model before using complete training data. Additionally, the feedback-based training process during the formal training phase further enhances data utilization. Notably, under unchanged evaluation protocols, we achieve a new state-of-the-art performance on the Med-GRIT-270k dataset, and the Med-CLIP Guided Rotary Position Embedding approach presents potential for generalizing to universal model position encoding. </p>
<blockquote>
<p>åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰æ˜¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦å­ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡è¦æ±‚è§†è§‰é—®ç­”ç³»ç»Ÿåˆ†ææä¾›çš„å›¾åƒå’Œç›¸åº”çš„é—®é¢˜ï¼Œæä¾›åˆç†çš„åˆ†æå’Œå»ºè®®ï¼Œä»¥ååŠ©åŒ»ç–—ä¸“ä¸šäººå£«è¿›è¡Œç—…ç†è¯Šæ–­ï¼Œæˆ–è€…ç†æƒ³æƒ…å†µä¸‹ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿç‹¬ç«‹æä¾›æ­£ç¡®çš„è¯Šæ–­ã€‚æ­¤å¤–ï¼Œæ›´é«˜çº§çš„Med-VQAä»»åŠ¡æ¶‰åŠå¼•ç”¨å’Œæ¥åœ°ï¼Œè¿™è¦æ±‚ç³»ç»Ÿä¸ä»…å‡†ç¡®ç†è§£åŒ»å­¦å›¾åƒï¼Œè¿˜èƒ½æŒ‡å‡ºå›¾åƒä¸­ç‰¹å®šçš„ç”Ÿç‰©ä½ç½®ã€‚è™½ç„¶è®¸å¤šå¤§å‹é¢„è®­ç»ƒæ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†ç›¸å½“çš„VQAèƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦æˆåƒé¢†åŸŸä»å­˜åœ¨æŒ‘æˆ˜ã€‚åŒ»å­¦å›¾åƒä¸­ç”Ÿç‰©ç‰¹å¾çš„å¤æ‚æ€§ä»¥åŠé«˜è´¨é‡åŒ»å­¦å›¾åƒæ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œå†åŠ ä¸Šå½“å‰æ¨¡å‹åœ¨ç»“æ„å’Œè®­ç»ƒèŒƒå¼ä¸Šå¹¶ä¸ä¸“é—¨é’ˆå¯¹åŒ»å­¦é¢†åŸŸï¼Œé˜»ç¢äº†æ¨¡å‹çš„å…¨é¢æ³›åŒ–ã€‚è¿™å¯¼è‡´è§†è§‰æ¥åœ°ä¸­çš„å¹»è§‰ç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ClinKDæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹æ¨¡å‹ä½ç½®ç¼–ç è¿›è¡Œäº†ä¿®æ”¹ï¼Œå¹¶é‡‡ç”¨äº†å¤šæ ·åŒ–çš„è®­ç»ƒè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨Med-CLIPå¼•å¯¼æ—‹è½¬ä½ç½®åµŒå…¥æ³•ï¼Œæé«˜äº†æ¨¡å‹å¯¹å›¾åƒå’Œæ¨¡æ€å˜åŒ–çš„ç†è§£èƒ½åŠ›ã€‚éšåï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨å®Œæ•´è®­ç»ƒæ•°æ®ä¹‹å‰ï¼Œåˆ©ç”¨è’¸é¦æ³•ä¸ºæ¨¡å‹æä¾›å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæ­£å¼è®­ç»ƒé˜¶æ®µçš„åŸºäºåé¦ˆçš„è®­ç»ƒè¿‡ç¨‹è¿›ä¸€æ­¥æé«˜äº†æ•°æ®åˆ©ç”¨ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¸å˜çš„è¯„ä¼°åè®®ä¸‹ï¼Œæˆ‘ä»¬åœ¨Med-GRIT-270kæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°çš„æ€§èƒ½ï¼Œå¹¶ä¸”Med-CLIPå¼•å¯¼æ—‹è½¬ä½ç½®åµŒå…¥æ³•ä¸ºé€šç”¨æ¨¡å‹ä½ç½®ç¼–ç æä¾›äº†æ½œåœ¨çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05928v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰æ˜¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦å­ä»»åŠ¡ã€‚å®ƒè¦æ±‚è§†è§‰é—®ç­”ç³»ç»Ÿåˆ†æå›¾åƒå’Œç›¸åº”é—®é¢˜ï¼Œæä¾›åˆç†çš„åˆ†æå’Œå»ºè®®ï¼ŒååŠ©åŒ»ç–—ä¸“ä¸šäººå£«è¿›è¡Œç—…ç†è¯Šæ–­ï¼Œæˆ–ä½¿ç³»ç»Ÿèƒ½å¤Ÿç‹¬ç«‹æä¾›æ­£ç¡®çš„è¯Šæ–­ã€‚æœ¬æ–‡ä»‹ç»äº†ClinKDæ¨¡å‹ï¼Œé€šè¿‡æ”¹è¿›æ¨¡å‹ä½ç½®ç¼–ç å’Œå¤šæ ·åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„è§†è§‰é—®ç­”æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-VQAæ˜¯VQAé¢†åŸŸä¸­çš„é‡è¦å­ä»»åŠ¡ï¼Œè¦æ±‚ç³»ç»Ÿåˆ†æåŒ»å­¦å›¾åƒå’Œç›¸åº”é—®é¢˜ï¼Œæä¾›è¯Šæ–­è¾…åŠ©ã€‚</li>
<li>å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬åŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ã€é«˜è´¨é‡æ•°æ®é›†ç¨€ç¼ºä»¥åŠæ¨¡å‹æ¶æ„å’Œè®­ç»ƒèŒƒå¼çš„ä¸é’ˆå¯¹æ€§ã€‚</li>
<li>ClinKDæ¨¡å‹é€šè¿‡æ”¹è¿›æ¨¡å‹ä½ç½®ç¼–ç å’Œå¤šæ ·åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>Med-CLIP Guided Rotary Position Embeddingæ–¹æ³•æé«˜äº†æ¨¡å‹å¯¹å›¾åƒå’Œæ¨¡æ€å˜åŒ–çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>è’¸é¦æ³•ç”¨äºå‘æ¨¡å‹æä¾›å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡ã€‚</li>
<li>åœ¨Med-GRIT-270kæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05928v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-pressure-structural-and-lattice-dynamics-study-of-Î±-In-2-Se-3"><a href="#High-pressure-structural-and-lattice-dynamics-study-of-Î±-In-2-Se-3" class="headerlink" title="High pressure structural and lattice dynamics study of   Î±-In$_2$Se$_3$"></a>High pressure structural and lattice dynamics study of   Î±-In$_2$Se$_3$</h2><p><strong>Authors:Shiyu Feng, Baihong Sun, Wenting Lu, Haikai Zou, Chenxin Wei, Qian Zhang, Bihan Wang, Martin Kunz, Hirokazu Kadobayashi, Azkar Saeed Ahmad, Elad Koren, Elissaios Stavrou</strong></p>
<p>Layered $\alpha$-In$_2$Se$_3$has been studied using a concomitant in-situ synchrotron angle dispersive powder x-ray diffraction and Raman spectroscopy study in a diamond anvil cell up to 60+ GPa, at room temperature. Helium, that remains fairly hydrostatic up to the highest pressure in this study, was used as the pressure-transmitting medium. The results from both experimental methods reveal a pressure-induced structural phase transition from $\alpha$-In$_2$Se$_3$ to a monoclinic $\beta$â€™-In2Se3 structure at $\approx$1 GPa, in agreement with previous studies. Based on our detailed measurements using both experimental techniques and F-f formalism, the $\beta$â€™-In$_2$Se$_3$ structure remains stable up to 45 GPa, without a clear indication of a phase transition towards the previously reported $\beta$-In2Se3 phase. Above this pressure, In$_2$Se$_3$ adopts a disordered solid-solution-like orthorhombic structure, phase IV. The results are discussed in comparison with the relevant previous studies of $\alpha$-In$_2$Se$_3$ under pressure. </p>
<blockquote>
<p>é‡‡ç”¨åŒæ­¥åŠ é€Ÿå™¨è§’åº¦å‘æ•£ç²‰æœ«Xå°„çº¿è¡å°„å’Œæ‹‰æ›¼å…‰è°±è”åˆåŸä½ç ”ç©¶æ³•ï¼Œåœ¨é‡‘åˆšçŸ³å‹ç §é«˜å‹å®éªŒè£…ç½®ä¸­å¯¹å±‚çŠ¶Î±-Inâ‚‚Seâ‚ƒè¿›è¡Œäº†é«˜å‹ç ”ç©¶ï¼Œæœ€é«˜å‹åŠ›è¾¾åˆ°60+ GPaï¼Œç¯å¢ƒæ¸©åº¦ä¸ºå®¤æ¸©ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæœ€é«˜å‹åŠ›ä¸‹çš„æ°¦æ°”ä»ä¿æŒç›¸å½“çš„æ°´é™å‹åŠ›ï¼Œè¢«ç”¨ä½œå‹åŠ›ä¼ é€’ä»‹è´¨ã€‚ä¸¤ç§å®éªŒæ–¹æ³•çš„ç»“æœå‡è¡¨æ˜ï¼ŒÎ±-Inâ‚‚Seâ‚ƒåœ¨çº¦1 GPaå‹åŠ›ä¸‹å‘ç”Ÿç»“æ„ç›¸å˜ï¼Œè½¬å˜ä¸ºå•æ–œÎ²â€™-Inâ‚‚Seâ‚ƒç»“æ„ï¼Œä¸å‰äººçš„ç ”ç©¶ç»“æœä¸€è‡´ã€‚åŸºäºæˆ‘ä»¬åˆ©ç”¨è¿™ä¸¤ç§å®éªŒæŠ€æœ¯å’ŒF-få½¢å¼ä¸»ä¹‰è¿›è¡Œçš„è¯¦ç»†æµ‹é‡ï¼ŒÎ²â€™-Inâ‚‚Seâ‚ƒç»“æ„åœ¨é«˜è¾¾45 GPaæ—¶ä¿æŒç¨³å®šï¼Œæ²¡æœ‰æ˜æ˜¾çš„ç›¸è½¬å˜ä¸ºå…ˆå‰æŠ¥é“çš„Î²-Inâ‚‚Seâ‚ƒç›¸çš„è¿¹è±¡ã€‚åœ¨æ­¤å‹åŠ›ä¹‹ä¸Šï¼ŒInâ‚‚Seâ‚ƒå‘ˆç°æ— åºçš„ç±»ä¼¼å›ºæ€æº¶æ¶²çš„æ­£äº¤ç»“æ„ï¼Œå³ç¬¬IVç›¸ã€‚ç»“æœä¸å‰äººå…³äºÎ±-Inâ‚‚Seâ‚ƒå—å‹çš„ç›¸å…³ç ”ç©¶è¿›è¡Œäº†æ¯”è¾ƒå’Œè®¨è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05927v1">PDF</a> 16 Pages, 6 figures</p>
<p><strong>Summary</strong><br>     Î±-Inâ‚‚Seâ‚ƒåœ¨é‡‘åˆšçŸ³å‹ç §ä¸‹è¿›è¡ŒåŒæ­¥è¾å°„è§’åº¦åˆ†æ•£ç²‰æœ«Xå°„çº¿è¡å°„å’Œæ‹‰æ›¼å…‰è°±ç ”ç©¶ï¼Œæ˜¾ç¤ºå…¶èƒ½æ‰¿å—é«˜è¾¾60 GPaä»¥ä¸Šçš„å‹åŠ›ã€‚åœ¨å®¤æ¸©ä¸‹ï¼Œä½¿ç”¨æ°¦æ°”ä½œä¸ºå‹åŠ›ä¼ é€’ä»‹è´¨ï¼Œè§‚å¯Ÿåˆ°å‹åŠ›è¯±å¯¼çš„ç»“æ„ç›¸å˜ä»Î±-Inâ‚‚Seâ‚ƒè½¬å˜ä¸ºå•æ–œÎ²â€™-Inâ‚‚Seâ‚ƒç»“æ„ï¼Œå¹¶åœ¨çº¦45 GPaå‹åŠ›ä¸‹ä¿æŒç¨³å®šï¼Œæ— æ˜æ˜¾çš„ç›¸å˜è¿¹è±¡ã€‚è¶…è¿‡æ­¤å‹åŠ›ï¼ŒInâ‚‚Seâ‚ƒå‘ˆç°æ— åºçš„å›ºæº¶ä½“çŠ¶æ­£äº¤ç»“æ„ã€‚ä¸ä¹‹å‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œè¿™äº›ç»“æœæä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Î±-Inâ‚‚Seâ‚ƒèƒ½æ‰¿å—é«˜è¾¾60 GPaä»¥ä¸Šçš„å‹åŠ›ã€‚</li>
<li>åœ¨å®¤æ¸©ä¸‹ï¼Œé€šè¿‡åŒæ­¥è¾å°„è§’åº¦åˆ†æ•£ç²‰æœ«Xå°„çº¿è¡å°„å’Œæ‹‰æ›¼å…‰è°±ç ”ç©¶Î±-Inâ‚‚Seâ‚ƒã€‚</li>
<li>å‹åŠ›è¯±å¯¼çš„ç»“æ„ç›¸å˜ä»Î±-Inâ‚‚Seâ‚ƒè½¬å˜ä¸ºå•æ–œÎ²â€™-Inâ‚‚Seâ‚ƒç»“æ„åœ¨çº¦1 GPaä¸‹å‘ç”Ÿã€‚</li>
<li>Î²â€™-Inâ‚‚Seâ‚ƒç»“æ„åœ¨é«˜è¾¾çº¦45 GPaçš„å‹åŠ›ä¸‹ä¿æŒç¨³å®šã€‚</li>
<li>åœ¨æ›´é«˜çš„å‹åŠ›ä¸‹ï¼ŒInâ‚‚Seâ‚ƒå‘ˆç°å‡ºæ— åºçš„å›ºæº¶ä½“çŠ¶æ­£äº¤ç»“æ„ï¼ˆphase IVï¼‰ã€‚</li>
<li>æ­¤ç ”ç©¶ç»“æœä¸ä¹‹å‰å…³äºÎ±-Inâ‚‚Seâ‚ƒçš„ç ”ç©¶ç›¸æ¯”æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05927v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fast-Omni-Directional-Image-Super-Resolution-Adapting-the-Implicit-Image-Function-with-Pixel-and-Semantic-Wise-Spherical-Geometric-Priors"><a href="#Fast-Omni-Directional-Image-Super-Resolution-Adapting-the-Implicit-Image-Function-with-Pixel-and-Semantic-Wise-Spherical-Geometric-Priors" class="headerlink" title="Fast Omni-Directional Image Super-Resolution: Adapting the Implicit   Image Function with Pixel and Semantic-Wise Spherical Geometric Priors"></a>Fast Omni-Directional Image Super-Resolution: Adapting the Implicit   Image Function with Pixel and Semantic-Wise Spherical Geometric Priors</h2><p><strong>Authors:Xuelin Shen, Yitong Wang, Silin Zheng, Kang Xiao, Wenhan Yang, Xu Wang</strong></p>
<p>In the context of Omni-Directional Image (ODI) Super-Resolution (SR), the unique challenge arises from the non-uniform oversampling characteristics caused by EquiRectangular Projection (ERP). Considerable efforts in designing complex spherical convolutions or polyhedron reprojection offer significant performance improvements but at the expense of cumbersome processing procedures and slower inference speeds. Under these circumstances, this paper proposes a new ODI-SR model characterized by its capacity to perform Fast and Arbitrary-scale ODI-SR processes, denoted as FAOR. The key innovation lies in adapting the implicit image function from the planar image domain to the ERP image domain by incorporating spherical geometric priors at both the latent representation and image reconstruction stages, in a low-overhead manner. Specifically, at the latent representation stage, we adopt a pair of pixel-wise and semantic-wise sphere-to-planar distortion maps to perform affine transformations on the latent representation, thereby incorporating it with spherical properties. Moreover, during the image reconstruction stage, we introduce a geodesic-based resampling strategy, aligning the implicit image function with spherical geometrics without introducing additional parameters. As a result, the proposed FAOR outperforms the state-of-the-art ODI-SR models with a much faster inference speed. Extensive experimental results and ablation studies have demonstrated the effectiveness of our design. </p>
<blockquote>
<p>åœ¨å…¨æ™¯å›¾åƒï¼ˆODIï¼‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œé¢ä¸´çš„æŒ‘æˆ˜æ¥è‡ªäºç­‰çŸ©å½¢æŠ•å½±ï¼ˆERPï¼‰å¼•èµ·çš„éå‡åŒ€è¿‡é‡‡æ ·ç‰¹æ€§ã€‚è®¾è®¡å¤æ‚çš„çƒå½¢å·ç§¯æˆ–å¤šé¢ä½“é‡æŠ•å½±éœ€è¦å¤§é‡çš„åŠªåŠ›ï¼Œè™½ç„¶è¿™æä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œä½†å´å¸¦æ¥äº†ç¹ççš„å¤„ç†ç¨‹åºå’Œè¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ODI-SRæ¨¡å‹ï¼Œå…¶ç‰¹ç‚¹æ˜¯èƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œä»»æ„å°ºåº¦çš„ODI-SRå¤„ç†ï¼Œè¢«ç§°ä¸ºFAORã€‚ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡å°†éšå¼å›¾åƒå‡½æ•°ä»å¹³é¢å›¾åƒåŸŸé€‚åº”åˆ°ERPå›¾åƒåŸŸï¼Œä»¥ä½å¼€é”€çš„æ–¹å¼åœ¨æ½œåœ¨è¡¨ç¤ºå’Œå›¾åƒé‡å»ºé˜¶æ®µèå…¥çƒå½¢å‡ ä½•å…ˆéªŒã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ½œåœ¨è¡¨ç¤ºé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€å¯¹åƒç´ çº§å’Œè¯­ä¹‰çº§çš„çƒé¢åˆ°å¹³é¢å¤±çœŸå›¾ï¼Œå¯¹æ½œåœ¨è¡¨ç¤ºè¿›è¡Œä»¿å°„å˜æ¢ï¼Œä»è€Œå°†å…¶ä¸çƒå½¢å±æ€§ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œåœ¨å›¾åƒé‡å»ºé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæµ‹åœ°çº¿çš„é‡é‡‡æ ·ç­–ç•¥ï¼Œä½¿éšå¼å›¾åƒå‡½æ•°ä¸çƒå½¢å‡ ä½•å¯¹é½ï¼Œè€Œæ— éœ€å¼•å…¥é¢å¤–çš„å‚æ•°ã€‚å› æ­¤ï¼Œæ‰€æå‡ºçš„FAORåœ¨æ¨ç†é€Ÿåº¦ä¸Šå¤§å¤§ä¼˜äºç°æœ‰çš„ODI-SRæ¨¡å‹ã€‚å¤§é‡çš„å®éªŒå’Œæ¶ˆèç ”ç©¶è¯æ˜äº†æˆ‘ä»¬çš„è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05902v1">PDF</a> 9 pages, 4 figures, AAAI 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹Omni-Directional Imageï¼ˆODIï¼‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸­ç”±EquiRectangular Projectionï¼ˆERPï¼‰å¼•èµ·çš„éå‡åŒ€è¿‡é‡‡æ ·ç‰¹æ€§æ‰€å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ODI-SRæ¨¡å‹FAORï¼Œå®ƒèƒ½å¤Ÿåœ¨ä½å¼€é”€çš„æƒ…å†µä¸‹ï¼Œåœ¨æ½œåœ¨è¡¨ç¤ºå’Œå›¾åƒé‡å»ºé˜¶æ®µèå…¥çƒå½¢å‡ ä½•å…ˆéªŒï¼Œå¿«é€Ÿæ‰§è¡Œä»»æ„å°ºåº¦çš„ODI-SRå¤„ç†ã€‚é€šè¿‡é‡‡ç”¨åƒç´ çº§å’Œè¯­ä¹‰çº§çš„çƒåˆ°å¹³é¢å¤±çœŸæ˜ å°„è¿›è¡Œä»¿å°„å˜æ¢ï¼Œä»¥åŠåŸºäºæµ‹åœ°çº¿çš„é‡é‡‡æ ·ç­–ç•¥ï¼ŒFAORæ¨¡å‹åœ¨ä¿æŒé«˜é€Ÿæ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹æœ€å…ˆè¿›ODI-SRæ¨¡å‹çš„æ€§èƒ½è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ODI-SRé¢ä¸´ERPå¼•èµ·çš„éå‡åŒ€è¿‡é‡‡æ ·ç‰¹æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚å¤æ‚çƒå½¢å·ç§¯æˆ–å¤šè¾¹å½¢é‡æ–°æŠ•å½±è™½èƒ½æé«˜æ€§èƒ½ï¼Œä½†å¤„ç†è¿‡ç¨‹ç¹çä¸”æ¨ç†é€Ÿåº¦æ…¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„FAORæ¨¡å‹èƒ½åœ¨æ½œåœ¨è¡¨ç¤ºå’Œå›¾åƒé‡å»ºé˜¶æ®µèå…¥çƒå½¢å‡ ä½•å…ˆéªŒã€‚</li>
<li>FAORæ¨¡å‹é€šè¿‡çƒåˆ°å¹³é¢å¤±çœŸæ˜ å°„è¿›è¡Œä»¿å°„å˜æ¢ï¼Œé€‚åº”çƒå½¢å›¾åƒç‰¹æ€§ã€‚</li>
<li>å›¾åƒé‡å»ºé˜¶æ®µé‡‡ç”¨åŸºäºæµ‹åœ°çº¿çš„é‡é‡‡æ ·ç­–ç•¥ï¼Œä¸çƒå½¢å‡ ä½•å¯¹é½ï¼Œæ— é¢å¤–å‚æ•°ã€‚</li>
<li>FAORæ¨¡å‹å®ç°äº†å¯¹æœ€å…ˆè¿›ODI-SRæ¨¡å‹çš„æ€§èƒ½è¶…è¶Šï¼ŒåŒæ—¶ä¿æŒäº†å¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05902v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05902v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05902v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05902v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05902v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Image-Based-Alzheimerâ€™s-Disease-Detection-Using-Pretrained-Convolutional-Neural-Network-Models"><a href="#Image-Based-Alzheimerâ€™s-Disease-Detection-Using-Pretrained-Convolutional-Neural-Network-Models" class="headerlink" title="Image-Based Alzheimerâ€™s Disease Detection Using Pretrained Convolutional   Neural Network Models"></a>Image-Based Alzheimerâ€™s Disease Detection Using Pretrained Convolutional   Neural Network Models</h2><p><strong>Authors:Nasser A Alsadhan</strong></p>
<p>Alzheimerâ€™s disease is an untreatable, progressive brain disorder that slowly robs people of their memory, thinking abilities, and ultimately their capacity to complete even the most basic tasks. Among older adults, it is the most frequent cause of dementia. Although there is presently no treatment for Alzheimerâ€™s disease, scientific trials are ongoing to discover drugs to combat the condition. Treatments to slow the signs of dementia are also available. Many researchers throughout the world became interested in developing computer-aided diagnosis systems to aid in the early identification of this deadly disease and assure an accurate diagnosis. In particular, image based approaches have been coupled with machine learning techniques to address the challenges of Alzheimerâ€™s disease detection. This study proposes a computer aided diagnosis system to detect Alzheimerâ€™s disease from biomarkers captured using neuroimaging techniques. The proposed approach relies on deep learning techniques to extract the relevant visual features from the image collection to accurately predict the Alzheimerâ€™s class value. In the experiments, standard datasets and pre-trained deep learning models were investigated. Moreover, standard performance measures were used to assess the modelsâ€™ performances. The obtained results proved that VGG16-based models outperform the state of the art performance. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…æ˜¯ä¸€ç§æ— æ³•æ²»ç–—çš„ã€è¿›å±•æ€§çš„å¤§è„‘ç–¾ç—…ï¼Œå®ƒä¼šé€æ¸å‰¥å¤ºäººä»¬çš„è®°å¿†ã€æ€ç»´èƒ½åŠ›å’Œæœ€ç»ˆå®Œæˆæœ€åŸºç¡€ä»»åŠ¡çš„èƒ½åŠ›ã€‚åœ¨è€å¹´äººä¸­ï¼Œå®ƒæ˜¯å¯¼è‡´ç—´å‘†æœ€å¸¸è§çš„åŸå› ã€‚è™½ç„¶ç›®å‰è¿˜æ²¡æœ‰æ²»ç–—é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ–¹æ³•ï¼Œä½†ç§‘å­¦å®¶ä»¬æ­£åœ¨è¿›è¡Œè¯ç‰©è¯•éªŒä»¥å¯¹æŠ—è¿™ç§ç–¾ç—…ã€‚ä¹Ÿæœ‰æ²»ç–—æ¥ç¼“è§£ç—´å‘†ç—‡çŠ¶çš„æ–¹æ³•ã€‚ä¸–ç•Œä¸Šè®¸å¤šç ”ç©¶è€…å¯¹å¼€å‘è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿæ„Ÿå…´è¶£ï¼Œä»¥å¸®åŠ©æ—©æœŸå‘ç°è¿™ç§è‡´å‘½çš„ç–¾ç—…å¹¶ç¡®ä¿å‡†ç¡®è¯Šæ–­ã€‚ç‰¹åˆ«æ˜¯ï¼Œå›¾åƒæ–¹æ³•å·²ç»ä¸æœºå™¨å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥è§£å†³é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿï¼Œé€šè¿‡ç¥ç»æˆåƒæŠ€æœ¯æ•è·çš„ç”Ÿç‰©æ ‡å¿—ç‰©æ¥æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¾èµ–äºæ·±åº¦å­¦ä¹ æŠ€æœ¯ä»å›¾åƒé›†åˆä¸­æå–ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä»¥å‡†ç¡®é¢„æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»å€¼ã€‚åœ¨å®éªŒä¸­ï¼Œè°ƒæŸ¥äº†æ ‡å‡†æ•°æ®é›†å’Œé¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è€Œä¸”ï¼Œä½¿ç”¨æ ‡å‡†æ€§èƒ½åº¦é‡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ‰€å¾—ç»“æœè¯æ˜ï¼ŒåŸºäºVGG16çš„æ¨¡å‹ä¼˜äºç°æœ‰æŠ€æœ¯æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05815v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦æè¿°äº†å¯¹è€å¹´ç—´å‘†ç—‡ï¼ˆAlzheimerâ€™s diseaseï¼‰çš„ç ”ç©¶ç°çŠ¶ã€‚è™½ç„¶å½“å‰æ²¡æœ‰æ²»ç–—æ–¹æ³•ï¼Œä½†ç§‘å­¦å®¶ä»¬æ­£åœ¨å¯»æ‰¾å¯¹æŠ—è¿™ç§ç–¾ç—…çš„è¯ç‰©ã€‚åŒæ—¶ï¼Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿè¢«å¼€å‘å‡ºæ¥å¸®åŠ©æ—©æœŸè¯†åˆ«è¿™ç§ç–¾ç—…å¹¶ä¿è¯å‡†ç¡®è¯Šæ–­ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿï¼Œé€šè¿‡ç¥ç»å½±åƒæŠ€æœ¯æ•æ‰ç”Ÿç‰©æ ‡å¿—ç‰©æ¥æ£€æµ‹è€å¹´ç—´å‘†ç—‡ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºVGG16æ¨¡å‹çš„æ€§èƒ½è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Alzheimerâ€™s diseaseæ˜¯å¸¸è§çš„ç—´å‘†ç—‡çŠ¶ï¼Œç›®å‰æ— æ³•æ²»ç–—ï¼Œä½†ç§‘å­¦å®¶æ­£åœ¨å¯»æ‰¾ç›¸å…³è¯ç‰©ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿè¢«ç”¨äºæ—©æœŸè¯†åˆ«å’Œå‡†ç¡®è¯Šæ–­è€å¹´ç—´å‘†ç—‡ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è¯Šæ–­ç³»ç»Ÿï¼Œé€šè¿‡ç¥ç»å½±åƒæŠ€æœ¯æ£€æµ‹è€å¹´ç—´å‘†ç—‡ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡æå–å›¾åƒç›¸å…³ç‰¹å¾æ¥å‡†ç¡®é¢„æµ‹è€å¹´ç—´å‘†ç—‡çš„åˆ†ç±»å€¼ã€‚</li>
<li>å®éªŒé‡‡ç”¨æ ‡å‡†æ•°æ®é›†å’Œé¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åŸºäºVGG16æ¨¡å‹çš„æ€§èƒ½è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05815v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05815v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05815v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05815v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05815v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05815v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Validity-first-automatic-polycube-labeling-for-CAD-models"><a href="#Validity-first-automatic-polycube-labeling-for-CAD-models" class="headerlink" title="Validity-first automatic polycube labeling for CAD models"></a>Validity-first automatic polycube labeling for CAD models</h2><p><strong>Authors:SÃ©bastien Mestrallet, Christophe Bourcier, Franck Ledoux</strong></p>
<p>For many simulation codes, block-structured hex meshes remain preferred while their automatic generation is unsolved. We investigate the usage of a polycube-based approach. More specifically, we focus on the labeling stage, which consists in assigning each boundary facet to one of the 6 signed principal axis. Similar works are confronted with 2 challenges: over-constraining validity criteria, and the conflated processing of validity criteria with quality metrics. We tackle these obstacles with automatic routines based on semi-global labeling operators. Our approach is successfully tested on CAD models, which are of interest for many numerical simulation problems. </p>
<blockquote>
<p>å¯¹äºè®¸å¤šä»¿çœŸä»£ç æ¥è¯´ï¼Œè™½ç„¶å—ç»“æ„åŒ–çš„å…­è¾¹å½¢ç½‘æ ¼çš„è‡ªåŠ¨ç”Ÿæˆå°šæœªå¾—åˆ°è§£å†³ï¼Œä½†å®ƒä»ç„¶æ˜¯æœ€å—æ¬¢è¿çš„é€‰æ‹©ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºå¤šè¾¹å½¢çš„å¤„ç†æ–¹æ³•çš„åº”ç”¨ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯æ ‡è®°é˜¶æ®µï¼Œè¯¥é˜¶æ®µå°†æ¯ä¸ªè¾¹ç•Œé¢åˆ†é…ç»™å…­ä¸ªå¸¦ç¬¦å·çš„ä¸»è½´ä¹‹ä¸€ã€‚ç±»ä¼¼çš„ç ”ç©¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè¿‡äºä¸¥æ ¼çš„åˆç†æ€§æ ‡å‡†ä»¥åŠåˆç†æ€§æ ‡å‡†ä¸è´¨é‡æŒ‡æ ‡çš„æ··æ·†å¤„ç†ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºåŠå…¨å±€æ ‡è®°è¿ç®—ç¬¦çš„è‡ªåŠ¨ç¨‹åºæ¥è§£å†³è¿™äº›éšœç¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨CADæ¨¡å‹ä¸Šæµ‹è¯•æˆåŠŸï¼Œè¿™å¯¹è®¸å¤šæ•°å€¼æ¨¡æ‹Ÿé—®é¢˜å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05712v1">PDF</a> 14 pages. Source code:   <a target="_blank" rel="noopener" href="https://github.com/LIHPC-Computational-Geometry/validity-first-polycube-labeling">https://github.com/LIHPC-Computational-Geometry/validity-first-polycube-labeling</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨è®¸å¤šä»¿çœŸä»£ç ä¸­é¦–é€‰çš„å—ç»“æ„å…­è¾¹å½¢ç½‘æ ¼çš„è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ã€‚ç ”ç©¶äº†ä¸€ç§åŸºäºå¤šè¾¹å½¢çš„å¤„ç†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ ‡ç­¾é˜¶æ®µï¼Œå³ä¸ºæ¯ä¸ªè¾¹ç•Œé¢åˆ†é…ç»™å…­ä¸ªæœ‰ç¬¦å·ä¸»è½´ä¹‹ä¸€çš„è¿‡ç¨‹ã€‚ç±»ä¼¼çš„ç ”ç©¶é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šè¿‡äºä¸¥æ ¼çš„æœ‰æ•ˆæ€§æ ‡å‡†å’Œæ··æ·†çš„æœ‰æ•ˆæ€§æ ‡å‡†ä¸è´¨é‡æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶é‡‡ç”¨åŸºäºåŠå…¨å±€æ ‡ç­¾ç®—å­çš„è‡ªåŠ¨ç¨‹åºè§£å†³äº†è¿™äº›éšœç¢ï¼Œå¹¶åœ¨CADæ¨¡å‹ä¸Šè¿›è¡Œäº†æˆåŠŸæµ‹è¯•ï¼Œè¿™å¯¹è®¸å¤šæ•°å€¼æ¨¡æ‹Ÿé—®é¢˜å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å—ç»“æ„å…­è¾¹å½¢ç½‘æ ¼åœ¨è®¸å¤šä»¿çœŸä»£ç ä¸­å—åˆ°é’çï¼Œä½†å…¶è‡ªåŠ¨ç”Ÿæˆä»æ˜¯æœªè§£å†³çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶äº†ä¸€ç§åŸºäºå¤šè¾¹å½¢çš„å¤„ç†æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ ‡ç­¾é˜¶æ®µæ˜¯å…³é”®ç¯èŠ‚ï¼Œæ¶‰åŠä¸ºæ¯ä¸ªè¾¹ç•Œé¢åˆ†é…å…­ä¸ªæœ‰ç¬¦å·ä¸»è½´ä¹‹ä¸€çš„ä»»åŠ¡ã€‚</li>
<li>ç±»ä¼¼çš„ç ”ç©¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè¿‡äºä¸¥æ ¼çš„æœ‰æ•ˆæ€§æ ‡å‡†å’Œæ··æ·†çš„æœ‰æ•ˆæ€§æ ‡å‡†ä¸è´¨é‡æŒ‡æ ‡çš„å¤„ç†é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡é‡‡ç”¨åŸºäºåŠå…¨å±€æ ‡ç­¾ç®—å­çš„è‡ªåŠ¨ç¨‹åºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•åœ¨CADæ¨¡å‹ä¸Šè¿›è¡Œäº†æˆåŠŸæµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05712v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semantic-Data-Augmentation-Enhanced-Invariant-Risk-Minimization-for-Medical-Image-Domain-Generalization"><a href="#Semantic-Data-Augmentation-Enhanced-Invariant-Risk-Minimization-for-Medical-Image-Domain-Generalization" class="headerlink" title="Semantic Data Augmentation Enhanced Invariant Risk Minimization for   Medical Image Domain Generalization"></a>Semantic Data Augmentation Enhanced Invariant Risk Minimization for   Medical Image Domain Generalization</h2><p><strong>Authors:Yaoyao Zhu, Xiuding Cai, Yingkai Wang, Yu Yao, Xu Luo, Zhongliang Fu</strong></p>
<p>Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºå…¶æ‰«æè®¾å¤‡ä¾›åº”å•†ã€æˆåƒåè®®å’Œæ“ä½œäººå‘˜ç­‰æ–¹é¢çš„å·®å¼‚å¯¼è‡´çš„æ•°æ®å¼‚è´¨æ€§ï¼Œå…¶ä¸´åºŠåº”ç”¨å¾€å¾€å—åˆ°é˜»ç¢ã€‚ä¸å˜é£é™©æœ€å°åŒ–ï¼ˆIRMï¼‰ç­‰æ–¹æ³•æ—¨åœ¨è§£å†³åˆ†å¸ƒå¤–æ¨å¹¿çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼ŒVIRMé€šè¿‡è§£å†³ç‰¹å¾æ”¯æŒé‡å ä¸è¶³çš„é—®é¢˜æ”¹è¿›äº†IRMï¼Œæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åŒ»å­¦æˆåƒæ–¹é¢é¢ä¸´ç€æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå¢å¼ºç­–ç•¥æ•ˆç‡ä½ä¸‹çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é¢å‘åŸŸçš„æ–¹å‘é€‰æ‹©å™¨ï¼Œä»¥æ›¿ä»£VIRMä¸­ä½¿ç”¨çš„éšæœºå¢å¼ºç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸŸé—´åæ–¹å·®ä½œä¸ºå¢å¼ºæ–¹å‘çš„æŒ‡å¯¼ï¼Œå¼•å¯¼æ•°æ®å¢å¼ºæœå‘ç›®æ ‡åŸŸã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†åŸŸå·®å¼‚ï¼Œæé«˜äº†æ³›åŒ–æ€§èƒ½ã€‚åœ¨å¤šä¸­å¿ƒç³–å°¿ç—…è§†ç½‘è†œç—…å˜æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’ŒåŸŸå·®å¼‚æ˜¾è‘—çš„æ¡ä»¶ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05593v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­æ·±åº¦å­¦ä¹ å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æ•°æ®å¼‚è´¨æ€§é˜»ç¢äº†å…¶ä¸´åºŠåº”ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹é¢†åŸŸå¯¼å‘æ–¹å‘é€‰æ‹©å™¨ï¼Œæ›¿ä»£VIRMä¸­çš„éšæœºå¢å¼ºç­–ç•¥ï¼Œåˆ©ç”¨è·¨åŸŸåæ–¹å·®ä½œä¸ºå¢å¼ºæ–¹å‘çš„æŒ‡å¯¼ï¼Œæœ‰æ•ˆå‡å°‘åŸŸå·®å¼‚ï¼Œæé«˜æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸Šå–å¾—æ˜¾è‘—æˆå°±ï¼Œä½†æ•°æ®å¼‚è´¨æ€§æ˜¯ä¸´åºŠåº”ç”¨ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>IRMæ–¹æ³•æ—¨åœ¨è§£å†³åˆ†å¸ƒå¤–æ¨å¹¿çš„æŒ‘æˆ˜ï¼Œä½†å­˜åœ¨ä¸è¶³ã€‚</li>
<li>VIRMæ–¹æ³•è§£å†³äº†IRMä¸­ç‰¹å¾æ”¯æŒé‡å ä¸è¶³çš„é—®é¢˜ï¼Œå±•ç°å‡ºè‰¯å¥½æ½œåŠ›ã€‚</li>
<li>åŒ»å­¦æˆåƒä¸­é¢ä¸´çš„é—®é¢˜æ˜¯æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå¢å¼ºç­–ç•¥æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>æ–°å‹é¢†åŸŸå¯¼å‘æ–¹å‘é€‰æ‹©å™¨è¢«æå‡ºï¼Œä»¥æ›¿ä»£VIRMä¸­çš„éšæœºå¢å¼ºç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è·¨åŸŸåæ–¹å·®ä½œä¸ºå¢å¼ºæ–¹å‘çš„æŒ‡å¯¼ï¼Œæœ‰æ•ˆå‡å°‘åŸŸå·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05593v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05593v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05593v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05593v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05593v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LMS-Net-A-Learned-Mumford-Shah-Network-For-Few-Shot-Medical-Image-Segmentation"><a href="#LMS-Net-A-Learned-Mumford-Shah-Network-For-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="LMS-Net: A Learned Mumford-Shah Network For Few-Shot Medical Image   Segmentation"></a>LMS-Net: A Learned Mumford-Shah Network For Few-Shot Medical Image   Segmentation</h2><p><strong>Authors:Shengdong Zhang, Fan Jia, Xiang Li, Hao Zhang, Jun Shi, Liyan Ma, Shihui Ying</strong></p>
<p>Few-shot semantic segmentation (FSS) methods have shown great promise in handling data-scarce scenarios, particularly in medical image segmentation tasks. However, most existing FSS architectures lack sufficient interpretability and fail to fully incorporate the underlying physical structures of semantic regions. To address these issues, in this paper, we propose a novel deep unfolding network, called the Learned Mumford-Shah Network (LMS-Net), for the FSS task. Specifically, motivated by the effectiveness of pixel-to-prototype comparison in prototypical FSS methods and the capability of deep priors to model complex spatial structures, we leverage our learned Mumford-Shah model (LMS model) as a mathematical foundation to integrate these insights into a unified framework. By reformulating the LMS model into prototype update and mask update tasks, we propose an alternating optimization algorithm to solve it efficiently. Further, the iterative steps of this algorithm are unfolded into corresponding network modules, resulting in LMS-Net with clear interpretability. Comprehensive experiments on three publicly available medical segmentation datasets verify the effectiveness of our method, demonstrating superior accuracy and robustness in handling complex structures and adapting to challenging segmentation scenarios. These results highlight the potential of LMS-Net to advance FSS in medical imaging applications. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/SDZhang01/LMSNet">https://github.com/SDZhang01/LMSNet</a> </p>
<blockquote>
<p>å°æ ·è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯çš„å¤„ç†ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„FSSæ¶æ„ç¼ºä¹è¶³å¤Ÿçš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†èå…¥è¯­ä¹‰åŒºåŸŸçš„åº•å±‚ç‰©ç†ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å±•å¼€ç½‘ç»œï¼Œç§°ä¸ºå­¦ä¹ Mumford-Shahç½‘ç»œï¼ˆLMS-Netï¼‰ï¼Œç”¨äºFSSä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼Œå—åˆ°åŸå‹FSSæ–¹æ³•ä¸­çš„åƒç´ åˆ°åŸå‹æ¯”è¾ƒçš„æœ‰æ•ˆæ€§å’Œæ·±åº¦å…ˆéªŒå»ºæ¨¡å¤æ‚ç©ºé—´ç»“æ„çš„èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬ä»¥å­¦ä¹ åˆ°çš„Mumford-Shahæ¨¡å‹ï¼ˆLMSæ¨¡å‹ï¼‰ä½œä¸ºæ•°å­¦åŸºç¡€ï¼Œå°†è¿™äº›è§è§£æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚é€šè¿‡å°†LMSæ¨¡å‹é‡æ–°åˆ¶å®šä¸ºåŸå‹æ›´æ–°å’Œæ©è†œæ›´æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äº¤æ›¿ä¼˜åŒ–ç®—æ³•æ¥æœ‰æ•ˆåœ°è§£å†³å®ƒã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•çš„è¿­ä»£æ­¥éª¤è¢«å±•å¼€æˆç›¸åº”çš„ç½‘ç»œæ¨¡å—ï¼Œä»è€Œå½¢æˆäº†å…·æœ‰æ˜ç¡®å¯è§£é‡Šæ€§çš„LMS-Netã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤„ç†å¤æ‚ç»“æ„å’Œé€‚åº”å…·æœ‰æŒ‘æˆ˜æ€§çš„åˆ†å‰²åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¿™äº›ç»“æœçªå‡ºäº†LMS-Netåœ¨åŒ»å­¦æˆåƒåº”ç”¨çš„FSSä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/SDZhang01/LMSNet">https://github.com/SDZhang01/LMSNet</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05473v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜å¯é€šè¿‡å°‘æ•°è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•è§£å†³ï¼Œä½†ç°æœ‰æ¶æ„ç¼ºä¹è¶³å¤Ÿçš„è§£é‡Šæ€§ä¸”æœªèƒ½å……åˆ†èå…¥è¯­ä¹‰åŒºåŸŸçš„åº•å±‚ç‰©ç†ç»“æ„ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ·±åº¦å±•å¼€ç½‘ç»œâ€”â€”å­¦ä¹ Mumford-Shahç½‘ç»œï¼ˆLMS-Netï¼‰ï¼Œä»¥åº”å¯¹è¿™äº›é—®é¢˜ã€‚ç»“åˆåŸå‹FSæ–¹æ³•å’Œæ·±åº¦å…ˆéªŒæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œåˆ©ç”¨å­¦ä¹ Mumford-Shahæ¨¡å‹ï¼ˆLMSæ¨¡å‹ï¼‰ä½œä¸ºæ•°å­¦åŸºç¡€è¿›è¡Œæ•´åˆã€‚é€šè¿‡äº¤æ›¿ä¼˜åŒ–ç®—æ³•é«˜æ•ˆæ±‚è§£LMSæ¨¡å‹ï¼Œå¹¶å±•å¼€è¿­ä»£æ­¥éª¤ä¸ºç›¸åº”çš„ç½‘ç»œæ¨¡å—ï¼Œå®ç°å…·æœ‰æ˜ç¡®è§£é‡Šæ€§çš„LMS-Netã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œå±•ç°å‡ºå¤„ç†å¤æ‚ç»“æ„å’Œé€‚åº”æŒ‘æˆ˜åˆ†å‰²åœºæ™¯çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FSSæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¤„ç†æ•°æ®ç¨€ç¼ºé—®é¢˜å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰FSSæ¶æ„ç¼ºä¹è§£é‡Šæ€§ï¼Œæœªèƒ½å……åˆ†èå…¥è¯­ä¹‰åŒºåŸŸçš„ç‰©ç†ç»“æ„ã€‚</li>
<li>æå‡ºäº†æ–°å‹ç½‘ç»œLMS-Netï¼Œç»“åˆåŸå‹FSæ–¹æ³•å’Œæ·±åº¦å…ˆéªŒæ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>åˆ©ç”¨å­¦ä¹ Mumford-Shahæ¨¡å‹ï¼ˆLMSæ¨¡å‹ï¼‰ä½œä¸ºæ•°å­¦åŸºç¡€è¿›è¡Œæ•´åˆã€‚</li>
<li>é€šè¿‡äº¤æ›¿ä¼˜åŒ–ç®—æ³•é«˜æ•ˆæ±‚è§£LMSæ¨¡å‹ã€‚</li>
<li>LMS-Netå…·æœ‰æ˜ç¡®çš„è§£é‡Šæ€§ï¼Œé€šè¿‡å±•å¼€è¿­ä»£æ­¥éª¤ä¸ºç›¸åº”çš„ç½‘ç»œæ¨¡å—ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†LMS-Netçš„ä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05473v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05473v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05473v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05473v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Novel-Convolutional-Free-Method-for-3D-Medical-Imaging-Segmentation"><a href="#A-Novel-Convolutional-Free-Method-for-3D-Medical-Imaging-Segmentation" class="headerlink" title="A Novel Convolutional-Free Method for 3D Medical Imaging Segmentation"></a>A Novel Convolutional-Free Method for 3D Medical Imaging Segmentation</h2><p><strong>Authors:Canxuan Gang</strong></p>
<p>Segmentation of 3D medical images is a critical task for accurate diagnosis and treatment planning. Convolutional neural networks (CNNs) have dominated the field, achieving significant success in 3D medical image segmentation. However, CNNs struggle with capturing long-range dependencies and global context, limiting their performance, particularly for fine and complex structures. Recent transformer-based models, such as TransUNet and nnFormer, have demonstrated promise in addressing these limitations, though they still rely on hybrid CNN-transformer architectures. This paper introduces a novel, fully convolutional-free model based on transformer architecture and self-attention mechanisms for 3D medical image segmentation. Our approach focuses on improving multi-semantic segmentation accuracy and addressing domain adaptation challenges between thick and thin slice CT images. We propose a joint loss function that facilitates effective segmentation of thin slices based on thick slice annotations, overcoming limitations in dataset availability. Furthermore, we present a benchmark dataset for multi-semantic segmentation on thin slices, addressing a gap in current medical imaging research. Our experiments demonstrate the superiority of the proposed model over traditional and hybrid architectures, offering new insights into the future of convolution-free medical image segmentation. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„å…³é”®ä»»åŠ¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨è¯¥é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œå¹¶åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼ŒCNNåœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–æ€§å’Œå…¨å±€ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç²¾ç»†å’Œå¤æ‚ç»“æ„çš„åˆ†å‰²ã€‚æœ€è¿‘åŸºäºtransformerçš„æ¨¡å‹ï¼Œå¦‚TransUNetå’ŒnnFormerï¼Œæ˜¾ç¤ºå‡ºè§£å†³è¿™äº›é™åˆ¶çš„æ½œåŠ›ï¼Œå°½ç®¡å®ƒä»¬ä»ç„¶ä¾èµ–äºæ··åˆçš„CNN-transformeræ¶æ„ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ã€å®Œå…¨åŸºäºtransformeræ¶æ„å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ã€ä¸å«å·ç§¯çš„ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºæé«˜å¤šè¯­ä¹‰åˆ†å‰²çš„ç²¾åº¦ï¼Œå¹¶è§£å†³åšåˆ‡ç‰‡å’Œè–„åˆ‡ç‰‡CTå›¾åƒä¹‹é—´çš„åŸŸé€‚åº”æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆæŸå¤±å‡½æ•°ï¼Œå®ƒåŸºäºåšåˆ‡ç‰‡æ³¨é‡Šæœ‰æ•ˆåœ°åˆ†å‰²è–„åˆ‡ç‰‡ï¼Œå…‹æœäº†æ•°æ®é›†å¯ç”¨æ€§æ–¹é¢çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºè–„åˆ‡ç‰‡ä¸Šçš„å¤šè¯­ä¹‰åˆ†å‰²æä¾›äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œè§£å†³äº†å½“å‰åŒ»å­¦æˆåƒç ”ç©¶ä¸­çš„ç©ºç™½ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†æ‰€æå‡ºæ¨¡å‹åœ¨æ€§èƒ½å’Œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿå’Œæ··åˆæ¶æ„ï¼Œä¸ºæ— å·ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„æœªæ¥æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05396v1">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformeræ¶æ„å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å…¨æ–°å·ç§¯è‡ªç”±æ¨¡å‹ï¼Œç”¨äºæ”¹è¿›ä¸‰ç»´åŒ»å­¦å›¾åƒçš„å¤šè¯­ä¹‰åˆ†å‰²ç²¾åº¦å¹¶è§£å†³åšåˆ‡ç‰‡ä¸è–„åˆ‡ç‰‡CTå›¾åƒä¹‹é—´çš„åŸŸé€‚åº”æŒ‘æˆ˜ã€‚æå‡ºè”åˆæŸå¤±å‡½æ•°ï¼ŒåŸºäºåšåˆ‡ç‰‡æ³¨é‡Šå®ç°è–„åˆ‡ç‰‡çš„æœ‰æ•ˆåˆ†å‰²ï¼Œè§£å†³æ•°æ®é›†å¯ç”¨æ€§é™åˆ¶é—®é¢˜ã€‚åŒæ—¶æä¾›å¤šè¯­ä¹‰åˆ†å‰²çš„è–„åˆ‡ç‰‡åŸºå‡†æ•°æ®é›†ï¼Œå¡«è¡¥å½“å‰åŒ»å­¦æˆåƒç ”ç©¶çš„ç©ºç™½ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨æ€§èƒ½å’Œå‡†ç¡®æ€§ä¸Šä¼˜äºä¼ ç»Ÿå’Œæ··åˆæ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è®ºæ–‡ä»‹ç»äº†é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºTransformeræ¶æ„å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æ”¹è¿›å¤šè¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®åº¦ã€‚</li>
<li>æ–°æ¨¡å‹è§£å†³äº†å·ç§¯ç¥ç»ç½‘ç»œåœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å’Œå…¨å±€ä¸Šä¸‹æ–‡æ—¶çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†å’Œå¤æ‚ç»“æ„ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†åšåˆ‡ç‰‡ä¸è–„åˆ‡ç‰‡CTå›¾åƒä¹‹é—´çš„åŸŸé€‚åº”æŒ‘æˆ˜ï¼Œé€šè¿‡æå‡ºè”åˆæŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨åšåˆ‡ç‰‡çš„æ³¨é‡Šä¿¡æ¯å®ç°è–„åˆ‡ç‰‡çš„å‡†ç¡®åˆ†å‰²ã€‚</li>
<li>è®ºæ–‡è§£å†³äº†åŒ»å­¦æˆåƒç ”ç©¶ä¸­æ•°æ®é›†å¯ç”¨æ€§å¯¹æ¨¡å‹è®­ç»ƒçš„é™åˆ¶é—®é¢˜ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå¤šè¯­ä¹‰åˆ†å‰²çš„è–„åˆ‡ç‰‡åŸºå‡†æ•°æ®é›†ï¼Œå¡«è¡¥äº†å½“å‰åŒ»å­¦æˆåƒç ”ç©¶çš„ç©ºç™½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05396v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05396v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05396v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Homeomorphism-Prior-for-False-Positive-and-Negative-Problem-in-Medical-Image-Dense-Contrastive-Representation-Learning"><a href="#Homeomorphism-Prior-for-False-Positive-and-Negative-Problem-in-Medical-Image-Dense-Contrastive-Representation-Learning" class="headerlink" title="Homeomorphism Prior for False Positive and Negative Problem in Medical   Image Dense Contrastive Representation Learning"></a>Homeomorphism Prior for False Positive and Negative Problem in Medical   Image Dense Contrastive Representation Learning</h2><p><strong>Authors:Yuting He, Boyu Wang, Rongjun Ge, Yang Chen, Guanyu Yang, Shuo Li</strong></p>
<p>Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&amp;N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixelsâ€™ correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: <a target="_blank" rel="noopener" href="https://github.com/YutingHe-list/GEMINI">https://github.com/YutingHe-list/GEMINI</a>. </p>
<blockquote>
<p>å¯†é›†å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆDCRLï¼‰æå¤§åœ°æé«˜äº†å›¾åƒå¯†é›†é¢„æµ‹ä»»åŠ¡çš„å­¦ä¹ æ•ˆç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨é™ä½åŒ»å­¦å›¾åƒæ”¶é›†å’Œå¯†é›†æ³¨é‡Šçš„å·¨å¤§æˆæœ¬æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŒ»å­¦å›¾åƒçš„ç‰¹æ€§å¯¼è‡´äº†å¯¹åº”å‘ç°çš„ä¸å¯é æ€§ï¼Œä»è€Œåœ¨DCRLä¸­æå‡ºäº†å¤§è§„æ¨¡è¯¯æŠ¥å’Œæ¼æŠ¥ï¼ˆFPï¼†Nï¼‰å¯¹çš„å¼€æ”¾é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GEoMetric vIsual deNse sImilarityï¼ˆGEMINIï¼‰å­¦ä¹ ï¼Œå®ƒå°†åŒèƒšå…ˆéªŒçŸ¥è¯†åµŒå…¥åˆ°DCRLä¸­ï¼Œä¸ºå®ç°å¯é å¯¹åº”å‘ç°å’Œæœ‰æ•ˆçš„å¯†é›†å¯¹æ¯”æä¾›äº†æ”¯æŒã€‚æˆ‘ä»¬æå‡ºäº†å¯å˜å½¢åŒèƒšå­¦ä¹ ï¼ˆDHLï¼‰ï¼Œå®ƒæ¨¡æ‹ŸåŒ»å­¦å›¾åƒçš„åŒèƒšå…³ç³»å¹¶å­¦ä¹ ä¼°è®¡å¯å˜å½¢æ˜ å°„ä»¥é¢„æµ‹æ‹“æ‰‘ä¿æŒä¸‹çš„åƒç´ å¯¹åº”å…³ç³»ã€‚è¿™æœ‰æ•ˆåœ°å‡å°‘äº†é…å¯¹æœç´¢ç©ºé—´ï¼Œå¹¶é€šè¿‡æ¢¯åº¦é©±åŠ¨äº†è´Ÿå¯¹çš„éšå¼å’Œè½¯å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å‡ ä½•è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆGSSï¼‰ï¼Œå®ƒæå–ç‰¹å¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯æ¥æµ‹é‡å¯¹åº”å­¦ä¹ çš„å¯¹é½ç¨‹åº¦ã€‚å®ƒå°†æé«˜å˜å½¢çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ï¼Œå¯é åœ°æ„å»ºæ­£å¯¹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¸å‹è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸Šå®ç°äº†ä¸¤ä¸ªå®ç”¨å˜ä½“è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„ä»¤äººé¼“èˆçš„ç»“æœä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæˆ‘ä»¬çš„å·¨å¤§ä¼˜åŠ¿ã€‚æˆ‘ä»¬å°†åœ¨ä»¥ä¸‹é“¾æ¥å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/YutingHe-list/GEMINI%E3%80%82">https://github.com/YutingHe-list/GEMINIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05282v1">PDF</a> Accepted by T-PAMI 2025</p>
<p><strong>Summary</strong></p>
<p>DCRLåœ¨å›¾åƒå¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒä¸­å­˜åœ¨å¯¹åº”å…³ç³»ä¸å¯é çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºGEMINIå­¦ä¹ ï¼ŒåµŒå…¥homeomorphismå…ˆéªŒï¼Œå®ç°å¯é çš„å¯¹åº”å…³ç³»å‘ç°ï¼Œæœ‰æ•ˆå‡å°‘DCRLä¸­çš„FP&amp;Nå¯¹ã€‚é€šè¿‡DHLå»ºæ¨¡åŒ»å­¦å›¾åƒhomeomorphismå¹¶å­¦ä¹ ä¼°è®¡å˜å½¢æ˜ å°„æ¥é¢„æµ‹åƒç´ å¯¹åº”å…³ç³»ï¼Œæœ‰æ•ˆå‡å°‘é…å¯¹æœç´¢ç©ºé—´ã€‚åŒæ—¶æå‡ºGSSï¼Œæå–ç‰¹å¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯æ¥æµ‹é‡å¯¹åº”å…³ç³»çš„å­¦ä¹ å¯¹é½ç¨‹åº¦ï¼Œæé«˜å­¦ä¹ å’Œå˜å½¢æ•ˆç‡ã€‚å®éªŒåœ¨ä¸¤ä¸ªå…¸å‹è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸Šå®ç°ä¸¤ç§å®ç”¨å˜ä½“ï¼Œåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„ä¼˜å¼‚è¡¨ç°è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCRLåœ¨åŒ»å­¦å›¾åƒå¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å­˜åœ¨å¯¹åº”å…³ç³»ä¸å¯é çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºGEMINIå­¦ä¹ ï¼ŒåµŒå…¥homeomorphismå…ˆéªŒï¼Œè§£å†³DCRLä¸­çš„FP&amp;Nå¯¹é—®é¢˜ã€‚</li>
<li>DHLå»ºæ¨¡åŒ»å­¦å›¾åƒhomeomorphismå¹¶å­¦ä¹ ä¼°è®¡å˜å½¢æ˜ å°„ä»¥é¢„æµ‹åƒç´ å¯¹åº”å…³ç³»ã€‚</li>
<li>GSSç”¨äºæå–ç‰¹å¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜å­¦ä¹ å’Œå˜å½¢æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜GEMINIåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä»£ç å°†åœ¨ç›¸å…³é“¾æ¥ä¸Šå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05282v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05282v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05282v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05282v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="L2GNet-Optimal-Local-to-Global-Representation-of-Anatomical-Structures-for-Generalized-Medical-Image-Segmentation"><a href="#L2GNet-Optimal-Local-to-Global-Representation-of-Anatomical-Structures-for-Generalized-Medical-Image-Segmentation" class="headerlink" title="L2GNet: Optimal Local-to-Global Representation of Anatomical Structures   for Generalized Medical Image Segmentation"></a>L2GNet: Optimal Local-to-Global Representation of Anatomical Structures   for Generalized Medical Image Segmentation</h2><p><strong>Authors:Vandan Gorade, Sparsh Mittal, Neethi Dasu, Rekha Singhal, KC Santosh, Debesh Jha</strong></p>
<p>Continuous Latent Space (CLS) and Discrete Latent Space (DLS) models, like AttnUNet and VQUNet, have excelled in medical image segmentation. In contrast, Synergistic Continuous and Discrete Latent Space (CDLS) models show promise in handling fine and coarse-grained information. However, they struggle with modeling long-range dependencies. CLS or CDLS-based models, such as TransUNet or SynergyNet are adept at capturing long-range dependencies. Since they rely heavily on feature pooling or aggregation using self-attention, they may capture dependencies among redundant regions. This hinders comprehension of anatomical structure content, poses challenges in modeling intra-class and inter-class dependencies, increases false negatives and compromises generalization. Addressing these issues, we propose L2GNet, which learns global dependencies by relating discrete codes obtained from DLS using optimal transport and aligning codes on a trainable reference. L2GNet achieves discriminative on-the-fly representation learning without an additional weight matrix in self-attention models, making it computationally efficient for medical applications. Extensive experiments on multi-organ segmentation and cardiac datasets demonstrate L2GNetâ€™s superiority over state-of-the-art methods, including the CDLS method SynergyNet, offering an novel approach to enhance deep learning modelsâ€™ performance in medical image analysis. </p>
<blockquote>
<p>è¿ç»­æ½œåœ¨ç©ºé—´ï¼ˆCLSï¼‰å’Œç¦»æ•£æ½œåœ¨ç©ºé—´ï¼ˆDLSï¼‰æ¨¡å‹ï¼Œå¦‚AttnUNetå’ŒVQUNetï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒååŒè¿ç»­å’Œç¦»æ•£æ½œåœ¨ç©ºé—´ï¼ˆCDLSï¼‰æ¨¡å‹åœ¨å¤„ç†ç²¾ç»†å’Œç²—ç³™ç²’åº¦ä¿¡æ¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢é‡åˆ°å›°éš¾ã€‚åŸºäºCLSæˆ–CDLSçš„æ¨¡å‹ï¼Œå¦‚TransUNetæˆ–SynergyNetï¼Œæ“…é•¿æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç”±äºå®ƒä»¬å¤§é‡ä¾èµ–ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„ç‰¹å¾æ± åŒ–æˆ–èšåˆï¼Œå› æ­¤å¯èƒ½ä¼šæ•è·å†—ä½™åŒºåŸŸä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™å¦¨ç¢äº†å¯¹è§£å‰–ç»“æ„å†…å®¹çš„ç†è§£ï¼Œå¯¹å»ºæ¨¡ç±»å†…å’Œç±»é—´ä¾èµ–å…³ç³»å¸¦æ¥æŒ‘æˆ˜ï¼Œå¢åŠ äº†å‡é˜´æ€§ï¼Œå¹¶æŸå®³äº†æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†L2GNetã€‚å®ƒé€šè¿‡åˆ©ç”¨ç¦»æ•£ä»£ç ï¼ˆä»DLSè·å¾—ï¼‰åœ¨æœ€ä¼˜ä¼ è¾“ä¸­è¿›è¡Œå…³è”ï¼Œå¹¶åœ¨å¯è®­ç»ƒå‚è€ƒä¸Šè¿›è¡Œä»£ç å¯¹é½ï¼Œä»è€Œå­¦ä¹ å…¨å±€ä¾èµ–å…³ç³»ã€‚L2GNetå®ç°äº†åˆ¤åˆ«å¼çš„å³æ—¶è¡¨ç¤ºå­¦ä¹ ï¼Œæ— éœ€åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å‹ä¸­ä½¿ç”¨é¢å¤–çš„æƒé‡çŸ©é˜µï¼Œä½¿å…¶åœ¨è®¡ç®—ä¸Šéå¸¸é€‚åˆåŒ»å­¦åº”ç”¨ã€‚åœ¨å¤šå™¨å®˜åˆ†å‰²å’Œå¿ƒè„æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒL2GNetä¼˜äºåŒ…æ‹¬SynergyNetåœ¨å†…çš„æœ€æ–°æ–¹æ³•ï¼Œä¸ºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ€§èƒ½æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05229v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿ç»­æ½œåœ¨ç©ºé—´ï¼ˆCLSï¼‰å’Œç¦»æ•£æ½œåœ¨ç©ºé—´ï¼ˆDLSï¼‰æ¨¡å‹çš„AttnUNetå’ŒVQUNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºè‰²ã€‚ååŒè¿ç»­å’Œç¦»æ•£æ½œåœ¨ç©ºé—´ï¼ˆCDLSï¼‰æ¨¡å‹åœ¨å¤„ç†ç²¾ç»†å’Œç²—ç³™ç²’åº¦ä¿¡æ¯æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ—¶é‡åˆ°å›°éš¾ã€‚TransUNetæˆ–SynergyNetç­‰åŸºäºCLSæˆ–CDLSçš„æ¨¡å‹æ“…é•¿æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†å¯èƒ½å› ä¾èµ–ç‰¹å¾æ± åŒ–æˆ–è‡ªæ³¨æ„åŠ›èšåˆè€Œæ•æ‰å†—ä½™åŒºåŸŸçš„ä¾èµ–å…³ç³»ï¼Œè¿™ä¼šå½±å“å¯¹è§£å‰–ç»“æ„å†…å®¹çš„ç†è§£ï¼Œå»ºæ¨¡ç±»å†…å’Œç±»é—´ä¾èµ–å…³ç³»æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¢åŠ è¯¯æ£€å¹¶å½±å“æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†L2GNetï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç¦»æ•£ä»£ç å­¦ä¹ å…¨å±€ä¾èµ–å…³ç³»ï¼Œä½¿ç”¨æœ€ä¼˜ä¼ è¾“å°†DLSä¸­çš„ä»£ç å…³è”èµ·æ¥ï¼Œå¹¶åœ¨å¯è®­ç»ƒå‚è€ƒä¸Šè¿›è¡Œä»£ç å¯¹é½ã€‚L2GNetå®ç°äº†åˆ¤åˆ«å¼çš„å³æ—¶è¡¨ç¤ºå­¦ä¹ ï¼Œæ— éœ€åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å‹ä¸­ä½¿ç”¨é¢å¤–çš„æƒé‡çŸ©é˜µï¼Œä½¿å…¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šé€‚åˆåŒ»å­¦åº”ç”¨ã€‚åœ¨å¤šéƒ¨ä½åˆ†å‰²å’Œå¿ƒè„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒL2GNetä¼˜äºåŒ…æ‹¬SynergyNetåœ¨å†…çš„æœ€æ–°æ–¹æ³•ï¼Œä¸ºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ€§èƒ½æä¾›äº†æ–°é¢–çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLSå’ŒDLSæ¨¡å‹ï¼ˆå¦‚AttnUNetå’ŒVQUNetï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CDLSæ¨¡å‹åœ¨å¤„ç†ç²¾ç»†å’Œç²—ç³™ç²’åº¦ä¿¡æ¯æ—¶å…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>CLSæˆ–CDLSæ¨¡å‹ï¼ˆå¦‚TransUNetå’ŒSynergyNetï¼‰æ“…é•¿æ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œä½†å¯èƒ½å—åˆ°å†—ä½™åŒºåŸŸä¾èµ–çš„å½±å“ã€‚</li>
<li>L2GNeté€šè¿‡ç»“åˆDLSçš„ç¦»æ•£ä»£ç å­¦ä¹ å…¨å±€ä¾èµ–å…³ç³»ï¼Œä½¿ç”¨æœ€ä¼˜ä¼ è¾“å’Œä»£ç å¯¹é½æ¥æå‡æ€§èƒ½ã€‚</li>
<li>L2GNetå®ç°äº†åˆ¤åˆ«å¼çš„å³æ—¶è¡¨ç¤ºå­¦ä¹ ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«é€‚åˆåŒ»å­¦åº”ç”¨ã€‚</li>
<li>L2GNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤„ç†å¤æ‚åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„SynergyNetã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_5_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.05229v1/page_5_3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Consistent-and-Controllable-Image-Synthesis-for-Face-Editing"><a href="#Towards-Consistent-and-Controllable-Image-Synthesis-for-Face-Editing" class="headerlink" title="Towards Consistent and Controllable Image Synthesis for Face Editing"></a>Towards Consistent and Controllable Image Synthesis for Face Editing</h2><p><strong>Authors:Mengting Wei, Tuomas Varanka, Yante Li, Xingxun Jiang, Huai-Qian Khor, Guoying Zhao</strong></p>
<p>Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/weimengting/RigFace">https://github.com/weimengting/RigFace</a>. </p>
<blockquote>
<p>é¢éƒ¨ç¼–è¾‘æ–¹æ³•å¯¹äºè™šæ‹ŸåŒ–èº«ã€æ•°å­—äººç±»åˆæˆå’Œèº«ä»½ä¿ç•™ç­‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿä¸ŠåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æŠ€æœ¯æ„å»ºã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å…³æ³¨å·²ç»è½¬å‘æ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åœ¨å›¾é‡å»ºæ–¹é¢å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹åœ¨æ§åˆ¶ç‰¹å®šå±æ€§ä»¥åŠä¿æŒå…¶ä»–æœªæ›´æ”¹å±æ€§çš„ä¸€è‡´æ€§æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯èº«ä»½ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶æ–¹ä¾¿å¯¹é¢éƒ¨å›¾åƒè¿›è¡Œæ›´ä¾¿æ·çš„ç¼–è¾‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨Stable-Diffusionï¼ˆSDï¼‰æ¨¡å‹å’Œç²—ç•¥çš„3Dé¢éƒ¨æ¨¡å‹çš„åŠ›é‡æ¥æ§åˆ¶è‚–åƒç…§ç‰‡çš„å…‰çº¿ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿ã€‚æˆ‘ä»¬å‘ç°æ­¤ä»»åŠ¡ä¸»è¦æ¶‰åŠè¦ç¼–è¾‘çš„ç›®æ ‡èƒŒæ™¯ã€èº«ä»½å’Œé¢éƒ¨å±æ€§ã€‚æˆ‘ä»¬åŠªåŠ›å……åˆ†è§£å¼€è¿™äº›å› ç´ çš„æ§åˆ¶ä»¥å®ç°é¢éƒ¨ç¼–è¾‘çš„ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆè¢«ç§°ä¸ºRigFaceï¼‰åŒ…æ‹¬ï¼š1ï¼‰ç©ºé—´å±æ€§ç¼–ç å™¨ï¼Œå®ƒæä¾›ç²¾ç¡®ä¸”è§£è€¦çš„èƒŒæ™¯ã€å§¿åŠ¿ã€è¡¨æƒ…å’Œå…‰çº¿æ¡ä»¶ï¼›2ï¼‰é«˜ä¸€è‡´æ€§FaceFusionæ–¹æ³•ï¼Œå°†èº«ä»½ç‰¹å¾ä»èº«ä»½ç¼–ç å™¨è½¬ç§»åˆ°é¢„è®­ç»ƒSDæ¨¡å‹çš„é™å™ªUNetä¸­ï¼›3ï¼‰å±æ€§è§¦å‘å™¨å°†è¿™äº›æ¡ä»¶æ³¨å…¥åˆ°é™å™ªUNetä¸­ã€‚ä¸ç°æœ‰çš„é¢éƒ¨ç¼–è¾‘æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨èº«ä»½ä¿ç•™å’Œé€¼çœŸåº¦æ–¹é¢å®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/weimengting/RigFace%E3%80%82">https://github.com/weimengting/RigFaceã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02465v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆStable-Diffusionæ¨¡å‹å’Œç®€å•3Dé¢éƒ¨æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶è‚–åƒç…§ç‰‡çš„å…‰çº¿ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿ã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾ç¡®ä¸”è§£è€¦çš„æ¡ä»¶ï¼Œå®ç°èƒŒæ™¯ã€å§¿åŠ¿ã€è¡¨è¾¾å’Œå…‰ç…§çš„æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒé¢éƒ¨ç¼–è¾‘çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•è¢«ç§°ä¸ºRigFaceï¼ŒåŒ…æ‹¬ç©ºé—´å±æ€§ç¼–ç å™¨ã€é«˜ä¸€è‡´æ€§FaceFusionæ–¹æ³•å’Œå±æ€§è§¦å‘å™¨ï¼Œå¯å®ç°ä¸ç°æœ‰é¢éƒ¨ç¼–è¾‘æ¨¡å‹ç›¸æ¯”æ‹Ÿç”šè‡³æ›´ä¼˜è¶Šçš„èº«ä»½ä¿ç•™å’Œé€¼çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨ç¼–è¾‘æ–¹æ³•å¯¹äºè™šæ‹ŸåŒ–èº«ã€æ•°å­—äººç±»åˆæˆå’Œèº«ä»½ä¿ç•™ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦åŸºäºGANæŠ€æœ¯ï¼Œè€Œæœ€è¿‘çš„ç ”ç©¶ç„¦ç‚¹å·²è½¬å‘æ‰©æ•£æ¨¡å‹ï¼Œå› å…¶æˆåŠŸåº”ç”¨äºå›¾åƒé‡å»ºã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ§åˆ¶ç‰¹å®šå±æ€§å’Œä¿æŒæœªæ”¹å˜å±æ€§çš„ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯èº«ä»½ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆStable-Diffusionæ¨¡å‹å’Œç®€å•3Dé¢éƒ¨æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œä»¥æ§åˆ¶è‚–åƒç…§ç‰‡çš„å…‰çº¿ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç²¾ç¡®ä¸”è§£è€¦çš„æ¡ä»¶å®ç°èƒŒæ™¯ã€å§¿åŠ¿ã€è¡¨è¾¾å’Œå…‰ç…§çš„æ§åˆ¶ï¼Œå¹¶ä¿æŒé¢éƒ¨ç¼–è¾‘çš„ä¸€è‡´æ€§ã€‚</li>
<li>RigFaceæ–¹æ³•åŒ…æ‹¬ç©ºé—´å±æ€§ç¼–ç å™¨ã€é«˜ä¸€è‡´æ€§FaceFusionæ–¹æ³•å’Œå±æ€§è§¦å‘å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.02465v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LEAD-Large-Foundation-Model-for-EEG-Based-Alzheimerâ€™s-Disease-Detection"><a href="#LEAD-Large-Foundation-Model-for-EEG-Based-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="LEAD: Large Foundation Model for EEG-Based Alzheimerâ€™s Disease Detection"></a>LEAD: Large Foundation Model for EEG-Based Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Yihe Wang, Nan Huang, Nadia Mammone, Marco Cecchi, Xiang Zhang</strong></p>
<p>Electroencephalogram (EEG) provides a non-invasive, highly accessible, and cost-effective solution for Alzheimerâ€™s Disease (AD) detection. However, existing methods, whether based on manual feature extraction or deep learning, face two major challenges: the lack of large-scale datasets for robust feature learning and evaluation, and poor detection performance due to inter-subject variations. To address these challenges, we curate an EEG-AD corpus containing 813 subjects, which forms the worldâ€™s largest EEG-AD dataset to the best of our knowledge. Using this unique dataset, we propose LEAD, the first large foundation model for EEG-based AD detection. Our method encompasses an entire pipeline, from data selection and preprocessing to self-supervised contrastive pretraining, fine-tuning, and key setups such as subject-independent evaluation and majority voting for subject-level detection. We pre-train the model on 11 EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised pre-training design includes sample-level and subject-level contrasting to extract useful general EEG features. Fine-tuning is performed on 5 channel-aligned datasets together. The backbone encoder incorporates temporal and channel embeddings to capture features across both temporal and spatial dimensions. Our method demonstrates outstanding AD detection performance, achieving up to a 9.86% increase in F1 score at the sample-level and up to a 9.31% at the subject-level compared to state-of-the-art methods. The results of our model strongly confirm the effectiveness of contrastive pre-training and channel-aligned unified fine-tuning for addressing inter-subject variation. The source code is at <a target="_blank" rel="noopener" href="https://github.com/DL4mHealth/LEAD">https://github.com/DL4mHealth/LEAD</a>. </p>
<blockquote>
<p>è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¸ºæ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—‡ï¼ˆADï¼‰æä¾›äº†ä¸€ç§éä¾µå…¥æ€§ã€æ˜“äºè·å–ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºæ‰‹åŠ¨ç‰¹å¾æå–è¿˜æ˜¯æ·±åº¦å­¦ä¹ ï¼Œéƒ½é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ç”¨äºç¨³å¥ç‰¹å¾å­¦ä¹ å’Œè¯„ä¼°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠå› å—è¯•è€…é—´å·®å¼‚å¯¼è‡´çš„æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç²¾å¿ƒç­–åˆ’äº†ä¸€ä¸ªEEG-ADè¯­æ–™åº“ï¼ŒåŒ…å«813åå—è¯•è€…ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ä¸–ç•Œä¸Šæœ€å¤§çš„EEG-ADæ•°æ®é›†ã€‚ä½¿ç”¨è¯¥ç‹¬ç‰¹æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†LEADâ€”â€”é¦–ä¸ªç”¨äºEEGåŸºADæ£€æµ‹çš„å¤§å‹åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶µç›–äº†æ•´ä¸ªæµç¨‹ï¼Œä»æ•°æ®é€‰æ‹©ã€é¢„å¤„ç†åˆ°è‡ªæˆ‘ç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒã€å¾®è°ƒä»¥åŠä¸»ä½“ç‹¬ç«‹è¯„ä¼°ã€ä¸»ä½“å±‚é¢æ£€æµ‹çš„å¤šæ•°è¡¨å†³ç­‰å…³é”®è®¾ç½®ã€‚æˆ‘ä»¬åœ¨11ä¸ªEEGæ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨5ä¸ªADæ•°æ®é›†ä¸Šè¿›è¡Œç»Ÿä¸€å¾®è°ƒã€‚æˆ‘ä»¬çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒè®¾è®¡åŒ…æ‹¬æ ·æœ¬çº§å’Œä¸»ä½“çº§çš„å¯¹æ¯”ï¼Œä»¥æå–æœ‰ç”¨çš„é€šç”¨EEGç‰¹å¾ã€‚å¾®è°ƒæ˜¯åœ¨5ä¸ªé€šé“å¯¹é½çš„æ•°æ®é›†ä¸Šä¸€èµ·è¿›è¡Œçš„ã€‚ä¸»å¹²ç¼–ç å™¨ç»“åˆæ—¶é—´åµŒå…¥å’Œé€šé“åµŒå…¥ï¼Œä»¥æ•è·æ—¶é—´å’Œç©ºé—´ä¸¤ä¸ªç»´åº¦çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºäº†å“è¶Šçš„ADæ£€æµ‹æ€§èƒ½ï¼Œåœ¨æ ·æœ¬çº§ä¸ŠF1åˆ†æ•°æé«˜äº†9.86%ï¼Œåœ¨ä¸»ä½“çº§ä¸Šæé«˜äº†9.31%ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ã€‚æˆ‘ä»¬æ¨¡å‹çš„ç»“æœå¼ºçƒˆåœ°è¯å®äº†å¯¹æ¯”é¢„è®­ç»ƒå’Œé€šé“å¯¹é½ç»Ÿä¸€å¾®è°ƒåœ¨è§£å†³å—è¯•è€…é—´å·®å¼‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/DL4mHealth/LEAD%E3%80%82">https://github.com/DL4mHealth/LEADã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01678v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä½¿ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹çš„ä¸€ç§æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†å’Œä¸»ä½“é—´å·®å¼‚å¯¼è‡´çš„æ£€æµ‹æ€§èƒ½ä¸ä½³ï¼Œä½œè€…åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«813åå—è¯•è€…çš„EEG-ADè¯­æ–™åº“ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†LEADæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä»æ•°æ®é€‰æ‹©ã€é¢„å¤„ç†åˆ°è‡ªç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒã€å¾®è°ƒç­‰ä¸€ç³»åˆ—æµç¨‹ï¼Œå¹¶é‡‡ç”¨ä¸»ä½“ç‹¬ç«‹è¯„ä¼°ã€å¤šæ•°æŠ•ç¥¨ç­‰å…³é”®è®¾ç½®è¿›è¡Œä¸»ä½“å±‚é¢çš„æ£€æµ‹ã€‚æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œé‡‡ç”¨æ ·æœ¬çº§å’Œä¸»ä½“çº§å¯¹æ¯”è®¾è®¡æå–é€šç”¨EEGç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨ADæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæ ·æœ¬çº§F1åˆ†æ•°æé«˜äº†9.86%ï¼Œä¸»ä½“çº§æé«˜äº†9.31%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EEGä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æä¾›äº†éä¾µå…¥æ€§ã€é«˜åº¦å¯åŠä¸”æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰EEG-ADæ£€æµ‹æ–¹æ³•é¢ä¸´ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†å’Œä¸»ä½“é—´å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†åŒ…å«813åå—è¯•è€…çš„ä¸–ç•Œæœ€å¤§EEG-ADæ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†LEADæ¨¡å‹ï¼Œé‡‡ç”¨ä»æ•°æ®é€‰æ‹©åˆ°é¢„è®­ç»ƒã€å¾®è°ƒç­‰ä¸€ç³»åˆ—æµç¨‹è¿›è¡ŒADæ£€æµ‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ ·æœ¬çº§å’Œä¸»ä½“çº§å¯¹æ¯”è®¾è®¡è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä»¥æå–é€šç”¨EEGç‰¹å¾ã€‚</li>
<li>LEADæ¨¡å‹åœ¨ADæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”æœ‰æ˜æ˜¾æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.01678v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.01678v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.01678v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions"><a href="#Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions" class="headerlink" title="Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions"></a>Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions</h2><p><strong>Authors:Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti</strong></p>
<p>Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading&#x2F;subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathoGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathoGen code is available for open use by the research community through GitHub at <a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathoGen">https://github.com/Samiran-Dey/PathoGen</a>. </p>
<blockquote>
<p>æœ€æ–°çš„ç ”ç©¶å¼ºè°ƒäº†åŸºäºäººå·¥æ™ºèƒ½çš„æ•°å­—ç—…ç†å’Œè½¬å½•ç»„ç‰¹å¾çš„å¤šæ¨¡å¼èåˆèƒ½å¤Ÿæå‡ç™Œç—‡è¯Šæ–­ï¼ˆåˆ†çº§&#x2F;äºšå‹ï¼‰å’Œé¢„åï¼ˆç”Ÿå­˜é£é™©ï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–å¹¶ä¸å®ç”¨ã€‚åœ¨é‚£é‡Œï¼Œç»„ç»‡ç—…ç†å­¦ä»ç„¶æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè½¬å½•ç»„æµ‹è¯•å¾ˆå°‘è¢«è¦æ±‚ï¼ˆè‡³å°‘åœ¨å…¬å…±åŒ»ç–—ä½“ç³»ä¸­ï¼‰ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬æ–°å‹åŸºäºæ‰©æ•£çš„è·¨æ¨¡æ€ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹PathoGenï¼Œæˆ‘ä»¬è¯æ˜äº†ç”±æ•°å­—ç—…ç†å­¦åˆæˆçš„åŸºå› è¡¨è¾¾èƒ½å¤Ÿè”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©ï¼Œå…·æœ‰é«˜å‡†ç¡®æ€§ï¼ˆæœ€å…ˆè¿›çš„æ€§èƒ½ï¼‰ã€ç¡®å®šæ€§ï¼ˆé€šè¿‡ä¸€è‡´è¦†ç›–ä¿è¯ï¼‰å’Œå¯è§£é‡Šæ€§ï¼ˆé€šè¿‡åˆ†å¸ƒå¼æ³¨æ„åŠ›å›¾ï¼‰ã€‚PathoGenä»£ç å¯é€šè¿‡GitHubä¾›ç ”ç©¶ç•Œå¼€æ”¾ä½¿ç”¨ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathoGen%E3%80%82">https://github.com/Samiran-Dey/PathoGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00568v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°å…´ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„å¤šæ¨¡æ€èåˆæ•°å­—ç—…ç†ä¸è½¬å½•ç»„ç‰¹å¾å¯æå‡ç™Œç—‡è¯Šæ–­ï¼ˆåˆ†çº§&#x2F;äºšå‹ï¼‰å’Œé¢„åï¼ˆç”Ÿå­˜é£é™©ï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–å¹¶ä¸ç°å®ï¼Œå› ä¸ºç»„ç»‡ç—…ç†å­¦ä»æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè½¬å½•ç»„æµ‹è¯•åœ¨å…¬å…±åŒ»ç–—ä½“ç³»ä¸­å¾ˆå°‘ä½¿ç”¨ã€‚é€šè¿‡æˆ‘ä»¬çš„æ–°å‹æ‰©æ•£åŸºç¡€è·¨æ¨¡æ€ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹PathoGenï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»æ•°å­—ç—…ç†å­¦åˆæˆçš„åŸºå› è¡¨è¾¾èƒ½å¤Ÿç²¾å‡†åœ°è”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©ï¼ŒåŒæ—¶å…·å¤‡é«˜åº¦å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚PathoGenä»£ç å·²å…¬å¼€ä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ï¼Œå¯é€šè¿‡GitHubè®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathoGen">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½å¤šæ¨¡æ€èåˆèƒ½æé«˜ç™Œç—‡è¯Šæ–­ä¸é¢„åé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç›®å‰ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­ä¸ç°å®ã€‚</li>
<li>ç»„ç»‡ç—…ç†å­¦ä»æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè½¬å½•ç»„æµ‹è¯•ä½¿ç”¨è¾ƒå°‘ã€‚</li>
<li>PathoGenæ¨¡å‹å¯ä»æ•°å­—ç—…ç†å­¦åˆæˆåŸºå› è¡¨è¾¾è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>PathoGenæ¨¡å‹é¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©å…·å¤‡é«˜åº¦å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>PathoGenä»£ç å·²å…¬å¼€ï¼Œä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.00568v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.00568v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.00568v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2502.00568v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-Labels-Advancing-Open-Vocabulary-Segmentation-With-Vision-Language-Models"><a href="#Beyond-Labels-Advancing-Open-Vocabulary-Segmentation-With-Vision-Language-Models" class="headerlink" title="Beyond-Labels: Advancing Open-Vocabulary Segmentation With   Vision-Language Models"></a>Beyond-Labels: Advancing Open-Vocabulary Segmentation With   Vision-Language Models</h2><p><strong>Authors:Muhammad Atta ur Rahman</strong></p>
<p>Self-supervised learning can resolve numerous image or linguistic processing problems when effectively trained. This study investigated simple yet efficient methods for adapting previously learned foundation models for open-vocabulary semantic segmentation tasks. Our research proposed â€œBeyond-Labels,â€ a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen image representations with language concepts. This strategy allows the model to successfully actualize enormous knowledge from pretrained models without requiring extensive retraining, making the model data-efficient and scalable. Furthermore, we efficiently captured positional information in images using Fourier embeddings, thus improving the generalization across various image sizes, addressing one of the key limitations of previous methods. Extensive ablation tests were performed to investigate the important components of our proposed method; when tested against the common benchmark PASCAL-5i, it demonstrated superior performance despite being trained on frozen image and language characteristics. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ åœ¨å¾—åˆ°æœ‰æ•ˆçš„è®­ç»ƒåï¼Œå¯ä»¥è§£å†³è®¸å¤šå›¾åƒæˆ–è¯­è¨€å¤„ç†æ–¹é¢çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†é€‚åº”å…ˆå‰å­¦ä¹ çš„åŸºç¡€æ¨¡å‹ç”¨äºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†â€œè¶…è¶Šæ ‡ç­¾â€çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åŸºäºå˜å‹å™¨çš„èåˆæ¨¡å—ï¼Œå®ƒä½¿ç”¨å°‘é‡çš„å›¾åƒåˆ†å‰²æ•°æ®æ¥èåˆå†»ç»“çš„å›¾åƒè¡¨ç¤ºä¸è¯­è¨€æ¦‚å¿µã€‚è¿™ä¸€ç­–ç•¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸éœ€è¦å¤§é‡é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå®ç°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å·¨å¤§çŸ¥è¯†ï¼Œä½¿æ¨¡å‹å…·æœ‰æ•°æ®é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å‚…é‡Œå¶åµŒå…¥æœ‰æ•ˆåœ°æ•è·äº†å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ä¸åŒå›¾åƒå¤§å°ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ï¼Œè§£å†³äº†ä»¥å‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ä¹‹ä¸€ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèè¯•éªŒï¼Œä»¥ç ”ç©¶æˆ‘ä»¬æå‡ºæ–¹æ³•çš„é‡è¦ç»„æˆï¼›åœ¨é’ˆå¯¹é€šç”¨åŸºå‡†PASCAL-5içš„æµ‹è¯•ä¸­ï¼Œå³ä½¿åœ¨å†»ç»“çš„å›¾åƒå’Œè¯­è¨€ç‰¹å¾ä¸Šè¿›è¡Œçš„è®­ç»ƒï¼Œä¹Ÿè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16769v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå°†å…ˆå‰å­¦ä¹ çš„åŸºæœ¬æ¨¡å‹é€‚åº”äºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚ç ”ç©¶æå‡ºäº†â€œè¶…è¶Šæ ‡ç­¾â€çš„è½»é‡çº§transformerèåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å°‘é‡çš„å›¾åƒåˆ†å‰²æ•°æ®å°†å†»ç»“çš„å›¾åƒè¡¨ç¤ºä¸è¯­è¨€æ¦‚å¿µç›¸èåˆã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸éœ€è¦å¤§é‡é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æˆåŠŸå®ç°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å·¨å¤§çŸ¥è¯†ï¼Œä½¿æ¨¡å‹æ›´åŠ é«˜æ•ˆä¸”å¯æ‰©å±•ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨å‚…é‡Œå¶åµŒå…¥æœ‰æ•ˆåœ°æ•è·å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒå›¾åƒå¤§å°ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè§£å†³äº†ä¹‹å‰æ–¹æ³•çš„å±€é™æ€§ä¹‹ä¸€ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèæµ‹è¯•éªŒè¯äº†æ‰€ææ–¹æ³•çš„é‡è¦ç»„ä»¶ï¼Œé’ˆå¯¹å¸¸ç”¨åŸºå‡†PASCAL-5içš„æµ‹è¯•æ˜¾ç¤ºäº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œå³ä½¿åœ¨å†»ç»“çš„å›¾åƒå’Œè¯­è¨€ç‰¹å¾ä¸Šè¿›è¡Œè®­ç»ƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ è§£å†³å›¾åƒæˆ–è¯­è¨€å¤„ç†é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œè¶…è¶Šæ ‡ç­¾â€çš„èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—åŸºäºè½»é‡çº§transformerã€‚</li>
<li>â€œè¶…è¶Šæ ‡ç­¾â€æ–¹æ³•ä½¿ç”¨å°‘é‡å›¾åƒåˆ†å‰²æ•°æ®èåˆå†»ç»“çš„å›¾åƒè¡¨ç¤ºä¸è¯­è¨€æ¦‚å¿µã€‚</li>
<li>æ¨¡å‹æ— éœ€å¤§é‡é‡æ–°è®­ç»ƒå³å¯å®ç°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„çŸ¥è¯†ï¼Œæé«˜æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡å‚…é‡Œå¶åµŒå…¥æ•è·å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œæ”¹è¿›äº†æ¨¡å‹åœ¨ä¸åŒå›¾åƒå¤§å°ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¶ˆèæµ‹è¯•éªŒè¯äº†æ‰€ææ–¹æ³•çš„å…³é”®ç»„ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2501.16769v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2501.16769v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2501.16769v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2501.16769v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_åŒ»å­¦å›¾åƒ/2501.16769v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_TTS/2502.05236v1/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time   Scaling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Diffusion Models/2412.12771v2/page_3_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  A Large-scale AI-generated Image Inpainting Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
