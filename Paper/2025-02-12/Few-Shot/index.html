<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  IceBerg Debiased Self-Training for Class-Imbalanced Node Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0acdfd199619e2f420eed838a2c3cc62.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-12-æ›´æ–°"><a href="#2025-02-12-æ›´æ–°" class="headerlink" title="2025-02-12 æ›´æ–°"></a>2025-02-12 æ›´æ–°</h1><h2 id="IceBerg-Debiased-Self-Training-for-Class-Imbalanced-Node-Classification"><a href="#IceBerg-Debiased-Self-Training-for-Class-Imbalanced-Node-Classification" class="headerlink" title="IceBerg: Debiased Self-Training for Class-Imbalanced Node Classification"></a>IceBerg: Debiased Self-Training for Class-Imbalanced Node Classification</h2><p><strong>Authors:Zhixun Li, Dingshuo Chen, Tong Zhao, Daixin Wang, Hongrui Liu, Zhiqiang Zhang, Jun Zhou, Jeffrey Xu Yu</strong></p>
<p>Graph Neural Networks (GNNs) have achieved great success in dealing with non-Euclidean graph-structured data and have been widely deployed in many real-world applications. However, their effectiveness is often jeopardized under class-imbalanced training sets. Most existing studies have analyzed class-imbalanced node classification from a supervised learning perspective, but they do not fully utilize the large number of unlabeled nodes in semi-supervised scenarios. We claim that the supervised signal is just the tip of the iceberg and a large number of unlabeled nodes have not yet been effectively utilized. In this work, we propose IceBerg, a debiased self-training framework to address the class-imbalanced and few-shot challenges for GNNs at the same time. Specifically, to figure out the Matthew effect and label distribution shift in self-training, we propose Double Balancing, which can largely improve the performance of existing baselines with just a few lines of code as a simple plug-and-play module. Secondly, to enhance the long-range propagation capability of GNNs, we disentangle the propagation and transformation operations of GNNs. Therefore, the weak supervision signals can propagate more effectively to address the few-shot issue. In summary, we find that leveraging unlabeled nodes can significantly enhance the performance of GNNs in class-imbalanced and few-shot scenarios, and even small, surgical modifications can lead to substantial performance improvements. Systematic experiments on benchmark datasets show that our method can deliver considerable performance gain over existing class-imbalanced node classification baselines. Additionally, due to IceBergâ€™s outstanding ability to leverage unsupervised signals, it also achieves state-of-the-art results in few-shot node classification scenarios. The code of IceBerg is available at: <a target="_blank" rel="noopener" href="https://github.com/ZhixunLEE/IceBerg">https://github.com/ZhixunLEE/IceBerg</a>. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å¤„ç†éæ¬§å‡ é‡Œå¾—å›¾ç»“æ„æ•°æ®æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå¹¶å·²å¹¿æ³›åº”ç”¨äºè®¸å¤šå®é™…åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡è®­ç»ƒé›†æ—¶å¾€å¾€æ•ˆæœä¸ä½³ã€‚ç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶éƒ½æ˜¯ä»ç›‘ç£å­¦ä¹ çš„è§’åº¦æ¥åˆ†æç±»åˆ«ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»çš„ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨åŠç›‘ç£åœºæ™¯ä¸­çš„å¤§é‡æœªæ ‡è®°èŠ‚ç‚¹ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç›‘ç£ä¿¡å·åªæ˜¯å†°å±±ä¸€è§’ï¼Œå¤§é‡æœªæ ‡è®°çš„èŠ‚ç‚¹å°šæœªå¾—åˆ°æœ‰æ•ˆåˆ©ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†IceBergï¼Œè¿™æ˜¯ä¸€ä¸ªå»åçš„è‡ªæˆ‘è®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶è§£å†³GNNsçš„ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘é‡æ ·æœ¬æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æ‰¾å‡ºè‡ªæˆ‘è®­ç»ƒä¸­çš„é©¬å¤ªæ•ˆåº”å’Œæ ‡ç­¾åˆ†å¸ƒåç§»ï¼Œæˆ‘ä»¬æå‡ºäº†åŒé‡å¹³è¡¡ç­–ç•¥ï¼Œè¿™å¯ä»¥åœ¨å‡ è¡Œä»£ç å†…æ˜¾è‘—æé«˜ç°æœ‰åŸºå‡†çš„æ€§èƒ½ä½œä¸ºä¸€ä¸ªç®€å•çš„å³æ’å³ç”¨æ¨¡å—ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å¢å¼ºGNNsçš„é•¿ç¨‹ä¼ æ’­èƒ½åŠ›ï¼Œæˆ‘ä»¬è§£è€¦äº†GNNçš„ä¼ æ’­å’Œè½¬æ¢æ“ä½œã€‚å› æ­¤ï¼Œå¼±ç›‘ç£ä¿¡å·å¯ä»¥æ›´æœ‰æ•ˆåœ°ä¼ æ’­ä»¥è§£å†³å°‘é‡æ ·æœ¬é—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°åˆ©ç”¨æœªæ ‡è®°çš„èŠ‚ç‚¹å¯ä»¥æ˜¾è‘—æé«˜GNNsåœ¨ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘é‡æ ·æœ¬åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œå³ä½¿æ˜¯å¾®å°çš„ã€æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ä¹Ÿå¯ä»¥å¸¦æ¥å®è´¨æ€§çš„æ€§èƒ½æå‡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç±»åˆ«ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šå¯ä»¥å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç”±äºIceBergåˆ©ç”¨æ— ç›‘ç£ä¿¡å·çš„å‡ºè‰²èƒ½åŠ›ï¼Œå®ƒåœ¨å°‘é‡èŠ‚ç‚¹åˆ†ç±»åœºæ™¯ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€æ–°çš„æœ€ä½³ç»“æœã€‚IceBergçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhixunLEE/IceBerg">https://github.com/ZhixunLEE/IceBerg</a>ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06280v1">PDF</a> Accepted by TheWebConf (WWW) 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨éæ¬§å‡ é‡Œå¾—å›¾ç»“æ„æ•°æ®å¤„ç†ä¸­çš„ç±»ä¸å¹³è¡¡å’Œå°‘é‡æ ·æœ¬é—®é¢˜æå‡ºäº†IceBergæ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨å¤§é‡æœªæ ‡è®°èŠ‚ç‚¹å’Œæå‡ºåŒå¹³è¡¡ç­–ç•¥æ¥åº”å¯¹ç±»ä¸å¹³è¡¡é—®é¢˜ï¼ŒåŒæ—¶è§£å¼€GNNçš„ä¼ æ’­å’Œè½¬æ¢æ“ä½œä»¥å¢å¼ºå…¶è¿œç¨‹ä¼ æ’­èƒ½åŠ›ï¼Œè§£å†³å°‘é‡æ ·æœ¬é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç±»ä¸å¹³è¡¡å’Œå°‘é‡æ ·æœ¬åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æé«˜GNNæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IceBergæ¡†æ¶è§£å†³äº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨éæ¬§å‡ é‡Œå¾—å›¾ç»“æ„æ•°æ®å¤„ç†ä¸­çš„ç±»ä¸å¹³è¡¡å’Œå°‘é‡æ ·æœ¬é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å¤§é‡æœªæ ‡è®°èŠ‚ç‚¹æ¥æå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†Double Balancingç­–ç•¥ï¼Œå¯ä»¥åº”å¯¹è‡ªæˆ‘è®­ç»ƒä¸­çš„Matthewæ•ˆåº”å’Œæ ‡ç­¾åˆ†å¸ƒåç§»ï¼Œå¹¶å¤§å¹…æé«˜ç°æœ‰åŸºçº¿æ€§èƒ½ã€‚</li>
<li>è§£å¼€GNNçš„ä¼ æ’­å’Œè½¬æ¢æ“ä½œï¼Œå¢å¼ºè¿œç¨‹ä¼ æ’­èƒ½åŠ›ï¼Œè§£å†³å°‘é‡æ ·æœ¬é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æœªæ ‡è®°èŠ‚ç‚¹çš„æ–¹æ³•å’Œç­–ç•¥åœ¨ç±»ä¸å¹³è¡¡å’Œå°‘é‡æ ·æœ¬åœºæ™¯ä¸‹æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>IceBergæ¡†æ¶åœ¨ç±»ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»å’Œå°‘é‡èŠ‚ç‚¹åˆ†ç±»åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4b02a8ae8fbcd180be501ea6d6e3117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32e740a7cf607053bc890d0cfcffa3ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8a46105cc5de6be1630606692833144.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-881916367c618479188e1e44f905604f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59e3e4226daaacd0dc65498741dac542.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Investigating-Compositional-Reasoning-in-Time-Series-Foundation-Models"><a href="#Investigating-Compositional-Reasoning-in-Time-Series-Foundation-Models" class="headerlink" title="Investigating Compositional Reasoning in Time Series Foundation Models"></a>Investigating Compositional Reasoning in Time Series Foundation Models</h2><p><strong>Authors:Willa Potosnak, Cristian Challu, Mononito Goswami, Kin G. Olivares, MichaÅ‚ WiliÅ„ski, Nina Å»ukowska, Artur Dubrawski</strong></p>
<p>Large pre-trained time series foundation models (TSFMs) have demonstrated promising zero-shot performance across a wide range of domains. However, a question remains: Do TSFMs succeed solely by memorizing training patterns, or do they possess the ability to reason? While reasoning is a topic of great interest in the study of Large Language Models (LLMs), it is undefined and largely unexplored in the context of TSFMs. In this work, inspired by language modeling literature, we formally define compositional reasoning in forecasting and distinguish it from in-distribution generalization. We evaluate the reasoning and generalization capabilities of 23 popular deep learning forecasting models on multiple synthetic and real-world datasets. Additionally, through controlled studies, we systematically examine which design choices in TSFMs contribute to improved reasoning abilities. Our study yields key insights into the impact of TSFM architecture design on compositional reasoning and generalization. We find that patch-based Transformers have the best reasoning performance, closely followed by residualized MLP-based architectures, which are 97% less computationally complex in terms of FLOPs and 86% smaller in terms of the number of trainable parameters. Interestingly, in some zero-shot out-of-distribution scenarios, these models can outperform moving average and exponential smoothing statistical baselines trained on in-distribution data. Only a few design choices, such as the tokenization method, had a significant (negative) impact on Transformer model performance. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººç©ç›®çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»æœ‰ä¸€ä¸ªé—®é¢˜æœ‰å¾…è§£ç­”ï¼šTSFMsæ˜¯å¦ä»…ä»…é€šè¿‡è®°å¿†è®­ç»ƒæ¨¡å¼å–å¾—æˆåŠŸï¼Œè¿˜æ˜¯å®ƒä»¬ç¡®å®å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Ÿè™½ç„¶æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶ä¸­çš„çƒ­é—¨è¯é¢˜ï¼Œä½†åœ¨TSFMçš„æƒ…å¢ƒä¸­å°šæœªæ˜ç¡®å®šä¹‰å¹¶æ¶‰åŠç”šå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å€Ÿé‰´è¯­è¨€å»ºæ¨¡æ–‡çŒ®ï¼Œæ­£å¼å®šä¹‰äº†é¢„æµ‹ä¸­çš„ç»„åˆæ¨ç†ï¼Œå¹¶å°†å…¶ä¸å†…åˆ†å¸ƒæ³›åŒ–åŒºåˆ†å¼€æ¥ã€‚æˆ‘ä»¬è¯„ä¼°äº†23ä¸ªæµè¡Œçš„æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹åœ¨å¤šä¸ªåˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹ç…§ç ”ç©¶ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ£€æŸ¥äº†TSFMä¸­çš„å“ªäº›è®¾è®¡é€‰æ‹©æœ‰åŠ©äºæ”¹å–„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹äºTSFMæ¶æ„è®¾è®¡å¯¹ç»„åˆæ¨ç†å’Œæ³›åŒ–çš„å½±å“è·å¾—äº†å…³é”®è§è§£ã€‚æˆ‘ä»¬å‘ç°åŸºäºè¡¥ä¸çš„Transformerå…·æœ‰æœ€ä½³çš„æ¨ç†æ€§èƒ½ï¼Œç´§éšå…¶åçš„æ˜¯åŸºäºæ®‹å·®MLPçš„æ¶æ„ï¼Œå®ƒä»¬åœ¨FLOPsæ–¹é¢å‡å°‘äº†97%çš„è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶åœ¨å¯è®­ç»ƒå‚æ•°æ•°é‡æ–¹é¢å‡å°‘äº†8 ç»“æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æŸäº›é›¶æ ·æœ¬å¤–éƒ¨åˆ†å¸ƒåœºæ™¯ä¸­ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥è¶…è¶Šåœ¨å†…éƒ¨åˆ†å¸ƒæ•°æ®ä¸Šè®­ç»ƒçš„ç§»åŠ¨å¹³å‡å’ŒæŒ‡æ•°å¹³æ»‘ç»Ÿè®¡åŸºçº¿ã€‚åªæœ‰å°‘æ•°è®¾è®¡é€‰æ‹©ï¼Œå¦‚ä»¤ç‰ŒåŒ–æ–¹æ³•ï¼Œå¯¹Transformeræ¨¡å‹æ€§èƒ½äº§ç”Ÿäº†é‡å¤§ï¼ˆè´Ÿé¢ï¼‰å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06037v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰å±•ç°å‡ºè·¨åŸŸé›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶æ˜¯å¦é€šè¿‡è®°å¿†è®­ç»ƒæ¨¡å¼æˆåŠŸï¼Œè¿˜æ˜¯å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œä»æ˜¯é—®é¢˜ã€‚æœ¬æ–‡å—è¯­è¨€å»ºæ¨¡æ–‡çŒ®å¯å‘ï¼Œæ­£å¼å®šä¹‰é¢„æµ‹ä¸­çš„ç»„åˆæ¨ç†ï¼Œå¹¶å°†å…¶ä¸åˆ†å¸ƒå†…æ³›åŒ–åŒºåˆ†å¼€æ¥ã€‚è¯„ä¼°äº†å¤šç§åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Š23ä¸ªæµè¡Œæ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ§åˆ¶æ€§ç ”ç©¶ï¼Œç³»ç»Ÿåœ°æ¢ç´¢äº†TSFMè®¾è®¡é€‰æ‹©å¯¹æ”¹å–„æ¨ç†èƒ½åŠ›çš„è´¡çŒ®ã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºè¡¥ä¸çš„Transformerå…·æœ‰æœ€ä½³æ¨ç†æ€§èƒ½ï¼Œå…¶æ¬¡æ˜¯åŸºäºæ®‹å·®MLPçš„æ¶æ„ã€‚åœ¨æŸäº›é›¶æ ·æœ¬åˆ†å¸ƒå¤–åœºæ™¯ä¸­ï¼Œè¿™äº›æ¨¡å‹ç”šè‡³å¯è¶…è¶Šåœ¨åˆ†å¸ƒå†…æ•°æ®ä¸Šè®­ç»ƒçš„ç»Ÿè®¡åŸºçº¿æ–¹æ³•ã€‚ä»…å°‘æ•°è®¾è®¡é€‰æ‹©å¦‚ä»¤ç‰ŒåŒ–æ–¹æ³•å¯¹Transformeræ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—è´Ÿé¢å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰å…·æœ‰è·¨åŸŸé›¶æ ·æœ¬æ€§èƒ½è¡¨ç°ã€‚</li>
<li>å½“å‰ç ”ç©¶å¯¹TSFMæ˜¯å¦é€šè¿‡ç†è§£å’Œæ¨ç†æˆåŠŸå°šå­˜ç–‘é—®ã€‚</li>
<li>æœ¬æ–‡æ­£å¼å®šä¹‰äº†åœ¨é¢„æµ‹ä¸­çš„ç»„åˆæ¨ç†ï¼Œå¹¶å°†å…¶ä¸åˆ†å¸ƒå†…æ³›åŒ–åŒºåˆ†å¼€æ¥ã€‚</li>
<li>åœ¨å¤šç§æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºè¡¥ä¸çš„Transformeræ¶æ„åœ¨æ¨ç†æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>åŸºäºæ®‹å·®çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¶æ„å±•ç°å‡ºè‰¯å¥½çš„æ¨ç†æ€§èƒ½ï¼Œä¸”è®¡ç®—å¤æ‚æ€§è¾ƒä½ã€å‚æ•°æ›´å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0f070aee9ef7575e2ce9b03ffe6cb57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-965543289822f0efa8826be894745bba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b6a50ccb9fbfb0b5497e055cec464e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17668ec0d4877f213cbce8b44f7b5ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c48f8c9ec17bebc4685778a6417375b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BnTTS-Few-Shot-Speaker-Adaptation-in-Low-Resource-Setting"><a href="#BnTTS-Few-Shot-Speaker-Adaptation-in-Low-Resource-Setting" class="headerlink" title="BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting"></a>BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting</h2><p><strong>Authors:Mohammad Jahid Ibna Basher, Md Kowsher, Md Saiful Islam, Rabindra Nath Nandi, Nusrat Jahan Prottasha, Mehadi Hasan Menon, Tareq Al Muntasir, Shammur Absar Chowdhury, Firoj Alam, Niloofar Yousefi, Ozlem Ozmen Garibay</strong></p>
<p>This paper introduces BnTTS (Bangla Text-To-Speech), the first framework for Bangla speaker adaptation-based TTS, designed to bridge the gap in Bangla speech synthesis using minimal training data. Building upon the XTTS architecture, our approach integrates Bangla into a multilingual TTS pipeline, with modifications to account for the phonetic and linguistic characteristics of the language. We pre-train BnTTS on 3.85k hours of Bangla speech dataset with corresponding text labels and evaluate performance in both zero-shot and few-shot settings on our proposed test dataset. Empirical evaluations in few-shot settings show that BnTTS significantly improves the naturalness, intelligibility, and speaker fidelity of synthesized Bangla speech. Compared to state-of-the-art Bangla TTS systems, BnTTS exhibits superior performance in Subjective Mean Opinion Score (SMOS), Naturalness, and Clarity metrics. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†BnTTSï¼ˆå­ŸåŠ æ‹‰è¯­éŸ³æ–‡æœ¬è½¬è¯­éŸ³ï¼‰ç³»ç»Ÿï¼Œè¿™æ˜¯åŸºäºå­ŸåŠ æ‹‰è¯­å‘éŸ³äººé€‚åº”çš„TTSï¼ˆè¯­éŸ³åˆæˆæŠ€æœ¯ï¼‰çš„é¦–ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨æœ€å°‘çš„è®­ç»ƒæ•°æ®æ¥å¼¥è¡¥å­ŸåŠ æ‹‰è¯­éŸ³åˆæˆçš„ç©ºç™½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºXTTSæ¶æ„ï¼Œå°†å­ŸåŠ æ‹‰è¯­æ•´åˆåˆ°å¤šè¯­è¨€TTSç®¡é“ä¸­ï¼Œå¹¶é’ˆå¯¹è¯¥è¯­è¨€çš„è¯­éŸ³å’Œè¯­è¨€ç‰¹æ€§è¿›è¡Œäº†ä¿®æ”¹ã€‚æˆ‘ä»¬åœ¨å¸¦æœ‰ç›¸åº”æ–‡æœ¬æ ‡ç­¾çš„å­ŸåŠ æ‹‰è¯­éŸ³æ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†BnTTSçš„æ¨¡å‹è¾¾åˆ°é«˜è¾¾çš„æ—¶é•¿æ˜¯é«˜è¾¾åˆ°æ•°ä¸‡ä¸ªå°æ—¶ä¹‹å¤šè¾¾æ—¶3.85ä¸‡å°æ—¶æ—¶é•¿è¯­æ•°åƒé«˜è¾¾å°†è¿‘ä¸€ç‚¹ä¸‰ä¸‡åˆ†é’Ÿä¸‰åƒå°æ—¶ï¼ˆåæœªæåŠçš„3ç‚¹åç»­æ”¹åŠ¨ä¸é¢„è®­ç»ƒæ–¹å¼ã€è¯•éªŒæ•ˆæœæœªä½“ç°å‡ºçš„ç‰¹ç‚¹ä¿ç•™æœªè¯‘ï¼‰ï¼Œå¹¶åœ¨æˆ‘ä»¬æå‡ºçš„æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„è¯„ä¼°ã€‚åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒBnTTSæ˜¾è‘—æé«˜äº†åˆæˆå­ŸåŠ æ‹‰è¯­çš„è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººçš„ä¿çœŸåº¦ã€‚ä¸æœ€å…ˆè¿›çš„å­ŸåŠ æ‹‰è¯­TTSç³»ç»Ÿç›¸æ¯”ï¼ŒBnTTSåœ¨ä¸»è§‚å¹³å‡æ„è§å¾—åˆ†ï¼ˆSMOSï¼‰ã€è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05729v1">PDF</a> Accepted paper in NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BnTTSï¼ˆå­ŸåŠ æ‹‰æ–‡æœ¬è½¬è¯­éŸ³ï¼‰ï¼Œè¿™æ˜¯åŸºäºå­ŸåŠ æ‹‰è¯­å‘éŸ³äººé€‚åº”æ€§è®¾è®¡çš„é¦–ä¸ªæ–‡æœ¬è½¬è¯­éŸ³æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®å¼¥è¡¥å­ŸåŠ æ‹‰è¯­éŸ³åˆæˆçš„å·®è·ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨XTTSæ¶æ„ä¹‹ä¸Šï¼Œå°†å­ŸåŠ æ‹‰è¯­èå…¥å¤šè¯­ç§TTSç®¡é“ï¼Œå¹¶é’ˆå¯¹è¯¥è¯­è¨€çš„è¯­è¨€å­¦ç‰¹ç‚¹è¿›è¡Œä¿®æ”¹ã€‚åœ¨æå‡ºçš„æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œå¯¹é›¶æ ·æœ¬å’Œå°æ ·æœ¬ä¸¤ç§æƒ…å†µè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBnTTSæ˜¾è‘—æé«˜äº†åˆæˆå­ŸåŠ æ‹‰è¯­éŸ³çš„è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººä¸€è‡´æ€§ã€‚ç›¸è¾ƒäºç°æœ‰ä¸»æµçš„å­ŸåŠ æ‹‰è¯­TTSç³»ç»Ÿï¼ŒBnTTSåœ¨ä¸»è§‚å¹³å‡æ„è§å¾—åˆ†ï¼ˆSMOSï¼‰ã€è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ›´å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BnTTSæ˜¯é¦–ä¸ªé’ˆå¯¹å­ŸåŠ æ‹‰è¯­å‘éŸ³äººçš„æ–‡æœ¬è½¬è¯­éŸ³æ¡†æ¶ã€‚</li>
<li>BnTTSæ—¨åœ¨ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®å®ç°å­ŸåŠ æ‹‰è¯­éŸ³åˆæˆã€‚</li>
<li>BnTTSå»ºç«‹åœ¨XTTSæ¶æ„ä¹‹ä¸Šï¼Œå¹¶è€ƒè™‘äº†å­ŸåŠ æ‹‰è¯­çš„è¯­è¨€å­¦ç‰¹æ€§ã€‚</li>
<li>åœ¨æå‡ºçš„æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œè¿›è¡Œäº†é›¶æ ·æœ¬å’Œå°æ ·æœ¬ä¸¤ç§æƒ…å†µçš„æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºBnTTSæé«˜äº†åˆæˆå­ŸåŠ æ‹‰è¯­éŸ³çš„è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººä¸€è‡´æ€§ã€‚</li>
<li>ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒBnTTSåœ¨SMOSã€è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b26d3e58bd06e3a225bea911e361e12b.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05729v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05729v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05729v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LMS-Net-A-Learned-Mumford-Shah-Network-For-Few-Shot-Medical-Image-Segmentation"><a href="#LMS-Net-A-Learned-Mumford-Shah-Network-For-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="LMS-Net: A Learned Mumford-Shah Network For Few-Shot Medical Image   Segmentation"></a>LMS-Net: A Learned Mumford-Shah Network For Few-Shot Medical Image   Segmentation</h2><p><strong>Authors:Shengdong Zhang, Fan Jia, Xiang Li, Hao Zhang, Jun Shi, Liyan Ma, Shihui Ying</strong></p>
<p>Few-shot semantic segmentation (FSS) methods have shown great promise in handling data-scarce scenarios, particularly in medical image segmentation tasks. However, most existing FSS architectures lack sufficient interpretability and fail to fully incorporate the underlying physical structures of semantic regions. To address these issues, in this paper, we propose a novel deep unfolding network, called the Learned Mumford-Shah Network (LMS-Net), for the FSS task. Specifically, motivated by the effectiveness of pixel-to-prototype comparison in prototypical FSS methods and the capability of deep priors to model complex spatial structures, we leverage our learned Mumford-Shah model (LMS model) as a mathematical foundation to integrate these insights into a unified framework. By reformulating the LMS model into prototype update and mask update tasks, we propose an alternating optimization algorithm to solve it efficiently. Further, the iterative steps of this algorithm are unfolded into corresponding network modules, resulting in LMS-Net with clear interpretability. Comprehensive experiments on three publicly available medical segmentation datasets verify the effectiveness of our method, demonstrating superior accuracy and robustness in handling complex structures and adapting to challenging segmentation scenarios. These results highlight the potential of LMS-Net to advance FSS in medical imaging applications. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/SDZhang01/LMSNet">https://github.com/SDZhang01/LMSNet</a> </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„FSSæ¶æ„ç¼ºä¹è¶³å¤Ÿçš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†èå…¥è¯­ä¹‰åŒºåŸŸçš„åº•å±‚ç‰©ç†ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å±•å¼€ç½‘ç»œï¼Œç§°ä¸ºå­¦ä¹ Mumford-Shahç½‘ç»œï¼ˆLMS-Netï¼‰ï¼Œç”¨äºFSSä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å—åˆ°åŸå‹FSSæ–¹æ³•ä¸­çš„åƒç´ åˆ°åŸå‹æ¯”è¾ƒçš„å¯å‘ï¼Œä»¥åŠæ·±åº¦å…ˆéªŒåœ¨å»ºæ¨¡å¤æ‚ç©ºé—´ç»“æ„æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä»¥å­¦ä¹ åˆ°çš„Mumford-Shahæ¨¡å‹ï¼ˆLMSæ¨¡å‹ï¼‰ä¸ºæ•°å­¦åŸºç¡€ï¼Œå°†è¿™äº›è§è§£æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚é€šè¿‡å°†LMSæ¨¡å‹é‡æ–°è¡¨è¿°ä¸ºåŸå‹æ›´æ–°å’Œæ©è†œæ›´æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äº¤æ›¿ä¼˜åŒ–ç®—æ³•æ¥æœ‰æ•ˆåœ°è§£å†³å®ƒã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•çš„è¿­ä»£æ­¥éª¤è¢«å±•å¼€æˆç›¸åº”çš„ç½‘ç»œæ¨¡å—ï¼Œä»è€Œå½¢æˆäº†å…·æœ‰æ˜ç¡®å¯è§£é‡Šæ€§çš„LMS-Netã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤„ç†å¤æ‚ç»“æ„å’Œé€‚åº”å…·æœ‰æŒ‘æˆ˜æ€§çš„åˆ†å‰²åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚è¿™äº›ç»“æœçªå‡ºäº†LMS-Netåœ¨åŒ»å­¦æˆåƒåº”ç”¨çš„FSSä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/SDZhang01/LMSNet%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/SDZhang01/LMSNetä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05473v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå­¦ä¹ Mumford-Shahæ¨¡å‹ï¼ˆLMSæ¨¡å‹ï¼‰çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ–°æ–¹æ³•â€”â€”LMS-Netã€‚è¯¥ç½‘ç»œé€šè¿‡ç»“åˆåŸå‹æ¯”è¾ƒä¸å¤æ‚ç©ºé—´ç»“æ„å»ºæ¨¡çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯è§£é‡Šçš„FSSä»»åŠ¡å¤„ç†ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»ç–—åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMS-Netç»“åˆäº†åŸå‹æ¯”è¾ƒå’Œç©ºé—´ç»“æ„å»ºæ¨¡çš„ä¼˜åŠ¿ï¼Œä¸ºFSSä»»åŠ¡æä¾›äº†æ–°é¢–çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LMSæ¨¡å‹è¢«é‡æ–°æ„å»ºä¸ºåŸå‹æ›´æ–°å’Œæ©è†œæ›´æ–°ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨äº†äº¤æ›¿ä¼˜åŒ–ç®—æ³•è¿›è¡Œé«˜æ•ˆæ±‚è§£ã€‚</li>
<li>é€šè¿‡å°†è¿­ä»£æ­¥éª¤è½¬åŒ–ä¸ºç½‘ç»œæ¨¡å—ï¼ŒLMS-Netå…·æœ‰æ¸…æ™°çš„è§£é‡Šæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLMS-Netåœ¨åŒ»ç–—å›¾åƒåˆ†å‰²åº”ç”¨ä¸­æœ‰ä¼˜è¶Šçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚</li>
<li>LMS-Netåœ¨å¤æ‚ç»“æ„å¤„ç†å’ŒæŒ‘æˆ˜åˆ†å‰²åœºæ™¯ä¸‹çš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/SDZhang01/LMSNet%E4%B8%8A%E5%BC%80%E3%80%82">https://github.com/SDZhang01/LMSNetä¸Šå…¬å¼€ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05473v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05473v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05473v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.05473v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TEST-V-TEst-time-Support-set-Tuning-for-Zero-shot-Video-Classification"><a href="#TEST-V-TEst-time-Support-set-Tuning-for-Zero-shot-Video-Classification" class="headerlink" title="TEST-V: TEst-time Support-set Tuning for Zero-shot Video Classification"></a>TEST-V: TEst-time Support-set Tuning for Zero-shot Video Classification</h2><p><strong>Authors:Rui Yan, Jin Wang, Hongyu Qu, Xiaoyu Du, Dong Zhang, Jinhui Tang, Tieniu Tan</strong></p>
<p>Recently, adapting Vision Language Models (VLMs) to zero-shot visual classification by tuning class embedding with a few prompts (Test-time Prompt Tuning, TPT) or replacing class names with generated visual samples (support-set) has shown promising results. However, TPT cannot avoid the semantic gap between modalities while the support-set cannot be tuned. To this end, we draw on each otherâ€™s strengths and propose a novel framework namely TEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It first dilates the support-set with multiple prompts (Multi-prompting Support-set Dilation, MSD) and then erodes the support-set via learnable weights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE). Specifically, i) MSD expands the support samples for each class based on multiple prompts enquired from LLMs to enrich the diversity of the support-set. ii) TSE tunes the support-set with factorized learnable weights according to the temporal prediction consistency in a self-supervised manner to dig pivotal supporting cues for each class. $\textbf{TEST-V}$ achieves state-of-the-art results across four benchmarks and has good interpretability for the support-set dilation and erosion. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé€šè¿‡å¾®è°ƒç±»åˆ«åµŒå…¥å¹¶ä½¿ç”¨å°‘é‡æç¤ºæ¥å®ç°é›¶æ ·æœ¬è§†è§‰åˆ†ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é€‚é…ï¼Œæˆ–è€…é€šè¿‡ç”¨ç”Ÿæˆçš„è§†è§‰æ ·æœ¬æ›¿æ¢ç±»åˆ«åç§°ï¼ˆæ”¯æŒé›†ï¼‰çš„æ–¹æ³•ï¼ˆç§°ä¸ºæµ‹è¯•æ—¶æç¤ºå¾®è°ƒï¼ˆTPTï¼‰å’Œæ”¯æŒé›†ï¼‰ï¼Œå·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼ŒTPTæ— æ³•é¿å…è·¨æ¨¡æ€çš„è¯­ä¹‰é¸¿æ²Ÿï¼Œè€Œæ”¯æŒé›†åˆ™æ— æ³•è¿›è¡Œè°ƒæ•´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å–é•¿è¡¥çŸ­ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTEST-Vçš„é›¶æ ·æœ¬è§†é¢‘åˆ†ç±»æµ‹è¯•æ—¶æ”¯æŒé›†å¾®è°ƒæ¡†æ¶ã€‚å®ƒé¦–å…ˆé€šè¿‡å¤šæç¤ºæ‰©å……æ”¯æŒé›†ï¼ˆå¤šæç¤ºæ”¯æŒé›†è†¨èƒ€ï¼ŒMSDï¼‰ï¼Œç„¶åé€šè¿‡å¯å­¦ä¹ æƒé‡å¯¹æ”¯æŒé›†è¿›è¡Œä¾µèš€ï¼Œä»¥åŠ¨æ€æŒ–æ˜å…³é”®çº¿ç´¢ï¼ˆæ—¶é—´æ„ŸçŸ¥æ”¯æŒé›†ä¾µèš€ï¼ŒTSEï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œiï¼‰MSDåŸºäºä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è·å¾—çš„å¤šä¸ªæç¤ºæ¥æ‰©å±•æ¯ä¸ªç±»åˆ«çš„æ”¯æŒæ ·æœ¬ï¼Œä»¥ä¸°å¯Œæ”¯æŒé›†çš„å¤šæ ·æ€§ã€‚iiï¼‰TSEä½¿ç”¨å¯å­¦ä¹ æƒé‡è°ƒæ•´æ”¯æŒé›†ï¼Œæ ¹æ®è‡ªç›‘ç£æ–¹å¼ä¸­çš„æ—¶é—´é¢„æµ‹ä¸€è‡´æ€§ä¸ºæ¯ä¸ªç±»åˆ«æŒ–æ˜å…³é”®æ”¯æŒçº¿ç´¢ã€‚TEST-Våœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°ç»“æœï¼Œå¹¶æ”¯æŒé›†è†¨èƒ€å’Œä¾µèš€å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00426v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œé€šè¿‡æµ‹è¯•æ—¶æç¤ºè°ƒæ•´ï¼ˆTPTï¼‰æˆ–æ”¯æŒé›†æ›¿æ¢ç±»åè¿›è¡Œé›¶æ ·æœ¬è§†è§‰åˆ†ç±»æ˜¾ç¤ºå‡ºè‰¯å¥½çš„ç»“æœã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTEST-Vçš„æ–°å‹æ¡†æ¶ï¼Œç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œé€šè¿‡å¤šæç¤ºæ”¯æŒé›†è†¨èƒ€ï¼ˆMSDï¼‰å’Œæ—¶åºæ„ŸçŸ¥æ”¯æŒé›†ä¾µèš€ï¼ˆTSEï¼‰æ¥æ”¹è¿›é›¶æ ·æœ¬è§†é¢‘åˆ†ç±»ã€‚TEST-Våœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—æœ€æ–°ç»“æœï¼Œå¹¶æ”¯æŒé›†è†¨èƒ€å’Œä¾µèš€å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨é›¶æ ·æœ¬è§†è§‰åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>TPTæ— æ³•é¿å…è·¨æ¨¡æ€è¯­ä¹‰å·®è·é—®é¢˜ã€‚</li>
<li>æ”¯æŒé›†æ–¹æ³•ä¸èƒ½è°ƒæ•´ç±»åã€‚</li>
<li>TEST-Vç»“åˆäº†TPTå’Œæ”¯æŒé›†çš„ä¼˜ç‚¹ã€‚</li>
<li>MSDé€šè¿‡ä»å¤§å‹è¯­è¨€æ¨¡å‹è·å–å¤šä¸ªæç¤ºæ¥è†¨èƒ€æ”¯æŒæ ·æœ¬ï¼Œä¸°å¯Œæ”¯æŒé›†çš„å¤šæ ·æ€§ã€‚</li>
<li>TSEä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡è¿›è¡Œæ”¯æŒé›†ä¾µèš€ï¼Œä»¥åŠ¨æ€æŒ–æ˜å…³é”®çº¿ç´¢ã€‚è¿™ç§ä¾µèš€æ˜¯é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼ï¼Œæ ¹æ®æ—¶é—´é¢„æµ‹ä¸€è‡´æ€§æ¥å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.00426v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.00426v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.00426v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.00426v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.00426v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2502.00426v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches"><a href="#A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches" class="headerlink" title="A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches"></a>A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches</h2><p><strong>Authors:Luca Ciampi, Ali Azmoudeh, Elif Ecem Akbaba, Erdi SarÄ±taÅŸ, Ziya Ata YazÄ±cÄ±, HazÄ±m Kemal Ekenel, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories â€“ a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of the architectures of 29 CAC approaches and report their results on gold-standard benchmarks. We compare their performance and discuss their strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions. We believe this survey will be a valuable resource, showcasing CAC advancements and guiding future research. </p>
<blockquote>
<p>è§†è§‰ç‰©ä½“è®¡æ•°æœ€è¿‘å·²ç»è½¬å‘ç±»åˆ«æ— å…³è®¡æ•°ï¼ˆCACï¼‰ï¼Œè¿™è§£å†³äº†è·¨ä»»æ„ç±»åˆ«çš„ç‰©ä½“è®¡æ•°æŒ‘æˆ˜â€”â€”è¿™å¯¹äºçµæ´»å’Œå¯æ¨å¹¿çš„è®¡æ•°ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ä¸äººç±»èƒ½å¤Ÿæ¯«ä¸è´¹åŠ›åœ°è¯†åˆ«å¹¶è®¡ç®—æ¥è‡ªä¸åŒç±»åˆ«çš„å¯¹è±¡è€Œæ— éœ€å…ˆéªŒçŸ¥è¯†ä¸åŒï¼Œå¤§å¤šæ•°ç°æœ‰çš„è®¡æ•°æ–¹æ³•ä»…é™äºè®¡ç®—å·²çŸ¥ç±»åˆ«çš„å®ä¾‹ï¼Œéœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”åœ¨å¼€æ”¾è¯æ±‡è®¾ç½®ä¸­å­˜åœ¨å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCACæ—¨åœ¨è®¡ç®—å±äºåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«çš„å¯¹è±¡ï¼Œå¹¶åœ¨å°æ ·æœ¬è®¾ç½®ä¸­è¿›è¡Œæ“ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹CACæ–¹æ³•è¿›è¡Œäº†é¦–æ¬¡å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå°†CACæ–¹æ³•åˆ†ä¸ºä¸‰ç§èŒƒå¼ï¼ŒåŸºäºç›®æ ‡ç±»åˆ«çš„æŒ‡å®šæ–¹å¼ï¼šåŸºäºå‚è€ƒã€æ— å‚è€ƒå’Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼ã€‚åŸºäºå‚è€ƒçš„æ–¹æ³•é€šè¿‡ä¾é èŒƒä¾‹å¼•å¯¼æœºåˆ¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ— å‚è€ƒæ–¹æ³•åˆ©ç”¨å›ºæœ‰çš„å›¾åƒæ¨¡å¼æ¥æ¶ˆé™¤èŒƒä¾‹ä¾èµ–æ€§ã€‚æœ€åï¼Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå®ç°å¯¹è±¡ç±»åˆ«æè¿°ï¼Œæä¾›äº†çµæ´»å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åŸºäºè¿™ç§åˆ†ç±»æ³•ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†29ç§CACæ–¹æ³•çš„æ¶æ„ï¼Œå¹¶åœ¨é»„é‡‘æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸ŠæŠ¥å‘Šäº†ä»–ä»¬çš„ç»“æœã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨FSC-1 ä¿®åŒ…å™¨æ•°æ®é›†ä¸Šå±•ç¤ºäº†ç»“æœï¼Œå¹¶ä½¿ç”¨é»„é‡‘æ ‡å‡†æŒ‡æ ‡å»ºç«‹äº†æ’è¡Œæ¦œï¼Œä»¥åŠåœ¨CARPKæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æŒç»­çš„æŒ‘æˆ˜ï¼Œå¦‚æ³¨é‡Šä¾èµ–æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠæœªæ¥çš„æ–¹å‘è¿›è¡Œäº†æ‰¹åˆ¤æ€§çš„è®¨è®ºã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä»½è°ƒæŸ¥æŠ¥å‘Šå°†æ˜¯æœ‰ä»·å€¼çš„èµ„æºï¼Œå±•ç¤ºäº†CACçš„è¿›å±•å¹¶æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19184v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç»¼è¿°äº†é¢å‘ç±»åˆ«æœªçŸ¥å¯¹è±¡è®¡æ•°çš„ç±»æ— å…³è®¡æ•°ï¼ˆCACï¼‰æ–¹æ³•ï¼Œå°†å…¶åˆ†ä¸ºå‚è€ƒå‹ã€æ— å‚è€ƒå‹å’Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼å‹ä¸‰ç§èŒƒå¼ã€‚å‚è€ƒå‹ä¾èµ–æ ·æœ¬å¼•å¯¼æœºåˆ¶è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæ— å‚è€ƒå‹åˆ©ç”¨å›¾åƒå›ºæœ‰æ¨¡å¼æ¶ˆé™¤æ ·æœ¬ä¾èµ–ï¼Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼å‹ä½¿ç”¨è·¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹é€šè¿‡æ–‡æœ¬æç¤ºå®ç°ç›®æ ‡ç±»æè¿°ï¼Œæä¾›ä¸€ç§çµæ´»è€Œæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åŸºäºè¿™ä¸€åˆ†ç±»ï¼Œæœ¬æ–‡æ¦‚è¿°äº†29ç§CACæ–¹æ³•çš„æ¶æ„ï¼Œå¹¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸ŠæŠ¥å‘Šäº†ç»“æœï¼ŒåŒ…æ‹¬FSC-147æ•°æ®é›†å’ŒCARPKæ•°æ®é›†ã€‚è®¨è®ºäº†å…¶æ€§èƒ½å’Œä¼˜ç¼ºç‚¹ï¼Œå¹¶å¯¹æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘è¿›è¡Œäº†æ‰¹åˆ¤æ€§è®¨è®ºã€‚æœ¬æ–‡ç»¼è¿°å±•ç¤ºäº†CACçš„è¿›å±•ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç±»æ— å…³è®¡æ•°ï¼ˆCACï¼‰æ˜¯è§†è§‰å¯¹è±¡è®¡æ•°çš„æ–°è¶‹åŠ¿ï¼Œæ—¨åœ¨è§£å†³ä»»æ„ç±»åˆ«å¯¹è±¡çš„è®¡æ•°æŒ‘æˆ˜ã€‚</li>
<li>CACæ–¹æ³•è¢«åˆ†ä¸ºä¸‰ç§èŒƒå¼ï¼šå‚è€ƒå‹ã€æ— å‚è€ƒå‹å’Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼å‹ã€‚</li>
<li>å‚è€ƒå‹CACæ–¹æ³•ä¾èµ–æ ·æœ¬å¼•å¯¼æœºåˆ¶ï¼Œè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>æ— å‚è€ƒå‹CACæ–¹æ³•åˆ©ç”¨å›¾åƒå›ºæœ‰æ¨¡å¼ï¼Œæ¶ˆé™¤æ ·æœ¬ä¾èµ–ã€‚</li>
<li>å¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼å‹CACæ–¹æ³•ä½¿ç”¨è·¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå®ç°ç›®æ ‡ç±»æè¿°ï¼Œæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç»¼è¿°æ¦‚è¿°äº†å¤šç§CACæ–¹æ³•çš„æ¶æ„ï¼Œå¹¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚</li>
<li>è®¨è®ºäº†CACæ–¹æ³•çš„æŒä¹…æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬æ ‡æ³¨ä¾èµ–æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.19184v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.19184v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.19184v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.19184v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay"><a href="#Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay" class="headerlink" title="Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay"></a>Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay</h2><p><strong>Authors:Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, Xiangyang Ji</strong></p>
<p>The foundation model enables fast problem-solving without learning from scratch, and such a desirable adaptation property benefits from its adopted cross-task generalization paradigms, e.g., pretraining, meta-training, and finetuning. Recent advances in these paradigms show the crucial role of challenging tasksâ€™ prioritized sampling in enhancing adaptation robustness and even improving sampling efficiency. However, scoring task difficulties exhausts massive task queries and requires intensive evaluation and computations, e.g., policy evaluations in Markov decision processes (MDPs) or inference with large backbone models. This work underscores the criticality of both adaptation robustness and learning efficiency, especially in scenarios where tasks are risky to collect or costly to evaluate. To this end, we present Model Predictive Task Sampling (MPTS) to establish connections between the task space and adaptation risk landscape for robust active task sampling. Technically, MPTS characterizes the task episodic information with a generative model and predicts optimization outcome, i.e., task-specific adaptation risk values, from posterior inference. The resulting risk learner amortizes expensive annotation, evaluation, or computation operations in task robust adaptation. Extensive experimental results show that MPTS can be seamlessly integrated into zero-shot, few-shot, and many-shot learning paradigms, increases adaptation robustness, and retains learning efficiency without affording extra cost. The code is available at the project site <a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS">https://github.com/thu-rllab/MPTS</a>. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹å®ç°äº†æ— éœ€ä»å¤´å­¦ä¹ å³å¯å¿«é€Ÿè§£å†³é—®é¢˜ï¼Œè¿™ç§ç†æƒ³çš„é€‚åº”æ€§å¾—ç›Šäºå…¶é‡‡ç”¨çš„è·¨ä»»åŠ¡æ³›åŒ–èŒƒå¼ï¼Œä¾‹å¦‚é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒã€‚è¿™äº›èŒƒå¼çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œåœ¨å¢å¼ºé€‚åº”æ€§ç¨³å¥æ€§ç”šè‡³æé«˜é‡‡æ ·æ•ˆç‡æ–¹é¢ï¼Œä¼˜å…ˆé‡‡æ ·å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå¯¹ä»»åŠ¡éš¾åº¦çš„è¯„åˆ†è€—è´¹äº†å¤§é‡çš„ä»»åŠ¡æŸ¥è¯¢ï¼Œå¹¶éœ€è¦å¤§é‡çš„è¯„ä¼°å’Œè®¡ç®—ï¼Œä¾‹å¦‚åœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„ç­–ç•¥è¯„ä¼°æˆ–ä½¿ç”¨å¤§å‹éª¨å¹²æ¨¡å‹çš„æ¨ç†ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡æ”¶é›†é£é™©å¤§æˆ–è¯„ä¼°æˆæœ¬é«˜çš„åœºæ™¯ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œä»¥åœ¨ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ä¹‹é—´å»ºç«‹è”ç³»ï¼Œä»¥å®ç°ç¨³å¥çš„æ´»åŠ¨ä»»åŠ¡é‡‡æ ·ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼ŒMPTSä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¯¹ä»»åŠ¡çš„ç‰‡æ®µä¿¡æ¯è¿›è¡Œè¡¨å¾ï¼Œå¹¶é€šè¿‡åéªŒæ¨ç†é¢„æµ‹ä¼˜åŒ–ç»“æœï¼Œå³ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©å€¼ã€‚æ‰€å¾—çš„é£é™©å­¦ä¹ è€…å‡å°‘äº†æ˜‚è´µçš„æ³¨é‡Šã€è¯„ä¼°æˆ–è®¡ç®—æ“ä½œåœ¨ä»»åŠ¡ç¨³å¥é€‚åº”ä¸­çš„åº”ç”¨ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMPTSå¯ä»¥æ— ç¼åœ°èå…¥é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ä¸­ï¼Œæé«˜é€‚åº”ç¨³å¥æ€§å¹¶ä¿æŒå­¦ä¹ æ•ˆç‡ï¼Œæ— éœ€é¢å¤–æˆæœ¬ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-rllab/MPTSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11039v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹é‡‡ç”¨è·¨ä»»åŠ¡æ³›åŒ–æ¨¡å¼æå‡äº†é€‚åº”æ€§è´¨ï¼Œé€šè¿‡ä¼˜å…ˆé‡‡æ ·æŒ‘æˆ˜æ€§ä»»åŠ¡å¢å¼ºé€‚åº”ç¨³å¥æ€§å¹¶æ”¹å–„é‡‡æ ·æ•ˆç‡ã€‚æå‡ºæ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰å»ºç«‹ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚é—´çš„è”ç³»ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆ»ç”»ä»»åŠ¡ç‰‡æ®µä¿¡æ¯å¹¶ç»“åˆåéªŒæ¨æ–­é¢„æµ‹ä¼˜åŒ–ç»“æœï¼Œå³ä»»åŠ¡ç‰¹å®šé€‚åº”é£é™©å€¼ï¼Œé™ä½æ˜‚è´µæ ‡æ³¨ã€è¯„ä¼°æˆ–è®¡ç®—æ“ä½œçš„æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒMPTSå¯æ— ç¼èå…¥é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œæé«˜é€‚åº”ç¨³å¥æ€§å¹¶ä¿æŒå­¦ä¹ æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨ä»»åŠ¡æ³›åŒ–æ¨¡å¼æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§ï¼Œæœ‰åŠ©äºå¿«é€Ÿè§£å†³é—®é¢˜ã€‚</li>
<li>ä¼˜å…ˆé‡‡æ ·æŒ‘æˆ˜æ€§ä»»åŠ¡èƒ½å¢å¼ºæ¨¡å‹çš„é€‚åº”ç¨³å¥æ€§å¹¶æ”¹å–„é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰æ—¨åœ¨å»ºç«‹ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>MPTSåˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆ»ç”»ä»»åŠ¡ç‰‡æ®µä¿¡æ¯ï¼Œç»“åˆåéªŒæ¨æ–­é¢„æµ‹ä¼˜åŒ–ç»“æœã€‚</li>
<li>MPTSé™ä½äº†æ ‡æ³¨ã€è¯„ä¼°å’Œè®¡ç®—æ“ä½œçš„æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜MPTSå¯èå…¥å¤šç§å­¦ä¹ èŒƒå¼ï¼Œæé«˜é€‚åº”ç¨³å¥æ€§å¹¶ä¿æŒå­¦ä¹ æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.11039v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.11039v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.11039v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.11039v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Compute-More-Faithful"><a href="#Inference-Time-Compute-More-Faithful" class="headerlink" title="Inference-Time-Compute: More Faithful?"></a>Inference-Time-Compute: More Faithful?</h2><p><strong>Authors:James Chua, Owain Evans</strong></p>
<p>Models trained specifically to generate long Chains of Thought (CoTs) have recently achieved impressive results. We refer to these models as Inference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful compared to traditional non-ITC models? We evaluate three ITC models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test if models articulate a cue in their prompt that influences their answers to MMLU questions. For example, when the cue â€œA Stanford Professor thinks the answer is Dâ€ is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 ITC model articulates the cue 59% of the time, compared to 7% for the non-ITC DeepSeek. We set a strict requirement on articulating â€“ these must describe how the cue makes the models switch their answer - simply mentioning the cue does not count. We evaluate 7 types of cue, such as misleading few-shot examples and anchoring on past responses. ITC models articulate cues that influence them much more reliably than all the 7 non-ITC models tested, such as Claude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time. Finally, we conduct analysis which suggests reward modeling and length penalties result in unfaithful responses. However, our study has important limitations. We cannot evaluate OpenAIâ€™s SOTA o3 model. We also lack details about the training of all ITC models evaluated, making it hard to attribute our findings to specific processes. Faithfulness of CoT is an important property for AI Safety. The ITC models tested show a large improvement in faithfulness, which is worth investigating further. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä¸“é—¨è®­ç»ƒç”¨äºç”Ÿæˆé•¿ç¯‡å¹…çš„æ€è€ƒé“¾ï¼ˆCoTsï¼‰çš„æ¨¡å‹å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚æˆ‘ä»¬å°†è¿™äº›æ¨¡å‹ç§°ä¸ºæ¨ç†æ—¶é—´è®¡ç®—ï¼ˆITCï¼‰æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„éITCæ¨¡å‹ç›¸æ¯”ï¼ŒITCæ¨¡å‹çš„æ€è€ƒé“¾æ˜¯å¦æ›´åŠ å¿ å®ï¼Ÿæˆ‘ä»¬å¯¹åŸºäºQwen-2.5ã€Gemini-2å’ŒDeepSeek-V3-Baseçš„ä¸‰ä¸ªITCæ¨¡å‹è¿›è¡Œäº†å¿ å®æ€è€ƒé“¾çš„ç°æœ‰æµ‹è¯•è¯„ä¼°ã€‚ä¸ºäº†è¡¡é‡å¿ å®åº¦ï¼Œæˆ‘ä»¬æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿåœ¨å…¶æç¤ºä¸­é˜è¿°ä¸€ä¸ªçº¿ç´¢ï¼Œè¯¥çº¿ç´¢ä¼šå½±å“å®ƒä»¬å¯¹MMLUé—®é¢˜çš„å›ç­”ã€‚ä¾‹å¦‚ï¼Œå½“æç¤ºä¸­åŠ å…¥â€œæ–¯å¦ç¦æ•™æˆè®¤ä¸ºç­”æ¡ˆæ˜¯Dâ€çš„çº¿ç´¢æ—¶ï¼Œæ¨¡å‹æœ‰æ—¶ä¼šæ”¹å˜ç­”æ¡ˆé€‰æ‹©Dã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒDeepSeek-R1çš„ITCæ¨¡å‹èƒ½å¤Ÿé˜è¿°è¯¥çº¿ç´¢çš„59%ï¼Œè€ŒéITCçš„DeepSeekåªæœ‰7%èƒ½å¤Ÿé˜è¿°ã€‚æˆ‘ä»¬å¯¹é˜è¿°æå‡ºäº†ä¸¥æ ¼è¦æ±‚â€”â€”è¿™äº›å¿…é¡»æè¿°çº¿ç´¢æ˜¯å¦‚ä½•ä½¿æ¨¡å‹æ”¹å˜ç­”æ¡ˆçš„â€”â€”ä»…ä»…æåˆ°çº¿ç´¢å¹¶ä¸ç®—æ•°ã€‚æˆ‘ä»¬è¯„ä¼°äº†7ç§ç±»å‹çš„çº¿ç´¢ï¼Œå¦‚è¯¯å¯¼æ€§çš„å°‘é‡ç¤ºä¾‹å’ŒåŸºäºè¿‡å»å›åº”çš„é”šå®šã€‚ITCæ¨¡å‹èƒ½å¤Ÿæ›´å¯é åœ°é˜è¿°å½±å“ä»–ä»¬çš„çº¿ç´¢ï¼Œç›¸è¾ƒäºæ‰€æµ‹è¯•çš„7ä¸ªéITCæ¨¡å‹ï¼ˆå¦‚Claude-3.5-Sonnetå’ŒGPT-4oï¼‰ï¼Œåè€…å¾€å¾€æ— æ³•é˜è¿°ä»»ä½•çº¿ç´¢ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†åˆ†æï¼Œå‘ç°å¥–åŠ±å»ºæ¨¡å’Œé•¿åº¦æƒ©ç½šä¼šå¯¼è‡´ä¸å¿ å®çš„å›åº”ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å­˜åœ¨é‡è¦å±€é™æ€§ã€‚æˆ‘ä»¬æ— æ³•è¯„ä¼°OpenAIçš„æœ€å…ˆè¿›o3æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¼ºä¹æ‰€è¯„ä¼°çš„æ‰€æœ‰ITCæ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ï¼Œè¿™ä½¿å¾—éš¾ä»¥å°†æˆ‘ä»¬çš„å‘ç°å½’å› äºç‰¹å®šçš„è¿‡ç¨‹ã€‚æ€è€ƒé“¾çš„å¿ å®æ€§æ˜¯äººå·¥æ™ºèƒ½å®‰å…¨æ€§çš„é‡è¦å±æ€§ã€‚æ‰€æµ‹è¯•çš„ITCæ¨¡å‹åœ¨å¿ å®æ€§æ–¹é¢è¡¨ç°å‡ºå¤§å¹…åº¦æ”¹è¿›ï¼Œå€¼å¾—è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08156v2">PDF</a> 10 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒä¸“é—¨ç”¨äºç”Ÿæˆé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰çš„æ¨¡å‹å·²å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè¿™äº›æ¨¡å‹è¢«ç§°ä¸ºæ¨ç†æ—¶é—´è®¡ç®—ï¼ˆITCï¼‰æ¨¡å‹ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸‰ç§ITCæ¨¡å‹ï¼ˆåŸºäºQwen-2.5ã€Gemini-2å’ŒDeepSeek-V3-Baseï¼‰åœ¨å¿ å®CoTæ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½è¡¨è¾¾æç¤ºä¸­çš„çº¿ç´¢æ¥å½±å“å…¶å¯¹MMLUé—®é¢˜çš„å›ç­”æ¥è¡¡é‡å¿ å®æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒITCæ¨¡å‹æ¯”æ‰€æœ‰æµ‹è¯•çš„7ç§éITCæ¨¡å‹æ›´å¯é åœ°è¡¨è¾¾å½±å“å®ƒä»¬çš„çº¿ç´¢ã€‚æœ€åï¼Œåˆ†æè¡¨æ˜å¥–åŠ±å»ºæ¨¡å’Œé•¿åº¦æƒ©ç½šå¯èƒ½å¯¼è‡´ä¸å¿ å®çš„å›åº”ã€‚ä½†æœ¬ç ”ç©¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•è¯„ä¼°OpenAIçš„æœ€å…ˆè¿›æ¨¡å‹o3ï¼Œä¸”ç¼ºä¹æ‰€è¯„ä¼°ITCæ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ITCæ¨¡å‹åœ¨ç”Ÿæˆé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æµ‹è¯•æ¨¡å‹å¯¹MMLUé—®é¢˜çš„å›ç­”æ¥è¡¡é‡ITCæ¨¡å‹çš„å¿ å®æ€§ã€‚</li>
<li>ITCæ¨¡å‹æ¯”éITCæ¨¡å‹æ›´å¯é åœ°è¡¨è¾¾å½±å“å®ƒä»¬çš„çº¿ç´¢ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºå¥–åŠ±å»ºæ¨¡å’Œé•¿åº¦æƒ©ç½šå¯èƒ½å¯¼è‡´ä¸å¿ å®çš„å›åº”ã€‚</li>
<li>ç ”ç©¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•è¯„ä¼°OpenAIçš„SOTA o3æ¨¡å‹ã€‚</li>
<li>ç¼ºä¹æ‰€è¯„ä¼°ITCæ¨¡å‹çš„è¯¦ç»†è®­ç»ƒä¿¡æ¯ï¼Œéš¾ä»¥ç¡®å®šç ”ç©¶ç»“æœçš„å…·ä½“åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.08156v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.08156v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.08156v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.08156v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.08156v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.08156v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Semantic-Captioning-Benchmark-Dataset-and-Graph-Aware-Few-Shot-In-Context-Learning-for-SQL2Text"><a href="#Semantic-Captioning-Benchmark-Dataset-and-Graph-Aware-Few-Shot-In-Context-Learning-for-SQL2Text" class="headerlink" title="Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot   In-Context Learning for SQL2Text"></a>Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot   In-Context Learning for SQL2Text</h2><p><strong>Authors:Ali Al-Lawati, Jason Lucas, Prasenjit Mitra</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which translates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: <a target="_blank" rel="noopener" href="https://github.com/aliwister/ast-icl">https://github.com/aliwister/ast-icl</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆæ­£å¼ä»£ç è¡¨ç¤ºçš„è¯­ä¹‰è§£æã€‚ç„¶è€Œï¼Œå°†ä»£ç ç¿»è¯‘æˆè‡ªç„¶è¯­è¨€çš„é€†å‘è¿‡ç¨‹ï¼Œå³è¯­ä¹‰å­—å¹•ï¼Œå—åˆ°çš„å…³æ³¨åº¦è¾ƒä½ã€‚éšç€LLMè¢«é›†æˆåˆ°ä»£ç ç”Ÿæˆã€å®‰å…¨åˆ†æå’Œæ•™è‚²å¹³å°ï¼Œè¿™é¡¹ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºSQLæŸ¥è¯¢çš„å­—å¹•ç”Ÿæˆï¼ˆSQL2Textï¼‰ï¼Œä»¥è§£å†³åœ¨LLMç”Ÿæˆçš„ä»£ç å­˜åœ¨æ½œåœ¨å®‰å…¨é£é™©çš„æ—¶ä»£ï¼Œå¯¹ç†è§£å’Œè§£é‡ŠSQLæŸ¥è¯¢çš„è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬é€šè¿‡å¯¹GPT-4oå¼•å…¥è¿­ä»£ICLæç¤ºæ¥é‡æ–°åˆ©ç”¨Text2SQLæ•°æ®é›†è¿›è¡ŒSQL2Textä»»åŠ¡ï¼Œç”Ÿæˆå¤šä¸ªé™„åŠ è¯è¯­ï¼Œå¢å¼ºäº†åå‘ä»»åŠ¡æ•°æ®é›†çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å®éªŒé‡‡ç”¨ä¸åŒçš„æ ·æœ¬é€‰æ‹©æ–¹æ³•è¿›è¡ŒåŸºäºä¸Šä¸‹æ–‡çš„å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå¹¶å¼ºè°ƒæ›´å°ã€è®¡ç®—æ•ˆç‡æ›´é«˜çš„LLMã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨SQLçš„å†…åœ¨å›¾å½¢å±æ€§è¿›è¡ŒICLæ ·æœ¬é€‰æ‹©æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼ŒBLEUåˆ†æ•°æé«˜äº†é«˜è¾¾39%ï¼Œå¹¶ä¸”æ¯”å…¶ä»–æ–¹æ³•æä¾›äº†æ›´å¥½çš„ç»“æœã€‚æ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/aliwister/ast-icl%E3%80%82">https://github.com/aliwister/ast-iclã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03166v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰è§£æä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå³æŠŠè‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºå½¢å¼åŒ–ä»£ç è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°†ä»£ç è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„åå‘è¿‡ç¨‹â€”â€”è¯­ä¹‰æè¿°ï¼Œéšç€è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆã€å®‰å…¨åˆ†æå’Œæ•™è‚²ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡ä¸“æ³¨äºSQLæŸ¥è¯¢çš„æè¿°ï¼ˆSQL2Textï¼‰ï¼Œè§£å†³åœ¨LLMç”Ÿæˆçš„ä»£ç å¯èƒ½å¸¦æ¥å®‰å…¨é£é™©çš„æ—¶ä»£ï¼Œç†è§£å’Œè§£é‡ŠSQLæŸ¥è¯¢çš„è¿«åˆ‡éœ€æ±‚ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨GPT-4oè¿›è¡Œè¿­ä»£ICLæç¤ºï¼Œç”Ÿæˆå¤šä¸ªé™„åŠ è¡¨è¿°ï¼Œå¢å¼ºæ•°æ®é›†å¯¹åå‘ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚å®éªŒé‡‡ç”¨åŸºäºä¸åŒæ ·æœ¬é€‰æ‹©æ–¹æ³•çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå¼ºè°ƒæ›´å°ã€æ›´è®¡ç®—é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨SQLçš„å†…åœ¨å›¾å±æ€§è¿›è¡ŒICLæ ·æœ¬é€‰æ‹©æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼ŒBLEUåˆ†æ•°æé«˜è¾¾39%ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰è§£æä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¯­ä¹‰æè¿°ï¼ˆå°†ä»£ç è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼‰åŒæ ·é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç ç”Ÿæˆã€å®‰å…¨åˆ†æå’Œæ•™è‚²é¢†åŸŸã€‚</li>
<li>SQLæŸ¥è¯¢çš„æè¿°ï¼ˆSQL2Textï¼‰æ˜¯ç†è§£å’Œè§£é‡ŠSQLæŸ¥è¯¢çš„å…³é”®ï¼Œå°¤å…¶åœ¨LLMç”Ÿæˆçš„ä»£ç ç¯å¢ƒä¸­ã€‚</li>
<li>ä½¿ç”¨GPT-4oå’Œè¿­ä»£ICLæç¤ºæŠ€æœ¯ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªé™„åŠ è¡¨è¿°å¢å¼ºæ•°æ®é›†å¯¹åå‘ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚</li>
<li>åŸºäºä¸åŒæ ·æœ¬é€‰æ‹©æ–¹æ³•çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨SQLçš„å†…åœ¨å›¾å±æ€§è¿›è¡Œæ ·æœ¬é€‰æ‹©æ•ˆæœæœ€ä½³ã€‚</li>
<li>ä¸éšæœºé€‰æ‹©ç›¸æ¯”ï¼Œæœ€ä½³æ–¹æ³•èƒ½æé«˜BLEUåˆ†æ•°è¾¾39%ã€‚</li>
<li>è¯¥ç ”ç©¶å…¬å¼€åˆ†äº«äº†æ•°æ®é›†å’Œä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.03166v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.03166v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.03166v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2501.03166v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SLM-Mod-Small-Language-Models-Surpass-LLMs-at-Content-Moderation"><a href="#SLM-Mod-Small-Language-Models-Surpass-LLMs-at-Content-Moderation" class="headerlink" title="SLM-Mod: Small Language Models Surpass LLMs at Content Moderation"></a>SLM-Mod: Small Language Models Surpass LLMs at Content Moderation</h2><p><strong>Authors:Xianyang Zhan, Agam Goyal, Yilun Chen, Eshwar Chandrasekharan, Koustuv Saha</strong></p>
<p>Large language models (LLMs) have shown promise in many natural language understanding tasks, including content moderation. However, these models can be expensive to query in real-time and do not allow for a community-specific approach to content moderation. To address these challenges, we explore the use of open-source small language models (SLMs) for community-specific content moderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by comparing their performance against much larger open- and closed-sourced models in both a zero-shot and few-shot setting. Using 150K comments from 15 popular Reddit communities, we find that SLMs outperform zero-shot LLMs at content moderation â€“ 11.5% higher accuracy and 25.7% higher recall on average across all communities. Moreover, few-shot in-context learning leads to only a marginal increase in the performance of LLMs, still lacking compared to SLMs. We further show the promise of cross-community content moderation, which has implications for new communities and the development of cross-platform moderation techniques. Finally, we outline directions for future work on language model based content moderation. Code and models can be found at <a target="_blank" rel="noopener" href="https://github.com/AGoyal0512/SLM-Mod">https://github.com/AGoyal0512/SLM-Mod</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬å†…å®¹å®¡æ ¸ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®æ—¶æŸ¥è¯¢æ—¶å¯èƒ½ä¼šå¾ˆæ˜‚è´µï¼Œå¹¶ä¸å…è®¸é’ˆå¯¹ç‰¹å®šç¤¾åŒºçš„å†…å®¹å®¡æ ¸é‡‡å–ç‰¹å®šæ–¹æ³•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¼€æºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è¿›è¡Œç‰¹å®šç¤¾åŒºçš„å†…å®¹å®¡æ ¸ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å°†æ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œå¯¹SLMï¼ˆå°äº15Bå‚æ•°ï¼‰è¿›è¡Œå¾®è°ƒå¹¶è¯„ä¼°ï¼Œåˆ†åˆ«åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­è¿›è¡Œã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª15ä¸ªæµè¡ŒRedditç¤¾åŒºçš„15ä¸‡æ¡è¯„è®ºå‘ç°ï¼ŒSLMåœ¨å†…å®¹å®¡æ ¸æ–¹é¢è¡¨ç°å‡ºä¼˜äºé›¶æ ·æœ¬LLMçš„æ€§èƒ½â€”â€”åœ¨æ‰€æœ‰ç¤¾åŒºä¸­å¹³å‡é«˜å‡º11.5%çš„å‡†ç¡®ç‡å’Œ25.7%çš„å¬å›ç‡ã€‚æ­¤å¤–ï¼Œå°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ åªä¼šç•¥å¾®æé«˜LLMçš„æ€§èƒ½ï¼Œä»ç„¶ä¸åŠSLMã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†è·¨ç¤¾åŒºå†…å®¹å®¡æ ¸çš„æ½œåŠ›ï¼Œè¿™å¯¹æ–°ç¤¾åŒºå’Œè·¨å¹³å°å®¡æ ¸æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†åŸºäºè¯­è¨€æ¨¡å‹çš„å†…å®¹å®¡æ ¸çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AGoyal0512/SLM-Mod%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AGoyal0512/SLM-Modæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13155v2">PDF</a> NAACL 2025 (Main): 17 pages, 8 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¼€æºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰è¿›è¡Œç¤¾åŒºç‰¹å®šå†…å®¹å®¡æ ¸ä»»åŠ¡çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶è¯„ä¼°å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½ï¼Œå‘ç°å°å‹è¯­è¨€æ¨¡å‹åœ¨å†…å®¹å®¡æ ¸æ–¹é¢çš„è¡¨ç°ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾åŒºç‰¹å®šæƒ…å¢ƒä¸‹ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†è·¨ç¤¾åŒºå†…å®¹å®¡æ ¸çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥è¯­è¨€æ¨¡å‹åœ¨å†…å®¹å®¡æ ¸é¢†åŸŸçš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸­æœ‰æ½œåŠ›ï¼Œä½†å­˜åœ¨å®æ—¶æŸ¥è¯¢æˆæœ¬é«˜å’Œç¼ºä¹ç¤¾åŒºç‰¹å®šæ–¹æ³•çš„é—®é¢˜ã€‚</li>
<li>å¼€æºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å¯ä½œä¸ºç¤¾åŒºç‰¹å®šå†…å®¹å®¡æ ¸ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼ŒSLMsçš„è¡¨ç°ä¼˜äºLLMsï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾åŒºç‰¹å®šæƒ…å¢ƒä¸‹ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡æ¯”é›¶æ ·æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹é«˜å‡º11.5%ï¼Œå¬å›ç‡åˆ™é«˜å‡º25.7%ã€‚</li>
<li>å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡æœ‰é™ï¼Œä»ä¸åŠå°å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è·¨ç¤¾åŒºå†…å®¹å®¡æ ¸å…·æœ‰æ½œåŠ›ï¼Œå¯¹æœªæ¥æ–°ç¤¾åŒºå’Œè·¨å¹³å°å®¡æ ¸æŠ€æœ¯çš„å‘å±•æœ‰å¯ç¤ºä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2410.13155v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2410.13155v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2410.13155v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2410.13155v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="How-Effectively-Do-LLMs-Extract-Feature-Sentiment-Pairs-from-App-Reviews"><a href="#How-Effectively-Do-LLMs-Extract-Feature-Sentiment-Pairs-from-App-Reviews" class="headerlink" title="How Effectively Do LLMs Extract Feature-Sentiment Pairs from App   Reviews?"></a>How Effectively Do LLMs Extract Feature-Sentiment Pairs from App   Reviews?</h2><p><strong>Authors:Faiz Ali Shah, Ahmed Sabir, Rajesh Sharma, Dietmar Pfahl</strong></p>
<p>Automatic analysis of user reviews to understand user sentiments toward app functionality (i.e. app features) helps align development efforts with user expectations and needs. Recent advances in Large Language Models (LLMs) such as ChatGPT have shown impressive performance on several new tasks without updating the modelâ€™s parameters i.e. using zero or a few labeled examples, but the capabilities of LLMs are yet unexplored for feature-specific sentiment analysis. The goal of our study is to explore the capabilities of LLMs to perform feature-specific sentiment analysis of user reviews. This study compares the performance of state-of-the-art LLMs, including GPT-4, ChatGPT, and different variants of Llama-2 chat, against previous approaches for extracting app features and associated sentiments in zero-shot, 1-shot, and 5-shot scenarios. The results indicate that GPT-4 outperforms the rule-based SAFE by 17% in f1-score for extracting app features in the zero-shot scenario, with 5-shot further improving it by 6%. However, the fine-tuned RE-BERT exceeds GPT-4 by 6% in f1-score. For predicting positive and neutral sentiments, GPT-4 achieves f1-scores of 76% and 45% in the zero-shot setting, which improve by 7% and 23% in the 5-shot setting, respectively. Our study conducts a thorough evaluation of both proprietary and open-source LLMs to provide an objective assessment of their performance in extracting feature-sentiment pairs. </p>
<blockquote>
<p>å¯¹ç”¨æˆ·è¯„è®ºè¿›è¡Œè‡ªåŠ¨åˆ†æï¼Œä»¥äº†è§£ç”¨æˆ·å¯¹åº”ç”¨åŠŸèƒ½ï¼ˆå³åº”ç”¨ç‰¹æ€§ï¼‰çš„æƒ…æ„Ÿï¼Œæœ‰åŠ©äºå°†å¼€å‘åŠªåŠ›ä¸ç”¨æˆ·æœŸæœ›å’Œéœ€æ±‚å¯¹é½ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTçš„è¿›æ­¥åœ¨è®¸å¤šæ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨ä¸æ›´æ–°æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå³ä½¿ç”¨é›¶ä¸ªæˆ–å°‘æ•°æ ‡æ³¨æ ·æœ¬ä¹Ÿèƒ½è¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†å¯¹äºç‰¹æ€§ç‰¹å®šçš„æƒ…æ„Ÿåˆ†æï¼ŒLLMçš„èƒ½åŠ›å°šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç›®æ ‡æ˜¯æ¢ç´¢LLMæ‰§è¡Œç‰¹æ€§ç‰¹å®šçš„ç”¨æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¯”è¾ƒäº†æœ€å‰æ²¿çš„LLMï¼ŒåŒ…æ‹¬GPT-4ã€ChatGPTå’Œä¸åŒå˜ç§çš„Llama-2èŠå¤©ï¼Œä»¥åŠä»¥å¾€çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œäº”æ ·æœ¬åœºæ™¯ä¸­æå–åº”ç”¨ç‰¹æ€§å’Œç›¸å…³æƒ…æ„Ÿçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-4åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­æå–åº”ç”¨ç‰¹æ€§çš„f1åˆ†æ•°ä¸Šæ¯”åŸºäºè§„åˆ™çš„SAFEé«˜å‡º17%ï¼Œäº”æ ·æœ¬åœºæ™¯è¿›ä¸€æ­¥æé«˜äº†6%ã€‚ç„¶è€Œï¼Œç»è¿‡å¾®è°ƒåçš„RE-BERTåœ¨f1åˆ†æ•°ä¸Šè¶…è¿‡äº†GPT-4çš„6%ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼ŒGPT-4é¢„æµ‹æ­£é¢å’Œä¸­æ€§æƒ…æ„Ÿçš„f1åˆ†æ•°åˆ†åˆ«ä¸º76%å’Œ45%ï¼Œåœ¨äº”æ ·æœ¬è®¾ç½®ä¸­åˆ†åˆ«æé«˜äº†7%å’Œ23%ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹ä¸“æœ‰å’Œå¼€æºçš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œä»¥å®¢è§‚è¯„ä¼°å®ƒä»¬åœ¨æå–ç‰¹æ€§æƒ…æ„Ÿå¯¹ä¸­çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07162v3">PDF</a> The summary of the project is available at <a target="_blank" rel="noopener" href="https://bit.ly/3XGcRM1">https://bit.ly/3XGcRM1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç‰¹å®šåŠŸèƒ½æƒ…æ„Ÿåˆ†æçš„ç”¨æˆ·è¯„è®ºè‡ªåŠ¨åˆ†æã€‚ç ”ç©¶å¯¹æ¯”äº†æœ€å‰æ²¿çš„LLMsï¼ŒåŒ…æ‹¬GPT-4ã€ChatGPTå’ŒLlama-2çš„ä¸åŒå˜ç§ï¼Œåœ¨é›¶æ ·æœ¬ã€ä¸€ç¤ºä¾‹å’Œäº”ç¤ºä¾‹åœºæ™¯ä¸‹æå–åº”ç”¨åŠŸèƒ½å’Œç›¸å…³æƒ…æ„Ÿçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…¶ä»–æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚æœ¬ç ”ç©¶å¯¹ä¸“æœ‰å’Œå¼€æºLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œä¸ºåŠŸèƒ½æƒ…æ„Ÿå¯¹æå–æ€§èƒ½æä¾›äº†å®¢è§‚è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”¨æˆ·è¯„è®ºçš„æƒ…æ„Ÿåˆ†æä¸­å…·æœ‰æ½œåŠ›ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹åº”ç”¨åŠŸèƒ½ç‰¹å®šæƒ…æ„Ÿåˆ†æã€‚</li>
<li>GPT-4åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„åº”ç”¨åŠŸèƒ½æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…¶ä»–æ¨¡å‹å’Œæ–¹æ³•çš„æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>åœ¨å¤„ç†ç§¯æå’Œä¸­æ€§æƒ…æ„Ÿé¢„æµ‹æ—¶ï¼ŒGPT-4åœ¨äº”ç¤ºä¾‹è®¾ç½®ä¸‹çš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>LLMsåœ¨æƒ…æ„Ÿåˆ†æä¸­çš„è¡¨ç°ä»å¾…è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹ä¸“æœ‰å’Œå¼€æºLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæä¾›äº†å…³äºåŠŸèƒ½æƒ…æ„Ÿå¯¹æå–æ€§èƒ½çš„å®¢è§‚è¯„ä¼°ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2409.07162v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2409.07162v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2409.07162v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2409.07162v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Advancing-Fine-Grained-Classification-by-Structure-and-Subject-Preserving-Augmentation"><a href="#Advancing-Fine-Grained-Classification-by-Structure-and-Subject-Preserving-Augmentation" class="headerlink" title="Advancing Fine-Grained Classification by Structure and Subject   Preserving Augmentation"></a>Advancing Fine-Grained Classification by Structure and Subject   Preserving Augmentation</h2><p><strong>Authors:Eyal Michaeli, Ohad Fried</strong></p>
<p>Fine-grained visual classification (FGVC) involves classifying closely related sub-classes. This task is difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation. Recent advancements in text-to-image diffusion models offer new possibilities for augmenting classification datasets. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on Text2Image generation or Img2Img methods, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the datasetâ€™s diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation. We conduct extensive experiments and benchmark SaSPA against both traditional and recent generative data augmentation methods. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training, contextual bias, and few-shot classification. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal proportion of synthetic data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/EyalMichaeli/SaSPA-Aug">https://github.com/EyalMichaeli/SaSPA-Aug</a>. </p>
<blockquote>
<p>ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰æ¶‰åŠå¯¹å¯†åˆ‡ç›¸å…³çš„å­ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚ç”±äºç±»åˆ«ä¹‹é—´çš„ç»†å¾®å·®å¼‚å’Œé«˜ç±»å†…æ–¹å·®ï¼Œæ­¤ä»»åŠ¡å¾ˆå›°éš¾ã€‚æ­¤å¤–ï¼ŒFGVCæ•°æ®é›†é€šå¸¸å¾ˆå°ä¸”éš¾ä»¥æ”¶é›†ï¼Œä»è€Œå‡¸æ˜¾å‡ºå¯¹æœ‰æ•ˆæ•°æ®å¢å¼ºçš„è¿«åˆ‡éœ€æ±‚ã€‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºå¢å¼ºåˆ†ç±»æ•°æ®é›†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚è™½ç„¶è¿™äº›æ¨¡å‹å·²è¢«ç”¨äºç”Ÿæˆåˆ†ç±»ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼Œä½†å®ƒä»¬åœ¨å¯¹FGVCæ¨¡å‹è¿›è¡Œå…¨æ•°æ®é›†è®­ç»ƒçš„æœ‰æ•ˆæ€§æ–¹é¢å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ€è¿‘ä¾èµ–äºText2Imageç”Ÿæˆæˆ–Img2Imgæ–¹æ³•çš„æŠ€å·§ï¼Œå¾€å¾€éš¾ä»¥ç”Ÿæˆå‡†ç¡®ä»£è¡¨ç±»çš„å›¾åƒï¼ŒåŒæ—¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿®æ”¹å®ƒä»¬ä»¥å¢åŠ æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SaSPAï¼šç»“æ„ä¸»é¢˜ä¿ç•™å¢å¼ºæ³•ã€‚ä¸æœ€è¿‘çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä½¿ç”¨çœŸå®å›¾åƒä½œä¸ºæŒ‡å¯¼ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆçš„çµæ´»æ€§å¹¶ä¿ƒè¿›äº†æ›´å¤§çš„å¤šæ ·æ€§ã€‚ä¸ºç¡®ä¿å‡†ç¡®çš„ç±»åˆ«è¡¨ç¤ºï¼Œæˆ‘ä»¬é€šè¿‡å›¾åƒè¾¹ç¼˜å’Œä¸»é¢˜è¡¨ç¤ºè¿›è¡Œæ¡ä»¶è®¾ç½®æ¥é‡‡ç”¨æ¡ä»¶æœºåˆ¶ã€‚æˆ‘ä»¬å¯¹SaSPAè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶ä¸ä¼ ç»Ÿå’Œæœ€æ–°çš„ç”Ÿæˆæ•°æ®å¢å¼ºæ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚SaSPAåœ¨å¤šç§è®¾ç½®ä¸‹å§‹ç»ˆè¶…è¶Šæ‰€æœ‰æ—¢å®šçš„åŸºçº¿ï¼ŒåŒ…æ‹¬å…¨æ•°æ®é›†è®­ç»ƒã€ä¸Šä¸‹æ–‡åå·®å’Œå°‘é•œå¤´åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†åœ¨ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡ŒFGVCæ¨¡å‹æ–¹é¢çš„æœ‰è¶£æ¨¡å¼ï¼›ä¾‹å¦‚ï¼Œæˆ‘ä»¬å‘ç°çœŸå®æ•°æ®çš„ä½¿ç”¨é‡ä¸åˆæˆæ•°æ®çš„æœ€ä½³æ¯”ä¾‹ä¹‹é—´å­˜åœ¨å…³ç³»ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EyalMichaeli/SaSPA-Aug">https://github.com/EyalMichaeli/SaSPA-Aug</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14551v3">PDF</a> Accepted to NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Fine-grainedè§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­çš„æ•°æ®å¢å¼ºæŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•SaSPAï¼Œè¯¥æ–¹æ³•ä¸ä¾èµ–çœŸå®å›¾åƒä½œä¸ºæŒ‡å¯¼ï¼Œé€šè¿‡å›¾åƒè¾¹ç¼˜å’Œä¸»é¢˜è¡¨ç¤ºè¿›è¡Œæ¡ä»¶æ§åˆ¶ï¼Œç¡®ä¿å‡†ç¡®ç±»è¡¨ç¤ºçš„åŒæ—¶ä¿ƒè¿›ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œçµæ´»æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSaSPAåœ¨å¤šä¸ªè®¾ç½®ä¸­éƒ½ä¼˜äºä¼ ç»Ÿå’Œæœ€æ–°çš„ç”Ÿæˆæ•°æ®å¢å¼ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ä½¿ç”¨åˆæˆæ•°æ®å¯¹FGVCæ¨¡å‹çš„æœ‰è¶£æ¨¡å¼ï¼Œå¦‚çœŸå®æ•°æ®ä½¿ç”¨é‡å¯¹åˆæˆæ•°æ®æœ€ä½³æ¯”ä¾‹çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fine-grainedè§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰é¢ä¸´æ•°æ®æ”¶é›†çš„å›°éš¾å’Œç±»é—´ç»†å¾®å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ºFGVCçš„æ•°æ®å¢å¼ºæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>SaSPAæ–¹æ³•ä¸ä¾èµ–çœŸå®å›¾åƒä½œä¸ºæŒ‡å¯¼ï¼Œæé«˜äº†ç”Ÿæˆçš„çµæ´»æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>SaSPAé€šè¿‡æ¡ä»¶æœºåˆ¶ï¼Œå¦‚å›¾åƒè¾¹ç¼˜å’Œä¸»é¢˜è¡¨ç¤ºï¼Œç¡®ä¿å‡†ç¡®çš„ç±»è¡¨ç¤ºã€‚</li>
<li>å®éªŒè¡¨æ˜SaSPAåœ¨å¤šä¸ªè®¾ç½®ä¸­éƒ½ä¼˜äºå…¶ä»–æ•°æ®å¢å¼ºæ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨åˆæˆæ•°æ®çš„æœ‰è¶£æ¨¡å¼è¢«å‘ç°ï¼Œå¦‚çœŸå®æ•°æ®ä¸åˆæˆæ•°æ®çš„æœ€ä½³æ¯”ä¾‹å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2406.14551v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2406.14551v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2406.14551v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2406.14551v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="XAMPLER-Learning-to-Retrieve-Cross-Lingual-In-Context-Examples"><a href="#XAMPLER-Learning-to-Retrieve-Cross-Lingual-In-Context-Examples" class="headerlink" title="XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples"></a>XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples</h2><p><strong>Authors:Peiqin Lin, AndrÃ© F. T. Martins, Hinrich SchÃ¼tze</strong></p>
<p>Recent studies indicate that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving relevant in-context examples tailored to the input query, enhances few-shot in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, poses challenges due to the scarcity of cross-lingual retrievers and annotated data. Thus, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever based on Glot500, a multilingual small language model, using positive and negative English examples constructed from the predictions of a multilingual large language model, i.e., MaLA500. Leveraging the cross-lingual capacity of the retriever, it can directly retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on two multilingual text classification benchmarks, namely SIB200 with 176 languages and MasakhaNEWS with 16 languages, demonstrate that XAMPLER substantially improves the in-context learning performance across languages. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cisnlp/XAMPLER">https://github.com/cisnlp/XAMPLER</a>. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ç°æˆçš„æˆ–ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ£€ç´¢å™¨ï¼Œèƒ½å¤Ÿæ£€ç´¢ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œä»è€Œå¢å¼ºè‹±è¯­çš„å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•é€‚åº”åˆ°å…¶ä»–è¯­è¨€ï¼Œå°¤å…¶æ˜¯ä½èµ„æºè¯­è¨€ï¼Œå´é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè·¨è¯­è¨€æ£€ç´¢å™¨å’Œæ³¨é‡Šæ•°æ®ç¨€ç¼ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†XAMPLERï¼šè·¨è¯­è¨€ç¤ºä¾‹æ£€ç´¢æ–¹æ³•ï¼Œå®ƒä¸“é—¨é’ˆå¯¹ä»…ä½¿ç”¨æ³¨é‡Šè‹±è¯­æ•°æ®çš„è·¨è¯­è¨€ä¸Šä¸‹æ–‡å­¦ä¹ æŒ‘æˆ˜è€Œè®¾è®¡ã€‚XAMPLERé¦–å…ˆåŸºäºGlot500ï¼ˆä¸€ç§å°å‹å¤šè¯­è¨€æ¨¡å‹ï¼‰è®­ç»ƒæ£€ç´¢å™¨ï¼Œä½¿ç”¨ç”±å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå³MaLA500ï¼‰é¢„æµ‹æ„å»ºçš„æ­£é¢å’Œè´Ÿé¢è‹±è¯­ç¤ºä¾‹ã€‚åˆ©ç”¨æ£€ç´¢å™¨çš„è·¨è¯­è¨€åŠŸèƒ½ï¼Œå®ƒå¯ä»¥ç›´æ¥æ£€ç´¢è‹±è¯­ç¤ºä¾‹ä½œä¸ºç›®æ ‡è¯­è¨€çš„å°‘æ ·æœ¬ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚åœ¨åŒ…æ‹¬176ç§è¯­è¨€çš„SIB200å’ŒåŒ…æ‹¬16ç§è¯­è¨€çš„MasakhaNEWSä¸¤ä¸ªå¤šè¯­ç§æ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒXAMPLERæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cisnlp/XAMPLER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cisnlp/XAMPLERæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.05116v3">PDF</a> NAACL 2025 Findings</p>
<p><strong>Summary</strong><br>     æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ç°æˆçš„æˆ–ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ£€ç´¢å™¨ï¼Œèƒ½å¤Ÿé’ˆå¯¹è¾“å…¥æŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œä»è€Œå¢å¼ºè‹±è¯­çš„å°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•é€‚åº”åˆ°å…¶ä»–è¯­è¨€ï¼Œç‰¹åˆ«æ˜¯èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œé¢ä¸´ç¼ºä¹è·¨è¯­è¨€æ£€ç´¢å™¨å’Œæ³¨é‡Šæ•°æ®çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†XAMPLERï¼šè·¨è¯­è¨€ç¤ºä¾‹æ£€ç´¢æ–¹æ³•ï¼Œå®ƒåªéœ€ä½¿ç”¨è‹±è¯­æ³¨é‡Šæ•°æ®å³å¯è§£å†³è·¨è¯­è¨€ä¸Šä¸‹æ–‡å­¦ä¹ çš„æŒ‘æˆ˜ã€‚XAMPLERé¦–å…ˆåŸºäºGlot500ï¼ˆä¸€ç§å°å‹å¤šè¯­è¨€æ¨¡å‹ï¼‰è®­ç»ƒæ£€ç´¢å™¨ï¼Œä½¿ç”¨æ­£ä¾‹å’Œè´Ÿä¾‹è‹±è¯­ç¤ºä¾‹æ„å»ºè¿™äº›æ•°æ®æ¥è‡ªä¸€ä¸ªå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå³MaLA500ï¼‰çš„é¢„æµ‹ç»“æœã€‚åˆ©ç”¨æ£€ç´¢å™¨çš„è·¨è¯­è¨€èƒ½åŠ›ï¼Œå®ƒå¯ä»¥ç›´æ¥æ£€ç´¢è‹±è¯­ç¤ºä¾‹ä½œä¸ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ·æœ¬ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚åœ¨SIB200ï¼ˆåŒ…å«176ç§è¯­è¨€ï¼‰å’ŒMasakhaNEWSï¼ˆåŒ…å«16ç§è¯­è¨€ï¼‰ä¸¤ä¸ªå¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒXAMPLERæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨ç°æˆçš„æˆ–ç²¾ç»†è°ƒæ•´çš„æ£€ç´¢å™¨å¯ä»¥å¢å¼ºè‹±è¯­çš„å°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>é€‚åº”åˆ°å…¶ä»–è¯­è¨€é¢ä¸´ç¼ºä¹è·¨è¯­è¨€æ£€ç´¢å™¨å’Œæ³¨é‡Šæ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>XAMPLERæ–¹æ³•é€šè¿‡è®­ç»ƒè·¨è¯­è¨€æ£€ç´¢å™¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ£€ç´¢å™¨åŸºäºGlot500æ¨¡å‹å¹¶ä½¿ç”¨æ­£ä¾‹å’Œè´Ÿä¾‹è‹±è¯­ç¤ºä¾‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>XAMPLERèƒ½å¤Ÿç›´æ¥æ£€ç´¢è‹±è¯­ç¤ºä¾‹ä½œä¸ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ·æœ¬ç¤ºä¾‹ã€‚</li>
<li>åœ¨å¤šä¸ªå¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šï¼ŒXAMPLERæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœã€‚</li>
<li>XAMPLERçš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.05116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2405.05116v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2405.05116v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2405.05116v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2405.05116v3/page_3_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="How-Effective-are-Large-Language-Models-in-Generating-Software-Specifications"><a href="#How-Effective-are-Large-Language-Models-in-Generating-Software-Specifications" class="headerlink" title="How Effective are Large Language Models in Generating Software   Specifications?"></a>How Effective are Large Language Models in Generating Software   Specifications?</h2><p><strong>Authors:Danning Xie, Byungwoo Yoo, Nan Jiang, Mijung Kim, Lin Tan, Xiangyu Zhang, Judy S. Lee</strong></p>
<p>Software specifications are essential for many Software Engineering (SE) tasks such as bug detection and test generation. Many existing approaches are proposed to extract the specifications defined in natural language form (e.g., comments) into formal machine readable form (e.g., first order logic). However, existing approaches suffer from limited generalizability and require manual efforts. The recent emergence of Large Language Models (LLMs), which have been successfully applied to numerous SE tasks, offers a promising avenue for automating this process. In this paper, we conduct the first empirical study to evaluate the capabilities of LLMs for generating software specifications from software comments or documentation. We evaluate LLMs performance with Few Shot Learning (FSL) and compare the performance of 13 state of the art LLMs with traditional approaches on three public datasets. In addition, we conduct a comparative diagnosis of the failure cases from both LLMs and traditional methods, identifying their unique strengths and weaknesses. Our study offers valuable insights for future research to improve specification generation. </p>
<blockquote>
<p>è½¯ä»¶è§„æ ¼å¯¹äºè®¸å¤šè½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰ä»»åŠ¡ï¼Œå¦‚æ•…éšœæ£€æµ‹å’Œæµ‹è¯•ç”Ÿæˆï¼Œéƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚è®¸å¤šç°æœ‰æ–¹æ³•æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€å½¢å¼ï¼ˆä¾‹å¦‚æ³¨é‡Šï¼‰ä¸­å®šä¹‰çš„è§„å®šæå–ä¸ºæ­£å¼çš„æœºå™¨å¯è¯»å½¢å¼ï¼ˆä¾‹å¦‚ä¸€é˜¶é€»è¾‘ï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„é€šç”¨æ€§æœ‰é™ï¼Œéœ€è¦äººå·¥æ“ä½œã€‚æœ€è¿‘å‡ºç°çš„è‡ªç„¶è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆåŠŸåº”ç”¨äºè®¸å¤šSEä»»åŠ¡ï¼Œä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä»è½¯ä»¶æ³¨é‡Šæˆ–æ–‡æ¡£ä¸­ç”Ÿæˆè½¯ä»¶è§„æ ¼çš„èƒ½åŠ›è¿›è¡Œäº†é¦–æ¬¡å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰çš„èƒŒæ™¯ä¸‹è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†è¿™ç§æ€§èƒ½è¡¨ç°å’ŒåŸºäºä¼ ç»Ÿæ–¹æ³•çš„è¡¨ç°è¡¨ç°æ›´ä½³çš„åäº”ä¸ªæ¨¡å‹çš„è¡¨ç°æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¯”è¾ƒåˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¼ ç»Ÿæ–¹æ³•å„è‡ªåœ¨å¤„ç†å¤±è´¥æ¡ˆä¾‹ä¸­çš„ä¼˜åŠ¿ä¸ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ”¹è¿›è§„æ ¼ç”Ÿæˆæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.03324v3">PDF</a> This paper is accepted by the IEEE International Conference on   Software Analysis, Evolution and Reengineering (SANER) 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€å½¢å¼å®šä¹‰çš„è½¯ä»¶è§„æ ¼ï¼ˆå¦‚æ³¨é‡Šï¼‰æå–å¹¶å°†å…¶è½¬åŒ–ä¸ºæ­£å¼çš„å¯è¯»æœºå™¨å½¢å¼ï¼ˆå¦‚ä¸€é˜¶é€»è¾‘ï¼‰çš„ç ”ç©¶æ–¹æ³•å¤‡å—å…³æ³¨ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸€èˆ¬åŒ–èƒ½åŠ›æœ‰é™å’Œéœ€è¦äººå·¥å¹²é¢„çš„é—®é¢˜ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†è‡ªåŠ¨åŒ–å¤„ç†çš„æ½œåœ¨é€”å¾„ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å®è¯è¯„ä¼°äº†LLMä»è½¯ä»¶æ³¨é‡Šæˆ–æ–‡æ¡£ä¸­ç”Ÿæˆè½¯ä»¶è§„æ ¼çš„èƒ½åŠ›ï¼Œå¹¶å¯¹æ¯”äº†å…¶åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸‹çš„æ€§èƒ½ä¸ä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å¯¹LLMå’Œä¼ ç»Ÿæ–¹æ³•çš„å¤±è´¥æ¡ˆä¾‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œè¯†åˆ«äº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœªæ¥çš„è§„æ ¼ç”Ÿæˆç ”ç©¶æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶è§„æ ¼åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œç°æœ‰çš„è½¬æ¢æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè‡ªåŠ¨åŒ–å¤„ç†æä¾›äº†æ½œåœ¨é€”å¾„ã€‚</li>
<li>LLMåœ¨ç”Ÿæˆè½¯ä»¶è§„æ ¼æ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸‹è¡¨ç°çªå‡ºã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†13ç§æœ€æ–°LLMä¸ä¼ ç»Ÿæ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>LLMä¸ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤±è´¥æ¡ˆä¾‹æ—¶å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹æ”¹è¿›æœªæ¥çš„è§„æ ¼ç”Ÿæˆç ”ç©¶å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.03324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2306.03324v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2306.03324v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2306.03324v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2306.03324v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2306.03324v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_Few-Shot/2306.03324v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-12\./crop_I2I Translation/2411.12502v2/page_0_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  A Data-Efficient Pan-Tumor Foundation Model for Oncology CT   Interpretation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8a55e07fb790a3e01288324e7e95b5cc.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  Visual Agentic AI for Spatial Reasoning with a Dynamic API
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18588k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
