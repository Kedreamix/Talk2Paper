<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2024-12-12  Generative Semantic Communication Architectures, Technologies, and   Applications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    32.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    134 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Generative-Semantic-Communication-Architectures-Technologies-and-Applications"><a href="#Generative-Semantic-Communication-Architectures-Technologies-and-Applications" class="headerlink" title="Generative Semantic Communication: Architectures, Technologies, and   Applications"></a>Generative Semantic Communication: Architectures, Technologies, and   Applications</h2><p><strong>Authors:Jinke Ren, Yaping Sun, Hongyang Du, Weiwen Yuan, Chongjie Wang, Xianda Wang, Yingbin Zhou, Ziwei Zhu, Fangxin Wang, Shuguang Cui</strong></p>
<p>This paper delves into the applications of generative artificial intelligence (GAI) in semantic communication (SemCom) and presents a thorough study. Three popular SemCom systems enabled by classical GAI models are first introduced, including variational autoencoders, generative adversarial networks, and diffusion models. For each system, the fundamental concept of the GAI model, the corresponding SemCom architecture, and the associated literature review of recent efforts are elucidated. Then, a novel generative SemCom system is proposed by incorporating the cutting-edge GAI technology-large language models (LLMs). This system features two LLM-based AI agents at both the transmitter and receiver, serving as “brains” to enable powerful information understanding and content regeneration capabilities, respectively. This innovative design allows the receiver to directly generate the desired content, instead of recovering the bit stream, based on the coded semantic information conveyed by the transmitter. Therefore, it shifts the communication mindset from “information recovery” to “information regeneration” and thus ushers in a new era of generative SemCom. A case study on point-to-point video retrieval is presented to demonstrate the superiority of the proposed generative SemCom system, showcasing a 99.98% reduction in communication overhead and a 53% improvement in retrieval accuracy compared to the traditional communication system. Furthermore, four typical application scenarios for generative SemCom are delineated, followed by a discussion of three open issues warranting future investigation. In a nutshell, this paper provides a holistic set of guidelines for applying GAI in SemCom, paving the way for the efficient implementation of generative SemCom in future wireless networks. </p>
<blockquote>
<p>本文深入探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并进行了全面研究。首先介绍了三种由经典GAI模型启用的流行的SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，都阐述了GAI模型的基本概念、相应的SemCom架构以及最近的文献综述。然后，通过结合最前沿的GAI技术——大型语言模型（LLM），提出了一种新型的生成式SemCom系统。该系统在发射端和接收端都采用了基于LLM的AI代理，分别作为“大脑”，以实现强大的信息理解和内容再生能力。这种创新设计允许接收端根据发射端传递的编码语义信息直接生成所需内容，而不是恢复比特流。因此，它将通信思维从“信息恢复”转变为“信息再生”，从而开启了生成式SemCom的新时代。通过点对点视频检索的案例分析，展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，通信开销减少了99.98%，检索精度提高了5a 3%。此外，还介绍了生成式SemCom的四个典型应用场景，并讨论了三个需要未来研究的开放问题。总之，本文为GAI在SemCom中的应用提供了一套全面的指导方针，为未来无线网络中生成式SemCom的有效实施奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08642v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong><br>     本文研究了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并介绍了三种由经典GAI模型启用的SemCom系统。文章提出了结合最新GAI技术的大型语言模型（LLM）的生成式SemCom系统，实现了从“信息恢复”到“信息再生”的转变。该系统在点对点视频检索案例研究中表现出卓越性能，相较于传统通信系统，减少了99.98%的通信开销，提高了53%的检索精度。本文为GAI在SemCom中的应用提供了一套全面的指南，为未来的无线网络中实现高效的生成式SemCom铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了生成式人工智能（GAI）在语义通信（SemCom）中的应用及其研究背景。</li>
<li>阐述了三种基于经典GAI模型的SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。</li>
<li>提出了一种结合大型语言模型（LLM）的创新型生成式SemCom系统，实现了强大的信息理解和内容再生能力。</li>
<li>该系统在点对点视频检索案例研究中表现出卓越性能，显著降低了通信开销并提高了检索精度。</li>
<li>文章讨论了生成式SemCom在四个典型应用场景中的潜力。</li>
<li>强调了未来在开放问题上需要进一步研究和探索的方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0947d019af14f992664fd549f68a4b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-992887657b0d055041f816e833fbba5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb684a360750521f8bc80cff6b8f03cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f890f4c21e1505742bf4b41549683065.jpg" align="middle">
</details>




<h2 id="Fast-Prompt-Alignment-for-Text-to-Image-Generation"><a href="#Fast-Prompt-Alignment-for-Text-to-Image-Generation" class="headerlink" title="Fast Prompt Alignment for Text-to-Image Generation"></a>Fast Prompt Alignment for Text-to-Image Generation</h2><p><strong>Authors:Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang</strong></p>
<p>Text-to-image generation has advanced rapidly, yet aligning complex textual prompts with generated visuals remains challenging, especially with intricate object relationships and fine-grained details. This paper introduces Fast Prompt Alignment (FPA), a prompt optimization framework that leverages a one-pass approach, enhancing text-to-image alignment efficiency without the iterative overhead typical of current methods like OPT2I. FPA uses large language models (LLMs) for single-iteration prompt paraphrasing, followed by fine-tuning or in-context learning with optimized prompts to enable real-time inference, reducing computational demands while preserving alignment fidelity. Extensive evaluations on the COCO Captions and PartiPrompts datasets demonstrate that FPA achieves competitive text-image alignment scores at a fraction of the processing time, as validated through both automated metrics (TIFA, VQA) and human evaluation. A human study with expert annotators further reveals a strong correlation between human alignment judgments and automated scores, underscoring the robustness of FPA’s improvements. The proposed method showcases a scalable, efficient alternative to iterative prompt optimization, enabling broader applicability in real-time, high-demand settings. The codebase is provided to facilitate further research: <a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a> </p>
<blockquote>
<p>文本到图像的生成技术已经迅速发展，然而，将复杂的文本提示与生成的图像对齐仍然具有挑战性，特别是在复杂对象关系和精细细节方面。本文介绍了快速提示对齐（FPA）技术，这是一种提示优化框架，它采用一次通过的方法，提高了文本到图像的对齐效率，而无需使用当前方法（如OPT2I）所特有的迭代开销。FPA利用大型语言模型（LLM）进行单次迭代提示同义替换，随后通过优化提示进行微调或上下文内学习，以实现实时推理，降低了计算需求同时保持了对齐保真度。在COCO Captions和PartiPrompts数据集上的广泛评估表明，FPA在处理时间的一小部分内实现了具有竞争力的文本图像对齐得分，这已通过自动化指标（TIFA、VQA）和人类评估得到了验证。由专家注释者进行的人类研究进一步揭示了人类对齐判断与自动化得分之间的强烈相关性，强调了FPA改进的稳健性。所提出的方法展示了迭代提示优化的可扩展、高效替代方案，可在实时、高需求环境中得到更广泛的应用。代码库已提供，以方便进一步的研究：<a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08639v1">PDF</a> TikTok Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文提出了Fast Prompt Alignment（FPA）框架，利用大型语言模型进行单次迭代提示优化，实现文本到图像对齐的实时推理，大大提高了对齐效率并降低了计算需求。在COCO Captions和PartiPrompts数据集上的评估表明，FPA在文本图像对齐得分上表现优异，同时处理时间大幅减少。此外，专家标注人员的研究进一步验证了FPA改进的稳健性。该框架为实时高需求场景下的提示优化提供了可扩展、高效的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fast Prompt Alignment (FPA) 框架旨在解决文本到图像生成中对齐复杂文本提示与生成图像的问题。</li>
<li>FPA 利用大型语言模型进行单次迭代提示优化，提升文本到图像对齐效率。</li>
<li>FPA 实现了实时推理，减少计算需求的同时保持了对齐精度。</li>
<li>在 COCO Captions 和 PartiPrompts 数据集上的评估证明了 FPA 的高效性能。</li>
<li>通过专家标注人员的研究验证了 FPA 改进的稳健性。</li>
<li>FPA 提供了一种可扩展、高效的替代方案，适用于实时高需求场景下的提示优化。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f2dd326d41094289fdc7eb85cac210a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f6e9bfd70925ec0ec9b4c06d074f163.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fdf53b899e19af7c79ca324d7775040.jpg" align="middle">
</details>




<h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>多模态生成模型需要一种统一的方法来处理离散数据（如文本和代码）和连续数据（如图像、音频、视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），它使用因果Transformer无缝集成了连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入下一个令牌扩散来进行这些向量的自回归生成。此外，为了解决自回归建模中的方差崩溃问题，我们开发了σ-VAE。大量实验表明，LatentLM在各种模态中都非常有效。在图像生成方面，LatentLM在性能和可扩展性方面都超越了Diffusion Transformers。当集成到多模态大型语言模型中时，LatentLM提供了一个通用接口，统一了多模态生成和理解。实验结果表明，在扩大训练令牌规模方面，LatentLM与Transfusion和向量量化模型相比取得了有利的表现。在文本到语音的合成中，LatentLM在说话人相似性和稳健性方面超越了最先进的VALL-E 2模型，同时需要减少10倍的解码步骤。结果证明LatentLM是一种高度有效和可扩展的方法，可以推动大型多模态模型的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于离散数据（如文本和代码）和连续数据（如图像、音频和视频）的多模态生成模型需要一种统一的方法来处理。本研究提出了潜在语言建模（LatentLM），它利用因果Transformer无缝集成连续和离散数据。通过变分自编码器（VAE）表示连续数据为潜在向量，并引入下一个符号扩散进行这些向量的自回归生成。此外，为了解决自回归建模中的方差崩溃问题，开发了σ-VAE。实验表明，LatentLM在各种模态中的有效性。在图像生成方面，LatentLM在性能和可扩展性方面都超越了Diffusion Transformers。当集成到多模态大型语言模型中时，LatentLM提供了一个统一多模态生成和理解的通用接口。实验结果证明，LatentLM在扩大训练符号方面的表现优于Transfusion和向量量化模型。在文本到语音的合成中，LatentLM在说话人相似性和稳健性方面优于当前主流的VALL-E 2模型，并且只需要10倍的解码步骤。结果证明了LatentLM在推进大型多模态模型方面的高效性和可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LatentLM提出了一个统一的方法来处理离散和连续数据的多模态生成模型。</li>
<li>通过变分自编码器（VAE）表示连续数据为潜在向量，并引入下一个符号扩散进行自回归生成。</li>
<li>σ-VAE的引入解决了自回归建模中的方差崩溃问题。</li>
<li>LatentLM在图像生成方面表现出优异的性能和可扩展性。</li>
<li>LatentLM集成到多模态大型语言模型时，提供了统一的接口。</li>
<li>在扩大训练符号方面，LatentLM的表现优于其他模型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics"><a href="#Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics" class="headerlink" title="Synthetic Vision: Training Vision-Language Models to Understand Physics"></a>Synthetic Vision: Training Vision-Language Models to Understand Physics</h2><p><strong>Authors:Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan</strong></p>
<p>Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to enhance VLMs’ physical reasoning capabilities using simulated data. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs generated from simulations relevant to physical reasoning tasks. Second, we introduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to create scene descriptions enriched with physical properties and processes. During physical reasoning tasks, these PCBs can be leveraged as context to assist a Large Language Model (LLM) to improve its performance. We evaluate both of our approaches using multiple benchmarks, including a new stability detection QA dataset called Falling Tower, which includes both simulated and real-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM can significantly outperform larger state-of-the-art foundational models. We also show that integrating PCBs boosts the performance of foundational LLMs on physical reasoning tasks. Using the real-world scenes from the Falling Tower dataset, we also validate the robustness of both approaches in Sim2Real transfer. Our results highlight the utility that simulated data can have in the creation of learning systems capable of advanced physical reasoning. </p>
<blockquote>
<p>涉及动态环境中物体行为的解释、理解和预测的物理推理仍然是当前视觉语言模型（VLMs）的一个重大挑战。在这项工作中，我们提出了两种使用模拟数据增强VLM物理推理能力的方法。首先，我们使用与物理推理任务相关的模拟生成的问答（QA）对来微调预训练的VLM。其次，我们引入了物理上下文构建器（PCBs），这是经过精细调整的专门用于创建场景描述的VLM，其中包含丰富的物理属性和过程。在物理推理任务中，可以利用这些PCB作为上下文，帮助大型语言模型（LLM）提高其性能。我们使用多个基准测试来评估我们的两种方法，包括一个新的名为“坠落之塔”的稳定性检测问答数据集，其中包括模拟和真实场景，以及CLEVRER。我们证明，经过小型问答对微调过的VLM可以显著优于较大的最新基础模型。我们还表明，集成PCB可以提高基础LLM在物理推理任务上的性能。我们还使用来自“坠落之塔”数据集的真实场景验证了两种方法的Sim2Real迁移鲁棒性。我们的结果强调了模拟数据在创建能够进行高级物理推理的学习系统方面的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了两种使用模拟数据增强视觉语言模型（VLMs）物理推理能力的方法。首先，通过对预训练的VLM进行微调，利用与物理推理任务相关的模拟生成的问答对来增强其物理推理能力。其次，引入了物理语境构建器（PCBs），它们是经过微调专门用于创建场景描述的VLMs，富含物理属性和过程。在物理推理任务中，可以利用这些PCBs作为上下文来辅助大型语言模型（LLM）提高性能。评估方法包括新的稳定性检测问答数据集Falling Tower和CLEVRER等多个基准测试。结果显示，小型问答微调VLM显著优于大型先进基础模型，集成PCBs则提高了基础LLM在物理推理任务上的性能。在Falling Tower数据集的实时场景上验证了两种方法在模拟到真实迁移的稳健性。总结起来，本文强调模拟数据在创建具备高级物理推理能力的学习系统中的应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>物理推理是当前视觉语言模型（VLMs）面临的重要挑战，涉及动态环境中物体行为的解释、理解和预测。</li>
<li>提出了两种增强VLMs物理推理能力的方法：微调预训练VLM使用模拟生成的与物理推理任务相关的问答对；引入物理语境构建器（PCBs）作为上下文辅助大型语言模型（LLM）。</li>
<li>通过多个基准测试，包括新的稳定性检测问答数据集Falling Tower和CLEVRER，对两种方法进行了评估。</li>
<li>小型问答微调VLM表现出显著性能，优于大型先进基础模型；集成PCBs提高了基础LLM在物理推理任务上的性能。</li>
<li>两种方法在模拟到真实迁移的稳健性上得到了验证，特别是在实时场景上。</li>
<li>本文强调了模拟数据在创建具备高级物理推理能力的学习系统中的应用价值。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e2f305523ebfa259f73cf14e4f47132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6539bfab04a65e9677b31143ef8a9ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa20a5f614e80dba0b3c546f3ff6045b.jpg" align="middle">
</details>




<h2 id="Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models"><a href="#Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models"></a>Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models</h2><p><strong>Authors:Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong</strong></p>
<p>Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic">https://github.com/jiah-li/magic</a>. </p>
<blockquote>
<p>尽管利用对齐技术训练大型语言模型（LLM）以增强生成内容的安全性的技术已经取得了进展，但这些模型仍然容易受到越狱攻击的影响，这是一种暴露LLM安全漏洞的对抗性攻击方法。值得注意的是，贪婪坐标梯度（GCG）方法已经证明能够自动生成对抗性后缀，突破最先进的大型语言模型。然而，GCG涉及的优化过程非常耗时，使得越狱管道效率低下。在本文中，我们研究了GCG的过程，并发现了间接效应的问题，这是GCG优化的关键瓶颈。为此，我们提出了模型攻击梯度指数GCG（MAGIC），通过利用后缀令牌的梯度信息来解决间接效应问题，从而通过减少计算和迭代次数来加速程序。我们在AdvBench上的实验表明，MAGIC实现了高达1.5倍的速度提升，同时保持攻击成功率（ASR）与其他基线持平甚至更高。我们的MAGIC在Llama-2上实现了74%的ASR，在对GPT-3.5进行迁移攻击时实现了54%的ASR。代码可在<a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jiah-li/magic上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08615v1">PDF</a> 13 pages,2 figures, accepted by The 31st International Conference on   Computational Linguistics</p>
<p><strong>摘要</strong></p>
<p>随着大型语言模型（LLM）的对齐技术不断进步，其生成内容的安全性得到了提升。然而，这些模型仍然面临一种名为“越狱”的对抗性攻击方法，暴露了LLM的安全漏洞。本文研究了贪婪坐标梯度（GCG）方法，该方法能够自动生成对抗性后缀以破解最新LLM。然而，GCG涉及的优化过程非常耗时，使得越狱管道效率低下。本文调查了GCG的过程，并确定了间接效应这一关键瓶颈。为此，我们提出了模型攻击梯度指数GCG（MAGIC），通过利用后缀标记的梯度信息来解决间接效应问题，从而通过减少计算和迭代次数来加速程序。在AdvBench上的实验表明，MAGIC实现了最高达1.5倍的速度提升，同时保持攻击成功率（ASR）与其他基线持平甚至更高。我们的MAGIC在对Llama-2的攻击中实现了74%的ASR，在对GPT-3.5进行迁移攻击时实现了54%的ASR。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）仍面临对抗性攻击，即“越狱”，这暴露了其安全漏洞。</li>
<li>贪婪坐标梯度（GCG）方法可以自动生成的对抗性后缀来破解先进的LLM，但优化过程高度耗时。</li>
<li>GCG方法的瓶颈在于间接效应。</li>
<li>提出了模型攻击梯度指数GCG（MAGIC）来解决间接效应问题，通过利用梯度信息加速越狱过程。</li>
<li>MAGIC在AdvBench实验上实现了速度提升，同时保持或提高攻击成功率（ASR）。</li>
<li>MAGIC对Llama-2的攻击成功率为74%，对GPT-3.5的迁移攻击成功率为54%。</li>
<li>MAGIC的代码已公开提供。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-58d6ac4218806ce271467b0e57043d2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1402f5cd5315046c630f6d625906afaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f3cc516321175daee06622fd2d711f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-740263989a70e39b604d8c517b02ca4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08274e4db02b3e1e8c5285e734fedd36.jpg" align="middle">
</details>




<h2 id="Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval"><a href="#Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval" class="headerlink" title="Preference Discerning with LLM-Enhanced Generative Retrieval"></a>Preference Discerning with LLM-Enhanced Generative Retrieval</h2><p><strong>Authors:Fabian Paischer, Liu Yang, Linfeng Liu, Shuai Shao, Kaveh Hassani, Jiacheng Li, Ricky Chen, Zhang Gabriel Li, Xialo Gao, Wei Shao, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Hamid Eghbalzadeh</strong></p>
<p>Sequential recommendation systems aim to provide personalized recommendations for users based on their interaction history. To achieve this, they often incorporate auxiliary information, such as textual descriptions of items and auxiliary tasks, like predicting user preferences and intent. Despite numerous efforts to enhance these models, they still suffer from limited personalization. To address this issue, we propose a new paradigm, which we term preference discerning. In preference dscerning, we explicitly condition a generative sequential recommendation system on user preferences within its context. To this end, we generate user preferences using Large Language Models (LLMs) based on user reviews and item-specific data. To evaluate preference discerning capabilities of sequential recommendation systems, we introduce a novel benchmark that provides a holistic evaluation across various scenarios, including preference steering and sentiment following. We assess current state-of-the-art methods using our benchmark and show that they struggle to accurately discern user preferences. Therefore, we propose a new method named Mender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce $\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods and achieves state-of-the-art performance on our benchmark. Our results show that Mender can be effectively guided by human preferences even though they have not been observed during training, paving the way toward more personalized sequential recommendation systems. We will open-source the code and benchmarks upon publication. </p>
<blockquote>
<p>顺序推荐系统旨在根据用户的交互历史为其提供个性化推荐。为了实现这一目标，它们通常会融入辅助信息，如物品的文本描述和辅助任务，如预测用户偏好和意图。尽管已经有很多努力来提升这些模型，但它们仍然面临着个性化有限的挑战。为了解决这个问题，我们提出了一种新的方法，我们称之为偏好识别。在偏好识别中，我们显式地将生成式顺序推荐系统建立在用户偏好之上。为此，我们基于用户评论和特定物品数据，使用大型语言模型（LLM）生成用户偏好。为了评估顺序推荐系统的偏好识别能力，我们引入了一个新的基准测试，该测试可以在各种情境下进行全面评估，包括偏好控制和情感跟随。我们使用此基准测试评估了当前最先进的方法，并发现它们在准确识别用户偏好方面存在困难。因此，我们提出了一种新的方法，名为Mender（多模式偏好辨别器），它在现有方法的基础上进行了改进，并在我们的基准测试上达到了最先进的性能。我们的结果表明，即使在没有在训练中观察到人类偏好的情况下，Mender也可以有效地被人类偏好所引导，为更个性化的顺序推荐系统铺平了道路。在发布时，我们将公开源代码和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08604v1">PDF</a> 11 pages + references and appendix</p>
<p><strong>Summary</strong></p>
<p>基于用户历史交互的序列推荐系统旨在为用户提供个性化推荐。为达到此目的，它们经常结合辅助信息，如物品的文字描述和预测用户偏好和意图的辅助任务。尽管有很多提高这些模型的努力，它们仍然面临个性化有限的挑战。为解决此问题，我们提出了一个新的范式——偏好辨别。在偏好辨别中，我们明确地在生成序列推荐系统中以用户偏好为条件。为此，我们基于用户评论和特定物品数据使用大型语言模型（LLM）生成用户偏好。为了评估序列推荐系统的偏好辨别能力，我们引入了一个新的基准测试，该测试全面评价了不同场景，包括偏好引导和情感跟随。我们评估了当前先进的方法，并发现它们在准确辨别用户偏好方面存在困难。因此，我们提出了一种新的方法——Mender（多模态偏好鉴别器），它在现有方法的基础上进行了改进，并在我们的基准测试中达到了最新性能。结果显示，Mender即使在没有在训练中观察到的人类偏好也能进行有效指导，为更个性化的序列推荐系统铺平了道路。我们将公开代码和基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>序列推荐系统旨在基于用户历史交互提供个性化推荐。</li>
<li>现有序列推荐系统存在个性化有限的挑战。</li>
<li>提出新的范式——偏好辨别来解决这个问题。</li>
<li>利用大型语言模型（LLM）生成用户偏好。</li>
<li>引入新的基准测试来评估序列推荐系统的偏好辨别能力。</li>
<li>当前先进方法在准确辨别用户偏好方面存在困难。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-61ed96decf60a1e589ef18daa7655bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59fa7d1b83e1d1accb0dc4597b2f1772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e64ce1c732f27adb0a7a700843471db3.jpg" align="middle">
</details>




<h2 id="Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden"><a href="#Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden" class="headerlink" title="Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?"></a>Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?</h2><p><strong>Authors:Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro</strong></p>
<p>By means of pilot experiments for the language pair German and Galician, this paper examines the concept of efficiency and intelligence in lexicography and artificial intelligence, AI. The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained. Both quantitative and qualitative methods are used for this purpose. The analysis is based on the evaluation of the outputs of several sessions with the same prompt in ChatGPT 3.5. On the one hand, the algorithmic performance of intelligent systems is evaluated in comparison with data from lexicographical works. On the other hand, the ChatGPT data supplied is analysed using specific text passages of the aforementioned lexicographical text type. The results of this study not only help to evaluate the efficiency of this chatbot regarding the creation of dictionary articles, but also to delve deeper into the concept of intelligence, the thought processes and the actions to be carried out in both disciplines. </p>
<blockquote>
<p>本文通过针对德语和加利西亚语语言对的试点实验，探讨了词典学、人工智能（AI）中的效率和智能概念。这些实验的目的是实证地、统计地了解ChatGPT 3.5对词典文本类型（词典词条）的响应，以及该聊天机器人所训练的词典数据。为此目的，使用了定量和定性方法。分析是基于对ChatGPT 3.5中相同提示下多次会话输出的评估进行的。一方面，智能系统的算法性能与词典作品的数据进行评估比较。另一方面，使用上述词典文本类型的特定文本段落分析ChatGPT提供的数据。本研究的结果不仅有助于评估该聊天机器人在创建词典文章方面的效率，而且有助于更深入地了解智能的概念、思维过程和这两个学科中需要执行的操作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08599v1">PDF</a> 25 pages, in German language</p>
<p><strong>Summary</strong></p>
<p>本论文通过德语和加利西亚语的试点实验，探讨了词典编纂与人工智能中的效率与智能概念。实验旨在实证和统计地分析ChatGPT 3.5对于词典条目的回应，并研究该聊天机器人所训练的词典数据。研究采用了定量和定性方法，通过对同一提示下ChatGPT 3.5的多轮输出进行评估，对比智能系统的算法性能与词典作品的数据。此外，还使用特定的词典文本类型对ChatGPT的数据进行了深入分析。本研究结果不仅有助于评估该聊天机器人在创建词典条目方面的效率，还深入探讨了智能、思维过程和行动在词典编纂与人工智能两个领域中的应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究通过德语和加利西亚语的试点实验，探索了词典编纂和人工智能中的效率与智能。</li>
<li>实验旨在实证分析ChatGPT 3.5对词典条目的回应以及其所训练的词典数据。</li>
<li>研究采用了定量和定性方法，重点评估了ChatGPT 3.5的多轮输出。</li>
<li>研究对比了智能系统的算法性能与词典作品的数据。</li>
<li>特定的词典文本类型被用于深入分析ChatGPT的数据。</li>
<li>研究结果有助于评估聊天机器人在创建词典条目方面的效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-175b29133a78d0d939af8cbd1fa0ec67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ad5e61c6be29db174ba7ef06c55f62.jpg" align="middle">
</details>




<h2 id="Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks"><a href="#Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks" class="headerlink" title="Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks"></a>Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks</h2><p><strong>Authors:Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi</strong></p>
<p>Ensuring that Software Requirements Specifications (SRS) align with higher-level organizational or national requirements is vital, particularly in regulated environments such as finance and aerospace. In these domains, maintaining consistency, adhering to regulatory frameworks, minimizing errors, and meeting critical expectations are essential for the reliable functioning of systems. The widespread adoption of large language models (LLMs) highlights their immense potential, yet there remains considerable scope for improvement in retrieving relevant information and enhancing reasoning capabilities. This study demonstrates that integrating a robust Graph-RAG framework with advanced prompt engineering techniques, such as Chain of Thought and Tree of Thought, can significantly enhance performance. Compared to baseline RAG methods and simple prompting strategies, this approach delivers more accurate and context-aware results. While this method demonstrates significant improvements in performance, it comes with challenges. It is both costly and more complex to implement across diverse contexts, requiring careful adaptation to specific scenarios. Additionally, its effectiveness heavily relies on having complete and accurate input data, which may not always be readily available, posing further limitations to its scalability and practicality. </p>
<blockquote>
<p>确保软件需求规格（SRS）与更高层次的组织或国家要求相一致是至关重要的，特别是在金融和航空等受监管的环境中。在这些领域，保持一致性、遵守监管框架、减少错误和满足关键期望是系统可靠运行的关键。大型语言模型（LLM）的广泛应用突显了其巨大的潜力，但在检索相关信息和提高推理能力方面仍有很大的改进空间。本研究表明，将稳健的图-RAG框架与先进的提示工程技术（如思维链和思维树）相结合，可以显著提高性能。与基线RAG方法和简单提示策略相比，这种方法提供了更准确和上下文感知的结果。虽然这种方法在性能上取得了显著的改进，但它也带来了挑战。在多种背景下实施既昂贵又更复杂，需要谨慎适应特定场景。此外，其有效性严重依赖于完整和准确的数据输入，这可能并不总是轻易可用，进一步限制了其可扩展性和实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文将介绍如何确保软件需求规格（SRS）与组织或国家的高层次需求一致的重要性，特别是在金融和航空等受监管的环境中的重要性。同时，文章强调了大型语言模型（LLM）在提高信息检索和推理能力方面的巨大潜力及仍有改进的空间。该研究展示了一个稳健的Graph-RAG框架与先进的提示工程技术相结合，如链式思维和树状思维，可以显著提高性能。虽然这种方法在性能上取得了显著改进，但其成本高、实现复杂且涉及多样情境适应性调整。此外，该方法的有效性取决于是否拥有完整和准确的数据输入，这进一步限制了其可扩展性和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键见解的要点摘要：</p>
<ol>
<li>软件需求规格与高层级组织或国家要求的对齐在金融和航空等领域极其重要。</li>
<li>大型语言模型（LLM）在信息检索和推理能力方面展现出巨大潜力。但仍需改进以满足具体需求。</li>
<li>集成Graph-RAG框架与先进的提示工程技术可以提高性能，如链式思维和树状思维方法。</li>
<li>与基线相比，该方法提供了更准确和上下文感知的结果。然而，其成本高且实现复杂。</li>
<li>该方法需要适应不同的情境，并具有应对多种情况的复杂性挑战。它的成功应用需谨慎实施特定场景的策略和调整。</li>
<li>方法的有效性取决于拥有完整和准确的数据输入，这限制了其在实际应用中的可扩展性。在实际应用中需注意数据的完整性和准确性问题。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7979fb6e8cc3e979d2e01584d841f3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc73788341c65dd14908806ee3ec036d.jpg" align="middle">
</details>




<h2 id="Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning"><a href="#Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning" class="headerlink" title="Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning"></a>Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning</h2><p><strong>Authors:Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang</strong></p>
<p>Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance. </p>
<blockquote>
<p>编码器模型（例如BERT、RoBERTa）和大语言模型（LLM，例如Llama3）已被广泛应用于文本分类任务。然而，关于基于编码器的模型和LLM在文本分类中的性能对比的系统性研究仍然缺乏，尤其是在涉及微调时。本研究采用了不同规模和架构的多种模型和方法，包括微调和预先训练的方法。我们首先评估了这些LLM在20新闻组（20NG）和MASSIVE数据集上的性能，并将其与仅使用编码器的RoBERTa模型进行了比较。此外，我们还探索了两种模型类型的多任务功能，通过结合多个分类任务（包括意图检测和槽填充），使用这两个数据集的数据构建单一模型。我们的结果表明，完全微调后的Llama3-70B模型在各种分类任务和数据集上的性能优于RoBERTa大型模型和其他解码器LLM。此外，经过整合的多任务微调LLM在跨数据集的两个任务中的性能与双模型设置相匹配。总体而言，我们的研究提供了关于仅编码器和LLM模型在文本分类任务上的全面基准测试，并展示了一种将两个或多个完全调校的解码器LLM结合起来，以减少延迟并维持等效性能的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08587v1">PDF</a> 9 pages, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文对比研究了编码器模型（如BERT、RoBERTa）与大型语言模型（LLMs，如Llama3）在文本分类任务上的性能。研究采用多种规模和架构的模型和方法，包括微调和预先训练的方法，在20Newsgroups（20NG）和MASSIVE数据集上评估了LLMs与RoBERTa的性能。此外，研究还探索了两种模型类型的多任务能力，通过将多个分类任务（如意图检测和槽填充）整合到单一模型中。结果显示，完全微调后的Llama3-70B模型在各项分类任务和数据集上表现优于RoBERTa-large和其他解码器LLMs。同时，多任务微调后的LLMs在两项任务上的表现与双模型设置相匹配。总体而言，本文提供了编码器模型和LLM在文本分类任务上的全面基准测试，并展示了如何结合两个或多个完全微调后的解码器LLMs以实现降低延迟和等效性能的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比了编码器模型（如BERT、RoBERTa）和大型语言模型（LLMs，如Llama3）在文本分类任务上的性能。</li>
<li>在多个数据集（包括20Newsgroups和MASSIVE）上进行了广泛的实验评估。</li>
<li>LLMs在各项分类任务和数据集上的表现通常优于RoBERTa。</li>
<li>完全微调后的Llama3-70B模型表现出最佳性能。</li>
<li>多任务能力方面，多任务微调后的LLMs表现良好，可与双模型设置相匹配。</li>
<li>研究展示了结合多个完全微调后的解码器LLMs的方法，以实现降低延迟和等效性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-78a24e96e4b407822203e3c32af5b7ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c65aecbe11c6d8d21a7daee52179b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd99579b4a6b1de138e0cc90d02460d.jpg" align="middle">
</details>




<h2 id="TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs"><a href="#TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs" class="headerlink" title="TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs"></a>TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs</h2><p><strong>Authors:Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.   We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models. </p>
<blockquote>
<p>大型语言模型（LLM）的推理需要大量的计算和内存资源，特别是在关键注意力机制方面。虽然量化技术和加速算法（如FlashAttention）已经提高了整体推理的效率，但它们解决的问题不同：量化关注权重激活操作，而FlashAttention改进了执行但需要高精度格式。最近的键值（KV）缓存量化减少了内存带宽，但仍然需要浮点反量化进行注意力操作。我们提出了TurboAttention，这是一种全面的方法，能够实现对注意力的量化执行，同时解决内存和计算效率问题。我们的解决方案引入了两项关键创新：FlashQ，一种逐头注意力量化技术，能够实现KV缓存的压缩和激活激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它消除了在注意力指数运算期间反量化到FP32的需要。实验结果表明，TurboAttention实现了注意力速度的1.2-1.8倍提升，减少了KV缓存大小超过4.4倍，并且在各种数据集和模型上相对于FP16基线实现了最高达2.37倍的最大吞吐量，超越了最先进的量化和压缩技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM的推理需要大量的计算和内存资源，特别是在关键注意力机制方面。现有的技术如量化和FlashAttention等虽提高了推理效率，但仍存在不足。本文提出TurboAttention，一种综合的注意力量化执行方法，同时解决内存和计算效率问题。TurboAttention包括两项创新技术：FlashQ用于实现KV缓存压缩和量化执行激活乘法，以及基于稀疏性的Softmax近似（SAS），消除注意力指数运算中的去量化需求。实验结果显示TurboAttention在注意力方面实现了1.2-1.8倍的速度提升，KV缓存大小减少超过4.4倍，同时在各种数据集和模型上超越了现有量化技术。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM推理依赖于大量的计算和内存资源，特别是在注意力机制上。</li>
<li>量化和加速算法如FlashAttention已提高推理效率，但仍存在不足。</li>
<li>TurboAttention是一种综合方法，旨在实现注意力的量化执行，同时提高内存和计算效率。</li>
<li>TurboAttention包括两项关键技术：FlashQ用于KV缓存压缩和激活乘法的量化执行，以及SAS消除注意力运算中的去量化步骤。</li>
<li>实验结果表明TurboAttention在注意力处理上实现了显著的速度提升，并大幅减小了KV缓存大小。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2aa13ec6d967123c700eac65289299ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c9472feee293a58946daa11dab8b79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5d83ed70042472422f235eb4d579530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f134495953af18158cd020ded942ff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40958ea88bed04942910eb4a10c27a9c.jpg" align="middle">
</details>




<h2 id="Can-We-Generate-Visual-Programs-Without-Prompting-LLMs"><a href="#Can-We-Generate-Visual-Programs-Without-Prompting-LLMs" class="headerlink" title="Can We Generate Visual Programs Without Prompting LLMs?"></a>Can We Generate Visual Programs Without Prompting LLMs?</h2><p><strong>Authors:Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem</strong></p>
<p>Visual programming prompts LLMs (large language mod-els) to generate executable code for visual tasks like visual question answering (VQA). Prompt-based methods are difficult to improve while also being unreliable and costly in both time and money. Our goal is to develop an efficient visual programming system without 1) using prompt-based LLMs at inference time and 2) a large set of program and answer annotations. We develop a synthetic data augmentation approach and alternative program generation method based on decoupling programs into higher-level skills called templates and the corresponding arguments. Our results show that with data augmentation, prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive with state-of-the art models with the added benefit of much faster inference </p>
<blockquote>
<p>视觉编程提示大型语言模型（LLMs）为视觉任务生成可执行代码，如视觉问答（VQA）。基于提示的方法很难改进，同时不可靠，时间和金钱成本都很高。我们的目标是在不使用基于提示的LLM进行推断的情况下，并且不使用大量的程序和答案注释来开发高效的视觉编程系统。我们开发了一种合成数据增强方法，以及基于将程序解耦为高级技能（称为模板）和相应参数的替代程序生成方法。我们的结果表明，通过数据增强，无需提示的小型LLM（约1B参数）与最先进的模型相比具有竞争力，并且具有更快的推理优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于视觉编程提示，大型语言模型（LLM）可生成用于视觉任务（如视觉问答）的可执行代码。但提示型方法既难以改进，又存在可靠性不足的问题，且消耗大量时间和成本。我们的目标是开发一种高效的视觉编程系统，不使用提示型LLM进行推理，且无需大量程序和答案注释。我们提出了一种基于合成数据增强的方法，并通过将程序解耦为高级技能（称为模板）和相应参数来生成替代程序。研究结果表明，通过数据增强，无需提示的小型LLM与当前先进模型相当，并具备更快的推理优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉编程提示使LLM能够生成用于视觉任务的代码。</li>
<li>提示型方法存在改进困难、可靠性不足和成本高昂的问题。</li>
<li>目标开发一种高效的视觉编程系统，不使用提示型LLM进行推理。</li>
<li>提出基于合成数据增强的方法改进模型性能。</li>
<li>通过将程序解耦为模板和参数来生成替代程序。</li>
<li>数据增强使小型LLM表现与先进模型相当。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71a052fea07471cdd5b92ce33f76a2e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-495a2ed1307492f0e4d3b1fb89c1e5e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2871162a30b0025d43f44b1d6134418f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09544747a6fdbe241269e89ec78cea69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64a390556423516931dab19b62b4f26c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87feed1c478cec6f5d341f9548563b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d023468a4290179e5a74db0a79de7ee3.jpg" align="middle">
</details>




<h2 id="Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning"><a href="#Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning" class="headerlink" title="Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning"></a>Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning</h2><p><strong>Authors:Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreačić, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</strong></p>
<p>Large Language Models are trained on extensive datasets that often contain sensitive, human-generated information, raising significant concerns about privacy breaches. While certified unlearning approaches offer strong privacy guarantees, they rely on restrictive model assumptions that are not applicable to LLMs. As a result, various unlearning heuristics have been proposed, with the associated privacy risks assessed only empirically. The standard evaluation pipelines typically randomly select data for removal from the training set, apply unlearning techniques, and use membership inference attacks to compare the unlearned models against models retrained without the to-be-unlearned data. However, since every data point is subject to the right to be forgotten, unlearning should be considered in the worst-case scenario from the privacy perspective. Prior work shows that data outliers may exhibit higher memorization effects. Intuitively, they are harder to be unlearn and thus the privacy risk of unlearning them is underestimated in the current evaluation. In this paper, we leverage minority data to identify such a critical flaw in previously widely adopted evaluations. We substantiate this claim through carefully designed experiments, including unlearning canaries related to minority groups, inspired by privacy auditing literature. Using personally identifiable information as a representative minority identifier, we demonstrate that minority groups experience at least 20% more privacy leakage in most cases across six unlearning approaches, three MIAs, three benchmark datasets, and two LLMs of different scales. Given that the right to be forgotten should be upheld for every individual, we advocate for a more rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation framework represents an initial step toward ensuring more equitable assessments of LLM unlearning efficacy. </p>
<blockquote>
<p>大型语言模型（LLM）的训练数据集广泛且常包含敏感的人类生成信息，引发了关于隐私泄露的重大担忧。虽然经过认证的遗忘处理方法提供了强大的隐私保证，但它们依赖于不适用于LLM的模型假设。因此，已经提出了各种遗忘启发式方法，并对相关的隐私风险进行了实证评估。标准的评估流程通常从训练集中随机选择数据进行删除，应用遗忘技术，并使用成员推理攻击来比较未学习的模型与重新训练的模型（无需遗忘数据）。然而，由于每个数据点都有被遗忘的权利，从隐私的角度来看，遗忘应在最坏的情况下进行考虑。先前的研究表明，数据异常值可能表现出更高的记忆效果。直觉上，它们更难被遗忘，因此当前评估低估了遗忘它们的隐私风险。在本文中，我们利用少数群体数据来识别先前广泛采用的评估中的这一关键缺陷。我们通过精心设计的实验证实了这一主张，包括与少数群体相关的遗忘信标（canary），这是受隐私审计文献启发的。以个人身份信息作为代表性的少数群体标识符，我们证明在大多数情况下，在六种遗忘方法、三种成员推理攻击、三个基准数据集和两个不同规模的大型语言模型中，少数群体的隐私泄露至少增加了20%。鉴于每个个体都有被遗忘的权利，我们主张对大型语言模型的遗忘方法进行更严格的评估。我们的少数群体意识评估框架是确保大型语言模型遗忘效率评估更加公平的第一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08559v1">PDF</a> </p>
<p><strong>摘要</strong><br>大型语言模型（LLM）的训练数据包含大量敏感的人类生成信息，引发了关于隐私泄露的担忧。虽然经过认证的遗忘方法提供强有力的隐私保障，但它们依赖于不适用于LLM的模型假设。因此，人们提出了各种遗忘启发式方法，并对相关的隐私风险进行了实证评估。标准评估流程通常随机选择数据进行训练集删除，应用遗忘技术，并使用成员推理攻击来比较未学习的模型与不使用待遗忘数据的重新训练的模型。然而，从隐私角度来看，由于每个数据点都有被遗忘的权利，因此应考虑最坏的情况进行遗忘。先前的研究表明，数据异常值可能表现出更高的记忆效应，即它们更难被遗忘，因此当前评估中的隐私风险被低估了。本文利用少数数据来识别以前广泛采用的评估中的这一关键缺陷。通过精心设计的实验，包括与少数群体相关的遗忘金丝雀（受隐私审计文献启发），我们证明在大多数情况下，少数群体至少会经历20%以上的隐私泄露。因此，我们主张对LLM的遗忘方法进行更严格的评估。我们的面向少数的评估框架是确保更公平地评估LLM遗忘效率的第一步。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）的训练涉及敏感信息的隐私问题。</li>
<li>现有遗忘技术主要基于模型假设，可能不适用于LLM。</li>
<li>标准评估流程主要关注随机数据删除和成员推理攻击的比较，但未充分考虑最坏情况的遗忘。</li>
<li>数据异常值（即少数群体）在遗忘过程中可能具有更高的隐私泄露风险。</li>
<li>本文通过实验证明，少数群体在遗忘过程中至少面临20%以上的隐私泄露风险。</li>
<li>现有评估方法未能充分评估LLM遗忘方法的效果和公平性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d7df70e264508e34d7458412e0bc8a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-612dcf7498baba74ceee6359bb90c887.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c0407d01398c0d835aecbd08661ead9.jpg" align="middle">
</details>




<h2 id="MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback"><a href="#MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback" class="headerlink" title="MaestroMotif: Skill Design from Artificial Intelligence Feedback"></a>MaestroMotif: Skill Design from Artificial Intelligence Feedback</h2><p><strong>Authors:Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D’Oro</strong></p>
<p>Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM’s feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM’s code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability. </p>
<blockquote>
<p>描述自然语言中的技能具有将人类关于决策制定的知识注入人工智能系统的潜力。我们提出了MaestroMotif方法，这是一种人工智能辅助技能设计方法，能够产生高性能和可适应的代理。MaestroMotif利用大型语言模型（LLM）的功能来有效地创建和重用技能。它首先使用LLM的反馈来自动设计对应于每个技能的奖励，从他们的自然语言描述开始。然后，它利用LLM的代码生成能力与强化学习相结合，对技能进行训练，并将它们组合起来实现用语言指定的复杂行为。我们在NetHack学习环境（NLE）的一系列复杂任务中评估了MaestroMotif，结果表明它在性能和可用性方面都超越了现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08542v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了MaestroMotif方法，这是一种AI辅助技能设计方式，能生成高性能且能适应不同情境的AI代理。该方法利用自然语言描述技能的能力，通过大型语言模型（LLM）自动设计相应技能的奖励，并结合强化学习进行技能培训和复杂行为的实现。在NetHack学习环境的复杂任务测试中，MaestroMotif表现出卓越的性能和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaestroMotif是一种AI辅助技能设计方法，利用大型语言模型（LLM）实现技能的有效创建和再利用。</li>
<li>MaestroMotif通过自然语言描述技能，自动设计相应技能的奖励。</li>
<li>该方法结合强化学习进行技能培训，并能实现复杂的行为。</li>
<li>MaestroMotif在NetHack学习环境的复杂任务测试中表现优异。</li>
<li>MaestroMotif注重技能的适应性和性能的优化。</li>
<li>通过利用大型语言模型的能力，MaestroMotif可以注入人类决策知识到AI系统中。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f665febf0a88f026fb77766c9392837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-628a7f2a23e6422217c4fab0551ffeae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a055c6f27078f47409556b8e37ea80d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3b8bb5f78ec046bbf77ef4055438e59.jpg" align="middle">
</details>




<h2 id="EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance"><a href="#EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance" class="headerlink" title="EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance"></a>EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance</h2><p><strong>Authors:Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang</strong></p>
<p>As large language models (LLMs) continue to advance, the demand for higher quality and faster processing of long contexts across various applications is growing. KV cache is widely adopted as it stores previously generated key and value tokens, effectively reducing redundant computations during inference. However, as memory overhead becomes a significant concern, efficient compression of KV cache has gained increasing attention. Most existing methods perform compression from two perspectives: identifying important tokens and designing compression strategies. However, these approaches often produce biased distributions of important tokens due to the influence of accumulated attention scores or positional encoding. Furthermore, they overlook the sparsity and redundancy across different heads, which leads to difficulties in preserving the most effective information at the head level. To this end, we propose EMS to overcome these limitations, while achieving better KV cache compression under extreme compression ratios. Specifically, we introduce a Global-Local score that combines accumulated attention scores from both global and local KV tokens to better identify the token importance. For the compression strategy, we design an adaptive and unified Evict-then-Merge framework that accounts for the sparsity and redundancy of KV tokens across different heads. Additionally, we implement the head-wise parallel compression through a zero-class mechanism to enhance efficiency. Extensive experiments demonstrate our SOTA performance even under extreme compression ratios. EMS consistently achieves the lowest perplexity, improves scores by over 1.28 points across four LLMs on LongBench under a 256 cache budget, and preserves 95% retrieval accuracy with a cache budget less than 2% of the context length in the Needle-in-a-Haystack task. </p>
<blockquote>
<p>随着大型语言模型（LLM）的持续进步，对跨各种应用的高质量和快速处理长文本的需求不断增长。KV缓存因其能够存储先前生成的键和值令牌而得到广泛应用，从而有效地减少了推理过程中的冗余计算。然而，内存开销成为一个重要的问题，KV缓存的有效压缩也引起了越来越多的关注。现有的大多数方法从两个角度进行压缩：识别重要令牌和设计压缩策略。然而，这些方法通常会产生由于累积的注意力分数或位置编码的影响而产生的重要令牌的偏向分布。此外，它们忽视了不同头之间的稀疏性和冗余性，这使得在头部层面保留最有效信息变得困难。为此，我们提出EMS来克服这些局限性，同时实现在极端压缩比率下更好的KV缓存压缩。具体来说，我们引入了一个全局-局部分数，该分数结合了全局和局部KV令牌上的累积注意力分数，以更好地识别令牌的重要性。对于压缩策略，我们设计了一个自适应的、统一的逐出合并框架（Evict-then-Merge），该框架考虑了不同头之间KV令牌的稀疏性和冗余性。此外，我们通过零类机制实现了头部并行压缩，以提高效率。大量实验表明，即使在极端压缩比率下，我们的性能也达到了领先水平。EMS在极端压缩比率下始终实现了最低的困惑度，在LongBench上的四个LLM得分提高了超过1.28点，在Haystack任务中的Needle-in-a-Haystack场景下，在缓存预算少于上下文长度的2%的情况下保留了95%的检索准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08521v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大语言模型（LLM）中KV缓存的压缩问题。随着LLM的不断发展，对高质量、快速处理长上下文的需求不断增长，而KV缓存由于能够存储先前生成的键和值令牌，有效减少推理过程中的冗余计算而受到广泛采用。然而，内存开销成为一个重要的问题，因此KV缓存的压缩受到越来越多的关注。针对现有方法存在的局限性，如识别重要令牌时的偏见分布、忽略不同头部之间的稀疏性和冗余性等，本文提出了一种新的KV缓存压缩方法EMS。该方法通过结合全局和局部令牌的重要性得分来识别重要令牌，并采用自适应统一的驱逐合并框架进行压缩。实验表明，即使在极端压缩比率下，EMS也能实现更好的KV缓存压缩性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM的发展推动了KV缓存压缩的需求。</li>
<li>KV缓存存储键和值令牌，减少冗余计算。</li>
<li>现有KV缓存压缩方法存在识别重要令牌时的偏见分布问题。</li>
<li>EMS通过结合全局和局部令牌的重要性得分来识别重要令牌。</li>
<li>EMS采用自适应统一的驱逐合并框架进行压缩，考虑不同头部之间的稀疏性和冗余性。</li>
<li>EMS实现了高效的头并行压缩。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-38b496138c97e1dfd0f692985690d782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a51c48e21df8dbfe8058c928883ebc9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f36fbc21c25055b288075b463776d2e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d840c002b790302fa195ba1f62a5dacd.jpg" align="middle">
</details>




<h2 id="Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation"><a href="#Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation" class="headerlink" title="Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation"></a>Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation</h2><p><strong>Authors:Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang</strong></p>
<p>The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction. </p>
<blockquote>
<p>重排器和生成器是增强检索生成（即RAG）管道中的两个关键组件，负责排列相关文档和生成答复。然而，由于预训练数据和目标的不同，重排器排列的相关文档和生成器支持回答问题所需的文档之间存在不可避免的差距。为了解决这一差距，我们提出了RADIO，这是一个具有理性蒸馏（RAtionale DIstillatiOn）的新型实用偏好对齐框架。具体来说，我们首先提出了一种利用大型语言模型（LLM）的推理能力来提取用于回答查询所需的依据的理性提取方法。随后，设计了一种基于理性的对齐过程，根据提取的依据重新排列文档，并通过微调重排器来对齐偏好。我们在三个数据集的两个任务上进行了大量实验，以证明我们的方法相较于基准方法的有效性。我们的代码已在线发布，以方便复制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08519v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在检索增强生成（RAG）管道中，排名器和生成器两个核心组件的作用及其面临的挑战。针对两者在预训练数据和目标上的差异导致的差距问题，提出了一种名为RADIO的新型偏好对齐框架，利用大型语言模型（LLM）的推理能力提取回答查询所需的理由，基于这些理由重新排名文档，并微调排名器以对齐偏好。通过两个任务在三个数据集上的实验验证了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RANKER和生成器是RAG管道中的关键组件，分别负责排名相关文档和生成响应。</li>
<li>由于预训练数据和目标的不同，RANKER和生成器之间存在差距。</li>
<li>提出了一个名为RADIO的新型偏好对齐框架来解决这一差距问题。</li>
<li>使用大型语言模型（LLM）的推理能力提取回答查询的理由。</li>
<li>基于提取的理由重新排名文档，并微调RANKER以对齐偏好。</li>
<li>在两个任务、三个数据集上的实验验证了RADIO方法的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-07bd96bd382f50d4b0be40a29e0ad2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ff74513b2d945c44687fdee91eb0ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e8a25f53efe8135ab5000dfaaa6a7b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5ab945745fa9b9f0e527f229d00a670.jpg" align="middle">
</details>




<h2 id="POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications"><a href="#POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications" class="headerlink" title="POINTS1.5: Building a Vision-Language Model towards Real World   Applications"></a>POINTS1.5: Building a Vision-Language Model towards Real World   Applications</h2><p><strong>Authors:Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, Jie Zhou</strong></p>
<p>Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters </p>
<blockquote>
<p>视觉语言模型近期取得了重大进展，在各种任务中表现出卓越的性能，例如光学字符识别和复杂图表分析。基于这一趋势，我们推出了一款新的视觉语言模型——POINTS1.5，旨在在各种现实应用中获得卓越表现。POINTS1.5是POINTS1.0的增强版，并融入了若干关键创新：一）我们替换了原始的CLIP视觉编码器，该编码器具有固定图像分辨率，采用NaViT风格的视觉编码器，支持原生动态高分辨率。这使得POINTS1.5能够处理任何分辨率的图像，而无需将它们分割成瓦片。二）我们为POINTS1.5增加了双语支持，这极大地提高了其在中文方面的能力。由于视觉语言模型的中文开源数据集稀缺，我们从互联网收集了大量图像，并结合手动和自动方法进行标注。三）我们为视觉指令调整数据集提出了一系列严格的过滤方法。我们全面评估了所有这些过滤方法，并选择了最有效的过滤方法来获得最终的视觉指令调整集。得益于这些创新，POINTS1.5在性能上显著超越了POINTS1.0，并在一系列现实应用中表现出强大的性能。值得注意的是，POINTS1.5-7B在少于4亿个令牌上进行训练，在参数少于10亿的模型中排名第一，荣登OpenCompass排行榜首位。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08443v1">PDF</a> </p>
<p><strong>Summary</strong>：新型视觉语言模型POINTS1.5基于以往模型的优势进行创新，通过支持动态高分辨率图像处理和增强中文支持等功能，提升了在各种实际应用中的性能。通过严格筛选视觉指令微调数据集，并采用先进的过滤方法，其在多个指标上显著超越了前代模型POINTS1.0，表现强劲。同时，POINTS1.5在训练数据量方面也进行了优化，能够在少于4亿令牌的训练下达到优秀性能，且在OpenCompass排行榜上同类模型参数数量级内表现名列前茅。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>POINTS1.5是新一代的视觉语言模型，基于先前模型POINTs系列进行改进。</li>
<li>该模型通过引入NaViT风格的动态高分辨率视觉编码器，提升了图像处理能力。</li>
<li>POINTS1.5支持双语，特别是增强了中文处理能力。为解决中文数据集稀缺问题，该模型收集并标注了大量互联网图像。</li>
<li>模型通过严格的过滤方法筛选视觉指令微调数据集，并进行综合评估选择最有效的过滤方法获得最终数据集。</li>
<li>POINTS1.5在性能上显著超越前代模型POINTS1.0，特别是在训练数据量优化方面表现出色。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc8c0e0a9e3ea04dd5e87e92c23e234d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e46b078851b25402b8004072ad51698.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d7787c431966427ede1048c8a9a6984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2581b57e819a19b0d70e0fc3e8455559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ade5bc08b53234389efb475173eb8d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14763de648f4e6faa4d40ae81fa249ac.jpg" align="middle">
</details>




<h2 id="The-Roles-of-English-in-Evaluating-Multilingual-Language-Models"><a href="#The-Roles-of-English-in-Evaluating-Multilingual-Language-Models" class="headerlink" title="The Roles of English in Evaluating Multilingual Language Models"></a>The Roles of English in Evaluating Multilingual Language Models</h2><p><strong>Authors:Wessel Poelman, Miryam de Lhoneux</strong></p>
<p>Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from this imprecise method and instead focus on furthering language understanding. </p>
<blockquote>
<p>多语言自然语言处理得到了越来越多的关注，针对多种语言发布了众多模型、基准测试方法和手段。在跨语言评估中，英语常被用来提示语言模型（LMs），这主要是为了克服其他语言指令调整数据的缺乏。在这篇立场论文中，我们阐述了英语在跨语言LM评估中的两个作用：作为接口和作为自然语言。我们认为这两个作用的目标不同：任务执行与语言理解。通过数据集和评估设置的示例，这个差异得到了强调。许多作品明确地用英语作为接口来提升任务执行效率。我们建议摒弃这种不精确的方法，转而专注于提高语言理解能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08392v1">PDF</a> NoDaLiDa 2025</p>
<p><strong>Summary</strong></p>
<p>在多种语言自然语言处理领域中，英语在多语言语言模型评估中发挥着重要作用，主要体现在作为接口和自然语言两个方面。尽管其在任务性能上的表现备受关注，但其语言理解能力的差异也尤为重要。文中通过具体数据集和评估实例展示了这一差异。本文呼吁转向关注语言理解，而不是简单地以英语作为提高任务性能的接口。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>英语在多语言自然语言处理中扮演重要角色，用于促进语言模型（LMs）的多语言评估。</li>
<li>英语在评估中的两种角色是：作为接口和作为自然语言。</li>
<li>使用英语作为接口主要用于提高任务性能，但其存在局限性，未能充分体现语言理解能力。</li>
<li>重视语言理解能力的提升而非仅仅以英语为接口的方法进行评估的重要性。</li>
<li>当前许多工作利用英语作为界面推动任务执行的方式有待改进。</li>
<li>英语作为评估语言的一部分具有复杂的双重角色和多种目的考量。需要更加注重以不同视角进行评估的策略研究和实践创新。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4e4913fb529f06a6d8ce3be82bddd61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-425a52c1120dea05a7467888bdb16f84.jpg" align="middle">
</details>




<h2 id="SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs"><a href="#SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs" class="headerlink" title="SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs"></a>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs</h2><p><strong>Authors:Sultan Alrashed</strong></p>
<p>We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI’s Tulu 3 post-training pipeline to enhance Huggingface’s SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval ($\Delta$11%), and mathematical reasoning with 51.6% on GSM8K ($\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC ($\Delta5.4%$). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models. </p>
<blockquote>
<p>我们推出SmolTulu-1.7b-Instruct，本报告将其称为SmolTulu-DPO-1130，这是一款经过指令调整的语言模型，它适应了AllenAI的Tulu 3后训练管道，以强化Huggingface的SmolLM2-1.7B基础模型。我们通过使用1.35亿参数模型的全面实证分析证明，学习率与批次大小之间的关系会以一种任务依赖的方式显著影响模型性能。我们的研究发现了一个明确的划分：推理任务（如ARC和GSM8K）受益于较高的学习率与批次大小的比例，而模式识别任务（如HellaSwag和IFeval）在较低的比例下表现出最佳性能。这些见解为SmolTulu的开发提供了指导，SmolTulu在指令遵循方面达到了小于2B参数模型中的最新技术水平，在IFeval上得分67.7%（提高了11%），在GSM8K数学推理上得分51.6%（提高了3.4%），另一版本在ARC上得分57.1%（提高了5.4%）。我们发布我们的模型、训练配方和消融研究，以促进对高效模型对齐的进一步研究，表明优化动力学的仔细调整有助于缩小小型和大型语言模型之间的能力差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08347v1">PDF</a> 10 pages, 4 figures, and 13 tables. For the SmolTulu-1.7b-instruct   model, see: <a target="_blank" rel="noopener" href="https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct">https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct</a></p>
<p><strong>Summary</strong></p>
<p>SmolTulu-DPO-1130模型是基于Huggingface的SmolLM2-1.7B基础模型，通过采用AllenAI的Tulu 3后训练管道进行适应，开发了一种指令优化语言模型。研究发现学习率与批次大小之间的关系对模型性能有显著影响，不同任务有不同的最优比率。SmolTulu模型在指令遵循和数学推理任务上取得了卓越性能，并在相关数据集上取得了最新成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmolTulu-DPO-1130是基于Huggingface的SmolLM2-1.7B基础模型开发的一种指令优化语言模型。</li>
<li>模型采用了AllenAI的Tulu 3后训练管道进行适应。</li>
<li>学习率与批次大小的关系对模型性能有重要影响，不同任务需要不同的最优比率。</li>
<li>在指令遵循和数学推理任务上，SmolTulu模型取得了卓越性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-24a7f844f882bf1f6c5dd15898647b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cd5849efc9a4186ec9b7fd8c1be2dc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da3951edd3c067eb7bc6364e65cd3c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36b8936c38972af9d7874a2416d73f91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b4385e2bbae30872b51b25a5ff9ad95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9194cf58ba8f617b42c5e6436bb8b20f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a4ac62ce8ba497822f4f90331bb613.jpg" align="middle">
</details>




<h2 id="Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training"><a href="#Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training" class="headerlink" title="Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training"></a>Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training</h2><p><strong>Authors:Shijian Wang, Linxin Song, Jieyu Zhang, Ryotaro Shimizu, Ao Luo, Li Yao, Cunjian Chen, Julian McAuley, Hanqian Wu</strong></p>
<p>Current multimodal language models (MLMs) evaluation and training approaches overlook the influence of instruction format, presenting an elephant-in-the-room problem. Previous research deals with this problem by manually crafting instructions, failing to yield significant insights due to limitations in diversity and scalability. In this work, we propose a programmatic instruction template generator capable of producing over 39B unique template combinations by filling randomly sampled positional synonyms into weighted sampled meta templates, enabling us to comprehensively examine the MLM’s performance across diverse instruction templates. Our experiments across eight common MLMs on five benchmark datasets reveal that MLMs have high template sensitivities with at most 29% performance gaps between different templates. We further augment the instruction tuning dataset of LLaVA-1.5 with our template generator and perform instruction tuning on LLaVA-1.5-7B and LLaVA-1.5-13B. Models tuned on our augmented dataset achieve the best overall performance when compared with the same scale MLMs tuned on at most 75 times the scale of our augmented dataset, highlighting the importance of instruction templates in MLM training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a> . </p>
<blockquote>
<p>当前的多模态语言模型（MLM）评估和训练方法忽视了指令格式的影响，这就像一个房间里的大象问题。之前的研究通过手动制定指令来解决这个问题，但由于多样性和可扩展性的限制，未能产生重大见解。在这项工作中，我们提出了一种程序化指令模板生成器，通过向加权采样元模板中填充随机采样位置同义词，能够产生超过3.9万亿种独特的模板组合，从而能够全面考察MLM在不同指令模板下的性能。我们在五个基准数据集上对八个常见MLM进行的实验表明，MLM对模板的敏感性很高，不同模板之间性能差异最大达到29%。我们进一步使用我们的模板生成器增强了LLaVA-1.5的指令调整数据集，并在LLaVA-1.5-7B和LLaVA-1.5-13B上进行了指令调整。与我们增强数据集上调校的相同规模MLM相比，模型在整体性能上达到最佳，这表明在MLM训练中指令模板的重要性。代码可通过<a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/shijian2001/TemplateMatters获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08307v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a></p>
<p><strong>Summary</strong></p>
<p>本文指出当前的多模态语言模型（MLMs）在评估和训练过程中忽视了指令格式的影响，为此提出了一种程序化指令模板生成器。该生成器通过随机填充加权采样元模板中的位置同义词，能够产生超过39亿种独特的模板组合，从而全面考察MLM在不同指令模板下的性能。实验结果显示，MLMs对模板的敏感性很高，不同模板间性能差异最大可达29%。通过增强LLaVA-1.5的指令微调数据集并使用该模板生成器，模型在LLaVA-1.5-7B和LLaVA-1.5-13B上的性能得到了提升。与在扩大规模的数据集上训练的相同规模的MLMs相比，使用增强数据集训练的模型取得了最佳性能，突显了指令模板在MLM训练中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的多模态语言模型（MLMs）评估和训练忽略了指令格式的影响，这成为了一个需要关注的问题。</li>
<li>提出了一种程序化指令模板生成器，能够产生大量的独特模板组合，以全面考察MLM的性能。</li>
<li>MLMs对模板的敏感性高，不同模板间性能差异较大。</li>
<li>通过增强指令微调数据集并使用提出的模板生成器，模型性能得到了提升。</li>
<li>使用增强数据集训练的模型性能优于在扩大规模的数据集上训练的相同规模的MLMs。</li>
<li>指令模板在MLM训练中具有重要意义。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9cf18eab5fd423cf391c7a0d944472c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc8c42e8cb0aed2474be1f549a2daa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92fd609fdd9fbcf040f463966ba2a464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab98d0c0816798cff3762dfe29d4bbdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9158b85ce187a5753e0f5956ebbfa739.jpg" align="middle">
</details>




<h2 id="M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis"><a href="#M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis" class="headerlink" title="M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis"></a>M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis</h2><p><strong>Authors:Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang</strong></p>
<p>Sentiment analysis and emotion recognition are crucial for applications such as human-computer interaction and depression detection. Traditional unimodal methods often fail to capture the complexity of emotional expressions due to conflicting signals from different modalities. Current Multimodal Large Language Models (MLLMs) also face challenges in detecting subtle facial expressions and addressing a wide range of emotion-related tasks. To tackle these issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy for general-purpose MLLMs. It employs a combined approach to train models on tasks such as multimodal sentiment analysis, emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction. We also introduce the Emotion Multitask dataset (EMT), a custom dataset that supports these five tasks. Our model, Emotion Universe (EmoVerse), is built on a basic MLLM framework without modifications, yet it achieves substantial improvements across these tasks when trained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-of-the-art results in sentiment and emotion tasks. These results highlight the effectiveness of M2SE in enhancing multimodal emotion perception. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE">https://github.com/xiaoyaoxinyi/M2SE</a>. </p>
<blockquote>
<p>情感分析和情绪识别在人机交互和抑郁症检测等应用中至关重要。传统的单模态方法由于来自不同模态的信号相互冲突，往往无法捕捉情绪表达的复杂性。当前的多模态大型语言模型（MLLMs）在检测微妙的面部表情和应对各种情绪相关任务时也面临挑战。为了解决这些问题，我们提出了M2SE，这是一种用于通用MLLMs的多阶段多任务情感和情绪指令调整策略。它采用组合方法，在诸如多模态情感分析、情绪识别、面部表情识别、情感推理推断和情绪因果对提取等任务上训练模型。我们还介绍了情感多任务数据集（EMT），这是一个支持这五个任务的专业数据集。我们的模型“情感宇宙”（EmoVerse）建立在基本的MLLM框架上，无需修改，但在使用M2SE策略进行训练时，这些任务上的表现得到了显著提高。大量实验表明，EmoVerse优于现有方法，在情感和情绪任务中达到最新水平。这些结果凸显了M2SE在提高多模态情感感知方面的有效性。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaoyaoxinyi/M2SE中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了情感分析和情绪识别在多模态交互和抑郁症检测等领域的重要性。传统单模态方法无法捕捉情绪表达的复杂性，而现有的多模态大型语言模型（MLLMs）在检测细微面部表情和应对各种情绪相关任务时面临挑战。为此，本文提出了M2SE，一种用于通用MLLMs的多阶段多任务情感与情绪指令调整策略。该策略结合了多种任务训练模型，如多模态情感分析、情绪识别、面部表情识别、情感推理推断和情感因果关系提取等。同时，本文还介绍了支持这五个任务的情感多任务数据集EMT。实验结果表明，基于M2SE策略训练的模型Emotion Universe（EmoVerse）在多个任务上实现了显著的提升，并在情感与情绪任务上取得了最新成果。这证明了M2SE策略在提高多模态情感感知方面的有效性。数据集和代码可在GitHub上获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感分析和情绪识别对于人机交互和抑郁症检测等应用至关重要。</li>
<li>传统单模态方法难以捕捉情绪表达的复杂性，需要多模态方法来解决这一问题。</li>
<li>M2SE是一种用于通用MLLMs的多阶段多任务情感与情绪指令调整策略，结合了多种任务训练模型。</li>
<li>M2SE策略包括多任务如多模态情感分析、情绪识别等。</li>
<li>引入的情感多任务数据集EMT支持五个任务，有助于模型训练。</li>
<li>基于M2SE策略训练的模型Emotion Universe（EmoVerse）在多个任务上实现了显著的提升。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aa73ededcaefa000e8edfeba52a89b72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c263fb335b04bc682be5a1cfa8e6e297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4d6b6e0f4051342c6a0898d92d999f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32acf009fc023384915a65c05d3f43ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6c980356ed97ed5bc3e5d3fac353bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9184bb36e2f4b1291065f58d8c14d467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b8821a0398494275c6700434720d2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c8749cd2121f800ea47ff15bb3e861f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ab60fd15fbdd7102b7bf0b9ceb0523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f47588ea960ad6d8e96c0eaec02120.jpg" align="middle">
</details>




<h2 id="SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models"><a href="#SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models" class="headerlink" title="SAT: Spatial Aptitude Training for Multimodal Language Models"></a>SAT: Spatial Aptitude Training for Multimodal Language Models</h2><p><strong>Authors:Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko</strong></p>
<p>Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23%$ on CVBench, $8%$ on the harder BLINK benchmark, and $18%$ on VSR. When instruction-tuned on SAT, our 13B model matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data&#x2F;code is available at <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a> . </p>
<blockquote>
<p>空间感知是智能的基本组成部分。尽管许多研究强调大型多模态语言模型（MLM）在推理空间方面存在困难，但它们只测试静态空间推理，例如对物体的相对位置进行分类。然而，现实世界的应用需要动态能力，如观点采择和第一人称动作识别。作为提高空间智能的路线图，我们引入了SAT（空间适应能力训练），它超越了静态相对物体位置问题，涵盖了更动态的任务。SAT包含21.8万个问答对，涉及训练和测试集的2.2万个合成场景。使用逼真的物理引擎生成，我们的数据集可以任意扩展，并轻松扩展到新的动作、场景和3D资产。我们发现，即使在静态问题上表现相对较好的MLM也很难准确回答动态空间问题。此外，我们还表明，SAT指令调整数据不仅提高了SAT上的动态空间推理能力，还提高了现有真实图像空间基准的零样本性能：CVBench上提高23%，难度更高的BLINK基准上提高8%，以及VSR上提高18%。当在SAT上进行指令调整时，我们的13B模型在空间推理方面与较大的专有MLM（如GPT4-V和Gemini-3-1.0）相匹配。我们的数据和代码可在<a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07755v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a></p>
<p><strong>摘要</strong></p>
<p>空间感知是智能的基本组成部分。许多研究强调大型多模态语言模型（MLMs）在静态空间推理方面存在困难，但仅限于分类物体的相对位置等任务。然而，现实世界的应用需要更动态的能力，如视角和基于自我中心的行动识别等。为了改善空间智能，我们引入了SAT（空间能力训练），它超越了静态相对物体位置问题，涵盖了更动态的任务。SAT包含用于训练的合成场景中的问答对以及测试集的问答对。我们的数据集是使用逼真的物理引擎生成的，可以任意扩展并轻松扩展到新的动作、场景和三维资产。我们发现即使在静态问题上表现良好的MLM也很难准确回答动态空间问题。此外，我们还表明，SAT指令调整数据不仅提高了SAT上的动态空间推理能力，还提高了现有真实图像空间基准测试上的零样本性能：CVBench提高23%，难度更大的BLINK基准测试提高8%，以及VSR提高18%。当在SAT上经过指令调整后，我们的模型能在空间推理方面匹配较大的专有MLM（如GPT-4 V和双子座3-1.0）。我们的数据和代码可在<a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/%E8%8E%B7%E5%8F%96%E3%80%82">http://arijitray1993.github.io/SAT/获取。</a></p>
<p><strong>关键见解</strong></p>
<p>一、空间感知是智能的重要组成部分，对于现实世界的应用至关重要。<br>二、当前研究主要关注静态空间推理任务，例如物体相对位置的分类。然而，实际应用需要模型具备动态能力，如视角采取和基于自我中心的行动识别。这启示我们不仅需要研究静态空间感知，还要关注动态空间感知的研究。<br>三、引入新的数据集SAT（Spatial Aptitude Training），包含合成场景中的问答对，旨在提高模型的空间智能能力。数据集使用逼真的物理引擎生成，可任意扩展并轻松扩展到新的动作、场景和三维资产。这表明数据集的多样性和可扩展性对于训练模型至关重要。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4726f5787390ded5869518fbae49fa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1ac83f59cc86700f58e482ccc874082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8ee4e23739632ca4492284e948ea332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db09dd969f25a2ed0382bcf5c8ce13cc.jpg" align="middle">
</details>




<h2 id="Scaling-Sequential-Recommendation-Models-with-Transformers"><a href="#Scaling-Sequential-Recommendation-Models-with-Transformers" class="headerlink" title="Scaling Sequential Recommendation Models with Transformers"></a>Scaling Sequential Recommendation Models with Transformers</h2><p><strong>Authors:Pablo Zivic, Hernan Vazquez, Jorge Sanchez</strong></p>
<p>Modeling user preferences has been mainly addressed by looking at users’ interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation.   The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models.   Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation.   We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application.   We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency.   We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.   Code and models can be found at <a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt">https://github.com/mercadolibre/srt</a> </p>
<blockquote>
<p>通过查看用户与系统中可用元素之间的交互历史来主要解决用户偏好建模的问题。根据历史数据为个体量身定制内容是顺序推荐的主要目标。问题的性质以及在各个领域中观察到的良好性能，激发了使用变压器架构的动机。当模型参数数量增加时，该架构在利用越来越多的训练数据方面证明是有效的。这种扩展行为引起了极大的关注，因为它为设计更大的模型以及训练提供了有价值的指导。从训练大型语言模型中观察到的扩展定律中汲取灵感，我们探索了顺序推荐的类似原则。我们使用完整的亚马逊产品数据集，该数据集在其他研究中仅得到部分探索，并揭示了与语言模型中发现的类似的扩展行为。最优计算训练是可能的，但需要仔细分析特定于应用程序的计算性能权衡。我们还表明，通过在小任务特定领域上微调更大的预训练模型，性能扩展可以转化为下游任务。我们的方法和发现提供了在真实的高维偏好空间中训练模型和部署的战略路线图，有助于提高训练和推理效率。我们希望本文能够架起潜力无限的变压器与现实中复杂的高维顺序推荐系统之间的桥梁。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mercadolibre/srt找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要探讨如何通过用户的互动历史进行用户偏好建模，以及如何利用Transformer架构在序列推荐中个性化内容。文章通过利用亚马逊产品数据集，探索了类似语言模型的缩放规律，并展示了性能缩放如何应用于下游任务。此外，文章还强调了计算最优训练的重要性，并指出需要针对特定应用的计算性能权衡进行仔细分析。本文的研究结果和发现为在高维偏好空间中训练和优化模型提供了战略路线图。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章关注通过用户与系统元素的互动历史来建模用户偏好，这是序列推荐的主要目标。</li>
<li>Transformer架构已被证明在利用大量训练数据和提高模型参数数量方面非常有效，已成为该领域的主要研究焦点。</li>
<li>文章利用亚马逊产品数据集，发现了类似语言模型的缩放行为。</li>
<li>计算最优训练对序列推荐至关重要，但需要仔细分析特定应用的计算性能权衡。</li>
<li>更大的预训练模型可以通过微调适应小任务特定领域，展示性能缩放。</li>
<li>文章的研究结果提供了一个在高维偏好空间中训练和优化模型的战略路线图。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c1432e8c0cfc30fe4ce86a58f601e82f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f92d5d9f7e8fd994ab15fcdfbd147e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c5c840c67efc37f67360b32c677b1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-496f2027a50d34beb6c7b71d6e2dcca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b52962e1da932f53ab0187435c70ac4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d57eea9b2995fc84c92ca1a92d6276d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b5b0ccde95d9e8ffabc00bb100fa636.jpg" align="middle">
</details>




<h2 id="Causal-World-Representation-in-the-GPT-Model"><a href="#Causal-World-Representation-in-the-GPT-Model" class="headerlink" title="Causal World Representation in the GPT Model"></a>Causal World Representation in the GPT Model</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules. We find that the GPT model tends to generate next moves that adhere to the game rules for sequences for which the attention mechanism encodes a causal structure with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure. </p>
<blockquote>
<p>生成预训练Transformer（GPT）模型是否仅经过训练以预测下一个令牌，还是它们是否从世界模型中隐式学习生成序列的方式，即一个令牌接一个令牌地生成序列？我们通过推导GPT中注意力机制的因果解释，并建议由此产生的因果世界模型来探讨这个问题。此外，我们提出GPT模型在推理时间可以用于零射击因果结构学习，适用于内部分布序列。在Othello棋盘游戏的受控合成环境中进行了实证评估。使用在现实世界游戏中预训练的GPT模型（旨在获胜），测试仅遵循游戏规则合成数据。我们发现GPT模型倾向于生成遵循游戏规则的下一步动作序列，这些序列的注意力机制编码了高置信度的因果结构。总体而言，在GPT模型生成不遵守游戏规则的移动情况下，它也未能捕捉到任何因果结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v1">PDF</a> NeurIPS 2024 Workshop on Causality and Large Models (CaLM)</p>
<p><strong>Summary</strong></p>
<p>GPT模型是否仅通过预测下一个令牌进行训练，还是它们会隐式地从一个世界模型中学习并生成序列中的每一个令牌？本文通过推导GPT注意力机制的因果解释来探讨这个问题，并提出了由此产生的因果世界模型。此外，我们认为GPT模型在推理时可用于零射击因果结构学习，适用于内部分布序列。在Othello游戏的受控合成环境中进行了实证评估。一个经过真实世界游戏训练的GPT模型被测试在仅遵循游戏规则合成数据上。我们发现，当注意力机制编码高置信度的因果结构时，GPT模型倾向于生成遵循游戏规则的下一步动作序列。一般来说，当GPT模型生成的行动不遵循游戏规则时，它也无法捕捉到任何因果关系结构。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT模型不仅学习预测下一个令牌，而且从世界模型中隐式学习生成序列。</li>
<li>通过推导GPT注意力机制的因果解释，提出了一个因果世界模型。</li>
<li>GPT模型在推理阶段可用于零射击因果结构学习，适用于内部分布序列。</li>
<li>在Othello游戏的受控合成环境中进行了实证评估。</li>
<li>GPT模型在遵循游戏规则的合成数据上表现出生成遵循规则的下一步动作的趋势。</li>
<li>当GPT模型无法生成遵循游戏规则的行动时，它也无法捕捉到任何因果关系结构。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd533512299dae43bee0963fb6015772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg" align="middle">
</details>




<h2 id="HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass"><a href="#HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass" class="headerlink" title="HARP: Hesitation-Aware Reframing in Transformer Inference Pass"></a>HARP: Hesitation-Aware Reframing in Transformer Inference Pass</h2><p><strong>Authors:Romain Storaï, Seung-won Hwang</strong></p>
<p>This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to “off-the-shelf” Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact. </p>
<blockquote>
<p>本文旨在通过解决推理步骤中可变的计算需求来提高大型语言模型的性能，其中一些令牌需要比其他令牌更多的计算资源。我们提出了HARP，这是对“现货”Transformer前向传递的简单修改。它借鉴了决策制定中的犹豫和框架效应，当模型在令牌生成过程中遇到不确定性时，HARP会选择性地应用额外的计算。我们的方法通过在困难的决策点暂停并重新构建输入以获取不同视角来模仿人类的认知过程。与其他方法不同，HARP具有模型无关性、无需训练且易于实现。我们在各种下游任务和模型规模上全面评估了我们的方法，展示了高达+5.16%的性能提升。值得注意的是，HARP在保持推理时间是束搜索的两倍速度的同时实现了这些收益。简单而又能带来显著收益，HARP为在最小计算影响的情况下提高基于Transformer的语言模型性能提供了实用解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07282v1">PDF</a> </p>
<p><strong>Summary</strong><br>在简化背景下，本文提出一种名为HARP的方法，旨在通过针对推理步骤中的可变计算需求改进大型语言模型的性能。在遇到不确定的令牌生成时，HARP有选择性地应用额外的计算，模仿人类在决策过程中的犹豫和重构思维。此方法无需特殊训练，易于实现，并且在各种下游任务和模型大小上的评估中均表现出性能改进，最高可达+5.16%。同时，HARP保持推理时间是光束搜索的两倍快。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HARP旨在通过解决推理步骤中的可变计算需求，提高大型语言模型的性能。</li>
<li>HARP利用犹豫和决策制定中的重构效应，有选择地在模型遇到不确定性时应用额外的计算。</li>
<li>HARP模仿人类的认知过程，在决策点暂停并重新构建输入以获取不同视角。</li>
<li>HARP是一种模型无关、无需额外训练且易于实现的方法。</li>
<li>在各种下游任务和模型大小上评估，HARP显示出性能改进，最高可达+5.16%。</li>
<li>HARP在加快推理速度的同时维持了模型性能的提升。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bd54425be6c50fa98ef8656d707413c8.jpg" align="middle">

</details>




<h2 id="A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis"><a href="#A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis" class="headerlink" title="A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis"></a>A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis</h2><p><strong>Authors:Nimisha Ghosh, Daniele Santoni, Indrajit Saha, Giovanni Felici</strong></p>
<p>In recent times, Transformer-based language models are making quite an impact in the field of natural language processing. As relevant parallels can be drawn between biological sequences and natural languages, the models used in NLP can be easily extended and adapted for various applications in bioinformatics. In this regard, this paper introduces the major developments of Transformer-based models in the recent past in the context of nucleotide sequences. We have reviewed and analysed a large number of application-based papers on this subject, giving evidence of the main characterizing features and to different approaches that may be adopted to customize such powerful computational machines. We have also provided a structured description of the functioning of Transformers, that may enable even first time users to grab the essence of such complex architectures. We believe this review will help the scientific community in understanding the various applications of Transformer-based language models to nucleotide sequences. This work will motivate the readers to build on these methodologies to tackle also various other problems in the field of bioinformatics. </p>
<blockquote>
<p>近年来，基于Transformer的语言模型在自然语言处理领域产生了巨大的影响。由于生物序列和自然语言之间存在相似性，自然语言处理中使用的模型可以很容易地扩展和适应生物信息学的各种应用。在这方面，本文介绍了近期在核苷酸序列背景下基于Transformer模型的主要发展。我们回顾并分析了大量关于这一主题的应用论文，提供了主要特征证据和可能采取的不同方法来定制这些强大的计算机。我们还提供了Transformer工作机制的结构化描述，甚至可以让首次接触的用户也能理解这些复杂的架构。我们相信这次回顾将有助于科学界了解基于Transformer的语言模型在核苷酸序列方面的各种应用。这项工作将激励读者基于这些方法来解决生物信息学领域的各种问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07201v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该论文探讨了基于Transformer的模型在自然语言处理领域的应用，并指出这些模型可以轻易扩展到生物信息学领域。论文详细介绍了Transformer的主要特点和应用，包括其在核苷酸序列分析方面的应用，有助于科学界了解这些模型在生物信息学中的多种应用，并激励读者在此基础上解决其他生物信息学问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于Transformer的模型在自然语言处理领域有重大影响。</li>
<li>Transformer模型可轻易扩展到生物信息学领域。</li>
<li>论文详细阐述了Transformer的主要特点和应用。</li>
<li>Transformer模型在核苷酸序列分析方面有着重要应用。</li>
<li>该论文有助于科学界了解Transformer模型在生物信息学中的多种应用。</li>
<li>论文提供的结构化描述使得初学者也能理解Transformer的复杂架构。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-479dbd5c07313b7fbbcd5d183ab4d03c.jpg" align="middle">
</details>




<h2 id="ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models"><a href="#ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models" class="headerlink" title="ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models"></a>ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models</h2><p><strong>Authors:Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, silvio savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu</strong></p>
<p>With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image-based queries. Existing practices rely on powerful but costly large language models (LLMs) or multimodal language models (MLMs) to produce instruction data. These are often prone to hallucinations, licensing issues and the generation process is often hard to scale and interpret. In this work, we present a programmatic approach that employs scene graphs as symbolic representations of images and human-written programs to systematically synthesize vision-centric instruction data. Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy. By implementing a suite of 24 single-image, 14 multi-image instruction generators, and a scene graph generation pipeline, we build a scalable, cost-effective system: ProVision which produces diverse question-answer pairs concerning objects, attributes, relations, depth, etc., for any given image. Applied to Visual Genome and DataComp datasets, we generate over 10 million instruction data points, ProVision-10M, and leverage them in both pretraining and instruction tuning stages of MLMs. When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D split of CVBench, along with a 3% increase in performance on QBench2, RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8% improvement on Mantis-Eval. Incorporation of our data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks. </p>
<blockquote>
<p>随着多模态应用的兴起，指令数据对于训练能够理解复杂图像查询的多模态语言模型变得至关重要。现有实践依赖于强大但成本高昂的大型语言模型（LLM）或多模态语言模型（MLM）来生成指令数据。这些方法往往容易出现幻觉、许可问题，且生成过程往往难以扩展和解释。在这项工作中，我们提出了一种程序化方法，它利用场景图作为图像的象征表示和人类编写的程序来系统地合成以视觉为中心的指令数据。我们的方法确保了数据生成过程的可解释性和可控性，在保持事实准确性的同时，实现了高效扩展。通过实施24个单图像、14个多图像指令生成器以及场景图生成管道，我们建立了一个可扩展、成本效益高的系统：ProVision。该系统可针对给定图像生成涉及对象、属性、关系、深度等方面的多样化问答对。在Visual Genome和DataComp数据集上应用时，我们生成了超过1000万的指令数据点——ProVision-10M，并将其用于MLM的预训练和指令调整阶段。在指令调整阶段采用我们的单图像指令数据，在CVBench的2D分割和3D分割上分别提高了7%和8%的性能，同时在QBench2、RealWorldQA和MMMU上的性能提高了3%。我们的多图像指令数据在Mantis-Eval上提高了8%。在xGen-MM-4B的预训练和微调阶段都融入了我们的数据，在11个基准测试上的平均改进了1.6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07012v2">PDF</a> code: <a target="_blank" rel="noopener" href="https://github.com/JieyuZ2/ProVision">https://github.com/JieyuZ2/ProVision</a> dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Salesforce/ProVision-10M">https://huggingface.co/datasets/Salesforce/ProVision-10M</a></p>
<p><strong>摘要</strong><br>    随着多模态应用的兴起，指令数据对于训练能够理解复杂图像查询的多模态语言模型至关重要。本研究提出了一种程序化方法，利用场景图作为图像的象征性表示和人为编写的程序来系统地合成以视觉为中心指令数据。该方法确保了数据生成的可解释性和可控性，同时保持了事实准确性并实现了高效规模化。通过实施一系列单图像和多图像指令生成器以及场景图生成管道，我们建立了可扩展、成本效益高的系统——ProVision，该系统可为给定图像生成关于对象、属性、关系、深度等的多样化问答对。在Visual Genome和DataComp数据集上应用时，我们生成了超过1亿个指令数据点——ProVision-10M，并将其用于MLM的预训练和指令调整阶段。在指令调整阶段采用我们的单图像指令数据，在CVBench的2D分割和3D分割上分别提高了7%和8%的性能，同时在QBench2、RealWorldQA和MMMU上的性能也提高了3%。我们的多图像指令数据在Mantis-Eval上提高了8%。在xGen-MM-4B的预训练和微调阶段都融入了我们的数据，在11个基准测试中平均提高了1.6%的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>随着多模态应用的普及，指令数据对于训练能够理解复杂图像查询的多模态语言模型变得至关重要。</li>
<li>现有方法依赖强大的大型语言模型（LLMs）或多模态语言模型（MLMs）来生成指令数据，但存在虚构、许可问题和难以扩展的生成过程。</li>
<li>研究提出了一种新的程序化方法，使用场景图和人为编写的程序来合成以视觉为中心的指令数据，确保了数据生成的可解释性和可控性。</li>
<li>该方法实现了高效规模化并保持了事实准确性。通过实施多个单图像和多图像指令生成器及场景图生成管道，建立了可扩展的系统——ProVision。</li>
<li>ProVision能够为任何给定图像生成多样化的问答对，涵盖了对象、属性、关系、深度等。</li>
<li>在多个数据集上应用ProVision生成的指令数据，取得了显著的性能提升。在多个基准测试中的平均性能提高了1.6%。</li>
<li>ProVision系统的成本效益高，可为训练多模态语言模型提供有价值的资源。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7e8cb97cdfe610306866f0725881bbf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c79b6de64dbde4592691764c5cef14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbabc542464f3caae8fe55262ded5040.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fcbc119bff24db4fd8bb63ed2894e66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c42593b541d8ad98e8a0d1d9645bdb5.jpg" align="middle">
</details>




<h2 id="Understanding-Factual-Recall-in-Transformers-via-Associative-Memories"><a href="#Understanding-Factual-Recall-in-Transformers-via-Associative-Memories" class="headerlink" title="Understanding Factual Recall in Transformers via Associative Memories"></a>Understanding Factual Recall in Transformers via Associative Memories</h2><p><strong>Authors:Eshaan Nichani, Jason D. Lee, Alberto Bietti</strong></p>
<p>Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior. </p>
<blockquote>
<p>大型语言模型展现出了令人印象深刻的执行事实性回忆的能力。先前的研究发现，在事实性回忆任务上训练的转换器储存信息的速率与其参数数量成正比。在我们的研究中，我们展示了浅层转换器可以通过结合关联记忆来获得接近最优的存储容量。我们首先证明，无论是线性关联记忆还是MLP关联记忆，其存储容量都随着参数数量的增加而线性扩展。接下来，我们引入了一个合成的事实性回忆任务，并证明了一个带有单层自注意力机制后跟MLP的转换器在自注意力参数或MLP参数总数线性增加时（对数因素除外）可以达到任务上的百分之百准确率。特别是，转换器可以在使用值矩阵或MLP作为关联记忆来存储事实数据集之间进行权衡。我们通过这些表达力分析的结果，补充了在我们的事实性回忆任务上训练的简化线性注意力模型的梯度流轨迹分析，该分析表明模型表现出序列学习行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06538v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型展现出强大的事实记忆能力。本研究证明浅层变压器可通过结合关联记忆达到近乎最优的存储能力。我们证明了线性与MLP关联记忆的存储能力与参数数量呈线性关系。通过合成的事实记忆任务，我们发现单层自注意力后接MLP的变压器，在自注意力参数或MLP参数数量与事实数量呈线性关系（对数因素内）时，可达到100%准确率。特别是，变压器可在使用值矩阵或MLP作为关联记忆来存储事实数据集之间进行权衡。我们通过这些表达力结果，补充了简化线性注意力模型在事实记忆任务上梯度流轨迹的分析，显示模型展现出序贯学习行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型具备强大的事实记忆能力。</li>
<li>浅层变压器结合关联记忆可达到近乎最优的存储能力。</li>
<li>线性与MLP关联记忆的存储能力与参数数量呈线性关系。</li>
<li>变压器在特定条件下可达到100%的事实记忆任务准确率。</li>
<li>变压器可在使用值矩阵与MLP作为关联记忆之间进行权衡。</li>
<li>简化线性注意力模型在事实记忆任务上展现出序贯学习行为。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9f8343ea89c2fe7f963d845465390cfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32c839c5936fedb7031cd8088b478b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eefd36c16f7b4f17174d08857bb5b0a8.jpg" align="middle">
</details>




<h2 id="LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations"><a href="#LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations" class="headerlink" title="LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations"></a>LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations</h2><p><strong>Authors:Mingjie Xu, Mengyang Wu, Yuzhi Zhao, Jason Chun Lok Li, Weifeng Ou</strong></p>
<p>Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMs’ inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: <a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG">https://github.com/Endlinc/LLaVA-SpaceSGG</a>. </p>
<blockquote>
<p>场景图生成（SGG）将视觉场景转换为结构化的图表示，为复杂的视觉任务提供更深的场景理解。然而，现有的SGG模型经常忽略重要的空间关系，并且在开放词汇表的背景下泛化能力较弱。为了解决这些局限性，我们提出了LLaVA-SpaceSGG，这是一个为开放词汇表SGG设计的多模式大型语言模型（MLLM），具有增强的空间关系建模。为了训练它，我们收集了SGG指令调整数据集，命名为SpaceSGG。该数据集是通过结合公开数据集并使用我们的数据构建管道中的开源模型合成数据而构建的。它结合了目标位置、目标关系和深度信息，形成三种数据格式：空间SGG描述、问答和对话。为了提高MLLMs的内在能力向SGG任务的转移，我们引入了两阶段训练模式。实验表明，LLaVA-SpaceSGG在开放词汇表SGG方法上的表现优于其他方法，与基线相比，召回率提高了8.6%，平均召回率提高了28.4%。我们的代码库、数据集和训练模型可在以下GitHub链接上公开访问：<a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG%E3%80%82">https://github.com/Endlinc/LLaVA-SpaceSGG。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06322v1">PDF</a> Accepted by the WACV 2025, including supplementary material</p>
<p><strong>Summary</strong><br>空间场景图生成（SGG）是深度场景理解的重要技术，可将复杂视觉任务转换为结构化图形表示。然而，现有SGG模型在开放词汇环境中存在空间关系建模不足和泛化能力受限的问题。为此，我们提出了LLaVA-SpaceSGG模型，一种适用于开放词汇SGG的多模态大型语言模型（MLLM）。我们收集并构建了名为SpaceSGG的SGG指令微调数据集，用于训练模型。实验表明，LLaVA-SpaceSGG相较于其他开放词汇SGG方法表现出更高的性能，召回率提高了8.6%，平均召回率提高了28.4%。我们的代码库、数据集和训练模型已在GitHub上公开访问。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGG技术能够将复杂视觉任务转换为结构化图形表示，提高场景理解的深度。</li>
<li>当前SGG模型存在忽视关键空间关系和泛化能力不足的问题。</li>
<li>LLaVA-SpaceSGG是一种基于多模态大型语言模型（MLLM）的开放词汇SGG模型。</li>
<li>为训练LLaVA-SpaceSGG模型，我们构建了名为SpaceSGG的SGG指令微调数据集。</li>
<li>SpaceSGG数据集结合了目标位置、目标关系和深度信息，包含三种数据格式。</li>
<li>采用两阶段训练范式增强MLLM向SGG任务的知识迁移能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d43c40300ef813ab9c294c6d2d9da8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-667007beb268b4ed3183f8ffceb27f82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05320e9817235d934fa69ec3d1db206e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a013cdba343996ad6176ba72488718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89be98b84bbb87523661e45a4b5f1ff1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4568902a2ea3a695ad701e9d47dd19c.jpg" align="middle">
</details>




<h2 id="Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness"><a href="#Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness" class="headerlink" title="Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness"></a>Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness</h2><p><strong>Authors:Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles–informativeness, uniqueness, and representativeness–for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 100.8% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the “Less is More” philosophy in MLLM development. </p>
<blockquote>
<p>指令调整微调预先训练好的多模态大型语言模型（MLLMs）来处理真实任务。然而，视觉指令数据集的快速扩张导致了数据冗余，从而产生了过高的计算成本。我们提出了一种协作框架DataTailor，它利用三个关键原则——信息性、唯一性和代表性——进行有效的数据选择。我们认为，有价值的样本应该具有任务的信息性、非冗余性，并代表样本分布（即非异常值）。我们进一步提出了针对每项原则的实际评分方法，该方法可自动适应给定的数据集，无需繁琐的超参数调整。在各种基准测试上的综合实验表明，DataTailor在仅使用15%数据的情况下，实现了全数据微调性能的100.8%，在降低计算成本的同时保持了卓越的结果。这体现了MLLM发展中的“少即是多”理念。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06293v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong><br>指令微调对预训练的多模态大型语言模型（MLLMs）进行微调以处理现实世界任务。随着视觉指令数据集的快速扩展，数据冗余问题愈发严重，导致计算成本过高。为此，我们提出了一个协作框架DataTailor，该框架利用信息性、唯一性和代表性三个关键原则进行高效的数据选择。我们通过实用的评分方式对这三个原则进行衡量，自动适应给定的数据集，无需繁琐的超参数调整。在多个基准测试上的综合实验表明，DataTailor在仅使用15%数据的情况下，实现了全数据微调性能的100.8%，显著降低了计算成本，同时保持了卓越的结果，这体现了MLLM发展中的“少即是多”理念。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令微调是优化预训练多模态大型语言模型（MLLMs）处理现实任务的方法。</li>
<li>数据冗余问题随着视觉指令数据集的扩大而加剧，导致计算成本上升。</li>
<li>DataTailor框架通过信息性、唯一性和代表性三个原则进行高效数据选择。</li>
<li>有效的样本应具有任务信息性、非冗余性，并能代表样本分布。</li>
<li>DataTailor通过实用的评分方式自动适应数据集，避免繁琐的超参数调整。</li>
<li>在多个基准测试上，DataTailor在仅使用少量数据的情况下取得了令人瞩目的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-16ba4959c013b5b5057db43147dbcf1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d1796b5c2c43c9484a2a5ccd1a518a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da737588d95e9d9a1795383aa0d36a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6ddefd66bc4617e55355e51f2d01f13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b558db0836cbaac7eb611b67eef6d9.jpg" align="middle">
</details>




<h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by “selecting sparsely and computing densely”. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>当前针对LLM的PEFT方法只能同时实现高质量、高效训练或可扩展的服务，但不能三者兼顾。为了解决这个问题，我们研究了稀疏微调技术，并观察到其显著提高了泛化能力。利用这一关键见解，我们针对LLM提出了一系列结构化稀疏微调（S$^{2}$FT）方法，这些方法在微调性能、训练效率和推理可扩展性方面均达到了业界最佳水平。S$^{2}$FT通过“稀疏选择、密集计算”来实现这一目标。它分别选择MHA和FFN模块中每个Transformer块的几个头和通道。接下来，它对LLM中耦合结构两侧的权重矩阵进行共排列，将每一层中选择的组件连接成密集的子矩阵。最后，S$^{2}$FT对所有子矩阵进行就地梯度更新。通过理论分析和实验结果，我们的方法可以防止过拟合和遗忘，在常识推理和算术推理方面都达到了最先进的性能，与LoRA相比平均提高了4.6%和1.3%，在指令调优后推广到不同领域时，比全量FT高出11.5%。通过使用我们的部分反向传播算法，S$^{2}$FT在训练内存方面节省了高达3倍，与全量FT相比，延迟提高了1.5-2.7倍，同时在两个指标上都平均比LoRA提高了10%。我们进一步证明，S$^{2}$FT中的权重更新可以被解耦为适配器，为实现多个微调模型的有效融合、快速切换和高效并行提供服务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>当前针对LLM的PEFT方法往往只能同时满足高质量、高效训练和可伸缩服务中的两项，无法实现三者兼顾。为解决这一局限，本研究探讨了稀疏微调技术并观察到其显著提高泛化能力。基于此关键发现，我们提出了针对LLM的结构化稀疏微调（Structured Sparse Fine-Tuning，S$^{2}$FT）方法系列，同时实现了最先进的微调性能、训练效率和推理可伸缩性。S$^{2}$FT通过“稀疏选择、密集计算”实现这一目标。它选择Transformer块中MHA和FFN模块的少数头部和通道，对LLM中耦合结构的两侧权重矩阵进行共置换，将每一层中选定的组件连接成密集子矩阵。最后，S$^{2}$FT对所有子矩阵执行原地梯度更新。通过理论分析和实证结果，本研究方法可以防止过拟合和遗忘，在常识和算术推理方面表现出卓越性能，与LoRA相比平均提高了4.6%和1.3%，在指令调优后推广到不同领域时比全量微调高出11.5%。通过使用部分反向传播算法，S$^{2}$FT在训练内存方面节省了高达3倍，与全量微调相比延迟提高了1.5-2.7倍，同时在两个指标上平均比LoRA提高了10%。此外，本研究进一步证明S$^{2}$FT中的权重更新可以解耦为适配器，为融合多个微调模型提供了有效、快速切换和高效并行处理的能力。</p>
<p><strong>要点归纳</strong></p>
<ol>
<li>当前PEFT方法在LLM上难以同时实现高质量、高效训练和可伸缩服务三者。</li>
<li>研究通过稀疏微调技术观察到显著泛化能力改进。</li>
<li>提出结构化稀疏微调（S$^{2}$FT）方法，结合稀疏选择与密集计算策略。</li>
<li>S$^{2}$FT选择Transformer块中的关键组件并进行权重矩阵置换，形成密集子矩阵。</li>
<li>S$^{2}$FT通过原地梯度更新优化性能，减少训练内存使用并提高推理速度。</li>
<li>S$^{2}$FT在常识和算术推理方面表现出卓越性能，相比LoRA有平均10%的提升。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9dc097c198906efa869b0f4455639b1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52615bada518b68dec445640650fa2f0.jpg" align="middle">
</details>




<h2 id="The-BrowserGym-Ecosystem-for-Web-Agent-Research"><a href="#The-BrowserGym-Ecosystem-for-Web-Agent-Research" class="headerlink" title="The BrowserGym Ecosystem for Web Agent Research"></a>The BrowserGym Ecosystem for Web Agent Research</h2><p><strong>Authors:Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste</strong></p>
<p>The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMs) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropic’s latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models. </p>
<blockquote>
<p>BrowserGym生态系统满足了日益增长的对于高效评估和基准测试web代理的需求，特别是那些利用自动化和大型语言模型（LLM）进行web交互任务的代理。许多现有的基准测试受到碎片化以及评估方法不一致的影响，使得实现可靠的对比和可重复的结果具有挑战性。BrowserGym旨在通过提供一个统一的、类似于gym的环境来解决这个问题，该环境具有明确定义的观察空间和动作空间，促进各种基准测试的标准化评估。结合AgentLab这一互补框架，它有助于代理的创建、测试和分析，BrowserGym提供了集成新基准测试的灵活性，同时确保一致的评估和综合的实验管理。这种标准化方法旨在减少开发web代理的时间和复杂性，支持更可靠的对比，促进对代理行为的深入分析，并可能产生更适应、更强大的代理，最终加速LLM驱动的自动化创新。作为支持证据，我们进行了首个大规模、多基准的web代理实验，比较了当前BrowserGym中所有可用基准测试上6个最新前沿LLM的性能。我们的研究结果发现，除与视觉相关的任务外，GPT-4o表现优越，OpenAI和Anthropic的最新模型之间存在很大的差异，Claude-3.5-Sonnet几乎在所有基准测试中均领先。尽管取得了这些进展，我们的结果强调，由于现实世界的web环境的固有复杂性和当前模型的局限性，构建稳健高效的web代理仍然是一个巨大的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05467v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>BrowserGym生态系统解决了对高效评估和基准测试网络代理的日益增长的需求，特别是那些利用自动化和大语言模型（LLM）进行网络交互任务的项目。BrowserGym旨在通过提供一个统一的、类似于gym的环境，以及定义良好的观测和行动空间，解决现有基准测试中的碎片化和不一致评估方法的问题，从而推动标准化评估在多样化基准测试中的应用。结合AgentLab框架，一个辅助创建、测试和分析代理的框架，BrowserGym提供了灵活集成新基准测试的能力，同时确保一致的评估和综合的实验管理。这种标准化方法旨在减少开发网络代理的时间和复杂性，支持更可靠的对比和对代理行为的深入分析，可以产生更灵活、能力更强的代理，最终加速LLM驱动的自动化创新。我们以大型、跨多个基准测试的网络代理实验作为支持证据，比较了目前BrowserGym中所有基准测试上六个最先进LLM的性能。结果表明OpenAI和Antropic的最新模型之间存在显著差异，Claude-3.5-Sonnet在几乎所有基准测试中领先，但在视觉相关任务上GPT-4o表现更优。尽管取得了进展，但我们的结果强调，由于真实世界网络环境的固有复杂性和当前模型的局限性，构建健壮且高效的Web代理仍然是一个重大挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrowserGym生态系统解决了现有网络代理评估和基准测试的碎片化问题。</li>
<li>它提供了一个统一的评估环境，促进了标准化评估方法的实施。</li>
<li>AgentLab框架与BrowserGym结合，有助于创建、测试和分析代理。</li>
<li>通过实验验证了LLM在多个基准测试上的性能差异。</li>
<li>Claude-3.5-Sonnet在大多数基准测试中表现优异，但在视觉任务上GPT-4o表现更好。</li>
<li>构建健壮且高效的Web代理仍然面临重大挑战，主要因为真实网络环境的复杂性和模型的局限性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3858dbdffadf165a29416f9620a164c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c364e85d448c813f00dd1b59ad2c14cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc3f41d720e59c26ee217c4a0b296a37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f346ae8041ecb94aee3f20ace0688668.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-361a8173f1daf7a58469d04ffcd267c8.jpg" align="middle">
</details>




<h2 id="VoiceBench-Benchmarking-LLM-Based-Voice-Assistants"><a href="#VoiceBench-Benchmarking-LLM-Based-Voice-Assistants" class="headerlink" title="VoiceBench: Benchmarking LLM-Based Voice Assistants"></a>VoiceBench: Benchmarking LLM-Based Voice Assistants</h2><p><strong>Authors:Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li</strong></p>
<p>Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field. </p>
<blockquote>
<p>基于大型语言模型（LLM）的成功，最近的进展，如GPT-4o，已经能够通过基于LLM的语音助手实现实时语音交互，与传统的基于文本的交互相比，为用户提供了显著改善的体验。然而，缺乏用于评估这些语音交互能力的基准测试阻碍了基于LLM的语音助手的发展。当前的评估主要集中在自动语音识别（ASR）或清洁语音的一般知识评估上，忽视了涉及多种说话人特征、环境和内容因素的更复杂、真实世界的场景。为了解决这一问题，我们引入了VoiceBench，这是第一个旨在提供基于LLM的语音助手的多元化评估的基准测试。VoiceBench还包括真实和合成的语音指令，这些指令融入了上述三个关键的真实世界变化因素。广泛的实验揭示了当前基于LLM的语音助手模型的局限性，并为该领域的未来研究和开发提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17196v3">PDF</a> Work in progress. Data is available at   <a target="_blank" rel="noopener" href="https://github.com/MatthewCYM/VoiceBench">https://github.com/MatthewCYM/VoiceBench</a></p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的成功，GPT-4o等最新进展使得通过LLM语音助手进行实时语音交互成为可能，相比传统的文本交互方式提供了显著的用户体验改进。然而，缺乏评估这些语音交互能力的基准测试阻碍了LLM语音助手的发展。为解决这个问题，我们推出了VoiceBench基准测试，这是首个全面评估LLM语音助手的多方面评价工具。VoiceBench包含真实和合成语音指令，涵盖了现实世界中的关键变化因素。大量实验揭示了当前LLM语音助手的局限性，为今后的研究和发展提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs的进步推动了实时语音交互的发展，通过LLM语音助手实现。</li>
<li>GPT-4o等最新技术显著提升了用户体验。</li>
<li>目前缺乏评估LLM语音助手交互能力的基准测试。</li>
<li>VoiceBench是首个全面评估LLM语音助手的基准测试。</li>
<li>VoiceBench包含真实和合成的语音指令，模拟现实世界的复杂场景。</li>
<li>现有LLM语音助手存在局限性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69ae0a5443de6281affd9aaaa8657b10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86de805d5a4c3dd28764ed73475c70f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc8f15069422afe64369d4abe5a0a4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15a9acc59b778b744382cabc12652f69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20de57dd7f623666968b886a207ce214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b1aa211796e45875eb98816b841157.jpg" align="middle">
</details>




<h2 id="ChatGPT-and-biometrics-an-assessment-of-face-recognition-gender-detection-and-age-estimation-capabilities"><a href="#ChatGPT-and-biometrics-an-assessment-of-face-recognition-gender-detection-and-age-estimation-capabilities" class="headerlink" title="ChatGPT and biometrics: an assessment of face recognition, gender   detection, and age estimation capabilities"></a>ChatGPT and biometrics: an assessment of face recognition, gender   detection, and age estimation capabilities</h2><p><strong>Authors:Ahmad Hassanpour, Yasamin Kowsari, Hatef Otroshi Shahreza, Bian Yang, Sebastien Marcel</strong></p>
<p>This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics. </p>
<blockquote>
<p>本文探讨了大型语言模型（LLM）如ChatGPT在生物识别任务中的应用。我们特别研究了ChatGPT在执行与生物识别相关的任务时的能力，重点是人脸识别、性别检测和年龄估计。由于生物识别被视为敏感信息，ChatGPT避免回答直接提示，因此我们制定了一种提示策略来绕过其安全措施并评估其生物识别任务的能力。我们的研究表明，ChatGPT能够识别面部身份并准确区分两张面部图像。此外，实验结果表明，在性别检测方面表现出显著性能，年龄估计任务的准确性也合理。我们的研究为LLM和基础模型在生物识别领域的应用提供了广阔的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02965v2">PDF</a> Published as a conference paper at IEEE International Conference on   Image Processing (ICIP) 2024</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）如ChatGPT在生物识别任务中的应用。文章重点研究了ChatGPT在人脸识别、性别识别和年龄估计等生物识别相关任务的能力。由于生物识别被视为敏感信息，ChatGPT避免回答直接提示，因此研究团队设计了一种提示策略来绕过其安全保护机制以评估其在生物识别任务中的能力。研究表明，ChatGPT能够准确识别面部身份并区分不同的面部图像。此外，实验结果显示其在性别识别方面表现出卓越的性能，而在年龄估计任务中达到合理的准确性。本文的发现揭示了LLMs和基础模型在生物识别领域应用中的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究探讨了大型语言模型（LLMs）在生物识别任务中的应用，特别是人脸识别、性别识别和年龄估计等方面。</li>
<li>ChatGPT能够准确识别面部身份并区分不同的面部图像。</li>
<li>实验结果显示ChatGPT在性别识别方面表现出卓越的性能。</li>
<li>在年龄估计任务中，ChatGPT达到合理的准确性。</li>
<li>由于生物识别信息的敏感性，研究团队需要设计巧妙的提示策略来评估ChatGPT在生物识别任务中的能力。</li>
<li>文章指出LLMs和基础模型在生物识别领域具有巨大的应用潜力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e9e651fd1feac326cf6297b5f795d32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8abcd76d4814ccdac70d5f15e6e7b2ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e684a1facd6a7212a5eea4d32cd177f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91576408a68972db813b5df3d0a88ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d41005013df030fdaf16dee34a633eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee32a078534523be1a4ae7da60b6c3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1abe3d40763850b3806c94cb8c74a522.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cd3981abbdb5f8b48c3a269513ab875.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-26403bcf86a4e06aa0a0588d5f8dc98d.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2024-12-12  EOV-Seg Efficient Open-Vocabulary Panoptic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">14807.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
