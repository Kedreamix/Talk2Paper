<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Generative Semantic Communication Architectures, Technologies, and   Applications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    29.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    120 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Generative-Semantic-Communication-Architectures-Technologies-and-Applications"><a href="#Generative-Semantic-Communication-Architectures-Technologies-and-Applications" class="headerlink" title="Generative Semantic Communication: Architectures, Technologies, and   Applications"></a>Generative Semantic Communication: Architectures, Technologies, and   Applications</h2><p><strong>Authors:Jinke Ren, Yaping Sun, Hongyang Du, Weiwen Yuan, Chongjie Wang, Xianda Wang, Yingbin Zhou, Ziwei Zhu, Fangxin Wang, Shuguang Cui</strong></p>
<p>This paper delves into the applications of generative artificial intelligence (GAI) in semantic communication (SemCom) and presents a thorough study. Three popular SemCom systems enabled by classical GAI models are first introduced, including variational autoencoders, generative adversarial networks, and diffusion models. For each system, the fundamental concept of the GAI model, the corresponding SemCom architecture, and the associated literature review of recent efforts are elucidated. Then, a novel generative SemCom system is proposed by incorporating the cutting-edge GAI technology-large language models (LLMs). This system features two LLM-based AI agents at both the transmitter and receiver, serving as â€œbrainsâ€ to enable powerful information understanding and content regeneration capabilities, respectively. This innovative design allows the receiver to directly generate the desired content, instead of recovering the bit stream, based on the coded semantic information conveyed by the transmitter. Therefore, it shifts the communication mindset from â€œinformation recoveryâ€ to â€œinformation regenerationâ€ and thus ushers in a new era of generative SemCom. A case study on point-to-point video retrieval is presented to demonstrate the superiority of the proposed generative SemCom system, showcasing a 99.98% reduction in communication overhead and a 53% improvement in retrieval accuracy compared to the traditional communication system. Furthermore, four typical application scenarios for generative SemCom are delineated, followed by a discussion of three open issues warranting future investigation. In a nutshell, this paper provides a holistic set of guidelines for applying GAI in SemCom, paving the way for the efficient implementation of generative SemCom in future wireless networks. </p>
<blockquote>
<p>æœ¬æ–‡æ·±å…¥æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰åœ¨è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚é¦–å…ˆä»‹ç»äº†ç”±ç»å…¸GAIæ¨¡å‹å¯ç”¨çš„ä¸‰ä¸ªæµè¡Œçš„SemComç³»ç»Ÿï¼ŒåŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªç³»ç»Ÿï¼Œéƒ½é˜è¿°äº†GAIæ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µã€ç›¸åº”çš„SemComæ¶æ„ä»¥åŠæœ€è¿‘çš„æ–‡çŒ®ç»¼è¿°ã€‚ç„¶åï¼Œé€šè¿‡é‡‡ç”¨æœ€å‰æ²¿çš„GAIæŠ€æœ¯â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆå¼SemComç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨å‘å°„ç«¯å’Œæ¥æ”¶ç«¯éƒ½é‡‡ç”¨äº†åŸºäºLLMçš„AIä»£ç†ï¼Œåˆ†åˆ«ä½œä¸ºâ€œå¤§è„‘â€ï¼Œå®ç°äº†å¼ºå¤§çš„ä¿¡æ¯ç†è§£å’Œå†…å®¹å†ç”Ÿèƒ½åŠ›ã€‚è¿™ç§åˆ›æ–°è®¾è®¡å…è®¸æ¥æ”¶ç«¯æ ¹æ®å‘å°„ç«¯ä¼ é€’çš„ç¼–ç è¯­ä¹‰ä¿¡æ¯ç›´æ¥ç”Ÿæˆæ‰€éœ€å†…å®¹ï¼Œè€Œä¸æ˜¯æ¢å¤æ¯”ç‰¹æµã€‚å› æ­¤ï¼Œå®ƒå°†é€šä¿¡å¿ƒæ€ä»â€œä¿¡æ¯æ¢å¤â€è½¬å˜ä¸ºâ€œä¿¡æ¯å†ç”Ÿâ€ï¼Œä»è€Œå¼€å¯äº†ç”Ÿæˆå¼SemComçš„æ–°æ—¶ä»£ã€‚é€šè¿‡ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢çš„æ¡ˆä¾‹åˆ†æï¼Œå±•ç¤ºäº†æ‰€æå‡ºçš„ç”Ÿæˆå¼SemComç³»ç»Ÿçš„ä¼˜è¶Šæ€§ï¼Œä¸ä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿç›¸æ¯”ï¼Œé€šä¿¡å¼€é”€å‡å°‘äº†99.98%ï¼Œæ£€ç´¢ç²¾åº¦æé«˜äº†5data-hiddenstyle%å³å¤§å¹…æå‡æˆç»©çš„è¡¨ç°å¯ä½¿ç”¨å…·ä½“çš„ç™¾åˆ†æ¯”è¡¨ç¤ºç²¾å‡†æ•°å€¼æ•ˆæœæœ€ä½³æ•ˆæœæœ€é«˜æœ€å¥½æ˜ç¡®æ¸…æ™°çš„æå‡æŒ‡æ ‡è¡¨è¾¾å¯æ”¹ä¸ºå…·ä½“é‡åŒ–å½¢å¼æ›´ä¸ºçªå‡ºä¸”é†’ç›®æ”¹åï¼šé€šè¿‡ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢çš„æ¡ˆä¾‹å±•ç¤ºï¼Œæ‰€æå‡ºçš„ç”Ÿæˆå¼SemComç³»ç»Ÿåœ¨é€šä¿¡å¼€é”€ä¸Šå‡å°‘äº†é«˜è¾¾ç™¾åˆ†ä¹‹ä¹åä¹ç‚¹ä¹å…«ï¼ˆç›¸æ¯”ä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿï¼‰ï¼Œåœ¨æ£€ç´¢ç²¾åº¦ä¸Šæé«˜äº†ç™¾åˆ†ä¹‹äº”åä¸‰çš„è¿›æ­¥æ•ˆæœååˆ†æ˜¾è‘—æœ¬æ–‡è¿˜æ¦‚è¿°äº†ç”Ÿæˆå¼SemComçš„å››ä¸ªå…¸å‹åº”ç”¨åœºæ™¯ä»¥åŠä¸‰ä¸ªæœ‰å¾…æœªæ¥ç ”ç©¶çš„å¼€æ”¾é—®é¢˜æ€»è€Œè¨€ä¹‹æœ¬æ–‡ä¸ºå°†GAIåº”ç”¨äºSemComæä¾›äº†ä¸€å¥—å…¨é¢çš„æŒ‡å—ä¸ºæœªæ¥æ— çº¿ç½‘ç»œä¸­å®ç°é«˜æ•ˆçš„ç”Ÿæˆå¼SemComé“ºè®¾äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08642v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰åœ¨è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶ä»‹ç»äº†ä¸‰ç§ç”±ç»å…¸GAIæ¨¡å‹å¯ç”¨çš„SemComç³»ç»Ÿã€‚æ–‡ç« æå‡ºä¸€ç§ç»“åˆæœ€æ–°GAIæŠ€æœ¯â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼SemComç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨å‘å°„ç«¯å’Œæ¥æ”¶ç«¯åˆ†åˆ«é‡‡ç”¨ä¸¤ä¸ªLLMåŸºAIä»£ç†ï¼Œå®ç°äº†å¼ºå¤§çš„ä¿¡æ¯ç†è§£å’Œå†…å®¹å†ç”Ÿèƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿä½¿æ¥æ”¶å™¨èƒ½å¤Ÿæ ¹æ®å‘å°„å™¨ä¼ é€’çš„ç¼–ç è¯­ä¹‰ä¿¡æ¯ç›´æ¥ç”Ÿæˆæ‰€éœ€å†…å®¹ï¼Œä»è€Œå®ç°ä»â€œä¿¡æ¯æ¢å¤â€åˆ°â€œä¿¡æ¯å†ç”Ÿâ€çš„è½¬å˜ã€‚é€šè¿‡ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢çš„æ¡ˆä¾‹åˆ†æï¼Œå±•ç¤ºäº†è¯¥ç³»ç»Ÿçš„ä¼˜è¶Šæ€§ï¼Œä¸ä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿç›¸æ¯”ï¼Œé€šä¿¡å¼€é”€å‡å°‘äº†99.98%ï¼Œæ£€ç´¢ç²¾åº¦æé«˜äº†53%ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†å››ç§å…¸å‹çš„ç”Ÿæˆå¼SemComåº”ç”¨åœºæ™¯ï¼Œä»¥åŠä¸‰ä¸ªéœ€è¦æœªæ¥ç ”ç©¶çš„å¼€æ”¾é—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡æä¾›äº†å°†GAIåº”ç”¨äºSemComçš„å…¨é¢æŒ‡å—ï¼Œä¸ºæœªæ¥æ— çº¿ç½‘ç»œä¸­å®ç°é«˜æ•ˆçš„ç”Ÿæˆå¼SemComé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰åœ¨è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„åº”ç”¨åŠå…¶èƒŒæ™¯ã€‚</li>
<li>è¯¦è¿°äº†ä¸‰ç§åŸºäºç»å…¸GAIæ¨¡å‹çš„SemComç³»ç»Ÿï¼šå˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°çš„ç”Ÿæˆå¼SemComç³»ç»Ÿï¼Œå®ç°äº†ä¿¡æ¯ç†è§£å’Œå†…å®¹å†ç”Ÿã€‚</li>
<li>è¯¥ç³»ç»Ÿä½¿æ¥æ”¶å™¨èƒ½åŸºäºç¼–ç è¯­ä¹‰ä¿¡æ¯ç›´æ¥ç”Ÿæˆå†…å®¹ï¼Œå®ç°ä»â€œä¿¡æ¯æ¢å¤â€åˆ°â€œä¿¡æ¯å†ç”Ÿâ€çš„è½¬å˜ã€‚</li>
<li>é€šè¿‡ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†æ–°ç³»ç»Ÿç›¸è¾ƒäºä¼ ç»Ÿç³»ç»Ÿçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>ä»‹ç»äº†å››ç§ç”Ÿæˆå¼SemComçš„å…¸å‹åº”ç”¨åœºæ™¯ä»¥åŠæœªæ¥çš„æ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0947d019af14f992664fd549f68a4b1e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-992887657b0d055041f816e833fbba5d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fb684a360750521f8bc80cff6b8f03cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f890f4c21e1505742bf4b41549683065.jpg" align="middle">
</details>




<h2 id="Fast-Prompt-Alignment-for-Text-to-Image-Generation"><a href="#Fast-Prompt-Alignment-for-Text-to-Image-Generation" class="headerlink" title="Fast Prompt Alignment for Text-to-Image Generation"></a>Fast Prompt Alignment for Text-to-Image Generation</h2><p><strong>Authors:Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang</strong></p>
<p>Text-to-image generation has advanced rapidly, yet aligning complex textual prompts with generated visuals remains challenging, especially with intricate object relationships and fine-grained details. This paper introduces Fast Prompt Alignment (FPA), a prompt optimization framework that leverages a one-pass approach, enhancing text-to-image alignment efficiency without the iterative overhead typical of current methods like OPT2I. FPA uses large language models (LLMs) for single-iteration prompt paraphrasing, followed by fine-tuning or in-context learning with optimized prompts to enable real-time inference, reducing computational demands while preserving alignment fidelity. Extensive evaluations on the COCO Captions and PartiPrompts datasets demonstrate that FPA achieves competitive text-image alignment scores at a fraction of the processing time, as validated through both automated metrics (TIFA, VQA) and human evaluation. A human study with expert annotators further reveals a strong correlation between human alignment judgments and automated scores, underscoring the robustness of FPAâ€™s improvements. The proposed method showcases a scalable, efficient alternative to iterative prompt optimization, enabling broader applicability in real-time, high-demand settings. The codebase is provided to facilitate further research: <a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„ç”ŸæˆæŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œç„¶è€Œï¼Œå°†å¤æ‚çš„æ–‡æœ¬æç¤ºä¸ç”Ÿæˆçš„å›¾åƒå¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤æ‚å¯¹è±¡å…³ç³»å’Œç²¾ç»†ç»†èŠ‚çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡ä»‹ç»äº†å¿«é€Ÿæç¤ºå¯¹é½ï¼ˆFPAï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ä¸€æ¬¡é€šè¿‡çš„æ–¹æ³•ï¼Œæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½æ•ˆç‡ï¼Œè€Œæ— éœ€åƒOPT2Iç­‰å½“å‰æ–¹æ³•é‚£æ ·çš„è¿­ä»£å¼€é”€ã€‚FPAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå•æ¬¡è¿­ä»£çš„æç¤ºåŒä¹‰æ›¿æ¢ï¼Œéšåé€šè¿‡ä¼˜åŒ–æç¤ºè¿›è¡Œå¾®è°ƒæˆ–ä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼Œä»¥å®ç°å®æ—¶æ¨ç†ï¼Œé™ä½äº†è®¡ç®—éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹é½ä¿çœŸåº¦ã€‚åœ¨COCO Captionså’ŒPartiPromptsæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFPAåœ¨å¤„ç†æ—¶é—´çš„ä¸€å°éƒ¨åˆ†å†…å°±å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ–‡æœ¬å›¾åƒå¯¹é½å¾—åˆ†ï¼Œè¿™å·²é€šè¿‡è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼ˆTIFAã€VQAï¼‰å’Œäººç±»è¯„ä¼°å¾—åˆ°äº†éªŒè¯ã€‚ä¸ä¸“å®¶æ³¨é‡Šè€…è¿›è¡Œçš„äººç±»ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†äººç±»å¯¹é½åˆ¤æ–­ä¸è‡ªåŠ¨åŒ–å¾—åˆ†ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œçªæ˜¾äº†FPAæ”¹è¿›çš„ç¨³å¥æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•å±•ç¤ºäº†è¿­ä»£æç¤ºä¼˜åŒ–çš„å¯æ‰©å±•ã€é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå¯åœ¨å®æ—¶ã€é«˜éœ€æ±‚ç¯å¢ƒä¸­æ›´å¹¿æ³›åº”ç”¨ã€‚ä»£ç åº“å·²æä¾›ï¼Œä»¥æ–¹ä¾¿è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08639v1">PDF</a> TikTok Technical Report</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†å¿«é€Ÿæç¤ºå¯¹é½ï¼ˆFPAï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå•æ¬¡è¿­ä»£æç¤ºè½¬è¿°çš„æç¤ºä¼˜åŒ–æ¡†æ¶ã€‚å®ƒæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½æ•ˆç‡ï¼Œæ— éœ€åƒOPT2Iç­‰å½“å‰æ–¹æ³•é‚£æ ·è¿›è¡Œè¿­ä»£ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´æˆ–ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œä¼˜åŒ–æç¤ºï¼Œå¯å®ç°å®æ—¶æ¨ç†ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—éœ€æ±‚å¹¶ä¿æŒå¯¹é½ä¿çœŸåº¦ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFPAåœ¨COCO Captionså’ŒPartiPromptsæ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ–‡æœ¬å›¾åƒå¯¹é½åˆ†æ•°ï¼ŒåŒæ—¶å¤„ç†æ—¶é—´å¤§å¤§ç¼©çŸ­ã€‚é€šè¿‡ä¸“å®¶æ³¨é‡Šå‘˜è¿›è¡Œçš„äººç±»ç ”ç©¶è¡¨æ˜ï¼Œäººç±»å¯¹é½åˆ¤æ–­ä¸è‡ªåŠ¨è¯„åˆ†ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼Œçªå‡ºäº†FPAæ”¹è¿›çš„ç¨³å¥æ€§ã€‚è¯¥æè®®çš„æ–¹æ³•å±•ç¤ºäº†å¯æ‰©å±•æ€§å’Œé«˜æ•ˆæ€§ï¼Œå¯ä½œä¸ºè¿­ä»£æç¤ºä¼˜åŒ–çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºå®æ—¶é«˜éœ€æ±‚åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FPAæ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯¹é½é—®é¢˜ã€‚</li>
<li>FPAé‡‡ç”¨å•æ¬¡è¿­ä»£æç¤ºè½¬è¿°æ–¹æ³•ï¼Œæé«˜å¯¹é½æ•ˆç‡ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿæ–¹æ³•çš„è¿­ä»£å†—ä½™ã€‚</li>
<li>FPAç»“åˆäº†ç²¾ç»†è°ƒæ•´æˆ–ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œä»¥å®ç°å®æ—¶æ¨ç†å¹¶å‡å°‘è®¡ç®—éœ€æ±‚ã€‚</li>
<li>åœ¨COCO Captionså’ŒPartiPromptsæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜FPAå¯¹é½åˆ†æ•°å…·æœ‰ç«äº‰åŠ›ä¸”å¤„ç†æ—¶é—´æ˜¾è‘—å‡å°‘ã€‚</li>
<li>äººç±»ç ”ç©¶éªŒè¯äº†FPAä¸äººç±»å¯¹é½åˆ¤æ–­ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§ã€‚</li>
<li>FPAæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºå®æ—¶é«˜éœ€æ±‚åœºæ™¯ä¸‹çš„æ–‡æœ¬å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f2dd326d41094289fdc7eb85cac210a2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f6e9bfd70925ec0ec9b4c06d074f163.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9fdf53b899e19af7c79ca324d7775040.jpg" align="middle">
</details>




<h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œä»£ç ï¼‰å’Œè¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨è¯­è¨€å»ºæ¨¡ï¼ˆLatentLMï¼‰ï¼Œå®ƒä½¿ç”¨å› æœTransformeræ— ç¼é›†æˆäº†è¿ç»­å’Œç¦»æ•£æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è¿ç»­æ•°æ®è¡¨ç¤ºä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œæ‰©æ•£æ¥è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†Ïƒ-VAEã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLatentLMåœ¨å„ç§æ¨¡æ€ä¸­éƒ½éå¸¸æœ‰æ•ˆã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒLatentLMåœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢éƒ½è¶…è¶Šäº†Diffusion Transformersã€‚å½“é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼ŒLatentLMæä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£ï¼Œç»Ÿä¸€äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰©å¤§è®­ç»ƒä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒLatentLMä¸Transfusionå’Œå‘é‡é‡åŒ–æ¨¡å‹ç›¸æ¯”å–å¾—äº†æœ‰åˆ©çš„æ€§èƒ½ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ–¹é¢ï¼ŒLatentLMåœ¨è¯­éŸ³ç›¸ä¼¼æ€§å’Œç¨³å¥æ€§æ–¹é¢è¶…è¶Šäº†æœ€æ–°çš„VALL-E 2æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦10å€æ›´å°‘çš„è§£ç æ­¥éª¤ã€‚ç»“æœè¯æ˜LatentLMæ˜¯ä¸€ç§é«˜åº¦æœ‰æ•ˆå’Œå¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯æ¨åŠ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Latent Language Modelingï¼ˆLatentLMï¼‰æ˜¯ä¸€ç§ç»Ÿä¸€å¤„ç†ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œä»£ç ï¼‰å’Œè¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼‰çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ã€‚å®ƒé€šè¿‡å› æœTransformeræ— ç¼é›†æˆè¿ç»­å’Œç¦»æ•£æ•°æ®ï¼Œå¹¶ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¡¨ç¤ºè¿ç»­æ•°æ®ä¸ºæ½œåœ¨å‘é‡ã€‚LatentLMè§£å†³äº†æ–¹å·®å´©æºƒçš„é—®é¢˜ï¼Œå¹¶åœ¨å¤šæ¨¡æ€é¢†åŸŸè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨å›¾åƒç”Ÿæˆã€æ–‡æœ¬è½¬è¯­éŸ³åˆæˆç­‰æ–¹é¢ï¼ŒLatentLMç›¸è¾ƒäºå…¶ä»–æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„æ•ˆæœå’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Language Modeling (LatentLM) æ˜¯ä¸€ç§å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤„ç†ç¦»æ•£å’Œè¿ç»­æ•°æ®ã€‚</li>
<li>LatentLM åˆ©ç”¨å› æœTransformeræ— ç¼é›†æˆæ•°æ®ï¼Œå¹¶ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å¤„ç†è¿ç»­æ•°æ®ã€‚</li>
<li>LatentLM é€šè¿‡å¼•å…¥ä¸‹ä¸€ä¸ªç¬¦å·æ‰©æ•£è¿›è¡Œè‡ªå›å½’ç”Ÿæˆæ½œåœ¨å‘é‡ã€‚</li>
<li>$\sigma$-VAE è¢«å¼€å‘å‡ºæ¥è§£å†³æ–¹å·®å´©æºƒé—®é¢˜ï¼Œå¯¹äºè‡ªå›å½’å»ºæ¨¡è‡³å…³é‡è¦ã€‚</li>
<li>LatentLM åœ¨å›¾åƒç”Ÿæˆã€æ–‡æœ¬è½¬è¯­éŸ³åˆæˆç­‰é¢†åŸŸå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>LatentLM åœ¨å¤„ç†è®­ç»ƒä»¤ç‰Œæ‰©å±•è®¾ç½®æ—¶ï¼Œç›¸è¾ƒäºTransfusionå’Œå‘é‡é‡åŒ–æ¨¡å‹è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics"><a href="#Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics" class="headerlink" title="Synthetic Vision: Training Vision-Language Models to Understand Physics"></a>Synthetic Vision: Training Vision-Language Models to Understand Physics</h2><p><strong>Authors:Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan</strong></p>
<p>Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to enhance VLMsâ€™ physical reasoning capabilities using simulated data. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs generated from simulations relevant to physical reasoning tasks. Second, we introduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to create scene descriptions enriched with physical properties and processes. During physical reasoning tasks, these PCBs can be leveraged as context to assist a Large Language Model (LLM) to improve its performance. We evaluate both of our approaches using multiple benchmarks, including a new stability detection QA dataset called Falling Tower, which includes both simulated and real-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM can significantly outperform larger state-of-the-art foundational models. We also show that integrating PCBs boosts the performance of foundational LLMs on physical reasoning tasks. Using the real-world scenes from the Falling Tower dataset, we also validate the robustness of both approaches in Sim2Real transfer. Our results highlight the utility that simulated data can have in the creation of learning systems capable of advanced physical reasoning. </p>
<blockquote>
<p>æ¶‰åŠåŠ¨æ€ç¯å¢ƒä¸­ç‰©ä½“è¡Œä¸ºçš„è§£é‡Šã€ç†è§£å’Œé¢„æµ‹çš„ç‰©ç†æ¨ç†ä»ç„¶æ˜¯å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å¢å¼ºVLMsç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ç‰©ç†æ¨ç†ä»»åŠ¡ç›¸å…³çš„æ¨¡æ‹Ÿç”Ÿæˆçš„é—®ç­”ï¼ˆQAï¼‰å¯¹æ¥å¾®è°ƒé¢„è®­ç»ƒçš„VLMã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‰©ç†ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼ˆPCBsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºåˆ›å»ºä¸°å¯Œç‰©ç†å±æ€§å’Œè¿‡ç¨‹çš„åœºæ™¯æè¿°çš„å¾®è°ƒVLMsã€‚åœ¨è¿›è¡Œç‰©ç†æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¯ä»¥åˆ©ç”¨è¿™äº›PCBä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜å…¶æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æˆ‘ä»¬çš„ä¸¤ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ–°çš„ç¨³å®šæ€§æ£€æµ‹é—®ç­”æ•°æ®é›†Falling Towerå’ŒCLEVRERã€‚æˆ‘ä»¬è¯æ˜ï¼Œç»è¿‡å°å‹é—®ç­”æ•°æ®å¾®è°ƒè¿‡çš„VLMå¯ä»¥æ˜¾è‘—ä¼˜äºæ›´å¤§çš„æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé›†æˆPCBå¯ä»¥æé«˜åŸºç¡€LLMåœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ¥è‡ªFalling Toweræ•°æ®é›†çš„çœŸå®åœºæ™¯éªŒè¯äº†ä¸¤ç§æ–¹æ³•åœ¨Sim2Realè¿ç§»ä¸­çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†æ¨¡æ‹Ÿæ•°æ®åœ¨åˆ›å»ºèƒ½å¤Ÿè¿›è¡Œé«˜çº§ç‰©ç†æ¨ç†çš„å­¦ä¹ ç³»ç»Ÿæ–¹é¢çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸¤ç§åˆ©ç”¨æ¨¡æ‹Ÿæ•°æ®æå‡VLMç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚ä¸€æ˜¯é€šè¿‡æ¨¡æ‹Ÿç‰©ç†æ¨ç†ä»»åŠ¡çš„é—®ç­”å¯¹è¿›è¡Œå¾®è°ƒï¼›äºŒæ˜¯å¼•å…¥ç‰©ç†è¯­å¢ƒæ„å»ºå™¨ï¼ˆPCBï¼‰ï¼Œé’ˆå¯¹åœºæ™¯æè¿°åŠ å…¥ç‰©ç†å±æ€§å’Œè¿‡ç¨‹ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰©ç†æ¨ç†è¡¨ç°ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬æ–°çš„ç¨³å®šæ€§æ£€æµ‹é—®ç­”æ•°æ®é›†Falling Towerï¼ŒéªŒè¯äº†è¿™ä¸¤ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå°å‹é—®ç­”å¾®è°ƒVLMå¯æ˜¾è‘—ä¼˜äºå¤§å‹å…ˆè¿›åŸºç¡€æ¨¡å‹ï¼Œé›†æˆPCBèƒ½æé«˜LLMåœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨Falling Toweræ•°æ®é›†çš„çœŸå®åœºæ™¯ä¸­ä¹ŸéªŒè¯äº†ä¸¤ç§æ–¹æ³•åœ¨æ¨¡æ‹Ÿåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æå‡å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¹ç‰©ä½“è¡Œä¸ºçš„è§£é‡Šã€ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸¤ç§å¢å¼ºVLMç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼šé€šè¿‡æ¨¡æ‹Ÿæ•°æ®çš„é—®ç­”å¯¹è¿›è¡Œå¾®è°ƒï¼Œä»¥åŠå¼•å…¥ç‰©ç†è¯­å¢ƒæ„å»ºå™¨ï¼ˆPCBï¼‰ã€‚</li>
<li>é—®ç­”å¯¹å¾®è°ƒæ–¹æ³•é’ˆå¯¹æ¨¡æ‹Ÿç‰©ç†æ¨ç†ä»»åŠ¡ç”Ÿæˆé—®ç­”å¯¹ï¼Œæé«˜VLMå¯¹æ­¤ç±»ä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>PCBæ˜¯ä¸€ç§é’ˆå¯¹åœºæ™¯æè¿°åŠ å…¥ç‰©ç†å±æ€§å’Œè¿‡ç¨‹çš„ç²¾ç»†è°ƒæ•´æ¨¡å‹ï¼Œå¯ä½œä¸ºè¯­å¢ƒå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ç‰©ç†æ¨ç†è¡¨ç°ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬æ–°çš„ç¨³å®šæ€§æ£€æµ‹é—®ç­”æ•°æ®é›†Falling Towerï¼ŒéªŒè¯äº†è¿™ä¸¤ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å°å‹é—®ç­”å¾®è°ƒVLMå¯æ˜¾è‘—ä¼˜äºå¤§å‹å…ˆè¿›åŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e2f305523ebfa259f73cf14e4f47132.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6539bfab04a65e9677b31143ef8a9ac.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fa20a5f614e80dba0b3c546f3ff6045b.jpg" align="middle">
</details>




<h2 id="Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models"><a href="#Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models"></a>Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models</h2><p><strong>Authors:Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong</strong></p>
<p>Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic">https://github.com/jiah-li/magic</a>. </p>
<blockquote>
<p>å°½ç®¡ä½¿ç”¨å¯¹é½æŠ€æœ¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¢å¼ºç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§çš„æŠ€æœ¯æœ‰æ‰€è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°â€œè¶Šç‹±â€æ”»å‡»çš„å½±å“ï¼Œè¿™æ˜¯ä¸€ç§æš´éœ²LLMå®‰å…¨æ¼æ´çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè´ªå©ªåæ ‡æ¢¯åº¦ï¼ˆGCGï¼‰æ–¹æ³•å·²æ˜¾ç¤ºå‡ºèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¯¹æŠ—æ€§åç¼€ï¼Œçªç ´æœ€æ–°LLMçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒGCGæ¶‰åŠçš„ä¼˜åŒ–è¿‡ç¨‹éå¸¸è€—æ—¶ï¼Œä½¿å¾—è¶Šç‹±ç®¡é“æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†GCGçš„è¿‡ç¨‹ï¼Œå¹¶å‘ç°äº†é—´æ¥æ•ˆåº”çš„é—®é¢˜ï¼Œè¿™æ˜¯GCGä¼˜åŒ–çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å‹æ”»å‡»æ¢¯åº¦æŒ‡æ•°GCGï¼ˆMAGICï¼‰ï¼Œå®ƒé€šè¿‡åˆ©ç”¨åç¼€æ ‡è®°çš„æ¢¯åº¦ä¿¡æ¯æ¥è§£å†³é—´æ¥æ•ˆåº”é—®é¢˜ï¼Œä»è€Œé€šè¿‡å‡å°‘è®¡ç®—å’Œè¿­ä»£æ¬¡æ•°æ¥åŠ é€Ÿç¨‹åºã€‚æˆ‘ä»¬åœ¨AdvBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGICå®ç°äº†é«˜è¾¾1.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ä¸å…¶ä»–åŸºå‡†ç›¸å½“ç”šè‡³æ›´é«˜ã€‚æˆ‘ä»¬çš„MAGICåœ¨Llama-2ä¸Šå®ç°äº†74%çš„ASRï¼Œåœ¨å¯¹GPT-3.5è¿›è¡Œè¿ç§»æ”»å‡»æ—¶å®ç°äº†54%çš„ASRã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic">https://github.com/jiah-li/magic</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08615v1">PDF</a> 13 pages,2 figures, accepted by The 31st International Conference on   Computational Linguistics</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨æ€§é—®é¢˜è¿›è¡Œäº†ç ”ç©¶ã€‚å°½ç®¡ç°æœ‰çš„LLMé‡‡ç”¨äº†å¯¹é½æŠ€æœ¯æ¥å¢å¼ºç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§ï¼Œä½†ä»å­˜åœ¨è¢«è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼ˆä¸€ç§æš´éœ²LLMå®‰å…¨æ¼æ´çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼‰æ”»å‡»çš„é£é™©ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è´ªå¿ƒåæ ‡æ¢¯åº¦ï¼ˆGCGï¼‰æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆå¯¹æŠ—åç¼€ä»¥å®ç°è¶Šç‹±æ•ˆæœã€‚ç„¶è€Œï¼ŒGCGçš„ä¼˜åŒ–è¿‡ç¨‹ååˆ†è€—æ—¶ï¼Œé™ä½äº†è¶Šç‹±æµç¨‹çš„æ•ˆç‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å‘ç°äº†é—´æ¥æ•ˆåº”è¿™ä¸€å…³é”®ç“¶é¢ˆæ‰€åœ¨ï¼Œå¹¶æå‡ºäº†åä¸ºMAGICçš„è§£å†³æ–¹æ¡ˆã€‚MAGICé€šè¿‡åˆ©ç”¨åç¼€æ ‡è®°çš„æ¢¯åº¦ä¿¡æ¯æ¥è§£å†³é—´æ¥æ•ˆåº”é—®é¢˜ï¼Œå‡å°‘äº†è®¡ç®—å’Œè¿­ä»£æ¬¡æ•°ï¼Œä»è€ŒåŠ é€Ÿäº†è¶Šç‹±è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMAGICåœ¨ä¿æŒæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†æœ€é«˜è¾¾1.5å€çš„é€Ÿåº¦æå‡ã€‚åœ¨Llama-2ä¸Šï¼ŒMAGICçš„ASRè¾¾åˆ°äº†74%ï¼Œåœ¨GPT-3.5ä¸Šè¿›è¡Œäº†è¿ç§»æ”»å‡»æ—¶ASRä¸º54%ã€‚ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°½ç®¡é‡‡ç”¨å¯¹é½æŠ€æœ¯å¢å¼ºå®‰å…¨æ€§ï¼Œä½†ä»é¢ä¸´è¶Šç‹±æ”»å‡»æ–¹æ³•çš„å¨èƒã€‚</li>
<li>è´ªå¿ƒåæ ‡æ¢¯åº¦ï¼ˆGCGï¼‰æ–¹æ³•èƒ½è‡ªåŠ¨ç”Ÿæˆå¯¹æŠ—åç¼€ä»¥å®ç°è¶Šç‹±æ•ˆæœï¼Œä½†ä¼˜åŒ–è¿‡ç¨‹è€—æ—¶è¾ƒé•¿ã€‚</li>
<li>ç ”ç©¶äººå‘˜å‘ç°äº†é—´æ¥æ•ˆåº”è¿™ä¸€å…³é”®ç“¶é¢ˆï¼Œå½±å“äº†GCGä¼˜åŒ–çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†åä¸ºMAGICçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åˆ©ç”¨åç¼€æ ‡è®°çš„æ¢¯åº¦ä¿¡æ¯è§£å†³é—´æ¥æ•ˆåº”é—®é¢˜ï¼Œä»è€ŒåŠ é€Ÿè¶Šç‹±è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMAGICåœ¨ä¿æŒæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†é€Ÿåº¦æå‡ã€‚</li>
<li>MAGICåœ¨Llama-2ä¸Šçš„ASRè¾¾åˆ°äº†74%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-58d6ac4218806ce271467b0e57043d2d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1402f5cd5315046c630f6d625906afaf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5f3cc516321175daee06622fd2d711f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-740263989a70e39b604d8c517b02ca4e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-08274e4db02b3e1e8c5285e734fedd36.jpg" align="middle">
</details>




<h2 id="Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval"><a href="#Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval" class="headerlink" title="Preference Discerning with LLM-Enhanced Generative Retrieval"></a>Preference Discerning with LLM-Enhanced Generative Retrieval</h2><p><strong>Authors:Fabian Paischer, Liu Yang, Linfeng Liu, Shuai Shao, Kaveh Hassani, Jiacheng Li, Ricky Chen, Zhang Gabriel Li, Xialo Gao, Wei Shao, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Hamid Eghbalzadeh</strong></p>
<p>Sequential recommendation systems aim to provide personalized recommendations for users based on their interaction history. To achieve this, they often incorporate auxiliary information, such as textual descriptions of items and auxiliary tasks, like predicting user preferences and intent. Despite numerous efforts to enhance these models, they still suffer from limited personalization. To address this issue, we propose a new paradigm, which we term preference discerning. In preference dscerning, we explicitly condition a generative sequential recommendation system on user preferences within its context. To this end, we generate user preferences using Large Language Models (LLMs) based on user reviews and item-specific data. To evaluate preference discerning capabilities of sequential recommendation systems, we introduce a novel benchmark that provides a holistic evaluation across various scenarios, including preference steering and sentiment following. We assess current state-of-the-art methods using our benchmark and show that they struggle to accurately discern user preferences. Therefore, we propose a new method named Mender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce $\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods and achieves state-of-the-art performance on our benchmark. Our results show that Mender can be effectively guided by human preferences even though they have not been observed during training, paving the way toward more personalized sequential recommendation systems. We will open-source the code and benchmarks upon publication. </p>
<blockquote>
<p>é¡ºåºæ¨èç³»ç»Ÿæ—¨åœ¨æ ¹æ®ç”¨æˆ·çš„äº¤äº’å†å²ä¸ºå…¶æä¾›ä¸ªæ€§åŒ–æ¨èã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå®ƒä»¬é€šå¸¸ä¼šèå…¥è¾…åŠ©ä¿¡æ¯ï¼Œå¦‚ç‰©å“çš„æ–‡æœ¬æè¿°å’Œè¾…åŠ©ä»»åŠ¡ï¼Œå¦‚é¢„æµ‹ç”¨æˆ·åå¥½å’Œæ„å›¾ã€‚å°½ç®¡äººä»¬å·²ç»åšäº†å¾ˆå¤šåŠªåŠ›æ¥ä¼˜åŒ–è¿™äº›æ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€ä¸ªæ€§åŒ–ä¸è¶³çš„å›°å¢ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåå¥½è¾¨åˆ«ã€‚åœ¨åå¥½è¾¨åˆ«ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°å°†ä¸€ä¸ªç”Ÿæˆå‹é¡ºåºæ¨èç³»ç»Ÿå»ºç«‹åœ¨ç”¨æˆ·åå¥½ä¸Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®ç”¨æˆ·è¯„è®ºå’Œç‰¹å®šäºé¡¹ç›®çš„æ•°æ®ç”Ÿæˆç”¨æˆ·åå¥½ã€‚ä¸ºäº†è¯„ä¼°é¡ºåºæ¨èç³»ç»Ÿçš„åå¥½è¾¨åˆ«èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•å¯ä»¥åœ¨å„ç§åœºæ™¯ä¸‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬åå¥½æ§åˆ¶å’Œæƒ…æ„Ÿè·Ÿéšã€‚æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯¹å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨å‡†ç¡®è¾¨åˆ«ç”¨æˆ·åå¥½æ–¹é¢é‡åˆ°å›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåä¸ºMenderï¼ˆå¤šæ¨¡å¼åå¥½è¾¨è¯†å™¨ï¼‰ï¼Œå®ƒæ”¹è¿›äº†ç°æœ‰çš„æ–¹æ³•å¹¶åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰åœ¨è®­ç»ƒä¸­è§‚å¯Ÿåˆ°äººç±»åå¥½çš„æƒ…å†µä¸‹ï¼ŒMenderä¹Ÿå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼äººç±»åå¥½ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–çš„é¡ºåºæ¨èç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬å°†åœ¨å‘å¸ƒæ—¶å…¬å¼€æºä»£ç å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08604v1">PDF</a> 11 pages + references and appendix</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·å†å²äº¤äº’çš„åºåˆ—æ¨èç³»ç»Ÿæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–æ¨èã€‚å®ƒä»¬å¸¸èåˆè¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚ç‰©å“æ–‡æœ¬æè¿°ï¼‰å’Œä»»åŠ¡ï¼ˆå¦‚é¢„æµ‹ç”¨æˆ·åå¥½å’Œæ„å›¾ï¼‰æ¥æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ–¹é¢ä»æœ‰å±€é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ–¹æ³•â€”â€”åå¥½åˆ¤åˆ«ã€‚åœ¨åå¥½åˆ¤åˆ«ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°åœ¨ç”Ÿæˆå¼åºåˆ—æ¨èç³»ç»Ÿä¸­åŠ å…¥ç”¨æˆ·åå¥½æ¡ä»¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºç”¨æˆ·è¯„ä»·å’Œç‰©å“ç‰¹å®šæ•°æ®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç”¨æˆ·åå¥½ã€‚ä¸ºè¯„ä¼°åºåˆ—æ¨èç³»ç»Ÿçš„åå¥½åˆ¤åˆ«èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°åŸºå‡†ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸åŒçš„åœºæ™¯ä¸­å…¨é¢è¯„ä»·ç³»ç»Ÿæ€§èƒ½ï¼ŒåŒ…æ‹¬åå¥½æ§åˆ¶å’Œæƒ…æ„Ÿè¿½è¸ªã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºå½“å‰é¡¶çº§æ–¹æ³•éš¾ä»¥å‡†ç¡®åˆ¤æ–­ç”¨æˆ·åå¥½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Menderï¼ˆå¤šæ¨¡æ€åå¥½åˆ¤åˆ«å™¨ï¼‰ï¼Œå®ƒåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„è¯„ä¼°åŸºå‡†ä¸Šè¾¾åˆ°äº†é¡¶çº§æ€§èƒ½ã€‚Menderç”šè‡³å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§‚å¯Ÿåˆ°çš„äººç±»åå¥½æŒ‡å¯¼ä¸‹è¿›è¡Œæœ‰æ•ˆå·¥ä½œï¼Œä¸ºæ›´ä¸ªæ€§åŒ–çš„åºåˆ—æ¨èç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åºåˆ—æ¨èç³»ç»Ÿæ—¨åœ¨åŸºäºç”¨æˆ·å†å²äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èï¼Œä½†ç°æœ‰æ¨¡å‹çš„ä¸ªæ€§åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ–¹æ³•â€”â€”åå¥½åˆ¤åˆ«ï¼Œä»¥æ”¹è¿›åºåˆ—æ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºç”¨æˆ·è¯„ä»·å’Œç‰©å“æ•°æ®ç”Ÿæˆç”¨æˆ·åå¥½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥å…¨é¢è¯„ä¼°åºåˆ—æ¨èç³»ç»Ÿçš„åå¥½åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>å½“å‰é¡¶çº§æ–¹æ³•åœ¨åˆ¤åˆ«ç”¨æˆ·åå¥½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Menderï¼Œå®ƒåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶è¾¾åˆ°äº†é¡¶çº§æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-61ed96decf60a1e589ef18daa7655bb3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-59fa7d1b83e1d1accb0dc4597b2f1772.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e64ce1c732f27adb0a7a700843471db3.jpg" align="middle">
</details>




<h2 id="Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden"><a href="#Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden" class="headerlink" title="Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?"></a>Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?</h2><p><strong>Authors:Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro</strong></p>
<p>By means of pilot experiments for the language pair German and Galician, this paper examines the concept of efficiency and intelligence in lexicography and artificial intelligence, AI. The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained. Both quantitative and qualitative methods are used for this purpose. The analysis is based on the evaluation of the outputs of several sessions with the same prompt in ChatGPT 3.5. On the one hand, the algorithmic performance of intelligent systems is evaluated in comparison with data from lexicographical works. On the other hand, the ChatGPT data supplied is analysed using specific text passages of the aforementioned lexicographical text type. The results of this study not only help to evaluate the efficiency of this chatbot regarding the creation of dictionary articles, but also to delve deeper into the concept of intelligence, the thought processes and the actions to be carried out in both disciplines. </p>
<blockquote>
<p>æœ¬æ–‡é€šè¿‡é’ˆå¯¹å¾·è¯­å’ŒåŠ åˆ©è¥¿äºšè¯­è¯­è¨€å¯¹çš„è¯•ç‚¹å®éªŒï¼Œæ¢è®¨äº†è¯å…¸å­¦å’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸­çš„æ•ˆç‡å’Œæ™ºèƒ½æ¦‚å¿µã€‚å®éªŒçš„ç›®çš„æ˜¯å®è¯å’Œç»Ÿè®¡åœ°äº†è§£ChatGPT 3.5å¯¹è¯å…¸è¯æ¡è¿™ä¸€è¯å…¸æ–‡æœ¬ç±»å‹çš„å›åº”ï¼Œä»¥åŠè¯¥èŠå¤©æœºå™¨äººæ‰€è®­ç»ƒçš„è¯å…¸æ•°æ®ã€‚ä¸ºæ­¤ç›®çš„ï¼Œæ—¢é‡‡ç”¨äº†å®šé‡æ–¹æ³•ï¼Œä¹Ÿé‡‡ç”¨äº†å®šæ€§æ–¹æ³•ã€‚åˆ†ææ˜¯åŸºäºå¯¹ChatGPT 3.5ä¸­ç›¸åŒæç¤ºä¸‹å¤šæ¬¡ä¼šè¯è¾“å‡ºçš„è¯„ä¼°ã€‚ä¸€æ–¹é¢ï¼Œä¸è¯å…¸ä½œå“çš„æ•°æ®åº“ç›¸æ¯”ï¼Œå¯¹æ™ºèƒ½ç³»ç»Ÿçš„ç®—æ³•æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨ä¸Šè¿°è¯å…¸æ–‡æœ¬ç±»å‹çš„ç‰¹å®šæ–‡æœ¬æ®µè½åˆ†æäº†ChatGPTæä¾›çš„æ•°æ®ã€‚è¿™é¡¹ç ”ç©¶çš„ç»“æœä¸ä»…æœ‰åŠ©äºè¯„ä¼°è¯¥èŠå¤©æœºå™¨äººåœ¨åˆ›å»ºè¯å…¸æ–‡ç« æ–¹é¢çš„æ•ˆç‡ï¼Œè€Œä¸”æœ‰åŠ©äºæ›´æ·±å…¥åœ°äº†è§£æ™ºèƒ½çš„æ¦‚å¿µã€æ€ç»´è¿‡ç¨‹å’Œä¸¤ä¸ªå­¦ç§‘ä¸­çš„è¡ŒåŠ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08599v1">PDF</a> 25 pages, in German language</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é€šè¿‡å¾·è¯­å’ŒåŠ åˆ©è¥¿äºšè¯­çš„è¯•ç‚¹å®éªŒï¼Œæ¢è®¨äº†è¯å…¸ç¼–çº‚ä¸äººå·¥æ™ºèƒ½ä¸­çš„æ•ˆç‡ä¸æ™ºèƒ½æ¦‚å¿µã€‚å®éªŒæ—¨åœ¨å®è¯åœ°äº†è§£ChatGPT 3.5å¯¹äºè¯å…¸æ¡ç›®çš„ååº”ï¼Œå¹¶åˆ†æè¯¥èŠå¤©æœºå™¨äººæ‰€è®­ç»ƒçš„è¯å…¸æ•°æ®ã€‚è®ºæ–‡é‡‡ç”¨å®šé‡å’Œå®šæ€§æ–¹æ³•è¿›è¡Œåˆ†æï¼Œé€šè¿‡è¯„ä¼°å¤šä¸ªä¼šè¯çš„è¾“å‡ºç»“æœï¼Œå¯¹æ¯”æ™ºèƒ½ç³»ç»Ÿçš„ç®—æ³•æ€§èƒ½ä¸è¯å…¸ä½œå“æ•°æ®ã€‚æœ¬ç ”ç©¶ç»“æœä¸ä»…æœ‰åŠ©äºè¯„ä¼°è¯¥èŠå¤©æœºå™¨äººåœ¨åˆ›å»ºè¯å…¸æ¡ç›®æ–¹é¢çš„æ•ˆç‡ï¼Œè€Œä¸”æœ‰åŠ©äºæ·±å…¥äº†è§£æ™ºèƒ½ã€æ€ç»´è¿‡ç¨‹å’Œä¸¤ä¸ªå­¦ç§‘ä¸­çš„è¡ŒåŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é€šè¿‡å¾·è¯­å’ŒåŠ åˆ©è¥¿äºšè¯­çš„è¯•ç‚¹å®éªŒï¼Œæ¢ç´¢äº†è¯å…¸ç¼–çº‚ä¸äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ•ˆç‡ä¸æ™ºèƒ½é—®é¢˜ã€‚</li>
<li>å®éªŒç›®çš„æ˜¯å®è¯åœ°äº†è§£ChatGPT 3.5å¯¹äºè¯å…¸æ¡ç›®çš„ååº”ï¼Œå¹¶åˆ†æå…¶è®­ç»ƒçš„è¯å…¸æ•°æ®ã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨äº†å®šé‡å’Œå®šæ€§æ–¹æ³•ï¼Œè¯„ä¼°äº†ChatGPT 3.5åœ¨å¤šä¸ªä¼šè¯ä¸­çš„è¾“å‡ºè¡¨ç°ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹æ¯”äº†æ™ºèƒ½ç³»ç»Ÿçš„ç®—æ³•æ€§èƒ½ä¸è¯å…¸ä½œå“æ•°æ®ã€‚</li>
<li>æœ¬ç ”ç©¶æœ‰åŠ©äºè¯„ä¼°èŠå¤©æœºå™¨äººåœ¨åˆ›å»ºè¯å…¸æ¡ç›®æ–¹é¢çš„æ•ˆç‡ã€‚</li>
<li>ç»“æœæ­ç¤ºäº†æ™ºèƒ½ã€æ€ç»´è¿‡ç¨‹å’Œä¸¤ä¸ªå­¦ç§‘ä¸­çš„è¡ŒåŠ¨çš„æ·±å…¥ç†è§£ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-175b29133a78d0d939af8cbd1fa0ec67.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-25ad5e61c6be29db174ba7ef06c55f62.jpg" align="middle">
</details>




<h2 id="Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks"><a href="#Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks" class="headerlink" title="Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks"></a>Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks</h2><p><strong>Authors:Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi</strong></p>
<p>Ensuring that Software Requirements Specifications (SRS) align with higher-level organizational or national requirements is vital, particularly in regulated environments such as finance and aerospace. In these domains, maintaining consistency, adhering to regulatory frameworks, minimizing errors, and meeting critical expectations are essential for the reliable functioning of systems. The widespread adoption of large language models (LLMs) highlights their immense potential, yet there remains considerable scope for improvement in retrieving relevant information and enhancing reasoning capabilities. This study demonstrates that integrating a robust Graph-RAG framework with advanced prompt engineering techniques, such as Chain of Thought and Tree of Thought, can significantly enhance performance. Compared to baseline RAG methods and simple prompting strategies, this approach delivers more accurate and context-aware results. While this method demonstrates significant improvements in performance, it comes with challenges. It is both costly and more complex to implement across diverse contexts, requiring careful adaptation to specific scenarios. Additionally, its effectiveness heavily relies on having complete and accurate input data, which may not always be readily available, posing further limitations to its scalability and practicality. </p>
<blockquote>
<p>ç¡®ä¿è½¯ä»¶éœ€æ±‚è§„æ ¼ï¼ˆSRSï¼‰ä¸æ›´é«˜å±‚æ¬¡çš„ç»„ç»‡æˆ–å›½å®¶è¦æ±‚ä¸€è‡´æ˜¯éå¸¸å…³é”®çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èå’Œèˆªç©ºç­‰å—ç›‘ç®¡çš„ç¯å¢ƒä¸­ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œä¿æŒä¸€è‡´æ€§ã€éµå®ˆç›‘ç®¡æ¡†æ¶ã€å‡å°‘é”™è¯¯ä»¥åŠæ»¡è¶³å…³é”®æœŸæœ›å¯¹äºç³»ç»Ÿçš„å¯é è¿è¡Œè‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›é‡‡ç”¨çªæ˜¾äº†å…¶å·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯å’Œå¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œå°†ç¨³å¥çš„å›¾-RAGæ¡†æ¶ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ˆå¦‚æ€ç»´é“¾å’Œæ€ç»´æ ‘ï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ä¸åŸºçº¿RAGæ–¹æ³•å’Œç®€å•æç¤ºç­–ç•¥ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç»“æœã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œä½†å®ƒä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨å¤šç§èƒŒæ™¯ä¸‹å®æ–½æ—¢æ˜‚è´µåˆå¤æ‚ï¼Œéœ€è¦è°¨æ…é€‚åº”ç‰¹å®šåœºæ™¯ã€‚æ­¤å¤–ï¼Œå…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºå®Œæ•´å’Œå‡†ç¡®è¾“å…¥æ•°æ®çš„å­˜åœ¨ï¼Œè¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯å¯ç«‹å³è·å¾—çš„ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶éœ€æ±‚è§„æ ¼ï¼ˆSRSï¼‰ä¸é«˜é˜¶ç»„ç»‡æˆ–å›½å®¶éœ€æ±‚çš„åŒ¹é…æ–¹é¢æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èå’Œèˆªç©ºç­‰ç›‘ç®¡ç¯å¢ƒä¸­ã€‚æ•´åˆGraph-RAGæ¡†æ¶ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå¦‚Chain of Thoughtå’ŒTree of Thoughtï¼Œå¯æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†åœ¨å®æ–½ä¸­é¢ä¸´æˆæœ¬å’Œå¤æ‚æ€§æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¾èµ–äºå®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®è¾“å…¥ï¼Œè¿™åœ¨å®è·µä¸­å¯èƒ½éš¾ä»¥è·å¾—ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨é‡‘èå’Œèˆªç©ºç­‰ç›‘ç®¡ç¯å¢ƒä¸­ï¼Œè½¯ä»¶éœ€æ±‚è§„æ ¼ï¼ˆSRSï¼‰ä¸é«˜é˜¶ç»„ç»‡æˆ–å›½å®¶éœ€æ±‚çš„å¯¹é½è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æå‡ç³»ç»Ÿæ€§èƒ½æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>é›†æˆGraph-RAGæ¡†æ¶å’Œæç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ˆå¦‚Chain of Thoughtå’ŒTree of Thoughtï¼‰èƒ½æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ã€‚</li>
<li>æ­¤æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚</li>
<li>è¯¥æ–¹æ³•çš„å®æ–½æˆæœ¬è¾ƒé«˜ï¼Œä¸”æ›´å¤æ‚ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œé€‚åº”ã€‚</li>
<li>æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºå®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®è¾“å…¥ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éš¾ä»¥è·å¾—ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7979fb6e8cc3e979d2e01584d841f3b0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fc73788341c65dd14908806ee3ec036d.jpg" align="middle">
</details>




<h2 id="Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning"><a href="#Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning" class="headerlink" title="Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning"></a>Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning</h2><p><strong>Authors:Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang</strong></p>
<p>Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance. </p>
<blockquote>
<p>åªæœ‰ç¼–ç å™¨æ¨¡å‹ï¼ˆä¾‹å¦‚BERTã€RoBERTaï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œä¾‹å¦‚Llama3ï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…³äºåŸºäºç¼–ç å™¨çš„æ¨¡å‹ä¸LLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„æ€§èƒ½å¯¹æ¯”çš„ç³»ç»Ÿæ€§ç ”ç©¶ä»æœ‰æ‰€æ¬ ç¼ºï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¾®è°ƒæ—¶æ›´æ˜¯å¦‚æ­¤ã€‚æœ¬ç ”ç©¶é‡‡ç”¨äº†ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„æ¨¡å‹å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬å¾®è°ƒå’Œé¢„å…ˆè®­ç»ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨20 Newsgroupsï¼ˆ20NGï¼‰å’ŒMASSIVEæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™äº›LLMçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸åªæœ‰ç¼–ç å™¨çš„RoBERTaæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸¤ç§æ¨¡å‹ç±»å‹çš„å¤šä»»åŠ¡åŠŸèƒ½ï¼Œé€šè¿‡å°†å¤šä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ„å›¾æ£€æµ‹å’Œæ§½å¡«å……ï¼‰ç»“åˆåˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œä½¿ç”¨è¿™ä¸¤ä¸ªæ•°æ®é›†çš„æ•°æ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå®Œå…¨å¾®è°ƒåçš„Llama3-70Bæ¨¡å‹åœ¨å„ç§åˆ†ç±»ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºRoBERTa-largeå’Œå…¶ä»–è§£ç å™¨LLMã€‚æ­¤å¤–ï¼Œç»è¿‡æ•´åˆçš„å¤šä»»åŠ¡å¾®è°ƒLLMåœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸åŒæ¨¡å‹è®¾ç½®ç›¸åŒ¹é…ï¼Œæ¶‰åŠä¸¤ä¸ªæ•°æ®é›†ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¯¹åªæœ‰ç¼–ç å™¨å’ŒLLMæ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§å°†ä¸¤ä¸ªæˆ–å¤šä¸ªå®Œå…¨è°ƒæ ¡çš„è§£ç å™¨LLMç»“åˆèµ·æ¥ä»¥å‡å°‘å»¶è¿Ÿå¹¶ä¿æŒæ€§èƒ½çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08587v1">PDF</a> 9 pages, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¯¹æ¯”ç ”ç©¶äº†ç¼–ç å™¨æ¨¡å‹ï¼ˆå¦‚BERTã€RoBERTaï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œå¦‚Llama3ï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡å¤šé¡¹å®éªŒï¼Œæœ¬æ–‡å‘ç°å…¨é‡å¾®è°ƒåçš„Llama3-70Bæ¨¡å‹åœ¨å„ç±»æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºRoBERTa-largeå’Œå…¶ä»–è§£ç å™¨LLMã€‚æ­¤å¤–ï¼Œæ•´åˆå¤šä»»åŠ¡åçš„LLMsæ€§èƒ½å¯ä¸åŒæ¨¡å‹æ¶æ„ç›¸å½“ã€‚ç ”ç©¶æä¾›äº†å¯¹ç¼–ç å™¨æ¨¡å‹å’ŒLLMåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“åˆä¸¤ä¸ªæˆ–å¤šä¸ªå…¨é‡å¾®è°ƒè§£ç å™¨LLMçš„æ–¹æ³•ï¼Œä»¥é™ä½å»¶è¿Ÿå¹¶ä¿æŒç­‰æ•ˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¯¹æ¯”äº†ç¼–ç å™¨æ¨¡å‹ï¼ˆå¦‚RoBERTaï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å¤šç§æ¨¡å‹å’Œæ–¹æ³•çš„å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„æ¨¡å‹ï¼Œä»¥åŠå¾®è°ƒä¸é¢„è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>åœ¨20Newsgroupså’ŒMASSIVEæ•°æ®é›†ä¸Šè¯„ä¼°LLMçš„æ€§èƒ½ï¼Œå¹¶ä¸RoBERTaæ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚</li>
<li>å‘ç°å…¨é‡å¾®è°ƒåçš„Llama3-70Bæ¨¡å‹åœ¨å¤šç§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºRoBERTa-largeå’Œå…¶ä»–è§£ç å™¨LLMã€‚</li>
<li>æ•´åˆå¤šä»»åŠ¡çš„LLMsæ€§èƒ½å¯ä¸åŒæ¨¡å‹æ¶æ„ç›¸å½“ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å…¨é¢çš„ç¼–ç å™¨æ¨¡å‹å’ŒLLMåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¯„ä¼°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78a24e96e4b407822203e3c32af5b7ae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3c65aecbe11c6d8d21a7daee52179b60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffd99579b4a6b1de138e0cc90d02460d.jpg" align="middle">
</details>




<h2 id="TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs"><a href="#TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs" class="headerlink" title="TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs"></a>TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs</h2><p><strong>Authors:Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.   We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®æ³¨æ„åŠ›æœºåˆ¶æ–¹é¢ã€‚è™½ç„¶é‡åŒ–æŠ€æœ¯å’ŒåŠ é€Ÿç®—æ³•ï¼ˆå¦‚FlashAttentionï¼‰å·²ç»æé«˜äº†æ•´ä½“æ¨ç†çš„æ•ˆç‡ï¼Œä½†å®ƒä»¬è§£å†³çš„é—®é¢˜ä¸åŒï¼šé‡åŒ–å…³æ³¨æƒé‡æ¿€æ´»æ“ä½œï¼Œè€ŒFlashAttentionåˆ™æé«˜äº†æ‰§è¡Œæ•ˆç‡ä½†éœ€è¦é«˜ç²¾åº¦æ ¼å¼ã€‚æœ€è¿‘çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é‡åŒ–å‡å°‘äº†å†…å­˜å¸¦å®½ï¼Œä½†ä»éœ€è¦åœ¨æ³¨æ„åŠ›æ“ä½œä¸­è¿›è¡Œæµ®ç‚¹åé‡åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†TurboAttentionï¼Œè¿™æ˜¯ä¸€ç§å…¨é¢çš„æ³¨æ„åŠ›é‡åŒ–æ‰§è¡Œæ–¹æ³•ï¼ŒåŒæ—¶è§£å†³äº†å†…å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šFlashQï¼Œä¸€ç§é€å¤´æ³¨æ„åŠ›é‡åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿå‹ç¼©KVç¼“å­˜å¹¶æ‰§è¡Œæ¿€æ´»-æ¿€æ´»ä¹˜æ³•çš„é‡åŒ–ï¼›ä»¥åŠåŸºäºç¨€ç–æ€§çš„Softmaxè¿‘ä¼¼ï¼ˆSASï¼‰ï¼Œå®ƒæ¶ˆé™¤äº†åœ¨æ³¨æ„åŠ›æŒ‡æ•°è¿ç®—æœŸé—´åé‡åŒ–åˆ°FP32çš„éœ€è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTurboAttentionå®ç°äº†æ³¨æ„åŠ›åŠ é€Ÿ1.2-1.8å€ï¼ŒKVç¼“å­˜å¤§å°å‡å°‘äº†è¶…è¿‡4.4å€ï¼Œå¹¶ä¸”åœ¨å„ç§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾FP16åŸºçº¿2.37å€çš„æœ€å¤§ååé‡ï¼Œè¶…è¿‡äº†æœ€æ–°çš„é‡åŒ–å’Œå‹ç¼©æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08585v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>LLMæ¨ç†éœ€è¦å¤§é‡è®¡ç®—ä¸å†…å­˜ï¼Œç‰¹åˆ«æ˜¯å…³é”®æ³¨æ„åŠ›æœºåˆ¶ã€‚ç°æœ‰æŠ€æœ¯å¦‚é‡åŒ–å’ŒFlashAttentionç­‰è™½æå‡æ•ˆç‡ï¼Œä½†å„æœ‰ä¾§é‡ã€‚æœ¬æ–‡æå‡ºTurboAttentionï¼Œé€šè¿‡FlashQå’ŒSparsity-based Softmax Approximation (SAS)ä¸¤å¤§åˆ›æ–°ï¼ŒåŒæ—¶è§£å†³å†…å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTurboAttentionåŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼Œç¼©å°KVç¼“å­˜å¤§å°ï¼Œæå‡æœ€å¤§ååé‡ï¼Œä¸”åœ¨å¤šç§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šä¼˜äºç°æœ‰é‡åŒ–ä¸å‹ç¼©æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMæ¨ç†ä¾èµ–å¤§é‡è®¡ç®—å’Œå†…å­˜ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é‡åŒ–å’ŒFlashAttentionç­‰æŠ€æœ¯è™½æå‡æ•ˆç‡ï¼Œä½†å„æœ‰å±€é™ã€‚</li>
<li>TurboAttentioné€šè¿‡FlashQå’ŒSASä¸¤å¤§åˆ›æ–°ï¼ŒåŒæ—¶è§£å†³å†…å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>FlashQæŠ€æœ¯å®ç°KVç¼“å­˜å‹ç¼©å’Œæ¿€æ´»ä¹˜æ³•é‡åŒ–æ‰§è¡Œã€‚</li>
<li>SASæ¶ˆé™¤æ³¨æ„åŠ›æŒ‡æ•°è¿ç®—ä¸­æµ®ç‚¹è§£é‡åŒ–çš„éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºTurboAttentionæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦å’Œç¼“å­˜æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2aa13ec6d967123c700eac65289299ba.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-12c9472feee293a58946daa11dab8b79.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f5d83ed70042472422f235eb4d579530.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7f134495953af18158cd020ded942ff6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-40958ea88bed04942910eb4a10c27a9c.jpg" align="middle">
</details>




<h2 id="Can-We-Generate-Visual-Programs-Without-Prompting-LLMs"><a href="#Can-We-Generate-Visual-Programs-Without-Prompting-LLMs" class="headerlink" title="Can We Generate Visual Programs Without Prompting LLMs?"></a>Can We Generate Visual Programs Without Prompting LLMs?</h2><p><strong>Authors:Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem</strong></p>
<p>Visual programming prompts LLMs (large language mod-els) to generate executable code for visual tasks like visual question answering (VQA). Prompt-based methods are difficult to improve while also being unreliable and costly in both time and money. Our goal is to develop an efficient visual programming system without 1) using prompt-based LLMs at inference time and 2) a large set of program and answer annotations. We develop a synthetic data augmentation approach and alternative program generation method based on decoupling programs into higher-level skills called templates and the corresponding arguments. Our results show that with data augmentation, prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive with state-of-the art models with the added benefit of much faster inference </p>
<blockquote>
<p>è§†è§‰ç¼–ç¨‹æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè§†è§‰ä»»åŠ¡ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€‚åŸºäºæç¤ºçš„æ–¹æ³•å¾ˆéš¾æ”¹è¿›ï¼ŒåŒæ—¶ä¸å¯é ï¼Œæ—¶é—´å’Œé‡‘é’±æˆæœ¬éƒ½å¾ˆé«˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ä¸ä½¿ç”¨åŸºäºæç¤ºçš„LLMè¿›è¡Œæ¨æ–­çš„æƒ…å†µä¸‹ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨å¤§é‡çš„ç¨‹åºå’Œç­”æ¡ˆæ³¨é‡Šæ¥å¼€å‘é«˜æ•ˆçš„è§†è§‰ç¼–ç¨‹ç³»ç»Ÿã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•å’ŒåŸºäºå°†ç¨‹åºè§£è€¦ä¸ºç§°ä¸ºæ¨¡æ¿çš„é«˜çº§æŠ€èƒ½å’Œç›¸åº”å‚æ•°çš„æ›¿ä»£ç¨‹åºç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ•°æ®å¢å¼ºï¼Œæ— æç¤ºçš„å°å‹LLMï¼ˆçº¦1Bå‚æ•°ï¼‰ä¸æœ€æ–°æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶å¸¦æ¥æ›´å¿«çš„æ¨æ–­ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰ç¼–ç¨‹æç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ç”Ÿæˆç”¨äºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ï¼‰çš„å¯æ‰§è¡Œä»£ç ã€‚ç„¶è€Œï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•éš¾ä»¥æ”¹è¿›ï¼Œä¸”ä¸å¯é ã€è€—æ—¶è€—é’±ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§é«˜æ•ˆçš„è§†è§‰ç¼–ç¨‹ç³»ç»Ÿï¼Œä¸ä½¿ç”¨åŸºäºæç¤ºçš„LLMè¿›è¡Œæ¨æ–­ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤§é‡çš„ç¨‹åºå’Œç­”æ¡ˆæ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»¥åŠåŸºäºå°†ç¨‹åºè§£è€¦ä¸ºé«˜çº§æŠ€èƒ½ï¼ˆç§°ä¸ºæ¨¡æ¿ï¼‰å’Œç›¸åº”å‚æ•°çš„æ›¿ä»£ç¨‹åºç”Ÿæˆæ–¹æ³•ã€‚ç»“æœè¯æ˜ï¼Œé€šè¿‡æ•°æ®å¢å¼ºï¼Œæ— éœ€æç¤ºçš„å°å‹LLMï¼ˆçº¦1Bå‚æ•°ï¼‰çš„æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç«äº‰ï¼Œå¹¶å…·å¤‡æ›´å¿«çš„æ¨æ–­é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ç¼–ç¨‹æç¤ºä½¿LLMèƒ½å¤Ÿç”Ÿæˆç”¨äºè§†è§‰ä»»åŠ¡çš„å¯æ‰§è¡Œä»£ç ã€‚</li>
<li>åŸºäºæç¤ºçš„æ–¹æ³•å­˜åœ¨æ”¹è¿›å›°éš¾ã€ä¸å¯é å’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§é«˜æ•ˆçš„è§†è§‰ç¼–ç¨‹ç³»ç»Ÿï¼Œä¸ä½¿ç”¨åŸºäºæç¤ºçš„LLMè¿›è¡Œæ¨æ–­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è§£è€¦ç¨‹åºä¸ºé«˜çº§æŠ€èƒ½ï¼ˆæ¨¡æ¿ï¼‰å’Œç›¸åº”å‚æ•°æ¥ç”Ÿæˆæ›¿ä»£ç¨‹åºã€‚</li>
<li>æ•°æ®å¢å¼ºä½¿å°å‹LLMçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-71a052fea07471cdd5b92ce33f76a2e9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-495a2ed1307492f0e4d3b1fb89c1e5e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2871162a30b0025d43f44b1d6134418f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-09544747a6fdbe241269e89ec78cea69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-64a390556423516931dab19b62b4f26c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-87feed1c478cec6f5d341f9548563b75.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d023468a4290179e5a74db0a79de7ee3.jpg" align="middle">
</details>




<h2 id="Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning"><a href="#Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning" class="headerlink" title="Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning"></a>Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning</h2><p><strong>Authors:Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora KreaÄiÄ‡, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</strong></p>
<p>Large Language Models are trained on extensive datasets that often contain sensitive, human-generated information, raising significant concerns about privacy breaches. While certified unlearning approaches offer strong privacy guarantees, they rely on restrictive model assumptions that are not applicable to LLMs. As a result, various unlearning heuristics have been proposed, with the associated privacy risks assessed only empirically. The standard evaluation pipelines typically randomly select data for removal from the training set, apply unlearning techniques, and use membership inference attacks to compare the unlearned models against models retrained without the to-be-unlearned data. However, since every data point is subject to the right to be forgotten, unlearning should be considered in the worst-case scenario from the privacy perspective. Prior work shows that data outliers may exhibit higher memorization effects. Intuitively, they are harder to be unlearn and thus the privacy risk of unlearning them is underestimated in the current evaluation. In this paper, we leverage minority data to identify such a critical flaw in previously widely adopted evaluations. We substantiate this claim through carefully designed experiments, including unlearning canaries related to minority groups, inspired by privacy auditing literature. Using personally identifiable information as a representative minority identifier, we demonstrate that minority groups experience at least 20% more privacy leakage in most cases across six unlearning approaches, three MIAs, three benchmark datasets, and two LLMs of different scales. Given that the right to be forgotten should be upheld for every individual, we advocate for a more rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation framework represents an initial step toward ensuring more equitable assessments of LLM unlearning efficacy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ…å«æ•æ„Ÿäººç±»ç”Ÿæˆä¿¡æ¯çš„åºå¤§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¼•å‘äº†å…³äºéšç§æ³„éœ²çš„ä¸¥é‡å…³åˆ‡ã€‚è™½ç„¶ç»è¿‡è®¤è¯çš„é—å¿˜å¤„ç†æ–¹æ³•æä¾›å¼ºæœ‰åŠ›çš„éšç§ä¿è¯ï¼Œä½†å®ƒä»¬ä¾èµ–äºä¸é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é™åˆ¶æ€§æ¨¡å‹å‡è®¾ã€‚å› æ­¤ï¼Œå·²ç»æå‡ºäº†å„ç§é—å¿˜å¯å‘å¼æ–¹æ³•ï¼Œå¹¶å¯¹ç›¸å…³çš„éšç§é£é™©è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚æ ‡å‡†çš„è¯„ä¼°æµç¨‹é€šå¸¸ä¼šä»è®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©æ•°æ®è¿›è¡Œåˆ é™¤ï¼Œåº”ç”¨é—å¿˜æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨æˆå‘˜æ¨ç†æ”»å‡»æ¥æ¯”è¾ƒæœªå­¦ä¹ çš„æ¨¡å‹ä¸é‡æ–°è®­ç»ƒçš„æ¨¡å‹ï¼ˆæ— éœ€é—å¿˜æ•°æ®ï¼‰ã€‚ç„¶è€Œï¼Œç”±äºæ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰è¢«é—å¿˜çš„æƒåˆ©ï¼Œä»éšç§çš„è§’åº¦æ¥çœ‹ï¼Œé—å¿˜åº”åœ¨æœ€åçš„æƒ…å†µä¸‹è€ƒè™‘ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®å¼‚å¸¸å€¼å¯èƒ½è¡¨ç°å‡ºæ›´é«˜çš„è®°å¿†æ•ˆæœã€‚ç›´è§‰ä¸Šï¼Œå®ƒä»¬æ›´éš¾è¢«é—å¿˜ï¼Œå› æ­¤å½“å‰è¯„ä¼°ä¸­å¯¹é—å¿˜å®ƒä»¬çš„éšç§é£é™©è¢«ä½ä¼°äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å°‘æ•°æ•°æ®æ¥è¯†åˆ«ä»¥å‰å¹¿æ³›é‡‡ç”¨çš„è¯„ä¼°ä¸­çš„è¿™ä¸€å…³é”®ç¼ºé™·ã€‚æˆ‘ä»¬é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å®éªŒè¯å®äº†è¿™ä¸€ä¸»å¼ ï¼ŒåŒ…æ‹¬ä¸å°‘æ•°ç¾¤ä½“ç›¸å…³çš„é—å¿˜é‡‘ä¸é›€ï¼ˆå—éšç§å®¡è®¡æ–‡çŒ®å¯å‘ï¼‰ã€‚ä»¥ä¸ªäººä¿¡æ¯ä½œä¸ºå°‘æ•°ç¾¤ä½“çš„ä»£è¡¨æ ‡è¯†ç¬¦ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä¸å…­ç§é—å¿˜æ–¹æ³•ã€ä¸‰ç§MIAã€ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå°‘æ•°ç¾¤ä½“çš„éšç§æ³„éœ²è‡³å°‘å¢åŠ äº†20%ã€‚é‰´äºæ¯ä¸ªäººéƒ½æœ‰è¢«é—å¿˜çš„æƒåˆ©ï¼Œæˆ‘ä»¬ä¸»å¼ å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜æ–¹æ³•è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„å°‘æ•°ç¾¤ä½“æ„è¯†è¯„ä¼°æ¡†æ¶æ˜¯ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹é—å¿˜æ•ˆåŠ›è¯„ä¼°æ›´åŠ å…¬å¹³çš„ç¬¬ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08559v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¹¿æ³›æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶‰åŠæ•æ„Ÿã€äººç±»ç”Ÿæˆçš„ä¿¡æ¯å¼•å‘äº†éšç§æ³„éœ²çš„æ‹…å¿§ã€‚è™½ç„¶è®¤è¯é—å¿˜æ–¹æ³•æä¾›å¼ºæœ‰åŠ›çš„éšç§ä¿è¯ï¼Œä½†å®ƒä»¬ä¾èµ–äºä¸é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é™åˆ¶æ€§æ¨¡å‹å‡è®¾ã€‚å› æ­¤ï¼Œæå‡ºäº†å„ç§é—å¿˜å¯å‘å¼æ–¹æ³•ï¼Œå¹¶ä»…ä»å®è¯è§’åº¦è¯„ä¼°äº†ç›¸å…³çš„éšç§é£é™©ã€‚æ ‡å‡†è¯„ä¼°æµç¨‹é€šå¸¸éšæœºé€‰æ‹©æ•°æ®è¿›è¡Œè®­ç»ƒé›†åˆ é™¤ï¼Œåº”ç”¨é—å¿˜æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨æˆå‘˜æ¨ç†æ”»å‡»æ¥æ¯”è¾ƒæœªé—å¿˜æ¨¡å‹ä¸æœªè¿›è¡Œé—å¿˜æ•°æ®é‡æ–°è®­ç»ƒçš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œä»éšç§è§’åº¦æ¥çœ‹ï¼Œæ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰è¢«é—å¿˜çš„æƒåˆ©ï¼Œå› æ­¤åº”è€ƒè™‘æœ€åæƒ…å†µä¸‹çš„é—å¿˜æƒ…å†µã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®å¼‚å¸¸å€¼å¯èƒ½è¡¨ç°å‡ºæ›´é«˜çš„è®°å¿†æ•ˆåº”ï¼Œå³å®ƒä»¬æ›´éš¾è¢«é—å¿˜ï¼Œå› æ­¤å½“å‰è¯„ä¼°ä½ä¼°äº†é—å¿˜å®ƒä»¬çš„éšç§é£é™©ã€‚æœ¬æ–‡åˆ©ç”¨å°‘æ•°æ•°æ®æ¥è¯†åˆ«ä»¥å‰å¹¿æ³›é‡‡ç”¨çš„è¯„ä¼°ä¸­çš„è¿™ä¸€å…³é”®ç¼ºé™·ã€‚æœ¬æ–‡é€šè¿‡ç²¾å¿ƒè®¾è®¡å®éªŒæ¥è¯å®è¿™ä¸€ä¸»å¼ ï¼ŒåŒ…æ‹¬ä¸å°‘æ•°ç¾¤ä½“ç›¸å…³çš„é—å¿˜çŠ¬çš„é—å¿˜ï¼Œçµæ„Ÿæ¥è‡ªéšç§å®¡è®¡æ–‡çŒ®ã€‚ä»¥ä¸ªäººèº«ä»½ä¿¡æ¯ä½œä¸ºå°‘æ•°ç¾¤ä½“çš„ä»£è¡¨æ ‡è¯†ç¬¦ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯¹äºå…­ç§é—å¿˜æ–¹æ³•ã€ä¸‰ç§MIAã€ä¸‰ç§åŸºå‡†æ•°æ®é›†å’Œä¸¤ç§ä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°‘æ•°ç¾¤ä½“è‡³å°‘ç»å†äº†2.æ›´å¤šçš„éšç§æ³„éœ²æƒ…å†µå‘¼åå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜æ–¹æ³•è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„é¢å‘å°‘æ•°ç¾¤ä½“çš„è¯„ä¼°æ¡†æ¶æ˜¯ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹é—å¿˜æ•ˆç‡æ›´å…¬å¹³è¯„ä¼°çš„åˆæ­¥æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ¶‰åŠæ•æ„Ÿä¿¡æ¯å¼•å‘éšç§æ³„éœ²çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰é—å¿˜æ–¹æ³•è¯„ä¼°ä¾èµ–äºæ¨¡å‹å‡è®¾ä¸é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ•°æ®å¼‚å¸¸å€¼ï¼ˆå°‘æ•°ç¾¤ä½“ï¼‰åœ¨é—å¿˜è¿‡ç¨‹ä¸­è¡¨ç°å‡ºæ›´é«˜çš„éšç§æ³„éœ²é£é™©ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æµç¨‹å¯èƒ½ä½ä¼°é—å¿˜æŸäº›æ•°æ®çš„éšç§é£é™©ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯å°‘æ•°ç¾¤ä½“åœ¨é—å¿˜è¿‡ç¨‹ä¸­çš„éšç§æ³„éœ²æƒ…å†µæ›´ä¸ºä¸¥é‡ã€‚</li>
<li>éœ€è¦æ›´ä¸¥æ ¼çš„è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5d7df70e264508e34d7458412e0bc8a8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-612dcf7498baba74ceee6359bb90c887.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9c0407d01398c0d835aecbd08661ead9.jpg" align="middle">
</details>




<h2 id="MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback"><a href="#MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback" class="headerlink" title="MaestroMotif: Skill Design from Artificial Intelligence Feedback"></a>MaestroMotif: Skill Design from Artificial Intelligence Feedback</h2><p><strong>Authors:Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca Dâ€™Oro</strong></p>
<p>Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLMâ€™s feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLMâ€™s code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability. </p>
<blockquote>
<p>æè¿°è‡ªç„¶è¯­è¨€ä¸­çš„æŠ€èƒ½å…·æœ‰å°†äººç±»å…³äºå†³ç­–åˆ¶å®šçš„çŸ¥è¯†æ³¨å…¥äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†MaestroMotifï¼Œè¿™æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½è¾…åŠ©æŠ€èƒ½è®¾è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜æ€§èƒ½å’Œå¯é€‚åº”çš„ä»£ç†ã€‚MaestroMotifåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æ¥æœ‰æ•ˆåœ°åˆ›å»ºå’Œé‡å¤ä½¿ç”¨æŠ€èƒ½ã€‚å®ƒé¦–å…ˆä½¿ç”¨LLMçš„åé¦ˆæ¥è‡ªåŠ¨è®¾è®¡å¯¹åº”äºæ¯ä¸ªæŠ€èƒ½çš„å¥–åŠ±ï¼Œä»ä»–ä»¬çš„è‡ªç„¶è¯­è¨€æè¿°å¼€å§‹ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨LLMçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œè®­ç»ƒæŠ€èƒ½å¹¶å°†å®ƒä»¬ç»„åˆèµ·æ¥å®ç°ç”¨è¯­è¨€æŒ‡å®šçš„å¤æ‚è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨NetHackå­¦ä¹ ç¯å¢ƒï¼ˆNLEï¼‰çš„ä¸€ç³»åˆ—å¤æ‚ä»»åŠ¡ä¸­è¯„ä¼°äº†MaestroMotifï¼Œç»“æœè¡¨æ˜å®ƒåœ¨æ€§èƒ½å’Œå¯ç”¨æ€§æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08542v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MaestroMotifæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§AIè¾…åŠ©æŠ€èƒ½è®¾è®¡æ–¹æ³•ï¼Œå¯é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æŠ€èƒ½å¹¶å°†å…¶æ³¨å…¥AIç³»ç»Ÿï¼Œç”Ÿæˆç›¸åº”å¥–åŠ±ï¼Œä»è€Œåˆ›å»ºå’Œå¤ç”¨æŠ€èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡ŒæŠ€èƒ½è®­ç»ƒå’Œç»„åˆï¼Œå®ç°å¤æ‚è¡Œä¸ºçš„å®æ–½ã€‚åœ¨NetHackå­¦ä¹ ç¯å¢ƒï¼ˆNLEï¼‰çš„å¤æ‚ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒMaestroMotifè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œå¯ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaestroMotifæ˜¯ä¸€ç§AIè¾…åŠ©æŠ€èƒ½è®¾è®¡æ–¹æ³•ï¼Œå¯å°†è‡ªç„¶è¯­è¨€æè¿°çš„æŠ€èƒ½æ³¨å…¥AIç³»ç»Ÿã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆä¸æŠ€èƒ½ç›¸å¯¹åº”çš„å¥–åŠ±ã€‚</li>
<li>MaestroMotifç»“åˆLLMçš„ä»£ç ç”Ÿæˆèƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡ŒæŠ€èƒ½è®­ç»ƒå’Œç»„åˆã€‚</li>
<li>MaestroMotifåœ¨NetHackå­¦ä¹ ç¯å¢ƒï¼ˆNLEï¼‰çš„å¤æ‚ä»»åŠ¡è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>MaestroMotifæé«˜äº†æŠ€èƒ½è®¾è®¡çš„æ€§èƒ½å’Œå¯ç”¨æ€§ã€‚</li>
<li>LLMåœ¨æŠ€èƒ½è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6f665febf0a88f026fb77766c9392837.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-628a7f2a23e6422217c4fab0551ffeae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a055c6f27078f47409556b8e37ea80d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e3b8bb5f78ec046bbf77ef4055438e59.jpg" align="middle">
</details>




<h2 id="EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance"><a href="#EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance" class="headerlink" title="EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance"></a>EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance</h2><p><strong>Authors:Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang</strong></p>
<p>As large language models (LLMs) continue to advance, the demand for higher quality and faster processing of long contexts across various applications is growing. KV cache is widely adopted as it stores previously generated key and value tokens, effectively reducing redundant computations during inference. However, as memory overhead becomes a significant concern, efficient compression of KV cache has gained increasing attention. Most existing methods perform compression from two perspectives: identifying important tokens and designing compression strategies. However, these approaches often produce biased distributions of important tokens due to the influence of accumulated attention scores or positional encoding. Furthermore, they overlook the sparsity and redundancy across different heads, which leads to difficulties in preserving the most effective information at the head level. To this end, we propose EMS to overcome these limitations, while achieving better KV cache compression under extreme compression ratios. Specifically, we introduce a Global-Local score that combines accumulated attention scores from both global and local KV tokens to better identify the token importance. For the compression strategy, we design an adaptive and unified Evict-then-Merge framework that accounts for the sparsity and redundancy of KV tokens across different heads. Additionally, we implement the head-wise parallel compression through a zero-class mechanism to enhance efficiency. Extensive experiments demonstrate our SOTA performance even under extreme compression ratios. EMS consistently achieves the lowest perplexity, improves scores by over 1.28 points across four LLMs on LongBench under a 256 cache budget, and preserves 95% retrieval accuracy with a cache budget less than 2% of the context length in the Needle-in-a-Haystack task. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­è¿›æ­¥ï¼Œå¯¹é«˜è´¨é‡å’Œå¿«é€Ÿå¤„ç†å„ç§åº”ç”¨ä¸­çš„é•¿æ–‡æœ¬çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚KVç¼“å­˜å› å…¶èƒ½å¤Ÿå­˜å‚¨å…ˆå‰ç”Ÿæˆçš„é”®å’Œå€¼ä»¤ç‰Œè€Œå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—ã€‚ç„¶è€Œï¼Œéšç€å†…å­˜å¼€é”€æˆä¸ºä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼ŒKVç¼“å­˜çš„æœ‰æ•ˆå‹ç¼©è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä»ä¸¤ä¸ªè§’åº¦è¿›è¡Œå‹ç¼©ï¼šè¯†åˆ«é‡è¦ä»¤ç‰Œå’Œè®¾è®¡å‹ç¼©ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿç”±äºç´¯ç§¯çš„æ³¨æ„åŠ›åˆ†æ•°æˆ–ä½ç½®ç¼–ç çš„å½±å“è€Œå¯¼è‡´çš„é‡è¦ä»¤ç‰Œåˆ†å¸ƒåå·®ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¿½è§†äº†ä¸åŒå¤´ä¹‹é—´çš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ï¼Œå¯¼è‡´åœ¨å¤´éƒ¨å±‚é¢ä¿ç•™æœ€æœ‰æ•ˆä¿¡æ¯æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºEMSæ¥å…‹æœè¿™äº›å±€é™æ€§ï¼Œåœ¨æç«¯çš„å‹ç¼©æ¯”ä¾‹ä¸‹å®ç°æ›´å¥½çš„KVç¼“å­˜å‹ç¼©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨åˆ†æ•°ï¼Œè¯¥åˆ†æ•°ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨KVä»¤ç‰Œä¸Šçš„ç´¯ç§¯æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ä»¤ç‰Œçš„é‡è¦æ€§ã€‚å¯¹äºå‹ç¼©ç­–ç•¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”çš„ç»Ÿä¸€é€å‡ºååˆå¹¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†ä¸åŒå¤´ä¹‹é—´KVä»¤ç‰Œçš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é›¶ç±»æœºåˆ¶å®ç°äº†å¤´éƒ¨å¹¶è¡Œå‹ç¼©ï¼Œä»¥æé«˜æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æç«¯çš„å‹ç¼©æ¯”ä¾‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ€§èƒ½ä¹Ÿè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚EMSå§‹ç»ˆè¾¾åˆ°äº†æœ€ä½çš„å›°æƒ‘åº¦ï¼Œåœ¨LongBenchä¸Šçš„å››ä¸ªLLMä¸­å¾—åˆ†æé«˜äº†è¶…è¿‡1.28ç‚¹ï¼Œåœ¨é¢„ç®—ä¸º256çš„ç¼“å­˜ä¸­ä¿ç•™äº†95%çš„æ£€ç´¢å‡†ç¡®æ€§ï¼Œè€Œåœ¨Haystackä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸åˆ°é¢„ç®—çš„2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08521v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸æ–­å‘å±•ï¼Œå¯¹é«˜è´¨é‡å’Œå¿«é€Ÿå¤„ç†é•¿æ–‡æœ¬çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚KVç¼“å­˜å› å…¶èƒ½å¤Ÿå­˜å‚¨å…ˆå‰ç”Ÿæˆçš„é”®å’Œå€¼ä»¤ç‰Œè€Œå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—ã€‚ç„¶è€Œï¼Œéšç€å†…å­˜å¼€é”€æˆä¸ºä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼ŒKVç¼“å­˜çš„å‹ç¼©æ•ˆç‡é€æ¸å—åˆ°å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä»è¯†åˆ«å’Œå‹ç¼©ç­–ç•¥ä¸¤ä¸ªè§’åº¦è¿›è¡Œå‹ç¼©ï¼Œä½†å­˜åœ¨åè§åˆ†å¸ƒå’Œé‡è¦ä»¤ç‰Œçš„é—®é¢˜ï¼Œå¿½è§†äº†ä¸åŒå¤´ä¹‹é—´çš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºEMSæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨ä»¤ç‰Œåˆ†æ•°çš„ç´¯ç§¯æ³¨æ„åŠ›å¾—åˆ†æ¥æ›´å¥½åœ°è¯†åˆ«ä»¤ç‰Œé‡è¦æ€§ï¼Œå¹¶è®¾è®¡è‡ªé€‚åº”ç»Ÿä¸€çš„é€å‡ºåˆå¹¶æ¡†æ¶æ¥å¤„ç†KVä»¤ç‰Œçš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æç«¯å‹ç¼©æ¯”ä¸‹ï¼ŒEMSä¹Ÿèƒ½å®ç°æ›´å¥½çš„KVç¼“å­˜å‹ç¼©æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMså¤„ç†é•¿æ–‡æœ¬çš„éœ€æ±‚å¢é•¿ï¼ŒKVç¼“å­˜å› å…¶å‡å°‘å†—ä½™è®¡ç®—è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>KVç¼“å­˜å‹ç¼©æˆä¸ºå…³æ³¨ç„¦ç‚¹ï¼Œå› å†…å­˜å¼€é”€é—®é¢˜æ—¥ç›Šçªå‡ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨åè§åˆ†å¸ƒå’Œé‡è¦ä»¤ç‰Œçš„é—®é¢˜ï¼Œå¿½è§†ä¸åŒå¤´ä¹‹é—´çš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚</li>
<li>EMSæ–¹æ³•é€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨ä»¤ç‰Œåˆ†æ•°çš„ç´¯ç§¯æ³¨æ„åŠ›å¾—åˆ†æ¥è¯†åˆ«ä»¤ç‰Œé‡è¦æ€§ã€‚</li>
<li>EMSè®¾è®¡äº†è‡ªé€‚åº”ç»Ÿä¸€çš„é€å‡ºåˆå¹¶æ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†KVä»¤ç‰Œçš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-38b496138c97e1dfd0f692985690d782.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a51c48e21df8dbfe8058c928883ebc9c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f36fbc21c25055b288075b463776d2e4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d840c002b790302fa195ba1f62a5dacd.jpg" align="middle">
</details>




<h2 id="Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation"><a href="#Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation" class="headerlink" title="Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation"></a>Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation</h2><p><strong>Authors:Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang</strong></p>
<p>The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction. </p>
<blockquote>
<p>åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­ï¼Œé‡æ’å™¨å’Œç”Ÿæˆå™¨æ˜¯ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£æ’åˆ—ç›¸å…³æ–‡æ¡£å’Œç”Ÿæˆå“åº”ã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä¹‹é—´çš„å·®å¼‚ï¼Œé‡æ’å™¨æ’åˆ—çš„ç›¸å…³æ–‡æ¡£ä¸ç”Ÿæˆå™¨æ”¯æŒå›ç­”æŸ¥è¯¢æ‰€éœ€çš„æ–‡æ¡£ä¹‹é—´å­˜åœ¨ä¸å¯é¿å…çš„é¸¿æ²Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é¸¿æ²Ÿï¼Œæˆ‘ä»¬æå‡ºäº†RADIOï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ç†æ€§è’¸é¦ï¼ˆRAtionale DIstillatiOnï¼‰çš„æ–°é¢–ä¸”å®ç”¨çš„åå¥½å¯¹é½æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ¥æå–å›ç­”æŸ¥è¯¢æ‰€éœ€ç†æ®çš„ç†æ€§æå–æ–¹æ³•ã€‚éšåï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºç†æ€§çš„å¯¹é½è¿‡ç¨‹ï¼Œæ ¹æ®æå–çš„ç†æ®é‡æ–°æ’åˆ—æ–‡æ¡£ï¼Œå¹¶å¾®è°ƒé‡æ’å™¨ä»¥å¯¹é½åå¥½ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†çš„ä¸¤ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨çº¿å‘å¸ƒï¼Œä»¥æ–¹ä¾¿å¤åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08519v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Retrieval-Augmented Generationï¼ˆRAGï¼‰ç®¡é“ä¸­çš„ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šrerankerå’Œgeneratorã€‚ä¸ºäº†è§£å†³ä¸¤è€…åœ¨æ’åç›¸å…³æ–‡æ¡£å’Œæ”¯æŒå›ç­”æŸ¥è¯¢æ–¹é¢çš„å·®å¼‚ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRADIOçš„æ–°å‹å®ç”¨åå¥½å¯¹é½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Large Language Modelsï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ¥æå–å›ç­”æŸ¥è¯¢æ‰€éœ€çš„ç†ç”±ï¼Œå¹¶åŸºäºè¿™äº›ç†ç”±é‡æ–°æ’åæ–‡æ¡£å’Œå¯¹rerankerè¿›è¡Œå¾®è°ƒä»¥å¯¹é½åå¥½ã€‚ç»è¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆï¼Œç›¸å…³ä»£ç å·²åœ¨çº¿å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Retrieval-Augmented Generationä¸­çš„rerankerå’Œgeneratorä¹‹é—´å­˜åœ¨å·®è·ï¼Œå½±å“å›ç­”æŸ¥è¯¢çš„æ•ˆæœã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºRADIOçš„åå¥½å¯¹é½æ¡†æ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨Large Language Modelsçš„æ¨ç†èƒ½åŠ›æå–å›ç­”æŸ¥è¯¢çš„ç†ç”±ã€‚</li>
<li>åŸºäºæå–çš„ç†ç”±é‡æ–°æ’åæ–‡æ¡£ã€‚</li>
<li>å¯¹rerankerè¿›è¡Œå¾®è°ƒä»¥å¯¹é½åå¥½ï¼Œä»¥ç¼©å°ä¸generatorä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªä»»åŠ¡åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-07bd96bd382f50d4b0be40a29e0ad2bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0ff74513b2d945c44687fdee91eb0ece.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7e8a25f53efe8135ab5000dfaaa6a7b5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c5ab945745fa9b9f0e527f229d00a670.jpg" align="middle">
</details>




<h2 id="POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications"><a href="#POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications" class="headerlink" title="POINTS1.5: Building a Vision-Language Model towards Real World   Applications"></a>POINTS1.5: Building a Vision-Language Model towards Real World   Applications</h2><p><strong>Authors:Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, Jie Zhou</strong></p>
<p>Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹è¿‘æœŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¾‹å¦‚å…‰å­¦å­—ç¬¦è¯†åˆ«å’Œå¤æ‚å›¾è¡¨åˆ†æã€‚åŸºäºè¿™ä¸€è¶‹åŠ¿ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹â€”â€”POINTS1.5ï¼Œæ—¨åœ¨åœ¨å„ç§ç°å®åº”ç”¨ä¸­è·å¾—å“è¶Šè¡¨ç°ã€‚POINTS1.5æ˜¯POINTS1.0çš„å¢å¼ºç‰ˆï¼Œå¹¶èå…¥äº†è‹¥å¹²å…³é”®åˆ›æ–°ï¼š</p>
</blockquote>
<p>ä¸€ï¼‰æˆ‘ä»¬æ›¿æ¢äº†åŸå§‹çš„CLIPè§†è§‰ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å…·æœ‰å›ºå®šå›¾åƒåˆ†è¾¨ç‡ï¼Œä½¿ç”¨NaViTé£æ ¼çš„è§†è§‰ç¼–ç å™¨ï¼Œæ”¯æŒåŸç”ŸåŠ¨æ€é«˜åˆ†è¾¨ç‡ã€‚è¿™ä½¿å¾—POINTS1.5èƒ½å¤Ÿå¤„ç†ä»»ä½•åˆ†è¾¨ç‡çš„å›¾åƒï¼Œè€Œæ— éœ€å°†å®ƒä»¬åˆ†å‰²æˆç“¦ç‰‡ã€‚</p>
<p>äºŒï¼‰æˆ‘ä»¬ä¸ºPOINTS1.5å¢åŠ äº†åŒè¯­æ”¯æŒï¼Œè¿™æå¤§åœ°æé«˜äº†å…¶åœ¨ä¸­æ–‡æ–¹é¢çš„èƒ½åŠ›ã€‚ç”±äºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡å¼€æºæ•°æ®é›†ç¨€ç¼ºï¼Œæˆ‘ä»¬ä»äº’è”ç½‘æ”¶é›†äº†å¤§é‡å›¾åƒï¼Œå¹¶ä½¿ç”¨æ‰‹åŠ¨å’Œè‡ªåŠ¨æ–¹æ³•ç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œæ ‡æ³¨ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08443v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸï¼Œè§†ç•Œè¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚åœ¨æ­¤è¶‹åŠ¿ä¸‹æ¨å‡ºæ–°ä¸€ä»£è§†ç•Œè¯­è¨€æ¨¡å‹POINTS1.5ï¼Œå®ƒæ˜¯POINTS1.0çš„å‡çº§ç‰ˆï¼Œå…·æœ‰å¤šé¡¹å…³é”®æŠ€æœ¯æ”¹è¿›ã€‚åŒ…æ‹¬é‡‡ç”¨æ”¯æŒåŠ¨æ€é«˜åˆ†è¾¨ç‡çš„NaViTé£æ ¼è§†è§‰ç¼–ç å™¨ï¼Œæå‡ä¸­æ–‡æ”¯æŒèƒ½åŠ›ï¼Œå¹¶æ”¶é›†å¤§é‡äº’è”ç½‘å›¾åƒè¿›è¡Œæ ‡æ³¨ï¼Œä»¥åŠä¸ºè§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†æä¾›ä¸¥æ ¼è¿‡æ»¤æ–¹æ³•ã€‚è¿™äº›åˆ›æ–°ä½¿POINTS1.5æ˜¾è‘—ä¼˜äºPOINTS1.0ï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒPOINTS1.5-7Båœ¨è®­ç»ƒä»¤ç‰Œå°‘äº4äº¿çš„æƒ…å†µä¸‹å°±åœ¨OpenCompassæ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸€ï¼Œå¹¶ä¸”åœ¨å‚æ•°å°‘äº10äº¿çš„æ¨¡å‹ä¸­ç‹¬å é³Œå¤´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>POINTS1.5æ˜¯æ–°ä¸€ä»£è§†ç•Œè¯­è¨€æ¨¡å‹ï¼ŒåŸºäºPOINTS1.0è¿›è¡Œå‡çº§ã€‚</li>
<li>é‡‡ç”¨NaViTé£æ ¼çš„è§†è§‰ç¼–ç å™¨ä»¥æ”¯æŒåŠ¨æ€é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚</li>
<li>æä¾›å¯¹ä¸­æ–‡çš„å¼ºåŠ›æ”¯æŒï¼Œé€šè¿‡æ”¶é›†å¹¶æ ‡æ³¨å¤§é‡äº’è”ç½‘å›¾åƒå¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>ä¸¥æ ¼è¿‡æ»¤æ–¹æ³•ç”¨äºåˆ›å»ºè§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>POINTS1.5æ€§èƒ½å“è¶Šï¼Œæ˜¾è‘—ä¼˜äºä¸Šä¸€ä»£æ¨¡å‹ã€‚</li>
<li>POINTS1.5-7Bè®­ç»ƒä»¤ç‰Œæ•°å°‘äº4äº¿å³åœ¨OpenCompassæ’è¡Œæ¦œä¸Šé¢†å…ˆã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dc8c0e0a9e3ea04dd5e87e92c23e234d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9e46b078851b25402b8004072ad51698.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9d7787c431966427ede1048c8a9a6984.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2581b57e819a19b0d70e0fc3e8455559.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9ade5bc08b53234389efb475173eb8d6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-14763de648f4e6faa4d40ae81fa249ac.jpg" align="middle">
</details>




<h2 id="The-Roles-of-English-in-Evaluating-Multilingual-Language-Models"><a href="#The-Roles-of-English-in-Evaluating-Multilingual-Language-Models" class="headerlink" title="The Roles of English in Evaluating Multilingual Language Models"></a>The Roles of English in Evaluating Multilingual Language Models</h2><p><strong>Authors:Wessel Poelman, Miryam de Lhoneux</strong></p>
<p>Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from this imprecise method and instead focus on furthering language understanding. </p>
<blockquote>
<p>å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œé’ˆå¯¹å¤šç§è¯­è¨€çš„æ¨¡å‹ã€åŸºå‡†æµ‹è¯•æ–¹æ³•å’ŒæŠ€æœ¯ä¸æ–­æ¨å‡ºã€‚åœ¨è·¨è¯­è¨€è¯„ä¼°ä¸­ï¼Œè‹±è¯­å¸¸è¢«ç”¨æ¥æç¤ºè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ï¼Œä¸»è¦æ˜¯ä¸ºäº†å…‹æœå…¶ä»–è¯­è¨€æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„ç¼ºä¹ã€‚åœ¨è¿™ç¯‡ç«‹åœºè®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é˜è¿°äº†è‹±è¯­åœ¨å¤šè¯­è¨€LMè¯„ä¼°ä¸­çš„ä¸¤ä¸ªä½œç”¨ï¼šä½œä¸ºæ¥å£å’Œä½œä¸ºè‡ªç„¶è¯­è¨€ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸¤ä¸ªè§’è‰²æœ‰ä¸åŒçš„ç›®æ ‡ï¼šä»»åŠ¡æ€§èƒ½ä¸è¯­è¨€ç†è§£ã€‚é€šè¿‡æ•°æ®é›†å’Œè¯„ä¼°è®¾ç½®çš„ä¾‹å­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è¿™ä¸€ç‚¹ã€‚è®¸å¤šç ”ç©¶æ˜ç¡®ä½¿ç”¨è‹±è¯­ä½œä¸ºæ¥å£æ¥æé«˜ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬å»ºè®®æ”¾å¼ƒè¿™ç§ä¸ç²¾ç¡®çš„æ–¹æ³•ï¼Œè½¬è€Œä¸“æ³¨äºæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08392v1">PDF</a> NoDaLiDa 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè‹±è¯­åœ¨å¤šè¯­è¨€è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æœ¬æ–‡æ¢è®¨äº†è‹±è¯­åœ¨è¯„ä¼°ä¸­çš„ä¸¤ç§è§’è‰²ï¼šä½œä¸ºæ¥å£å’Œä½œä¸ºè‡ªç„¶è¯­è¨€ã€‚æ–‡ç« æŒ‡å‡ºè¿™ä¸¤ç§è§’è‰²æœ‰ä¸åŒçš„ç›®æ ‡ï¼Œåˆ†åˆ«æ˜¯ä»»åŠ¡æ€§èƒ½ä¸è¯­è¨€ç†è§£ã€‚æ–‡ç« é€šè¿‡æ•°æ®é›†å’Œè¯„ä¼°è®¾ç½®çš„ä¾‹å­å¼ºè°ƒäº†è¿™ç§å·®å¼‚ï¼Œå¹¶å»ºè®®æ”¾å¼ƒä½¿ç”¨è‹±è¯­ä½œä¸ºæ¥å£ä»¥æé«˜ä»»åŠ¡æ€§èƒ½çš„æ–¹æ³•ï¼Œè½¬è€Œä¸“æ³¨äºæé«˜è¯­è¨€ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‹±è¯­åœ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚</li>
<li>è‹±è¯­åœ¨è¯„ä¼°å¤šè¯­è¨€è¯­è¨€æ¨¡å‹æ—¶æœ‰ä¸¤ç§è§’è‰²ï¼šä½œä¸ºæ¥å£å’Œä½œä¸ºè‡ªç„¶è¯­è¨€ã€‚</li>
<li>ä½¿ç”¨è‹±è¯­ä½œä¸ºæ¥å£å¯ä»¥æé«˜ä»»åŠ¡æ€§èƒ½ï¼Œä½†è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªç²¾ç¡®çš„æ–¹æ³•ã€‚</li>
<li>è¯„ä¼°å¤šè¯­è¨€è¯­è¨€æ¨¡å‹æ—¶ï¼Œåº”å…³æ³¨è¯­è¨€ç†è§£è€Œéä»…ä»…ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ–‡ç« é€šè¿‡æ•°æ®é›†å’Œè¯„ä¼°è®¾ç½®çš„ä¾‹å­æ¥å¼ºè°ƒè‹±è¯­åœ¨è¯„ä¼°ä¸­çš„ä¸åŒè§’è‰²å’Œç›®æ ‡ã€‚</li>
<li>æ–‡ç« å»ºè®®æ”¾å¼ƒä½¿ç”¨è‹±è¯­ä½œä¸ºæ¥å£çš„æ–¹æ³•ï¼Œå¹¶è½¬å‘ä¸“æ³¨äºæé«˜è¯­è¨€ç†è§£çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e4913fb529f06a6d8ce3be82bddd61e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-425a52c1120dea05a7467888bdb16f84.jpg" align="middle">
</details>




<h2 id="SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs"><a href="#SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs" class="headerlink" title="SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs"></a>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs</h2><p><strong>Authors:Sultan Alrashed</strong></p>
<p>We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAIâ€™s Tulu 3 post-training pipeline to enhance Huggingfaceâ€™s SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval ($\Delta$11%), and mathematical reasoning with 51.6% on GSM8K ($\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC ($\Delta5.4%$). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SmolTulu-1.7b-Instructï¼Œæœ¬æŠ¥å‘Šç§°å…¶ä¸ºSmolTulu-DPO-1130ï¼Œè¿™æ˜¯ä¸€æ¬¾ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€‚åº”äº†AllenAIçš„Tulu 3åè®­ç»ƒç®¡é“ï¼Œä»¥ä¼˜åŒ–Huggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨1.35äº¿å‚æ•°æ¨¡å‹è¿›è¡Œçš„ç»¼åˆå®è¯åˆ†æè¡¨æ˜ï¼Œå­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°ä¹‹é—´çš„å…³ç³»ä¼šæ˜¾è‘—å½±å“æ¨¡å‹åœ¨ä»»åŠ¡ä¾èµ–æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„åˆ†æ­§ï¼šæ¨ç†ä»»åŠ¡ï¼ˆå¦‚ARCå’ŒGSM8Kï¼‰å¾—ç›Šäºè¾ƒé«˜çš„å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°æ¯”ä¾‹ï¼Œè€Œæ¨¡å¼è¯†åˆ«ä»»åŠ¡ï¼ˆå¦‚HellaSwagå’ŒIFevalï¼‰åœ¨è¾ƒä½çš„æ¯”ä¾‹ä¸‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚è¿™äº›è§è§£ä¸ºSmolTuluçš„å¼€å‘æä¾›äº†æŒ‡å¯¼ï¼ŒSmolTuluåœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢è¾¾åˆ°äº†å­2Bå‚æ•°æ¨¡å‹ä¸­çš„æœ€æ–°æ°´å¹³ï¼Œåœ¨IFevalä¸Šå¾—åˆ†ä¸º67.7%ï¼ˆæé«˜äº†11%ï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢ï¼ŒGSM8Kå¾—åˆ†ä¸º51.6%ï¼ˆæé«˜äº†3.4%ï¼‰ï¼Œå¦ä¸€ç‰ˆæœ¬åœ¨ARCä¸Šå¾—åˆ†è¾¾åˆ°57.1%ï¼ˆæé«˜äº†5.4%ï¼‰ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€è®­ç»ƒé…æ–¹å’Œæ¶ˆèç ”ç©¶ï¼Œä»¥ä¿ƒè¿›æ•ˆç‡æ¨¡å‹å¯¹é½çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œè¡¨æ˜ä¼˜åŒ–åŠ¨åŠ›å­¦çš„ä»”ç»†é€‚åº”æœ‰åŠ©äºç¼©å°å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„èƒ½åŠ›å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08347v1">PDF</a> 10 pages, 4 figures, and 13 tables. For the SmolTulu-1.7b-instruct   model, see: <a target="_blank" rel="noopener" href="https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct">https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†SmolTulu-1.7b-Instructè¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºHuggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨äº†AllenAIçš„Tulu 3åè®­ç»ƒç®¡é“è¿›è¡Œæ”¹è¿›ã€‚é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œå­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°ä¹‹é—´çš„å…³ç³»ä¼šä»»åŠ¡ä¾èµ–åœ°å½±å“æ¨¡å‹æ€§èƒ½ã€‚åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¾ƒé«˜çš„å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°æ¯”ä¾‹æœ‰åŠ©äºæ¨¡å‹è¡¨ç°ï¼Œè€Œåœ¨æ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¾ƒä½çš„æ¯”ä¾‹åˆ™è¡¨ç°æœ€ä½³ã€‚è¿™äº›è§è§£ä¸ºSmolTuluçš„å¼€å‘æä¾›äº†æŒ‡å¯¼ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†å­2Bå‚æ•°æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬å‘å¸ƒæ¨¡å‹ã€è®­ç»ƒé…æ–¹å’Œåˆ‡é™¤ç ”ç©¶ï¼Œä»¥æ¨åŠ¨é«˜æ•ˆæ¨¡å‹å¯¹é½ç ”ç©¶ï¼Œè¡¨æ˜ä¼˜åŒ–åŠ¨åŠ›å­¦çš„ç»†å¿ƒé€‚åº”æœ‰åŠ©äºç¼©å°å¤§å°è¯­è¨€æ¨¡å‹ä¹‹é—´çš„èƒ½åŠ›å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SmolTulu-1.7b-Instructæ˜¯ä¸€ä¸ªåŸºäºHuggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹æ”¹è¿›è€Œæ¥çš„æŒ‡ä»¤è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</li>
<li>å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°çš„å…³ç³»å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œè¿™åœ¨ä¸åŒä»»åŠ¡ä¸­æœ‰ä¸åŒçš„è¡¨ç°ã€‚</li>
<li>åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¾ƒé«˜çš„å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°æ¯”ä¾‹æœ‰åŠ©äºæé«˜æ¨¡å‹è¡¨ç°ã€‚</li>
<li>åœ¨æ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¾ƒä½çš„å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°æ¯”ä¾‹æœ‰åŠ©äºæ¨¡å‹è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>SmolTuluåœ¨æŒ‡ä»¤éµå¾ªå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†å­2Bå‚æ•°æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½æ°´å¹³ã€‚</li>
<li>æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹ã€è®­ç»ƒé…æ–¹å’Œåˆ‡é™¤ç ”ç©¶ä»¥ä¿ƒè¿›é«˜æ•ˆæ¨¡å‹å¯¹é½ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-24a7f844f882bf1f6c5dd15898647b0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6cd5849efc9a4186ec9b7fd8c1be2dc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-da3951edd3c067eb7bc6364e65cd3c28.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-36b8936c38972af9d7874a2416d73f91.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9b4385e2bbae30872b51b25a5ff9ad95.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9194cf58ba8f617b42c5e6436bb8b20f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-16a4ac62ce8ba497822f4f90331bb613.jpg" align="middle">
</details>




<h2 id="Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training"><a href="#Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training" class="headerlink" title="Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training"></a>Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training</h2><p><strong>Authors:Shijian Wang, Linxin Song, Jieyu Zhang, Ryotaro Shimizu, Ao Luo, Li Yao, Cunjian Chen, Julian McAuley, Hanqian Wu</strong></p>
<p>Current multimodal language models (MLMs) evaluation and training approaches overlook the influence of instruction format, presenting an elephant-in-the-room problem. Previous research deals with this problem by manually crafting instructions, failing to yield significant insights due to limitations in diversity and scalability. In this work, we propose a programmatic instruction template generator capable of producing over 39B unique template combinations by filling randomly sampled positional synonyms into weighted sampled meta templates, enabling us to comprehensively examine the MLMâ€™s performance across diverse instruction templates. Our experiments across eight common MLMs on five benchmark datasets reveal that MLMs have high template sensitivities with at most 29% performance gaps between different templates. We further augment the instruction tuning dataset of LLaVA-1.5 with our template generator and perform instruction tuning on LLaVA-1.5-7B and LLaVA-1.5-13B. Models tuned on our augmented dataset achieve the best overall performance when compared with the same scale MLMs tuned on at most 75 times the scale of our augmented dataset, highlighting the importance of instruction templates in MLM training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a> . </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰è¯„ä¼°å’Œè®­ç»ƒæ–¹æ³•å¿½è§†äº†æŒ‡ä»¤æ ¼å¼çš„å½±å“ï¼Œè¿™å°±åƒä¸€ä¸ªæˆ¿é—´é‡Œçš„å¤§è±¡é—®é¢˜ã€‚ä¹‹å‰çš„ç ”ç©¶é€šè¿‡äººå·¥æ„å»ºæŒ‡ä»¤æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºå¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§çš„é™åˆ¶ï¼Œæœªèƒ½äº§ç”Ÿæ˜¾è‘—çš„è§è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå™¨ï¼Œé€šè¿‡å‘åŠ æƒé‡‡æ ·çš„å…ƒæ¨¡æ¿ä¸­å¡«å……éšæœºé‡‡æ ·çš„ä½ç½®åŒä¹‰è¯ï¼Œèƒ½å¤Ÿäº§ç”Ÿè¶…è¿‡3 ç»“æ®å°†æ–‡åŒå†…å®¹çš„å…³é”®æ•°æ®æœ‰æ‰€ä¸åŒäº§ç”Ÿäº†å¤§çº¦æ¯ç§å…³é”®è¯ç‰¹å¾æ ¼å¼æˆ‘ä»¬æ—¨åœ¨è§£å†³çš„ç­–ç•¥çš„é—®é¢˜æ˜¯ç”¨å…³é”®è¯ä¿¡æ¯çš„ç®€æ´çš„è¯­è¨€å›ç­”å…¶ä¸­å¼•å…¥æ•°æ®å­—æ®µå¯ä»¥æ ¹æ®å›ç­”å’Œé—®é¢˜è¿›è¡Œæœ‰æ•ˆçµæ´»ã€‚é‡‡ç”¨è¿™ä¸€ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå…¨é¢è€ƒå¯ŸMLMåœ¨ä¸åŒæŒ‡ä»¤æ¨¡æ¿ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¯¹å…«ä¸ªå¸¸è§çš„MLMè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMLMå¯¹æ¨¡æ¿çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä¸åŒæ¨¡æ¿ä¹‹é—´æ€§èƒ½å·®è·æœ€å¤§å¯è¾¾29%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿ç”Ÿæˆå™¨æ‰©å……äº†LLaVA-1.5çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¹¶åœ¨LLaVA-1.5-7Bå’ŒLLaVA-1.5-13Bä¸Šè¿›è¡Œäº†æŒ‡ä»¤è°ƒæ•´ã€‚ä¸æˆ‘ä»¬å¢å¼ºçš„æ•°æ®é›†ç›¸æ¯”è°ƒæ ¡çš„æ¨¡å‹çš„è¡¨ç°ä¸å…¶ä»–å¤§å‹è®­ç»ƒè¯­æ–™åº“çš„åŒç±»è§„æ¨¡MLMè¡¨ç°æ›´ä¼˜åœ¨æœ€åæƒ…å†µä¸‹å¢å¼ºäº†è®­ç»ƒæ•°æ®é›†ä¸‰å€æ•ˆæœæ¯”è¾ƒæ˜¾ç¤ºæœ¬é¡¹ç ”ç©¶çš„è®­ç»ƒè¯­æ–™åº“çš„é‡è¦æ€§ä¸è¨€è€Œå–»æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ï¼Œåœ°å€æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters%E3%80%82">https://github.com/shijian2001/TemplateMattersã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08307v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨è¯„ä¼°å’Œè®­ç»ƒè¿‡ç¨‹ä¸­å¿½è§†äº†æŒ‡ä»¤æ ¼å¼çš„å½±å“ï¼Œä¸ºæ­¤æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå™¨ã€‚è¯¥ç”Ÿæˆå™¨é€šè¿‡éšæœºå¡«å……åŠ æƒé‡‡æ ·å…ƒæ¨¡æ¿ä¸­çš„ä½ç½®åŒä¹‰è¯ï¼Œèƒ½å¤Ÿäº§ç”Ÿè¶…è¿‡39äº¿ç§ç‹¬ç‰¹çš„æ¨¡æ¿ç»„åˆï¼Œä»è€Œå…¨é¢è€ƒå¯ŸMLMåœ¨ä¸åŒæŒ‡ä»¤æ¨¡æ¿ä¸‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLMså¯¹æ¨¡æ¿çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä¸åŒæ¨¡æ¿é—´æ€§èƒ½å·®å¼‚æœ€å¤§å¯è¾¾29%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹LLaVA-1.5æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†è¿›è¡Œæ¨¡æ¿å¢å¼ºï¼Œå¹¶åœ¨LLaVA-1.5-7Bå’ŒLLaVA-1.5-13Bä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œæ¨¡å‹åœ¨ä¸å…¶ä»–è§„æ¨¡ç›¸åŒçš„MLMsç›¸æ¯”æ—¶å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†æŒ‡ä»¤æ¨¡æ¿åœ¨MLMè®­ç»ƒä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰è¯„ä¼°å’Œè®­ç»ƒè¿‡ç¨‹ä¸­å¿½è§†äº†æŒ‡ä»¤æ ¼å¼çš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿäº§ç”Ÿå¤§é‡ç‹¬ç‰¹çš„æ¨¡æ¿ç»„åˆã€‚</li>
<li>MLMså¯¹æ¨¡æ¿çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä¸åŒæ¨¡æ¿é—´æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>é€šè¿‡æ¨¡æ¿å¢å¼ºæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æŒ‡ä»¤æ¨¡æ¿åœ¨MLMè®­ç»ƒä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸å…¶ä»–è§„æ¨¡ç›¸åŒçš„MLMsç›¸æ¯”æ—¶å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9cf18eab5fd423cf391c7a0d944472c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3bc8c42e8cb0aed2474be1f549a2daa9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-92fd609fdd9fbcf040f463966ba2a464.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ab98d0c0816798cff3762dfe29d4bbdd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9158b85ce187a5753e0f5956ebbfa739.jpg" align="middle">
</details>




<h2 id="M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis"><a href="#M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis" class="headerlink" title="M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis"></a>M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis</h2><p><strong>Authors:Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang</strong></p>
<p>Sentiment analysis and emotion recognition are crucial for applications such as human-computer interaction and depression detection. Traditional unimodal methods often fail to capture the complexity of emotional expressions due to conflicting signals from different modalities. Current Multimodal Large Language Models (MLLMs) also face challenges in detecting subtle facial expressions and addressing a wide range of emotion-related tasks. To tackle these issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy for general-purpose MLLMs. It employs a combined approach to train models on tasks such as multimodal sentiment analysis, emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction. We also introduce the Emotion Multitask dataset (EMT), a custom dataset that supports these five tasks. Our model, Emotion Universe (EmoVerse), is built on a basic MLLM framework without modifications, yet it achieves substantial improvements across these tasks when trained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-of-the-art results in sentiment and emotion tasks. These results highlight the effectiveness of M2SE in enhancing multimodal emotion perception. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE">https://github.com/xiaoyaoxinyi/M2SE</a>. </p>
<blockquote>
<p>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨äººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ç”±äºæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡å·ç›¸äº’å†²çªï¼Œå¾€å¾€æ— æ³•æ•æ‰æƒ…ç»ªè¡¨è¾¾çš„å¤æ‚æ€§ã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ£€æµ‹ç»†å¾®é¢éƒ¨è¡¨æƒ…å’Œåº”å¯¹å„ç§æƒ…ç»ªç›¸å…³ä»»åŠ¡æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2SEï¼Œè¿™æ˜¯ä¸€ç§ä¸ºé€šç”¨MLLMsè®¾è®¡çš„å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿå’Œæƒ…ç»ªæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ã€‚å®ƒé‡‡ç”¨ç»„åˆæ–¹æ³•ï¼Œåœ¨è¯¸å¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€æƒ…æ„Ÿæ¨ç†æ¨æ–­å’Œæƒ…æ„Ÿæˆå› å¯¹æå–ç­‰ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†æƒ…æ„Ÿå¤šä»»åŠ¡æ•°æ®é›†ï¼ˆEMTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒè¿™äº”ä¸ªä»»åŠ¡çš„ä¸“ä¸šæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€œæƒ…æ„Ÿå®‡å®™â€ï¼ˆEmoVerseï¼‰å»ºç«‹åœ¨åŸºæœ¬çš„MLLMæ¡†æ¶ä¸Šï¼Œæ— éœ€ä¿®æ”¹ï¼Œä½†åœ¨ä½¿ç”¨M2SEç­–ç•¥è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰äº†æ˜¾è‘—æé«˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEmoVerseä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æƒ…æ„Ÿå’Œæƒ…ç»ªä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚è¿™äº›ç»“æœçªå‡ºäº†M2SEåœ¨æé«˜å¤šæ¨¡æ€æƒ…ç»ªæ„ŸçŸ¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaoyaoxinyi/M2SEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨äººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰é¢†åŸŸçš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ç”±äºä¸åŒæ¨¡æ€ä¿¡å·çš„å†²çªï¼Œå¾€å¾€æ— æ³•æ•æ‰æƒ…æ„Ÿè¡¨è¾¾çš„å¤æ‚æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†M2SEï¼Œå³å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿæƒ…ç»ªæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œç”¨äºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥ç­–ç•¥ç»“åˆå¤šç§ä»»åŠ¡è®­ç»ƒæ¨¡å‹ï¼Œå¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€æƒ…æ„Ÿæ¨ç†å’Œæƒ…æ„Ÿå› æœé…å¯¹æå–ç­‰ã€‚åŒæ—¶ï¼Œå¼•å…¥Emotion Multitaskæ•°æ®é›†ï¼ˆEMTï¼‰æ”¯æŒè¿™äº”ä¸ªä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºM2SEç­–ç•¥çš„æ¨¡å‹Emotion Universeï¼ˆEmoVerseï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¾¾åˆ°äº†æƒ…æ„Ÿä¸æƒ…ç»ªé¢†åŸŸçš„æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œå¦‚äººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ã€‚</li>
<li>ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•åœ¨æƒ…æ„Ÿè¡¨è¾¾æ•æ‰ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºä¸åŒæ¨¡æ€ä¿¡å·çš„å†²çªã€‚</li>
<li>M2SEç­–ç•¥è¢«æå‡ºä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«é—®é¢˜ï¼ŒåŒ…æ‹¬å¤šé˜¶æ®µå’Œå¤šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>M2SEç­–ç•¥ç»“åˆäº†å¤šç§ä»»åŠ¡ï¼Œå¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ç­‰ã€‚</li>
<li>å¼•å…¥Emotion Multitaskæ•°æ®é›†ï¼ˆEMTï¼‰ä»¥æ”¯æŒå¤šä»»åŠ¡è®­ç»ƒçš„éœ€æ±‚ã€‚</li>
<li>åŸºäºM2SEç­–ç•¥çš„æ¨¡å‹Emotion Universeï¼ˆEmoVerseï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aa73ededcaefa000e8edfeba52a89b72.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c263fb335b04bc682be5a1cfa8e6e297.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4d6b6e0f4051342c6a0898d92d999f1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32acf009fc023384915a65c05d3f43ff.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd6c980356ed97ed5bc3e5d3fac353bb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9184bb36e2f4b1291065f58d8c14d467.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-19b8821a0398494275c6700434720d2e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5c8749cd2121f800ea47ff15bb3e861f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-81ab60fd15fbdd7102b7bf0b9ceb0523.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-65f47588ea960ad6d8e96c0eaec02120.jpg" align="middle">
</details>




<h2 id="SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models"><a href="#SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models" class="headerlink" title="SAT: Spatial Aptitude Training for Multimodal Language Models"></a>SAT: Spatial Aptitude Training for Multimodal Language Models</h2><p><strong>Authors:Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko</strong></p>
<p>Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23%$ on CVBench, $8%$ on the harder BLINK benchmark, and $18%$ on VSR. When instruction-tuned on SAT, our 13B model matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data&#x2F;code is available at <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a> . </p>
<blockquote>
<p>ç©ºé—´æ„ŸçŸ¥æ˜¯æ™ºèƒ½çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚è™½ç„¶è®¸å¤šç ”ç©¶å¼ºè°ƒå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨æ¨ç†ç©ºé—´æ—¶é‡åˆ°å›°éš¾ï¼Œä½†å®ƒä»¬åªæµ‹è¯•é™æ€ç©ºé—´æ¨ç†ï¼Œä¾‹å¦‚å¯¹ç‰©ä½“çš„ç›¸å¯¹ä½ç½®è¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„éƒ¨ç½²éœ€è¦åŠ¨æ€èƒ½åŠ›ï¼Œå¦‚è§‚ç‚¹æå–å’Œç¬¬ä¸€äººç§°åŠ¨ä½œè¯†åˆ«ã€‚ä½œä¸ºæé«˜ç©ºé—´æ™ºèƒ½çš„è·¯çº¿å›¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†SATï¼ˆç©ºé—´é€‚åº”èƒ½åŠ›è®­ç»ƒï¼‰ï¼Œå®ƒè¶…è¶Šäº†é™æ€çš„ç›¸å¯¹ç‰©ä½“ä½ç½®é—®é¢˜ï¼Œæ¶µç›–äº†æ›´åŠ¨æ€çš„ä»»åŠ¡ã€‚SATåŒ…å«21.8ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠè®­ç»ƒå’Œæµ‹è¯•é›†çš„2.2ä¸‡ä¸ªåˆæˆåœºæ™¯ã€‚ä½¿ç”¨é€¼çœŸçš„ç‰©ç†å¼•æ“ç”Ÿæˆï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥ä»»æ„æ‰©å±•ï¼Œå¹¶è½»æ¾æ‰©å±•åˆ°æ–°çš„åŠ¨ä½œã€åœºæ™¯å’Œ3Dèµ„äº§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿åœ¨é™æ€é—®é¢˜ä¸Šè¡¨ç°ç›¸å¯¹è¾ƒå¥½çš„MLMä¹Ÿéš¾ä»¥å‡†ç¡®å›ç­”åŠ¨æ€ç©ºé—´é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼ŒSATæŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸ä»…æé«˜äº†SATä¸Šçš„åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿˜æé«˜äº†ç°æœ‰çœŸå®å›¾åƒç©ºé—´åŸºå‡†æµ‹è¯•ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ï¼šCVBenchä¸Šæé«˜23%ï¼Œéš¾åº¦æ›´é«˜çš„BLINKåŸºå‡†æµ‹è¯•ä¸Šæé«˜8%ï¼Œä»¥åŠVSRä¸Šæé«˜19%ã€‚å½“åœ¨SATä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´æ—¶ï¼Œæˆ‘ä»¬çš„1.3Bæ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¸å¤§å‹ä¸“æœ‰MLMï¼ˆå¦‚GPT4-Vå’ŒGemini-3-1.0ï¼‰ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/%E6%89%BE%E5%88%B0%E3%80%82">http://arijitray1993.github.io/SAT/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07755v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a></p>
<p><strong>Summary</strong><br>     ç©ºé—´æ„ŸçŸ¥æ˜¯æ™ºèƒ½çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å½“å‰å¤šæ•°ç ”ç©¶é›†ä¸­äºæµ‹è¯•å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰åœ¨é™æ€ç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œä½†ç°å®ä¸–ç•Œéœ€è¦åŠ¨æ€çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¦‚è§†è§’åˆ¤æ–­å’Œè‡ªä¸»åŠ¨ä½œè¯†åˆ«ç­‰ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºç©ºé—´èƒ½åŠ›è®­ç»ƒï¼ˆSATï¼‰ï¼Œå¹¶æ„å»ºç›¸åº”çš„æ•°æ®é›†ï¼Œç”¨äºæå‡æ¨¡å‹å¯¹åŠ¨æ€ç©ºé—´ä»»åŠ¡çš„æ™ºèƒ½æ°´å¹³ã€‚å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿å¯¹é™æ€é—®é¢˜è¡¨ç°è‰¯å¥½çš„MLMåœ¨åŠ¨æ€ç©ºé—´é—®é¢˜ä¸Šä»æ˜¾ä¸è¶³ï¼Œè€Œé€šè¿‡SATè®­ç»ƒçš„æ•°æ®èƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨å¤šç§ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´æ„ŸçŸ¥æ˜¯æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œç°å®ä¸–ç•Œä¸­éœ€è¦åŠ¨æ€çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨é™æ€ç©ºé—´æ¨ç†ï¼Œè€ŒçœŸå®åœºæ™¯éœ€è¦æ›´å¤æ‚çš„åŠ¨æ€ç©ºé—´ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥Spatial Aptitude Training (SAT)ä½œä¸ºæå‡ç©ºé—´æ™ºèƒ½çš„è·¯çº¿å›¾ã€‚</li>
<li>SATæ•°æ®é›†åŒ…å«21.8ä¸‡é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œå¯ç”¨äºè®­ç»ƒæ¨¡å‹è¿›è¡ŒåŠ¨æ€ç©ºé—´ä»»åŠ¡ã€‚</li>
<li>SATæ•°æ®é›†ä½¿ç”¨é€¼çœŸçš„ç‰©ç†å¼•æ“ç”Ÿæˆï¼Œå¯ä»»æ„æ‰©å±•ï¼Œæ˜“äºæ·»åŠ æ–°åŠ¨ä½œã€åœºæ™¯å’Œ3Dèµ„äº§ã€‚</li>
<li>MLMåœ¨é™æ€é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŠ¨æ€ç©ºé—´é—®é¢˜ä¸Šä»æœ‰å›°éš¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4726f5787390ded5869518fbae49fa4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d1ac83f59cc86700f58e482ccc874082.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8ee4e23739632ca4492284e948ea332.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db09dd969f25a2ed0382bcf5c8ce13cc.jpg" align="middle">
</details>




<h2 id="Scaling-Sequential-Recommendation-Models-with-Transformers"><a href="#Scaling-Sequential-Recommendation-Models-with-Transformers" class="headerlink" title="Scaling Sequential Recommendation Models with Transformers"></a>Scaling Sequential Recommendation Models with Transformers</h2><p><strong>Authors:Pablo Zivic, Hernan Vazquez, Jorge Sanchez</strong></p>
<p>Modeling user preferences has been mainly addressed by looking at usersâ€™ interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation.   The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models.   Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation.   We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application.   We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency.   We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.   Code and models can be found at <a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt">https://github.com/mercadolibre/srt</a> </p>
<blockquote>
<p>é€šè¿‡æŸ¥çœ‹ç”¨æˆ·ä¸ç³»ç»Ÿä¸­å¯ç”¨å…ƒç´ ä¹‹é—´çš„äº¤äº’å†å²æ¥ä¸»è¦è§£å†³ç”¨æˆ·åå¥½å»ºæ¨¡çš„é—®é¢˜ã€‚æ ¹æ®å†å²æ•°æ®ä¸ºä¸ªä½“é‡èº«å®šåˆ¶å†…å®¹æ˜¯é¡ºåºæ¨èçš„ä¸»è¦ç›®æ ‡ã€‚é—®é¢˜çš„æ€§è´¨ä»¥åŠåœ¨å„ä¸ªé¢†åŸŸä¸­è§‚å¯Ÿåˆ°çš„è‰¯å¥½æ€§èƒ½ï¼Œæ¿€å‘äº†ä½¿ç”¨å˜å‹å™¨æ¶æ„çš„åŠ¨æœºã€‚å½“æ¨¡å‹å‚æ•°æ•°é‡å¢åŠ æ—¶ï¼Œè¯¥æ¶æ„åœ¨åˆ©ç”¨è¶Šæ¥è¶Šå¤šçš„è®­ç»ƒæ•°æ®æ–¹é¢è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚è¿™ç§æ‰©å±•è¡Œä¸ºå¼•èµ·äº†æå¤§çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒä¸ºè®¾è®¡æ›´å¤§çš„æ¨¡å‹ä»¥åŠè®­ç»ƒæä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚ä»è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„æ‰©å±•å®šå¾‹ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æ¢ç´¢äº†é¡ºåºæ¨èçš„ç±»ä¼¼åŸåˆ™ã€‚æˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„äºšé©¬é€Šäº§å“æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨å…¶ä»–ç ”ç©¶ä¸­ä»…å¾—åˆ°éƒ¨åˆ†æ¢ç´¢ï¼Œå¹¶æ­ç¤ºäº†ä¸è¯­è¨€æ¨¡å‹ä¸­å‘ç°çš„ç±»ä¼¼çš„æ‰©å±•è¡Œä¸ºã€‚æœ€ä¼˜è®¡ç®—è®­ç»ƒæ˜¯å¯èƒ½çš„ï¼Œä½†éœ€è¦ä»”ç»†åˆ†æç‰¹å®šäºåº”ç”¨ç¨‹åºçš„è®¡ç®—æ€§èƒ½æƒè¡¡ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡åœ¨å°ä»»åŠ¡ç‰¹å®šé¢†åŸŸä¸Šå¯¹è¾ƒå¤§çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ€§èƒ½æ‰©å±•å¯ä»¥åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’Œå‘ç°ä¸ºåœ¨é«˜ç»´åå¥½ç©ºé—´ä¸­å®é™…è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²æä¾›äº†æˆ˜ç•¥è·¯çº¿å›¾ï¼Œæœ‰åŠ©äºæé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬æ–‡èƒ½å¤Ÿæ¶èµ·æ½œåœ¨å˜å‹å™¨ä¸å¤æ‚é«˜ç»´é¡ºåºæ¨èä¹‹é—´çš„æ¡¥æ¢åœ¨ç°å®ä¸–ç•Œçš„æ¨èç³»ç»Ÿä¸­ï¼Œæ¬¢è¿è®¿é—®æˆ‘ä»¬çš„GitHubä»£ç ä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt%E3%80%82">https://github.com/mercadolibre/srtã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07585v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦é€šè¿‡æ¢è®¨ç”¨æˆ·ä¸ç³»ç»Ÿå†…ä¸åŒå…ƒç´ çš„äº¤äº’å†å²æ¥å»ºæ¨¡ç”¨æˆ·åå¥½ã€‚åŸºäºå†å²æ•°æ®çš„ä¸ªæ€§åŒ–å†…å®¹æ¨èæ˜¯é¡ºåºæ¨èçš„ä¸»è¦ç›®æ ‡ã€‚æœ¬æ–‡å—åˆ°transformeræ¶æ„åœ¨åˆ©ç”¨å¤§é‡è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°æ–¹é¢çš„è‰¯å¥½æ€§èƒ½çš„å¯å‘ï¼Œè¯¥æ¶æ„åœ¨å¤„ç†å¤§å‹æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„ç¼©æ”¾è¡Œä¸ºï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œæ¢ç´¢é¡ºåºæ¨èçš„ç±»ä¼¼åŸåˆ™ã€‚æˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„äºšé©¬é€Šäº§å“æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨å…¶ä»–ç ”ç©¶ä¸­ä»…å¾—åˆ°éƒ¨åˆ†æ¢ç´¢ï¼Œæ­ç¤ºä¸è¯­è¨€æ¨¡å‹ç›¸ä¼¼çš„ç¼©æ”¾è¡Œä¸ºã€‚è®¡ç®—æœ€ä¼˜è®­ç»ƒæ˜¯å¯èƒ½çš„ï¼Œä½†éœ€è¦ä»”ç»†åˆ†æç‰¹å®šäºåº”ç”¨ç¨‹åºçš„è®¡ç®—æ€§èƒ½æƒè¡¡ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ€§èƒ½ç¼©æ”¾å¯ä»¥é€šè¿‡å¯¹è¾ƒå°ä»»åŠ¡ç‰¹å®šåŸŸè¿›è¡Œå¾®è°ƒçš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹è½¬åŒ–ä¸ºä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’Œå‘ç°ä¸ºé«˜ç»´åå¥½ç©ºé—´ä¸­æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„æˆ˜ç•¥è·¯çº¿å›¾æä¾›äº†æŒ‡å¯¼ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡èƒ½å¤Ÿå¼¥è¡¥Transformerçš„æ½œåŠ›ä¸ç°å®ä¸–ç•Œä¸­é«˜ç»´é¡ºåºæ¨èå†…åœ¨å¤æ‚æ€§ä¹‹é—´çš„é¸¿æ²Ÿã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt%E5%8D%BB%E5%8F%AF%E4%BB%A5%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mercadolibre/srtæ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>è®ºæ–‡èšç„¦äºé€šè¿‡ç”¨æˆ·ä¸ç³»ç»Ÿå…ƒç´ çš„äº¤äº’å†å²å»ºæ¨¡ç”¨æˆ·åå¥½ï¼Œè¿™æ˜¯é¡ºåºæ¨èçš„ä¸»è¦ç›®æ ‡ã€‚</li>
<li>Transformeræ¶æ„åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡å‹æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ç¼©æ”¾è¡Œä¸ºï¼Œå¯¹è®¾è®¡æ›´å¤§æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</li>
<li>è®ºæ–‡å€Ÿé‰´è¯­è¨€æ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œæ¢ç´¢é¡ºåºæ¨èçš„ç±»ä¼¼åŸåˆ™ã€‚</li>
<li>ä½¿ç”¨å®Œæ•´çš„äºšé©¬é€Šäº§å“æ•°æ®é›†ï¼Œæ­ç¤ºä¸å…¶ä»–ç ”ç©¶ä¸­ä¸åŒçš„ç¼©æ”¾è¡Œä¸ºã€‚</li>
<li>è®¡ç®—æœ€ä¼˜è®­ç»ƒéœ€è¦åˆ†æç‰¹å®šäºåº”ç”¨ç¨‹åºçš„è®¡ç®—æ€§èƒ½æƒè¡¡ã€‚</li>
<li>è®ºæ–‡å±•ç¤ºäº†æ€§èƒ½ç¼©æ”¾ä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„è”ç³»ï¼Œé€šè¿‡å¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ä»¥é€‚åº”å°å‹ç‰¹å®šä»»åŠ¡é¢†åŸŸã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c1432e8c0cfc30fe4ce86a58f601e82f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f92d5d9f7e8fd994ab15fcdfbd147e1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3c5c840c67efc37f67360b32c677b1d2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-496f2027a50d34beb6c7b71d6e2dcca0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7b52962e1da932f53ab0187435c70ac4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3d57eea9b2995fc84c92ca1a92d6276d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8b5b0ccde95d9e8ffabc00bb100fa636.jpg" align="middle">
</details>




<h2 id="Causal-World-Representation-in-the-GPT-Model"><a href="#Causal-World-Representation-in-the-GPT-Model" class="headerlink" title="Causal World Representation in the GPT Model"></a>Causal World Representation in the GPT Model</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules. We find that the GPT model tends to generate next moves that adhere to the game rules for sequences for which the attention mechanism encodes a causal structure with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure. </p>
<blockquote>
<p>ç”Ÿæˆé¢„è®­ç»ƒTransformerï¼ˆGPTï¼‰æ¨¡å‹æ˜¯å¦ä»…ç»è¿‡è®­ç»ƒä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œè¿˜æ˜¯å®ƒä»¬æ˜¯å¦ä»ä¸–ç•Œæ¨¡å‹ä¸­éšå¼å­¦ä¹ ç”Ÿæˆåºåˆ—çš„æ–¹å¼ï¼Œå³ä¸€ä¸ªä»¤ç‰Œæ¥ä¸€ä¸ªä»¤ç‰Œåœ°ç”Ÿæˆåºåˆ—ï¼Ÿæˆ‘ä»¬é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œå¹¶æ®æ­¤æå‡ºä¸€ä¸ªå› æœä¸–ç•Œæ¨¡å‹æ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºGPTæ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œå¯ä»¥ç”¨äºæ‰§è¡Œå†…éƒ¨åˆ†å¸ƒåºåˆ—çš„é›¶åŸºç¡€å› æœç»“æ„å­¦ä¹ ã€‚å®è¯è¯„ä¼°æ˜¯åœ¨å—æ§çš„åˆæˆç¯å¢ƒä¸­è¿›è¡Œçš„ï¼Œä½¿ç”¨Othelloæ¸¸æˆçš„è®¾ç½®å’Œè§„åˆ™ã€‚GPTç»è¿‡åœ¨ç°å®ä¸–ç•Œæ¸¸æˆä¸­é¢„è®­ç»ƒä»¥èµ¢å¾—æ¯”èµ›ä¸ºç›®æ ‡ï¼Œåœ¨ä»…éµå¾ªæ¸¸æˆè§„åˆ™çš„äººå·¥æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°GPTæ¨¡å‹å€¾å‘äºç”Ÿæˆç¬¦åˆæ¸¸æˆè§„åˆ™çš„ä¸‹ä¸€æ­¥åŠ¨ä½œåºåˆ—ï¼Œå¯¹äºæ³¨æ„åŠ›æœºåˆ¶ç¼–ç å…·æœ‰é«˜ç½®ä¿¡åº¦çš„å› æœç»“æ„åºåˆ—ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨GPTæ¨¡å‹ç”Ÿæˆçš„è¡ŒåŠ¨ä¸ç¬¦åˆæ¸¸æˆè§„åˆ™çš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿæ— æ³•æ•æ‰åˆ°ä»»ä½•å› æœç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v1">PDF</a> NeurIPS 2024 Workshop on Causality and Large Models (CaLM)</p>
<p><strong>Summary</strong></p>
<p>GPTæ¨¡å‹æ˜¯å¦ä»…é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼Œè¿˜æ˜¯å®ƒä»¬ä¼šéšå¼åœ°å­¦ä¹ ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡è¯¥æ¨¡å‹é€ä¸ªç”Ÿæˆåºåˆ—ï¼Ÿæœ¬æ–‡é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šæ¥æ¢è®¨è¿™ä¸€é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºGPTæ¨¡å‹åœ¨æ¨ç†æ—¶å¯ç”¨äºé›¶å°„å‡»å› æœç»“æ„å­¦ä¹ ï¼Œé€‚ç”¨äºå†…éƒ¨åºåˆ—åˆ†å¸ƒã€‚åœ¨æ§åˆ¶çš„åˆæˆç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œä½¿ç”¨Othelloæ¸¸æˆçš„è®¾ç½®å’Œè§„åˆ™ã€‚ç»è¿‡åœ¨çœŸå®ä¸–ç•Œæ¸¸æˆä¸­è®­ç»ƒçš„GPTæ¨¡å‹è¢«æµ‹è¯•åœ¨ä»…éµå¾ªæ¸¸æˆè§„åˆ™åˆæˆæ•°æ®ä¸Šã€‚æˆ‘ä»¬å‘ç°ï¼ŒGPTæ¨¡å‹å€¾å‘äºç”Ÿæˆéµå¾ªæ¸¸æˆè§„åˆ™çš„ä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œå¯¹äºæ³¨æ„åŠ›æœºåˆ¶ç¼–ç å…·æœ‰å¾ˆé«˜ç½®ä¿¡åº¦çš„å› æœç»“æ„åºåˆ—è€Œè¨€å°¤å…¶å¦‚æ­¤ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨GPTæ¨¡å‹ç”Ÿæˆä¸éµå¾ªæ¸¸æˆè§„åˆ™çš„è¡ŒåŠ¨çš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿæ— æ³•æ•æ‰åˆ°ä»»ä½•å› æœå…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTæ¨¡å‹ä¸ä»…ä»…æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œè€Œæ˜¯éšå¼åœ°å­¦ä¹ ä¸€ç§ä¸–ç•Œæ¨¡å‹æ¥ç”Ÿæˆåºåˆ—ã€‚</li>
<li>é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œæå‡ºäº†å› æœä¸–ç•Œæ¨¡å‹çš„è§‚å¿µã€‚</li>
<li>GPTæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ç”¨äºé›¶å°„å‡»å› æœç»“æ„å­¦ä¹ ï¼Œé€‚ç”¨äºå†…éƒ¨åºåˆ—åˆ†å¸ƒã€‚</li>
<li>åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„Othelloæ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯ç ”ç©¶æ¥éªŒè¯è¿™ä¸€ç†è®ºã€‚</li>
<li>GPTæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ¸¸æˆä¸­è®­ç»ƒåï¼Œèƒ½å¤Ÿåœ¨åˆæˆæ•°æ®ä¸Šç”Ÿæˆéµå¾ªæ¸¸æˆè§„åˆ™çš„ä¸‹ä¸€æ­¥åŠ¨ä½œã€‚</li>
<li>GPTæ¨¡å‹ç”Ÿæˆçš„åŠ¨ä½œæ˜¯å¦éµå¾ªè§„åˆ™ä¸å…¶èƒ½å¦æ•æ‰åˆ°åºåˆ—çš„å› æœå…³ç³»ä¹‹é—´æœ‰å¯†åˆ‡å…³ç³»ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd533512299dae43bee0963fb6015772.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg" align="middle">
</details>




<h2 id="HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass"><a href="#HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass" class="headerlink" title="HARP: Hesitation-Aware Reframing in Transformer Inference Pass"></a>HARP: Hesitation-Aware Reframing in Transformer Inference Pass</h2><p><strong>Authors:Romain StoraÃ¯, Seung-won Hwang</strong></p>
<p>This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to â€œoff-the-shelfâ€ Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡è§£å†³æ¨ç†æ­¥éª¤ä¸­å¯å˜çš„è®¡ç®—éœ€æ±‚æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶ä¸­ä¸€äº›æ ‡è®°ç¬¦éœ€è¦æ¯”å…¶ä»–æ ‡è®°ç¬¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚æˆ‘ä»¬æå‡ºäº†HARPï¼Œè¿™æ˜¯å¯¹â€œç°æˆçš„â€Transformerå‰å‘ä¼ æ’­çš„ç®€å•ä¿®æ”¹ã€‚å®ƒå€Ÿé‰´äº†å†³ç­–åˆ¶å®šä¸­çš„çŠ¹è±«å’Œæ¡†æ¶æ•ˆåº”ï¼Œå½“æ¨¡å‹åœ¨ç”Ÿæˆæ ‡è®°ç¬¦æ—¶é‡åˆ°ä¸ç¡®å®šæ€§æ—¶ï¼ŒHARPä¼šé€‰æ‹©æ€§åœ°åº”ç”¨é¢å¤–çš„è®¡ç®—ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨éš¾ä»¥å†³ç­–çš„ç‚¹ä¸Šæš‚åœå¹¶é‡æ–°è°ƒæ•´è¾“å…¥è§†è§’æ¥æ¨¡ä»¿äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒHARPå…·æœ‰æ¨¡å‹æ— å…³æ€§ã€æ— éœ€è®­ç»ƒä¸”æ˜“äºå®ç°çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸Šå…¨é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†æ€§èƒ½æé«˜äº†é«˜è¾¾+5.16%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHARPåœ¨ä¿æŒæ¨ç†æ—¶é—´æ˜¯å…‰æŸæœç´¢çš„ä¸¤å€çš„åŒæ—¶å®ç°äº†è¿™äº›æ”¶ç›Šã€‚ç®€å•è€Œæ•ˆæœæ˜¾è‘—ï¼ŒHARPä¸ºåœ¨æœ€å°è®¡ç®—å½±å“çš„æƒ…å†µä¸‹æé«˜åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡è§£å†³æ¨ç†æ­¥éª¤ä¸­å¯å˜çš„è®¡ç®—éœ€æ±‚æ¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚é’ˆå¯¹æŸäº›ä»¤ç‰Œéœ€è¦æ¯”å…¶å®ƒä»¤ç‰Œæ›´å¤šçš„è®¡ç®—èµ„æºçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHARPçš„ç®€å•ä¿®æ”¹æ–¹æ¡ˆï¼Œç”¨äºæ”¹è¿›â€œå³ä¹°å³ç”¨â€Transformerå‰å‘ä¼ é€’ã€‚HARPå€Ÿé‰´äº†å†³ç­–åˆ¶å®šä¸­çš„çŠ¹è±«å’Œæ¡†æ¶æ•ˆåº”ï¼Œåœ¨æ¨¡å‹åœ¨ä»¤ç‰Œç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°ä¸ç¡®å®šæ€§æ—¶é€‰æ‹©æ€§åœ°åº”ç”¨é¢å¤–çš„è®¡ç®—ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡ä»¿äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œåœ¨å›°éš¾çš„å†³ç­–ç‚¹ä¸Šæš‚åœå¹¶é‡æ–°æ„å»ºè¾“å…¥ä»¥è·å¾—ä¸åŒçš„è§†è§’ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒHARPå…·æœ‰æ¨¡å‹æ— å…³æ€§ã€æ— éœ€è®­ç»ƒä¸”æ˜“äºå®ç°çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹å¤§å°ä¸Šå…¨é¢è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æ€§èƒ½æ”¹è¿›å¯è¾¾+5.16%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHARPåœ¨ä¿æŒæ¨ç†é€Ÿåº¦æ¯”æŸæœç´¢å¿«ä¸¤å€çš„åŒæ—¶å®ç°äº†è¿™äº›æ”¶ç›Šã€‚HARPç®€å•è€Œæ•ˆæœæ˜¾è‘—ï¼Œä¸ºå¢å¼ºåŸºäºTransformerçš„è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œä¸”è®¡ç®—å½±å“è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HARPæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­é‡åˆ°çš„è®¡ç®—éœ€æ±‚é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æŸäº›éœ€è¦æ›´å¤šè®¡ç®—èµ„æºçš„ä»¤ç‰Œã€‚</li>
<li>HARPå€Ÿé‰´äº†çŠ¹è±«å’Œæ¡†æ¶æ•ˆåº”ï¼Œåœ¨æ¨¡å‹é‡åˆ°ä¸ç¡®å®šæ€§æ—¶å¢åŠ é¢å¤–çš„è®¡ç®—ã€‚</li>
<li>HARPæ¨¡ä»¿äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œåœ¨å†³ç­–è¿‡ç¨‹ä¸­æš‚åœå¹¶é‡æ–°æ„å»ºè¾“å…¥ã€‚</li>
<li>HARPå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€è®­ç»ƒï¼Œæ˜“äºå®æ–½ã€‚</li>
<li>åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹å¤§å°ä¸Šï¼ŒHARPçš„æ€§èƒ½æ”¹è¿›å¯è¾¾+5.16%ã€‚</li>
<li>HARPä¿æŒäº†æ¯”æŸæœç´¢æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶å®ç°äº†æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bd54425be6c50fa98ef8656d707413c8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-963b17fc1c818a02111506456b683518.jpg" align="middle">
</details>




<h2 id="A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis"><a href="#A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis" class="headerlink" title="A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis"></a>A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis</h2><p><strong>Authors:Nimisha Ghosh, Daniele Santoni, Indrajit Saha, Giovanni Felici</strong></p>
<p>In recent times, Transformer-based language models are making quite an impact in the field of natural language processing. As relevant parallels can be drawn between biological sequences and natural languages, the models used in NLP can be easily extended and adapted for various applications in bioinformatics. In this regard, this paper introduces the major developments of Transformer-based models in the recent past in the context of nucleotide sequences. We have reviewed and analysed a large number of application-based papers on this subject, giving evidence of the main characterizing features and to different approaches that may be adopted to customize such powerful computational machines. We have also provided a structured description of the functioning of Transformers, that may enable even first time users to grab the essence of such complex architectures. We believe this review will help the scientific community in understanding the various applications of Transformer-based language models to nucleotide sequences. This work will motivate the readers to build on these methodologies to tackle also various other problems in the field of bioinformatics. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸäº§ç”Ÿäº†å·¨å¤§å½±å“ã€‚ç”±äºç”Ÿç‰©åºåˆ—å’Œè‡ªç„¶è¯­è¨€ä¹‹é—´å­˜åœ¨ç›¸ä¼¼æ€§ï¼ŒNLPä¸­ä½¿ç”¨çš„æ¨¡å‹å¯ä»¥è½»æ¾åœ°æ‰©å±•å¹¶é€‚åº”ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å„ç§åº”ç”¨ã€‚åœ¨è¿™æ–¹é¢ï¼Œæœ¬æ–‡ä»‹ç»äº†æœ€è¿‘è¿‡å»åŸºäºTransformerçš„æ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—æ–¹é¢çš„ä¸»è¦å‘å±•ã€‚æˆ‘ä»¬å›é¡¾å¹¶åˆ†æäº†å¤§é‡å…³äºæ­¤ä¸»é¢˜çš„åº”ç”¨è®ºæ–‡ï¼Œè¯æ˜äº†ä¸»è¦ç‰¹å¾ä»¥åŠå¯èƒ½é‡‡ç”¨çš„ä¸åŒæ–¹æ³•æ¥å®šåˆ¶è¿™äº›å¼ºå¤§çš„è®¡ç®—æœºã€‚æˆ‘ä»¬è¿˜æä¾›äº†Transformerè¿è¡Œæœºåˆ¶çš„ç»“æ„åŒ–æè¿°ï¼Œç”šè‡³å¯ä»¥è®©é¦–æ¬¡æ¥è§¦çš„ç”¨æˆ·ä¹Ÿèƒ½ç†è§£è¿™äº›å¤æ‚æ¶æ„çš„æ ¸å¿ƒã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™æ¬¡å›é¡¾å°†æœ‰åŠ©äºç§‘å­¦ç•Œäº†è§£åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—æ–¹é¢çš„å„ç§åº”ç”¨ã€‚è¿™é¡¹å·¥ä½œå°†æ¿€åŠ±è¯»è€…åŸºäºè¿™äº›æ–¹æ³•æ¥è§£å†³ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸçš„å„ç§é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07201v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸTransformeræ¨¡å‹åœ¨å¤„ç†æ ¸è‹·é…¸åºåˆ—æ–¹é¢çš„ä¸»è¦è¿›å±•ï¼Œåˆ†æäº†ç›¸å…³åº”ç”¨è®ºæ–‡ï¼Œå¹¶è¯¦ç»†æè¿°äº†Transformerçš„å·¥ä½œåŸç†ã€‚æœ¬æ–‡æœ‰åŠ©äºç§‘å­¦ç•Œäº†è§£Transformeræ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—å¤„ç†æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶é¼“åŠ±åŸºäºè¿™äº›æ–¹æ³•è§£å†³ç”Ÿç‰©ä¿¡æ¯å­¦çš„å…¶ä»–é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>Transformeræ¨¡å‹å¯åº”ç”¨äºç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸã€‚</li>
<li>Transformeræ¨¡å‹åœ¨å¤„ç†æ ¸è‹·é…¸åºåˆ—æ–¹é¢å±•ç°å‡ºé‡è¦è¿›å±•ã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†å¤§é‡å…³äºTransformeræ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—å¤„ç†æ–¹é¢çš„åº”ç”¨è®ºæ–‡ã€‚</li>
<li>æœ¬æ–‡è¯¦ç»†æè¿°äº†Transformerçš„å·¥ä½œåŸç†ã€‚</li>
<li>æœ¬æ–‡æœ‰åŠ©äºç§‘å­¦ç•Œäº†è§£Transformeræ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—å¤„ç†æ–¹é¢çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-479dbd5c07313b7fbbcd5d183ab4d03c.jpg" align="middle">
</details>




<h2 id="ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models"><a href="#ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models" class="headerlink" title="ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models"></a>ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models</h2><p><strong>Authors:Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, silvio savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu</strong></p>
<p>With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image-based queries. Existing practices rely on powerful but costly large language models (LLMs) or multimodal language models (MLMs) to produce instruction data. These are often prone to hallucinations, licensing issues and the generation process is often hard to scale and interpret. In this work, we present a programmatic approach that employs scene graphs as symbolic representations of images and human-written programs to systematically synthesize vision-centric instruction data. Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy. By implementing a suite of 24 single-image, 14 multi-image instruction generators, and a scene graph generation pipeline, we build a scalable, cost-effective system: ProVision which produces diverse question-answer pairs concerning objects, attributes, relations, depth, etc., for any given image. Applied to Visual Genome and DataComp datasets, we generate over 10 million instruction data points, ProVision-10M, and leverage them in both pretraining and instruction tuning stages of MLMs. When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D split of CVBench, along with a 3% increase in performance on QBench2, RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8% improvement on Mantis-Eval. Incorporation of our data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€åº”ç”¨çš„å…´èµ·ï¼ŒæŒ‡ä»¤æ•°æ®å¯¹äºè®­ç»ƒèƒ½å¤Ÿç†è§£å¤æ‚å›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰å®è·µä¾èµ–äºå¼ºå¤§ä½†æˆæœ¬é«˜æ˜‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰æ¥ç”ŸæˆæŒ‡ä»¤æ•°æ®ã€‚è¿™äº›æ–¹æ³•å¾€å¾€å®¹æ˜“å‡ºç°å¹»è§‰ã€è®¸å¯é—®é¢˜ï¼Œä¸”ç”Ÿæˆè¿‡ç¨‹å¾€å¾€éš¾ä»¥æ‰©å±•å’Œè§£é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨åœºæ™¯å›¾ä½œä¸ºå›¾åƒçš„è±¡å¾è¡¨ç¤ºå’Œäººç±»ç¼–å†™çš„ç¨‹åºæ¥ç³»ç»Ÿåœ°åˆæˆä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼Œåœ¨ä¿æŒäº‹å®å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆæ‰©å±•ã€‚é€šè¿‡å®æ–½24ä¸ªå•å›¾åƒã€14ä¸ªå¤šå›¾åƒæŒ‡ä»¤ç”Ÿæˆå™¨ä»¥åŠåœºæ™¯å›¾ç”Ÿæˆç®¡é“ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ã€æˆæœ¬æ•ˆç›Šé«˜çš„ç³»ç»Ÿï¼šProVisionã€‚è¯¥ç³»ç»Ÿå¯é’ˆå¯¹ç»™å®šå›¾åƒç”Ÿæˆæ¶‰åŠå¯¹è±¡ã€å±æ€§ã€å…³ç³»ã€æ·±åº¦ç­‰æ–¹é¢çš„å¤šæ ·åŒ–é—®ç­”å¯¹ã€‚åœ¨Visual Genomeå’ŒDataCompæ•°æ®é›†ä¸Šåº”ç”¨æ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡1000ä¸‡çš„æŒ‡ä»¤æ•°æ®ç‚¹ï¼Œå³ProVision-10Mï¼Œå¹¶å°†å…¶ç”¨äºMLMçš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µé‡‡ç”¨æˆ‘ä»¬çš„å•å›¾åƒæŒ‡ä»¤æ•°æ®ï¼Œåœ¨CVBenchçš„2Dåˆ†å‰²å’Œ3Dåˆ†å‰²ä¸Šåˆ†åˆ«æé«˜äº†7%å’Œ8%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨QBench2ã€RealWorldQAå’ŒMMMUä¸Šçš„æ€§èƒ½ä¹Ÿæé«˜äº†3%ã€‚æˆ‘ä»¬çš„å¤šå›¾åƒæŒ‡ä»¤æ•°æ®åœ¨Mantis-Evalä¸Šæé«˜äº†8%ã€‚åœ¨xGen-MM-4Bçš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½çº³å…¥æˆ‘ä»¬çš„æ•°æ®ï¼Œå¹³å‡åœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†1.6%çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07012v2">PDF</a> code: <a target="_blank" rel="noopener" href="https://github.com/JieyuZ2/ProVision">https://github.com/JieyuZ2/ProVision</a> dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Salesforce/ProVision-10M">https://huggingface.co/datasets/Salesforce/ProVision-10M</a></p>
<p><strong>æ‘˜è¦</strong><br>    éšç€å¤šæ¨¡æ€åº”ç”¨çš„å…´èµ·ï¼ŒæŒ‡ä»¤æ•°æ®å¯¹äºè®­ç»ƒèƒ½å¤Ÿç†è§£å¤æ‚å›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•ä¾èµ–äºå¼ºå¤§ä½†æ˜‚è´µçš„è¯­è¨€æ¨¡å‹æˆ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¥ç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œè¿™å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€æˆæƒé—®é¢˜ï¼Œä¸”ç”Ÿæˆè¿‡ç¨‹éš¾ä»¥æ‰©å±•å’Œè§£é‡Šã€‚æœ¬ç ”ç©¶é‡‡ç”¨åœºæ™¯å›¾ä½œä¸ºå›¾åƒçš„è±¡å¾æ€§è¡¨ç¤ºå’Œäººå·¥ç¼–å†™ç¨‹åºï¼Œç³»ç»Ÿæ€§åœ°åˆæˆä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤æ•°æ®ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†æ•°æ®ç”Ÿæˆçš„è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼ŒåŒæ—¶åœ¨ä¿æŒäº‹å®å‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†é«˜æ•ˆæ‰©å±•ã€‚é€šè¿‡å®æ–½ä¸€ç³»åˆ—å›¾åƒæŒ‡ä»¤ç”Ÿæˆå™¨åŠåœºæ™¯å›¾ç”Ÿæˆç®¡é“ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¯ä¼¸ç¼©ã€ç»æµçš„ProVisionç³»ç»Ÿï¼Œé’ˆå¯¹ç»™å®šçš„å›¾åƒç”Ÿæˆäº†å…³äºå¯¹è±¡ã€å±æ€§ã€å…³ç³»ç­‰çš„å¤šæ ·åŒ–é—®ç­”å¯¹ã€‚åœ¨Visual Genomeå’ŒDataCompæ•°æ®é›†ä¸Šåº”ç”¨æ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡1äº¿ä¸ªæŒ‡ä»¤æ•°æ®ç‚¹ï¼Œå³ProVision-10Mï¼Œå¹¶å°†å…¶ç”¨äºMLMsçš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µé‡‡ç”¨æˆ‘ä»¬çš„å•å›¾åƒæŒ‡ä»¤æ•°æ®ï¼Œåœ¨CVBenchçš„2Dåˆ†å‰²å’Œ3Dåˆ†å‰²ä¸Šåˆ†åˆ«æé«˜äº†7%å’Œ8%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨QBench2ã€RealWorldQAå’ŒMMMUä¸Šçš„æ€§èƒ½ä¹Ÿæé«˜äº†3%ã€‚æˆ‘ä»¬çš„å¤šå›¾åƒæŒ‡ä»¤æ•°æ®åœ¨Mantis-Evalä¸Šæé«˜äº†8%ã€‚åœ¨xGen-MM-4Bçš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µèå…¥æˆ‘ä»¬çš„æ•°æ®ï¼Œåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ”¹è¿›ç‡ä¸º1.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€å¤šæ¨¡æ€åº”ç”¨çš„æ™®åŠï¼ŒæŒ‡ä»¤æ•°æ®å¯¹è®­ç»ƒèƒ½å¤Ÿç†è§£å¤æ‚å›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹æˆ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œå­˜åœ¨å¹»è§‰ã€æˆæƒå’Œæ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åœºæ™¯å›¾å’Œäººå·¥ç¼–å†™ç¨‹åºç³»ç»Ÿæ€§åœ°åˆæˆè§†è§‰ä¸ºä¸­å¿ƒæŒ‡ä»¤æ•°æ®çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç¡®ä¿äº†æ•°æ®ç”Ÿæˆçš„è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼Œå¹¶å®ç°äº†é«˜æ•ˆæ‰©å±•å’Œä¿æŒäº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å®æ–½ä¸€ç³»åˆ—å›¾åƒæŒ‡ä»¤ç”Ÿæˆå™¨ï¼Œå»ºç«‹äº†å¯ä¼¸ç¼©çš„ProVisionç³»ç»Ÿï¼Œé’ˆå¯¹ç»™å®šå›¾åƒç”Ÿæˆå¤šæ ·åŒ–é—®ç­”å¯¹ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ProVisionç³»ç»Ÿï¼Œç”Ÿæˆè¶…è¿‡1äº¿ä¸ªæŒ‡ä»¤æ•°æ®ç‚¹ã€‚</li>
<li>è¿™äº›æŒ‡ä»¤æ•°æ®åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e8cb97cdfe610306866f0725881bbf9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-92c79b6de64dbde4592691764c5cef14.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dbabc542464f3caae8fe55262ded5040.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7fcbc119bff24db4fd8bb63ed2894e66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e5f2e25d56c2684e221ae531b2a976f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6c42593b541d8ad98e8a0d1d9645bdb5.jpg" align="middle">
</details>




<h2 id="Understanding-Factual-Recall-in-Transformers-via-Associative-Memories"><a href="#Understanding-Factual-Recall-in-Transformers-via-Associative-Memories" class="headerlink" title="Understanding Factual Recall in Transformers via Associative Memories"></a>Understanding Factual Recall in Transformers via Associative Memories</h2><p><strong>Authors:Eshaan Nichani, Jason D. Lee, Alberto Bietti</strong></p>
<p>Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº‹å®å›å¿†æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ä¹‹å‰çš„ç ”ç©¶å‘ç°ï¼Œåœ¨äº‹å®å›å¿†ä»»åŠ¡ä¸Šè®­ç»ƒçš„å˜å‹å™¨çš„ä¿¡æ¯å­˜å‚¨é‡ä¸å‚æ•°æ•°é‡æˆæ¯”ä¾‹ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æµ…å±‚å˜å‹å™¨å¯ä»¥é€šè¿‡ç»“åˆå…³è”è®°å¿†æ¥è·å¾—è¿‘ä¹æœ€ä¼˜çš„å­˜å‚¨èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œçº¿æ€§å…³è”è®°å¿†å’Œå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å…³è”è®°å¿†å­˜å‚¨èƒ½åŠ›éšç€å‚æ•°æ•°é‡çš„å¢åŠ è€Œçº¿æ€§æ‰©å±•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆæˆçš„äº‹å®å›å¿†ä»»åŠ¡ï¼Œå¹¶è¯æ˜äº†ä¸€ä¸ªå•å±‚è‡ªæ³¨æ„åŠ›æœºåˆ¶åè·Ÿä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºçš„å˜å‹å™¨ï¼Œæ¯å½“è‡ªæ³¨æ„åŠ›çš„æ€»å‚æ•°æˆ–MLPå‚æ•°æ•°é‡ï¼ˆç›´è‡³å¯¹æ•°å› ç´ ï¼‰ä¸äº‹å®æ•°é‡çº¿æ€§ç›¸å…³æ—¶ï¼Œéƒ½èƒ½åœ¨è¯¥ä»»åŠ¡ä¸Šè¾¾åˆ°ç™¾åˆ†ä¹‹ç™¾çš„å‡†ç¡®ç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œå˜å‹å™¨å¯ä»¥åœ¨ä½¿ç”¨å€¼çŸ©é˜µæˆ–åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥æœºä½œä¸ºå…³è”è®°å¿†æ¥å­˜å‚¨äº‹å®æ•°æ®é›†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬é€šè¿‡åˆ†æç®€åŒ–çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨æˆ‘ä»¬çš„äº‹å®å›å¿†ä»»åŠ¡ä¸Šçš„æ¢¯åº¦æµåŠ¨è½¨è¿¹æ¥è¡¥å……è¿™äº›è¡¨è¾¾æ€§ç»“æœï¼Œæ˜¾ç¤ºæ¨¡å‹è¡¨ç°å‡ºåºåˆ—å­¦ä¹ è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06538v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„äº‹å®è®°å¿†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è¯æ˜æµ…å±‚å˜å‹å™¨å¯é€šè¿‡ç»“åˆå…³è”è®°å¿†è¾¾åˆ°è¿‘ä¹æœ€ä¼˜çš„å­˜å‚¨èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜äº†çº¿æ€§ä¸MLPå…³è”è®°å¿†çš„å­˜å‚¨èƒ½åŠ›ä¸å‚æ•°æ•°é‡å‘ˆçº¿æ€§å…³ç³»ã€‚åœ¨åˆæˆçš„äº‹å®å›å¿†ä»»åŠ¡ä¸­ï¼Œå•å±‚è‡ªæ³¨æ„åŠ›åçš„MLPå˜å‹å™¨ï¼Œåœ¨è‡ªæ³¨æ„åŠ›å‚æ•°æˆ–MLPå‚æ•°æ•°é‡ä¸äº‹å®æ•°é‡å‘ˆçº¿æ€§å…³ç³»ï¼ˆå¯¹æ•°å› ç´ å†…ï¼‰æ—¶ï¼Œå¯è¾¾åˆ°100%å‡†ç¡®ç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œå˜å‹å™¨å¯ä»¥åœ¨ä½¿ç”¨å€¼çŸ©é˜µæˆ–MLPä½œä¸ºå…³è”è®°å¿†æ¥å­˜å‚¨äº‹å®æ•°æ®é›†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬é€šè¿‡è¿™äº›è¡¨è¾¾åŠ›ç»“æœï¼Œè¡¥å……äº†ç®€åŒ–çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨äº‹å®å›å¿†ä»»åŠ¡ä¸Šæ¢¯åº¦æµè½¨è¿¹çš„åˆ†æï¼Œæ˜¾ç¤ºæ¨¡å‹è¡¨ç°å‡ºåºåˆ—å­¦ä¹ è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å‡ºè‰²çš„äº‹å®è®°å¿†èƒ½åŠ›ã€‚</li>
<li>æµ…å±‚å˜å‹å™¨ç»“åˆå…³è”è®°å¿†å¯è¾¾åˆ°è¿‘ä¹æœ€ä¼˜çš„å­˜å‚¨èƒ½åŠ›ã€‚</li>
<li>çº¿æ€§ä¸MLPå…³è”è®°å¿†çš„å­˜å‚¨èƒ½åŠ›ä¸å‚æ•°æ•°é‡å‘ˆçº¿æ€§å…³ç³»ã€‚</li>
<li>åœ¨åˆæˆçš„äº‹å®å›å¿†ä»»åŠ¡ä¸­ï¼Œå˜å‹å™¨ç»“åˆè‡ªæ³¨æ„åŠ›ä¸MLPå¯å–å¾—æé«˜å‡†ç¡®ç‡ã€‚</li>
<li>å˜å‹å™¨å¯ä»¥åœ¨ä½¿ç”¨ä¸åŒçš„è®°å¿†æœºåˆ¶ï¼ˆå€¼çŸ©é˜µæˆ–MLPï¼‰ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</li>
<li>æ¨¡å‹åœ¨äº‹å®è®°å¿†ä»»åŠ¡ä¸­è¡¨ç°å‡ºåºåˆ—å­¦ä¹ è¡Œä¸ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9f8343ea89c2fe7f963d845465390cfc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32c839c5936fedb7031cd8088b478b51.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-eefd36c16f7b4f17174d08857bb5b0a8.jpg" align="middle">
</details>




<h2 id="LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations"><a href="#LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations" class="headerlink" title="LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations"></a>LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations</h2><p><strong>Authors:Mingjie Xu, Mengyang Wu, Yuzhi Zhao, Jason Chun Lok Li, Weifeng Ou</strong></p>
<p>Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMsâ€™ inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: <a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG">https://github.com/Endlinc/LLaVA-SpaceSGG</a>. </p>
<blockquote>
<p>åœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGï¼‰å°†è§†è§‰åœºæ™¯è½¬æ¢ä¸ºç»“æ„åŒ–å›¾è¡¨ç¤ºï¼Œä¸ºå¤æ‚çš„è§†è§‰ä»»åŠ¡æä¾›æ›´æ·±å…¥çš„åœºæ™¯ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SGGæ¨¡å‹ç»å¸¸å¿½ç•¥é‡è¦çš„ç©ºé—´å…³ç³»ï¼Œå¹¶ä¸”åœ¨å¼€æ”¾è¯æ±‡ä¸Šä¸‹æ–‡ä¸­çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SpaceSGGï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå¼€æ”¾è¯æ±‡SGGè®¾è®¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´å…³ç³»å»ºæ¨¡ã€‚ä¸ºäº†è®­ç»ƒå®ƒï¼Œæˆ‘ä»¬æ”¶é›†äº†SGGæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå‘½åä¸ºSpaceSGGã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡ç»“åˆå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æ•°æ®æ„å»ºç®¡é“ä¸­ä½¿ç”¨å¼€æºæ¨¡å‹åˆæˆæ•°æ®è€Œæ„å»ºçš„ã€‚å®ƒç»“åˆäº†ç›®æ ‡ä½ç½®ã€ç›®æ ‡å…³ç³»å’Œæ·±åº¦ä¿¡æ¯ï¼Œäº§ç”Ÿä¸‰ç§æ•°æ®æ ¼å¼ï¼šç©ºé—´SGGæè¿°ã€é—®ç­”å’Œå¯¹è¯ã€‚ä¸ºäº†æé«˜MLLMsçš„å†…åœ¨èƒ½åŠ›å‘SGGä»»åŠ¡çš„è½¬ç§»ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-SpaceSGGåœ¨å…¬å¼€è¯æ±‡SGGæ–¹æ³•ä¸Šå…·æœ‰å‡ºè‰²è¡¨ç°ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå¬å›ç‡æé«˜äº†8.6%ï¼Œå¹³å‡å¬å›ç‡æé«˜äº†28.4%ã€‚æˆ‘ä»¬çš„ä»£ç åº“ã€æ•°æ®é›†å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨ä»¥ä¸‹GitHubé“¾æ¥ä¸Šå…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG%E3%80%82">https://github.com/Endlinc/LLaVA-SpaceSGGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06322v1">PDF</a> Accepted by the WACV 2025, including supplementary material</p>
<p><strong>Summary</strong></p>
<p>SGGï¼ˆåœºæ™¯å›¾ç”Ÿæˆï¼‰æŠ€æœ¯èƒ½å¤Ÿå°†è§†è§‰åœºæ™¯è½¬åŒ–ä¸ºç»“æ„åŒ–å›¾è¡¨ç¤ºï¼Œä¸ºå¤æ‚è§†è§‰ä»»åŠ¡æä¾›æ›´æ·±å…¥çš„åœºæ™¯ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰SGGæ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡ä¸Šä¸‹æ–‡ä¸­çš„ç©ºé—´å…³ç³»å»ºæ¨¡å­˜åœ¨å±€é™æ€§å’Œæ³›åŒ–å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SpaceSGGï¼Œä¸€ä¸ªä¸ºå¼€æ”¾è¯æ±‡SGGè®¾è®¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´å…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªè¡Œæ„å»ºçš„SpaceSGGæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†ç»“åˆå…¬å¼€æ•°æ®é›†å’Œå¼€æºæ¨¡å‹çš„æ•°æ®æ„å»ºæµç¨‹è¿›è¡Œåˆæˆã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-SpaceSGGåœ¨å¼€æ”¾è¯æ±‡SGGæ–¹æ³•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¬å›ç‡æé«˜8.6%ï¼Œå¹³å‡å¬å›ç‡æé«˜28.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGGæŠ€æœ¯èƒ½å¤Ÿå°†è§†è§‰åœºæ™¯è½¬åŒ–ä¸ºç»“æ„åŒ–å›¾å½¢è¡¨ç¤ºï¼ŒåŠ æ·±åœºæ™¯ç†è§£ï¼Œç”¨äºå¤æ‚è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰SGGæ¨¡å‹åœ¨ç©ºé—´å…³ç³»å»ºæ¨¡å’Œå¼€æ”¾è¯æ±‡ä¸Šä¸‹æ–‡æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LLaVA-SpaceSGGæ˜¯ä¸€ä¸ªä¸ºå¼€æ”¾è¯æ±‡SGGè®¾è®¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´å…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>SpaceSGGæ•°æ®é›†æ˜¯ä¸“é—¨ä¸ºLLaVA-SpaceSGGæ¨¡å‹è®­ç»ƒè€Œæ„å»ºï¼Œç»“åˆäº†å¤šç§æ•°æ®æ¥æºã€‚</li>
<li>LLaVA-SpaceSGGæ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œä»¥æ›´å¥½åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›è½¬ç§»åˆ°SGGä»»åŠ¡ä¸Šã€‚</li>
<li>LLaVA-SpaceSGGåœ¨å¼€æ”¾è¯æ±‡SGGæ–¹æ³•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å¬å›ç‡å’Œå¹³å‡å¬å›ç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5d43c40300ef813ab9c294c6d2d9da8b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-667007beb268b4ed3183f8ffceb27f82.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-05320e9817235d934fa69ec3d1db206e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8a013cdba343996ad6176ba72488718.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-89be98b84bbb87523661e45a4b5f1ff1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4568902a2ea3a695ad701e9d47dd19c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7b62a30b1a3eebfc0cd40c49eab39df3.jpg" align="middle">
</details>




<h2 id="Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness"><a href="#Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness" class="headerlink" title="Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness"></a>Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness</h2><p><strong>Authors:Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principlesâ€“informativeness, uniqueness, and representativenessâ€“for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 100.8% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the â€œLess is Moreâ€ philosophy in MLLM development. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å¾®è°ƒé¢„å…ˆè®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»¥å¤„ç†ç°å®ä¸–ç•Œä»»åŠ¡ã€‚ç„¶è€Œï¼Œè§†è§‰æŒ‡ä»¤æ•°æ®é›†çš„å¿«é€Ÿæ‰©å¼ å¯¼è‡´äº†æ•°æ®å†—ä½™ï¼Œä»è€Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œæ¡†æ¶DataTailorï¼Œå®ƒåˆ©ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™â€”â€”ä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§â€”â€”è¿›è¡Œæœ‰æ•ˆæ•°æ®é€‰æ‹©ã€‚æˆ‘ä»¬è®¤ä¸ºæœ‰ä»·å€¼çš„æ ·æœ¬åº”è¯¥å…·æœ‰ä»»åŠ¡çš„ä¿¡æ¯æ€§ã€éå†—ä½™æ€§ï¼Œå¹¶ä»£è¡¨æ ·æœ¬åˆ†å¸ƒï¼ˆå³éå¼‚å¸¸å€¼ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹æ¯é¡¹åŸåˆ™è¿›è¡Œæ‰“åˆ†çš„å®é™…æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯è‡ªåŠ¨é€‚åº”ç»™å®šçš„æ•°æ®é›†ï¼Œæ— éœ€ç¹ççš„è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDataTailoråœ¨ä»…ä½¿ç”¨15%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å…¨æ•°æ®å¾®è°ƒæ€§èƒ½çš„100.8%ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†å“è¶Šçš„ç»“æœã€‚è¿™ä½“ç°äº†MLLMå‘å±•ä¸­çš„â€œå°‘å³æ˜¯å¤šâ€ç†å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06293v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong><br>    é’ˆå¯¹é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæå‡ºä¸€ç§åˆ©ç”¨ä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§ä¸‰å¤§åŸåˆ™çš„æ•°æ®é€‰æ‹©æ¡†æ¶DataTailorï¼Œæœ‰æ•ˆåº”å¯¹çœŸå®ä»»åŠ¡åœºæ™¯ä¸­çš„æ•°æ®å†—ä½™é—®é¢˜ï¼Œå¤§å¹…é™ä½è®¡ç®—æˆæœ¬ã€‚DataTailorèƒ½åœ¨ä¿æŒä¼˜è¶Šæ€§èƒ½çš„åŒæ—¶å‡å°‘æ•°æ®é‡éœ€æ±‚ï¼Œå‡å°‘çš„æ•°æ®è¾¾åˆ°ä½¿ç”¨ä»…æ•°æ®15%ï¼Œå…¶æ€§èƒ½é«˜è¾¾ä½¿ç”¨å…¨éƒ¨æ•°æ®çš„æ€§èƒ½æ ‡å‡†çš„ç™¾åˆ†ä¹‹ç™¾é›¶å…«ç‚¹ä»¥ä¸Šã€‚æ­¤æˆæœå±•ç°äº†åœ¨MLLMå‘å±•ä¸Šçš„â€œå°‘å³æ˜¯å¤šâ€ç†å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DataTailoræ˜¯ä¸€ä¸ªåä½œæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ•°æ®é€‰æ‹©ä»¥è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨ä¸‰å¤§åŸåˆ™ï¼šä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§æ¥ç¡®ä¿æ•°æ®çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
<li>æ•°æ®å†—ä½™æ˜¯çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼Œå®ƒå¯¹æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æˆæœ¬é€ æˆäº†æ˜¾è‘—å½±å“ã€‚æ•°æ®å†—ä½™å¯èƒ½ä¼šé˜»ç¢æ¨¡å‹å­¦ä¹ å’Œæ•ˆç‡æå‡ï¼Œå½±å“æ€§èƒ½çš„è¡¨ç°ã€‚åœ¨é‡‡ç”¨MLLMsè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œæ•°æ®å†—ä½™é—®é¢˜å°¤ä¸ºçªå‡ºã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-16ba4959c013b5b5057db43147dbcf1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56d1796b5c2c43c9484a2a5ccd1a518a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5da737588d95e9d9a1795383aa0d36a0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d6ddefd66bc4617e55355e51f2d01f13.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-17b558db0836cbaac7eb611b67eef6d9.jpg" align="middle">
</details>




<h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by â€œselecting sparsely and computing denselyâ€. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„PEFTæ–¹æ³•å¯ä»¥å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒæˆ–å¯æ‰©å±•çš„æœåŠ¡ï¼Œä½†æ— æ³•åŒæ—¶å®ç°ä¸‰è€…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç¨€ç–å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨è¿™ä¸€å…³é”®è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—é’ˆå¯¹LLMçš„ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆS$^{2}$FTï¼‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§ã€‚S$^{2}$FTé€šè¿‡â€œç¨€ç–é€‰æ‹©ã€å¯†é›†è®¡ç®—â€æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®ƒåˆ†åˆ«é€‰æ‹©MHAå’ŒFFNæ¨¡å—ä¸­æ¯ä¸ªTransformerå—çš„å‡ ä¸ªå¤´å’Œé€šé“ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå¯¹LLMä¸­è€¦åˆç»“æ„ä¸¤ä¾§çš„æƒé‡çŸ©é˜µè¿›è¡Œé‡æ–°æ’åˆ—ç»„åˆï¼Œå°†æ‰€é€‰ç»„ä»¶åœ¨æ¯ä¸€å±‚ä¸­è¿æ¥æˆå¯†é›†çš„å­çŸ©é˜µã€‚æœ€åï¼ŒS$^{2}$FTå¯¹æ‰€æœ‰å­çŸ©é˜µè¿›è¡ŒåŸåœ°æ¢¯åº¦æ›´æ–°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜ï¼Œåœ¨å¸¸è¯†æ¨ç†å’Œç®—æœ¯æ¨ç†æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä¸LoRAç›¸æ¯”ï¼Œå¹³å‡æ”¹è¿›äº†4.6%å’Œ1.3%ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒåæ¨å¹¿åˆ°ä¸åŒé¢†åŸŸæ—¶ï¼Œè¾ƒå…¨é‡å¾®è°ƒï¼ˆfull FTï¼‰é«˜å‡º11.5%ã€‚ä½¿ç”¨æˆ‘ä»¬çš„éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTåœ¨è®­ç»ƒå†…å­˜æ–¹é¢èŠ‚çœäº†é«˜è¾¾3å€ï¼Œä¸å…¨é‡å¾®è°ƒç›¸æ¯”ï¼Œå»¶è¿Ÿæé«˜äº†1.5-2.7å€ï¼ŒåŒæ—¶åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šéƒ½è¾ƒLoRAå¹³å‡æé«˜äº†10%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼ŒS$^{2}$FTä¸­çš„æƒé‡æ›´æ–°å¯ä»¥è§£è€¦ä¸ºé€‚é…å™¨ï¼Œå®ç°å¯¹å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆèåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡Œå¤„ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆSÂ²FTï¼‰æ–¹æ³•ã€‚æ­¤æ–¹æ³•åœ¨é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯ä¼¸ç¼©æœåŠ¡ä¸‰ä¸ªæ–¹é¢å‡æœ‰çªç ´ã€‚é€šè¿‡é€‰æ‹©ç¨€ç–éƒ¨åˆ†è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ç†è®ºåˆ†æå’Œå®éªŒç»“æœçš„æ”¯æŒä¸‹ï¼ŒSÂ²FTä¸ä»…é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜ï¼Œè€Œä¸”åœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSÂ²FTé‡‡ç”¨éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒèŠ‚çœè®­ç»ƒå†…å­˜ï¼Œæé«˜å»¶è¿Ÿï¼ŒåŒæ—¶å¹³å‡æå‡æ€§èƒ½è¾¾10%ã€‚æœ€åï¼ŒSÂ²FTçš„é‡é‡æ›´æ–°å¯è§£è€¦ä¸ºé€‚é…å™¨ï¼Œå®ç°å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆèåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡ŒæœåŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SÂ²FTæ–¹æ³•èƒ½å¤Ÿåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯ä¼¸ç¼©æœåŠ¡çš„ä¸‰è€…å…¼é¡¾ã€‚</li>
<li>é€šè¿‡ç¨€ç–å¾®è°ƒæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SÂ²FTæ–¹æ³•é€šè¿‡é€‰æ‹©MHAå’ŒFFNæ¨¡å—çš„éƒ¨åˆ†å¤´éƒ¨å’Œé€šé“è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨LLMçš„è€¦åˆç»“æ„ä¸¤ä¾§é‡æ–°æ’åˆ—æƒé‡çŸ©é˜µï¼Œå®ç°â€œé€‰æ‹©ç¨€ç–ï¼Œè®¡ç®—å¯†é›†â€ã€‚</li>
<li>SÂ²FTé‡‡ç”¨éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒèŠ‚çœè®­ç»ƒå†…å­˜ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>SÂ²FTåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡æ”¹è¿›è¶…è¿‡å…¶ä»–æ–¹æ³•ã€‚</li>
<li>SÂ²FTçš„é‡é‡æ›´æ–°å¯ä»¥è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¾¿äºæ¨¡å‹èåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡ŒæœåŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9dc097c198906efa869b0f4455639b1b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-52615bada518b68dec445640650fa2f0.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
