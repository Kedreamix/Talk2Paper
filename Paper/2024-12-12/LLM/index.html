<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Generative Semantic Communication Architectures, Technologies, and   Applications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    32.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    134 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Generative-Semantic-Communication-Architectures-Technologies-and-Applications"><a href="#Generative-Semantic-Communication-Architectures-Technologies-and-Applications" class="headerlink" title="Generative Semantic Communication: Architectures, Technologies, and   Applications"></a>Generative Semantic Communication: Architectures, Technologies, and   Applications</h2><p><strong>Authors:Jinke Ren, Yaping Sun, Hongyang Du, Weiwen Yuan, Chongjie Wang, Xianda Wang, Yingbin Zhou, Ziwei Zhu, Fangxin Wang, Shuguang Cui</strong></p>
<p>This paper delves into the applications of generative artificial intelligence (GAI) in semantic communication (SemCom) and presents a thorough study. Three popular SemCom systems enabled by classical GAI models are first introduced, including variational autoencoders, generative adversarial networks, and diffusion models. For each system, the fundamental concept of the GAI model, the corresponding SemCom architecture, and the associated literature review of recent efforts are elucidated. Then, a novel generative SemCom system is proposed by incorporating the cutting-edge GAI technology-large language models (LLMs). This system features two LLM-based AI agents at both the transmitter and receiver, serving as â€œbrainsâ€ to enable powerful information understanding and content regeneration capabilities, respectively. This innovative design allows the receiver to directly generate the desired content, instead of recovering the bit stream, based on the coded semantic information conveyed by the transmitter. Therefore, it shifts the communication mindset from â€œinformation recoveryâ€ to â€œinformation regenerationâ€ and thus ushers in a new era of generative SemCom. A case study on point-to-point video retrieval is presented to demonstrate the superiority of the proposed generative SemCom system, showcasing a 99.98% reduction in communication overhead and a 53% improvement in retrieval accuracy compared to the traditional communication system. Furthermore, four typical application scenarios for generative SemCom are delineated, followed by a discussion of three open issues warranting future investigation. In a nutshell, this paper provides a holistic set of guidelines for applying GAI in SemCom, paving the way for the efficient implementation of generative SemCom in future wireless networks. </p>
<blockquote>
<p>æœ¬æ–‡æ·±å…¥æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰åœ¨è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚é¦–å…ˆä»‹ç»äº†ä¸‰ç§ç”±ç»å…¸GAIæ¨¡å‹å¯ç”¨çš„æµè¡Œçš„SemComç³»ç»Ÿï¼ŒåŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªç³»ç»Ÿï¼Œéƒ½é˜è¿°äº†GAIæ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µã€ç›¸åº”çš„SemComæ¶æ„ä»¥åŠæœ€è¿‘çš„æ–‡çŒ®ç»¼è¿°ã€‚ç„¶åï¼Œé€šè¿‡ç»“åˆæœ€å‰æ²¿çš„GAIæŠ€æœ¯â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆå¼SemComç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨å‘å°„ç«¯å’Œæ¥æ”¶ç«¯éƒ½é‡‡ç”¨äº†åŸºäºLLMçš„AIä»£ç†ï¼Œåˆ†åˆ«ä½œä¸ºâ€œå¤§è„‘â€ï¼Œä»¥å®ç°å¼ºå¤§çš„ä¿¡æ¯ç†è§£å’Œå†…å®¹å†ç”Ÿèƒ½åŠ›ã€‚è¿™ç§åˆ›æ–°è®¾è®¡å…è®¸æ¥æ”¶ç«¯æ ¹æ®å‘å°„ç«¯ä¼ é€’çš„ç¼–ç è¯­ä¹‰ä¿¡æ¯ç›´æ¥ç”Ÿæˆæ‰€éœ€å†…å®¹ï¼Œè€Œä¸æ˜¯æ¢å¤æ¯”ç‰¹æµã€‚å› æ­¤ï¼Œå®ƒå°†é€šä¿¡æ€ç»´ä»â€œä¿¡æ¯æ¢å¤â€è½¬å˜ä¸ºâ€œä¿¡æ¯å†ç”Ÿâ€ï¼Œä»è€Œå¼€å¯äº†ç”Ÿæˆå¼SemComçš„æ–°æ—¶ä»£ã€‚é€šè¿‡ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢çš„æ¡ˆä¾‹åˆ†æï¼Œå±•ç¤ºäº†æ‰€æå‡ºçš„ç”Ÿæˆå¼SemComç³»ç»Ÿçš„ä¼˜è¶Šæ€§ï¼Œä¸ä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿç›¸æ¯”ï¼Œé€šä¿¡å¼€é”€å‡å°‘äº†99.98%ï¼Œæ£€ç´¢ç²¾åº¦æé«˜äº†5a 3%ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ç”Ÿæˆå¼SemComçš„å››ä¸ªå…¸å‹åº”ç”¨åœºæ™¯ï¼Œå¹¶è®¨è®ºäº†ä¸‰ä¸ªéœ€è¦æœªæ¥ç ”ç©¶çš„å¼€æ”¾é—®é¢˜ã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡ä¸ºGAIåœ¨SemComä¸­çš„åº”ç”¨æä¾›äº†ä¸€å¥—å…¨é¢çš„æŒ‡å¯¼æ–¹é’ˆï¼Œä¸ºæœªæ¥æ— çº¿ç½‘ç»œä¸­ç”Ÿæˆå¼SemComçš„æœ‰æ•ˆå®æ–½å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08642v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰åœ¨è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶ä»‹ç»äº†ä¸‰ç§ç”±ç»å…¸GAIæ¨¡å‹å¯ç”¨çš„SemComç³»ç»Ÿã€‚æ–‡ç« æå‡ºäº†ç»“åˆæœ€æ–°GAIæŠ€æœ¯çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼SemComç³»ç»Ÿï¼Œå®ç°äº†ä»â€œä¿¡æ¯æ¢å¤â€åˆ°â€œä¿¡æ¯å†ç”Ÿâ€çš„è½¬å˜ã€‚è¯¥ç³»ç»Ÿåœ¨ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢æ¡ˆä¾‹ç ”ç©¶ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿï¼Œå‡å°‘äº†99.98%çš„é€šä¿¡å¼€é”€ï¼Œæé«˜äº†53%çš„æ£€ç´¢ç²¾åº¦ã€‚æœ¬æ–‡ä¸ºGAIåœ¨SemComä¸­çš„åº”ç”¨æä¾›äº†ä¸€å¥—å…¨é¢çš„æŒ‡å—ï¼Œä¸ºæœªæ¥çš„æ— çº¿ç½‘ç»œä¸­å®ç°é«˜æ•ˆçš„ç”Ÿæˆå¼SemComé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰åœ¨è¯­ä¹‰é€šä¿¡ï¼ˆSemComï¼‰ä¸­çš„åº”ç”¨åŠå…¶ç ”ç©¶èƒŒæ™¯ã€‚</li>
<li>é˜è¿°äº†ä¸‰ç§åŸºäºç»å…¸GAIæ¨¡å‹çš„SemComç³»ç»Ÿï¼ŒåŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ›æ–°å‹ç”Ÿæˆå¼SemComç³»ç»Ÿï¼Œå®ç°äº†å¼ºå¤§çš„ä¿¡æ¯ç†è§£å’Œå†…å®¹å†ç”Ÿèƒ½åŠ›ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨ç‚¹å¯¹ç‚¹è§†é¢‘æ£€ç´¢æ¡ˆä¾‹ç ”ç©¶ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡å¼€é”€å¹¶æé«˜äº†æ£€ç´¢ç²¾åº¦ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†ç”Ÿæˆå¼SemComåœ¨å››ä¸ªå…¸å‹åº”ç”¨åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚</li>
<li>å¼ºè°ƒäº†æœªæ¥åœ¨å¼€æ”¾é—®é¢˜ä¸Šéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢çš„æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0947d019af14f992664fd549f68a4b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-992887657b0d055041f816e833fbba5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb684a360750521f8bc80cff6b8f03cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f890f4c21e1505742bf4b41549683065.jpg" align="middle">
</details>




<h2 id="Fast-Prompt-Alignment-for-Text-to-Image-Generation"><a href="#Fast-Prompt-Alignment-for-Text-to-Image-Generation" class="headerlink" title="Fast Prompt Alignment for Text-to-Image Generation"></a>Fast Prompt Alignment for Text-to-Image Generation</h2><p><strong>Authors:Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang</strong></p>
<p>Text-to-image generation has advanced rapidly, yet aligning complex textual prompts with generated visuals remains challenging, especially with intricate object relationships and fine-grained details. This paper introduces Fast Prompt Alignment (FPA), a prompt optimization framework that leverages a one-pass approach, enhancing text-to-image alignment efficiency without the iterative overhead typical of current methods like OPT2I. FPA uses large language models (LLMs) for single-iteration prompt paraphrasing, followed by fine-tuning or in-context learning with optimized prompts to enable real-time inference, reducing computational demands while preserving alignment fidelity. Extensive evaluations on the COCO Captions and PartiPrompts datasets demonstrate that FPA achieves competitive text-image alignment scores at a fraction of the processing time, as validated through both automated metrics (TIFA, VQA) and human evaluation. A human study with expert annotators further reveals a strong correlation between human alignment judgments and automated scores, underscoring the robustness of FPAâ€™s improvements. The proposed method showcases a scalable, efficient alternative to iterative prompt optimization, enabling broader applicability in real-time, high-demand settings. The codebase is provided to facilitate further research: <a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„ç”ŸæˆæŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œç„¶è€Œï¼Œå°†å¤æ‚çš„æ–‡æœ¬æç¤ºä¸ç”Ÿæˆçš„å›¾åƒå¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¯¹è±¡å…³ç³»å’Œç²¾ç»†ç»†èŠ‚æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†å¿«é€Ÿæç¤ºå¯¹é½ï¼ˆFPAï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ä¸€æ¬¡é€šè¿‡çš„æ–¹æ³•ï¼Œæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½æ•ˆç‡ï¼Œè€Œæ— éœ€ä½¿ç”¨å½“å‰æ–¹æ³•ï¼ˆå¦‚OPT2Iï¼‰æ‰€ç‰¹æœ‰çš„è¿­ä»£å¼€é”€ã€‚FPAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå•æ¬¡è¿­ä»£æç¤ºåŒä¹‰æ›¿æ¢ï¼Œéšåé€šè¿‡ä¼˜åŒ–æç¤ºè¿›è¡Œå¾®è°ƒæˆ–ä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼Œä»¥å®ç°å®æ—¶æ¨ç†ï¼Œé™ä½äº†è®¡ç®—éœ€æ±‚åŒæ—¶ä¿æŒäº†å¯¹é½ä¿çœŸåº¦ã€‚åœ¨COCO Captionså’ŒPartiPromptsæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFPAåœ¨å¤„ç†æ—¶é—´çš„ä¸€å°éƒ¨åˆ†å†…å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ–‡æœ¬å›¾åƒå¯¹é½å¾—åˆ†ï¼Œè¿™å·²é€šè¿‡è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼ˆTIFAã€VQAï¼‰å’Œäººç±»è¯„ä¼°å¾—åˆ°äº†éªŒè¯ã€‚ç”±ä¸“å®¶æ³¨é‡Šè€…è¿›è¡Œçš„äººç±»ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†äººç±»å¯¹é½åˆ¤æ–­ä¸è‡ªåŠ¨åŒ–å¾—åˆ†ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œå¼ºè°ƒäº†FPAæ”¹è¿›çš„ç¨³å¥æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•å±•ç¤ºäº†è¿­ä»£æç¤ºä¼˜åŒ–çš„å¯æ‰©å±•ã€é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå¯åœ¨å®æ—¶ã€é«˜éœ€æ±‚ç¯å¢ƒä¸­å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚ä»£ç åº“å·²æä¾›ï¼Œä»¥æ–¹ä¾¿è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08639v1">PDF</a> TikTok Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Fast Prompt Alignmentï¼ˆFPAï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå•æ¬¡è¿­ä»£æç¤ºä¼˜åŒ–ï¼Œå®ç°æ–‡æœ¬åˆ°å›¾åƒå¯¹é½çš„å®æ—¶æ¨ç†ï¼Œå¤§å¤§æé«˜äº†å¯¹é½æ•ˆç‡å¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚åœ¨COCO Captionså’ŒPartiPromptsæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFPAåœ¨æ–‡æœ¬å›¾åƒå¯¹é½å¾—åˆ†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å¤„ç†æ—¶é—´å¤§å¹…å‡å°‘ã€‚æ­¤å¤–ï¼Œä¸“å®¶æ ‡æ³¨äººå‘˜çš„ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†FPAæ”¹è¿›çš„ç¨³å¥æ€§ã€‚è¯¥æ¡†æ¶ä¸ºå®æ—¶é«˜éœ€æ±‚åœºæ™¯ä¸‹çš„æç¤ºä¼˜åŒ–æä¾›äº†å¯æ‰©å±•ã€é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fast Prompt Alignment (FPA) æ¡†æ¶æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å¯¹é½å¤æ‚æ–‡æœ¬æç¤ºä¸ç”Ÿæˆå›¾åƒçš„é—®é¢˜ã€‚</li>
<li>FPA åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå•æ¬¡è¿­ä»£æç¤ºä¼˜åŒ–ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒå¯¹é½æ•ˆç‡ã€‚</li>
<li>FPA å®ç°äº†å®æ—¶æ¨ç†ï¼Œå‡å°‘è®¡ç®—éœ€æ±‚çš„åŒæ—¶ä¿æŒäº†å¯¹é½ç²¾åº¦ã€‚</li>
<li>åœ¨ COCO Captions å’Œ PartiPrompts æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº† FPA çš„é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä¸“å®¶æ ‡æ³¨äººå‘˜çš„ç ”ç©¶éªŒè¯äº† FPA æ”¹è¿›çš„ç¨³å¥æ€§ã€‚</li>
<li>FPA æä¾›äº†ä¸€ç§å¯æ‰©å±•ã€é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºå®æ—¶é«˜éœ€æ±‚åœºæ™¯ä¸‹çš„æç¤ºä¼˜åŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f2dd326d41094289fdc7eb85cac210a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f6e9bfd70925ec0ec9b4c06d074f163.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fdf53b899e19af7c79ca324d7775040.jpg" align="middle">
</details>




<h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œä»£ç ï¼‰å’Œè¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨è¯­è¨€å»ºæ¨¡ï¼ˆLatentLMï¼‰ï¼Œå®ƒä½¿ç”¨å› æœTransformeræ— ç¼é›†æˆäº†è¿ç»­å’Œç¦»æ•£æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è¿ç»­æ•°æ®è¡¨ç¤ºä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œæ‰©æ•£æ¥è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†Ïƒ-VAEã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLatentLMåœ¨å„ç§æ¨¡æ€ä¸­éƒ½éå¸¸æœ‰æ•ˆã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒLatentLMåœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢éƒ½è¶…è¶Šäº†Diffusion Transformersã€‚å½“é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼ŒLatentLMæä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£ï¼Œç»Ÿä¸€äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰©å¤§è®­ç»ƒä»¤ç‰Œè§„æ¨¡æ–¹é¢ï¼ŒLatentLMä¸Transfusionå’Œå‘é‡é‡åŒ–æ¨¡å‹ç›¸æ¯”å–å¾—äº†æœ‰åˆ©çš„è¡¨ç°ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆä¸­ï¼ŒLatentLMåœ¨è¯´è¯äººç›¸ä¼¼æ€§å’Œç¨³å¥æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„VALL-E 2æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦å‡å°‘10å€çš„è§£ç æ­¥éª¤ã€‚ç»“æœè¯æ˜LatentLMæ˜¯ä¸€ç§é«˜åº¦æœ‰æ•ˆå’Œå¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯ä»¥æ¨åŠ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œä»£ç ï¼‰å’Œè¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼‰çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†ã€‚æœ¬ç ”ç©¶æå‡ºäº†æ½œåœ¨è¯­è¨€å»ºæ¨¡ï¼ˆLatentLMï¼‰ï¼Œå®ƒåˆ©ç”¨å› æœTransformeræ— ç¼é›†æˆè¿ç»­å’Œç¦»æ•£æ•°æ®ã€‚é€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¡¨ç¤ºè¿ç»­æ•°æ®ä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªç¬¦å·æ‰©æ•£è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ï¼Œå¼€å‘äº†Ïƒ-VAEã€‚å®éªŒè¡¨æ˜ï¼ŒLatentLMåœ¨å„ç§æ¨¡æ€ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒLatentLMåœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢éƒ½è¶…è¶Šäº†Diffusion Transformersã€‚å½“é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼ŒLatentLMæä¾›äº†ä¸€ä¸ªç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£çš„é€šç”¨æ¥å£ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒLatentLMåœ¨æ‰©å¤§è®­ç»ƒç¬¦å·æ–¹é¢çš„è¡¨ç°ä¼˜äºTransfusionå’Œå‘é‡é‡åŒ–æ¨¡å‹ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆä¸­ï¼ŒLatentLMåœ¨è¯´è¯äººç›¸ä¼¼æ€§å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºå½“å‰ä¸»æµçš„VALL-E 2æ¨¡å‹ï¼Œå¹¶ä¸”åªéœ€è¦10å€çš„è§£ç æ­¥éª¤ã€‚ç»“æœè¯æ˜äº†LatentLMåœ¨æ¨è¿›å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ–¹é¢çš„é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LatentLMæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†ç¦»æ•£å’Œè¿ç»­æ•°æ®çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¡¨ç¤ºè¿ç»­æ•°æ®ä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªç¬¦å·æ‰©æ•£è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚</li>
<li>Ïƒ-VAEçš„å¼•å…¥è§£å†³äº†è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ã€‚</li>
<li>LatentLMåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>LatentLMé›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œæä¾›äº†ç»Ÿä¸€çš„æ¥å£ã€‚</li>
<li>åœ¨æ‰©å¤§è®­ç»ƒç¬¦å·æ–¹é¢ï¼ŒLatentLMçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics"><a href="#Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics" class="headerlink" title="Synthetic Vision: Training Vision-Language Models to Understand Physics"></a>Synthetic Vision: Training Vision-Language Models to Understand Physics</h2><p><strong>Authors:Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan</strong></p>
<p>Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to enhance VLMsâ€™ physical reasoning capabilities using simulated data. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs generated from simulations relevant to physical reasoning tasks. Second, we introduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to create scene descriptions enriched with physical properties and processes. During physical reasoning tasks, these PCBs can be leveraged as context to assist a Large Language Model (LLM) to improve its performance. We evaluate both of our approaches using multiple benchmarks, including a new stability detection QA dataset called Falling Tower, which includes both simulated and real-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM can significantly outperform larger state-of-the-art foundational models. We also show that integrating PCBs boosts the performance of foundational LLMs on physical reasoning tasks. Using the real-world scenes from the Falling Tower dataset, we also validate the robustness of both approaches in Sim2Real transfer. Our results highlight the utility that simulated data can have in the creation of learning systems capable of advanced physical reasoning. </p>
<blockquote>
<p>æ¶‰åŠåŠ¨æ€ç¯å¢ƒä¸­ç‰©ä½“è¡Œä¸ºçš„è§£é‡Šã€ç†è§£å’Œé¢„æµ‹çš„ç‰©ç†æ¨ç†ä»ç„¶æ˜¯å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å¢å¼ºVLMç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ç‰©ç†æ¨ç†ä»»åŠ¡ç›¸å…³çš„æ¨¡æ‹Ÿç”Ÿæˆçš„é—®ç­”ï¼ˆQAï¼‰å¯¹æ¥å¾®è°ƒé¢„è®­ç»ƒçš„VLMã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‰©ç†ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼ˆPCBsï¼‰ï¼Œè¿™æ˜¯ç»è¿‡ç²¾ç»†è°ƒæ•´çš„ä¸“é—¨ç”¨äºåˆ›å»ºåœºæ™¯æè¿°çš„VLMï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„ç‰©ç†å±æ€§å’Œè¿‡ç¨‹ã€‚åœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¿™äº›PCBä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜å…¶æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æˆ‘ä»¬çš„ä¸¤ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ–°çš„åä¸ºâ€œå è½ä¹‹å¡”â€çš„ç¨³å®šæ€§æ£€æµ‹é—®ç­”æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬æ¨¡æ‹Ÿå’ŒçœŸå®åœºæ™¯ï¼Œä»¥åŠCLEVRERã€‚æˆ‘ä»¬è¯æ˜ï¼Œç»è¿‡å°å‹é—®ç­”å¯¹å¾®è°ƒè¿‡çš„VLMå¯ä»¥æ˜¾è‘—ä¼˜äºè¾ƒå¤§çš„æœ€æ–°åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé›†æˆPCBå¯ä»¥æé«˜åŸºç¡€LLMåœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ¥è‡ªâ€œå è½ä¹‹å¡”â€æ•°æ®é›†çš„çœŸå®åœºæ™¯éªŒè¯äº†ä¸¤ç§æ–¹æ³•çš„Sim2Realè¿ç§»é²æ£’æ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†æ¨¡æ‹Ÿæ•°æ®åœ¨åˆ›å»ºèƒ½å¤Ÿè¿›è¡Œé«˜çº§ç‰©ç†æ¨ç†çš„å­¦ä¹ ç³»ç»Ÿæ–¹é¢çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸¤ç§ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„VLMè¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨ä¸ç‰©ç†æ¨ç†ä»»åŠ¡ç›¸å…³çš„æ¨¡æ‹Ÿç”Ÿæˆçš„é—®ç­”å¯¹æ¥å¢å¼ºå…¶ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ç‰©ç†è¯­å¢ƒæ„å»ºå™¨ï¼ˆPCBsï¼‰ï¼Œå®ƒä»¬æ˜¯ç»è¿‡å¾®è°ƒä¸“é—¨ç”¨äºåˆ›å»ºåœºæ™¯æè¿°çš„VLMsï¼Œå¯Œå«ç‰©ç†å±æ€§å’Œè¿‡ç¨‹ã€‚åœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¿™äº›PCBsä½œä¸ºä¸Šä¸‹æ–‡æ¥è¾…åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜æ€§èƒ½ã€‚è¯„ä¼°æ–¹æ³•åŒ…æ‹¬æ–°çš„ç¨³å®šæ€§æ£€æµ‹é—®ç­”æ•°æ®é›†Falling Towerå’ŒCLEVRERç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°å‹é—®ç­”å¾®è°ƒVLMæ˜¾è‘—ä¼˜äºå¤§å‹å…ˆè¿›åŸºç¡€æ¨¡å‹ï¼Œé›†æˆPCBsåˆ™æé«˜äº†åŸºç¡€LLMåœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨Falling Toweræ•°æ®é›†çš„å®æ—¶åœºæ™¯ä¸ŠéªŒè¯äº†ä¸¤ç§æ–¹æ³•åœ¨æ¨¡æ‹Ÿåˆ°çœŸå®è¿ç§»çš„ç¨³å¥æ€§ã€‚æ€»ç»“èµ·æ¥ï¼Œæœ¬æ–‡å¼ºè°ƒæ¨¡æ‹Ÿæ•°æ®åœ¨åˆ›å»ºå…·å¤‡é«˜çº§ç‰©ç†æ¨ç†èƒ½åŠ›çš„å­¦ä¹ ç³»ç»Ÿä¸­çš„åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†æ¨ç†æ˜¯å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´çš„é‡è¦æŒ‘æˆ˜ï¼Œæ¶‰åŠåŠ¨æ€ç¯å¢ƒä¸­ç‰©ä½“è¡Œä¸ºçš„è§£é‡Šã€ç†è§£å’Œé¢„æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§å¢å¼ºVLMsç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼šå¾®è°ƒé¢„è®­ç»ƒVLMä½¿ç”¨æ¨¡æ‹Ÿç”Ÿæˆçš„ä¸ç‰©ç†æ¨ç†ä»»åŠ¡ç›¸å…³çš„é—®ç­”å¯¹ï¼›å¼•å…¥ç‰©ç†è¯­å¢ƒæ„å»ºå™¨ï¼ˆPCBsï¼‰ä½œä¸ºä¸Šä¸‹æ–‡è¾…åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é€šè¿‡å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ–°çš„ç¨³å®šæ€§æ£€æµ‹é—®ç­”æ•°æ®é›†Falling Towerå’ŒCLEVRERï¼Œå¯¹ä¸¤ç§æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å°å‹é—®ç­”å¾®è°ƒVLMè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œä¼˜äºå¤§å‹å…ˆè¿›åŸºç¡€æ¨¡å‹ï¼›é›†æˆPCBsæé«˜äº†åŸºç¡€LLMåœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä¸¤ç§æ–¹æ³•åœ¨æ¨¡æ‹Ÿåˆ°çœŸå®è¿ç§»çš„ç¨³å¥æ€§ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ—¶åœºæ™¯ä¸Šã€‚</li>
<li>æœ¬æ–‡å¼ºè°ƒäº†æ¨¡æ‹Ÿæ•°æ®åœ¨åˆ›å»ºå…·å¤‡é«˜çº§ç‰©ç†æ¨ç†èƒ½åŠ›çš„å­¦ä¹ ç³»ç»Ÿä¸­çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e2f305523ebfa259f73cf14e4f47132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6539bfab04a65e9677b31143ef8a9ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa20a5f614e80dba0b3c546f3ff6045b.jpg" align="middle">
</details>




<h2 id="Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models"><a href="#Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models"></a>Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models</h2><p><strong>Authors:Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong</strong></p>
<p>Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic">https://github.com/jiah-li/magic</a>. </p>
<blockquote>
<p>å°½ç®¡åˆ©ç”¨å¯¹é½æŠ€æœ¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¢å¼ºç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§çš„æŠ€æœ¯å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ï¼Œè¿™æ˜¯ä¸€ç§æš´éœ²LLMå®‰å…¨æ¼æ´çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè´ªå©ªåæ ‡æ¢¯åº¦ï¼ˆGCGï¼‰æ–¹æ³•å·²ç»è¯æ˜èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¯¹æŠ—æ€§åç¼€ï¼Œçªç ´æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼ŒGCGæ¶‰åŠçš„ä¼˜åŒ–è¿‡ç¨‹éå¸¸è€—æ—¶ï¼Œä½¿å¾—è¶Šç‹±ç®¡é“æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†GCGçš„è¿‡ç¨‹ï¼Œå¹¶å‘ç°äº†é—´æ¥æ•ˆåº”çš„é—®é¢˜ï¼Œè¿™æ˜¯GCGä¼˜åŒ–çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å‹æ”»å‡»æ¢¯åº¦æŒ‡æ•°GCGï¼ˆMAGICï¼‰ï¼Œé€šè¿‡åˆ©ç”¨åç¼€ä»¤ç‰Œçš„æ¢¯åº¦ä¿¡æ¯æ¥è§£å†³é—´æ¥æ•ˆåº”é—®é¢˜ï¼Œä»è€Œé€šè¿‡å‡å°‘è®¡ç®—å’Œè¿­ä»£æ¬¡æ•°æ¥åŠ é€Ÿç¨‹åºã€‚æˆ‘ä»¬åœ¨AdvBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGICå®ç°äº†é«˜è¾¾1.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ä¸å…¶ä»–åŸºçº¿æŒå¹³ç”šè‡³æ›´é«˜ã€‚æˆ‘ä»¬çš„MAGICåœ¨Llama-2ä¸Šå®ç°äº†74%çš„ASRï¼Œåœ¨å¯¹GPT-3.5è¿›è¡Œè¿ç§»æ”»å‡»æ—¶å®ç°äº†54%çš„ASRã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jiah-li/magicä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08615v1">PDF</a> 13 pages,2 figures, accepted by The 31st International Conference on   Computational Linguistics</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½æŠ€æœ¯ä¸æ–­è¿›æ­¥ï¼Œå…¶ç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§å¾—åˆ°äº†æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶é¢ä¸´ä¸€ç§åä¸ºâ€œè¶Šç‹±â€çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ï¼Œæš´éœ²äº†LLMçš„å®‰å…¨æ¼æ´ã€‚æœ¬æ–‡ç ”ç©¶äº†è´ªå©ªåæ ‡æ¢¯åº¦ï¼ˆGCGï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¯¹æŠ—æ€§åç¼€ä»¥ç ´è§£æœ€æ–°LLMã€‚ç„¶è€Œï¼ŒGCGæ¶‰åŠçš„ä¼˜åŒ–è¿‡ç¨‹éå¸¸è€—æ—¶ï¼Œä½¿å¾—è¶Šç‹±ç®¡é“æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡è°ƒæŸ¥äº†GCGçš„è¿‡ç¨‹ï¼Œå¹¶ç¡®å®šäº†é—´æ¥æ•ˆåº”è¿™ä¸€å…³é”®ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å‹æ”»å‡»æ¢¯åº¦æŒ‡æ•°GCGï¼ˆMAGICï¼‰ï¼Œé€šè¿‡åˆ©ç”¨åç¼€æ ‡è®°çš„æ¢¯åº¦ä¿¡æ¯æ¥è§£å†³é—´æ¥æ•ˆåº”é—®é¢˜ï¼Œä»è€Œé€šè¿‡å‡å°‘è®¡ç®—å’Œè¿­ä»£æ¬¡æ•°æ¥åŠ é€Ÿç¨‹åºã€‚åœ¨AdvBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGICå®ç°äº†æœ€é«˜è¾¾1.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ä¸å…¶ä»–åŸºçº¿æŒå¹³ç”šè‡³æ›´é«˜ã€‚æˆ‘ä»¬çš„MAGICåœ¨å¯¹Llama-2çš„æ”»å‡»ä¸­å®ç°äº†74%çš„ASRï¼Œåœ¨å¯¹GPT-3.5è¿›è¡Œè¿ç§»æ”»å‡»æ—¶å®ç°äº†54%çš„ASRã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é¢ä¸´å¯¹æŠ—æ€§æ”»å‡»ï¼Œå³â€œè¶Šç‹±â€ï¼Œè¿™æš´éœ²äº†å…¶å®‰å…¨æ¼æ´ã€‚</li>
<li>è´ªå©ªåæ ‡æ¢¯åº¦ï¼ˆGCGï¼‰æ–¹æ³•å¯ä»¥è‡ªåŠ¨ç”Ÿæˆçš„å¯¹æŠ—æ€§åç¼€æ¥ç ´è§£å…ˆè¿›çš„LLMï¼Œä½†ä¼˜åŒ–è¿‡ç¨‹é«˜åº¦è€—æ—¶ã€‚</li>
<li>GCGæ–¹æ³•çš„ç“¶é¢ˆåœ¨äºé—´æ¥æ•ˆåº”ã€‚</li>
<li>æå‡ºäº†æ¨¡å‹æ”»å‡»æ¢¯åº¦æŒ‡æ•°GCGï¼ˆMAGICï¼‰æ¥è§£å†³é—´æ¥æ•ˆåº”é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯åŠ é€Ÿè¶Šç‹±è¿‡ç¨‹ã€‚</li>
<li>MAGICåœ¨AdvBenchå®éªŒä¸Šå®ç°äº†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚</li>
<li>MAGICå¯¹Llama-2çš„æ”»å‡»æˆåŠŸç‡ä¸º74%ï¼Œå¯¹GPT-3.5çš„è¿ç§»æ”»å‡»æˆåŠŸç‡ä¸º54%ã€‚</li>
<li>MAGICçš„ä»£ç å·²å…¬å¼€æä¾›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58d6ac4218806ce271467b0e57043d2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1402f5cd5315046c630f6d625906afaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f3cc516321175daee06622fd2d711f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-740263989a70e39b604d8c517b02ca4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08274e4db02b3e1e8c5285e734fedd36.jpg" align="middle">
</details>




<h2 id="Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval"><a href="#Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval" class="headerlink" title="Preference Discerning with LLM-Enhanced Generative Retrieval"></a>Preference Discerning with LLM-Enhanced Generative Retrieval</h2><p><strong>Authors:Fabian Paischer, Liu Yang, Linfeng Liu, Shuai Shao, Kaveh Hassani, Jiacheng Li, Ricky Chen, Zhang Gabriel Li, Xialo Gao, Wei Shao, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Hamid Eghbalzadeh</strong></p>
<p>Sequential recommendation systems aim to provide personalized recommendations for users based on their interaction history. To achieve this, they often incorporate auxiliary information, such as textual descriptions of items and auxiliary tasks, like predicting user preferences and intent. Despite numerous efforts to enhance these models, they still suffer from limited personalization. To address this issue, we propose a new paradigm, which we term preference discerning. In preference dscerning, we explicitly condition a generative sequential recommendation system on user preferences within its context. To this end, we generate user preferences using Large Language Models (LLMs) based on user reviews and item-specific data. To evaluate preference discerning capabilities of sequential recommendation systems, we introduce a novel benchmark that provides a holistic evaluation across various scenarios, including preference steering and sentiment following. We assess current state-of-the-art methods using our benchmark and show that they struggle to accurately discern user preferences. Therefore, we propose a new method named Mender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce $\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods and achieves state-of-the-art performance on our benchmark. Our results show that Mender can be effectively guided by human preferences even though they have not been observed during training, paving the way toward more personalized sequential recommendation systems. We will open-source the code and benchmarks upon publication. </p>
<blockquote>
<p>é¡ºåºæ¨èç³»ç»Ÿæ—¨åœ¨æ ¹æ®ç”¨æˆ·çš„äº¤äº’å†å²ä¸ºå…¶æä¾›ä¸ªæ€§åŒ–æ¨èã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå®ƒä»¬é€šå¸¸ä¼šèå…¥è¾…åŠ©ä¿¡æ¯ï¼Œå¦‚ç‰©å“çš„æ–‡æœ¬æè¿°å’Œè¾…åŠ©ä»»åŠ¡ï¼Œå¦‚é¢„æµ‹ç”¨æˆ·åå¥½å’Œæ„å›¾ã€‚å°½ç®¡å·²ç»æœ‰å¾ˆå¤šåŠªåŠ›æ¥æå‡è¿™äº›æ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€ä¸ªæ€§åŒ–æœ‰é™çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåå¥½è¯†åˆ«ã€‚åœ¨åå¥½è¯†åˆ«ä¸­ï¼Œæˆ‘ä»¬æ˜¾å¼åœ°å°†ç”Ÿæˆå¼é¡ºåºæ¨èç³»ç»Ÿå»ºç«‹åœ¨ç”¨æˆ·åå¥½ä¹‹ä¸Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºç”¨æˆ·è¯„è®ºå’Œç‰¹å®šç‰©å“æ•°æ®ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç”¨æˆ·åå¥½ã€‚ä¸ºäº†è¯„ä¼°é¡ºåºæ¨èç³»ç»Ÿçš„åå¥½è¯†åˆ«èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•å¯ä»¥åœ¨å„ç§æƒ…å¢ƒä¸‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬åå¥½æ§åˆ¶å’Œæƒ…æ„Ÿè·Ÿéšã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤åŸºå‡†æµ‹è¯•è¯„ä¼°äº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨å‡†ç¡®è¯†åˆ«ç”¨æˆ·åå¥½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåä¸ºMenderï¼ˆå¤šæ¨¡å¼åå¥½è¾¨åˆ«å™¨ï¼‰ï¼Œå®ƒåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰åœ¨è®­ç»ƒä¸­è§‚å¯Ÿåˆ°äººç±»åå¥½çš„æƒ…å†µä¸‹ï¼ŒMenderä¹Ÿå¯ä»¥æœ‰æ•ˆåœ°è¢«äººç±»åå¥½æ‰€å¼•å¯¼ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–çš„é¡ºåºæ¨èç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚åœ¨å‘å¸ƒæ—¶ï¼Œæˆ‘ä»¬å°†å…¬å¼€æºä»£ç å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08604v1">PDF</a> 11 pages + references and appendix</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·å†å²äº¤äº’çš„åºåˆ—æ¨èç³»ç»Ÿæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–æ¨èã€‚ä¸ºè¾¾åˆ°æ­¤ç›®çš„ï¼Œå®ƒä»¬ç»å¸¸ç»“åˆè¾…åŠ©ä¿¡æ¯ï¼Œå¦‚ç‰©å“çš„æ–‡å­—æè¿°å’Œé¢„æµ‹ç”¨æˆ·åå¥½å’Œæ„å›¾çš„è¾…åŠ©ä»»åŠ¡ã€‚å°½ç®¡æœ‰å¾ˆå¤šæé«˜è¿™äº›æ¨¡å‹çš„åŠªåŠ›ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ä¸ªæ€§åŒ–æœ‰é™çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„èŒƒå¼â€”â€”åå¥½è¾¨åˆ«ã€‚åœ¨åå¥½è¾¨åˆ«ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°åœ¨ç”Ÿæˆåºåˆ—æ¨èç³»ç»Ÿä¸­ä»¥ç”¨æˆ·åå¥½ä¸ºæ¡ä»¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºç”¨æˆ·è¯„è®ºå’Œç‰¹å®šç‰©å“æ•°æ®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç”¨æˆ·åå¥½ã€‚ä¸ºäº†è¯„ä¼°åºåˆ—æ¨èç³»ç»Ÿçš„åå¥½è¾¨åˆ«èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•å…¨é¢è¯„ä»·äº†ä¸åŒåœºæ™¯ï¼ŒåŒ…æ‹¬åå¥½å¼•å¯¼å’Œæƒ…æ„Ÿè·Ÿéšã€‚æˆ‘ä»¬è¯„ä¼°äº†å½“å‰å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨å‡†ç¡®è¾¨åˆ«ç”¨æˆ·åå¥½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Menderï¼ˆå¤šæ¨¡æ€åå¥½é‰´åˆ«å™¨ï¼‰ï¼Œå®ƒåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMenderå³ä½¿åœ¨æ²¡æœ‰åœ¨è®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„äººç±»åå¥½ä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆæŒ‡å¯¼ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–çš„åºåˆ—æ¨èç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬å°†å…¬å¼€ä»£ç å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åºåˆ—æ¨èç³»ç»Ÿæ—¨åœ¨åŸºäºç”¨æˆ·å†å²äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èã€‚</li>
<li>ç°æœ‰åºåˆ—æ¨èç³»ç»Ÿå­˜åœ¨ä¸ªæ€§åŒ–æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ–°çš„èŒƒå¼â€”â€”åå¥½è¾¨åˆ«æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç”¨æˆ·åå¥½ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°åºåˆ—æ¨èç³»ç»Ÿçš„åå¥½è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>å½“å‰å…ˆè¿›æ–¹æ³•åœ¨å‡†ç¡®è¾¨åˆ«ç”¨æˆ·åå¥½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61ed96decf60a1e589ef18daa7655bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59fa7d1b83e1d1accb0dc4597b2f1772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e64ce1c732f27adb0a7a700843471db3.jpg" align="middle">
</details>




<h2 id="Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden"><a href="#Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden" class="headerlink" title="Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?"></a>Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?</h2><p><strong>Authors:Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro</strong></p>
<p>By means of pilot experiments for the language pair German and Galician, this paper examines the concept of efficiency and intelligence in lexicography and artificial intelligence, AI. The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained. Both quantitative and qualitative methods are used for this purpose. The analysis is based on the evaluation of the outputs of several sessions with the same prompt in ChatGPT 3.5. On the one hand, the algorithmic performance of intelligent systems is evaluated in comparison with data from lexicographical works. On the other hand, the ChatGPT data supplied is analysed using specific text passages of the aforementioned lexicographical text type. The results of this study not only help to evaluate the efficiency of this chatbot regarding the creation of dictionary articles, but also to delve deeper into the concept of intelligence, the thought processes and the actions to be carried out in both disciplines. </p>
<blockquote>
<p>æœ¬æ–‡é€šè¿‡é’ˆå¯¹å¾·è¯­å’ŒåŠ åˆ©è¥¿äºšè¯­è¯­è¨€å¯¹çš„è¯•ç‚¹å®éªŒï¼Œæ¢è®¨äº†è¯å…¸å­¦ã€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸­çš„æ•ˆç‡å’Œæ™ºèƒ½æ¦‚å¿µã€‚è¿™äº›å®éªŒçš„ç›®çš„æ˜¯å®è¯åœ°ã€ç»Ÿè®¡åœ°äº†è§£ChatGPT 3.5å¯¹è¯å…¸æ–‡æœ¬ç±»å‹ï¼ˆè¯å…¸è¯æ¡ï¼‰çš„å“åº”ï¼Œä»¥åŠè¯¥èŠå¤©æœºå™¨äººæ‰€è®­ç»ƒçš„è¯å…¸æ•°æ®ã€‚ä¸ºæ­¤ç›®çš„ï¼Œä½¿ç”¨äº†å®šé‡å’Œå®šæ€§æ–¹æ³•ã€‚åˆ†ææ˜¯åŸºäºå¯¹ChatGPT 3.5ä¸­ç›¸åŒæç¤ºä¸‹å¤šæ¬¡ä¼šè¯è¾“å‡ºçš„è¯„ä¼°è¿›è¡Œçš„ã€‚ä¸€æ–¹é¢ï¼Œæ™ºèƒ½ç³»ç»Ÿçš„ç®—æ³•æ€§èƒ½ä¸è¯å…¸ä½œå“çš„æ•°æ®è¿›è¡Œè¯„ä¼°æ¯”è¾ƒã€‚å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨ä¸Šè¿°è¯å…¸æ–‡æœ¬ç±»å‹çš„ç‰¹å®šæ–‡æœ¬æ®µè½åˆ†æChatGPTæä¾›çš„æ•°æ®ã€‚æœ¬ç ”ç©¶çš„ç»“æœä¸ä»…æœ‰åŠ©äºè¯„ä¼°è¯¥èŠå¤©æœºå™¨äººåœ¨åˆ›å»ºè¯å…¸æ–‡ç« æ–¹é¢çš„æ•ˆç‡ï¼Œè€Œä¸”æœ‰åŠ©äºæ›´æ·±å…¥åœ°äº†è§£æ™ºèƒ½çš„æ¦‚å¿µã€æ€ç»´è¿‡ç¨‹å’Œè¿™ä¸¤ä¸ªå­¦ç§‘ä¸­éœ€è¦æ‰§è¡Œçš„æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08599v1">PDF</a> 25 pages, in German language</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é€šè¿‡å¾·è¯­å’ŒåŠ åˆ©è¥¿äºšè¯­çš„è¯•ç‚¹å®éªŒï¼Œæ¢è®¨äº†è¯å…¸ç¼–çº‚ä¸äººå·¥æ™ºèƒ½ä¸­çš„æ•ˆç‡ä¸æ™ºèƒ½æ¦‚å¿µã€‚å®éªŒæ—¨åœ¨å®è¯å’Œç»Ÿè®¡åœ°åˆ†æChatGPT 3.5å¯¹äºè¯å…¸æ¡ç›®çš„å›åº”ï¼Œå¹¶ç ”ç©¶è¯¥èŠå¤©æœºå™¨äººæ‰€è®­ç»ƒçš„è¯å…¸æ•°æ®ã€‚ç ”ç©¶é‡‡ç”¨äº†å®šé‡å’Œå®šæ€§æ–¹æ³•ï¼Œé€šè¿‡å¯¹åŒä¸€æç¤ºä¸‹ChatGPT 3.5çš„å¤šè½®è¾“å‡ºè¿›è¡Œè¯„ä¼°ï¼Œå¯¹æ¯”æ™ºèƒ½ç³»ç»Ÿçš„ç®—æ³•æ€§èƒ½ä¸è¯å…¸ä½œå“çš„æ•°æ®ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨ç‰¹å®šçš„è¯å…¸æ–‡æœ¬ç±»å‹å¯¹ChatGPTçš„æ•°æ®è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚æœ¬ç ”ç©¶ç»“æœä¸ä»…æœ‰åŠ©äºè¯„ä¼°è¯¥èŠå¤©æœºå™¨äººåœ¨åˆ›å»ºè¯å…¸æ¡ç›®æ–¹é¢çš„æ•ˆç‡ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†æ™ºèƒ½ã€æ€ç»´è¿‡ç¨‹å’Œè¡ŒåŠ¨åœ¨è¯å…¸ç¼–çº‚ä¸äººå·¥æ™ºèƒ½ä¸¤ä¸ªé¢†åŸŸä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é€šè¿‡å¾·è¯­å’ŒåŠ åˆ©è¥¿äºšè¯­çš„è¯•ç‚¹å®éªŒï¼Œæ¢ç´¢äº†è¯å…¸ç¼–çº‚å’Œäººå·¥æ™ºèƒ½ä¸­çš„æ•ˆç‡ä¸æ™ºèƒ½ã€‚</li>
<li>å®éªŒæ—¨åœ¨å®è¯åˆ†æChatGPT 3.5å¯¹è¯å…¸æ¡ç›®çš„å›åº”ä»¥åŠå…¶æ‰€è®­ç»ƒçš„è¯å…¸æ•°æ®ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å®šé‡å’Œå®šæ€§æ–¹æ³•ï¼Œé‡ç‚¹è¯„ä¼°äº†ChatGPT 3.5çš„å¤šè½®è¾“å‡ºã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†æ™ºèƒ½ç³»ç»Ÿçš„ç®—æ³•æ€§èƒ½ä¸è¯å…¸ä½œå“çš„æ•°æ®ã€‚</li>
<li>ç‰¹å®šçš„è¯å…¸æ–‡æœ¬ç±»å‹è¢«ç”¨äºæ·±å…¥åˆ†æChatGPTçš„æ•°æ®ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºè¯„ä¼°èŠå¤©æœºå™¨äººåœ¨åˆ›å»ºè¯å…¸æ¡ç›®æ–¹é¢çš„æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-175b29133a78d0d939af8cbd1fa0ec67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ad5e61c6be29db174ba7ef06c55f62.jpg" align="middle">
</details>




<h2 id="Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks"><a href="#Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks" class="headerlink" title="Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks"></a>Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks</h2><p><strong>Authors:Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi</strong></p>
<p>Ensuring that Software Requirements Specifications (SRS) align with higher-level organizational or national requirements is vital, particularly in regulated environments such as finance and aerospace. In these domains, maintaining consistency, adhering to regulatory frameworks, minimizing errors, and meeting critical expectations are essential for the reliable functioning of systems. The widespread adoption of large language models (LLMs) highlights their immense potential, yet there remains considerable scope for improvement in retrieving relevant information and enhancing reasoning capabilities. This study demonstrates that integrating a robust Graph-RAG framework with advanced prompt engineering techniques, such as Chain of Thought and Tree of Thought, can significantly enhance performance. Compared to baseline RAG methods and simple prompting strategies, this approach delivers more accurate and context-aware results. While this method demonstrates significant improvements in performance, it comes with challenges. It is both costly and more complex to implement across diverse contexts, requiring careful adaptation to specific scenarios. Additionally, its effectiveness heavily relies on having complete and accurate input data, which may not always be readily available, posing further limitations to its scalability and practicality. </p>
<blockquote>
<p>ç¡®ä¿è½¯ä»¶éœ€æ±‚è§„æ ¼ï¼ˆSRSï¼‰ä¸æ›´é«˜å±‚æ¬¡çš„ç»„ç»‡æˆ–å›½å®¶è¦æ±‚ç›¸ä¸€è‡´æ˜¯è‡³å…³é‡è¦çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èå’Œèˆªç©ºç­‰å—ç›‘ç®¡çš„ç¯å¢ƒä¸­ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œä¿æŒä¸€è‡´æ€§ã€éµå®ˆç›‘ç®¡æ¡†æ¶ã€å‡å°‘é”™è¯¯å’Œæ»¡è¶³å…³é”®æœŸæœ›æ˜¯ç³»ç»Ÿå¯é è¿è¡Œçš„å…³é”®ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨çªæ˜¾äº†å…¶å·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯å’Œæé«˜æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œå°†ç¨³å¥çš„å›¾-RAGæ¡†æ¶ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ˆå¦‚æ€ç»´é“¾å’Œæ€ç»´æ ‘ï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ä¸åŸºçº¿RAGæ–¹æ³•å’Œç®€å•æç¤ºç­–ç•¥ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç»“æœã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†å®ƒä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨å¤šç§èƒŒæ™¯ä¸‹å®æ–½æ—¢æ˜‚è´µåˆæ›´å¤æ‚ï¼Œéœ€è¦è°¨æ…é€‚åº”ç‰¹å®šåœºæ™¯ã€‚æ­¤å¤–ï¼Œå…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºå®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®è¾“å…¥ï¼Œè¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯è½»æ˜“å¯ç”¨ï¼Œè¿›ä¸€æ­¥é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•ç¡®ä¿è½¯ä»¶éœ€æ±‚è§„æ ¼ï¼ˆSRSï¼‰ä¸ç»„ç»‡æˆ–å›½å®¶çš„é«˜å±‚æ¬¡éœ€æ±‚ä¸€è‡´çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èå’Œèˆªç©ºç­‰å—ç›‘ç®¡çš„ç¯å¢ƒä¸­çš„é‡è¦æ€§ã€‚åŒæ—¶ï¼Œæ–‡ç« å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æé«˜ä¿¡æ¯æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›åŠä»æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†ä¸€ä¸ªç¨³å¥çš„Graph-RAGæ¡†æ¶ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ç›¸ç»“åˆï¼Œå¦‚é“¾å¼æ€ç»´å’Œæ ‘çŠ¶æ€ç»´ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†å…¶æˆæœ¬é«˜ã€å®ç°å¤æ‚ä¸”æ¶‰åŠå¤šæ ·æƒ…å¢ƒé€‚åº”æ€§è°ƒæ•´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦æ‹¥æœ‰å®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®è¾“å…¥ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è§è§£çš„è¦ç‚¹æ‘˜è¦ï¼š</p>
<ol>
<li>è½¯ä»¶éœ€æ±‚è§„æ ¼ä¸é«˜å±‚çº§ç»„ç»‡æˆ–å›½å®¶è¦æ±‚çš„å¯¹é½åœ¨é‡‘èå’Œèˆªç©ºç­‰é¢†åŸŸæå…¶é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ä½†ä»éœ€æ”¹è¿›ä»¥æ»¡è¶³å…·ä½“éœ€æ±‚ã€‚</li>
<li>é›†æˆGraph-RAGæ¡†æ¶ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯å¯ä»¥æé«˜æ€§èƒ½ï¼Œå¦‚é“¾å¼æ€ç»´å’Œæ ‘çŠ¶æ€ç»´æ–¹æ³•ã€‚</li>
<li>ä¸åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æä¾›äº†æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç»“æœã€‚ç„¶è€Œï¼Œå…¶æˆæœ¬é«˜ä¸”å®ç°å¤æ‚ã€‚</li>
<li>è¯¥æ–¹æ³•éœ€è¦é€‚åº”ä¸åŒçš„æƒ…å¢ƒï¼Œå¹¶å…·æœ‰åº”å¯¹å¤šç§æƒ…å†µçš„å¤æ‚æ€§æŒ‘æˆ˜ã€‚å®ƒçš„æˆåŠŸåº”ç”¨éœ€è°¨æ…å®æ–½ç‰¹å®šåœºæ™¯çš„ç­–ç•¥å’Œè°ƒæ•´ã€‚</li>
<li>æ–¹æ³•çš„æœ‰æ•ˆæ€§å–å†³äºæ‹¥æœ‰å®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®è¾“å…¥ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚åœ¨å®é™…åº”ç”¨ä¸­éœ€æ³¨æ„æ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7979fb6e8cc3e979d2e01584d841f3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc73788341c65dd14908806ee3ec036d.jpg" align="middle">
</details>




<h2 id="Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning"><a href="#Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning" class="headerlink" title="Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning"></a>Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning</h2><p><strong>Authors:Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang</strong></p>
<p>Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance. </p>
<blockquote>
<p>ç¼–ç å™¨æ¨¡å‹ï¼ˆä¾‹å¦‚BERTã€RoBERTaï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œä¾‹å¦‚Llama3ï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…³äºåŸºäºç¼–ç å™¨çš„æ¨¡å‹å’ŒLLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„æ€§èƒ½å¯¹æ¯”çš„ç³»ç»Ÿæ€§ç ”ç©¶ä»ç„¶ç¼ºä¹ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¾®è°ƒæ—¶ã€‚æœ¬ç ”ç©¶é‡‡ç”¨äº†ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„å¤šç§æ¨¡å‹å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬å¾®è°ƒå’Œé¢„å…ˆè®­ç»ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆè¯„ä¼°äº†è¿™äº›LLMåœ¨20æ–°é—»ç»„ï¼ˆ20NGï¼‰å’ŒMASSIVEæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ä»…ä½¿ç”¨ç¼–ç å™¨çš„RoBERTaæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸¤ç§æ¨¡å‹ç±»å‹çš„å¤šä»»åŠ¡åŠŸèƒ½ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ„å›¾æ£€æµ‹å’Œæ§½å¡«å……ï¼‰ï¼Œä½¿ç”¨è¿™ä¸¤ä¸ªæ•°æ®é›†çš„æ•°æ®æ„å»ºå•ä¸€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå®Œå…¨å¾®è°ƒåçš„Llama3-70Bæ¨¡å‹åœ¨å„ç§åˆ†ç±»ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºRoBERTaå¤§å‹æ¨¡å‹å’Œå…¶ä»–è§£ç å™¨LLMã€‚æ­¤å¤–ï¼Œç»è¿‡æ•´åˆçš„å¤šä»»åŠ¡å¾®è°ƒLLMåœ¨è·¨æ•°æ®é›†çš„ä¸¤ä¸ªä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸åŒæ¨¡å‹è®¾ç½®ç›¸åŒ¹é…ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†å…³äºä»…ç¼–ç å™¨å’ŒLLMæ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§å°†ä¸¤ä¸ªæˆ–å¤šä¸ªå®Œå…¨è°ƒæ ¡çš„è§£ç å™¨LLMç»“åˆèµ·æ¥ï¼Œä»¥å‡å°‘å»¶è¿Ÿå¹¶ç»´æŒç­‰æ•ˆæ€§èƒ½çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08587v1">PDF</a> 9 pages, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¯¹æ¯”ç ”ç©¶äº†ç¼–ç å™¨æ¨¡å‹ï¼ˆå¦‚BERTã€RoBERTaï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼Œå¦‚Llama3ï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç ”ç©¶é‡‡ç”¨å¤šç§è§„æ¨¡å’Œæ¶æ„çš„æ¨¡å‹å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬å¾®è°ƒå’Œé¢„å…ˆè®­ç»ƒçš„æ–¹æ³•ï¼Œåœ¨20Newsgroupsï¼ˆ20NGï¼‰å’ŒMASSIVEæ•°æ®é›†ä¸Šè¯„ä¼°äº†LLMsä¸RoBERTaçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢ç´¢äº†ä¸¤ç§æ¨¡å‹ç±»å‹çš„å¤šä»»åŠ¡èƒ½åŠ›ï¼Œé€šè¿‡å°†å¤šä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æ„å›¾æ£€æµ‹å’Œæ§½å¡«å……ï¼‰æ•´åˆåˆ°å•ä¸€æ¨¡å‹ä¸­ã€‚ç»“æœæ˜¾ç¤ºï¼Œå®Œå…¨å¾®è°ƒåçš„Llama3-70Bæ¨¡å‹åœ¨å„é¡¹åˆ†ç±»ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºRoBERTa-largeå’Œå…¶ä»–è§£ç å™¨LLMsã€‚åŒæ—¶ï¼Œå¤šä»»åŠ¡å¾®è°ƒåçš„LLMsåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸åŒæ¨¡å‹è®¾ç½®ç›¸åŒ¹é…ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡æä¾›äº†ç¼–ç å™¨æ¨¡å‹å’ŒLLMåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•ç»“åˆä¸¤ä¸ªæˆ–å¤šä¸ªå®Œå…¨å¾®è°ƒåçš„è§£ç å™¨LLMsä»¥å®ç°é™ä½å»¶è¿Ÿå’Œç­‰æ•ˆæ€§èƒ½çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”äº†ç¼–ç å™¨æ¨¡å‹ï¼ˆå¦‚BERTã€RoBERTaï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼Œå¦‚Llama3ï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆåŒ…æ‹¬20Newsgroupså’ŒMASSIVEï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ã€‚</li>
<li>LLMsåœ¨å„é¡¹åˆ†ç±»ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°é€šå¸¸ä¼˜äºRoBERTaã€‚</li>
<li>å®Œå…¨å¾®è°ƒåçš„Llama3-70Bæ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¤šä»»åŠ¡èƒ½åŠ›æ–¹é¢ï¼Œå¤šä»»åŠ¡å¾®è°ƒåçš„LLMsè¡¨ç°è‰¯å¥½ï¼Œå¯ä¸åŒæ¨¡å‹è®¾ç½®ç›¸åŒ¹é…ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†ç»“åˆå¤šä¸ªå®Œå…¨å¾®è°ƒåçš„è§£ç å™¨LLMsçš„æ–¹æ³•ï¼Œä»¥å®ç°é™ä½å»¶è¿Ÿå’Œç­‰æ•ˆæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78a24e96e4b407822203e3c32af5b7ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c65aecbe11c6d8d21a7daee52179b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd99579b4a6b1de138e0cc90d02460d.jpg" align="middle">
</details>




<h2 id="TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs"><a href="#TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs" class="headerlink" title="TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs"></a>TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs</h2><p><strong>Authors:Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.   We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®æ³¨æ„åŠ›æœºåˆ¶æ–¹é¢ã€‚è™½ç„¶é‡åŒ–æŠ€æœ¯å’ŒåŠ é€Ÿç®—æ³•ï¼ˆå¦‚FlashAttentionï¼‰å·²ç»æé«˜äº†æ•´ä½“æ¨ç†çš„æ•ˆç‡ï¼Œä½†å®ƒä»¬è§£å†³çš„é—®é¢˜ä¸åŒï¼šé‡åŒ–å…³æ³¨æƒé‡æ¿€æ´»æ“ä½œï¼Œè€ŒFlashAttentionæ”¹è¿›äº†æ‰§è¡Œä½†éœ€è¦é«˜ç²¾åº¦æ ¼å¼ã€‚æœ€è¿‘çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é‡åŒ–å‡å°‘äº†å†…å­˜å¸¦å®½ï¼Œä½†ä»ç„¶éœ€è¦æµ®ç‚¹åé‡åŒ–è¿›è¡Œæ³¨æ„åŠ›æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº†TurboAttentionï¼Œè¿™æ˜¯ä¸€ç§å…¨é¢çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°å¯¹æ³¨æ„åŠ›çš„é‡åŒ–æ‰§è¡Œï¼ŒåŒæ—¶è§£å†³å†…å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šFlashQï¼Œä¸€ç§é€å¤´æ³¨æ„åŠ›é‡åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°KVç¼“å­˜çš„å‹ç¼©å’Œæ¿€æ´»æ¿€æ´»ä¹˜æ³•çš„é‡åŒ–æ‰§è¡Œï¼›ä»¥åŠåŸºäºç¨€ç–æ€§çš„Softmaxè¿‘ä¼¼ï¼ˆSASï¼‰ï¼Œå®ƒæ¶ˆé™¤äº†åœ¨æ³¨æ„åŠ›æŒ‡æ•°è¿ç®—æœŸé—´åé‡åŒ–åˆ°FP32çš„éœ€è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTurboAttentionå®ç°äº†æ³¨æ„åŠ›é€Ÿåº¦çš„1.2-1.8å€æå‡ï¼Œå‡å°‘äº†KVç¼“å­˜å¤§å°è¶…è¿‡4.4å€ï¼Œå¹¶ä¸”åœ¨å„ç§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šç›¸å¯¹äºFP16åŸºçº¿å®ç°äº†æœ€é«˜è¾¾2.37å€çš„æœ€å¤§ååé‡ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„é‡åŒ–å’Œå‹ç¼©æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMçš„æ¨ç†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®æ³¨æ„åŠ›æœºåˆ¶æ–¹é¢ã€‚ç°æœ‰çš„æŠ€æœ¯å¦‚é‡åŒ–å’ŒFlashAttentionç­‰è™½æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œä½†ä»å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºTurboAttentionï¼Œä¸€ç§ç»¼åˆçš„æ³¨æ„åŠ›é‡åŒ–æ‰§è¡Œæ–¹æ³•ï¼ŒåŒæ—¶è§£å†³å†…å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚TurboAttentionåŒ…æ‹¬ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯ï¼šFlashQç”¨äºå®ç°KVç¼“å­˜å‹ç¼©å’Œé‡åŒ–æ‰§è¡Œæ¿€æ´»ä¹˜æ³•ï¼Œä»¥åŠåŸºäºç¨€ç–æ€§çš„Softmaxè¿‘ä¼¼ï¼ˆSASï¼‰ï¼Œæ¶ˆé™¤æ³¨æ„åŠ›æŒ‡æ•°è¿ç®—ä¸­çš„å»é‡åŒ–éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºTurboAttentionåœ¨æ³¨æ„åŠ›æ–¹é¢å®ç°äº†1.2-1.8å€çš„é€Ÿåº¦æå‡ï¼ŒKVç¼“å­˜å¤§å°å‡å°‘è¶…è¿‡4.4å€ï¼ŒåŒæ—¶åœ¨å„ç§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¶…è¶Šäº†ç°æœ‰é‡åŒ–æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMæ¨ç†ä¾èµ–äºå¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸Šã€‚</li>
<li>é‡åŒ–å’ŒåŠ é€Ÿç®—æ³•å¦‚FlashAttentionå·²æé«˜æ¨ç†æ•ˆç‡ï¼Œä½†ä»å­˜åœ¨ä¸è¶³ã€‚</li>
<li>TurboAttentionæ˜¯ä¸€ç§ç»¼åˆæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ³¨æ„åŠ›çš„é‡åŒ–æ‰§è¡Œï¼ŒåŒæ—¶æé«˜å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>TurboAttentionåŒ…æ‹¬ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼šFlashQç”¨äºKVç¼“å­˜å‹ç¼©å’Œæ¿€æ´»ä¹˜æ³•çš„é‡åŒ–æ‰§è¡Œï¼Œä»¥åŠSASæ¶ˆé™¤æ³¨æ„åŠ›è¿ç®—ä¸­çš„å»é‡åŒ–æ­¥éª¤ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜TurboAttentionåœ¨æ³¨æ„åŠ›å¤„ç†ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œå¹¶å¤§å¹…å‡å°äº†KVç¼“å­˜å¤§å°ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2aa13ec6d967123c700eac65289299ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c9472feee293a58946daa11dab8b79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5d83ed70042472422f235eb4d579530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f134495953af18158cd020ded942ff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40958ea88bed04942910eb4a10c27a9c.jpg" align="middle">
</details>




<h2 id="Can-We-Generate-Visual-Programs-Without-Prompting-LLMs"><a href="#Can-We-Generate-Visual-Programs-Without-Prompting-LLMs" class="headerlink" title="Can We Generate Visual Programs Without Prompting LLMs?"></a>Can We Generate Visual Programs Without Prompting LLMs?</h2><p><strong>Authors:Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem</strong></p>
<p>Visual programming prompts LLMs (large language mod-els) to generate executable code for visual tasks like visual question answering (VQA). Prompt-based methods are difficult to improve while also being unreliable and costly in both time and money. Our goal is to develop an efficient visual programming system without 1) using prompt-based LLMs at inference time and 2) a large set of program and answer annotations. We develop a synthetic data augmentation approach and alternative program generation method based on decoupling programs into higher-level skills called templates and the corresponding arguments. Our results show that with data augmentation, prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive with state-of-the art models with the added benefit of much faster inference </p>
<blockquote>
<p>è§†è§‰ç¼–ç¨‹æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè§†è§‰ä»»åŠ¡ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€‚åŸºäºæç¤ºçš„æ–¹æ³•å¾ˆéš¾æ”¹è¿›ï¼ŒåŒæ—¶ä¸å¯é ï¼Œæ—¶é—´å’Œé‡‘é’±æˆæœ¬éƒ½å¾ˆé«˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ä¸ä½¿ç”¨åŸºäºæç¤ºçš„LLMè¿›è¡Œæ¨æ–­çš„æƒ…å†µä¸‹ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨å¤§é‡çš„ç¨‹åºå’Œç­”æ¡ˆæ³¨é‡Šæ¥å¼€å‘é«˜æ•ˆçš„è§†è§‰ç¼–ç¨‹ç³»ç»Ÿã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»¥åŠåŸºäºå°†ç¨‹åºè§£è€¦ä¸ºé«˜çº§æŠ€èƒ½ï¼ˆç§°ä¸ºæ¨¡æ¿ï¼‰å’Œç›¸åº”å‚æ•°çš„æ›¿ä»£ç¨‹åºç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ•°æ®å¢å¼ºï¼Œæ— éœ€æç¤ºçš„å°å‹LLMï¼ˆçº¦1Bå‚æ•°ï¼‰ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¿«çš„æ¨ç†ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰ç¼–ç¨‹æç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”Ÿæˆç”¨äºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ï¼‰çš„å¯æ‰§è¡Œä»£ç ã€‚ä½†æç¤ºå‹æ–¹æ³•æ—¢éš¾ä»¥æ”¹è¿›ï¼Œåˆå­˜åœ¨å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä¸”æ¶ˆè€—å¤§é‡æ—¶é—´å’Œæˆæœ¬ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§é«˜æ•ˆçš„è§†è§‰ç¼–ç¨‹ç³»ç»Ÿï¼Œä¸ä½¿ç”¨æç¤ºå‹LLMè¿›è¡Œæ¨ç†ï¼Œä¸”æ— éœ€å¤§é‡ç¨‹åºå’Œç­”æ¡ˆæ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆæˆæ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å°†ç¨‹åºè§£è€¦ä¸ºé«˜çº§æŠ€èƒ½ï¼ˆç§°ä¸ºæ¨¡æ¿ï¼‰å’Œç›¸åº”å‚æ•°æ¥ç”Ÿæˆæ›¿ä»£ç¨‹åºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ•°æ®å¢å¼ºï¼Œæ— éœ€æç¤ºçš„å°å‹LLMä¸å½“å‰å…ˆè¿›æ¨¡å‹ç›¸å½“ï¼Œå¹¶å…·å¤‡æ›´å¿«çš„æ¨ç†ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ç¼–ç¨‹æç¤ºä½¿LLMèƒ½å¤Ÿç”Ÿæˆç”¨äºè§†è§‰ä»»åŠ¡çš„ä»£ç ã€‚</li>
<li>æç¤ºå‹æ–¹æ³•å­˜åœ¨æ”¹è¿›å›°éš¾ã€å¯é æ€§ä¸è¶³å’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>ç›®æ ‡å¼€å‘ä¸€ç§é«˜æ•ˆçš„è§†è§‰ç¼–ç¨‹ç³»ç»Ÿï¼Œä¸ä½¿ç”¨æç¤ºå‹LLMè¿›è¡Œæ¨ç†ã€‚</li>
<li>æå‡ºåŸºäºåˆæˆæ•°æ®å¢å¼ºçš„æ–¹æ³•æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å°†ç¨‹åºè§£è€¦ä¸ºæ¨¡æ¿å’Œå‚æ•°æ¥ç”Ÿæˆæ›¿ä»£ç¨‹åºã€‚</li>
<li>æ•°æ®å¢å¼ºä½¿å°å‹LLMè¡¨ç°ä¸å…ˆè¿›æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71a052fea07471cdd5b92ce33f76a2e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-495a2ed1307492f0e4d3b1fb89c1e5e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2871162a30b0025d43f44b1d6134418f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09544747a6fdbe241269e89ec78cea69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64a390556423516931dab19b62b4f26c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87feed1c478cec6f5d341f9548563b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d023468a4290179e5a74db0a79de7ee3.jpg" align="middle">
</details>




<h2 id="Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning"><a href="#Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning" class="headerlink" title="Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning"></a>Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning</h2><p><strong>Authors:Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora KreaÄiÄ‡, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</strong></p>
<p>Large Language Models are trained on extensive datasets that often contain sensitive, human-generated information, raising significant concerns about privacy breaches. While certified unlearning approaches offer strong privacy guarantees, they rely on restrictive model assumptions that are not applicable to LLMs. As a result, various unlearning heuristics have been proposed, with the associated privacy risks assessed only empirically. The standard evaluation pipelines typically randomly select data for removal from the training set, apply unlearning techniques, and use membership inference attacks to compare the unlearned models against models retrained without the to-be-unlearned data. However, since every data point is subject to the right to be forgotten, unlearning should be considered in the worst-case scenario from the privacy perspective. Prior work shows that data outliers may exhibit higher memorization effects. Intuitively, they are harder to be unlearn and thus the privacy risk of unlearning them is underestimated in the current evaluation. In this paper, we leverage minority data to identify such a critical flaw in previously widely adopted evaluations. We substantiate this claim through carefully designed experiments, including unlearning canaries related to minority groups, inspired by privacy auditing literature. Using personally identifiable information as a representative minority identifier, we demonstrate that minority groups experience at least 20% more privacy leakage in most cases across six unlearning approaches, three MIAs, three benchmark datasets, and two LLMs of different scales. Given that the right to be forgotten should be upheld for every individual, we advocate for a more rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation framework represents an initial step toward ensuring more equitable assessments of LLM unlearning efficacy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ•°æ®é›†å¹¿æ³›ä¸”å¸¸åŒ…å«æ•æ„Ÿçš„äººç±»ç”Ÿæˆä¿¡æ¯ï¼Œå¼•å‘äº†å…³äºéšç§æ³„éœ²çš„é‡å¤§æ‹…å¿§ã€‚è™½ç„¶ç»è¿‡è®¤è¯çš„é—å¿˜å¤„ç†æ–¹æ³•æä¾›äº†å¼ºå¤§çš„éšç§ä¿è¯ï¼Œä½†å®ƒä»¬ä¾èµ–äºä¸é€‚ç”¨äºLLMçš„æ¨¡å‹å‡è®¾ã€‚å› æ­¤ï¼Œå·²ç»æå‡ºäº†å„ç§é—å¿˜å¯å‘å¼æ–¹æ³•ï¼Œå¹¶å¯¹ç›¸å…³çš„éšç§é£é™©è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚æ ‡å‡†çš„è¯„ä¼°æµç¨‹é€šå¸¸ä»è®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©æ•°æ®è¿›è¡Œåˆ é™¤ï¼Œåº”ç”¨é—å¿˜æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨æˆå‘˜æ¨ç†æ”»å‡»æ¥æ¯”è¾ƒæœªå­¦ä¹ çš„æ¨¡å‹ä¸é‡æ–°è®­ç»ƒçš„æ¨¡å‹ï¼ˆæ— éœ€é—å¿˜æ•°æ®ï¼‰ã€‚ç„¶è€Œï¼Œç”±äºæ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰è¢«é—å¿˜çš„æƒåˆ©ï¼Œä»éšç§çš„è§’åº¦æ¥çœ‹ï¼Œé—å¿˜åº”åœ¨æœ€åçš„æƒ…å†µä¸‹è¿›è¡Œè€ƒè™‘ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®å¼‚å¸¸å€¼å¯èƒ½è¡¨ç°å‡ºæ›´é«˜çš„è®°å¿†æ•ˆæœã€‚ç›´è§‰ä¸Šï¼Œå®ƒä»¬æ›´éš¾è¢«é—å¿˜ï¼Œå› æ­¤å½“å‰è¯„ä¼°ä½ä¼°äº†é—å¿˜å®ƒä»¬çš„éšç§é£é™©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å°‘æ•°ç¾¤ä½“æ•°æ®æ¥è¯†åˆ«å…ˆå‰å¹¿æ³›é‡‡ç”¨çš„è¯„ä¼°ä¸­çš„è¿™ä¸€å…³é”®ç¼ºé™·ã€‚æˆ‘ä»¬é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å®éªŒè¯å®äº†è¿™ä¸€ä¸»å¼ ï¼ŒåŒ…æ‹¬ä¸å°‘æ•°ç¾¤ä½“ç›¸å…³çš„é—å¿˜ä¿¡æ ‡ï¼ˆcanaryï¼‰ï¼Œè¿™æ˜¯å—éšç§å®¡è®¡æ–‡çŒ®å¯å‘çš„ã€‚ä»¥ä¸ªäººèº«ä»½ä¿¡æ¯ä½œä¸ºä»£è¡¨æ€§çš„å°‘æ•°ç¾¤ä½“æ ‡è¯†ç¬¦ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåœ¨å…­ç§é—å¿˜æ–¹æ³•ã€ä¸‰ç§æˆå‘˜æ¨ç†æ”»å‡»ã€ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå°‘æ•°ç¾¤ä½“çš„éšç§æ³„éœ²è‡³å°‘å¢åŠ äº†20%ã€‚é‰´äºæ¯ä¸ªä¸ªä½“éƒ½æœ‰è¢«é—å¿˜çš„æƒåˆ©ï¼Œæˆ‘ä»¬ä¸»å¼ å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜æ–¹æ³•è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„å°‘æ•°ç¾¤ä½“æ„è¯†è¯„ä¼°æ¡†æ¶æ˜¯ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹é—å¿˜æ•ˆç‡è¯„ä¼°æ›´åŠ å…¬å¹³çš„ç¬¬ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08559v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ•°æ®åŒ…å«å¤§é‡æ•æ„Ÿçš„äººç±»ç”Ÿæˆä¿¡æ¯ï¼Œå¼•å‘äº†å…³äºéšç§æ³„éœ²çš„æ‹…å¿§ã€‚è™½ç„¶ç»è¿‡è®¤è¯çš„é—å¿˜æ–¹æ³•æä¾›å¼ºæœ‰åŠ›çš„éšç§ä¿éšœï¼Œä½†å®ƒä»¬ä¾èµ–äºä¸é€‚ç”¨äºLLMçš„æ¨¡å‹å‡è®¾ã€‚å› æ­¤ï¼Œäººä»¬æå‡ºäº†å„ç§é—å¿˜å¯å‘å¼æ–¹æ³•ï¼Œå¹¶å¯¹ç›¸å…³çš„éšç§é£é™©è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚æ ‡å‡†è¯„ä¼°æµç¨‹é€šå¸¸éšæœºé€‰æ‹©æ•°æ®è¿›è¡Œè®­ç»ƒé›†åˆ é™¤ï¼Œåº”ç”¨é—å¿˜æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨æˆå‘˜æ¨ç†æ”»å‡»æ¥æ¯”è¾ƒæœªå­¦ä¹ çš„æ¨¡å‹ä¸ä¸ä½¿ç”¨å¾…é—å¿˜æ•°æ®çš„é‡æ–°è®­ç»ƒçš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œä»éšç§è§’åº¦æ¥çœ‹ï¼Œç”±äºæ¯ä¸ªæ•°æ®ç‚¹éƒ½æœ‰è¢«é—å¿˜çš„æƒåˆ©ï¼Œå› æ­¤åº”è€ƒè™‘æœ€åçš„æƒ…å†µè¿›è¡Œé—å¿˜ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®å¼‚å¸¸å€¼å¯èƒ½è¡¨ç°å‡ºæ›´é«˜çš„è®°å¿†æ•ˆåº”ï¼Œå³å®ƒä»¬æ›´éš¾è¢«é—å¿˜ï¼Œå› æ­¤å½“å‰è¯„ä¼°ä¸­çš„éšç§é£é™©è¢«ä½ä¼°äº†ã€‚æœ¬æ–‡åˆ©ç”¨å°‘æ•°æ•°æ®æ¥è¯†åˆ«ä»¥å‰å¹¿æ³›é‡‡ç”¨çš„è¯„ä¼°ä¸­çš„è¿™ä¸€å…³é”®ç¼ºé™·ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸å°‘æ•°ç¾¤ä½“ç›¸å…³çš„é—å¿˜é‡‘ä¸é›€ï¼ˆå—éšç§å®¡è®¡æ–‡çŒ®å¯å‘ï¼‰ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå°‘æ•°ç¾¤ä½“è‡³å°‘ä¼šç»å†20%ä»¥ä¸Šçš„éšç§æ³„éœ²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸»å¼ å¯¹LLMçš„é—å¿˜æ–¹æ³•è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„é¢å‘å°‘æ•°çš„è¯„ä¼°æ¡†æ¶æ˜¯ç¡®ä¿æ›´å…¬å¹³åœ°è¯„ä¼°LLMé—å¿˜æ•ˆç‡çš„ç¬¬ä¸€æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ¶‰åŠæ•æ„Ÿä¿¡æ¯çš„éšç§é—®é¢˜ã€‚</li>
<li>ç°æœ‰é—å¿˜æŠ€æœ¯ä¸»è¦åŸºäºæ¨¡å‹å‡è®¾ï¼Œå¯èƒ½ä¸é€‚ç”¨äºLLMã€‚</li>
<li>æ ‡å‡†è¯„ä¼°æµç¨‹ä¸»è¦å…³æ³¨éšæœºæ•°æ®åˆ é™¤å’Œæˆå‘˜æ¨ç†æ”»å‡»çš„æ¯”è¾ƒï¼Œä½†æœªå……åˆ†è€ƒè™‘æœ€åæƒ…å†µçš„é—å¿˜ã€‚</li>
<li>æ•°æ®å¼‚å¸¸å€¼ï¼ˆå³å°‘æ•°ç¾¤ä½“ï¼‰åœ¨é—å¿˜è¿‡ç¨‹ä¸­å¯èƒ½å…·æœ‰æ›´é«˜çš„éšç§æ³„éœ²é£é™©ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜ï¼Œå°‘æ•°ç¾¤ä½“åœ¨é—å¿˜è¿‡ç¨‹ä¸­è‡³å°‘é¢ä¸´20%ä»¥ä¸Šçš„éšç§æ³„éœ²é£é™©ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•æœªèƒ½å……åˆ†è¯„ä¼°LLMé—å¿˜æ–¹æ³•çš„æ•ˆæœå’Œå…¬å¹³æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d7df70e264508e34d7458412e0bc8a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-612dcf7498baba74ceee6359bb90c887.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c0407d01398c0d835aecbd08661ead9.jpg" align="middle">
</details>




<h2 id="MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback"><a href="#MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback" class="headerlink" title="MaestroMotif: Skill Design from Artificial Intelligence Feedback"></a>MaestroMotif: Skill Design from Artificial Intelligence Feedback</h2><p><strong>Authors:Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca Dâ€™Oro</strong></p>
<p>Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLMâ€™s feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLMâ€™s code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability. </p>
<blockquote>
<p>æè¿°è‡ªç„¶è¯­è¨€ä¸­çš„æŠ€èƒ½å…·æœ‰å°†äººç±»å…³äºå†³ç­–åˆ¶å®šçš„çŸ¥è¯†æ³¨å…¥äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†MaestroMotifæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½è¾…åŠ©æŠ€èƒ½è®¾è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜æ€§èƒ½å’Œå¯é€‚åº”çš„ä»£ç†ã€‚MaestroMotifåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½æ¥æœ‰æ•ˆåœ°åˆ›å»ºå’Œé‡ç”¨æŠ€èƒ½ã€‚å®ƒé¦–å…ˆä½¿ç”¨LLMçš„åé¦ˆæ¥è‡ªåŠ¨è®¾è®¡å¯¹åº”äºæ¯ä¸ªæŠ€èƒ½çš„å¥–åŠ±ï¼Œä»ä»–ä»¬çš„è‡ªç„¶è¯­è¨€æè¿°å¼€å§‹ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨LLMçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå¯¹æŠ€èƒ½è¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†å®ƒä»¬ç»„åˆèµ·æ¥å®ç°ç”¨è¯­è¨€æŒ‡å®šçš„å¤æ‚è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨NetHackå­¦ä¹ ç¯å¢ƒï¼ˆNLEï¼‰çš„ä¸€ç³»åˆ—å¤æ‚ä»»åŠ¡ä¸­è¯„ä¼°äº†MaestroMotifï¼Œç»“æœè¡¨æ˜å®ƒåœ¨æ€§èƒ½å’Œå¯ç”¨æ€§æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08542v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MaestroMotifæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§AIè¾…åŠ©æŠ€èƒ½è®¾è®¡æ–¹å¼ï¼Œèƒ½ç”Ÿæˆé«˜æ€§èƒ½ä¸”èƒ½é€‚åº”ä¸åŒæƒ…å¢ƒçš„AIä»£ç†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æŠ€èƒ½çš„èƒ½åŠ›ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡ç›¸åº”æŠ€èƒ½çš„å¥–åŠ±ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡ŒæŠ€èƒ½åŸ¹è®­å’Œå¤æ‚è¡Œä¸ºçš„å®ç°ã€‚åœ¨NetHackå­¦ä¹ ç¯å¢ƒçš„å¤æ‚ä»»åŠ¡æµ‹è¯•ä¸­ï¼ŒMaestroMotifè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaestroMotifæ˜¯ä¸€ç§AIè¾…åŠ©æŠ€èƒ½è®¾è®¡æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°æŠ€èƒ½çš„æœ‰æ•ˆåˆ›å»ºå’Œå†åˆ©ç”¨ã€‚</li>
<li>MaestroMotifé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æŠ€èƒ½ï¼Œè‡ªåŠ¨è®¾è®¡ç›¸åº”æŠ€èƒ½çš„å¥–åŠ±ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡ŒæŠ€èƒ½åŸ¹è®­ï¼Œå¹¶èƒ½å®ç°å¤æ‚çš„è¡Œä¸ºã€‚</li>
<li>MaestroMotifåœ¨NetHackå­¦ä¹ ç¯å¢ƒçš„å¤æ‚ä»»åŠ¡æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MaestroMotifæ³¨é‡æŠ€èƒ½çš„é€‚åº”æ€§å’Œæ€§èƒ½çš„ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ŒMaestroMotifå¯ä»¥æ³¨å…¥äººç±»å†³ç­–çŸ¥è¯†åˆ°AIç³»ç»Ÿä¸­ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f665febf0a88f026fb77766c9392837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-628a7f2a23e6422217c4fab0551ffeae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a055c6f27078f47409556b8e37ea80d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3b8bb5f78ec046bbf77ef4055438e59.jpg" align="middle">
</details>




<h2 id="EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance"><a href="#EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance" class="headerlink" title="EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance"></a>EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance</h2><p><strong>Authors:Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang</strong></p>
<p>As large language models (LLMs) continue to advance, the demand for higher quality and faster processing of long contexts across various applications is growing. KV cache is widely adopted as it stores previously generated key and value tokens, effectively reducing redundant computations during inference. However, as memory overhead becomes a significant concern, efficient compression of KV cache has gained increasing attention. Most existing methods perform compression from two perspectives: identifying important tokens and designing compression strategies. However, these approaches often produce biased distributions of important tokens due to the influence of accumulated attention scores or positional encoding. Furthermore, they overlook the sparsity and redundancy across different heads, which leads to difficulties in preserving the most effective information at the head level. To this end, we propose EMS to overcome these limitations, while achieving better KV cache compression under extreme compression ratios. Specifically, we introduce a Global-Local score that combines accumulated attention scores from both global and local KV tokens to better identify the token importance. For the compression strategy, we design an adaptive and unified Evict-then-Merge framework that accounts for the sparsity and redundancy of KV tokens across different heads. Additionally, we implement the head-wise parallel compression through a zero-class mechanism to enhance efficiency. Extensive experiments demonstrate our SOTA performance even under extreme compression ratios. EMS consistently achieves the lowest perplexity, improves scores by over 1.28 points across four LLMs on LongBench under a 256 cache budget, and preserves 95% retrieval accuracy with a cache budget less than 2% of the context length in the Needle-in-a-Haystack task. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­è¿›æ­¥ï¼Œå¯¹è·¨å„ç§åº”ç”¨çš„é«˜è´¨é‡å’Œå¿«é€Ÿå¤„ç†é•¿æ–‡æœ¬çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚KVç¼“å­˜å› å…¶èƒ½å¤Ÿå­˜å‚¨å…ˆå‰ç”Ÿæˆçš„é”®å’Œå€¼ä»¤ç‰Œè€Œå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—ã€‚ç„¶è€Œï¼Œå†…å­˜å¼€é”€æˆä¸ºä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼ŒKVç¼“å­˜çš„æœ‰æ•ˆå‹ç¼©ä¹Ÿå¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä»ä¸¤ä¸ªè§’åº¦è¿›è¡Œå‹ç¼©ï¼šè¯†åˆ«é‡è¦ä»¤ç‰Œå’Œè®¾è®¡å‹ç¼©ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿç”±äºç´¯ç§¯çš„æ³¨æ„åŠ›åˆ†æ•°æˆ–ä½ç½®ç¼–ç çš„å½±å“è€Œäº§ç”Ÿçš„é‡è¦ä»¤ç‰Œçš„åå‘åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¿½è§†äº†ä¸åŒå¤´ä¹‹é—´çš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ï¼Œè¿™ä½¿å¾—åœ¨å¤´éƒ¨å±‚é¢ä¿ç•™æœ€æœ‰æ•ˆä¿¡æ¯å˜å¾—å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºEMSæ¥å…‹æœè¿™äº›å±€é™æ€§ï¼ŒåŒæ—¶å®ç°åœ¨æç«¯å‹ç¼©æ¯”ç‡ä¸‹æ›´å¥½çš„KVç¼“å­˜å‹ç¼©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨åˆ†æ•°ï¼Œè¯¥åˆ†æ•°ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨KVä»¤ç‰Œä¸Šçš„ç´¯ç§¯æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ä»¤ç‰Œçš„é‡è¦æ€§ã€‚å¯¹äºå‹ç¼©ç­–ç•¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”çš„ã€ç»Ÿä¸€çš„é€å‡ºåˆå¹¶æ¡†æ¶ï¼ˆEvict-then-Mergeï¼‰ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†ä¸åŒå¤´ä¹‹é—´KVä»¤ç‰Œçš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é›¶ç±»æœºåˆ¶å®ç°äº†å¤´éƒ¨å¹¶è¡Œå‹ç¼©ï¼Œä»¥æé«˜æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æç«¯å‹ç¼©æ¯”ç‡ä¸‹ï¼Œæˆ‘ä»¬çš„æ€§èƒ½ä¹Ÿè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚EMSåœ¨æç«¯å‹ç¼©æ¯”ç‡ä¸‹å§‹ç»ˆå®ç°äº†æœ€ä½çš„å›°æƒ‘åº¦ï¼Œåœ¨LongBenchä¸Šçš„å››ä¸ªLLMå¾—åˆ†æé«˜äº†è¶…è¿‡1.28ç‚¹ï¼Œåœ¨Haystackä»»åŠ¡ä¸­çš„Needle-in-a-Haystackåœºæ™¯ä¸‹ï¼Œåœ¨ç¼“å­˜é¢„ç®—å°‘äºä¸Šä¸‹æ–‡é•¿åº¦çš„2%çš„æƒ…å†µä¸‹ä¿ç•™äº†95%çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08521v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­KVç¼“å­˜çš„å‹ç¼©é—®é¢˜ã€‚éšç€LLMçš„ä¸æ–­å‘å±•ï¼Œå¯¹é«˜è´¨é‡ã€å¿«é€Ÿå¤„ç†é•¿ä¸Šä¸‹æ–‡çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œè€ŒKVç¼“å­˜ç”±äºèƒ½å¤Ÿå­˜å‚¨å…ˆå‰ç”Ÿæˆçš„é”®å’Œå€¼ä»¤ç‰Œï¼Œæœ‰æ•ˆå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—è€Œå—åˆ°å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œå†…å­˜å¼€é”€æˆä¸ºä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼Œå› æ­¤KVç¼“å­˜çš„å‹ç¼©å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„å±€é™æ€§ï¼Œå¦‚è¯†åˆ«é‡è¦ä»¤ç‰Œæ—¶çš„åè§åˆ†å¸ƒã€å¿½ç•¥ä¸åŒå¤´éƒ¨ä¹‹é—´çš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ç­‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•EMSã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨ä»¤ç‰Œçš„é‡è¦æ€§å¾—åˆ†æ¥è¯†åˆ«é‡è¦ä»¤ç‰Œï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”ç»Ÿä¸€çš„é©±é€åˆå¹¶æ¡†æ¶è¿›è¡Œå‹ç¼©ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æç«¯å‹ç¼©æ¯”ç‡ä¸‹ï¼ŒEMSä¹Ÿèƒ½å®ç°æ›´å¥½çš„KVç¼“å­˜å‹ç¼©æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„å‘å±•æ¨åŠ¨äº†KVç¼“å­˜å‹ç¼©çš„éœ€æ±‚ã€‚</li>
<li>KVç¼“å­˜å­˜å‚¨é”®å’Œå€¼ä»¤ç‰Œï¼Œå‡å°‘å†—ä½™è®¡ç®—ã€‚</li>
<li>ç°æœ‰KVç¼“å­˜å‹ç¼©æ–¹æ³•å­˜åœ¨è¯†åˆ«é‡è¦ä»¤ç‰Œæ—¶çš„åè§åˆ†å¸ƒé—®é¢˜ã€‚</li>
<li>EMSé€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨ä»¤ç‰Œçš„é‡è¦æ€§å¾—åˆ†æ¥è¯†åˆ«é‡è¦ä»¤ç‰Œã€‚</li>
<li>EMSé‡‡ç”¨è‡ªé€‚åº”ç»Ÿä¸€çš„é©±é€åˆå¹¶æ¡†æ¶è¿›è¡Œå‹ç¼©ï¼Œè€ƒè™‘ä¸åŒå¤´éƒ¨ä¹‹é—´çš„ç¨€ç–æ€§å’Œå†—ä½™æ€§ã€‚</li>
<li>EMSå®ç°äº†é«˜æ•ˆçš„å¤´å¹¶è¡Œå‹ç¼©ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38b496138c97e1dfd0f692985690d782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a51c48e21df8dbfe8058c928883ebc9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f36fbc21c25055b288075b463776d2e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d840c002b790302fa195ba1f62a5dacd.jpg" align="middle">
</details>




<h2 id="Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation"><a href="#Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation" class="headerlink" title="Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation"></a>Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation</h2><p><strong>Authors:Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang</strong></p>
<p>The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction. </p>
<blockquote>
<p>é‡æ’å™¨å’Œç”Ÿæˆå™¨æ˜¯å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆå³RAGï¼‰ç®¡é“ä¸­çš„ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œè´Ÿè´£æ’åˆ—ç›¸å…³æ–‡æ¡£å’Œç”Ÿæˆç­”å¤ã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒæ•°æ®å’Œç›®æ ‡çš„ä¸åŒï¼Œé‡æ’å™¨æ’åˆ—çš„ç›¸å…³æ–‡æ¡£å’Œç”Ÿæˆå™¨æ”¯æŒå›ç­”é—®é¢˜æ‰€éœ€çš„æ–‡æ¡£ä¹‹é—´å­˜åœ¨ä¸å¯é¿å…çš„å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RADIOï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ç†æ€§è’¸é¦ï¼ˆRAtionale DIstillatiOnï¼‰çš„æ–°å‹å®ç”¨åå¥½å¯¹é½æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ¥æå–ç”¨äºå›ç­”æŸ¥è¯¢æ‰€éœ€çš„ä¾æ®çš„ç†æ€§æå–æ–¹æ³•ã€‚éšåï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºç†æ€§çš„å¯¹é½è¿‡ç¨‹ï¼Œæ ¹æ®æå–çš„ä¾æ®é‡æ–°æ’åˆ—æ–‡æ¡£ï¼Œå¹¶é€šè¿‡å¾®è°ƒé‡æ’å™¨æ¥å¯¹é½åå¥½ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†çš„ä¸¤ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨çº¿å‘å¸ƒï¼Œä»¥æ–¹ä¾¿å¤åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08519v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­ï¼Œæ’åå™¨å’Œç”Ÿæˆå™¨ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä½œç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¸¤è€…åœ¨é¢„è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä¸Šçš„å·®å¼‚å¯¼è‡´çš„å·®è·é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRADIOçš„æ–°å‹åå¥½å¯¹é½æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æå–å›ç­”æŸ¥è¯¢æ‰€éœ€çš„ç†ç”±ï¼ŒåŸºäºè¿™äº›ç†ç”±é‡æ–°æ’åæ–‡æ¡£ï¼Œå¹¶å¾®è°ƒæ’åå™¨ä»¥å¯¹é½åå¥½ã€‚é€šè¿‡ä¸¤ä¸ªä»»åŠ¡åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RANKERå’Œç”Ÿæˆå™¨æ˜¯RAGç®¡é“ä¸­çš„å…³é”®ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£æ’åç›¸å…³æ–‡æ¡£å’Œç”Ÿæˆå“åº”ã€‚</li>
<li>ç”±äºé¢„è®­ç»ƒæ•°æ®å’Œç›®æ ‡çš„ä¸åŒï¼ŒRANKERå’Œç”Ÿæˆå™¨ä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºRADIOçš„æ–°å‹åå¥½å¯¹é½æ¡†æ¶æ¥è§£å†³è¿™ä¸€å·®è·é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æå–å›ç­”æŸ¥è¯¢çš„ç†ç”±ã€‚</li>
<li>åŸºäºæå–çš„ç†ç”±é‡æ–°æ’åæ–‡æ¡£ï¼Œå¹¶å¾®è°ƒRANKERä»¥å¯¹é½åå¥½ã€‚</li>
<li>åœ¨ä¸¤ä¸ªä»»åŠ¡ã€ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†RADIOæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-07bd96bd382f50d4b0be40a29e0ad2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ff74513b2d945c44687fdee91eb0ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e8a25f53efe8135ab5000dfaaa6a7b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5ab945745fa9b9f0e527f229d00a670.jpg" align="middle">
</details>




<h2 id="POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications"><a href="#POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications" class="headerlink" title="POINTS1.5: Building a Vision-Language Model towards Real World   Applications"></a>POINTS1.5: Building a Vision-Language Model towards Real World   Applications</h2><p><strong>Authors:Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, Jie Zhou</strong></p>
<p>Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹è¿‘æœŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¾‹å¦‚å…‰å­¦å­—ç¬¦è¯†åˆ«å’Œå¤æ‚å›¾è¡¨åˆ†æã€‚åŸºäºè¿™ä¸€è¶‹åŠ¿ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹â€”â€”POINTS1.5ï¼Œæ—¨åœ¨åœ¨å„ç§ç°å®åº”ç”¨ä¸­è·å¾—å“è¶Šè¡¨ç°ã€‚POINTS1.5æ˜¯POINTS1.0çš„å¢å¼ºç‰ˆï¼Œå¹¶èå…¥äº†è‹¥å¹²å…³é”®åˆ›æ–°ï¼šä¸€ï¼‰æˆ‘ä»¬æ›¿æ¢äº†åŸå§‹çš„CLIPè§†è§‰ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å…·æœ‰å›ºå®šå›¾åƒåˆ†è¾¨ç‡ï¼Œé‡‡ç”¨NaViTé£æ ¼çš„è§†è§‰ç¼–ç å™¨ï¼Œæ”¯æŒåŸç”ŸåŠ¨æ€é«˜åˆ†è¾¨ç‡ã€‚è¿™ä½¿å¾—POINTS1.5èƒ½å¤Ÿå¤„ç†ä»»ä½•åˆ†è¾¨ç‡çš„å›¾åƒï¼Œè€Œæ— éœ€å°†å®ƒä»¬åˆ†å‰²æˆç“¦ç‰‡ã€‚äºŒï¼‰æˆ‘ä»¬ä¸ºPOINTS1.5å¢åŠ äº†åŒè¯­æ”¯æŒï¼Œè¿™æå¤§åœ°æé«˜äº†å…¶åœ¨ä¸­æ–‡æ–¹é¢çš„èƒ½åŠ›ã€‚ç”±äºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡å¼€æºæ•°æ®é›†ç¨€ç¼ºï¼Œæˆ‘ä»¬ä»äº’è”ç½‘æ”¶é›†äº†å¤§é‡å›¾åƒï¼Œå¹¶ç»“åˆæ‰‹åŠ¨å’Œè‡ªåŠ¨æ–¹æ³•è¿›è¡Œæ ‡æ³¨ã€‚ä¸‰ï¼‰æˆ‘ä»¬ä¸ºè§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†æå‡ºäº†ä¸€ç³»åˆ—ä¸¥æ ¼çš„è¿‡æ»¤æ–¹æ³•ã€‚æˆ‘ä»¬å…¨é¢è¯„ä¼°äº†æ‰€æœ‰è¿™äº›è¿‡æ»¤æ–¹æ³•ï¼Œå¹¶é€‰æ‹©äº†æœ€æœ‰æ•ˆçš„è¿‡æ»¤æ–¹æ³•æ¥è·å¾—æœ€ç»ˆçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´é›†ã€‚å¾—ç›Šäºè¿™äº›åˆ›æ–°ï¼ŒPOINTS1.5åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†POINTS1.0ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—ç°å®åº”ç”¨ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPOINTS1.5-7Båœ¨å°‘äº4äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å‚æ•°å°‘äº10äº¿çš„æ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼Œè£ç™»OpenCompassæ’è¡Œæ¦œé¦–ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08443v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæ–°å‹è§†è§‰è¯­è¨€æ¨¡å‹POINTS1.5åŸºäºä»¥å¾€æ¨¡å‹çš„ä¼˜åŠ¿è¿›è¡Œåˆ›æ–°ï¼Œé€šè¿‡æ”¯æŒåŠ¨æ€é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†å’Œå¢å¼ºä¸­æ–‡æ”¯æŒç­‰åŠŸèƒ½ï¼Œæå‡äº†åœ¨å„ç§å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡ä¸¥æ ¼ç­›é€‰è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å…ˆè¿›çš„è¿‡æ»¤æ–¹æ³•ï¼Œå…¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†å‰ä»£æ¨¡å‹POINTS1.0ï¼Œè¡¨ç°å¼ºåŠ²ã€‚åŒæ—¶ï¼ŒPOINTS1.5åœ¨è®­ç»ƒæ•°æ®é‡æ–¹é¢ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨å°‘äº4äº¿ä»¤ç‰Œçš„è®­ç»ƒä¸‹è¾¾åˆ°ä¼˜ç§€æ€§èƒ½ï¼Œä¸”åœ¨OpenCompassæ’è¡Œæ¦œä¸ŠåŒç±»æ¨¡å‹å‚æ•°æ•°é‡çº§å†…è¡¨ç°ååˆ—å‰èŒ…ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>POINTS1.5æ˜¯æ–°ä¸€ä»£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºå…ˆå‰æ¨¡å‹POINTsç³»åˆ—è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥NaViTé£æ ¼çš„åŠ¨æ€é«˜åˆ†è¾¨ç‡è§†è§‰ç¼–ç å™¨ï¼Œæå‡äº†å›¾åƒå¤„ç†èƒ½åŠ›ã€‚</li>
<li>POINTS1.5æ”¯æŒåŒè¯­ï¼Œç‰¹åˆ«æ˜¯å¢å¼ºäº†ä¸­æ–‡å¤„ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³ä¸­æ–‡æ•°æ®é›†ç¨€ç¼ºé—®é¢˜ï¼Œè¯¥æ¨¡å‹æ”¶é›†å¹¶æ ‡æ³¨äº†å¤§é‡äº’è”ç½‘å›¾åƒã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä¸¥æ ¼çš„è¿‡æ»¤æ–¹æ³•ç­›é€‰è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œå¹¶è¿›è¡Œç»¼åˆè¯„ä¼°é€‰æ‹©æœ€æœ‰æ•ˆçš„è¿‡æ»¤æ–¹æ³•è·å¾—æœ€ç»ˆæ•°æ®é›†ã€‚</li>
<li>POINTS1.5åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šå‰ä»£æ¨¡å‹POINTS1.0ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®é‡ä¼˜åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc8c0e0a9e3ea04dd5e87e92c23e234d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e46b078851b25402b8004072ad51698.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d7787c431966427ede1048c8a9a6984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2581b57e819a19b0d70e0fc3e8455559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ade5bc08b53234389efb475173eb8d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14763de648f4e6faa4d40ae81fa249ac.jpg" align="middle">
</details>




<h2 id="The-Roles-of-English-in-Evaluating-Multilingual-Language-Models"><a href="#The-Roles-of-English-in-Evaluating-Multilingual-Language-Models" class="headerlink" title="The Roles of English in Evaluating Multilingual Language Models"></a>The Roles of English in Evaluating Multilingual Language Models</h2><p><strong>Authors:Wessel Poelman, Miryam de Lhoneux</strong></p>
<p>Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from this imprecise method and instead focus on furthering language understanding. </p>
<blockquote>
<p>å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œé’ˆå¯¹å¤šç§è¯­è¨€å‘å¸ƒäº†ä¼—å¤šæ¨¡å‹ã€åŸºå‡†æµ‹è¯•æ–¹æ³•å’Œæ‰‹æ®µã€‚åœ¨è·¨è¯­è¨€è¯„ä¼°ä¸­ï¼Œè‹±è¯­å¸¸è¢«ç”¨æ¥æç¤ºè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ï¼Œè¿™ä¸»è¦æ˜¯ä¸ºäº†å…‹æœå…¶ä»–è¯­è¨€æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„ç¼ºä¹ã€‚åœ¨è¿™ç¯‡ç«‹åœºè®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é˜è¿°äº†è‹±è¯­åœ¨è·¨è¯­è¨€LMè¯„ä¼°ä¸­çš„ä¸¤ä¸ªä½œç”¨ï¼šä½œä¸ºæ¥å£å’Œä½œä¸ºè‡ªç„¶è¯­è¨€ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸¤ä¸ªä½œç”¨çš„ç›®æ ‡ä¸åŒï¼šä»»åŠ¡æ‰§è¡Œä¸è¯­è¨€ç†è§£ã€‚é€šè¿‡æ•°æ®é›†å’Œè¯„ä¼°è®¾ç½®çš„ç¤ºä¾‹ï¼Œè¿™ä¸ªå·®å¼‚å¾—åˆ°äº†å¼ºè°ƒã€‚è®¸å¤šä½œå“æ˜ç¡®åœ°ç”¨è‹±è¯­ä½œä¸ºæ¥å£æ¥æå‡ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚æˆ‘ä»¬å»ºè®®æ‘’å¼ƒè¿™ç§ä¸ç²¾ç¡®çš„æ–¹æ³•ï¼Œè½¬è€Œä¸“æ³¨äºæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08392v1">PDF</a> NoDaLiDa 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤šç§è¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œè‹±è¯­åœ¨å¤šè¯­è¨€è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä¸»è¦ä½“ç°åœ¨ä½œä¸ºæ¥å£å’Œè‡ªç„¶è¯­è¨€ä¸¤ä¸ªæ–¹é¢ã€‚å°½ç®¡å…¶åœ¨ä»»åŠ¡æ€§èƒ½ä¸Šçš„è¡¨ç°å¤‡å—å…³æ³¨ï¼Œä½†å…¶è¯­è¨€ç†è§£èƒ½åŠ›çš„å·®å¼‚ä¹Ÿå°¤ä¸ºé‡è¦ã€‚æ–‡ä¸­é€šè¿‡å…·ä½“æ•°æ®é›†å’Œè¯„ä¼°å®ä¾‹å±•ç¤ºäº†è¿™ä¸€å·®å¼‚ã€‚æœ¬æ–‡å‘¼åè½¬å‘å…³æ³¨è¯­è¨€ç†è§£ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ä»¥è‹±è¯­ä½œä¸ºæé«˜ä»»åŠ¡æ€§èƒ½çš„æ¥å£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‹±è¯­åœ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç”¨äºä¿ƒè¿›è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„å¤šè¯­è¨€è¯„ä¼°ã€‚</li>
<li>è‹±è¯­åœ¨è¯„ä¼°ä¸­çš„ä¸¤ç§è§’è‰²æ˜¯ï¼šä½œä¸ºæ¥å£å’Œä½œä¸ºè‡ªç„¶è¯­è¨€ã€‚</li>
<li>ä½¿ç”¨è‹±è¯­ä½œä¸ºæ¥å£ä¸»è¦ç”¨äºæé«˜ä»»åŠ¡æ€§èƒ½ï¼Œä½†å…¶å­˜åœ¨å±€é™æ€§ï¼Œæœªèƒ½å……åˆ†ä½“ç°è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>é‡è§†è¯­è¨€ç†è§£èƒ½åŠ›çš„æå‡è€Œéä»…ä»…ä»¥è‹±è¯­ä¸ºæ¥å£çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°çš„é‡è¦æ€§ã€‚</li>
<li>å½“å‰è®¸å¤šå·¥ä½œåˆ©ç”¨è‹±è¯­ä½œä¸ºç•Œé¢æ¨åŠ¨ä»»åŠ¡æ‰§è¡Œçš„æ–¹å¼æœ‰å¾…æ”¹è¿›ã€‚</li>
<li>è‹±è¯­ä½œä¸ºè¯„ä¼°è¯­è¨€çš„ä¸€éƒ¨åˆ†å…·æœ‰å¤æ‚çš„åŒé‡è§’è‰²å’Œå¤šç§ç›®çš„è€ƒé‡ã€‚éœ€è¦æ›´åŠ æ³¨é‡ä»¥ä¸åŒè§†è§’è¿›è¡Œè¯„ä¼°çš„ç­–ç•¥ç ”ç©¶å’Œå®è·µåˆ›æ–°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e4913fb529f06a6d8ce3be82bddd61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-425a52c1120dea05a7467888bdb16f84.jpg" align="middle">
</details>




<h2 id="SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs"><a href="#SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs" class="headerlink" title="SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs"></a>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs</h2><p><strong>Authors:Sultan Alrashed</strong></p>
<p>We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAIâ€™s Tulu 3 post-training pipeline to enhance Huggingfaceâ€™s SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval ($\Delta$11%), and mathematical reasoning with 51.6% on GSM8K ($\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC ($\Delta5.4%$). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºSmolTulu-1.7b-Instructï¼Œæœ¬æŠ¥å‘Šå°†å…¶ç§°ä¸ºSmolTulu-DPO-1130ï¼Œè¿™æ˜¯ä¸€æ¬¾ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€‚åº”äº†AllenAIçš„Tulu 3åè®­ç»ƒç®¡é“ï¼Œä»¥å¼ºåŒ–Huggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨1.35äº¿å‚æ•°æ¨¡å‹çš„å…¨é¢å®è¯åˆ†æè¯æ˜ï¼Œå­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°ä¹‹é—´çš„å…³ç³»ä¼šä»¥ä¸€ç§ä»»åŠ¡ä¾èµ–çš„æ–¹å¼æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸€ä¸ªæ˜ç¡®çš„åˆ’åˆ†ï¼šæ¨ç†ä»»åŠ¡ï¼ˆå¦‚ARCå’ŒGSM8Kï¼‰å—ç›Šäºè¾ƒé«˜çš„å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°çš„æ¯”ä¾‹ï¼Œè€Œæ¨¡å¼è¯†åˆ«ä»»åŠ¡ï¼ˆå¦‚HellaSwagå’ŒIFevalï¼‰åœ¨è¾ƒä½çš„æ¯”ä¾‹ä¸‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚è¿™äº›è§è§£ä¸ºSmolTuluçš„å¼€å‘æä¾›äº†æŒ‡å¯¼ï¼ŒSmolTuluåœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢è¾¾åˆ°äº†å°äº2Bå‚æ•°æ¨¡å‹ä¸­çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨IFevalä¸Šå¾—åˆ†67.7%ï¼ˆæé«˜äº†11%ï¼‰ï¼Œåœ¨GSM8Kæ•°å­¦æ¨ç†ä¸Šå¾—åˆ†51.6%ï¼ˆæé«˜äº†3.4%ï¼‰ï¼Œå¦ä¸€ç‰ˆæœ¬åœ¨ARCä¸Šå¾—åˆ†57.1%ï¼ˆæé«˜äº†5.4%ï¼‰ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€è®­ç»ƒé…æ–¹å’Œæ¶ˆèç ”ç©¶ï¼Œä»¥ä¿ƒè¿›å¯¹é«˜æ•ˆæ¨¡å‹å¯¹é½çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œè¡¨æ˜ä¼˜åŒ–åŠ¨åŠ›å­¦çš„ä»”ç»†è°ƒæ•´æœ‰åŠ©äºç¼©å°å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„èƒ½åŠ›å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08347v1">PDF</a> 10 pages, 4 figures, and 13 tables. For the SmolTulu-1.7b-instruct   model, see: <a target="_blank" rel="noopener" href="https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct">https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct</a></p>
<p><strong>Summary</strong></p>
<p>SmolTulu-DPO-1130æ¨¡å‹æ˜¯åŸºäºHuggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡é‡‡ç”¨AllenAIçš„Tulu 3åè®­ç»ƒç®¡é“è¿›è¡Œé€‚åº”ï¼Œå¼€å‘äº†ä¸€ç§æŒ‡ä»¤ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶å‘ç°å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°ä¹‹é—´çš„å…³ç³»å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„æœ€ä¼˜æ¯”ç‡ã€‚SmolTuluæ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨ç›¸å…³æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmolTulu-DPO-1130æ˜¯åŸºäºHuggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹å¼€å‘çš„ä¸€ç§æŒ‡ä»¤ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨äº†AllenAIçš„Tulu 3åè®­ç»ƒç®¡é“è¿›è¡Œé€‚åº”ã€‚</li>
<li>å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°çš„å…³ç³»å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„æœ€ä¼˜æ¯”ç‡ã€‚</li>
<li>åœ¨æŒ‡ä»¤éµå¾ªå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒSmolTuluæ¨¡å‹å–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-24a7f844f882bf1f6c5dd15898647b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cd5849efc9a4186ec9b7fd8c1be2dc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da3951edd3c067eb7bc6364e65cd3c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36b8936c38972af9d7874a2416d73f91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b4385e2bbae30872b51b25a5ff9ad95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9194cf58ba8f617b42c5e6436bb8b20f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a4ac62ce8ba497822f4f90331bb613.jpg" align="middle">
</details>




<h2 id="Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training"><a href="#Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training" class="headerlink" title="Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training"></a>Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training</h2><p><strong>Authors:Shijian Wang, Linxin Song, Jieyu Zhang, Ryotaro Shimizu, Ao Luo, Li Yao, Cunjian Chen, Julian McAuley, Hanqian Wu</strong></p>
<p>Current multimodal language models (MLMs) evaluation and training approaches overlook the influence of instruction format, presenting an elephant-in-the-room problem. Previous research deals with this problem by manually crafting instructions, failing to yield significant insights due to limitations in diversity and scalability. In this work, we propose a programmatic instruction template generator capable of producing over 39B unique template combinations by filling randomly sampled positional synonyms into weighted sampled meta templates, enabling us to comprehensively examine the MLMâ€™s performance across diverse instruction templates. Our experiments across eight common MLMs on five benchmark datasets reveal that MLMs have high template sensitivities with at most 29% performance gaps between different templates. We further augment the instruction tuning dataset of LLaVA-1.5 with our template generator and perform instruction tuning on LLaVA-1.5-7B and LLaVA-1.5-13B. Models tuned on our augmented dataset achieve the best overall performance when compared with the same scale MLMs tuned on at most 75 times the scale of our augmented dataset, highlighting the importance of instruction templates in MLM training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a> . </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰è¯„ä¼°å’Œè®­ç»ƒæ–¹æ³•å¿½è§†äº†æŒ‡ä»¤æ ¼å¼çš„å½±å“ï¼Œè¿™å°±åƒä¸€ä¸ªæˆ¿é—´é‡Œçš„å¤§è±¡é—®é¢˜ã€‚ä¹‹å‰çš„ç ”ç©¶é€šè¿‡æ‰‹åŠ¨åˆ¶å®šæŒ‡ä»¤æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºå¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§çš„é™åˆ¶ï¼Œæœªèƒ½äº§ç”Ÿé‡å¤§è§è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå™¨ï¼Œé€šè¿‡å‘åŠ æƒé‡‡æ ·å…ƒæ¨¡æ¿ä¸­å¡«å……éšæœºé‡‡æ ·ä½ç½®åŒä¹‰è¯ï¼Œèƒ½å¤Ÿäº§ç”Ÿè¶…è¿‡3.9ä¸‡äº¿ç§ç‹¬ç‰¹çš„æ¨¡æ¿ç»„åˆï¼Œä»è€Œèƒ½å¤Ÿå…¨é¢è€ƒå¯ŸMLMåœ¨ä¸åŒæŒ‡ä»¤æ¨¡æ¿ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¯¹å…«ä¸ªå¸¸è§MLMè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMLMå¯¹æ¨¡æ¿çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä¸åŒæ¨¡æ¿ä¹‹é—´æ€§èƒ½å·®å¼‚æœ€å¤§è¾¾åˆ°29%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿ç”Ÿæˆå™¨å¢å¼ºäº†LLaVA-1.5çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¹¶åœ¨LLaVA-1.5-7Bå’ŒLLaVA-1.5-13Bä¸Šè¿›è¡Œäº†æŒ‡ä»¤è°ƒæ•´ã€‚ä¸æˆ‘ä»¬å¢å¼ºæ•°æ®é›†ä¸Šè°ƒæ ¡çš„ç›¸åŒè§„æ¨¡MLMç›¸æ¯”ï¼Œæ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šè¾¾åˆ°æœ€ä½³ï¼Œè¿™è¡¨æ˜åœ¨MLMè®­ç»ƒä¸­æŒ‡ä»¤æ¨¡æ¿çš„é‡è¦æ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/shijian2001/TemplateMattersè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08307v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨è¯„ä¼°å’Œè®­ç»ƒè¿‡ç¨‹ä¸­å¿½è§†äº†æŒ‡ä»¤æ ¼å¼çš„å½±å“ï¼Œä¸ºæ­¤æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå™¨ã€‚è¯¥ç”Ÿæˆå™¨é€šè¿‡éšæœºå¡«å……åŠ æƒé‡‡æ ·å…ƒæ¨¡æ¿ä¸­çš„ä½ç½®åŒä¹‰è¯ï¼Œèƒ½å¤Ÿäº§ç”Ÿè¶…è¿‡39äº¿ç§ç‹¬ç‰¹çš„æ¨¡æ¿ç»„åˆï¼Œä»è€Œå…¨é¢è€ƒå¯ŸMLMåœ¨ä¸åŒæŒ‡ä»¤æ¨¡æ¿ä¸‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLMså¯¹æ¨¡æ¿çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä¸åŒæ¨¡æ¿é—´æ€§èƒ½å·®å¼‚æœ€å¤§å¯è¾¾29%ã€‚é€šè¿‡å¢å¼ºLLaVA-1.5çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å¹¶ä½¿ç”¨è¯¥æ¨¡æ¿ç”Ÿæˆå™¨ï¼Œæ¨¡å‹åœ¨LLaVA-1.5-7Bå’ŒLLaVA-1.5-13Bä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ä¸åœ¨æ‰©å¤§è§„æ¨¡çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç›¸åŒè§„æ¨¡çš„MLMsç›¸æ¯”ï¼Œä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œçªæ˜¾äº†æŒ‡ä»¤æ¨¡æ¿åœ¨MLMè®­ç»ƒä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰è¯„ä¼°å’Œè®­ç»ƒå¿½ç•¥äº†æŒ‡ä»¤æ ¼å¼çš„å½±å“ï¼Œè¿™æˆä¸ºäº†ä¸€ä¸ªéœ€è¦å…³æ³¨çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿäº§ç”Ÿå¤§é‡çš„ç‹¬ç‰¹æ¨¡æ¿ç»„åˆï¼Œä»¥å…¨é¢è€ƒå¯ŸMLMçš„æ€§èƒ½ã€‚</li>
<li>MLMså¯¹æ¨¡æ¿çš„æ•æ„Ÿæ€§é«˜ï¼Œä¸åŒæ¨¡æ¿é—´æ€§èƒ½å·®å¼‚è¾ƒå¤§ã€‚</li>
<li>é€šè¿‡å¢å¼ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å¹¶ä½¿ç”¨æå‡ºçš„æ¨¡æ¿ç”Ÿæˆå™¨ï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚</li>
<li>ä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºåœ¨æ‰©å¤§è§„æ¨¡çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç›¸åŒè§„æ¨¡çš„MLMsã€‚</li>
<li>æŒ‡ä»¤æ¨¡æ¿åœ¨MLMè®­ç»ƒä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cf18eab5fd423cf391c7a0d944472c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc8c42e8cb0aed2474be1f549a2daa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92fd609fdd9fbcf040f463966ba2a464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab98d0c0816798cff3762dfe29d4bbdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9158b85ce187a5753e0f5956ebbfa739.jpg" align="middle">
</details>




<h2 id="M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis"><a href="#M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis" class="headerlink" title="M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis"></a>M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis</h2><p><strong>Authors:Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang</strong></p>
<p>Sentiment analysis and emotion recognition are crucial for applications such as human-computer interaction and depression detection. Traditional unimodal methods often fail to capture the complexity of emotional expressions due to conflicting signals from different modalities. Current Multimodal Large Language Models (MLLMs) also face challenges in detecting subtle facial expressions and addressing a wide range of emotion-related tasks. To tackle these issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy for general-purpose MLLMs. It employs a combined approach to train models on tasks such as multimodal sentiment analysis, emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction. We also introduce the Emotion Multitask dataset (EMT), a custom dataset that supports these five tasks. Our model, Emotion Universe (EmoVerse), is built on a basic MLLM framework without modifications, yet it achieves substantial improvements across these tasks when trained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-of-the-art results in sentiment and emotion tasks. These results highlight the effectiveness of M2SE in enhancing multimodal emotion perception. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE">https://github.com/xiaoyaoxinyi/M2SE</a>. </p>
<blockquote>
<p>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨äººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ç”±äºæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡å·ç›¸äº’å†²çªï¼Œå¾€å¾€æ— æ³•æ•æ‰æƒ…ç»ªè¡¨è¾¾çš„å¤æ‚æ€§ã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ£€æµ‹å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…å’Œåº”å¯¹å„ç§æƒ…ç»ªç›¸å…³ä»»åŠ¡æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2SEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé€šç”¨MLLMsçš„å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿå’Œæƒ…ç»ªæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ã€‚å®ƒé‡‡ç”¨ç»„åˆæ–¹æ³•ï¼Œåœ¨è¯¸å¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€æƒ…æ„Ÿæ¨ç†æ¨æ–­å’Œæƒ…ç»ªå› æœå¯¹æå–ç­‰ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†æƒ…æ„Ÿå¤šä»»åŠ¡æ•°æ®é›†ï¼ˆEMTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒè¿™äº”ä¸ªä»»åŠ¡çš„ä¸“ä¸šæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€œæƒ…æ„Ÿå®‡å®™â€ï¼ˆEmoVerseï¼‰å»ºç«‹åœ¨åŸºæœ¬çš„MLLMæ¡†æ¶ä¸Šï¼Œæ— éœ€ä¿®æ”¹ï¼Œä½†åœ¨ä½¿ç”¨M2SEç­–ç•¥è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEmoVerseä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æƒ…æ„Ÿå’Œæƒ…ç»ªä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†M2SEåœ¨æé«˜å¤šæ¨¡æ€æƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaoyaoxinyi/M2SEä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨å¤šæ¨¡æ€äº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰é¢†åŸŸçš„é‡è¦æ€§ã€‚ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•æ— æ³•æ•æ‰æƒ…ç»ªè¡¨è¾¾çš„å¤æ‚æ€§ï¼Œè€Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ£€æµ‹ç»†å¾®é¢éƒ¨è¡¨æƒ…å’Œåº”å¯¹å„ç§æƒ…ç»ªç›¸å…³ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†M2SEï¼Œä¸€ç§ç”¨äºé€šç”¨MLLMsçš„å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿä¸æƒ…ç»ªæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¤šç§ä»»åŠ¡è®­ç»ƒæ¨¡å‹ï¼Œå¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€æƒ…æ„Ÿæ¨ç†æ¨æ–­å’Œæƒ…æ„Ÿå› æœå…³ç³»æå–ç­‰ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†æ”¯æŒè¿™äº”ä¸ªä»»åŠ¡çš„æƒ…æ„Ÿå¤šä»»åŠ¡æ•°æ®é›†EMTã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºM2SEç­–ç•¥è®­ç»ƒçš„æ¨¡å‹Emotion Universeï¼ˆEmoVerseï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œå¹¶åœ¨æƒ…æ„Ÿä¸æƒ…ç»ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚è¿™è¯æ˜äº†M2SEç­–ç•¥åœ¨æé«˜å¤šæ¨¡æ€æƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«å¯¹äºäººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•éš¾ä»¥æ•æ‰æƒ…ç»ªè¡¨è¾¾çš„å¤æ‚æ€§ï¼Œéœ€è¦å¤šæ¨¡æ€æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>M2SEæ˜¯ä¸€ç§ç”¨äºé€šç”¨MLLMsçš„å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿä¸æƒ…ç»ªæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œç»“åˆäº†å¤šç§ä»»åŠ¡è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>M2SEç­–ç•¥åŒ…æ‹¬å¤šä»»åŠ¡å¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ç­‰ã€‚</li>
<li>å¼•å…¥çš„æƒ…æ„Ÿå¤šä»»åŠ¡æ•°æ®é›†EMTæ”¯æŒäº”ä¸ªä»»åŠ¡ï¼Œæœ‰åŠ©äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>åŸºäºM2SEç­–ç•¥è®­ç»ƒçš„æ¨¡å‹Emotion Universeï¼ˆEmoVerseï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa73ededcaefa000e8edfeba52a89b72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c263fb335b04bc682be5a1cfa8e6e297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4d6b6e0f4051342c6a0898d92d999f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32acf009fc023384915a65c05d3f43ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6c980356ed97ed5bc3e5d3fac353bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9184bb36e2f4b1291065f58d8c14d467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b8821a0398494275c6700434720d2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c8749cd2121f800ea47ff15bb3e861f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ab60fd15fbdd7102b7bf0b9ceb0523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f47588ea960ad6d8e96c0eaec02120.jpg" align="middle">
</details>




<h2 id="SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models"><a href="#SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models" class="headerlink" title="SAT: Spatial Aptitude Training for Multimodal Language Models"></a>SAT: Spatial Aptitude Training for Multimodal Language Models</h2><p><strong>Authors:Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko</strong></p>
<p>Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23%$ on CVBench, $8%$ on the harder BLINK benchmark, and $18%$ on VSR. When instruction-tuned on SAT, our 13B model matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data&#x2F;code is available at <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a> . </p>
<blockquote>
<p>ç©ºé—´æ„ŸçŸ¥æ˜¯æ™ºèƒ½çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡è®¸å¤šç ”ç©¶å¼ºè°ƒå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰åœ¨æ¨ç†ç©ºé—´æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†å®ƒä»¬åªæµ‹è¯•é™æ€ç©ºé—´æ¨ç†ï¼Œä¾‹å¦‚å¯¹ç‰©ä½“çš„ç›¸å¯¹ä½ç½®è¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„åº”ç”¨éœ€è¦åŠ¨æ€èƒ½åŠ›ï¼Œå¦‚è§‚ç‚¹é‡‡æ‹©å’Œç¬¬ä¸€äººç§°åŠ¨ä½œè¯†åˆ«ã€‚ä½œä¸ºæé«˜ç©ºé—´æ™ºèƒ½çš„è·¯çº¿å›¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†SATï¼ˆç©ºé—´é€‚åº”èƒ½åŠ›è®­ç»ƒï¼‰ï¼Œå®ƒè¶…è¶Šäº†é™æ€ç›¸å¯¹ç‰©ä½“ä½ç½®é—®é¢˜ï¼Œæ¶µç›–äº†æ›´åŠ¨æ€çš„ä»»åŠ¡ã€‚SATåŒ…å«21.8ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠè®­ç»ƒå’Œæµ‹è¯•é›†çš„2.2ä¸‡ä¸ªåˆæˆåœºæ™¯ã€‚ä½¿ç”¨é€¼çœŸçš„ç‰©ç†å¼•æ“ç”Ÿæˆï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥ä»»æ„æ‰©å±•ï¼Œå¹¶è½»æ¾æ‰©å±•åˆ°æ–°çš„åŠ¨ä½œã€åœºæ™¯å’Œ3Dèµ„äº§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿åœ¨é™æ€é—®é¢˜ä¸Šè¡¨ç°ç›¸å¯¹è¾ƒå¥½çš„MLMä¹Ÿå¾ˆéš¾å‡†ç¡®å›ç­”åŠ¨æ€ç©ºé—´é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒSATæŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸ä»…æé«˜äº†SATä¸Šçš„åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿˜æé«˜äº†ç°æœ‰çœŸå®å›¾åƒç©ºé—´åŸºå‡†çš„é›¶æ ·æœ¬æ€§èƒ½ï¼šCVBenchä¸Šæé«˜23%ï¼Œéš¾åº¦æ›´é«˜çš„BLINKåŸºå‡†ä¸Šæé«˜8%ï¼Œä»¥åŠVSRä¸Šæé«˜18%ã€‚å½“åœ¨SATä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´æ—¶ï¼Œæˆ‘ä»¬çš„13Bæ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¸è¾ƒå¤§çš„ä¸“æœ‰MLMï¼ˆå¦‚GPT4-Vå’ŒGemini-3-1.0ï¼‰ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07755v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç©ºé—´æ„ŸçŸ¥æ˜¯æ™ºèƒ½çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚è®¸å¤šç ”ç©¶å¼ºè°ƒå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨é™æ€ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†ä»…é™äºåˆ†ç±»ç‰©ä½“çš„ç›¸å¯¹ä½ç½®ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„åº”ç”¨éœ€è¦æ›´åŠ¨æ€çš„èƒ½åŠ›ï¼Œå¦‚è§†è§’å’ŒåŸºäºè‡ªæˆ‘ä¸­å¿ƒçš„è¡ŒåŠ¨è¯†åˆ«ç­‰ã€‚ä¸ºäº†æ”¹å–„ç©ºé—´æ™ºèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SATï¼ˆç©ºé—´èƒ½åŠ›è®­ç»ƒï¼‰ï¼Œå®ƒè¶…è¶Šäº†é™æ€ç›¸å¯¹ç‰©ä½“ä½ç½®é—®é¢˜ï¼Œæ¶µç›–äº†æ›´åŠ¨æ€çš„ä»»åŠ¡ã€‚SATåŒ…å«ç”¨äºè®­ç»ƒçš„åˆæˆåœºæ™¯ä¸­çš„é—®ç­”å¯¹ä»¥åŠæµ‹è¯•é›†çš„é—®ç­”å¯¹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯ä½¿ç”¨é€¼çœŸçš„ç‰©ç†å¼•æ“ç”Ÿæˆçš„ï¼Œå¯ä»¥ä»»æ„æ‰©å±•å¹¶è½»æ¾æ‰©å±•åˆ°æ–°çš„åŠ¨ä½œã€åœºæ™¯å’Œä¸‰ç»´èµ„äº§ã€‚æˆ‘ä»¬å‘ç°å³ä½¿åœ¨é™æ€é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½çš„MLMä¹Ÿå¾ˆéš¾å‡†ç¡®å›ç­”åŠ¨æ€ç©ºé—´é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒSATæŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸ä»…æé«˜äº†SATä¸Šçš„åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿˜æé«˜äº†ç°æœ‰çœŸå®å›¾åƒç©ºé—´åŸºå‡†æµ‹è¯•ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ï¼šCVBenchæé«˜23%ï¼Œéš¾åº¦æ›´å¤§çš„BLINKåŸºå‡†æµ‹è¯•æé«˜8%ï¼Œä»¥åŠVSRæé«˜18%ã€‚å½“åœ¨SATä¸Šç»è¿‡æŒ‡ä»¤è°ƒæ•´åï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½åœ¨ç©ºé—´æ¨ç†æ–¹é¢åŒ¹é…è¾ƒå¤§çš„ä¸“æœ‰MLMï¼ˆå¦‚GPT-4 Vå’ŒåŒå­åº§3-1.0ï¼‰ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/%E8%8E%B7%E5%8F%96%E3%80%82">http://arijitray1993.github.io/SAT/è·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€ç©ºé—´æ„ŸçŸ¥æ˜¯æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºç°å®ä¸–ç•Œçš„åº”ç”¨è‡³å…³é‡è¦ã€‚<br>äºŒã€å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨é™æ€ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“ç›¸å¯¹ä½ç½®çš„åˆ†ç±»ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨éœ€è¦æ¨¡å‹å…·å¤‡åŠ¨æ€èƒ½åŠ›ï¼Œå¦‚è§†è§’é‡‡å–å’ŒåŸºäºè‡ªæˆ‘ä¸­å¿ƒçš„è¡ŒåŠ¨è¯†åˆ«ã€‚è¿™å¯ç¤ºæˆ‘ä»¬ä¸ä»…éœ€è¦ç ”ç©¶é™æ€ç©ºé—´æ„ŸçŸ¥ï¼Œè¿˜è¦å…³æ³¨åŠ¨æ€ç©ºé—´æ„ŸçŸ¥çš„ç ”ç©¶ã€‚<br>ä¸‰ã€å¼•å…¥æ–°çš„æ•°æ®é›†SATï¼ˆSpatial Aptitude Trainingï¼‰ï¼ŒåŒ…å«åˆæˆåœºæ™¯ä¸­çš„é—®ç­”å¯¹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½èƒ½åŠ›ã€‚æ•°æ®é›†ä½¿ç”¨é€¼çœŸçš„ç‰©ç†å¼•æ“ç”Ÿæˆï¼Œå¯ä»»æ„æ‰©å±•å¹¶è½»æ¾æ‰©å±•åˆ°æ–°çš„åŠ¨ä½œã€åœºæ™¯å’Œä¸‰ç»´èµ„äº§ã€‚è¿™è¡¨æ˜æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§å¯¹äºè®­ç»ƒæ¨¡å‹è‡³å…³é‡è¦ã€‚</p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4726f5787390ded5869518fbae49fa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1ac83f59cc86700f58e482ccc874082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8ee4e23739632ca4492284e948ea332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db09dd969f25a2ed0382bcf5c8ce13cc.jpg" align="middle">
</details>




<h2 id="Scaling-Sequential-Recommendation-Models-with-Transformers"><a href="#Scaling-Sequential-Recommendation-Models-with-Transformers" class="headerlink" title="Scaling Sequential Recommendation Models with Transformers"></a>Scaling Sequential Recommendation Models with Transformers</h2><p><strong>Authors:Pablo Zivic, Hernan Vazquez, Jorge Sanchez</strong></p>
<p>Modeling user preferences has been mainly addressed by looking at usersâ€™ interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation.   The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models.   Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation.   We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application.   We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency.   We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.   Code and models can be found at <a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt">https://github.com/mercadolibre/srt</a> </p>
<blockquote>
<p>é€šè¿‡æŸ¥çœ‹ç”¨æˆ·ä¸ç³»ç»Ÿä¸­å¯ç”¨å…ƒç´ ä¹‹é—´çš„äº¤äº’å†å²æ¥ä¸»è¦è§£å†³ç”¨æˆ·åå¥½å»ºæ¨¡çš„é—®é¢˜ã€‚æ ¹æ®å†å²æ•°æ®ä¸ºä¸ªä½“é‡èº«å®šåˆ¶å†…å®¹æ˜¯é¡ºåºæ¨èçš„ä¸»è¦ç›®æ ‡ã€‚é—®é¢˜çš„æ€§è´¨ä»¥åŠåœ¨å„ä¸ªé¢†åŸŸä¸­è§‚å¯Ÿåˆ°çš„è‰¯å¥½æ€§èƒ½ï¼Œæ¿€å‘äº†ä½¿ç”¨å˜å‹å™¨æ¶æ„çš„åŠ¨æœºã€‚å½“æ¨¡å‹å‚æ•°æ•°é‡å¢åŠ æ—¶ï¼Œè¯¥æ¶æ„åœ¨åˆ©ç”¨è¶Šæ¥è¶Šå¤šçš„è®­ç»ƒæ•°æ®æ–¹é¢è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚è¿™ç§æ‰©å±•è¡Œä¸ºå¼•èµ·äº†æå¤§çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒä¸ºè®¾è®¡æ›´å¤§çš„æ¨¡å‹ä»¥åŠè®­ç»ƒæä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚ä»è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„æ‰©å±•å®šå¾‹ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æ¢ç´¢äº†é¡ºåºæ¨èçš„ç±»ä¼¼åŸåˆ™ã€‚æˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„äºšé©¬é€Šäº§å“æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨å…¶ä»–ç ”ç©¶ä¸­ä»…å¾—åˆ°éƒ¨åˆ†æ¢ç´¢ï¼Œå¹¶æ­ç¤ºäº†ä¸è¯­è¨€æ¨¡å‹ä¸­å‘ç°çš„ç±»ä¼¼çš„æ‰©å±•è¡Œä¸ºã€‚æœ€ä¼˜è®¡ç®—è®­ç»ƒæ˜¯å¯èƒ½çš„ï¼Œä½†éœ€è¦ä»”ç»†åˆ†æç‰¹å®šäºåº”ç”¨ç¨‹åºçš„è®¡ç®—æ€§èƒ½æƒè¡¡ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡åœ¨å°ä»»åŠ¡ç‰¹å®šé¢†åŸŸä¸Šå¾®è°ƒæ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ€§èƒ½æ‰©å±•å¯ä»¥è½¬åŒ–ä¸ºä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’Œå‘ç°æä¾›äº†åœ¨çœŸå®çš„é«˜ç»´åå¥½ç©ºé—´ä¸­è®­ç»ƒæ¨¡å‹å’Œéƒ¨ç½²çš„æˆ˜ç•¥è·¯çº¿å›¾ï¼Œæœ‰åŠ©äºæé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬æ–‡èƒ½å¤Ÿæ¶èµ·æ½œåŠ›æ— é™çš„å˜å‹å™¨ä¸ç°å®ä¸­å¤æ‚çš„é«˜ç»´é¡ºåºæ¨èç³»ç»Ÿä¹‹é—´çš„æ¡¥æ¢ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mercadolibre/srtæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨å¦‚ä½•é€šè¿‡ç”¨æˆ·çš„äº’åŠ¨å†å²è¿›è¡Œç”¨æˆ·åå¥½å»ºæ¨¡ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨Transformeræ¶æ„åœ¨åºåˆ—æ¨èä¸­ä¸ªæ€§åŒ–å†…å®¹ã€‚æ–‡ç« é€šè¿‡åˆ©ç”¨äºšé©¬é€Šäº§å“æ•°æ®é›†ï¼Œæ¢ç´¢äº†ç±»ä¼¼è¯­è¨€æ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œå¹¶å±•ç¤ºäº†æ€§èƒ½ç¼©æ”¾å¦‚ä½•åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼ºè°ƒäº†è®¡ç®—æœ€ä¼˜è®­ç»ƒçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦é’ˆå¯¹ç‰¹å®šåº”ç”¨çš„è®¡ç®—æ€§èƒ½æƒè¡¡è¿›è¡Œä»”ç»†åˆ†æã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœå’Œå‘ç°ä¸ºåœ¨é«˜ç»´åå¥½ç©ºé—´ä¸­è®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹æä¾›äº†æˆ˜ç•¥è·¯çº¿å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« å…³æ³¨é€šè¿‡ç”¨æˆ·ä¸ç³»ç»Ÿå…ƒç´ çš„äº’åŠ¨å†å²æ¥å»ºæ¨¡ç”¨æˆ·åå¥½ï¼Œè¿™æ˜¯åºåˆ—æ¨èçš„ä¸»è¦ç›®æ ‡ã€‚</li>
<li>Transformeræ¶æ„å·²è¢«è¯æ˜åœ¨åˆ©ç”¨å¤§é‡è®­ç»ƒæ•°æ®å’Œæé«˜æ¨¡å‹å‚æ•°æ•°é‡æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»è¦ç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>æ–‡ç« åˆ©ç”¨äºšé©¬é€Šäº§å“æ•°æ®é›†ï¼Œå‘ç°äº†ç±»ä¼¼è¯­è¨€æ¨¡å‹çš„ç¼©æ”¾è¡Œä¸ºã€‚</li>
<li>è®¡ç®—æœ€ä¼˜è®­ç»ƒå¯¹åºåˆ—æ¨èè‡³å…³é‡è¦ï¼Œä½†éœ€è¦ä»”ç»†åˆ†æç‰¹å®šåº”ç”¨çš„è®¡ç®—æ€§èƒ½æƒè¡¡ã€‚</li>
<li>æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é€šè¿‡å¾®è°ƒé€‚åº”å°ä»»åŠ¡ç‰¹å®šé¢†åŸŸï¼Œå±•ç¤ºæ€§èƒ½ç¼©æ”¾ã€‚</li>
<li>æ–‡ç« çš„ç ”ç©¶ç»“æœæä¾›äº†ä¸€ä¸ªåœ¨é«˜ç»´åå¥½ç©ºé—´ä¸­è®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹çš„æˆ˜ç•¥è·¯çº¿å›¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1432e8c0cfc30fe4ce86a58f601e82f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f92d5d9f7e8fd994ab15fcdfbd147e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c5c840c67efc37f67360b32c677b1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-496f2027a50d34beb6c7b71d6e2dcca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b52962e1da932f53ab0187435c70ac4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d57eea9b2995fc84c92ca1a92d6276d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b5b0ccde95d9e8ffabc00bb100fa636.jpg" align="middle">
</details>




<h2 id="Causal-World-Representation-in-the-GPT-Model"><a href="#Causal-World-Representation-in-the-GPT-Model" class="headerlink" title="Causal World Representation in the GPT Model"></a>Causal World Representation in the GPT Model</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules. We find that the GPT model tends to generate next moves that adhere to the game rules for sequences for which the attention mechanism encodes a causal structure with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure. </p>
<blockquote>
<p>ç”Ÿæˆé¢„è®­ç»ƒTransformerï¼ˆGPTï¼‰æ¨¡å‹æ˜¯å¦ä»…ç»è¿‡è®­ç»ƒä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œè¿˜æ˜¯å®ƒä»¬æ˜¯å¦ä»ä¸–ç•Œæ¨¡å‹ä¸­éšå¼å­¦ä¹ ç”Ÿæˆåºåˆ—çš„æ–¹å¼ï¼Œå³ä¸€ä¸ªä»¤ç‰Œæ¥ä¸€ä¸ªä»¤ç‰Œåœ°ç”Ÿæˆåºåˆ—ï¼Ÿæˆ‘ä»¬é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œå¹¶å»ºè®®ç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹æ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºGPTæ¨¡å‹åœ¨æ¨ç†æ—¶é—´å¯ä»¥ç”¨äºé›¶å°„å‡»å› æœç»“æ„å­¦ä¹ ï¼Œé€‚ç”¨äºå†…éƒ¨åˆ†å¸ƒåºåˆ—ã€‚åœ¨Othelloæ£‹ç›˜æ¸¸æˆçš„å—æ§åˆæˆç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚ä½¿ç”¨åœ¨ç°å®ä¸–ç•Œæ¸¸æˆä¸­é¢„è®­ç»ƒçš„GPTæ¨¡å‹ï¼ˆæ—¨åœ¨è·èƒœï¼‰ï¼Œæµ‹è¯•ä»…éµå¾ªæ¸¸æˆè§„åˆ™åˆæˆæ•°æ®ã€‚æˆ‘ä»¬å‘ç°GPTæ¨¡å‹å€¾å‘äºç”Ÿæˆéµå¾ªæ¸¸æˆè§„åˆ™çš„ä¸‹ä¸€æ­¥åŠ¨ä½œåºåˆ—ï¼Œè¿™äº›åºåˆ—çš„æ³¨æ„åŠ›æœºåˆ¶ç¼–ç äº†é«˜ç½®ä¿¡åº¦çš„å› æœç»“æ„ã€‚æ€»ä½“è€Œè¨€ï¼Œåœ¨GPTæ¨¡å‹ç”Ÿæˆä¸éµå®ˆæ¸¸æˆè§„åˆ™çš„ç§»åŠ¨æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿæœªèƒ½æ•æ‰åˆ°ä»»ä½•å› æœç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v1">PDF</a> NeurIPS 2024 Workshop on Causality and Large Models (CaLM)</p>
<p><strong>Summary</strong></p>
<p>GPTæ¨¡å‹æ˜¯å¦ä»…é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼Œè¿˜æ˜¯å®ƒä»¬ä¼šéšå¼åœ°ä»ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ä¸­å­¦ä¹ å¹¶ç”Ÿæˆåºåˆ—ä¸­çš„æ¯ä¸€ä¸ªä»¤ç‰Œï¼Ÿæœ¬æ–‡é€šè¿‡æ¨å¯¼GPTæ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šæ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºäº†ç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤ä¸ºGPTæ¨¡å‹åœ¨æ¨ç†æ—¶å¯ç”¨äºé›¶å°„å‡»å› æœç»“æ„å­¦ä¹ ï¼Œé€‚ç”¨äºå†…éƒ¨åˆ†å¸ƒåºåˆ—ã€‚åœ¨Othelloæ¸¸æˆçš„å—æ§åˆæˆç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚ä¸€ä¸ªç»è¿‡çœŸå®ä¸–ç•Œæ¸¸æˆè®­ç»ƒçš„GPTæ¨¡å‹è¢«æµ‹è¯•åœ¨ä»…éµå¾ªæ¸¸æˆè§„åˆ™åˆæˆæ•°æ®ä¸Šã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“æ³¨æ„åŠ›æœºåˆ¶ç¼–ç é«˜ç½®ä¿¡åº¦çš„å› æœç»“æ„æ—¶ï¼ŒGPTæ¨¡å‹å€¾å‘äºç”Ÿæˆéµå¾ªæ¸¸æˆè§„åˆ™çš„ä¸‹ä¸€æ­¥åŠ¨ä½œåºåˆ—ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå½“GPTæ¨¡å‹ç”Ÿæˆçš„è¡ŒåŠ¨ä¸éµå¾ªæ¸¸æˆè§„åˆ™æ—¶ï¼Œå®ƒä¹Ÿæ— æ³•æ•æ‰åˆ°ä»»ä½•å› æœå…³ç³»ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTæ¨¡å‹ä¸ä»…å­¦ä¹ é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œè€Œä¸”ä»ä¸–ç•Œæ¨¡å‹ä¸­éšå¼å­¦ä¹ ç”Ÿæˆåºåˆ—ã€‚</li>
<li>é€šè¿‡æ¨å¯¼GPTæ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œæå‡ºäº†ä¸€ä¸ªå› æœä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>GPTæ¨¡å‹åœ¨æ¨ç†é˜¶æ®µå¯ç”¨äºé›¶å°„å‡»å› æœç»“æ„å­¦ä¹ ï¼Œé€‚ç”¨äºå†…éƒ¨åˆ†å¸ƒåºåˆ—ã€‚</li>
<li>åœ¨Othelloæ¸¸æˆçš„å—æ§åˆæˆç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚</li>
<li>GPTæ¨¡å‹åœ¨éµå¾ªæ¸¸æˆè§„åˆ™çš„åˆæˆæ•°æ®ä¸Šè¡¨ç°å‡ºç”Ÿæˆéµå¾ªè§„åˆ™çš„ä¸‹ä¸€æ­¥åŠ¨ä½œçš„è¶‹åŠ¿ã€‚</li>
<li>å½“GPTæ¨¡å‹æ— æ³•ç”Ÿæˆéµå¾ªæ¸¸æˆè§„åˆ™çš„è¡ŒåŠ¨æ—¶ï¼Œå®ƒä¹Ÿæ— æ³•æ•æ‰åˆ°ä»»ä½•å› æœå…³ç³»ç»“æ„ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd533512299dae43bee0963fb6015772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg" align="middle">
</details>




<h2 id="HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass"><a href="#HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass" class="headerlink" title="HARP: Hesitation-Aware Reframing in Transformer Inference Pass"></a>HARP: Hesitation-Aware Reframing in Transformer Inference Pass</h2><p><strong>Authors:Romain StoraÃ¯, Seung-won Hwang</strong></p>
<p>This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to â€œoff-the-shelfâ€ Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡è§£å†³æ¨ç†æ­¥éª¤ä¸­å¯å˜çš„è®¡ç®—éœ€æ±‚æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶ä¸­ä¸€äº›ä»¤ç‰Œéœ€è¦æ¯”å…¶ä»–ä»¤ç‰Œæ›´å¤šçš„è®¡ç®—èµ„æºã€‚æˆ‘ä»¬æå‡ºäº†HARPï¼Œè¿™æ˜¯å¯¹â€œç°è´§â€Transformerå‰å‘ä¼ é€’çš„ç®€å•ä¿®æ”¹ã€‚å®ƒå€Ÿé‰´äº†å†³ç­–åˆ¶å®šä¸­çš„çŠ¹è±«å’Œæ¡†æ¶æ•ˆåº”ï¼Œå½“æ¨¡å‹åœ¨ä»¤ç‰Œç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°ä¸ç¡®å®šæ€§æ—¶ï¼ŒHARPä¼šé€‰æ‹©æ€§åœ°åº”ç”¨é¢å¤–çš„è®¡ç®—ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨å›°éš¾çš„å†³ç­–ç‚¹æš‚åœå¹¶é‡æ–°æ„å»ºè¾“å…¥ä»¥è·å–ä¸åŒè§†è§’æ¥æ¨¡ä»¿äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒHARPå…·æœ‰æ¨¡å‹æ— å…³æ€§ã€æ— éœ€è®­ç»ƒä¸”æ˜“äºå®ç°ã€‚æˆ‘ä»¬åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸Šå…¨é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†é«˜è¾¾+5.16%çš„æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHARPåœ¨ä¿æŒæ¨ç†æ—¶é—´æ˜¯æŸæœç´¢çš„ä¸¤å€é€Ÿåº¦çš„åŒæ—¶å®ç°äº†è¿™äº›æ”¶ç›Šã€‚ç®€å•è€Œåˆèƒ½å¸¦æ¥æ˜¾è‘—æ”¶ç›Šï¼ŒHARPä¸ºåœ¨æœ€å°è®¡ç®—å½±å“çš„æƒ…å†µä¸‹æé«˜åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹æ€§èƒ½æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07282v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç®€åŒ–èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºHARPçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é’ˆå¯¹æ¨ç†æ­¥éª¤ä¸­çš„å¯å˜è®¡ç®—éœ€æ±‚æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨é‡åˆ°ä¸ç¡®å®šçš„ä»¤ç‰Œç”Ÿæˆæ—¶ï¼ŒHARPæœ‰é€‰æ‹©æ€§åœ°åº”ç”¨é¢å¤–çš„è®¡ç®—ï¼Œæ¨¡ä»¿äººç±»åœ¨å†³ç­–è¿‡ç¨‹ä¸­çš„çŠ¹è±«å’Œé‡æ„æ€ç»´ã€‚æ­¤æ–¹æ³•æ— éœ€ç‰¹æ®Šè®­ç»ƒï¼Œæ˜“äºå®ç°ï¼Œå¹¶ä¸”åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹å¤§å°ä¸Šçš„è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºæ€§èƒ½æ”¹è¿›ï¼Œæœ€é«˜å¯è¾¾+5.16%ã€‚åŒæ—¶ï¼ŒHARPä¿æŒæ¨ç†æ—¶é—´æ˜¯å…‰æŸæœç´¢çš„ä¸¤å€å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HARPæ—¨åœ¨é€šè¿‡è§£å†³æ¨ç†æ­¥éª¤ä¸­çš„å¯å˜è®¡ç®—éœ€æ±‚ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>HARPåˆ©ç”¨çŠ¹è±«å’Œå†³ç­–åˆ¶å®šä¸­çš„é‡æ„æ•ˆåº”ï¼Œæœ‰é€‰æ‹©åœ°åœ¨æ¨¡å‹é‡åˆ°ä¸ç¡®å®šæ€§æ—¶åº”ç”¨é¢å¤–çš„è®¡ç®—ã€‚</li>
<li>HARPæ¨¡ä»¿äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œåœ¨å†³ç­–ç‚¹æš‚åœå¹¶é‡æ–°æ„å»ºè¾“å…¥ä»¥è·å–ä¸åŒè§†è§’ã€‚</li>
<li>HARPæ˜¯ä¸€ç§æ¨¡å‹æ— å…³ã€æ— éœ€é¢å¤–è®­ç»ƒä¸”æ˜“äºå®ç°çš„æ–¹æ³•ã€‚</li>
<li>åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹å¤§å°ä¸Šè¯„ä¼°ï¼ŒHARPæ˜¾ç¤ºå‡ºæ€§èƒ½æ”¹è¿›ï¼Œæœ€é«˜å¯è¾¾+5.16%ã€‚</li>
<li>HARPåœ¨åŠ å¿«æ¨ç†é€Ÿåº¦çš„åŒæ—¶ç»´æŒäº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bd54425be6c50fa98ef8656d707413c8.jpg" align="middle">

</details>




<h2 id="A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis"><a href="#A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis" class="headerlink" title="A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis"></a>A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis</h2><p><strong>Authors:Nimisha Ghosh, Daniele Santoni, Indrajit Saha, Giovanni Felici</strong></p>
<p>In recent times, Transformer-based language models are making quite an impact in the field of natural language processing. As relevant parallels can be drawn between biological sequences and natural languages, the models used in NLP can be easily extended and adapted for various applications in bioinformatics. In this regard, this paper introduces the major developments of Transformer-based models in the recent past in the context of nucleotide sequences. We have reviewed and analysed a large number of application-based papers on this subject, giving evidence of the main characterizing features and to different approaches that may be adopted to customize such powerful computational machines. We have also provided a structured description of the functioning of Transformers, that may enable even first time users to grab the essence of such complex architectures. We believe this review will help the scientific community in understanding the various applications of Transformer-based language models to nucleotide sequences. This work will motivate the readers to build on these methodologies to tackle also various other problems in the field of bioinformatics. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸäº§ç”Ÿäº†å·¨å¤§çš„å½±å“ã€‚ç”±äºç”Ÿç‰©åºåˆ—å’Œè‡ªç„¶è¯­è¨€ä¹‹é—´å­˜åœ¨ç›¸ä¼¼æ€§ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä½¿ç”¨çš„æ¨¡å‹å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•å’Œé€‚åº”ç”Ÿç‰©ä¿¡æ¯å­¦çš„å„ç§åº”ç”¨ã€‚åœ¨è¿™æ–¹é¢ï¼Œæœ¬æ–‡ä»‹ç»äº†è¿‘æœŸåœ¨æ ¸è‹·é…¸åºåˆ—èƒŒæ™¯ä¸‹åŸºäºTransformeræ¨¡å‹çš„ä¸»è¦å‘å±•ã€‚æˆ‘ä»¬å›é¡¾å¹¶åˆ†æäº†å¤§é‡å…³äºè¿™ä¸€ä¸»é¢˜çš„åº”ç”¨è®ºæ–‡ï¼Œæä¾›äº†ä¸»è¦ç‰¹å¾è¯æ®å’Œå¯èƒ½é‡‡å–çš„ä¸åŒæ–¹æ³•æ¥å®šåˆ¶è¿™äº›å¼ºå¤§çš„è®¡ç®—æœºã€‚æˆ‘ä»¬è¿˜æä¾›äº†Transformerå·¥ä½œæœºåˆ¶çš„ç»“æ„åŒ–æè¿°ï¼Œç”šè‡³å¯ä»¥è®©é¦–æ¬¡æ¥è§¦çš„ç”¨æˆ·ä¹Ÿèƒ½ç†è§£è¿™äº›å¤æ‚çš„æ¶æ„ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™æ¬¡å›é¡¾å°†æœ‰åŠ©äºç§‘å­¦ç•Œäº†è§£åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—æ–¹é¢çš„å„ç§åº”ç”¨ã€‚è¿™é¡¹å·¥ä½œå°†æ¿€åŠ±è¯»è€…åŸºäºè¿™äº›æ–¹æ³•æ¥è§£å†³ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸçš„å„ç§é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07201v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æ¢è®¨äº†åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºè¿™äº›æ¨¡å‹å¯ä»¥è½»æ˜“æ‰©å±•åˆ°ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸã€‚è®ºæ–‡è¯¦ç»†ä»‹ç»äº†Transformerçš„ä¸»è¦ç‰¹ç‚¹å’Œåº”ç”¨ï¼ŒåŒ…æ‹¬å…¶åœ¨æ ¸è‹·é…¸åºåˆ—åˆ†ææ–¹é¢çš„åº”ç”¨ï¼Œæœ‰åŠ©äºç§‘å­¦ç•Œäº†è§£è¿™äº›æ¨¡å‹åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å¤šç§åº”ç”¨ï¼Œå¹¶æ¿€åŠ±è¯»è€…åœ¨æ­¤åŸºç¡€ä¸Šè§£å†³å…¶ä»–ç”Ÿç‰©ä¿¡æ¯å­¦é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰é‡å¤§å½±å“ã€‚</li>
<li>Transformeræ¨¡å‹å¯è½»æ˜“æ‰©å±•åˆ°ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸã€‚</li>
<li>è®ºæ–‡è¯¦ç»†é˜è¿°äº†Transformerçš„ä¸»è¦ç‰¹ç‚¹å’Œåº”ç”¨ã€‚</li>
<li>Transformeræ¨¡å‹åœ¨æ ¸è‹·é…¸åºåˆ—åˆ†ææ–¹é¢æœ‰ç€é‡è¦åº”ç”¨ã€‚</li>
<li>è¯¥è®ºæ–‡æœ‰åŠ©äºç§‘å­¦ç•Œäº†è§£Transformeræ¨¡å‹åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å¤šç§åº”ç”¨ã€‚</li>
<li>è®ºæ–‡æä¾›çš„ç»“æ„åŒ–æè¿°ä½¿å¾—åˆå­¦è€…ä¹Ÿèƒ½ç†è§£Transformerçš„å¤æ‚æ¶æ„ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-479dbd5c07313b7fbbcd5d183ab4d03c.jpg" align="middle">
</details>




<h2 id="ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models"><a href="#ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models" class="headerlink" title="ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models"></a>ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models</h2><p><strong>Authors:Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, silvio savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu</strong></p>
<p>With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image-based queries. Existing practices rely on powerful but costly large language models (LLMs) or multimodal language models (MLMs) to produce instruction data. These are often prone to hallucinations, licensing issues and the generation process is often hard to scale and interpret. In this work, we present a programmatic approach that employs scene graphs as symbolic representations of images and human-written programs to systematically synthesize vision-centric instruction data. Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy. By implementing a suite of 24 single-image, 14 multi-image instruction generators, and a scene graph generation pipeline, we build a scalable, cost-effective system: ProVision which produces diverse question-answer pairs concerning objects, attributes, relations, depth, etc., for any given image. Applied to Visual Genome and DataComp datasets, we generate over 10 million instruction data points, ProVision-10M, and leverage them in both pretraining and instruction tuning stages of MLMs. When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D split of CVBench, along with a 3% increase in performance on QBench2, RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8% improvement on Mantis-Eval. Incorporation of our data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€åº”ç”¨çš„å…´èµ·ï¼ŒæŒ‡ä»¤æ•°æ®å¯¹äºè®­ç»ƒèƒ½å¤Ÿç†è§£å¤æ‚å›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰å®è·µä¾èµ–äºå¼ºå¤§ä½†æˆæœ¬é«˜æ˜‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰æ¥ç”ŸæˆæŒ‡ä»¤æ•°æ®ã€‚è¿™äº›æ–¹æ³•å¾€å¾€å®¹æ˜“å‡ºç°å¹»è§‰ã€è®¸å¯é—®é¢˜ï¼Œä¸”ç”Ÿæˆè¿‡ç¨‹å¾€å¾€éš¾ä»¥æ‰©å±•å’Œè§£é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨åœºæ™¯å›¾ä½œä¸ºå›¾åƒçš„è±¡å¾è¡¨ç¤ºå’Œäººç±»ç¼–å†™çš„ç¨‹åºæ¥ç³»ç»Ÿåœ°åˆæˆä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼Œåœ¨ä¿æŒäº‹å®å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆæ‰©å±•ã€‚é€šè¿‡å®æ–½24ä¸ªå•å›¾åƒã€14ä¸ªå¤šå›¾åƒæŒ‡ä»¤ç”Ÿæˆå™¨ä»¥åŠåœºæ™¯å›¾ç”Ÿæˆç®¡é“ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ã€æˆæœ¬æ•ˆç›Šé«˜çš„ç³»ç»Ÿï¼šProVisionã€‚è¯¥ç³»ç»Ÿå¯é’ˆå¯¹ç»™å®šå›¾åƒç”Ÿæˆæ¶‰åŠå¯¹è±¡ã€å±æ€§ã€å…³ç³»ã€æ·±åº¦ç­‰æ–¹é¢çš„å¤šæ ·åŒ–é—®ç­”å¯¹ã€‚åœ¨Visual Genomeå’ŒDataCompæ•°æ®é›†ä¸Šåº”ç”¨æ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡1000ä¸‡çš„æŒ‡ä»¤æ•°æ®ç‚¹â€”â€”ProVision-10Mï¼Œå¹¶å°†å…¶ç”¨äºMLMçš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µé‡‡ç”¨æˆ‘ä»¬çš„å•å›¾åƒæŒ‡ä»¤æ•°æ®ï¼Œåœ¨CVBenchçš„2Dåˆ†å‰²å’Œ3Dåˆ†å‰²ä¸Šåˆ†åˆ«æé«˜äº†7%å’Œ8%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨QBench2ã€RealWorldQAå’ŒMMMUä¸Šçš„æ€§èƒ½æé«˜äº†3%ã€‚æˆ‘ä»¬çš„å¤šå›¾åƒæŒ‡ä»¤æ•°æ®åœ¨Mantis-Evalä¸Šæé«˜äº†8%ã€‚åœ¨xGen-MM-4Bçš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½èå…¥äº†æˆ‘ä»¬çš„æ•°æ®ï¼Œåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ”¹è¿›äº†1.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07012v2">PDF</a> code: <a target="_blank" rel="noopener" href="https://github.com/JieyuZ2/ProVision">https://github.com/JieyuZ2/ProVision</a> dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Salesforce/ProVision-10M">https://huggingface.co/datasets/Salesforce/ProVision-10M</a></p>
<p><strong>æ‘˜è¦</strong><br>    éšç€å¤šæ¨¡æ€åº”ç”¨çš„å…´èµ·ï¼ŒæŒ‡ä»¤æ•°æ®å¯¹äºè®­ç»ƒèƒ½å¤Ÿç†è§£å¤æ‚å›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æ–¹æ³•ï¼Œåˆ©ç”¨åœºæ™¯å›¾ä½œä¸ºå›¾åƒçš„è±¡å¾æ€§è¡¨ç¤ºå’Œäººä¸ºç¼–å†™çš„ç¨‹åºæ¥ç³»ç»Ÿåœ°åˆæˆä»¥è§†è§‰ä¸ºä¸­å¿ƒæŒ‡ä»¤æ•°æ®ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†æ•°æ®ç”Ÿæˆçš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼ŒåŒæ—¶ä¿æŒäº†äº‹å®å‡†ç¡®æ€§å¹¶å®ç°äº†é«˜æ•ˆè§„æ¨¡åŒ–ã€‚é€šè¿‡å®æ–½ä¸€ç³»åˆ—å•å›¾åƒå’Œå¤šå›¾åƒæŒ‡ä»¤ç”Ÿæˆå™¨ä»¥åŠåœºæ™¯å›¾ç”Ÿæˆç®¡é“ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¯æ‰©å±•ã€æˆæœ¬æ•ˆç›Šé«˜çš„ç³»ç»Ÿâ€”â€”ProVisionï¼Œè¯¥ç³»ç»Ÿå¯ä¸ºç»™å®šå›¾åƒç”Ÿæˆå…³äºå¯¹è±¡ã€å±æ€§ã€å…³ç³»ã€æ·±åº¦ç­‰çš„å¤šæ ·åŒ–é—®ç­”å¯¹ã€‚åœ¨Visual Genomeå’ŒDataCompæ•°æ®é›†ä¸Šåº”ç”¨æ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡1äº¿ä¸ªæŒ‡ä»¤æ•°æ®ç‚¹â€”â€”ProVision-10Mï¼Œå¹¶å°†å…¶ç”¨äºMLMçš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µé‡‡ç”¨æˆ‘ä»¬çš„å•å›¾åƒæŒ‡ä»¤æ•°æ®ï¼Œåœ¨CVBenchçš„2Dåˆ†å‰²å’Œ3Dåˆ†å‰²ä¸Šåˆ†åˆ«æé«˜äº†7%å’Œ8%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨QBench2ã€RealWorldQAå’ŒMMMUä¸Šçš„æ€§èƒ½ä¹Ÿæé«˜äº†3%ã€‚æˆ‘ä»¬çš„å¤šå›¾åƒæŒ‡ä»¤æ•°æ®åœ¨Mantis-Evalä¸Šæé«˜äº†8%ã€‚åœ¨xGen-MM-4Bçš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½èå…¥äº†æˆ‘ä»¬çš„æ•°æ®ï¼Œåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†1.6%çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éšç€å¤šæ¨¡æ€åº”ç”¨çš„æ™®åŠï¼ŒæŒ‡ä»¤æ•°æ®å¯¹äºè®­ç»ƒèƒ½å¤Ÿç†è§£å¤æ‚å›¾åƒæŸ¥è¯¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰æ¥ç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œä½†å­˜åœ¨è™šæ„ã€è®¸å¯é—®é¢˜å’Œéš¾ä»¥æ‰©å±•çš„ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¨‹åºåŒ–æ–¹æ³•ï¼Œä½¿ç”¨åœºæ™¯å›¾å’Œäººä¸ºç¼–å†™çš„ç¨‹åºæ¥åˆæˆä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤æ•°æ®ï¼Œç¡®ä¿äº†æ•°æ®ç”Ÿæˆçš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é«˜æ•ˆè§„æ¨¡åŒ–å¹¶ä¿æŒäº†äº‹å®å‡†ç¡®æ€§ã€‚é€šè¿‡å®æ–½å¤šä¸ªå•å›¾åƒå’Œå¤šå›¾åƒæŒ‡ä»¤ç”Ÿæˆå™¨åŠåœºæ™¯å›¾ç”Ÿæˆç®¡é“ï¼Œå»ºç«‹äº†å¯æ‰©å±•çš„ç³»ç»Ÿâ€”â€”ProVisionã€‚</li>
<li>ProVisionèƒ½å¤Ÿä¸ºä»»ä½•ç»™å®šå›¾åƒç”Ÿæˆå¤šæ ·åŒ–çš„é—®ç­”å¯¹ï¼Œæ¶µç›–äº†å¯¹è±¡ã€å±æ€§ã€å…³ç³»ã€æ·±åº¦ç­‰ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ProVisionç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡æ€§èƒ½æé«˜äº†1.6%ã€‚</li>
<li>ProVisionç³»ç»Ÿçš„æˆæœ¬æ•ˆç›Šé«˜ï¼Œå¯ä¸ºè®­ç»ƒå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æä¾›æœ‰ä»·å€¼çš„èµ„æºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e8cb97cdfe610306866f0725881bbf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c79b6de64dbde4592691764c5cef14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbabc542464f3caae8fe55262ded5040.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fcbc119bff24db4fd8bb63ed2894e66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c42593b541d8ad98e8a0d1d9645bdb5.jpg" align="middle">
</details>




<h2 id="Understanding-Factual-Recall-in-Transformers-via-Associative-Memories"><a href="#Understanding-Factual-Recall-in-Transformers-via-Associative-Memories" class="headerlink" title="Understanding Factual Recall in Transformers via Associative Memories"></a>Understanding Factual Recall in Transformers via Associative Memories</h2><p><strong>Authors:Eshaan Nichani, Jason D. Lee, Alberto Bietti</strong></p>
<p>Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ‰§è¡Œäº‹å®æ€§å›å¿†çš„èƒ½åŠ›ã€‚å…ˆå‰çš„ç ”ç©¶å‘ç°ï¼Œåœ¨äº‹å®æ€§å›å¿†ä»»åŠ¡ä¸Šè®­ç»ƒçš„è½¬æ¢å™¨å‚¨å­˜ä¿¡æ¯çš„é€Ÿç‡ä¸å…¶å‚æ•°æ•°é‡æˆæ­£æ¯”ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æµ…å±‚è½¬æ¢å™¨å¯ä»¥é€šè¿‡ç»“åˆå…³è”è®°å¿†æ¥è·å¾—æ¥è¿‘æœ€ä¼˜çš„å­˜å‚¨å®¹é‡ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œæ— è®ºæ˜¯çº¿æ€§å…³è”è®°å¿†è¿˜æ˜¯MLPå…³è”è®°å¿†ï¼Œå…¶å­˜å‚¨å®¹é‡éƒ½éšç€å‚æ•°æ•°é‡çš„å¢åŠ è€Œçº¿æ€§æ‰©å±•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆæˆçš„äº‹å®æ€§å›å¿†ä»»åŠ¡ï¼Œå¹¶è¯æ˜äº†ä¸€ä¸ªå¸¦æœ‰å•å±‚è‡ªæ³¨æ„åŠ›æœºåˆ¶åè·ŸMLPçš„è½¬æ¢å™¨åœ¨è‡ªæ³¨æ„åŠ›å‚æ•°æˆ–MLPå‚æ•°æ€»æ•°çº¿æ€§å¢åŠ æ—¶ï¼ˆå¯¹æ•°å› ç´ é™¤å¤–ï¼‰å¯ä»¥è¾¾åˆ°ä»»åŠ¡ä¸Šçš„ç™¾åˆ†ä¹‹ç™¾å‡†ç¡®ç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œè½¬æ¢å™¨å¯ä»¥åœ¨ä½¿ç”¨å€¼çŸ©é˜µæˆ–MLPä½œä¸ºå…³è”è®°å¿†æ¥å­˜å‚¨äº‹å®æ•°æ®é›†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬é€šè¿‡è¿™äº›è¡¨è¾¾åŠ›åˆ†æçš„ç»“æœï¼Œè¡¥å……äº†åœ¨æˆ‘ä»¬çš„äº‹å®æ€§å›å¿†ä»»åŠ¡ä¸Šè®­ç»ƒçš„ç®€åŒ–çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„æ¢¯åº¦æµè½¨è¿¹åˆ†æï¼Œè¯¥åˆ†æè¡¨æ˜æ¨¡å‹è¡¨ç°å‡ºåºåˆ—å­¦ä¹ è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06538v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„äº‹å®è®°å¿†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è¯æ˜æµ…å±‚å˜å‹å™¨å¯é€šè¿‡ç»“åˆå…³è”è®°å¿†è¾¾åˆ°è¿‘ä¹æœ€ä¼˜çš„å­˜å‚¨èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜äº†çº¿æ€§ä¸MLPå…³è”è®°å¿†çš„å­˜å‚¨èƒ½åŠ›ä¸å‚æ•°æ•°é‡å‘ˆçº¿æ€§å…³ç³»ã€‚é€šè¿‡åˆæˆçš„äº‹å®è®°å¿†ä»»åŠ¡ï¼Œæˆ‘ä»¬å‘ç°å•å±‚è‡ªæ³¨æ„åŠ›åæ¥MLPçš„å˜å‹å™¨ï¼Œåœ¨è‡ªæ³¨æ„åŠ›å‚æ•°æˆ–MLPå‚æ•°æ•°é‡ä¸äº‹å®æ•°é‡å‘ˆçº¿æ€§å…³ç³»ï¼ˆå¯¹æ•°å› ç´ å†…ï¼‰æ—¶ï¼Œå¯è¾¾åˆ°100%å‡†ç¡®ç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œå˜å‹å™¨å¯åœ¨ä½¿ç”¨å€¼çŸ©é˜µæˆ–MLPä½œä¸ºå…³è”è®°å¿†æ¥å­˜å‚¨äº‹å®æ•°æ®é›†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬é€šè¿‡è¿™äº›è¡¨è¾¾åŠ›ç»“æœï¼Œè¡¥å……äº†ç®€åŒ–çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨äº‹å®è®°å¿†ä»»åŠ¡ä¸Šæ¢¯åº¦æµè½¨è¿¹çš„åˆ†æï¼Œæ˜¾ç¤ºæ¨¡å‹å±•ç°å‡ºåºè´¯å­¦ä¹ è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„äº‹å®è®°å¿†èƒ½åŠ›ã€‚</li>
<li>æµ…å±‚å˜å‹å™¨ç»“åˆå…³è”è®°å¿†å¯è¾¾åˆ°è¿‘ä¹æœ€ä¼˜çš„å­˜å‚¨èƒ½åŠ›ã€‚</li>
<li>çº¿æ€§ä¸MLPå…³è”è®°å¿†çš„å­˜å‚¨èƒ½åŠ›ä¸å‚æ•°æ•°é‡å‘ˆçº¿æ€§å…³ç³»ã€‚</li>
<li>å˜å‹å™¨åœ¨ç‰¹å®šæ¡ä»¶ä¸‹å¯è¾¾åˆ°100%çš„äº‹å®è®°å¿†ä»»åŠ¡å‡†ç¡®ç‡ã€‚</li>
<li>å˜å‹å™¨å¯åœ¨ä½¿ç”¨å€¼çŸ©é˜µä¸MLPä½œä¸ºå…³è”è®°å¿†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</li>
<li>ç®€åŒ–çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨äº‹å®è®°å¿†ä»»åŠ¡ä¸Šå±•ç°å‡ºåºè´¯å­¦ä¹ è¡Œä¸ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f8343ea89c2fe7f963d845465390cfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32c839c5936fedb7031cd8088b478b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eefd36c16f7b4f17174d08857bb5b0a8.jpg" align="middle">
</details>




<h2 id="LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations"><a href="#LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations" class="headerlink" title="LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations"></a>LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations</h2><p><strong>Authors:Mingjie Xu, Mengyang Wu, Yuzhi Zhao, Jason Chun Lok Li, Weifeng Ou</strong></p>
<p>Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMsâ€™ inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: <a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG">https://github.com/Endlinc/LLaVA-SpaceSGG</a>. </p>
<blockquote>
<p>åœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGï¼‰å°†è§†è§‰åœºæ™¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„å›¾è¡¨ç¤ºï¼Œä¸ºå¤æ‚çš„è§†è§‰ä»»åŠ¡æä¾›æ›´æ·±çš„åœºæ™¯ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SGGæ¨¡å‹ç»å¸¸å¿½ç•¥é‡è¦çš„ç©ºé—´å…³ç³»ï¼Œå¹¶ä¸”åœ¨å¼€æ”¾è¯æ±‡è¡¨çš„èƒŒæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SpaceSGGï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¼€æ”¾è¯æ±‡è¡¨SGGè®¾è®¡çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´å…³ç³»å»ºæ¨¡ã€‚ä¸ºäº†è®­ç»ƒå®ƒï¼Œæˆ‘ä»¬æ”¶é›†äº†SGGæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå‘½åä¸ºSpaceSGGã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡ç»“åˆå…¬å¼€æ•°æ®é›†å¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®æ„å»ºç®¡é“ä¸­çš„å¼€æºæ¨¡å‹åˆæˆæ•°æ®è€Œæ„å»ºçš„ã€‚å®ƒç»“åˆäº†ç›®æ ‡ä½ç½®ã€ç›®æ ‡å…³ç³»å’Œæ·±åº¦ä¿¡æ¯ï¼Œå½¢æˆä¸‰ç§æ•°æ®æ ¼å¼ï¼šç©ºé—´SGGæè¿°ã€é—®ç­”å’Œå¯¹è¯ã€‚ä¸ºäº†æé«˜MLLMsçš„å†…åœ¨èƒ½åŠ›å‘SGGä»»åŠ¡çš„è½¬ç§»ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-SpaceSGGåœ¨å¼€æ”¾è¯æ±‡è¡¨SGGæ–¹æ³•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œå¬å›ç‡æé«˜äº†8.6%ï¼Œå¹³å‡å¬å›ç‡æé«˜äº†28.4%ã€‚æˆ‘ä»¬çš„ä»£ç åº“ã€æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨ä»¥ä¸‹GitHubé“¾æ¥ä¸Šå…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG%E3%80%82">https://github.com/Endlinc/LLaVA-SpaceSGGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06322v1">PDF</a> Accepted by the WACV 2025, including supplementary material</p>
<p><strong>Summary</strong><br>ç©ºé—´åœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGï¼‰æ˜¯æ·±åº¦åœºæ™¯ç†è§£çš„é‡è¦æŠ€æœ¯ï¼Œå¯å°†å¤æ‚è§†è§‰ä»»åŠ¡è½¬æ¢ä¸ºç»“æ„åŒ–å›¾å½¢è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰SGGæ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸­å­˜åœ¨ç©ºé—´å…³ç³»å»ºæ¨¡ä¸è¶³å’Œæ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SpaceSGGæ¨¡å‹ï¼Œä¸€ç§é€‚ç”¨äºå¼€æ”¾è¯æ±‡SGGçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚æˆ‘ä»¬æ”¶é›†å¹¶æ„å»ºäº†åä¸ºSpaceSGGçš„SGGæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-SpaceSGGç›¸è¾ƒäºå…¶ä»–å¼€æ”¾è¯æ±‡SGGæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œå¬å›ç‡æé«˜äº†8.6%ï¼Œå¹³å‡å¬å›ç‡æé«˜äº†28.4%ã€‚æˆ‘ä»¬çš„ä»£ç åº“ã€æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹å·²åœ¨GitHubä¸Šå…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGGæŠ€æœ¯èƒ½å¤Ÿå°†å¤æ‚è§†è§‰ä»»åŠ¡è½¬æ¢ä¸ºç»“æ„åŒ–å›¾å½¢è¡¨ç¤ºï¼Œæé«˜åœºæ™¯ç†è§£çš„æ·±åº¦ã€‚</li>
<li>å½“å‰SGGæ¨¡å‹å­˜åœ¨å¿½è§†å…³é”®ç©ºé—´å…³ç³»å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>LLaVA-SpaceSGGæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¼€æ”¾è¯æ±‡SGGæ¨¡å‹ã€‚</li>
<li>ä¸ºè®­ç»ƒLLaVA-SpaceSGGæ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºäº†åä¸ºSpaceSGGçš„SGGæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>SpaceSGGæ•°æ®é›†ç»“åˆäº†ç›®æ ‡ä½ç½®ã€ç›®æ ‡å…³ç³»å’Œæ·±åº¦ä¿¡æ¯ï¼ŒåŒ…å«ä¸‰ç§æ•°æ®æ ¼å¼ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼å¢å¼ºMLLMå‘SGGä»»åŠ¡çš„çŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d43c40300ef813ab9c294c6d2d9da8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-667007beb268b4ed3183f8ffceb27f82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05320e9817235d934fa69ec3d1db206e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a013cdba343996ad6176ba72488718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89be98b84bbb87523661e45a4b5f1ff1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4568902a2ea3a695ad701e9d47dd19c.jpg" align="middle">
</details>




<h2 id="Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness"><a href="#Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness" class="headerlink" title="Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness"></a>Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness</h2><p><strong>Authors:Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principlesâ€“informativeness, uniqueness, and representativenessâ€“for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 100.8% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the â€œLess is Moreâ€ philosophy in MLLM development. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å¾®è°ƒé¢„å…ˆè®­ç»ƒå¥½çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥å¤„ç†çœŸå®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè§†è§‰æŒ‡ä»¤æ•°æ®é›†çš„å¿«é€Ÿæ‰©å¼ å¯¼è‡´äº†æ•°æ®å†—ä½™ï¼Œä»è€Œäº§ç”Ÿäº†è¿‡é«˜çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œæ¡†æ¶DataTailorï¼Œå®ƒåˆ©ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™â€”â€”ä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§â€”â€”è¿›è¡Œæœ‰æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæœ‰ä»·å€¼çš„æ ·æœ¬åº”è¯¥å…·æœ‰ä»»åŠ¡çš„ä¿¡æ¯æ€§ã€éå†—ä½™æ€§ï¼Œå¹¶ä»£è¡¨æ ·æœ¬åˆ†å¸ƒï¼ˆå³éå¼‚å¸¸å€¼ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹æ¯é¡¹åŸåˆ™çš„å®é™…è¯„åˆ†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨é€‚åº”ç»™å®šçš„æ•°æ®é›†ï¼Œæ— éœ€ç¹ççš„è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDataTailoråœ¨ä»…ä½¿ç”¨15%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å…¨æ•°æ®å¾®è°ƒæ€§èƒ½çš„100.8%ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†å“è¶Šçš„ç»“æœã€‚è¿™ä½“ç°äº†MLLMå‘å±•ä¸­çš„â€œå°‘å³æ˜¯å¤šâ€ç†å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06293v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong><br>æŒ‡ä»¤å¾®è°ƒå¯¹é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œå¾®è°ƒä»¥å¤„ç†ç°å®ä¸–ç•Œä»»åŠ¡ã€‚éšç€è§†è§‰æŒ‡ä»¤æ•°æ®é›†çš„å¿«é€Ÿæ‰©å±•ï¼Œæ•°æ®å†—ä½™é—®é¢˜æ„ˆå‘ä¸¥é‡ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä½œæ¡†æ¶DataTailorï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§ä¸‰ä¸ªå…³é”®åŸåˆ™è¿›è¡Œé«˜æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚æˆ‘ä»¬é€šè¿‡å®ç”¨çš„è¯„åˆ†æ–¹å¼å¯¹è¿™ä¸‰ä¸ªåŸåˆ™è¿›è¡Œè¡¡é‡ï¼Œè‡ªåŠ¨é€‚åº”ç»™å®šçš„æ•°æ®é›†ï¼Œæ— éœ€ç¹ççš„è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDataTailoråœ¨ä»…ä½¿ç”¨15%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å…¨æ•°æ®å¾®è°ƒæ€§èƒ½çš„100.8%ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„ç»“æœï¼Œè¿™ä½“ç°äº†MLLMå‘å±•ä¸­çš„â€œå°‘å³æ˜¯å¤šâ€ç†å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒæ˜¯ä¼˜åŒ–é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤„ç†ç°å®ä»»åŠ¡çš„æ–¹æ³•ã€‚</li>
<li>æ•°æ®å†—ä½™é—®é¢˜éšç€è§†è§‰æŒ‡ä»¤æ•°æ®é›†çš„æ‰©å¤§è€ŒåŠ å‰§ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬ä¸Šå‡ã€‚</li>
<li>DataTailoræ¡†æ¶é€šè¿‡ä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§ä¸‰ä¸ªåŸåˆ™è¿›è¡Œé«˜æ•ˆæ•°æ®é€‰æ‹©ã€‚</li>
<li>æœ‰æ•ˆçš„æ ·æœ¬åº”å…·æœ‰ä»»åŠ¡ä¿¡æ¯æ€§ã€éå†—ä½™æ€§ï¼Œå¹¶èƒ½ä»£è¡¨æ ·æœ¬åˆ†å¸ƒã€‚</li>
<li>DataTailoré€šè¿‡å®ç”¨çš„è¯„åˆ†æ–¹å¼è‡ªåŠ¨é€‚åº”æ•°æ®é›†ï¼Œé¿å…ç¹ççš„è¶…å‚æ•°è°ƒæ•´ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒDataTailoråœ¨ä»…ä½¿ç”¨å°‘é‡æ•°æ®çš„æƒ…å†µä¸‹å–å¾—äº†ä»¤äººç©ç›®çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16ba4959c013b5b5057db43147dbcf1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d1796b5c2c43c9484a2a5ccd1a518a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da737588d95e9d9a1795383aa0d36a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6ddefd66bc4617e55355e51f2d01f13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b558db0836cbaac7eb611b67eef6d9.jpg" align="middle">
</details>




<h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by â€œselecting sparsely and computing denselyâ€. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹LLMçš„PEFTæ–¹æ³•åªèƒ½åŒæ—¶å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒæˆ–å¯æ‰©å±•çš„æœåŠ¡ï¼Œä½†ä¸èƒ½ä¸‰è€…å…¼é¡¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç¨€ç–å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨è¿™ä¸€å…³é”®è§è§£ï¼Œæˆ‘ä»¬é’ˆå¯¹LLMæå‡ºäº†ä¸€ç³»åˆ—ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆS$^{2}$FTï¼‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§æ–¹é¢å‡è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚S$^{2}$FTé€šè¿‡â€œç¨€ç–é€‰æ‹©ã€å¯†é›†è®¡ç®—â€æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®ƒåˆ†åˆ«é€‰æ‹©MHAå’ŒFFNæ¨¡å—ä¸­æ¯ä¸ªTransformerå—çš„å‡ ä¸ªå¤´å’Œé€šé“ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå¯¹LLMä¸­è€¦åˆç»“æ„ä¸¤ä¾§çš„æƒé‡çŸ©é˜µè¿›è¡Œå…±æ’åˆ—ï¼Œå°†æ¯ä¸€å±‚ä¸­é€‰æ‹©çš„ç»„ä»¶è¿æ¥æˆå¯†é›†çš„å­çŸ©é˜µã€‚æœ€åï¼ŒS$^{2}$FTå¯¹æ‰€æœ‰å­çŸ©é˜µè¿›è¡Œå°±åœ°æ¢¯åº¦æ›´æ–°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜ï¼Œåœ¨å¸¸è¯†æ¨ç†å’Œç®—æœ¯æ¨ç†æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸LoRAç›¸æ¯”å¹³å‡æé«˜äº†4.6%å’Œ1.3%ï¼Œåœ¨æŒ‡ä»¤è°ƒä¼˜åæ¨å¹¿åˆ°ä¸åŒé¢†åŸŸæ—¶ï¼Œæ¯”å…¨é‡FTé«˜å‡º11.5%ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTåœ¨è®­ç»ƒå†…å­˜æ–¹é¢èŠ‚çœäº†é«˜è¾¾3å€ï¼Œä¸å…¨é‡FTç›¸æ¯”ï¼Œå»¶è¿Ÿæé«˜äº†1.5-2.7å€ï¼ŒåŒæ—¶åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šéƒ½å¹³å‡æ¯”LoRAæé«˜äº†10%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼ŒS$^{2}$FTä¸­çš„æƒé‡æ›´æ–°å¯ä»¥è¢«è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¸ºå®ç°å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆèåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡Œæä¾›æœåŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å½“å‰é’ˆå¯¹LLMçš„PEFTæ–¹æ³•å¾€å¾€åªèƒ½åŒæ—¶æ»¡è¶³é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯ä¼¸ç¼©æœåŠ¡ä¸­çš„ä¸¤é¡¹ï¼Œæ— æ³•å®ç°ä¸‰è€…å…¼é¡¾ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†ç¨€ç–å¾®è°ƒæŠ€æœ¯å¹¶è§‚å¯Ÿåˆ°å…¶æ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºæ­¤å…³é”®å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹LLMçš„ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆStructured Sparse Fine-Tuningï¼ŒS$^{2}$FTï¼‰æ–¹æ³•ç³»åˆ—ï¼ŒåŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯ä¼¸ç¼©æ€§ã€‚S$^{2}$FTé€šè¿‡â€œç¨€ç–é€‰æ‹©ã€å¯†é›†è®¡ç®—â€å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®ƒé€‰æ‹©Transformerå—ä¸­MHAå’ŒFFNæ¨¡å—çš„å°‘æ•°å¤´éƒ¨å’Œé€šé“ï¼Œå¯¹LLMä¸­è€¦åˆç»“æ„çš„ä¸¤ä¾§æƒé‡çŸ©é˜µè¿›è¡Œå…±ç½®æ¢ï¼Œå°†æ¯ä¸€å±‚ä¸­é€‰å®šçš„ç»„ä»¶è¿æ¥æˆå¯†é›†å­çŸ©é˜µã€‚æœ€åï¼ŒS$^{2}$FTå¯¹æ‰€æœ‰å­çŸ©é˜µæ‰§è¡ŒåŸåœ°æ¢¯åº¦æ›´æ–°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç»“æœï¼Œæœ¬ç ”ç©¶æ–¹æ³•å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜ï¼Œåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸LoRAç›¸æ¯”å¹³å‡æé«˜äº†4.6%å’Œ1.3%ï¼Œåœ¨æŒ‡ä»¤è°ƒä¼˜åæ¨å¹¿åˆ°ä¸åŒé¢†åŸŸæ—¶æ¯”å…¨é‡å¾®è°ƒé«˜å‡º11.5%ã€‚é€šè¿‡ä½¿ç”¨éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTåœ¨è®­ç»ƒå†…å­˜æ–¹é¢èŠ‚çœäº†é«˜è¾¾3å€ï¼Œä¸å…¨é‡å¾®è°ƒç›¸æ¯”å»¶è¿Ÿæé«˜äº†1.5-2.7å€ï¼ŒåŒæ—¶åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šå¹³å‡æ¯”LoRAæé«˜äº†10%ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜S$^{2}$FTä¸­çš„æƒé‡æ›´æ–°å¯ä»¥è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¸ºèåˆå¤šä¸ªå¾®è°ƒæ¨¡å‹æä¾›äº†æœ‰æ•ˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡Œå¤„ç†çš„èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å½“å‰PEFTæ–¹æ³•åœ¨LLMä¸Šéš¾ä»¥åŒæ—¶å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯ä¼¸ç¼©æœåŠ¡ä¸‰è€…ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç¨€ç–å¾®è°ƒæŠ€æœ¯è§‚å¯Ÿåˆ°æ˜¾è‘—æ³›åŒ–èƒ½åŠ›æ”¹è¿›ã€‚</li>
<li>æå‡ºç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆS$^{2}$FTï¼‰æ–¹æ³•ï¼Œç»“åˆç¨€ç–é€‰æ‹©ä¸å¯†é›†è®¡ç®—ç­–ç•¥ã€‚</li>
<li>S$^{2}$FTé€‰æ‹©Transformerå—ä¸­çš„å…³é”®ç»„ä»¶å¹¶è¿›è¡Œæƒé‡çŸ©é˜µç½®æ¢ï¼Œå½¢æˆå¯†é›†å­çŸ©é˜µã€‚</li>
<li>S$^{2}$FTé€šè¿‡åŸåœ°æ¢¯åº¦æ›´æ–°ä¼˜åŒ–æ€§èƒ½ï¼Œå‡å°‘è®­ç»ƒå†…å­˜ä½¿ç”¨å¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
<li>S$^{2}$FTåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”LoRAæœ‰å¹³å‡10%çš„æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9dc097c198906efa869b0f4455639b1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52615bada518b68dec445640650fa2f0.jpg" align="middle">
</details>




<h2 id="The-BrowserGym-Ecosystem-for-Web-Agent-Research"><a href="#The-BrowserGym-Ecosystem-for-Web-Agent-Research" class="headerlink" title="The BrowserGym Ecosystem for Web Agent Research"></a>The BrowserGym Ecosystem for Web Agent Research</h2><p><strong>Authors:Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, LÃ©o Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han LÃ¹, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste</strong></p>
<p>The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMs) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropicâ€™s latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models. </p>
<blockquote>
<p>BrowserGymç”Ÿæ€ç³»ç»Ÿæ»¡è¶³äº†æ—¥ç›Šå¢é•¿çš„å¯¹äºé«˜æ•ˆè¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•webä»£ç†çš„éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯é‚£äº›åˆ©ç”¨è‡ªåŠ¨åŒ–å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œwebäº¤äº’ä»»åŠ¡çš„ä»£ç†ã€‚è®¸å¤šç°æœ‰çš„åŸºå‡†æµ‹è¯•å—åˆ°ç¢ç‰‡åŒ–ä»¥åŠè¯„ä¼°æ–¹æ³•ä¸ä¸€è‡´çš„å½±å“ï¼Œä½¿å¾—å®ç°å¯é çš„å¯¹æ¯”å’Œå¯é‡å¤çš„ç»“æœå…·æœ‰æŒ‘æˆ˜æ€§ã€‚BrowserGymæ—¨åœ¨é€šè¿‡æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ã€ç±»ä¼¼äºgymçš„ç¯å¢ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç¯å¢ƒå…·æœ‰æ˜ç¡®å®šä¹‰çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œä¿ƒè¿›å„ç§åŸºå‡†æµ‹è¯•çš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚ç»“åˆAgentLabè¿™ä¸€äº’è¡¥æ¡†æ¶ï¼Œå®ƒæœ‰åŠ©äºä»£ç†çš„åˆ›å»ºã€æµ‹è¯•å’Œåˆ†æï¼ŒBrowserGymæä¾›äº†é›†æˆæ–°åŸºå‡†æµ‹è¯•çš„çµæ´»æ€§ï¼ŒåŒæ—¶ç¡®ä¿ä¸€è‡´çš„è¯„ä¼°å’Œç»¼åˆçš„å®éªŒç®¡ç†ã€‚è¿™ç§æ ‡å‡†åŒ–æ–¹æ³•æ—¨åœ¨å‡å°‘å¼€å‘webä»£ç†çš„æ—¶é—´å’Œå¤æ‚æ€§ï¼Œæ”¯æŒæ›´å¯é çš„å¯¹æ¯”ï¼Œä¿ƒè¿›å¯¹ä»£ç†è¡Œä¸ºçš„æ·±å…¥åˆ†æï¼Œå¹¶å¯èƒ½äº§ç”Ÿæ›´é€‚åº”ã€æ›´å¼ºå¤§çš„ä»£ç†ï¼Œæœ€ç»ˆåŠ é€ŸLLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–åˆ›æ–°ã€‚ä½œä¸ºæ”¯æŒè¯æ®ï¼Œæˆ‘ä»¬è¿›è¡Œäº†é¦–ä¸ªå¤§è§„æ¨¡ã€å¤šåŸºå‡†çš„webä»£ç†å®éªŒï¼Œæ¯”è¾ƒäº†å½“å‰BrowserGymä¸­æ‰€æœ‰å¯ç”¨åŸºå‡†æµ‹è¯•ä¸Š6ä¸ªæœ€æ–°å‰æ²¿LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå‘ç°ï¼Œé™¤ä¸è§†è§‰ç›¸å…³çš„ä»»åŠ¡å¤–ï¼ŒGPT-4oè¡¨ç°ä¼˜è¶Šï¼ŒOpenAIå’ŒAnthropicçš„æœ€æ–°æ¨¡å‹ä¹‹é—´å­˜åœ¨å¾ˆå¤§çš„å·®å¼‚ï¼ŒClaude-3.5-Sonnetå‡ ä¹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡é¢†å…ˆã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œæˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œç”±äºç°å®ä¸–ç•Œçš„webç¯å¢ƒçš„å›ºæœ‰å¤æ‚æ€§å’Œå½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œæ„å»ºç¨³å¥é«˜æ•ˆçš„webä»£ç†ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05467v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>BrowserGymç”Ÿæ€ç³»ç»Ÿè§£å†³äº†å¯¹é«˜æ•ˆè¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•ç½‘ç»œä»£ç†çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯é‚£äº›åˆ©ç”¨è‡ªåŠ¨åŒ–å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç½‘ç»œäº¤äº’ä»»åŠ¡çš„é¡¹ç›®ã€‚BrowserGymæ—¨åœ¨é€šè¿‡æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ã€ç±»ä¼¼äºgymçš„ç¯å¢ƒï¼Œä»¥åŠå®šä¹‰è‰¯å¥½çš„è§‚æµ‹å’Œè¡ŒåŠ¨ç©ºé—´ï¼Œè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„ç¢ç‰‡åŒ–å’Œä¸ä¸€è‡´è¯„ä¼°æ–¹æ³•çš„é—®é¢˜ï¼Œä»è€Œæ¨åŠ¨æ ‡å‡†åŒ–è¯„ä¼°åœ¨å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­çš„åº”ç”¨ã€‚ç»“åˆAgentLabæ¡†æ¶ï¼Œä¸€ä¸ªè¾…åŠ©åˆ›å»ºã€æµ‹è¯•å’Œåˆ†æä»£ç†çš„æ¡†æ¶ï¼ŒBrowserGymæä¾›äº†çµæ´»é›†æˆæ–°åŸºå‡†æµ‹è¯•çš„èƒ½åŠ›ï¼ŒåŒæ—¶ç¡®ä¿ä¸€è‡´çš„è¯„ä¼°å’Œç»¼åˆçš„å®éªŒç®¡ç†ã€‚è¿™ç§æ ‡å‡†åŒ–æ–¹æ³•æ—¨åœ¨å‡å°‘å¼€å‘ç½‘ç»œä»£ç†çš„æ—¶é—´å’Œå¤æ‚æ€§ï¼Œæ”¯æŒæ›´å¯é çš„å¯¹æ¯”å’Œå¯¹ä»£ç†è¡Œä¸ºçš„æ·±å…¥åˆ†æï¼Œå¯ä»¥äº§ç”Ÿæ›´çµæ´»ã€èƒ½åŠ›æ›´å¼ºçš„ä»£ç†ï¼Œæœ€ç»ˆåŠ é€ŸLLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–åˆ›æ–°ã€‚æˆ‘ä»¬ä»¥å¤§å‹ã€è·¨å¤šä¸ªåŸºå‡†æµ‹è¯•çš„ç½‘ç»œä»£ç†å®éªŒä½œä¸ºæ”¯æŒè¯æ®ï¼Œæ¯”è¾ƒäº†ç›®å‰BrowserGymä¸­æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šå…­ä¸ªæœ€å…ˆè¿›LLMçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜OpenAIå’ŒAntropicçš„æœ€æ–°æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒClaude-3.5-Sonnetåœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­é¢†å…ˆï¼Œä½†åœ¨è§†è§‰ç›¸å…³ä»»åŠ¡ä¸ŠGPT-4oè¡¨ç°æ›´ä¼˜ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œç”±äºçœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒçš„å›ºæœ‰å¤æ‚æ€§å’Œå½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œæ„å»ºå¥å£®ä¸”é«˜æ•ˆçš„Webä»£ç†ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrowserGymç”Ÿæ€ç³»ç»Ÿè§£å†³äº†ç°æœ‰ç½‘ç»œä»£ç†è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚</li>
<li>å®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°ç¯å¢ƒï¼Œä¿ƒè¿›äº†æ ‡å‡†åŒ–è¯„ä¼°æ–¹æ³•çš„å®æ–½ã€‚</li>
<li>AgentLabæ¡†æ¶ä¸BrowserGymç»“åˆï¼Œæœ‰åŠ©äºåˆ›å»ºã€æµ‹è¯•å’Œåˆ†æä»£ç†ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†LLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>Claude-3.5-Sonnetåœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è§†è§‰ä»»åŠ¡ä¸ŠGPT-4oè¡¨ç°æ›´å¥½ã€‚</li>
<li>æ„å»ºå¥å£®ä¸”é«˜æ•ˆçš„Webä»£ç†ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦å› ä¸ºçœŸå®ç½‘ç»œç¯å¢ƒçš„å¤æ‚æ€§å’Œæ¨¡å‹çš„å±€é™æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3858dbdffadf165a29416f9620a164c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c364e85d448c813f00dd1b59ad2c14cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc3f41d720e59c26ee217c4a0b296a37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f346ae8041ecb94aee3f20ace0688668.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-361a8173f1daf7a58469d04ffcd267c8.jpg" align="middle">
</details>




<h2 id="VoiceBench-Benchmarking-LLM-Based-Voice-Assistants"><a href="#VoiceBench-Benchmarking-LLM-Based-Voice-Assistants" class="headerlink" title="VoiceBench: Benchmarking LLM-Based Voice Assistants"></a>VoiceBench: Benchmarking LLM-Based Voice Assistants</h2><p><strong>Authors:Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li</strong></p>
<p>Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸï¼Œæœ€è¿‘çš„è¿›å±•ï¼Œå¦‚GPT-4oï¼Œå·²ç»èƒ½å¤Ÿé€šè¿‡åŸºäºLLMçš„è¯­éŸ³åŠ©æ‰‹å®ç°å®æ—¶è¯­éŸ³äº¤äº’ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„äº¤äº’ç›¸æ¯”ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ˜¾è‘—æ”¹å–„çš„ä½“éªŒã€‚ç„¶è€Œï¼Œç¼ºä¹ç”¨äºè¯„ä¼°è¿™äº›è¯­éŸ³äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†åŸºäºLLMçš„è¯­éŸ³åŠ©æ‰‹çš„å‘å±•ã€‚å½“å‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ¸…æ´è¯­éŸ³çš„ä¸€èˆ¬çŸ¥è¯†è¯„ä¼°ä¸Šï¼Œå¿½è§†äº†æ¶‰åŠå¤šç§è¯´è¯äººç‰¹å¾ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ çš„æ›´å¤æ‚ã€çœŸå®ä¸–ç•Œçš„åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoiceBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨æä¾›åŸºäºLLMçš„è¯­éŸ³åŠ©æ‰‹çš„å¤šå…ƒåŒ–è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ã€‚VoiceBenchè¿˜åŒ…æ‹¬çœŸå®å’Œåˆæˆçš„è¯­éŸ³æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤èå…¥äº†ä¸Šè¿°ä¸‰ä¸ªå…³é”®çš„çœŸå®ä¸–ç•Œå˜åŒ–å› ç´ ã€‚å¹¿æ³›çš„å®éªŒæ­ç¤ºäº†å½“å‰åŸºäºLLMçš„è¯­éŸ³åŠ©æ‰‹æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17196v3">PDF</a> Work in progress. Data is available at   <a target="_blank" rel="noopener" href="https://github.com/MatthewCYM/VoiceBench">https://github.com/MatthewCYM/VoiceBench</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸï¼ŒGPT-4oç­‰æœ€æ–°è¿›å±•ä½¿å¾—é€šè¿‡LLMè¯­éŸ³åŠ©æ‰‹è¿›è¡Œå®æ—¶è¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„æ–‡æœ¬äº¤äº’æ–¹å¼æä¾›äº†æ˜¾è‘—çš„ç”¨æˆ·ä½“éªŒæ”¹è¿›ã€‚ç„¶è€Œï¼Œç¼ºä¹è¯„ä¼°è¿™äº›è¯­éŸ³äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†LLMè¯­éŸ³åŠ©æ‰‹çš„å‘å±•ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VoiceBenchåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°LLMè¯­éŸ³åŠ©æ‰‹çš„å¤šæ–¹é¢è¯„ä»·å·¥å…·ã€‚VoiceBenchåŒ…å«çœŸå®å’Œåˆæˆè¯­éŸ³æŒ‡ä»¤ï¼Œæ¶µç›–äº†ç°å®ä¸–ç•Œä¸­çš„å…³é”®å˜åŒ–å› ç´ ã€‚å¤§é‡å®éªŒæ­ç¤ºäº†å½“å‰LLMè¯­éŸ³åŠ©æ‰‹çš„å±€é™æ€§ï¼Œä¸ºä»Šåçš„ç ”ç©¶å’Œå‘å±•æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„è¿›æ­¥æ¨åŠ¨äº†å®æ—¶è¯­éŸ³äº¤äº’çš„å‘å±•ï¼Œé€šè¿‡LLMè¯­éŸ³åŠ©æ‰‹å®ç°ã€‚</li>
<li>GPT-4oç­‰æœ€æ–°æŠ€æœ¯æ˜¾è‘—æå‡äº†ç”¨æˆ·ä½“éªŒã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°LLMè¯­éŸ³åŠ©æ‰‹äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VoiceBenchæ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°LLMè¯­éŸ³åŠ©æ‰‹çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VoiceBenchåŒ…å«çœŸå®å’Œåˆæˆçš„è¯­éŸ³æŒ‡ä»¤ï¼Œæ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å¤æ‚åœºæ™¯ã€‚</li>
<li>ç°æœ‰LLMè¯­éŸ³åŠ©æ‰‹å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69ae0a5443de6281affd9aaaa8657b10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86de805d5a4c3dd28764ed73475c70f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc8f15069422afe64369d4abe5a0a4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15a9acc59b778b744382cabc12652f69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20de57dd7f623666968b886a207ce214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b1aa211796e45875eb98816b841157.jpg" align="middle">
</details>




<h2 id="ChatGPT-and-biometrics-an-assessment-of-face-recognition-gender-detection-and-age-estimation-capabilities"><a href="#ChatGPT-and-biometrics-an-assessment-of-face-recognition-gender-detection-and-age-estimation-capabilities" class="headerlink" title="ChatGPT and biometrics: an assessment of face recognition, gender   detection, and age estimation capabilities"></a>ChatGPT and biometrics: an assessment of face recognition, gender   detection, and age estimation capabilities</h2><p><strong>Authors:Ahmad Hassanpour, Yasamin Kowsari, Hatef Otroshi Shahreza, Bian Yang, Sebastien Marcel</strong></p>
<p>This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTåœ¨ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬ç‰¹åˆ«ç ”ç©¶äº†ChatGPTåœ¨æ‰§è¡Œä¸ç”Ÿç‰©è¯†åˆ«ç›¸å…³çš„ä»»åŠ¡æ—¶çš„èƒ½åŠ›ï¼Œé‡ç‚¹æ˜¯äººè„¸è¯†åˆ«ã€æ€§åˆ«æ£€æµ‹å’Œå¹´é¾„ä¼°è®¡ã€‚ç”±äºç”Ÿç‰©è¯†åˆ«è¢«è§†ä¸ºæ•æ„Ÿä¿¡æ¯ï¼ŒChatGPTé¿å…å›ç­”ç›´æ¥æç¤ºï¼Œå› æ­¤æˆ‘ä»¬åˆ¶å®šäº†ä¸€ç§æç¤ºç­–ç•¥æ¥ç»•è¿‡å…¶å®‰å…¨æªæ–½å¹¶è¯„ä¼°å…¶ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒChatGPTèƒ½å¤Ÿè¯†åˆ«é¢éƒ¨èº«ä»½å¹¶å‡†ç¡®åŒºåˆ†ä¸¤å¼ é¢éƒ¨å›¾åƒã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ€§åˆ«æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œå¹´é¾„ä¼°è®¡ä»»åŠ¡çš„å‡†ç¡®æ€§ä¹Ÿåˆç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºLLMå’ŒåŸºç¡€æ¨¡å‹åœ¨ç”Ÿç‰©è¯†åˆ«é¢†åŸŸçš„åº”ç”¨æä¾›äº†å¹¿é˜”çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02965v2">PDF</a> Published as a conference paper at IEEE International Conference on   Image Processing (ICIP) 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTåœ¨ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶äº†ChatGPTåœ¨äººè„¸è¯†åˆ«ã€æ€§åˆ«è¯†åˆ«å’Œå¹´é¾„ä¼°è®¡ç­‰ç”Ÿç‰©è¯†åˆ«ç›¸å…³ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç”±äºç”Ÿç‰©è¯†åˆ«è¢«è§†ä¸ºæ•æ„Ÿä¿¡æ¯ï¼ŒChatGPTé¿å…å›ç­”ç›´æ¥æç¤ºï¼Œå› æ­¤ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§æç¤ºç­–ç•¥æ¥ç»•è¿‡å…¶å®‰å…¨ä¿æŠ¤æœºåˆ¶ä»¥è¯„ä¼°å…¶åœ¨ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒChatGPTèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«é¢éƒ¨èº«ä»½å¹¶åŒºåˆ†ä¸åŒçš„é¢éƒ¨å›¾åƒã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨æ€§åˆ«è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè€Œåœ¨å¹´é¾„ä¼°è®¡ä»»åŠ¡ä¸­è¾¾åˆ°åˆç†çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡çš„å‘ç°æ­ç¤ºäº†LLMså’ŒåŸºç¡€æ¨¡å‹åœ¨ç”Ÿç‰©è¯†åˆ«é¢†åŸŸåº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯äººè„¸è¯†åˆ«ã€æ€§åˆ«è¯†åˆ«å’Œå¹´é¾„ä¼°è®¡ç­‰æ–¹é¢ã€‚</li>
<li>ChatGPTèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«é¢éƒ¨èº«ä»½å¹¶åŒºåˆ†ä¸åŒçš„é¢éƒ¨å›¾åƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºChatGPTåœ¨æ€§åˆ«è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¹´é¾„ä¼°è®¡ä»»åŠ¡ä¸­ï¼ŒChatGPTè¾¾åˆ°åˆç†çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç”±äºç”Ÿç‰©è¯†åˆ«ä¿¡æ¯çš„æ•æ„Ÿæ€§ï¼Œç ”ç©¶å›¢é˜Ÿéœ€è¦è®¾è®¡å·§å¦™çš„æç¤ºç­–ç•¥æ¥è¯„ä¼°ChatGPTåœ¨ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºLLMså’ŒåŸºç¡€æ¨¡å‹åœ¨ç”Ÿç‰©è¯†åˆ«é¢†åŸŸå…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e9e651fd1feac326cf6297b5f795d32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8abcd76d4814ccdac70d5f15e6e7b2ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e684a1facd6a7212a5eea4d32cd177f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91576408a68972db813b5df3d0a88ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d41005013df030fdaf16dee34a633eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee32a078534523be1a4ae7da60b6c3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1abe3d40763850b3806c94cb8c74a522.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cd3981abbdb5f8b48c3a269513ab875.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-26403bcf86a4e06aa0a0588d5f8dc98d.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  EOV-Seg Efficient Open-Vocabulary Panoptic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14807.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
