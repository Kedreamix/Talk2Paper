<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2024-12-12  Generative Semantic Communication Architectures, Technologies, and   Applications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    29.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    120 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Generative-Semantic-Communication-Architectures-Technologies-and-Applications"><a href="#Generative-Semantic-Communication-Architectures-Technologies-and-Applications" class="headerlink" title="Generative Semantic Communication: Architectures, Technologies, and   Applications"></a>Generative Semantic Communication: Architectures, Technologies, and   Applications</h2><p><strong>Authors:Jinke Ren, Yaping Sun, Hongyang Du, Weiwen Yuan, Chongjie Wang, Xianda Wang, Yingbin Zhou, Ziwei Zhu, Fangxin Wang, Shuguang Cui</strong></p>
<p>This paper delves into the applications of generative artificial intelligence (GAI) in semantic communication (SemCom) and presents a thorough study. Three popular SemCom systems enabled by classical GAI models are first introduced, including variational autoencoders, generative adversarial networks, and diffusion models. For each system, the fundamental concept of the GAI model, the corresponding SemCom architecture, and the associated literature review of recent efforts are elucidated. Then, a novel generative SemCom system is proposed by incorporating the cutting-edge GAI technology-large language models (LLMs). This system features two LLM-based AI agents at both the transmitter and receiver, serving as “brains” to enable powerful information understanding and content regeneration capabilities, respectively. This innovative design allows the receiver to directly generate the desired content, instead of recovering the bit stream, based on the coded semantic information conveyed by the transmitter. Therefore, it shifts the communication mindset from “information recovery” to “information regeneration” and thus ushers in a new era of generative SemCom. A case study on point-to-point video retrieval is presented to demonstrate the superiority of the proposed generative SemCom system, showcasing a 99.98% reduction in communication overhead and a 53% improvement in retrieval accuracy compared to the traditional communication system. Furthermore, four typical application scenarios for generative SemCom are delineated, followed by a discussion of three open issues warranting future investigation. In a nutshell, this paper provides a holistic set of guidelines for applying GAI in SemCom, paving the way for the efficient implementation of generative SemCom in future wireless networks. </p>
<blockquote>
<p>本文深入探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并进行了全面研究。首先介绍了由经典GAI模型启用的三个流行的SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，都阐述了GAI模型的基本概念、相应的SemCom架构以及最近的文献综述。然后，通过采用最前沿的GAI技术——大型语言模型（LLM），提出了一种新型的生成式SemCom系统。该系统在发射端和接收端都采用了基于LLM的AI代理，分别作为“大脑”，实现了强大的信息理解和内容再生能力。这种创新设计允许接收端根据发射端传递的编码语义信息直接生成所需内容，而不是恢复比特流。因此，它将通信心态从“信息恢复”转变为“信息再生”，从而开启了生成式SemCom的新时代。通过点对点视频检索的案例分析，展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，通信开销减少了99.98%，检索精度提高了5data-hiddenstyle%即大幅提升成绩的表现可使用具体的百分比表示精准数值效果最佳效果最高最好明确清晰的提升指标表达可改为具体量化形式更为突出且醒目改后：通过点对点视频检索的案例展示，所提出的生成式SemCom系统在通信开销上减少了高达百分之九十九点九八（相比传统通信系统），在检索精度上提高了百分之五十三的进步效果十分显著本文还概述了生成式SemCom的四个典型应用场景以及三个有待未来研究的开放问题总而言之本文为将GAI应用于SemCom提供了一套全面的指南为未来无线网络中实现高效的生成式SemCom铺设了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08642v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong><br>     本文探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并介绍了三种由经典GAI模型启用的SemCom系统。文章提出一种结合最新GAI技术——大型语言模型（LLM）的生成式SemCom系统。该系统在发射端和接收端分别采用两个LLM基AI代理，实现了强大的信息理解和内容再生能力。该系统使接收器能够根据发射器传递的编码语义信息直接生成所需内容，从而实现从“信息恢复”到“信息再生”的转变。通过点对点视频检索的案例分析，展示了该系统的优越性，与传统通信系统相比，通信开销减少了99.98%，检索精度提高了53%。此外，还介绍了四种典型的生成式SemCom应用场景，以及三个需要未来研究的开放问题。总的来说，本文提供了将GAI应用于SemCom的全面指南，为未来无线网络中实现高效的生成式SemCom铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了生成式人工智能（GAI）在语义通信（SemCom）中的应用及其背景。</li>
<li>详述了三种基于经典GAI模型的SemCom系统：变分自编码器、生成对抗网络和扩散模型。</li>
<li>提出了一种结合大型语言模型（LLM）的新的生成式SemCom系统，实现了信息理解和内容再生。</li>
<li>该系统使接收器能基于编码语义信息直接生成内容，实现从“信息恢复”到“信息再生”的转变。</li>
<li>通过点对点视频检索案例，展示了新系统相较于传统系统的显著优势。</li>
<li>介绍了四种生成式SemCom的典型应用场景以及未来的潜在应用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0947d019af14f992664fd549f68a4b1e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-992887657b0d055041f816e833fbba5d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fb684a360750521f8bc80cff6b8f03cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f890f4c21e1505742bf4b41549683065.jpg" align="middle">
</details>




<h2 id="Fast-Prompt-Alignment-for-Text-to-Image-Generation"><a href="#Fast-Prompt-Alignment-for-Text-to-Image-Generation" class="headerlink" title="Fast Prompt Alignment for Text-to-Image Generation"></a>Fast Prompt Alignment for Text-to-Image Generation</h2><p><strong>Authors:Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang</strong></p>
<p>Text-to-image generation has advanced rapidly, yet aligning complex textual prompts with generated visuals remains challenging, especially with intricate object relationships and fine-grained details. This paper introduces Fast Prompt Alignment (FPA), a prompt optimization framework that leverages a one-pass approach, enhancing text-to-image alignment efficiency without the iterative overhead typical of current methods like OPT2I. FPA uses large language models (LLMs) for single-iteration prompt paraphrasing, followed by fine-tuning or in-context learning with optimized prompts to enable real-time inference, reducing computational demands while preserving alignment fidelity. Extensive evaluations on the COCO Captions and PartiPrompts datasets demonstrate that FPA achieves competitive text-image alignment scores at a fraction of the processing time, as validated through both automated metrics (TIFA, VQA) and human evaluation. A human study with expert annotators further reveals a strong correlation between human alignment judgments and automated scores, underscoring the robustness of FPA’s improvements. The proposed method showcases a scalable, efficient alternative to iterative prompt optimization, enabling broader applicability in real-time, high-demand settings. The codebase is provided to facilitate further research: <a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a> </p>
<blockquote>
<p>文本到图像的生成技术已经迅速发展，然而，将复杂的文本提示与生成的图像对齐仍然具有挑战性，特别是在涉及复杂对象关系和精细细节的情况下。本文介绍了快速提示对齐（FPA）技术，这是一种提示优化框架，它采用一次通过的方法，提高了文本到图像的对齐效率，而无需像OPT2I等当前方法那样的迭代开销。FPA利用大型语言模型（LLM）进行单次迭代的提示同义替换，随后通过优化提示进行微调或上下文内学习，以实现实时推理，降低了计算需求，同时保持了对齐保真度。在COCO Captions和PartiPrompts数据集上的广泛评估表明，FPA在处理时间的一小部分内就实现了具有竞争力的文本图像对齐得分，这已通过自动化指标（TIFA、VQA）和人类评估得到了验证。与专家注释者进行的人类研究进一步揭示了人类对齐判断与自动化得分之间的强烈相关性，突显了FPA改进的稳健性。所提出的方法展示了迭代提示优化的可扩展、高效替代方案，可在实时、高需求环境中更广泛应用。代码库已提供，以方便进一步的研究：<a target="_blank" rel="noopener" href="https://github.com/tiktok/fast_prompt_alignment">https://github.com/tiktok/fast_prompt_alignment</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08639v1">PDF</a> TikTok Technical Report</p>
<p><strong>Summary</strong><br>文本介绍了快速提示对齐（FPA）技术，这是一种利用大型语言模型（LLM）进行单次迭代提示转述的提示优化框架。它提高了文本到图像的对齐效率，无需像OPT2I等当前方法那样进行迭代。通过精细调整或上下文学习进行优化提示，可实现实时推理，同时减少计算需求并保持对齐保真度。广泛评估表明，FPA在COCO Captions和PartiPrompts数据集上实现了具有竞争力的文本图像对齐分数，同时处理时间大大缩短。通过专家注释员进行的人类研究表明，人类对齐判断与自动评分之间存在强烈的相关性，突出了FPA改进的稳健性。该提议的方法展示了可扩展性和高效性，可作为迭代提示优化的替代方案，适用于实时高需求场景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FPA是一种基于大型语言模型的提示优化框架，用于文本到图像生成过程中的对齐问题。</li>
<li>FPA采用单次迭代提示转述方法，提高对齐效率，摒弃了传统方法的迭代冗余。</li>
<li>FPA结合了精细调整或上下文学习技术，以实现实时推理并减少计算需求。</li>
<li>在COCO Captions和PartiPrompts数据集上的评估表明FPA对齐分数具有竞争力且处理时间显著减少。</li>
<li>人类研究验证了FPA与人类对齐判断之间的强烈相关性，证明了其稳健性。</li>
<li>FPA提供了一个可扩展和高效的替代方案，适用于实时高需求场景下的文本图像生成任务。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f2dd326d41094289fdc7eb85cac210a2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f6e9bfd70925ec0ec9b4c06d074f163.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9fdf53b899e19af7c79ca324d7775040.jpg" align="middle">
</details>




<h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>多模态生成模型需要一种统一的方法来处理离散数据（如文本和代码）和连续数据（如图像、音频、视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），它使用因果Transformer无缝集成了连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入下一个令牌扩散来进行这些向量的自回归生成。此外，为了解决自回归建模中的方差崩溃问题，我们开发了σ-VAE。大量实验表明，LatentLM在各种模态中都非常有效。在图像生成方面，LatentLM在性能和可扩展性方面都超越了Diffusion Transformers。当集成到多模态大型语言模型中时，LatentLM提供了一个通用接口，统一了多模态生成和理解。实验结果表明，在扩大训练令牌的情况下，LatentLM与Transfusion和向量量化模型相比取得了有利的性能。在文本到语音合成方面，LatentLM在语音相似性和稳健性方面超越了最新的VALL-E 2模型，同时需要10倍更少的解码步骤。结果证明LatentLM是一种高度有效和可扩展的方法，可推动大型多模态模型的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Latent Language Modeling（LatentLM）是一种统一处理离散数据（如文本和代码）和连续数据（如图像、音频和视频）的多模态生成模型。它通过因果Transformer无缝集成连续和离散数据，并使用变分自编码器（VAE）表示连续数据为潜在向量。LatentLM解决了方差崩溃的问题，并在多模态领域表现出优异的性能。在图像生成、文本转语音合成等方面，LatentLM相较于其他模型展现出更高的效果和可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Language Modeling (LatentLM) 是一种多模态生成模型，能处理离散和连续数据。</li>
<li>LatentLM 利用因果Transformer无缝集成数据，并使用变分自编码器（VAE）处理连续数据。</li>
<li>LatentLM 通过引入下一个符号扩散进行自回归生成潜在向量。</li>
<li>$\sigma$-VAE 被开发出来解决方差崩溃问题，对于自回归建模至关重要。</li>
<li>LatentLM 在图像生成、文本转语音合成等领域展现出优越的性能和可扩展性。</li>
<li>LatentLM 在处理训练令牌扩展设置时，相较于Transfusion和向量量化模型表现出较好的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics"><a href="#Synthetic-Vision-Training-Vision-Language-Models-to-Understand-Physics" class="headerlink" title="Synthetic Vision: Training Vision-Language Models to Understand Physics"></a>Synthetic Vision: Training Vision-Language Models to Understand Physics</h2><p><strong>Authors:Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan</strong></p>
<p>Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to enhance VLMs’ physical reasoning capabilities using simulated data. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs generated from simulations relevant to physical reasoning tasks. Second, we introduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to create scene descriptions enriched with physical properties and processes. During physical reasoning tasks, these PCBs can be leveraged as context to assist a Large Language Model (LLM) to improve its performance. We evaluate both of our approaches using multiple benchmarks, including a new stability detection QA dataset called Falling Tower, which includes both simulated and real-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM can significantly outperform larger state-of-the-art foundational models. We also show that integrating PCBs boosts the performance of foundational LLMs on physical reasoning tasks. Using the real-world scenes from the Falling Tower dataset, we also validate the robustness of both approaches in Sim2Real transfer. Our results highlight the utility that simulated data can have in the creation of learning systems capable of advanced physical reasoning. </p>
<blockquote>
<p>涉及动态环境中物体行为的解释、理解和预测的物理推理仍然是当前视觉语言模型（VLMs）面临的一个重大挑战。在这项工作中，我们提出了两种使用模拟数据增强VLMs物理推理能力的方法。首先，我们使用与物理推理任务相关的模拟生成的问答（QA）对来微调预训练的VLM。其次，我们引入了物理上下文构建器（PCBs），这是一种专门用于创建丰富物理属性和过程的场景描述的微调VLMs。在进行物理推理任务时，可以利用这些PCB作为上下文，帮助大型语言模型（LLM）提高其性能。我们使用多个基准测试来评估我们的两种方法，包括一个新的稳定性检测问答数据集Falling Tower和CLEVRER。我们证明，经过小型问答数据微调过的VLM可以显著优于更大的最先进的基础模型。我们还表明，集成PCB可以提高基础LLM在物理推理任务上的性能。我们还使用来自Falling Tower数据集的真实场景验证了两种方法在Sim2Real迁移中的稳健性。我们的结果强调了模拟数据在创建能够进行高级物理推理的学习系统方面的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练的语言视觉模型（VLM）在物理推理方面存在挑战。本文提出两种利用模拟数据提升VLM物理推理能力的方法。一是通过模拟物理推理任务的问答对进行微调；二是引入物理语境构建器（PCB），针对场景描述加入物理属性和过程，提升大型语言模型（LLM）的物理推理表现。在多个基准测试中，包括新的稳定性检测问答数据集Falling Tower，验证了这两种方法的有效性。结果表明，小型问答微调VLM可显著优于大型先进基础模型，集成PCB能提高LLM在物理推理任务上的性能。在Falling Tower数据集的真实场景中也验证了两种方法在模拟到现实的迁移能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的语言视觉模型（VLM）在物理推理方面存在挑战，需要提升其在动态环境中对物体行为的解释、理解和预测能力。</li>
<li>本文提出两种增强VLM物理推理能力的方法：通过模拟数据的问答对进行微调，以及引入物理语境构建器（PCB）。</li>
<li>问答对微调方法针对模拟物理推理任务生成问答对，提高VLM对此类任务的适应性。</li>
<li>PCB是一种针对场景描述加入物理属性和过程的精细调整模型，可作为语境帮助大型语言模型（LLM）提升物理推理表现。</li>
<li>在多个基准测试中，包括新的稳定性检测问答数据集Falling Tower，验证了这两种方法的有效性。</li>
<li>小型问答微调VLM可显著优于大型先进基础模型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e2f305523ebfa259f73cf14e4f47132.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6539bfab04a65e9677b31143ef8a9ac.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fa20a5f614e80dba0b3c546f3ff6045b.jpg" align="middle">
</details>




<h2 id="Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models"><a href="#Exploiting-the-Index-Gradients-for-Optimization-Based-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models"></a>Exploiting the Index Gradients for Optimization-Based Jailbreaking on   Large Language Models</h2><p><strong>Authors:Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong</strong></p>
<p>Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic">https://github.com/jiah-li/magic</a>. </p>
<blockquote>
<p>尽管使用对齐技术训练大型语言模型（LLM）以增强生成内容的安全性的技术有所进展，但这些模型仍然容易受到“越狱”攻击的影响，这是一种暴露LLM安全漏洞的对抗性攻击方法。值得注意的是，贪婪坐标梯度（GCG）方法已显示出能够自动生成对抗性后缀，突破最新LLM的能力。然而，GCG涉及的优化过程非常耗时，使得越狱管道效率低下。在本文中，我们研究了GCG的过程，并发现了间接效应的问题，这是GCG优化的关键瓶颈。为此，我们提出了模型攻击梯度指数GCG（MAGIC），它通过利用后缀标记的梯度信息来解决间接效应问题，从而通过减少计算和迭代次数来加速程序。我们在AdvBench上的实验表明，MAGIC实现了高达1.5倍的速度提升，同时保持攻击成功率（ASR）与其他基准相当甚至更高。我们的MAGIC在Llama-2上实现了74%的ASR，在对GPT-3.5进行迁移攻击时实现了54%的ASR。代码可在<a target="_blank" rel="noopener" href="https://github.com/jiah-li/magic">https://github.com/jiah-li/magic</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08615v1">PDF</a> 13 pages,2 figures, accepted by The 31st International Conference on   Computational Linguistics</p>
<p><strong>Summary</strong></p>
<p>本论文针对大型语言模型（LLM）的安全性问题进行了研究。尽管现有的LLM采用了对齐技术来增强生成内容的安全性，但仍存在被越狱攻击方法（一种暴露LLM安全漏洞的对抗攻击方法）攻击的风险。研究人员通过贪心坐标梯度（GCG）方法自动生成对抗后缀以实现越狱效果。然而，GCG的优化过程十分耗时，降低了越狱流程的效率。针对这一问题，研究人员发现了间接效应这一关键瓶颈所在，并提出了名为MAGIC的解决方案。MAGIC通过利用后缀标记的梯度信息来解决间接效应问题，减少了计算和迭代次数，从而加速了越狱过程。实验表明，MAGIC在保持攻击成功率（ASR）的同时，实现了最高达1.5倍的速度提升。在Llama-2上，MAGIC的ASR达到了74%，在GPT-3.5上进行了迁移攻击时ASR为54%。代码已公开发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）尽管采用对齐技术增强安全性，但仍面临越狱攻击方法的威胁。</li>
<li>贪心坐标梯度（GCG）方法能自动生成对抗后缀以实现越狱效果，但优化过程耗时较长。</li>
<li>研究人员发现了间接效应这一关键瓶颈，影响了GCG优化的效率。</li>
<li>提出了名为MAGIC的解决方案，通过利用后缀标记的梯度信息解决间接效应问题，从而加速越狱过程。</li>
<li>实验表明，MAGIC在保持攻击成功率（ASR）的同时，实现了速度提升。</li>
<li>MAGIC在Llama-2上的ASR达到了74%。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-58d6ac4218806ce271467b0e57043d2d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1402f5cd5315046c630f6d625906afaf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5f3cc516321175daee06622fd2d711f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-740263989a70e39b604d8c517b02ca4e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-08274e4db02b3e1e8c5285e734fedd36.jpg" align="middle">
</details>




<h2 id="Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval"><a href="#Preference-Discerning-with-LLM-Enhanced-Generative-Retrieval" class="headerlink" title="Preference Discerning with LLM-Enhanced Generative Retrieval"></a>Preference Discerning with LLM-Enhanced Generative Retrieval</h2><p><strong>Authors:Fabian Paischer, Liu Yang, Linfeng Liu, Shuai Shao, Kaveh Hassani, Jiacheng Li, Ricky Chen, Zhang Gabriel Li, Xialo Gao, Wei Shao, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Hamid Eghbalzadeh</strong></p>
<p>Sequential recommendation systems aim to provide personalized recommendations for users based on their interaction history. To achieve this, they often incorporate auxiliary information, such as textual descriptions of items and auxiliary tasks, like predicting user preferences and intent. Despite numerous efforts to enhance these models, they still suffer from limited personalization. To address this issue, we propose a new paradigm, which we term preference discerning. In preference dscerning, we explicitly condition a generative sequential recommendation system on user preferences within its context. To this end, we generate user preferences using Large Language Models (LLMs) based on user reviews and item-specific data. To evaluate preference discerning capabilities of sequential recommendation systems, we introduce a novel benchmark that provides a holistic evaluation across various scenarios, including preference steering and sentiment following. We assess current state-of-the-art methods using our benchmark and show that they struggle to accurately discern user preferences. Therefore, we propose a new method named Mender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce $\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods and achieves state-of-the-art performance on our benchmark. Our results show that Mender can be effectively guided by human preferences even though they have not been observed during training, paving the way toward more personalized sequential recommendation systems. We will open-source the code and benchmarks upon publication. </p>
<blockquote>
<p>顺序推荐系统旨在根据用户的交互历史为其提供个性化推荐。为了实现这一目标，它们通常会融入辅助信息，如物品的文本描述和辅助任务，如预测用户偏好和意图。尽管人们已经做了很多努力来优化这些模型，但它们仍然面临着个性化不足的困境。为了解决这个问题，我们提出了一种新的方法，我们称之为偏好辨别。在偏好辨别中，我们明确地将一个生成型顺序推荐系统建立在用户偏好上。为此，我们利用大型语言模型（LLM）根据用户评论和特定于项目的数据生成用户偏好。为了评估顺序推荐系统的偏好辨别能力，我们引入了一种新的基准测试，该测试可以在各种场景下进行全面评估，包括偏好控制和情感跟随。我们使用我们的基准测试对当前最先进的方法进行评估，并发现它们在准确辨别用户偏好方面遇到困难。因此，我们提出了一种新方法，名为Mender（多模式偏好辨识器），它改进了现有的方法并在我们的基准测试中实现了最先进的性能。我们的结果表明，即使在没有在训练中观察到人类偏好的情况下，Mender也可以有效地引导人类偏好，为更个性化的顺序推荐系统铺平了道路。我们将在发布时公开源代码和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08604v1">PDF</a> 11 pages + references and appendix</p>
<p><strong>Summary</strong></p>
<p>基于用户历史交互的序列推荐系统旨在为用户提供个性化推荐。它们常融合辅助信息（如物品文本描述）和任务（如预测用户偏好和意图）来提升性能。然而，这些模型在个性化方面仍有局限。为解决此问题，我们提出了一个新的方法——偏好判别。在偏好判别中，我们明确地在生成式序列推荐系统中加入用户偏好条件。为此，我们基于用户评价和物品特定数据使用大型语言模型（LLM）生成用户偏好。为评估序列推荐系统的偏好判别能力，我们引入了一个全新的评估基准，它能够在不同的场景中全面评价系统性能，包括偏好控制和情感追踪。评估结果显示当前顶级方法难以准确判断用户偏好。因此，我们提出了一种新的方法——Mender（多模态偏好判别器），它在现有方法的基础上进行了改进，并在我们的评估基准上达到了顶级性能。Mender甚至可以在训练过程中未观察到的人类偏好指导下进行有效工作，为更个性化的序列推荐系统铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>序列推荐系统旨在基于用户历史交互提供个性化推荐，但现有模型的个性化能力有限。</li>
<li>提出了一个新的方法——偏好判别，以改进序列推荐系统的性能。</li>
<li>利用大型语言模型（LLM）基于用户评价和物品数据生成用户偏好。</li>
<li>引入了一个新的评估基准，以全面评估序列推荐系统的偏好判别能力。</li>
<li>当前顶级方法在判别用户偏好方面存在困难。</li>
<li>提出了一种新的方法——Mender，它在现有方法的基础上进行了改进，并达到了顶级性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-61ed96decf60a1e589ef18daa7655bb3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-59fa7d1b83e1d1accb0dc4597b2f1772.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e64ce1c732f27adb0a7a700843471db3.jpg" align="middle">
</details>




<h2 id="Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden"><a href="#Der-Effizienz-und-Intelligenzbegriff-in-der-Lexikographie-und-kuenstlichen-Intelligenz-kann-ChatGPT-die-lexikographische-Textsorte-nachbilden" class="headerlink" title="Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?"></a>Der Effizienz- und Intelligenzbegriff in der Lexikographie und   kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte   nachbilden?</h2><p><strong>Authors:Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro</strong></p>
<p>By means of pilot experiments for the language pair German and Galician, this paper examines the concept of efficiency and intelligence in lexicography and artificial intelligence, AI. The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained. Both quantitative and qualitative methods are used for this purpose. The analysis is based on the evaluation of the outputs of several sessions with the same prompt in ChatGPT 3.5. On the one hand, the algorithmic performance of intelligent systems is evaluated in comparison with data from lexicographical works. On the other hand, the ChatGPT data supplied is analysed using specific text passages of the aforementioned lexicographical text type. The results of this study not only help to evaluate the efficiency of this chatbot regarding the creation of dictionary articles, but also to delve deeper into the concept of intelligence, the thought processes and the actions to be carried out in both disciplines. </p>
<blockquote>
<p>本文通过针对德语和加利西亚语语言对的试点实验，探讨了词典学和人工智能（AI）中的效率和智能概念。实验的目的是实证和统计地了解ChatGPT 3.5对词典词条这一词典文本类型的回应，以及该聊天机器人所训练的词典数据。为此目的，既采用了定量方法，也采用了定性方法。分析是基于对ChatGPT 3.5中相同提示下多次会话输出的评估。一方面，与词典作品的数据库相比，对智能系统的算法性能进行了评估。另一方面，使用上述词典文本类型的特定文本段落分析了ChatGPT提供的数据。这项研究的结果不仅有助于评估该聊天机器人在创建词典文章方面的效率，而且有助于更深入地了解智能的概念、思维过程和两个学科中的行动。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08599v1">PDF</a> 25 pages, in German language</p>
<p><strong>Summary</strong></p>
<p>本论文通过德语和加利西亚语的试点实验，探讨了词典编纂与人工智能中的效率与智能概念。实验旨在实证地了解ChatGPT 3.5对于词典条目的反应，并分析该聊天机器人所训练的词典数据。论文采用定量和定性方法进行分析，通过评估多个会话的输出结果，对比智能系统的算法性能与词典作品数据。本研究结果不仅有助于评估该聊天机器人在创建词典条目方面的效率，而且有助于深入了解智能、思维过程和两个学科中的行动。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究通过德语和加利西亚语的试点实验，探索了词典编纂与人工智能领域的效率与智能问题。</li>
<li>实验目的是实证地了解ChatGPT 3.5对于词典条目的反应，并分析其训练的词典数据。</li>
<li>论文采用了定量和定性方法，评估了ChatGPT 3.5在多个会话中的输出表现。</li>
<li>研究结果对比了智能系统的算法性能与词典作品数据。</li>
<li>本研究有助于评估聊天机器人在创建词典条目方面的效率。</li>
<li>结果揭示了智能、思维过程和两个学科中的行动的深入理解。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-175b29133a78d0d939af8cbd1fa0ec67.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-25ad5e61c6be29db174ba7ef06c55f62.jpg" align="middle">
</details>




<h2 id="Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks"><a href="#Leveraging-Graph-RAG-and-Prompt-Engineering-to-Enhance-LLM-Based-Automated-Requirement-Traceability-and-Compliance-Checks" class="headerlink" title="Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks"></a>Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based   Automated Requirement Traceability and Compliance Checks</h2><p><strong>Authors:Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi</strong></p>
<p>Ensuring that Software Requirements Specifications (SRS) align with higher-level organizational or national requirements is vital, particularly in regulated environments such as finance and aerospace. In these domains, maintaining consistency, adhering to regulatory frameworks, minimizing errors, and meeting critical expectations are essential for the reliable functioning of systems. The widespread adoption of large language models (LLMs) highlights their immense potential, yet there remains considerable scope for improvement in retrieving relevant information and enhancing reasoning capabilities. This study demonstrates that integrating a robust Graph-RAG framework with advanced prompt engineering techniques, such as Chain of Thought and Tree of Thought, can significantly enhance performance. Compared to baseline RAG methods and simple prompting strategies, this approach delivers more accurate and context-aware results. While this method demonstrates significant improvements in performance, it comes with challenges. It is both costly and more complex to implement across diverse contexts, requiring careful adaptation to specific scenarios. Additionally, its effectiveness heavily relies on having complete and accurate input data, which may not always be readily available, posing further limitations to its scalability and practicality. </p>
<blockquote>
<p>确保软件需求规格（SRS）与更高层次的组织或国家要求一致是非常关键的，特别是在金融和航空等受监管的环境中。在这些领域，保持一致性、遵守监管框架、减少错误以及满足关键期望对于系统的可靠运行至关重要。大型语言模型（LLM）的广泛采用突显了其巨大的潜力，但在检索相关信息和增强推理能力方面仍有很大的改进空间。本研究表明，将稳健的图-RAG框架与先进的提示工程技术（如思维链和思维树）相结合，可以显著提高性能。与基线RAG方法和简单提示策略相比，这种方法提供了更准确和上下文感知的结果。虽然这种方法在性能上显示出显著改进，但它也带来了挑战。在多种背景下实施既昂贵又复杂，需要谨慎适应特定场景。此外，其有效性严重依赖于完整和准确输入数据的存在，这可能并不总是可立即获得的，这进一步限制了其可扩展性和实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在软件需求规格（SRS）与高阶组织或国家需求的匹配方面扮演重要角色，特别是在金融和航空等监管环境中。整合Graph-RAG框架与先进的提示工程技术，如Chain of Thought和Tree of Thought，可显著提高性能，但在实施中面临成本和复杂性挑战。该方法依赖于完整和准确的数据输入，这在实践中可能难以获得，限制了其可扩展性和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在金融和航空等监管环境中，软件需求规格（SRS）与高阶组织或国家需求的对齐至关重要。</li>
<li>大型语言模型（LLM）在提升系统性能方面具有巨大潜力。</li>
<li>集成Graph-RAG框架和提示工程技术（如Chain of Thought和Tree of Thought）能显著提高LLM的性能。</li>
<li>此方法相比传统方法表现出更好的准确性和上下文意识。</li>
<li>该方法的实施成本较高，且更复杂，需要针对特定场景进行适应。</li>
<li>方法的有效性严重依赖于完整和准确的数据输入，这在实际应用中可能难以获得。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7979fb6e8cc3e979d2e01584d841f3b0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fc73788341c65dd14908806ee3ec036d.jpg" align="middle">
</details>




<h2 id="Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning"><a href="#Advancing-Single-and-Multi-task-Text-Classification-through-Large-Language-Model-Fine-tuning" class="headerlink" title="Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning"></a>Advancing Single- and Multi-task Text Classification through Large   Language Model Fine-tuning</h2><p><strong>Authors:Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang</strong></p>
<p>Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance. </p>
<blockquote>
<p>只有编码器模型（例如BERT、RoBERTa）和大语言模型（LLM，例如Llama3）已被广泛应用于文本分类任务。然而，关于基于编码器的模型与LLM在文本分类中的性能对比的系统性研究仍有所欠缺，特别是在涉及微调时更是如此。本研究采用了不同规模和架构的模型和方法，包括微调和预先训练的方法。我们首先在20 Newsgroups（20NG）和MASSIVE数据集上评估了这些LLM的性能，并将其与只有编码器的RoBERTa模型进行了比较。此外，我们还探讨了两种模型类型的多任务功能，通过将多个分类任务（包括意图检测和槽填充）结合到一个单一模型中，使用这两个数据集的数据。我们的结果表明，完全微调后的Llama3-70B模型在各种分类任务和数据集上的性能优于RoBERTa-large和其他解码器LLM。此外，经过整合的多任务微调LLM在两个任务上的性能与双模型设置相匹配，涉及两个数据集。总体而言，我们的研究对只有编码器和LLM模型在文本分类任务上进行了全面的基准测试，并展示了一种将两个或多个完全调校的解码器LLM结合起来以减少延迟并保持性能的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08587v1">PDF</a> 9 pages, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文对比研究了编码器模型（如BERT、RoBERTa）与大型语言模型（LLM，如Llama3）在文本分类任务上的性能。通过多项实验，本文发现全量微调后的Llama3-70B模型在各类文本分类任务和数据集上的表现优于RoBERTa-large和其他解码器LLM。此外，整合多任务后的LLMs性能可与双模型架构相当。研究提供了对编码器模型和LLM在文本分类任务的全面评估，并提出了一种结合两个或多个全量微调解码器LLM的方法，以降低延迟并保持等效性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文对比了编码器模型（如RoBERTa）与大型语言模型（LLM）在文本分类任务上的性能。</li>
<li>使用多种模型和方法的实验评估，包括不同规模和架构的模型，以及微调与预训练的方法。</li>
<li>在20Newsgroups和MASSIVE数据集上评估LLM的性能，并与RoBERTa模型进行对比。</li>
<li>发现全量微调后的Llama3-70B模型在多种文本分类任务和数据集上的表现优于RoBERTa-large和其他解码器LLM。</li>
<li>整合多任务的LLMs性能可与双模型架构相当。</li>
<li>研究提供了全面的编码器模型和LLM在文本分类任务的评估。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78a24e96e4b407822203e3c32af5b7ae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3c65aecbe11c6d8d21a7daee52179b60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffd99579b4a6b1de138e0cc90d02460d.jpg" align="middle">
</details>




<h2 id="TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs"><a href="#TURBOATTENTION-Efficient-Attention-Approximation-For-High-Throughputs-LLMs" class="headerlink" title="TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs"></a>TURBOATTENTION: Efficient Attention Approximation For High Throughputs   LLMs</h2><p><strong>Authors:Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.   We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models. </p>
<blockquote>
<p>大型语言模型（LLM）的推理需要大量的计算和内存资源，特别是在关键注意力机制方面。虽然量化技术和加速算法（如FlashAttention）已经提高了整体推理的效率，但它们解决的问题不同：量化关注权重激活操作，而FlashAttention则提高了执行效率但需要高精度格式。最近的键值（KV）缓存量化减少了内存带宽，但仍需要在注意力操作中进行浮点反量化。我们提出了TurboAttention，这是一种全面的注意力量化执行方法，同时解决了内存和计算效率问题。我们的解决方案引入了两个关键创新点：FlashQ，一种逐头注意力量化技术，能够压缩KV缓存并执行激活-激活乘法的量化；以及基于稀疏性的Softmax近似（SAS），它消除了在注意力指数运算期间反量化到FP32的需要。实验结果表明，TurboAttention实现了注意力加速1.2-1.8倍，KV缓存大小减少了超过4.4倍，并且在各种数据集和模型上实现了高达FP16基线2.37倍的最大吞吐量，超过了最新的量化和压缩技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08585v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>LLM推理需要大量计算与内存，特别是关键注意力机制。现有技术如量化和FlashAttention等虽提升效率，但各有侧重。本文提出TurboAttention，通过FlashQ和Sparsity-based Softmax Approximation (SAS)两大创新，同时解决内存和计算效率问题。实验结果显示，TurboAttention加速注意力计算，缩小KV缓存大小，提升最大吞吐量，且在多种数据集和模型上优于现有量化与压缩技术。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM推理依赖大量计算和内存，特别是注意力机制。</li>
<li>量化和FlashAttention等技术虽提升效率，但各有局限。</li>
<li>TurboAttention通过FlashQ和SAS两大创新，同时解决内存和计算效率问题。</li>
<li>FlashQ技术实现KV缓存压缩和激活乘法量化执行。</li>
<li>SAS消除注意力指数运算中浮点解量化的需求。</li>
<li>实验结果显示TurboAttention显著提升推理速度和缓存效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2aa13ec6d967123c700eac65289299ba.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-12c9472feee293a58946daa11dab8b79.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f5d83ed70042472422f235eb4d579530.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7f134495953af18158cd020ded942ff6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-40958ea88bed04942910eb4a10c27a9c.jpg" align="middle">
</details>




<h2 id="Can-We-Generate-Visual-Programs-Without-Prompting-LLMs"><a href="#Can-We-Generate-Visual-Programs-Without-Prompting-LLMs" class="headerlink" title="Can We Generate Visual Programs Without Prompting LLMs?"></a>Can We Generate Visual Programs Without Prompting LLMs?</h2><p><strong>Authors:Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem</strong></p>
<p>Visual programming prompts LLMs (large language mod-els) to generate executable code for visual tasks like visual question answering (VQA). Prompt-based methods are difficult to improve while also being unreliable and costly in both time and money. Our goal is to develop an efficient visual programming system without 1) using prompt-based LLMs at inference time and 2) a large set of program and answer annotations. We develop a synthetic data augmentation approach and alternative program generation method based on decoupling programs into higher-level skills called templates and the corresponding arguments. Our results show that with data augmentation, prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive with state-of-the art models with the added benefit of much faster inference </p>
<blockquote>
<p>视觉编程提示大型语言模型（LLMs）为视觉任务生成可执行代码，如视觉问答（VQA）。基于提示的方法很难改进，同时不可靠，时间和金钱成本都很高。我们的目标是在不使用基于提示的LLM进行推断的情况下，并且不使用大量的程序和答案注释来开发高效的视觉编程系统。我们开发了一种合成数据增强方法和基于将程序解耦为称为模板的高级技能和相应参数的替代程序生成方法。我们的结果表明，通过数据增强，无提示的小型LLM（约1B参数）与最新模型具有竞争力，并带来更快的推断优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于视觉编程提示，大型语言模型（LLM）可以生成用于视觉任务（如视觉问答）的可执行代码。然而，基于提示的方法难以改进，且不可靠、耗时耗钱。我们的目标是开发一种高效的视觉编程系统，不使用基于提示的LLM进行推断，并且不需要大量的程序和答案注释。我们提出了一种合成数据增强方法，以及基于将程序解耦为高级技能（称为模板）和相应参数的替代程序生成方法。结果证明，通过数据增强，无需提示的小型LLM（约1B参数）的性能可与最先进的模型竞争，并具备更快的推断速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉编程提示使LLM能够生成用于视觉任务的可执行代码。</li>
<li>基于提示的方法存在改进困难、不可靠和成本高昂的问题。</li>
<li>目标是开发一种高效的视觉编程系统，不使用基于提示的LLM进行推断。</li>
<li>提出了一种合成数据增强方法以提高模型性能。</li>
<li>通过解耦程序为高级技能（模板）和相应参数来生成替代程序。</li>
<li>数据增强使小型LLM的性能与最先进的模型相当。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-71a052fea07471cdd5b92ce33f76a2e9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-495a2ed1307492f0e4d3b1fb89c1e5e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2871162a30b0025d43f44b1d6134418f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-09544747a6fdbe241269e89ec78cea69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-64a390556423516931dab19b62b4f26c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-87feed1c478cec6f5d341f9548563b75.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d023468a4290179e5a74db0a79de7ee3.jpg" align="middle">
</details>




<h2 id="Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning"><a href="#Underestimated-Privacy-Risks-for-Minority-Populations-in-Large-Language-Model-Unlearning" class="headerlink" title="Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning"></a>Underestimated Privacy Risks for Minority Populations in Large Language   Model Unlearning</h2><p><strong>Authors:Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreačić, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</strong></p>
<p>Large Language Models are trained on extensive datasets that often contain sensitive, human-generated information, raising significant concerns about privacy breaches. While certified unlearning approaches offer strong privacy guarantees, they rely on restrictive model assumptions that are not applicable to LLMs. As a result, various unlearning heuristics have been proposed, with the associated privacy risks assessed only empirically. The standard evaluation pipelines typically randomly select data for removal from the training set, apply unlearning techniques, and use membership inference attacks to compare the unlearned models against models retrained without the to-be-unlearned data. However, since every data point is subject to the right to be forgotten, unlearning should be considered in the worst-case scenario from the privacy perspective. Prior work shows that data outliers may exhibit higher memorization effects. Intuitively, they are harder to be unlearn and thus the privacy risk of unlearning them is underestimated in the current evaluation. In this paper, we leverage minority data to identify such a critical flaw in previously widely adopted evaluations. We substantiate this claim through carefully designed experiments, including unlearning canaries related to minority groups, inspired by privacy auditing literature. Using personally identifiable information as a representative minority identifier, we demonstrate that minority groups experience at least 20% more privacy leakage in most cases across six unlearning approaches, three MIAs, three benchmark datasets, and two LLMs of different scales. Given that the right to be forgotten should be upheld for every individual, we advocate for a more rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation framework represents an initial step toward ensuring more equitable assessments of LLM unlearning efficacy. </p>
<blockquote>
<p>大型语言模型在包含敏感人类生成信息的庞大数据集上进行训练，引发了关于隐私泄露的严重关切。虽然经过认证的遗忘处理方法提供强有力的隐私保证，但它们依赖于不适用于大型语言模型的限制性模型假设。因此，已经提出了各种遗忘启发式方法，并对相关的隐私风险进行了实证评估。标准的评估流程通常会从训练集中随机选择数据进行删除，应用遗忘技术，并使用成员推理攻击来比较未学习的模型与重新训练的模型（无需遗忘数据）。然而，由于每个数据点都有被遗忘的权利，从隐私的角度来看，遗忘应在最坏的情况下考虑。先前的研究表明，数据异常值可能表现出更高的记忆效果。直觉上，它们更难被遗忘，因此当前评估中对遗忘它们的隐私风险被低估了。在本文中，我们利用少数数据来识别以前广泛采用的评估中的这一关键缺陷。我们通过精心设计的实验证实了这一主张，包括与少数群体相关的遗忘金丝雀（受隐私审计文献启发）。以个人信息作为少数群体的代表标识符，我们证明在大多数情况下，与六种遗忘方法、三种MIA、三个基准数据集和两个不同规模的大型语言模型中，少数群体的隐私泄露至少增加了20%。鉴于每个人都有被遗忘的权利，我们主张对大型语言模型的遗忘方法进行更严格的评估。我们的少数群体意识评估框架是确保大型语言模型遗忘效力评估更加公平的第一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08559v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型在广泛数据集上进行训练，涉及敏感、人类生成的信息引发了隐私泄露的担忧。虽然认证遗忘方法提供强有力的隐私保证，但它们依赖于不适用于大型语言模型的限制性模型假设。因此，提出了各种遗忘启发式方法，并仅从实证角度评估了相关的隐私风险。标准评估流程通常随机选择数据进行训练集删除，应用遗忘技术，并使用成员推理攻击来比较未遗忘模型与未进行遗忘数据重新训练的模型。然而，从隐私角度来看，每个数据点都有被遗忘的权利，因此应考虑最坏情况下的遗忘情况。先前的研究表明，数据异常值可能表现出更高的记忆效应，即它们更难被遗忘，因此当前评估低估了遗忘它们的隐私风险。本文利用少数数据来识别以前广泛采用的评估中的这一关键缺陷。本文通过精心设计实验来证实这一主张，包括与少数群体相关的遗忘犬的遗忘，灵感来自隐私审计文献。以个人身份信息作为少数群体的代表标识符，我们证明在大多数情况下，对于六种遗忘方法、三种MIA、三种基准数据集和两种不同规模的大型语言模型，少数群体至少经历了2.更多的隐私泄露情况呼吁对大型语言模型的遗忘方法进行更严格的评估。我们的面向少数群体的评估框架是确保大型语言模型遗忘效率更公平评估的初步步骤。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的训练涉及敏感信息引发隐私泄露的担忧。</li>
<li>现有遗忘方法评估依赖于模型假设不适用于大型语言模型。</li>
<li>数据异常值（少数群体）在遗忘过程中表现出更高的隐私泄露风险。</li>
<li>现有评估流程可能低估遗忘某些数据的隐私风险。</li>
<li>通过实验验证少数群体在遗忘过程中的隐私泄露情况更为严重。</li>
<li>需要更严格的评估大型语言模型的遗忘方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5d7df70e264508e34d7458412e0bc8a8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-612dcf7498baba74ceee6359bb90c887.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9c0407d01398c0d835aecbd08661ead9.jpg" align="middle">
</details>




<h2 id="MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback"><a href="#MaestroMotif-Skill-Design-from-Artificial-Intelligence-Feedback" class="headerlink" title="MaestroMotif: Skill Design from Artificial Intelligence Feedback"></a>MaestroMotif: Skill Design from Artificial Intelligence Feedback</h2><p><strong>Authors:Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D’Oro</strong></p>
<p>Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM’s feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM’s code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability. </p>
<blockquote>
<p>描述自然语言中的技能具有将人类关于决策制定的知识注入人工智能系统的潜力。我们提出了MaestroMotif，这是一种人工智能辅助技能设计方法，能够产生高性能和可适应的代理。MaestroMotif利用大型语言模型（LLM）的能力来有效地创建和重复使用技能。它首先使用LLM的反馈来自动设计对应于每个技能的奖励，从他们的自然语言描述开始。然后，它利用LLM的代码生成能力与强化学习相结合，训练技能并将它们组合起来实现用语言指定的复杂行为。我们在NetHack学习环境（NLE）的一系列复杂任务中评估了MaestroMotif，结果表明它在性能和可用性方面都超越了现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08542v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了MaestroMotif方法，这是一种AI辅助技能设计方法，可通过自然语言描述技能并将其注入AI系统，生成相应奖励，从而创建和复用技能。该方法利用大型语言模型（LLM）的能力，结合强化学习进行技能训练和组合，实现复杂行为的实施。在NetHack学习环境（NLE）的复杂任务评估中，MaestroMotif表现出优越的性能和可用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaestroMotif是一种AI辅助技能设计方法，可将自然语言描述的技能注入AI系统。</li>
<li>该方法利用LLM自动生成与技能相对应的奖励。</li>
<li>MaestroMotif结合LLM的代码生成能力和强化学习进行技能训练和组合。</li>
<li>MaestroMotif在NetHack学习环境（NLE）的复杂任务评估中表现出优越性能。</li>
<li>MaestroMotif提高了技能设计的性能和可用性。</li>
<li>LLM在技能设计和训练过程中起到了关键作用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6f665febf0a88f026fb77766c9392837.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-628a7f2a23e6422217c4fab0551ffeae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a055c6f27078f47409556b8e37ea80d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e3b8bb5f78ec046bbf77ef4055438e59.jpg" align="middle">
</details>




<h2 id="EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance"><a href="#EMS-Adaptive-Evict-then-Merge-Strategy-for-Head-wise-KV-Cache-Compression-Based-on-Global-Local-Importance" class="headerlink" title="EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance"></a>EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache   Compression Based on Global-Local Importance</h2><p><strong>Authors:Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang</strong></p>
<p>As large language models (LLMs) continue to advance, the demand for higher quality and faster processing of long contexts across various applications is growing. KV cache is widely adopted as it stores previously generated key and value tokens, effectively reducing redundant computations during inference. However, as memory overhead becomes a significant concern, efficient compression of KV cache has gained increasing attention. Most existing methods perform compression from two perspectives: identifying important tokens and designing compression strategies. However, these approaches often produce biased distributions of important tokens due to the influence of accumulated attention scores or positional encoding. Furthermore, they overlook the sparsity and redundancy across different heads, which leads to difficulties in preserving the most effective information at the head level. To this end, we propose EMS to overcome these limitations, while achieving better KV cache compression under extreme compression ratios. Specifically, we introduce a Global-Local score that combines accumulated attention scores from both global and local KV tokens to better identify the token importance. For the compression strategy, we design an adaptive and unified Evict-then-Merge framework that accounts for the sparsity and redundancy of KV tokens across different heads. Additionally, we implement the head-wise parallel compression through a zero-class mechanism to enhance efficiency. Extensive experiments demonstrate our SOTA performance even under extreme compression ratios. EMS consistently achieves the lowest perplexity, improves scores by over 1.28 points across four LLMs on LongBench under a 256 cache budget, and preserves 95% retrieval accuracy with a cache budget less than 2% of the context length in the Needle-in-a-Haystack task. </p>
<blockquote>
<p>随着大型语言模型（LLM）的持续进步，对高质量和快速处理各种应用中的长文本的需求不断增长。KV缓存因其能够存储先前生成的键和值令牌而得到广泛应用，有效地减少了推理过程中的冗余计算。然而，随着内存开销成为一个重要的问题，KV缓存的有效压缩越来越受到关注。现有的大多数方法从两个角度进行压缩：识别重要令牌和设计压缩策略。然而，这些方法通常会产生由于累积的注意力分数或位置编码的影响而导致的重要令牌分布偏差。此外，它们忽视了不同头之间的稀疏性和冗余性，导致在头部层面保留最有效信息时遇到困难。为此，我们提出EMS来克服这些局限性，在极端的压缩比例下实现更好的KV缓存压缩。具体来说，我们引入了一个全局-局部分数，该分数结合了全局和局部KV令牌上的累积注意力分数，以更好地识别令牌的重要性。对于压缩策略，我们设计了一个自适应的统一逐出后合并框架，该框架考虑了不同头之间KV令牌的稀疏性和冗余性。此外，我们通过零类机制实现了头部并行压缩，以提高效率。大量实验表明，即使在极端的压缩比例下，我们的性能也达到了最新水平。EMS始终达到了最低的困惑度，在LongBench上的四个LLM中得分提高了超过1.28点，在预算为256的缓存中保留了95%的检索准确性，而在Haystack任务中的上下文长度不到预算的2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08521v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLM）的不断发展，对高质量和快速处理长文本的需求不断增长。KV缓存因其能够存储先前生成的键和值令牌而得到广泛应用，有效减少了推理过程中的冗余计算。然而，随着内存开销成为一个重要的问题，KV缓存的压缩效率逐渐受到关注。现有方法主要从识别和压缩策略两个角度进行压缩，但存在偏见分布和重要令牌的问题，忽视了不同头之间的稀疏性和冗余性。为此，我们提出EMS方法，通过结合全局和局部令牌分数的累积注意力得分来更好地识别令牌重要性，并设计自适应统一的逐出合并框架来处理KV令牌的稀疏性和冗余性。实验表明，即使在极端压缩比下，EMS也能实现更好的KV缓存压缩性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs处理长文本的需求增长，KV缓存因其减少冗余计算而受到关注。</li>
<li>KV缓存压缩成为关注焦点，因内存开销问题日益突出。</li>
<li>现有方法存在偏见分布和重要令牌的问题，忽视不同头之间的稀疏性和冗余性。</li>
<li>EMS方法通过结合全局和局部令牌分数的累积注意力得分来识别令牌重要性。</li>
<li>EMS设计了自适应统一的逐出合并框架，旨在处理KV令牌的稀疏性和冗余性。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-38b496138c97e1dfd0f692985690d782.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a51c48e21df8dbfe8058c928883ebc9c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f36fbc21c25055b288075b463776d2e4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d840c002b790302fa195ba1f62a5dacd.jpg" align="middle">
</details>




<h2 id="Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation"><a href="#Bridging-Relevance-and-Reasoning-Rationale-Distillation-in-Retrieval-Augmented-Generation" class="headerlink" title="Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation"></a>Bridging Relevance and Reasoning: Rationale Distillation in   Retrieval-Augmented Generation</h2><p><strong>Authors:Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang</strong></p>
<p>The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction. </p>
<blockquote>
<p>在检索增强生成（RAG）管道中，重排器和生成器是两个关键组件，分别负责排列相关文档和生成响应。然而，由于预训练数据和目标之间的差异，重排器排列的相关文档与生成器支持回答查询所需的文档之间存在不可避免的鸿沟。为了解决这一鸿沟，我们提出了RADIO，这是一个具有理性蒸馏（RAtionale DIstillatiOn）的新颖且实用的偏好对齐框架。具体来说，我们首先提出了一种利用大型语言模型（LLM）的推理能力来提取回答查询所需理据的理性提取方法。随后，设计了一种基于理性的对齐过程，根据提取的理据重新排列文档，并微调重排器以对齐偏好。我们在三个数据集的两个任务上进行了大量实验，以证明我们的方法相较于基准方法的有效性。我们的代码已在线发布，以方便复制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08519v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Retrieval-Augmented Generation（RAG）管道中的两个关键组件：reranker和generator。为了解决两者在排名相关文档和支持回答查询方面的差异，提出了一种名为RADIO的新型实用偏好对齐框架，该框架结合了Large Language Models（LLMs）的推理能力来提取回答查询所需的理由，并基于这些理由重新排名文档和对reranker进行微调以对齐偏好。经过大量实验验证，该方法相较于基线方法更为有效，相关代码已在线发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Retrieval-Augmented Generation中的reranker和generator之间存在差距，影响回答查询的效果。</li>
<li>提出了一个名为RADIO的偏好对齐框架来解决上述问题。</li>
<li>利用Large Language Models的推理能力提取回答查询的理由。</li>
<li>基于提取的理由重新排名文档。</li>
<li>对reranker进行微调以对齐偏好，以缩小与generator之间的差异。</li>
<li>通过两个任务在三个数据集上的实验验证了该方法的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-07bd96bd382f50d4b0be40a29e0ad2bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0ff74513b2d945c44687fdee91eb0ece.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7e8a25f53efe8135ab5000dfaaa6a7b5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c5ab945745fa9b9f0e527f229d00a670.jpg" align="middle">
</details>




<h2 id="POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications"><a href="#POINTS1-5-Building-a-Vision-Language-Model-towards-Real-World-Applications" class="headerlink" title="POINTS1.5: Building a Vision-Language Model towards Real World   Applications"></a>POINTS1.5: Building a Vision-Language Model towards Real World   Applications</h2><p><strong>Authors:Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, Jie Zhou</strong></p>
<p>Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters </p>
<blockquote>
<p>视觉语言模型近期取得了重大进展，在各种任务中表现出卓越的性能，例如光学字符识别和复杂图表分析。基于这一趋势，我们推出了一款新的视觉语言模型——POINTS1.5，旨在在各种现实应用中获得卓越表现。POINTS1.5是POINTS1.0的增强版，并融入了若干关键创新：</p>
</blockquote>
<p>一）我们替换了原始的CLIP视觉编码器，该编码器具有固定图像分辨率，使用NaViT风格的视觉编码器，支持原生动态高分辨率。这使得POINTS1.5能够处理任何分辨率的图像，而无需将它们分割成瓦片。</p>
<p>二）我们为POINTS1.5增加了双语支持，这极大地提高了其在中文方面的能力。由于视觉语言模型的中文开源数据集稀缺，我们从互联网收集了大量图像，并使用手动和自动方法相结合的方式进行标注。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08443v1">PDF</a> </p>
<p><strong>Summary</strong>：近期，视界语言模型在多任务处理方面取得显著进展。在此趋势下推出新一代视界语言模型POINTS1.5，它是POINTS1.0的升级版，具有多项关键技术改进。包括采用支持动态高分辨率的NaViT风格视觉编码器，提升中文支持能力，并收集大量互联网图像进行标注，以及为视觉指令微调数据集提供严格过滤方法。这些创新使POINTS1.5显著优于POINTS1.0，并在多个真实应用场景中表现出强劲性能。特别是，POINTS1.5-7B在训练令牌少于4亿的情况下就在OpenCompass排行榜上排名第一，并且在参数少于10亿的模型中独占鳌头。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>POINTS1.5是新一代视界语言模型，基于POINTS1.0进行升级。</li>
<li>采用NaViT风格的视觉编码器以支持动态高分辨率图像处理。</li>
<li>提供对中文的强力支持，通过收集并标注大量互联网图像增强模型能力。</li>
<li>严格过滤方法用于创建视觉指令微调数据集。</li>
<li>POINTS1.5性能卓越，显著优于上一代模型。</li>
<li>POINTS1.5-7B训练令牌数少于4亿即在OpenCompass排行榜上领先。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dc8c0e0a9e3ea04dd5e87e92c23e234d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9e46b078851b25402b8004072ad51698.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9d7787c431966427ede1048c8a9a6984.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2581b57e819a19b0d70e0fc3e8455559.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9ade5bc08b53234389efb475173eb8d6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-14763de648f4e6faa4d40ae81fa249ac.jpg" align="middle">
</details>




<h2 id="The-Roles-of-English-in-Evaluating-Multilingual-Language-Models"><a href="#The-Roles-of-English-in-Evaluating-Multilingual-Language-Models" class="headerlink" title="The Roles of English in Evaluating Multilingual Language Models"></a>The Roles of English in Evaluating Multilingual Language Models</h2><p><strong>Authors:Wessel Poelman, Miryam de Lhoneux</strong></p>
<p>Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from this imprecise method and instead focus on furthering language understanding. </p>
<blockquote>
<p>多语言自然语言处理正受到越来越多的关注，针对多种语言的模型、基准测试方法和技术不断推出。在跨语言评估中，英语常被用来提示语言模型（LMs），主要是为了克服其他语言指令调整数据的缺乏。在这篇立场论文中，我们阐述了英语在多语言LM评估中的两个作用：作为接口和作为自然语言。我们认为这两个角色有不同的目标：任务性能与语言理解。通过数据集和评估设置的例子，我们强调了这一点。许多研究明确使用英语作为接口来提高任务性能。我们建议放弃这种不精确的方法，转而专注于提高语言理解能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08392v1">PDF</a> NoDaLiDa 2025</p>
<p><strong>Summary</strong></p>
<p>随着多语言自然语言处理技术的不断发展，英语在多语言语言模型评估中扮演着重要的角色。本文探讨了英语在评估中的两种角色：作为接口和作为自然语言。文章指出这两种角色有不同的目标，分别是任务性能与语言理解。文章通过数据集和评估设置的例子强调了这种差异，并建议放弃使用英语作为接口以提高任务性能的方法，转而专注于提高语言理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>英语在多语言自然语言处理中扮演着重要角色。</li>
<li>英语在评估多语言语言模型时有两种角色：作为接口和作为自然语言。</li>
<li>使用英语作为接口可以提高任务性能，但这并不是一个精确的方法。</li>
<li>评估多语言语言模型时，应关注语言理解而非仅仅任务性能。</li>
<li>文章通过数据集和评估设置的例子来强调英语在评估中的不同角色和目标。</li>
<li>文章建议放弃使用英语作为接口的方法，并转向专注于提高语言理解的研究方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e4913fb529f06a6d8ce3be82bddd61e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-425a52c1120dea05a7467888bdb16f84.jpg" align="middle">
</details>




<h2 id="SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs"><a href="#SmolTulu-Higher-Learning-Rate-to-Batch-Size-Ratios-Can-Lead-to-Better-Reasoning-in-SLMs" class="headerlink" title="SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs"></a>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better   Reasoning in SLMs</h2><p><strong>Authors:Sultan Alrashed</strong></p>
<p>We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI’s Tulu 3 post-training pipeline to enhance Huggingface’s SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval ($\Delta$11%), and mathematical reasoning with 51.6% on GSM8K ($\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC ($\Delta5.4%$). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models. </p>
<blockquote>
<p>我们推出了SmolTulu-1.7b-Instruct，本报告称其为SmolTulu-DPO-1130，这是一款经过指令调整的语言模型，它适应了AllenAI的Tulu 3后训练管道，以优化Huggingface的SmolLM2-1.7B基础模型。我们通过使用1.35亿参数模型进行的综合实证分析表明，学习率与批次大小之间的关系会显著影响模型在任务依赖方面的性能。我们的研究结果显示出明显的分歧：推理任务（如ARC和GSM8K）得益于较高的学习率与批次大小比例，而模式识别任务（如HellaSwag和IFeval）在较低的比例下表现出最佳性能。这些见解为SmolTulu的开发提供了指导，SmolTulu在指令遵循方面达到了子2B参数模型中的最新水平，在IFeval上得分为67.7%（提高了11%），在数学推理方面，GSM8K得分为51.6%（提高了3.4%），另一版本在ARC上得分达到57.1%（提高了5.4%）。我们发布我们的模型、训练配方和消融研究，以促进效率模型对齐的进一步研究，表明优化动力学的仔细适应有助于缩小小型和大型语言模型之间的能力差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08347v1">PDF</a> 10 pages, 4 figures, and 13 tables. For the SmolTulu-1.7b-instruct   model, see: <a target="_blank" rel="noopener" href="https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct">https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct</a></p>
<p><strong>Summary</strong>：</p>
<p>我们推出了SmolTulu-1.7b-Instruct语言模型，该模型基于Huggingface的SmolLM2-1.7B基础模型，采用了AllenAI的Tulu 3后训练管道进行改进。通过实证研究发现，学习率与批次大小之间的关系会任务依赖地影响模型性能。在推理任务中，较高的学习率与批次大小比例有助于模型表现，而在模式识别任务中，较低的比例则表现最佳。这些见解为SmolTulu的开发提供了指导，该模型在指令遵循和数学推理任务上达到了子2B参数模型中的最佳性能水平。我们发布模型、训练配方和切除研究，以推动高效模型对齐研究，表明优化动力学的细心适应有助于缩小大小语言模型之间的能力差距。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>SmolTulu-1.7b-Instruct是一个基于Huggingface的SmolLM2-1.7B基础模型改进而来的指令训练语言模型。</li>
<li>学习率与批次大小的关系对模型性能有重要影响，这在不同任务中有不同的表现。</li>
<li>在推理任务中，较高的学习率与批次大小比例有助于提高模型表现。</li>
<li>在模式识别任务中，较低的学习率与批次大小比例有助于模型达到最佳性能。</li>
<li>SmolTulu在指令遵循和数学推理任务上达到了子2B参数模型中的最佳性能水平。</li>
<li>我们发布了模型、训练配方和切除研究以促进高效模型对齐研究的进一步发展。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-24a7f844f882bf1f6c5dd15898647b0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6cd5849efc9a4186ec9b7fd8c1be2dc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-da3951edd3c067eb7bc6364e65cd3c28.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-36b8936c38972af9d7874a2416d73f91.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9b4385e2bbae30872b51b25a5ff9ad95.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9194cf58ba8f617b42c5e6436bb8b20f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-16a4ac62ce8ba497822f4f90331bb613.jpg" align="middle">
</details>




<h2 id="Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training"><a href="#Template-Matters-Understanding-the-Role-of-Instruction-Templates-in-Multimodal-Language-Model-Evaluation-and-Training" class="headerlink" title="Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training"></a>Template Matters: Understanding the Role of Instruction Templates in   Multimodal Language Model Evaluation and Training</h2><p><strong>Authors:Shijian Wang, Linxin Song, Jieyu Zhang, Ryotaro Shimizu, Ao Luo, Li Yao, Cunjian Chen, Julian McAuley, Hanqian Wu</strong></p>
<p>Current multimodal language models (MLMs) evaluation and training approaches overlook the influence of instruction format, presenting an elephant-in-the-room problem. Previous research deals with this problem by manually crafting instructions, failing to yield significant insights due to limitations in diversity and scalability. In this work, we propose a programmatic instruction template generator capable of producing over 39B unique template combinations by filling randomly sampled positional synonyms into weighted sampled meta templates, enabling us to comprehensively examine the MLM’s performance across diverse instruction templates. Our experiments across eight common MLMs on five benchmark datasets reveal that MLMs have high template sensitivities with at most 29% performance gaps between different templates. We further augment the instruction tuning dataset of LLaVA-1.5 with our template generator and perform instruction tuning on LLaVA-1.5-7B and LLaVA-1.5-13B. Models tuned on our augmented dataset achieve the best overall performance when compared with the same scale MLMs tuned on at most 75 times the scale of our augmented dataset, highlighting the importance of instruction templates in MLM training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a> . </p>
<blockquote>
<p>当前的多模态语言模型（MLM）评估和训练方法忽视了指令格式的影响，这就像一个房间里的大象问题。之前的研究通过人工构建指令来解决这个问题，但由于多样性和可扩展性的限制，未能产生显著的见解。在这项工作中，我们提出了一种程序化指令模板生成器，通过向加权采样的元模板中填充随机采样的位置同义词，能够产生超过3 结据将文同内容的关键数据有所不同产生了大约每种关键词特征格式我们旨在解决的策略的问题是用关键词信息的简洁的语言回答其中引入数据字段可以根据回答和问题进行有效灵活。采用这一生成器，我们能够全面考察MLM在不同指令模板下的性能。我们在五个基准数据集上对八个常见的MLM进行的实验表明，MLM对模板的敏感性很高，不同模板之间性能差距最大可达29%。我们进一步使用我们的模板生成器扩充了LLaVA-1.5的指令调整数据集，并在LLaVA-1.5-7B和LLaVA-1.5-13B上进行了指令调整。与我们增强的数据集相比调校的模型的表现与其他大型训练语料库的同类规模MLM表现更优在最坏情况下增强了训练数据集三倍效果比较显示本项研究的训练语料库的重要性不言而喻我们的代码已在GitHub上公开，地址是：<a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters%E3%80%82">https://github.com/shijian2001/TemplateMatters。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08307v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/shijian2001/TemplateMatters">https://github.com/shijian2001/TemplateMatters</a></p>
<p><strong>Summary</strong></p>
<p>本文指出当前的多模态语言模型（MLMs）在评估和训练过程中忽视了指令格式的影响，为此提出了一种程序化指令模板生成器。该生成器通过随机填充加权采样元模板中的位置同义词，能够产生超过39亿种独特的模板组合，从而全面考察MLM在不同指令模板下的性能。实验结果显示，MLMs对模板的敏感性很高，不同模板间性能差异最大可达29%。此外，通过对LLaVA-1.5指令调优数据集进行模板增强，并在LLaVA-1.5-7B和LLaVA-1.5-13B上进行指令调优，模型在与其他规模相同的MLMs相比时取得了最佳性能，这突显了指令模板在MLM训练中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的多模态语言模型（MLMs）评估和训练过程中忽视了指令格式的影响。</li>
<li>提出了一种程序化指令模板生成器，能够产生大量独特的模板组合。</li>
<li>MLMs对模板的敏感性很高，不同模板间性能差异显著。</li>
<li>通过模板增强指令调优数据集，可以提高模型性能。</li>
<li>指令模板在MLM训练中具有重要意义。</li>
<li>模型在与其他规模相同的MLMs相比时取得了最佳性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9cf18eab5fd423cf391c7a0d944472c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3bc8c42e8cb0aed2474be1f549a2daa9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-92fd609fdd9fbcf040f463966ba2a464.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ab98d0c0816798cff3762dfe29d4bbdd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9158b85ce187a5753e0f5956ebbfa739.jpg" align="middle">
</details>




<h2 id="M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis"><a href="#M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis" class="headerlink" title="M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis"></a>M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis</h2><p><strong>Authors:Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang</strong></p>
<p>Sentiment analysis and emotion recognition are crucial for applications such as human-computer interaction and depression detection. Traditional unimodal methods often fail to capture the complexity of emotional expressions due to conflicting signals from different modalities. Current Multimodal Large Language Models (MLLMs) also face challenges in detecting subtle facial expressions and addressing a wide range of emotion-related tasks. To tackle these issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy for general-purpose MLLMs. It employs a combined approach to train models on tasks such as multimodal sentiment analysis, emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction. We also introduce the Emotion Multitask dataset (EMT), a custom dataset that supports these five tasks. Our model, Emotion Universe (EmoVerse), is built on a basic MLLM framework without modifications, yet it achieves substantial improvements across these tasks when trained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-of-the-art results in sentiment and emotion tasks. These results highlight the effectiveness of M2SE in enhancing multimodal emotion perception. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE">https://github.com/xiaoyaoxinyi/M2SE</a>. </p>
<blockquote>
<p>情感分析和情绪识别在人机交互和抑郁症检测等应用中至关重要。传统的单模态方法由于来自不同模态的信号相互冲突，往往无法捕捉情绪表达的复杂性。当前的多模态大型语言模型（MLLMs）在检测细微面部表情和应对各种情绪相关任务时也面临挑战。为了解决这些问题，我们提出了M2SE，这是一种为通用MLLMs设计的多阶段多任务情感和情绪指令调整策略。它采用组合方法，在诸如多模态情感分析、情绪识别、面部表情识别、情感推理推断和情感成因对提取等任务上训练模型。我们还介绍了情感多任务数据集（EMT），这是一个支持这五个任务的专业数据集。我们的模型“情感宇宙”（EmoVerse）建立在基本的MLLM框架上，无需修改，但在使用M2SE策略进行训练时，这些任务上的表现有了显著提高。大量实验表明，EmoVerse优于现有方法，在情感和情绪任务中达到最新水平。这些结果突出了M2SE在提高多模态情绪感知方面的有效性。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaoyaoxinyi/M2SE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了情感分析和情绪识别在人机交互和抑郁症检测等领域的重要性。传统的单模态方法由于不同模态信号的冲突，往往无法捕捉情感表达的复杂性。针对这一问题，提出了M2SE，即多阶段多任务情感情绪指令调整策略，用于通用多模态大型语言模型（MLLM）。该策略结合多种任务训练模型，如多模态情感分析、情绪识别、面部表情识别、情感推理和情感因果配对提取等。同时，引入Emotion Multitask数据集（EMT）支持这五个任务。实验结果表明，基于M2SE策略的模型Emotion Universe（EmoVerse）在各项任务上取得了显著的提升，达到了情感与情绪领域的最佳水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感分析和情绪识别在多个领域具有关键作用，如人机交互和抑郁症检测。</li>
<li>传统单模态方法在情感表达捕捉上存在局限性，因为不同模态信号的冲突。</li>
<li>M2SE策略被提出以解决多模态情感识别问题，包括多阶段和多任务训练。</li>
<li>M2SE策略结合了多种任务，如多模态情感分析、情绪识别等。</li>
<li>引入Emotion Multitask数据集（EMT）以支持多任务训练的需求。</li>
<li>基于M2SE策略的模型Emotion Universe（EmoVerse）在各项任务上取得了显著进步。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aa73ededcaefa000e8edfeba52a89b72.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c263fb335b04bc682be5a1cfa8e6e297.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4d6b6e0f4051342c6a0898d92d999f1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32acf009fc023384915a65c05d3f43ff.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd6c980356ed97ed5bc3e5d3fac353bb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9184bb36e2f4b1291065f58d8c14d467.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-19b8821a0398494275c6700434720d2e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5c8749cd2121f800ea47ff15bb3e861f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-81ab60fd15fbdd7102b7bf0b9ceb0523.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-65f47588ea960ad6d8e96c0eaec02120.jpg" align="middle">
</details>




<h2 id="SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models"><a href="#SAT-Spatial-Aptitude-Training-for-Multimodal-Language-Models" class="headerlink" title="SAT: Spatial Aptitude Training for Multimodal Language Models"></a>SAT: Spatial Aptitude Training for Multimodal Language Models</h2><p><strong>Authors:Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko</strong></p>
<p>Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23%$ on CVBench, $8%$ on the harder BLINK benchmark, and $18%$ on VSR. When instruction-tuned on SAT, our 13B model matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data&#x2F;code is available at <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a> . </p>
<blockquote>
<p>空间感知是智能的基本组成部分。虽然许多研究强调大型多模态语言模型（MLMs）在推理空间时遇到困难，但它们只测试静态空间推理，例如对物体的相对位置进行分类。然而，现实世界的部署需要动态能力，如观点提取和第一人称动作识别。作为提高空间智能的路线图，我们引入了SAT（空间适应能力训练），它超越了静态的相对物体位置问题，涵盖了更动态的任务。SAT包含21.8万个问答对，涉及训练和测试集的2.2万个合成场景。使用逼真的物理引擎生成，我们的数据集可以任意扩展，并轻松扩展到新的动作、场景和3D资产。我们发现，即使在静态问题上表现相对较好的MLM也难以准确回答动态空间问题。此外，我们还发现，SAT指令调整数据不仅提高了SAT上的动态空间推理能力，还提高了现有真实图像空间基准测试中的零样本性能：CVBench上提高23%，难度更高的BLINK基准测试上提高8%，以及VSR上提高19%。当在SAT上进行指令调整时，我们的1.3B模型在空间推理方面与大型专有MLM（如GPT4-V和Gemini-3-1.0）相匹配。我们的数据和代码可在<a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/%E6%89%BE%E5%88%B0%E3%80%82">http://arijitray1993.github.io/SAT/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07755v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="http://arijitray1993.github.io/SAT/">http://arijitray1993.github.io/SAT/</a></p>
<p><strong>Summary</strong><br>     空间感知是智能的基本组成部分。当前多数研究集中于测试大型多模态语言模型（MLM）在静态空间推理方面的能力，但现实世界需要动态的空间感知能力，如视角判断和自主动作识别等。为此，本文提出空间能力训练（SAT），并构建相应的数据集，用于提升模型对动态空间任务的智能水平。实验显示，即使对静态问题表现良好的MLM在动态空间问题上仍显不足，而通过SAT训练的数据能够显著提升其在多种空间推理基准测试上的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间感知是智能的核心组成部分，现实世界中需要动态的空间感知能力。</li>
<li>当前研究主要关注静态空间推理，而真实场景需要更复杂的动态空间任务。</li>
<li>引入Spatial Aptitude Training (SAT)作为提升空间智能的路线图。</li>
<li>SAT数据集包含21.8万问题答案对，可用于训练模型进行动态空间任务。</li>
<li>SAT数据集使用逼真的物理引擎生成，可任意扩展，易于添加新动作、场景和3D资产。</li>
<li>MLM在静态问题上表现良好，但在动态空间问题上仍有困难。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4726f5787390ded5869518fbae49fa4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d1ac83f59cc86700f58e482ccc874082.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8ee4e23739632ca4492284e948ea332.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db09dd969f25a2ed0382bcf5c8ce13cc.jpg" align="middle">
</details>




<h2 id="Scaling-Sequential-Recommendation-Models-with-Transformers"><a href="#Scaling-Sequential-Recommendation-Models-with-Transformers" class="headerlink" title="Scaling Sequential Recommendation Models with Transformers"></a>Scaling Sequential Recommendation Models with Transformers</h2><p><strong>Authors:Pablo Zivic, Hernan Vazquez, Jorge Sanchez</strong></p>
<p>Modeling user preferences has been mainly addressed by looking at users’ interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation.   The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models.   Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation.   We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application.   We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency.   We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.   Code and models can be found at <a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt">https://github.com/mercadolibre/srt</a> </p>
<blockquote>
<p>通过查看用户与系统中可用元素之间的交互历史来主要解决用户偏好建模的问题。根据历史数据为个体量身定制内容是顺序推荐的主要目标。问题的性质以及在各个领域中观察到的良好性能，激发了使用变压器架构的动机。当模型参数数量增加时，该架构在利用越来越多的训练数据方面证明是有效的。这种扩展行为引起了极大的关注，因为它为设计更大的模型以及训练提供了有价值的指导。从训练大型语言模型中观察到的扩展定律中汲取灵感，我们探索了顺序推荐的类似原则。我们使用完整的亚马逊产品数据集，该数据集在其他研究中仅得到部分探索，并揭示了与语言模型中发现的类似的扩展行为。最优计算训练是可能的，但需要仔细分析特定于应用程序的计算性能权衡。我们还表明，通过在小任务特定领域上对较大的预训练模型进行微调，性能扩展可以应用于下游任务。我们的方法和发现为在高维偏好空间中实际进行模型训练和部署提供了战略路线图，有助于提高训练和推理效率。我们希望本文能够架起潜在变压器与复杂高维顺序推荐之间的桥梁在现实世界的推荐系统中，欢迎访问我们的GitHub代码仓库：<a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt%E3%80%82">https://github.com/mercadolibre/srt。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07585v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文主要通过探讨用户与系统内不同元素的交互历史来建模用户偏好。基于历史数据的个性化内容推荐是顺序推荐的主要目标。本文受到transformer架构在利用大量训练数据和模型参数方面的良好性能的启发，该架构在处理大型模型设计和训练时表现出显著的缩放行为，受到广泛关注。本文借鉴大型语言模型的缩放规律，探索顺序推荐的类似原则。我们使用完整的亚马逊产品数据集，该数据集在其他研究中仅得到部分探索，揭示与语言模型相似的缩放行为。计算最优训练是可能的，但需要仔细分析特定于应用程序的计算性能权衡。我们还表明，性能缩放可以通过对较小任务特定域进行微调的大型预训练模型转化为下游任务。我们的方法和发现为高维偏好空间中模型训练和部署的战略路线图提供了指导，提高了训练和推理的效率。我们希望这篇论文能够弥补Transformer的潜力与现实世界中高维顺序推荐内在复杂性之间的鸿沟。相关代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/mercadolibre/srt%E5%8D%BB%E5%8F%AF%E4%BB%A5%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mercadolibre/srt找到。</a></p>
<p><strong>要点提炼</strong></p>
<ol>
<li>论文聚焦于通过用户与系统元素的交互历史建模用户偏好，这是顺序推荐的主要目标。</li>
<li>Transformer架构在处理大规模模型时表现出良好的性能缩放行为，对设计更大模型提供了有价值的指导。</li>
<li>论文借鉴语言模型的缩放规律，探索顺序推荐的类似原则。</li>
<li>使用完整的亚马逊产品数据集，揭示与其他研究中不同的缩放行为。</li>
<li>计算最优训练需要分析特定于应用程序的计算性能权衡。</li>
<li>论文展示了性能缩放与下游任务之间的联系，通过微调大型预训练模型以适应小型特定任务领域。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c1432e8c0cfc30fe4ce86a58f601e82f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f92d5d9f7e8fd994ab15fcdfbd147e1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3c5c840c67efc37f67360b32c677b1d2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-496f2027a50d34beb6c7b71d6e2dcca0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7b52962e1da932f53ab0187435c70ac4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3d57eea9b2995fc84c92ca1a92d6276d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8b5b0ccde95d9e8ffabc00bb100fa636.jpg" align="middle">
</details>




<h2 id="Causal-World-Representation-in-the-GPT-Model"><a href="#Causal-World-Representation-in-the-GPT-Model" class="headerlink" title="Causal World Representation in the GPT Model"></a>Causal World Representation in the GPT Model</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules. We find that the GPT model tends to generate next moves that adhere to the game rules for sequences for which the attention mechanism encodes a causal structure with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure. </p>
<blockquote>
<p>生成预训练Transformer（GPT）模型是否仅经过训练以预测下一个令牌，还是它们是否从世界模型中隐式学习生成序列的方式，即一个令牌接一个令牌地生成序列？我们通过推导GPT中注意力机制的因果解释，并据此提出一个因果世界模型来探讨这个问题。此外，我们提出GPT模型在推理时，可以用于执行内部分布序列的零基础因果结构学习。实证评估是在受控的合成环境中进行的，使用Othello游戏的设置和规则。GPT经过在现实世界游戏中预训练以赢得比赛为目标，在仅遵循游戏规则的人工数据上进行测试。我们发现GPT模型倾向于生成符合游戏规则的下一步动作序列，对于注意力机制编码具有高置信度的因果结构序列。一般来说，在GPT模型生成的行动不符合游戏规则的情况下，它也无法捕捉到任何因果结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v1">PDF</a> NeurIPS 2024 Workshop on Causality and Large Models (CaLM)</p>
<p><strong>Summary</strong></p>
<p>GPT模型是否仅通过预测下一个令牌进行训练，还是它们会隐式地学习一个世界模型，通过该模型逐个生成序列？本文通过推导GPT中注意力机制的因果解释来探讨这一问题，并提出了由此产生的因果世界模型。此外，我们提出GPT模型在推理时可用于零射击因果结构学习，适用于内部序列分布。在控制的合成环境中进行了实证研究，使用Othello游戏的设置和规则。经过在真实世界游戏中训练的GPT模型被测试在仅遵循游戏规则合成数据上。我们发现，GPT模型倾向于生成遵循游戏规则的下一步动作，对于注意力机制编码具有很高置信度的因果结构序列而言尤其如此。一般来说，在GPT模型生成不遵循游戏规则的行动的情况下，它也无法捕捉到任何因果关系。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT模型不仅仅是预测下一个令牌，而是隐式地学习一种世界模型来生成序列。</li>
<li>通过推导GPT中注意力机制的因果解释，提出了因果世界模型的观念。</li>
<li>GPT模型在推理过程中可用于零射击因果结构学习，适用于内部序列分布。</li>
<li>在一个模拟的Othello游戏环境中进行了实证研究来验证这一理论。</li>
<li>GPT模型在真实世界游戏中训练后，能够在合成数据上生成遵循游戏规则的下一步动作。</li>
<li>GPT模型生成的动作是否遵循规则与其能否捕捉到序列的因果关系之间有密切关系。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd533512299dae43bee0963fb6015772.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg" align="middle">
</details>




<h2 id="HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass"><a href="#HARP-Hesitation-Aware-Reframing-in-Transformer-Inference-Pass" class="headerlink" title="HARP: Hesitation-Aware Reframing in Transformer Inference Pass"></a>HARP: Hesitation-Aware Reframing in Transformer Inference Pass</h2><p><strong>Authors:Romain Storaï, Seung-won Hwang</strong></p>
<p>This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to “off-the-shelf” Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact. </p>
<blockquote>
<p>本文旨在通过解决推理步骤中可变的计算需求来提高大型语言模型的性能，其中一些标记符需要比其他标记符更多的计算资源。我们提出了HARP，这是对“现成的”Transformer前向传播的简单修改。它借鉴了决策制定中的犹豫和框架效应，当模型在生成标记符时遇到不确定性时，HARP会选择性地应用额外的计算。我们的方法通过在难以决策的点上暂停并重新调整输入视角来模仿人类的认知过程。与其他方法不同，HARP具有模型无关性、无需训练且易于实现的特点。我们在各种下游任务和模型规模上全面评估了我们的方法，证明了性能提高了高达+5.16%。值得注意的是，HARP在保持推理时间是光束搜索的两倍的同时实现了这些收益。简单而效果显著，HARP为在最小计算影响的情况下提高基于Transformer的语言模型的性能提供了切实可行的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文旨在通过解决推理步骤中可变的计算需求来提高大语言模型的性能。针对某些令牌需要比其它令牌更多的计算资源的问题，本文提出了一种名为HARP的简单修改方案，用于改进“即买即用”Transformer前向传递。HARP借鉴了决策制定中的犹豫和框架效应，在模型在令牌生成过程中遇到不确定性时选择性地应用额外的计算。该方法通过模仿人类的认知过程，在困难的决策点上暂停并重新构建输入以获得不同的视角。与其他方法相比，HARP具有模型无关性、无需训练且易于实现的特点。我们在各种下游任务和模型大小上全面评估了该方法，证明了其性能改进可达+5.16%。值得注意的是，HARP在保持推理速度比束搜索快两倍的同时实现了这些收益。HARP简单而效果显著，为增强基于Transformer的语言模型的性能提供了切实可行的解决方案，且计算影响较小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HARP旨在解决大语言模型中遇到的计算需求问题，特别是针对某些需要更多计算资源的令牌。</li>
<li>HARP借鉴了犹豫和框架效应，在模型遇到不确定性时增加额外的计算。</li>
<li>HARP模仿人类的认知过程，在决策过程中暂停并重新构建输入。</li>
<li>HARP具有模型无关性，无需训练，易于实施。</li>
<li>在各种下游任务和模型大小上，HARP的性能改进可达+5.16%。</li>
<li>HARP保持了比束搜索更快的推理速度，同时实现了性能提升。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bd54425be6c50fa98ef8656d707413c8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-963b17fc1c818a02111506456b683518.jpg" align="middle">
</details>




<h2 id="A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis"><a href="#A-Review-on-the-Applications-of-Transformer-based-language-models-for-Nucleotide-Sequence-Analysis" class="headerlink" title="A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis"></a>A Review on the Applications of Transformer-based language models for   Nucleotide Sequence Analysis</h2><p><strong>Authors:Nimisha Ghosh, Daniele Santoni, Indrajit Saha, Giovanni Felici</strong></p>
<p>In recent times, Transformer-based language models are making quite an impact in the field of natural language processing. As relevant parallels can be drawn between biological sequences and natural languages, the models used in NLP can be easily extended and adapted for various applications in bioinformatics. In this regard, this paper introduces the major developments of Transformer-based models in the recent past in the context of nucleotide sequences. We have reviewed and analysed a large number of application-based papers on this subject, giving evidence of the main characterizing features and to different approaches that may be adopted to customize such powerful computational machines. We have also provided a structured description of the functioning of Transformers, that may enable even first time users to grab the essence of such complex architectures. We believe this review will help the scientific community in understanding the various applications of Transformer-based language models to nucleotide sequences. This work will motivate the readers to build on these methodologies to tackle also various other problems in the field of bioinformatics. </p>
<blockquote>
<p>近年来，基于Transformer的语言模型在自然语言处理领域产生了巨大影响。由于生物序列和自然语言之间存在相似性，NLP中使用的模型可以轻松地扩展并适应生物信息学中的各种应用。在这方面，本文介绍了最近过去基于Transformer的模型在核苷酸序列方面的主要发展。我们回顾并分析了大量关于此主题的应用论文，证明了主要特征以及可能采用的不同方法来定制这些强大的计算机。我们还提供了Transformer运行机制的结构化描述，甚至可以让首次接触的用户也能理解这些复杂架构的核心。我们相信这次回顾将有助于科学界了解基于Transformer的语言模型在核苷酸序列方面的各种应用。这项工作将激励读者基于这些方法来解决生物信息学领域的各种问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07201v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Transformer的自然语言处理模型在生物信息学领域有广泛应用前景。本文介绍了近期Transformer模型在处理核苷酸序列方面的主要进展，分析了相关应用论文，并详细描述了Transformer的工作原理。本文有助于科学界了解Transformer模型在核苷酸序列处理方面的应用，并鼓励基于这些方法解决生物信息学的其他问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型在自然语言处理领域具有显著影响。</li>
<li>Transformer模型可应用于生物信息学领域。</li>
<li>Transformer模型在处理核苷酸序列方面展现出重要进展。</li>
<li>本文分析了大量关于Transformer模型在核苷酸序列处理方面的应用论文。</li>
<li>本文详细描述了Transformer的工作原理。</li>
<li>本文有助于科学界了解Transformer模型在核苷酸序列处理方面的应用前景。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-479dbd5c07313b7fbbcd5d183ab4d03c.jpg" align="middle">
</details>




<h2 id="ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models"><a href="#ProVision-Programmatically-Scaling-Vision-centric-Instruction-Data-for-Multimodal-Language-Models" class="headerlink" title="ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models"></a>ProVision: Programmatically Scaling Vision-centric Instruction Data for   Multimodal Language Models</h2><p><strong>Authors:Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, silvio savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu</strong></p>
<p>With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image-based queries. Existing practices rely on powerful but costly large language models (LLMs) or multimodal language models (MLMs) to produce instruction data. These are often prone to hallucinations, licensing issues and the generation process is often hard to scale and interpret. In this work, we present a programmatic approach that employs scene graphs as symbolic representations of images and human-written programs to systematically synthesize vision-centric instruction data. Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy. By implementing a suite of 24 single-image, 14 multi-image instruction generators, and a scene graph generation pipeline, we build a scalable, cost-effective system: ProVision which produces diverse question-answer pairs concerning objects, attributes, relations, depth, etc., for any given image. Applied to Visual Genome and DataComp datasets, we generate over 10 million instruction data points, ProVision-10M, and leverage them in both pretraining and instruction tuning stages of MLMs. When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D split of CVBench, along with a 3% increase in performance on QBench2, RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8% improvement on Mantis-Eval. Incorporation of our data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks. </p>
<blockquote>
<p>随着多模态应用的兴起，指令数据对于训练能够理解复杂图像查询的多模态语言模型变得至关重要。现有实践依赖于强大但成本高昂的大型语言模型（LLM）或多模态语言模型（MLM）来生成指令数据。这些方法往往容易出现幻觉、许可问题，且生成过程往往难以扩展和解释。在这项工作中，我们提出了一种程序化方法，它利用场景图作为图像的象征表示和人类编写的程序来系统地合成以视觉为中心的指令数据。我们的方法确保了数据生成过程的可解释性和可控性，在保持事实准确性的同时，实现了高效扩展。通过实施24个单图像、14个多图像指令生成器以及场景图生成管道，我们建立了一个可扩展、成本效益高的系统：ProVision。该系统可针对给定图像生成涉及对象、属性、关系、深度等方面的多样化问答对。在Visual Genome和DataComp数据集上应用时，我们生成了超过1000万的指令数据点，即ProVision-10M，并将其用于MLM的预训练和指令调整阶段。在指令调整阶段采用我们的单图像指令数据，在CVBench的2D分割和3D分割上分别提高了7%和8%的性能，同时在QBench2、RealWorldQA和MMMU上的性能也提高了3%。我们的多图像指令数据在Mantis-Eval上提高了8%。在xGen-MM-4B的预训练和微调阶段都纳入我们的数据，平均在11个基准测试上提高了1.6%的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07012v2">PDF</a> code: <a target="_blank" rel="noopener" href="https://github.com/JieyuZ2/ProVision">https://github.com/JieyuZ2/ProVision</a> dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Salesforce/ProVision-10M">https://huggingface.co/datasets/Salesforce/ProVision-10M</a></p>
<p><strong>摘要</strong><br>    随着多模态应用的兴起，指令数据对于训练能够理解复杂图像查询的多模态语言模型至关重要。现有的方法依赖于强大但昂贵的语言模型或多模态语言模型来生成指令数据，这容易产生幻觉、授权问题，且生成过程难以扩展和解释。本研究采用场景图作为图像的象征性表示和人工编写程序，系统性地合成以视觉为中心的指令数据。这种方法确保了数据生成的解释性和可控性，同时在保持事实准确性的同时实现了高效扩展。通过实施一系列图像指令生成器及场景图生成管道，我们建立了可伸缩、经济的ProVision系统，针对给定的图像生成了关于对象、属性、关系等的多样化问答对。在Visual Genome和DataComp数据集上应用时，我们生成了超过1亿个指令数据点，即ProVision-10M，并将其用于MLMs的预训练和指令调整阶段。在指令调整阶段采用我们的单图像指令数据，在CVBench的2D分割和3D分割上分别提高了7%和8%的性能，同时在QBench2、RealWorldQA和MMMU上的性能也提高了3%。我们的多图像指令数据在Mantis-Eval上提高了8%。在xGen-MM-4B的预训练和微调阶段融入我们的数据，在11个基准测试上的平均改进率为1.6%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>随着多模态应用的普及，指令数据对训练能够理解复杂图像查询的多模态语言模型至关重要。</li>
<li>现有方法依赖大型语言模型或多模态语言模型生成指令数据，存在幻觉、授权和扩展性问题。</li>
<li>研究采用场景图和人工编写程序系统性地合成视觉为中心指令数据的方法。</li>
<li>该方法确保了数据生成的解释性和可控性，并实现了高效扩展和保持事实准确性。</li>
<li>通过实施一系列图像指令生成器，建立了可伸缩的ProVision系统，针对给定图像生成多样化问答对。</li>
<li>在多个数据集上应用ProVision系统，生成超过1亿个指令数据点。</li>
<li>这些指令数据在多个基准测试上显著提高了模型性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e8cb97cdfe610306866f0725881bbf9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-92c79b6de64dbde4592691764c5cef14.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dbabc542464f3caae8fe55262ded5040.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7fcbc119bff24db4fd8bb63ed2894e66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e5f2e25d56c2684e221ae531b2a976f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6c42593b541d8ad98e8a0d1d9645bdb5.jpg" align="middle">
</details>




<h2 id="Understanding-Factual-Recall-in-Transformers-via-Associative-Memories"><a href="#Understanding-Factual-Recall-in-Transformers-via-Associative-Memories" class="headerlink" title="Understanding Factual Recall in Transformers via Associative Memories"></a>Understanding Factual Recall in Transformers via Associative Memories</h2><p><strong>Authors:Eshaan Nichani, Jason D. Lee, Alberto Bietti</strong></p>
<p>Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior. </p>
<blockquote>
<p>大型语言模型在事实回忆方面表现出了令人印象深刻的能力。之前的研究发现，在事实回忆任务上训练的变压器的信息存储量与参数数量成比例。在我们的研究中，我们展示了浅层变压器可以通过结合关联记忆来获得近乎最优的存储能力。我们首先证明，线性关联记忆和多层感知机（MLP）关联记忆存储能力随着参数数量的增加而线性扩展。接下来，我们引入了一个合成的事实回忆任务，并证明了一个单层自注意力机制后跟一个多层感知机的变压器，每当自注意力的总参数或MLP参数数量（直至对数因素）与事实数量线性相关时，都能在该任务上达到百分之百的准确率。特别是，变压器可以在使用值矩阵或利用多层感知机作为关联记忆来存储事实数据集之间进行权衡。我们通过分析简化线性注意力模型在我们的事实回忆任务上的梯度流动轨迹来补充这些表达性结果，显示模型表现出序列学习行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06538v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型展现出强大的事实记忆能力。本研究证明浅层变压器可通过结合关联记忆达到近乎最优的存储能力。我们证明了线性与MLP关联记忆的存储能力与参数数量呈线性关系。在合成的事实回忆任务中，单层自注意力后的MLP变压器，在自注意力参数或MLP参数数量与事实数量呈线性关系（对数因素内）时，可达到100%准确率。特别是，变压器可以在使用值矩阵或MLP作为关联记忆来存储事实数据集之间进行权衡。我们通过这些表达力结果，补充了简化线性注意力模型在事实回忆任务上梯度流轨迹的分析，显示模型表现出序列学习行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型具备出色的事实记忆能力。</li>
<li>浅层变压器结合关联记忆可达到近乎最优的存储能力。</li>
<li>线性与MLP关联记忆的存储能力与参数数量呈线性关系。</li>
<li>在合成的事实回忆任务中，变压器结合自注意力与MLP可取得极高准确率。</li>
<li>变压器可以在使用不同的记忆机制（值矩阵或MLP）之间进行权衡。</li>
<li>模型在事实记忆任务中表现出序列学习行为。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9f8343ea89c2fe7f963d845465390cfc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32c839c5936fedb7031cd8088b478b51.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-eefd36c16f7b4f17174d08857bb5b0a8.jpg" align="middle">
</details>




<h2 id="LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations"><a href="#LLaVA-SpaceSGG-Visual-Instruct-Tuning-for-Open-vocabulary-Scene-Graph-Generation-with-Enhanced-Spatial-Relations" class="headerlink" title="LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations"></a>LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph   Generation with Enhanced Spatial Relations</h2><p><strong>Authors:Mingjie Xu, Mengyang Wu, Yuzhi Zhao, Jason Chun Lok Li, Weifeng Ou</strong></p>
<p>Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMs’ inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: <a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG">https://github.com/Endlinc/LLaVA-SpaceSGG</a>. </p>
<blockquote>
<p>场景图生成（SGG）将视觉场景转换为结构化图表示，为复杂的视觉任务提供更深入的场景理解。然而，现有的SGG模型经常忽略重要的空间关系，并且在开放词汇上下文中的泛化能力方面存在困难。为了解决这些局限性，我们提出了LLaVA-SpaceSGG，这是一种为开放词汇SGG设计的多模态大型语言模型（MLLM），具有增强的空间关系建模。为了训练它，我们收集了SGG指令调整数据集，命名为SpaceSGG。该数据集是通过结合公开可用的数据集，并在我们的数据构建管道中使用开源模型合成数据而构建的。它结合了目标位置、目标关系和深度信息，产生三种数据格式：空间SGG描述、问答和对话。为了提高MLLMs的内在能力向SGG任务的转移，我们引入了两阶段训练范式。实验表明，LLaVA-SpaceSGG在公开词汇SGG方法上具有出色表现，相较于基线模型，召回率提高了8.6%，平均召回率提高了28.4%。我们的代码库、数据集和训练好的模型可在以下GitHub链接上公开访问：<a target="_blank" rel="noopener" href="https://github.com/Endlinc/LLaVA-SpaceSGG%E3%80%82">https://github.com/Endlinc/LLaVA-SpaceSGG。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06322v1">PDF</a> Accepted by the WACV 2025, including supplementary material</p>
<p><strong>Summary</strong></p>
<p>SGG（场景图生成）技术能够将视觉场景转化为结构化图表示，为复杂视觉任务提供更深入的场景理解。然而，现有SGG模型在开放词汇上下文中的空间关系建模存在局限性和泛化困难。为此，我们提出了LLaVA-SpaceSGG，一个为开放词汇SGG设计的多模态大型语言模型，具有增强的空间关系建模能力。我们使用自行构建的SpaceSGG数据集进行训练，该数据集结合公开数据集和开源模型的数据构建流程进行合成。实验表明，LLaVA-SpaceSGG在开放词汇SGG方法上的表现优于其他方法，召回率提高8.6%，平均召回率提高28.4%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGG技术能够将视觉场景转化为结构化图形表示，加深场景理解，用于复杂视觉任务。</li>
<li>现有SGG模型在空间关系建模和开放词汇上下文泛化方面存在局限性。</li>
<li>LLaVA-SpaceSGG是一个为开放词汇SGG设计的多模态大型语言模型，具有增强的空间关系建模能力。</li>
<li>SpaceSGG数据集是专门为LLaVA-SpaceSGG模型训练而构建，结合了多种数据来源。</li>
<li>LLaVA-SpaceSGG模型采用两阶段训练范式，以更好地将大型语言模型的内在能力转移到SGG任务上。</li>
<li>LLaVA-SpaceSGG在开放词汇SGG方法上的表现优于其他方法，具有更高的召回率和平均召回率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5d43c40300ef813ab9c294c6d2d9da8b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-667007beb268b4ed3183f8ffceb27f82.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-05320e9817235d934fa69ec3d1db206e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8a013cdba343996ad6176ba72488718.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-89be98b84bbb87523661e45a4b5f1ff1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4568902a2ea3a695ad701e9d47dd19c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7b62a30b1a3eebfc0cd40c49eab39df3.jpg" align="middle">
</details>




<h2 id="Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness"><a href="#Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness" class="headerlink" title="Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness"></a>Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness</h2><p><strong>Authors:Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles–informativeness, uniqueness, and representativeness–for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 100.8% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the “Less is More” philosophy in MLLM development. </p>
<blockquote>
<p>指令调整微调预先训练的多模态大型语言模型（MLLMs）以处理现实世界任务。然而，视觉指令数据集的快速扩张导致了数据冗余，从而增加了计算成本。我们提出了一种协作框架DataTailor，它利用三个关键原则——信息性、唯一性和代表性——进行有效数据选择。我们认为有价值的样本应该具有任务的信息性、非冗余性，并代表样本分布（即非异常值）。我们进一步提出了针对每项原则进行打分的实际方法，这些方法可自动适应给定的数据集，无需繁琐的超参数调整。在各种基准测试上的综合实验表明，DataTailor在仅使用15%数据的情况下，实现了全数据微调性能的100.8%，在降低计算成本的同时保持了卓越的结果。这体现了MLLM发展中的“少即是多”理念。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06293v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong><br>    针对预训练的多模态大型语言模型（MLLMs），提出一种利用信息性、唯一性和代表性三大原则的数据选择框架DataTailor，有效应对真实任务场景中的数据冗余问题，大幅降低计算成本。DataTailor能在保持优越性能的同时减少数据量需求，减少的数据达到使用仅数据15%，其性能高达使用全部数据的性能标准的百分之百零八点以上。此成果展现了在MLLM发展上的“少即是多”理念。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DataTailor是一个协作框架，旨在优化数据选择以解决多模态大型语言模型在实际任务中的计算成本问题。它通过利用三大原则：信息性、唯一性和代表性来确保数据的有效性和高效性。</li>
<li>数据冗余是真实世界任务中一个重要的问题，它对模型性能和计算成本造成了显著影响。数据冗余可能会阻碍模型学习和效率提升，影响性能的表现。在采用MLLMs进行指令微调时，数据冗余问题尤为突出。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-16ba4959c013b5b5057db43147dbcf1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56d1796b5c2c43c9484a2a5ccd1a518a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5da737588d95e9d9a1795383aa0d36a0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d6ddefd66bc4617e55355e51f2d01f13.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-17b558db0836cbaac7eb611b67eef6d9.jpg" align="middle">
</details>




<h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by “selecting sparsely and computing densely”. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>当前针对大型语言模型（LLM）的PEFT方法可以实现高质量、高效训练或可扩展的服务，但无法同时实现三者。为了解决这个问题，我们研究了稀疏微调技术，并观察到其显著提高了泛化能力。利用这一关键见解，我们提出了一系列针对LLM的结构化稀疏微调（S$^{2}$FT）方法，这些方法同时实现了最先进的微调性能、训练效率和推理可扩展性。S$^{2}$FT通过“稀疏选择、密集计算”来实现这一目标。它分别选择MHA和FFN模块中每个Transformer块的几个头和通道。接下来，它对LLM中耦合结构两侧的权重矩阵进行重新排列组合，将所选组件在每一层中连接成密集的子矩阵。最后，S$^{2}$FT对所有子矩阵进行原地梯度更新。通过理论分析和实验结果，我们的方法可以防止过拟合和遗忘，在常识推理和算术推理方面都达到了最新性能水平。与LoRA相比，平均改进了4.6%和1.3%，在指令微调后推广到不同领域时，较全量微调（full FT）高出11.5%。使用我们的部分反向传播算法，S$^{2}$FT在训练内存方面节省了高达3倍，与全量微调相比，延迟提高了1.5-2.7倍，同时在两个指标上都较LoRA平均提高了10%。我们进一步证明，S$^{2}$FT中的权重更新可以解耦为适配器，实现对多个微调模型的有效融合、快速切换和高效并行处理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型语言模型（LLM）的新型结构化稀疏微调（S²FT）方法。此方法在高质量、高效训练和可伸缩服务三个方面均有突破。通过选择稀疏部分进行微调，实现了出色的泛化能力。在理论分析和实验结果的支持下，S²FT不仅防止过拟合和遗忘，而且在常识和算术推理方面表现出卓越性能。此外，S²FT采用部分反向传播算法，节省训练内存，提高延迟，同时平均提升性能达10%。最后，S²FT的重量更新可解耦为适配器，实现多个微调模型的有效融合、快速切换和高效并行服务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>S²FT方法能够在大型语言模型（LLM）中实现高质量、高效训练和可伸缩服务的三者兼顾。</li>
<li>通过稀疏微调提高了模型的泛化能力。</li>
<li>S²FT方法通过选择MHA和FFN模块的部分头部和通道进行微调，并在LLM的耦合结构两侧重新排列权重矩阵，实现“选择稀疏，计算密集”。</li>
<li>S²FT采用部分反向传播算法，节省训练内存，提高训练效率。</li>
<li>S²FT在常识和算术推理方面表现出卓越性能，平均改进超过其他方法。</li>
<li>S²FT的重量更新可以解耦为适配器，便于模型融合、快速切换和高效并行服务。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9dc097c198906efa869b0f4455639b1b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-52615bada518b68dec445640650fa2f0.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
