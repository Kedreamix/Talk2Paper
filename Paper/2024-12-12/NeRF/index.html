<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-12  GN-FRGeneralizable Neural Radiance Fields for Flare Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="GN-FR-Generalizable-Neural-Radiance-Fields-for-Flare-Removal"><a href="#GN-FR-Generalizable-Neural-Radiance-Fields-for-Flare-Removal" class="headerlink" title="GN-FR:Generalizable Neural Radiance Fields for Flare Removal"></a>GN-FR:Generalizable Neural Radiance Fields for Flare Removal</h2><p><strong>Authors:Gopi Raju Matta, Rahul Siddartha, Rongali Simhachala Venkata Girish, Sumit Sharma, Kaushik Mitra</strong></p>
<p>Flare, an optical phenomenon resulting from unwanted scattering and reflections within a lens system, presents a significant challenge in imaging. The diverse patterns of flares, such as halos, streaks, color bleeding, and haze, complicate the flare removal process. Existing traditional and learning-based methods have exhibited limited efficacy due to their reliance on single-image approaches, where flare removal is highly ill-posed. We address this by framing flare removal as a multi-view image problem, taking advantage of the view-dependent nature of flare artifacts. This approach leverages information from neighboring views to recover details obscured by flare in individual images. Our proposed framework, GN-FR (Generalizable Neural Radiance Fields for Flare Removal), can render flare-free views from a sparse set of input images affected by lens flare and generalizes across different scenes in an unsupervised manner. GN-FR incorporates several modules within the Generalizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation (FMG), View Sampler (VS), and Point Sampler (PS). To overcome the impracticality of capturing both flare-corrupted and flare-free data, we introduce a masking loss function that utilizes mask information in an unsupervised setting. Additionally, we present a 3D multi-view flare dataset, comprising 17 real flare scenes with 782 images, 80 real flare patterns, and their corresponding annotated flare-occupancy masks. To our knowledge, this is the first work to address flare removal within a Neural Radiance Fields (NeRF) framework. </p>
<blockquote>
<p>光晕是一种光学现象，由镜头系统中的非必要散射和反射造成，对成像造成了重大挑战。光晕的多种形态，如光环、条纹、色彩溢出和朦胧，使得光晕去除过程复杂化。现有的传统和基于学习的方法由于依赖于单图像方法而表现出有限的效力，其中光晕去除是高度不适定的。我们通过将光晕去除作为多视图图像问题来解决这一问题，利用光晕伪影的视图相关性。这种方法利用邻近视图的信息来恢复单个图像中由光晕掩盖的细节。我们提出的框架GN-FR（用于光晕去除的可泛化神经辐射场）可以从受光晕影响的一组稀疏输入图像中呈现无光晕的视图，并以无监督的方式泛化到不同的场景。GN-FR在可泛化NeRF转换器（GNT）框架中融入了多个模块：光晕占用掩模生成器（FMG）、视图采样器（VS）和点采样器（PS）。为了克服同时捕获受光晕破坏和无光晕数据的不切实际性，我们引入了一个掩模损失函数，该函数在无监督设置中使用掩模信息。此外，我们还推出了一个包含17个真实光晕场景、782张图像、80种真实光晕模式及其相应的注释光晕占用掩模的3D多视图光晕数据集。据我们所知，这是首次在神经辐射场（NeRF）框架内解决光晕去除的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08200v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>光学现象中的耀斑，由于镜头系统中的意外散射和反射而产生，对成像造成了重大挑战。耀斑的多样表现形态，如光晕、条纹、色彩溢出和雾霭，使得耀斑去除过程复杂化。现有的传统和基于学习的方法因依赖单图像方法而表现出有限的效能，其中耀斑去除的高度不稳定性。本文通过将耀斑去除视为多视图图像问题来应对这一问题，利用耀斑伪影的视图相关性。该方法可从邻近视图的信息中恢复单个图像中被耀斑掩盖的细节。本文提出的框架GN-FR（用于耀斑去除的可泛化神经辐射场），可以从一组稀疏的输入图像中渲染无耀斑的视图，并对不同的场景进行无监督的泛化。GN-FR在Generalizable NeRF Transformer（GNT）框架内结合了多个模块：耀斑占用掩模生成（FMG）、视图采样器（VS）和点采样器（PS）。为了克服同时获取耀斑损坏和无耀斑数据的不切实际性，我们引入了一种掩模损失函数，在无监督环境中利用掩模信息。此外，我们还推出了一个包含17个真实耀斑场景、782张图像、80种真实耀斑模式及其相应标注的耀斑占用掩模的3D多视图耀斑数据集。据我们所知，这是首次在神经辐射场（NeRF）框架内解决耀斑去除问题的研究。</p>
<p><strong>要点</strong></p>
<ol>
<li>引入了一种新的方法来解决光学现象中的耀斑问题，该问题由于镜头系统中的意外散射和反射而产生，对成像质量造成严重影响。</li>
<li>提出了一种基于多视图图像的方法，利用视图相关性来解决耀斑去除问题，提高了去除效果。</li>
<li>介绍了GN-FR框架，该框架结合多个模块在Generalizable NeRF Transformer（GNT）中实现了有效的耀斑去除。</li>
<li>创新性地使用了掩模损失函数，可以在无监督环境中利用掩模信息，提高了模型的泛化能力。</li>
<li>构建了一个全新的3D多视图耀斑数据集，为相关研究提供了丰富的数据资源。</li>
<li>据了解，该论文是首次在神经辐射场（NeRF）框架内解决耀斑去除问题的研究。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-865539fc727c81c8f18cb19e5f5784aa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b9409dca8aeca45aa30b12b65e1f5e88.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fff4e385e9c0cbb536f55f17aa8764ed.jpg" align="middle">
</details>




<h2 id="NeRF-NQA-No-Reference-Quality-Assessment-for-Scenes-Generated-by-NeRF-and-Neural-View-Synthesis-Methods"><a href="#NeRF-NQA-No-Reference-Quality-Assessment-for-Scenes-Generated-by-NeRF-and-Neural-View-Synthesis-Methods" class="headerlink" title="NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF   and Neural View Synthesis Methods"></a>NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF   and Neural View Synthesis Methods</h2><p><strong>Authors:Qiang Qu, Hanxue Liang, Xiaoming Chen, Yuk Ying Chung, Yiran Shen</strong></p>
<p>Neural View Synthesis (NVS) has demonstrated efficacy in generating high-fidelity dense viewpoint videos using a image set with sparse views. However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not tailored for the scenes with dense viewpoints synthesized by NVS and NeRF variants, thus, they often fall short in capturing the perceptual quality, including spatial and angular aspects of NVS-synthesized scenes. Furthermore, the lack of dense ground truth views makes the full reference quality assessment on NVS-synthesized scenes challenging. For instance, datasets such as LLFF provide only sparse images, insufficient for complete full-reference assessments. To address the issues above, we propose NeRF-NQA, the first no-reference quality assessment method for densely-observed scenes synthesized from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment strategy, integrating both viewwise and pointwise approaches, to evaluate the quality of NVS-generated scenes. The viewwise approach assesses the spatial quality of each individual synthesized view and the overall inter-views consistency, while the pointwise approach focuses on the angular qualities of scene surface points and their compound inter-point quality. Extensive evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality assessment methods (from fields of image, video, and light-field assessment). The results demonstrate NeRF-NQA outperforms the existing assessment methods significantly and it shows substantial superiority on assessing NVS-synthesized scenes without references. An implementation of this paper are available at <a target="_blank" rel="noopener" href="https://github.com/VincentQQu/NeRF-NQA">https://github.com/VincentQQu/NeRF-NQA</a>. </p>
<blockquote>
<p>神经视图合成（NVS）已经显示出使用稀疏视图图像集生成高保真密集视点视频的效能。然而，现有的质量评估方法，如PSNR、SSIM和LPIPS，并不适用于由NVS和NeRF变体合成的密集视点场景，因此，它们在捕捉感知质量方面常常不足，包括NVS合成场景的的空间和角度方面。此外，缺乏密集的地面真实视图使得对NVS合成场景进行全参考质量评估具有挑战性。例如，LLFF等数据集只提供稀疏图像，不足以进行完整的全参考评估。为了解决上述问题，我们提出了NeRF-NQA，这是第一种无需参考、针对由NVS和NeRF变体合成的密集观察场景的质量评估方法。NeRF-NQA采用联合质量评估策略，结合了视图级和点级方法，来评估NVS生成场景的质量。视图级方法评估每个单独合成视图的空间质量和整体视图间的一致性，而点级方法则关注场景表面点的角度质量以及它们之间的复合点质量。进行了广泛评估，将NeRF-NQA与23种主流视觉质量评估方法（来自图像、视频和光场评估领域）进行比较。结果表明，NeRF-NQA在评估NVS合成场景方面显著优于现有评估方法，并且在无参考评估方面表现出极大的优越性。该论文的实现可访问<a target="_blank" rel="noopener" href="https://github.com/VincentQQu/NeRF-NQA%E3%80%82">https://github.com/VincentQQu/NeRF-NQA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08029v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>神经网络视图合成（NVS）技术能生成高保真密集视角视频，但现有质量评估方法如PSNR、SSIM和LPIPS并不适合评估NVS和NeRF变体合成的密集视角场景的质量。为此，我们提出NeRF-NQA，一种无需参考的密集观察场景质量评估方法。NeRF-NQA采用联合质量评估策略，结合视图级和点级方法，评估NVS生成场景的质量。实验结果表明，NeRF-NQA显著优于现有评估方法，在无参考评估NVS合成场景方面表现出显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NVS技术可以生成高保真密集视角视频，但现有质量评估方法不适用于评估其质量。</li>
<li>NeRF-NQA是首个针对NVS和NeRF变体合成的密集观察场景的无参考质量评估方法。</li>
<li>NeRF-NQA采用联合质量评估策略，包括视图级和点级方法。</li>
<li>视图级方法评估每个合成视图的空间质量和视图间的一致性。</li>
<li>点级方法关注场景表面点的角度质量及其复合点间质量。</li>
<li>与主流视觉质量评估方法相比，NeRF-NQA表现出显著优势。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-000e979692aa4c8d8f144717029a4813.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-775ff7b6a460a7114cf693b922a4bd06.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78219cb1d384ca8d12dbd03ce6eedf46.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abb5c891b1930a5afba3f2ee8b69ede5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-347f43769b5127f2924204f680331dde.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e06e2865f8f7f4232268634bd5d071bc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5cef61c5ce79d5b8b606d40762f67d11.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-34e0eff78638dad61378f2ff30c22917.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-731036771f824acedc6fe22837474de9.jpg" align="middle">
</details>




<h2 id="EventSplat-3D-Gaussian-Splatting-from-Moving-Event-Cameras-for-Real-time-Rendering"><a href="#EventSplat-3D-Gaussian-Splatting-from-Moving-Event-Cameras-for-Real-time-Rendering" class="headerlink" title="EventSplat: 3D Gaussian Splatting from Moving Event Cameras for   Real-time Rendering"></a>EventSplat: 3D Gaussian Splatting from Moving Event Cameras for   Real-time Rendering</h2><p><strong>Authors:Toshiya Yura, Ashkan Mirzaei, Igor Gilitschenski</strong></p>
<p>We introduce a method for using event camera data in novel view synthesis via Gaussian Splatting. Event cameras offer exceptional temporal resolution and a high dynamic range. Leveraging these capabilities allows us to effectively address the novel view synthesis challenge in the presence of fast camera motion. For initialization of the optimization process, our approach uses prior knowledge encoded in an event-to-video model. We also use spline interpolation for obtaining high quality poses along the event camera trajectory. This enhances the reconstruction quality from fast-moving cameras while overcoming the computational limitations traditionally associated with event-based Neural Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that our results achieve higher visual fidelity and better performance than existing event-based NeRF approaches while being an order of magnitude faster to render. </p>
<blockquote>
<p>我们介绍了一种利用事件相机数据通过高斯拼贴进行新颖视图合成的方法。事件相机提供出色的时间分辨率和高动态范围。利用这些功能，我们可以有效解决快速相机运动下新颖视图合成的挑战。为了初始化优化过程，我们的方法使用事件到视频的模型编码的先验知识。我们还使用样条插值来获得事件相机轨迹上的高质量姿态。这提高了快速移动相机的重建质量，同时克服了传统上与基于事件的神视辐射场（NeRF）方法相关的计算限制。我们的实验评估表明，我们的结果实现了更高的视觉保真度和性能，并且渲染速度比现有基于事件的NeRF方法快一个数量级。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07293v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种利用事件相机数据进行新型视图合成的方法，该方法通过高斯光斑技术实现。事件相机具有出色的时间分辨率和高动态范围，利用这些优势能有效解决快速相机运动下的新视图合成挑战。此方法采用事件到视频的模型编码先验知识来进行优化过程的初始化，并使用样条插值法获取高质量的姿态沿事件相机轨迹，从而提高重建质量并克服传统事件基础上的神经辐射场（NeRF）方法的计算限制。实验评估表明，该方法实现了更高的视觉保真度和更好的性能，并且渲染速度比现有事件基础上的NeRF方法快一个数量级。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用事件相机的出色时间分辨率和高动态范围数据进行新型视图合成。</li>
<li>采用事件到视频的模型编码先验知识来进行优化初始化。</li>
<li>使用样条插值法获取高质量的姿态沿事件相机轨迹。</li>
<li>提高重建质量，克服传统事件基础上的NeRF方法的计算限制。</li>
<li>实现更高的视觉保真度和更好的性能。</li>
<li>渲染速度比现有事件基础上的NeRF方法快一个数量级。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d334757298e3a609e928d5ed7294448e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-19e5b8ae6652639ff3b16a241816277d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d149ec977c9fe15354931880a088b743.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eb0e3141278ff281d9541c89ddc28abc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e40e4fc8211446e76a71bf1452fa8ae9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-637fa5732e7af15df7f6e821040e650f.jpg" align="middle">
</details>




<h2 id="Diffusing-Differentiable-Representations"><a href="#Diffusing-Differentiable-Representations" class="headerlink" title="Diffusing Differentiable Representations"></a>Diffusing Differentiable Representations</h2><p><strong>Authors:Yash Savani, Marc Finzi, J. Zico Kolter</strong></p>
<p>We introduce a novel, training-free method for sampling differentiable representations (diffreps) using pretrained diffusion models. Rather than merely mode-seeking, our method achieves sampling by “pulling back” the dynamics of the reverse-time process–from the image space to the diffrep parameter space–and updating the parameters according to this pulled-back process. We identify an implicit constraint on the samples induced by the diffrep and demonstrate that addressing this constraint significantly improves the consistency and detail of the generated objects. Our method yields diffreps with substantially improved quality and diversity for images, panoramas, and 3D NeRFs compared to existing techniques. Our approach is a general-purpose method for sampling diffreps, expanding the scope of problems that diffusion models can tackle. </p>
<blockquote>
<p>我们介绍了一种利用预训练的扩散模型进行可微分表示（diffrep）采样的新型无训练方法。我们的方法不是单纯地寻求模式，而是通过“拉回”反向时间过程的动态来实现采样——从图像空间到diffrep参数空间，并根据拉回的过程更新参数。我们确定了由diffrep引起的样本上的隐式约束，并证明解决此约束可以显著提高生成对象的连贯性和细节。与现有技术相比，我们的方法为图像、全景图和3D NeRF生成了质量和多样性显著提高的diffrep。我们的方法是通用的diffrep采样方法，扩大了扩散模型可以解决的问题范围。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06981v1">PDF</a> Published at NeurIPS 2024</p>
<p><strong>摘要</strong><br>    本文介绍了一种新型、无需训练的方法，利用预训练的扩散模型进行可分化表示（diffrep）采样。不同于传统的模式寻求方法，本文方法通过反向时间的动态“拉回”过程，从图像空间到diffrep参数空间进行采样，并根据拉回的过程更新参数。文章识别了diffrep所隐含的样本约束，并证明解决这一约束能显著提高生成物体的一致性和细节。相较于现有技术，本文方法在图像、全景图和3D NeRF的diffrep生成中，质量和多样性均有显著提高。这是一种通用的diffrep采样方法，扩大了扩散模型可解决的问题范围。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>引入了一种新型、无需训练的方法，利用预训练的扩散模型进行采样。</li>
<li>通过反向时间的动态“拉回”过程实现从图像空间到diffrep参数空间的采样。</li>
<li>识别了diffrep采样中的隐含约束。</li>
<li>解决这一约束能显著提高生成物体的一致性和细节。</li>
<li>在图像、全景图和3D NeRF的生成中，diffrep的质量和多样性有明显提升。</li>
<li>该方法是一种通用的diffrep采样方法。</li>
<li>扩大了扩散模型可解决的问题范围。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a6baeb493ba47cd67e8aa3e97f6481b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dc293729eac042afd34df15350e3572d.jpg" align="middle">
</details>




<h2 id="Enhancing-operational-wind-downscaling-capabilities-over-Canada-Application-of-a-Conditional-Wasserstein-GAN-methodology"><a href="#Enhancing-operational-wind-downscaling-capabilities-over-Canada-Application-of-a-Conditional-Wasserstein-GAN-methodology" class="headerlink" title="Enhancing operational wind downscaling capabilities over Canada:   Application of a Conditional Wasserstein GAN methodology"></a>Enhancing operational wind downscaling capabilities over Canada:   Application of a Conditional Wasserstein GAN methodology</h2><p><strong>Authors:Jorge Guevara, Victor Nascimento, Johannes Schmude, Daniel Salles, Simon Corbeil-Létourneau, Madalina Surcel, Dominique Brunet</strong></p>
<p>Wind downscaling is essential for improving the spatial resolution of weather forecasts, particularly in operational Numerical Weather Prediction (NWP). This study advances wind downscaling by extending the DownGAN framework introduced by Annau et al.,to operational datasets from the Global Deterministic Prediction System (GDPS) and High-Resolution Deterministic Prediction System (HRDPS), covering the entire Canadian domain. We enhance the model by incorporating high-resolution static covariates, such as HRDPS-derived topography, into a Conditional Wasserstein Generative Adversarial Network with Gradient Penalty, implemented using a UNET-based generator. Following the DownGAN framework, our methodology integrates low-resolution GDPS forecasts (15 km, 10-day horizon) and high-resolution HRDPS forecasts (2.5 km, 48-hour horizon) with Frequency Separation techniques adapted from computer vision. Through robust training and inference over the Canadian region, we demonstrate the operational scalability of our approach, achieving significant improvements in wind downscaling accuracy. Statistical validation highlights reductions in root mean square error (RMSE) and log spectral distance (LSD) metrics compared to the original DownGAN. High-resolution conditioning covariates and Frequency Separation strategies prove instrumental in enhancing model performance. This work underscores the potential for extending high-resolution wind forecasts beyond the 48-hour horizon, bridging the gap to the 10-day low resolution global forecast window. </p>
<blockquote>
<p>风场降尺度化对于提高天气预报的空间分辨率至关重要，特别是在业务数值天气预报（NWP）中。本研究通过扩展Annau等人介绍的DownGAN框架，将风场降尺度化技术应用于全球确定性预测系统（GDPS）和高分辨率确定性预测系统（HRDPS）的业务数据集，覆盖整个加拿大区域。我们通过将高分辨率静态协变量（如HRDPS衍生的地形）纳入带有梯度惩罚的条件Wasserstein生成对抗网络，增强了模型的功能。该网络采用基于UNET的生成器实现。遵循DownGAN框架，我们的方法结合了低分辨率GDPS预测（15公里，10天视野）和高分辨率HRDPS预测（2.5公里，48小时视野），并采用计算机视觉中的频率分离技术。通过在加拿大地区的稳健训练和推理，我们证明了该方法在业务环境中的可扩展性，并在风场降尺度化精度方面取得了显著改进。统计验证显示，与原始DownGAN相比，均方根误差（RMSE）和对数谱距离（LSD）指标有所降低。高分辨率条件协变量和频率分离策略对于提高模型性能起到了关键作用。这项工作强调了将高分辨率风场预测扩大到超过48小时视野的潜力，以缩小与10天低分辨率全球预报窗口的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06958v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究利用DownGAN框架扩展了风降尺度技术，将其应用于全球确定性预测系统（GDPS）和高分辨率确定性预测系统（HRDPS）的操作数据集，覆盖整个加拿大区域。通过引入高分辨率静态协变量和条件Wasserstein生成对抗网络等技术，实现了显著的风降尺度精度提升。研究证明该方法的操作可扩展性，并强调其在提高长期高分辨率天气预报潜力方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究扩展了DownGAN框架，应用于操作数据集GDPS和HRDPS，覆盖整个加拿大区域。</li>
<li>通过结合高分辨率静态协变量，增强了模型的性能。</li>
<li>采用条件Wasserstein生成对抗网络和带有梯度惩罚的U-NET生成器实现了模型优化。</li>
<li>利用频率分离技术，成功整合了低分辨率和高分辨率天气预报数据。</li>
<li>通过稳健的训练和推理，展示了方法的操作可扩展性。</li>
<li>统计验证显示，与原始DownGAN相比，该方法的均方根误差（RMSE）和对数谱距离（LSD）指标有所降低。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-23348f6549c3cbaff32b630d8a7b8b6e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-73a9fce5db263ff4f008c491d71257fd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-533fac644ef9b0c641f7c4e040b6977f.jpg" align="middle">
</details>




<h2 id="MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting"><a href="#MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting" class="headerlink" title="MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting"></a>MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu</strong></p>
<p>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>. </p>
<blockquote>
<p>重建高保真3D头像对于虚拟现实等各种应用至关重要。开创性的方法使用神经辐射场（NeRF）重建逼真的头像，但受到训练和渲染速度的限制。基于3D高斯拼贴（3DGS）的最近的方法显著提高了训练和渲染的效率。然而，3DGS的表面不一致导致几何精度不高；后来的2DGS使用2D表面元素以提高几何精度，但以牺牲渲染保真度为代价。为了利用2DGS和3DGS两者的优点，我们提出了一种名为MixedGaussianAvatar的新方法，用于逼真且几何精确的头像重建。我们的主要想法是使用2D高斯重建3D头像的表面，以确保几何精度。我们将2D高斯附加到FLAME模型的三角网格上，并在2DGS的渲染质量不足的地方连接到额外的3D高斯，创建混合的2D-3D高斯表示。这些2D-3D高斯可以使用FLAME参数进行动画处理。我们还引入了一种渐进的训练策略，首先训练2D高斯，然后对混合的2D-3D高斯进行微调。我们通过全面的实验展示了MixedGaussianAvatar的优势。代码将在<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04955v2">PDF</a> Project: <a target="_blank" rel="noopener" href="https://chenvoid.github.io/MGA/">https://chenvoid.github.io/MGA/</a></p>
<p><strong>Summary</strong></p>
<p>基于神经辐射场（NeRF）的先进方法在实现高质量的三维头像重建方面取得了重大突破，但在训练和渲染速度上存在局限。为提高效率，研究者提出基于三维高斯拼贴（3DGS）的方法，但存在几何精度不足的问题。后续的二维高斯拼贴（2DGS）虽提高了几何精度，但牺牲了渲染质量。本研究结合两者的优势，提出名为MixedGaussianAvatar的新型方法，利用二维高斯重建三维头部的表面，确保几何精度，并在必要时使用三维高斯补充。该方法将二维高斯贴合于FLAME模型的三角网格上，并通过逐步训练策略优化混合的二维与三维高斯。MixedGaussianAvatar的实验结果证明了其优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Neural Radiance Fields (NeRF)已被用于创建高质量的三维头像重建。</li>
<li>现有方法如3DGS和2DGS分别在效率和几何精度或渲染质量方面存在局限。</li>
<li>MixedGaussianAvatar结合了2DGS和3DGS的优点，确保几何精度的同时提高渲染质量。</li>
<li>该方法利用FLAME模型的三角网格贴合二维高斯，并在需要时补充三维高斯。</li>
<li>采用逐步训练策略优化混合的二维与三维高斯。</li>
<li>MixedGaussianAvatar的实验结果证明了其优越性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fee7aa56253f4eb6856f0bdf9d9655e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0ef5787956810f1e111d21adf0bdcf5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ce4f964cf25207a6db5a28f7f85bd755.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-440cea39b9c601afb4654560b1a89e0a.png" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43282ee6f05919c0b760a138b7ff6c40.jpg" align="middle">
</details>




<h2 id="Learning-based-Multi-View-Stereo-A-Survey"><a href="#Learning-based-Multi-View-Stereo-A-Survey" class="headerlink" title="Learning-based Multi-View Stereo: A Survey"></a>Learning-based Multi-View Stereo: A Survey</h2><p><strong>Authors:Fangjinhua Wang, Qingtian Zhu, Di Chang, Quankai Gao, Junlin Han, Tong Zhang, Richard Hartley, Marc Pollefeys</strong></p>
<p>3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented&#x2F;Virtual Reality (AR&#x2F;VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area. </p>
<blockquote>
<p>三维重建旨在恢复场景的密集三维结构。它在增强&#x2F;虚拟现实（AR&#x2F;VR）、自动驾驶和机器人技术等各种应用中发挥着至关重要的作用。利用从不同视角捕捉的场景的多个视图，多视图立体（MVS）算法合成全面的三维表示，实现在复杂环境中的精确重建。由于其效率和有效性，MVS已成为基于图像的3D重建的关键方法。最近，随着深度学习取得的成功，已经提出了许多基于学习的MVS方法，与传统方法相比，这些方法取得了令人印象深刻的效果。我们将这些基于学习的方法分为以下几类：基于深度图的方法、基于体素的方法、基于NeRF的方法、基于三维高斯Splatting的方法和大型前馈方法。其中，我们重点关注基于深度图的方法，由于其简洁性、灵活性和可扩展性，它们成为MVS的主要家族。在本文综述中，我们全面回顾了截至当前日期的文献。我们调查了这些基于学习的方法，总结了它们在流行基准测试上的性能，并讨论了该领域未来研究的充满希望的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15235v2">PDF</a> </p>
<p><strong>Summary</strong><br>三维重建旨在恢复场景的密集三维结构，对于增强现实、虚拟现实、自动驾驶和机器人技术等领域具有关键作用。利用从不同视角捕捉的场景的多个视图，多视图立体（MVS）算法合成全面的三维表示，实现在复杂环境中的精确重建。最近，随着深度学习技术的成功应用，许多基于学习的MVS方法被提出，相对于传统方法取得了令人印象深刻的性能表现。本文主要关注基于深度图的方法，它们是MVS的主要家族，具有简洁性、灵活性和可扩展性。本文对此领域的文献进行了全面综述，调查了这些基于学习的方法，总结了它们在流行基准测试上的性能表现，并讨论了该领域的未来研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D重建旨在恢复场景的密集三维结构，广泛应用于AR&#x2F;VR、自动驾驶和机器人技术等领域。</li>
<li>多视图立体（MVS）算法能从不同视角捕捉场景并合成全面的三维表示。</li>
<li>基于深度学习的MVS方法相比传统方法取得了显著的性能提升。</li>
<li>基于深度图的方法是MVS的主要方法，具有简洁性、灵活性和可扩展性。</li>
<li>现有文献对于基于学习的方法进行了全面综述。</li>
<li>这些方法在流行基准测试上的性能表现被总结出来。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a5e5346a998309aa296c8385f856de80.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-22ada0555da5fef891a724a431157d98.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-03bb12f1f648696bd6045b65e15edfd1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a69c8889a02a0a54adc1f576279f164.jpg" align="middle">
</details>




<h2 id="Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions"><a href="#Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions" class="headerlink" title="Rethinking Score Distillation as a Bridge Between Image Distributions"></a>Rethinking Score Distillation as a Bridge Between Image Distributions</h2><p><strong>Authors:David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa</strong></p>
<p>Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains. Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications. In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution. Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target). We argue that current methods’ characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution. We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead. Our method can be easily applied across many domains, matching or beating the performance of specialized methods. We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real. We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors. </p>
<blockquote>
<p>得分蒸馏采样（SDS）已被证明是一个重要工具，能够在数据稀缺领域使用大规模扩散先验来完成任务。然而，SDS存在一些特征性伪影，限制了其在通用应用中的实用性。在本文中，我们通过将其视为从源分布到目标分布的最佳成本传输路径的解决方式，来推进对SDS及其变体的行为的理解。在这种新解释下，这些方法试图将损坏的图像（源）传输到自然图像分布（目标）。我们认为当前方法的特征性伪影是由（1）最佳路径的线性近似和（2）源分布估计不佳造成的。我们表明，校准源分布的文本条件可以在几乎没有额外开销的情况下产生高质量的生成和翻译结果。我们的方法可以轻松应用于许多领域，达到或超过专业方法的性能。我们在文本到二维、基于文本的NeRF优化、绘画到现实图像的翻译、光学幻觉生成和三维草图到现实等应用中展示了其实用性。我们将方法与现有的分数蒸馏采样方法进行了比较，并证明它能够产生具有真实色彩的高频细节。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09417v2">PDF</a> NeurIPS 2024. Project webpage: <a target="_blank" rel="noopener" href="https://sds-bridge.github.io/">https://sds-bridge.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了评分蒸馏采样（SDS）在处理数据稀缺领域任务时利用大规模扩散先验的优势，并针对SDS存在的特征性伪影问题进行了探究。文章将SDS及其变体看作是从源分布到目标分布的最优成本传输路径的求解过程。通过调整源分布的文本条件，文章提出了一种校准方法，能够产生高质量生成和翻译结果，且额外开销较小。该方法可广泛应用于多个领域，与专项方法相比具有匹配或超越的性能。它在文本到二维图像、基于文本的NeRF优化、绘画到现实图像的翻译、光学幻觉生成和三维草图到现实等应用中展示了实用性。通过与现有评分蒸馏采样方法的比较，该方法能够产生具有真实色彩的高频细节。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>评分蒸馏采样（SDS）在处理数据稀缺领域任务时，能够利用大规模扩散先验。</li>
<li>SDS方法存在特征性伪影问题，限制了其在通用应用中的实用性。</li>
<li>文章将SDS及其变体解读为从源分布到目标分布的最优成本传输路径的求解过程。</li>
<li>通过调整源分布的文本条件，提出了一种校准方法，能够产生高质量的生成和翻译结果。</li>
<li>该方法可广泛应用于多个领域，包括文本到二维图像转换、基于文本的NeRF优化等。</li>
<li>与其他评分蒸馏采样方法相比，该方法能够产生具有真实色彩的高频细节。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ab688bcbe79403b9dc0a82fa87e55b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-583f9aab3efdf9acf3d30ae12a8d5845.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd2ee8b180e521f6c21caaa035b7cc5f.jpg" align="middle">
</details>




<h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p>
<p>3D Gaussian Splatting (3DGS) has significantly advanced 3D scene reconstruction and novel view synthesis. However, like Neural Radiance Fields (NeRF), 3DGS struggles with accurately modeling physical reflections, particularly in mirrors, leading to incorrect reconstructions and inconsistent reflective properties. To address this challenge, we introduce Mirror-3DGS, a novel framework designed to accurately handle mirror geometries and reflections, thereby generating realistic mirror reflections. By incorporating mirror attributes into 3DGS and leveraging plane mirror imaging principles, Mirror-3DGS simulates a mirrored viewpoint from behind the mirror, enhancing the realism of scene renderings. Extensive evaluations on both synthetic and real-world scenes demonstrate that our method can render novel views with improved fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF, especially in mirror regions. </p>
<blockquote>
<p>3D高斯融合（3DGS）在三维场景重建和新型视图合成方面取得了重大进展。然而，与神经辐射场（NeRF）一样，3DGS在准确模拟物理反射方面存在困难，特别是在镜子中，导致重建不准确和反射属性不一致。为了应对这一挑战，我们引入了Mirror-3DGS，这是一个设计用于准确处理镜子几何形状和反射的新型框架，从而生成逼真的镜子反射。通过将镜子属性融入3DGS并利用平面镜像成像原理，Mirror-3DGS模拟了镜子后面的镜像视点，增强了场景渲染的逼真度。对合成场景和真实场景的大量评估表明，我们的方法能够在实时中以更高的保真度呈现新型视图，超越了最先进的Mirror-NeRF，特别是在镜子区域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01168v2">PDF</a> IEEE International Conference on Visual Communications and Image   Processing (VCIP 2024, Oral)</p>
<p><strong>Summary</strong></p>
<p>基于三维高斯拼贴技术（3DGS），新的框架Mirror-3DGS被提出用于准确处理镜子几何形状和反射，解决了镜像渲染不真实的问题。通过结合镜子属性和平面镜像成像原理，它能够模拟从镜子后面的视角来呈现镜像，提高场景渲染的真实性。该方法在合成场景和真实场景中的评估表现优异，能够在实时渲染中提高保真度，特别是在镜子区域的表现超过了当前最先进的Mirror-NeRF技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mirror-3DGS是为了解决在三维场景重建和新型视图合成中，镜像反射建模不准确的问题而提出的。</li>
<li>它结合了镜子属性和平面镜像成像原理，以准确处理镜子几何形状和反射。</li>
<li>通过模拟从镜子后面的视角呈现镜像，Mirror-3DGS提高了场景渲染的真实性。</li>
<li>该方法在合成场景和真实场景中表现优异，特别是在镜子区域的渲染效果。</li>
<li>Mirror-3DGS超越了当前最先进的Mirror-NeRF技术，能够在实时渲染中提高保真度。</li>
<li>它的引入有助于推动三维场景重建和视图合成的进一步发展。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dd7869e208d9fb8d79482f7e49bf8dfd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-547b2159e15ad9259a5f5758f4258655.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc1bfa303ba582248284272ab2f58d3c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c71cd65107819c859fa830f5e804483d.jpg" align="middle">
</details>




<h2 id="UniSDF-Unifying-Neural-Representations-for-High-Fidelity-3D-Reconstruction-of-Complex-Scenes-with-Reflections"><a href="#UniSDF-Unifying-Neural-Representations-for-High-Fidelity-3D-Reconstruction-of-Complex-Scenes-with-Reflections" class="headerlink" title="UniSDF: Unifying Neural Representations for High-Fidelity 3D   Reconstruction of Complex Scenes with Reflections"></a>UniSDF: Unifying Neural Representations for High-Fidelity 3D   Reconstruction of Complex Scenes with Reflections</h2><p><strong>Authors:Fangjinhua Wang, Marie-Julie Rakotosaona, Michael Niemeyer, Richard Szeliski, Marc Pollefeys, Federico Tombari</strong></p>
<p>Neural 3D scene representations have shown great potential for 3D reconstruction from 2D images. However, reconstructing real-world captures of complex scenes still remains a challenge. Existing generic 3D reconstruction methods often struggle to represent fine geometric details and do not adequately model reflective surfaces of large-scale scenes. Techniques that explicitly focus on reflective surfaces can model complex and detailed reflections by exploiting better reflection parameterizations. However, we observe that these methods are often not robust in real scenarios where non-reflective as well as reflective components are present. In this work, we propose UniSDF, a general purpose 3D reconstruction method that can reconstruct large complex scenes with reflections. We investigate both camera view as well as reflected view-based color parameterization techniques and find that explicitly blending these representations in 3D space enables reconstruction of surfaces that are more geometrically accurate, especially for reflective surfaces. We further combine this representation with a multi-resolution grid backbone that is trained in a coarse-to-fine manner, enabling faster reconstructions than prior methods. Extensive experiments on object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF 360 and Ref-NeRF real demonstrate that our method is able to robustly reconstruct complex large-scale scenes with fine details and reflective surfaces, leading to the best overall performance. Project page: \url{<a target="_blank" rel="noopener" href="https://fangjinhuawang.github.io/UniSDF%7D">https://fangjinhuawang.github.io/UniSDF}</a>. </p>
<blockquote>
<p>神经三维场景表示在从二维图像进行三维重建方面显示出巨大潜力。然而，重建复杂场景的真实世界捕获仍然是一个挑战。现有的通用三维重建方法往往难以表示精细的几何细节，并且不足以对大规模场景的反射面进行建模。专注于反射表面的技术可以通过利用更好的反射参数化来模拟复杂和详细的反射。然而，我们观察到，当存在非反射和反射组件时，这些方法在现实场景中往往不够稳健。在这项工作中，我们提出了UniSDF，这是一种通用三维重建方法，可以重建具有反射的大规模复杂场景。我们研究了基于相机视角和反射视角的颜色参数化技术，并发现明确地在三维空间中混合这些表示可以重建更几何精确的表面，特别是对于反射表面。我们将这种表示与以粗到细方式训练的多分辨率网格主干相结合，实现了比以往方法更快的重建速度。在对象级数据集DTU、Shiny Blender以及无界数据集Mip-NeRF 360和Ref-NeRF real上的大量实验表明，我们的方法能够稳健地重建具有精细细节和反射表面的复杂大规模场景，取得了最佳的整体性能。项目页面：<a target="_blank" rel="noopener" href="https://fangjinhuawang.github.io/UniSDF%E3%80%82">https://fangjinhuawang.github.io/UniSDF。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.13285v2">PDF</a> NeurIPS 2024 camera ready</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Neural 3D场景表示在3D重建领域的潜力，尤其是针对复杂场景的重建。现有通用3D重建方法在表示精细几何细节和模拟大型场景的反射表面方面存在挑战。本文提出了一种通用目的的3D重建方法UniSDF，能够重建具有反射的大型复杂场景。通过调查相机视角和反射视角的颜色参数化技术，并明确地将这些表示混合在3D空间中，实现了对反射表面的更精确几何重建。此外，结合多分辨率网格主干进行粗到细的训练，实现了比现有方法更快的重建速度。在对象级数据集DTU、Shiny Blender以及无界数据集Mip-NeRF 360和Ref-NeRF real上的广泛实验表明，该方法能够稳健地重建具有精细细节和反射表面的复杂大型场景，取得了最佳的整体性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Neural 3D场景表示在3D重建中具有巨大潜力，尤其是在复杂场景方面。</li>
<li>现有通用3D重建方法在模拟反射表面和精细几何细节方面存在挑战。</li>
<li>UniSDF是一种新型的3D重建方法，能够重建具有反射的大型复杂场景。</li>
<li>UniSDF通过混合相机视角和反射视角的颜色参数化表示，提高了对反射表面的几何准确性。</li>
<li>UniSDF采用多分辨率网格主干进行粗到细的训练，实现了更快的重建速度。</li>
<li>在多个数据集上的实验表明，UniSDF在重建具有精细细节和反射表面的复杂大型场景方面表现最佳。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-44ad05589092be4a2ce220e1e63f9eea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-42cb3052d279bf31b0971c292f0b5ce8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56725227a346517f56bf4697cd76f5dc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-df5b2dd7ca8cc571de05d576959e4b3c.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>对于光学相干断层扫描血管造影（OCTA）图像，有限的扫描率导致视野（FOV）与成像分辨率之间的权衡。虽然较大的FOV图像可能会揭示更多的旁中心凹血管病变，但由于分辨率较低，其应用受到很大阻碍。为了提高分辨率，以前的工作只有通过使用配对数据进行训练才能达到令人满意的效果，但现实世界的应用受到收集大规模配对图像的挑战的限制。因此，对未配对方法的需求很大。生成对抗网络（GAN）已在未配对设置中广泛使用，但它可能难以准确保留细微的毛细血管细节，这些对于OCTA是至关重要的生物标志物。本文的方法旨在通过利用频率信息来保留这些细节，将细节视为高频（hf），将粗粒背景视为低频（lf）。总的来说，我们提出了一种基于GAN的未配对超分辨率方法用于OCTA图像，并通过双路径生成器特别强调了hf细微毛细血管。为了促进重建图像的精确光谱，我们还为鉴别器提出了频率感知对抗性损失，并为端到端优化引入了频率感知焦点一致性损失。实验表明，我们的方法在定量和视觉上均优于其他最先进的未配对方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于GAN的无配对超分辨率方法，用于处理OCTA图像。通过利用频率信息，该方法旨在保留关键的毛细血管细节，并通过双路径生成器突出高频精细毛细血管。同时，为了精确重构图像谱，还引入了频率感知对抗损失和端到端优化的频率感知焦点一致性损失。实验表明，该方法在定量和视觉上都优于其他最先进的无配对方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCTA图像中，扫描率受限导致视野与成像分辨率之间的权衡。</li>
<li>较大的视野可能揭示更多的旁中心视网膜血管病变，但其分辨率较低。</li>
<li>以往的研究在训练时依赖配对数据，但真实世界应用中配对图像的收集挑战重重。</li>
<li>本文提出了一种基于GAN的无配对超分辨率方法处理OCTA图像。</li>
<li>方法利用频率信息来保留毛细血管细节，通过双路径生成器突出高频精细毛细血管。</li>
<li>引入了频率感知对抗损失和频率感知焦点一致性损失，以进行精确图像重构和端到端优化。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>




<h2 id="MC-NeRF-Multi-Camera-Neural-Radiance-Fields-for-Multi-Camera-Image-Acquisition-Systems"><a href="#MC-NeRF-Multi-Camera-Neural-Radiance-Fields-for-Multi-Camera-Image-Acquisition-Systems" class="headerlink" title="MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image   Acquisition Systems"></a>MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image   Acquisition Systems</h2><p><strong>Authors:Yu Gao, Lutong Su, Hao Liang, Yufeng Yue, Yi Yang, Mengyin Fu</strong></p>
<p>Neural Radiance Fields (NeRF) use multi-view images for 3D scene representation, demonstrating remarkable performance. As one of the primary sources of multi-view images, multi-camera systems encounter challenges such as varying intrinsic parameters and frequent pose changes. Most previous NeRF-based methods assume a unique camera and rarely consider multi-camera scenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic parameters still remain susceptible to suboptimal solutions when these parameters are poor initialized. In this paper, we propose MC-NeRF, a method that enables joint optimization of both intrinsic and extrinsic parameters alongside NeRF. The method also supports each image corresponding to independent camera parameters. First, we tackle coupling issue and the degenerate case that arise from the joint optimization between intrinsic and extrinsic parameters. Second, based on the proposed solutions, we introduce an efficient calibration image acquisition scheme for multi-camera systems, including the design of calibration object. Finally, we present an end-to-end network with training sequence that enables the estimation of intrinsic and extrinsic parameters, along with the rendering network. Furthermore, recognizing that most existing datasets are designed for a unique camera, we construct a real multi-camera image acquisition system and create a corresponding new dataset, which includes both simulated data and real-world captured images. Experiments confirm the effectiveness of our method when each image corresponds to different camera parameters. Specifically, we use multi-cameras, each with different intrinsic and extrinsic parameters in real-world system, to achieve 3D scene representation without providing initial poses. </p>
<blockquote>
<p>神经辐射场（NeRF）利用多视角图像进行3D场景表示，并展现出卓越的性能。作为多视角图像的主要来源之一，多相机系统面临着内在参数变化以及姿态频繁变化等挑战。之前的大多数基于NeRF的方法都假设有一个独特的相机，并且很少考虑多相机场景。此外，即使有些NeRF方法能够优化内在和外在参数，但在这些参数初始化不佳时，仍然容易陷入次优解。在本文中，我们提出了MC-NeRF方法，该方法能够同时优化内在和外在参数以及与NeRF相关的参数。该方法还支持每张图像对应独立的相机参数。首先，我们解决了在内在和外在参数联合优化过程中出现的耦合问题和退化情况。其次，基于这些解决方案，我们为多相机系统引入了有效的校准图像采集方案，包括校准对象的设计。最后，我们提出了一个端到端的网络训练序列，该网络能够估计内在和外在参数，以及渲染网络。此外，我们认识到大多数现有数据集都是为单一相机设计的，因此我们构建了一个真正的多相机图像采集系统，并创建了一个相应的新数据集，该数据集包括模拟数据和真实世界捕获的图像。实验证实，当每张图像对应不同的相机参数时，我们的方法非常有效。具体来说，我们使用具有不同内在和外在参数的多相机真实系统，在不提供初始姿态的情况下实现3D场景表示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07846v4">PDF</a> This manuscript is currently under review</p>
<p><strong>Summary</strong></p>
<p>本文提出了MC-NeRF方法，实现了对NeRF中的内在参数和外在参数的联合优化，支持每张图像对应独立的相机参数。解决了联合优化中的耦合问题和退化情况，引入高效的多相机系统校准图像采集方案，包括校准对象的设计。同时构建了真实的多相机图像采集系统并创建了相应的新数据集，实验证明该方法在真实世界系统中每张图像对应不同相机参数时的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MC-NeRF实现了对NeRF中的内在参数和外在参数的联合优化。</li>
<li>支持每张图像对应独立的相机参数。</li>
<li>解决了联合优化中出现的耦合问题和退化情况。</li>
<li>引入高效的多相机系统校准图像采集方案。</li>
<li>构建真实的多相机图像采集系统并创建新的数据集。</li>
<li>实验证明该方法在真实世界系统中的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-101df8512aee8de8d27c2059897a721f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-37c79cf2eacdceafe52fb024a2637ea6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-48274d60983d14e283021d015f2583f7.png" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-688c42b03634426b1c3218e979797176.jpg" align="middle">
</details>




<h2 id="ChromaDistill-Colorizing-Monochrome-Radiance-Fields-with-Knowledge-Distillation"><a href="#ChromaDistill-Colorizing-Monochrome-Radiance-Fields-with-Knowledge-Distillation" class="headerlink" title="ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge   Distillation"></a>ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge   Distillation</h2><p><strong>Authors:Ankit Dhiman, R Srinath, Srinjay Sarkar, Lokesh R Boregowda, R Venkatesh Babu</strong></p>
<p>Colorization is a well-explored problem in the domains of image and video processing. However, extending colorization to 3D scenes presents significant challenges. Recent Neural Radiance Field (NeRF) and Gaussian-Splatting(3DGS) methods enable high-quality novel-view synthesis for multi-view images. However, the question arises: How can we colorize these 3D representations? This work presents a method for synthesizing colorized novel views from input grayscale multi-view images. Using image or video colorization methods to colorize novel views from these 3D representations naively will yield output with severe inconsistencies. We introduce a novel method to use powerful image colorization models for colorizing 3D representations. We propose a distillation-based method that transfers color from these networks trained on natural images to the target 3D representation. Notably, this strategy does not add any additional weights or computational overhead to the original representation during inference. Extensive experiments demonstrate that our method produces high-quality colorized views for indoor and outdoor scenes, showcasing significant cross-view consistency advantages over baseline approaches. Our method is agnostic to the underlying 3D representation and easily generalizable to NeRF and 3DGS methods. Further, we validate the efficacy of our approach in several diverse applications: 1.) Infra-Red (IR) multi-view images and 2.) Legacy grayscale multi-view image sequences. Project Webpage: <a target="_blank" rel="noopener" href="https://val.cds.iisc.ac.in/chroma-distill.github.io/">https://val.cds.iisc.ac.in/chroma-distill.github.io/</a> </p>
<blockquote>
<p>着色是图像和视频处理领域已经得到很好研究的问题。然而，将着色扩展到3D场景面临重大挑战。最近的神经辐射场（NeRF）和高斯溅射（3DGS）方法能够实现多视角图像的高质量新型视图合成。但问题是：我们如何对这些3D表示进行着色？这项工作提出了一种从输入灰度多视角图像合成彩色新型视图的方法。直接使用图像或视频着色方法来对这些3D表示进行着色会导致输出存在严重的不一致性。我们引入了一种新的方法，使用强大的图像着色模型对3D表示进行着色。我们提出了一种基于蒸馏的方法，将从自然图像上训练的网络的颜色转移到目标3D表示上。值得注意的是，此策略在推理期间不会给原始表示增加任何额外的权重或计算开销。大量实验表明，我们的方法为室内和室外场景生成了高质量彩色视图，与基准方法相比，跨视图一致性具有显著优势。我们的方法对底层3D表示具有中立性，可轻松推广到NeRF和3DGS方法。此外，我们通过几个不同的应用验证了我们的方法的有效性：1）红外（IR）多视角图像和2）遗留灰度多视角图像序列。项目网页：<a target="_blank" rel="noopener" href="https://val.cds.iisc.ac.in/chroma-distill.github.io/">https://val.cds.iisc.ac.in/chroma-distill.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07668v2">PDF</a> WACV 2025, AI3DCC @ ICCV 2023</p>
<p><strong>Summary</strong></p>
<p>本文探索了将色彩化技术应用于三维场景表现的问题。文章介绍了如何将图像或视频的色彩化方法应用于从三维表示生成的新型视角图像上，并提出了一种基于蒸馏的方法，将训练于自然图像的网络中的颜色转移到目标三维表示上，实现了高质量的颜色化视图合成。此方法对于室内和室外场景均有效，相比基准方法具有显著的跨视图一致性优势，并且可轻松推广到NeRF和3DGS方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文探讨了将色彩化扩展到三维场景的挑战。</li>
<li>提出了基于蒸馏的方法，将图像颜色从自然图像网络转移到三维表示。</li>
<li>方法实现了高质量的颜色化视图合成，适用于室内和室外场景。</li>
<li>与基准方法相比，该方法具有显著的跨视图一致性优势。</li>
<li>方法对于不同的三维表示方法具有通用性，可轻松推广至NeRF和3DGS。</li>
<li>方法在多种应用中得到验证，包括红外多视角图像和遗留灰度多视角图像序列。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f55c89070e6b20b7fa1fb91b960a64d1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-68a542cf75dab4c514d896440e5f6784.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0fee3ef8eb02763ac0e262c36d2324bb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d53fe9bb1b08659516504f156fc15c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-994a03b85c48a19dcb7fa3c8c745338a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-84c6da1255580971a1b76ca9b966593b.jpg" align="middle">
</details>




<h2 id="Diverse-Similarity-Encoder-for-Deep-GAN-Inversion"><a href="#Diverse-Similarity-Encoder-for-Deep-GAN-Inversion" class="headerlink" title="Diverse Similarity Encoder for Deep GAN Inversion"></a>Diverse Similarity Encoder for Deep GAN Inversion</h2><p><strong>Authors:Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</strong></p>
<p>Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN. </p>
<blockquote>
<p>当前深度生成对抗网络（GANs）已经可以合成高质量（HQ）图像，因此使用GANs学习表示是有利的。GAN反演是新兴方法之一，研究如何将图像反演到潜在空间。现有的GAN编码器可以在StyleGAN上进行图像反演，但不能适应其他深度GAN。我们提出了一种新的方法来解决这个问题。通过评估潜在向量和图像之间的不同相似性，我们设计了一种自适应编码器，名为差异相似性编码器（DSE），可以扩展到各种最先进的GAN。DSE使GAN能够从HQ图像重建更高保真度的图像，无论是合成的还是真实的图像。DSE具有统一的卷积块，可以很好地适应主流深度GAN，例如PGGAN、StyleGAN和BigGAN。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.10201v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为多样相似性编码器（DSE）的新型自适应GAN编码器方法，可应用于多种先进的生成对抗网络（GANs）。通过评估潜在向量和图像之间的不同相似性，DSE能够使GANs重构出更高保真度的图像，无论是合成图像还是真实图像。该方法能够适应主流深度GANs，如PGGAN、StyleGAN和BigGAN等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前深度生成对抗网络（GANs）可以合成高质量图像，因此使用GANs进行表示学习备受青睐。</li>
<li>GAN反演是新兴方法之一，研究如何将图像反演到潜在空间。</li>
<li>现有GAN编码器可以在StyleGAN上进行图像反演，但不能适应其他深度GANs。</li>
<li>提出了一种名为多样相似性编码器（DSE）的新型自适应GAN编码器方法来解决这一问题。</li>
<li>DSE通过评估潜在向量和图像之间的不同相似性进行设计。</li>
<li>DSE能够应用于多种先进的GANs，如PGGAN、StyleGAN和BigGAN等。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2c0940fc7f9e47f7ef709ff59469c9b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26229e3100c8c64811dfe13fae729ff8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-991864f280fa90ecd2fc3e0b5b8d9de9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-849a0bfe560207f45ea80d5eb85b9501.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/3DGS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_3DGS/2412.03844v2/page_0_0.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-12  SLGaussian Fast Language Gaussian Splatting in Sparse Views
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
