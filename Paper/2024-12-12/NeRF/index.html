<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  GN-FRGeneralizable Neural Radiance Fields for Flare Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="GN-FR-Generalizable-Neural-Radiance-Fields-for-Flare-Removal"><a href="#GN-FR-Generalizable-Neural-Radiance-Fields-for-Flare-Removal" class="headerlink" title="GN-FR:Generalizable Neural Radiance Fields for Flare Removal"></a>GN-FR:Generalizable Neural Radiance Fields for Flare Removal</h2><p><strong>Authors:Gopi Raju Matta, Rahul Siddartha, Rongali Simhachala Venkata Girish, Sumit Sharma, Kaushik Mitra</strong></p>
<p>Flare, an optical phenomenon resulting from unwanted scattering and reflections within a lens system, presents a significant challenge in imaging. The diverse patterns of flares, such as halos, streaks, color bleeding, and haze, complicate the flare removal process. Existing traditional and learning-based methods have exhibited limited efficacy due to their reliance on single-image approaches, where flare removal is highly ill-posed. We address this by framing flare removal as a multi-view image problem, taking advantage of the view-dependent nature of flare artifacts. This approach leverages information from neighboring views to recover details obscured by flare in individual images. Our proposed framework, GN-FR (Generalizable Neural Radiance Fields for Flare Removal), can render flare-free views from a sparse set of input images affected by lens flare and generalizes across different scenes in an unsupervised manner. GN-FR incorporates several modules within the Generalizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation (FMG), View Sampler (VS), and Point Sampler (PS). To overcome the impracticality of capturing both flare-corrupted and flare-free data, we introduce a masking loss function that utilizes mask information in an unsupervised setting. Additionally, we present a 3D multi-view flare dataset, comprising 17 real flare scenes with 782 images, 80 real flare patterns, and their corresponding annotated flare-occupancy masks. To our knowledge, this is the first work to address flare removal within a Neural Radiance Fields (NeRF) framework. </p>
<blockquote>
<p>å…‰æ™•æ˜¯ä¸€ç§å…‰å­¦ç°è±¡ï¼Œç”±é•œå¤´ç³»ç»Ÿä¸­çš„éå¿…è¦æ•£å°„å’Œåå°„é€ æˆï¼Œå¯¹æˆåƒé€ æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å…‰æ™•çš„å¤šç§å½¢æ€ï¼Œå¦‚å…‰ç¯ã€æ¡çº¹ã€è‰²å½©æº¢å‡ºå’Œæœ¦èƒ§ï¼Œä½¿å¾—å…‰æ™•å»é™¤è¿‡ç¨‹å¤æ‚åŒ–ã€‚ç°æœ‰çš„ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ç”±äºä¾èµ–äºå•å›¾åƒæ–¹æ³•è€Œè¡¨ç°å‡ºæœ‰é™çš„æ•ˆåŠ›ï¼Œå…¶ä¸­å…‰æ™•å»é™¤æ˜¯é«˜åº¦ä¸é€‚å®šçš„ã€‚æˆ‘ä»¬é€šè¿‡å°†å…‰æ™•å»é™¤ä½œä¸ºå¤šè§†å›¾å›¾åƒé—®é¢˜æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨å…‰æ™•ä¼ªå½±çš„è§†å›¾ç›¸å…³æ€§ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨é‚»è¿‘è§†å›¾çš„ä¿¡æ¯æ¥æ¢å¤å•ä¸ªå›¾åƒä¸­ç”±å…‰æ™•æ©ç›–çš„ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶GN-FRï¼ˆç”¨äºå…‰æ™•å»é™¤çš„å¯æ³›åŒ–ç¥ç»è¾å°„åœºï¼‰å¯ä»¥ä»å—å…‰æ™•å½±å“çš„ä¸€ç»„ç¨€ç–è¾“å…¥å›¾åƒä¸­å‘ˆç°æ— å…‰æ™•çš„è§†å›¾ï¼Œå¹¶ä»¥æ— ç›‘ç£çš„æ–¹å¼æ³›åŒ–åˆ°ä¸åŒçš„åœºæ™¯ã€‚GN-FRåœ¨å¯æ³›åŒ–NeRFè½¬æ¢å™¨ï¼ˆGNTï¼‰æ¡†æ¶ä¸­èå…¥äº†å¤šä¸ªæ¨¡å—ï¼šå…‰æ™•å ç”¨æ©æ¨¡ç”Ÿæˆå™¨ï¼ˆFMGï¼‰ã€è§†å›¾é‡‡æ ·å™¨ï¼ˆVSï¼‰å’Œç‚¹é‡‡æ ·å™¨ï¼ˆPSï¼‰ã€‚ä¸ºäº†å…‹æœåŒæ—¶æ•è·å—å…‰æ™•ç ´åå’Œæ— å…‰æ™•æ•°æ®çš„ä¸åˆ‡å®é™…æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ©æ¨¡æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨æ— ç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨æ©æ¨¡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªåŒ…å«17ä¸ªçœŸå®å…‰æ™•åœºæ™¯ã€782å¼ å›¾åƒã€80ç§çœŸå®å…‰æ™•æ¨¡å¼åŠå…¶ç›¸åº”çš„æ³¨é‡Šå…‰æ™•å ç”¨æ©æ¨¡çš„3Då¤šè§†å›¾å…‰æ™•æ•°æ®é›†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¡†æ¶å†…è§£å†³å…‰æ™•å»é™¤çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08200v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…‰å­¦ç°è±¡ä¸­çš„è€€æ–‘ï¼Œç”±äºé•œå¤´ç³»ç»Ÿä¸­çš„æ„å¤–æ•£å°„å’Œåå°„è€Œäº§ç”Ÿï¼Œå¯¹æˆåƒé€ æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è€€æ–‘çš„å¤šæ ·è¡¨ç°å½¢æ€ï¼Œå¦‚å…‰æ™•ã€æ¡çº¹ã€è‰²å½©æº¢å‡ºå’Œé›¾éœ­ï¼Œä½¿å¾—è€€æ–‘å»é™¤è¿‡ç¨‹å¤æ‚åŒ–ã€‚ç°æœ‰çš„ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•å› ä¾èµ–å•å›¾åƒæ–¹æ³•è€Œè¡¨ç°å‡ºæœ‰é™çš„æ•ˆèƒ½ï¼Œå…¶ä¸­è€€æ–‘å»é™¤çš„é«˜åº¦ä¸ç¨³å®šæ€§ã€‚æœ¬æ–‡é€šè¿‡å°†è€€æ–‘å»é™¤è§†ä¸ºå¤šè§†å›¾å›¾åƒé—®é¢˜æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨è€€æ–‘ä¼ªå½±çš„è§†å›¾ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•å¯ä»é‚»è¿‘è§†å›¾çš„ä¿¡æ¯ä¸­æ¢å¤å•ä¸ªå›¾åƒä¸­è¢«è€€æ–‘æ©ç›–çš„ç»†èŠ‚ã€‚æœ¬æ–‡æå‡ºçš„æ¡†æ¶GN-FRï¼ˆç”¨äºè€€æ–‘å»é™¤çš„å¯æ³›åŒ–ç¥ç»è¾å°„åœºï¼‰ï¼Œå¯ä»¥ä»ä¸€ç»„ç¨€ç–çš„è¾“å…¥å›¾åƒä¸­æ¸²æŸ“æ— è€€æ–‘çš„è§†å›¾ï¼Œå¹¶å¯¹ä¸åŒçš„åœºæ™¯è¿›è¡Œæ— ç›‘ç£çš„æ³›åŒ–ã€‚GN-FRåœ¨Generalizable NeRF Transformerï¼ˆGNTï¼‰æ¡†æ¶å†…ç»“åˆäº†å¤šä¸ªæ¨¡å—ï¼šè€€æ–‘å ç”¨æ©æ¨¡ç”Ÿæˆï¼ˆFMGï¼‰ã€è§†å›¾é‡‡æ ·å™¨ï¼ˆVSï¼‰å’Œç‚¹é‡‡æ ·å™¨ï¼ˆPSï¼‰ã€‚ä¸ºäº†å…‹æœåŒæ—¶è·å–è€€æ–‘æŸåå’Œæ— è€€æ–‘æ•°æ®çš„ä¸åˆ‡å®é™…æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ©æ¨¡æŸå¤±å‡½æ•°ï¼Œåœ¨æ— ç›‘ç£ç¯å¢ƒä¸­åˆ©ç”¨æ©æ¨¡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªåŒ…å«17ä¸ªçœŸå®è€€æ–‘åœºæ™¯ã€782å¼ å›¾åƒã€80ç§çœŸå®è€€æ–‘æ¨¡å¼åŠå…¶ç›¸åº”æ ‡æ³¨çš„è€€æ–‘å ç”¨æ©æ¨¡çš„3Då¤šè§†å›¾è€€æ–‘æ•°æ®é›†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¡†æ¶å†…è§£å†³è€€æ–‘å»é™¤é—®é¢˜çš„ç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³å…‰å­¦ç°è±¡ä¸­çš„è€€æ–‘é—®é¢˜ï¼Œè¯¥é—®é¢˜ç”±äºé•œå¤´ç³»ç»Ÿä¸­çš„æ„å¤–æ•£å°„å’Œåå°„è€Œäº§ç”Ÿï¼Œå¯¹æˆåƒè´¨é‡é€ æˆä¸¥é‡å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†å›¾å›¾åƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨è§†å›¾ç›¸å…³æ€§æ¥è§£å†³è€€æ–‘å»é™¤é—®é¢˜ï¼Œæé«˜äº†å»é™¤æ•ˆæœã€‚</li>
<li>ä»‹ç»äº†GN-FRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆå¤šä¸ªæ¨¡å—åœ¨Generalizable NeRF Transformerï¼ˆGNTï¼‰ä¸­å®ç°äº†æœ‰æ•ˆçš„è€€æ–‘å»é™¤ã€‚</li>
<li>åˆ›æ–°æ€§åœ°ä½¿ç”¨äº†æ©æ¨¡æŸå¤±å‡½æ•°ï¼Œå¯ä»¥åœ¨æ— ç›‘ç£ç¯å¢ƒä¸­åˆ©ç”¨æ©æ¨¡ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå…¨æ–°çš„3Då¤šè§†å›¾è€€æ–‘æ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®èµ„æºã€‚</li>
<li>æ®äº†è§£ï¼Œè¯¥è®ºæ–‡æ˜¯é¦–æ¬¡åœ¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¡†æ¶å†…è§£å†³è€€æ–‘å»é™¤é—®é¢˜çš„ç ”ç©¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-865539fc727c81c8f18cb19e5f5784aa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b9409dca8aeca45aa30b12b65e1f5e88.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fff4e385e9c0cbb536f55f17aa8764ed.jpg" align="middle">
</details>




<h2 id="NeRF-NQA-No-Reference-Quality-Assessment-for-Scenes-Generated-by-NeRF-and-Neural-View-Synthesis-Methods"><a href="#NeRF-NQA-No-Reference-Quality-Assessment-for-Scenes-Generated-by-NeRF-and-Neural-View-Synthesis-Methods" class="headerlink" title="NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF   and Neural View Synthesis Methods"></a>NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF   and Neural View Synthesis Methods</h2><p><strong>Authors:Qiang Qu, Hanxue Liang, Xiaoming Chen, Yuk Ying Chung, Yiran Shen</strong></p>
<p>Neural View Synthesis (NVS) has demonstrated efficacy in generating high-fidelity dense viewpoint videos using a image set with sparse views. However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not tailored for the scenes with dense viewpoints synthesized by NVS and NeRF variants, thus, they often fall short in capturing the perceptual quality, including spatial and angular aspects of NVS-synthesized scenes. Furthermore, the lack of dense ground truth views makes the full reference quality assessment on NVS-synthesized scenes challenging. For instance, datasets such as LLFF provide only sparse images, insufficient for complete full-reference assessments. To address the issues above, we propose NeRF-NQA, the first no-reference quality assessment method for densely-observed scenes synthesized from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment strategy, integrating both viewwise and pointwise approaches, to evaluate the quality of NVS-generated scenes. The viewwise approach assesses the spatial quality of each individual synthesized view and the overall inter-views consistency, while the pointwise approach focuses on the angular qualities of scene surface points and their compound inter-point quality. Extensive evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality assessment methods (from fields of image, video, and light-field assessment). The results demonstrate NeRF-NQA outperforms the existing assessment methods significantly and it shows substantial superiority on assessing NVS-synthesized scenes without references. An implementation of this paper are available at <a target="_blank" rel="noopener" href="https://github.com/VincentQQu/NeRF-NQA">https://github.com/VincentQQu/NeRF-NQA</a>. </p>
<blockquote>
<p>ç¥ç»è§†å›¾åˆæˆï¼ˆNVSï¼‰å·²ç»æ˜¾ç¤ºå‡ºä½¿ç”¨ç¨€ç–è§†å›¾å›¾åƒé›†ç”Ÿæˆé«˜ä¿çœŸå¯†é›†è§†ç‚¹è§†é¢‘çš„æ•ˆèƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå¦‚PSNRã€SSIMå’ŒLPIPSï¼Œå¹¶ä¸é€‚ç”¨äºç”±NVSå’ŒNeRFå˜ä½“åˆæˆçš„å¯†é›†è§†ç‚¹åœºæ™¯ï¼Œå› æ­¤ï¼Œå®ƒä»¬åœ¨æ•æ‰æ„ŸçŸ¥è´¨é‡æ–¹é¢å¸¸å¸¸ä¸è¶³ï¼ŒåŒ…æ‹¬NVSåˆæˆåœºæ™¯çš„çš„ç©ºé—´å’Œè§’åº¦æ–¹é¢ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¯†é›†çš„åœ°é¢çœŸå®è§†å›¾ä½¿å¾—å¯¹NVSåˆæˆåœºæ™¯è¿›è¡Œå…¨å‚è€ƒè´¨é‡è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¾‹å¦‚ï¼ŒLLFFç­‰æ•°æ®é›†åªæä¾›ç¨€ç–å›¾åƒï¼Œä¸è¶³ä»¥è¿›è¡Œå®Œæ•´çš„å…¨å‚è€ƒè¯„ä¼°ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeRF-NQAï¼Œè¿™æ˜¯ç¬¬ä¸€ç§æ— éœ€å‚è€ƒã€é’ˆå¯¹ç”±NVSå’ŒNeRFå˜ä½“åˆæˆçš„å¯†é›†è§‚å¯Ÿåœºæ™¯çš„è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚NeRF-NQAé‡‡ç”¨è”åˆè´¨é‡è¯„ä¼°ç­–ç•¥ï¼Œç»“åˆäº†è§†å›¾çº§å’Œç‚¹çº§æ–¹æ³•ï¼Œæ¥è¯„ä¼°NVSç”Ÿæˆåœºæ™¯çš„è´¨é‡ã€‚è§†å›¾çº§æ–¹æ³•è¯„ä¼°æ¯ä¸ªå•ç‹¬åˆæˆè§†å›¾çš„ç©ºé—´è´¨é‡å’Œæ•´ä½“è§†å›¾é—´çš„ä¸€è‡´æ€§ï¼Œè€Œç‚¹çº§æ–¹æ³•åˆ™å…³æ³¨åœºæ™¯è¡¨é¢ç‚¹çš„è§’åº¦è´¨é‡ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å¤åˆç‚¹è´¨é‡ã€‚è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå°†NeRF-NQAä¸23ç§ä¸»æµè§†è§‰è´¨é‡è¯„ä¼°æ–¹æ³•ï¼ˆæ¥è‡ªå›¾åƒã€è§†é¢‘å’Œå…‰åœºè¯„ä¼°é¢†åŸŸï¼‰è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒNeRF-NQAåœ¨è¯„ä¼°NVSåˆæˆåœºæ™¯æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰è¯„ä¼°æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ— å‚è€ƒè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºæå¤§çš„ä¼˜è¶Šæ€§ã€‚è¯¥è®ºæ–‡çš„å®ç°å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/VincentQQu/NeRF-NQA%E3%80%82">https://github.com/VincentQQu/NeRF-NQAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08029v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè§†å›¾åˆæˆï¼ˆNVSï¼‰æŠ€æœ¯èƒ½ç”Ÿæˆé«˜ä¿çœŸå¯†é›†è§†è§’è§†é¢‘ï¼Œä½†ç°æœ‰è´¨é‡è¯„ä¼°æ–¹æ³•å¦‚PSNRã€SSIMå’ŒLPIPSå¹¶ä¸é€‚åˆè¯„ä¼°NVSå’ŒNeRFå˜ä½“åˆæˆçš„å¯†é›†è§†è§’åœºæ™¯çš„è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºNeRF-NQAï¼Œä¸€ç§æ— éœ€å‚è€ƒçš„å¯†é›†è§‚å¯Ÿåœºæ™¯è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚NeRF-NQAé‡‡ç”¨è”åˆè´¨é‡è¯„ä¼°ç­–ç•¥ï¼Œç»“åˆè§†å›¾çº§å’Œç‚¹çº§æ–¹æ³•ï¼Œè¯„ä¼°NVSç”Ÿæˆåœºæ™¯çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-NQAæ˜¾è‘—ä¼˜äºç°æœ‰è¯„ä¼°æ–¹æ³•ï¼Œåœ¨æ— å‚è€ƒè¯„ä¼°NVSåˆæˆåœºæ™¯æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NVSæŠ€æœ¯å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸå¯†é›†è§†è§’è§†é¢‘ï¼Œä½†ç°æœ‰è´¨é‡è¯„ä¼°æ–¹æ³•ä¸é€‚ç”¨äºè¯„ä¼°å…¶è´¨é‡ã€‚</li>
<li>NeRF-NQAæ˜¯é¦–ä¸ªé’ˆå¯¹NVSå’ŒNeRFå˜ä½“åˆæˆçš„å¯†é›†è§‚å¯Ÿåœºæ™¯çš„æ— å‚è€ƒè´¨é‡è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>NeRF-NQAé‡‡ç”¨è”åˆè´¨é‡è¯„ä¼°ç­–ç•¥ï¼ŒåŒ…æ‹¬è§†å›¾çº§å’Œç‚¹çº§æ–¹æ³•ã€‚</li>
<li>è§†å›¾çº§æ–¹æ³•è¯„ä¼°æ¯ä¸ªåˆæˆè§†å›¾çš„ç©ºé—´è´¨é‡å’Œè§†å›¾é—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>ç‚¹çº§æ–¹æ³•å…³æ³¨åœºæ™¯è¡¨é¢ç‚¹çš„è§’åº¦è´¨é‡åŠå…¶å¤åˆç‚¹é—´è´¨é‡ã€‚</li>
<li>ä¸ä¸»æµè§†è§‰è´¨é‡è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼ŒNeRF-NQAè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-000e979692aa4c8d8f144717029a4813.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-775ff7b6a460a7114cf693b922a4bd06.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78219cb1d384ca8d12dbd03ce6eedf46.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abb5c891b1930a5afba3f2ee8b69ede5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-347f43769b5127f2924204f680331dde.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e06e2865f8f7f4232268634bd5d071bc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5cef61c5ce79d5b8b606d40762f67d11.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-34e0eff78638dad61378f2ff30c22917.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-731036771f824acedc6fe22837474de9.jpg" align="middle">
</details>




<h2 id="EventSplat-3D-Gaussian-Splatting-from-Moving-Event-Cameras-for-Real-time-Rendering"><a href="#EventSplat-3D-Gaussian-Splatting-from-Moving-Event-Cameras-for-Real-time-Rendering" class="headerlink" title="EventSplat: 3D Gaussian Splatting from Moving Event Cameras for   Real-time Rendering"></a>EventSplat: 3D Gaussian Splatting from Moving Event Cameras for   Real-time Rendering</h2><p><strong>Authors:Toshiya Yura, Ashkan Mirzaei, Igor Gilitschenski</strong></p>
<p>We introduce a method for using event camera data in novel view synthesis via Gaussian Splatting. Event cameras offer exceptional temporal resolution and a high dynamic range. Leveraging these capabilities allows us to effectively address the novel view synthesis challenge in the presence of fast camera motion. For initialization of the optimization process, our approach uses prior knowledge encoded in an event-to-video model. We also use spline interpolation for obtaining high quality poses along the event camera trajectory. This enhances the reconstruction quality from fast-moving cameras while overcoming the computational limitations traditionally associated with event-based Neural Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that our results achieve higher visual fidelity and better performance than existing event-based NeRF approaches while being an order of magnitude faster to render. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨äº‹ä»¶ç›¸æœºæ•°æ®é€šè¿‡é«˜æ–¯æ‹¼è´´è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆçš„æ–¹æ³•ã€‚äº‹ä»¶ç›¸æœºæä¾›å‡ºè‰²çš„æ—¶é—´åˆ†è¾¨ç‡å’Œé«˜åŠ¨æ€èŒƒå›´ã€‚åˆ©ç”¨è¿™äº›åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆè§£å†³å¿«é€Ÿç›¸æœºè¿åŠ¨ä¸‹æ–°é¢–è§†å›¾åˆæˆçš„æŒ‘æˆ˜ã€‚ä¸ºäº†åˆå§‹åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨äº‹ä»¶åˆ°è§†é¢‘çš„æ¨¡å‹ç¼–ç çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ ·æ¡æ’å€¼æ¥è·å¾—äº‹ä»¶ç›¸æœºè½¨è¿¹ä¸Šçš„é«˜è´¨é‡å§¿æ€ã€‚è¿™æé«˜äº†å¿«é€Ÿç§»åŠ¨ç›¸æœºçš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶å…‹æœäº†ä¼ ç»Ÿä¸Šä¸åŸºäºäº‹ä»¶çš„ç¥è§†è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•ç›¸å…³çš„è®¡ç®—é™åˆ¶ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»“æœå®ç°äº†æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ€§èƒ½ï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æ¯”ç°æœ‰åŸºäºäº‹ä»¶çš„NeRFæ–¹æ³•å¿«ä¸€ä¸ªæ•°é‡çº§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07293v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨äº‹ä»¶ç›¸æœºæ•°æ®è¿›è¡Œæ–°å‹è§†å›¾åˆæˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é«˜æ–¯å…‰æ–‘æŠ€æœ¯å®ç°ã€‚äº‹ä»¶ç›¸æœºå…·æœ‰å‡ºè‰²çš„æ—¶é—´åˆ†è¾¨ç‡å’Œé«˜åŠ¨æ€èŒƒå›´ï¼Œåˆ©ç”¨è¿™äº›ä¼˜åŠ¿èƒ½æœ‰æ•ˆè§£å†³å¿«é€Ÿç›¸æœºè¿åŠ¨ä¸‹çš„æ–°è§†å›¾åˆæˆæŒ‘æˆ˜ã€‚æ­¤æ–¹æ³•é‡‡ç”¨äº‹ä»¶åˆ°è§†é¢‘çš„æ¨¡å‹ç¼–ç å…ˆéªŒçŸ¥è¯†æ¥è¿›è¡Œä¼˜åŒ–è¿‡ç¨‹çš„åˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨æ ·æ¡æ’å€¼æ³•è·å–é«˜è´¨é‡çš„å§¿æ€æ²¿äº‹ä»¶ç›¸æœºè½¨è¿¹ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡å¹¶å…‹æœä¼ ç»Ÿäº‹ä»¶åŸºç¡€ä¸Šçš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•çš„è®¡ç®—é™åˆ¶ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æ¯”ç°æœ‰äº‹ä»¶åŸºç¡€ä¸Šçš„NeRFæ–¹æ³•å¿«ä¸€ä¸ªæ•°é‡çº§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨äº‹ä»¶ç›¸æœºçš„å‡ºè‰²æ—¶é—´åˆ†è¾¨ç‡å’Œé«˜åŠ¨æ€èŒƒå›´æ•°æ®è¿›è¡Œæ–°å‹è§†å›¾åˆæˆã€‚</li>
<li>é‡‡ç”¨äº‹ä»¶åˆ°è§†é¢‘çš„æ¨¡å‹ç¼–ç å…ˆéªŒçŸ¥è¯†æ¥è¿›è¡Œä¼˜åŒ–åˆå§‹åŒ–ã€‚</li>
<li>ä½¿ç”¨æ ·æ¡æ’å€¼æ³•è·å–é«˜è´¨é‡çš„å§¿æ€æ²¿äº‹ä»¶ç›¸æœºè½¨è¿¹ã€‚</li>
<li>æé«˜é‡å»ºè´¨é‡ï¼Œå…‹æœä¼ ç»Ÿäº‹ä»¶åŸºç¡€ä¸Šçš„NeRFæ–¹æ³•çš„è®¡ç®—é™åˆ¶ã€‚</li>
<li>å®ç°æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¸²æŸ“é€Ÿåº¦æ¯”ç°æœ‰äº‹ä»¶åŸºç¡€ä¸Šçš„NeRFæ–¹æ³•å¿«ä¸€ä¸ªæ•°é‡çº§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d334757298e3a609e928d5ed7294448e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-19e5b8ae6652639ff3b16a241816277d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d149ec977c9fe15354931880a088b743.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eb0e3141278ff281d9541c89ddc28abc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e40e4fc8211446e76a71bf1452fa8ae9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-637fa5732e7af15df7f6e821040e650f.jpg" align="middle">
</details>




<h2 id="Diffusing-Differentiable-Representations"><a href="#Diffusing-Differentiable-Representations" class="headerlink" title="Diffusing Differentiable Representations"></a>Diffusing Differentiable Representations</h2><p><strong>Authors:Yash Savani, Marc Finzi, J. Zico Kolter</strong></p>
<p>We introduce a novel, training-free method for sampling differentiable representations (diffreps) using pretrained diffusion models. Rather than merely mode-seeking, our method achieves sampling by â€œpulling backâ€ the dynamics of the reverse-time processâ€“from the image space to the diffrep parameter spaceâ€“and updating the parameters according to this pulled-back process. We identify an implicit constraint on the samples induced by the diffrep and demonstrate that addressing this constraint significantly improves the consistency and detail of the generated objects. Our method yields diffreps with substantially improved quality and diversity for images, panoramas, and 3D NeRFs compared to existing techniques. Our approach is a general-purpose method for sampling diffreps, expanding the scope of problems that diffusion models can tackle. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¯å¾®åˆ†è¡¨ç¤ºï¼ˆdiffrepï¼‰é‡‡æ ·çš„æ–°å‹æ— è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯å•çº¯åœ°å¯»æ±‚æ¨¡å¼ï¼Œè€Œæ˜¯é€šè¿‡â€œæ‹‰å›â€åå‘æ—¶é—´è¿‡ç¨‹çš„åŠ¨æ€æ¥å®ç°é‡‡æ ·â€”â€”ä»å›¾åƒç©ºé—´åˆ°diffrepå‚æ•°ç©ºé—´ï¼Œå¹¶æ ¹æ®æ‹‰å›çš„è¿‡ç¨‹æ›´æ–°å‚æ•°ã€‚æˆ‘ä»¬ç¡®å®šäº†ç”±diffrepå¼•èµ·çš„æ ·æœ¬ä¸Šçš„éšå¼çº¦æŸï¼Œå¹¶è¯æ˜è§£å†³æ­¤çº¦æŸå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆå¯¹è±¡çš„è¿è´¯æ€§å’Œç»†èŠ‚ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºå›¾åƒã€å…¨æ™¯å›¾å’Œ3D NeRFç”Ÿæˆäº†è´¨é‡å’Œå¤šæ ·æ€§æ˜¾è‘—æé«˜çš„diffrepã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šç”¨çš„diffrepé‡‡æ ·æ–¹æ³•ï¼Œæ‰©å¤§äº†æ‰©æ•£æ¨¡å‹å¯ä»¥è§£å†³çš„é—®é¢˜èŒƒå›´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06981v1">PDF</a> Published at NeurIPS 2024</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¯åˆ†åŒ–è¡¨ç¤ºï¼ˆdiffrepï¼‰é‡‡æ ·ã€‚ä¸åŒäºä¼ ç»Ÿçš„æ¨¡å¼å¯»æ±‚æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•é€šè¿‡åå‘æ—¶é—´çš„åŠ¨æ€â€œæ‹‰å›â€è¿‡ç¨‹ï¼Œä»å›¾åƒç©ºé—´åˆ°diffrepå‚æ•°ç©ºé—´è¿›è¡Œé‡‡æ ·ï¼Œå¹¶æ ¹æ®æ‹‰å›çš„è¿‡ç¨‹æ›´æ–°å‚æ•°ã€‚æ–‡ç« è¯†åˆ«äº†diffrepæ‰€éšå«çš„æ ·æœ¬çº¦æŸï¼Œå¹¶è¯æ˜è§£å†³è¿™ä¸€çº¦æŸèƒ½æ˜¾è‘—æé«˜ç”Ÿæˆç‰©ä½“çš„ä¸€è‡´æ€§å’Œç»†èŠ‚ã€‚ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å›¾åƒã€å…¨æ™¯å›¾å’Œ3D NeRFçš„diffrepç”Ÿæˆä¸­ï¼Œè´¨é‡å’Œå¤šæ ·æ€§å‡æœ‰æ˜¾è‘—æé«˜ã€‚è¿™æ˜¯ä¸€ç§é€šç”¨çš„diffrepé‡‡æ ·æ–¹æ³•ï¼Œæ‰©å¤§äº†æ‰©æ•£æ¨¡å‹å¯è§£å†³çš„é—®é¢˜èŒƒå›´ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡‡æ ·ã€‚</li>
<li>é€šè¿‡åå‘æ—¶é—´çš„åŠ¨æ€â€œæ‹‰å›â€è¿‡ç¨‹å®ç°ä»å›¾åƒç©ºé—´åˆ°diffrepå‚æ•°ç©ºé—´çš„é‡‡æ ·ã€‚</li>
<li>è¯†åˆ«äº†diffrepé‡‡æ ·ä¸­çš„éšå«çº¦æŸã€‚</li>
<li>è§£å†³è¿™ä¸€çº¦æŸèƒ½æ˜¾è‘—æé«˜ç”Ÿæˆç‰©ä½“çš„ä¸€è‡´æ€§å’Œç»†èŠ‚ã€‚</li>
<li>åœ¨å›¾åƒã€å…¨æ™¯å›¾å’Œ3D NeRFçš„ç”Ÿæˆä¸­ï¼Œdiffrepçš„è´¨é‡å’Œå¤šæ ·æ€§æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¯ä¸€ç§é€šç”¨çš„diffrepé‡‡æ ·æ–¹æ³•ã€‚</li>
<li>æ‰©å¤§äº†æ‰©æ•£æ¨¡å‹å¯è§£å†³çš„é—®é¢˜èŒƒå›´ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a6baeb493ba47cd67e8aa3e97f6481b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dc293729eac042afd34df15350e3572d.jpg" align="middle">
</details>




<h2 id="Enhancing-operational-wind-downscaling-capabilities-over-Canada-Application-of-a-Conditional-Wasserstein-GAN-methodology"><a href="#Enhancing-operational-wind-downscaling-capabilities-over-Canada-Application-of-a-Conditional-Wasserstein-GAN-methodology" class="headerlink" title="Enhancing operational wind downscaling capabilities over Canada:   Application of a Conditional Wasserstein GAN methodology"></a>Enhancing operational wind downscaling capabilities over Canada:   Application of a Conditional Wasserstein GAN methodology</h2><p><strong>Authors:Jorge Guevara, Victor Nascimento, Johannes Schmude, Daniel Salles, Simon Corbeil-LÃ©tourneau, Madalina Surcel, Dominique Brunet</strong></p>
<p>Wind downscaling is essential for improving the spatial resolution of weather forecasts, particularly in operational Numerical Weather Prediction (NWP). This study advances wind downscaling by extending the DownGAN framework introduced by Annau et al.,to operational datasets from the Global Deterministic Prediction System (GDPS) and High-Resolution Deterministic Prediction System (HRDPS), covering the entire Canadian domain. We enhance the model by incorporating high-resolution static covariates, such as HRDPS-derived topography, into a Conditional Wasserstein Generative Adversarial Network with Gradient Penalty, implemented using a UNET-based generator. Following the DownGAN framework, our methodology integrates low-resolution GDPS forecasts (15 km, 10-day horizon) and high-resolution HRDPS forecasts (2.5 km, 48-hour horizon) with Frequency Separation techniques adapted from computer vision. Through robust training and inference over the Canadian region, we demonstrate the operational scalability of our approach, achieving significant improvements in wind downscaling accuracy. Statistical validation highlights reductions in root mean square error (RMSE) and log spectral distance (LSD) metrics compared to the original DownGAN. High-resolution conditioning covariates and Frequency Separation strategies prove instrumental in enhancing model performance. This work underscores the potential for extending high-resolution wind forecasts beyond the 48-hour horizon, bridging the gap to the 10-day low resolution global forecast window. </p>
<blockquote>
<p>é£åœºé™å°ºåº¦åŒ–å¯¹äºæé«˜å¤©æ°”é¢„æŠ¥çš„ç©ºé—´åˆ†è¾¨ç‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸šåŠ¡æ•°å€¼å¤©æ°”é¢„æŠ¥ï¼ˆNWPï¼‰ä¸­ã€‚æœ¬ç ”ç©¶é€šè¿‡æ‰©å±•Annauç­‰äººä»‹ç»çš„DownGANæ¡†æ¶ï¼Œå°†é£åœºé™å°ºåº¦åŒ–æŠ€æœ¯åº”ç”¨äºå…¨çƒç¡®å®šæ€§é¢„æµ‹ç³»ç»Ÿï¼ˆGDPSï¼‰å’Œé«˜åˆ†è¾¨ç‡ç¡®å®šæ€§é¢„æµ‹ç³»ç»Ÿï¼ˆHRDPSï¼‰çš„ä¸šåŠ¡æ•°æ®é›†ï¼Œè¦†ç›–æ•´ä¸ªåŠ æ‹¿å¤§åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡å°†é«˜åˆ†è¾¨ç‡é™æ€åå˜é‡ï¼ˆå¦‚HRDPSè¡ç”Ÿçš„åœ°å½¢ï¼‰çº³å…¥å¸¦æœ‰æ¢¯åº¦æƒ©ç½šçš„æ¡ä»¶Wassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå¢å¼ºäº†æ¨¡å‹çš„åŠŸèƒ½ã€‚è¯¥ç½‘ç»œé‡‡ç”¨åŸºäºUNETçš„ç”Ÿæˆå™¨å®ç°ã€‚éµå¾ªDownGANæ¡†æ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä½åˆ†è¾¨ç‡GDPSé¢„æµ‹ï¼ˆ15å…¬é‡Œï¼Œ10å¤©è§†é‡ï¼‰å’Œé«˜åˆ†è¾¨ç‡HRDPSé¢„æµ‹ï¼ˆ2.5å…¬é‡Œï¼Œ48å°æ—¶è§†é‡ï¼‰ï¼Œå¹¶é‡‡ç”¨è®¡ç®—æœºè§†è§‰ä¸­çš„é¢‘ç‡åˆ†ç¦»æŠ€æœ¯ã€‚é€šè¿‡åœ¨åŠ æ‹¿å¤§åœ°åŒºçš„ç¨³å¥è®­ç»ƒå’Œæ¨ç†ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¸šåŠ¡ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§ï¼Œå¹¶åœ¨é£åœºé™å°ºåº¦åŒ–ç²¾åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç»Ÿè®¡éªŒè¯æ˜¾ç¤ºï¼Œä¸åŸå§‹DownGANç›¸æ¯”ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å’Œå¯¹æ•°è°±è·ç¦»ï¼ˆLSDï¼‰æŒ‡æ ‡æœ‰æ‰€é™ä½ã€‚é«˜åˆ†è¾¨ç‡æ¡ä»¶åå˜é‡å’Œé¢‘ç‡åˆ†ç¦»ç­–ç•¥å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å°†é«˜åˆ†è¾¨ç‡é£åœºé¢„æµ‹æ‰©å¤§åˆ°è¶…è¿‡48å°æ—¶è§†é‡çš„æ½œåŠ›ï¼Œä»¥ç¼©å°ä¸10å¤©ä½åˆ†è¾¨ç‡å…¨çƒé¢„æŠ¥çª—å£çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06958v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨DownGANæ¡†æ¶æ‰©å±•äº†é£é™å°ºåº¦æŠ€æœ¯ï¼Œå°†å…¶åº”ç”¨äºå…¨çƒç¡®å®šæ€§é¢„æµ‹ç³»ç»Ÿï¼ˆGDPSï¼‰å’Œé«˜åˆ†è¾¨ç‡ç¡®å®šæ€§é¢„æµ‹ç³»ç»Ÿï¼ˆHRDPSï¼‰çš„æ“ä½œæ•°æ®é›†ï¼Œè¦†ç›–æ•´ä¸ªåŠ æ‹¿å¤§åŒºåŸŸã€‚é€šè¿‡å¼•å…¥é«˜åˆ†è¾¨ç‡é™æ€åå˜é‡å’Œæ¡ä»¶Wassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰æŠ€æœ¯ï¼Œå®ç°äº†æ˜¾è‘—çš„é£é™å°ºåº¦ç²¾åº¦æå‡ã€‚ç ”ç©¶è¯æ˜è¯¥æ–¹æ³•çš„æ“ä½œå¯æ‰©å±•æ€§ï¼Œå¹¶å¼ºè°ƒå…¶åœ¨æé«˜é•¿æœŸé«˜åˆ†è¾¨ç‡å¤©æ°”é¢„æŠ¥æ½œåŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ‰©å±•äº†DownGANæ¡†æ¶ï¼Œåº”ç”¨äºæ“ä½œæ•°æ®é›†GDPSå’ŒHRDPSï¼Œè¦†ç›–æ•´ä¸ªåŠ æ‹¿å¤§åŒºåŸŸã€‚</li>
<li>é€šè¿‡ç»“åˆé«˜åˆ†è¾¨ç‡é™æ€åå˜é‡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æ¡ä»¶Wassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå¸¦æœ‰æ¢¯åº¦æƒ©ç½šçš„U-NETç”Ÿæˆå™¨å®ç°äº†æ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>åˆ©ç”¨é¢‘ç‡åˆ†ç¦»æŠ€æœ¯ï¼ŒæˆåŠŸæ•´åˆäº†ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å¤©æ°”é¢„æŠ¥æ•°æ®ã€‚</li>
<li>é€šè¿‡ç¨³å¥çš„è®­ç»ƒå’Œæ¨ç†ï¼Œå±•ç¤ºäº†æ–¹æ³•çš„æ“ä½œå¯æ‰©å±•æ€§ã€‚</li>
<li>ç»Ÿè®¡éªŒè¯æ˜¾ç¤ºï¼Œä¸åŸå§‹DownGANç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å’Œå¯¹æ•°è°±è·ç¦»ï¼ˆLSDï¼‰æŒ‡æ ‡æœ‰æ‰€é™ä½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-23348f6549c3cbaff32b630d8a7b8b6e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-73a9fce5db263ff4f008c491d71257fd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-533fac644ef9b0c641f7c4e040b6977f.jpg" align="middle">
</details>




<h2 id="MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting"><a href="#MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting" class="headerlink" title="MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting"></a>MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu</strong></p>
<p>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>. </p>
<blockquote>
<p>é‡å»ºé«˜ä¿çœŸ3Då¤´åƒå¯¹äºè™šæ‹Ÿç°å®ç­‰å„ç§åº”ç”¨è‡³å…³é‡è¦ã€‚å¼€åˆ›æ€§çš„æ–¹æ³•ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é‡å»ºé€¼çœŸçš„å¤´åƒï¼Œä½†å—åˆ°è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦çš„é™åˆ¶ã€‚åŸºäº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æœ€è¿‘çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œæ¸²æŸ“çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œ3DGSçš„è¡¨é¢ä¸ä¸€è‡´å¯¼è‡´å‡ ä½•ç²¾åº¦ä¸é«˜ï¼›åæ¥çš„2DGSä½¿ç”¨2Dè¡¨é¢å…ƒç´ ä»¥æé«˜å‡ ä½•ç²¾åº¦ï¼Œä½†ä»¥ç‰ºç‰²æ¸²æŸ“ä¿çœŸåº¦ä¸ºä»£ä»·ã€‚ä¸ºäº†åˆ©ç”¨2DGSå’Œ3DGSä¸¤è€…çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMixedGaussianAvatarçš„æ–°æ–¹æ³•ï¼Œç”¨äºé€¼çœŸä¸”å‡ ä½•ç²¾ç¡®çš„å¤´åƒé‡å»ºã€‚æˆ‘ä»¬çš„ä¸»è¦æƒ³æ³•æ˜¯ä½¿ç”¨2Dé«˜æ–¯é‡å»º3Då¤´åƒçš„è¡¨é¢ï¼Œä»¥ç¡®ä¿å‡ ä½•ç²¾åº¦ã€‚æˆ‘ä»¬å°†2Dé«˜æ–¯é™„åŠ åˆ°FLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼ä¸Šï¼Œå¹¶åœ¨2DGSçš„æ¸²æŸ“è´¨é‡ä¸è¶³çš„åœ°æ–¹è¿æ¥åˆ°é¢å¤–çš„3Dé«˜æ–¯ï¼Œåˆ›å»ºæ··åˆçš„2D-3Dé«˜æ–¯è¡¨ç¤ºã€‚è¿™äº›2D-3Dé«˜æ–¯å¯ä»¥ä½¿ç”¨FLAMEå‚æ•°è¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè®­ç»ƒ2Dé«˜æ–¯ï¼Œç„¶åå¯¹æ··åˆçš„2D-3Dé«˜æ–¯è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®éªŒå±•ç¤ºäº†MixedGaussianAvatarçš„ä¼˜åŠ¿ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04955v2">PDF</a> Project: <a target="_blank" rel="noopener" href="https://chenvoid.github.io/MGA/">https://chenvoid.github.io/MGA/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å…ˆè¿›æ–¹æ³•åœ¨å®ç°é«˜è´¨é‡çš„ä¸‰ç»´å¤´åƒé‡å»ºæ–¹é¢å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†åœ¨è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ä¸Šå­˜åœ¨å±€é™ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œç ”ç©¶è€…æå‡ºåŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ï¼Œä½†å­˜åœ¨å‡ ä½•ç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚åç»­çš„äºŒç»´é«˜æ–¯æ‹¼è´´ï¼ˆ2DGSï¼‰è™½æé«˜äº†å‡ ä½•ç²¾åº¦ï¼Œä½†ç‰ºç‰²äº†æ¸²æŸ“è´¨é‡ã€‚æœ¬ç ”ç©¶ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œæå‡ºåä¸ºMixedGaussianAvatarçš„æ–°å‹æ–¹æ³•ï¼Œåˆ©ç”¨äºŒç»´é«˜æ–¯é‡å»ºä¸‰ç»´å¤´éƒ¨çš„è¡¨é¢ï¼Œç¡®ä¿å‡ ä½•ç²¾åº¦ï¼Œå¹¶åœ¨å¿…è¦æ—¶ä½¿ç”¨ä¸‰ç»´é«˜æ–¯è¡¥å……ã€‚è¯¥æ–¹æ³•å°†äºŒç»´é«˜æ–¯è´´åˆäºFLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼ä¸Šï¼Œå¹¶é€šè¿‡é€æ­¥è®­ç»ƒç­–ç•¥ä¼˜åŒ–æ··åˆçš„äºŒç»´ä¸ä¸‰ç»´é«˜æ–¯ã€‚MixedGaussianAvatarçš„å®éªŒç»“æœè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Neural Radiance Fields (NeRF)å·²è¢«ç”¨äºåˆ›å»ºé«˜è´¨é‡çš„ä¸‰ç»´å¤´åƒé‡å»ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚3DGSå’Œ2DGSåˆ†åˆ«åœ¨æ•ˆç‡å’Œå‡ ä½•ç²¾åº¦æˆ–æ¸²æŸ“è´¨é‡æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>MixedGaussianAvatarç»“åˆäº†2DGSå’Œ3DGSçš„ä¼˜ç‚¹ï¼Œç¡®ä¿å‡ ä½•ç²¾åº¦çš„åŒæ—¶æé«˜æ¸²æŸ“è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨FLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼è´´åˆäºŒç»´é«˜æ–¯ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¡¥å……ä¸‰ç»´é«˜æ–¯ã€‚</li>
<li>é‡‡ç”¨é€æ­¥è®­ç»ƒç­–ç•¥ä¼˜åŒ–æ··åˆçš„äºŒç»´ä¸ä¸‰ç»´é«˜æ–¯ã€‚</li>
<li>MixedGaussianAvatarçš„å®éªŒç»“æœè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fee7aa56253f4eb6856f0bdf9d9655e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0ef5787956810f1e111d21adf0bdcf5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ce4f964cf25207a6db5a28f7f85bd755.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-440cea39b9c601afb4654560b1a89e0a.png" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43282ee6f05919c0b760a138b7ff6c40.jpg" align="middle">
</details>




<h2 id="Learning-based-Multi-View-Stereo-A-Survey"><a href="#Learning-based-Multi-View-Stereo-A-Survey" class="headerlink" title="Learning-based Multi-View Stereo: A Survey"></a>Learning-based Multi-View Stereo: A Survey</h2><p><strong>Authors:Fangjinhua Wang, Qingtian Zhu, Di Chang, Quankai Gao, Junlin Han, Tong Zhang, Richard Hartley, Marc Pollefeys</strong></p>
<p>3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented&#x2F;Virtual Reality (AR&#x2F;VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area. </p>
<blockquote>
<p>ä¸‰ç»´é‡å»ºæ—¨åœ¨æ¢å¤åœºæ™¯çš„å¯†é›†ä¸‰ç»´ç»“æ„ã€‚å®ƒåœ¨å¢å¼º&#x2F;è™šæ‹Ÿç°å®ï¼ˆAR&#x2F;VRï¼‰ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰å„ç§åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åˆ©ç”¨ä»ä¸åŒè§†è§’æ•æ‰çš„åœºæ™¯çš„å¤šä¸ªè§†å›¾ï¼Œå¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰ç®—æ³•åˆæˆå…¨é¢çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå®ç°åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç²¾ç¡®é‡å»ºã€‚ç”±äºå…¶æ•ˆç‡å’Œæœ‰æ•ˆæ€§ï¼ŒMVSå·²æˆä¸ºåŸºäºå›¾åƒçš„3Dé‡å»ºçš„å…³é”®æ–¹æ³•ã€‚æœ€è¿‘ï¼Œéšç€æ·±åº¦å­¦ä¹ å–å¾—çš„æˆåŠŸï¼Œå·²ç»æå‡ºäº†è®¸å¤šåŸºäºå­¦ä¹ çš„MVSæ–¹æ³•ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚æˆ‘ä»¬å°†è¿™äº›åŸºäºå­¦ä¹ çš„æ–¹æ³•åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼šåŸºäºæ·±åº¦å›¾çš„æ–¹æ³•ã€åŸºäºä½“ç´ çš„æ–¹æ³•ã€åŸºäºNeRFçš„æ–¹æ³•ã€åŸºäºä¸‰ç»´é«˜æ–¯Splattingçš„æ–¹æ³•å’Œå¤§å‹å‰é¦ˆæ–¹æ³•ã€‚å…¶ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨åŸºäºæ·±åº¦å›¾çš„æ–¹æ³•ï¼Œç”±äºå…¶ç®€æ´æ€§ã€çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå®ƒä»¬æˆä¸ºMVSçš„ä¸»è¦å®¶æ—ã€‚åœ¨æœ¬æ–‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å…¨é¢å›é¡¾äº†æˆªè‡³å½“å‰æ—¥æœŸçš„æ–‡çŒ®ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†è¿™äº›åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œæ€»ç»“äº†å®ƒä»¬åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è®¨è®ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„å……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15235v2">PDF</a> </p>
<p><strong>Summary</strong><br>ä¸‰ç»´é‡å»ºæ—¨åœ¨æ¢å¤åœºæ™¯çš„å¯†é›†ä¸‰ç»´ç»“æ„ï¼Œå¯¹äºå¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ã€‚åˆ©ç”¨ä»ä¸åŒè§†è§’æ•æ‰çš„åœºæ™¯çš„å¤šä¸ªè§†å›¾ï¼Œå¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰ç®—æ³•åˆæˆå…¨é¢çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå®ç°åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç²¾ç¡®é‡å»ºã€‚æœ€è¿‘ï¼Œéšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æˆåŠŸåº”ç”¨ï¼Œè®¸å¤šåŸºäºå­¦ä¹ çš„MVSæ–¹æ³•è¢«æå‡ºï¼Œç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½è¡¨ç°ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨åŸºäºæ·±åº¦å›¾çš„æ–¹æ³•ï¼Œå®ƒä»¬æ˜¯MVSçš„ä¸»è¦å®¶æ—ï¼Œå…·æœ‰ç®€æ´æ€§ã€çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚æœ¬æ–‡å¯¹æ­¤é¢†åŸŸçš„æ–‡çŒ®è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œè°ƒæŸ¥äº†è¿™äº›åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œæ€»ç»“äº†å®ƒä»¬åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶è®¨è®ºäº†è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dé‡å»ºæ—¨åœ¨æ¢å¤åœºæ™¯çš„å¯†é›†ä¸‰ç»´ç»“æ„ï¼Œå¹¿æ³›åº”ç”¨äºAR&#x2F;VRã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸã€‚</li>
<li>å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰ç®—æ³•èƒ½ä»ä¸åŒè§†è§’æ•æ‰åœºæ™¯å¹¶åˆæˆå…¨é¢çš„ä¸‰ç»´è¡¨ç¤ºã€‚</li>
<li>åŸºäºæ·±åº¦å­¦ä¹ çš„MVSæ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>åŸºäºæ·±åº¦å›¾çš„æ–¹æ³•æ˜¯MVSçš„ä¸»è¦æ–¹æ³•ï¼Œå…·æœ‰ç®€æ´æ€§ã€çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ç°æœ‰æ–‡çŒ®å¯¹äºåŸºäºå­¦ä¹ çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚</li>
<li>è¿™äº›æ–¹æ³•åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¡¨ç°è¢«æ€»ç»“å‡ºæ¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a5e5346a998309aa296c8385f856de80.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-22ada0555da5fef891a724a431157d98.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-03bb12f1f648696bd6045b65e15edfd1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a69c8889a02a0a54adc1f576279f164.jpg" align="middle">
</details>




<h2 id="Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions"><a href="#Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions" class="headerlink" title="Rethinking Score Distillation as a Bridge Between Image Distributions"></a>Rethinking Score Distillation as a Bridge Between Image Distributions</h2><p><strong>Authors:David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa</strong></p>
<p>Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains. Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications. In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution. Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target). We argue that current methodsâ€™ characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution. We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead. Our method can be easily applied across many domains, matching or beating the performance of specialized methods. We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real. We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors. </p>
<blockquote>
<p>å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰å·²è¢«è¯æ˜æ˜¯ä¸€ä¸ªé‡è¦å·¥å…·ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸä½¿ç”¨å¤§è§„æ¨¡æ‰©æ•£å…ˆéªŒæ¥å®Œæˆä»»åŠ¡ã€‚ç„¶è€Œï¼ŒSDSå­˜åœ¨ä¸€äº›ç‰¹å¾æ€§ä¼ªå½±ï¼Œé™åˆ¶äº†å…¶åœ¨é€šç”¨åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶è§†ä¸ºä»æºåˆ†å¸ƒåˆ°ç›®æ ‡åˆ†å¸ƒçš„æœ€ä½³æˆæœ¬ä¼ è¾“è·¯å¾„çš„è§£å†³æ–¹å¼ï¼Œæ¥æ¨è¿›å¯¹SDSåŠå…¶å˜ä½“çš„è¡Œä¸ºçš„ç†è§£ã€‚åœ¨è¿™ç§æ–°è§£é‡Šä¸‹ï¼Œè¿™äº›æ–¹æ³•è¯•å›¾å°†æŸåçš„å›¾åƒï¼ˆæºï¼‰ä¼ è¾“åˆ°è‡ªç„¶å›¾åƒåˆ†å¸ƒï¼ˆç›®æ ‡ï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºå½“å‰æ–¹æ³•çš„ç‰¹å¾æ€§ä¼ªå½±æ˜¯ç”±ï¼ˆ1ï¼‰æœ€ä½³è·¯å¾„çš„çº¿æ€§è¿‘ä¼¼å’Œï¼ˆ2ï¼‰æºåˆ†å¸ƒä¼°è®¡ä¸ä½³é€ æˆçš„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ ¡å‡†æºåˆ†å¸ƒçš„æ–‡æœ¬æ¡ä»¶å¯ä»¥åœ¨å‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€çš„æƒ…å†µä¸‹äº§ç”Ÿé«˜è´¨é‡çš„ç”Ÿæˆå’Œç¿»è¯‘ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ¾åº”ç”¨äºè®¸å¤šé¢†åŸŸï¼Œè¾¾åˆ°æˆ–è¶…è¿‡ä¸“ä¸šæ–¹æ³•çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬åˆ°äºŒç»´ã€åŸºäºæ–‡æœ¬çš„NeRFä¼˜åŒ–ã€ç»˜ç”»åˆ°ç°å®å›¾åƒçš„ç¿»è¯‘ã€å…‰å­¦å¹»è§‰ç”Ÿæˆå’Œä¸‰ç»´è‰å›¾åˆ°ç°å®ç­‰åº”ç”¨ä¸­å±•ç¤ºäº†å…¶å®ç”¨æ€§ã€‚æˆ‘ä»¬å°†æ–¹æ³•ä¸ç°æœ‰çš„åˆ†æ•°è’¸é¦é‡‡æ ·æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯æ˜å®ƒèƒ½å¤Ÿäº§ç”Ÿå…·æœ‰çœŸå®è‰²å½©çš„é«˜é¢‘ç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09417v2">PDF</a> NeurIPS 2024. Project webpage: <a target="_blank" rel="noopener" href="https://sds-bridge.github.io/">https://sds-bridge.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åœ¨å¤„ç†æ•°æ®ç¨€ç¼ºé¢†åŸŸä»»åŠ¡æ—¶åˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å…ˆéªŒçš„ä¼˜åŠ¿ï¼Œå¹¶é’ˆå¯¹SDSå­˜åœ¨çš„ç‰¹å¾æ€§ä¼ªå½±é—®é¢˜è¿›è¡Œäº†æ¢ç©¶ã€‚æ–‡ç« å°†SDSåŠå…¶å˜ä½“çœ‹ä½œæ˜¯ä»æºåˆ†å¸ƒåˆ°ç›®æ ‡åˆ†å¸ƒçš„æœ€ä¼˜æˆæœ¬ä¼ è¾“è·¯å¾„çš„æ±‚è§£è¿‡ç¨‹ã€‚é€šè¿‡è°ƒæ•´æºåˆ†å¸ƒçš„æ–‡æœ¬æ¡ä»¶ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ ¡å‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡ç”Ÿæˆå’Œç¿»è¯‘ç»“æœï¼Œä¸”é¢å¤–å¼€é”€è¾ƒå°ã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä¸ä¸“é¡¹æ–¹æ³•ç›¸æ¯”å…·æœ‰åŒ¹é…æˆ–è¶…è¶Šçš„æ€§èƒ½ã€‚å®ƒåœ¨æ–‡æœ¬åˆ°äºŒç»´å›¾åƒã€åŸºäºæ–‡æœ¬çš„NeRFä¼˜åŒ–ã€ç»˜ç”»åˆ°ç°å®å›¾åƒçš„ç¿»è¯‘ã€å…‰å­¦å¹»è§‰ç”Ÿæˆå’Œä¸‰ç»´è‰å›¾åˆ°ç°å®ç­‰åº”ç”¨ä¸­å±•ç¤ºäº†å®ç”¨æ€§ã€‚é€šè¿‡ä¸ç°æœ‰è¯„åˆ†è’¸é¦é‡‡æ ·æ–¹æ³•çš„æ¯”è¾ƒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿå…·æœ‰çœŸå®è‰²å½©çš„é«˜é¢‘ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åœ¨å¤„ç†æ•°æ®ç¨€ç¼ºé¢†åŸŸä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å…ˆéªŒã€‚</li>
<li>SDSæ–¹æ³•å­˜åœ¨ç‰¹å¾æ€§ä¼ªå½±é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨é€šç”¨åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚</li>
<li>æ–‡ç« å°†SDSåŠå…¶å˜ä½“è§£è¯»ä¸ºä»æºåˆ†å¸ƒåˆ°ç›®æ ‡åˆ†å¸ƒçš„æœ€ä¼˜æˆæœ¬ä¼ è¾“è·¯å¾„çš„æ±‚è§£è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡è°ƒæ•´æºåˆ†å¸ƒçš„æ–‡æœ¬æ¡ä»¶ï¼Œæå‡ºäº†ä¸€ç§æ ¡å‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„ç”Ÿæˆå’Œç¿»è¯‘ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°äºŒç»´å›¾åƒè½¬æ¢ã€åŸºäºæ–‡æœ¬çš„NeRFä¼˜åŒ–ç­‰ã€‚</li>
<li>ä¸å…¶ä»–è¯„åˆ†è’¸é¦é‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿå…·æœ‰çœŸå®è‰²å½©çš„é«˜é¢‘ç»†èŠ‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ab688bcbe79403b9dc0a82fa87e55b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-583f9aab3efdf9acf3d30ae12a8d5845.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd2ee8b180e521f6c21caaa035b7cc5f.jpg" align="middle">
</details>




<h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p>
<p>3D Gaussian Splatting (3DGS) has significantly advanced 3D scene reconstruction and novel view synthesis. However, like Neural Radiance Fields (NeRF), 3DGS struggles with accurately modeling physical reflections, particularly in mirrors, leading to incorrect reconstructions and inconsistent reflective properties. To address this challenge, we introduce Mirror-3DGS, a novel framework designed to accurately handle mirror geometries and reflections, thereby generating realistic mirror reflections. By incorporating mirror attributes into 3DGS and leveraging plane mirror imaging principles, Mirror-3DGS simulates a mirrored viewpoint from behind the mirror, enhancing the realism of scene renderings. Extensive evaluations on both synthetic and real-world scenes demonstrate that our method can render novel views with improved fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF, especially in mirror regions. </p>
<blockquote>
<p>3Dé«˜æ–¯èåˆï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œä¸ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¸€æ ·ï¼Œ3DGSåœ¨å‡†ç¡®æ¨¡æ‹Ÿç‰©ç†åå°„æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå­ä¸­ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®å’Œåå°„å±æ€§ä¸ä¸€è‡´ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Mirror-3DGSï¼Œè¿™æ˜¯ä¸€ä¸ªè®¾è®¡ç”¨äºå‡†ç¡®å¤„ç†é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„çš„æ–°å‹æ¡†æ¶ï¼Œä»è€Œç”Ÿæˆé€¼çœŸçš„é•œå­åå°„ã€‚é€šè¿‡å°†é•œå­å±æ€§èå…¥3DGSå¹¶åˆ©ç”¨å¹³é¢é•œåƒæˆåƒåŸç†ï¼ŒMirror-3DGSæ¨¡æ‹Ÿäº†é•œå­åé¢çš„é•œåƒè§†ç‚¹ï¼Œå¢å¼ºäº†åœºæ™¯æ¸²æŸ“çš„é€¼çœŸåº¦ã€‚å¯¹åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯çš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å®æ—¶ä¸­ä»¥æ›´é«˜çš„ä¿çœŸåº¦å‘ˆç°æ–°å‹è§†å›¾ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„Mirror-NeRFï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå­åŒºåŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01168v2">PDF</a> IEEE International Conference on Visual Communications and Image   Processing (VCIP 2024, Oral)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼ˆ3DGSï¼‰ï¼Œæ–°çš„æ¡†æ¶Mirror-3DGSè¢«æå‡ºç”¨äºå‡†ç¡®å¤„ç†é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„ï¼Œè§£å†³äº†é•œåƒæ¸²æŸ“ä¸çœŸå®çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆé•œå­å±æ€§å’Œå¹³é¢é•œåƒæˆåƒåŸç†ï¼Œå®ƒèƒ½å¤Ÿæ¨¡æ‹Ÿä»é•œå­åé¢çš„è§†è§’æ¥å‘ˆç°é•œåƒï¼Œæé«˜åœºæ™¯æ¸²æŸ“çš„çœŸå®æ€§ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯ä¸­çš„è¯„ä¼°è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶æ¸²æŸ“ä¸­æé«˜ä¿çœŸåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå­åŒºåŸŸçš„è¡¨ç°è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„Mirror-NeRFæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mirror-3DGSæ˜¯ä¸ºäº†è§£å†³åœ¨ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆä¸­ï¼Œé•œåƒåå°„å»ºæ¨¡ä¸å‡†ç¡®çš„é—®é¢˜è€Œæå‡ºçš„ã€‚</li>
<li>å®ƒç»“åˆäº†é•œå­å±æ€§å’Œå¹³é¢é•œåƒæˆåƒåŸç†ï¼Œä»¥å‡†ç¡®å¤„ç†é•œå­å‡ ä½•å½¢çŠ¶å’Œåå°„ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿä»é•œå­åé¢çš„è§†è§’å‘ˆç°é•œåƒï¼ŒMirror-3DGSæé«˜äº†åœºæ™¯æ¸²æŸ“çš„çœŸå®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå­åŒºåŸŸçš„æ¸²æŸ“æ•ˆæœã€‚</li>
<li>Mirror-3DGSè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„Mirror-NeRFæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶æ¸²æŸ“ä¸­æé«˜ä¿çœŸåº¦ã€‚</li>
<li>å®ƒçš„å¼•å…¥æœ‰åŠ©äºæ¨åŠ¨ä¸‰ç»´åœºæ™¯é‡å»ºå’Œè§†å›¾åˆæˆçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dd7869e208d9fb8d79482f7e49bf8dfd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-547b2159e15ad9259a5f5758f4258655.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc1bfa303ba582248284272ab2f58d3c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c71cd65107819c859fa830f5e804483d.jpg" align="middle">
</details>




<h2 id="UniSDF-Unifying-Neural-Representations-for-High-Fidelity-3D-Reconstruction-of-Complex-Scenes-with-Reflections"><a href="#UniSDF-Unifying-Neural-Representations-for-High-Fidelity-3D-Reconstruction-of-Complex-Scenes-with-Reflections" class="headerlink" title="UniSDF: Unifying Neural Representations for High-Fidelity 3D   Reconstruction of Complex Scenes with Reflections"></a>UniSDF: Unifying Neural Representations for High-Fidelity 3D   Reconstruction of Complex Scenes with Reflections</h2><p><strong>Authors:Fangjinhua Wang, Marie-Julie Rakotosaona, Michael Niemeyer, Richard Szeliski, Marc Pollefeys, Federico Tombari</strong></p>
<p>Neural 3D scene representations have shown great potential for 3D reconstruction from 2D images. However, reconstructing real-world captures of complex scenes still remains a challenge. Existing generic 3D reconstruction methods often struggle to represent fine geometric details and do not adequately model reflective surfaces of large-scale scenes. Techniques that explicitly focus on reflective surfaces can model complex and detailed reflections by exploiting better reflection parameterizations. However, we observe that these methods are often not robust in real scenarios where non-reflective as well as reflective components are present. In this work, we propose UniSDF, a general purpose 3D reconstruction method that can reconstruct large complex scenes with reflections. We investigate both camera view as well as reflected view-based color parameterization techniques and find that explicitly blending these representations in 3D space enables reconstruction of surfaces that are more geometrically accurate, especially for reflective surfaces. We further combine this representation with a multi-resolution grid backbone that is trained in a coarse-to-fine manner, enabling faster reconstructions than prior methods. Extensive experiments on object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF 360 and Ref-NeRF real demonstrate that our method is able to robustly reconstruct complex large-scale scenes with fine details and reflective surfaces, leading to the best overall performance. Project page: \url{<a target="_blank" rel="noopener" href="https://fangjinhuawang.github.io/UniSDF%7D">https://fangjinhuawang.github.io/UniSDF}</a>. </p>
<blockquote>
<p>ç¥ç»ä¸‰ç»´åœºæ™¯è¡¨ç¤ºåœ¨ä»äºŒç»´å›¾åƒè¿›è¡Œä¸‰ç»´é‡å»ºæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œé‡å»ºå¤æ‚åœºæ™¯çš„çœŸå®ä¸–ç•Œæ•è·ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„é€šç”¨ä¸‰ç»´é‡å»ºæ–¹æ³•å¾€å¾€éš¾ä»¥è¡¨ç¤ºç²¾ç»†çš„å‡ ä½•ç»†èŠ‚ï¼Œå¹¶ä¸”ä¸è¶³ä»¥å¯¹å¤§è§„æ¨¡åœºæ™¯çš„åå°„é¢è¿›è¡Œå»ºæ¨¡ã€‚ä¸“æ³¨äºåå°„è¡¨é¢çš„æŠ€æœ¯å¯ä»¥é€šè¿‡åˆ©ç”¨æ›´å¥½çš„åå°„å‚æ•°åŒ–æ¥æ¨¡æ‹Ÿå¤æ‚å’Œè¯¦ç»†çš„åå°„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“å­˜åœ¨éåå°„å’Œåå°„ç»„ä»¶æ—¶ï¼Œè¿™äº›æ–¹æ³•åœ¨ç°å®åœºæ™¯ä¸­å¾€å¾€ä¸å¤Ÿç¨³å¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniSDFï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨ä¸‰ç»´é‡å»ºæ–¹æ³•ï¼Œå¯ä»¥é‡å»ºå…·æœ‰åå°„çš„å¤§è§„æ¨¡å¤æ‚åœºæ™¯ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºç›¸æœºè§†è§’å’Œåå°„è§†è§’çš„é¢œè‰²å‚æ•°åŒ–æŠ€æœ¯ï¼Œå¹¶å‘ç°æ˜ç¡®åœ°åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ··åˆè¿™äº›è¡¨ç¤ºå¯ä»¥é‡å»ºæ›´å‡ ä½•ç²¾ç¡®çš„è¡¨é¢ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåå°„è¡¨é¢ã€‚æˆ‘ä»¬å°†è¿™ç§è¡¨ç¤ºä¸ä»¥ç²—åˆ°ç»†æ–¹å¼è®­ç»ƒçš„å¤šåˆ†è¾¨ç‡ç½‘æ ¼ä¸»å¹²ç›¸ç»“åˆï¼Œå®ç°äº†æ¯”ä»¥å¾€æ–¹æ³•æ›´å¿«çš„é‡å»ºé€Ÿåº¦ã€‚åœ¨å¯¹è±¡çº§æ•°æ®é›†DTUã€Shiny Blenderä»¥åŠæ— ç•Œæ•°æ®é›†Mip-NeRF 360å’ŒRef-NeRF realä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç¨³å¥åœ°é‡å»ºå…·æœ‰ç²¾ç»†ç»†èŠ‚å’Œåå°„è¡¨é¢çš„å¤æ‚å¤§è§„æ¨¡åœºæ™¯ï¼Œå–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://fangjinhuawang.github.io/UniSDF%E3%80%82">https://fangjinhuawang.github.io/UniSDFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.13285v2">PDF</a> NeurIPS 2024 camera ready</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Neural 3Dåœºæ™¯è¡¨ç¤ºåœ¨3Dé‡å»ºé¢†åŸŸçš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤æ‚åœºæ™¯çš„é‡å»ºã€‚ç°æœ‰é€šç”¨3Dé‡å»ºæ–¹æ³•åœ¨è¡¨ç¤ºç²¾ç»†å‡ ä½•ç»†èŠ‚å’Œæ¨¡æ‹Ÿå¤§å‹åœºæ™¯çš„åå°„è¡¨é¢æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨ç›®çš„çš„3Dé‡å»ºæ–¹æ³•UniSDFï¼Œèƒ½å¤Ÿé‡å»ºå…·æœ‰åå°„çš„å¤§å‹å¤æ‚åœºæ™¯ã€‚é€šè¿‡è°ƒæŸ¥ç›¸æœºè§†è§’å’Œåå°„è§†è§’çš„é¢œè‰²å‚æ•°åŒ–æŠ€æœ¯ï¼Œå¹¶æ˜ç¡®åœ°å°†è¿™äº›è¡¨ç¤ºæ··åˆåœ¨3Dç©ºé—´ä¸­ï¼Œå®ç°äº†å¯¹åå°„è¡¨é¢çš„æ›´ç²¾ç¡®å‡ ä½•é‡å»ºã€‚æ­¤å¤–ï¼Œç»“åˆå¤šåˆ†è¾¨ç‡ç½‘æ ¼ä¸»å¹²è¿›è¡Œç²—åˆ°ç»†çš„è®­ç»ƒï¼Œå®ç°äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¿«çš„é‡å»ºé€Ÿåº¦ã€‚åœ¨å¯¹è±¡çº§æ•°æ®é›†DTUã€Shiny Blenderä»¥åŠæ— ç•Œæ•°æ®é›†Mip-NeRF 360å’ŒRef-NeRF realä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç¨³å¥åœ°é‡å»ºå…·æœ‰ç²¾ç»†ç»†èŠ‚å’Œåå°„è¡¨é¢çš„å¤æ‚å¤§å‹åœºæ™¯ï¼Œå–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Neural 3Dåœºæ™¯è¡¨ç¤ºåœ¨3Dé‡å»ºä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯æ–¹é¢ã€‚</li>
<li>ç°æœ‰é€šç”¨3Dé‡å»ºæ–¹æ³•åœ¨æ¨¡æ‹Ÿåå°„è¡¨é¢å’Œç²¾ç»†å‡ ä½•ç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>UniSDFæ˜¯ä¸€ç§æ–°å‹çš„3Dé‡å»ºæ–¹æ³•ï¼Œèƒ½å¤Ÿé‡å»ºå…·æœ‰åå°„çš„å¤§å‹å¤æ‚åœºæ™¯ã€‚</li>
<li>UniSDFé€šè¿‡æ··åˆç›¸æœºè§†è§’å’Œåå°„è§†è§’çš„é¢œè‰²å‚æ•°åŒ–è¡¨ç¤ºï¼Œæé«˜äº†å¯¹åå°„è¡¨é¢çš„å‡ ä½•å‡†ç¡®æ€§ã€‚</li>
<li>UniSDFé‡‡ç”¨å¤šåˆ†è¾¨ç‡ç½‘æ ¼ä¸»å¹²è¿›è¡Œç²—åˆ°ç»†çš„è®­ç»ƒï¼Œå®ç°äº†æ›´å¿«çš„é‡å»ºé€Ÿåº¦ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniSDFåœ¨é‡å»ºå…·æœ‰ç²¾ç»†ç»†èŠ‚å’Œåå°„è¡¨é¢çš„å¤æ‚å¤§å‹åœºæ™¯æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-44ad05589092be4a2ce220e1e63f9eea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-42cb3052d279bf31b0971c292f0b5ce8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56725227a346517f56bf4697cd76f5dc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-df5b2dd7ca8cc571de05d576959e4b3c.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>å¯¹äºå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ï¼ˆOCTAï¼‰å›¾åƒï¼Œæœ‰é™çš„æ‰«æç‡å¯¼è‡´è§†é‡ï¼ˆFOVï¼‰ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚è™½ç„¶è¾ƒå¤§çš„FOVå›¾åƒå¯èƒ½ä¼šæ­ç¤ºæ›´å¤šçš„æ—ä¸­å¿ƒå‡¹è¡€ç®¡ç—…å˜ï¼Œä½†ç”±äºåˆ†è¾¨ç‡è¾ƒä½ï¼Œå…¶åº”ç”¨å—åˆ°å¾ˆå¤§é˜»ç¢ã€‚ä¸ºäº†æé«˜åˆ†è¾¨ç‡ï¼Œä»¥å‰çš„å·¥ä½œåªæœ‰é€šè¿‡ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒæ‰èƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œä½†ç°å®ä¸–ç•Œçš„åº”ç”¨å—åˆ°æ”¶é›†å¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æŒ‘æˆ˜çš„é™åˆ¶ã€‚å› æ­¤ï¼Œå¯¹æœªé…å¯¹æ–¹æ³•çš„éœ€æ±‚å¾ˆå¤§ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å·²åœ¨æœªé…å¯¹è®¾ç½®ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒå¯èƒ½éš¾ä»¥å‡†ç¡®ä¿ç•™ç»†å¾®çš„æ¯›ç»†è¡€ç®¡ç»†èŠ‚ï¼Œè¿™äº›å¯¹äºOCTAæ˜¯è‡³å…³é‡è¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æœ¬æ–‡çš„æ–¹æ³•æ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢‘ç‡ä¿¡æ¯æ¥ä¿ç•™è¿™äº›ç»†èŠ‚ï¼Œå°†ç»†èŠ‚è§†ä¸ºé«˜é¢‘ï¼ˆhfï¼‰ï¼Œå°†ç²—ç²’èƒŒæ™¯è§†ä¸ºä½é¢‘ï¼ˆlfï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„æœªé…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ç”¨äºOCTAå›¾åƒï¼Œå¹¶é€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨ç‰¹åˆ«å¼ºè°ƒäº†hfç»†å¾®æ¯›ç»†è¡€ç®¡ã€‚ä¸ºäº†ä¿ƒè¿›é‡å»ºå›¾åƒçš„ç²¾ç¡®å…‰è°±ï¼Œæˆ‘ä»¬è¿˜ä¸ºé‰´åˆ«å™¨æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æ€§æŸå¤±ï¼Œå¹¶ä¸ºç«¯åˆ°ç«¯ä¼˜åŒ–å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰ä¸Šå‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æœªé…å¯¹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºå¤„ç†OCTAå›¾åƒã€‚é€šè¿‡åˆ©ç”¨é¢‘ç‡ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨ä¿ç•™å…³é”®çš„æ¯›ç»†è¡€ç®¡ç»†èŠ‚ï¼Œå¹¶é€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨çªå‡ºé«˜é¢‘ç²¾ç»†æ¯›ç»†è¡€ç®¡ã€‚åŒæ—¶ï¼Œä¸ºäº†ç²¾ç¡®é‡æ„å›¾åƒè°±ï¼Œè¿˜å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œç«¯åˆ°ç«¯ä¼˜åŒ–çš„é¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰ä¸Šéƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ— é…å¯¹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCTAå›¾åƒä¸­ï¼Œæ‰«æç‡å—é™å¯¼è‡´è§†é‡ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>è¾ƒå¤§çš„è§†é‡å¯èƒ½æ­ç¤ºæ›´å¤šçš„æ—ä¸­å¿ƒè§†ç½‘è†œè¡€ç®¡ç—…å˜ï¼Œä½†å…¶åˆ†è¾¨ç‡è¾ƒä½ã€‚</li>
<li>ä»¥å¾€çš„ç ”ç©¶åœ¨è®­ç»ƒæ—¶ä¾èµ–é…å¯¹æ•°æ®ï¼Œä½†çœŸå®ä¸–ç•Œåº”ç”¨ä¸­é…å¯¹å›¾åƒçš„æ”¶é›†æŒ‘æˆ˜é‡é‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•å¤„ç†OCTAå›¾åƒã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨é¢‘ç‡ä¿¡æ¯æ¥ä¿ç•™æ¯›ç»†è¡€ç®¡ç»†èŠ‚ï¼Œé€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨çªå‡ºé«˜é¢‘ç²¾ç»†æ¯›ç»†è¡€ç®¡ã€‚</li>
<li>å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥è¿›è¡Œç²¾ç¡®å›¾åƒé‡æ„å’Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>




<h2 id="MC-NeRF-Multi-Camera-Neural-Radiance-Fields-for-Multi-Camera-Image-Acquisition-Systems"><a href="#MC-NeRF-Multi-Camera-Neural-Radiance-Fields-for-Multi-Camera-Image-Acquisition-Systems" class="headerlink" title="MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image   Acquisition Systems"></a>MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image   Acquisition Systems</h2><p><strong>Authors:Yu Gao, Lutong Su, Hao Liang, Yufeng Yue, Yi Yang, Mengyin Fu</strong></p>
<p>Neural Radiance Fields (NeRF) use multi-view images for 3D scene representation, demonstrating remarkable performance. As one of the primary sources of multi-view images, multi-camera systems encounter challenges such as varying intrinsic parameters and frequent pose changes. Most previous NeRF-based methods assume a unique camera and rarely consider multi-camera scenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic parameters still remain susceptible to suboptimal solutions when these parameters are poor initialized. In this paper, we propose MC-NeRF, a method that enables joint optimization of both intrinsic and extrinsic parameters alongside NeRF. The method also supports each image corresponding to independent camera parameters. First, we tackle coupling issue and the degenerate case that arise from the joint optimization between intrinsic and extrinsic parameters. Second, based on the proposed solutions, we introduce an efficient calibration image acquisition scheme for multi-camera systems, including the design of calibration object. Finally, we present an end-to-end network with training sequence that enables the estimation of intrinsic and extrinsic parameters, along with the rendering network. Furthermore, recognizing that most existing datasets are designed for a unique camera, we construct a real multi-camera image acquisition system and create a corresponding new dataset, which includes both simulated data and real-world captured images. Experiments confirm the effectiveness of our method when each image corresponds to different camera parameters. Specifically, we use multi-cameras, each with different intrinsic and extrinsic parameters in real-world system, to achieve 3D scene representation without providing initial poses. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åˆ©ç”¨å¤šè§†è§’å›¾åƒè¿›è¡Œ3Dåœºæ™¯è¡¨ç¤ºï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä½œä¸ºå¤šè§†è§’å›¾åƒçš„ä¸»è¦æ¥æºä¹‹ä¸€ï¼Œå¤šç›¸æœºç³»ç»Ÿé¢ä¸´ç€å†…åœ¨å‚æ•°å˜åŒ–ä»¥åŠå§¿æ€é¢‘ç¹å˜åŒ–ç­‰æŒ‘æˆ˜ã€‚ä¹‹å‰çš„å¤§å¤šæ•°åŸºäºNeRFçš„æ–¹æ³•éƒ½å‡è®¾æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„ç›¸æœºï¼Œå¹¶ä¸”å¾ˆå°‘è€ƒè™‘å¤šç›¸æœºåœºæ™¯ã€‚æ­¤å¤–ï¼Œå³ä½¿æœ‰äº›NeRFæ–¹æ³•èƒ½å¤Ÿä¼˜åŒ–å†…åœ¨å’Œå¤–åœ¨å‚æ•°ï¼Œä½†åœ¨è¿™äº›å‚æ•°åˆå§‹åŒ–ä¸ä½³æ—¶ï¼Œä»ç„¶å®¹æ˜“é™·å…¥æ¬¡ä¼˜è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MC-NeRFæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–å†…åœ¨å’Œå¤–åœ¨å‚æ•°ä»¥åŠä¸NeRFç›¸å…³çš„å‚æ•°ã€‚è¯¥æ–¹æ³•è¿˜æ”¯æŒæ¯å¼ å›¾åƒå¯¹åº”ç‹¬ç«‹çš„ç›¸æœºå‚æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨å†…åœ¨å’Œå¤–åœ¨å‚æ•°è”åˆä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°çš„è€¦åˆé—®é¢˜å’Œé€€åŒ–æƒ…å†µã€‚å…¶æ¬¡ï¼ŒåŸºäºè¿™äº›è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬ä¸ºå¤šç›¸æœºç³»ç»Ÿå¼•å…¥äº†æœ‰æ•ˆçš„æ ¡å‡†å›¾åƒé‡‡é›†æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ ¡å‡†å¯¹è±¡çš„è®¾è®¡ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç½‘ç»œè®­ç»ƒåºåˆ—ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿä¼°è®¡å†…åœ¨å’Œå¤–åœ¨å‚æ•°ï¼Œä»¥åŠæ¸²æŸ“ç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°å¤§å¤šæ•°ç°æœ‰æ•°æ®é›†éƒ½æ˜¯ä¸ºå•ä¸€ç›¸æœºè®¾è®¡çš„ï¼Œå› æ­¤æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçœŸæ­£çš„å¤šç›¸æœºå›¾åƒé‡‡é›†ç³»ç»Ÿï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªç›¸åº”çš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•è·çš„å›¾åƒã€‚å®éªŒè¯å®ï¼Œå½“æ¯å¼ å›¾åƒå¯¹åº”ä¸åŒçš„ç›¸æœºå‚æ•°æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éå¸¸æœ‰æ•ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å…·æœ‰ä¸åŒå†…åœ¨å’Œå¤–åœ¨å‚æ•°çš„å¤šç›¸æœºçœŸå®ç³»ç»Ÿï¼Œåœ¨ä¸æä¾›åˆå§‹å§¿æ€çš„æƒ…å†µä¸‹å®ç°3Dåœºæ™¯è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07846v4">PDF</a> This manuscript is currently under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†MC-NeRFæ–¹æ³•ï¼Œå®ç°äº†å¯¹NeRFä¸­çš„å†…åœ¨å‚æ•°å’Œå¤–åœ¨å‚æ•°çš„è”åˆä¼˜åŒ–ï¼Œæ”¯æŒæ¯å¼ å›¾åƒå¯¹åº”ç‹¬ç«‹çš„ç›¸æœºå‚æ•°ã€‚è§£å†³äº†è”åˆä¼˜åŒ–ä¸­çš„è€¦åˆé—®é¢˜å’Œé€€åŒ–æƒ…å†µï¼Œå¼•å…¥é«˜æ•ˆçš„å¤šç›¸æœºç³»ç»Ÿæ ¡å‡†å›¾åƒé‡‡é›†æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ ¡å‡†å¯¹è±¡çš„è®¾è®¡ã€‚åŒæ—¶æ„å»ºäº†çœŸå®çš„å¤šç›¸æœºå›¾åƒé‡‡é›†ç³»ç»Ÿå¹¶åˆ›å»ºäº†ç›¸åº”çš„æ–°æ•°æ®é›†ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œç³»ç»Ÿä¸­æ¯å¼ å›¾åƒå¯¹åº”ä¸åŒç›¸æœºå‚æ•°æ—¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MC-NeRFå®ç°äº†å¯¹NeRFä¸­çš„å†…åœ¨å‚æ•°å’Œå¤–åœ¨å‚æ•°çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>æ”¯æŒæ¯å¼ å›¾åƒå¯¹åº”ç‹¬ç«‹çš„ç›¸æœºå‚æ•°ã€‚</li>
<li>è§£å†³äº†è”åˆä¼˜åŒ–ä¸­å‡ºç°çš„è€¦åˆé—®é¢˜å’Œé€€åŒ–æƒ…å†µã€‚</li>
<li>å¼•å…¥é«˜æ•ˆçš„å¤šç›¸æœºç³»ç»Ÿæ ¡å‡†å›¾åƒé‡‡é›†æ–¹æ¡ˆã€‚</li>
<li>æ„å»ºçœŸå®çš„å¤šç›¸æœºå›¾åƒé‡‡é›†ç³»ç»Ÿå¹¶åˆ›å»ºæ–°çš„æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-101df8512aee8de8d27c2059897a721f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-37c79cf2eacdceafe52fb024a2637ea6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-48274d60983d14e283021d015f2583f7.png" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-688c42b03634426b1c3218e979797176.jpg" align="middle">
</details>




<h2 id="ChromaDistill-Colorizing-Monochrome-Radiance-Fields-with-Knowledge-Distillation"><a href="#ChromaDistill-Colorizing-Monochrome-Radiance-Fields-with-Knowledge-Distillation" class="headerlink" title="ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge   Distillation"></a>ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge   Distillation</h2><p><strong>Authors:Ankit Dhiman, R Srinath, Srinjay Sarkar, Lokesh R Boregowda, R Venkatesh Babu</strong></p>
<p>Colorization is a well-explored problem in the domains of image and video processing. However, extending colorization to 3D scenes presents significant challenges. Recent Neural Radiance Field (NeRF) and Gaussian-Splatting(3DGS) methods enable high-quality novel-view synthesis for multi-view images. However, the question arises: How can we colorize these 3D representations? This work presents a method for synthesizing colorized novel views from input grayscale multi-view images. Using image or video colorization methods to colorize novel views from these 3D representations naively will yield output with severe inconsistencies. We introduce a novel method to use powerful image colorization models for colorizing 3D representations. We propose a distillation-based method that transfers color from these networks trained on natural images to the target 3D representation. Notably, this strategy does not add any additional weights or computational overhead to the original representation during inference. Extensive experiments demonstrate that our method produces high-quality colorized views for indoor and outdoor scenes, showcasing significant cross-view consistency advantages over baseline approaches. Our method is agnostic to the underlying 3D representation and easily generalizable to NeRF and 3DGS methods. Further, we validate the efficacy of our approach in several diverse applications: 1.) Infra-Red (IR) multi-view images and 2.) Legacy grayscale multi-view image sequences. Project Webpage: <a target="_blank" rel="noopener" href="https://val.cds.iisc.ac.in/chroma-distill.github.io/">https://val.cds.iisc.ac.in/chroma-distill.github.io/</a> </p>
<blockquote>
<p>ç€è‰²æ˜¯å›¾åƒå’Œè§†é¢‘å¤„ç†é¢†åŸŸå·²ç»å¾—åˆ°å¾ˆå¥½ç ”ç©¶çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå°†ç€è‰²æ‰©å±•åˆ°3Dåœºæ™¯é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰æ–¹æ³•èƒ½å¤Ÿå®ç°å¤šè§†è§’å›¾åƒçš„é«˜è´¨é‡æ–°å‹è§†å›¾åˆæˆã€‚ä½†é—®é¢˜æ˜¯ï¼šæˆ‘ä»¬å¦‚ä½•å¯¹è¿™äº›3Dè¡¨ç¤ºè¿›è¡Œç€è‰²ï¼Ÿè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§ä»è¾“å…¥ç°åº¦å¤šè§†è§’å›¾åƒåˆæˆå½©è‰²æ–°å‹è§†å›¾çš„æ–¹æ³•ã€‚ç›´æ¥ä½¿ç”¨å›¾åƒæˆ–è§†é¢‘ç€è‰²æ–¹æ³•æ¥å¯¹è¿™äº›3Dè¡¨ç¤ºè¿›è¡Œç€è‰²ä¼šå¯¼è‡´è¾“å‡ºå­˜åœ¨ä¸¥é‡çš„ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨å¼ºå¤§çš„å›¾åƒç€è‰²æ¨¡å‹å¯¹3Dè¡¨ç¤ºè¿›è¡Œç€è‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œå°†ä»è‡ªç„¶å›¾åƒä¸Šè®­ç»ƒçš„ç½‘ç»œçš„é¢œè‰²è½¬ç§»åˆ°ç›®æ ‡3Dè¡¨ç¤ºä¸Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ­¤ç­–ç•¥åœ¨æ¨ç†æœŸé—´ä¸ä¼šç»™åŸå§‹è¡¨ç¤ºå¢åŠ ä»»ä½•é¢å¤–çš„æƒé‡æˆ–è®¡ç®—å¼€é”€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºå®¤å†…å’Œå®¤å¤–åœºæ™¯ç”Ÿæˆäº†é«˜è´¨é‡å½©è‰²è§†å›¾ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè·¨è§†å›¾ä¸€è‡´æ€§å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹åº•å±‚3Dè¡¨ç¤ºå…·æœ‰ä¸­ç«‹æ€§ï¼Œå¯è½»æ¾æ¨å¹¿åˆ°NeRFå’Œ3DGSæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å‡ ä¸ªä¸åŒçš„åº”ç”¨éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼š1ï¼‰çº¢å¤–ï¼ˆIRï¼‰å¤šè§†è§’å›¾åƒå’Œ2ï¼‰é—ç•™ç°åº¦å¤šè§†è§’å›¾åƒåºåˆ—ã€‚é¡¹ç›®ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="https://val.cds.iisc.ac.in/chroma-distill.github.io/">https://val.cds.iisc.ac.in/chroma-distill.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07668v2">PDF</a> WACV 2025, AI3DCC @ ICCV 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å°†è‰²å½©åŒ–æŠ€æœ¯åº”ç”¨äºä¸‰ç»´åœºæ™¯è¡¨ç°çš„é—®é¢˜ã€‚æ–‡ç« ä»‹ç»äº†å¦‚ä½•å°†å›¾åƒæˆ–è§†é¢‘çš„è‰²å½©åŒ–æ–¹æ³•åº”ç”¨äºä»ä¸‰ç»´è¡¨ç¤ºç”Ÿæˆçš„æ–°å‹è§†è§’å›¾åƒä¸Šï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œå°†è®­ç»ƒäºè‡ªç„¶å›¾åƒçš„ç½‘ç»œä¸­çš„é¢œè‰²è½¬ç§»åˆ°ç›®æ ‡ä¸‰ç»´è¡¨ç¤ºä¸Šï¼Œå®ç°äº†é«˜è´¨é‡çš„é¢œè‰²åŒ–è§†å›¾åˆæˆã€‚æ­¤æ–¹æ³•å¯¹äºå®¤å†…å’Œå®¤å¤–åœºæ™¯å‡æœ‰æ•ˆï¼Œç›¸æ¯”åŸºå‡†æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„è·¨è§†å›¾ä¸€è‡´æ€§ä¼˜åŠ¿ï¼Œå¹¶ä¸”å¯è½»æ¾æ¨å¹¿åˆ°NeRFå’Œ3DGSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ¢è®¨äº†å°†è‰²å½©åŒ–æ‰©å±•åˆ°ä¸‰ç»´åœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œå°†å›¾åƒé¢œè‰²ä»è‡ªç„¶å›¾åƒç½‘ç»œè½¬ç§»åˆ°ä¸‰ç»´è¡¨ç¤ºã€‚</li>
<li>æ–¹æ³•å®ç°äº†é«˜è´¨é‡çš„é¢œè‰²åŒ–è§†å›¾åˆæˆï¼Œé€‚ç”¨äºå®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚</li>
<li>ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„è·¨è§†å›¾ä¸€è‡´æ€§ä¼˜åŠ¿ã€‚</li>
<li>æ–¹æ³•å¯¹äºä¸åŒçš„ä¸‰ç»´è¡¨ç¤ºæ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯è½»æ¾æ¨å¹¿è‡³NeRFå’Œ3DGSã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§åº”ç”¨ä¸­å¾—åˆ°éªŒè¯ï¼ŒåŒ…æ‹¬çº¢å¤–å¤šè§†è§’å›¾åƒå’Œé—ç•™ç°åº¦å¤šè§†è§’å›¾åƒåºåˆ—ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f55c89070e6b20b7fa1fb91b960a64d1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-68a542cf75dab4c514d896440e5f6784.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0fee3ef8eb02763ac0e262c36d2324bb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d53fe9bb1b08659516504f156fc15c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-994a03b85c48a19dcb7fa3c8c745338a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-84c6da1255580971a1b76ca9b966593b.jpg" align="middle">
</details>




<h2 id="Diverse-Similarity-Encoder-for-Deep-GAN-Inversion"><a href="#Diverse-Similarity-Encoder-for-Deep-GAN-Inversion" class="headerlink" title="Diverse Similarity Encoder for Deep GAN Inversion"></a>Diverse Similarity Encoder for Deep GAN Inversion</h2><p><strong>Authors:Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</strong></p>
<p>Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN. </p>
<blockquote>
<p>å½“å‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å·²ç»å¯ä»¥åˆæˆé«˜è´¨é‡ï¼ˆHQï¼‰å›¾åƒï¼Œå› æ­¤ä½¿ç”¨GANså­¦ä¹ è¡¨ç¤ºæ˜¯æœ‰åˆ©çš„ã€‚GANåæ¼”æ˜¯æ–°å…´æ–¹æ³•ä¹‹ä¸€ï¼Œç ”ç©¶å¦‚ä½•å°†å›¾åƒåæ¼”åˆ°æ½œåœ¨ç©ºé—´ã€‚ç°æœ‰çš„GANç¼–ç å™¨å¯ä»¥åœ¨StyleGANä¸Šè¿›è¡Œå›¾åƒåæ¼”ï¼Œä½†ä¸èƒ½é€‚åº”å…¶ä»–æ·±åº¦GANã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ç¼–ç å™¨ï¼Œåä¸ºå·®å¼‚ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰ï¼Œå¯ä»¥æ‰©å±•åˆ°å„ç§æœ€å…ˆè¿›çš„GANã€‚DSEä½¿GANèƒ½å¤Ÿä»HQå›¾åƒé‡å»ºæ›´é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œæ— è®ºæ˜¯åˆæˆçš„è¿˜æ˜¯çœŸå®çš„å›¾åƒã€‚DSEå…·æœ‰ç»Ÿä¸€çš„å·ç§¯å—ï¼Œå¯ä»¥å¾ˆå¥½åœ°é€‚åº”ä¸»æµæ·±åº¦GANï¼Œä¾‹å¦‚PGGANã€StyleGANå’ŒBigGANã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.10201v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰çš„æ–°å‹è‡ªé€‚åº”GANç¼–ç å™¨æ–¹æ³•ï¼Œå¯åº”ç”¨äºå¤šç§å…ˆè¿›çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚é€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§ï¼ŒDSEèƒ½å¤Ÿä½¿GANsé‡æ„å‡ºæ›´é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œæ— è®ºæ˜¯åˆæˆå›¾åƒè¿˜æ˜¯çœŸå®å›¾åƒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”ä¸»æµæ·±åº¦GANsï¼Œå¦‚PGGANã€StyleGANå’ŒBigGANç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å¯ä»¥åˆæˆé«˜è´¨é‡å›¾åƒï¼Œå› æ­¤ä½¿ç”¨GANsè¿›è¡Œè¡¨ç¤ºå­¦ä¹ å¤‡å—é’çã€‚</li>
<li>GANåæ¼”æ˜¯æ–°å…´æ–¹æ³•ä¹‹ä¸€ï¼Œç ”ç©¶å¦‚ä½•å°†å›¾åƒåæ¼”åˆ°æ½œåœ¨ç©ºé—´ã€‚</li>
<li>ç°æœ‰GANç¼–ç å™¨å¯ä»¥åœ¨StyleGANä¸Šè¿›è¡Œå›¾åƒåæ¼”ï¼Œä½†ä¸èƒ½é€‚åº”å…¶ä»–æ·±åº¦GANsã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºå¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰çš„æ–°å‹è‡ªé€‚åº”GANç¼–ç å™¨æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DSEé€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§è¿›è¡Œè®¾è®¡ã€‚</li>
<li>DSEèƒ½å¤Ÿåº”ç”¨äºå¤šç§å…ˆè¿›çš„GANsï¼Œå¦‚PGGANã€StyleGANå’ŒBigGANç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2c0940fc7f9e47f7ef709ff59469c9b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26229e3100c8c64811dfe13fae729ff8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-991864f280fa90ecd2fc3e0b5b8d9de9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-849a0bfe560207f45ea80d5eb85b9501.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/3DGS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_3DGS/2412.03844v2/page_0_0.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  SLGaussian Fast Language Gaussian Splatting in Sparse Views
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
