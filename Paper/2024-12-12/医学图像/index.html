<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    27.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    114 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p>
<p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the modelâ€™s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models. </p>
<blockquote>
<p>å›¾åƒåå°„æ¶ˆé™¤å¯¹äºæ¢å¤å›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚æ‰­æ›²çš„å›¾åƒä¼šå¯¹ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å•å¹…å›¾åƒè¿›è¡Œå›¾åƒåå°„æ¶ˆé™¤çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æ²¡æœ‰ä¸“æ³¨äºæ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§å¯ä»¥æ¨å¹¿åˆ°å›¾åƒåˆ°å›¾åƒé—®é¢˜çš„æ–°è®­ç»ƒæŠ€æœ¯ï¼Œè¾“å…¥å’Œè¾“å‡ºåœ¨æ€§è´¨ä¸Šç›¸ä¼¼ã€‚è¿™ç§æŠ€æœ¯ä½“ç°åœ¨æˆ‘ä»¬çš„å¤šæ­¥æŸå¤±æœºåˆ¶ä¸­ï¼Œè¯¥æœºåˆ¶åœ¨å»é™¤åå°„çš„ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨Pix2Pix GANåˆæˆé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹å­¦ä¹ æ›´å¥½åå°„å»é™¤æ¨¡å¼çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„èŒƒå›´æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å…¶ç¼ºä¹åå°„æ·±åº¦ä¼°è®¡çš„å±æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SIR^2åŸºå‡†å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåå°„å»é™¤æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒå³å¯å®Œæˆã€‚ç ”ç©¶é‡ç‚¹ä¸åœ¨äºæ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§å¯æ¨å¹¿è‡³ç±»ä¼¼å›¾åƒåˆ°å›¾åƒé—®é¢˜çš„æ–°è®­ç»ƒæŠ€æœ¯ã€‚é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†åå°„å»é™¤ä»»åŠ¡ã€‚åŒæ—¶ï¼Œåˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨äº†ä»ç¯å¢ƒå›¾åƒæ·±åº¦ä¼°è®¡ä¸­æå–çš„èŒƒå›´æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å…¶ç¼ºä¹åå°„æ·±åº¦ä¼°è®¡çš„ç‰¹æ€§ã€‚è¯¥æ–¹æ³•åœ¨SIR^2åŸºå‡†æµ‹è¯•å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒåå°„å»é™¤æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒå³å¯å®Œæˆã€‚</li>
<li>ç ”ç©¶çš„é‡ç‚¹ä¸åœ¨äºæ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯æ¨å¹¿è‡³ç±»ä¼¼çš„å›¾åƒåˆ°å›¾åƒé—®é¢˜ã€‚</li>
<li>é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶æœ‰æ•ˆè§£å†³äº†åå°„å»é™¤ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œç”¨äºè§£å†³åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨äº†ä»ç¯å¢ƒå›¾åƒæ·±åº¦ä¼°è®¡ä¸­æå–çš„èŒƒå›´æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨äº†åå°„ç¼ºä¹æ·±åº¦ä¼°è®¡çš„ç‰¹æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90610239ae13c51e1614f127958f2a81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bde3fb0cbe989335b0d5bb7a4d46e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b852c3cdd4ace9f1729fdab0540434.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab1d495bb0788c654b34111e35dbd40d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f69c1d9eb0ee3bcc36085da2e8bd9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9175c1aae7fa57889e8641ff09cdbe6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f9871afd6e42b8e1cadb0f3e2faa7fc.jpg" align="middle">
</details>




<h2 id="Annotation-Efficient-Task-Guidance-for-Medical-Segment-Anything"><a href="#Annotation-Efficient-Task-Guidance-for-Medical-Segment-Anything" class="headerlink" title="Annotation-Efficient Task Guidance for Medical Segment Anything"></a>Annotation-Efficient Task Guidance for Medical Segment Anything</h2><p><strong>Authors:Tyler Ward, Abdullah-Al-Zubaer Imran</strong></p>
<p>Medical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose SAM-Mix, a novel multitask learning framework for medical image segmentation that uses class activation maps produced by an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the SAM framework. Experimental evaluations on the public LiTS dataset confirm the effectiveness of SAM-Mix for simultaneous classification and segmentation of the liver from abdominal computed tomography (CT) scans. When trained for 90% fewer epochs on only 50 labeled 2D slices, representing just 0.04% of the available labeled training data, SAM-Mix achieves a Dice improvement of 5.1% over the best baseline model. The generalization results for SAM-Mix are even more impressive, with the same model configuration yielding a 25.4% Dice improvement on a cross-domain segmentation task. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tbwa233/SAM-Mix">https://github.com/tbwa233/SAM-Mix</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“ç€è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ä¼ ç»Ÿçš„å…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œé€šå¸¸é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨è·å¾—ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œè¿˜å®¹æ˜“å‡ºé”™ã€‚è¿™çªæ˜¾äº†å¯¹å‡†ç¡®ã€è‡ªåŠ¨å’Œæ ‡æ³¨æ•ˆç‡é«˜çš„è®­ç»ƒè¿™äº›æ¨¡å‹æ–¹æ³•çš„éœ€è¦ã€‚æˆ‘ä»¬æå‡ºäº†SAM-Mixï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå®ƒä½¿ç”¨è¾…åŠ©åˆ†ç±»å™¨ç”Ÿæˆçš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ï¼Œè¯¥åˆ†æ”¯åŸºäºSAMæ¡†æ¶ã€‚åœ¨å…¬å…±LiTSæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯å®äº†SAM-Mixåœ¨è…¹éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æ(CT)çš„è‚è„åˆ†ç±»å’Œåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚å½“ä»…åœ¨50ä¸ªæ ‡è®°çš„2Dåˆ‡ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆåªå å¯ç”¨æ ‡è®°è®­ç»ƒæ•°æ®çš„0.04%ï¼‰ï¼Œå¹¶ä¸”è®­ç»ƒå‘¨æœŸå‡å°‘90%æ—¶ï¼ŒSAM-Mixç›¸å¯¹äºæœ€ä½³åŸºçº¿æ¨¡å‹å®ç°äº†5.1%çš„Diceæ”¹è¿›ã€‚SAM-Mixçš„æ³›åŒ–ç»“æœæ›´ä»¤äººå°è±¡æ·±åˆ»ï¼Œç›¸åŒçš„æ¨¡å‹é…ç½®åœ¨è·¨åŸŸåˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†25.4%çš„Diceæ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tbwa233/SAM-Mix%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tbwa233/SAM-Mixè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08575v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ä¼ ç»Ÿå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œä¸”æ˜“å‡ºé”™ã€‚å› æ­¤ï¼Œéœ€è¦å‡†ç¡®ã€è‡ªåŠ¨ã€æ ‡æ³¨æ•ˆç‡é«˜çš„è®­ç»ƒè¿™äº›æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºSAM-Mixï¼Œä¸€ç§åŸºäºè¾…åŠ©åˆ†ç±»å™¨äº§ç”Ÿçš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯é¢„æµ‹çš„æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ã€‚åœ¨å…¬å…±LiTSæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯å®ï¼ŒSAM-Mixåœ¨è‚è„çš„è…¹éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°æœ‰æ•ˆã€‚åœ¨ä»…ä½¿ç”¨50ä¸ªæ ‡è®°çš„äºŒç»´åˆ‡ç‰‡ï¼ˆä»…å å¯ç”¨æ ‡è®°è®­ç»ƒæ•°æ®çš„0.04%ï¼‰è¿›è¡Œ90%æ›´å°‘å‘¨æœŸçš„è®­ç»ƒæ—¶ï¼ŒSAM-Mixç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹å®ç°äº†5.1%çš„Diceç³»æ•°æå‡ã€‚SAM-Mixçš„æ³›åŒ–ç»“æœæ›´ä»¤äººå°è±¡æ·±åˆ»ï¼ŒåŒä¸€æ¨¡å‹é…ç½®åœ¨è·¨åŸŸåˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†25.4%çš„Diceç³»æ•°æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨æˆåƒå·¥ä½œæµä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå½±å“åŸºäºå›¾åƒçš„å¤šé¡¹å†³ç­–ã€‚</li>
<li>ä¼ ç»Ÿå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡çš„æ‰‹åŠ¨æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼Œè¿™æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>SAM-Mixæ˜¯ä¸€ç§æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨è¾…åŠ©åˆ†ç±»å™¨äº§ç”Ÿçš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼é¢„æµ‹ã€‚</li>
<li>åœ¨LiTSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-Mixåœ¨è‚è„CTæ‰«æçš„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨ä½¿ç”¨æå°‘é‡æ ‡è®°æ•°æ®è®­ç»ƒæ—¶ï¼ŒSAM-Mixå®ç°äº†æ˜¾è‘—çš„Diceç³»æ•°æå‡ã€‚</li>
<li>SAM-Mixå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨è·¨åŸŸåˆ†å‰²ä»»åŠ¡ä¸Šä¹Ÿæœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f8963e21f8368ebc29d5d6ad7e7e7da6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34460691ad28ad121640fa4140bf2c64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0711e35a0527a96a6282f002fa604f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2174ea02c6a5fae098acb84e7146db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746dd92df2e44efa0bdbac49c75d306e.jpg" align="middle">
</details>




<h2 id="ConDSeg-A-General-Medical-Image-Segmentation-Framework-via-Contrast-Driven-Feature-Enhancement"><a href="#ConDSeg-A-General-Medical-Image-Segmentation-Framework-via-Contrast-Driven-Feature-Enhancement" class="headerlink" title="ConDSeg: A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement"></a>ConDSeg: A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement</h2><p><strong>Authors:Mengqi Lei, Haochen Wu, Xinhua Lv, Xin Wang</strong></p>
<p>Medical image segmentation plays an important role in clinical decision making, treatment planning, and disease tracking. However, it still faces two major challenges. On the one hand, there is often a &#96;&#96;soft boundaryâ€™â€™ between foreground and background in medical images, with poor illumination and low contrast further reducing the distinguishability of foreground and background within the image. On the other hand, co-occurrence phenomena are widespread in medical images, and learning these features is misleading to the modelâ€™s judgment. To address these challenges, we propose a general framework called Contrast-Driven Medical Image Segmentation (ConDSeg). First, we develop a contrastive training strategy called Consistency Reinforcement. It is designed to improve the encoderâ€™s robustness in various illumination and contrast scenarios, enabling the model to extract high-quality features even in adverse environments. Second, we introduce a Semantic Information Decoupling module, which is able to decouple features from the encoder into foreground, background, and uncertainty regions, gradually acquiring the ability to reduce uncertainty during training. The Contrast-Driven Feature Aggregation module then contrasts the foreground and background features to guide multi-level feature fusion and key feature enhancement, further distinguishing the entities to be segmented. We also propose a Size-Aware Decoder to solve the scale singularity of the decoder. It accurately locate entities of different sizes in the image, thus avoiding erroneous learning of co-occurrence features. Extensive experiments on five medical image datasets across three scenarios demonstrate the state-of-the-art performance of our method, proving its advanced nature and general applicability to various medical image segmentation scenarios. Our released code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg%7D">https://github.com/Mengqi-Lei/ConDSeg}</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå†³ç­–ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…è¿½è¸ªä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼ŒåŒ»å­¦å›¾åƒä¸­å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´é€šå¸¸å­˜åœ¨ä¸€ä¸ªâ€œè½¯è¾¹ç•Œâ€ï¼Œç…§æ˜ä¸è‰¯å’Œä½å¯¹æ¯”åº¦è¿›ä¸€æ­¥é™ä½äº†å›¾åƒä¸­å‰æ™¯å’ŒèƒŒæ™¯çš„è¾¨åˆ«èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŒ»å­¦å›¾åƒä¸­æ™®éå­˜åœ¨å…±ç°ç°è±¡ï¼Œå­¦ä¹ è¿™äº›ç‰¹å¾ä¼šå¯¹æ¨¡å‹çš„åˆ¤æ–­äº§ç”Ÿè¯¯å¯¼ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œç§°ä¸ºContrast-Driven Medical Image Segmentationï¼ˆConDSegï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºConsistency Reinforcementçš„å¯¹æ¯”è®­ç»ƒç­–ç•¥ã€‚å®ƒæ˜¯ä¸ºäº†æé«˜ç¼–ç å™¨åœ¨å„ç§å…‰ç…§å’Œå¯¹æ¯”åº¦åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œä½¿æ¨¡å‹å³ä½¿åœ¨æ¶åŠ£ç¯å¢ƒä¸­ä¹Ÿèƒ½æå–é«˜è´¨é‡çš„ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªSemantic Information Decouplingæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°†ç¼–ç å™¨çš„ç‰¹å¾è§£è€¦ä¸ºå‰æ™¯ã€èƒŒæ™¯å’Œä¸ç¡®å®šåŒºåŸŸï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸è·å¾—å‡å°‘ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚ç„¶åï¼ŒContrast-Driven Feature Aggregationæ¨¡å—å¯¹æ¯”å‰æ™¯å’ŒèƒŒæ™¯ç‰¹å¾ï¼Œå¼•å¯¼å¤šçº§åˆ«ç‰¹å¾èåˆå’Œå…³é”®ç‰¹å¾å¢å¼ºï¼Œè¿›ä¸€æ­¥åŒºåˆ†è¦åˆ†å‰²çš„å®ä½“ã€‚ä¸ºäº†è§£å†³è§£ç å™¨çš„å°ºåº¦å•ä¸€æ€§é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†Size-Aware Decoderã€‚å®ƒèƒ½å¤Ÿå‡†ç¡®åœ°å®šä½å›¾åƒä¸­ä¸åŒå¤§å°çš„å®ä½“ï¼Œä»è€Œé¿å…å¯¹å…±ç°ç‰¹å¾çš„é”™è¯¯å­¦ä¹ ã€‚åœ¨ä¸‰ç§åœºæ™¯ä¸‹çš„äº”ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ä¸­çš„å…ˆè¿›æ€§å’Œé€šç”¨é€‚ç”¨æ€§ã€‚æˆ‘ä»¬å‘å¸ƒçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Mengqi-Lei/ConDSegè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08345v1">PDF</a> This paper has been accepted by AAAI-2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå†³ç­–ã€æ²»ç–—è®¡åˆ’ä»¥åŠç–¾ç—…è¿½è¸ªä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç„¶è€Œä»å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸€æ˜¯åŒ»å­¦å›¾åƒä¸­å‰æ™¯ä¸èƒŒæ™¯é—´å­˜åœ¨â€œè½¯è¾¹ç•Œâ€ï¼ŒåŠ ä¹‹ç…§æ˜ä¸è‰¯ã€å¯¹æ¯”åº¦ä½ï¼Œé™ä½äº†å‰æ™¯ä¸èƒŒæ™¯çš„è¾¨è¯†åº¦ã€‚äºŒæ˜¯åŒ»å­¦å›¾åƒä¸­æ™®éå­˜åœ¨å…±ç°ç°è±¡ï¼Œå¯¹æ¨¡å‹åˆ¤æ–­é€ æˆè¯¯å¯¼ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§é€šç”¨æ¡†æ¶â€”â€”å¯¹æ¯”é©±åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆConDSegï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œåä¸ºä¸€è‡´æ€§å¼ºåŒ–ï¼ˆConsistency Reinforcementï¼‰ï¼Œæ—¨åœ¨æé«˜ç¼–ç å™¨åœ¨å„ç§ç…§æ˜å’Œå¯¹æ¯”åº¦åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œä½¿æ¨¡å‹èƒ½åœ¨æ¶åŠ£ç¯å¢ƒä¸­æå–é«˜è´¨é‡ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥è¯­ä¹‰ä¿¡æ¯è§£è€¦æ¨¡å—ï¼Œèƒ½å¤Ÿè§£è€¦ç¼–ç å™¨ä¸­çš„ç‰¹å¾ä¸ºå‰æ™¯ã€èƒŒæ™¯å’Œä¸ç¡®å®šæ€§åŒºåŸŸï¼Œé€æ­¥åœ¨è®­ç»ƒä¸­å‡å°‘ä¸ç¡®å®šæ€§ã€‚å¯¹æ¯”é©±åŠ¨ç‰¹å¾èšåˆæ¨¡å—åˆ™å¯¹æ¯”å‰æ™¯å’ŒèƒŒæ™¯ç‰¹å¾ï¼Œå¼•å¯¼å¤šçº§åˆ«ç‰¹å¾èåˆå’Œå…³é”®ç‰¹å¾å¢å¼ºï¼Œè¿›ä¸€æ­¥åŒºåˆ†è¦åˆ†å‰²çš„å®ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å°ºå¯¸æ„ŸçŸ¥è§£ç å™¨ï¼Œä»¥è§£å†³è§£ç å™¨å°ºåº¦å•ä¸€æ€§é—®é¢˜ã€‚å®ƒèƒ½å‡†ç¡®å®šä½å›¾åƒä¸­ä¸åŒå°ºå¯¸çš„å®ä½“ï¼Œä»è€Œé¿å…å¯¹å…±ç°ç‰¹å¾çš„é”™è¯¯å­¦ä¹ ã€‚åœ¨äº”ç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å…ˆè¿›æ€§ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ä¸­çš„é€šç”¨é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨[<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg]%E3%80%82">https://github.com/Mengqi-Lei/ConDSeg]ã€‚</a></p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<p>ä¸€ã€åŒ»å­¦å›¾åƒåˆ†å‰²çš„é‡è¦æ€§åŠå…¶ä¸¤å¤§æŒ‘æˆ˜ï¼šå‰æ™¯ä¸èƒŒæ™¯è¾¨è¯†å›°éš¾åŠå…±ç°ç°è±¡çš„å½±å“ã€‚<br>äºŒã€æå‡ºçš„é€šç”¨æ¡†æ¶â€”â€”å¯¹æ¯”é©±åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆConDSegï¼‰ã€‚<br>ä¸‰ã€ä¸€è‡´æ€§å¼ºåŒ–å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œæé«˜ç¼–ç å™¨åœ¨å„ç§ç…§æ˜å’Œå¯¹æ¯”åº¦åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚<br>å››ã€è¯­ä¹‰ä¿¡æ¯è§£è€¦æ¨¡å—ï¼Œèƒ½å¤ŸåŒºåˆ†å‰æ™¯ã€èƒŒæ™¯å’Œä¸ç¡®å®šæ€§åŒºåŸŸã€‚<br>äº”ã€å¯¹æ¯”é©±åŠ¨ç‰¹å¾èšåˆæ¨¡å—ï¼Œé€šè¿‡å¯¹æ¯”å‰æ™¯å’ŒèƒŒæ™¯ç‰¹å¾æ¥å¢å¼ºåˆ†å‰²æ•ˆæœã€‚<br>å…­ã€å°ºå¯¸æ„ŸçŸ¥è§£ç å™¨ï¼Œè§£å†³è§£ç å™¨å°ºåº¦å•ä¸€æ€§é—®é¢˜ï¼Œå‡†ç¡®å®šä½ä¸åŒå°ºå¯¸å®ä½“ã€‚</p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5e9c9ba596a235cbb8ff1626e493968.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1111958a25251b300892de6e87afbd7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0991ba2786d73f43a32d07228887ae29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db84c70ba0ddaa56e5773d07a884d1ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fd884bd31bd79ae7ebef0254bf653a8.jpg" align="middle">
</details>




<h2 id="Lightweight-Method-for-Interactive-3D-Medical-Image-Segmentation-with-Multi-Round-Result-Fusion"><a href="#Lightweight-Method-for-Interactive-3D-Medical-Image-Segmentation-with-Multi-Round-Result-Fusion" class="headerlink" title="Lightweight Method for Interactive 3D Medical Image Segmentation with   Multi-Round Result Fusion"></a>Lightweight Method for Interactive 3D Medical Image Segmentation with   Multi-Round Result Fusion</h2><p><strong>Authors:Bingzhi Shen, Lufan Chang, Siqi Chen, Shuxiang Guo, Hao Liu</strong></p>
<p>In medical imaging, precise annotation of lesions or organs is often required. However, 3D volumetric images typically consist of hundreds or thousands of slices, making the annotation process extremely time-consuming and laborious. Recently, the Segment Anything Model (SAM) has drawn widespread attention due to its remarkable zero-shot generalization capabilities in interactive segmentation. While researchers have explored adapting SAM for medical applications, such as using SAM adapters or constructing 3D SAM models, a key question remains: Can traditional CNN networks achieve the same strong zero-shot generalization in this task? In this paper, we propose the Lightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a novel approach demonstrating the potential of compact CNN-based models. Built upon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D prompt mask from user hints. This mask is then propagated through the 3D sequence via the Memory Module. To refine and stabilize results during interaction, the Multi-Round Result Fusion (MRF) Module selects and merges optimal masks from multiple rounds. Our extensive experiments across multiple datasets and modalities demonstrate LIM-Netâ€™s competitive performance. It exhibits stronger generalization to unseen data compared to SAM-based models, with competitive accuracy while requiring fewer interactions. Notably, LIM-Netâ€™s lightweight design offers significant advantages in deployment and inference efficiency, with low GPU memory consumption suitable for resource-constrained environments. These promising results demonstrate LIM-Net can serve as a strong baseline, complementing and contrasting with popular SAM models to further boost effective interactive medical image segmentation. The code will be released at \url{<a target="_blank" rel="noopener" href="https://github.com/goodtime-123/LIM-Net%7D">https://github.com/goodtime-123/LIM-Net}</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç²¾ç¡®åœ°æ ‡æ³¨ç—…å˜æˆ–å™¨å®˜é€šå¸¸æ˜¯éå¸¸å¿…è¦çš„ã€‚ç„¶è€Œï¼Œ3Dä½“ç§¯å›¾åƒé€šå¸¸ç”±æ•°ç™¾æˆ–æ•°åƒä¸ªåˆ‡ç‰‡ç»„æˆï¼Œä½¿å¾—æ ‡æ³¨è¿‡ç¨‹æä¸ºè€—æ—¶ä¸”ç¹çã€‚æœ€è¿‘ï¼Œç”±äºå…¶åœ¨äº¤äº’å¼åˆ†å‰²ä¸­çš„å‡ºè‰²é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡ç ”ç©¶è€…ä»¬å·²ç»æ¢ç´¢äº†å°†SAMç”¨äºåŒ»å­¦åº”ç”¨ï¼Œä¾‹å¦‚ä½¿ç”¨SAMé€‚é…å™¨æˆ–æ„å»º3D SAMæ¨¡å‹ï¼Œä½†ä¸€ä¸ªå…³é”®é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šä¼ ç»ŸCNNç½‘ç»œèƒ½å¦åœ¨æ­¤ä»»åŠ¡ä¸­å®ç°åŒæ ·çš„å¼ºå¤§é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„è½»å‹äº¤äº’å¼ç½‘ç»œï¼ˆLIM-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å±•ç¤ºç´§å‡‘CNNæ¨¡å‹æ½œåŠ›çš„æ–°å‹æ–¹æ³•ã€‚LIM-Netå»ºç«‹åœ¨2D CNNä¸»å¹²ç½‘ç»œä¸Šï¼Œé€šè¿‡ç”¨æˆ·æç¤ºç”Ÿæˆ2Dæç¤ºé®ç½©æ¥å¯åŠ¨åˆ†å‰²ã€‚è¯¥é®ç½©ç„¶åé€šè¿‡å†…å­˜æ¨¡å—ä¼ æ’­åˆ°æ•´ä¸ª3Dåºåˆ—ä¸­ã€‚ä¸ºäº†åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­è¿›è¡Œç»“æœç»†åŒ–ä¸ç¨³å®šï¼Œå¤šè½®ç»“æœèåˆï¼ˆMRFï¼‰æ¨¡å—ä¼šé€‰æ‹©å¹¶åˆå¹¶æ¥è‡ªå¤šè½®çš„ä¼˜è´¨é®ç½©ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡æ€ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†LIM-Netçš„ç«äº‰æ€§èƒ½ã€‚ä¸åŸºäºSAMçš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨æœªè§æ•°æ®ä¸Šå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æ‹¥æœ‰å‡ºè‰²çš„å‡†ç¡®æ€§å¹¶éœ€è¦æ›´å°‘çš„äº¤äº’æ“ä½œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLIM-Netçš„è½»é‡çº§è®¾è®¡åœ¨éƒ¨ç½²å’Œæ¨ç†æ•ˆç‡æ–¹é¢æä¾›äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶ä½GPUå†…å­˜æ¶ˆè€—é€‚åˆèµ„æºå—é™çš„ç¯å¢ƒã€‚è¿™äº›ä»¤äººé¼“èˆçš„ç»“æœè¡¨æ˜ï¼ŒLIM-Netå¯ä»¥ä½œä¸ºå¼ºå¤§çš„åŸºçº¿ï¼Œä¸æµè¡Œçš„SAMæ¨¡å‹ç›¸è¾…ç›¸æˆï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æœ‰æ•ˆçš„äº¤äº’å¼åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚ä»£ç å°†åœ¨\url{<a target="_blank" rel="noopener" href="https://github.com/goodtime-123/LIM-Net%7D%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/goodtime-123/LIM-Net}ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08315v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹åŒ»å­¦æˆåƒä¸­çš„ç—…ç¶æˆ–å™¨å®˜ç²¾ç¡®æ ‡æ³¨é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†3Dä½“ç§¯å›¾åƒæ—¶é¢ä¸´æ—¶é—´æ¶ˆè€—å¤§ã€å·¥ä½œé‡å¤§ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç´§å‡‘CNNæ¨¡å‹çš„äº¤äº’å¼ç½‘ç»œï¼ˆLIM-Netï¼‰ï¼Œç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚LIM-Netåˆ©ç”¨ç”¨æˆ·æç¤ºç”ŸæˆäºŒç»´æç¤ºæ©è†œï¼Œå¹¶é€šè¿‡å†…å­˜æ¨¡å—åœ¨ä¸‰ç»´åºåˆ—ä¸­ä¼ æ’­ã€‚å¤šè½®ç»“æœèåˆæ¨¡å—é€‰æ‹©å’Œåˆå¹¶æœ€ä½³æ©è†œï¼Œæé«˜äº¤äº’è¿‡ç¨‹ä¸­çš„ç»“æœç²¾åº¦å’Œç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒLIM-Netåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡æ€ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œç›¸è¾ƒäºSAMæ¨¡å‹å…·æœ‰æ›´å¼ºçš„æœªè§æ•°æ®æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡†ç¡®åº¦é«˜ã€äº¤äº’æ¬¡æ•°å°‘ã€‚å…¶è½»é‡çº§è®¾è®¡æœ‰åˆ©äºéƒ¨ç½²å’Œæ¨ç†æ•ˆç‡ï¼Œä½GPUå†…å­˜æ¶ˆè€—é€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚å› æ­¤ï¼ŒLIM-Netæœ‰æœ›æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ‰æ•ˆåŸºçº¿æ–¹æ³•ã€‚æ‘˜è¦è‡ªç½‘ç»œç§‘å­¦ç ”ç©¶çš„å¾®æ–‡æœ¬æå‡ºçš„æ–¹æ³•å’Œå®ç°è¡¨æ˜é‡è¦åº”ç”¨å‰æ™¯ï¼Œä½†å®é™…åº”ç”¨éœ€è¿›ä¸€æ­¥éªŒè¯å’Œæ”¹è¿›ã€‚æœ¬æ–‡çš„å¼€æºä»£ç å°†å‘å¸ƒåœ¨GitHubä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚æ€»ç»“å®Œæ¯•ã€‚æ ¸å¿ƒä¿¡æ¯ç®€æ˜æ‰¼è¦ï¼Œä¸è¶…è¿‡ä¸€ç™¾å­—ã€‚è¯·æ³¨æ„ï¼Œç”±äºæŠ€æœ¯ç»†èŠ‚å’Œå…·ä½“å®éªŒæ•°æ®æœªåœ¨æ–‡æœ¬ä¸­æä¾›ï¼Œæ‘˜è¦å¯èƒ½æ— æ³•æ¶µç›–æ‰€æœ‰ç»†èŠ‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>åŒ»å­¦æˆåƒä¸­ï¼Œå¯¹ç—…ç¶æˆ–å™¨å®˜çš„ç²¾ç¡®æ ‡æ³¨æ˜¯é‡è¦éœ€æ±‚ï¼Œä½†åœ¨å¤„ç†3Dä½“ç§¯å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨äº¤äº’å¼åˆ†å‰²æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€ŒSAMæ¶æ„è¾ƒä¸ºå¤æ‚å’Œåºå¤§å¯èƒ½å¯¹å®æ—¶è¿è¡Œå†…å­˜è¦æ±‚ä¸¥è‹›å’Œå¯¹å¤§å‹ç½‘ç»œçš„ç®—åŠ›èµ„æºè¦æ±‚å¤æ‚ç›¸å¯¹æ¶ˆè€—æ›´å¤§ä¸”æ•°æ®ç¹æ‚çš„ç‰¹å¾æ˜“é€ æˆå“åº”è¾ƒæ…¢å¯¹ç–¾ç—…é˜²æ²»æ— ç›Šè¿™æ˜¯é‡ç‚¹éœ€è¦è€ƒè™‘å’Œæ”¹è¿›çš„ç¼ºé™·å…³é”®ä¼˜åŠ¿æ— æ³•å¾—ä»¥å……åˆ†ä½“ç°å¼€å‘é€Ÿåº¦å¿«å…·ä½“é€Ÿåº¦ç›¸æ¯”è¯¥ç¼ºç‚¹å¼±å ç”¨ç³»ç»Ÿèµ„æºå°çš„ç®—æ³•åˆ™æ›´æœ‰ä¼˜åŠ¿ä¹Ÿæ›´é€‚åˆåº”ç”¨äºåŒ»å­¦å›¾åƒé¢†åŸŸäºŸå¾…æ¢ç´¢ã€‚ ç²¾ç®€åŒ–çš„CNNæ¨¡å‹å¯èƒ½å®ç°ç±»ä¼¼æ€§èƒ½ä½†å…·æœ‰è½»é‡çº§ä¼˜åŠ¿æ›´é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„æ–°å°è¯•æ˜¯æœ¬æ–‡çš„ç„¦ç‚¹ã€‚æå‡ºä¸€ç§åŸºäºç´§å‡‘CNNæ¨¡å‹çš„äº¤äº’å¼ç½‘ç»œï¼ˆLIM-Netï¼‰ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4308d5c082ab7082ab8b412042d79199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e7d895ce5328bfc4db8d65b50d46ecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35e1da9a05109b718a704506233ac831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4da00482af694a156e14865d7ffc632c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-141861cea76943b9e373186dab8e6c73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c419c732d54e2cc11114c09345ba4036.jpg" align="middle">
</details>




<h2 id="Unified-HT-CNNs-Architecture-Transfer-Learning-for-Segmenting-Diverse-Brain-Tumors-in-MRI-from-Gliomas-to-Pediatric-Tumors"><a href="#Unified-HT-CNNs-Architecture-Transfer-Learning-for-Segmenting-Diverse-Brain-Tumors-in-MRI-from-Gliomas-to-Pediatric-Tumors" class="headerlink" title="Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse   Brain Tumors in MRI from Gliomas to Pediatric Tumors"></a>Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse   Brain Tumors in MRI from Gliomas to Pediatric Tumors</h2><p><strong>Authors:Ramy A. Zeineldin, Franziska Mathis-Ullrich</strong></p>
<p>Accurate segmentation of brain tumors from 3D multimodal MRI is vital for diagnosis and treatment planning across diverse brain tumors. This paper addresses the challenges posed by the BraTS 2023, presenting a unified transfer learning approach that applies to a broader spectrum of brain tumors. We introduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural Networks optimized through transfer learning for varied brain tumor segmentation. This method captures spatial and contextual details from MRI data, fine-tuned on diverse datasets representing common tumor types. Through transfer learning, HT-CNNs utilize the learned representations from one task to improve generalization in another, harnessing the power of pre-trained models on large datasets and fine-tuning them on specific tumor types. We preprocess diverse datasets from multiple international distributions, ensuring representativeness for the most common brain tumors. Our rigorous evaluation employs standardized quantitative metrics across all tumor types, ensuring robustness and generalizability. The proposed ensemble model achieves superior segmentation results across the BraTS validation datasets over the previous winning methods. Comprehensive quantitative evaluations using the DSC and HD95 demonstrate the effectiveness of our approach. Qualitative segmentation predictions further validate the high-quality outputs produced by our model. Our findings underscore the potential of transfer learning and ensemble approaches in medical image segmentation, indicating a substantial enhancement in clinical decision-making and patient care. Despite facing challenges related to post-processing and domain gaps, our study sets a new precedent for future research for brain tumor segmentation. The docker image for the code and models has been made publicly available, <a target="_blank" rel="noopener" href="https://hub.docker.com/r/razeineldin/ht-cnns">https://hub.docker.com/r/razeineldin/ht-cnns</a>. </p>
<blockquote>
<p>å¯¹3Då¤šæ¨¡æ€MRIä¸­çš„è„‘è‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºå„ç§è„‘è‚¿ç˜¤çš„è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æœ¬æ–‡é’ˆå¯¹BraTS 2023æå‡ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¿ç§»å­¦ä¹ çš„æ–¹æ³•ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„è„‘è‚¿ç˜¤è°±ã€‚æˆ‘ä»¬å¼•å…¥äº†HT-CNNsï¼Œè¿™æ˜¯ä¸€ç§æ··åˆTransformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„é›†æˆæ–¹æ³•ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ é’ˆå¯¹å„ç§è„‘è‚¿ç˜¤åˆ†å‰²è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰MRIæ•°æ®ä¸­çš„ç©ºé—´å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå¹¶åœ¨ä»£è¡¨å¸¸è§è‚¿ç˜¤ç±»å‹çš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒHT-CNNsåˆ©ç”¨ä¸€ä¸ªä»»åŠ¡ä¸­å­¦åˆ°çš„è¡¨ç¤ºæ¥æé«˜å¦ä¸€ä¸ªä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆ©ç”¨åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹çš„å¨åŠ›ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šè‚¿ç˜¤ç±»å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¯¹æ¥è‡ªå¤šä¸ªå›½é™…åˆ†å¸ƒçš„å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œäº†é¢„å¤„ç†ï¼Œä»¥ç¡®ä¿å¯¹æœ€å¸¸è§è„‘è‚¿ç˜¤çš„ä»£è¡¨æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æ ‡å‡†åŒ–çš„å®šé‡æŒ‡æ ‡å¯¹æ‰€æœ‰è‚¿ç˜¤ç±»å‹è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œç¡®ä¿ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„é›†æˆæ¨¡å‹åœ¨BraTSéªŒè¯æ•°æ®é›†ä¸Šå–å¾—äº†æ›´å¥½çš„åˆ†å‰²ç»“æœã€‚ä½¿ç”¨DSCå’ŒHD95çš„ç»¼åˆå®šé‡è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®šæ€§çš„åˆ†å‰²é¢„æµ‹è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹äº§ç”Ÿçš„é«˜è´¨é‡è¾“å‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†è¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ï¼Œè¡¨æ˜åœ¨ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚å°½ç®¡é¢ä¸´ä¸åå¤„ç†å’Œé¢†åŸŸå·®è·ç›¸å…³çš„æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²çš„æœªæ¥å‘å±•æ ‘ç«‹äº†æ–°çš„æ ‡æ†ã€‚ä»£ç çš„dockeré•œåƒå’Œæ¨¡å‹å·²ç»å…¬å¼€å‘å¸ƒï¼Œå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://hub.docker.com/r/razeineldin/ht-cnns%E8%AE%BF%E9%97%AE%E3%80%82">https://hub.docker.com/r/razeineldin/ht-cnnsè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08240v1">PDF</a> Accepted in the Computer Assisted Radiology and Surgery (CARS 2024)   Conference</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆHybrid Transformerså’ŒConvolutional Neural Networksï¼ˆHT-CNNsï¼‰çš„ç»Ÿä¸€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä»3Då¤šæ¨¡æ€MRIä¸­å‡†ç¡®åˆ†å‰²å„ç§è„‘è‚¿ç˜¤ã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œé’ˆå¯¹ç‰¹å®šè‚¿ç˜¤ç±»å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°ä¼˜è¶Šçš„åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºä¸€ç§è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„è„‘è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>å¼•å…¥HT-CNNsæ¨¡å‹ï¼Œç»“åˆHybrid Transformerså’ŒConvolutional Neural Networksï¼Œèƒ½å¤Ÿä»MRIæ•°æ®ä¸­æ•æ‰ç©ºé—´ä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚</li>
<li>ä½¿ç”¨è¿ç§»å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒæ•°æ®é›†ä¸Šå¾®è°ƒï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡é¢„å¤„ç†äº†å¤šç§æ•°æ®é›†ï¼Œç¡®ä¿å¯¹æœ€å¸¸è§è„‘è‚¿ç˜¤çš„ä»£è¡¨æ€§ã€‚</li>
<li>é€šè¿‡æ ‡å‡†åŒ–å®šé‡æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå±•ç¤ºæ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨BraTSéªŒè¯æ•°æ®é›†ä¸Šçš„åˆ†å‰²ç»“æœä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98a3bc662666518e5cf295280b397d5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc8a08cab975018a29f5595807500dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a716f2ac9c8dd98123f992d9011aa01d.jpg" align="middle">
</details>




<h2 id="Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM"><a href="#Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM" class="headerlink" title="Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM"></a>Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM</h2><p><strong>Authors:Hiromasa Suzuki, Naomi Tsuji, Yoshiaki Kanemaru, Megumi Shidatsu, Laura Olivera-Nieto, Samar Safi-Harb, Shigeo S. Kimura, Eduardo de la Fuente, Sabrina Casanova, Kaya Mori, Xiaojie Wang, Sei Kato, Dai Tateishi, Hideki Uchiyama, Takaaki Tanaka, Hiroyuki Uchida, Shun Inoue, Dezhi Huang, Marianne Lemoine-Goumard, Daiki Miura, Shoji Ogawa, Shogo B. Kobayashi, Chris Done, Maxime Parra, MarÃ­a DÃ­az Trigo, Teo MuÃ±oz-Darias, Montserrat Armas Padilla, Ryota Tomaru, Yoshihiro Ueda</strong></p>
<p>A recent report on the detection of very-high-energy gamma rays from V4641 Sagittarii (V4641 Sgr) up to <del>0.8 peta-electronvolt has made it the second confirmed â€œPeVatronâ€ microquasar. Here we report on the observation of V4641 Sgr with X-Ray Imaging and Spectroscopy Mission (XRISM) in September 2024. Thanks to the large field of view and low background, the CCD imager Xtend successfully detected for the first time X-ray extended emission around V4641 Sgr with a significance of &gt; 4.5 sigma and &gt; 10 sigma based on our imaging and spectral analysis, respectively. The spatial extent is estimated to have a radius of $7 \pm 3$ arcmin ($13 \pm 5$ pc at a distance of 6.2 kpc) assuming a Gaussian-like radial distribution, which suggests that the particle acceleration site is within ~10 pc of the microquasar. If the X-ray morphology traces the diffusion of accelerated electrons, this spatial extent can be explained by either an enhanced magnetic field (</del>80 uG) or a suppressed diffusion coefficient (~$10^{27}$ cm$^2$ s$^{-1}$ at 100 TeV). The integrated X-ray flux, (4-6)$\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ (2-10 keV), would require a magnetic field strength higher than the galactic mean (&gt; 8 uG) if the diffuse X-ray emission originates from synchrotron radiation and the gamma-ray emission is predominantly hadronic. If the X-rays are of thermal origin, the measured extension, temperature, and plasma density can be explained by a jet with a luminosity of ~$2\times 10^{39}$ erg s$^{-1}$, which is comparable to the Eddington luminosity of this system. </p>
<blockquote>
<p>æœ€è¿‘çš„ä¸€ä»½å…³äºä»V4641å¤©ç®­æ˜Ÿï¼ˆV4641 Sgrï¼‰æ£€æµ‹åˆ°è¶…é«˜èƒ½ä¼½é©¬å°„çº¿çš„æŠ¥å‘Šï¼Œèƒ½é‡é«˜è¾¾~0.8æ‹ç”µå­ä¼ç‰¹ï¼Œä½¿å…¶æˆä¸ºç¬¬äºŒä¸ªç¡®è®¤çš„â€œæ‹ç”µå­ä¼ç‰¹åŠ é€Ÿå™¨â€å¾®ç±»æ˜Ÿã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†2024å¹´9æœˆä½¿ç”¨Xå°„çº¿æˆåƒå’Œå…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰å¯¹V4641 Sgrçš„è§‚å¯Ÿç»“æœã€‚ç”±äºXRISMå…·æœ‰è¾ƒå¤§çš„è§†åœºå’Œè¾ƒä½çš„èƒŒæ™¯ï¼Œå…¶CCDæˆåƒä»ªXtendé¦–æ¬¡æˆåŠŸåœ°æ£€æµ‹åˆ°äº†V4641 Sgrå‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„ï¼Œå…¶æ˜¾è‘—æ€§åŸºäºæˆ‘ä»¬çš„æˆåƒå’Œå…‰è°±åˆ†æåˆ†åˆ«å¤§äº4.5 sigmaå’Œå¤§äº10 sigmaã€‚å‡è®¾å…¶å¾„å‘åˆ†å¸ƒç±»ä¼¼äºé«˜æ–¯åˆ†å¸ƒï¼Œä¼°è®¡çš„ç©ºé—´èŒƒå›´åŠå¾„ä¸º$7Â±3$è§’åˆ†ï¼ˆåœ¨è·ç¦»6.2åƒç§’å·®è·çš„æƒ…å†µä¸‹ä¸º$13Â±5$ç§’å·®è·ï¼‰ï¼Œè¿™è¡¨æ˜ç²’å­åŠ é€ŸåŒºåŸŸä½äºå¾®ç±»æ˜Ÿå‘¨å›´çº¦10ç§’å·®è·çš„èŒƒå›´å†…ã€‚å¦‚æœXå°„çº¿çš„å½¢æ€è¿½è¸ªäº†åŠ é€Ÿç”µå­çš„æ‰©æ•£ï¼Œé‚£ä¹ˆè¿™ä¸ªç©ºé—´èŒƒå›´å¯ä»¥ç”¨å¢å¼ºçš„ç£åœºï¼ˆçº¦80å¾®é«˜æ–¯ï¼‰æˆ–æŠ‘åˆ¶çš„æ‰©æ•£ç³»æ•°ï¼ˆåœ¨100TeVæ—¶çº¦ä¸º$10^{27}$å˜ç±³$^2$ç§’$^{-1}$ï¼‰æ¥è§£é‡Šã€‚å¦‚æœæ¼«å°„Xå°„çº¿å‘å°„æ¥è‡ªåŒæ­¥è¾å°„ä¸”ä¼½é©¬å°„çº¿å‘å°„ä¸»è¦æ˜¯å¼ºå­å‘å°„ï¼Œé‚£ä¹ˆç§¯åˆ†Xå°„çº¿æµé‡ä¸ºï¼ˆ4-6ï¼‰Ã— è®¾å®šäº†ä½ çš„åˆå§‹è´¦æˆ·åå’Œæ—¶åŒºå®Œæˆè´¦æˆ·çš„æ³¨å†Œï¼‰å…ƒã€‚ï¼ˆè¿™å¯ä»¥æŒ‡å‚¨è“„å­˜æ¬¾ã€‚è¡Œä¸šç‰¹è‰²æ˜¯æŒ‡è¯¥ç±»å…¬å¸çš„ç»è¥æ¨¡å¼åœ¨ä¸å…¶ä»–å…¬å¸çš„å¯¹æ¯”ä¸‹æœ‰å…¶ç‹¬ç‰¹æ€§ï¼‰ï¼Œå•ä½æ—¶é—´ä¸ºå¹³æ–¹å˜ç±³ç§’ä¹˜ä»¥$å¤– $-$å†…åŸŸæœ€å°æœ‰è€—ç›¸å¹²æ‰©å±•æœ‰ç¼ºå®šæœ€ä¼˜æ’å€å˜æ•´è§£å¼•å«äº¤ç¯æ—¶åº¦æœ€å°ä¸”ç»´éœ€å…¨ä¹˜ååº¦åº¦ï¼Œåˆ™ä¼šè¦æ±‚ç£åœºå¼ºåº¦é«˜äºé“¶æ²³ç³»å¹³å‡å€¼ï¼ˆå¤§äº8å¾®é«˜æ–¯ï¼‰ã€‚å¦‚æœXå°„çº¿å…·æœ‰çƒ­èµ·æºç‰¹å¾ï¼Œæ‰€æµ‹å¾—çš„æ‰©å±•ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“å¯†åº¦å¯ä»¥ç”±å…‰åº¦çº¦ä¸º$å†…å¤–æœ¬å€é”€åˆ’ç€åº—æ¬¾ä¸¤è®¾åŠäº›è¥åŸºèƒ½å°ä»€è±¡æ³•è€Œäº§ä½“æ€§ä¼šæˆ–å¼ºç…§å‘æ®åå„¿ç¾å…ƒä¹°æŒ‰å’Œè§å¥½å“å‰æµèƒ½å‡æ‰€éƒ¨äºŒåº—éƒ½åˆè€…æµå› å›½èµ·ä¿ä½œä¸å€ä¼šè¾ƒï¿¥åŠ¨å·¦å³åˆ°åƒæˆ–å„è¿é¢Ã—ä¸“æ¿€æ±‡å¾„é‡Œéä¼˜æ•ˆå™¨ç¾¤å—ç¡®æ˜ åˆ’è§‚ä»æ”¹æ±‡ä½ä½¿å“æ´»è¾ƒæ±‡ä¸œèå‘˜çœå‘¨å˜äº¿å¤©æ‰€å®å› é¢†è¿›ä¿¡å…¬å‘¨ç½‘é€šèåŸºäººå½“å¼ºå¸¦å®¹æ•°ä½“å¤§ä¸ªå­¦åŒå°è¿›å¿ƒä½¿é€šæ¯è”è®¡ç®—å‡ºçš„æ•°å€¼æ¥ä¼°ç®—å…¶äº®åº¦ï¼Œè¯¥äº®åº¦ä¸ç³»ç»Ÿçš„çˆ±ä¸é¡¿å…‰åº¦ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08089v1">PDF</a> 9 pages, 5 figures, accepted for publication in ApJL</p>
<p><strong>Summary</strong><br>    V4641 Sagittariiçš„æœ€æ–°æŠ¥å‘Šè§‚å¯Ÿåˆ°å…¶å‘å°„çš„è¶…é«˜èƒ½ä¼½é©¬å°„çº¿ï¼Œä½¿å…¶æˆä¸ºç¬¬äºŒä¸ªç¡®è®¤çš„â€œPeVatronâ€å¾®ç±»æ˜Ÿã€‚åˆ©ç”¨Xå°„çº¿æˆåƒå’Œå…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰è§‚æµ‹ï¼Œé¦–æ¬¡æ£€æµ‹åˆ°V4641 Sgrå‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„ï¼Œå…¶ç©ºé—´èŒƒå›´æš—ç¤ºç²’å­åŠ é€Ÿä½ç‚¹è·ç¦»å¾®ç±»æ˜Ÿçº¦10å…‰å¹´ã€‚Xå°„çº¿çš„å½¢æ€å¯èƒ½è¿½è¸ªåŠ é€Ÿç”µå­çš„æ‰©æ•£ï¼Œè¿™å¯ä»¥ç”±å¢å¼ºçš„ç£åœºæˆ–æŠ‘åˆ¶çš„æ‰©æ•£ç³»æ•°æ¥è§£é‡Šã€‚å¦‚æœXå°„çº¿æ˜¯åŒæ­¥è¾å°„èµ·æºï¼Œåˆ™ä¼½é©¬å°„çº¿ä¸»è¦æ˜¯å¼ºå­è¿‡ç¨‹ï¼›è‹¥æ˜¯çƒ­èµ·æºï¼Œåˆ™è§‚æµ‹åˆ°çš„æ‰©å±•ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“å¯†åº¦å¯ç”±äº®åº¦ä¸Edingtonç›¸å½“çš„çƒ­å–·æµè§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>V4641 Sagittariiè¢«ç¡®è®¤ä¸ºç¬¬äºŒä¸ªâ€œPeVatronâ€å¾®ç±»æ˜Ÿï¼Œèƒ½å¤Ÿå‘å°„è¶…é«˜èƒ½ä¼½é©¬å°„çº¿ã€‚</li>
<li>åˆ©ç”¨XRISMè§‚æµ‹ï¼Œé¦–æ¬¡æ£€æµ‹åˆ°V4641 Sgrå‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„ï¼Œå…·æœ‰æ˜¾è‘—çš„ç©ºé—´èŒƒå›´ã€‚</li>
<li>ç²’å­åŠ é€Ÿä½ç‚¹è·ç¦»å¾®ç±»æ˜Ÿçº¦10å…‰å¹´ï¼Œæš—ç¤ºåŠ é€Ÿç”µå­çš„æ‰©æ•£è·¯å¾„ã€‚</li>
<li>Xå°„çº¿çš„å½¢æ€å¯èƒ½ä¸ç£åœºå¼ºåº¦æœ‰å…³ï¼Œè¡¨ç°ä¸ºå¢å¼ºç£åœºæˆ–æŠ‘åˆ¶æ‰©æ•£ç³»æ•°çš„è¯æ®ã€‚</li>
<li>è‹¥Xå°„çº¿æ¥è‡ªåŒæ­¥è¾å°„ï¼Œåˆ™ä¼½é©¬å°„çº¿ä¸»è¦æ˜¯å¼ºå­è¿‡ç¨‹ï¼›è‹¥ä¸ºçƒ­èµ·æºï¼Œåˆ™è§‚æµ‹åˆ°çš„ç‰¹æ€§å¯ç”±çƒ­å–·æµè§£é‡Šã€‚</li>
<li>åŒæ­¥è¾å°„èµ·æºçš„Xå°„çº¿è¦æ±‚ç£åœºå¼ºåº¦é«˜äºé“¶æ²³ç³»å¹³å‡å€¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a2b19b158dfb1d0c58b7aff98022526c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72c15493fab70e3833a02e374cd4b4b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee798d1003758e369c83e7667a798c19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cac89c474f11bf24a62b900f39970d17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4354e72bf6504d124144b3307fd5725b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b880e3fd2f1a9acd5075154c622764f.jpg" align="middle">
</details>




<h2 id="How-to-select-slices-for-annotation-to-train-best-performing-deep-learning-segmentation-models-for-cross-sectional-medical-images"><a href="#How-to-select-slices-for-annotation-to-train-best-performing-deep-learning-segmentation-models-for-cross-sectional-medical-images" class="headerlink" title="How to select slices for annotation to train best-performing deep   learning segmentation models for cross-sectional medical images?"></a>How to select slices for annotation to train best-performing deep   learning segmentation models for cross-sectional medical images?</h2><p><strong>Authors:Yixin Zhang, Kevin Kramer, Maciej A. Mazurowski</strong></p>
<p>Automated segmentation of medical images highly depends on the availability of accurate manual image annotations. Such annotations are very time-consuming and costly to generate, and often require specialized expertise, particularly for cross-sectional images which contain many slices for each patient. It is crucial to ensure the best use of annotation resources. In this paper, we systematically answer the question of how to select slices of cross-sectional medical images in order to maximize performance of the resulting deep learning segmentation models. We conducted experiments on 4 medical imaging segmentation tasks with varying annotation budgets, numbers of annotated cases, numbers of annotated slices per volume, slice selection techniques, and mask interpolations. We found that:   1) It is almost always preferable to annotate fewer slices per volume and more volumes given an annotation budget. 2) Selecting slices for annotation by unsupervised active learning (UAL) is not superior to selecting slices randomly or at fixed intervals, provided that each volume is allocated the same number of annotated slices. 3) Interpolating masks between annotated slices rarely enhances model performance, with exceptions of some specific configuration for 3D models. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå‡†ç¡®çš„æ‰‹åŠ¨å›¾åƒæ³¨é‡Šçš„å¯ç”¨æ€§ã€‚è¿™äº›æ³¨é‡Šçš„ç”Ÿæˆéå¸¸è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ï¼Œé€šå¸¸éœ€è¦ä¸“ä¸šä¸“é•¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŒ…å«å¤šä¸ªåˆ‡ç‰‡çš„æ¨ªæˆªé¢å›¾åƒè€Œè¨€ã€‚ç¡®ä¿æœ€ä½³åˆ©ç”¨æ³¨é‡Šèµ„æºè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å›ç­”äº†å¦‚ä½•é€‰æ‹©æ¨ªæˆªé¢åŒ»å­¦å›¾åƒçš„åˆ‡ç‰‡ï¼Œä»¥ä¾¿æœ€å¤§é™åº¦åœ°æé«˜æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å…·æœ‰ä¸åŒæ³¨é‡Šé¢„ç®—ã€æ ‡æ³¨ç—…ä¾‹æ•°ã€æ¯å·æ ‡æ³¨åˆ‡ç‰‡æ•°ã€åˆ‡ç‰‡é€‰æ‹©æŠ€æœ¯å’Œæ©è†œæ’å€¼çš„å››ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬å‘ç°ï¼š1ï¼‰åœ¨ç»™å®šæ³¨é‡Šé¢„ç®—çš„æƒ…å†µä¸‹ï¼Œæ¯å·æ³¨é‡Šçš„åˆ‡ç‰‡æ›´å°‘è€Œæ€»çš„å·æ•°æ›´å¤šå‡ ä¹æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚2ï¼‰é€šè¿‡æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ ï¼ˆUALï¼‰é€‰æ‹©æ³¨é‡Šåˆ‡ç‰‡å¹¶ä¸ä¼˜äºéšæœºé€‰æ‹©åˆ‡ç‰‡æˆ–åœ¨å›ºå®šé—´éš”é€‰æ‹©åˆ‡ç‰‡ï¼Œå‰ææ˜¯æ¯å·åˆ†é…çš„æ³¨é‡Šåˆ‡ç‰‡æ•°é‡ç›¸åŒã€‚3ï¼‰åœ¨æ ‡æ³¨åˆ‡ç‰‡ä¹‹é—´æ’å€¼æ©è†œå¾ˆå°‘èƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¯¹äºæŸäº›ç‰¹å®šé…ç½®çš„3Dæ¨¡å‹å­˜åœ¨ä¾‹å¤–æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨æœ‰é™çš„æ ‡æ³¨èµ„æºä¸‹ï¼Œé€‰æ‹©æˆªé¢åŒ»å­¦å›¾åƒä¸­çš„åˆ‡ç‰‡ä»¥æœ€å¤§åŒ–æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ ‡æ³¨é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ›´å€¾å‘äºå¯¹æ¯ä¸ªä½“ç§¯æ ‡æ³¨è¾ƒå°‘çš„åˆ‡ç‰‡ä½†å¢åŠ æ ‡æ³¨çš„ä½“ç§¯æ•°é‡ï¼›é€šè¿‡æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ é€‰æ‹©åˆ‡ç‰‡å¹¶ä¸ä¼˜äºéšæœºé€‰æ‹©æˆ–æŒ‰å›ºå®šé—´éš”é€‰æ‹©åˆ‡ç‰‡ï¼Œå‰ææ˜¯æ¯ä¸ªä½“ç§¯åˆ†é…çš„æ ‡æ³¨åˆ‡ç‰‡æ•°é‡ç›¸åŒï¼›åœ¨ç‰¹å®šé…ç½®ä¸‹ï¼Œå¯¹æ ‡æ³¨åˆ‡ç‰‡è¿›è¡Œæ©è†œæ’å€¼å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æœ‰é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨æ ‡æ³¨é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå»ºè®®å¯¹æ¯ä¸ªä½“ç§¯æ ‡æ³¨è¾ƒå°‘çš„åˆ‡ç‰‡å¹¶å¢åŠ æ ‡æ³¨çš„ä½“ç§¯æ•°é‡ã€‚</li>
<li>é€šè¿‡æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ é€‰æ‹©åˆ‡ç‰‡è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ä¸æ€»æ˜¯æœ€ä¼˜é€‰æ‹©ï¼Œéšæœºé€‰æ‹©æˆ–æŒ‰å›ºå®šé—´éš”é€‰æ‹©åˆ‡ç‰‡ä¹Ÿå¯ã€‚</li>
<li>æ’å€¼æ©è†œå¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æœ‰é™ï¼Œä»…åœ¨ç‰¹å®šé…ç½®ä¸‹å¯¹3Dæ¨¡å‹æœ‰ä¸€äº›ç§¯æå½±å“ã€‚</li>
<li>åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²é«˜åº¦ä¾èµ–äºå‡†ç¡®çš„æ‰‹åŠ¨å›¾åƒæ ‡æ³¨ã€‚</li>
<li>æ ‡æ³¨èµ„æºçš„ä½¿ç”¨è‡³å…³é‡è¦ï¼Œéœ€è¦å¯»æ‰¾æœ€æœ‰æ•ˆçš„æ–¹æ³•åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨èµ„æºã€‚</li>
<li>ä¸åŒæ ‡æ³¨é¢„ç®—ã€æ ‡æ³¨æ¡ˆä¾‹æ•°é‡ã€æ¯ä½“ç§¯æ ‡æ³¨åˆ‡ç‰‡æ•°é‡ã€åˆ‡ç‰‡é€‰æ‹©æŠ€æœ¯å’Œæ©è†œæ’å€¼æ–¹æ³•éƒ½ä¼šå½±å“æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e2f8d6f14b9f862bf6bfe501cd70a90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f4dfb516a53f2ccf6d05d82b837a8b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-beb507bc5306f706bbdfc05dc0b37d45.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82437981b6ea344d9d6232eb346573b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13d10fbbe4c60cc14ef516c7f3bf3020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101fa80dc8cbc812cdf00bf65f88a54d.jpg" align="middle">
</details>




<h2 id="BSAFusion-A-Bidirectional-Stepwise-Feature-Alignment-Network-for-Unaligned-Medical-Image-Fusion"><a href="#BSAFusion-A-Bidirectional-Stepwise-Feature-Alignment-Network-for-Unaligned-Medical-Image-Fusion" class="headerlink" title="BSAFusion: A Bidirectional Stepwise Feature Alignment Network for   Unaligned Medical Image Fusion"></a>BSAFusion: A Bidirectional Stepwise Feature Alignment Network for   Unaligned Medical Image Fusion</h2><p><strong>Authors:Huafeng Li, Dayong Su, Qing Cai, Yafei Zhang</strong></p>
<p>If unaligned multimodal medical images can be simultaneously aligned and fused using a single-stage approach within a unified processing framework, it will not only achieve mutual promotion of dual tasks but also help reduce the complexity of the model. However, the design of this model faces the challenge of incompatible requirements for feature fusion and alignment; specifically, feature alignment requires consistency among corresponding features, whereas feature fusion requires the features to be complementary to each other. To address this challenge, this paper proposes an unaligned medical image fusion method called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F) strategy. To reduce the negative impact of modality differences on cross-modal feature matching, we incorporate the Modal Discrepancy-Free Feature Representation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature Representation Head (MFRH) to integrate the global information of the input image. By injecting the information contained in MFRH of the current image into other modality images, it effectively reduces the impact of modality differences on feature alignment while preserving the complementary information carried by different images. In terms of feature alignment, BSFA-F employs a bidirectional stepwise alignment deformation field prediction strategy based on the path independence of vector displacement between two points. This strategy solves the problem of large spans and inaccurate deformation field prediction in single-step alignment. Finally, Multi-Modal Feature Fusion block achieves the fusion of aligned features. The experimental results across multiple datasets demonstrate the effectiveness of our method. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/slrl123/BSAFusion">https://github.com/slrl123/BSAFusion</a>. </p>
<blockquote>
<p>å¦‚æœèƒ½å¤Ÿåœ¨ç»Ÿä¸€å¤„ç†æ¡†æ¶å†…é‡‡ç”¨å•é˜¶æ®µæ–¹æ³•åŒæ—¶å¯¹æœªå¯¹é½çš„å¤šæ¨¡å¼åŒ»å­¦å›¾åƒè¿›è¡Œå¯¹é½å’Œèåˆï¼Œä¸ä»…å¯ä»¥å®ç°åŒé‡ä»»åŠ¡çš„ç›¸äº’ä¿ƒè¿›ï¼Œè¿˜æœ‰åŠ©äºé™ä½æ¨¡å‹çš„å¤æ‚æ€§ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹çš„è®¾è®¡é¢ä¸´ç€ç‰¹å¾èåˆä¸å¯¹é½è¦æ±‚ä¸å…¼å®¹çš„æŒ‘æˆ˜ï¼›å…·ä½“æ¥è¯´ï¼Œç‰¹å¾å¯¹é½è¦æ±‚ç›¸åº”ç‰¹å¾ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè€Œç‰¹å¾èåˆè¦æ±‚ç‰¹å¾ä¹‹é—´ç›¸äº’è¡¥å……ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºåŒå‘é€æ­¥ç‰¹å¾å¯¹é½ä¸èåˆï¼ˆBSFA-Fï¼‰ç­–ç•¥çš„æœªå¯¹é½åŒ»å­¦å›¾åƒèåˆæ–¹æ³•ã€‚ä¸ºäº†å‡å°‘æ¨¡æ€å·®å¼‚å¯¹è·¨æ¨¡æ€ç‰¹å¾åŒ¹é…çš„è´Ÿé¢å½±å“ï¼Œæˆ‘ä»¬å°†æ¨¡æ€å·®å¼‚æ— å…³ç‰¹å¾è¡¨ç¤ºï¼ˆMDF-FRï¼‰æ–¹æ³•çº³å…¥BSFA-Fã€‚MDF-FRåˆ©ç”¨æ¨¡æ€ç‰¹å¾è¡¨ç¤ºå¤´ï¼ˆMFRHï¼‰æ¥æ•´åˆè¾“å…¥å›¾åƒçš„å…¨å±€ä¿¡æ¯ã€‚é€šè¿‡å°†å½“å‰å›¾åƒçš„MFRHä¸­æ‰€åŒ…å«çš„ä¿¡æ¯æ³¨å…¥åˆ°å…¶ä»–æ¨¡æ€å›¾åƒä¸­ï¼Œåœ¨ä¿æŒä¸åŒå›¾åƒæ‰€æºå¸¦çš„äº’è¡¥ä¿¡æ¯çš„åŒæ—¶ï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨¡æ€å·®å¼‚å¯¹ç‰¹å¾å¯¹é½çš„å½±å“ã€‚åœ¨ç‰¹å¾å¯¹é½æ–¹é¢ï¼ŒBSFA-Fé‡‡ç”¨äº†ä¸€ç§åŸºäºä¸¤ç‚¹é—´çŸ¢é‡ä½ç§»è·¯å¾„ç‹¬ç«‹æ€§çš„åŒå‘é€æ­¥å¯¹é½å˜å½¢åœºé¢„æµ‹ç­–ç•¥ã€‚è¯¥ç­–ç•¥è§£å†³äº†å•æ­¥å¯¹é½ä¸­è·¨åº¦å¤§ã€å˜å½¢åœºé¢„æµ‹ä¸å‡†ç¡®çš„é—®é¢˜ã€‚æœ€åï¼Œå¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—å®ç°äº†å¯¹é½ç‰¹å¾çš„èåˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/slrl123/BSAFusion%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/slrl123/BSAFusionä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08050v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¤šæ¨¡æ€èåˆæŒ‘æˆ˜åœ¨äºç‰¹å¾èåˆä¸å¯¹é½çš„è¦æ±‚ä¸å…¼å®¹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºBSFA-Fçš„æœªå¯¹é½åŒ»å­¦å›¾åƒèåˆæ–¹æ³•ï¼Œç»“åˆMDF-FRå‡å°‘æ¨¡æ€å·®å¼‚å¯¹ç‰¹å¾åŒ¹é…çš„å½±å“ï¼Œé€šè¿‡åŒå‘é€æ­¥ç‰¹å¾å¯¹é½ç­–ç•¥è§£å†³å•æ­¥å¯¹é½ä¸­å¤§é—®é¢˜åŠå˜å½¢åœºé¢„æµ‹ä¸å‡†ç¡®çš„é—®é¢˜ã€‚å¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—å®ç°ç‰¹å¾èåˆã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœªå¯¹é½çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆéœ€è¦åŒæ—¶è§£å†³ç‰¹å¾èåˆä¸å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å¾å¯¹é½è¦æ±‚å¯¹åº”ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œè€Œç‰¹å¾èåˆè¦æ±‚ç‰¹å¾çš„äº’è¡¥æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„BSFA-Fæ–¹æ³•ç»“åˆMDF-FRæ¥å‡å°‘æ¨¡æ€å·®å¼‚å¯¹è·¨æ¨¡æ€ç‰¹å¾åŒ¹é…çš„ä¸åˆ©å½±å“ã€‚</li>
<li>MDF-FRé€šè¿‡èå…¥å…¨å±€ä¿¡æ¯æé«˜ä¸åŒæ¨¡æ€å›¾åƒä¹‹é—´çš„å…¼å®¹æ€§ã€‚</li>
<li>BSFA-Fé‡‡ç”¨åŸºäºä¸¤ç‚¹é—´çŸ¢é‡ä½ç§»è·¯å¾„ç‹¬ç«‹æ€§çš„åŒå‘é€æ­¥å¯¹é½ç­–ç•¥ï¼Œè§£å†³å•æ­¥å¯¹é½ä¸­çš„å¤§é—®é¢˜åŠå˜å½¢åœºé¢„æµ‹ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>å¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—å®ç°ç‰¹å¾èåˆï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b7f9da0ccb0468c7ca65de121f4bad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd124758ae02ffa585225d8a1bf6229b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13f45227edc532325d166e2d3fb3997c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77004519d46519be535555dbeef99d1f.jpg" align="middle">
</details>




<h2 id="Graph-convolutional-networks-enable-fast-hemorrhagic-stroke-monitoring-with-electrical-impedance-tomography"><a href="#Graph-convolutional-networks-enable-fast-hemorrhagic-stroke-monitoring-with-electrical-impedance-tomography" class="headerlink" title="Graph convolutional networks enable fast hemorrhagic stroke monitoring   with electrical impedance tomography"></a>Graph convolutional networks enable fast hemorrhagic stroke monitoring   with electrical impedance tomography</h2><p><strong>Authors:J. Toivanen, V. Kolehmainen, A. Paldanius, A. HÃ¤nninen, A. Hauptmann, S. J. Hamilton</strong></p>
<p>Objective: To develop a fast image reconstruction method for stroke monitoring with electrical impedance tomography with image quality comparable to computationally expensive nonlinear model-based methods. Methods: A post-processing approach with graph convolutional networks is employed. Utilizing the flexibility of the graph setting, a graph U-net is trained on linear difference reconstructions from 2D simulated stroke data and applied to fully 3D images from realistic simulated and experimental data. An additional network, trained on 3D vs. 2D images, is also considered for comparison. Results: Post-processing the linear difference reconstructions through the graph U-net significantly improved the image quality, resulting in images comparable to, or better than, the time-intensive nonlinear reconstruction method (a few minutes vs. several hours). Conclusion: Pairing a fast reconstruction method, such as linear difference imaging, with post-processing through a graph U-net provided significant improvements, at a negligible computational cost. Training in the graph framework vs classic pixel-based setting (CNN) allowed the ability to train on 2D cross-sectional images and process 3D volumes providing a nearly 50x savings in data simulation costs with no noticeable loss in quality. Significance: The proposed approach of post-processing a linear difference reconstruction with the graph U-net could be a feasible approach for on-line monitoring of hemorrhagic stroke. </p>
<blockquote>
<p>ç›®æ ‡ï¼šå¼€å‘ä¸€ç§åŸºäºç”µé˜»æŠ—æˆåƒæŠ€æœ¯çš„å¿«é€Ÿå›¾åƒé‡å»ºæ–¹æ³•ï¼Œç”¨äºä¸­é£ç›‘æµ‹ï¼Œå…¶å›¾åƒè´¨é‡å¯ä¸è®¡ç®—å¯†é›†å‹çš„éçº¿æ€§æ¨¡å‹æ–¹æ³•ç›¸åª²ç¾ã€‚æ–¹æ³•ï¼šé‡‡ç”¨å›¾å·ç§¯ç½‘ç»œçš„åå¤„ç†æ–¹æ³•ã€‚åˆ©ç”¨å›¾è®¾ç½®çš„çµæ´»æ€§ï¼Œå¯¹æ¥è‡ªäºŒç»´æ¨¡æ‹Ÿä¸­é£æ•°æ®çš„çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œå›¾U-netè®­ç»ƒï¼Œå¹¶åº”ç”¨äºæ¥è‡ªçœŸå®æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®çš„å®Œå…¨ä¸‰ç»´å›¾åƒã€‚å¦å¤–ï¼Œè¿˜è€ƒè™‘äº†ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´ä¸äºŒç»´å›¾åƒçš„æ¯”è¾ƒç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚ç»“æœï¼šé€šè¿‡å›¾U-netå¯¹çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œåå¤„ç†ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ï¼Œå¾—åˆ°çš„å›¾åƒä¸è€—æ—¶è¾ƒé•¿çš„éçº¿æ€§é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæ•ˆæœç›¸å½“æˆ–æ›´å¥½ï¼ˆå‡ åˆ†é’Ÿä¸æ•°å°æ—¶ï¼‰ã€‚ç»“è®ºï¼šå°†å¿«é€Ÿé‡å»ºæ–¹æ³•ï¼ˆå¦‚çº¿æ€§å·®åˆ†æˆåƒï¼‰ä¸å›¾U-netçš„åå¤„ç†ç›¸ç»“åˆï¼Œåœ¨å‡ ä¹å¯ä»¥å¿½ç•¥çš„è®¡ç®—æˆæœ¬ä¸‹ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚åœ¨å›¾å½¢æ¡†æ¶ä¸­è¿›è¡Œè®­ç»ƒä¸ä¼ ç»Ÿçš„åŸºäºåƒç´ çš„è®¾ç½®ï¼ˆCNNï¼‰ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨äºŒç»´æ¨ªæˆªé¢å›¾åƒä¸Šè®­ç»ƒå¹¶åœ¨ä¸‰ç»´ä½“ç§¯ä¸Šè¿›è¡Œå¤„ç†ï¼ŒèŠ‚çœäº†è¿‘50å€çš„æ•°æ®æ¨¡æ‹Ÿæˆæœ¬ï¼Œä¸”è´¨é‡æ— æ˜æ˜¾æŸå¤±ã€‚æ„ä¹‰ï¼šé‡‡ç”¨å›¾U-netå¯¹çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œåå¤„ç†çš„å»ºè®®æ–¹æ³•å¯èƒ½æˆä¸ºåœ¨çº¿ç›‘æµ‹å‡ºè¡€æ€§ä¸­é£çš„å¯è¡Œæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07888v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong><br>     é‡‡ç”¨å›¾å·ç§¯ç½‘ç»œåå¤„ç†çš„æ–¹æ³•ï¼Œç»“åˆçº¿æ€§å·®åˆ†é‡å»ºæŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿå›¾åƒé‡å»ºæ–¹æ³•ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…è·å¾—é«˜è´¨é‡çš„å›¾åƒï¼Œä¸ä¼ ç»Ÿè€—æ—¶çš„éçº¿æ€§é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œåˆ©ç”¨å›¾æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„æ¨¡æ‹Ÿæˆæœ¬å¤„ç†ä¸‰ç»´å›¾åƒï¼Œä¸”è´¨é‡æ— æ˜æ˜¾æŸå¤±ã€‚è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ä¸ºåœ¨çº¿ç›‘æµ‹å‡ºè¡€æ€§å’ä¸­æä¾›äº†æ–°çš„æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®æ ‡ï¼šå¼€å‘ä¸€ç§å¿«é€Ÿå›¾åƒé‡å»ºæ–¹æ³•ï¼Œç”¨äºå’ä¸­ç›‘æµ‹çš„ç”µé˜»æŠ—å±‚ææˆåƒæŠ€æœ¯ï¼Œå…¶å›¾åƒè´¨é‡å¯ä¸è®¡ç®—æ˜‚è´µçš„éçº¿æ€§æ¨¡å‹æ–¹æ³•ç›¸æ¯”è¾ƒã€‚</li>
<li>æ–¹æ³•ï¼šé‡‡ç”¨åŸºäºå›¾å·ç§¯ç½‘ç»œçš„åå¤„ç†æ–¹æ³•ï¼Œè®­ç»ƒå›¾U-netå¯¹çº¿æ€§å·®åˆ†é‡å»ºçš„äºŒç»´æ¨¡æ‹Ÿå’ä¸­æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¹¶åº”ç”¨äºä¸‰ç»´çœŸå®å›¾åƒæ•°æ®ã€‚</li>
<li>ç»“æœï¼šé€šè¿‡å›¾U-netåå¤„ç†çº¿æ€§å·®åˆ†é‡å»ºç»“æœï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ï¼Œå¾—åˆ°çš„å›¾åƒä¸è€—æ—¶è¾ƒé•¿çš„éçº¿æ€§é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰ç›¸å½“æˆ–æ›´å¥½çš„è´¨é‡ã€‚</li>
<li>ç»“è®ºï¼šå°†å¿«é€Ÿé‡å»ºæ–¹æ³•ä¸å›¾U-netåå¤„ç†ç›¸ç»“åˆï¼Œåœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ã€‚</li>
<li>è®­ç»ƒæ–¹å¼ï¼šåˆ©ç”¨å›¾æ¡†æ¶è¿›è¡Œè®­ç»ƒä¸ä¼ ç»Ÿåƒç´ åŸºç¡€è®¾ç½®ï¼ˆCNNï¼‰ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨äºŒç»´æ¨ªæˆªé¢å›¾åƒä¸Šè®­ç»ƒå¹¶å¤„ç†ä¸‰ç»´ä½“ç§¯æ•°æ®ï¼Œå®ç°äº†æ•°æ®æ¨¡æ‹Ÿæˆæœ¬çš„è¿‘50å€èŠ‚çº¦ï¼ŒåŒæ—¶è´¨é‡æ— æ˜æ˜¾æŸå¤±ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜ç‚¹ï¼šå¿«é€Ÿã€é«˜è´¨é‡ã€è®¡ç®—æˆæœ¬ä½ã€é€‚åº”æ€§å¼ºï¼ˆå¯å¤„ç†äºŒç»´å’Œä¸‰ç»´æ•°æ®ï¼‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cec0d9ea2574124bedabdb795b08cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8340f2963b7d0c187df515b7e9d5ef60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-250af44af9a6961da5b7ae4a1198c4da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ae456f5a70d16e855adde48a9c354f.jpg" align="middle">
</details>




<h2 id="XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder"><a href="#XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder" class="headerlink" title="XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder"></a>XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder</h2><p><strong>Authors:Shenghao Zhu, Yifei Chen, Shuo Jiang, Weihong Chen, Chang Liu, Yuanhan Wang, Xu Chen, Yifan Ke, Feiwei Qin, Zhu Zhu, Changmiao Wang</strong></p>
<p>Neurogliomas are among the most aggressive forms of cancer, presenting considerable challenges in both treatment and monitoring due to their unpredictable biological behavior. Magnetic resonance imaging (MRI) is currently the preferred method for diagnosing and monitoring gliomas. However, the lack of specific imaging techniques often compromises the accuracy of tumor segmentation during the imaging process. To address this issue, we introduce the XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder framework with the Vision XLSTM module to reconstruct missing MRI modalities. By deeply fusing spatial and temporal features, it enhances tumor segmentation performance. The key innovation of our approach is the Self-Attention Variational Encoder (SAVE) module, which improves the integration of modal features. Additionally, it optimizes the interaction of features between segmentation and reconstruction tasks through the Squeeze-Fusion-Excitation Cross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset demonstrate that our model significantly outperforms existing advanced methods in handling cases where modalities are missing. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED">https://github.com/Quanato607/XLSTM-HVED</a>. </p>
<blockquote>
<p>ç¥ç»èƒ¶è´¨ç˜¤æ˜¯æœ€å…·ä¾µè¢­æ€§çš„ç™Œç—‡å½¢å¼ä¹‹ä¸€ï¼Œç”±äºå…¶ä¸å¯é¢„æµ‹çš„ç”Ÿç‰©è¡Œä¸ºï¼Œç»™æ²»ç–—å’Œç›‘æµ‹å¸¦æ¥äº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ç›®å‰ï¼Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯è¯Šæ–­å’Œç›‘æµ‹èƒ¶è´¨ç˜¤çš„é¦–é€‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç¼ºä¹ç‰¹å®šçš„æˆåƒæŠ€æœ¯å¾€å¾€ä¼šå½±å“æˆåƒè¿‡ç¨‹ä¸­è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†XLSTM-HVEDæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼‚æ¨¡å¼ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’ŒVision XLSTMæ¨¡å—ï¼Œä»¥é‡å»ºç¼ºå¤±çš„MRIæ¨¡å¼ã€‚é€šè¿‡æ·±åº¦èåˆç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæé«˜äº†è‚¿ç˜¤åˆ†å‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ–¹æ³•çš„å…³é”®åˆ›æ–°ç‚¹æ˜¯è‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—ï¼Œå®ƒæ”¹è¿›äº†æ¨¡å¼ç‰¹å¾çš„èåˆã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡æŒ¤å‹-èåˆ-æ¿€åŠ±äº¤å‰æ„è¯†ï¼ˆSFECAï¼‰æ¨¡å—ä¼˜åŒ–äº†åˆ†å‰²å’Œé‡å»ºä»»åŠ¡ä¹‹é—´ç‰¹å¾çš„äº¤äº’ã€‚æˆ‘ä»¬ä½¿ç”¨BraTS 2024æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡å¼çš„æƒ…å†µæ—¶ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„é«˜çº§æ–¹æ³•ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Quanato607/XLSTM-HVEDä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07804v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¥ç»èƒ¶è´¨ç˜¤åœ¨æ²»ç–—å’Œç›‘æµ‹æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å…¶ä¸å¯é¢„æµ‹çš„ç”Ÿç‰©è¡Œä¸ºã€‚ä¸ºè§£å†³ç£å…±æŒ¯æˆåƒæŠ€æœ¯åœ¨è‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§çš„é™åˆ¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼‚è´¨æ¨¡æ€ç¼–ç å™¨è§£ç å™¨æ¡†æ¶ä¸è§†è§‰XLSTMæ¨¡å—çš„XLSTM-HVEDæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦èåˆç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæé«˜è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚å…³é”®åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†è‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨æ¨¡å—å’ŒæŒ¤å‹èåˆæ¿€åŠ±äº¤å‰æ„ŸçŸ¥æ¨¡å—ï¼Œä¼˜åŒ–äº†æ¨¡æ€ç‰¹å¾çš„èåˆä»¥åŠåˆ†å‰²ä¸é‡å»ºä»»åŠ¡ä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚ä½¿ç”¨BraTS 2024æ•°æ®é›†çš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»èƒ¶è´¨ç˜¤æ˜¯æå…·ä¾µè¢­æ€§çš„ç™Œç—‡å½¢å¼ï¼Œæ²»ç–—ä¸ç›‘æµ‹å­˜åœ¨æŒ‘æˆ˜ï¼Œå› å…¶ç”Ÿç‰©è¡Œä¸ºä¸å¯é¢„æµ‹ã€‚</li>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ç›®å‰è¯Šæ–­ä¸ç›‘æµ‹èƒ¶è´¨ç˜¤çš„é¦–é€‰æ–¹æ³•ï¼Œä½†ç¼ºä¹ç‰¹å®šæˆåƒæŠ€æœ¯ä¼šå½±å“è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥çš„XLSTM-HVEDæ¨¡å‹é›†æˆäº†å¼‚è´¨æ¨¡æ€ç¼–ç å™¨è§£ç å™¨æ¡†æ¶ä¸è§†è§‰XLSTMæ¨¡å—ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦èåˆç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæé«˜äº†è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨æ¨¡å—æ˜¯æ¨¡å‹çš„å…³é”®åˆ›æ–°ç‚¹ï¼Œæ”¹è¿›äº†æ¨¡æ€ç‰¹å¾çš„èåˆã€‚</li>
<li>SFECAæ¨¡å—ä¼˜åŒ–äº†åˆ†å‰²ä¸é‡å»ºä»»åŠ¡ä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf8f374bd558795dcd9cc86a7d3bcae5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da60db8443b675a68c3c47ba18158641.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-970e1b1c8819dbfa82c0b6a5bd066932.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ac994a06936a3646fed8efd39998301.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-078694a99b64803abd171ca0ae8cce30.jpg" align="middle">
</details>




<h2 id="Image-Retrieval-with-Intra-Sweep-Representation-Learning-for-Neck-Ultrasound-Scanning-Guidance"><a href="#Image-Retrieval-with-Intra-Sweep-Representation-Learning-for-Neck-Ultrasound-Scanning-Guidance" class="headerlink" title="Image Retrieval with Intra-Sweep Representation Learning for Neck   Ultrasound Scanning Guidance"></a>Image Retrieval with Intra-Sweep Representation Learning for Neck   Ultrasound Scanning Guidance</h2><p><strong>Authors:Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E. Salcudean</strong></p>
<p>Purpose: Intraoperative ultrasound (US) can enhance real-time visualization in transoral robotic surgery. The surgeon creates a mental map with a pre-operative scan. Then, a surgical assistant performs freehand US scanning during the surgery while the surgeon operates at the remote surgical console. Communicating the target scanning plane in the surgeonâ€™s mental map is difficult. Automatic image retrieval can help match intraoperative images to preoperative scans, guiding the assistant to adjust the US probe toward the target plane. Methods: We propose a self-supervised contrastive learning approach to match intraoperative US views to a preoperative image database. We introduce a novel contrastive learning strategy that leverages intra-sweep similarity and US probe location to improve feature encoding. Additionally, our model incorporates a flexible threshold to reject unsatisfactory matches. Results: Our method achieves 92.30% retrieval accuracy on simulated data and outperforms state-of-the-art temporal-based contrastive learning approaches. Our ablation study demonstrates that using probe location in the optimization goal improves image representation, suggesting that semantic information can be extracted from probe location. We also present our approach on real patient data to show the feasibility of the proposed US probe localization system despite tissue deformation from tongue retraction. Conclusion: Our contrastive learning method, which utilizes intra-sweep similarity and US probe location, enhances US image representation learning. We also demonstrate the feasibility of using our image retrieval method to provide neck US localization on real patient US after tongue retraction. </p>
<blockquote>
<p>ç›®çš„ï¼šæœ¯ä¸­è¶…å£°ï¼ˆUSï¼‰èƒ½å¢å¼ºç»å£æœºå™¨äººæ‰‹æœ¯çš„å®æ—¶å¯è§†åŒ–æ•ˆæœã€‚å¤–ç§‘åŒ»ç”Ÿé€šè¿‡æœ¯å‰æ‰«æå»ºç«‹å¿ƒç†åœ°å›¾ã€‚ç„¶åï¼Œæ‰‹æœ¯åŠ©ç†åœ¨æ‰‹æœ¯ä¸­æ‰§è¡Œè‡ªç”±æ‰‹è¶…å£°æ‰«æï¼Œè€Œå¤–ç§‘åŒ»ç”Ÿåˆ™åœ¨è¿œç¨‹æ‰‹æœ¯å°ä¸Šæ“ä½œã€‚åœ¨åŒ»ç”Ÿçš„å¿ƒä¸­ä¼ è¾¾ç›®æ ‡æ‰«æå¹³é¢æ˜¯å¾ˆå›°éš¾çš„ã€‚è‡ªåŠ¨å›¾åƒæ£€ç´¢æŠ€æœ¯å¯ä»¥å¸®åŠ©åŒ¹é…æœ¯ä¸­å›¾åƒä¸æœ¯å‰æ‰«æç»“æœï¼ŒæŒ‡å¯¼åŠ©ç†è°ƒæ•´è¶…å£°æ¢å¤´è‡³ç›®æ ‡å¹³é¢ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æœ¯ä¸­è¶…å£°è§†å›¾ä¸æœ¯å‰å›¾åƒæ•°æ®åº“åŒ¹é…çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨å•æ¬¡æ‰«æå†…çš„ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®ï¼Œæ”¹å–„ç‰¹å¾ç¼–ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨çµæ´»çš„é˜ˆå€¼æ¥æ‹’ç»ä¸æ»¡æ„çš„åŒ¹é…ç»“æœã€‚ç»“æœï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šå®ç°äº†92.30%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œå¹¶è¶…è¶Šäº†åŸºäºæ—¶é—´å¯¹æ¯”å­¦ä¹ çš„å‰æ²¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯æ˜ï¼Œåœ¨ä¼˜åŒ–ç›®æ ‡ä¸­ä½¿ç”¨æ¢å¤´ä½ç½®å¯ä»¥æ”¹å–„å›¾åƒè¡¨ç¤ºï¼Œè¿™è¡¨æ˜å¯ä»¥ä»æ¢å¤´ä½ç½®ä¸­æå–è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨çœŸå®æ‚£è€…æ•°æ®ä¸Šåº”ç”¨æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»¥å±•ç¤ºå³ä½¿åœ¨èˆŒå¤´å›ç¼©å¼•èµ·çš„ç»„ç»‡å˜å½¢æƒ…å†µä¸‹ï¼Œæ‰€æå‡ºçš„è¶…å£°æ¢å¤´å®šä½ç³»ç»Ÿä¹Ÿæ˜¯å¯è¡Œçš„ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•åˆ©ç”¨å•æ¬¡æ‰«æå†…çš„ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®ï¼Œæé«˜äº†è¶…å£°å›¾åƒè¡¨ç¤ºå­¦ä¹ æ•ˆæœã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†ä½¿ç”¨æˆ‘ä»¬çš„å›¾åƒæ£€ç´¢æ–¹æ³•åœ¨çœŸå®æ‚£è€…è¶…å£°ä¸­å®ç°é¢ˆéƒ¨è¶…å£°å®šä½çš„å¯è¡Œæ€§ï¼Œå³ä½¿å­˜åœ¨èˆŒå¤´å›ç¼©çš„æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07741v1">PDF</a> 12 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç»å£æœºå™¨äººæ‰‹æœ¯ä¸­ï¼Œå¦‚ä½•åˆ©ç”¨æœ¯ä¸­è¶…å£°ï¼ˆUSï¼‰å¢å¼ºå®æ—¶å¯è§†åŒ–ã€‚æå‡ºä¸€ç§è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå°†æœ¯ä¸­è¶…å£°è§†å›¾ä¸æœ¯å‰å›¾åƒæ•°æ®åº“è¿›è¡ŒåŒ¹é…ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æ‰«æŸ¥å†…çš„ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®æ”¹è¿›ç‰¹å¾ç¼–ç ï¼Œå®ç°92.3%çš„æ¨¡æ‹Ÿæ•°æ®æ£€ç´¢å‡†ç¡®ç‡ï¼Œå¹¶å±•ç¤ºäº†åœ¨çœŸå®æ‚£è€…æ•°æ®ä¸Šçš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¯ä¸­è¶…å£°å¯å¢å¼ºç»å£æœºå™¨äººæ‰‹æœ¯ä¸­çš„å®æ—¶å¯è§†åŒ–ã€‚</li>
<li>æœ¯å‰æ‰«æå¸®åŠ©å¤–ç§‘åŒ»ç”Ÿå½¢æˆå¿ƒç†åœ°å›¾ã€‚</li>
<li>æœ¯ä¸­å›¾åƒä¸æœ¯å‰æ‰«æçš„åŒ¹é…å¯é€šè¿‡è‡ªåŠ¨å›¾åƒæ£€ç´¢å®ç°ï¼ŒæŒ‡å¼•åŠ©ç†è°ƒæ•´è¶…å£°æ¢å¤´è‡³ç›®æ ‡å¹³é¢ã€‚</li>
<li>å¼•å…¥è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ŒåŒ¹é…æœ¯ä¸­è¶…å£°è§†å›¾ä¸æœ¯å‰å›¾åƒæ•°æ®åº“ã€‚</li>
<li>é‡‡ç”¨äº†æ–°é¢–çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æ‰«æŸ¥å†…çš„ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®æ”¹å–„ç‰¹å¾ç¼–ç ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†æ¨¡æ‹Ÿæ•°æ®ä¸Š92.3%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¸”ä¼˜äºåŸºäºæ—¶é—´å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-18d2e81de84ffc241cab2130930dfd3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e03e141bb85ffedbb4ff855fe4af47ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b0fb40e0ec865d273e5ffdbebf76bdc.jpg" align="middle">
</details>




<h2 id="SKIPNet-Spatial-Attention-Skip-Connections-for-Enhanced-Brain-Tumor-Classification"><a href="#SKIPNet-Spatial-Attention-Skip-Connections-for-Enhanced-Brain-Tumor-Classification" class="headerlink" title="SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor   Classification"></a>SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor   Classification</h2><p><strong>Authors:Khush Mendiratta, Shweta Singh, Pratik Chattopadhyay</strong></p>
<p>Early detection of brain tumors through magnetic resonance imaging (MRI) is essential for timely treatment, yet access to diagnostic facilities remains limited in remote areas. Gliomas, the most common primary brain tumors, arise from the carcinogenesis of glial cells in the brain and spinal cord, with glioblastoma patients having a median survival time of less than 14 months. MRI serves as a non-invasive and effective method for tumor detection, but manual segmentation of brain MRI scans has traditionally been a labor-intensive task for neuroradiologists. Recent advancements in computer-aided design (CAD), machine learning (ML), and deep learning (DL) offer promising solutions for automating this process. This study proposes an automated deep learning model for brain tumor detection and classification using MRI data. The model, incorporating spatial attention, achieved 96.90% accuracy, enhancing the aggregation of contextual information for better pattern recognition. Experimental results demonstrate that the proposed approach outperforms baseline models, highlighting its robustness and potential for advancing automated MRI-based brain tumor analysis. </p>
<blockquote>
<p>é€šè¿‡ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ—©æœŸæ£€æµ‹è„‘è‚¿ç˜¤å¯¹äºåŠæ—¶æ²»ç–—è‡³å…³é‡è¦ï¼Œä½†åœ¨åè¿œåœ°åŒºï¼Œè¯Šæ–­è®¾å¤‡çš„å¯åŠæ€§ä»ç„¶æœ‰é™ã€‚èƒ¶è´¨ç˜¤æ˜¯æœ€å¸¸è§çš„åŸå‘æ€§è„‘è‚¿ç˜¤ï¼Œèµ·æºäºå¤§è„‘å’Œè„Šé«“èƒ¶è´¨ç»†èƒçš„ç™Œå˜ï¼Œå…¶ä¸­èƒ¶è´¨æ¯ç»†èƒç˜¤æ‚£è€…çš„ä¸­ä½ç”Ÿå­˜æ—¶é—´ä¸åˆ°14ä¸ªæœˆã€‚MRIä½œä¸ºä¸€ç§éä¾µå…¥æ€§çš„æœ‰æ•ˆè‚¿ç˜¤æ£€æµ‹æ–¹æ³•ï¼Œä½†æ‰‹åŠ¨åˆ†å‰²è„‘éƒ¨MRIæ‰«æå¯¹ç¥ç»æ”¾å°„ç§‘åŒ»ç”Ÿæ¥è¯´ä¸€ç›´æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹çš„ä»»åŠ¡ã€‚è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ã€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨MRIæ•°æ®è‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»è„‘è‚¿ç˜¤çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¾¾åˆ°äº†96.90%çš„å‡†ç¡®ç‡ï¼Œé€šè¿‡å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èšé›†æ¥å¢å¼ºæ¨¡å¼è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨æ¨è¿›åŸºäºMRIçš„è‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†æçš„ç¨³å¥æ€§å’Œæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07736v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨æ—©æœŸé€šè¿‡ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ£€æµ‹è„‘è‚¿ç˜¤çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—èµ„æºæœ‰é™çš„åè¿œåœ°åŒºã€‚é’ˆå¯¹èƒ¶è´¨ç»†èƒç™Œå¼•å‘çš„å¸¸è§è„‘è‚¿ç˜¤â€”â€”èƒ¶è´¨ç˜¤ï¼Œæ–‡ç« ä»‹ç»äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»è„‘è‚¿ç˜¤çš„æœ€æ–°è¿›å±•ã€‚è¯¥æ¨¡å‹ç»“åˆç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡†ç¡®ç‡è¾¾åˆ°96.90%ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°èšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥è¯†åˆ«æ¨¡å¼ï¼Œä¸ºåŸºäºMRIçš„è„‘è‚¿ç˜¤åˆ†ææä¾›äº†å…ˆè¿›è‡ªåŠ¨åŒ–çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—©æœŸæ£€æµ‹å¯¹åŠæ—¶æ²»ç–—å’Œæ”¹å–„è„‘è‚¿ç˜¤æ‚£è€…ç”Ÿå­˜ç‡è‡³å…³é‡è¦ã€‚</li>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯æ£€æµ‹è„‘è‚¿ç˜¤çš„éä¾µå…¥æ€§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿæ‰‹åŠ¨åˆ†å‰²MRIæ‰«æå›¾åƒå¯¹ç¥ç»æ”¾å°„ç§‘åŒ»ç”Ÿè€Œè¨€æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ã€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè‡ªåŠ¨åŒ–åˆ†å‰²æä¾›äº†æœºä¼šã€‚</li>
<li>æå‡ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“åˆäº†ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†é«˜è¾¾96.90%çš„å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°èšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥è¿›è¡Œæ¨¡å¼è¯†åˆ«ï¼Œä»è€Œæé«˜äº†è„‘è‚¿ç˜¤æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3c99d08fc951c00a0b627c9bbf7fb77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0bbece77157bb3cfa7ed04a99c0aa2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d0fb6e1edc2df6ac77b6a62072d3488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef8978440c1a86f4c1714418fbdb4a15.jpg" align="middle">
</details>




<h2 id="Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model"><a href="#Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model" class="headerlink" title="Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model"></a>Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Jianfeng Guo, Feng Yang, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Motion artifacts present in magnetic resonance imaging (MRI) can seriously interfere with clinical diagnosis. Removing motion artifacts is a straightforward solution and has been extensively studied. However, paired data are still heavily relied on in recent works and the perturbations in k-space (frequency domain) are not well considered, which limits their applications in the clinical field. To address these issues, we propose a novel unsupervised purification method which leverages pixel-frequency information of noisy MRI images to guide a pre-trained diffusion model to recover clean MRI images. Specifically, considering that motion artifacts are mainly concentrated in high-frequency components in k-space, we utilize the low-frequency components as the guide to ensure correct tissue textures. Additionally, given that high-frequency and pixel information are helpful for recovering shape and detail textures, we design alternate complementary masks to simultaneously destroy the artifact structure and exploit useful information. Quantitative experiments are performed on datasets from different tissues and show that our method achieves superior performance on several metrics. Qualitative evaluations with radiologists also show that our method provides better clinical feedback. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD">https://github.com/medcx/PFAD</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šä¸¥é‡å¹²æ‰°ä¸´åºŠè¯Šæ–­ã€‚å»é™¤è¿åŠ¨ä¼ªå½±æ˜¯ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶ä»ç„¶ä¸¥é‡ä¾èµ–é…å¯¹æ•°æ®ï¼Œè€Œkç©ºé—´ï¼ˆé¢‘ç‡åŸŸï¼‰ä¸­çš„æ‰°åŠ¨å¹¶æœªå¾—åˆ°å¾ˆå¥½çš„è€ƒè™‘ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠé¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å™ªå£°MRIå›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´çš„MRIå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œè€ƒè™‘åˆ°è¿åŠ¨ä¼ªå½±ä¸»è¦é›†ä¸­åœ¨kç©ºé—´çš„é«˜é¢‘æˆåˆ†ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ã€‚æ­¤å¤–ï¼Œé‰´äºé«˜é¢‘å’Œåƒç´ ä¿¡æ¯æœ‰åŠ©äºæ¢å¤å½¢çŠ¶å’Œç»†èŠ‚çº¹ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†äº¤æ›¿çš„äº’è¡¥æ©è†œæ¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶åˆ©ç”¨æœ‰ç”¨ä¿¡æ¯ã€‚åœ¨ä¸åŒç»„ç»‡æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„å®šæ€§è¯„ä¼°ä¹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æ›´å¥½çš„ä¸´åºŠåé¦ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/medcx/PFADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07590v2">PDF</a> 12 pages, 8 figures, AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å™ªå£°MRIå›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯å¼•å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´MRIå›¾åƒï¼Œä»¥è§£å†³ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨ä¼ªå½±é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä½é¢‘é¢‘è°±æˆåˆ†ä½œä¸ºæŒ‡å—ï¼Œç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ï¼Œå¹¶é€šè¿‡äº¤æ›¿äº’è¡¥æ©è†œåŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè·å¾—äº†æ”¾å°„ç§‘åŒ»ç”Ÿçš„å¥½è¯„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šå¹²æ‰°ä¸´åºŠè¯Šæ–­ï¼Œå»é™¤ä¼ªå½±æ˜¯é‡è¦ä¸”è¢«å¹¿æ³›ç ”ç©¶çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»ä¾èµ–é…å¯¹æ•°æ®ï¼Œä¸”åœ¨kç©ºé—´ï¼ˆé¢‘ç‡åŸŸï¼‰çš„æ‰°åŠ¨æœªå¾—åˆ°å……åˆ†è€ƒè™‘ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠçš„åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨åƒç´ é¢‘ç‡ä¿¡æ¯å»é™¤MRIå›¾åƒä¸­çš„è¿åŠ¨ä¼ªå½±ã€‚</li>
<li>æ–¹æ³•ä¸»è¦åˆ©ç”¨ä½é¢‘é¢‘è°±æˆåˆ†ä½œä¸ºæŒ‡å—ï¼Œç¡®ä¿æ¢å¤çš„ç»„ç»‡çº¹ç†æ­£ç¡®ã€‚</li>
<li>é€šè¿‡è®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œï¼Œè¯¥æ–¹æ³•èƒ½åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨çš„å›¾åƒä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸æ¯”ï¼Œæä¾›äº†æ›´å¥½çš„ä¸´åºŠåé¦ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e0ff7c13591035b978b06d36f1f533ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f99b9342819d5c3237e084dec6be6f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cfd4272ea20d1f177b432a0446e8119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47ef4c87738d300c706dc2cbf4506649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8186300dbbe1c884fae5bbebd2c36e3a.jpg" align="middle">
</details>




<h2 id="CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings"><a href="#CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings" class="headerlink" title="CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings"></a>CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings</h2><p><strong>Authors:Jiazuo Mu, Fuyi Yang, Yanshun Zhang, Junxiong Zhang, Yongjian Luo, Lan Xu, Yujiao Shi, Jingyi Yu, Yingliang Zhang</strong></p>
<p>We introduce CADSpotting, an efficient method for panoptic symbol spotting in large-scale architectural CAD drawings. Existing approaches struggle with the diversity of symbols, scale variations, and overlapping elements in CAD designs. CADSpotting overcomes these challenges by representing each primitive with dense points instead of a single primitive point, described by essential attributes like coordinates and color. Building upon a unified 3D point cloud model for joint semantic, instance, and panoptic segmentation, CADSpotting learns robust feature representations. To enable accurate segmentation in large, complex drawings, we further propose a novel Sliding Window Aggregation (SWA) technique, combining weighted voting and Non-Maximum Suppression (NMS). Moreover, we introduce a large-scale CAD dataset named LS-CAD to support our experiments. Each floorplan in LS-CAD has an average coverage of 1,000 square meter(versus 100 square meter in the existing dataset), providing a valuable benchmark for symbol spotting research. Experimental results on FloorPlanCAD and LS-CAD datasets demonstrate that CADSpotting outperforms existing methods, showcasing its robustness and scalability for real-world CAD applications. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CADSpottingï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§è§„æ¨¡å»ºç­‘CADå›¾çº¸ä¸­è¿›è¡Œå…¨æ™¯ç¬¦å·è¯†åˆ«çš„é«˜æ•ˆæ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹CADè®¾è®¡ä¸­çš„ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œå…ƒç´ é‡å ç­‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚CADSpottingé€šè¿‡ç”¨å¯†é›†ç‚¹è¡¨ç¤ºæ¯ä¸ªåŸºæœ¬ä½“è€Œä¸æ˜¯å•ä¸ªåŸºæœ¬ç‚¹æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œè¿™äº›å¯†é›†ç‚¹ç”±åæ ‡å’Œé¢œè‰²ç­‰åŸºæœ¬å±æ€§æè¿°ã€‚åŸºäºç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹è¿›è¡Œè”åˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ï¼ŒCADSpottingå­¦ä¹ é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†åœ¨å¤§è§„æ¨¡å¤æ‚å›¾çº¸ä¸­å®ç°å‡†ç¡®åˆ†å‰²ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†åŠ æƒæŠ•ç¥¨å’Œéæœ€å¤§æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡CADæ•°æ®é›†LS-CADæ¥æ”¯æŒæˆ‘ä»¬çš„å®éªŒã€‚LS-CADä¸­çš„æ¯ä¸ªå¹³é¢å›¾å¹³å‡è¦†ç›–é¢ç§¯ä¸º1000å¹³æ–¹ç±³ï¼ˆè€Œç°æœ‰æ•°æ®é›†ä¸­çš„å¹³é¢å›¾è¦†ç›–é¢ç§¯ä¸º100å¹³æ–¹ç±³ï¼‰ï¼Œä¸ºç¬¦å·è¯†åˆ«ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åŸºå‡†ã€‚åœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCADSpottingä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•ŒCADåº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07377v2">PDF</a> 16pages, 12 figures, Project web-page:   <a target="_blank" rel="noopener" href="https://dgeneai.github.io/cadspotting-pages/">https://dgeneai.github.io/cadspotting-pages/</a></p>
<p><strong>Summary</strong></p>
<p>CADSpottingæ–¹æ³•ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡å»ºç­‘CADå›¾çº¸çš„å…¨è§†ç¬¦å·è¯†åˆ«æŠ€æœ¯ã€‚å®ƒå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’ŒCADè®¾è®¡å…ƒç´ é‡å æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯†é›†ç‚¹è¡¨ç¤ºæ¯ä¸ªåŸºæœ¬å…ƒç´ ï¼Œç»“åˆåæ ‡å’Œé¢œè‰²ç­‰å…³é”®å±æ€§è¿›è¡Œæè¿°ï¼Œå¹¶åœ¨ç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹ä¸Šè¿›è¡Œè”åˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨è§†åˆ†å‰²ï¼Œå­¦ä¹ ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œç»“åˆåŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰æ–¹æ³•ï¼Œç¡®ä¿åœ¨å¤§è§„æ¨¡å¤æ‚å›¾çº¸ä¸­çš„ç²¾ç¡®åˆ†å‰²ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†å¤§è§„æ¨¡çš„CADæ•°æ®é›†LS-CADæ¥æ”¯æŒå®éªŒï¼Œä¸ºç¬¦å·è¯†åˆ«ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCADSpottingåœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºå…¶åœ¨çœŸå®ä¸–ç•ŒCADåº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CADSpottingæ˜¯ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡å»ºç­‘CADå›¾çº¸çš„é«˜æ•ˆå…¨è§†ç¬¦å·è¯†åˆ«æ–¹æ³•ã€‚</li>
<li>å®ƒé€šè¿‡å¯†é›†ç‚¹è¡¨ç¤ºæ¯ä¸ªåŸºæœ¬å…ƒç´ ï¼Œä»¥åº”å¯¹ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œé‡å å…ƒç´ çš„é—®é¢˜ã€‚</li>
<li>CADSpottingåœ¨ç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹ä¸Šè¿›è¡Œè”åˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨è§†åˆ†å‰²ï¼Œå­¦ä¹ ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œç»“åˆåŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ï¼Œæé«˜åœ¨å¤§è§„æ¨¡å¤æ‚å›¾çº¸ä¸­çš„åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥äº†LS-CADæ•°æ®é›†ï¼Œä¸ºç¬¦å·è¯†åˆ«ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„åŸºå‡†ã€‚</li>
<li>CADSpottingåœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6acfd33441396571604c8d9789dc5258.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13bde01420f845dd6e0b6e98dc30dc13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ea845c18501410b9494f3a2279825f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74183a67bf019daaf73b322cbdd87af4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf0696eeb6c88313f5a301ce4341f201.jpg" align="middle">
</details>




<h2 id="A-Generative-Victim-Model-for-Segmentation"><a href="#A-Generative-Victim-Model-for-Segmentation" class="headerlink" title="A Generative Victim Model for Segmentation"></a>A Generative Victim Model for Segmentation</h2><p><strong>Authors:Aixuan Li, Jing Zhang, Jiawei Shi, Yiran Zhong, Yuchao Dai</strong></p>
<p>We find that the well-trained victim models (VMs), against which the attacks are generated, serve as fundamental prerequisites for adversarial attacks, i.e. a segmentation VM is needed to generate attacks for segmentation. In this context, the victim model is assumed to be robust to achieve effective adversarial perturbation generation. Instead of focusing on improving the robustness of the task-specific victim models, we shift our attention to image generation. From an image generation perspective, we derive a novel VM for segmentation, aiming to generate adversarial perturbations for segmentation tasks without requiring models explicitly designed for image segmentation. Our approach to adversarial attack generation diverges from conventional white-box or black-box attacks, offering a fresh outlook on adversarial attack strategies. Experiments show that our attack method is able to generate effective adversarial attacks with good transferability. </p>
<blockquote>
<p>æˆ‘ä»¬å‘ç°ï¼Œé’ˆå¯¹ç”Ÿæˆçš„æ”»å‡»è®­ç»ƒè‰¯å¥½çš„å—å®³è€…æ¨¡å‹ï¼ˆVMsï¼‰æ˜¯å¯¹æŠ—æ”»å‡»çš„åŸºæœ¬å‰æã€‚ä¾‹å¦‚ï¼Œå¯¹äºåˆ†å‰²æ”»å‡»ï¼Œéœ€è¦ä¸€ä¸ªåˆ†å‰²çš„VMæ¥ç”Ÿæˆæ”»å‡»ã€‚åœ¨æ­¤æƒ…å†µä¸‹ï¼Œå‡è®¾å—å®³è€…æ¨¡å‹å…·æœ‰é²æ£’æ€§ä»¥å®ç°æœ‰æ•ˆçš„å¯¹æŠ—æ‰°åŠ¨ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„ç ”ç©¶ä¸“æ³¨äºæé«˜é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å—å®³è€…æ¨¡å‹çš„é²æ£’æ€§ä¸åŒï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘äº†å›¾åƒç”Ÿæˆã€‚ä»å›¾åƒç”Ÿæˆçš„è§’åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹VMç”¨äºåˆ†å‰²ä»»åŠ¡ï¼Œæ—¨åœ¨é’ˆå¯¹åˆ†å‰²ä»»åŠ¡ç”Ÿæˆå¯¹æŠ—æ‰°åŠ¨ï¼Œæ— éœ€ä½¿ç”¨ä¸“ä¸ºå›¾åƒåˆ†å‰²è®¾è®¡çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å¯¹æŠ—æ€§æ”»å‡»ç”Ÿæˆæ–¹æ³•ä¸ä¼ ç»Ÿçš„ç™½ç›’æˆ–é»‘ç›’æ”»å‡»æœ‰æ‰€ä¸åŒï¼Œä¸ºå¯¹æŠ—æ€§æ”»å‡»ç­–ç•¥æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”»å‡»æ–¹æ³•å¯ä»¥ç”Ÿæˆæœ‰æ•ˆä¸”å…·æœ‰è‰¯å¥½è¿ç§»æ€§çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07274v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶å‘ç°é’ˆå¯¹å¯¹æŠ—æ”»å‡»ç”Ÿæˆï¼Œéœ€è¦è®­ç»ƒè‰¯å¥½çš„å—å®³è€…æ¨¡å‹ï¼ˆVMsï¼‰ã€‚è¿™äº›VMså¯¹äºç”Ÿæˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¯¹æŠ—æ”»å‡»è‡³å…³é‡è¦ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹VMç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆé’ˆå¯¹åˆ†å‰²ä»»åŠ¡çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œæ— éœ€ä¸“é—¨è®¾è®¡å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚æœ¬ç ”ç©¶ä¸­çš„æ”»å‡»ç”Ÿæˆæ–¹æ³•ä¸ä¼ ç»Ÿçš„ç™½ç›’æˆ–é»‘ç›’æ”»å‡»ä¸åŒï¼Œä¸ºå¯¹æŠ—æ”»å‡»ç­–ç•¥æä¾›äº†æ–°æ€è·¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æœ‰æ•ˆæ€§å’Œè‰¯å¥½è¿ç§»æ€§çš„å¯¹æŠ—æ”»å‡»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å—å®³è€…æ¨¡å‹ï¼ˆVMsï¼‰åœ¨ç”Ÿæˆå¯¹æŠ—æ”»å‡»ä¸­èµ·åˆ°åŸºç¡€å…ˆå†³æ¡ä»¶çš„ä½œç”¨ã€‚</li>
<li>é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰çš„å¯¹æŠ—æ”»å‡»ç”Ÿæˆéœ€è¦ç›¸åº”çš„ä»»åŠ¡ç‰¹å®šVMsã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹VMï¼Œç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œæ— éœ€ä¸“é—¨è®¾è®¡å›¾åƒåˆ†å‰²æ¨¡å‹å³å¯ç”Ÿæˆå¯¹æŠ—æ‰°åŠ¨ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸­çš„æ”»å‡»ç”Ÿæˆæ–¹æ³•ä¸ä¼ ç»Ÿçš„ç™½ç›’å’Œé»‘ç›’æ”»å‡»ä¸åŒã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ”»å‡»å…·æœ‰æœ‰æ•ˆæ€§å’Œè‰¯å¥½çš„è¿ç§»æ€§ã€‚</li>
<li>ç ”ç©¶å°†æ³¨æ„åŠ›ä»æé«˜ä»»åŠ¡ç‰¹å®šå—å®³è€…æ¨¡å‹çš„é²æ£’æ€§è½¬ç§»åˆ°å›¾åƒç”Ÿæˆä¸Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3386373274bf35b74341744960889772.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae93012c3db0e323385522d7b3550f2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3678047a7eb8aede78369e8eb634db0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57c2921f2d7c470cb1cbea7d56450694.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-322c4067ff17c395b45308f2941b0df3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f906aab71a7bcfa08a499e86065affa.jpg" align="middle">
</details>




<h2 id="MPSI-Mamba-enhancement-model-for-pixel-wise-sequential-interaction-Image-Super-Resolution"><a href="#MPSI-Mamba-enhancement-model-for-pixel-wise-sequential-interaction-Image-Super-Resolution" class="headerlink" title="MPSI: Mamba enhancement model for pixel-wise sequential interaction   Image Super-Resolution"></a>MPSI: Mamba enhancement model for pixel-wise sequential interaction   Image Super-Resolution</h2><p><strong>Authors:Yuchun He, Yuhan He</strong></p>
<p>Single image super-resolution (SR) has long posed a challenge in the field of computer vision. While the advent of deep learning has led to the emergence of numerous methods aimed at tackling this persistent issue, the current methodologies still encounter challenges in modeling long sequence information, leading to limitations in effectively capturing the global pixel interactions. To tackle this challenge and achieve superior SR outcomes, we propose the Mamba pixel-wise sequential interaction network (MPSI), aimed at enhancing the establishment of long-range connections of information, particularly focusing on pixel-wise sequential interaction. We propose the Channel-Mamba Block (CMB) to capture comprehensive pixel interaction information by effectively modeling long sequence information. Moreover, in the existing SR methodologies, there persists the issue of the neglect of features extracted by preceding layers, leading to the loss of valuable feature information. While certain existing models strive to preserve these features, they frequently encounter difficulty in establishing connections across all layers. To overcome this limitation, MPSI introduces the Mamba channel recursion module (MCRM), which maximizes the retention of valuable feature information from early layers, thereby facilitating the acquisition of pixel sequence interaction information from multiple-level layers. Through extensive experimentation, we demonstrate that MPSI outperforms existing super-resolution methods in terms of image reconstruction results, attaining state-of-the-art performance. </p>
<blockquote>
<p>å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ çš„å‡ºç°å¯¼è‡´å‡ºç°äº†è®¸å¤šæ—¨åœ¨è§£å†³è¿™ä¸€æŒä¹…æ€§é—®é¢˜çš„æ–¹æ³•ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨å»ºæ¨¡é•¿åºåˆ—ä¿¡æ¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´åœ¨æœ‰æ•ˆæ•è·å…¨å±€åƒç´ äº¤äº’æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜å¹¶å®ç°æ›´ä¼˜è¶Šçš„è¶…åˆ†è¾¨ç‡ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†Mambaåƒç´ çº§é¡ºåºäº¤äº’ç½‘ç»œï¼ˆMPSIï¼‰ï¼Œæ—¨åœ¨å¢å¼ºé•¿è·ç¦»ä¿¡æ¯è¿æ¥çš„å»ºç«‹ï¼Œç‰¹åˆ«ä¾§é‡äºåƒç´ çº§çš„é¡ºåºäº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†Channel-Mamba Blockï¼ˆCMBï¼‰ï¼Œé€šè¿‡æœ‰æ•ˆåœ°å¯¹é•¿åºåˆ—ä¿¡æ¯è¿›è¡Œå»ºæ¨¡æ¥æ•æ‰å…¨é¢çš„åƒç´ äº¤äº’ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œåœ¨ç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ä¸­ï¼Œä»å­˜åœ¨å¿½è§†å…ˆå‰å±‚æå–çš„ç‰¹å¾çš„é—®é¢˜ï¼Œå¯¼è‡´æœ‰ä»·å€¼çš„ç‰¹å¾ä¿¡æ¯ä¸¢å¤±ã€‚è™½ç„¶æŸäº›ç°æœ‰æ¨¡å‹åŠªåŠ›ä¿ç•™è¿™äº›ç‰¹å¾ï¼Œä½†å®ƒä»¬ç»å¸¸åœ¨å»ºç«‹è·¨æ‰€æœ‰å±‚çš„è¿æ¥æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼ŒMPSIå¼•å…¥äº†Mambaé€šé“é€’å½’æ¨¡å—ï¼ˆMCRMï¼‰ï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™æ—©æœŸå±‚çš„å®è´µç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œä¾¿äºä»å¤šå±‚è·å–åƒç´ åºåˆ—äº¤äº’ä¿¡æ¯ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜MPSIåœ¨å›¾åƒé‡å»ºç»“æœæ–¹é¢ä¼˜äºç°æœ‰è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07222v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨å¤„ç†å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é—®é¢˜ä¸Šä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å»ºæ¨¡é•¿åºåˆ—ä¿¡æ¯å’Œå…¨å±€åƒç´ äº¤äº’æ–¹é¢çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Mambaåƒç´ çº§é¡ºåºäº¤äº’ç½‘ç»œï¼ˆMPSIï¼‰ï¼Œæ—¨åœ¨å¢å¼ºé•¿è·ç¦»ä¿¡æ¯è¿æ¥ï¼Œå°¤å…¶æ³¨é‡åƒç´ çº§çš„é¡ºåºäº¤äº’ã€‚é€šè¿‡å¼•å…¥Channel-Mamba Blockï¼ˆCMBï¼‰å’ŒMambaé€šé“é€’å½’æ¨¡å—ï¼ˆMCRMï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åƒç´ äº¤äº’ä¿¡æ¯ï¼Œå¹¶ä¿ç•™æ—©æœŸå±‚æå–çš„ç‰¹å¾ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒMPSIåœ¨å›¾åƒé‡å»ºç»“æœä¸Šä¼˜äºç°æœ‰è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ·±åº¦å­¦ä¹ åœ¨å¤„ç†å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é—®é¢˜æ—¶é¢ä¸´å»ºæ¨¡é•¿åºåˆ—ä¿¡æ¯å’Œå…¨å±€åƒç´ äº¤äº’çš„æŒ‘æˆ˜ã€‚</li>
<li>Mambaåƒç´ çº§é¡ºåºäº¤äº’ç½‘ç»œï¼ˆMPSIï¼‰æ—¨åœ¨å¢å¼ºé•¿è·ç¦»ä¿¡æ¯è¿æ¥ï¼Œæ³¨é‡åƒç´ çº§çš„é¡ºåºäº¤äº’ã€‚</li>
<li>Channel-Mamba Blockï¼ˆCMBï¼‰èƒ½æœ‰æ•ˆæ•æ‰åƒç´ äº¤äº’ä¿¡æ¯ï¼Œé€šè¿‡å»ºæ¨¡é•¿åºåˆ—ä¿¡æ¯å®ç°ã€‚</li>
<li>ç°æœ‰SRæ–¹æ³•å¿½è§†äº†æ—©æœŸå±‚æå–çš„ç‰¹å¾ä¿¡æ¯ï¼Œå¯¼è‡´æœ‰ä»·å€¼çš„ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>MPSIé€šè¿‡å¼•å…¥Mambaé€šé“é€’å½’æ¨¡å—ï¼ˆMCRMï¼‰å…‹æœè¿™ä¸€å±€é™ï¼Œä¿ç•™æ—©æœŸå±‚çš„ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>MPSIåœ¨å›¾åƒé‡å»ºç»“æœä¸Šä¼˜äºç°æœ‰è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5a352a32aa00db9a1913158fbd7833f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f12d7e5520bff972f6dd7327efd90dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1922ec5ad44f6e0243a03e6c8b2340bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7672402262b46eddefd095e76fc27a7b.jpg" align="middle">
</details>




<h2 id="Integrating-MedCLIP-and-Cross-Modal-Fusion-for-Automatic-Radiology-Report-Generation"><a href="#Integrating-MedCLIP-and-Cross-Modal-Fusion-for-Automatic-Radiology-Report-Generation" class="headerlink" title="Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology   Report Generation"></a>Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology   Report Generation</h2><p><strong>Authors:Qianhao Han, Junyi Liu, Zengchang Qin, Zheng Zheng</strong></p>
<p>Automating radiology report generation can significantly reduce the workload of radiologists and enhance the accuracy, consistency, and efficiency of clinical documentation.We propose a novel cross-modal framework that uses MedCLIP as both a vision extractor and a retrieval mechanism to improve the process of medical report generation.By extracting retrieved report features and image features through an attention-based extract module, and integrating them with a fusion module, our method improves the coherence and clinical relevance of generated reports.Experimental results on the widely used IU-Xray dataset demonstrate the effectiveness of our approach, showing improvements over commonly used methods in both report quality and relevance.Additionally, ablation studies provide further validation of the framework, highlighting the importance of accurate report retrieval and feature integration in generating comprehensive medical reports. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¯ä»¥æ˜¾è‘—å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œæé«˜ä¸´åºŠæ–‡æ¡£çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ¡†æ¶ï¼Œä½¿ç”¨MedCLIPä½œä¸ºè§†è§‰æå–å™¨å’Œæ£€ç´¢æœºåˆ¶ï¼Œä»¥æ”¹è¿›åŒ»ç–—æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡æ³¨æ„åŠ›åŸºç¡€çš„æå–æ¨¡å—æå–æ£€ç´¢æŠ¥å‘Šç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸èåˆæ¨¡å—é›†æˆï¼Œä»è€Œæé«˜ç”ŸæˆæŠ¥å‘Šçš„ä¸€è‡´æ€§ä¸´åºŠç›¸å…³æ€§ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„IU-Xrayæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æŠ¥å‘Šè´¨é‡å’Œç›¸å…³æ€§æ–¹é¢éƒ½ä¼˜äºå¸¸ç”¨æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†å‡†ç¡®æŠ¥å‘Šæ£€ç´¢å’Œç‰¹å¾èåˆåœ¨ç”Ÿæˆå…¨é¢åŒ»ç–—æŠ¥å‘Šä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07141v1">PDF</a> Accepted in IEEE Big Data 2024</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆèƒ½æ˜¾è‘—å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œæé«˜ä¸´åºŠè®°å½•çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ•ˆç‡ã€‚æå‡ºä¸€ç§æ–°å‹è·¨æ¨¡æ€æ¡†æ¶ï¼Œåˆ©ç”¨MedCLIPä½œä¸ºè§†è§‰æå–å™¨å’Œæ£€ç´¢æœºåˆ¶ï¼Œæ”¹è¿›åŒ»å­¦æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡æå–æŠ¥å‘Šç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œç»“åˆèåˆæ¨¡å—ï¼Œæé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„ä¸€è‡´æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„IU-Xrayæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŠ¥å‘Šè´¨é‡å’Œç›¸å…³æ€§æ–¹é¢å‡ä¼˜äºå¸¸ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç”ŸæˆåŒ»å­¦æŠ¥å‘Šèƒ½å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚</li>
<li>æå‡ºçš„è·¨æ¨¡æ€æ¡†æ¶åˆ©ç”¨MedCLIPæŠ€æœ¯æé«˜æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>æ¡†æ¶é€šè¿‡æå–æŠ¥å‘Šå’Œå›¾åƒç‰¹å¾ï¼Œå¢å¼ºæŠ¥å‘Šçš„ä¸€è‡´æ€§ã€‚</li>
<li>èåˆæ¨¡å—æé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„ä¸´åºŠç›¸å…³æ€§ã€‚</li>
<li>åœ¨IU-Xrayæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸å…¶ä»–å¸¸ç”¨æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æŠ¥å‘Šè´¨é‡å’Œç›¸å…³æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7a7c7d28fdcbba352c751fa6b8325a72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21af0fac58a1d895dac0815584c37392.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-679030cb5848059a930bb69fef69e131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27adfe8f337874b6e293db1173a60669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d42d2cde228f05bbffeaafd7376ce0.jpg" align="middle">
</details>




<h2 id="Enhancing-LLMs-for-Impression-Generation-in-Radiology-Reports-through-a-Multi-Agent-System"><a href="#Enhancing-LLMs-for-Impression-Generation-in-Radiology-Reports-through-a-Multi-Agent-System" class="headerlink" title="Enhancing LLMs for Impression Generation in Radiology Reports through a   Multi-Agent System"></a>Enhancing LLMs for Impression Generation in Radiology Reports through a   Multi-Agent System</h2><p><strong>Authors:Fang Zeng, Zhiliang Lyu, Quanzheng Li, Xiang Li</strong></p>
<p>This study introduces â€œRadCouncil,â€ a multi-agent Large Language Model (LLM) framework designed to enhance the generation of impressions in radiology reports from the finding section. RadCouncil comprises three specialized agents: 1) a â€œRetrievalâ€ Agent that identifies and retrieves similar reports from a vector database, 2) a â€œRadiologistâ€ Agent that generates impressions based on the finding section of the given report plus the exemplar reports retrieved by the Retrieval Agent, and 3) a â€œReviewerâ€ Agent that evaluates the generated impressions and provides feedback. The performance of RadCouncil was evaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and qualitative criteria assessed by GPT-4, using chest X-ray as a case study. Experiment results show improvements in RadCouncil over the single-agent approach across multiple dimensions, including diagnostic accuracy, stylistic concordance, and clarity. This study highlights the potential of utilizing multiple interacting LLM agents, each with a dedicated task, to enhance performance in specialized medical tasks and the development of more robust and adaptable healthcare AI solutions. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†â€RadCouncilâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ ¹æ®å‘ç°éƒ¨åˆ†ç”Ÿæˆæ”¾å°„æŠ¥å‘Šçš„å°è±¡ã€‚RadCouncilåŒ…å«ä¸‰ä¸ªä¸“ä¸šä»£ç†ï¼š1ï¼‰â€œæ£€ç´¢â€ä»£ç†ï¼Œç”¨äºä»å‘é‡æ•°æ®åº“ä¸­è¯†åˆ«å’Œæ£€ç´¢ç±»ä¼¼æŠ¥å‘Šï¼›2ï¼‰â€œæ”¾å°„ç§‘åŒ»ç”Ÿâ€ä»£ç†ï¼Œæ ¹æ®ç»™å®šæŠ¥å‘Šçš„å‘ç°éƒ¨åˆ†ä»¥åŠæ£€ç´¢åˆ°çš„ç¤ºä¾‹æŠ¥å‘Šç”Ÿæˆå°è±¡ï¼›3ï¼‰â€œè¯„å®¡â€ä»£ç†ï¼Œè¯„ä¼°ç”Ÿæˆçš„å°è±¡å¹¶æä¾›åé¦ˆã€‚ä½¿ç”¨èƒ¸éƒ¨Xå°„çº¿ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œé€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆBLEUã€ROUGEã€BERTScoreï¼‰å’ŒGPT-4è¯„ä¼°çš„å®šæ€§æ ‡å‡†å¯¹RadCouncilçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•ä»£ç†æ–¹æ³•ç›¸æ¯”ï¼ŒRadCouncilåœ¨å¤šä¸ªç»´åº¦ä¸Šæœ‰æ‰€æ”¹è¿›ï¼ŒåŒ…æ‹¬è¯Šæ–­å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨å¤šä¸ªäº¤äº’LLMä»£ç†çš„æ½œåŠ›ï¼Œæ¯ä¸ªä»£ç†éƒ½æœ‰ç‰¹å®šçš„ä»»åŠ¡ï¼Œä»¥æé«˜åœ¨ç‰¹æ®ŠåŒ»ç–—ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶å¼€å‘æ›´å¼ºå¤§ã€æ›´é€‚åº”çš„åŒ»ç–—ä¿å¥äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†åä¸ºRadCouncilçš„å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æé«˜æ”¾å°„æŠ¥å‘Šå°è±¡ç”Ÿæˆçš„èƒ½åŠ›ã€‚RadCouncilåŒ…å«ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šæ£€ç´¢æ™ºèƒ½ä½“è´Ÿè´£ä»å‘é‡æ•°æ®åº“ä¸­è¯†åˆ«å’Œæ£€ç´¢ç›¸ä¼¼æŠ¥å‘Šï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“åŸºäºç»™å®šæŠ¥å‘Šçš„å‘ç°éƒ¨åˆ†å’Œæ£€ç´¢åˆ°çš„ç¤ºä¾‹æŠ¥å‘Šç”Ÿæˆå°è±¡ï¼Œä»¥åŠè¯„å®¡æ™ºèƒ½ä½“è´Ÿè´£å¯¹ç”Ÿæˆçš„å°è±¡è¿›è¡Œè¯„ä¼°å¹¶æä¾›åé¦ˆã€‚ä½¿ç”¨èƒ¸éƒ¨Xå°„çº¿ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œé€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆBLEUã€ROUGEã€BERTScoreï¼‰å’Œå®šæ€§æ ‡å‡†å¯¹RadCouncilè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•æ™ºèƒ½ä½“æ–¹æ³•ç›¸æ¯”ï¼ŒRadCouncilåœ¨è¯Šæ–­å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ç­‰å¤šä¸ªç»´åº¦éƒ½æœ‰æ‰€æé«˜ã€‚æœ¬ç ”ç©¶çªå‡ºäº†åˆ©ç”¨å¤šä¸ªç›¸äº’ä½œç”¨çš„LLMæ™ºèƒ½ä½“çš„æ½œåŠ›ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰ä¸“é—¨çš„ä»»åŠ¡ï¼Œä»¥æé«˜åœ¨ç‰¹æ®ŠåŒ»ç–—ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶æ¨åŠ¨å¼€å‘æ›´å¼ºå¤§å’Œé€‚åº”æ€§æ›´å¼ºçš„åŒ»ç–—äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadCouncilæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºåŸºäºæ”¾å°„å­¦å‘ç°ç”Ÿæˆçš„æŠ¥å‘Šå°è±¡ã€‚</li>
<li>RadCouncilåŒ…å«ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šæ£€ç´¢æ™ºèƒ½ä½“ã€æ”¾å°„ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“å’Œè¯„å®¡æ™ºèƒ½ä½“ã€‚</li>
<li>æ£€ç´¢æ™ºèƒ½ä½“è´Ÿè´£ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸ä¼¼æŠ¥å‘Šã€‚</li>
<li>æ”¾å°„ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“åŸºäºå‘ç°éƒ¨åˆ†å’Œæ£€ç´¢åˆ°çš„æŠ¥å‘Šç”Ÿæˆå°è±¡ã€‚</li>
<li>è¯„å®¡æ™ºèƒ½ä½“è¯„ä¼°ç”Ÿæˆçš„å°è±¡å¹¶æä¾›åé¦ˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRadCouncilåœ¨è¯Šæ–­å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ç­‰æ–¹é¢ä¼˜äºå•æ™ºèƒ½ä½“æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59d2af3ad6002fa86690d6df776dc7d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bcee7913a2d5500cd18531e166ef87a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1e1e94c5d66367ab0682056274d131a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86c404870bf09752020923aade3db570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e2ad7a6e1384bbae6966326e5527cd.jpg" align="middle">
</details>




<h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, BjÃ¶rn Ommer</strong></p>
<p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks. </p>
<blockquote>
<p>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä¸¤ç§èŒƒå¼åœ¨å„ç§åº”ç”¨ä¸­å—åˆ°äº†å…³æ³¨ï¼šåŸºäºä¸‹ä¸€ä¸ªé›†åˆé¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€ä¸ªå™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼Œä¾‹å¦‚æ‰©æ•£æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹æ¥è¿æ¥å®ƒä»¬ï¼Œå¹¶æ¢ç´¢å®ƒä»¬åœ¨è§†è§‰é¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€çš„è®¾è®¡ç©ºé—´ä¸­å¯¹ä¸¤ç§ç±»å‹çš„æ¨¡å‹è¿›è¡Œäº†é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ã€æ¸©åº¦ã€å¼•å¯¼å¼ºåº¦ç­‰ï¼Œä»¥å¯æ‰©å±•çš„æ–¹å¼ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†å‰²ï¼‰é‡æ–°å®šä½ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„[MASK]æ ‡è®°çš„å»æ©ç è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œå„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…è®­ç»ƒä¸€æ¬¡æ¥å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡æ¥å®ç°çµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰ä¸Šè¿°æ¢ç´¢éƒ½å¼•é¢†æˆ‘ä»¬æ„å»ºäº†åä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–å…·æœ‰ä¸ä»¥å‰åŸºäºç¦»æ•£çŠ¶æ€çš„æ–¹æ³•ç›¸ç«äº‰çš„æ€§èƒ½ï¼Œå¦‚ImageNet256ã€MS COCOå’Œè§†é¢‘æ•°æ®é›†FaceForensicsã€‚æ€»ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œæˆ‘ä»¬å¯ä»¥æ¶èµ·è¿æ¥æ©ç ç”Ÿæˆæ¨¡å‹å’Œéè‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„æ¡¥æ¢ï¼Œä»¥åŠç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v2">PDF</a> Technical Report (WIP), Project Page(code, model, dataset):   <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p>
<p><strong>Summary</strong><br>     ç¦»æ•£çŠ¶æ€æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹ä¸­å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œè¯¥ç ”ç©¶æå‡ºç»“åˆä¸¤ç§æµè¡Œçš„èŒƒå¼â€”â€”åŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œå¹¶åœ¨è§†è§‰é¢†åŸŸæ¢ç´¢å…¶å¯æ‰©å±•æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€è®¾è®¡ç©ºé—´è¿›è¡Œé€æ­¥åˆ†æï¼Œé‡æ–°æ„å»ºåˆ¤åˆ«ä»»åŠ¡ä½œä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»æ©ç è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥çµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚æœ€ç»ˆæ„å»ºäº†ä¸€ä¸ªåä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„æ©ç ï¼Œå¯ä»¥è¿æ¥æ©ç ç”Ÿæˆå’Œéè‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼Œä»¥åŠç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å±•ç¤ºäº†ç¦»æ•£çŠ¶æ€æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹ä¸­çš„æ½œåŠ›ï¼Œç»“åˆäº†åŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ã€‚</li>
<li>é€šè¿‡ç»Ÿä¸€è®¾è®¡ç©ºé—´è¿›è¡Œé€æ­¥åˆ†æï¼Œæ¢è®¨äº†ä¸¤ç§æ¨¡å‹çš„æ‰©å±•æ€§ã€‚</li>
<li>ç ”ç©¶å°†åˆ¤åˆ«ä»»åŠ¡é‡æ–°æ„å»ºä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»æ©ç è¿‡ç¨‹ï¼Œå®ç°äº†çµæ´»çš„é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œå®ç°äº†å¤šç§åŸºå‡†æµ‹è¯•ä¸­çš„é¡¶å°–æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„æ©ç ï¼Œè¿æ¥äº†ä¸åŒç±»å‹çš„ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«ä»»åŠ¡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84482f8d037dfd8fca4b36a654af2c7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7bd916bdace6c59e807a319127cb89ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3c9e8e97bdb09e0544dbb86f3d0601f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c35d03a36efcc9698532982aad9a05f.jpg" align="middle">
</details>




<h2 id="Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation"><a href="#Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation" class="headerlink" title="Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation"></a>Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation</h2><p><strong>Authors:Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao</strong></p>
<p>Fine-grained remote sensing image segmentation is essential for accurately identifying detailed objects in remote sensing images. Recently, vision transformer models (VTMs) pre-trained on large-scale datasets have demonstrated strong zero-shot generalization. However, directly applying them to specific tasks may lead to domain shift. We introduce a novel end-to-end learning paradigm combining knowledge guidance with domain refinement to enhance performance. We present two key components: the Feature Alignment Module (FAM) and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based backbone with those from the pretrained VTMâ€™s encoder using channel transformation and spatial interpolation, and transfers knowledge via KL divergence and L2 normalization constraint. FMM further adapts the knowledge to the specific domain to address domain shift. We also introduce a fine-grained grass segmentation dataset and demonstrate, through experiments on two datasets, that our method achieves a significant improvement of 2.57 mIoU on the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the potential of combining knowledge transfer and domain adaptation to overcome domain-related challenges and data limitations. The project page is available at <a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/">https://xavierjiezou.github.io/KTDA/</a>. </p>
<blockquote>
<p>ç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†ç‰©ä½“è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨æ¨¡å‹ï¼ˆVTMï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å…¶åº”ç”¨äºç‰¹å®šä»»åŠ¡å¯èƒ½ä¼šå¯¼è‡´é¢†åŸŸåç§»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’Œé¢†åŸŸç²¾ç‚¼çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼ï¼Œå°†å¯¹æ¥è‡ªåŸºäºCNNçš„éª¨å¹²ç½‘çš„ç‰¹å¾ä¸é¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚FMMè¿›ä¸€æ­¥å°†çŸ¥è¯†é€‚åº”åˆ°ç‰¹å®šé¢†åŸŸï¼Œä»¥è§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªç²¾ç»†çš„è‰åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‰æ•°æ®é›†ä¸Šå®ç°äº†2.57 mIoUçš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šå®ç°äº†3.73 mIoUã€‚ç»“æœçªå‡ºäº†ç»“åˆçŸ¥è¯†è½¬ç§»å’Œé¢†åŸŸé€‚åº”çš„æ½œåŠ›ï¼Œå¯ä»¥å…‹æœä¸é¢†åŸŸç›¸å…³çš„æŒ‘æˆ˜å’Œæ•°æ®é™åˆ¶ã€‚é¡¹ç›®é¡µé¢å¯åœ¨[<a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/]%E8%AE%BF%E9%97%AE%E3%80%82">https://xavierjiezou.github.io/KTDA/]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06664v2">PDF</a> 6 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹ç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²éœ€è¦å‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†ç‰©ä½“ã€‚æœ¬ç ”ç©¶ç»“åˆçŸ¥è¯†æŒ‡å¯¼å’Œé¢†åŸŸç²¾ç‚¼ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ã€‚ç ”ç©¶ä¸­å…³é”®éƒ¨åˆ†åŒ…æ‹¬ç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡å¯¹é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼ï¼Œå¯¹é½åŸºäºCNNçš„ä¸»å¹²ç½‘ç»œä¸é¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè½¬ç§»çŸ¥è¯†ã€‚FMMè¿›ä¸€æ­¥é€‚åº”çŸ¥è¯†ä»¥åº”å¯¹é¢†åŸŸåç§»ã€‚å®éªŒè¡¨æ˜ï¼Œç»“åˆçŸ¥è¯†è½¬ç§»å’Œé¢†åŸŸé€‚åº”çš„æ–¹æ³•åœ¨è‰ç±»æ•°æ®é›†ä¸Šæé«˜äº†2.57 mIoUï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šæé«˜äº†3.73 mIoUã€‚è¯¥æ–¹æ³•çš„æ½œåŠ›åœ¨äºå…‹æœé¢†åŸŸç›¸å…³æŒ‘æˆ˜å’Œåº”å¯¹æ•°æ®å±€é™æ€§ã€‚é¡¹ç›®é¡µé¢å¯åœ¨é“¾æ¥<a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/%E6%89%BE%E5%88%B0%E3%80%82">https://xavierjiezou.github.io/KTDA/æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†ç‰©ä½“è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œç»“åˆäº†çŸ¥è¯†æŒ‡å¯¼å’Œé¢†åŸŸç²¾ç‚¼ã€‚</li>
<li>ç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰é€šè¿‡é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼å¯¹é½ç‰¹å¾ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè½¬ç§»çŸ¥è¯†ã€‚</li>
<li>ç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰è¿›ä¸€æ­¥é€‚åº”çŸ¥è¯†ä»¥è§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜è¿˜å¼•å…¥äº†ä¸€ä¸ªç²¾ç»†çš„è‰åœ°åˆ†å‰²æ•°æ®é›†è¿›è¡Œå®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è‰ç±»æ•°æ®é›†ä¸Šæé«˜äº†2.57 mIoUï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šæé«˜äº†3.73 mIoUã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87bfc9a18bc03227574c21ff9bd6739a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72230074e4fabea54949d15fe0c98c06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ba22b25df8231db65d6a7f88f3dfc30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa4e7464f02406170b966d2bd8df31fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1b729ce65630b757bee9ab399fee7bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce9ec2299419671ee56177593d07d844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b2fd1466c02125be44e6c2c16e9695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc9e4ea87e00622bf0c9bc5f46c9b0a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-881a29ae763d8aff44085823f6e7f0ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f1e5e22a81ad979be1a060511a5e5e9.jpg" align="middle">
</details>




<h2 id="Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation"><a href="#Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation" class="headerlink" title="Semantic Consistency-Based Uncertainty Quantification for Factuality in   Radiology Report Generation"></a>Semantic Consistency-Based Uncertainty Quantification for Factuality in   Radiology Report Generation</h2><p><strong>Authors:Chenyu Wang, Weichao Zhou, Shantanu Ghosh, Kayhan Batmanghelich, Wenchao Li</strong></p>
<p>Radiology report generation (RRG) has shown great potential in assisting radiologists by automating the labor-intensive task of report writing. While recent advancements have improved the quality and coherence of generated reports, ensuring their factual correctness remains a critical challenge. Although generative medical Vision Large Language Models (VLLMs) have been proposed to address this issue, these models are prone to hallucinations and can produce inaccurate diagnostic information. To address these concerns, we introduce a novel Semantic Consistency-Based Uncertainty Quantification framework that provides both report-level and sentence-level uncertainties. Unlike existing approaches, our method does not require modifications to the underlying model or access to its inner state, such as output token logits, thus serving as a plug-and-play module that can be seamlessly integrated with state-of-the-art models. Extensive experiments demonstrate the efficacy of our method in detecting hallucinations and enhancing the factual accuracy of automatically generated radiology reports. By abstaining from high-uncertainty reports, our approach improves factuality scores by $10$%, achieved by rejecting $20$% of reports using the Radialog model on the MIMIC-CXR dataset. Furthermore, sentence-level uncertainty flags the lowest-precision sentence in each report with an $82.9$% success rate. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰åœ¨ååŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè‡ªåŠ¨åŒ–æŠ¥å‘Šæ’°å†™æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚è™½ç„¶è¿‘æœŸçš„å‘å±•æé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„è´¨é‡å’Œè¿è´¯æ€§ï¼Œä½†ç¡®ä¿å…¶å®äº‹æ±‚æ˜¯çš„æ­£ç¡®æ€§ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚å°½ç®¡å·²ç»æå‡ºäº†ç”Ÿæˆå¼åŒ»å­¦å½±åƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™äº›æ¨¡å‹å®¹æ˜“å‡ºç°è™šæ„ç°è±¡ï¼Œå¯èƒ½äº§ç”Ÿä¸å‡†ç¡®çš„è¯Šæ–­ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºè¯­ä¹‰ä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä¿®æ”¹åº•å±‚æ¨¡å‹æˆ–è®¿é—®å…¶å†…éƒ¨çŠ¶æ€ï¼Œå¦‚è¾“å‡ºä»¤ç‰Œå¯¹æ•°å‡ ç‡ç­‰ï¼Œå› æ­¤å¯ä»¥ä½œä¸ºæ— ç¼é›†æˆæœ€æ–°æŠ€æœ¯çš„å³æ’å³ç”¨æ¨¡å—ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ£€æµ‹è™šæ„ç°è±¡å’Œæé«˜è‡ªåŠ¨ç”Ÿæˆçš„åŒ»å­¦å½±åƒæŠ¥å‘Šçš„äº‹å®å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡é¿å…é«˜ä¸ç¡®å®šæ€§çš„æŠ¥å‘Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä½¿ç”¨Radialogæ¨¡å‹æ‹’ç»20%çš„æŠ¥å‘Šï¼Œæé«˜äº†äº‹å®å¾—åˆ†10%ã€‚æ­¤å¤–ï¼Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§å¯ä»¥æˆåŠŸæ ‡è®°æ¯ä¸ªæŠ¥å‘Šä¸­ç²¾åº¦æœ€ä½çš„é‚£å¥ï¼ŒæˆåŠŸç‡ä¸º82.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04606v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰çš„æ½œåŠ›ä¸æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§ã€‚æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¯­ä¹‰ä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åœ¨ä¸éœ€è¦ä¿®æ”¹åº•å±‚æ¨¡å‹æˆ–è®¿é—®å…¶å†…éƒ¨çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œæä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œæé«˜æŠ¥å‘Šçš„å‡†ç¡®æ€§å¹¶æ£€æµ‹å‡ºè™šæ„ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæé«˜è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„æŠ¥å‘Šçš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰å…·æœ‰è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šçš„æ½œåŠ›ï¼Œä½†ç¡®ä¿æŠ¥å‘Šçš„äº‹å®æ­£ç¡®æ€§ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç”Ÿæˆå¼åŒ»å­¦è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰è™½ç„¶æœ‰åŠ©äºæé«˜æŠ¥å‘Šè´¨é‡å’Œè¿è´¯æ€§ï¼Œä½†å®¹æ˜“äº§ç”Ÿè™šæ„ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºè¯­ä¹‰ä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œæé«˜æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸åº•å±‚æ¨¡å‹å…¼å®¹ï¼Œæ— éœ€è¿›è¡Œä¿®æ”¹æˆ–è®¿é—®å…¶å†…éƒ¨çŠ¶æ€ï¼Œå¯æ— ç¼é›†æˆåˆ°æœ€æ–°æ¨¡å‹ä¸­ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ£€æµ‹å‡ºè™šæ„ä¿¡æ¯ï¼Œæé«˜æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æ‹’ç»é«˜ä¸ç¡®å®šæ€§çš„æŠ¥å‘Šï¼Œä½¿ç”¨è¯¥æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„äº‹å®æ€§å¾—åˆ†æé«˜äº†10%ï¼ŒåŒæ—¶æ‹’ç»äº†20%çš„æŠ¥å‘Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2b4355629d9865d94922ff12a2af3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25dfae66ed217fc1a6777be1223b11fc.jpg" align="middle">
</details>




<h2 id="Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images"><a href="#Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images" class="headerlink" title="Mask of truth: model sensitivity to unexpected regions of medical images"></a>Mask of truth: model sensitivity to unexpected regions of medical images</h2><p><strong>Authors:ThÃ©o Sourget, Michelle Hestbek-MÃ¸ller, Amelia JimÃ©nez-SÃ¡nchez, Jack Junchi Xu, Veronika Cheplygina</strong></p>
<p>The development of larger models for medical image analysis has led to increased performance. However, it also affected our ability to explain and validate model decisions. Models can use non-relevant parts of images, also called spurious correlations or shortcuts, to obtain high performance on benchmark datasets but fail in real-world scenarios. In this work, we challenge the capacity of convolutional neural networks (CNN) to classify chest X-rays and eye fundus images while masking out clinically relevant parts of the image. We show that all models trained on the PadChest dataset, irrespective of the masking strategy, are able to obtain an Area Under the Curve (AUC) above random. Moreover, the models trained on full images obtain good performance on images without the region of interest (ROI), even superior to the one obtained on images only containing the ROI. We also reveal a possible spurious correlation in the Chaksu dataset while the performances are more aligned with the expectation of an unbiased model. We go beyond the performance analysis with the usage of the explainability method SHAP and the analysis of embeddings. We asked a radiology resident to interpret chest X-rays under different masking to complement our findings with clinical knowledge. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> and <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a> </p>
<blockquote>
<p>å¼€å‘ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„å¤§å‹æ¨¡å‹å·²ç»æé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå½±å“äº†æˆ‘ä»¬è§£é‡Šå’ŒéªŒè¯æ¨¡å‹å†³ç­–çš„èƒ½åŠ›ã€‚æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨å›¾åƒçš„éå…³é”®éƒ¨åˆ†ï¼Œä¹Ÿç§°ä¸ºå¶ç„¶å…³è”æˆ–æ·å¾„ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè·å¾—é«˜æ€§èƒ½ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­å´ä¼šå¤±è´¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹èƒ¸éƒ¨Xå…‰ç‰‡å’Œçœ¼åº•å›¾åƒè¿›è¡Œåˆ†ç±»çš„èƒ½åŠ›ï¼ŒåŒæ—¶å±è”½å›¾åƒä¸­ä¸´åºŠå…³é”®çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ‰€æœ‰åœ¨PadChestæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ— è®ºé‡‡ç”¨ä½•ç§å±è”½ç­–ç•¥ï¼Œéƒ½èƒ½å¤Ÿè·å¾—é«˜äºéšæœºçš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å®Œæ•´å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ— æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å›¾åƒä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨åªåŒ…å«ROIçš„å›¾åƒä¸Šçš„è¡¨ç°ä¹Ÿè¦ä¼˜è¶Šã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†Chaksuæ•°æ®é›†ä¸­å¯èƒ½å­˜åœ¨çš„å¶ç„¶å…³è”ï¼ŒåŒæ—¶æ€§èƒ½æ›´åŠ ç¬¦åˆæ— åè§æ¨¡å‹çš„é¢„æœŸã€‚é™¤äº†æ€§èƒ½åˆ†æï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†SHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†æã€‚æˆ‘ä»¬é‚€è¯·äº†ä¸€åæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨ä¸åŒå±è”½æ¡ä»¶ä¸‹è§£è¯»èƒ¸éƒ¨Xå…‰ç‰‡ï¼Œä»¥è¡¥å……æˆ‘ä»¬çš„ä¸´åºŠçŸ¥è¯†å‘ç°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a>å’Œ<a target="_blank" rel="noopener" href="https://github.com/TheoSurget/MMC_Masking_EyeFundus">https://github.com/TheoSurget/MMC_Masking_EyeFundus</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04030v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹å‘å±•æå‡æ€§èƒ½ï¼Œä½†éš¾ä»¥è§£é‡Šå’ŒéªŒè¯å†³ç­–ã€‚æ¨¡å‹å¯èƒ½åˆ©ç”¨å›¾åƒçš„éå…³é”®éƒ¨åˆ†è·å¾—é«˜åŸºå‡†æ•°æ®é›†æ€§èƒ½ï¼Œä½†åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å¤±è´¥ã€‚ç ”ç©¶æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œå¯¹èƒ¸éƒ¨Xå…‰å’Œçœ¼åº•å›¾åƒçš„åˆ†ç±»èƒ½åŠ›ï¼ŒåŒæ—¶æ©ç›–ä¸´åºŠä¸Šå…³é”®éƒ¨åˆ†ã€‚è®­ç»ƒäºå…¨å›¾åƒçš„æ¨¡å‹åœ¨ä¸å«æ„Ÿå…´è¶£åŒºåŸŸçš„å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ï¼Œç”šè‡³ä¼˜äºä»…å«æ„Ÿå…´è¶£åŒºåŸŸçš„å›¾åƒã€‚åˆ†ææ˜¾ç¤ºChaksuæ•°æ®é›†å­˜åœ¨å¯èƒ½çš„å¶ç„¶å…³è”ï¼Œæ€§èƒ½æ›´ç¬¦åˆæ— åè§æ¨¡å‹çš„é¢„æœŸã€‚ç»“åˆSHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†æï¼Œè¡¥å……ä¸´åºŠçŸ¥è¯†è§£è¯»èƒ¸éƒ¨Xå…‰çš„ä¸åŒæ©ç›–æƒ…å†µã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹æ€§èƒ½æå‡åŒæ—¶ï¼Œå­˜åœ¨éš¾ä»¥è§£é‡Šå’ŒéªŒè¯å†³ç­–çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å¯èƒ½åˆ©ç”¨éå…³é”®å›¾åƒéƒ¨åˆ†ï¼ˆå³å¶ç„¶å…³è”æˆ–æ·å¾„ï¼‰è·å¾—é«˜åŸºå‡†æ•°æ®é›†æ€§èƒ½ã€‚</li>
<li>åœ¨èƒ¸éƒ¨Xå…‰å’Œçœ¼åº•å›¾åƒåˆ†ç±»ä¸­ï¼Œæ©ç›–ä¸´åºŠä¸Šå…³é”®éƒ¨åˆ†çš„ç ”ç©¶æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œçš„åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒäºå…¨å›¾åƒçš„æ¨¡å‹åœ¨ä¸å«æ„Ÿå…´è¶£åŒºåŸŸçš„å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºæŸäº›åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸­å­˜åœ¨å¯èƒ½çš„å¶ç„¶å…³è”ã€‚</li>
<li>ç»“åˆSHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†ææ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02bfa24f2a85fbec93f4101d42a63051.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84e50c2254d9a037875c19d21c824498.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b0ac347bfbe4d0abc9fe8d4bcc7a2a.jpg" align="middle">
</details>




<h2 id="INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching"><a href="#INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching" class="headerlink" title="INRetouch: Context Aware Implicit Neural Representation for Photography   Retouching"></a>INRetouch: Context Aware Implicit Neural Representation for Photography   Retouching</h2><p><strong>Authors:Omar Elezabi, Marcos V. Conde, Zongwei Wu, Radu Timofte</strong></p>
<p>Professional photo editing remains challenging, requiring extensive knowledge of imaging pipelines and significant expertise. With the ubiquity of smartphone photography, there is an increasing demand for accessible yet sophisticated image editing solutions. While recent deep learning approaches, particularly style transfer methods, have attempted to automate this process, they often struggle with output fidelity, editing control, and complex retouching capabilities. We propose a novel retouch transfer approach that learns from professional edits through before-after image pairs, enabling precise replication of complex editing operations. To facilitate this research direction, we introduce a comprehensive Photo Retouching Dataset comprising 100,000 high-quality images edited using over 170 professional Adobe Lightroom presets. We develop a context-aware Implicit Neural Representation that learns to apply edits adaptively based on image content and context, requiring no pretraining and capable of learning from a single example. Our method extracts implicit transformations from reference edits and adaptively applies them to new images. Through extensive evaluation, we demonstrate that our approach not only surpasses existing methods in photo retouching but also enhances performance in related image reconstruction tasks like Gamut Mapping and Raw Reconstruction. By bridging the gap between professional editing capabilities and automated solutions, our work presents a significant step toward making sophisticated photo editing more accessible while maintaining high-fidelity results. Check the Project Page at <a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch">https://omaralezaby.github.io/inretouch</a> for more Results and information about Code and Dataset availability. </p>
<blockquote>
<p>ä¸“ä¸šç…§ç‰‡ç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ·±å…¥äº†è§£æˆåƒç®¡é“å’Œä¸°å¯Œçš„ä¸“ä¸šçŸ¥è¯†ã€‚éšç€æ™ºèƒ½æ‰‹æœºæ‘„å½±çš„æ™®åŠï¼Œäººä»¬å¯¹æ˜“äºä½¿ç”¨ä¸”é«˜çº§çš„å›¾åƒç¼–è¾‘è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚è™½ç„¶æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é£æ ¼è¿ç§»æ–¹æ³•ï¼Œå·²ç»å°è¯•è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä½†å®ƒä»¬åœ¨è¾“å‡ºä¿çœŸåº¦ã€ç¼–è¾‘æ§åˆ¶å’Œå¤æ‚æ¶¦é¥°åŠŸèƒ½æ–¹é¢å¾€å¾€é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¶¦é¥°è¿ç§»æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸“ä¸šç¼–è¾‘çš„å‰åå›¾åƒå¯¹æ¥å­¦ä¹ ï¼Œèƒ½å¤Ÿç²¾ç¡®å¤åˆ¶å¤æ‚çš„ç¼–è¾‘æ“ä½œã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç ”ç©¶æ–¹å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„ç…§ç‰‡æ¶¦é¥°æ•°æ®é›†ï¼ŒåŒ…å«ä½¿ç”¨è¶…è¿‡170ä¸ªä¸“ä¸šAdobe Lightroomé¢„è®¾ç¼–è¾‘çš„10ä¸‡å¼ é«˜è´¨é‡å›¾åƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŸºäºå›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘æ“ä½œï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒï¼Œå¹¶ä¸”èƒ½å¤Ÿä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»å‚è€ƒç¼–è¾‘ä¸­æå–éšå¼è½¬æ¢ï¼Œå¹¶è‡ªé€‚åº”åœ°åº”ç”¨äºæ–°å›¾åƒã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨ç…§ç‰‡æ¶¦é¥°æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”åœ¨ç›¸å…³å›¾åƒé‡å»ºä»»åŠ¡ï¼ˆå¦‚è‰²åŸŸæ˜ å°„å’ŒåŸå§‹é‡å»ºï¼‰æ–¹é¢ä¹Ÿæé«˜äº†æ€§èƒ½ã€‚é€šè¿‡ç¼©å°ä¸“ä¸šç¼–è¾‘èƒ½åŠ›å’Œè‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯åœ¨ä¿æŒé«˜ä¿çœŸç»“æœçš„åŒæ—¶ï¼Œä½¿é«˜çº§ç…§ç‰‡ç¼–è¾‘æ›´åŠ æ˜“äºè®¿é—®çš„é‡è¦ä¸€æ­¥ã€‚æœ‰å…³æ›´å¤šç»“æœå’Œå…³äºä»£ç å’Œæ•°æ®é›†å¯ç”¨æ€§çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch%E3%80%82">https://omaralezaby.github.io/inretouchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03848v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„ç…§ç‰‡ä¿®é¥°è½¬æ¢æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ å’Œå¤åˆ¶ä¸“ä¸šç¼–è¾‘çš„æ“ä½œï¼Œæé«˜äº†è‡ªåŠ¨åŒ–ç…§ç‰‡ç¼–è¾‘çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†ä¸€ä¸ªåŒ…å«åä¸‡å¼ é«˜è´¨é‡å›¾ç‰‡çš„æ•°æ®é›†ï¼Œä½¿ç”¨è¶…è¿‡170ç§Adobe Lightroomé¢„è®¾è¿›è¡Œç¼–è¾‘ã€‚ä»–ä»¬å¼€å‘äº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘æ“ä½œï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒä¸”èƒ½ä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨ç…§ç‰‡ä¿®é¥°æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œè¿˜åœ¨ç›¸å…³å›¾åƒé‡å»ºä»»åŠ¡å¦‚è‰²åŸŸæ˜ å°„å’ŒåŸå§‹é‡å»ºä¸­æå‡äº†æ€§èƒ½ï¼Œæ˜¯ä¸“ä¸šç¼–è¾‘ä¸è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ç…§ç‰‡ä¿®é¥°è½¬æ¢æ–¹æ³•ï¼Œèƒ½å­¦ä¹ å’Œå¤åˆ¶ä¸“ä¸šç¼–è¾‘çš„æ“ä½œã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªåŒ…å«åä¸‡å¼ é«˜è´¨é‡å›¾ç‰‡çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ç…§ç‰‡ç¼–è¾‘æŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½æ ¹æ®å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡è‡ªé€‚åº”åº”ç”¨ç¼–è¾‘æ“ä½œã€‚</li>
<li>æ–¹æ³•æ— éœ€é¢„å…ˆè®­ç»ƒä¸”èƒ½ä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç…§ç‰‡ä¿®é¥°å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥ç ”ç©¶ä¿ƒè¿›äº†ä¸“ä¸šç¼–è¾‘ä¸è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆçš„èåˆï¼Œä½¿é«˜è´¨é‡ç…§ç‰‡ç¼–è¾‘æ›´åŠ æ™®åŠå’Œä¾¿æ·ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5205fa79c3541d2e25d27f60df6c7d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-994ce5c2421a80781d77017943cd794d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a633c574e2936b565ab713c3f7c68ca4.jpg" align="middle">
</details>




<h2 id="INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis"><a href="#INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis" class="headerlink" title="INSIGHT: Explainable Weakly-Supervised Medical Image Analysis"></a>INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</h2><p><strong>Authors:Wenbo Zhang, Junyu Chen, Christopher Kanan</strong></p>
<p>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: <a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/">https://zhangdylan83.github.io/ewsmia/</a> </p>
<blockquote>
<p>ç”±äºä½“ç§¯æ‰«æå’Œå…¨å¹»ç¯ç‰‡ç—…ç†å›¾åƒï¼ˆWSIï¼‰çš„å°ºå¯¸è¾ƒå¤§ï¼Œé€šå¸¸é€šè¿‡å¯¹å±€éƒ¨åŒºåŸŸæå–åµŒå…¥è¿›è¡ŒåŠ å·¥ï¼Œç„¶åç”±èšåˆå™¨æ ¹æ®æ­¤é›†åˆè¿›è¡Œé¢„æµ‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•éœ€è¦äº‹åå¯è§†åŒ–æŠ€æœ¯ï¼ˆä¾‹å¦‚Grad-CAMï¼‰ï¼Œå¹¶ä¸”å¾€å¾€æ— æ³•å®šä½è™½å°ä½†ä¸´åºŠä¸Šè‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†INSIGHTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£èšåˆå™¨ï¼Œå®ƒå°†çƒ­å›¾ç”Ÿæˆä½œä¸ºå½’çº³åè§è¿›è¡Œé›†æˆã€‚ä»é¢„è®­ç»ƒçš„ç‰¹å¾å›¾å¼€å§‹ï¼ŒINSIGHTä½¿ç”¨å…·æœ‰è¾ƒå°å·ç§¯æ ¸çš„æ£€æµ‹æ¨¡å—æ¥æ•æ‰ç»†èŠ‚ï¼Œå¹¶ä½¿ç”¨å…·æœ‰æ›´å¹¿æ³›æ¥å—åŸŸçš„ä¸Šæ–‡æ¨¡å—æ¥æŠ‘åˆ¶å±€éƒ¨è¯¯æŠ¥ã€‚ç»“æœå†…éƒ¨çƒ­å›¾çªå‡ºäº†ä¸è¯Šæ–­ç›¸å…³çš„åŒºåŸŸã€‚åœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒINSIGHTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœï¼Œå¹¶åœ¨å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚é¡¹ç›®ç½‘ç«™å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š[<a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/]">https://zhangdylan83.github.io/ewsmia/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02012v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤„ç†å¤§å‹åŒ»å­¦å›¾åƒçš„æ–°æ–¹æ³•â€”â€”INSIGHTã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¼±ç›‘ç£èšåˆæŠ€æœ¯ï¼Œç»“åˆç”Ÿæˆçš„çƒ­å›¾ä½œä¸ºå½’çº³åç½®ï¼Œä»¥æ”¹è¿›ç°æœ‰çš„åŒ»å­¦å›¾åƒå¤„ç†æ–¹æ³•ã€‚INSIGHTèƒ½å¤Ÿä»é¢„è®­ç»ƒçš„ç‰¹å¾å›¾ä¸­æå–ç»†èŠ‚ï¼Œå¹¶ç”Ÿæˆå†…éƒ¨çƒ­å›¾çªå‡ºæ˜¾ç¤ºè¯Šæ–­ç›¸å…³åŒºåŸŸã€‚åœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒINSIGHTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœå’Œé«˜æ€§èƒ½çš„å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>INSIGHTæ˜¯ä¸€ç§å¤„ç†å¤§å‹åŒ»å­¦å›¾åƒçš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨å¼±ç›‘ç£èšåˆæŠ€æœ¯æ”¹è¿›ç°æœ‰çš„å¤„ç†æ–¹æ³•ã€‚</li>
<li>INSIGHTé›†æˆäº†ç”Ÿæˆçƒ­å›¾ä½œä¸ºå½’çº³åç½®ï¼Œèƒ½å¤Ÿä»é¢„è®­ç»ƒçš„ç‰¹å¾å›¾ä¸­æå–ç»†èŠ‚ã€‚</li>
<li>INSIGHTé€šè¿‡æ£€æµ‹æ¨¡å—å’Œä¸Šä¸‹æ–‡æ¨¡å—çš„è®¾è®¡ï¼Œèƒ½å¤Ÿæ•æ‰ç²¾ç»†ç»†èŠ‚å¹¶æŠ‘åˆ¶å±€éƒ¨è¯¯æŠ¥ã€‚</li>
<li>å†…éƒ¨çƒ­å›¾çªå‡ºäº†è¯Šæ–­ç›¸å…³çš„åŒºåŸŸã€‚</li>
<li>åœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒINSIGHTå®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœã€‚</li>
<li>INSIGHTåœ¨å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-50d3161b359075aa504049aa195c3727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afab69189b4acab6873b312975985e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d897ad24ed71752c1c42000c6ad919ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-223d9e3addb6f68585a0a9a76ad167dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce65697e1eabd4ed48df0052f446a885.jpg" align="middle">
</details>




<h2 id="Evaluating-Automated-Radiology-Report-Quality-through-Fine-Grained-Phrasal-Grounding-of-Clinical-Findings"><a href="#Evaluating-Automated-Radiology-Report-Quality-through-Fine-Grained-Phrasal-Grounding-of-Clinical-Findings" class="headerlink" title="Evaluating Automated Radiology Report Quality through Fine-Grained   Phrasal Grounding of Clinical Findings"></a>Evaluating Automated Radiology Report Quality through Fine-Grained   Phrasal Grounding of Clinical Findings</h2><p><strong>Authors:Razi Mahmood, Pingkun Yan, Diego Machado Reyes, Ge Wang, Mannudeep K. Kalra, Parisa Kaviani, Joy T. Wu, Tanveer Syeda-Mahmood</strong></p>
<p>Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, and severity of a large number of clinical findings. We then performed phrasal grounding to localize their associated anatomical regions on chest radiograph images. The textual and visual measures are then combined to rate the quality of the generated reports. We present results that compare this evaluation metric with other textual metrics on a gold standard dataset derived from the MIMIC collection and show its robustness and sensitivity to factual errors. </p>
<blockquote>
<p>è¿‘æœŸå·²ç»å¼€å‘äº†ä¸€äº›è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºä»…åŸºäºæ–‡æœ¬ä¿¡æ¯ï¼Œä½¿ç”¨è¯æ±‡ã€è¯­ä¹‰æˆ–ä¸´åºŠå‘½åå®ä½“è¯†åˆ«æ–¹æ³•ï¼Œè‡ªåŠ¨è¯„ä¼°èƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šä¸­ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ¥å‘Šçš„è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„æŠ¥å‘Šè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé¦–å…ˆæå–ç²¾ç»†çš„æ£€æµ‹ç»“æœæ¨¡å¼ï¼Œæ•æ‰å¤§é‡ä¸´åºŠæ£€æµ‹ç»“æœçš„éƒ¨ä½ã€å•ä¾§æ€§å’Œä¸¥é‡æ€§ã€‚ç„¶åæˆ‘ä»¬å¯¹çŸ­è¯­è¿›è¡Œå®šä½ï¼Œä»¥ç¡®å®šå…¶åœ¨èƒ¸éƒ¨æ”¾å°„å›¾åƒä¸Šçš„ç›¸å…³è§£å‰–åŒºåŸŸã€‚ç„¶åå°†æ–‡æœ¬å’Œè§†è§‰åº¦é‡ç»“åˆèµ·æ¥è¯„ä¼°ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡ã€‚æˆ‘ä»¬åœ¨ç”±MIMICé›†åˆè¡ç”Ÿçš„é»„é‡‘æ ‡å‡†æ•°æ®é›†ä¸Šï¼Œå¯¹æ¯”è¿™ç§è¯„ä¼°æŒ‡æ ‡ä¸å…¶ä»–æ–‡æœ¬æŒ‡æ ‡çš„å¯¹æ¯”ç»“æœï¼Œå±•ç¤ºå…¶ç¨³å¥æ€§å’Œå¯¹äº‹å®é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01031v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ–‡æœ¬ä¿¡æ¯è¯„ä¼°ç”Ÿæˆå¼AIæŠ¥å‘Šè´¨é‡çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ç²¾ç»†çš„ç—…å˜æ¨¡å¼æ•æ‰å¤§é‡ä¸´åºŠå‘ç°çš„éƒ¨ä½ã€å•ä¾§æ€§å’Œä¸¥é‡ç¨‹åº¦ï¼Œåœ¨èƒ¸éƒ¨æ”¾å°„å›¾åƒä¸Šè¿›è¡ŒçŸ­è¯­å®šä½ï¼Œå¹¶èåˆæ–‡æœ¬å’Œè§†è§‰æŒ‡æ ‡æ¥è¯„ä¼°æŠ¥å‘Šè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è¯„ä¼°æŒ‡æ ‡ä¸å…¶ä»–æ–‡æœ¬æŒ‡æ ‡ç›¸æ¯”ï¼Œåœ¨MIMICæ•°æ®é›†ä¸Šå…·æœ‰ç¨³å¥æ€§å’Œäº‹å®é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬ä¿¡æ¯çš„è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šè´¨é‡è¯„ä¼°æ–°æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬æå–ä¸´åºŠå‘ç°çš„ç²¾ç»†æ¨¡å¼å¹¶å®šä½å…¶ç›¸å…³è§£å‰–åŒºåŸŸã€‚</li>
<li>ç»“åˆæ–‡æœ¬å’Œè§†è§‰æŒ‡æ ‡è¯„ä¼°ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡ã€‚</li>
<li>å®éªŒåœ¨MIMICæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ˜¾ç¤ºæ–°è¯„ä¼°æŒ‡æ ‡çš„ç¨³å¥æ€§å’Œäº‹å®é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆå¼AIæŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†å¦‚ä½•å°†æ–°æ–¹æ³•ä¸å…¶ä»–æ–‡æœ¬æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e9d5d1cc123c89e2f808003b5997354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3341695585519580b5b428afaa04f41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-540b5e0fd385083840e138b7ba5807a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a04451d7dcf56546f0b9a4d826025b41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e43ec74bd938b7980b981c5f690c05fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613b07bdf0139137b5685ffc7e5fcb0a.jpg" align="middle">
</details>




<h2 id="Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy"><a href="#Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy" class="headerlink" title="Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy"></a>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy</h2><p><strong>Authors:Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</strong></p>
<p>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large GIE dataset, called EndoExtend24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of GIE medical image diagnosis. Specifically, the EVA-02 model, which is based on the ViT architecture and trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762 and a balanced accuracy of 37.1% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched EndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics. </p>
<blockquote>
<p>è§†é¢‘èƒ¶å›Šå†…é•œæŠ€æœ¯ä¸ºèƒƒè‚ é“å†…é•œï¼ˆGIEï¼‰è¯Šæ–­æä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰èƒƒè‚ é“çš„è¯¦ç»†å›¾åƒï¼Œä»è€Œå®ç°æ—©æœŸç–¾ç—…çš„æ£€æµ‹ã€‚ç„¶è€Œï¼Œå…¶æ½œåŠ›å—é™äºæˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œæˆåƒè¿‡ç¨‹å¯èƒ½éœ€è¦6-8å°æ—¶ï¼Œå¹¶å¯èƒ½äº§ç”Ÿé«˜è¾¾100ä¸‡å¼ å›¾åƒï¼Œå› æ­¤éœ€è¦è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æã€‚æ­¤å¤–ï¼Œè¿™äº›å›¾åƒçš„å·®å¼‚æ€§ï¼ŒåŠ ä¸Šéœ€è¦ä¸“å®¶æ ‡æ³¨ä»¥åŠå¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºï¼Œé™åˆ¶äº†å½“å‰åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºEndoExtend24çš„å¤§å‹GIEæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯é€šè¿‡åˆå¹¶åä¸ªç°æœ‰çš„å…¬å…±å’Œç§æœ‰æ•°æ®é›†åˆ›å»ºçš„ï¼Œå¯ç¡®ä¿è·¨åˆ†å‰²çš„æ‚£è€…å®Œæ•´æ€§ã€‚EndoExtend24åŒ…å«è¶…è¿‡22ä¸‡å¼ æ ‡è®°å›¾åƒï¼Œä»¥åŠåŠ¨æ€ç±»æ˜ å°„ï¼Œå…è®¸åœ¨ä¸åŒæ ‡ç­¾ç²’åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œç»Ÿä¸€è®­ç»ƒï¼Œæ”¯æŒå¤šè¾¾123ç§ä¸åŒçš„ç—…ç†å‘ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨åŸºäºè‡ªç›‘ç£çš„é€šç”¨å›¾åƒæ•°æ®å¯¹åŸºç¡€æ¨¡å‹è¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œä»¥é€‚åº”GIEåŒ»å­¦å›¾åƒè¯Šæ–­çš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒEVA-02æ¨¡å‹åŸºäºViTæ¶æ„æ„å»ºï¼Œåœ¨ImageNet-22kæ•°æ®é›†ä¸Šè¿›è¡Œé®æ©å›¾åƒå»ºæ¨¡ï¼ˆä½¿ç”¨EVA-CLIPä½œä¸ºé®æ©å›¾åƒå»ºæ¨¡çš„æ•™å¸ˆï¼‰ï¼Œåœ¨EndoExtend24æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥å®ç°åŸŸé€‚åº”ï¼Œå¹¶æœ€ç»ˆåœ¨Capsule Endoscopy 2024æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œåœ¨Capsule Endoscopy 2024æŒ‘æˆ˜èµ›ä¸­è£è·ç¬¬ä¸‰åã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„å®è§‚AUCè¾¾åˆ°0.762ï¼Œå¹³è¡¡ç²¾åº¦ä¸º37.1%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æˆ‘ä»¬åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’Œä¸°å¯Œçš„EndoExtend24æ•°æ®é›†åœ¨æ¨è¿›èƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21302v4">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…çª¥é•œæ£€æŸ¥ï¼ˆGIEï¼‰æä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„æˆåƒæ–¹æ³•ï¼Œèƒ½å¤Ÿè¯¦ç»†æ•æ‰èƒƒè‚ é“çš„å½±åƒï¼Œæœ‰åŠ©äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚ç„¶è€Œï¼Œç”±äºæˆåƒè¿‡ç¨‹äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œåˆ†æè¿‡ç¨‹éœ€è¦è‡ªåŠ¨åŒ–å¤„ç†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†EndoExtend24æ•°æ®é›†ï¼Œå¹¶å°è¯•é€šè¿‡åŸºäºåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œè¯Šæ–­ã€‚å…¶ä¸­EVA-02æ¨¡å‹åœ¨GIEåŒ»å­¦å›¾åƒè¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œåœ¨Capsule Endoscopy 2024 Challengeä¸­å–å¾—äº†ç¬¬ä¸‰åçš„å¥½æˆç»©ã€‚è¯¥æ¨¡å‹çš„è¡¨ç°åœ¨å®è§‚AUCå’Œå¹³è¡¡ç²¾åº¦æ–¹é¢éƒ½è¾¾åˆ°äº†æ˜¾è‘—æ°´å¹³ã€‚è¿™å‡¸æ˜¾äº†æˆ‘ä»¬çš„åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’Œä¸°å¯Œçš„EndoExtend24æ•°æ®é›†åœ¨æ¨åŠ¨èƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æä¾›äº†éä¾µå…¥æ€§çš„è¯¦ç»†å›¾åƒæ•è·æ–¹æ³•ï¼Œæœ‰åŠ©äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚</li>
<li>æˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æµ·é‡å›¾åƒæ•°æ®éœ€è¦è‡ªåŠ¨åŒ–åˆ†æå¤„ç†ã€‚</li>
<li>EndoExtend24æ•°æ®é›†é€šè¿‡åˆå¹¶å¤šä¸ªå…¬å…±å’Œç§æœ‰æ•°æ®é›†ï¼Œç¡®ä¿äº†æ‚£è€…æ•°æ®çš„å®Œæ•´æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡226,000å¼ æ ‡è®°å›¾åƒå’ŒåŠ¨æ€ç±»æ˜ å°„ï¼Œæ”¯æŒå¤šè¾¾123ç§ä¸åŒçš„ç—…ç†å‘ç°ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºåˆ©ç”¨åŸºäºåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡ŒGIEåŒ»å­¦å›¾åƒè¯Šæ–­ã€‚</li>
<li>EVA-02æ¨¡å‹åœ¨GIEåŒ»å­¦å›¾åƒè¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œåœ¨æ¯”èµ›ä¸­è·å¾—äº†ç¬¬ä¸‰åã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cf9713e7ffc155a83738fb6388d5f539.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c49edf263644ff07b1db7da8210619a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8f4db2dad3e28af984765d3201e9f3f.jpg" align="middle">
</details>




<h2 id="The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI"><a href="#The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI" class="headerlink" title="The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI"></a>The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI</h2><p><strong>Authors:Ahmed W. Moawad, Anastasia Janas, Ujjwal Baid, Divya Ramakrishnan, Rachit Saluja, Nader Ashraf, Nazanin Maleki, Leon Jekel, Nikolay Yordanov, Pascal Fehringer, Athanasios Gkampenis, Raisa Amiruddin, Amirreza Manteghinejad, Maruf Adewole, Jake Albrecht, Udunna Anazodo, Sanjay Aneja, Syed Muhammad Anwar, Timothy Bergquist, Veronica Chiang, Verena Chung, Gian Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Nastaran Khalili, Keyvan Farahani, Juan Eugenio Iglesias, Zhifan Jiang, Elaine Johanson, Anahita Fathi Kazerooni, Florian Kofler, Kiril Krantchev, Dominic LaBella, Koen Van Leemput, Hongwei Bran Li, Marius George Linguraru, Xinyang Liu, Zeke Meier, Bjoern H Menze, Harrison Moy, Klara Osenberg, Marie Piraud, Zachary Reitman, Russell Takeshi Shinohara, Chunhao Wang, Benedikt Wiestler, Walter Wiggins, Umber Shafique, Klara Willms, Arman Avesta, Khaled Bousabarah, Satrajit Chakrabarty, Nicolo Gennaro, Wolfgang Holler, Manpreet Kaur, Pamela LaMontagne, MingDe Lin, Jan Lost, Daniel S. Marcus, Ryan Maresca, Sarah Merkaj, Gabriel Cassinelli Pedersen, Marc von Reppert, Aristeidis Sotiras, Oleg Teytelboym, Niklas Tillmans, Malte Westerhoff, Ayda Youssef, Devon Godfrey, Scott Floyd, Andreas Rauschecker, Javier Villanueva-Meyer, Irada Pfluger, Jaeyoung Cho, Martin Bendszus, Gianluca Brugnara, Justin Cramer, Gloria J. Guzman Perez-Carillo, Derek R. Johnson, Anthony Kam, Benjamin Yin Ming Kwan, Lillian Lai, Neil U. Lall, Fatima Memon, Mark Krycia, Satya Narayana Patro, Bojan Petrovic, Tiffany Y. So, Gerard Thompson, Lei Wu, E. Brooke Schrickel, Anu Bansal, Frederik Barkhof, Cristina Besada, Sammy Chu, Jason Druzgal, Alexandru Dusoi, Luciano Farage, Fabricio Feltrin, Amy Fong, Steve H. Fung, R. Ian Gray, Ichiro Ikuta, Michael Iv, Alida A. Postma, Amit Mahajan, David Joyner, Chase Krumpelman, Laurent Letourneau-Guillon, Christie M. Lincoln, Mate E. Maros, Elka Miller, Fanny Moron, Esther A. Nimchinsky, Ozkan Ozsarlak, Uresh Patel, Saurabh Rohatgi, Atin Saha, Anousheh Sayah, Eric D. Schwartz, Robert Shih, Mark S. Shiroishi, Juan E. Small, Manoj Tanwar, Jewels Valerie, Brent D. Weinberg, Matthew L. White, Robert Young, Vahe M. Zohrabian, Aynur Azizova, Melanie Maria Theresa Bruseler, Mohanad Ghonim, Mohamed Ghonim, Abdullah Okar, Luca Pasquini, Yasaman Sharifi, Gagandeep Singh, Nico Sollmann, Theodora Soumala, Mahsa Taherzadeh, Philipp Vollmuth, Martha Foltyn-Dumitru, Ajay Malhotra, Aly H. Abayazeed, Francesco Dellepiane, Philipp Lohmann, Victor M. Perez-Garcia, Hesham Elhalawani, Maria Correia de Verdier, Sanaria Al-Rubaiey, Rui Duarte Armindo, Kholod Ashraf, Moamen M. Asla, Mohamed Badawy, Jeroen Bisschop, Nima Broomand Lomer, Jan Bukatz, Jim Chen, Petra Cimflova, Felix Corr, Alexis Crawley, Lisa Deptula, Tasneem Elakhdar, Islam H. Shawali, Shahriar Faghani, Alexandra Frick, Vaibhav Gulati, Muhammad Ammar Haider, Fatima Hierro, Rasmus Holmboe Dahl, Sarah Maria Jacobs, Kuang-chun Jim Hsieh, Sedat G. Kandemirli, Katharina Kersting, Laura Kida, Sofia Kollia, Ioannis Koukoulithras, Xiao Li, Ahmed Abouelatta, Aya Mansour, Ruxandra-Catrinel Maria-Zamfirescu, Marcela Marsiglia, Yohana Sarahi Mateo-Camacho, Mark McArthur, Olivia McDonnell, Maire McHugh, Mana Moassefi, Samah Mostafa Morsi, Alexander Munteanu, Khanak K. Nandolia, Syed Raza Naqvi, Yalda Nikanpour, Mostafa Alnoury, Abdullah Mohamed Aly Nouh, Francesca Pappafava, Markand D. Patel, Samantha Petrucci, Eric Rawie, Scott Raymond, Borna Roohani, Sadeq Sabouhi, Laura M. Sanchez-Garcia, Zoe Shaked, Pokhraj P. Suthar, Talissa Altes, Edvin Isufi, Yaseen Dhemesh, Jaime Gass, Jonathan Thacker, Abdul Rahman Tarabishy, Benjamin Turner, Sebastiano Vacca, George K. Vilanilam, Daniel Warren, David Weiss, Fikadu Worede, Sara Yousry, Wondwossen Lerebo, Alejandro Aristizabal, Alexandros Karargyris, Hasan Kassem, Sarthak Pati, Micah Sheller, Katherine E. Link, Evan Calabrese, Nourel hoda Tahon, Ayman Nada, Yuri S. Velichko, Spyridon Bakas, Jeffrey D. Rudie, Mariam Aboian</strong></p>
<p>The translation of AI-generated brain metastases (BM) segmentation into clinical practice relies heavily on diverse, high-quality annotated medical imaging datasets. The BraTS-METS 2023 challenge has gained momentum for testing and benchmarking algorithms using rigorously annotated internationally compiled real-world datasets. This study presents the results of the segmentation challenge and characterizes the challenging cases that impacted the performance of the winning algorithms. Untreated brain metastases on standard anatomic MRI sequences (T1, T2, FLAIR, T1PG) from eight contributed international datasets were annotated in stepwise method: published UNET algorithms, student, neuroradiologist, final approver neuroradiologist. Segmentations were ranked based on lesion-wise Dice and Hausdorff distance (HD95) scores. False positives (FP) and false negatives (FN) were rigorously penalized, receiving a score of 0 for Dice and a fixed penalty of 374 for HD95. Eight datasets comprising 1303 studies were annotated, with 402 studies (3076 lesions) released on Synapse as publicly available datasets to challenge competitors. Additionally, 31 studies (139 lesions) were held out for validation, and 59 studies (218 lesions) were used for testing. Segmentation accuracy was measured as rank across subjects, with the winning team achieving a LesionWise mean score of 7.9. Common errors among the leading teams included false negatives for small lesions and misregistration of masks in space.The BraTS-METS 2023 challenge successfully curated well-annotated, diverse datasets and identified common errors, facilitating the translation of BM segmentation across varied clinical environments and providing personalized volumetric reports to patients undergoing BM treatment. </p>
<blockquote>
<p>å°†AIç”Ÿæˆçš„è„‘è½¬ç§»ï¼ˆBMï¼‰åˆ†æ®µç¿»è¯‘åº”ç”¨äºä¸´åºŠå®è·µï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤šæ ·åŒ–ã€é«˜è´¨é‡æ ‡æ³¨çš„åŒ»å­¦å½±åƒæ•°æ®é›†ã€‚BraTS-METS 2023æŒ‘æˆ˜èµ›é€šè¿‡ä½¿ç”¨ä¸¥æ ¼æ ‡æ³¨çš„å›½é™…æ±‡ç¼–çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæµ‹è¯•å¹¶è¯„ä¼°ç®—æ³•æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†åˆ†å‰²æŒ‘æˆ˜çš„ç»“æœï¼Œå¹¶å¯¹å½±å“è·èƒœç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§ç—…ä¾‹è¿›è¡Œäº†ç‰¹å¾æè¿°ã€‚å¯¹æ¥è‡ªå…«ä¸ªå›½é™…æ•°æ®é›†çš„æœªç»å¤„ç†çš„è„‘è½¬ç§»ç˜¤åœ¨æ ‡å‡†è§£å‰–MRIåºåˆ—ï¼ˆT1ã€T2ã€FLAIRã€T1PGï¼‰ä¸Šé‡‡ç”¨é€æ­¥æ–¹æ³•è¿›è¡Œæ ‡æ³¨ï¼šå…¬å¼€UNETç®—æ³•ã€å­¦ç”Ÿã€ç¥ç»æ”¾å°„å­¦å®¶ã€æœ€ç»ˆå®¡æ‰¹ç¥ç»æ”¾å°„å­¦å®¶ã€‚æ ¹æ®ç—…ç¶Diceå’ŒHausdorffè·ç¦»ï¼ˆHD95ï¼‰å¾—åˆ†å¯¹åˆ†å‰²è¿›è¡Œæ’åã€‚å‡é˜³æ€§ï¼ˆFPï¼‰å’Œå‡é˜´æ€§ï¼ˆFNï¼‰å—åˆ°ä¸¥æ ¼æƒ©ç½šï¼ŒDiceå¾—åˆ†ä¸º0ï¼ŒHD95å›ºå®šæƒ©ç½šä¸º374ã€‚å…«ä¸ªæ•°æ®é›†å…±æ ‡æ³¨äº†1303é¡¹ç ”ç©¶ï¼Œå…¶ä¸­402é¡¹ç ”ç©¶ï¼ˆ3076ä¸ªç—…ç¶ï¼‰åœ¨Synapseä¸Šä½œä¸ºå…¬å¼€æ•°æ®é›†å‘å‚èµ›è€…å‘å¸ƒã€‚æ­¤å¤–ï¼Œè¿˜ç•™å‡º31é¡¹ç ”ç©¶ï¼ˆ139ä¸ªç—…ç¶ï¼‰ç”¨äºéªŒè¯ï¼Œ59é¡¹ç ”ç©¶ï¼ˆ218ä¸ªç—…ç¶ï¼‰ç”¨äºæµ‹è¯•ã€‚åˆ†å‰²å‡†ç¡®åº¦é€šè¿‡å—è¯•è€…æ’åæ¥è¡¡é‡ï¼Œç¬¬ä¸€åå›¢é˜Ÿçš„ç—…ç¶çº§å¹³å‡å¾—åˆ†ä¸º7.9ã€‚é¢†å…ˆå›¢é˜Ÿçš„å¸¸è§é”™è¯¯åŒ…æ‹¬å°ç—…ç¶çš„å‡é˜´æ€§å’Œå£ç½©çš„ç©ºé—´é”™ä½ã€‚BraTS-METS 2023æŒ‘æˆ˜èµ›æˆåŠŸç­›é€‰äº†æ ‡æ³¨è‰¯å¥½ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œå¹¶è¯†åˆ«äº†å¸¸è§é”™è¯¯ï¼Œä¿ƒè¿›äº†BMåˆ†æ®µçš„ä¸´åºŠç¿»è¯‘åº”ç”¨äºå„ç§ä¸´åºŠç¯å¢ƒï¼Œå¹¶ä¸ºæ¥å—BMæ²»ç–—çš„æ‚£è€…æä¾›ä¸ªæ€§åŒ–çš„ä½“ç§¯æŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.00838v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BraTS-METS 2023æŒ‘æˆ˜èµ›çš„ç»“æœï¼Œè¯¥æŒ‘æˆ˜èµ›ä½¿ç”¨ä¸¥æ ¼æ³¨é‡Šçš„å›½é™…æ±‡ç¼–çœŸå®ä¸–ç•Œæ•°æ®é›†æ¥æµ‹è¯•å’Œè¯„ä¼°ç®—æ³•åœ¨å¤§è„‘è½¬ç§»ç˜¤åˆ†å‰²æ–¹é¢çš„æ€§èƒ½ã€‚ç ”ç©¶é€šè¿‡å¯¹æœªæ²»ç–—çš„å¤§è„‘è½¬ç§»ç˜¤è¿›è¡Œé€æ­¥æ ‡æ³¨ï¼Œè¯„ä¼°äº†åˆ†å‰²æŒ‘æˆ˜ä¸­çš„ç®—æ³•æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºäº†å½±å“ç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ã€‚è¯¥æŒ‘æˆ˜æˆåŠŸåœ°æ•´ç†äº†ç»è¿‡è‰¯å¥½æ³¨é‡Šçš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œå¹¶ç¡®å®šäº†å¸¸è§é”™è¯¯ï¼Œä¿ƒè¿›äº†å¤§è„‘è½¬ç§»ç˜¤åˆ†å‰²åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ï¼Œä¸ºæ‚£è€…æä¾›ä¸ªæ€§åŒ–çš„ä½“ç§¯æŠ¥å‘Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BraTS-METS 2023æŒ‘æˆ˜èµ›ä½¿ç”¨ä¸¥æ ¼æ³¨é‡Šçš„å›½é™…æ±‡ç¼–çœŸå®ä¸–ç•Œæ•°æ®é›†è¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨é€æ­¥æ ‡æ³¨æ³•å¯¹æœªæ²»ç–—çš„å¤§è„‘è½¬ç§»ç˜¤è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶è¯„ä¼°äº†åˆ†å‰²ç®—æ³•çš„æ•ˆèƒ½ã€‚</li>
<li>æŒ‘æˆ˜èµ›ä¸­å­˜åœ¨å½±å“ç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ï¼Œå¦‚å°ç—…ç¶çš„æ¼æ£€å’Œé¢å…·çš„ç©ºé—´é”™ä½ç­‰ã€‚</li>
<li>æŒ‘æˆ˜èµ›æˆåŠŸæ•´ç†å‡ºå¤šæ ·åŒ–ä¸”ç»è¿‡è‰¯å¥½æ³¨é‡Šçš„æ•°æ®é›†ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¤§è„‘è½¬ç§»ç˜¤åˆ†å‰²æŠ€æœ¯åœ¨ä¸åŒä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚</li>
<li>æŒ‘æˆ˜èµ›ç¡®å®šäº†å¸¸è§é”™è¯¯ï¼Œå¦‚è¯¯åˆ†å‰²å’Œæ¼åˆ†å‰²ï¼Œè¿™æœ‰åŠ©äºæé«˜ç®—æ³•æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æŒ‘æˆ˜èµ›çš„æˆæœï¼Œä¿ƒè¿›äº†åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºæ‚£è€…æä¾›æ›´å‡†ç¡®çš„ä¸ªæ€§åŒ–ä½“ç§¯æŠ¥å‘Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b168721366c887167f46d5b0033e7041.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8e62a8822f6dfdf50634b68b964c5f1a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
