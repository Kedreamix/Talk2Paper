<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹æ–¹é¢ï¼Œå·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œä½¿ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å´é¢ä¸´ä¸‰å¤§æ ¹æœ¬é—®é¢˜ï¼šç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ ï¼Œä»¥æœ‰æ•ˆé€‚åº”é¢„è®­ç»ƒGNNçš„ä¸‹æ¸¸ä»»åŠ¡å’Œä»…æä¾›çš„å°‘æ•°è¯­ä¹‰æ ‡ç­¾æ ·æœ¬æ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬ç›‘ç£éƒ½æå¼±ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ä¸­ï¼Œé€šè¿‡åŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºæ¥å®ç°ã€‚ä¸ºäº†å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾æç¤ºæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ç¬¬ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„å›¾è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºå¾®è°ƒæ—¶çš„ç›‘ç£ä¸è¶³ï¼Œåœ¨æˆ‘ä»¬çš„èŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒä¸å˜ï¼Œå› æ­¤å¯å­¦ä¹ çš„å‚æ•°è¿œè¿œå°‘äºå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡åœ¨å®é™…æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„èŒƒå¼åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨æå¼±çš„æ–‡æœ¬ç›‘ç£å°†GNNæ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v1">PDF</a> Preprint, 26 pages</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡è§£å†³äº†åœ¨æ„å»ºå…·æœ‰å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„å›¾å½¢ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡å¼æç¤ºå­¦ä¹ ï¼Œæœ‰æ•ˆé€‚åº”åªæœ‰å°‘æ•°è¯­ä¹‰æ ‡ç­¾æ ·æœ¬çš„é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®çš„æƒ…å¢ƒã€‚æ–°æ–¹æ³•å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ä¸­ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œå¹¶å»ºç«‹äº†é¦–ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿå°†GNNsæ¨å¹¿åˆ°æœªè§ç±»åˆ«ï¼Œå…·æœ‰æå¼±çš„æ–‡æœ¬ç›‘ç£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åœ¨ä½¿ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ„å»ºè§†è§‰æ¨¡å‹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ„å»ºå¯è½¬ç§»çš„å›¾å½¢ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ–¹é¢ã€‚</li>
<li>æå‡ºåˆ©ç”¨å¤šæ¨¡å¼æç¤ºå­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åªæœ‰å°‘é‡æœ‰è¯­ä¹‰æ ‡ç­¾çš„æ ·æœ¬æ—¶ã€‚</li>
<li>é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›¸åŒç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºï¼Œæé«˜äº†ç°æœ‰å›¾æç¤ºæ–¹æ³•çš„æ•ˆæœã€‚</li>
<li>æå‡ºé¦–ä¸ªå›¾è¯­è¨€å¤šæ¨¡å¼æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>å»ºç«‹äº†é¦–ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œå¯ä»¥æ¨å¹¿åˆ°æœªè§ç±»åˆ«ï¼Œå…·æœ‰æå¼±çš„æ–‡æœ¬ç›‘ç£èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-418010f18183fa1efe9737e4f27a25e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fea15ced17268cfd3a73eccc52e763fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a61bf3a22face5a83a5c80d12004d0a7.jpg" align="middle">
</details>




<h2 id="Hyperband-based-Bayesian-Optimization-for-Black-box-Prompt-Selection"><a href="#Hyperband-based-Bayesian-Optimization-for-Black-box-Prompt-Selection" class="headerlink" title="Hyperband-based Bayesian Optimization for Black-box Prompt Selection"></a>Hyperband-based Bayesian Optimization for Black-box Prompt Selection</h2><p><strong>Authors:Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra</strong></p>
<p>Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks. As the most powerful models are proprietary and can only be invoked via an API, users often manually refine prompts in a black-box setting by adjusting instructions and few-shot examples until they achieve good performance as measured on a validation set. Recent methods addressing static black-box prompt selection face significant limitations: They often fail to leverage the inherent structure of prompts, treating instructions and few-shot exemplars as a single block of text. Moreover, they often lack query-efficiency by evaluating prompts on all validation instances, or risk sub-optimal selection of a prompt by using random subsets of validation instances. We introduce HbBoPs, a novel Hyperband-based Bayesian optimization method for black-box prompt selection addressing these key limitations. Our approach combines a structural-aware deep kernel Gaussian Process to model prompt performance with Hyperband as a multi-fidelity scheduler to select the number of validation instances for prompt evaluations. The structural-aware modeling approach utilizes separate embeddings for instructions and few-shot exemplars, enhancing the surrogate modelâ€™s ability to capture prompt performance and predict which prompt to evaluate next in a sample-efficient manner. Together with Hyperband as a multi-fidelity scheduler we further enable query-efficiency by adaptively allocating resources across different fidelity levels, keeping the total number of validation instances prompts are evaluated on low. Extensive evaluation across ten benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods. </p>
<blockquote>
<p>æœ€ä¼˜æç¤ºçš„é€‰æ‹©å¯¹äºä¸‹æ¸¸ä»»åŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½æœ€å¤§åŒ–è‡³å…³é‡è¦ã€‚ç”±äºæœ€å¼ºå¤§çš„æ¨¡å‹æ˜¯ä¸“æœ‰æ€§è´¨çš„ï¼Œåªèƒ½é€šè¿‡APIè¿›è¡Œè°ƒç”¨ï¼Œç”¨æˆ·é€šå¸¸ä¼šåœ¨ä¸€ä¸ªé»‘ç®±ç¯å¢ƒä¸­æ‰‹åŠ¨è°ƒæ•´æç¤ºï¼Œé€šè¿‡è°ƒæ•´æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œç›´åˆ°åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚è§£å†³é™æ€é»‘ç®±æç¤ºé€‰æ‹©æœ€è¿‘çš„æ–¹æ³•é¢ä¸´é‡å¤§å±€é™æ€§ï¼šå®ƒä»¬å¾€å¾€æœªèƒ½åˆ©ç”¨æç¤ºçš„å›ºæœ‰ç»“æ„ï¼Œå°†æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹è§†ä¸ºä¸€æ®µæ–‡æœ¬ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹æŸ¥è¯¢æ•ˆç‡ï¼Œé€šè¿‡å¯¹æ‰€æœ‰éªŒè¯å®ä¾‹è¿›è¡Œè¯„ä¼°æ¥è¯„ä¼°æç¤ºï¼Œæˆ–ä½¿ç”¨éªŒè¯å®ä¾‹çš„éšæœºå­é›†æ¥é€‰æ‹©æç¤ºï¼Œå­˜åœ¨é€‰æ‹©ä¸ä½³çš„é£é™©ã€‚æˆ‘ä»¬å¼•å…¥äº†HbBoPsï¼Œè¿™æ˜¯ä¸€ç§åŸºäºHyperbandçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³è¿™äº›å…³é”®é™åˆ¶çš„é»‘ç®±æç¤ºé€‰æ‹©é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦å†…æ ¸é«˜æ–¯è¿‡ç¨‹æ¥æ¨¡æ‹Ÿæç¤ºæ€§èƒ½ï¼Œä»¥åŠHyperbandä½œä¸ºä¸€ç§å¤šä¿çœŸè°ƒåº¦å™¨æ¥é€‰æ‹©ç”¨äºæç¤ºè¯„ä¼°çš„éªŒè¯å®ä¾‹æ•°é‡ã€‚ç»“æ„æ„ŸçŸ¥å»ºæ¨¡æ–¹æ³•ä½¿ç”¨å•ç‹¬çš„åµŒå…¥æ¥è¡¨ç¤ºæŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œæé«˜äº†æ›¿ä»£æ¨¡å‹æ•æ‰æç¤ºæ€§èƒ½çš„èƒ½åŠ›ï¼Œå¹¶ä»¥æ ·æœ¬é«˜æ•ˆçš„æ–¹å¼é¢„æµ‹ä¸‹ä¸€ä¸ªè¦è¯„ä¼°çš„æç¤ºã€‚ä¸Hyperbandå¤šä¿çœŸè°ƒåº¦å™¨ç›¸ç»“åˆï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡é€‚åº”æ€§åœ°åˆ†é…èµ„æºåœ¨ä¸åŒä¿çœŸåº¦çº§åˆ«ä¹‹é—´å®ç°äº†æŸ¥è¯¢æ•ˆç‡çš„æé«˜ï¼ŒåŒæ—¶ä¿æŒè¯„ä¼°æç¤ºæ‰€éœ€çš„éªŒè¯å®ä¾‹æ€»æ•°è¾ƒä½ã€‚åœ¨åä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHbBoPsä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07820v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€ä¼˜æç¤ºé€‰æ‹©å¯¹äºæœ€å¤§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚ç”±äºæœ€å¼ºå¤§çš„æ¨¡å‹æ˜¯ä¸“æœ‰æ€§è´¨çš„ï¼Œåªèƒ½é€šè¿‡APIè°ƒç”¨ï¼Œç”¨æˆ·é€šå¸¸åœ¨é»‘ç®±è®¾ç½®ä¸­æ‰‹åŠ¨è°ƒæ•´æç¤ºï¼Œé€šè¿‡è°ƒæ•´æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œç›´è‡³åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚é’ˆå¯¹é™æ€é»‘ç®±æç¤ºé€‰æ‹©çš„æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šå®ƒä»¬é€šå¸¸æ— æ³•åˆ©ç”¨æç¤ºçš„å›ºæœ‰ç»“æ„ï¼Œå°†æŒ‡ä»¤å’Œå°‘é‡èŒƒä¾‹è§†ä¸ºä¸€æ®µæ–‡æœ¬è¿›è¡Œå¤„ç†ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹æŸ¥è¯¢æ•ˆç‡ï¼Œå®ƒä»¬å¸¸å¸¸å¯¹æ‰€æœ‰éªŒè¯å®ä¾‹è¿›è¡Œè¯„ä¼°ï¼Œæˆ–ä½¿ç”¨éšæœºå­é›†è¿›è¡Œé€‰æ‹©ï¼Œå­˜åœ¨é€‰æ‹©æ¬¡ä¼˜æç¤ºçš„é£é™©ã€‚æˆ‘ä»¬å¼•å…¥äº†HbBoPsï¼Œè¿™æ˜¯ä¸€ç§åŸºäºHyperbandçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³è¿™äº›å…³é”®é™åˆ¶çš„é»‘ç®±æç¤ºé€‰æ‹©é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦å†…æ ¸é«˜æ–¯è¿‡ç¨‹æ¥å¯¹æç¤ºæ€§èƒ½è¿›è¡Œå»ºæ¨¡ï¼Œä»¥åŠä½¿ç”¨Hyperbandä½œä¸ºå¤šä¿çœŸè°ƒåº¦å™¨æ¥é€‰æ‹©éªŒè¯å®ä¾‹çš„æ•°é‡æ¥è¿›è¡Œæç¤ºè¯„ä¼°ã€‚ç»“æ„æ„ŸçŸ¥å»ºæ¨¡æ–¹æ³•ä½¿ç”¨å•ç‹¬çš„åµŒå…¥æ¥å¤„ç†æŒ‡ä»¤å’Œå°‘é‡èŒƒä¾‹ï¼Œå¢å¼ºäº†æ›¿ä»£æ¨¡å‹æ•æ‰æç¤ºæ€§èƒ½çš„èƒ½åŠ›ï¼Œå¹¶ä»¥æ ·æœ¬æ•ˆç‡é«˜çš„æ–¹å¼é¢„æµ‹ä¸‹ä¸€ä¸ªè¦è¯„ä¼°çš„æç¤ºã€‚åŠ ä¸ŠHyperbandä½œä¸ºå¤šä¿çœŸè°ƒåº¦å™¨ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡åœ¨ä¸åŒä¿çœŸåº¦çº§åˆ«ä¸Šè‡ªé€‚åº”åœ°åˆ†é…èµ„æºæ¥å®ç°æŸ¥è¯¢æ•ˆç‡ï¼Œä¿æŒè¯„ä¼°æç¤ºæ‰€éœ€çš„éªŒè¯å®ä¾‹æ€»æ•°è¾ƒä½ã€‚åœ¨åä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHbBoPsä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ€ä¼˜æç¤ºé€‰æ‹©å¯¹äºæœ€å¤§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é™æ€é»‘ç®±æç¤ºé€‰æ‹©æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹åˆ©ç”¨æç¤ºçš„å›ºæœ‰ç»“æ„å’ŒæŸ¥è¯¢æ•ˆç‡ã€‚</li>
<li>HbBoPsæ˜¯ä¸€ç§åŸºäºHyperbandçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºé»‘ç®±æç¤ºé€‰æ‹©ã€‚</li>
<li>HbBoPsç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦å†…æ ¸é«˜æ–¯è¿‡ç¨‹å»ºæ¨¡å’ŒHyperbandå¤šä¿çœŸè°ƒåº¦å™¨ï¼Œä»¥æé«˜æŸ¥è¯¢æ•ˆç‡å’Œæ ·æœ¬æ•ˆç‡ã€‚</li>
<li>ç»“æ„æ„ŸçŸ¥å»ºæ¨¡æ–¹æ³•ä½¿ç”¨å•ç‹¬çš„åµŒå…¥å¤„ç†æŒ‡ä»¤å’Œå°‘é‡èŒƒä¾‹ï¼Œå¢å¼ºäº†æ›¿ä»£æ¨¡å‹æ•æ‰æç¤ºæ€§èƒ½çš„èƒ½åŠ›ã€‚</li>
<li>HbBoPsé€šè¿‡åœ¨ä¸åŒä¿çœŸåº¦çº§åˆ«ä¸Šè‡ªé€‚åº”åœ°åˆ†é…èµ„æºï¼Œè¿›ä¸€æ­¥æé«˜æŸ¥è¯¢æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-01dd34d53faeba11e849bc1ac1eafbf6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0f10c3d9cc6dee2a4b307d0dd58def6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f1e1dc1fff300483efe9ad29df025fc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c4f2304b8053e1c0381a3f36dacfffa4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-17e210f00263e7d102dac7a9c22d5023.jpg" align="middle">
</details>




<h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition<del>(FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a \underline{\textbf{M}}atryoshka M\underline{\textbf{A}}mba and Co\underline{\textbf{N}}tras\underline{\textbf{T}}ive Le\underline{\textbf{A}}rning framework</del>(\textbf{Manta}). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. The code is released at <a target="_blank" rel="noopener" href="https://github.com/wenbohuang1002/Manta">https://github.com/wenbohuang1002/Manta</a>. </p>
<blockquote>
<p>åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œè§†é¢‘çš„é•¿å­åºåˆ—è‡ªç„¶åœ°æ›´æœ‰æ•ˆåœ°è¡¨è¾¾æ•´ä¸ªåŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸»æµåŸºäºTransformerçš„æ–¹æ³•çš„è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚æœ€è¿‘çš„Mambaåœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢è¡¨ç°å‡ºäº†æ•ˆç‡ï¼Œä½†ç›´æ¥å°†Mambaåº”ç”¨äºFSARå¿½ç•¥äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒåŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç´¯ç§¯ç±»å†…æ–¹å·®ï¼Œè¿™ä¼šå¯¹FSARæ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª<strong>Manta</strong>ï¼ˆMatryoshka Mambaå’Œå¯¹æ¯”æ€§å­¦ä¹ æ¡†æ¶ï¼‰ã€‚é¦–å…ˆï¼ŒMatryoshka Mambaå¼•å…¥äº†å¤šä¸ªå†…éƒ¨æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å¯¹å…¨å±€ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚å¤–éƒ¨æ¨¡å—æ•è·è¿™äº›å±€éƒ¨ç‰¹å¾çš„æ—¶é—´çº¿ä¾èµ–æ€§ï¼Œä»¥è¿›è¡Œéšå¼çš„æ—¶é—´å¯¹é½ã€‚å…¶æ¬¡ï¼Œç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œè®¾è®¡äº†ä¸€ç§æ··åˆå¯¹æ¯”å­¦ä¹ æ¨¡å¼ï¼Œä»¥å‡è½»ç±»å†…æ–¹å·®ç§¯ç´¯çš„ä¸åˆ©å½±å“ã€‚Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ æ¨¡å¼åœ¨Mantaçš„å¹¶è¡Œåˆ†æ”¯ä¸­è¿è¡Œï¼Œå¢å¼ºäº†Mambaå¯¹é•¿å­åºåˆ—çš„FSARèƒ½åŠ›ã€‚Mantaåœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°çŠ¶æ€çš„æ€§èƒ½ã€‚å¤§é‡çš„å®è¯ç ”ç©¶è¯æ˜ï¼ŒMantaä»å¤šä¸ªè§’åº¦æ˜¾è‘—æé«˜äº†é•¿å­åºåˆ—çš„FSARæ€§èƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wenbohuang1002/Manta%E3%80%82">https://github.com/wenbohuang1002/Mantaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨few-shotåŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢†åŸŸï¼Œé•¿è§†é¢‘å­åºåˆ—èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¡¨è¾¾å®Œæ•´åŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸»æµåŸºäºTransformerçš„æ–¹æ³•çš„è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚è¿‘æœŸMambaæ¨¡å‹åœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢è¡¨ç°å‡ºæ•ˆç‡ï¼Œä½†ç›´æ¥åº”ç”¨äºFSARæ—¶å¿½ç•¥äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒåŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç´¯ç§¯ç±»å†…å·®å¼‚ï¼Œå¯¹FSARæ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Mantaæ¡†æ¶ï¼Œå®ƒç»“åˆäº†Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ ç­–ç•¥ã€‚Matryoshka Mambaé€šè¿‡å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œéç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚å¤–éƒ¨æ¨¡å—åˆ™æ•æ‰è¿™äº›å±€éƒ¨ç‰¹å¾æ—¶é—´çº¿çš„ä¾èµ–æ€§ï¼Œå®ç°éšå¼æ—¶é—´å¯¹é½ã€‚æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ç»“åˆç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨å‡è½»ç±»å†…å·®å¼‚ç´¯ç§¯çš„è´Ÿé¢å½±å“ã€‚Mantaåœ¨å¤šä¸ªè‘—ååŸºå‡†æµ‹è¯•ä¸Šå®ç°ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬SSv2ã€Kineticsã€UCF101å’ŒHMDB51ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wenbohuang1002/Manta%E3%80%82">https://github.com/wenbohuang1002/Mantaã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mantaæ¡†æ¶ç»“åˆäº†Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥è§£å†³few-shotåŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>Matryoshka Mambaé€šè¿‡å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„è¯†åˆ«æ•ˆæœã€‚</li>
<li>å¤–éƒ¨æ¨¡å—æ•æ‰å±€éƒ¨ç‰¹å¾çš„æ—¶é—´çº¿ä¾èµ–æ€§ï¼Œä»¥è¿›è¡Œéšå¼çš„æ—¶é—´å¯¹é½ã€‚</li>
<li>æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ç»“åˆç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œä»¥å‡è½»ç±»å†…å·®å¼‚ç´¯ç§¯å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚</li>
<li>Mantaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤„ç†é•¿è§†é¢‘å­åºåˆ—çš„FSARä»»åŠ¡ã€‚</li>
<li>ä»£ç å·²ç»å‘å¸ƒåœ¨æŒ‡å®šçš„GitHubé“¾æ¥ä¸Šï¼Œæ–¹ä¾¿ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ä¸éªŒè¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c31ca7993f28d9e73b067de3c064c6c5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2d4a80e3c728a35dc9f095f98affe90e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-24cb41967c6881d93a2f54f1b672aa61.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>




<h2 id="ConceptSearch-Towards-Efficient-Program-Search-Using-LLMs-for-Abstraction-and-Reasoning-Corpus-ARC"><a href="#ConceptSearch-Towards-Efficient-Program-Search-Using-LLMs-for-Abstraction-and-Reasoning-Corpus-ARC" class="headerlink" title="ConceptSearch: Towards Efficient Program Search Using LLMs for   Abstraction and Reasoning Corpus (ARC)"></a>ConceptSearch: Towards Efficient Program Search Using LLMs for   Abstraction and Reasoning Corpus (ARC)</h2><p><strong>Authors:Kartik Singhal, Gautam Shroff</strong></p>
<p>The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to artificial intelligence, demanding broad generalization and few-shot learning capabilities that remain elusive for current deep learning methods, including large language models (LLMs). While LLMs excel in program synthesis, their direct application to ARC yields limited success. To address this, we introduce ConceptSearch, a novel function-search algorithm that leverages LLMs for program generation and employs a concept-based scoring method to guide the search efficiently. Unlike simplistic pixel-based metrics like Hamming distance, ConceptSearch evaluates programs on their ability to capture the underlying transformation concept reflected in the input-output examples. We explore three scoring functions: Hamming distance, a CNN-based scoring function, and an LLM-based natural language scoring function. Experimental results demonstrate the effectiveness of ConceptSearch, achieving a significant performance improvement over direct prompting with GPT-4. Moreover, our novel concept-based scoring exhibits up to 30% greater efficiency compared to Hamming distance, measured in terms of the number of iterations required to reach the correct solution. These findings highlight the potential of LLM-driven program search when integrated with concept-based guidance for tackling challenging generalization problems like ARC. </p>
<blockquote>
<p>æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰å¯¹äººå·¥æ™ºèƒ½æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¦æ±‚å…·å¤‡å¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›å’Œå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œè€Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­‰å°šæœªå…·å¤‡è¿™äº›èƒ½åŠ›ã€‚è™½ç„¶LLMåœ¨ç¨‹åºåˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç›´æ¥åº”ç”¨äºARCçš„æˆåŠŸç‡æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ConceptSearchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŠŸèƒ½æœç´¢ç®—æ³•ï¼Œå®ƒåˆ©ç”¨LLMè¿›è¡Œç¨‹åºç”Ÿæˆï¼Œå¹¶é‡‡ç”¨åŸºäºæ¦‚å¿µçš„åˆ†æ•°è®¡ç®—æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼æœç´¢ã€‚ä¸åŒäºç®€å•çš„åƒç´ çº§åº¦é‡æ ‡å‡†ï¼ˆå¦‚æ±‰æ˜è·ç¦»ï¼‰ï¼ŒConceptSearchæ ¹æ®ç¨‹åºæ•æ‰è¾“å…¥è¾“å‡ºç¤ºä¾‹ä¸­æ‰€åæ˜ çš„åŸºæœ¬è½¬æ¢æ¦‚å¿µçš„èƒ½åŠ›æ¥è¯„ä¼°ç¨‹åºã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§è¯„åˆ†åŠŸèƒ½ï¼šæ±‰æ˜è·ç¦»ã€åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„è¯„åˆ†åŠŸèƒ½ä»¥åŠåŸºäºLLMçš„è‡ªç„¶è¯­è¨€è¯„åˆ†åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConceptSearchçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡GPT-4ç›´æ¥æç¤ºå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ–°é¢–çš„æ¦‚å¿µåŸºç¡€è¯„åˆ†è¡¨ç°å‡ºé«˜è¾¾30%çš„æ•ˆç‡æå‡ï¼Œç›¸è¾ƒäºæ±‰æ˜è·ç¦»ï¼Œè¿™ä½“ç°åœ¨è¾¾åˆ°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ‰€éœ€è¿­ä»£æ¬¡æ•°ä¸Šã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“ä¸æ¦‚å¿µåŸºç¡€æŒ‡å¯¼ç›¸ç»“åˆæ—¶ï¼ŒLLMé©±åŠ¨çš„ç¨‹åºæœç´¢åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–é—®é¢˜ï¼ˆå¦‚ARCï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07322v2">PDF</a> Pre-print of paper accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰å¯¹äººå·¥æ™ºèƒ½æå‡ºçš„æŒ‘æˆ˜ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†…ï¼Œä»éš¾ä»¥åº”å¯¹å¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConceptSearchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‡½æ•°æœç´¢ç®—æ³•ï¼Œåˆ©ç”¨LLMsè¿›è¡Œç¨‹åºç”Ÿæˆï¼Œå¹¶é‡‡ç”¨åŸºäºæ¦‚å¿µçš„åˆ†æ•°è®¡ç®—æ–¹æ³•ï¼Œä»¥æŒ‡å¯¼é«˜æ•ˆæœç´¢ã€‚ä¸åŒäºç®€å•çš„åƒç´ çº§åº¦é‡æ ‡å‡†ï¼ˆå¦‚æ±‰æ˜è·ç¦»ï¼‰ï¼ŒConceptSearché€šè¿‡è¯„ä¼°ç¨‹åºåœ¨æ•æ‰è¾“å…¥è¾“å‡ºç¤ºä¾‹ä¸­åæ˜ çš„åº•å±‚è½¬æ¢æ¦‚å¿µçš„èƒ½åŠ›æ¥è¿›è¡Œè¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConceptSearchå¯¹äºæé«˜ç¨‹åºæ€§èƒ½çš„æœ‰æ•ˆæ€§æ˜æ˜¾ä¼˜äºç›´æ¥é‡‡ç”¨GPT-4æç¤ºçš„æ–¹å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–°å‹æ¦‚å¿µè¯„åˆ†æ³•ç›¸è¾ƒäºæ±‰æ˜è·ç¦»å±•ç°äº†é«˜è¾¾30%çš„æ•ˆç‡æå‡ï¼Œè¿™ä½“ç°åœ¨è¾¾åˆ°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ä¸Šã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ•´åˆæ¦‚å¿µæŒ‡å¯¼çš„LLMé©±åŠ¨ç¨‹åºæœç´¢åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–é—®é¢˜ï¼ˆå¦‚ARCï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARCå¯¹äººå·¥æ™ºèƒ½æå‡ºäº†æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹éš¾ä»¥åº”å¯¹ã€‚</li>
<li>ConceptSearchæ˜¯ä¸€ç§æ–°å‹çš„åŸºäºå‡½æ•°æœç´¢ç®—æ³•çš„æ–¹æ³•ï¼Œåˆ©ç”¨LLMsè¿›è¡Œç¨‹åºç”Ÿæˆï¼Œå¹¶å¼•å…¥äº†æ¦‚å¿µè¯„åˆ†æ–¹æ³•æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„åƒç´ çº§åº¦é‡æ–¹æ³•ä¸åŒï¼ŒConceptSearchè¯„ä¼°ç¨‹åºçš„åº•å±‚è½¬æ¢æ¦‚å¿µæ•æ‰èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ConceptSearchåœ¨æ€§èƒ½ä¸Šä¼˜äºç›´æ¥é‡‡ç”¨GPT-4æç¤ºçš„æ–¹æ³•ã€‚</li>
<li>ä¸æ±‰æ˜è·ç¦»ç›¸æ¯”ï¼Œæ–°å‹æ¦‚å¿µè¯„åˆ†æ³•å±•ç°äº†æ›´é«˜çš„æ•ˆç‡ï¼Œå‡å°‘äº†è¾¾åˆ°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ã€‚</li>
<li>LLMé©±åŠ¨çš„ConceptSearchæ•´åˆæ¦‚å¿µæŒ‡å¯¼æœ‰æ½œåŠ›è§£å†³æŒ‘æˆ˜æ€§çš„æ³›åŒ–é—®é¢˜å¦‚ARCã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2613cea523f279353fa3751f21a7065.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-db56a44e75b3188e43f133bd7e66c6d5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-df0d3fc5c766bf805ea80ae5c747e19a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0fec33225186bb757a2027c85b0d7450.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-604dfa5c9456293857026a42cf75e857.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4dd11d3ca1a63eab0de227cccb66a3cb.jpg" align="middle">
</details>




<h2 id="IntellectSeeker-A-Personalized-Literature-Management-System-with-the-Probabilistic-Model-and-Large-Language-Model"><a href="#IntellectSeeker-A-Personalized-Literature-Management-System-with-the-Probabilistic-Model-and-Large-Language-Model" class="headerlink" title="IntellectSeeker: A Personalized Literature Management System with the   Probabilistic Model and Large Language Model"></a>IntellectSeeker: A Personalized Literature Management System with the   Probabilistic Model and Large Language Model</h2><p><strong>Authors:Weizhen Bian, Siyan Liu, Yubo Zhou, Dezhi Chen, Yijie Liao, Zhenzhen Fan, Aobo Wang</strong></p>
<p>Faced with the burgeoning volume of academic literature, researchers often need help with uncertain article quality and mismatches in term searches using traditional academic engines. We introduce IntellectSeeker, an innovative and personalized intelligent academic literature management platform to address these challenges. This platform integrates a Large Language Model (LLM)â€“based semantic enhancement bot with a sophisticated probability model to personalize and streamline literature searches. We adopted the GPT-3.5-turbo model to transform everyday language into professional academic terms across various scenarios using multiple rounds of few-shot learning. This adaptation mainly benefits academic newcomers, effectively bridging the gap between general inquiries and academic terminology. The probabilistic model intelligently filters academic articles to align closely with the specific interests of users, which are derived from explicit needs and behavioral patterns. Moreover, IntellectSeeker incorporates an advanced recommendation system and text compression tools. These features enable intelligent article recommendations based on user interactions and present search results through concise one-line summaries and innovative word cloud visualizations, significantly enhancing research efficiency and user experience. IntellectSeeker offers academic researchers a highly customizable literature management solution with exceptional search precision and matching capabilities. The code can be found here: <a target="_blank" rel="noopener" href="https://github.com/LuckyBian/ISY5001">https://github.com/LuckyBian/ISY5001</a> </p>
<blockquote>
<p>é¢å¯¹æ—¥ç›Šå¢é•¿çš„å­¦æœ¯æ–‡çŒ®ï¼Œç ”ç©¶äººå‘˜é€šå¸¸éœ€è¦å¸®åŠ©åº”å¯¹ä¸ç¡®å®šçš„æ–‡ç« è´¨é‡å’Œåœ¨ä¼ ç»Ÿå­¦æœ¯æœç´¢å¼•æ“ä¸­æœ¯è¯­æœç´¢ä¸åŒ¹é…çš„é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºIntellectSeekerï¼Œä¸€ä¸ªåˆ›æ–°ä¸”ä¸ªæ€§åŒ–çš„æ™ºèƒ½å­¦æœ¯æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¯¥å¹³å°ç»“åˆåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰å¢å¼ºæœºå™¨äººå’Œé«˜çº§æ¦‚ç‡æ¨¡å‹ï¼Œå¯¹æ–‡çŒ®æœç´¢è¿›è¡Œä¸ªæ€§åŒ–å’Œç®€åŒ–ã€‚æˆ‘ä»¬é‡‡ç”¨GPT-3.5 Turboæ¨¡å‹ï¼Œé€šè¿‡å¤šè½®å°‘é‡å­¦ä¹ ï¼Œå°†æ—¥å¸¸è¯­è¨€è½¬åŒ–ä¸ºå„ç§åœºæ™¯ä¸­çš„ä¸“ä¸šå­¦æœ¯æœ¯è¯­ã€‚è¿™ç§é€‚åº”ä¸»è¦å¯¹å­¦æœ¯æ–°æ‰‹æœ‰ç›Šï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†æ™®é€šæŸ¥è¯¢å’Œå­¦æœ¯æœ¯è¯­ä¹‹é—´çš„å·®è·ã€‚æ¦‚ç‡æ¨¡å‹æ™ºèƒ½åœ°è¿‡æ»¤å­¦æœ¯æ–‡ç« ï¼Œä»¥ç´§å¯†ç¬¦åˆç”¨æˆ·çš„ç‰¹å®šå…´è¶£ï¼Œè¿™äº›å…´è¶£æ¥æºäºæ˜ç¡®çš„éœ€æ±‚å’Œè¡Œä¸ºæ¨¡å¼ã€‚æ­¤å¤–ï¼ŒIntellectSeekeré›†æˆäº†ä¸€ä¸ªé«˜çº§æ¨èç³»ç»Ÿå’Œæ–‡æœ¬å‹ç¼©å·¥å…·ã€‚è¿™äº›åŠŸèƒ½èƒ½å¤ŸåŸºäºç”¨æˆ·äº¤äº’è¿›è¡Œæ™ºèƒ½æ–‡ç« æ¨èï¼Œå¹¶é€šè¿‡ç®€æ´çš„ä¸€è¡Œæ‘˜è¦å’Œåˆ›æ–°è¯äº‘å¯è§†åŒ–å‘ˆç°æœç´¢ç»“æœï¼Œä»è€Œæ˜¾è‘—æé«˜ç ”ç©¶æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚IntellectSeekerä¸ºå­¦æœ¯ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªé«˜åº¦å¯å®šåˆ¶çš„æ–‡çŒ®ç®¡ç†è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å‡ºè‰²çš„æœç´¢ç²¾åº¦å’ŒåŒ¹é…èƒ½åŠ›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/LuckyBian/ISY5001">https://github.com/LuckyBian/ISY5001</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07213v1">PDF</a> </p>
<p><strong>Summary</strong><br>å­¦æœ¯æ–‡çŒ®æ•°é‡åºå¤§ï¼Œç ”ç©¶è€…é¢ä¸´æ–‡ç« è´¨é‡ä¸ç¡®å®šå’Œæœç´¢æœ¯è¯­ä¸åŒ¹é…çš„é—®é¢˜ã€‚IntellectSeekeræ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦æœ¯æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ¦‚ç‡æ¨¡å‹ï¼Œå®ç°ä¸ªæ€§åŒ–ã€é«˜æ•ˆçš„æ–‡çŒ®æœç´¢ã€‚é‡‡ç”¨GPT-3.5 Turboæ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå°†æ—¥å¸¸è¯­è¨€è½¬åŒ–ä¸ºä¸“ä¸šæœ¯è¯­ã€‚æ¦‚ç‡æ¨¡å‹æ™ºèƒ½è¿‡æ»¤å­¦æœ¯æ–‡ç« ï¼Œä¸ç”¨æˆ·ç‰¹å®šå…´è¶£ç´§å¯†ç»“åˆã€‚æ­¤å¤–ï¼ŒIntellectSeekerè¿˜æä¾›é«˜çº§æ¨èç³»ç»Ÿå’Œæ–‡æœ¬å‹ç¼©å·¥å…·ï¼ŒåŸºäºç”¨æˆ·äº¤äº’è¿›è¡Œæ™ºèƒ½æ–‡ç« æ¨èï¼Œå¹¶é€šè¿‡ç®€æ´çš„ä¸€è¡Œæ‘˜è¦å’Œåˆ›æ–°è¯äº‘å¯è§†åŒ–å‘ˆç°æœç´¢ç»“æœï¼Œæé«˜ç ”ç©¶æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>IntellectSeekeræ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦æœ¯æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œè§£å†³å­¦æœ¯æ–‡çŒ®æµ·é‡ã€è´¨é‡ä¸ç¡®å®šå’Œæœç´¢æœ¯è¯­ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>å¹³å°ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¦‚ç‡æ¨¡å‹ï¼Œå®ç°ä¸ªæ€§åŒ–ã€é«˜æ•ˆçš„æ–‡çŒ®æœç´¢ã€‚</li>
<li>é‡‡ç”¨GPT-3.5 Turboæ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå°†æ—¥å¸¸è¯­è¨€è½¬åŒ–ä¸ºä¸“ä¸šæœ¯è¯­ã€‚</li>
<li>æ¦‚ç‡æ¨¡å‹èƒ½å¤Ÿæ™ºèƒ½è¿‡æ»¤å­¦æœ¯æ–‡ç« ï¼Œç´§å¯†ç»“åˆç”¨æˆ·çš„ç‰¹å®šå…´è¶£ã€‚</li>
<li>IntellectSeekeræä¾›é«˜çº§æ¨èç³»ç»Ÿå’Œæ–‡æœ¬å‹ç¼©å·¥å…·ï¼Œæé«˜ç ”ç©¶æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚</li>
<li>å¹³å°èƒ½åŸºäºç”¨æˆ·äº¤äº’è¿›è¡Œæ™ºèƒ½æ–‡ç« æ¨èï¼Œå¹¶é€šè¿‡ç®€æ´çš„æ‘˜è¦å’Œè¯äº‘å¯è§†åŒ–å‘ˆç°æœç´¢ç»“æœã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e9f90485775ee221d9e9b233f49c3a4f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f3cac191be7491a185c315b81753a45.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-35127a4c7ad66a05d58f3811fc972eaf.jpg" align="middle">
</details>




<h2 id="MM-PoE-Multiple-Choice-Reasoning-via-Process-of-Elimination-using-Multi-Modal-Models"><a href="#MM-PoE-Multiple-Choice-Reasoning-via-Process-of-Elimination-using-Multi-Modal-Models" class="headerlink" title="MM-PoE: Multiple Choice Reasoning via. Process of Elimination using   Multi-Modal Models"></a>MM-PoE: Multiple Choice Reasoning via. Process of Elimination using   Multi-Modal Models</h2><p><strong>Authors:Sayak Chakrabarty, Souradip Pal</strong></p>
<p>This paper introduces Multiple Choice Reasoning via. Process of Elimination using Multi-Modal models, herein referred to as Multi-Modal Process of Elimination (MM-PoE). This novel methodology is engineered to augment the efficacy of Vision-Language Models (VLMs) in multiple-choice visual reasoning tasks. Diverging from conventional approaches that evaluate each option independently, MM-PoE employs a dual-step scoring paradigm that initially identifies and excludes implausible choices, subsequently concentrating on the most probable remaining options. This method emulates human test-taking strategies, where individuals typically eliminate clearly incorrect answers prior to selecting the optimal response. Our empirical evaluations, conducted across three benchmark datasets, reveal that MM-PoE significantly improves both zero-shot and few-shot performance of contemporary state-of-the-art VLMs. Critically, this approach not only broadens the application of the elimination process to multi-modal contexts but also allows few-shot experiments, thereby addressing two principal limitations concerning usage of PoE only in zero-shot settings and only with a language-only framework. As a result, MM-PoE not only refines the reasoning capabilities of VLMs but also broadens their applicability to complex visual question-answering scenarios. All code and documentation supporting our work are available at <a target="_blank" rel="noopener" href="https://pypi.org/project/mm-poe/">https://pypi.org/project/mm-poe/</a>, enabling researchers and practitioners to easily integrate and further develop these techniques. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡æ’é™¤æ³•ï¼ˆProcess of Eliminationï¼‰è¿›è¡Œå¤šæ¨¡æ€é€‰æ‹©æ¨ç†çš„æ–°æ–¹æ³•ï¼Œè¿™é‡Œç§°ä¸ºå¤šæ¨¡æ€æ’é™¤æ³•ï¼ˆMM-PoEï¼‰ã€‚è¿™ç§æ–°æ–¹æ³•æ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé€‰é¡¹è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªé€‰é¡¹çš„æ–¹æ³•ä¸åŒï¼ŒMM-PoEé‡‡ç”¨åŒæ­¥è¯„åˆ†èŒƒå¼ï¼Œé¦–å…ˆè¯†åˆ«å’Œæ’é™¤ä¸å¯èƒ½çš„é€‰é¡¹ï¼Œç„¶åä¸“æ³¨äºæœ€å¯èƒ½çš„å‰©ä½™é€‰é¡¹ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åº”è¯•ç­–ç•¥ï¼Œäººä»¬é€šå¸¸ä¼šåœ¨é€‰æ‹©æœ€ä½³ç­”æ¡ˆä¹‹å‰æ’é™¤æ˜æ˜¾é”™è¯¯çš„ç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMM-PoEæ˜¾è‘—æé«˜äº†å½“å‰æœ€å…ˆè¿›çš„VLMsçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…å°†æ’é™¤è¿‡ç¨‹æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒï¼Œè€Œä¸”å…è®¸è¿›è¡Œå°‘æ ·æœ¬å®éªŒï¼Œä»è€Œè§£å†³äº†PoEä»…ç”¨äºé›¶æ ·æœ¬è®¾ç½®å’Œä»…ç”¨äºè¯­è¨€æ¡†æ¶çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ã€‚å› æ­¤ï¼ŒMM-PoEä¸ä»…æé«˜äº†VLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”æ‰©å¤§äº†å®ƒä»¬åœ¨å¤æ‚è§†è§‰é—®ç­”åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚æ”¯æŒæˆ‘ä»¬å·¥ä½œçš„æ‰€æœ‰ä»£ç å’Œæ–‡æ¡£éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://pypi.org/project/mm-poe/%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E4%BD%BF%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E5%92%8C%E5%AE%9E%E8%B7%B5%E8%80%85%E8%83%BD%E5%A4%9F%E8%BD%BB%E6%9D%BE%E9%9B%86%E6%88%90%E5%B9%B6%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%BC%80%E5%8F%91%E8%BF%99%E4%BA%9B%E6%8A%80%E6%9C%AF%E3%80%82">https://pypi.org/project/mm-poe/ä¸Šæ‰¾åˆ°ï¼Œä½¿ç ”ç©¶äººå‘˜å’Œå®è·µè€…èƒ½å¤Ÿè½»æ¾é›†æˆå¹¶è¿›ä¸€æ­¥å¼€å‘è¿™äº›æŠ€æœ¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07148v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºä¸€ç§åä¸ºå¤šæ¨¡æ€æ’é™¤æ³•ï¼ˆMM-PoEï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé€‰è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸åŒäºç‹¬ç«‹è¯„ä¼°æ¯ä¸ªé€‰é¡¹çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒMM-PoEé‡‡ç”¨åŒæ­¥è¯„åˆ†èŒƒå¼ï¼Œé¦–å…ˆè¯†åˆ«å’Œæ’é™¤ä¸å¯èƒ½çš„é€‰æ‹©ï¼Œç„¶åé›†ä¸­è€ƒè™‘æœ€å¯èƒ½çš„å‰©ä½™é€‰é¡¹ã€‚è¯¥æ–¹æ³•æ¨¡æ‹Ÿäººç±»ç­”é¢˜ç­–ç•¥ï¼Œåœ¨é€‰æ‹©é¢˜ä¸­æ’é™¤æ˜æ˜¾é”™è¯¯çš„ç­”æ¡ˆå†é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMM-PoEæ˜¾è‘—æé«˜å½“å‰æœ€å…ˆè¿›çš„VLMsçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•Multi-Modal Process of Elimination (MM-PoE)ï¼Œç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé€‰è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>MM-PoEé‡‡ç”¨åŒæ­¥è¯„åˆ†èŒƒå¼ï¼Œé¦–å…ˆè¯†åˆ«å’Œæ’é™¤ä¸å¯èƒ½çš„é€‰æ‹©ã€‚</li>
<li>è¯¥æ–¹æ³•æ¨¡æ‹Ÿäººç±»ç­”é¢˜ç­–ç•¥ï¼Œæ’é™¤æ˜æ˜¾é”™è¯¯çš„ç­”æ¡ˆï¼Œå†é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚</li>
<li>MM-PoEæ˜¾è‘—æé«˜é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹å½“ä»£æœ€å…ˆè¿›çš„VLMsçš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æ‰©å¤§äº†æ’é™¤è¿‡ç¨‹åœ¨å¤šç§æ¨¡å¼ä¸Šä¸‹æ–‡ä¸­çš„åº”ç”¨ï¼Œè€Œä¸”å¯ä»¥è¿›è¡Œå°‘é‡çš„å®éªŒï¼Œè§£å†³äº†ä»¥å‰åªèƒ½åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ä½¿ç”¨æ’é™¤æ³•ä¸”ä»…é€‚ç”¨äºè¯­è¨€æ¡†æ¶çš„å±€é™æ€§ã€‚</li>
<li>MM-PoEä¸ä»…æé«˜äº†VLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”æ‰©å¤§äº†å…¶åœ¨å¤æ‚è§†è§‰é—®ç­”åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e7d85f3738de124135a143e9d0b2abc6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7fc346b79393506b801ac21874a648fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e074a958c938d2fa372530ba06dd6b6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4d76e49a8e6abadb644d5f675125c75f.jpg" align="middle">
</details>




<h2 id="DiffCLIP-Few-shot-Language-driven-Multimodal-Classifier"><a href="#DiffCLIP-Few-shot-Language-driven-Multimodal-Classifier" class="headerlink" title="DiffCLIP: Few-shot Language-driven Multimodal Classifier"></a>DiffCLIP: Few-shot Language-driven Multimodal Classifier</h2><p><strong>Authors:Jiaqing Zhang, Mingxiang Cao, Xue Yang, Kai Jiang, Yunsong Li</strong></p>
<p>Visual language models like Contrastive Language-Image Pretraining (CLIP) have shown impressive performance in analyzing natural images with language information. However, these models often encounter challenges when applied to specialized domains such as remote sensing due to the limited availability of image-text pairs for training. To tackle this issue, we introduce DiffCLIP, a novel framework that extends CLIP to effectively convey comprehensive language-driven semantic information for accurate classification of high-dimensional multimodal remote sensing images. DiffCLIP is a few-shot learning method that leverages unlabeled images for pretraining. It employs unsupervised mask diffusion learning to capture the distribution of diverse modalities without requiring labels. The modality-shared image encoder maps multimodal data into a unified subspace, extracting shared features with consistent parameters across modalities. A well-trained image encoder further enhances learning by aligning visual representations with class-label text information from CLIP. By integrating these approaches, DiffCLIP significantly boosts CLIP performance using a minimal number of image-text pairs. We evaluate DiffCLIP on widely used high-dimensional multimodal datasets, demonstrating its effectiveness in addressing few-shot annotated classification tasks. DiffCLIP achieves an overall accuracy improvement of 10.65% across three remote sensing datasets compared with CLIP, while utilizing only 2-shot image-text pairs. The code has been released at <a target="_blank" rel="noopener" href="https://github.com/icey-zhang/DiffCLIP">https://github.com/icey-zhang/DiffCLIP</a>. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†æå¸¦æœ‰è¯­è¨€ä¿¡æ¯çš„è‡ªç„¶å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ¨¡å‹åº”ç”¨äºé¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸæ—¶ï¼Œç”±äºå¯ç”¨äºè®­ç»ƒçš„å›¾ç‰‡æ–‡æœ¬é…å¯¹æ•°é‡æœ‰é™ï¼Œå¾€å¾€ä¼šé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªå°†CLIPæ‰©å±•åˆ°æœ‰æ•ˆä¼ è¾¾å…¨é¢çš„è¯­è¨€é©±åŠ¨è¯­ä¹‰ä¿¡æ¯çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¯¹é«˜ç»´å¤šæ¨¡å¼é¥æ„Ÿå›¾åƒè¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚DiffCLIPæ˜¯ä¸€ç§å°æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡è®°çš„å›¾åƒè¿›è¡Œé¢„è®­ç»ƒã€‚å®ƒé‡‡ç”¨æ— ç›‘ç£çš„æ©è†œæ‰©æ•£å­¦ä¹ æ¥æ•æ‰ä¸åŒæ¨¡å¼çš„åˆ†å¸ƒï¼Œæ— éœ€æ ‡ç­¾ã€‚æ¨¡æ€å…±äº«å›¾åƒç¼–ç å™¨å°†å¤šæ¨¡å¼æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€å­ç©ºé—´ï¼Œæå–è·¨æ¨¡æ€çš„ä¸€è‡´å‚æ•°å…±äº«ç‰¹å¾ã€‚ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å›¾åƒç¼–ç å™¨é€šè¿‡ä½¿è§†è§‰è¡¨ç¤ºä¸CLIPä¸­çš„ç±»åˆ«æ ‡ç­¾æ–‡æœ¬ä¿¡æ¯å¯¹é½ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å­¦ä¹ ã€‚é€šè¿‡æ•´åˆè¿™äº›æ–¹æ³•ï¼ŒDiffCLIPåœ¨æå°‘é‡çš„å›¾åƒæ–‡æœ¬é…å¯¹ä¸‹æ˜¾è‘—æå‡äº†CLIPçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„é«˜ç»´å¤šæ¨¡å¼æ•°æ®é›†ä¸Šè¯„ä¼°äº†DiffCLIPçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å®ƒåœ¨è§£å†³å°æ ·æœ¬æ ‡æ³¨åˆ†ç±»ä»»åŠ¡æ–¹é¢çš„æ•ˆæœã€‚ç›¸è¾ƒäºCLIPï¼ŒDiffCLIPåœ¨ä¸‰ä¸ªé¥æ„Ÿæ•°æ®é›†ä¸­æ€»ä½“å‡†ç¡®ç‡æé«˜äº†10.65%ï¼Œä¸”åœ¨ä»…ä½¿ç”¨2ä¸ªæ ·æœ¬çš„å›¾åƒæ–‡æœ¬é…å¯¹çš„æƒ…å†µä¸‹å–å¾—è¿™ä¸€æˆæœã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/icey-zhang/DiffCLIP%E3%80%82">https://github.com/icey-zhang/DiffCLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07119v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†æè‡ªç„¶å›¾åƒæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œå› ç¼ºä¹è¶³å¤Ÿçš„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºDiffCLIPï¼Œä¸€ä¸ªæ‰©å±•CLIPçš„æ–°å‹æ¡†æ¶ï¼Œèƒ½ä¼ è¾¾å…¨é¢çš„è¯­è¨€é©±åŠ¨è¯­ä¹‰ä¿¡æ¯ï¼Œç”¨äºå¯¹é«˜ç»´å¤šæ¨¡å¼é¥æ„Ÿå›¾åƒè¿›è¡Œç²¾ç¡®åˆ†ç±»ã€‚DiffCLIPé‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡è®°å›¾åƒè¿›è¡Œé¢„è®­ç»ƒï¼Œé‡‡ç”¨æ— ç›‘ç£æ©è†œæ‰©æ•£å­¦ä¹ æ•æ‰å„ç§æ¨¡å¼çš„åˆ†å¸ƒè€Œæ— éœ€æ ‡ç­¾ã€‚æ¨¡æ€å…±äº«å›¾åƒç¼–ç å™¨å°†å¤šæ¨¡å¼æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€å­ç©ºé—´ï¼Œæå–è·¨æ¨¡å¼çš„ä¸€è‡´å‚æ•°å…±äº«ç‰¹å¾ã€‚ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å›¾åƒç¼–ç å™¨é€šè¿‡å°†ä¸CLIPç±»åˆ«æ ‡ç­¾æ–‡æœ¬ä¿¡æ¯å¯¹é½çš„è§†è§‰è¡¨ç¤ºå¢å¼ºå­¦ä¹ ã€‚é€šè¿‡æ•´åˆè¿™äº›æ–¹æ³•ï¼ŒDiffCLIPåœ¨æå°‘å›¾åƒæ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†CLIPçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¸¸ç”¨é«˜ç»´å¤šæ¨¡å¼æ•°æ®é›†ä¸Šè¯„ä¼°äº†DiffCLIPï¼Œè¯æ˜å…¶åœ¨è§£å†³å°‘æ ·æœ¬æ ‡æ³¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ç›¸è¾ƒäºCLIPï¼ŒDiffCLIPåœ¨ä¸‰ä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šçš„æ€»ä½“å‡†ç¡®ç‡æé«˜äº†10.65%ï¼Œä¸”ä»…ä½¿ç”¨2ä¸ªæ ·æœ¬å›¾åƒæ–‡æœ¬å¯¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffCLIPæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ‰©å±•äº†CLIPæ¨¡å‹ï¼Œç”¨äºå¤„ç†é«˜ç»´å¤šæ¨¡å¼é¥æ„Ÿå›¾åƒçš„åˆ†ç±»é—®é¢˜ã€‚</li>
<li>DiffCLIPé‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡è®°å›¾åƒè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>é€šè¿‡æ— ç›‘ç£æ©è†œæ‰©æ•£å­¦ä¹ ï¼ŒDiffCLIPèƒ½æ•æ‰å„ç§æ¨¡å¼çš„åˆ†å¸ƒï¼Œè€Œæ— éœ€ä½¿ç”¨æ ‡ç­¾ã€‚</li>
<li>æ¨¡æ€å…±äº«å›¾åƒç¼–ç å™¨å°†å¤šæ¨¡å¼æ•°æ®æ•´åˆåˆ°ç»Ÿä¸€å­ç©ºé—´ï¼Œæå–è·¨æ¨¡å¼çš„å…±äº«ç‰¹å¾ã€‚</li>
<li>ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ä¸CLIPçš„ç±»åˆ«æ ‡ç­¾æ–‡æœ¬ä¿¡æ¯å¯¹é½ï¼Œå¢å¼ºäº†å­¦ä¹ æ•ˆæœã€‚</li>
<li>DiffCLIPåœ¨å¹¿æ³›ä½¿ç”¨çš„é«˜ç»´å¤šæ¨¡å¼æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜å…¶åœ¨è§£å†³å°‘æ ·æœ¬æ ‡æ³¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cb1a78ab4ee5014272178d3f194ba8e9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1003ffb19b00943ea67ee68d1b9e07cf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-788509afd98d72c0d1a264c7c3c9b394.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-41032838b9e9d9cc668a1bfaef2efb05.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-eb911d468c04f882dadd368a21cde2e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7b7ebfd08c31567487782d69cdb13a33.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bdec4947fe7a8873c2bea41e1b79ac59.jpg" align="middle">
</details>




<h2 id="FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering"><a href="#FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering" class="headerlink" title="FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering"></a>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering</h2><p><strong>Authors:Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji</strong></p>
<p>Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤šè·³é—®ç­”æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå®ƒéœ€è¦åœ¨å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šä¸ªä¿¡æ¯æºä¸Šè¿›è¡Œæ¨ç†ä»¥å›ç­”é—®é¢˜ã€‚è™½ç„¶è§†è§‰é—®ç­”å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¤šè·³è®¾ç½®ä»ç„¶æœªè¢«æ¢ç´¢ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•è·³é—®ç­”æˆ–å•ä¸€æ¨¡æ€ä¸Šï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆç°å®åœºæ™¯ï¼Œå¦‚åˆ†æå¤šæ¨¡æ€æ•™è‚²ææ–™ã€æ€»ç»“å†—é•¿çš„å­¦æœ¯è®ºæ–‡æˆ–è§£é‡Šç»“åˆå›¾è¡¨ã€å›¾åƒå’Œæ–‡æœ¬çš„ç§‘å­¦ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿå®ç°å¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªåŒ…å«äº”ä¸ªé˜¶æ®µçš„ç®¡é“ï¼Œæ¶‰åŠä»ç»´åŸºç™¾ç§‘è·å–ç›¸å…³çš„å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆå¹¶è¿›è¡ŒéªŒè¯ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†ä¸Šå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ ·æœ¬å¤§å°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œç»è¿‡æˆ‘ä»¬çš„åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹å¹³å‡åœ¨ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ä¸Šæ¯”ç»è¿‡äººå·¥æ”¶é›†æ•°æ®è®­ç»ƒçš„æ¨¡å‹é«˜å‡º1.9åˆ†ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•°æ®åˆæˆæ–¹æ³•å°†ä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹æä¾›åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07030v1">PDF</a> 20 pages, 11 figures, 10 tables, Submitted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤šæ¨¡æ€å¤šè·³é—®ç­”ä»»åŠ¡çš„éœ€æ±‚å’ŒæŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†çš„æ–¹æ³•ï¼Œæ—¨åœ¨è®­ç»ƒèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€å¤šè·³é—®ç­”çš„æ¨¡å‹ã€‚æ­¤æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªåŒ…å«äº”ä¸ªé˜¶æ®µçš„æµç¨‹ï¼ŒåŒ…æ‹¬ä»ç»´åŸºç™¾ç§‘è·å–ç›¸å…³å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆï¼Œå¹¶é€šè¿‡ä¸¥æ ¼æ ‡å‡†éªŒè¯æ•°æ®è´¨é‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨åŒç­‰æ ·æœ¬é‡ä¸‹ï¼Œä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å¹³å‡ç²¾ç¡®åŒ¹é…å¾—åˆ†ä¸Šä¼˜äºä½¿ç”¨äººå·¥æ”¶é›†æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤šè·³é—®ç­”ä»»åŠ¡éœ€è¦å¤„ç†å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šç§æ¥æºçš„ä¿¡æ¯ï¼Œå¯¹è§†è§‰é—®ç­”æŠ€æœ¯æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•è·³é—®ç­”æˆ–å•ä¸€æ¨¡æ€ä¸Šï¼Œæ— æ³•æ»¡è¶³åˆ†æå¤šæ¨¡æ€æ•™è‚²ææ–™ã€æ€»ç»“é•¿ç¯‡å­¦æœ¯è®ºæ–‡æˆ–è§£è¯»ç»“åˆå›¾è¡¨ã€æ–‡æœ¬çš„ç§‘å­¦ç ”ç©¶ç­‰çœŸå®åœºæ™¯çš„éœ€æ±‚ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡æ•°æ®é›†æ˜¯å¤šæ¨¡æ€å¤šè·³é—®ç­”é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ä¸»è¦åŸå› ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†çš„æ–°æ–¹æ³•ï¼ŒåŒ…å«äº”ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨è®­ç»ƒå¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ¶‰åŠä»ç»´åŸºç™¾ç§‘è·å–ç›¸å…³å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆï¼Œå¹¶é€šè¿‡ä¸¥æ ¼æ ‡å‡†ç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºä½¿ç”¨äººå·¥æ”¶é›†æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1c2dc25eae655aa1f9822823831cb116.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-17398bc33f88f8a6a7fa79dadeef87c7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-1c32c63d1ed4421e9e3f94c7f51067fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ba2a00dc67c5398d1097a53dc801633.jpg" align="middle">
</details>




<h2 id="AutoReason-Automatic-Few-Shot-Reasoning-Decomposition"><a href="#AutoReason-Automatic-Few-Shot-Reasoning-Decomposition" class="headerlink" title="AutoReason: Automatic Few-Shot Reasoning Decomposition"></a>AutoReason: Automatic Few-Shot Reasoning Decomposition</h2><p><strong>Authors:Arda Sevinc, Abdurrahman Gumus</strong></p>
<p>Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no capability to adjust itself to different queries.   In this work, we propose a system to automatically generate rationales using CoT. Our method improves multi-step implicit reasoning capabilities by decomposing the implicit query into several explicit questions. This provides interpretability for the model, improving reasoning in weaker LLMs. We test our approach with two Q&amp;A datasets: StrategyQA and HotpotQA. We show an increase in accuracy with both, especially on StrategyQA.   To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/miralab-ai/autoreason">https://github.com/miralab-ai/autoreason</a>. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æœ€è¿‘åœ¨ç ”ç©¶ä¸­è¢«å¼•å…¥ï¼Œä½œä¸ºä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é€æ­¥æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒCoTçš„åº”ç”¨æœ‰é™ï¼Œä¾‹å¦‚éœ€è¦æ‰‹å·¥åˆ¶ä½œçš„å°‘é‡æ ·æœ¬æç¤ºï¼Œå¹¶ä¸”æ— æ³•è‡ªè¡Œé€‚åº”ä¸åŒçš„æŸ¥è¯¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨CoTè‡ªåŠ¨ç”Ÿæˆç†ç”±çš„ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†éšå¼æŸ¥è¯¢åˆ†è§£ä¸ºå‡ ä¸ªæ˜¾å¼é—®é¢˜æ¥æé«˜å¤šæ­¥éšå¼æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ºæ¨¡å‹æä¾›äº†å¯è§£é‡Šæ€§ï¼Œæé«˜äº†è¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé—®ç­”æ•°æ®é›†StrategyQAå’ŒHotpotQAä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸¤è€…çš„å‡†ç¡®ç‡éƒ½æœ‰æ‰€æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨StrategyQAä¸Šã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæœ¬ç ”ç©¶çš„å®Œæ•´æºä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/miralab-ai/autoreason%E3%80%82">https://github.com/miralab-ai/autoreasonã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06975v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ”¹è¿›äº†åˆ†æ­¥æ¨ç†çš„æ–¹æ³•ã€‚ä½†ç°æœ‰ç ”ç©¶ä»å­˜åœ¨ä¸è¶³ï¼Œå¦‚éœ€è¦æ‰‹åŠ¨è®¾è®¡çš„å°‘æ•°èŒƒä¾‹æç¤ºä¸”æ— æ³•è‡ªé€‚åº”ä¸åŒæŸ¥è¯¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè§£é‡Šçš„ç³»ç»Ÿï¼Œåˆ©ç”¨CoTæå‡å¤šæ­¥éšå¼æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å°†éšå¼æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªæ˜¾å¼é—®é¢˜æ¥å®ç°ã€‚æ­¤æ–¹æ³•æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œå¢å¼ºäº†å¼±è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨StrategyQAå’ŒHotpotQAé—®ç­”æ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡æœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨StrategyQAä¸Šè¡¨ç°æ˜¾è‘—ã€‚ç›¸å…³ç ”ç©¶çš„å®Œæ•´æºä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä½œä¸ºä¸€ç§æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰ç ”ç©¶é™åˆ¶äº†CoTçš„åº”ç”¨èŒƒå›´ï¼Œä¾‹å¦‚éœ€è¦æ‰‹åŠ¨è®¾è®¡çš„å°‘æ•°èŒƒä¾‹æç¤ºä»¥åŠç¼ºä¹å¯¹ä¸åŒæŸ¥è¯¢çš„è‡ªé€‚åº”æ€§ã€‚</li>
<li>æå‡ºä¸€ç§è‡ªåŠ¨ç”Ÿæˆè§£é‡Šçš„ç³»ç»Ÿï¼Œåˆ©ç”¨CoTæå‡å¤šæ­¥éšå¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å°†éšå¼æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªæ˜¾å¼é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨StrategyQAå’ŒHotpotQAé—®ç­”æ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æé«˜äº†å‡†ç¡®ç‡ã€‚</li>
<li>ç‰¹åˆ«åœ¨StrategyQAæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c167f0777ce79c9a79fea76ffcc1c86e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9c9a3f488f58e157801516903c37d6bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-30804a39d9451a1e81c55cbb1e4d0ea7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c883d88f4370499ccd24f81b02c458b7.jpg" align="middle">
</details>




<h2 id="Fully-Open-Source-Moxin-7B-Technical-Report"><a href="#Fully-Open-Source-Moxin-7B-Technical-Report" class="headerlink" title="Fully Open Source Moxin-7B Technical Report"></a>Fully Open Source Moxin-7B Technical Report</h2><p><strong>Authors:Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</strong></p>
<p>Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be â€œopen-source,â€ which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of â€œopen scienceâ€ through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å†äº†é‡å¤§å˜é©ï¼Œå…¶å—æ¬¢è¿ç¨‹åº¦å’Œèƒ½åŠ›çš„è¿…é€Ÿä¸Šå‡æ ‡å¿—ç€è¿™ä¸€å˜é©ã€‚å¼•é¢†è¿™ä¸€å˜é©çš„æ˜¯ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4å’ŒGPT-o1ï¼Œå®ƒä»¬ç”±äºå‡ºè‰²çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§ï¼Œåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚åŒæ—¶ï¼Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚LLaMAå’ŒMistralï¼Œç”±äºå¯¹è·¨ä¸åŒåº”ç”¨çš„æ¨¡å‹å®šåˆ¶å’Œéƒ¨ç½²çš„ä¾¿æ·æ€§ï¼Œä¸ºLLMæ—¥ç›Šæ™®åŠåšå‡ºäº†å·¨å¤§è´¡çŒ®ã€‚å°½ç®¡å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸ºåˆ›æ–°å’Œç ”å‘å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æœºé‡ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹çš„å•†ä¸šåŒ–å¼•å‘äº†å…³äºé€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå®‰å…¨çš„æ‹…å¿§ã€‚è®¸å¤šå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æœªèƒ½æ»¡è¶³åŸºæœ¬çš„é€æ˜åº¦è¦æ±‚ï¼Œéšç’äº†å…³é”®ç»„ä»¶ï¼Œå¦‚è®­ç»ƒä»£ç å’Œæ•°æ®ï¼Œæœ‰äº›è™½ç„¶å£°ç§°æ˜¯â€œå¼€æºâ€ï¼Œä½†å´ä½¿ç”¨é™åˆ¶æ€§è®¸å¯ï¼Œè¿™å¯èƒ½é˜»ç¢å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥åˆ›æ–°ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Moxin 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒæ˜¯æ ¹æ®æ¨¡å‹å¼€æ”¾æ¡†æ¶ï¼ˆMOFï¼‰å¼€å‘çš„ã€‚MOFæ˜¯ä¸€ä¸ªæ’ååˆ†ç±»ç³»ç»Ÿï¼Œæ ¹æ®æ¨¡å‹çš„å®Œæ•´æ€§å’Œå¼€æ”¾æ€§æ¥è¯„ä¼°äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œéµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è·å–çš„åŸåˆ™ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å…¨é¢å‘å¸ƒé¢„è®­ç»ƒä»£ç å’Œé…ç½®ã€è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†ä»¥åŠä¸­é—´å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼Œè¾¾åˆ°äº†æ¨¡å‹å¼€æ”¾æ¡†æ¶ä¸­çš„æœ€é«˜åˆ†ç±»çº§åˆ«â€œå…¬å¼€ç§‘å­¦â€ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å°æ ·ä¾‹è¯„ä¼°ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06845v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å†äº†é‡å¤§å˜é©ï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚GPT-4å’ŒGPT-o1ç­‰ä¸“æœ‰LLMså› å…¶å“è¶Šæ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§è€Œå—åˆ°AIç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚åŒæ—¶ï¼Œå¼€æºLLMså¦‚LLaMAå’ŒMistralé€šè¿‡æ˜“äºå®šåˆ¶å’Œéƒ¨ç½²ç­‰ç‰¹ç‚¹ä¸ºLLMsçš„æ™®åŠåšå‡ºäº†å·¨å¤§è´¡çŒ®ã€‚ç„¶è€Œï¼Œå•†ä¸šåŒ–çš„LLMså¼•å‘äº†é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå®‰å…¨æ€§ç­‰æ‹…å¿§ã€‚æœ‰äº›å¼€æºLLMsç¼ºä¹é€æ˜åº¦ï¼Œä½¿ç”¨é™åˆ¶æ€§è®¸å¯é˜»ç¢åˆ›æ–°ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†éµå¾ªæ¨¡å‹å¼€æ”¾æ€§æ¡†æ¶ï¼ˆMOFï¼‰çš„Moxin 7Bæ¨¡å‹ã€‚è¯¥æ¨¡å‹éµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®çš„åŸåˆ™ï¼Œè¾¾åˆ°äº†MOFçš„æœ€é«˜åˆ†ç±»çº§åˆ«â€œå…¬å¼€ç§‘å­¦â€ï¼Œå¹¶åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç»å†é‡å¤§å˜é©ã€‚</li>
<li>ä¸“æœ‰LLMså¦‚GPT-4å’ŒGPT-o1å› å“è¶Šæ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§å—åˆ°AIç¤¾åŒºå…³æ³¨ã€‚</li>
<li>å¼€æºLLMså¦‚LLaMAå’ŒMistralæ¨åŠ¨äº†LLMsçš„æ™®åŠï¼Œå› å…¶æ˜“äºå®šåˆ¶å’Œéƒ¨ç½²ã€‚</li>
<li>å•†ä¸šåŒ–çš„LLMså¼•å‘é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå®‰å…¨æ€§ç­‰æ‹…å¿§ã€‚</li>
<li>éƒ¨åˆ†å¼€æºLLMsç¼ºä¹é€æ˜åº¦ï¼Œä½¿ç”¨é™åˆ¶æ€§è®¸å¯é˜»ç¢åˆ›æ–°ã€‚</li>
<li>Moxin 7Bæ¨¡å‹éµå¾ªæ¨¡å‹å¼€æ”¾æ€§æ¡†æ¶ï¼ˆMOFï¼‰ï¼Œå®ç°é«˜é€æ˜åº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c5d8f4d2036b738a4ca529ed6d87573d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-804d135864a1480ed317c9aa47e6972f.jpg" align="middle">
</details>




<h2 id="Few-Shot-Domain-Adaptation-for-Named-Entity-Recognition-via-Joint-Constrained-k-Means-and-Subspace-Selection"><a href="#Few-Shot-Domain-Adaptation-for-Named-Entity-Recognition-via-Joint-Constrained-k-Means-and-Subspace-Selection" class="headerlink" title="Few-Shot Domain Adaptation for Named-Entity Recognition via Joint   Constrained k-Means and Subspace Selection"></a>Few-Shot Domain Adaptation for Named-Entity Recognition via Joint   Constrained k-Means and Subspace Selection</h2><p><strong>Authors:Ayoub Hammal, Benno Uthayasooriyar, Caio Corro</strong></p>
<p>Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER on several English datasets. </p>
<blockquote>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å…·æœ‰ä¸åŒå®ä½“å®šä¹‰çš„è·¨åŸŸä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡é’ˆå¯¹å°‘æ ·æœ¬NERï¼Œæ—¨åœ¨ä»¥æœ€å°çš„ç›‘ç£å°†çŸ¥è¯†è½¬ç§»åˆ°æ–°é¢†åŸŸã€‚ä¸åŒäºä»¥å‰ä»…ä¾èµ–æœ‰é™æ ‡æ³¨æ•°æ®çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼±ç›‘ç£ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†å°‘é‡æœ‰æ ‡ç­¾æ•°æ®é›†ä¸å¤§é‡æ— æ ‡ç­¾æ•°æ®ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ ‡ç­¾ç›‘ç£ã€èšç±»å¤§å°çº¦æŸå’Œé¢†åŸŸç‰¹å®šåˆ¤åˆ«å­ç©ºé—´é€‰æ‹©æ¥æ‰©å±•k-meansç®—æ³•ã€‚è¿™ä¸€ç»Ÿä¸€æ¡†æ¶åœ¨å¤šä¸ªè‹±æ–‡æ•°æ®é›†çš„å°‘æ ·æœ¬NERä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00426v1">PDF</a> COLING 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é—®é¢˜ï¼Œæ—¨åœ¨å°†çŸ¥è¯†è½¬ç§»åˆ°æ–°é¢†åŸŸï¼Œåªéœ€å°‘é‡ç›‘ç£ã€‚ä¸åŒäºä¹‹å‰ä»…ä¾èµ–æœ‰é™æ ‡æ³¨æ•°æ®çš„åšæ³•ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¼±ç›‘ç£ç®—æ³•ï¼Œå°†å°é‡æœ‰æ ‡ç­¾æ•°æ®å¤§é‡æ— æ ‡ç­¾æ•°æ®ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•æ‰©å±•äº†K-meansç®—æ³•ï¼Œå¢åŠ æ ‡ç­¾ç›‘ç£ã€é›†ç¾¤å¤§å°çº¦æŸå’Œé¢†åŸŸç‰¹å®šåˆ¤åˆ«å­ç©ºé—´é€‰æ‹©ã€‚æ­¤ç»Ÿä¸€æ¡†æ¶åœ¨å¤šä¸ªè‹±è¯­æ•°æ®é›†ä¸Šå®ç°äº†å°‘æ ·æœ¬NERçš„æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å…³æ³¨äºå°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼Œæ—¨åœ¨è§£å†³è·¨é¢†åŸŸå®ä½“å®šä¹‰ä¸åŒå¯¼è‡´çš„é€‚ç”¨æ€§å—é™é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¼±ç›‘ç£ç®—æ³•ï¼Œç»“åˆå°é‡æœ‰æ ‡ç­¾æ•°æ®å’Œå¤§é‡æ— æ ‡ç­¾æ•°æ®ï¼Œä»¥è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚</li>
<li>æ‰©å±•äº†K-meansç®—æ³•ï¼Œå¢åŠ æ ‡ç­¾ç›‘ç£ã€é›†ç¾¤å¤§å°çº¦æŸå’Œé¢†åŸŸç‰¹å®šåˆ¤åˆ«å­ç©ºé—´é€‰æ‹©ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†åœ¨å¤šä¸ªè‹±è¯­æ•°æ®é›†ä¸Šçš„å°‘æ ·æœ¬NERæœ€ä½³ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•å…‹æœäº†ä»¥å¾€æ–¹æ³•ä»…ä¾èµ–æœ‰é™æ ‡æ³¨æ•°æ®çš„å±€é™æ€§ã€‚</li>
<li>æ­¤æ¡†æ¶çš„é€šç”¨æ€§ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºä¸åŒé¢†åŸŸçš„å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-536709c68b346c7db578e83970eab0e9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d7808cd24deade66be3a1ccf3e290c9.jpg" align="middle">
</details>




<h2 id="EFSA-Episodic-Few-Shot-Adaptation-for-Text-to-Image-Retrieval"><a href="#EFSA-Episodic-Few-Shot-Adaptation-for-Text-to-Image-Retrieval" class="headerlink" title="EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval"></a>EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval</h2><p><strong>Authors:Muhammad Huzaifa, Yova Kementchedjhieva</strong></p>
<p>Text-to-image retrieval is a critical task for managing diverse visual content, but common benchmarks for the task rely on small, single-domain datasets that fail to capture real-world complexity. Pre-trained vision-language models tend to perform well with easy negatives but struggle with hard negativesâ€“visually similar yet incorrect imagesâ€“especially in open-domain scenarios. To address this, we introduce Episodic Few-Shot Adaptation (EFSA), a novel test-time framework that adapts pre-trained models dynamically to a queryâ€™s domain by fine-tuning on top-k retrieved candidates and synthetic captions generated for them. EFSA improves performance across diverse domains while preserving generalization, as shown in evaluations on queries from eight highly distinct visual domains and an open-domain retrieval pool of over one million images. Our work highlights the potential of episodic few-shot adaptation to enhance robustness in the critical and understudied task of open-domain text-to-image retrieval. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ£€ç´¢æ˜¯ç®¡ç†å¤šæ ·è§†è§‰å†…å®¹çš„å…³é”®ä»»åŠ¡ï¼Œä½†å¸¸è§çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºå°å‹çš„å•ä¸€é¢†åŸŸæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ— æ³•æ•æ‰ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç®€å•çš„è´Ÿé¢æ ·æœ¬æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†éš¾ä»¥åŒºåˆ†çš„è´Ÿé¢æ ·æœ¬ï¼ˆè§†è§‰ä¸Šç›¸ä¼¼ä½†é”™è¯¯çš„å›¾åƒï¼‰æ—¶å´é‡åˆ°äº†å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾åŸŸåœºæ™¯ä¸­å°¤ä¸ºå¦‚æ­¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œæƒ…æ™¯å¼å°æ ·æœ¬é€‚åº”â€ï¼ˆEFSAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æµ‹è¯•æ—¶é—´æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¾®è°ƒtop-kæ£€ç´¢å‡ºçš„å€™é€‰å¯¹è±¡ä»¥åŠä¸ºå®ƒä»¬ç”Ÿæˆçš„åˆæˆæ ‡é¢˜æ¥åŠ¨æ€é€‚åº”æŸ¥è¯¢é¢†åŸŸä¸­çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚EFSAåœ¨å¤šç§é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ³›åŒ–èƒ½åŠ›ï¼Œæ­£å¦‚åœ¨æ¥è‡ªå…«ä¸ªé«˜åº¦ä¸åŒè§†è§‰é¢†åŸŸçš„æŸ¥è¯¢ä»¥åŠä¸€ä¸ªè¶…è¿‡ç™¾ä¸‡å¼ å›¾åƒçš„å¼€æ”¾åŸŸæ£€ç´¢æ± ä¸­çš„è¯„ä¼°æ‰€ç¤ºã€‚æˆ‘ä»¬çš„å·¥ä½œçªæ˜¾äº†æƒ…æ™¯å¼å°æ ·æœ¬é€‚åº”åœ¨å…³é”®çš„ã€å°šæœªå……åˆ†ç ”ç©¶çš„å¼€æ”¾åŸŸæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­æé«˜ç¨³å¥æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00139v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­å¯¹äºç®€å•çš„è´Ÿæ ·æœ¬è¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹äºå¤æ‚è´Ÿæ ·æœ¬ï¼Œç‰¹åˆ«æ˜¯å¼€æ”¾åŸŸåœºæ™¯ä¸­çš„è§†è§‰ç›¸ä¼¼ä½†é”™è¯¯çš„å›¾åƒå­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Episodic Few-Shot Adaptationï¼ˆEFSAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨æµ‹è¯•é˜¶æ®µåŠ¨æ€é€‚åº”æŸ¥è¯¢é¢†åŸŸï¼Œé€šè¿‡å¾®è°ƒå‰kä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å›¾åƒå’Œä¸ºå…¶ç”Ÿæˆçš„åˆæˆå­—å¹•æ¥å®ç°ã€‚EFSAåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡é¢ä¸´çš„æŒ‘æˆ˜åœ¨äºä½¿ç”¨å•ä¸€é¢†åŸŸçš„å°å‹æ•°æ®é›†ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
<li>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨å¤„ç†ç®€å•è´Ÿæ ·æœ¬æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†å¤æ‚è´Ÿæ ·æœ¬ï¼ˆå¦‚è§†è§‰ç›¸ä¼¼ä½†é”™è¯¯çš„å›¾åƒï¼‰æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†Episodic Few-Shot Adaptationï¼ˆEFSAï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´æ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€é€‚åº”æŸ¥è¯¢é¢†åŸŸã€‚</li>
<li>EFSAæ¡†æ¶é€šè¿‡åœ¨å‰kä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å›¾åƒä¸Šå¾®è°ƒå¹¶ä¸ºå…¶ç”Ÿæˆåˆæˆå­—å¹•æ¥å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>EFSAåœ¨å¤šä¸ªä¸åŒé¢†åŸŸçš„æŸ¥è¯¢è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¼€æ”¾åŸŸæ£€ç´¢æ± ä¸­è¶…è¿‡äº†ä¸€ç™¾ä¸‡å¼ å›¾åƒã€‚</li>
<li>EFSAæ¡†æ¶å¼ºè°ƒäº†åœ¨å…³é”®ä¸”æœªå……åˆ†ç ”ç©¶çš„å¼€æ”¾åŸŸæ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­å¢å¼ºç¨³å¥æ€§çš„æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4fd363f8e516b905076e4f4f4e3842b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a41b983f1a0732ba6a53e459beb5a122.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-36d6d79a4631dd957fbde9234b14e258.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-adbb00af663a85ae2eebfcd7307b7810.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-356d223843640a647c13536171602e29.jpg" align="middle">
</details>




<h2 id="ReverseNER-A-Self-Generated-Example-Driven-Framework-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models"><a href="#ReverseNER-A-Self-Generated-Example-Driven-Framework-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models" class="headerlink" title="ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot   Named Entity Recognition with Large Language Models"></a>ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot   Named Entity Recognition with Large Language Models</h2><p><strong>Authors:Anbang Wang, Difei Mei, Zhichao Zhang, Xiuxiu Bai, Ran Yao, Zewen Fang, Min Hu, Zhirui Cao, Haitao Sun, Yifeng Guo, Hongyao Zhou, Yu Guo</strong></p>
<p>This paper presents ReverseNER, a framework aimed at overcoming the limitations of large language models (LLMs) in zero-shot Named Entity Recognition (NER) tasks, particularly in cases where certain entity types have ambiguous boundaries. ReverseNER tackles this challenge by constructing a reliable example library with the reversed process of NER. Rather than beginning with sentences, this method uses an LLM to generate entities based on their definitions and then expands them into full sentences. During sentence generation, the LLM is guided to replicate the structure of a specific â€˜feature sentenceâ€™, extracted from the task sentences by clustering. This results in well-annotated sentences with clearly labeled entities, while preserving semantic and structural similarity to the task sentences. Once the example library is constructed, the method selects the most semantically similar example labels for each task sentence to support the LLMâ€™s inference. We also propose an entity-level self-consistency scoring mechanism to improve NER performance with LLMs. Experiments show that ReverseNER significantly outperforms traditional zero-shot NER with LLMs and surpasses several few-shot methods, marking a notable improvement in NER for domains with limited labeled data. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ReverseNERæ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›å®ä½“ç±»å‹è¾¹ç•Œæ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚ReverseNERé€šè¿‡æ„å»ºä¸€ä¸ªå¯é çš„ç¤ºä¾‹åº“æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥åº“é‡‡ç”¨NERçš„é€†å‘è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸æ˜¯ä»å¥å­å¼€å§‹ï¼Œè€Œæ˜¯ä½¿ç”¨LLMæ ¹æ®å®ä½“å®šä¹‰ç”Ÿæˆå®ä½“ï¼Œç„¶åå°†å…¶æ‰©å±•åˆ°å®Œæ•´çš„å¥å­ã€‚åœ¨ç”Ÿæˆå¥å­æ—¶ï¼ŒLLMè¢«å¼•å¯¼ä»¥å¤åˆ¶ä»ä»»åŠ¡å¥å­ä¸­æå–çš„ç‰¹å®šâ€œç‰¹å¾å¥å­â€çš„ç»“æ„ï¼Œé€šè¿‡èšç±»è¿›è¡Œæå–ã€‚è¿™ç”Ÿæˆäº†å¸¦æœ‰æ˜ç¡®æ ‡è®°å®ä½“çš„è‰¯å¥½æ³¨é‡Šå¥å­ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ä»»åŠ¡å¥å­çš„è¯­ä¹‰å’Œç»“æ„ç›¸ä¼¼æ€§ã€‚ä¸€æ—¦æ„å»ºäº†ç¤ºä¾‹åº“ï¼Œè¯¥æ–¹æ³•å°±ä¼šä¸ºæ¯ä¸ªä»»åŠ¡å¥å­é€‰æ‹©æœ€è¯­ä¹‰ç›¸ä¼¼çš„ç¤ºä¾‹æ ‡ç­¾ï¼Œä»¥æ”¯æŒLLMçš„æ¨æ–­ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å®ä½“çº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„åˆ†æœºåˆ¶ï¼Œä»¥æé«˜LLMçš„NERæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒReverseNERæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é›¶æ ·æœ¬NERå’ŒLLMè¶…è¶Šäº†å‡ ç§å°æ ·æœ¬æ–¹æ³•ï¼Œåœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„é¢†åŸŸæ˜¾è‘—æé«˜äº†NERæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00533v3">PDF</a> </p>
<p><strong>Summary</strong><br>é€†å‘NERæ¡†æ¶æ—¨åœ¨å…‹æœå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶åœ¨å®ä½“ç±»å‹è¾¹ç•Œæ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚å®ƒé€šè¿‡æ„å»ºå¯é çš„ç¤ºä¾‹åº“å’Œé‡‡ç”¨é€†å‘NERè¿‡ç¨‹æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åŸºäºå®ä½“å®šä¹‰ä½¿ç”¨LLMç”Ÿæˆå®ä½“ï¼Œç„¶åæ‰©å±•ä¸ºå®Œæ•´å¥å­ã€‚åœ¨ç”Ÿæˆå¥å­æ—¶ï¼ŒLLMé€šè¿‡èšç±»ä»ä»»åŠ¡å¥å­ä¸­æå–çš„ç‰¹å¾å¥å­è¿›è¡Œå¤åˆ¶ï¼Œä»¥äº§ç”Ÿæ ‡æ³¨æ¸…æ™°çš„å¥å­ã€‚æ„å»ºç¤ºä¾‹åº“åï¼Œæ–¹æ³•ä¼šé€‰æ‹©ä¸ä»»åŠ¡å¥å­æœ€ç›¸ä¼¼çš„ç¤ºä¾‹æ ‡ç­¾æ¥æ”¯æŒLLMçš„æ¨æ–­ã€‚è¿˜æå‡ºäº†ä¸€ç§å®ä½“çº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„åˆ†æœºåˆ¶ï¼Œä»¥æé«˜LLMçš„NERæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒReverseNERæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é›¶æ ·æœ¬NERå’Œå‡ ç§å°æ ·æœ¬æ–‡æœ¬æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„é¢†åŸŸï¼Œæ˜¾è‘—æé«˜äº†NERçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReverseNERæ¡†æ¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºå¯é çš„ç¤ºä¾‹åº“å¹¶é‡‡ç”¨é€†å‘NERè¿‡ç¨‹æ¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•ä½¿ç”¨LLMåŸºäºå®ä½“å®šä¹‰ç”Ÿæˆå®ä½“ï¼Œå¹¶æ‰©å±•ä¸ºå¥å­ã€‚</li>
<li>LLMåœ¨ç”Ÿæˆå¥å­æ—¶æ¨¡ä»¿ä»»åŠ¡å¥å­çš„ç»“æ„å’Œç‰¹å¾ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æœ€ç›¸ä¼¼çš„ç¤ºä¾‹æ ‡ç­¾æ”¯æŒLLMæ¨æ–­æ¥æé«˜NERæ€§èƒ½ã€‚</li>
<li>æå‡ºäº†å®ä½“çº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„åˆ†æœºåˆ¶ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºReverseNERåœ¨å¤šä¸ªæ–¹é¢ä¼˜äºä¼ ç»Ÿå’Œå°‘æ ·æœ¬æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„é¢†åŸŸã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-16a422ad5a61465fbb0fc09ae63b6521.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e4062550094aa078f557d3d317e4952a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-08fbee1dc7d3c218ac56a4097603fea5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fb9311789497aefa2d657067c487cf97.jpg" align="middle">
</details>




<h2 id="Probabilistic-Language-Image-Pre-Training"><a href="#Probabilistic-Language-Image-Pre-Training" class="headerlink" title="Probabilistic Language-Image Pre-Training"></a>Probabilistic Language-Image Pre-Training</h2><p><strong>Authors:Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun</strong></p>
<p>Vision-language models (VLMs) embed aligned image-text pairs into a joint space but often rely on deterministic embeddings, assuming a one-to-one correspondence between images and texts. This oversimplifies real-world relationships, which are inherently many-to-many, with multiple captions describing a single image and vice versa. We introduce Probabilistic Language-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained on a billion-scale image-text dataset using only probabilistic objectives, achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot accuracy with ViT-B&#x2F;16). ProLIP efficiently estimates uncertainty by an â€œuncertainty tokenâ€ without extra parameters. We also introduce a novel inclusion loss that enforces distributional inclusion relationships between image-text pairs and between original and masked inputs. Experiments demonstrate that, by leveraging uncertainty estimates, ProLIP benefits downstream tasks and aligns with intuitive notions of uncertainty, e.g., shorter texts being more uncertain and more general inputs including specific ones. Utilizing text uncertainties, we further improve ImageNet accuracy from 74.6% to 75.8% (under a few-shot setting), supporting the practical advantages of our probabilistic approach. The code is available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†å›¾åƒæ–‡æœ¬å¯¹åµŒå…¥åˆ°è”åˆç©ºé—´ä¸­ï¼Œä½†é€šå¸¸ä¾èµ–äºç¡®å®šæ€§åµŒå…¥ï¼Œå‡è®¾å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´å­˜åœ¨ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ã€‚è¿™ç®€åŒ–äº†çœŸå®ä¸–ç•Œä¸­çš„å…³ç³»ï¼ŒçœŸå®ä¸–ç•Œä¸­çš„å…³ç³»æ˜¯å›ºæœ‰çš„å¤šå¯¹å¤šå…³ç³»ï¼Œå¤šä¸ªå­—å¹•æè¿°å•ä¸ªå›¾åƒï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¦‚ç‡è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆProLIPï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨æ¦‚ç‡ç›®æ ‡åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ¦‚ç‡VLMï¼Œå®ƒå…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ViT-B&#x2F;16çš„ImageNeté›¶æ ·æœ¬å‡†ç¡®ç‡ä¸º74.6%ï¼‰ã€‚ProLIPé€šè¿‡ä¸€ä¸ªâ€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€æœ‰æ•ˆåœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹åŒ…å«æŸå¤±ï¼Œå®ƒå¼ºåˆ¶æ‰§è¡Œå›¾åƒæ–‡æœ¬å¯¹ä»¥åŠåŸå§‹å’Œå±è”½è¾“å…¥ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸ä¸ç¡®å®šæ€§çš„ç›´è§‚æ¦‚å¿µç›¸å»åˆï¼Œä¾‹å¦‚è¾ƒçŸ­çš„æ–‡æœ¬å…·æœ‰æ›´é«˜çš„ä¸ç¡®å®šæ€§ï¼ŒåŒ…æ‹¬ç‰¹å®šè¾“å…¥çš„æ›´é€šç”¨è¾“å…¥ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åˆ©ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†ImageNetçš„å‡†ç¡®ç‡ä»74.6%æé«˜åˆ°75.8%ï¼ˆåœ¨å°æ ·æœ¬è®¾ç½®ä¸‹ï¼‰ï¼Œè¿™æ”¯æŒäº†æˆ‘ä»¬æ¦‚ç‡æ–¹æ³•çš„å®é™…ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18857v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> HuggingFace Hub:   <a target="_blank" rel="noopener" href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291</a>   31 pages, 4.29 MB</p>
<p><strong>Summary</strong></p>
<p>åœ¨æ–‡æœ¬ä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªæ¦‚ç‡æ€§è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆProLIPï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ¦‚ç‡ç›®æ ‡å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚é€šè¿‡ä¸ç¡®å®šæ€§ä»¤ç‰Œæœ‰æ•ˆåœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„åŒ…å«æŸå¤±ï¼Œç”¨äºå¼ºåŒ–å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ä»¥åŠåŸå§‹å’Œæ©ç è¾“å…¥ä¹‹é—´çš„åŒ…å«å…³ç³»ã€‚åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡å¹¶ä¸ä¸ç¡®å®šæ€§ç›´è§‰ç›¸ç¬¦ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProLIPæ˜¯é¦–ä¸ªä½¿ç”¨æ¦‚ç‡ç›®æ ‡åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ¦‚ç‡æ€§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>ProLIPé€šè¿‡å¼•å…¥â€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€æ¥æœ‰æ•ˆåœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ²¡æœ‰å¢åŠ é¢å¤–çš„å‚æ•°ã€‚</li>
<li>ProLIPå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œåœ¨ImageNetä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡è¾¾åˆ°äº†74.6%ã€‚</li>
<li>ProLIPé€šè¿‡ä½¿ç”¨ä¸€ç§æ–°çš„åŒ…å«æŸå¤±ï¼Œå¼ºåŒ–äº†å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ä»¥åŠåŸå§‹å’Œæ©ç è¾“å…¥ä¹‹é—´çš„åŒ…å«å…³ç³»ã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ï¼Œå¹¶ä¸”ç¬¦åˆå…³äºä¸ç¡®å®šæ€§çš„ç›´è§‚è®¤è¯†ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ–‡æœ¬çš„ä¸ç¡®å®šæ€§ï¼Œå¯ä»¥åœ¨ImageNetä¸Šçš„å‡†ç¡®ç‡ä»74.6%æé«˜åˆ°75.8%ï¼Œè¿™æ”¯æŒäº†æ¦‚ç‡æ€§æ–¹æ³•çš„å®é™…ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-538fc994372ff98273c839a40f7af5cf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-27f0a4c409f2d43eb7d7ed60ff4c7a01.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6064d03104a4549ecd34104864a98776.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-047b7b5a14b02d96c9a1740dcfa61ffe.jpg" align="middle">
</details>




<h2 id="Fine-Tuning-CLIPâ€™s-Last-Visual-Projector-A-Few-Shot-Cornucopia"><a href="#Fine-Tuning-CLIPâ€™s-Last-Visual-Projector-A-Few-Shot-Cornucopia" class="headerlink" title="Fine-Tuning CLIPâ€™s Last Visual Projector: A Few-Shot Cornucopia"></a>Fine-Tuning CLIPâ€™s Last Visual Projector: A Few-Shot Cornucopia</h2><p><strong>Authors:Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick PÃ©rez, Raoul de Charette</strong></p>
<p>We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. This paper introduces an alternative way for CLIP adaptation without adding â€˜externalâ€™ parameters to optimize. We find that simply fine-tuning the last projection matrix of the vision encoder leads to performance better than all baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP. This simple approach, coined ProLIP, yields state-of-the-art performance on 11 few-shot classification benchmarks, few-shot domain generalization, cross-dataset transfer, base-to-new class generalization, and test-time adaptation. Code will be made available at: <a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP">https://github.com/astra-vision/ProLIP</a> . </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘å°†å¯¹æ¯”é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPï¼ŒRadfordç­‰äººï¼ˆ2021ï¼‰ï¼‰åº”ç”¨äºå°æ ·æœ¬åˆ†ç±»çš„é—®é¢˜ã€‚æ–‡çŒ®ä¸­è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•æ˜¯é€šè¿‡å­¦ä¹ å†»ç»“çš„è§†è§‰ç‰¹å¾çš„çº¿æ€§åˆ†ç±»å™¨ã€ä¼˜åŒ–è¯åµŒå…¥æˆ–å­¦ä¹ å¤–éƒ¨ç‰¹å¾é€‚é…å™¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä¸å¢åŠ ä¼˜åŒ–æ—¶çš„â€œå¤–éƒ¨â€å‚æ•°å¯¹CLIPè¿›è¡Œé€‚åº”çš„æ›¿ä»£æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°åªéœ€å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æœ€åä¸€ä¸ªæŠ•å½±çŸ©é˜µï¼Œå°±èƒ½è¾¾åˆ°è¶…è¿‡æ‰€æœ‰åŸºå‡†çº¿çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œé€šè¿‡æ­£åˆ™åŒ–è®­ç»ƒä½¿å¾®è°ƒåçš„çŸ©é˜µå’Œé¢„è®­ç»ƒçŸ©é˜µä¹‹é—´çš„è·ç¦»ï¼Œå¯ä»¥ä¸ºCLIPçš„é€‚åº”è¿‡ç¨‹å¢åŠ å¯é æ€§ã€‚è¿™ç§ç®€å•çš„æ–¹æ³•è¢«ç§°ä¸ºProLIPï¼Œåœ¨åŒ…æ‹¬å°æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ã€å°æ ·æœ¬åŸŸæ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€åŸºæœ¬åˆ°æ–°è¯¾ç¨‹æ³›åŒ–ä»¥åŠæµ‹è¯•æ—¶é—´é€‚é…ç­‰å¤šç§æƒ…å†µä¸‹ï¼Œéƒ½å–å¾—äº†å‰æ²¿çš„æ€§èƒ½è¡¨ç°ã€‚ç›¸å…³ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/astra-vision/ProLIPä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05270v2">PDF</a> </p>
<p><strong>Summary</strong><br>é€‚åº”CLIPæ¨¡å‹çš„å°‘æ•°é•œå¤´åˆ†ç±»é—®é¢˜ç ”ç©¶ä¸­ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹é€‚é…æ–¹å¼ã€‚æ— éœ€æ·»åŠ å¤–éƒ¨å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œåªéœ€å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æœ€åä¸€ä¸ªæŠ•å½±çŸ©é˜µå³å¯å–å¾—æ¯”åŸºçº¿æ›´å¥½çš„æ€§èƒ½ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£åˆ™åŒ–è°ƒæ•´ï¼Œè¯¥æ–¹æ³•å¯å®ç°æ›´å¯é çš„CLIPé€‚é…æ•ˆæœã€‚è¿™ç§ç®€æ´çš„æ–¹æ³•ç§°ä¸ºProLIPï¼Œå¯åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€æ–°çš„å°‘æ•°é•œå¤´åˆ†ç±»æ€§èƒ½è¡¨ç°ã€‚å…·ä½“æ•ˆæœå¯é€šè¿‡è®¿é—®å®˜æ–¹GitHubä»“åº“è¿›ä¸€æ­¥äº†è§£ã€‚ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP">https://github.com/astra-vision/ProLIP</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é’ˆå¯¹CLIPæ¨¡å‹çš„å°‘æ•°é•œå¤´åˆ†ç±»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é€‚é…æ–¹æ³•â€”â€”ProLIPã€‚</li>
<li>ProLIPæ–¹æ³•é€šè¿‡å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æœ€åä¸€ä¸ªæŠ•å½±çŸ©é˜µæ¥å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>æ­£åˆ™åŒ–è®­ç»ƒå¢å¼ºäº†ProLIPæ–¹æ³•çš„å¯é æ€§ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ProLIPæ–¹æ³•æ— éœ€æ·»åŠ å¤–éƒ¨å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œæ˜¯ä¸€ç§ç®€æ´æœ‰æ•ˆçš„é€‚é…æ–¹å¼ã€‚</li>
<li>ProLIPåœ¨å¤šç§åœºæ™¯å¦‚å°‘æ•°é•œå¤´åˆ†ç±»ã€é¢†åŸŸæ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»å’ŒåŸºç±»åˆ°æ–°ç±»çš„æ³›åŒ–ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b64c2ce5d15c1e023d2b330f9df85e44.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8cf4d3533200d22118db1342a15db2e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-022910213b360a9d807dd7486a1e8cf0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e605cd4bbcd7fe02f0cd5a6d0a36707.jpg" align="middle">
</details>




<h2 id="Acquiring-Bidirectionality-via-Large-and-Small-Language-Models"><a href="#Acquiring-Bidirectionality-via-Large-and-Small-Language-Models" class="headerlink" title="Acquiring Bidirectionality via Large and Small Language Models"></a>Acquiring Bidirectionality via Large and Small Language Models</h2><p><strong>Authors:Takumi Goto, Hiroyoshi Nagao, Yuta Koreeda</strong></p>
<p>Using token representation from bidirectional language models (LMs) such as BERT is still a widely used approach for token-classification tasks. Even though there exist much larger unidirectional LMs such as Llama-2, they are rarely used to replace the token representation of bidirectional LMs. In this work, we hypothesize that their lack of bidirectionality is keeping them behind. To that end, we propose to newly train a small backward LM and concatenate its representations to those of existing LM for downstream tasks. Through experiments in named entity recognition, we demonstrate that introducing backward model improves the benchmark performance more than 10 points. Furthermore, we show that the proposed method is especially effective for rare domains and in few-shot learning settings. </p>
<blockquote>
<p>ä½¿ç”¨æ¥è‡ªåŒå‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä»¤ç‰Œè¡¨ç¤ºï¼Œä»ç„¶æ˜¯ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡çš„å¹¿æ³›ä½¿ç”¨æ–¹æ³•ã€‚å°½ç®¡å­˜åœ¨æ›´å¤§çš„å•å‘è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚Llama-2ï¼Œä½†å®ƒä»¬å¾ˆå°‘è¢«ç”¨æ¥ä»£æ›¿åŒå‘è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œè¡¨ç¤ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‡è®¾å®ƒä»¬ç¼ºä¹åŒå‘æ€§æ˜¯å®ƒä»¬è½åçš„åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ–°è®­ç»ƒä¸€ä¸ªå°å‹çš„åå‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ç°æœ‰è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºè¿›è¡Œè¿æ¥ï¼Œä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚é€šè¿‡å‘½åå®ä½“è¯†åˆ«çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†å¼•å…¥åå‘æ¨¡å‹å°†åŸºå‡†æ€§èƒ½æé«˜äº†10ç‚¹ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯¹äºç½•è§é¢†åŸŸå’Œå°‘é‡å­¦ä¹ åœºæ™¯å°¤å…¶æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09640v2">PDF</a> Accepted by COLING2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºç»“åˆåŒå‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä»¤ç‰Œè¡¨ç¤ºä¸æ–°è®­ç»ƒçš„é€†å‘è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¡¨ç¤ºï¼Œç”¨äºä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œå¼•å…¥é€†å‘æ¨¡å‹èƒ½æé«˜å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡çš„åŸºå‡†æ€§èƒ½è¶…è¿‡10ä¸ªç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½•è§é¢†åŸŸå’Œå°‘æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­æ•ˆæœæ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒå‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä»¤ç‰Œè¡¨ç¤ºåœ¨ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ä¸­ä»å¹¿æ³›ä½¿ç”¨ã€‚</li>
<li>å¤§å‹å•å‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama-2ï¼‰åœ¨æ›¿æ¢ä»¤ç‰Œè¡¨ç¤ºæ–¹é¢åº”ç”¨è¾ƒå°‘ã€‚</li>
<li>æœ¬ç ”ç©¶å‡è®¾å•å‘æ€§å¯èƒ½æ˜¯å¤§å‹å•å‘è¯­è¨€æ¨¡å‹åº”ç”¨è¾ƒå°‘çš„åŸå› ã€‚</li>
<li>æå‡ºæ–°è®­ç»ƒå°å‹é€†å‘è¯­è¨€æ¨¡å‹å¹¶å°†å…¶è¡¨ç¤ºä¸ç°æœ‰è¯­è¨€æ¨¡å‹ç»“åˆç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œå¼•å…¥é€†å‘æ¨¡å‹æé«˜äº†åŸºå‡†æ€§èƒ½è¶…è¿‡10ä¸ªç‚¹ã€‚</li>
<li>å¼•å…¥é€†å‘æ¨¡å‹çš„æ–¹æ³•åœ¨ç½•è§é¢†åŸŸå’Œå°‘æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­è¡¨ç°å°¤å…¶å‡ºè‰²ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cf4caa6472d10144db09fb5fd447cb66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c249f7c2e3e376b7f5aefedb6d65578f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg" align="middle">
</details>




<h2 id="Adaptable-and-Reliable-Text-Classification-using-Large-Language-Models"><a href="#Adaptable-and-Reliable-Text-Classification-using-Large-Language-Models" class="headerlink" title="Adaptable and Reliable Text Classification using Large Language Models"></a>Adaptable and Reliable Text Classification using Large Language Models</h2><p><strong>Authors:Zhiqiang Wang, Yiran Pang, Yanbin Lin, Xingquan Zhu</strong></p>
<p>Text classification is fundamental in Natural Language Processing (NLP), and the advent of Large Language Models (LLMs) has revolutionized the field. This paper introduces an adaptable and reliable text classification paradigm, which leverages LLMs as the core component to address text classification tasks. Our system simplifies the traditional text classification workflows, reducing the need for extensive preprocessing and domain-specific expertise to deliver adaptable and reliable text classification results. We evaluated the performance of several LLMs, machine learning algorithms, and neural network-based architectures on four diverse datasets. Results demonstrate that certain LLMs surpass traditional methods in sentiment analysis, spam SMS detection, and multi-label classification. Furthermore, it is shown that the systemâ€™s performance can be further enhanced through few-shot or fine-tuning strategies, making the fine-tuned model the top performer across all datasets. Source code and datasets are available in this GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/yeyimilk/llm-zero-shot-classifiers">https://github.com/yeyimilk/llm-zero-shot-classifiers</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€é¡¹åŸºç¡€å·¥ä½œï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°å½»åº•æ”¹å˜äº†è¿™ä¸€é¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§çµæ´»å¯é çš„æ–‡æœ¬åˆ†ç±»èŒƒå¼ï¼Œè¯¥èŒƒå¼ä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ ¸å¿ƒç»„ä»¶æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç®€åŒ–äº†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ†ç±»å·¥ä½œæµç¨‹ï¼Œå‡å°‘äº†å¯¹ç¹çé¢„å¤„ç†å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„éœ€æ±‚ï¼Œä»¥æä¾›çµæ´»å¯é çš„æ–‡æœ¬åˆ†ç±»ç»“æœã€‚æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ã€æœºå™¨å­¦ä¹ ç®—æ³•å’ŒåŸºäºç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒæŸäº›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æã€åƒåœ¾çŸ­ä¿¡æ£€æµ‹å’Œå¤šå…ƒæ ‡ç­¾åˆ†ç±»æ–¹é¢çš„è¡¨ç°è¶…è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°æ ·æœ¬å­¦ä¹ æˆ–å¾®è°ƒç­–ç•¥ï¼Œç³»ç»Ÿçš„æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œä½¿å¾®è°ƒæ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚æºä»£ç å’Œæ•°æ®é›†å¯åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yeyimilk/llm-zero-shot-classifiers%E3%80%82">https://github.com/yeyimilk/llm-zero-shot-classifiersã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10523v3">PDF</a> ICDM Workshop ARRL 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„æ–°èŒƒå¼ã€‚è¯¥ç³»ç»Ÿçš„ä¼˜åŠ¿åœ¨äºç®€åŒ–äº†ä¼ ç»Ÿæ–‡æœ¬åˆ†ç±»çš„å·¥ä½œæµç¨‹ï¼Œå‡å°‘äº†å¤§é‡çš„é¢„å¤„ç†éœ€æ±‚ï¼Œä¸”èƒ½é€‚åº”å¤šç§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡å¯¹ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€æœºå™¨å­¦ä¹ ç®—æ³•å’Œç¥ç»ç½‘ç»œæ¶æ„åœ¨å››ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œå‘ç°æŸäº›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æã€åƒåœ¾çŸ­ä¿¡æ£€æµ‹å’Œæ ‡ç­¾åˆ†ç±»æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¾®è°ƒç­–ç•¥ï¼Œç³»ç»Ÿæ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œå¹¶åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ç›¸å…³æºä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºæ–‡æœ¬åˆ†ç±»é¢†åŸŸçš„æ ¸å¿ƒå·¥å…·ã€‚</li>
<li>LLMsç®€åŒ–äº†ä¼ ç»Ÿæ–‡æœ¬åˆ†ç±»çš„å·¥ä½œæµç¨‹ï¼Œé™ä½äº†é¢„å¤„ç†å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„éœ€æ±‚ã€‚</li>
<li>åœ¨æƒ…æ„Ÿåˆ†æã€åƒåœ¾çŸ­ä¿¡æ£€æµ‹å’Œæ ‡ç­¾åˆ†ç±»ç­‰å¤šä¸ªä»»åŠ¡ä¸­ï¼ŒæŸäº›LLMsçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>ç³»ç»Ÿæ€§èƒ½å¯ä»¥é€šè¿‡å¾®è°ƒç­–ç•¥è¿›ä¸€æ­¥æé«˜ã€‚</li>
<li>è®ºæ–‡æä¾›äº†è¯¦ç»†çš„å®éªŒæ–¹æ³•å’Œç»“æœåˆ†æã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªGitHubä»“åº“ï¼ŒåŒ…å«æºä»£ç å’Œæ•°æ®é›†ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f6280926f748abbcad6ee215ad2ae192.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-50bae6e3ef584105719b6db9f4a30fd6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dc39943db50b4fad6e3c6d5a9d9ceb28.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0bffdb666300130ea86abb5409a7c84b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-be4457da93f03603ab4cbf93bbc828b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-29f331e50dc2dc48bddea28c756fced3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-84eb2bdea990b8edfa354c8c1aeed3d9.jpg" align="middle">
</details>




<h2 id="Rho-1-Not-All-Tokens-Are-What-You-Need"><a href="#Rho-1-Not-All-Tokens-Are-What-You-Need" class="headerlink" title="Rho-1: Not All Tokens Are What You Need"></a>Rho-1: Not All Tokens Are What You Need</h2><p><strong>Authors:Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen</strong></p>
<p>Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that â€˜â€™9l trainingâ€™â€™. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training. </p>
<blockquote>
<p>ä¹‹å‰çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•éƒ½æ˜¯å°†æ‰€æœ‰è®­ç»ƒä»¤ç‰Œç»Ÿä¸€åº”ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„é¢„æµ‹æŸå¤±ã€‚æˆ‘ä»¬æŒ‘æˆ˜è¿™ä¸€å¸¸è§„ï¼Œæå‡ºäº†â€9lè®­ç»ƒâ€çš„è§‚ç‚¹ã€‚æˆ‘ä»¬çš„åˆæ­¥åˆ†æç ”ç©¶äº†è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œçº§è®­ç»ƒåŠ¨æ€ï¼Œå‘ç°äº†ä¸åŒä»¤ç‰Œä¹‹é—´ç‹¬ç‰¹çš„æŸå¤±æ¨¡å¼ã€‚åˆ©ç”¨è¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºRho-1çš„æ–°è¯­è¨€æ¨¡å‹ã€‚ä¸åŒäºä¼ ç»Ÿè¯­è¨€æ¨¡å‹å­¦ä¹ é¢„æµ‹è¯­æ–™åº“ä¸­çš„æ¯ä¸ªä¸‹ä¸€ä¸ªä»¤ç‰Œï¼ŒRho-1é‡‡ç”¨é€‰æ‹©æ€§è¯­è¨€å»ºæ¨¡ï¼ˆSelective Language Modelingï¼ŒSLMï¼‰ï¼Œå®ƒåªé’ˆå¯¹ä¸æ‰€éœ€åˆ†å¸ƒå¯¹é½çš„æœ‰ç”¨ä»¤ç‰Œè¿›è¡Œé€‰æ‹©æ€§è®­ç»ƒã€‚è¿™ç§æ–¹æ³•æ¶‰åŠä½¿ç”¨å‚è€ƒæ¨¡å‹å¯¹é¢„è®­ç»ƒä»¤ç‰Œè¿›è¡Œè¯„åˆ†ï¼Œç„¶åä½¿ç”¨æœ‰é’ˆå¯¹æ€§çš„æŸå¤±å¯¹å¾—åˆ†è¾ƒé«˜çš„ä»¤ç‰Œè¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚åœ¨è¿ç»­é¢„è®­ç»ƒè§„æ¨¡ä¸º15Bçš„OpenWebMathè¯­æ–™åº“æ—¶ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨9ä¸ªæ•°å­¦ä»»åŠ¡ä¸­ï¼ŒRhoolæ–¹æ³•çš„é›¶æ ·æœ¬å‡†ç¡®åº¦æé«˜äº†é«˜è¾¾ç™¾åˆ†ä¹‹ä¸‰åçš„ç»å¯¹ç™¾åˆ†æ¯”å€¼ã€‚ç»è¿‡å¾®è°ƒåï¼ŒRho-1-1Bå’Œ7Båœ¨MATHæ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº†æœ€æ–°çš„æœ€ä½³ç»“æœï¼Œåˆ†åˆ«ä¸ºç™¾åˆ†ä¹‹å››åå’Œå…­ç‚¹å…«ç™¾åˆ†ä¹‹äº”åå…«ï¼ˆMATHæ•°æ®é›†çš„æ€§èƒ½è¯„ä»·æ ‡å‡†ä¸å·²å‘è¡¨è®ºæ–‡æŒ‡æ ‡ä¸ç»Ÿä¸€çš„é—®é¢˜åœ¨æ‰€éš¾å…ï¼‰ã€‚ç„¶è€Œåªç”¨ç™¾åˆ†ä¹‹ä¸‰çš„é¢„è®­ç»ƒä»¤ç‰Œå°±èƒ½åŒ¹é…DeepSeekMathçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨è¿ç»­é¢„è®­ç»ƒè§„æ¨¡ä¸º80Bçš„æ™®é€šä»¤ç‰Œæ—¶ï¼ŒRho-1åœ¨åäº”ä¸ªä¸åŒä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†ç™¾åˆ†ä¹‹å…­ç‚¹å…«çš„å¹³å‡æ€§èƒ½ï¼Œæé«˜äº†è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.07965v3">PDF</a> First two authors equal contribution</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‘æˆ˜äº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹Rho-1ã€‚Rho-1é‡‡ç”¨é€‰æ‹©æ€§è¯­è¨€å»ºæ¨¡ï¼ˆSLMï¼‰çš„æ–¹å¼ï¼Œä¸åŒäºä¼ ç»Ÿè¯­è¨€æ¨¡å‹å¯¹æ¯ä¸€ä¸ªåç»­è¯è¿›è¡Œé¢„æµ‹çš„è®­ç»ƒæ–¹æ³•ï¼Œè€Œæ˜¯æ ¹æ®æœŸæœ›çš„è¯æ±‡åˆ†å¸ƒï¼Œä»…é’ˆå¯¹æœ‰ä»·å€¼çš„è¯æ±‡è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸€ç­–ç•¥æå¤§åœ°æå‡äº†é¢„è®­ç»ƒçš„æ•ˆç‡åŠè¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç»è¿‡è¿ç»­é¢„è®­ç»ƒåï¼Œå…¶åœ¨æ•°å­¦ä»»åŠ¡ä¸Šçš„å‡†ç¡®åº¦æå‡äº†é«˜è¾¾ç™¾åˆ†ä¹‹ä¸‰åã€‚å¹¶ä¸”å³ä¾¿æ˜¯åœ¨æœ‰é™çš„é¢„è®­ç»ƒè¯æ±‡é‡ä¸‹ï¼Œå…¶è¡¨ç°ä¹Ÿèƒ½ä¸é¡¶å°–æ¨¡å‹ç›¸æŠ—è¡¡ã€‚æ­¤å¤–ï¼Œåœ¨è¿ç»­é¢„è®­ç»ƒå¤§é‡é€šç”¨è¯æ±‡æ—¶ï¼Œå…¶å¹³å‡æ€§èƒ½æå‡è¶…è¿‡ç™¾åˆ†ä¹‹å…­ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rho-1é‡‡ç”¨äº†é€‰æ‹©æ€§è¯­è¨€å»ºæ¨¡ï¼ˆSLMï¼‰çš„ç­–ç•¥ï¼Œåªå¯¹æœ‰ä»·å€¼çš„è¯æ±‡è¿›è¡Œè®­ç»ƒï¼Œè€Œéä¼ ç»Ÿçš„é¢„æµ‹æ‰€æœ‰åç»­è¯æ±‡çš„æ–¹æ³•ã€‚è¿™ç§ç­–ç•¥æå‡äº†é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚</li>
<li>Rho-1å¯¹äºè¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå¼•å…¥äº†å‚è€ƒæ¨¡å‹ï¼Œå¯¹é¢„è®­ç»ƒè¯æ±‡è¿›è¡Œè¯„åˆ†ï¼Œé‡ç‚¹è®­ç»ƒå¾—åˆ†è¾ƒé«˜çš„è¯æ±‡ã€‚è¿™æå¤§åœ°æé«˜äº†æ¨¡å‹çš„é’ˆå¯¹æ€§åŠè¡¨ç°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5edbb86747b6c49ec4859a478d078f08.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-553a1738b7852d4b5bfc65951433efe5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cdbb3b3ca7130ce6ccc8219671e2628e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ac3417a06d64e238fb721266623f376e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6251b7c2d0a642fd2c586c2ba121eb29.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2952de91398a6a427d8c70821e213c0.jpg" align="middle">
</details>




<h2 id="Query-guided-Prototype-Evolution-Network-for-Few-Shot-Segmentation"><a href="#Query-guided-Prototype-Evolution-Network-for-Few-Shot-Segmentation" class="headerlink" title="Query-guided Prototype Evolution Network for Few-Shot Segmentation"></a>Query-guided Prototype Evolution Network for Few-Shot Segmentation</h2><p><strong>Authors:Runmin Cong, Hang Xiong, Jinpeng Chen, Wei Zhang, Qingming Huang, Yao Zhao</strong></p>
<p>Previous Few-Shot Segmentation (FSS) approaches exclusively utilize support features for prototype generation, neglecting the specific requirements of the query. To address this, we present the Query-guided Prototype Evolution Network (QPENet), a new method that integrates query features into the generation process of foreground and background prototypes, thereby yielding customized prototypes attuned to specific queries. The evolution of the foreground prototype is accomplished through a \textit{support-query-support} iterative process involving two new modules: Pseudo-prototype Generation (PPG) and Dual Prototype Evolution (DPE). The PPG module employs support features to create an initial prototype for the preliminary segmentation of the query image, resulting in a pseudo-prototype reflecting the unique needs of the current query. Subsequently, the DPE module performs reverse segmentation on support images using this pseudo-prototype, leading to the generation of evolved prototypes, which can be considered as custom solutions. As for the background prototype, the evolution begins with a global background prototype that represents the generalized features of all training images. We also design a Global Background Cleansing (GBC) module to eliminate potential adverse components mirroring the characteristics of the current foreground class. Experimental results on the PASCAL-$5^i$ and COCO-$20^i$ datasets attest to the substantial enhancements achieved by QPENet over prevailing state-of-the-art techniques, underscoring the validity of our ideas. </p>
<blockquote>
<p>ä¹‹å‰çš„å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•ä»…åˆ©ç”¨æ”¯æŒç‰¹å¾è¿›è¡ŒåŸå‹ç”Ÿæˆï¼Œå¿½ç•¥äº†æŸ¥è¯¢çš„ç‰¹å®šè¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æŸ¥è¯¢å¼•å¯¼åŸå‹æ¼”åŒ–ç½‘ç»œï¼ˆQPENetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†æŸ¥è¯¢ç‰¹å¾é›†æˆåˆ°å‰æ™¯å’ŒèƒŒæ™¯åŸå‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ–°æ–¹æ³•ï¼Œä»è€Œç”Ÿæˆé€‚åº”ç‰¹å®šæŸ¥è¯¢çš„å®šåˆ¶åŸå‹ã€‚å‰æ™¯åŸå‹çš„æ¼”åŒ–æ˜¯é€šè¿‡ä¸€ä¸ªæ¶‰åŠä¸¤ä¸ªæ–°æ¨¡å—çš„â€œæ”¯æŒ-æŸ¥è¯¢-æ”¯æŒâ€è¿­ä»£è¿‡ç¨‹å®ç°çš„ï¼Œè¿™ä¸¤ä¸ªæ¨¡å—æ˜¯ä¼ªåŸå‹ç”Ÿæˆï¼ˆPPGï¼‰å’ŒåŒåŸå‹æ¼”åŒ–ï¼ˆDPEï¼‰ã€‚PPGæ¨¡å—åˆ©ç”¨æ”¯æŒç‰¹å¾ä¸ºæŸ¥è¯¢å›¾åƒçš„åˆæ­¥åˆ†å‰²åˆ›å»ºåˆå§‹åŸå‹ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªåæ˜ å½“å‰æŸ¥è¯¢ç‹¬ç‰¹éœ€æ±‚çš„ä¼ªåŸå‹ã€‚éšåï¼ŒDPEæ¨¡å—ä½¿ç”¨è¿™ä¸ªä¼ªåŸå‹å¯¹æ”¯æŒå›¾åƒè¿›è¡Œåå‘åˆ†å‰²ï¼Œä»è€Œäº§ç”Ÿè¿›åŒ–çš„åŸå‹ï¼Œè¿™äº›åŸå‹å¯ä»¥è¢«è§†ä¸ºå®šåˆ¶çš„è§£å†³æ–¹æ¡ˆã€‚è‡³äºèƒŒæ™¯åŸå‹ï¼Œå…¶æ¼”åŒ–å§‹äºä»£è¡¨æ‰€æœ‰è®­ç»ƒå›¾åƒé€šç”¨ç‰¹å¾çš„å…¨å±€èƒŒæ™¯åŸå‹ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå…¨å±€èƒŒæ™¯æ¸…æ´ï¼ˆGBCï¼‰æ¨¡å—ï¼Œä»¥æ¶ˆé™¤å¯èƒ½çš„ä¸è‰¯æˆåˆ†ï¼Œè¿™äº›æˆåˆ†åæ˜ äº†å½“å‰å‰æ™¯ç±»çš„ç‰¹å¾ã€‚åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒQPENetç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯å®ç°äº†é‡å¤§æ”¹è¿›ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„åˆç†æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06488v2">PDF</a> Accepted by IEEE TMM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æŸ¥è¯¢å¼•å¯¼åŸå‹æ¼”åŒ–ç½‘ç»œï¼ˆQPENetï¼‰ï¼Œå°†æŸ¥è¯¢ç‰¹å¾é›†æˆåˆ°å‰æ™¯å’ŒèƒŒæ™¯åŸå‹çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»è€Œç”Ÿæˆé’ˆå¯¹ç‰¹å®šæŸ¥è¯¢è¿›è¡Œå®šåˆ¶åŒ–çš„åŸå‹ã€‚é€šè¿‡æ”¯æŒç‰¹å¾ç”Ÿæˆä¼ªåŸå‹ï¼Œå†é€šè¿‡åå‘åˆ†å‰²æ”¯æŒå›¾åƒç”Ÿæˆæ¼”åŒ–åçš„åŸå‹ã€‚å¯¹äºèƒŒæ™¯åŸå‹ï¼Œä½¿ç”¨å…¨å±€èƒŒæ™¯åŸå‹å¹¶è®¾è®¡å…¨çƒèƒŒæ™¯æ¸…æ´æ¨¡å—ä»¥æ¶ˆé™¤æ½œåœ¨çš„ä¸è‰¯æˆåˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQPENetåœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QPENetå°†æŸ¥è¯¢ç‰¹å¾èå…¥å‰æ™¯å’ŒèƒŒæ™¯åŸå‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œç”Ÿæˆé’ˆå¯¹ç‰¹å®šæŸ¥è¯¢å®šåˆ¶åŒ–çš„åŸå‹ã€‚</li>
<li>QPENeté€šè¿‡æ”¯æŒç‰¹å¾ç”Ÿæˆä¼ªåŸå‹ï¼Œå®ç°å‰æ™¯åŸå‹çš„æ¼”åŒ–ã€‚</li>
<li>ä¼ªåŸå‹ç”Ÿæˆæ¨¡å—ï¼ˆPPGï¼‰åˆ©ç”¨æ”¯æŒç‰¹å¾è¿›è¡Œåˆæ­¥åˆ†å‰²ï¼Œç”Ÿæˆåæ˜ å½“å‰æŸ¥è¯¢ç‹¬ç‰¹éœ€æ±‚çš„ä¼ªåŸå‹ã€‚</li>
<li>åŒåŸå‹æ¼”åŒ–æ¨¡å—ï¼ˆDPEï¼‰é€šè¿‡åå‘åˆ†å‰²æ”¯æŒå›¾åƒï¼Œä½¿ç”¨ä¼ªåŸå‹ç”Ÿæˆæ¼”åŒ–åçš„å‰æ™¯åŸå‹ã€‚</li>
<li>å¯¹äºèƒŒæ™¯åŸå‹ï¼Œä½¿ç”¨å…¨å±€èƒŒæ™¯åŸå‹ä»£è¡¨æ‰€æœ‰è®­ç»ƒå›¾åƒçš„ä¸€èˆ¬ç‰¹å¾ã€‚</li>
<li>å…¨çƒèƒŒæ™¯æ¸…æ´æ¨¡å—ï¼ˆGBCï¼‰ç”¨äºæ¶ˆé™¤å¯èƒ½çš„ä¸è‰¯æˆåˆ†ï¼Œåæ˜ å½“å‰å‰æ™¯ç±»çš„ç‰¹æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c103bf4ec5835efb51a0253bbfebb1c7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-97bf38f62345affa5e8c646a8ab1a288.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3bc1d370155ffbc0ccac3030a61bdc8e.jpg" align="middle">
</details>




<h2 id="Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning"><a href="#Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning"></a>Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Wangmeng Zuo, Chunmei Feng</strong></p>
<p>Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>. </p>
<blockquote>
<p>å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ—¨åœ¨åŸºäºéå¸¸æœ‰é™çš„è®­ç»ƒæ•°æ®è¿ç»­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸å¿˜è®°å·²é‡åˆ°çš„æ—§ç±»åˆ«ã€‚ç°æœ‰ç ”ç©¶ä»…ä¾èµ–äºçº¯è§†è§‰ç½‘ç»œï¼Œè€Œæœ¬æ–‡æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰æ¥è§£å†³FSCILé—®é¢˜ï¼Œå¹¶æå‡ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç§°ä¸ºåŸºäºåˆ†å¸ƒç‰¹å¾å›æ”¾çš„å­¦ä¹ æç¤ºï¼ˆLP-DiFï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å°±å¯ä»¥å¤§å¤§è¶…è¿‡æœ€æœ‰å½±å“åŠ›çš„æ–¹æ³•ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶é€‚åº”èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿ç»­ä»æ¯ä¸ªä¼šè¯ä¸­è·å–ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºäº†é˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­å¿˜è®°æ—§çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç»´æŒä¸€ä¸ªç”±è®­ç»ƒå›¾åƒç‰¹å¾å’Œå˜è‡ªåŠ¨ç¼–ç å™¨ç”Ÿæˆçš„åˆæˆç‰¹å¾ä¼°è®¡çš„å¯¹è§’åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾çº§é«˜æ–¯åˆ†å¸ƒæ¥ä¿ç•™æ¯ä¸ªç±»çš„æ—§çŸ¥è¯†ã€‚å½“è¿›å±•åˆ°ä¸€ä¸ªæ–°çš„ä¼šè¯æ—¶ï¼Œä¼ªç‰¹å¾æ˜¯ä»æ—§ç±»åˆ†å¸ƒä¸­é‡‡æ ·å¹¶ç»“åˆå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œä»è€Œèƒ½å¤Ÿä½¿æ¨¡å‹åœ¨ä¿ç•™æ—§çŸ¥è¯†çš„åŒæ—¶å­¦ä¹ æ–°çŸ¥è¯†ã€‚åœ¨CIFAR100ã€mini-ImageNetã€CUB-200ä¸‰ä¸ªæµè¡ŒåŸºå‡†ä»¥åŠæœ¬æ–‡æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„SUN-397å’ŒCUB-200$^*$åŸºå‡†ä¸Šçš„å®éªŒå±•ç¤ºäº†LP-DiFçš„ä¼˜è¶Šæ€§ï¼Œåœ¨FSCILé¢†åŸŸå®ç°äº†æ–°çš„æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF%E4%B8%8A%E3%80%82">https://github.com/1170300714/LP-DiFä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01598v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰ç½‘ç»œçš„å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ—¨åœ¨åˆ©ç”¨éå¸¸æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸æ–­å­¦ä¹ æ–°ç±»åˆ«è€Œä¸å¿˜è®°å·²é‡åˆ°çš„æ—§ç±»åˆ«ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåä¸ºLearning Prompt with Distribution-based Feature Replayï¼ˆLP-DiFï¼‰ï¼Œå€ŸåŠ©èåˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è§£å†³FSCILé—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤ŸæŒç»­æ•è·æ¯ä¸ªä¼šè¯çš„ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºé˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­å¿˜è®°æ—§çŸ¥è¯†ï¼Œæå‡ºäº†ä¼ªç‰¹å¾å†ç°æ–¹æ³•ã€‚æˆ‘ä»¬ä¿ç•™äº†æ¯ä¸ªç±»åˆ«çš„æ—§çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ç»´æŒç‰¹å¾å±‚é¢çš„é«˜æ–¯åˆ†å¸ƒä¼°è®¡å›¾åƒç‰¹å¾ç”Ÿæˆçš„ç‰¹å¾å‘é‡ã€‚å½“è¿›å…¥æ–°ä¼šè¯æ—¶ï¼Œä¼ªç‰¹å¾ä»æ—§ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·å¹¶ç»“åˆå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿ç•™æ—§çŸ¥è¯†çš„åŒæ—¶å­¦ä¹ æ–°çŸ¥è¯†ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¯æ˜äº†LP-DiFçš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†ç°æœ‰æŠ€æœ¯æ— æ³•è¶…è¶Šçš„æ°´å¹³ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šæä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LP-DiFæ¡†æ¶ç»“åˆäº†è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥è§£å†³å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é—®é¢˜ã€‚</li>
<li>CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å·²ç»æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æç¤ºè°ƒæ•´æŠ€æœ¯æé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤ŸæŒç»­æ•è·æ¯ä¸ªä¼šè¯çš„ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>ä¼ªç‰¹å¾å†ç°æ–¹æ³•ç”¨äºé˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­å¿˜è®°æ—§çŸ¥è¯†ã€‚</li>
<li>æ—§çŸ¥è¯†çš„ä¿ç•™æ˜¯é€šè¿‡ç»´æŒç‰¹å¾å±‚é¢çš„é«˜æ–¯åˆ†å¸ƒæ¥å®ç°çš„ï¼Œè¯¥åˆ†å¸ƒç”±è®­ç»ƒå›¾åƒçš„å›¾åƒç‰¹å¾å’Œé€šè¿‡VAEç”Ÿæˆçš„åˆæˆç‰¹å¾ä¼°ç®—å¾—å‡ºã€‚</li>
<li>é€šè¿‡é‡‡æ ·ä¼ªç‰¹å¾ç»“åˆå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œä»è€Œå®ç°æ—§çŸ¥è¯†çš„ä¿ç•™å’Œæ–°çŸ¥è¯†çš„å­¦ä¹ ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b68ef099e7713b014eeb2b3e7b49e8c2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a76b9eff0fad3d63dcb46aee459a0673.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0a24a512d0bc42141ba455a1b4b860e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f07311872d24975899a6d4fd3d1d5e60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-74c2a8fa494ec56d4ee19c96dabbeb8f.jpg" align="middle">
</details>




<h2 id="A-Prompt-Learning-Framework-for-Source-Code-Summarization"><a href="#A-Prompt-Learning-Framework-for-Source-Code-Summarization" class="headerlink" title="A Prompt Learning Framework for Source Code Summarization"></a>A Prompt Learning Framework for Source Code Summarization</h2><p><strong>Authors:Tingting Xu, Yun Miao, Chunrong Fang, Hanwei Qian, Xia Feng, Zhenpeng Chen, Chong Wang, Jian Zhang, Weisong Sun, Zhenyu Chen, Yang Liu</strong></p>
<p>(Source) code summarization is the task of automatically generating natural language summaries (also called comments) for given code snippets. Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks. The main adaptation schemes include instruction prompting, task-oriented (full-parameter) fine-tuning, and parameter-efficient fine-tuning (PEFT). However, instruction prompting involves designing crafted prompts and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs, and effective, tailored PEFT methods for code summarization are still lacking.   This paper proposes an effective prompt learning framework for code summarization called PromptCS. It no longer requires users to rack their brains to design effective prompts. Instead, PromptCS trains a prompt agent that can generate continuous prompts to unleash the potential for LLMs in code summarization. Compared to the human-written discrete prompt, the continuous prompts are produced under the guidance of LLMs and are therefore easier to understand by LLMs. PromptCS is non-invasive to LLMs and freezes the parameters of LLMs when training the prompt agent, which can greatly reduce the requirements for training resources. Our comprehensive experimental results show that PromptCS significantly outperforms instruction prompting schemes (including zero-shot learning and few-shot learning) on all four widely used metrics, and is comparable to the task-oriented fine-tuning scheme. In some base LLMs, e.g., StarCoderBase-1B and -3B, PromptCS even outperforms the task-oriented fine-tuning scheme. More importantly, the training efficiency of PromptCS is faster than the task-oriented fine-tuning scheme, with a more pronounced advantage on larger LLMs. </p>
<blockquote>
<p>ä»£ç æ‘˜è¦ä»»åŠ¡æ˜¯è‡ªåŠ¨ç”Ÿæˆç»™å®šä»£ç ç‰‡æ®µçš„è‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆä¹Ÿç§°ä¸ºæ³¨é‡Šï¼‰ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„æˆåŠŸåº”ç”¨ï¼Œè½¯ä»¶å·¥ç¨‹ç ”ç©¶äººå‘˜ä¹Ÿå°è¯•å°†LLMé€‚åº”äºè§£å†³ä»£ç æ‘˜è¦ä»»åŠ¡ã€‚ä¸»è¦çš„é€‚åº”æ–¹æ¡ˆåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€é¢å‘ä»»åŠ¡ï¼ˆå…¨å‚æ•°ï¼‰å¾®è°ƒä»¥åŠå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ã€‚ç„¶è€Œï¼ŒæŒ‡ä»¤æç¤ºéœ€è¦è®¾è®¡ç²¾å¿ƒåˆ¶ä½œçš„æç¤ºï¼Œå¹¶è¦æ±‚ç”¨æˆ·å…·å¤‡ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼Œè€Œé¢å‘ä»»åŠ¡çš„å¾®è°ƒéœ€è¦é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬ï¼Œé’ˆå¯¹ä»£ç æ‘˜è¦çš„æœ‰æ•ˆã€å®šåˆ¶åŒ–çš„PEFTæ–¹æ³•ä»ç„¶ç¼ºä¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.16066v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä»£ç æ‘˜è¦çš„æœ‰æ•ˆæç¤ºå­¦ä¹ æ¡†æ¶ï¼Œåä¸ºPromptCSã€‚å®ƒä¸å†è¦æ±‚ç”¨æˆ·è®¾è®¡æœ‰æ•ˆçš„æç¤ºï¼Œè€Œæ˜¯è®­ç»ƒä¸€ä¸ªæç¤ºä»£ç†æ¥ç”Ÿæˆè¿ç»­æç¤ºï¼Œé‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ‘˜è¦ä¸­çš„æ½œåŠ›ã€‚PromptCSåœ¨å››ä¸ªå¹¿æ³›åº”ç”¨æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºæŒ‡ä»¤æç¤ºæ–¹æ¡ˆï¼Œå¹¶ä¸”ä¸ä»»åŠ¡å¯¼å‘å¾®è°ƒæ–¹æ¡ˆç›¸å½“ã€‚åœ¨æŸäº›åŸºç¡€LLMsä¸Šï¼ŒPromptCSç”šè‡³è¶…è¶Šäº†ä»»åŠ¡å¯¼å‘å¾®è°ƒæ–¹æ¡ˆã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒPromptCSçš„è®­ç»ƒæ•ˆç‡æ¯”ä»»åŠ¡å¯¼å‘å¾®è°ƒæ–¹æ¡ˆæ›´å¿«ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´å¤§çš„LLMsä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç æ‘˜è¦ä»»åŠ¡æ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆç»™å®šä»£ç ç‰‡æ®µçš„è‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆä¹Ÿç§°ä¸ºæ³¨é‡Šï¼‰ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­ä¹Ÿè¢«ç”¨äºä»£ç æ‘˜è¦ä»»åŠ¡ã€‚</li>
<li>å½“å‰ä¸»è¦çš„é€‚åº”æ–¹æ¡ˆåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€ä»»åŠ¡å¯¼å‘çš„å¾®è°ƒä»¥åŠå‚æ•°æœ‰æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ã€‚</li>
<li>æŒ‡ä»¤æç¤ºéœ€è¦ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†å’Œè®¾è®¡æŠ€å·§æç¤ºï¼Œä»»åŠ¡å¯¼å‘çš„å¾®è°ƒåˆ™éœ€è¦é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬ã€‚</li>
<li>PromptCSæ˜¯ä¸€ç§æœ‰æ•ˆçš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»£ç æ‘˜è¦ä»»åŠ¡ã€‚å®ƒè®­ç»ƒä¸€ä¸ªæç¤ºä»£ç†æ¥ç”Ÿæˆè¿ç»­æç¤ºï¼Œè¿™äº›æç¤ºæ›´å®¹æ˜“è¢«LLMsç†è§£ã€‚</li>
<li>PromptCSåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºæŒ‡ä»¤æç¤ºæ–¹æ¡ˆï¼Œä¸ä»»åŠ¡å¯¼å‘å¾®è°ƒæ–¹æ¡ˆç›¸å½“æˆ–æ›´å¥½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-164bd8769567313059bee368a65b9ace.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ff42804d993c1ea7e5236f922cf8d31e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c1dd1263773a2b25c4a0b2c661b13688.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cb67bbec797180bf1eebe06f696dd8f8.jpg" align="middle">
</details>




<h2 id="Leveraging-Large-Language-Models-for-Node-Generation-in-Few-Shot-Learning-on-Text-Attributed-Graphs"><a href="#Leveraging-Large-Language-Models-for-Node-Generation-in-Few-Shot-Learning-on-Text-Attributed-Graphs" class="headerlink" title="Leveraging Large Language Models for Node Generation in Few-Shot   Learning on Text-Attributed Graphs"></a>Leveraging Large Language Models for Node Generation in Few-Shot   Learning on Text-Attributed Graphs</h2><p><strong>Authors:Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang</strong></p>
<p>Text-attributed graphs have recently garnered significant attention due to their wide range of applications in web domains. Existing methodologies employ word embedding models for acquiring text representations as node features, which are subsequently fed into Graph Neural Networks (GNNs) for training. Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data. Furthermore, the acquisition and labeling of extensive datasets are both costly and time-consuming endeavors. Consequently, few-shot learning has emerged as a crucial problem in the context of graph learning tasks. In order to tackle this challenge, we propose a lightweight paradigm called LLM4NG, which adopts a plug-and-play approach to empower text-attributed graphs through node generation using LLMs. Specifically, we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars. Subsequently, we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph. This approach harnesses LLMs for enhancing class-level information and seamlessly introduces labeled nodes and edges without modifying the raw dataset, thereby facilitating the node classification task in few-shot scenarios. Extensive experiments demonstrate the outstanding performance of our proposed paradigm, particularly in low-shot scenarios. For instance, in the 1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement over the baseline model. </p>
<blockquote>
<p>æ–‡æœ¬å±æ€§å›¾å› å…¶åœ¨ç½‘ç»œé¢†åŸŸçš„å¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨è¯åµŒå…¥æ¨¡å‹è·å–æ–‡æœ¬è¡¨ç¤ºä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œç„¶åè¾“å…¥å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰è¿›è¡Œè®­ç»ƒã€‚æœ€è¿‘ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°å¼•å…¥äº†å…¶åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œè¿™å¯ä»¥æå¤§åœ°å¢å¼ºå›¾çš„æ–‡æœ¬å±æ€§ã€‚æ­¤å¤–ï¼Œè·å–å’Œæ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†æ˜¯ä¸€é¡¹æ—¢æ˜‚è´µåˆè€—æ—¶çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œå°æ ·å­¦ä¹ å·²æˆä¸ºå›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„èŒƒå¼LLM4NGï¼Œå®ƒé€šè¿‡é‡‡ç”¨å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨LLMsé€šè¿‡èŠ‚ç‚¹ç”Ÿæˆæ¥å¢å¼ºæ–‡æœ¬å±æ€§å›¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMsä»æ ‡ç­¾ä¸­æå–è¯­ä¹‰ä¿¡æ¯ï¼Œç”Ÿæˆå±äºè¿™äº›ç±»åˆ«çš„æ ·æœ¬ä½œä¸ºèŒƒä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨è¾¹ç¼˜é¢„æµ‹å™¨æ•æ‰åŸå§‹æ•°æ®é›†å›ºæœ‰çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†æ–°ç”Ÿæˆçš„æ ·æœ¬é›†æˆåˆ°åŸå§‹å›¾ä¸­ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨LLMså¢å¼ºç±»çº§ä¿¡æ¯ï¼Œå¹¶æ— ç¼åœ°å¼•å…¥æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹å’Œè¾¹ç¼˜ï¼Œè€Œä¸ä¿®æ”¹åŸå§‹æ•°æ®é›†ï¼Œä»è€Œæœ‰åŠ©äºå°æ ·åœºæ™¯ä¸­çš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„èŒƒå¼è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ ·æœ¬åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨ogbn-arxivæ•°æ®é›†çš„1ä¸ªæ ·æœ¬è®¾ç½®ä¸‹ï¼ŒLLM4NGç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å®ç°äº†76%çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09872v2">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬å±æ€§å›¾å› å…¶åœ¨ç½‘é¡µåŸŸä¸­çš„å¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨è¯åµŒå…¥æ¨¡å‹è·å–æ–‡æœ¬è¡¨ç¤ºä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œå¹¶è¾“å…¥å›¾ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯æå¤§åœ°å¢å¼ºå›¾çš„æ–‡æœ¬å±æ€§ã€‚æ­¤å¤–ï¼Œè·å–å’Œæ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†æ˜¯æ˜‚è´µä¸”è€—æ—¶çš„ï¼Œå› æ­¤å°‘æ ·æœ¬å­¦ä¹ æˆä¸ºå›¾å­¦ä¹ ä»»åŠ¡çš„å…³é”®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„LLM4NGèŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–æ ‡ç­¾è¯­ä¹‰ä¿¡æ¯å¹¶ç”Ÿæˆæ ·æœ¬ä½œä¸ºèŒƒä¾‹ï¼Œé‡‡ç”¨å³æ’å³ç”¨æ–¹æ³•èµ‹èƒ½æ–‡æœ¬å±æ€§å›¾èŠ‚ç‚¹ç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡è¾¹ç¼˜é¢„æµ‹å™¨æ•æ‰åŸå§‹æ•°æ®é›†çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†æ–°ç”Ÿæˆçš„æ ·æœ¬é›†æˆåˆ°åŸå§‹å›¾ä¸­ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æé«˜ç±»çº§åˆ«ä¿¡æ¯ï¼Œæ— ç¼åœ°å¼•å…¥æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹å’Œè¾¹ç¼˜ï¼Œæ— éœ€ä¿®æ”¹åŸå§‹æ•°æ®é›†ï¼Œä»è€Œæœ‰åŠ©äºå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å±æ€§å›¾å› å…¶åœ¨å¤šç§é¢†åŸŸçš„åº”ç”¨è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°å¢å¼ºäº†å›¾çš„æ–‡æœ¬å±æ€§ä¿¡æ¯ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ æˆä¸ºå›¾å­¦ä¹ ä»»åŠ¡çš„å…³é”®é—®é¢˜ã€‚</li>
<li>LLM4NGèŒƒå¼é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹èµ‹èƒ½æ–‡æœ¬å±æ€§å›¾çš„èŠ‚ç‚¹ç”Ÿæˆã€‚</li>
<li>LLM4NGé‡‡ç”¨å³æ’å³ç”¨æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæ ·æœ¬å’Œè¾¹ç¼˜é¢„æµ‹æ¥å¢å¼ºå›¾çš„ä¿¡æ¯ã€‚</li>
<li>LLM4NGæé«˜äº†ç±»çº§åˆ«ä¿¡æ¯ï¼Œæœ‰åŠ©äºå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-293d82a1cfde71955a6d01ec02d26962.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-269b156153c6e7fe626fd09cc8e2dfbd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c99b46fe7e03e3681391ad86b945fa51.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a7ba1e0b1885451cc6ff5247acccb5c.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  BLADE Single-view Body Mesh Learning through Accurate Depth Estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-12f0a0d8cce34eb31556fb0f005645e2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Generative Semantic Communication Architectures, Technologies, and   Applications
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
