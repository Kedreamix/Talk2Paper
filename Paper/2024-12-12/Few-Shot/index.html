<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-e4062550094aa078f557d3d317e4952a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ä½¿ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å´é¢ä¸´ä¸‰å¤§æ ¹æœ¬é—®é¢˜ï¼šç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥æœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ï¼Œä»…ä½¿ç”¨å°‘é‡è¯­ä¹‰æ ‡æ³¨æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½å…·æœ‰æå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥åˆ°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ä¸­ï¼Œé€šè¿‡åŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºæ¥å®ç°ã€‚ä¸ºäº†å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾æç¤ºæ–¹æ³•ï¼Œç„¶åæå‡ºäº†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„é¦–ä¸ªå›¾è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºå¾®è°ƒæ—¶çš„ç›‘ç£ä¸è¶³ï¼Œåœ¨æˆ‘ä»¬çš„èŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒå†»ç»“çŠ¶æ€ï¼Œå› æ­¤å¯å­¦ä¹ çš„å‚æ•°è¿œè¿œå°‘äºå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„èŒƒå¼åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿå€ŸåŠ©æå¼±çš„æ–‡æœ¬ç›‘ç£å°†GNNæ¨å¹¿åˆ°æœªè§ç±»åˆ«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v1">PDF</a> Preprint, 26 pages</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬æ¢è®¨äº†åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ„å»ºè§†è§‰æ¨¡å‹æ—¶çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†é¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®æ—¶é¢ä¸´çš„ä¸‰å¤§é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºåˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ ï¼Œå°†å›¾ç›´æ¥åµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŒä¸€ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚ä½œè€…åœ¨æ”¹è¿›ç°æœ‰å›¾æç¤ºæ–¹æ³•çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†é¦–ä¸ªç”¨äºæŒ–æ˜é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„å›¾è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚ç”±äºå¾®è°ƒæ—¶çš„ç›‘ç£ä¸è¶³ï¼Œä½œè€…ä¿æŒé¢„è®­ç»ƒçš„GNNå’ŒLLMå†»ç»“çŠ¶æ€ï¼Œå› æ­¤å¯å­¦ä¹ å‚æ•°å°‘äºå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½œè€…åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œå¹¶å»ºç«‹äº†é¦–ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œå¯å°†GNNæ¨å¹¿åˆ°å…·æœ‰æå¼±æ–‡æœ¬ç›‘ç£çš„æœªè§ç±»åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨CLIPæ„å»ºè§†è§‰æ¨¡å‹æ—¶é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çš„ä¸åŒçº§åˆ«ã€ä»¥åŠé¢†åŸŸä¹‹é—´çš„æ¦‚å¿µå·®è·ã€‚</li>
<li>æå‡ºåˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥é€‚åº”é¢„è®­ç»ƒçš„GNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ã€‚</li>
<li>å°†å›¾ç›´æ¥åµŒå…¥LLMçš„åŒä¸€ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚</li>
<li>æ”¹è¿›äº†ç°æœ‰çš„å›¾æç¤ºæ–¹æ³•ï¼Œå¹¶æå‡ºäº†é¦–ä¸ªå›¾è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæŒ–æ˜é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ã€‚</li>
<li>ç”±äºç›‘ç£ä¸è¶³ï¼Œä¿æŒé¢„è®­ç»ƒçš„GNNå’ŒLLMå†»ç»“çŠ¶æ€ï¼Œé™ä½äº†å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-418010f18183fa1efe9737e4f27a25e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fea15ced17268cfd3a73eccc52e763fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61bf3a22face5a83a5c80d12004d0a7.jpg" align="middle">
</details>




<h2 id="Hyperband-based-Bayesian-Optimization-for-Black-box-Prompt-Selection"><a href="#Hyperband-based-Bayesian-Optimization-for-Black-box-Prompt-Selection" class="headerlink" title="Hyperband-based Bayesian Optimization for Black-box Prompt Selection"></a>Hyperband-based Bayesian Optimization for Black-box Prompt Selection</h2><p><strong>Authors:Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra</strong></p>
<p>Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks. As the most powerful models are proprietary and can only be invoked via an API, users often manually refine prompts in a black-box setting by adjusting instructions and few-shot examples until they achieve good performance as measured on a validation set. Recent methods addressing static black-box prompt selection face significant limitations: They often fail to leverage the inherent structure of prompts, treating instructions and few-shot exemplars as a single block of text. Moreover, they often lack query-efficiency by evaluating prompts on all validation instances, or risk sub-optimal selection of a prompt by using random subsets of validation instances. We introduce HbBoPs, a novel Hyperband-based Bayesian optimization method for black-box prompt selection addressing these key limitations. Our approach combines a structural-aware deep kernel Gaussian Process to model prompt performance with Hyperband as a multi-fidelity scheduler to select the number of validation instances for prompt evaluations. The structural-aware modeling approach utilizes separate embeddings for instructions and few-shot exemplars, enhancing the surrogate modelâ€™s ability to capture prompt performance and predict which prompt to evaluate next in a sample-efficient manner. Together with Hyperband as a multi-fidelity scheduler we further enable query-efficiency by adaptively allocating resources across different fidelity levels, keeping the total number of validation instances prompts are evaluated on low. Extensive evaluation across ten benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods. </p>
<blockquote>
<p>æœ€ä¼˜æç¤ºçš„é€‰æ‹©å¯¹äºä¸‹æ¸¸ä»»åŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½æœ€å¤§åŒ–è‡³å…³é‡è¦ã€‚ç”±äºæœ€å¼ºå¤§çš„æ¨¡å‹æ˜¯ä¸“æœ‰æ¨¡å‹ï¼Œåªèƒ½é€šè¿‡APIè¿›è¡Œè°ƒç”¨ï¼Œç”¨æˆ·é€šå¸¸åœ¨ä¸€ä¸ªé»‘ç®±ç¯å¢ƒä¸­æ‰‹åŠ¨è°ƒæ•´æç¤ºï¼Œé€šè¿‡è°ƒæ•´æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œç›´åˆ°å®ƒä»¬åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°è‰¯å¥½ã€‚æœ€è¿‘è§£å†³é™æ€é»‘ç®±æç¤ºé€‰æ‹©çš„æ–¹æ³•å­˜åœ¨é‡å¤§å±€é™æ€§ï¼šå®ƒä»¬å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨æç¤ºçš„å›ºæœ‰ç»“æ„ï¼Œå°†æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹è§†ä¸ºä¸€æ®µæ–‡æœ¬ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸é€šè¿‡åœ¨æ‰€æœ‰éªŒè¯å®ä¾‹ä¸Šè¯„ä¼°æç¤ºæ¥ç¼ºä¹æŸ¥è¯¢æ•ˆç‡ï¼Œæˆ–è€…ä½¿ç”¨éªŒè¯å®ä¾‹çš„éšæœºå­é›†æ¥é€‰æ‹©æç¤ºï¼Œå­˜åœ¨é€‰æ‹©æç¤ºæ¬¡ä¼˜çš„é£é™©ã€‚æˆ‘ä»¬å¼•å…¥äº†HbBoPsï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé»‘ç®±æç¤ºé€‰æ‹©çš„æ–°å‹åŸºäºHyperbandçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³äº†è¿™äº›å…³é”®å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦å†…æ ¸é«˜æ–¯è¿‡ç¨‹æ¥å¯¹æç¤ºæ€§èƒ½è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨Hyperbandä½œä¸ºå¤šä¿çœŸè°ƒåº¦å™¨æ¥é€‰æ‹©æç¤ºè¯„ä¼°æ‰€éœ€çš„éªŒè¯å®ä¾‹æ•°é‡ã€‚ç»“æ„æ„ŸçŸ¥å»ºæ¨¡æ–¹æ³•ä½¿ç”¨å•ç‹¬çš„åµŒå…¥æ¥ä»£è¡¨æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œæé«˜äº†æ›¿ä»£æ¨¡å‹æ•æ‰æç¤ºæ€§èƒ½çš„èƒ½åŠ›ï¼Œå¹¶ä»¥æ ·æœ¬æ•ˆç‡é«˜çš„æ–¹å¼é¢„æµ‹æ¥ä¸‹æ¥è¦è¯„ä¼°å“ªä¸ªæç¤ºã€‚ç»“åˆHyperbandå¤šä¿çœŸè°ƒåº¦å™¨ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡åœ¨ä¸åŒä¿çœŸåº¦çº§åˆ«ä¸Šè‡ªé€‚åº”åœ°åˆ†é…èµ„æºæ¥å®ç°æŸ¥è¯¢æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒè¯„ä¼°æç¤ºæ‰€éœ€çš„éªŒè¯å®ä¾‹æ€»æ•°è¾ƒä½ã€‚åœ¨åä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHbBoPsä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07820v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¼˜è´¨æç¤ºçš„é€‰æ‹©æä¸ºå…³é”®ã€‚å½“å‰æœ€ä¼˜æ¨¡å‹å…·æœ‰ä¸“æœ‰æ€§ï¼Œä»…èƒ½é€šè¿‡APIè°ƒç”¨ï¼Œç”¨æˆ·éœ€åœ¨é»‘ç®±è®¾ç½®ä¸‹æ‰‹åŠ¨è°ƒæ•´æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹æ¥ä¼˜åŒ–æç¤ºï¼Œç›´è‡³å…¶åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°ç†æƒ³çŠ¶æ€ã€‚é’ˆå¯¹é™æ€é»‘ç®±æç¤ºé€‰æ‹©çš„ç°æœ‰æ–¹æ³•å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼šå®ƒä»¬å¿½ç•¥äº†æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹çš„å†…éƒ¨ç»“æ„ï¼Œå°†å…¶è§†ä¸ºä¸€æ®µæ–‡æœ¬ï¼›åŒæ—¶ç¼ºä¹æŸ¥è¯¢æ•ˆç‡ï¼Œå¯¹æ‰€æœ‰éªŒè¯å®ä¾‹è¿›è¡Œè¯„ä¼°æˆ–å¯¹æç¤ºé€‰æ‹©äº§ç”Ÿä¸åˆ©å½±å“ã€‚æˆ‘ä»¬æ¨å‡ºäº†HbBoPsæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºHyperbandçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¿™äº›å…³é”®é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦æ ¸é«˜æ–¯è¿‡ç¨‹æ¥æ¨¡æ‹Ÿæç¤ºæ€§èƒ½ä¸Hyperbandå¤šä¿çœŸè°ƒåº¦å™¨æ¥é€‰æ‹©éªŒè¯å®ä¾‹æ•°é‡æ¥è¿›è¡Œæç¤ºè¯„ä¼°ã€‚è¯¥ç»“æ„æ„ŸçŸ¥å»ºæ¨¡æ–¹æ³•ä¸ºæŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹æä¾›ç‹¬ç«‹åµŒå…¥ï¼Œæé«˜ä»£ç†æ¨¡å‹æ•æ‰æç¤ºæ€§èƒ½çš„èƒ½åŠ›ï¼Œå¹¶ä»¥æ ·æœ¬æ•ˆç‡é«˜çš„æ–¹å¼é¢„æµ‹ä¸‹ä¸€æ­¥åº”è¯„ä¼°å“ªä¸ªæç¤ºã€‚æ­¤å¤–ï¼ŒHyperbandå¤šä¿çœŸè°ƒåº¦å™¨é€šè¿‡åœ¨ä¸åŒä¿çœŸåº¦çº§åˆ«ä¹‹é—´è‡ªé€‚åº”åˆ†é…èµ„æºæ¥æé«˜æŸ¥è¯¢æ•ˆç‡ï¼Œå‡å°‘éªŒè¯å®ä¾‹æ•°é‡è¯„ä¼°æç¤ºã€‚åœ¨åä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHbBoPsä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æç¤ºé€‰æ‹©å¯¹äºå¤§è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æœ€ä¼˜æ¨¡å‹é€šå¸¸åªèƒ½é€šè¿‡APIè®¿é—®ï¼Œç”¨æˆ·éœ€æ‰‹åŠ¨è°ƒæ•´æç¤ºä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>ç°æœ‰é™æ€é»‘ç®±æç¤ºé€‰æ‹©æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œå¦‚å¿½ç•¥æç¤ºçš„å†…éƒ¨ç»“æ„å’Œç¼ºä¹æŸ¥è¯¢æ•ˆç‡ã€‚</li>
<li>HbBoPsæ–¹æ³•ç»“åˆäº†ç»“æ„æ„ŸçŸ¥å»ºæ¨¡å’ŒHyperbandå¤šä¿çœŸè°ƒåº¦å™¨æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ç»“æ„æ„ŸçŸ¥å»ºæ¨¡æ–¹æ³•ä¸ºæŒ‡ä»¤å’Œç¤ºä¾‹æä¾›ç‹¬ç«‹åµŒå…¥ï¼Œæé«˜ä»£ç†æ¨¡å‹æ•æ‰æç¤ºæ€§èƒ½çš„èƒ½åŠ›ã€‚</li>
<li>Hyperbandå¤šä¿çœŸè°ƒåº¦å™¨å¯è‡ªé€‚åº”åˆ†é…èµ„æºï¼Œæé«˜æŸ¥è¯¢æ•ˆç‡å¹¶å‡å°‘éªŒè¯å®ä¾‹æ•°é‡è¯„ä¼°æç¤ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01dd34d53faeba11e849bc1ac1eafbf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f10c3d9cc6dee2a4b307d0dd58def6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f1e1dc1fff300483efe9ad29df025fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4f2304b8053e1c0381a3f36dacfffa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17e210f00263e7d102dac7a9c22d5023.jpg" align="middle">
</details>




<h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition<del>(FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a \underline{\textbf{M}}atryoshka M\underline{\textbf{A}}mba and Co\underline{\textbf{N}}tras\underline{\textbf{T}}ive Le\underline{\textbf{A}}rning framework</del>(\textbf{Manta}). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. The code is released at <a target="_blank" rel="noopener" href="https://github.com/wenbohuang1002/Manta">https://github.com/wenbohuang1002/Manta</a>. </p>
<blockquote>
<p>åœ¨å°‘æ•°åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œè§†é¢‘çš„é•¿å­åºåˆ—æ›´è‡ªç„¶åœ°è¡¨è¾¾æ•´ä¸ªåŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸»æµåŸºäºTransformerçš„æ–¹æ³•çš„è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚æœ€è¿‘çš„Mambaåœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢è¡¨ç°å‡ºæ•ˆç‡ï¼Œä½†ç›´æ¥å°†Mambaåº”ç”¨äºFSARå¿½è§†äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒåŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç´¯ç§¯ç±»å†…æ–¹å·®ï¼Œè¿™å¯¹FSARæ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œMantaâ€ï¼ˆMatryoshka Mambaå’Œå¯¹æ¯”æ€§å­¦ä¹ æ¡†æ¶ï¼‰çš„æ¡†æ¶ã€‚é¦–å…ˆï¼ŒMatryoshka Mambaå¼•å…¥äº†å¤šä¸ªå†…éƒ¨æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å¯¹å…¨å±€ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚å¤–éƒ¨æ¨¡å—æ•è·è¿™äº›å±€éƒ¨ç‰¹å¾ä¹‹é—´çš„æ—¶é—´çº¿ä¾èµ–æ€§ï¼Œä»¥è¿›è¡Œéšå¼æ—¶é—´å¯¹é½ã€‚å…¶æ¬¡ï¼Œç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•è®¾è®¡äº†ä¸€ç§æ··åˆå¯¹æ¯”å­¦ä¹ æ¨¡å¼ï¼Œæ—¨åœ¨å‡è½»ç±»å†…æ–¹å·®ç´¯ç§¯çš„è´Ÿé¢å½±å“ã€‚Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ æ¨¡å¼åœ¨Mantaçš„å¹¶è¡Œåˆ†æ”¯ä¸­è¿è¡Œï¼Œå¢å¼ºäº†Mambaå¯¹é•¿å­åºåˆ—çš„FSARèƒ½åŠ›ã€‚Mantaåœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å¤§é‡çš„å®è¯ç ”ç©¶è¯æ˜ï¼ŒMantaä»å¤šä¸ªè§’åº¦æ˜¾è‘—æé«˜äº†é•¿å­åºåˆ—çš„FSARæ€§èƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wenbohuang1002/Manta%E3%80%82">https://github.com/wenbohuang1002/Mantaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢†åŸŸçš„ä¸€ä¸ªç ”ç©¶é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºMantaçš„æ–°æ¡†æ¶ï¼Œç»“åˆäº†Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥è§£å†³å»ºæ¨¡é•¿åºåˆ—åŠ¨ä½œçš„æŒ‘æˆ˜ã€‚Mantaé€šè¿‡å¼•å…¥å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç»“åˆç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•çš„æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼æ¥å‡è½»åŒä¸€ç±»åˆ«å†…å˜å¼‚ç§¯ç´¯å¯¹FSARæ€§èƒ½çš„å½±å“ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Matryoshka Mambaè¢«å¼•å…¥ä»¥è§£å†³Transformeråœ¨å»ºæ¨¡é•¿åºåˆ—æ—¶é¢ä¸´çš„è®¡ç®—å¤æ‚æ€§æŒ‘æˆ˜ã€‚</li>
<li>Matryoshka Mambaé€šè¿‡å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚</li>
<li>å¤–æ¨¡å—ç”¨äºæ•æ‰æ—¶é—´åºåˆ—çš„ä¾èµ–å…³ç³»ï¼Œå®ç°éšå¼æ—¶é—´å¯¹é½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œç»“åˆç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•æ¥å‡è½»åŒä¸€ç±»åˆ«å†…å˜å¼‚ç§¯ç´¯å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚</li>
<li>Mantaæ¡†æ¶ç»“åˆäº†Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå®ç°äº†å¯¹é•¿åºåˆ—åŠ¨ä½œè¯†åˆ«çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>Mantaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬SSv2ã€Kineticsã€UCF101å’ŒHMDB51ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c31ca7993f28d9e73b067de3c064c6c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4a80e3c728a35dc9f095f98affe90e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24cb41967c6881d93a2f54f1b672aa61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>




<h2 id="ConceptSearch-Towards-Efficient-Program-Search-Using-LLMs-for-Abstraction-and-Reasoning-Corpus-ARC"><a href="#ConceptSearch-Towards-Efficient-Program-Search-Using-LLMs-for-Abstraction-and-Reasoning-Corpus-ARC" class="headerlink" title="ConceptSearch: Towards Efficient Program Search Using LLMs for   Abstraction and Reasoning Corpus (ARC)"></a>ConceptSearch: Towards Efficient Program Search Using LLMs for   Abstraction and Reasoning Corpus (ARC)</h2><p><strong>Authors:Kartik Singhal, Gautam Shroff</strong></p>
<p>The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to artificial intelligence, demanding broad generalization and few-shot learning capabilities that remain elusive for current deep learning methods, including large language models (LLMs). While LLMs excel in program synthesis, their direct application to ARC yields limited success. To address this, we introduce ConceptSearch, a novel function-search algorithm that leverages LLMs for program generation and employs a concept-based scoring method to guide the search efficiently. Unlike simplistic pixel-based metrics like Hamming distance, ConceptSearch evaluates programs on their ability to capture the underlying transformation concept reflected in the input-output examples. We explore three scoring functions: Hamming distance, a CNN-based scoring function, and an LLM-based natural language scoring function. Experimental results demonstrate the effectiveness of ConceptSearch, achieving a significant performance improvement over direct prompting with GPT-4. Moreover, our novel concept-based scoring exhibits up to 30% greater efficiency compared to Hamming distance, measured in terms of the number of iterations required to reach the correct solution. These findings highlight the potential of LLM-driven program search when integrated with concept-based guidance for tackling challenging generalization problems like ARC. </p>
<blockquote>
<p>æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰å¯¹äººå·¥æ™ºèƒ½æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¦æ±‚å…·å¤‡å¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›å’Œå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œè€Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ½éš¾ä»¥è¾¾åˆ°è¿™ä¸€è¦æ±‚ã€‚è™½ç„¶LLMåœ¨ç¨‹åºåˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç›´æ¥åº”ç”¨äºARCå´æ”¶æ•ˆç”šå¾®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ConceptSearchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å‡½æ•°æœç´¢ç®—æ³•ï¼Œå®ƒåˆ©ç”¨LLMè¿›è¡Œç¨‹åºç”Ÿæˆï¼Œå¹¶é‡‡ç”¨åŸºäºæ¦‚å¿µçš„åˆ†æ•°è®¡ç®—æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼æœç´¢ã€‚ä¸åŒäºç®€å•çš„åƒç´ çº§åº¦é‡æ ‡å‡†ï¼ˆå¦‚æ±‰æ˜è·ç¦»ï¼‰ï¼ŒConceptSearchæ ¹æ®ç¨‹åºæ•è·è¾“å…¥è¾“å‡ºç¤ºä¾‹ä¸­ä½“ç°çš„åŸºç¡€è½¬æ¢æ¦‚å¿µçš„èƒ½åŠ›æ¥è¯„ä¼°ç¨‹åºã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§è¯„åˆ†å‡½æ•°ï¼šæ±‰æ˜è·ç¦»ã€åŸºäºCNNçš„è¯„åˆ†å‡½æ•°å’ŒåŸºäºLLMçš„è‡ªç„¶è¯­è¨€è¯„åˆ†å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConceptSearchæ•ˆæœæ˜¾è‘—ï¼Œä¸ç›´æ¥ä½¿ç”¨GPT-4æç¤ºç›¸æ¯”ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§çš„åŸºäºæ¦‚å¿µçš„è¯„åˆ†æ–¹æ³•ï¼Œåœ¨è¾¾åˆ°æ­£ç¡®è§£æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°æ–¹é¢ï¼Œä¸æ±‰æ˜è·ç¦»ç›¸æ¯”ï¼Œæ•ˆç‡æé«˜äº†é«˜è¾¾30%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“ä¸åŸºäºæ¦‚å¿µçš„æŒ‡å¯¼ç›¸ç»“åˆæ—¶ï¼ŒLLMé©±åŠ¨çš„ç¨‹åºæœç´¢åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–é—®é¢˜ï¼ˆå¦‚ARCï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07322v2">PDF</a> Pre-print of paper accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹äººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åä¸ºConceptSearchçš„æ–°å‹å‡½æ•°æœç´¢ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç¨‹åºç”Ÿæˆï¼Œå¹¶é‡‡ç”¨åŸºäºæ¦‚å¿µçš„è¯„åˆ†æ–¹æ³•æ¥æœ‰æ•ˆæŒ‡å¯¼æœç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConceptSearchåœ¨GPT-4çš„ç›´æ¥æç¤ºä¸‹æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”æ–°å‹æ¦‚å¿µè¯„åˆ†æ–¹æ³•çš„æ•ˆç‡æ¯”Hammingè·ç¦»é«˜å‡º30%ã€‚è¿™ä¸ºè§£å†³ç±»ä¼¼ARCçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–é—®é¢˜æä¾›äº†æ½œåœ¨å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARCå¯¹äººå·¥æ™ºèƒ½æå‡ºäº†æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éš¾ä»¥åº”å¯¹ã€‚</li>
<li>LLMsåœ¨ç¨‹åºåˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç›´æ¥åº”ç”¨äºARCæ•ˆæœæœ‰é™ã€‚</li>
<li>å¼•å…¥ConceptSearchç®—æ³•ï¼Œç»“åˆLLMsè¿›è¡Œç¨‹åºç”Ÿæˆå’Œæ¦‚å¿µè¯„åˆ†æ–¹æ³•ã€‚</li>
<li>ConceptSearché€šè¿‡ä¸‰ç§è¯„åˆ†å‡½æ•°è¿›è¡Œè¯„ä¼°ï¼šHammingè·ç¦»ã€åŸºäºCNNçš„è¯„åˆ†å‡½æ•°å’ŒåŸºäºLLMçš„è‡ªç„¶è¯­è¨€è¯„åˆ†å‡½æ•°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒConceptSearchåœ¨GPT-4çš„ç›´æ¥æç¤ºä¸‹æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>åŸºäºæ¦‚å¿µçš„æ–°å‹è¯„åˆ†æ–¹æ³•ç›¸æ¯”Hammingè·ç¦»å±•ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2613cea523f279353fa3751f21a7065.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db56a44e75b3188e43f133bd7e66c6d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df0d3fc5c766bf805ea80ae5c747e19a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fec33225186bb757a2027c85b0d7450.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-604dfa5c9456293857026a42cf75e857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dd11d3ca1a63eab0de227cccb66a3cb.jpg" align="middle">
</details>




<h2 id="IntellectSeeker-A-Personalized-Literature-Management-System-with-the-Probabilistic-Model-and-Large-Language-Model"><a href="#IntellectSeeker-A-Personalized-Literature-Management-System-with-the-Probabilistic-Model-and-Large-Language-Model" class="headerlink" title="IntellectSeeker: A Personalized Literature Management System with the   Probabilistic Model and Large Language Model"></a>IntellectSeeker: A Personalized Literature Management System with the   Probabilistic Model and Large Language Model</h2><p><strong>Authors:Weizhen Bian, Siyan Liu, Yubo Zhou, Dezhi Chen, Yijie Liao, Zhenzhen Fan, Aobo Wang</strong></p>
<p>Faced with the burgeoning volume of academic literature, researchers often need help with uncertain article quality and mismatches in term searches using traditional academic engines. We introduce IntellectSeeker, an innovative and personalized intelligent academic literature management platform to address these challenges. This platform integrates a Large Language Model (LLM)â€“based semantic enhancement bot with a sophisticated probability model to personalize and streamline literature searches. We adopted the GPT-3.5-turbo model to transform everyday language into professional academic terms across various scenarios using multiple rounds of few-shot learning. This adaptation mainly benefits academic newcomers, effectively bridging the gap between general inquiries and academic terminology. The probabilistic model intelligently filters academic articles to align closely with the specific interests of users, which are derived from explicit needs and behavioral patterns. Moreover, IntellectSeeker incorporates an advanced recommendation system and text compression tools. These features enable intelligent article recommendations based on user interactions and present search results through concise one-line summaries and innovative word cloud visualizations, significantly enhancing research efficiency and user experience. IntellectSeeker offers academic researchers a highly customizable literature management solution with exceptional search precision and matching capabilities. The code can be found here: <a target="_blank" rel="noopener" href="https://github.com/LuckyBian/ISY5001">https://github.com/LuckyBian/ISY5001</a> </p>
<blockquote>
<p>é¢å¯¹æ—¥ç›Šå¢é•¿çš„å­¦æœ¯æ–‡çŒ®é‡ï¼Œç ”ç©¶äººå‘˜åœ¨æ–‡ç« è´¨é‡ä¸ç¡®å®šä»¥åŠä½¿ç”¨ä¼ ç»Ÿå­¦æœ¯æœç´¢å¼•æ“æ—¶æœ¯è¯­æœç´¢ä¸åŒ¹é…çš„é—®é¢˜æ—¶ï¼Œå¸¸å¸¸éœ€è¦å¸®åŠ©ã€‚æˆ‘ä»¬æ¨å‡ºIntellectSeekerï¼Œä¸€ä¸ªåˆ›æ–°ä¸”ä¸ªæ€§åŒ–çš„æ™ºèƒ½å­¦æœ¯æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¯¥å¹³å°é›†æˆåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰å¢å¼ºæœºå™¨äººå’Œé«˜çº§æ¦‚ç‡æ¨¡å‹ï¼Œä»¥ä¸ªæ€§åŒ–å’Œç®€åŒ–æ–‡çŒ®æœç´¢ã€‚æˆ‘ä»¬é‡‡ç”¨GPT-3.5 Turboæ¨¡å‹ï¼Œé€šè¿‡å¤šè½®å°æ ·æœ¬å­¦ä¹ ï¼Œå°†æ—¥å¸¸è¯­è¨€è½¬åŒ–ä¸ºå„ç§åœºæ™¯ä¸­çš„ä¸“ä¸šå­¦æœ¯æœ¯è¯­ã€‚è¿™ç§é€‚åº”ä¸»è¦å¯¹å­¦æœ¯æ–°æ‰‹æœ‰ç›Šï¼Œæœ‰æ•ˆåœ°å¼¥äº†ä¸€èˆ¬æŸ¥è¯¢å’Œå­¦æœ¯æœ¯è¯­ä¹‹é—´çš„å·®è·ã€‚æ¦‚ç‡æ¨¡å‹æ™ºèƒ½åœ°è¿‡æ»¤å­¦æœ¯æ–‡ç« ï¼Œä»¥ç´§å¯†ç¬¦åˆç”¨æˆ·çš„ç‰¹å®šå…´è¶£ï¼Œè¿™äº›å…´è¶£æ¥æºäºæ˜ç¡®çš„éœ€æ±‚å’Œè¡Œä¸ºæ¨¡å¼ã€‚æ­¤å¤–ï¼ŒIntellectSeekerè¿˜é‡‡ç”¨äº†å…ˆè¿›çš„æ¨èç³»ç»Ÿå’Œæ–‡æœ¬å‹ç¼©å·¥å…·ã€‚è¿™äº›åŠŸèƒ½èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·äº¤äº’è¿›è¡Œæ™ºèƒ½æ–‡ç« æ¨èï¼Œå¹¶é€šè¿‡ç®€æ´çš„ä¸€è¡Œæ‘˜è¦å’Œåˆ›æ–°çš„è¯äº‘å¯è§†åŒ–å‘ˆç°æœç´¢ç»“æœï¼Œä»è€Œæ˜¾è‘—æé«˜ç ”ç©¶æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚IntellectSeekerä¸ºå­¦æœ¯ç ”ç©¶äººå‘˜æä¾›äº†é«˜åº¦å¯å®šåˆ¶çš„æ–‡çŒ®ç®¡ç†è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å‡ºè‰²çš„æœç´¢ç²¾åº¦å’ŒåŒ¹é…èƒ½åŠ›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/LuckyBian/ISY5001">https://github.com/LuckyBian/ISY5001</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07213v1">PDF</a> </p>
<p><strong>Summary</strong><br>å­¦æœ¯æ–‡çŒ®æ•°é‡åºå¤§ï¼Œç ”ç©¶è€…é¢ä¸´æ–‡ç« è´¨é‡ä¸ç¡®å®šå’Œæœç´¢æœ¯è¯­ä¸åŒ¹é…çš„é—®é¢˜ã€‚IntellectSeekeræ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦æœ¯æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ¦‚ç‡æ¨¡å‹ï¼Œå®ç°ä¸ªæ€§åŒ–ã€é«˜æ•ˆçš„æ–‡çŒ®æœç´¢ã€‚é‡‡ç”¨GPT-3.5 Turboæ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå°†æ—¥å¸¸è¯­è¨€è½¬åŒ–ä¸ºä¸“ä¸šæœ¯è¯­ã€‚æ¦‚ç‡æ¨¡å‹æ™ºèƒ½è¿‡æ»¤å­¦æœ¯æ–‡ç« ï¼Œä¸ç”¨æˆ·ç‰¹å®šå…´è¶£ç´§å¯†å¯¹é½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬é«˜çº§æ¨èç³»ç»Ÿå’Œæ–‡æœ¬å‹ç¼©å·¥å…·ï¼Œå¯åŸºäºç”¨æˆ·äº¤äº’è¿›è¡Œæ™ºèƒ½æ–‡ç« æ¨èï¼Œå¹¶é€šè¿‡ç®€æ´çš„ä¸€è¡Œæ€»ç»“å’Œåˆ›æ–°çš„è¯äº‘å¯è§†åŒ–å‘ˆç°æœç´¢ç»“æœï¼Œæé«˜ç ”ç©¶æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IntellectSeekeræ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦æœ¯æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œè§£å†³å­¦æœ¯æ–‡çŒ®æµ·é‡ã€è´¨é‡ä¸ç¡®å®šå’Œæœç´¢æœ¯è¯­ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>å¹³å°é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¯­ä¹‰å¢å¼ºï¼Œé‡‡ç”¨GPT-3.5 Turboæ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå°†æ—¥å¸¸è¯­è¨€è½¬åŒ–ä¸ºä¸“ä¸šæœ¯è¯­ã€‚</li>
<li>æ¦‚ç‡æ¨¡å‹æ™ºèƒ½è¿‡æ»¤å­¦æœ¯æ–‡ç« ï¼Œä¸ç”¨æˆ·å…´è¶£ç´§å¯†å¯¹é½ï¼Œç»“åˆç”¨æˆ·æ˜ç¡®éœ€æ±‚å’Œè¡Œä¸ºæ¨¡å¼è¿›è¡Œæ¨èã€‚</li>
<li>IntellectSeekeråŒ…æ‹¬é«˜çº§æ¨èç³»ç»Ÿå’Œæ–‡æœ¬å‹ç¼©å·¥å…·ï¼Œæé«˜ç ”ç©¶æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚</li>
<li>å¹³å°æä¾›ä¸ªæ€§åŒ–çš„æ–‡çŒ®ç®¡ç†è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é«˜åº¦çš„å¯å®šåˆ¶æ€§å’Œæœç´¢ç²¾å‡†åº¦ã€‚</li>
<li>é€šè¿‡ç®€æ´çš„ä¸€è¡Œæ€»ç»“å’Œåˆ›æ–°çš„è¯äº‘å¯è§†åŒ–å‘ˆç°æœç´¢ç»“æœï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£æ–‡ç« æ ¸å¿ƒå†…å®¹å’Œç ”ç©¶é¢†åŸŸã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9f90485775ee221d9e9b233f49c3a4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3cac191be7491a185c315b81753a45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35127a4c7ad66a05d58f3811fc972eaf.jpg" align="middle">
</details>




<h2 id="MM-PoE-Multiple-Choice-Reasoning-via-Process-of-Elimination-using-Multi-Modal-Models"><a href="#MM-PoE-Multiple-Choice-Reasoning-via-Process-of-Elimination-using-Multi-Modal-Models" class="headerlink" title="MM-PoE: Multiple Choice Reasoning via. Process of Elimination using   Multi-Modal Models"></a>MM-PoE: Multiple Choice Reasoning via. Process of Elimination using   Multi-Modal Models</h2><p><strong>Authors:Sayak Chakrabarty, Souradip Pal</strong></p>
<p>This paper introduces Multiple Choice Reasoning via. Process of Elimination using Multi-Modal models, herein referred to as Multi-Modal Process of Elimination (MM-PoE). This novel methodology is engineered to augment the efficacy of Vision-Language Models (VLMs) in multiple-choice visual reasoning tasks. Diverging from conventional approaches that evaluate each option independently, MM-PoE employs a dual-step scoring paradigm that initially identifies and excludes implausible choices, subsequently concentrating on the most probable remaining options. This method emulates human test-taking strategies, where individuals typically eliminate clearly incorrect answers prior to selecting the optimal response. Our empirical evaluations, conducted across three benchmark datasets, reveal that MM-PoE significantly improves both zero-shot and few-shot performance of contemporary state-of-the-art VLMs. Critically, this approach not only broadens the application of the elimination process to multi-modal contexts but also allows few-shot experiments, thereby addressing two principal limitations concerning usage of PoE only in zero-shot settings and only with a language-only framework. As a result, MM-PoE not only refines the reasoning capabilities of VLMs but also broadens their applicability to complex visual question-answering scenarios. All code and documentation supporting our work are available at <a target="_blank" rel="noopener" href="https://pypi.org/project/mm-poe/">https://pypi.org/project/mm-poe/</a>, enabling researchers and practitioners to easily integrate and further develop these techniques. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡æ’é™¤æ³•ï¼ˆProcess of Eliminationï¼‰è¿›è¡Œå¤šæ¨¡æ€é€‰æ‹©æ¨ç†çš„æ–°æ–¹æ³•ï¼Œè¿™é‡Œç§°ä¸ºå¤šæ¨¡æ€æ’é™¤æ³•ï¼ˆMM-PoEï¼‰ã€‚è¿™ç§æ–°æ–¹æ³•æ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé€‰é¡¹è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªé€‰é¡¹çš„æ–¹æ³•ä¸åŒï¼ŒMM-PoEé‡‡ç”¨åŒæ­¥è¯„åˆ†èŒƒå¼ï¼Œé¦–å…ˆè¯†åˆ«å’Œæ’é™¤ä¸å¯èƒ½çš„é€‰é¡¹ï¼Œç„¶åä¸“æ³¨äºæœ€å¯èƒ½çš„å‰©ä½™é€‰é¡¹ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åº”è¯•ç­–ç•¥ï¼Œäººä»¬é€šå¸¸ä¼šåœ¨é€‰æ‹©æœ€ä½³ç­”æ¡ˆä¹‹å‰æ’é™¤æ˜æ˜¾é”™è¯¯çš„ç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMM-PoEæ˜¾è‘—æé«˜äº†å½“å‰æœ€å…ˆè¿›çš„VLMsçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…å°†æ’é™¤è¿‡ç¨‹æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒï¼Œè¿˜å…è®¸è¿›è¡Œå°‘æ ·æœ¬å®éªŒï¼Œä»è€Œè§£å†³äº†ä»¥å¾€åªå°†PoEç”¨äºé›¶æ ·æœ¬è®¾ç½®ä»¥åŠä»…ç”¨äºè¯­è¨€æ¡†æ¶çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ã€‚å› æ­¤ï¼ŒMM-PoEä¸ä»…æé«˜äº†VLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ‰©å¤§äº†å…¶åœ¨å¤æ‚è§†è§‰é—®ç­”åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚æ”¯æŒæˆ‘ä»¬å·¥ä½œçš„æ‰€æœ‰ä»£ç å’Œæ–‡æ¡£éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://pypi.org/project/mm-poe/%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E4%BD%BF%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E5%92%8C%E4%BB%8E%E4%B8%9A%E8%80%85%E8%83%BD%E5%A4%9F%E8%BD%BB%E6%9D%BE%E9%9B%86%E6%88%90%E5%B9%B6%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%BC%80%E5%8F%91%E8%BF%99%E4%BA%9B%E6%8A%80%E6%9C%AF%E3%80%82">https://pypi.org/project/mm-poe/ä¸Šæ‰¾åˆ°ï¼Œä½¿ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…èƒ½å¤Ÿè½»æ¾é›†æˆå¹¶è¿›ä¸€æ­¥å¼€å‘è¿™äº›æŠ€æœ¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07148v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ’é™¤æ³•çš„å¤šæ¨¡æ€é€‰æ‹©æ¨ç†ï¼ˆMulti-Modal Process of Eliminationï¼ŒMM-PoEï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé€‰è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒæ­¥è¯„åˆ†èŒƒå¼ï¼Œåˆæ­¥æ’é™¤ä¸å¯èƒ½çš„é€‰æ‹©ï¼Œç„¶åé›†ä¸­è€ƒè™‘æœ€å¯èƒ½çš„é€‰æ‹©ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMM-PoEæ˜¾è‘—æé«˜äº†å½“å‰æœ€å…ˆè¿›çš„VLMsçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œå¹¶è§£å†³äº†ä»¥å¾€æ’é™¤æ³•ä»…é€‚ç”¨äºé›¶æ ·æœ¬è®¾ç½®å’Œä»…é€‚ç”¨äºè¯­è¨€æ¡†æ¶çš„ä¸¤ä¸ªä¸»è¦å±€é™ã€‚å› æ­¤ï¼ŒMM-PoEä¸ä»…æé«˜äº†VLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”æ‰©å¤§äº†å…¶åœ¨å¤æ‚è§†è§‰é—®ç­”åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ’é™¤æ³•çš„å¤šæ¨¡æ€é€‰æ‹©æ¨ç†æ–¹æ³•ï¼ˆMM-PoEï¼‰ï¼Œç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé€‰è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>MM-PoEé‡‡ç”¨åŒæ­¥è¯„åˆ†èŒƒå¼ï¼Œå…ˆæ’é™¤ä¸å¯èƒ½çš„é€‰æ‹©ï¼Œå†é›†ä¸­è€ƒè™‘æœ€å¯èƒ½çš„é€‰æ‹©ï¼Œè¿™æ¨¡ä»¿äº†äººç±»ç­”é¢˜æ—¶çš„ç­–ç•¥ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒMM-PoEæ˜¾è‘—æé«˜äº†VLMsçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>MM-PoEè§£å†³äº†ä»¥å¾€æ’é™¤æ³•ä»…é€‚ç”¨äºé›¶æ ·æœ¬è®¾ç½®å’Œä»…é€‚ç”¨äºè¯­è¨€æ¡†æ¶çš„ä¸¤ä¸ªä¸»è¦å±€é™ã€‚</li>
<li>MM-PoEæé«˜äº†VLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ‰©å¤§äº†å…¶åœ¨å¤æ‚è§†è§‰é—®ç­”åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚</li>
<li>MM-PoEæ–¹æ³•æ‰€æœ‰æ”¯æŒä»£ç å’Œæ–‡æ¡£éƒ½å¯åœ¨[<a target="_blank" rel="noopener" href="https://pypi.org/project/mm-poe/]%E8%AE%BF%E9%97%AE%EF%BC%8C%E4%BE%BF%E4%BA%8E%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E5%92%8C%E5%AE%9E%E8%B7%B5%E8%80%85%E9%9B%86%E6%88%90%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%91%E5%B1%95%E8%BF%99%E4%BA%9B%E6%8A%80%E6%9C%AF%E3%80%82">https://pypi.org/project/mm-poe/]è®¿é—®ï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œå®è·µè€…é›†æˆå’Œè¿›ä¸€æ­¥å‘å±•è¿™äº›æŠ€æœ¯ã€‚</a></li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7d85f3738de124135a143e9d0b2abc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fc346b79393506b801ac21874a648fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e074a958c938d2fa372530ba06dd6b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d76e49a8e6abadb644d5f675125c75f.jpg" align="middle">
</details>




<h2 id="DiffCLIP-Few-shot-Language-driven-Multimodal-Classifier"><a href="#DiffCLIP-Few-shot-Language-driven-Multimodal-Classifier" class="headerlink" title="DiffCLIP: Few-shot Language-driven Multimodal Classifier"></a>DiffCLIP: Few-shot Language-driven Multimodal Classifier</h2><p><strong>Authors:Jiaqing Zhang, Mingxiang Cao, Xue Yang, Kai Jiang, Yunsong Li</strong></p>
<p>Visual language models like Contrastive Language-Image Pretraining (CLIP) have shown impressive performance in analyzing natural images with language information. However, these models often encounter challenges when applied to specialized domains such as remote sensing due to the limited availability of image-text pairs for training. To tackle this issue, we introduce DiffCLIP, a novel framework that extends CLIP to effectively convey comprehensive language-driven semantic information for accurate classification of high-dimensional multimodal remote sensing images. DiffCLIP is a few-shot learning method that leverages unlabeled images for pretraining. It employs unsupervised mask diffusion learning to capture the distribution of diverse modalities without requiring labels. The modality-shared image encoder maps multimodal data into a unified subspace, extracting shared features with consistent parameters across modalities. A well-trained image encoder further enhances learning by aligning visual representations with class-label text information from CLIP. By integrating these approaches, DiffCLIP significantly boosts CLIP performance using a minimal number of image-text pairs. We evaluate DiffCLIP on widely used high-dimensional multimodal datasets, demonstrating its effectiveness in addressing few-shot annotated classification tasks. DiffCLIP achieves an overall accuracy improvement of 10.65% across three remote sensing datasets compared with CLIP, while utilizing only 2-shot image-text pairs. The code has been released at <a target="_blank" rel="noopener" href="https://github.com/icey-zhang/DiffCLIP">https://github.com/icey-zhang/DiffCLIP</a>. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»“åˆè¯­è¨€ä¿¡æ¯åˆ†æè‡ªç„¶å›¾åƒæ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ¨¡å‹åº”ç”¨äºé¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸæ—¶ï¼Œç”±äºå¯ç”¨äºè®­ç»ƒçš„å›¾ç‰‡æ–‡æœ¬é…å¯¹æ•°é‡æœ‰é™ï¼Œç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªå°†CLIPæ‰©å±•åˆ°æœ‰æ•ˆä¼ è¾¾å…¨é¢çš„è¯­è¨€é©±åŠ¨è¯­ä¹‰ä¿¡æ¯çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¯¹é«˜ç»´å¤šæ¨¡å¼é¥æ„Ÿå›¾åƒè¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚DiffCLIPæ˜¯ä¸€ç§å°æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡è®°çš„å›¾åƒè¿›è¡Œé¢„è®­ç»ƒã€‚å®ƒé‡‡ç”¨æ— ç›‘ç£çš„æ©è†œæ‰©æ•£å­¦ä¹ æ¥æ•æ‰ä¸åŒæ¨¡å¼çš„åˆ†å¸ƒè€Œæ— éœ€æ ‡ç­¾ã€‚æ¨¡æ€å…±äº«å›¾åƒç¼–ç å™¨å°†å¤šæ¨¡å¼æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€å­ç©ºé—´ï¼Œæå–è·¨æ¨¡æ€çš„ä¸€è‡´å‚æ•°å…±äº«ç‰¹å¾ã€‚ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å›¾åƒç¼–ç å™¨é€šè¿‡å°†ä¸CLIPä¸­çš„ç±»åˆ«æ ‡ç­¾æ–‡æœ¬ä¿¡æ¯å¯¹é½çš„è§†è§‰è¡¨ç¤ºç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å­¦ä¹ ã€‚é€šè¿‡æ•´åˆè¿™äº›æ–¹æ³•ï¼ŒDiffCLIPä½¿ç”¨å°‘é‡çš„å›¾åƒæ–‡æœ¬é…å¯¹æ˜¾è‘—æå‡äº†CLIPçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„é«˜ç»´å¤šæ¨¡å¼æ•°æ®é›†ä¸Šè¯„ä¼°äº†DiffCLIPçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å®ƒåœ¨è§£å†³å°æ ·æœ¬æ ‡æ³¨åˆ†ç±»ä»»åŠ¡æ–¹é¢çš„æ•ˆæœã€‚ç›¸è¾ƒäºCLIPï¼ŒDiffCLIPåœ¨ä¸‰ä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šçš„æ€»ä½“å‡†ç¡®åº¦æé«˜äº†10.65%ï¼Œä¸”åœ¨ä»…ä½¿ç”¨2ä¸ªæ ·æœ¬çš„å›¾åƒæ–‡æœ¬é…å¯¹çš„æƒ…å†µä¸‹å–å¾—äº†è¿™ä¸€æˆæœã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/icey-zhang/DiffCLIP%E3%80%82">https://github.com/icey-zhang/DiffCLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07119v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åä¸ºDiffCLIPçš„æ–°å‹æ¡†æ¶æ‰©å±•äº†CLIPæ¨¡å‹ï¼Œä½¿å…¶èƒ½æœ‰æ•ˆä¼ è¾¾å…¨é¢çš„è¯­è¨€é©±åŠ¨è¯­ä¹‰ä¿¡æ¯ï¼Œç”¨äºå¯¹é«˜ç»´å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒè¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚DiffCLIPé‡‡ç”¨æ— ç›‘ç£çš„æ©è†œæ‰©æ•£å­¦ä¹ æ–¹æ³•æ•æ‰ä¸åŒæ¨¡æ€çš„åˆ†å¸ƒä¿¡æ¯ï¼Œæ— éœ€æ ‡ç­¾å³å¯è¿›è¡Œé¢„è®­ç»ƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡æ€å…±äº«å›¾åƒç¼–ç å™¨å°†å¤šæ¨¡æ€æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€å­ç©ºé—´ï¼Œæå–è·¨æ¨¡æ€çš„ä¸€è‡´å‚æ•°å…±äº«ç‰¹å¾ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼ŒDiffCLIPåœ¨ä»…ä½¿ç”¨å°‘é‡å›¾åƒæ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†CLIPçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffCLIPæˆåŠŸæ‰©å±•äº†CLIPæ¨¡å‹ï¼Œç”¨äºå¤„ç†é«˜ç»´å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒçš„åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>DiffCLIPé‡‡ç”¨æ— ç›‘ç£çš„æ©è†œæ‰©æ•£å­¦ä¹ æ–¹æ³•ï¼Œèƒ½åœ¨ç¼ºä¹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æ•æ‰ä¸åŒæ¨¡æ€çš„åˆ†å¸ƒä¿¡æ¯ã€‚</li>
<li>æ¨¡æ€å…±äº«å›¾åƒç¼–ç å™¨å°†å¤šæ¨¡æ€æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€å­ç©ºé—´ï¼Œæå‡è·¨æ¨¡æ€ç‰¹å¾æå–çš„æ•ˆæœã€‚</li>
<li>DiffCLIPåˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ï¼Œé€šè¿‡ç»“åˆCLIPçš„ç±»æ ‡ç­¾æ–‡æœ¬ä¿¡æ¯ï¼Œå¼ºåŒ–è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>ä»…ä½¿ç”¨å°‘é‡å›¾åƒæ–‡æœ¬å¯¹ï¼ŒDiffCLIPå°±èƒ½æ˜¾è‘—æé«˜CLIPçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¹¿æ³›ä½¿ç”¨çš„é«˜ç»´å¤šæ¨¡æ€æ•°æ®é›†ä¸Šï¼ŒDiffCLIPå±•ç¤ºå‡ºäº†ä¼˜ç§€çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å°‘æ ·æœ¬æ ‡æ³¨åˆ†ç±»ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>DiffCLIPçš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb1a78ab4ee5014272178d3f194ba8e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1003ffb19b00943ea67ee68d1b9e07cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788509afd98d72c0d1a264c7c3c9b394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41032838b9e9d9cc668a1bfaef2efb05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb911d468c04f882dadd368a21cde2e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b7ebfd08c31567487782d69cdb13a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdec4947fe7a8873c2bea41e1b79ac59.jpg" align="middle">
</details>




<h2 id="FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering"><a href="#FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering" class="headerlink" title="FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering"></a>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering</h2><p><strong>Authors:Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji</strong></p>
<p>Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤šè·³é—®ç­”æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå®ƒè¦æ±‚è·¨è¶Šå¤šç§ä¿¡æ¯æ¥æºï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰è¿›è¡Œæ¨ç†ä»¥å›ç­”é—®é¢˜ã€‚å°½ç®¡è§†è§‰é—®ç­”å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¤šè·³è®¾ç½®ä»ç„¶æœªè¢«æ¢ç´¢ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•è·³é—®ç­”æˆ–å•ä¸€æ¨¡æ€ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆç°å®ä¸–ç•Œåœºæ™¯ï¼Œå¦‚åˆ†æå¤šæ¨¡æ€æ•™è‚²ææ–™ã€æ€»ç»“å†—é•¿çš„å­¦æœ¯è®ºæ–‡æˆ–è§£é‡Šç»“åˆå›¾è¡¨ã€å›¾åƒå’Œæ–‡æœ¬çš„ç§‘å­¦ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¼•å…¥äº†åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè®­ç»ƒå¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ª5é˜¶æ®µçš„ç®¡é“ï¼Œæ¶‰åŠä»Wikipediaè·å–ç›¸å…³çš„å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆã€å¹¶é€šè¿‡ä¸¥æ ¼çš„æ ‡å‡†å¯¹å…¶è¿›è¡ŒéªŒè¯ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹å¹¶åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨æ ·æœ¬å¤§å°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œåœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¹³å‡ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æ–¹é¢æ¯”åœ¨äººç±»æ”¶é›†çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹é«˜å‡º1.9ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•°æ®åˆæˆæ–¹æ³•å°†ä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹æä¾›åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07030v1">PDF</a> 20 pages, 11 figures, 10 tables, Submitted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤šè·³é—®ç­”ä»»åŠ¡çš„æ–°å‹æ•°æ®é›†æ„å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡5é˜¶æ®µç®¡é“å®ç°ï¼ŒåŒ…æ‹¬ä»Wikipediaè·å–ç›¸å…³å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜ç­”æ¡ˆå¹¶é€šè¿‡ä¸¥æ ¼æ ‡å‡†éªŒè¯æ•°æ®è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ ·æœ¬é‡ä¸‹ï¼ŒåŸºäºåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹å¹³å‡ç²¾ç¡®åŒ¹é…åº¦é«˜å‡ºäººç±»æ”¶é›†æ•°æ®è®­ç»ƒçš„æ¨¡å‹1.9ã€‚è¯¥æ•°æ®åˆæˆæ–¹æ³•å°†ä¸ºå¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤šè·³é—®ç­”éœ€è¦è·¨è¶Šå¤šä¸ªä¿¡æ¯æºè¿›è¡Œæ¨ç†ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å•è·³é—®ç­”æˆ–å•ä¸€æ¨¡æ€ï¼Œä¸é€‚ç”¨äºåˆ†æå¤šæ¨¡æ€æ•™è‚²ææ–™ã€æ€»ç»“å­¦æœ¯æ–‡ç« æˆ–è§£è¯»ç»“åˆå›¾è¡¨ã€å›¾åƒå’Œæ–‡æœ¬çš„ç§‘ç ”ç ”ç©¶ç­‰ç°å®åœºæ™¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®é›†æ„å»ºæ–¹æ³•ï¼Œä¸“ä¸ºå¤šæ¨¡æ€å¤šè·³é—®ç­”ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…å«5ä¸ªé˜¶æ®µçš„ç®¡é“ï¼Œæ¶‰åŠä»Wikipediaè·å–ç›¸å…³å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å¹¶éªŒè¯æ•°æ®è´¨é‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒåŸºäºåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºåŸºäºäººç±»æ”¶é›†æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>åˆæˆæ•°æ®çš„æ–¹æ³•å…·æœ‰æ½œåŠ›ï¼Œä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹æä¾›åšå®åŸºç¡€ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c2dc25eae655aa1f9822823831cb116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17398bc33f88f8a6a7fa79dadeef87c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c32c63d1ed4421e9e3f94c7f51067fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ba2a00dc67c5398d1097a53dc801633.jpg" align="middle">
</details>




<h2 id="AutoReason-Automatic-Few-Shot-Reasoning-Decomposition"><a href="#AutoReason-Automatic-Few-Shot-Reasoning-Decomposition" class="headerlink" title="AutoReason: Automatic Few-Shot Reasoning Decomposition"></a>AutoReason: Automatic Few-Shot Reasoning Decomposition</h2><p><strong>Authors:Arda Sevinc, Abdurrahman Gumus</strong></p>
<p>Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no capability to adjust itself to different queries.   In this work, we propose a system to automatically generate rationales using CoT. Our method improves multi-step implicit reasoning capabilities by decomposing the implicit query into several explicit questions. This provides interpretability for the model, improving reasoning in weaker LLMs. We test our approach with two Q&amp;A datasets: StrategyQA and HotpotQA. We show an increase in accuracy with both, especially on StrategyQA.   To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/miralab-ai/autoreason">https://github.com/miralab-ai/autoreason</a>. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æœ€è¿‘åœ¨ç ”ç©¶ä¸­è¢«å¼•å…¥ä½œä¸ºä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é€æ­¥æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒCoTçš„åº”ç”¨å…·æœ‰ä¸€å®šçš„å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦æ‰‹å·¥åˆ¶ä½œçš„å°‘é‡ç¤ºä¾‹æç¤ºï¼Œå¹¶ä¸”æ— æ³•è‡ªè¡Œé€‚åº”ä¸åŒçš„æŸ¥è¯¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨CoTè‡ªåŠ¨ç”Ÿæˆç†ç”±çš„ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†éšå¼æŸ¥è¯¢åˆ†è§£æˆå‡ ä¸ªæ˜¾å¼é—®é¢˜æ¥æé«˜å¤šæ­¥éšå¼æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ºæ¨¡å‹æä¾›äº†å¯è§£é‡Šæ€§ï¼Œæé«˜äº†è¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé—®ç­”æ•°æ®é›†StrategyQAå’ŒHotpotQAä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸¤ä¸ªæ•°æ®é›†çš„å‡†ç¡®ç‡éƒ½æœ‰æ‰€æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨StrategyQAä¸Šã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæœ¬ç ”ç©¶çš„å®Œæ•´æºä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/miralab-ai/autoreason%E3%80%82">https://github.com/miralab-ai/autoreasonã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06975v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ”¹è¿›äº†åˆ†æ­¥æ¨ç†çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒCoTå­˜åœ¨å±€é™æ€§ï¼Œå¦‚éœ€è¦æ‰‹å·¥åˆ¶ä½œçš„å°‘æ•°æ¡ˆä¾‹æç¤ºï¼Œæ— æ³•é€‚åº”ä¸åŒçš„æŸ¥è¯¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆç†ç”±çš„ç³»ç»Ÿï¼Œä½¿ç”¨CoTæ¥æé«˜å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†éšå¼æŸ¥è¯¢åˆ†è§£æˆå¤šä¸ªæ˜¾å¼é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è§£é‡Šæ€§å¹¶å¢å¼ºè¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å·²ç»åœ¨GitHubä¸Šå…¬å¼€å®Œæ•´æºä»£ç ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶ã€‚åœ¨StrategyQAå’ŒHotpotQAé—®ç­”æ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡æœ‰æ‰€æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨StrategyQAä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†æ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CoTå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ‰‹å·¥åˆ¶ä½œçš„å°‘æ•°æ¡ˆä¾‹æç¤ºä¸”æ— æ³•é€‚åº”ä¸åŒæŸ¥è¯¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆç†ç”±çš„ç³»ç»Ÿï¼Œä½¿ç”¨CoTæé«˜å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°†éšå¼æŸ¥è¯¢åˆ†è§£æˆå¤šä¸ªæ˜¾å¼é—®é¢˜ä»¥æé«˜æ¨¡å‹è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨StrategyQAå’ŒHotpotQAæ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œå‡†ç¡®ç‡æœ‰æ‰€æé«˜ã€‚</li>
<li>ç³»ç»Ÿå¯æé«˜è¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c167f0777ce79c9a79fea76ffcc1c86e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9a3f488f58e157801516903c37d6bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30804a39d9451a1e81c55cbb1e4d0ea7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c883d88f4370499ccd24f81b02c458b7.jpg" align="middle">
</details>




<h2 id="Fully-Open-Source-Moxin-7B-Technical-Report"><a href="#Fully-Open-Source-Moxin-7B-Technical-Report" class="headerlink" title="Fully Open Source Moxin-7B Technical Report"></a>Fully Open Source Moxin-7B Technical Report</h2><p><strong>Authors:Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</strong></p>
<p>Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be â€œopen-source,â€ which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of â€œopen scienceâ€ through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å†äº†é‡å¤§å˜é©ï¼Œå…¶å—æ¬¢è¿ç¨‹åº¦å’Œèƒ½åŠ›éƒ½è¿…é€Ÿä¸Šå‡ã€‚å¼•é¢†è¿™ä¸€å˜é©çš„æ˜¯ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4å’ŒGPT-o1ï¼Œå®ƒä»¬ç”±äºå‡ºè‰²çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§è€Œå—åˆ°äººå·¥æ™ºèƒ½ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚åŒæ—¶ï¼Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚LLaMAå’ŒMistralï¼Œç”±äºèƒ½å¤Ÿåœ¨å„ç§åº”ç”¨ç¨‹åºä¸­è½»æ¾å®šåˆ¶å’Œéƒ¨ç½²æ¨¡å‹ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ—¥ç›Šæ™®åŠåšå‡ºäº†å·¨å¤§è´¡çŒ®ã€‚å°½ç®¡å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºåˆ›æ–°å’Œç ”å‘æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹çš„å•†ä¸šåŒ–å¼•å‘äº†å…³äºé€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚è®¸å¤šå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹æœªèƒ½æ»¡è¶³åŸºæœ¬çš„é€æ˜åº¦è¦æ±‚ï¼Œéšç’äº†å…³é”®ç»„ä»¶ï¼Œå¦‚è®­ç»ƒä»£ç å’Œæ•°æ®ï¼Œæœ‰äº›åˆ™æ‰“ç€â€œå¼€æºâ€çš„å£å·å´ä½¿ç”¨é™åˆ¶æ€§è®¸å¯ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥åˆ›æ–°ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å®Œå…¨ç¬¦åˆæ¨¡å‹å¼€æ”¾æ¡†æ¶ï¼ˆMOFï¼‰çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹â€”â€”Moxin 7Bã€‚MOFæ˜¯ä¸€ä¸ªè¯„çº§åˆ†ç±»ç³»ç»Ÿï¼Œæ ¹æ®æ¨¡å‹çš„å®Œæ•´æ€§å’Œå¼€æ”¾æ€§æ¥è¯„ä¼°äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œéµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®çš„åŸåˆ™ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å…¨é¢å‘å¸ƒé¢„è®­ç»ƒä»£ç å’Œé…ç½®ã€è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†ä»¥åŠä¸­é—´å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼Œè¾¾åˆ°äº†MOFåˆ†ç±»ä¸­çš„æœ€é«˜çº§åˆ«â€œå…¬å¼€ç§‘å­¦â€ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å°æ ·ä¾‹è¯„ä¼°ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06845v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å†äº†ä¸€æ¬¡é‡å¤§è½¬å˜ï¼Œå—åˆ°å¹¿æ³›å…³æ³¨å¹¶æå‡äº†èƒ½åŠ›ã€‚GPT-4å’ŒGPT-o1ç­‰ä¸“æœ‰LLMså› å‡ºè‰²çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§è€Œå—åˆ°AIç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚åŒæ—¶ï¼ŒLLaMAå’ŒMistralç­‰å¼€æºLLMsé€šè¿‡æ˜“äºå®šåˆ¶å’Œéƒ¨ç½²çš„ç‰¹ç‚¹æ¨åŠ¨äº†LLMsçš„æ™®åŠã€‚ç„¶è€Œï¼Œå•†ä¸šåŒ–çš„LLMså¼•å‘äº†å…³äºé€æ˜åº¦ã€å¯å¤åˆ¶æ€§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Moxin 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªéµå¾ªæ¨¡å‹å¼€æ”¾æ¡†æ¶ï¼ˆMOFï¼‰å¼€å‘çš„å®Œå…¨å¼€æºçš„LLMã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒä»£ç å’Œé…ç½®ã€è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†ä»¥åŠä¸­é—´å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹çš„å…¨é¢å‘å¸ƒæ–¹é¢è¾¾åˆ°äº†â€œå…¬å¼€ç§‘å­¦â€çš„æœ€é«˜ç­‰çº§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­å–å¾—äº†ä¼˜äºå…¶ä»–æµè¡Œ7Bæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨å°æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å†äº†é‡å¤§è½¬å˜ï¼Œå—åˆ°å¹¿æ³›å…³æ³¨å’Œæå‡èƒ½åŠ›ã€‚</li>
<li>ä¸“æœ‰LLMså¦‚GPT-4å’ŒGPT-o1å› æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§å—åˆ°å…³æ³¨ã€‚</li>
<li>å¼€æºLLMsæ¨åŠ¨äº†LLMsçš„æ™®åŠï¼Œä½†å­˜åœ¨å…³äºå•†ä¸šåŒ–LLMsçš„é€æ˜åº¦ã€å¯å¤åˆ¶æ€§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚</li>
<li>Moxin 7Bæ˜¯ä¸€ä¸ªéµå¾ªæ¨¡å‹å¼€æ”¾æ¡†æ¶ï¼ˆMOFï¼‰å¼€å‘çš„å¼€æºLLMã€‚</li>
<li>Moxin 7Bå®ç°äº†å…¨é¢çš„å¼€æ”¾ç§‘å­¦åŸåˆ™ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒä»£ç å’Œé…ç½®çš„å‘å¸ƒã€‚</li>
<li>å®éªŒè¡¨æ˜Moxin 7Båœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶åœ¨å°æ ·æœ¬è¯„ä¼°ä¸­å…·å¤‡ç«äº‰åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5d8f4d2036b738a4ca529ed6d87573d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-804d135864a1480ed317c9aa47e6972f.jpg" align="middle">
</details>




<h2 id="Intent-driven-In-context-Learning-for-Few-shot-Dialogue-State-Tracking"><a href="#Intent-driven-In-context-Learning-for-Few-shot-Dialogue-State-Tracking" class="headerlink" title="Intent-driven In-context Learning for Few-shot Dialogue State Tracking"></a>Intent-driven In-context Learning for Few-shot Dialogue State Tracking</h2><p><strong>Authors:Zihao Yi, Zhe Xu, Ying Shen</strong></p>
<p>Dialogue state tracking (DST) plays an essential role in task-oriented dialogue systems. However, userâ€™s input may contain implicit information, posing significant challenges for DST tasks. Additionally, DST data includes complex information, which not only contains a large amount of noise unrelated to the current turn, but also makes constructing DST datasets expensive. To address these challenges, we introduce Intent-driven In-context Learning for Few-shot DST (IDIC-DST). By extracting userâ€™s intent, we propose an Intent-driven Dialogue Information Augmentation module to augment the dialogue information, which can track dialogue states more effectively. Moreover, we mask noisy information from DST data and rewrite userâ€™s input in the Intent-driven Examples Retrieval module, where we retrieve similar examples. We then utilize a pre-trained large language model to update the dialogue state using the augmented dialogue information and examples. Experimental results demonstrate that IDIC-DST achieves state-of-the-art performance in few-shot settings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets. </p>
<blockquote>
<p>å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œç”¨æˆ·çš„è¾“å…¥å¯èƒ½åŒ…å«éšå«ä¿¡æ¯ï¼Œç»™DSTä»»åŠ¡å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒDSTæ•°æ®åŒ…å«å¤æ‚ä¿¡æ¯ï¼Œä¸ä»…åŒ…å«å¤§é‡ä¸å½“å‰è½®æ¬¡æ— å…³çš„å™ªå£°ï¼Œè€Œä¸”æ„å»ºDSTæ•°æ®é›†çš„æˆæœ¬ä¹Ÿå¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºå°‘é‡æ•°æ®DSTçš„æ„å›¾é©±åŠ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIDIC-DSTï¼‰ã€‚é€šè¿‡æå–ç”¨æˆ·çš„æ„å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ„å›¾é©±åŠ¨å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼Œä»¥å¢å¼ºå¯¹è¯ä¿¡æ¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è·Ÿè¸ªå¯¹è¯çŠ¶æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»DSTæ•°æ®ä¸­å±è”½äº†å˜ˆæ‚çš„ä¿¡æ¯ï¼Œå¹¶åœ¨æ„å›¾é©±åŠ¨çš„ä¾‹å­æ£€ç´¢æ¨¡å—ä¸­é‡å†™äº†ç”¨æˆ·çš„è¾“å…¥ï¼Œæˆ‘ä»¬æ£€ç´¢äº†ç±»ä¼¼çš„ä¾‹å­ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å¢å¼ºçš„å¯¹è¯ä¿¡æ¯å’Œä¾‹å­æ¥æ›´æ–°å¯¹è¯çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°‘é‡æ•°æ®è®¾ç½®ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†ç”¨æˆ·è¾“å…¥å¯èƒ½åŒ…å«éšæ™¦ä¿¡æ¯ï¼Œç»™DSTä»»åŠ¡å¸¦æ¥æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒDSTæ•°æ®åŒ…å«å¤æ‚ä¿¡æ¯ï¼Œä¸ä»…åŒ…å«å¤§é‡ä¸å½“å‰å¯¹è¯æ— å…³å™ªå£°ï¼Œä¸”æ„å»ºDSTæ•°æ®é›†æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ„å›¾çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIDIC-DSTï¼‰æ–¹æ³•ã€‚é€šè¿‡æå–ç”¨æˆ·æ„å›¾ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ„å›¾é©±åŠ¨å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—æ¥å¢å¼ºå¯¹è¯ä¿¡æ¯ï¼Œæ›´æœ‰æ•ˆåœ°è¿½è¸ªå¯¹è¯çŠ¶æ€ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä»DSTæ•°æ®ä¸­å±è”½å™ªå£°ä¿¡æ¯ï¼Œå¹¶åœ¨æ„å›¾é©±åŠ¨çš„ä¾‹å­æ£€ç´¢æ¨¡å—ä¸­é‡å†™ç”¨æˆ·è¾“å…¥ï¼Œæ£€ç´¢ç›¸ä¼¼ä¾‹å­ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®å¢å¼ºçš„å¯¹è¯ä¿¡æ¯å’Œä¾‹å­æ›´æ–°å¯¹è¯çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°æ ·æœ¬è®¾ç½®ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­éå¸¸é‡è¦ï¼Œä½†é¢ä¸´ç”¨æˆ·è¾“å…¥éšæ™¦ä¿¡æ¯å’Œæ•°æ®å¤æ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåŸºäºæ„å›¾çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIDIC-DSTï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ„å›¾é©±åŠ¨å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—å¢å¼ºå¯¹è¯ä¿¡æ¯ï¼Œæé«˜å¯¹è¯çŠ¶æ€è¿½è¸ªæ•ˆæœã€‚</li>
<li>IDIC-DSTèƒ½å¤Ÿå±è”½ä¸å½“å‰å¯¹è¯æ— å…³çš„å™ªå£°ä¿¡æ¯ï¼Œå¹¶é‡å†™ç”¨æˆ·è¾“å…¥ä»¥æ£€ç´¢ç›¸ä¼¼ä¾‹å­ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ›´æ–°å¯¹è¯çŠ¶æ€ã€‚</li>
<li>IDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°æ ·æœ¬è®¾ç½®ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ„å»ºæ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8742c6b8028b15b6244c52f761b6bde9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947f300819b2f915778e4b9639623290.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1cf68b86876326142a7ddfc117835ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf7ba27ee5a1f9a8924ebb587f9c4db.jpg" align="middle">
</details>




<h2 id="Few-Shot-Domain-Adaptation-for-Named-Entity-Recognition-via-Joint-Constrained-k-Means-and-Subspace-Selection"><a href="#Few-Shot-Domain-Adaptation-for-Named-Entity-Recognition-via-Joint-Constrained-k-Means-and-Subspace-Selection" class="headerlink" title="Few-Shot Domain Adaptation for Named-Entity Recognition via Joint   Constrained k-Means and Subspace Selection"></a>Few-Shot Domain Adaptation for Named-Entity Recognition via Joint   Constrained k-Means and Subspace Selection</h2><p><strong>Authors:Ayoub Hammal, Benno Uthayasooriyar, Caio Corro</strong></p>
<p>Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER on several English datasets. </p>
<blockquote>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å…·æœ‰ä¸åŒå®ä½“å®šä¹‰çš„è·¨åŸŸä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡é’ˆå¯¹å°‘æ ·æœ¬NERï¼Œæ—¨åœ¨ä»¥æœ€å°çš„ç›‘ç£å°†çŸ¥è¯†è½¬ç§»åˆ°æ–°é¢†åŸŸã€‚ä¸åŒäºä»¥å‰ä»…ä¾èµ–æœ‰é™æ ‡æ³¨æ•°æ®çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼±ç›‘ç£ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†å°‘é‡æœ‰æ ‡ç­¾æ•°æ®é›†ä¸å¤§é‡æ— æ ‡ç­¾æ•°æ®ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•äº†å¸¦æœ‰æ ‡ç­¾ç›‘ç£ã€é›†ç¾¤å¤§å°çº¦æŸå’Œç‰¹å®šäºé¢†åŸŸçš„åˆ¤åˆ«å­ç©ºé—´é€‰æ‹©çš„k-meansç®—æ³•ã€‚è¿™ä¸€ç»Ÿä¸€æ¡†æ¶åœ¨å¤šä¸ªè‹±æ–‡æ•°æ®é›†çš„å°‘æ ·æœ¬NERä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00426v1">PDF</a> COLING 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§å¼±ç›‘ç£ç®—æ³•æ¥è§£å†³ä¸åŒé¢†åŸŸä¸­çš„å®ä½“å®šä¹‰å·®å¼‚é—®é¢˜ã€‚è¯¥ç®—æ³•ç»“åˆäº†å°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨æ•°æ®ï¼Œæ‰©å±•äº†K-meansç®—æ³•ï¼Œå¹¶æ·»åŠ äº†æ ‡ç­¾ç›‘ç£ã€èšç±»å¤§å°çº¦æŸå’Œé¢†åŸŸç‰¹å®šçš„åˆ¤åˆ«å­ç©ºé—´é€‰æ‹©ã€‚è¯¥ç»Ÿä¸€æ¡†æ¶åœ¨å¤šä¸ªè‹±æ–‡æ•°æ®é›†ä¸Šå®ç°äº†å°‘æ ·æœ¬NERçš„æœ€ä¼˜ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­è·¨é¢†åŸŸåº”ç”¨çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä½“å®šä¹‰ä¸åŒçš„æƒ…å¢ƒä¸‹ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå¼±ç›‘ç£ç®—æ³•ï¼Œç»“åˆäº†å°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨æ•°æ®æ¥è¿›è¡Œå°‘æ ·æœ¬NERã€‚</li>
<li>è¯¥ç®—æ³•æ‰©å±•äº†K-meansç®—æ³•ï¼Œå¼•å…¥äº†æ ‡ç­¾ç›‘ç£ã€èšç±»å¤§å°çº¦æŸä»¥åŠç‰¹å®šé¢†åŸŸçš„åˆ¤åˆ«å­ç©ºé—´é€‰æ‹©ã€‚</li>
<li>æ­¤æ¡†æ¶åœ¨å¤šä¸ªè‹±æ–‡æ•°æ®é›†ä¸Šå®ç°äº†å°‘æ ·æœ¬NERçš„æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡æœ€å°‘çš„äººå·¥ç›‘ç£æ¥è½¬ç§»çŸ¥è¯†åˆ°æ–°é¢†åŸŸã€‚</li>
<li>é€šè¿‡ç»“åˆä¸åŒæ•°æ®æºå’Œç®—æ³•çš„ä¼˜åŒ–ï¼Œè¯¥è®ºæ–‡å®ç°äº†æ˜¾è‘—çš„ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-536709c68b346c7db578e83970eab0e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d7808cd24deade66be3a1ccf3e290c9.jpg" align="middle">
</details>




<h2 id="EFSA-Episodic-Few-Shot-Adaptation-for-Text-to-Image-Retrieval"><a href="#EFSA-Episodic-Few-Shot-Adaptation-for-Text-to-Image-Retrieval" class="headerlink" title="EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval"></a>EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval</h2><p><strong>Authors:Muhammad Huzaifa, Yova Kementchedjhieva</strong></p>
<p>Text-to-image retrieval is a critical task for managing diverse visual content, but common benchmarks for the task rely on small, single-domain datasets that fail to capture real-world complexity. Pre-trained vision-language models tend to perform well with easy negatives but struggle with hard negativesâ€“visually similar yet incorrect imagesâ€“especially in open-domain scenarios. To address this, we introduce Episodic Few-Shot Adaptation (EFSA), a novel test-time framework that adapts pre-trained models dynamically to a queryâ€™s domain by fine-tuning on top-k retrieved candidates and synthetic captions generated for them. EFSA improves performance across diverse domains while preserving generalization, as shown in evaluations on queries from eight highly distinct visual domains and an open-domain retrieval pool of over one million images. Our work highlights the potential of episodic few-shot adaptation to enhance robustness in the critical and understudied task of open-domain text-to-image retrieval. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒæ£€ç´¢æ˜¯ç®¡ç†å¤šæ ·è§†è§‰å†…å®¹çš„å…³é”®ä»»åŠ¡ï¼Œä½†å½“å‰å¸¸ç”¨çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºå°è§„æ¨¡ã€å•ä¸€é¢†åŸŸçš„æ•°æ®é›†ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç®€å•çš„è´Ÿæ ·æœ¬æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†è§†è§‰ç›¸ä¼¼ä½†é”™è¯¯çš„å›¾åƒæ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾åŸŸåœºæ™¯ä¸­ï¼Œå¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œç‰‡æ®µå¼å°æ ·æœ¬é€‚åº”â€ï¼ˆEFSAï¼‰è¿™ä¸€æ–°å‹æµ‹è¯•æ—¶é—´æ¡†æ¶ã€‚å®ƒé€šè¿‡å¾®è°ƒå‰kä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å¯¹è±¡å’Œä¸ºå…¶ç”Ÿæˆçš„åˆæˆå­—å¹•ï¼ŒåŠ¨æ€é€‚åº”æŸ¥è¯¢é¢†åŸŸä¸­çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚EFSAåœ¨ä¿æŒæ³›åŒ–èƒ½åŠ›çš„æƒ…å†µä¸‹æé«˜äº†è·¨ä¸åŒé¢†åŸŸçš„æ€§èƒ½ï¼Œæ­£å¦‚åœ¨æ¥è‡ªå…«ä¸ªé«˜åº¦ä¸åŒè§†è§‰é¢†åŸŸçš„æŸ¥è¯¢å’Œè¶…è¿‡ä¸€ç™¾ä¸‡å¼ å›¾åƒçš„å¼€æ”¾åŸŸæ£€ç´¢æ± ä¸­çš„è¯„ä¼°æ‰€ç¤ºã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†ç‰‡æ®µå¼å°æ ·æœ¬é€‚åº”åœ¨å…³é”®ä¸”å°šæœªå……åˆ†ç ”ç©¶çš„å¼€æ”¾åŸŸæ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­æé«˜ç¨³å¥æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00139v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­å¯¹äºç®€å•çš„è´Ÿæ ·æœ¬è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é¢å¯¹è§†è§‰ç›¸ä¼¼ä½†é”™è¯¯çš„å›¾åƒæ—¶ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾åŸŸåœºæ™¯ä¸­ï¼Œä¼šå‡ºç°æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´æ¡†æ¶â€”â€”Episodic Few-Shot Adaptationï¼ˆEFSAï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢çš„åŠ¨æ€åŸŸå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡ä¼˜åŒ–é¡¶ç«¯kä¸ªæ£€ç´¢å€™é€‰å’Œä¸ºå…¶ç”Ÿæˆçš„åˆæˆå­—å¹•æ¥å®ç°ã€‚EFSAåœ¨å¤šç§åŸŸçš„æŸ¥è¯¢è¯„ä¼°å’Œè¶…è¿‡ä¸€ç™¾ä¸‡å›¾åƒçš„å¼€æ”¾åŸŸæ£€ç´¢æ± ä¸­å±•ç°äº†å…¶æé«˜æ€§èƒ½çš„åŒæ—¶ä¿æŒæ³›åŒ–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œçªæ˜¾äº†é˜¶æ®µæ€§å°‘æ ·æœ¬é€‚åº”åœ¨å…³é”®çš„ä½†å°šæœªå……åˆ†ç ”ç©¶çš„å¼€æ”¾åŸŸæ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ£€ç´¢æ˜¯ç®¡ç†å¤šæ ·è§†è§‰å†…å®¹çš„å…³é”®ä»»åŠ¡ï¼Œä½†ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºå°è§„æ¨¡çš„å•ä¸€é¢†åŸŸæ•°æ®é›†ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
<li>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨é¢ä¸´è§†è§‰ä¸Šç›¸ä¼¼ä½†é”™è¯¯çš„å›¾åƒæ—¶ï¼ˆå³ç¡¬è´Ÿæ ·æœ¬ï¼‰ä¼šé‡åˆ°æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾åŸŸåœºæ™¯ä¸­ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´æ¡†æ¶â€”â€”Episodic Few-Shot Adaptationï¼ˆEFSAï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€é€‚åº”æŸ¥è¯¢çš„åŸŸï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>EFSAé€šè¿‡ä¼˜åŒ–é¡¶ç«¯kä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å›¾åƒå’Œä¸ºå…¶ç”Ÿæˆçš„åˆæˆå­—å¹•æ¥å®ç°å…¶é€‚åº”æ€§ã€‚</li>
<li>EFSAåœ¨å¤šä¸ªä¸åŒåŸŸçš„æŸ¥è¯¢è¯„ä¼°å’Œå¼€æ”¾åŸŸæ£€ç´¢æ± ä¸­å±•ç¤ºäº†å…¶æé«˜æ€§èƒ½çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ³›åŒ–æ€§ã€‚</li>
<li>EFSAçš„å¼•å…¥çªæ˜¾äº†é˜¶æ®µæ€§å°‘æ ·æœ¬é€‚åº”åœ¨å¼€æ”¾åŸŸæ–‡æœ¬è½¬å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fd363f8e516b905076e4f4f4e3842b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a41b983f1a0732ba6a53e459beb5a122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d6d79a4631dd957fbde9234b14e258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adbb00af663a85ae2eebfcd7307b7810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-356d223843640a647c13536171602e29.jpg" align="middle">
</details>




<h2 id="ReverseNER-A-Self-Generated-Example-Driven-Framework-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models"><a href="#ReverseNER-A-Self-Generated-Example-Driven-Framework-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models" class="headerlink" title="ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot   Named Entity Recognition with Large Language Models"></a>ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot   Named Entity Recognition with Large Language Models</h2><p><strong>Authors:Anbang Wang, Difei Mei, Zhichao Zhang, Xiuxiu Bai, Ran Yao, Zewen Fang, Min Hu, Zhirui Cao, Haitao Sun, Yifeng Guo, Hongyao Zhou, Yu Guo</strong></p>
<p>This paper presents ReverseNER, a framework aimed at overcoming the limitations of large language models (LLMs) in zero-shot Named Entity Recognition (NER) tasks, particularly in cases where certain entity types have ambiguous boundaries. ReverseNER tackles this challenge by constructing a reliable example library with the reversed process of NER. Rather than beginning with sentences, this method uses an LLM to generate entities based on their definitions and then expands them into full sentences. During sentence generation, the LLM is guided to replicate the structure of a specific â€˜feature sentenceâ€™, extracted from the task sentences by clustering. This results in well-annotated sentences with clearly labeled entities, while preserving semantic and structural similarity to the task sentences. Once the example library is constructed, the method selects the most semantically similar example labels for each task sentence to support the LLMâ€™s inference. We also propose an entity-level self-consistency scoring mechanism to improve NER performance with LLMs. Experiments show that ReverseNER significantly outperforms traditional zero-shot NER with LLMs and surpasses several few-shot methods, marking a notable improvement in NER for domains with limited labeled data. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ReverseNERæ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›å®ä½“ç±»å‹è¾¹ç•Œæ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚ReverseNERé€šè¿‡æ„å»ºå¯é çš„ç¤ºä¾‹åº“æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥åº“é‡‡ç”¨NERçš„é€†å‘è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸æ˜¯ä»å¥å­å¼€å§‹ï¼Œè€Œæ˜¯ä½¿ç”¨LLMæ ¹æ®å®ä½“å®šä¹‰ç”Ÿæˆå®ä½“ï¼Œç„¶åå°†å…¶æ‰©å±•åˆ°å®Œæ•´å¥å­ã€‚åœ¨ç”Ÿæˆå¥å­æ—¶ï¼ŒLLMè¢«å¼•å¯¼ä»¥ç‰¹å®šçš„â€œç‰¹å¾å¥å­â€ç»“æ„è¿›è¡Œå¤åˆ¶ï¼Œç‰¹å¾å¥å­æ˜¯ä»ä»»åŠ¡å¥å­ä¸­æå–å¹¶é€šè¿‡èšç±»è·å¾—ã€‚è¿™ä¼šäº§ç”Ÿå¸¦æœ‰æ˜ç¡®æ ‡æ³¨å®ä½“çš„è‰¯å¥½æ³¨é‡Šå¥å­ï¼ŒåŒæ—¶ä¿ç•™ä¸ä»»åŠ¡å¥å­çš„è¯­ä¹‰å’Œç»“æ„ç›¸ä¼¼æ€§ã€‚ä¸€æ—¦æ„å»ºäº†ç¤ºä¾‹åº“ï¼Œè¯¥æ–¹æ³•å°±ä¼šé€‰æ‹©æ¯ä¸ªä»»åŠ¡å¥å­ä¸­è¯­ä¹‰ä¸Šæœ€ç›¸ä¼¼çš„ç¤ºä¾‹æ ‡ç­¾æ¥æ”¯æŒLLMçš„æ¨æ–­ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å®ä½“çº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„åˆ†æœºåˆ¶ï¼Œä»¥æé«˜LLMçš„NERæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„é›¶æ ·æœ¬NERç›¸æ¯”ï¼ŒReverseNERæ˜¾è‘—æé«˜äº†LLMçš„æ€§èƒ½ï¼Œå¹¶è¶…è¶Šäº†å¤šç§å°æ ·æœ¬æ–¹æ³•ï¼Œåœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„é¢†åŸŸï¼ŒNERæœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00533v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨é›¶æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä½“ç±»å‹è¾¹ç•Œæ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºçš„ReverseNERæ¡†æ¶é€šè¿‡æ„å»ºå¯é çš„ç¤ºä¾‹åº“æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨åå‘è¿‡ç¨‹ç”Ÿæˆå®ä½“å¹¶æ‰©å±•ä¸ºå®Œæ•´å¥å­ã€‚ReverseNERé‡‡ç”¨åŸºäºå®šä¹‰ç”Ÿæˆå®ä½“çš„æ–¹æ³•ï¼Œå¹¶æŒ‡å¯¼è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¥å­æ—¶å¤åˆ¶ç‰¹å®šç‰¹å¾å¥å­çš„ç»“æ„ã€‚æ­¤æ–¹æ³•è¿˜æå‡ºäº†ä¸€ç§å®ä½“çº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„åˆ†æœºåˆ¶ï¼Œä»¥æé«˜LLMçš„NERæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒReverseNERæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é›¶æ ·æœ¬NERå’Œå‡ ç§å°æ ·æœ¬æ–‡æœ¬æ–¹æ³•ï¼Œåœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReverseNERæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­çš„å±€é™æ€§è€Œè®¾è®¡çš„æ¡†æ¶ã€‚</li>
<li>ReverseNERé€šè¿‡æ„å»ºå¯é çš„ç¤ºä¾‹åº“æ¥è§£å†³å®ä½“ç±»å‹è¾¹ç•Œæ¨¡ç³Šçš„é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨åå‘è¿‡ç¨‹ç”Ÿæˆå®ä½“å¹¶æ‰©å±•ä¸ºå¥å­ï¼ŒåŸºäºå®šä¹‰ç”Ÿæˆå®ä½“ã€‚</li>
<li>ReverseNERæŒ‡å¯¼è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¥å­æ—¶å¤åˆ¶ç‰¹å®šç‰¹å¾å¥å­çš„ç»“æ„ã€‚</li>
<li>ReverseNERæå‡ºäº†ä¸€ç§å®ä½“çº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„åˆ†æœºåˆ¶ï¼Œä»¥æé«˜LLMçš„NERæ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReverseNERåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„é¢†åŸŸã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16a422ad5a61465fbb0fc09ae63b6521.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4062550094aa078f557d3d317e4952a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08fbee1dc7d3c218ac56a4097603fea5.jpg" align="middle">
</details>




<h2 id="Probabilistic-Language-Image-Pre-Training"><a href="#Probabilistic-Language-Image-Pre-Training" class="headerlink" title="Probabilistic Language-Image Pre-Training"></a>Probabilistic Language-Image Pre-Training</h2><p><strong>Authors:Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun</strong></p>
<p>Vision-language models (VLMs) embed aligned image-text pairs into a joint space but often rely on deterministic embeddings, assuming a one-to-one correspondence between images and texts. This oversimplifies real-world relationships, which are inherently many-to-many, with multiple captions describing a single image and vice versa. We introduce Probabilistic Language-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained on a billion-scale image-text dataset using only probabilistic objectives, achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot accuracy with ViT-B&#x2F;16). ProLIP efficiently estimates uncertainty by an â€œuncertainty tokenâ€ without extra parameters. We also introduce a novel inclusion loss that enforces distributional inclusion relationships between image-text pairs and between original and masked inputs. Experiments demonstrate that, by leveraging uncertainty estimates, ProLIP benefits downstream tasks and aligns with intuitive notions of uncertainty, e.g., shorter texts being more uncertain and more general inputs including specific ones. Utilizing text uncertainties, we further improve ImageNet accuracy from 74.6% to 75.8% (under a few-shot setting), supporting the practical advantages of our probabilistic approach. The code is available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†å›¾åƒæ–‡æœ¬å¯¹åµŒå…¥åˆ°è”åˆç©ºé—´ä¸­ï¼Œä½†é€šå¸¸ä¾èµ–äºç¡®å®šæ€§åµŒå…¥ï¼Œå‡è®¾å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´å­˜åœ¨ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ã€‚è¿™ç®€åŒ–äº†çœŸå®ä¸–ç•Œä¸­çš„å…³ç³»ï¼ŒçœŸå®ä¸–ç•Œä¸­çš„å…³ç³»æ˜¯å›ºæœ‰çš„å¤šå¯¹å¤šå…³ç³»ï¼Œä¸€ä¸ªå›¾åƒå¯ä»¥ç”±å¤šä¸ªå­—å¹•æè¿°ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¦‚ç‡è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆProLIPï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªä½¿ç”¨æ¦‚ç‡ç›®æ ‡åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ¦‚ç‡VLMï¼Œå®ƒå…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ViT-B&#x2F;16çš„ImageNeté›¶æ ·æœ¬å‡†ç¡®ç‡ä¸º74.6%ï¼‰ã€‚ProLIPé€šè¿‡ä¸€ä¸ªâ€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€æœ‰æ•ˆåœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹åŒ…å«æŸå¤±ï¼Œå®ƒå¼ºåˆ¶å®æ–½å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´ä»¥åŠåŸå§‹è¾“å…¥å’Œæ©ç è¾“å…¥ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸ç›´è§‚çš„ä¸ç¡®å®šæ€§æ¦‚å¿µç›¸ç¬¦ï¼Œä¾‹å¦‚è¾ƒçŸ­çš„æ–‡æœ¬å…·æœ‰æ›´é«˜çš„ä¸ç¡®å®šæ€§ï¼ŒåŒ…å«ç‰¹å®šå†…å®¹çš„æ›´é€šç”¨è¾“å…¥ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åˆ©ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†ImageNetçš„å‡†ç¡®ç‡ï¼Œä»74.6%æé«˜åˆ°75.8%ï¼ˆåœ¨å°æ ·æœ¬è®¾ç½®ä¸‹ï¼‰ï¼Œè¿™æ”¯æŒäº†æˆ‘ä»¬æ¦‚ç‡æ–¹æ³•çš„å®é™…ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18857v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> HuggingFace Hub:   <a target="_blank" rel="noopener" href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291</a>   31 pages, 4.29 MB</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Probabilistic Language-Image Pre-trainingï¼ˆProLIPï¼‰æ¨¡å‹ï¼Œå®ƒæ˜¯é¦–ä¸ªåŸºäºæ¦‚ç‡çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œä½¿ç”¨æ¦‚ç‡ç›®æ ‡å‡½æ•°ï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¦‚ä»¥ViT-B&#x2F;16æ¨¡å‹è¾¾åˆ°74.6%çš„ImageNeté›¶æ ·æœ¬å‡†ç¡®ç‡ã€‚ProLIPé€šè¿‡â€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€æœ‰æ•ˆä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ— éœ€é¢å¤–å‚æ•°ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„åŒ…å«æŸå¤±ï¼Œå¼ºåˆ¶å®æ–½å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ä»¥åŠåŸå§‹å’Œå±è”½è¾“å…¥ä¹‹é—´çš„åŒ…å«å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸ä¸ç¡®å®šæ€§ç›´è§‚æ¦‚å¿µç›¸ç¬¦ï¼Œå¦‚è¾ƒçŸ­çš„æ–‡æœ¬å…·æœ‰æ›´é«˜çš„ä¸ç¡®å®šæ€§ï¼Œæ›´é€šç”¨çš„è¾“å…¥åŒ…æ‹¬ç‰¹å®šçš„è¾“å…¥ã€‚åˆ©ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼Œè¿›ä¸€æ­¥å°†ImageNetå‡†ç¡®ç‡ä»74.6%æé«˜åˆ°75.8%ï¼Œä½“ç°äº†æ¦‚ç‡æ–¹æ³•çš„å®é™…ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProLIPæ˜¯é¦–ä¸ªåŸºäºæ¦‚ç‡çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLMï¼‰ï¼Œåœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨æ¦‚ç‡ç›®æ ‡å‡½æ•°ï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¦‚ImageNeté›¶æ ·æœ¬å‡†ç¡®ç‡é«˜è¾¾74.6%ã€‚</li>
<li>ProLIPé€šè¿‡â€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ— éœ€é¢å¤–å‚æ•°ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŒ…å«æŸå¤±ï¼Œç”¨äºå¼ºåŒ–å›¾åƒæ–‡æœ¬å¯¹åŠåŸå§‹å’Œå±è”½è¾“å…¥ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œç¬¦åˆå…³äºä¸ç¡®å®šæ€§çš„ç›´è§‚ç†è§£ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼Œè¿›ä¸€æ­¥æå‡äº†ImageNetçš„å‡†ç¡®ç‡ã€‚</li>
<li>ProLIPçš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-538fc994372ff98273c839a40f7af5cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27f0a4c409f2d43eb7d7ed60ff4c7a01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6064d03104a4549ecd34104864a98776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-047b7b5a14b02d96c9a1740dcfa61ffe.jpg" align="middle">
</details>




<h2 id="Fine-Tuning-CLIPâ€™s-Last-Visual-Projector-A-Few-Shot-Cornucopia"><a href="#Fine-Tuning-CLIPâ€™s-Last-Visual-Projector-A-Few-Shot-Cornucopia" class="headerlink" title="Fine-Tuning CLIPâ€™s Last Visual Projector: A Few-Shot Cornucopia"></a>Fine-Tuning CLIPâ€™s Last Visual Projector: A Few-Shot Cornucopia</h2><p><strong>Authors:Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick PÃ©rez, Raoul de Charette</strong></p>
<p>We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. This paper introduces an alternative way for CLIP adaptation without adding â€˜externalâ€™ parameters to optimize. We find that simply fine-tuning the last projection matrix of the vision encoder leads to performance better than all baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP. This simple approach, coined ProLIP, yields state-of-the-art performance on 11 few-shot classification benchmarks, few-shot domain generalization, cross-dataset transfer, base-to-new class generalization, and test-time adaptation. Code will be made available at: <a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP">https://github.com/astra-vision/ProLIP</a> . </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘é€‚åº”å¯¹æ¯”é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼ŒRadfordç­‰äººï¼Œ2021å¹´ï¼‰è¿›è¡Œå°æ ·æœ¬åˆ†ç±»çš„é—®é¢˜ã€‚æ–‡çŒ®é€šè¿‡å­¦ä¹ å†»ç»“è§†è§‰ç‰¹å¾çš„çº¿æ€§åˆ†ç±»å™¨ã€ä¼˜åŒ–è¯åµŒå…¥æˆ–å­¦ä¹ å¤–éƒ¨ç‰¹å¾é€‚é…å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä¸å¢åŠ ä¼˜åŒ–â€œå¤–éƒ¨â€å‚æ•°çš„CLIPè‡ªé€‚åº”æ›¿ä»£æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä»…å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æœ€åä¸€ä¸ªæŠ•å½±çŸ©é˜µçš„æ€§èƒ½è¶…è¿‡äº†æ‰€æœ‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œç”¨å¾®è°ƒåçš„çŸ©é˜µå’Œé¢„è®­ç»ƒçŸ©é˜µä¹‹é—´çš„è·ç¦»å¯¹è®­ç»ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¯ä»¥å¢åŠ CLIPçš„é€‚åº”æ€§ã€‚è¿™ç§ç®€å•çš„æ–¹æ³•è¢«ç§°ä¸ºProLIPï¼Œåœ¨11ä¸ªå°æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ã€å°æ ·æœ¬åŸŸæ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€åŸºç¡€åˆ°æ–°ç±»åˆ«æ³›åŒ–å’Œæµ‹è¯•æ—¶é—´é€‚åº”ç­‰æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/astra-vision/ProLIPä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05270v2">PDF</a> </p>
<p><strong>Summary</strong><br>é€‚åº”CLIPæ¨¡å‹çš„å°‘æ•°é•œå¤´åˆ†ç±»é—®é¢˜å¾—åˆ°äº†å…³æ³¨ï¼Œå¸¸è§çš„ç­–ç•¥æ˜¯æ·»åŠ å¤–éƒ¨å‚æ•°æˆ–ä¼˜åŒ–å•è¯åµŒå…¥æ¥å­¦ä¹ çº¿æ€§åˆ†ç±»å™¨æˆ–å¤–éƒ¨ç‰¹å¾é€‚é…å™¨ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ä»…å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æœ€åä¸€ä¸ªæŠ•å½±çŸ©é˜µï¼Œè¿™ç§æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ­£åˆ™åŒ–è®­ç»ƒå¯ä»¥ä½¿å¾®è°ƒåçš„çŸ©é˜µä¸é¢„è®­ç»ƒçŸ©é˜µä¹‹é—´çš„è·ç¦»å¢åŠ ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”CLIPæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºProLIPï¼Œå®ƒåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ç§é’ˆå¯¹CLIPæ¨¡å‹çš„å°‘æ•°é•œå¤´åˆ†ç±»é—®é¢˜æ–°çš„è§£å†³ç­–ç•¥ã€‚è¯¥ç­–ç•¥æ— éœ€æ·»åŠ é¢å¤–çš„å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æœ€åä¸€ä¸ªæŠ•å½±çŸ©é˜µæ¥å®ç°ã€‚è¿™ç§æ–¹æ³•çš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b64c2ce5d15c1e023d2b330f9df85e44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf4d3533200d22118db1342a15db2e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-022910213b360a9d807dd7486a1e8cf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e605cd4bbcd7fe02f0cd5a6d0a36707.jpg" align="middle">
</details>




<h2 id="Acquiring-Bidirectionality-via-Large-and-Small-Language-Models"><a href="#Acquiring-Bidirectionality-via-Large-and-Small-Language-Models" class="headerlink" title="Acquiring Bidirectionality via Large and Small Language Models"></a>Acquiring Bidirectionality via Large and Small Language Models</h2><p><strong>Authors:Takumi Goto, Hiroyoshi Nagao, Yuta Koreeda</strong></p>
<p>Using token representation from bidirectional language models (LMs) such as BERT is still a widely used approach for token-classification tasks. Even though there exist much larger unidirectional LMs such as Llama-2, they are rarely used to replace the token representation of bidirectional LMs. In this work, we hypothesize that their lack of bidirectionality is keeping them behind. To that end, we propose to newly train a small backward LM and concatenate its representations to those of existing LM for downstream tasks. Through experiments in named entity recognition, we demonstrate that introducing backward model improves the benchmark performance more than 10 points. Furthermore, we show that the proposed method is especially effective for rare domains and in few-shot learning settings. </p>
<blockquote>
<p>ä½¿ç”¨æ¥è‡ªåŒå‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä»¤ç‰Œè¡¨ç¤ºï¼Œä»ç„¶æ˜¯ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡çš„å¹¿æ³›ä½¿ç”¨æ–¹æ³•ã€‚å°½ç®¡å­˜åœ¨æ›´å¤§çš„å•å‘è¯­è¨€æ¨¡å‹ï¼Œå¦‚Llama-2ï¼Œä½†å®ƒä»¬å¾ˆå°‘è¢«ç”¨æ¥æ›¿ä»£åŒå‘è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œè¡¨ç¤ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‡è®¾å®ƒä»¬çš„åŒå‘æ€§ç¼ºå¤±æ˜¯é˜»ç¢å®ƒä»¬åº”ç”¨çš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ–°è®­ç»ƒä¸€ä¸ªå°å‹çš„åå‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ç°æœ‰è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºè¿›è¡Œæ‹¼æ¥ï¼Œä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚é€šè¿‡å‘½åå®ä½“è¯†åˆ«çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†å¼•å…¥åå‘æ¨¡å‹å¯ä»¥æé«˜åŸºå‡†æ€§èƒ½è¶…è¿‡1eç‚¹ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯¹äºç½•è§é¢†åŸŸå’Œå°æ ·æœ¬å­¦ä¹ è®¾ç½®å°¤å…¶æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09640v2">PDF</a> Accepted by COLING2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨åŒå‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä»¤ç‰Œè¡¨ç¤ºç”¨äºä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ã€‚å°½ç®¡å­˜åœ¨æ›´å¤§çš„å•å‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama-2ï¼‰ï¼Œä½†å®ƒä»¬å¾ˆå°‘å–ä»£åŒå‘è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œè¡¨ç¤ºã€‚æœ¬ç ”ç©¶å‡è®¾å•å‘æ€§æ˜¯å…¶è½ååŸå› ä¹‹ä¸€ï¼Œå› æ­¤æå‡ºé‡æ–°è®­ç»ƒå°å‹åå‘è¯­è¨€æ¨¡å‹å¹¶å°†å…¶è¡¨ç¤ºä¸ç°æœ‰è¯­è¨€æ¨¡å‹ç»“åˆç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨å‘½åå®ä½“è¯†åˆ«å®éªŒä¸­å‘ç°ï¼Œå¼•å…¥åå‘æ¨¡å‹å°†åŸºå‡†æ€§èƒ½æé«˜äº†è¶…è¿‡10ä¸ªç‚¹ï¼Œå°¤å…¶åœ¨ç½•è§é¢†åŸŸå’Œå°‘æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­æ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒå‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä»¤ç‰Œè¡¨ç¤ºåœ¨ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ä¸­ä»ç„¶å¹¿æ³›ä½¿ç”¨ã€‚</li>
<li>å°½ç®¡å­˜åœ¨æ›´å¤§çš„å•å‘è¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒä»¬å¾ˆå°‘å–ä»£åŒå‘è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œè¡¨ç¤ºã€‚</li>
<li>æœ¬ç ”ç©¶å‡è®¾å•å‘æ€§æ˜¯é™åˆ¶å…¶æ€§èƒ½çš„åŸå› ä¹‹ä¸€ã€‚</li>
<li>æå‡ºé€šè¿‡é‡æ–°è®­ç»ƒå°å‹åå‘è¯­è¨€æ¨¡å‹å¹¶ä¸ç°æœ‰è¯­è¨€æ¨¡å‹ç»“åˆæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨å‘½åå®ä½“è¯†åˆ«å®éªŒä¸­ï¼Œå¼•å…¥åå‘æ¨¡å‹æ˜¾è‘—æé«˜äº†åŸºå‡†æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºç½•è§é¢†åŸŸç‰¹åˆ«æœ‰æ•ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf4caa6472d10144db09fb5fd447cb66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c249f7c2e3e376b7f5aefedb6d65578f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg" align="middle">
</details>




<h2 id="Adaptable-and-Reliable-Text-Classification-using-Large-Language-Models"><a href="#Adaptable-and-Reliable-Text-Classification-using-Large-Language-Models" class="headerlink" title="Adaptable and Reliable Text Classification using Large Language Models"></a>Adaptable and Reliable Text Classification using Large Language Models</h2><p><strong>Authors:Zhiqiang Wang, Yiran Pang, Yanbin Lin, Xingquan Zhu</strong></p>
<p>Text classification is fundamental in Natural Language Processing (NLP), and the advent of Large Language Models (LLMs) has revolutionized the field. This paper introduces an adaptable and reliable text classification paradigm, which leverages LLMs as the core component to address text classification tasks. Our system simplifies the traditional text classification workflows, reducing the need for extensive preprocessing and domain-specific expertise to deliver adaptable and reliable text classification results. We evaluated the performance of several LLMs, machine learning algorithms, and neural network-based architectures on four diverse datasets. Results demonstrate that certain LLMs surpass traditional methods in sentiment analysis, spam SMS detection, and multi-label classification. Furthermore, it is shown that the systemâ€™s performance can be further enhanced through few-shot or fine-tuning strategies, making the fine-tuned model the top performer across all datasets. Source code and datasets are available in this GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/yeyimilk/llm-zero-shot-classifiers">https://github.com/yeyimilk/llm-zero-shot-classifiers</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€é¡¹åŸºç¡€å·¥ä½œï¼Œè€Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè¯¥é¢†åŸŸå·²ç»å‘ç”Ÿäº†é©å‘½æ€§çš„å˜åŒ–ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§çµæ´»å¯é çš„æ–‡æœ¬åˆ†ç±»èŒƒå¼ï¼Œè¯¥èŒƒå¼ä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ ¸å¿ƒç»„ä»¶æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç®€åŒ–äº†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ†ç±»å·¥ä½œæµç¨‹ï¼Œå‡å°‘äº†å¤§é‡é¢„å¤„ç†å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†éœ€æ±‚ï¼Œä»¥æä¾›çµæ´»å¯é çš„æ–‡æœ¬åˆ†ç±»ç»“æœã€‚æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†å‡ ç§å¤§å‹è¯­è¨€æ¨¡å‹ã€æœºå™¨å­¦ä¹ ç®—æ³•å’ŒåŸºäºç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒæŸäº›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æã€åƒåœ¾çŸ­ä¿¡æ£€æµ‹å’Œå¤šåª’ä½“åˆ†ç±»æ–¹é¢çš„è¡¨ç°è¶…è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜æ˜¾ç¤ºå¯ä»¥é€šè¿‡å°‘é‡æ ·æœ¬æˆ–å¾®è°ƒç­–ç•¥è¿›ä¸€æ­¥æé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œä½¿å¾®è°ƒæ¨¡å‹æˆä¸ºæ‰€æœ‰æ•°æ®é›†ä¸Šçš„æœ€ä½³è¡¨ç°è€…ã€‚æºä»£ç å’Œæ•°æ®é›†å¯åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yeyimilk/llm-zero-shot-classifiers%E3%80%82">https://github.com/yeyimilk/llm-zero-shot-classifiersã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10523v3">PDF</a> ICDM Workshop ARRL 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†ä¼ ç»Ÿæ–‡æœ¬åˆ†ç±»æµç¨‹ï¼Œå‡å°‘äº†é¢„å¤„ç†çš„éœ€æ±‚ï¼Œå¹¶èƒ½é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒæŸäº›LLMåœ¨æƒ…æ„Ÿåˆ†æã€åƒåœ¾çŸ­ä¿¡æ£€æµ‹å’Œå¤šå…ƒæ ‡ç­¾åˆ†ç±»æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡å¾®è°ƒç­–ç•¥ï¼Œæ¨¡å‹æ€§èƒ½å¯è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºæ–‡æœ¬åˆ†ç±»é¢†åŸŸä¸­çš„æ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ–‡æœ¬åˆ†ç±»èŒƒå¼ï¼Œç®€åŒ–äº†æµç¨‹å¹¶å¢å¼ºäº†é€‚åº”æ€§ã€‚</li>
<li>ç ”ç©¶äº†LLMsåœ¨æƒ…æ„Ÿåˆ†æã€åƒåœ¾çŸ­ä¿¡æ£€æµ‹å’Œå¤šå…ƒæ ‡ç­¾åˆ†ç±»ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>LLMsåœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œæ–¹æ³•ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å¯é€šè¿‡å¾®è°ƒç­–ç•¥è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå¼€æºGitHubä»“åº“ç”¨äºåˆ†äº«æºä»£ç å’Œæ•°æ®é›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6280926f748abbcad6ee215ad2ae192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50bae6e3ef584105719b6db9f4a30fd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc39943db50b4fad6e3c6d5a9d9ceb28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bffdb666300130ea86abb5409a7c84b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be4457da93f03603ab4cbf93bbc828b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f331e50dc2dc48bddea28c756fced3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84eb2bdea990b8edfa354c8c1aeed3d9.jpg" align="middle">
</details>




<h2 id="Rho-1-Not-All-Tokens-Are-What-You-Need"><a href="#Rho-1-Not-All-Tokens-Are-What-You-Need" class="headerlink" title="Rho-1: Not All Tokens Are What You Need"></a>Rho-1: Not All Tokens Are What You Need</h2><p><strong>Authors:Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen</strong></p>
<p>Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that â€˜â€™9l trainingâ€™â€™. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training. </p>
<blockquote>
<p>ä¹‹å‰çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•éƒ½æ˜¯å°†æ‰€æœ‰è®­ç»ƒä»¤ç‰Œç»Ÿä¸€åº”ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„é¢„æµ‹æŸå¤±ã€‚æˆ‘ä»¬æŒ‘æˆ˜è¿™ä¸€å¸¸è§„ï¼Œå¹¶æå‡ºäº†â€œÏlè®­ç»ƒâ€ã€‚æˆ‘ä»¬çš„åˆæ­¥åˆ†æç ”ç©¶äº†è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œçº§è®­ç»ƒåŠ¨æ€ï¼Œå‘ç°ä¸åŒä»¤ç‰Œå­˜åœ¨ä¸åŒçš„æŸå¤±æ¨¡å¼ã€‚åˆ©ç”¨è¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºRho-1çš„æ–°è¯­è¨€æ¨¡å‹ã€‚ä¸åŒäºä¼ ç»Ÿçš„å­¦ä¹ é¢„æµ‹è¯­æ–™åº“ä¸­æ¯ä¸ªä¸‹ä¸€ä¸ªä»¤ç‰Œçš„LMsï¼ŒRho-1é‡‡ç”¨é€‰æ‹©æ€§è¯­è¨€å»ºæ¨¡ï¼ˆSelective Language Modelingï¼ŒSLMï¼‰ï¼Œè¯¥å»ºæ¨¡æ–¹æ³•é€‰æ‹©æ€§åœ°è®­ç»ƒæœ‰ç”¨çš„ä»¤ç‰Œä¸æ‰€éœ€çš„åˆ†å¸ƒå¯¹é½ã€‚è¿™ç§æ–¹æ³•æ¶‰åŠä½¿ç”¨å‚è€ƒæ¨¡å‹å¯¹é¢„è®­ç»ƒä»¤ç‰Œè¿›è¡Œè¯„åˆ†ï¼Œç„¶åä½¿ç”¨é›†ä¸­æŸå¤±å¯¹å…·æœ‰è¾ƒé«˜å¾—åˆ†çš„ä»¤ç‰Œè¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚åœ¨æŒç»­ä½¿ç”¨15B OpenWebMathè¯­æ–™åº“è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼ŒRho-1åœ¨9ä¸ªæ•°å­¦ä»»åŠ¡ä¸­çš„å°‘æ ·æœ¬å‡†ç¡®åº¦æé«˜äº†é«˜è¾¾30%ã€‚ç»è¿‡å¾®è°ƒåï¼ŒRho-1-1Bå’Œ7Båœ¨MATHæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ°´å¹³çš„ç»“æœï¼Œåˆ†åˆ«ä¸º40.6%å’Œ51.8%ï¼Œç›¸è¾ƒäºDeepSeekMathåªä½¿ç”¨äº†å…¶3%çš„é¢„è®­ç»ƒä»¤ç‰Œå³å¯ä¸ä¹‹åŒ¹æ•Œã€‚æ­¤å¤–ï¼Œå½“æŒç»­å¯¹ä¸€èˆ¬ä»¤ç‰Œè¿›è¡Œé¢„è®­ç»ƒè¾¾åˆ°80Bæ—¶ï¼ŒRho-1åœ¨æ¶µç›–ä¸åŒé¢†åŸŸçš„15é¡¹ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†6.8%ï¼Œæé«˜äº†è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.07965v3">PDF</a> First two authors equal contribution</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‘æˆ˜äº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹Rho-1ã€‚Rho-1é‡‡ç”¨é€‰æ‹©æ€§è¯­è¨€å»ºæ¨¡ï¼ˆSLMï¼‰çš„æ–¹å¼ï¼Œä¸åŒäºä¼ ç»Ÿè¯­è¨€æ¨¡å‹å¯¹æ¯ä¸€ä¸ªåç»­è¯è¿›è¡Œé¢„æµ‹çš„è®­ç»ƒæ–¹æ³•ï¼Œè€Œæ˜¯æ ¹æ®æœŸæœ›çš„è¯æ±‡åˆ†å¸ƒï¼Œä»…é’ˆå¯¹æœ‰ä»·å€¼çš„è¯æ±‡è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸€ç­–ç•¥ä½¿æ¨¡å‹èƒ½åœ¨ä»…æœ‰å°‘é‡é¢„è®­ç»ƒè¯æ±‡çš„æƒ…å†µä¸‹è¾¾åˆ°è¾ƒé«˜çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå±•ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rho-1å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•â€”â€”é€‰æ‹©æ€§è¯­è¨€å»ºæ¨¡ï¼ˆSLMï¼‰ã€‚</li>
<li>SLMä»…é’ˆå¯¹æœ‰ä»·å€¼çš„è¯æ±‡è¿›è¡Œè®­ç»ƒï¼Œä¸ä¼ ç»Ÿçš„å¯¹æ‰€æœ‰è¯æ±‡è¿›è¡Œé¢„æµ‹çš„æ–¹æ³•ä¸åŒã€‚</li>
<li>Rho-1åœ¨ä»…æœ‰å°‘é‡é¢„è®­ç»ƒè¯æ±‡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½æ°´å¹³ã€‚åœ¨MATHæ•°æ®é›†ä¸Šå–å¾—äº†çªç ´æ€§çš„ç»“æœã€‚</li>
<li>Rho-1åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡ä¸Šæœ‰æ‰€æå‡ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°å¹³å‡å¢å¼ºæ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-553a1738b7852d4b5bfc65951433efe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdbb3b3ca7130ce6ccc8219671e2628e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac3417a06d64e238fb721266623f376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6251b7c2d0a642fd2c586c2ba121eb29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2952de91398a6a427d8c70821e213c0.jpg" align="middle">
</details>




<h2 id="Query-guided-Prototype-Evolution-Network-for-Few-Shot-Segmentation"><a href="#Query-guided-Prototype-Evolution-Network-for-Few-Shot-Segmentation" class="headerlink" title="Query-guided Prototype Evolution Network for Few-Shot Segmentation"></a>Query-guided Prototype Evolution Network for Few-Shot Segmentation</h2><p><strong>Authors:Runmin Cong, Hang Xiong, Jinpeng Chen, Wei Zhang, Qingming Huang, Yao Zhao</strong></p>
<p>Previous Few-Shot Segmentation (FSS) approaches exclusively utilize support features for prototype generation, neglecting the specific requirements of the query. To address this, we present the Query-guided Prototype Evolution Network (QPENet), a new method that integrates query features into the generation process of foreground and background prototypes, thereby yielding customized prototypes attuned to specific queries. The evolution of the foreground prototype is accomplished through a \textit{support-query-support} iterative process involving two new modules: Pseudo-prototype Generation (PPG) and Dual Prototype Evolution (DPE). The PPG module employs support features to create an initial prototype for the preliminary segmentation of the query image, resulting in a pseudo-prototype reflecting the unique needs of the current query. Subsequently, the DPE module performs reverse segmentation on support images using this pseudo-prototype, leading to the generation of evolved prototypes, which can be considered as custom solutions. As for the background prototype, the evolution begins with a global background prototype that represents the generalized features of all training images. We also design a Global Background Cleansing (GBC) module to eliminate potential adverse components mirroring the characteristics of the current foreground class. Experimental results on the PASCAL-$5^i$ and COCO-$20^i$ datasets attest to the substantial enhancements achieved by QPENet over prevailing state-of-the-art techniques, underscoring the validity of our ideas. </p>
<blockquote>
<p>ä¹‹å‰çš„å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•ä»…åˆ©ç”¨æ”¯æŒç‰¹å¾è¿›è¡ŒåŸå‹ç”Ÿæˆï¼Œå¿½ç•¥äº†æŸ¥è¯¢çš„ç‰¹å®šè¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æŸ¥è¯¢å¼•å¯¼åŸå‹æ¼”åŒ–ç½‘ç»œï¼ˆQPENetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†æŸ¥è¯¢ç‰¹å¾é›†æˆåˆ°å‰æ™¯å’ŒèƒŒæ™¯åŸå‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ–°æ–¹æ³•ï¼Œä»è€Œç”Ÿæˆé€‚åº”ç‰¹å®šæŸ¥è¯¢çš„å®šåˆ¶åŸå‹ã€‚å‰æ™¯åŸå‹çš„æ¼”åŒ–æ˜¯é€šè¿‡ä¸€ä¸ªæ¶‰åŠä¸¤ä¸ªæ–°æ¨¡å—çš„\textit{æ”¯æŒ-æŸ¥è¯¢-æ”¯æŒ}è¿­ä»£è¿‡ç¨‹å®Œæˆçš„ï¼šä¼ªåŸå‹ç”Ÿæˆï¼ˆPPGï¼‰å’ŒåŒåŸå‹æ¼”åŒ–ï¼ˆDPEï¼‰ã€‚PPGæ¨¡å—åˆ©ç”¨æ”¯æŒç‰¹å¾ä¸ºæŸ¥è¯¢å›¾åƒçš„åˆæ­¥åˆ†å‰²åˆ›å»ºåˆå§‹åŸå‹ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªåæ˜ å½“å‰æŸ¥è¯¢ç‹¬ç‰¹éœ€æ±‚çš„ä¼ªåŸå‹ã€‚éšåï¼ŒDPEæ¨¡å—ä½¿ç”¨è¿™ä¸ªä¼ªåŸå‹å¯¹æ”¯æŒå›¾åƒè¿›è¡Œåå‘åˆ†å‰²ï¼Œä»è€Œäº§ç”Ÿè¿›åŒ–çš„åŸå‹ï¼Œè¿™äº›åŸå‹å¯ä»¥è¢«è§†ä¸ºå®šåˆ¶è§£å†³æ–¹æ¡ˆã€‚è‡³äºèƒŒæ™¯åŸå‹ï¼Œæ¼”åŒ–å§‹äºä»£è¡¨æ‰€æœ‰è®­ç»ƒå›¾åƒé€šç”¨ç‰¹å¾çš„å…¨å±€èƒŒæ™¯åŸå‹ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå…¨å±€èƒŒæ™¯æ¸…æ´ï¼ˆGBCï¼‰æ¨¡å—ï¼Œä»¥æ¶ˆé™¤å¯èƒ½çš„ä¸è‰¯æˆåˆ†ï¼Œåæ˜ å½“å‰å‰æ™¯ç±»çš„ç‰¹å¾ã€‚åœ¨PASCAL-$5^i$å’ŒCOCO-$20^i$æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒQPENetç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯å–å¾—äº†é‡å¤§æ”¹è¿›ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æƒ³æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06488v2">PDF</a> Accepted by IEEE TMM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŸ¥è¯¢å¼•å¯¼çš„åŸå‹æ¼”åŒ–ç½‘ç»œï¼ˆQPENetï¼‰æ¥è§£å†³å°æ ·åˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æŸ¥è¯¢ç‰¹å¾ï¼Œä¼˜åŒ–äº†å‰æ™¯å’ŒèƒŒæ™¯åŸå‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆé’ˆå¯¹ç‰¹å®šæŸ¥è¯¢å®šåˆ¶åŒ–çš„åŸå‹ã€‚é€šè¿‡æ”¯æŒç‰¹å¾ç”Ÿæˆä¼ªåŸå‹ï¼Œå†é€šè¿‡åå‘åˆ†å‰²ç”Ÿæˆæ¼”åŒ–åçš„åŸå‹ï¼Œå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QPENetè§£å†³äº†ç°æœ‰å°æ ·åˆ†å‰²æ–¹æ³•å¿½ç•¥æŸ¥è¯¢ç‰¹å®šéœ€æ±‚çš„é—®é¢˜ï¼Œé€šè¿‡é›†æˆæŸ¥è¯¢ç‰¹å¾æ¥ä¼˜åŒ–å‰æ™¯å’ŒèƒŒæ™¯åŸå‹çš„ç”Ÿæˆã€‚</li>
<li>QPENeté‡‡ç”¨æ”¯æŒ-æŸ¥è¯¢-æ”¯æŒçš„è¿­ä»£è¿‡ç¨‹ï¼Œé€šè¿‡Pseudo-prototype Generationï¼ˆPPGï¼‰å’ŒDual Prototype Evolutionï¼ˆDPEï¼‰ä¸¤ä¸ªæ–°æ¨¡å—å®ç°å‰æ™¯åŸå‹çš„æ¼”åŒ–ã€‚</li>
<li>PPGæ¨¡å—åˆ©ç”¨æ”¯æŒç‰¹å¾ä¸ºæŸ¥è¯¢å›¾åƒç”Ÿæˆåˆæ­¥åˆ†å‰²çš„ä¼ªåŸå‹ï¼Œåæ˜ å½“å‰æŸ¥è¯¢çš„ç‹¬ç‰¹éœ€æ±‚ã€‚</li>
<li>DPEæ¨¡å—ä½¿ç”¨ä¼ªåŸå‹å¯¹æ”¯æŒå›¾åƒè¿›è¡Œåå‘åˆ†å‰²ï¼Œç”Ÿæˆæ¼”åŒ–åçš„å‰æ™¯åŸå‹ï¼Œè¿™äº›åŸå‹å¯ä»¥è¢«è§†ä¸ºé’ˆå¯¹æŸ¥è¯¢çš„å®šåˆ¶è§£å†³æ–¹æ¡ˆã€‚</li>
<li>èƒŒæ™¯åŸå‹çš„æ¼”åŒ–ä»ä»£è¡¨æ‰€æœ‰è®­ç»ƒå›¾åƒé€šç”¨ç‰¹å¾çš„å…¨å±€èƒŒæ™¯åŸå‹å¼€å§‹ï¼Œå¹¶è®¾è®¡äº†Global Background Cleansingï¼ˆGBCï¼‰æ¨¡å—æ¥æ¶ˆé™¤å¯èƒ½çš„ä¸åˆ©æˆåˆ†ï¼Œåæ˜ å½“å‰å‰æ™¯ç±»çš„ç‰¹æ€§ã€‚</li>
<li>åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†QPENetç›¸è¾ƒäºç°æœ‰æŠ€æœ¯çš„å¤§å¹…æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c103bf4ec5835efb51a0253bbfebb1c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97bf38f62345affa5e8c646a8ab1a288.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc1d370155ffbc0ccac3030a61bdc8e.jpg" align="middle">
</details>




<h2 id="Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning"><a href="#Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning"></a>Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Wangmeng Zuo, Chunmei Feng</strong></p>
<p>Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰çš„ç›®æ ‡æ˜¯åŸºäºéå¸¸æœ‰é™çš„è®­ç»ƒæ•°æ®æŒç»­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸å¿˜æ‰ä¹‹å‰é‡åˆ°çš„æ—§ç±»åˆ«ã€‚ç°æœ‰ç ”ç©¶ä»…ä¾èµ–äºçº¯è§†è§‰ç½‘ç»œï¼Œè€Œæœ¬æ–‡åˆ™é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰æ¥è§£å†³FSCILé—®é¢˜ï¼Œå¹¶æå‡ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåä¸ºåŸºäºåˆ†å¸ƒç‰¹å¾å›æ”¾çš„å­¦ä¹ æç¤ºï¼ˆLP-DiFï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å³å¯å¤§å¹…è¶…è¶Šæœ€å…·å½±å“åŠ›çš„æ–¹æ³•ã€‚æ¥ç€ï¼Œæˆ‘ä»¬é‡‡ç”¨æç¤ºè°ƒæ•´æŠ€æœ¯æ¥è¿›ä¸€æ­¥æé«˜å…¶é€‚åº”èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸæŒç»­ä»æ¯ä¸ªä¼šè¯ä¸­æ•è·ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºäº†é˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­å¿˜è®°æ—§çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç»´æŒä¸€ä¸ªç”±è®­ç»ƒå›¾åƒç‰¹å¾å’Œå˜è‡ªåŠ¨ç¼–ç å™¨ç”Ÿæˆçš„åˆæˆç‰¹å¾ä¼°ç®—å¾—åˆ°çš„ç‰¹å¾çº§å¯¹è§’åæ–¹å·®çŸ©é˜µé«˜æ–¯åˆ†å¸ƒæ¥ä¿ç•™æ¯ä¸ªæ—§ç±»çš„çŸ¥è¯†ã€‚å½“è¿›å…¥æ–°ä¼šè¯æ—¶ï¼Œä¼ªç‰¹å¾é€šè¿‡ä»æ—§ç±»åˆ†å¸ƒä¸­é‡‡æ ·å¹¶ç»“åˆå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œä»è€Œèƒ½å¤Ÿä½¿æ¨¡å‹åœ¨å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚åœ¨CIFAR100ã€mini-ImageNetã€CUB-200ä¸‰ä¸ªæµè¡ŒåŸºå‡†ä»¥åŠæœ¬æ–‡æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„SUN-397å’ŒCUB-200$*$ä¸Šçš„å®éªŒå±•ç¤ºäº†LP-DiFçš„ä¼˜è¶Šæ€§ï¼Œåœ¨FSCILé¢†åŸŸè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01598v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡è§£å†³äº†å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å¹¶å¼•å…¥Learning Prompt with Distribution-based Feature Replayï¼ˆLP-DiFï¼‰æ¡†æ¶ï¼Œå®ç°äº†åœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸‹å¯¹æ–°ç±»çš„è¿ç»­å­¦ä¹ ï¼ŒåŒæ—¶ä¸é—å¿˜å·²å­¦ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLP-DiFåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LP-DiFæ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è§£å†³å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°ï¼Œå·²æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•çš„æ•ˆæœã€‚</li>
<li>å¼•å…¥æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè¿ç»­æ•æ‰æ¯ä¸ªä¼šè¯ä¸­çš„ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>æå‡ºä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ï¼Œé˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­é—å¿˜æ—§çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡ç»´æŠ¤ç‰¹å¾çº§é«˜æ–¯åˆ†å¸ƒæ¥ä¼°è®¡æ—§ç±»çš„çŸ¥è¯†ï¼Œå¹¶ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ç”Ÿæˆçš„åˆæˆç‰¹å¾è¿›è¡Œè¡¥å……ã€‚</li>
<li>ä¼ªç‰¹å¾ä¸å½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒç›¸ç»“åˆï¼Œä»¥ä¼˜åŒ–æç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b68ef099e7713b014eeb2b3e7b49e8c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a76b9eff0fad3d63dcb46aee459a0673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a24a512d0bc42141ba455a1b4b860e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f07311872d24975899a6d4fd3d1d5e60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74c2a8fa494ec56d4ee19c96dabbeb8f.jpg" align="middle">
</details>




<h2 id="A-Prompt-Learning-Framework-for-Source-Code-Summarization"><a href="#A-Prompt-Learning-Framework-for-Source-Code-Summarization" class="headerlink" title="A Prompt Learning Framework for Source Code Summarization"></a>A Prompt Learning Framework for Source Code Summarization</h2><p><strong>Authors:Tingting Xu, Yun Miao, Chunrong Fang, Hanwei Qian, Xia Feng, Zhenpeng Chen, Chong Wang, Jian Zhang, Weisong Sun, Zhenyu Chen, Yang Liu</strong></p>
<p>(Source) code summarization is the task of automatically generating natural language summaries (also called comments) for given code snippets. Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks. The main adaptation schemes include instruction prompting, task-oriented (full-parameter) fine-tuning, and parameter-efficient fine-tuning (PEFT). However, instruction prompting involves designing crafted prompts and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs, and effective, tailored PEFT methods for code summarization are still lacking.   This paper proposes an effective prompt learning framework for code summarization called PromptCS. It no longer requires users to rack their brains to design effective prompts. Instead, PromptCS trains a prompt agent that can generate continuous prompts to unleash the potential for LLMs in code summarization. Compared to the human-written discrete prompt, the continuous prompts are produced under the guidance of LLMs and are therefore easier to understand by LLMs. PromptCS is non-invasive to LLMs and freezes the parameters of LLMs when training the prompt agent, which can greatly reduce the requirements for training resources. Our comprehensive experimental results show that PromptCS significantly outperforms instruction prompting schemes (including zero-shot learning and few-shot learning) on all four widely used metrics, and is comparable to the task-oriented fine-tuning scheme. In some base LLMs, e.g., StarCoderBase-1B and -3B, PromptCS even outperforms the task-oriented fine-tuning scheme. More importantly, the training efficiency of PromptCS is faster than the task-oriented fine-tuning scheme, with a more pronounced advantage on larger LLMs. </p>
<blockquote>
<p>ä»£ç æ‘˜è¦ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„ä»£ç ç‰‡æ®µè‡ªåŠ¨ç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆä¹Ÿç§°ä¸ºæ³¨é‡Šï¼‰ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„æˆåŠŸåº”ç”¨ï¼Œè½¯ä»¶å·¥ç¨‹ç ”ç©¶äººå‘˜ä¹Ÿå°è¯•å°†LLMé€‚åº”äºè§£å†³ä»£ç æ‘˜è¦ä»»åŠ¡ã€‚ä¸»è¦çš„é€‚åº”æ–¹æ¡ˆåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€é¢å‘ä»»åŠ¡ï¼ˆå…¨å‚æ•°ï¼‰å¾®è°ƒä»¥åŠå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ã€‚ç„¶è€Œï¼ŒæŒ‡ä»¤æç¤ºéœ€è¦è®¾è®¡ç²¾å¿ƒçš„æç¤ºï¼Œå¹¶è¦æ±‚ç”¨æˆ·å…·å¤‡ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼›é¢å‘ä»»åŠ¡çš„å¾®è°ƒåˆ™éœ€è¦é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬ï¼Œé’ˆå¯¹ä»£ç æ‘˜è¦çš„æœ‰æ•ˆä¸”å®šåˆ¶åŒ–çš„PEFTæ–¹æ³•ä»ç„¶ç¼ºä¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.16066v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»£ç æ‘˜è¦ä»»åŠ¡åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¥ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºé€‚åº”ä»£ç æ‘˜è¦ä»»åŠ¡ï¼Œæå‡ºäº†åä¸ºPromptCSçš„æœ‰æ•ˆæç¤ºå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒæç¤ºä»£ç†ç”Ÿæˆè¿ç»­æç¤ºï¼Œé‡Šæ”¾LLMsåœ¨ä»£ç æ‘˜è¦ä¸­çš„æ½œåŠ›ã€‚ç›¸è¾ƒäºäººå·¥ç¼–å†™çš„ç¦»æ•£æç¤ºï¼Œè¿ç»­æç¤ºåœ¨LLMsçš„æŒ‡å¯¼ä¸‹ç”Ÿæˆï¼Œæ›´æ˜“è¢«LLMsç†è§£ã€‚PromptCSå‡å°‘å¯¹è®­ç»ƒèµ„æºçš„éœ€æ±‚ï¼Œä¸”åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºæŒ‡ä»¤æç¤ºæ–¹æ¡ˆï¼Œä¸ä»»åŠ¡å¯¼å‘çš„å¾®è°ƒæ–¹æ¡ˆç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›åŸºç¡€LLMsä¸Šè¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç æ‘˜è¦ä»»åŠ¡æ˜¯è‡ªåŠ¨ç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆå³æ³¨é‡Šï¼‰çš„ç¼–ç¨‹ä»»åŠ¡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­ä¹Ÿè¢«å°è¯•ç”¨äºä»£ç æ‘˜è¦ä»»åŠ¡ã€‚</li>
<li>ç›®å‰ä¸»è¦çš„é€‚åº”æ–¹æ¡ˆåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€ä»»åŠ¡å¯¼å‘çš„å¾®è°ƒä»¥åŠå‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ã€‚</li>
<li>PromptCSæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒæç¤ºä»£ç†ç”Ÿæˆè¿ç»­æç¤ºã€‚</li>
<li>è¿ç»­æç¤ºç›¸è¾ƒäºäººå·¥ç¼–å†™çš„ç¦»æ•£æç¤ºæ›´æ˜“è¢«LLMsç†è§£ã€‚</li>
<li>PromptCSåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºæŒ‡ä»¤æç¤ºæ–¹æ¡ˆï¼Œä¸ä»»åŠ¡å¯¼å‘çš„å¾®è°ƒæ–¹æ¡ˆç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-164bd8769567313059bee368a65b9ace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff42804d993c1ea7e5236f922cf8d31e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1dd1263773a2b25c4a0b2c661b13688.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb67bbec797180bf1eebe06f696dd8f8.jpg" align="middle">
</details>




<h2 id="Leveraging-Large-Language-Models-for-Node-Generation-in-Few-Shot-Learning-on-Text-Attributed-Graphs"><a href="#Leveraging-Large-Language-Models-for-Node-Generation-in-Few-Shot-Learning-on-Text-Attributed-Graphs" class="headerlink" title="Leveraging Large Language Models for Node Generation in Few-Shot   Learning on Text-Attributed Graphs"></a>Leveraging Large Language Models for Node Generation in Few-Shot   Learning on Text-Attributed Graphs</h2><p><strong>Authors:Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang</strong></p>
<p>Text-attributed graphs have recently garnered significant attention due to their wide range of applications in web domains. Existing methodologies employ word embedding models for acquiring text representations as node features, which are subsequently fed into Graph Neural Networks (GNNs) for training. Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data. Furthermore, the acquisition and labeling of extensive datasets are both costly and time-consuming endeavors. Consequently, few-shot learning has emerged as a crucial problem in the context of graph learning tasks. In order to tackle this challenge, we propose a lightweight paradigm called LLM4NG, which adopts a plug-and-play approach to empower text-attributed graphs through node generation using LLMs. Specifically, we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars. Subsequently, we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph. This approach harnesses LLMs for enhancing class-level information and seamlessly introduces labeled nodes and edges without modifying the raw dataset, thereby facilitating the node classification task in few-shot scenarios. Extensive experiments demonstrate the outstanding performance of our proposed paradigm, particularly in low-shot scenarios. For instance, in the 1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement over the baseline model. </p>
<blockquote>
<p>æ–‡æœ¬å±æ€§å›¾å› å…¶åœ¨ç½‘ç»œé¢†åŸŸçš„å¹¿æ³›åº”ç”¨è€Œæœ€è¿‘å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨è¯åµŒå…¥æ¨¡å‹è·å–æ–‡æœ¬è¡¨ç¤ºä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œç„¶åå°†å…¶è¾“å…¥å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰è¿›è¡Œè®­ç»ƒã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œå±•ç°äº†å…¶åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œè¿™å¯ä»¥æå¤§åœ°å¢å¼ºå›¾çš„æ–‡æœ¬å±æ€§ã€‚æ­¤å¤–ï¼Œè·å–å’Œæ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†éƒ½æ˜¯æˆæœ¬é«˜ä¸”è€—æ—¶çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œå°æ ·æœ¬å­¦ä¹ å·²æˆä¸ºå›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„å…³é”®é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„èŒƒå¼ï¼Œç§°ä¸ºLLM4NGï¼Œå®ƒé‡‡ç”¨å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡èŠ‚ç‚¹ç”Ÿæˆæ¥å¢å¼ºæ–‡æœ¬å±æ€§å›¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMä»æ ‡ç­¾ä¸­æå–è¯­ä¹‰ä¿¡æ¯ï¼Œç”Ÿæˆå±äºè¿™äº›ç±»åˆ«çš„æ ·æœ¬ä½œä¸ºèŒƒä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨è¾¹ç¼˜é¢„æµ‹å™¨æ¥æ•æ‰åŸå§‹æ•°æ®é›†å›ºæœ‰çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†æ–°ç”Ÿæˆçš„æ ·æœ¬é›†æˆåˆ°åŸå§‹å›¾ä¸­ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨LLMå¢å¼ºç±»çº§ä¿¡æ¯ï¼Œå¹¶æ— ç¼åœ°å¼•å…¥æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹å’Œè¾¹ç¼˜ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸå§‹æ•°æ®é›†ï¼Œä»è€Œä¿ƒè¿›äº†å°æ ·æœ¬åœºæ™¯ä¸­çš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„èŒƒå¼åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ogbn-arxivæ•°æ®é›†çš„1ä¸ªæ ·æœ¬è®¾ç½®ä¸‹ï¼ŒLLM4NGç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å®ç°äº†7 ç»“ç¼˜çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09872v2">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬å±æ€§å›¾å› å…¶åœ¨ç½‘é¡µåŸŸä¸­çš„å¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨è¯åµŒå…¥æ¨¡å‹è·å–æ–‡æœ¬è¡¨ç¤ºä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œå¹¶è¾“å…¥å›¾ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯æå¤§å¢å¼ºå›¾çš„æ–‡æœ¬å±æ€§ã€‚ä¸ºäº†è§£å†³æ•°æ®é›†è·å–å’Œæ ‡æ³¨çš„é«˜æˆæœ¬å’Œè€—æ—¶é—®é¢˜ï¼Œå°‘æ ·æœ¬å­¦ä¹ æˆä¸ºäº†å›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„å…³é”®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLM4NGçš„è½»é‡çº§èŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–æ ‡ç­¾è¯­ä¹‰ä¿¡æ¯å¹¶ç”Ÿæˆæ ·æœ¬ä½œä¸ºèŒƒä¾‹æ¥èµ‹èƒ½æ–‡æœ¬å±æ€§å›¾ã€‚é€šè¿‡è¾¹ç¼˜é¢„æµ‹å™¨æ•æ‰åŸå§‹æ•°æ®é›†çš„ç»“æ„ä¿¡æ¯å¹¶å°†æ–°ç”Ÿæˆçš„æ ·æœ¬é›†æˆåˆ°åŸå§‹å›¾ä¸­ï¼Œæ­¤èŒƒå¼åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æé«˜ç±»çº§ä¿¡æ¯ï¼Œæ— éœ€ä¿®æ”¹åŸå§‹æ•°æ®å³å¯è½»æ¾å¼•å…¥æ ‡è®°èŠ‚ç‚¹å’Œè¾¹ç¼˜ï¼Œä»è€ŒååŠ©å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯ä½æ ·æœ¬åœºæ™¯ä¸‹ï¼Œè¯¥èŒƒå¼è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å¦‚åœ¨ogbn-arxivæ•°æ®é›†çš„1æ¬¡æ‹æ‘„è®¾ç½®ä¸­ï¼ŒLLM4NGè¾ƒåŸºçº¿æ¨¡å‹æé«˜äº†76%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å±æ€§å›¾å› å…¶åœ¨å¤šç§åº”ç”¨é¢†åŸŸçš„æ™®åŠè€Œå¤‡å—å…³æ³¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›å¯å¢å¼ºå›¾çš„æ–‡æœ¬å±æ€§ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ æˆä¸ºå›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„å…³é”®ï¼Œå› ä¸ºæ•°æ®é›†è·å–å’Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>LLM4NGèŒƒå¼åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–æ ‡ç­¾è¯­ä¹‰ä¿¡æ¯å¹¶ç”Ÿæˆæ ·æœ¬ï¼Œä»¥èµ‹èƒ½æ–‡æœ¬å±æ€§å›¾ã€‚</li>
<li>é€šè¿‡è¾¹ç¼˜é¢„æµ‹å™¨æ•æ‰æ•°æ®é›†çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†æ–°ç”Ÿæˆçš„æ ·æœ¬é›†æˆåˆ°åŸå§‹å›¾ä¸­ã€‚</li>
<li>LLM4NGåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æé«˜ç±»çº§ä¿¡æ¯ï¼Œè½»æ¾å¼•å…¥æ ‡è®°èŠ‚ç‚¹å’Œè¾¹ç¼˜ï¼Œè¾…åŠ©å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„èŠ‚ç‚¹åˆ†ç±»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ ·æœ¬åœºæ™¯ä¸‹ï¼ŒLLM4NGèŒƒå¼è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-293d82a1cfde71955a6d01ec02d26962.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-269b156153c6e7fe626fd09cc8e2dfbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c99b46fe7e03e3681391ad86b945fa51.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f2e52d8ca53dde891901cf63796eac44.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  AdvWave Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0991ba2786d73f43a32d07228887ae29.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  ConDSeg A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
