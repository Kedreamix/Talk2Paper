<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8e62a8822f6dfdf50634b68b964c5f1a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    96 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="DMin-Scalable-Training-Data-Influence-Estimation-for-Diffusion-Models"><a href="#DMin-Scalable-Training-Data-Influence-Estimation-for-Diffusion-Models" class="headerlink" title="DMin: Scalable Training Data Influence Estimation for Diffusion Models"></a>DMin: Scalable Training Data Influence Estimation for Diffusion Models</h2><p><strong>Authors:Huawei Lin, Yingjie Lao, Weijie Zhao</strong></p>
<p>Identifying the training data samples that most influence a generated image is a critical task in understanding diffusion models, yet existing influence estimation methods are constrained to small-scale or LoRA-tuned models due to computational limitations. As diffusion models scale up, these methods become impractical. To address this challenge, we propose DMin (Diffusion Model influence), a scalable framework for estimating the influence of each training data sample on a given generated image. By leveraging efficient gradient compression and retrieval techniques, DMin reduces storage requirements from 339.39 TB to only 726 MB and retrieves the top-k most influential training samples in under 1 second, all while maintaining performance. Our empirical results demonstrate DMin is both effective in identifying influential training samples and efficient in terms of computational and storage requirements. </p>
<blockquote>
<p>è¯†åˆ«å’Œè¯„ä¼°å¯¹ç”Ÿæˆå›¾åƒå½±å“æœ€å¤§çš„è®­ç»ƒæ•°æ®æ ·æœ¬æ˜¯ç†è§£æ‰©æ•£æ¨¡å‹çš„å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œç°æœ‰çš„å½±å“è¯„ä¼°æ–¹æ³•ä»…é™äºå°è§„æ¨¡æˆ–ç»è¿‡LoRAè°ƒæ•´æ¨¡å‹çš„åº”ç”¨ã€‚éšç€æ‰©æ•£æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§ï¼Œè¿™äº›æ–¹æ³•å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DMinï¼ˆæ‰©æ•£æ¨¡å‹å½±å“ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºä¼°è®¡æ¯ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç»™å®šç”Ÿæˆå›¾åƒçš„å½±å“ã€‚é€šè¿‡åˆ©ç”¨é«˜æ•ˆçš„æ¢¯åº¦å‹ç¼©å’Œæ£€ç´¢æŠ€æœ¯ï¼ŒDMinå°†å­˜å‚¨éœ€æ±‚ä»339.39TBå‡å°‘åˆ°ä»…726MBï¼Œå¹¶åœ¨ä¸åˆ°1ç§’å†…æ£€ç´¢åˆ°å‰kä¸ªæœ€å…·å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒDMinåœ¨è¯†åˆ«æœ‰å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬æ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒåŒæ—¶åœ¨è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚æ–¹é¢ä¹Ÿéå¸¸é«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08637v1">PDF</a> 14 pages, 6 figures, 8 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“è¯„ä¼°æ˜¯ç†è§£æ‰©æ•£æ¨¡å‹çš„å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œç°æœ‰å½±å“è¯„ä¼°æ–¹æ³•ä»…é™äºå°è§„æ¨¡æˆ–ç»è¿‡LoRAè°ƒæ•´æ¨¡å‹çš„ä½¿ç”¨ã€‚éšç€æ‰©æ•£æ¨¡å‹çš„æ‰©å±•ï¼Œè¿™äº›æ–¹æ³•å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DMinï¼ˆæ‰©æ•£æ¨¡å‹å½±å“ï¼‰æ¡†æ¶ï¼Œç”¨äºä¼°ç®—æ¯ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç»™å®šç”Ÿæˆå›¾åƒçš„å½±å“ã€‚é€šè¿‡åˆ©ç”¨é«˜æ•ˆçš„æ¢¯åº¦å‹ç¼©å’Œæ£€ç´¢æŠ€æœ¯ï¼ŒDMinå°†å­˜å‚¨éœ€æ±‚ä»339.39TBå‡å°‘åˆ°ä»…726MBï¼Œå¹¶åœ¨ä¸åˆ°1ç§’å†…æ£€ç´¢åˆ°å‰kä¸ªæœ€å…·å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒDMinåœ¨è¯†åˆ«å…·æœ‰å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬æ–¹é¢æ—¢æœ‰æ•ˆï¼Œåˆåœ¨è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚æ–¹é¢é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­ï¼Œè¯„ä¼°è®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“æ˜¯å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰å½±å“è¯„ä¼°æ–¹æ³•å› è®¡ç®—é™åˆ¶è€Œå±€é™äºå°è§„æ¨¡æˆ–ç‰¹å®šæ¨¡å‹åº”ç”¨ã€‚</li>
<li>DMinæ¡†æ¶æ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ•°æ®å½±å“è¯„ä¼°æŒ‘æˆ˜ã€‚</li>
<li>DMinåˆ©ç”¨æ¢¯åº¦å‹ç¼©å’Œæ£€ç´¢æŠ€æœ¯ï¼Œæ˜¾è‘—å‡å°‘å­˜å‚¨éœ€æ±‚å¹¶åŠ å¿«æ£€ç´¢é€Ÿåº¦ã€‚</li>
<li>DMinèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ä¸åˆ°ä¸€ç§’å†…æ£€ç´¢åˆ°æœ€å…·å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>DMinåœ¨è¯†åˆ«å…·æœ‰å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
<li>DMinåœ¨é™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚çš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b91c6c92f0168d31ec8753dab1d6e4cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8430fd7a7b4d2e453e4c528a89a370c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43bf3c416ff96d38aeebeb435464daed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfd4f8873a6dfcd15ea8920862474249.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd5f6f9b1b1c461fd1cdba2a7dfb40b0.jpg" align="middle">
</details>




<h2 id="TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person"><a href="#TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person" class="headerlink" title="TryOffAnyone: Tiled Cloth Generation from a Dressed Person"></a>TryOffAnyone: Tiled Cloth Generation from a Dressed Person</h2><p><strong>Authors:Ioannis Xarchakos, Theodoros Koukopoulos</strong></p>
<p>The fashion industry is increasingly leveraging computer vision and deep learning technologies to enhance online shopping experiences and operational efficiencies. In this paper, we address the challenge of generating high-fidelity tiled garment images essential for personalized recommendations, outfit composition, and virtual try-on systems from photos of garments worn by models. Inspired by the success of Latent Diffusion Models (LDMs) in image-to-image translation, we propose a novel approach utilizing a fine-tuned StableDiffusion model. Our method features a streamlined single-stage network design, which integrates garmentspecific masks to isolate and process target clothing items effectively. By simplifying the network architecture through selective training of transformer blocks and removing unnecessary crossattention layers, we significantly reduce computational complexity while achieving state-of-the-art performance on benchmark datasets like VITON-HD. Experimental results demonstrate the effectiveness of our approach in producing high-quality tiled garment images for both full-body and half-body inputs. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone">https://github.com/ixarchakos/try-off-anyone</a> </p>
<blockquote>
<p>æ—¶å°šäº§ä¸šæ­£è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æå‡åœ¨çº¿è´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³ä»æ¨¡ç‰¹æ‰€ç©¿è¡£ç‰©ç…§ç‰‡ç”Ÿæˆé«˜è´¨é‡å¹³é“ºè¡£ç‰©å›¾åƒçš„æŒ‘æˆ˜ï¼Œè¿™äº›å›¾åƒå¯¹äºä¸ªæ€§åŒ–æ¨èã€æœè£…æ­é…å’Œè™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨å›¾åˆ°å›¾ç¿»è¯‘ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¾®è°ƒè¿‡çš„StableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œç»“åˆè¡£ç‰©ç‰¹å®šæ©è†œï¼Œæœ‰æ•ˆéš”ç¦»å’Œå¤„ç†ç›®æ ‡è¡£ç‰©é¡¹ç›®ã€‚é€šè¿‡é€‰æ‹©æ€§è®­ç»ƒå˜å‹å™¨å—å¹¶å»é™¤ä¸å¿…è¦çš„äº¤å‰æ³¨æ„å±‚ï¼Œç®€åŒ–ç½‘ç»œæ¶æ„ï¼Œæˆ‘ä»¬åœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œåœ¨VITON-HDç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¨èº«ä½“å’ŒåŠèº«ä½“è¾“å…¥æƒ…å†µä¸‹ï¼Œç”Ÿæˆé«˜è´¨é‡å¹³é“ºè¡£ç‰©å›¾åƒçš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ixarchakos/try-off-anyoneæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08573v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ—¶å°šäº§ä¸šæ­£ç§¯æè¿ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æå‡åœ¨çº¿è´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚æœ¬æ–‡è§£å†³ç”Ÿæˆé«˜ä¿çœŸå¹³é“ºæœè£…å›¾ç‰‡çš„æŒ‘æˆ˜ï¼Œè¿™äº›å›¾ç‰‡å¯¹äºä¸ªæ€§åŒ–æ¨èã€æ­é…ç»„åˆå’Œè™šæ‹Ÿè¯•è¡£ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨å›¾è½¬å›¾æˆåŠŸçš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¾®è°ƒStableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œç»“åˆæœè£…ç‰¹å®šæ©è†œï¼Œæœ‰æ•ˆéš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚é€šè¿‡é€‰æ‹©æ€§è®­ç»ƒå˜å‹å™¨å—å¹¶å»é™¤ä¸å¿…è¦çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œç®€åŒ–ç½‘ç»œæ¶æ„ï¼Œåœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œå®ç°åœ¨VITON-HDç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¨èº«ä½“å’ŒåŠèº«è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œéƒ½èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å¹³é“ºæœè£…å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ—¶å°šäº§ä¸šæ­£åœ¨åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æå‡åœ¨çº¿è´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚</li>
<li>ç”Ÿæˆé«˜ä¿çœŸå¹³é“ºæœè£…å›¾åƒå¯¹äºä¸ªæ€§åŒ–æ¨èã€æ­é…ç»„åˆå’Œè™šæ‹Ÿè¯•è¡£ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨å›¾è½¬å›¾æ–¹é¢çš„æˆåŠŸåº”ç”¨ä¸ºè§£å†³é—®é¢˜æä¾›äº†çµæ„Ÿã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨å¾®è°ƒStableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆæœè£…ç‰¹å®šæ©è†œï¼Œæœ‰æ•ˆéš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚</li>
<li>é€šè¿‡ç®€åŒ–ç½‘ç»œæ¶æ„ï¼Œå®ç°äº†åœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å¹³é“ºæœè£…å›¾åƒï¼Œé€‚ç”¨äºå…¨èº«ä½“å’ŒåŠèº«è¾“å…¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4858b57b9c75d47132c65f2050bc5fe1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2856b5c51d6af52f31848166ea39e62f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bd374d5d4a708e65208387522338bb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3127daa555f76175132b4ef21712c878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7748cecb9cb068dc6d44eb58141f9a77.jpg" align="middle">
</details>




<h2 id="Learning-Flow-Fields-in-Attention-for-Controllable-Person-Image-Generation"><a href="#Learning-Flow-Fields-in-Attention-for-Controllable-Person-Image-Generation" class="headerlink" title="Learning Flow Fields in Attention for Controllable Person Image   Generation"></a>Learning Flow Fields in Attention for Controllable Person Image   Generation</h2><p><strong>Authors:Zijian Zhou, Shikun Liu, Xiao Han, Haozhe Liu, Kam Woh Ng, Tian Xie, Yuren Cong, Hang Li, Mengmeng Xu, Juan-Manuel PÃ©rez-RÃºa, Aditya Patel, Tao Xiang, Miaojing Shi, Sen He</strong></p>
<p>Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the personâ€™s appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models. </p>
<blockquote>
<p>å¯æ§äººç‰©å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®å‚è€ƒå›¾åƒç”Ÿæˆäººç‰©å›¾åƒï¼Œå®ç°å¯¹äººç‰©å¤–è§‚æˆ–å§¿æ€çš„ç²¾ç¡®æ§åˆ¶ã€‚ç„¶è€Œï¼Œå°½ç®¡å…ˆå‰çš„æ–¹æ³•åœ¨æ•´ä½“å›¾åƒè´¨é‡ä¸Šå–å¾—äº†è¾ƒé«˜çš„æ°´å¹³ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šæ‰­æ›²å‚è€ƒå›¾åƒçš„ç»†å¾®çº¹ç†ç»†èŠ‚ã€‚æˆ‘ä»¬å°†è¿™äº›æ‰­æ›²å½’å› äºå¯¹å‚è€ƒå›¾åƒä¸­ç›¸åº”åŒºåŸŸçš„å…³æ³¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ æ³¨æ„åŠ›æµåœºï¼ˆLeffaï¼‰ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜ç¡®å¼•å¯¼ç›®æ ‡æŸ¥è¯¢åœ¨æ³¨æ„åŠ›å±‚å…³æ³¨æ­£ç¡®çš„å‚è€ƒé”®ï¼Œä»¥å®ç°ç²¾å‡†æ§åˆ¶ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒæ˜¯é€šè¿‡åŸºäºæ‰©æ•£åŸºå‡†çš„æ³¨æ„åŠ›å›¾ä¸Šçš„æ­£åˆ™åŒ–æŸå¤±æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLeffaåœ¨æ§åˆ¶å¤–è§‚ï¼ˆè™šæ‹Ÿè¯•ç©¿ï¼‰å’Œå§¿æ€ï¼ˆå§¿æ€è¿ç§»ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ˜¾è‘—å‡å°‘äº†ç»†å¾®ç»†èŠ‚å¤±çœŸï¼ŒåŒæ—¶ä¿æŒäº†é«˜å›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æŸå¤±æ¨¡å‹å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯ç”¨äºæé«˜å…¶ä»–æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08486v1">PDF</a> github: <a target="_blank" rel="noopener" href="https://github.com/franciszzj/Leffa">https://github.com/franciszzj/Leffa</a>, demo:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/franciszzj/Leffa">https://huggingface.co/spaces/franciszzj/Leffa</a>, model:   <a target="_blank" rel="noopener" href="https://huggingface.co/franciszzj/Leffa">https://huggingface.co/franciszzj/Leffa</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå‚è€ƒå›¾åƒç”Ÿæˆå¯æ§äººç‰©å›¾åƒçš„æ–¹æ³•æ—¨åœ¨æ ¹æ®å‚è€ƒå›¾åƒç”Ÿæˆäººç‰©å›¾åƒï¼Œå¹¶èƒ½ç²¾ç¡®æ§åˆ¶äººç‰©çš„å¤–è§‚æˆ–å§¿æ€ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•å¸¸å¸¸åœ¨ä¿æŒæ•´ä½“å›¾åƒè´¨é‡çš„åŒæ—¶å¿½è§†äº†ç»†èŠ‚å¤„çš„çº¹ç†å¤±çœŸã€‚æˆ‘ä»¬å°†è¿™ç§å¤±çœŸå½’å› äºå¯¹å‚è€ƒå›¾åƒä¸­å¯¹åº”åŒºåŸŸçš„å…³æ³¨åº¦ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ æ³¨æ„åŠ›æµåœºï¼ˆLeffaï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›å±‚æ˜ç¡®å¼•å¯¼ç›®æ ‡æŸ¥è¯¢å…³æ³¨æ­£ç¡®çš„å‚è€ƒå…³é”®ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡æ‰©æ•£åŸºå‡†æ¨¡å‹ä¸Šçš„æ³¨æ„åŠ›å›¾å®ç°æ­£åˆ™åŒ–æŸå¤±ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒLeffaåœ¨æ§åˆ¶å¤–è§‚ï¼ˆè™šæ‹Ÿè¯•ç©¿ï¼‰å’Œå§¿æ€ï¼ˆå§¿æ€è½¬ç§»ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†ç»†èŠ‚å¤±çœŸå¹¶ä¿æŒé«˜è´¨é‡å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æŸå¤±æ¨¡å‹å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯ç”¨äºæé«˜å…¶ä»–æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯æ§äººç‰©å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®å‚è€ƒå›¾åƒç”Ÿæˆäººç‰©å›¾åƒï¼Œå¹¶ç²¾ç¡®æ§åˆ¶å…¶å¤–è§‚æˆ–å§¿æ€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»†èŠ‚çº¹ç†æ—¶å­˜åœ¨å¤±çœŸé—®é¢˜ã€‚</li>
<li>çº¹ç†å¤±çœŸçš„åŸå› æ˜¯å¯¹å‚è€ƒå›¾åƒä¸­å¯¹åº”åŒºåŸŸçš„å…³æ³¨åº¦ä¸è¶³ã€‚</li>
<li>å­¦ä¹ æ³¨æ„åŠ›æµåœºï¼ˆLeffaï¼‰æ–¹æ³•é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›å±‚æ˜ç¡®å¼•å¯¼ç›®æ ‡æŸ¥è¯¢å…³æ³¨æ­£ç¡®çš„å‚è€ƒå…³é”®ä¿¡æ¯ã€‚</li>
<li>Leffaæ–¹æ³•é€šè¿‡æ‰©æ•£åŸºå‡†æ¨¡å‹ä¸Šçš„æ³¨æ„åŠ›å›¾å®ç°æ­£åˆ™åŒ–æŸå¤±ã€‚</li>
<li>Leffaåœ¨æ§åˆ¶å¤–è§‚å’Œå§¿æ€æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†ç»†èŠ‚å¤±çœŸã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54cb028b4daea989345268fd31a8c299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69066777cbb33ba1d954ab23046a5510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f4644bc452d3ca3cef1d2cfd7b36345.jpg" align="middle">
</details>




<h2 id="InvDiff-Invariant-Guidance-for-Bias-Mitigation-in-Diffusion-Models"><a href="#InvDiff-Invariant-Guidance-for-Bias-Mitigation-in-Diffusion-Models" class="headerlink" title="InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models"></a>InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models</h2><p><strong>Authors:Min Hou, Yueying Wu, Chang Xu, Yu-Hao Huang, Chenxi Bai, Le Wu, Jiang Bian</strong></p>
<p>As one of the most successful generative models, diffusion models have demonstrated remarkable efficacy in synthesizing high-quality images. These models learn the underlying high-dimensional data distribution in an unsupervised manner. Despite their success, diffusion models are highly data-driven and prone to inheriting the imbalances and biases present in real-world data. Some studies have attempted to address these issues by designing text prompts for known biases or using bias labels to construct unbiased data. While these methods have shown improved results, real-world scenarios often contain various unknown biases, and obtaining bias labels is particularly challenging. In this paper, we emphasize the necessity of mitigating bias in pre-trained diffusion models without relying on auxiliary bias annotations. To tackle this problem, we propose a framework, InvDiff, which aims to learn invariant semantic information for diffusion guidance. Specifically, we propose identifying underlying biases in the training data and designing a novel debiasing training objective. Then, we employ a lightweight trainable module that automatically preserves invariant semantic information and uses it to guide the diffusion modelâ€™s sampling process toward unbiased outcomes simultaneously. Notably, we only need to learn a small number of parameters in the lightweight learnable module without altering the pre-trained diffusion model. Furthermore, we provide a theoretical guarantee that the implementation of InvDiff is equivalent to reducing the error upper bound of generalization. Extensive experimental results on three publicly available benchmarks demonstrate that InvDiff effectively reduces biases while maintaining the quality of image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Hundredl/InvDiff">https://github.com/Hundredl/InvDiff</a>. </p>
<blockquote>
<p>ä½œä¸ºæœ€æˆåŠŸçš„ç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ï¼Œæ‰©æ•£æ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ•ˆç‡ã€‚è¿™äº›æ¨¡å‹ä»¥æ— ç›‘ç£çš„æ–¹å¼å­¦ä¹ æ½œåœ¨çš„é«˜ç»´æ•°æ®åˆ†å¸ƒã€‚å°½ç®¡å®ƒä»¬å¾ˆæˆåŠŸï¼Œä½†æ‰©æ•£æ¨¡å‹æ˜¯é«˜åº¦æ•°æ®é©±åŠ¨çš„ï¼Œå¾ˆå®¹æ˜“ç»§æ‰¿ç°å®æ•°æ®ä¸­å­˜åœ¨çš„ä¸å¹³è¡¡å’Œåè§ã€‚ä¸€äº›ç ”ç©¶è¯•å›¾é€šè¿‡ä¸ºå·²çŸ¥åè§è®¾è®¡æ–‡æœ¬æç¤ºæˆ–ä½¿ç”¨åè§æ ‡ç­¾æ¥æ„å»ºæ— åè§æ•°æ®æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæ”¹è¿›çš„ç»“æœï¼Œä½†ç°å®ä¸–ç•Œçš„åœºæ™¯é€šå¸¸åŒ…å«å„ç§æœªçŸ¥çš„åè§ï¼Œå¹¶ä¸”è·å–åè§æ ‡ç­¾å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å‡è½»åè§çš„é‡è¦æ€§ï¼Œæ— éœ€ä¾èµ–è¾…åŠ©åè§æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºInvDiffçš„æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ æ‰©æ•£æŒ‡å¯¼çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºè¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨åè§å¹¶è®¾è®¡ä¸€ç§æ–°çš„å»åè§è®­ç»ƒç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨è½»é‡çº§çš„å¯è®­ç»ƒæ¨¡å—è‡ªåŠ¨ä¿ç•™ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥äº§ç”Ÿæ— åè§çš„è¾“å‡ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨è½»é‡çº§çš„å¯è®­ç»ƒæ¨¡å—ä¸­å­¦ä¹ å°‘é‡çš„å‚æ•°ï¼Œæ— éœ€æ›´æ”¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜InvDiffçš„å®ç°ç­‰åŒäºé™ä½äº†æ³›åŒ–è¯¯å·®çš„ä¸Šç•Œã€‚åœ¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒInvDiffåœ¨å‡å°‘åè§çš„åŒæ—¶ä¿æŒäº†å›¾åƒç”Ÿæˆçš„å“è´¨ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Hundredl/InvDiff">https://github.com/Hundredl/InvDiff</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08480v1">PDF</a> KDD 2025</p>
<p><strong>Summary</strong><br>    æ‰©æ•£æ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ•ˆæœï¼Œä½†å®ƒä»¬é«˜åº¦ä¾èµ–æ•°æ®ï¼Œå¹¶å¯èƒ½ç»§æ‰¿ç°å®ä¸–ç•Œçš„åè§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€ä¾èµ–è¾…åŠ©åè§æ³¨é‡Šçš„æ¡†æ¶InvDiffï¼Œæ—¨åœ¨å­¦ä¹ æ‰©æ•£æŒ‡å¯¼çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨åè§å¹¶è®¾è®¡æ–°çš„å»åè§è®­ç»ƒç›®æ ‡ï¼Œä½¿ç”¨è½»é‡çº§å¯è®­ç»ƒæ¨¡å—è‡ªåŠ¨ä¿ç•™ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼Œå®ç°æ— åè§çš„ç»“æœã€‚åªéœ€åœ¨è½»é‡çº§å¯è®­ç»ƒæ¨¡å—ä¸­å­¦ä¹ å°‘é‡å‚æ•°ï¼Œæ— éœ€æ›´æ”¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInvDiffåœ¨å‡å°‘åè§çš„åŒæ—¶ä¿æŒäº†å›¾åƒç”Ÿæˆçš„å“è´¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨æ•°æ®é©±åŠ¨çš„é—®é¢˜ï¼Œå¯èƒ½ç»§æ‰¿ç°å®ä¸–ç•Œçš„åè§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°è¯•é€šè¿‡æ–‡æœ¬æç¤ºæˆ–åè§æ ‡ç­¾æ¥è§£å†³åè§é—®é¢˜ï¼Œä½†é¢å¯¹æœªçŸ¥åè§æ—¶æ•ˆæœæœ‰é™ï¼Œä¸”è·å–åè§æ ‡ç­¾å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ¬æ–‡å¼ºè°ƒæ— éœ€ä¾èµ–è¾…åŠ©åè§æ³¨é‡Šæ¥å‡è½»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„åè§çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºInvDiffçš„æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ æ‰©æ•£æŒ‡å¯¼çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯æ¥åº”å¯¹åè§é—®é¢˜ã€‚</li>
<li>InvDiffé€šè¿‡è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨åè§å¹¶è®¾è®¡å»åè§è®­ç»ƒç›®æ ‡æ¥å®ç°å…¶ç›®æ ‡ã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§å¯è®­ç»ƒæ¨¡å—è‡ªåŠ¨ä¿ç•™ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ä»¥å®ç°æ— åè§ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e62a8822f6dfdf50634b68b964c5f1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06d138724421f3c0520b072624a6ddbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f56f88fec6e82de1849708de23501d90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcdc020a6ab8c7711e65c8d147a5fd1d.jpg" align="middle">
</details>




<h2 id="CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis"><a href="#CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis" class="headerlink" title="CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis"></a>CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis</h2><p><strong>Authors:Mu Zhang, Yunfan Liu, Yue Liu, Hongtian Yu, Qixiang Ye</strong></p>
<p>Accurately depicting real-world landscapes in remote sensing (RS) images requires precise alignment between objects and their environment. However, most existing synthesis methods for natural images prioritize foreground control, often reducing the background to plain textures. This neglects the interaction between foreground and background, which can lead to incoherence in RS scenarios. In this paper, we introduce CC-Diff, a Diffusion Model-based approach for RS image generation with enhanced Context Coherence. To capture spatial interdependence, we propose a sequential pipeline where background generation is conditioned on synthesized foreground instances. Distinct learnable queries are also employed to model both the complex background texture and its semantic relation to the foreground. Extensive experiments demonstrate that CC-Diff outperforms state-of-the-art methods in visual fidelity, semantic accuracy, and positional precision, excelling in both RS and natural image domains. CC-Diff also shows strong trainability, improving detection accuracy by 2.04 mAP on DOTA and 2.25 mAP on the COCO benchmark. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒä¸­å‡†ç¡®æç»˜çœŸå®ä¸–ç•Œçš„æ™¯è§‚è¦æ±‚ç‰©ä½“ä¸å…¶ç¯å¢ƒä¹‹é—´æœ‰ç²¾ç¡®çš„å¯¹é½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è‡ªç„¶å›¾åƒåˆæˆæ–¹æ³•ä¼˜å…ˆå¯¹å‰æ™¯è¿›è¡Œæ§åˆ¶ï¼Œé€šå¸¸å°†èƒŒæ™¯ç®€åŒ–ä¸ºå¹³åŸçº¹ç†ã€‚è¿™å¿½ç•¥äº†å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„äº¤äº’ï¼Œå¯èƒ½å¯¼è‡´é¥æ„Ÿåœºæ™¯ä¸­çš„ä¸è¿è´¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CC-Diffï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå…·æœ‰å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚ä¸ºäº†æ•è·ç©ºé—´ç›¸å…³æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¡ºåºæµç¨‹ï¼Œå…¶ä¸­èƒŒæ™¯ç”Ÿæˆæ˜¯åœ¨åˆæˆçš„å‰æ™¯å®ä¾‹æ¡ä»¶ä¸‹è¿›è¡Œçš„ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸åŒçš„å¯å­¦ä¹ æŸ¥è¯¢æ¥å¯¹å¤æ‚çš„èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨é¥æ„Ÿå’Œè‡ªç„¶å›¾åƒé¢†åŸŸéƒ½è¡¨ç°å‡ºè‰²ã€‚CC-Diffè¿˜æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¯è®­ç»ƒæ€§ï¼Œåœ¨DOTAå’ŒCOCOåŸºå‡†æµ‹è¯•ä¸Šçš„æ£€æµ‹å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†2.04 mAPå’Œ2.25 mAPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08464v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹é¥æ„Ÿå›¾åƒä¸­çš„çœŸå®ä¸–ç•Œæ™¯è§‚å‡†ç¡®æç»˜éœ€è¦ç‰©ä½“ä¸å…¶ç¯å¢ƒä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚ç°æœ‰å¤§å¤šæ•°è‡ªç„¶å›¾åƒåˆæˆæ–¹æ³•ä¾§é‡äºå‰æ™¯æ§åˆ¶ï¼Œå¸¸å¸¸å°†èƒŒæ™¯ç®€åŒ–ä¸ºå•ä¸€çº¹ç†ï¼Œå¿½ç•¥äº†å‰æ™¯ä¸èƒŒæ™¯ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œå¯¼è‡´é¥æ„Ÿåœºæ™¯ä¸­çš„ä¸è¿è´¯æ€§ã€‚æœ¬æ–‡æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„CC-Diffæ–¹æ³•ï¼Œç”¨äºé¥æ„Ÿå›¾åƒç”Ÿæˆï¼Œå¢å¼ºä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚é€šè¿‡é‡‡ç”¨æ¡ä»¶èƒŒæ™¯ç”Ÿæˆçš„é¡ºåºæµç¨‹ä»¥åŠä½¿ç”¨ä¸åŒçš„å¯å­¦ä¹ æŸ¥è¯¢æ¥å»ºæ¨¡å¤æ‚èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»ï¼Œå®ç°ç©ºé—´ä¾èµ–æ€§æ•æ‰ã€‚å®éªŒè¡¨æ˜ï¼ŒCC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶åœ¨é¥æ„Ÿå’Œè‡ªç„¶å›¾åƒé¢†åŸŸå‡æœ‰å“è¶Šè¡¨ç°ï¼Œæé«˜äº†DOTAå’ŒCOCOåŸºå‡†æµ‹è¯•ä¸­çš„æ£€æµ‹ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒå‡†ç¡®æç»˜éœ€è¦å‰æ™¯ä¸èƒŒæ™¯é—´çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>ç°æœ‰è‡ªç„¶å›¾åƒåˆæˆæ–¹æ³•å¸¸å¿½ç•¥èƒŒæ™¯ï¼Œå¯¼è‡´é¥æ„Ÿåœºæ™¯ä¸­çš„ä¸è¿è´¯æ€§ã€‚</li>
<li>CC-Diffæ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå›¾åƒç”Ÿæˆï¼Œå¢å¼ºä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚</li>
<li>é‡‡ç”¨é¡ºåºæµç¨‹å®ç°èƒŒæ™¯ç”Ÿæˆçš„æ¡ä»¶åŒ–ï¼Œä»¥åŠä½¿ç”¨ä¸åŒçš„å¯å­¦ä¹ æŸ¥è¯¢å»ºæ¨¡å¤æ‚èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„å…³ç³»ã€‚</li>
<li>CC-Diffåœ¨è§†è§‰ã€è¯­ä¹‰å’Œå®šä½ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CC-Diffåœ¨é¥æ„Ÿå’Œè‡ªç„¶å›¾åƒé¢†åŸŸå‡å±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39f965fde7b978afd19e7cc0ca9f8b95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed910a52932186b8173136b43049cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0614f7a5ceff9a03b46ed443d78c6d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0df51cf4df07c894818863e62a13b9f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c037adc9b093b6f98a1c87b90e337a76.jpg" align="middle">
</details>




<h2 id="Grasp-Diffusion-Network-Learning-Grasp-Generators-from-Partial-Point-Clouds-with-Diffusion-Models-in-SO-3-xR3"><a href="#Grasp-Diffusion-Network-Learning-Grasp-Generators-from-Partial-Point-Clouds-with-Diffusion-Models-in-SO-3-xR3" class="headerlink" title="Grasp Diffusion Network: Learning Grasp Generators from Partial Point   Clouds with Diffusion Models in SO(3)xR3"></a>Grasp Diffusion Network: Learning Grasp Generators from Partial Point   Clouds with Diffusion Models in SO(3)xR3</h2><p><strong>Authors:Joao Carvalho, An T. Le, Philipp Jahr, Qiao Sun, Julen Urain, Dorothea Koert, Jan Peters</strong></p>
<p>Grasping objects successfully from a single-view camera is crucial in many robot manipulation tasks. An approach to solve this problem is to leverage simulation to create large datasets of pairs of objects and grasp poses, and then learn a conditional generative model that can be prompted quickly during deployment. However, the grasp pose data is highly multimodal since there are several ways to grasp an object. Hence, in this work, we learn a grasp generative model with diffusion models to sample candidate grasp poses given a partial point cloud of an object. A novel aspect of our method is to consider diffusion in the manifold space of rotations and to propose a collision-avoidance cost guidance to improve the grasp success rate during inference. To accelerate grasp sampling we use recent techniques from the diffusion literature to achieve faster inference times. We show in simulation and real-world experiments that our approach can grasp several objects from raw depth images with $90%$ success rate and benchmark it against several baselines. </p>
<blockquote>
<p>åœ¨ä¼—å¤šçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä»å•è§†è§’æ‘„åƒæœºæˆåŠŸæŠ“å–ç‰©ä½“è‡³å…³é‡è¦ã€‚ä¸€ç§è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•æ˜¯åˆ©ç”¨æ¨¡æ‹ŸæŠ€æœ¯åˆ›å»ºå¤§é‡ç‰©ä½“å’ŒæŠ“å–å§¿æ€çš„æ•°æ®é›†ï¼Œç„¶åå­¦ä¹ ä¸€ç§æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨éƒ¨ç½²æœŸé—´å¯ä»¥å¿«é€Ÿè¿›è¡Œæç¤ºã€‚ç„¶è€Œï¼Œç”±äºæŠ“å–ç‰©ä½“çš„æ–¹å¼æœ‰å¤šç§ï¼ŒæŠ“å–å§¿æ€æ•°æ®å…·æœ‰å¤šæ¨¡æ€æ€§ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ æŠ“å–ç”Ÿæˆæ¨¡å‹ï¼Œä»¥ç»™å®šç‰©ä½“çš„éƒ¨åˆ†ç‚¹äº‘æ¥é‡‡æ ·å€™é€‰æŠ“å–å§¿æ€ã€‚æˆ‘ä»¬æ–¹æ³•çš„ä¸€ä¸ªæ–°é¢–ä¹‹å¤„åœ¨äºè€ƒè™‘åœ¨æ—‹è½¬æµå½¢ç©ºé—´ä¸­çš„æ‰©æ•£ï¼Œå¹¶æå‡ºä¸€ç§é¿ç¢°æˆæœ¬å¼•å¯¼æ¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„æŠ“å–æˆåŠŸç‡ã€‚ä¸ºäº†åŠ å¿«æŠ“å–é‡‡æ ·é€Ÿåº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ‰©æ•£æ–‡çŒ®ä¸­çš„æœ€æ–°æŠ€æœ¯æ¥å®ç°æ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»åŸå§‹æ·±åº¦å›¾åƒä¸­æŠ“å–å¤šç§ç‰©ä½“ï¼ŒæˆåŠŸç‡è¾¾åˆ°90%ï¼Œå¹¶ä¸”ä¸å‡ ä¸ªåŸºå‡†æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ‰©æ•£æ¨¡å‹è§£å†³æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­çš„å•è§†è§’ç›¸æœºæŠ“å–é—®é¢˜ã€‚é€šè¿‡ä»¿çœŸåˆ›å»ºå¤§é‡å¯¹è±¡ä¸æŠ“å–å§¿æ€çš„æ•°æ®é›†ï¼Œå­¦ä¹ æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨éƒ¨ç½²æ—¶å¿«é€Ÿæç¤ºã€‚ç”±äºæŠ“å–å§¿æ€æ•°æ®å…·æœ‰å¤šæ¨¡æ€æ€§ï¼Œæœ¬æ–‡é‡‡ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ æŠ“å–ç”Ÿæˆæ¨¡å‹ï¼Œå¯¹ç»™å®šå¯¹è±¡çš„å±€éƒ¨ç‚¹äº‘è¿›è¡Œé‡‡æ ·å€™é€‰æŠ“å–å§¿æ€ã€‚æ–¹æ³•çš„åˆ›æ–°ä¹‹å¤„åœ¨äºè€ƒè™‘æ—‹è½¬æµå½¢ç©ºé—´çš„æ‰©æ•£ï¼Œå¹¶æå‡ºç¢°æ’é¿å…æˆæœ¬æŒ‡å¯¼ä»¥æé«˜æ¨ç†é˜¶æ®µçš„æŠ“å–æˆåŠŸç‡ã€‚åˆ©ç”¨æ‰©æ•£æ–‡çŒ®ä¸­çš„æœ€æ–°æŠ€æœ¯åŠ é€ŸæŠ“å–é‡‡æ ·ï¼Œå®ç°åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„é«˜æ•ˆæŠ“å–ï¼ŒæˆåŠŸç‡ä¸º90%ï¼Œä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨ä»¿çœŸåˆ›å»ºå¤§é‡å¯¹è±¡ä¸æŠ“å–å§¿æ€çš„æ•°æ®é›†ï¼Œè§£å†³å•è§†è§’ç›¸æœºæŠ“å–é—®é¢˜ã€‚</li>
<li>å­¦ä¹ æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨éƒ¨ç½²æ—¶å¿«é€Ÿæç¤ºã€‚</li>
<li>æŠ“å–å§¿æ€æ•°æ®å…·æœ‰å¤šæ¨¡æ€æ€§ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ æŠ“å–ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>æ–¹æ³•çš„åˆ›æ–°ç‚¹åœ¨äºè€ƒè™‘æ—‹è½¬æµå½¢ç©ºé—´çš„æ‰©æ•£ã€‚</li>
<li>æå‡ºç¢°æ’é¿å…æˆæœ¬æŒ‡å¯¼ï¼Œæé«˜æ¨ç†é˜¶æ®µçš„æŠ“å–æˆåŠŸç‡ã€‚</li>
<li>åˆ©ç”¨æœ€æ–°æŠ€æœ¯åŠ é€ŸæŠ“å–é‡‡æ ·ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å®ç°é«˜æ•ˆæŠ“å–ï¼ŒæˆåŠŸç‡ä¸º90%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c17eacc64fa0c9fb52341ea65b88bc6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c272602300f7c75c6511c67dad2747ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5ac35f945bb82796595229121aaebad.jpg" align="middle">
</details>




<h2 id="Unicorn-Unified-Neural-Image-Compression-with-One-Number-Reconstruction"><a href="#Unicorn-Unified-Neural-Image-Compression-with-One-Number-Reconstruction" class="headerlink" title="Unicorn: Unified Neural Image Compression with One Number Reconstruction"></a>Unicorn: Unified Neural Image Compression with One Number Reconstruction</h2><p><strong>Authors:Qi Zheng, Haozhi Wang, Zihao Liu, Jiaming Liu, Peiye Liu, Zhijian Hao, Yanheng Lu, Dimin Niu, Jinjia Zhou, Minge Jing, Yibo Fan</strong></p>
<p>Prevalent lossy image compression schemes can be divided into: 1) explicit image compression (EIC), including traditional standards and neural end-to-end algorithms; 2) implicit image compression (IIC) based on implicit neural representations (INR). The former is encountering impasses of either leveling off bitrate reduction at a cost of tremendous complexity while the latter suffers from excessive smoothing quality as well as lengthy decoder models. In this paper, we propose an innovative paradigm, which we dub \textbf{Unicorn} (\textbf{U}nified \textbf{N}eural \textbf{I}mage \textbf{C}ompression with \textbf{O}ne \textbf{N}number \textbf{R}econstruction). By conceptualizing the images as index-image pairs and learning the inherent distribution of pairs in a subtle neural network model, Unicorn can reconstruct a visually pleasing image from a randomly generated noise with only one index number. The neural model serves as the unified decoder of images while the noises and indexes corresponds to explicit representations. As a proof of concept, we propose an effective and efficient prototype of Unicorn based on latent diffusion models with tailored model designs. Quantitive and qualitative experimental results demonstrate that our prototype achieves significant bitrates reduction compared with EIC and IIC algorithms. More impressively, benefitting from the unified decoder, our compression ratio escalates as the quantity of images increases. We envision that more advanced model designs will endow Unicorn with greater potential in image compression. We will release our codes in \url{<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree%7D">https://github.com/uniqzheng/Unicorn-Laduree}</a>. </p>
<blockquote>
<p>å½“å‰æµè¡Œçš„æœ‰æŸå›¾åƒå‹ç¼©æ–¹æ¡ˆå¯åˆ†ä¸ºä¸¤ç±»ï¼šä¸€æ˜¯æ˜¾å¼å›¾åƒå‹ç¼©ï¼ˆEICï¼‰ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæ ‡å‡†å’Œç¥ç»ç«¯åˆ°ç«¯ç®—æ³•ï¼›äºŒæ˜¯åŸºäºéšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„éšå¼å›¾åƒå‹ç¼©ï¼ˆIICï¼‰ã€‚å‰è€…åœ¨é™ä½æ¯”ç‰¹ç‡æ—¶é¢ä¸´ç€å·¨å¤§çš„å¤æ‚æ€§ï¼Œè€Œåè€…åˆ™å­˜åœ¨è¿‡åº¦å¹³æ»‘è´¨é‡å’Œè¿‡é•¿çš„è§£ç å™¨æ¨¡å‹é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œç‹¬è§’å…½â€ï¼ˆUnicornï¼‰ï¼ˆç»Ÿä¸€ç¥ç»ç½‘ç»œå›¾åƒå‹ç¼©ï¼‰ã€‚é€šè¿‡å°†å›¾åƒæ¦‚å¿µåŒ–ä¸ºç´¢å¼•å›¾åƒå¯¹ï¼Œå¹¶åœ¨å¾®å¦™çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­å­¦ä¹ å¯¹ä¹‹é—´çš„å†…åœ¨åˆ†å¸ƒï¼Œç‹¬è§’å…½å¯ä»¥ä»éšæœºç”Ÿæˆçš„å™ªå£°ä¸­ä»…ç”¨å•ä¸ªç´¢å¼•å·é‡å»ºå‡ºä»¤äººæ„‰æ‚¦çš„å›¾åƒã€‚ç¥ç»ç½‘ç»œæ¨¡å‹å……å½“å›¾åƒçš„ç»Ÿä¸€è§£ç å™¨ï¼Œè€Œå™ªå£°å’Œç´¢å¼•å¯¹åº”äºæ˜¾å¼è¡¨ç¤ºã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆä¸”é«˜æ•ˆçš„ç‹¬è§’å…½åŸå‹ï¼Œå¹¶è¿›è¡Œäº†é‡èº«å®šåˆ¶çš„æ¨¡å‹è®¾è®¡ã€‚å®šé‡å’Œå®šæ€§çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸå‹ä¸EICå’ŒIICç®—æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„ç ç‡é™ä½ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œå¾—ç›Šäºç»Ÿä¸€çš„è§£ç å™¨ï¼Œéšç€å›¾åƒæ•°é‡çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¯”ä¹Ÿéšä¹‹æé«˜ã€‚æˆ‘ä»¬é¢„è®¡æ›´å…ˆè¿›çš„æ¨¡å‹è®¾è®¡å°†èµ‹äºˆç‹¬è§’å…½åœ¨å›¾åƒå‹ç¼©æ–¹é¢çš„æ›´å¤§æ½œåŠ›ã€‚æˆ‘ä»¬ä¼šåœ¨â€‹â€‹<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree%E2%80%8B%E2%80%8B%E4%B8%8A%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/uniqzheng/Unicorn-Ladureeâ€‹â€‹ä¸Šå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08210v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºUnicornçš„ç»Ÿä¸€ç¥ç»ç½‘ç»œå›¾åƒå‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡æŠŠå›¾åƒè§†ä¸ºç´¢å¼•-å›¾åƒå¯¹ï¼Œå¹¶åœ¨å¾®å¦™çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­å­¦ä¹ è¿™äº›å¯¹çš„å†…åœ¨åˆ†å¸ƒï¼Œèƒ½å¤Ÿä»éšæœºç”Ÿæˆçš„å™ªå£°ä¸­ä»…ç”¨å•ä¸ªç´¢å¼•å·é‡å»ºå‡ºè§†è§‰ä¸Šä»¤äººæ»¡æ„çš„å›¾åƒã€‚è¯¥æ–¹æ³•åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹è®¾è®¡åŸå‹ï¼Œä¸ç°æœ‰å›¾åƒå‹ç¼©ç®—æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ˜¾è‘—çš„ç ç‡é™ä½ï¼Œä¸”éšç€å›¾åƒæ•°é‡çš„å¢åŠ ï¼Œå‹ç¼©æ¯”æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¯¹ç°æœ‰çš„æœ‰æŸå›¾åƒå‹ç¼©æ–¹æ¡ˆè¿›è¡Œäº†åˆ†ç±»ï¼ŒåŒ…æ‹¬æ˜¾å¼å›¾åƒå‹ç¼©ï¼ˆEICï¼‰å’Œéšå¼å›¾åƒå‹ç¼©ï¼ˆIICï¼‰ã€‚</li>
<li>EICé¢ä¸´åœ¨é™ä½æ¯”ç‰¹ç‡æ—¶å¤æ‚åº¦æ€¥å‰§å¢åŠ çš„é—®é¢˜ï¼Œè€ŒIICåˆ™å­˜åœ¨å›¾åƒè´¨é‡è¿‡äºå¹³æ»‘å’Œè§£ç å™¨æ¨¡å‹è¿‡é•¿çš„é—®é¢˜ã€‚</li>
<li>Unicornæ–¹æ³•é€šè¿‡æŠŠå›¾åƒè§†ä¸ºç´¢å¼•-å›¾åƒå¯¹ï¼Œå¹¶åˆ©ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ å…¶å†…åœ¨åˆ†å¸ƒï¼Œå®ç°äº†ä»éšæœºå™ªå£°ä¸­é‡å»ºå›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>UnicornåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹è®¾è®¡åŸå‹ï¼Œå®ç°äº†ä¸ç°æœ‰ç®—æ³•ç›¸æ¯”çš„æ˜¾è‘—ç ç‡é™ä½ã€‚</li>
<li>Unicornå…·æœ‰éšç€å›¾åƒæ•°é‡å¢åŠ ï¼Œå‹ç¼©æ¯”æé«˜çš„ç‰¹ç‚¹ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„Unicornæ–¹æ³•å…·æœ‰æ½œåœ¨å›¾åƒå‹ç¼©çš„å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5e9526e87e92fe4967015f82a9583a1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8e36b788687535643c8ea55fb808f7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5427895e74d7b079fb9e86e3fd1cb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d03eb4b291b395751079d79dfdcd80f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec2e069491f59d900ff73613a434c81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95d33bebd679d05deea40b127573544a.jpg" align="middle">
</details>




<h2 id="Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing"><a href="#Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing" class="headerlink" title="Diffusion-Based Attention Warping for Consistent 3D Scene Editing"></a>Diffusion-Based Attention Warping for Consistent 3D Scene Editing</h2><p><strong>Authors:Eyal Gomel, Lior Wolf</strong></p>
<p>We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \url{<a target="_blank" rel="noopener" href="https://attention-warp.github.io}/">https://attention-warp.github.io}</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œ3Dåœºæ™¯ç¼–è¾‘çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ç¡®ä¿ä¸åŒè§†è§’ä¸‹çš„è§†å›¾ä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä»å•ä¸€å‚è€ƒå›¾åƒä¸­æå–çš„æ³¨æ„åŠ›ç‰¹å¾æ¥å®šä¹‰é¢„æœŸçš„ç¼–è¾‘ã€‚é€šè¿‡å°†è¿™äº›ç‰¹å¾ä¸ä»é«˜æ–¯å±•å¸ƒæ·±åº¦ä¼°è®¡ä¸­å¾—å‡ºçš„åœºæ™¯å‡ ä½•ç»“æ„è¿›è¡Œå¯¹é½ï¼Œå°†å®ƒä»¬å˜å½¢åˆ°å¤šä¸ªè§†è§’ã€‚å°†è¿™äº›å˜å½¢çš„ç‰¹å¾æ³¨å…¥åˆ°å…¶ä»–è§†è§’ï¼Œèƒ½å¤Ÿå®ç°ç¼–è¾‘çš„è¿è´¯ä¼ æ’­ï¼Œåœ¨3Dç©ºé—´ä¸­å®ç°é«˜ä¿çœŸå’Œç©ºé—´å¯¹é½ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆ3Dåœºæ™¯çš„é€šç”¨ç¼–è¾‘æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†åœºæ™¯æ“ä½œçš„èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://attention-warp.github.io/">https://attention-warp.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07984v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œ3Dåœºæ™¯ç¼–è¾‘çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›ç‰¹å¾ç¡®ä¿ä»ä¸åŒè§’åº¦è§‚çœ‹æ—¶åœºæ™¯çš„ä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å‚è€ƒå›¾åƒä¸­æå–æ³¨æ„åŠ›ç‰¹å¾æ¥å®šä¹‰é¢„æœŸçš„ç¼–è¾‘ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾é€šè¿‡é«˜æ–¯æ¨¡ç³Šæ·±åº¦ä¼°è®¡çš„åœºæ™¯å‡ ä½•è¿›è¡Œå¯¹é½ï¼Œåœ¨ä¸åŒè§†è§’è¿›è¡Œå˜å½¢ã€‚å°†è¿™äº›å˜å½¢çš„ç‰¹å¾æ³¨å…¥åˆ°å…¶ä»–è§†è§’ï¼Œå®ç°äº†ç¼–è¾‘çš„è¿è´¯ä¼ æ’­ï¼Œåœ¨3Dç©ºé—´ä¸­å®ç°äº†é«˜ä¿çœŸå’Œç©ºé—´å¯¹é½ã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„3Dåœºæ™¯ç¼–è¾‘æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¤§å¤§æå‡äº†åœºæ™¯æ“ä½œçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•è¿›è¡Œ3Dåœºæ™¯ç¼–è¾‘ã€‚</li>
<li>åˆ©ç”¨æ³¨æ„åŠ›ç‰¹å¾ç¡®ä¿ä¸åŒè§†è§’è§‚çœ‹åœºæ™¯çš„ä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚</li>
<li>é€šè¿‡é«˜æ–¯æ¨¡ç³Šæ·±åº¦ä¼°è®¡çš„åœºæ™¯å‡ ä½•å¯¹æ³¨æ„åŠ›ç‰¹å¾è¿›è¡Œå˜å½¢ã€‚</li>
<li>å°†å˜å½¢çš„ç‰¹å¾æ³¨å…¥åˆ°å…¶ä»–è§†è§’ï¼Œå®ç°ç¼–è¾‘çš„è¿è´¯ä¼ æ’­ã€‚</li>
<li>åœ¨3Dç©ºé—´ä¸­å®ç°äº†é«˜ä¿çœŸå’Œç©ºé—´å¯¹é½çš„ç¼–è¾‘æ•ˆæœã€‚</li>
<li>è¯„ä¼°è¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„3Dåœºæ™¯ç¼–è¾‘æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8f42584dfad759e1b2aecc279277861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5766ba7cdaf9078e0a54b1a828295c72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e83f73a0915f2745d1dec4a301df78d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a0003e9cacf1db217bda34beeb1a59b.jpg" align="middle">
</details>




<h2 id="Non-Normal-Diffusion-Models"><a href="#Non-Normal-Diffusion-Models" class="headerlink" title="Non-Normal Diffusion Models"></a>Non-Normal Diffusion Models</h2><p><strong>Authors:Henry Li</strong></p>
<p>Diffusion models generate samples by incrementally reversing a process that turns data into noise. We show that when the step size goes to zero, the reversed process is invariant to the distribution of these increments. This reveals a previously unconsidered parameter in the design of diffusion models: the distribution of the diffusion step $\Delta x_k :&#x3D; x_{k} - x_{k + 1}$. This parameter is implicitly set by default to be normally distributed in most diffusion models. By lifting this assumption, we generalize the framework for designing diffusion models and establish an expanded class of diffusion processes with greater flexibility in the choice of loss function used during training. We demonstrate the effectiveness of these models on density estimation and generative modeling tasks on standard image datasets, and show that different choices of the distribution of $\Delta x_k$ result in qualitatively different generated samples. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥åè½¬å°†æ•°æ®è½¬åŒ–ä¸ºå™ªå£°çš„è¿‡ç¨‹æ¥ç”Ÿæˆæ ·æœ¬ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå½“æ­¥é•¿è¶‹äºé›¶æ—¶ï¼Œåè½¬è¿‡ç¨‹å¯¹è¿™äº›å¢é‡çš„åˆ†å¸ƒæ˜¯ä¸å˜çš„ã€‚è¿™æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹è®¾è®¡ä¸­ä¸€ä¸ªä»¥å‰æœªè¢«è€ƒè™‘è¿‡çš„å‚æ•°ï¼šæ‰©æ•£æ­¥é•¿$\Delta x_k :&#x3D; x_{k} - x_{k + 1}$çš„åˆ†å¸ƒã€‚è¿™ä¸ªå‚æ•°åœ¨å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹ä¸­é»˜è®¤è¢«è®¾ä¸ºæ­£æ€åˆ†å¸ƒã€‚é€šè¿‡å–æ¶ˆè¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬æ¨å¹¿äº†è®¾è®¡æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰æ›´å¤§çµæ´»æ€§çš„æ‰©æ•£è¿‡ç¨‹ç±»ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥é€‰æ‹©æ›´å¤§çš„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†å›¾åƒæ•°æ®é›†ä¸Šå±•ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†ä¸åŒçš„$\Delta x_k$åˆ†å¸ƒé€‰æ‹©ä¼šå¯¼è‡´å®šæ€§ä¸åŒçš„ç”Ÿæˆæ ·æœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥åè½¬å°†æ•°æ®è½¬åŒ–ä¸ºå™ªå£°çš„è¿‡ç¨‹æ¥ç”Ÿæˆæ ·æœ¬ã€‚å½“æ­¥é•¿è¶‹äºé›¶æ—¶ï¼Œåè½¬è¿‡ç¨‹å¯¹å¢é‡åˆ†å¸ƒå…·æœ‰ä¸å˜æ€§ï¼Œæ­ç¤ºå‡ºæ‰©æ•£æ¨¡å‹è®¾è®¡ä¸­ä¸€ä¸ªæœªè¢«è€ƒè™‘è¿‡çš„å‚æ•°ï¼šæ‰©æ•£æ­¥é•¿Î”xkçš„åˆ†å¸ƒã€‚å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹é»˜è®¤å°†å…¶è®¾ä¸ºæ­£æ€åˆ†å¸ƒã€‚é€šè¿‡å–æ¶ˆè¿™ä¸€å‡è®¾ï¼Œæœ¬æ–‡æ¨å¹¿äº†æ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ¡†æ¶ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰æ›´å¤§çµæ´»æ€§çš„æ‰©æ•£è¿‡ç¨‹ç±»ï¼Œå¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€‰æ‹©ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚åœ¨æ ‡å‡†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œå¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡æ—¶ï¼Œæœ¬æ–‡å±•ç¤ºäº†ä¸åŒÎ”xkåˆ†å¸ƒé€‰æ‹©å¯¹ç”Ÿæˆæ ·æœ¬è´¨é‡çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥åè½¬æ•°æ®åˆ°å™ªå£°çš„è¿‡ç¨‹ç”Ÿæˆæ ·æœ¬ã€‚</li>
<li>æ‰©æ•£æ­¥é•¿Î”xkçš„åˆ†å¸ƒæ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„å‚æ•°ã€‚</li>
<li>å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹é»˜è®¤å°†æ‰©æ•£æ­¥é•¿Î”xkè®¾ä¸ºæ­£æ€åˆ†å¸ƒã€‚</li>
<li>å–æ¶ˆè¿™ä¸€å‡è®¾å¯ä»¥æ¨å¹¿æ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ¡†æ¶ï¼Œå¹¶å»ºç«‹æ›´çµæ´»çš„æ‰©æ•£è¿‡ç¨‹ç±»ã€‚</li>
<li>ä¸åŒçš„Î”xkåˆ†å¸ƒé€‰æ‹©ä¼šå½±å“ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚</li>
<li>åœ¨æ ‡å‡†å›¾åƒæ•°æ®é›†ä¸Šï¼Œæ–°çš„æ‰©æ•£æ¨¡å‹åœ¨å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e51bcb7d6661004cce30ab4aaaf7543b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fdee689b42dcc752a197d0f2a3aa59a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b38949908ad2849cbbc799f48b413fd.jpg" align="middle">
</details>




<h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models on some reward functions that are either designed by experts or learned from small-scale datasets. Existing methods for finetuning diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. Inspired by recent successes in generative flow networks (GFlowNets), a class of probabilistic models that sample with the unnormalized density of a reward function, we propose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), the first GFlowNet method that leverages the rich signal in reward gradients, together with an objective called $\nabla$-DB plus its variant residual $\nabla$-DB designed for prior-preserving diffusion alignment. We show that our proposed method achieves fast yet diversity- and prior-preserving alignment of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>é€šå¸¸äººä»¬é€šè¿‡æ”¶é›†ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é›†æ¥è®­ç»ƒå¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œä½†å¾€å¾€å¸Œæœ›å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸æŸäº›å¥–åŠ±å‡½æ•°å¯¹é½å¹¶è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›å¥–åŠ±å‡½æ•°æ˜¯ä¸“å®¶è®¾è®¡çš„æˆ–è€…æ˜¯ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚ç°æœ‰çš„å¾®è°ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒè¿‡ç¨‹ä¸­æ”¶æ•›é€Ÿåº¦æ…¢ç­‰é—®é¢˜ã€‚å—åˆ°ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰é¢†åŸŸçš„æœ€æ–°æˆåŠŸçš„å¯å‘ï¼Œä¸€ç±»ä½¿ç”¨å¥–åŠ±å‡½æ•°æœªå½’ä¸€åŒ–å¯†åº¦è¿›è¡Œé‡‡æ ·çš„æ¦‚ç‡æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„GFlowNetæ–¹æ³•ï¼Œç§°ä¸ºNabla-GFlowNetï¼ˆç®€ç§°$\nabla$-GFlowNetï¼‰ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·ä»¥åŠä¸€ä¸ªç§°ä¸º$\nabla$-DBçš„ç›®æ ‡åŠå…¶ä¸ºä¿ç•™å…ˆéªŒè®¾è®¡çš„å˜ä½“æ®‹å·®$\nabla$-DBçš„GFlowNetæ–¹æ³•ï¼Œä»¥å®ç°æ‰©æ•£å¯¹é½ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šç§çœŸå®å¥–åŠ±å‡½æ•°ä¸Šå¿«é€Ÿå®ç°äº†å¤§è§„æ¨¡æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¤šæ ·æ€§å’Œå…ˆéªŒçŸ¥è¯†ä¿ç•™å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v1">PDF</a> Technical Report (35 pages, 31 figures)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒé€šå¸¸æ˜¯é€šè¿‡åœ¨ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ”¶é›†æ•°æ®é›†æ¥è¿›è¡Œçš„ï¼Œç„¶è€Œï¼Œäººä»¬æ›´å¸Œæœ›å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è°ƒæ•´å¹¶å¯¹é½åˆ°æŸäº›å¥–åŠ±å‡½æ•°ä¸Šï¼Œè¿™äº›å¥–åŠ±å‡½æ•°æ˜¯ç”±ä¸“å®¶è®¾è®¡çš„æˆ–è€…æ˜¯ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚ç°æœ‰çš„å¾®è°ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒè¿‡ç¨‹ä¸­æ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚å—ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºNabla-GFlowNetçš„æ–°æ–¹æ³•ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·å¹¶ç»“åˆç§°ä¸º$\nabla$-DBçš„ç›®æ ‡åŠå…¶å˜ä½“çš„GFlowNetæ–¹æ³•ï¼Œå³ç”¨äºå…ˆéªŒä¿ç•™æ‰©æ•£å¯¹é½çš„å‰©ä½™$\nabla$-DBã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„ç°å®å¥–åŠ±å‡½æ•°ä¸Šå¿«é€Ÿå®ç°å¯¹Stable Diffusionç­‰å¤§å‹æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯¹é½ï¼ŒåŒæ—¶ä¿æŒå¤šæ ·æ€§å’Œå…ˆéªŒçŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒé€šå¸¸é›†ä¸­åœ¨ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é›†ä¸Šï¼Œä½†ä¸“å®¶è®¾è®¡çš„å¥–åŠ±å‡½æ•°æˆ–ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ çš„å¥–åŠ±å‡½æ•°çš„å¯¹é½å’Œå¾®è°ƒæ˜¯å¸¸è§çš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰å¾®è°ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>Nabla-GFlowNetæ˜¯é¦–ä¸ªç»“åˆå¥–åŠ±æ¢¯åº¦ä¿¡å·çš„GFlowNetæ–¹æ³•ã€‚</li>
<li>Nabla-GFlowNetåˆ©ç”¨ä¸°å¯Œçš„å¥–åŠ±æ¢¯åº¦ä¿¡å·ï¼Œæœ‰åŠ©äºå¿«é€Ÿå¯¹é½å¤§å‹æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•è®¾è®¡çš„ç›®æ ‡æ˜¯å®ç°å¤šæ ·æ€§å’Œå…ˆéªŒçŸ¥è¯†ä¿ç•™çš„å¯¹é½ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒNabla-GFlowNetåœ¨å¤šç§ç°å®å¥–åŠ±å‡½æ•°ä¸ŠæˆåŠŸå®ç°äº†Stable Diffusionæ¨¡å‹çš„å¯¹é½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6ad08bb0dc78455f591fd489bf8bf14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21950de2b5b2e07066fd3abf8159ca34.jpg" align="middle">
</details>




<h2 id="From-Slow-Bidirectional-to-Fast-Causal-Video-Generators"><a href="#From-Slow-Bidirectional-to-Fast-Causal-Video-Generators" class="headerlink" title="From Slow Bidirectional to Fast Causal Video Generators"></a>From Slow Bidirectional to Fast Causal Video Generators</h2><p><strong>Authors:Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</strong></p>
<p>Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to a causal transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacherâ€™s ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model supports fast streaming generation of high quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future. </p>
<blockquote>
<p>å½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–æ€§ï¼Œåœ¨äº¤äº’å¼åº”ç”¨ä¸­è¡¨ç°æŒ£æ‰ã€‚å•ä¸ªå¸§çš„ç”Ÿæˆéœ€è¦æ¨¡å‹å¤„ç†æ•´ä¸ªåºåˆ—ï¼ŒåŒ…æ‹¬æœªæ¥ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡å°†é¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£å˜å‹å™¨é€‚åº”ä¸ºå› æœå˜å‹å™¨æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¯¥å› æœå˜å‹å™¨å¯ä»¥å³æ—¶ç”Ÿæˆå¸§ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿï¼Œæˆ‘ä»¬å°†åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹ç²¾ç‚¼ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚ä¸ºäº†å®ç°ç¨³å®šå’Œé«˜è´¨é‡çš„è’¸é¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ•™å¸ˆODEè½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥åŠä¸€ç§ä¸å¯¹ç§°çš„è’¸é¦ç­–ç•¥ï¼Œå³ä½¿ç”¨åŒå‘æ•™å¸ˆç›‘ç£å› æœå­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ï¼Œå³ä½¿åœ¨çŸ­ç‰‡æ®µè®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é•¿æœŸè§†é¢‘åˆæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒåœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿæµå¼ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œè¿™å¾—ç›ŠäºKVç¼“å­˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†æµå¼è§†é¢‘åˆ°è§†é¢‘çš„è½¬æ¢ã€å›¾åƒåˆ°è§†é¢‘ä»¥åŠé›¶æ ·æœ¬æ–¹å¼çš„åŠ¨æ€æç¤ºã€‚æœªæ¥ï¼Œæˆ‘ä»¬å°†åŸºäºå¼€æºæ¨¡å‹å‘å¸ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07772v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://causvid.github.io/">https://causvid.github.io/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–è€Œäº§ç”Ÿçš„ç“¶é¢ˆï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡å°†ä¸€ä¸ªé¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£è½¬æ¢å™¨æ”¹ç¼–ä¸ºå› æœè½¬æ¢å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°äº†å³æ—¶ç”Ÿæˆå¸§ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼Œç ”ç©¶å›¢é˜Ÿå°†åˆ†å¸ƒåŒ¹é…è’¸é¦æ³•ï¼ˆDMDï¼‰æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹æµ“ç¼©ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆä»¥åŠä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œæœ‰æ•ˆå®ç°äº†ç¨³å®šä¸”é«˜è´¨é‡çš„è’¸é¦ï¼Œå‡å°‘äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç§¯ç´¯ã€‚è¯¥ç ”ç©¶æ¨¡å‹æ”¯æŒåœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶å®ç°è§†é¢‘åˆ°è§†é¢‘çš„æµå¼è½¬æ¢ã€å›¾åƒåˆ°è§†é¢‘ä»¥åŠé›¶æ ·æœ¬åŠ¨æ€æç¤ºç­‰åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒå‘æ³¨æ„åŠ›ä¾èµ–æ˜¯è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­çš„ç“¶é¢ˆã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ”¹ç¼–é¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£è½¬æ¢å™¨ä¸ºå› æœè½¬æ¢å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>åˆ†å¸ƒåŒ¹é…è’¸é¦æ³•ï¼ˆDMDï¼‰è¢«æˆåŠŸæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå°†æ‰©æ•£æ¨¡å‹çš„æ­¥éª¤ä»50æ­¥å‡å°‘åˆ°4æ­¥ã€‚</li>
<li>å¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆå’Œä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œå®ç°äº†ç¨³å®šä¸”é«˜è´¨é‡çš„è’¸é¦ã€‚</li>
<li>æ¨¡å‹æ”¯æŒå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶èƒ½åœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦è¿›è¡Œæµå¼ä¼ è¾“ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå®ç°è§†é¢‘åˆ°è§†é¢‘çš„æµå¼è½¬æ¢ã€å›¾åƒåˆ°è§†é¢‘è½¬æ¢ä»¥åŠé›¶æ ·æœ¬åŠ¨æ€æç¤ºåŠŸèƒ½ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿè®¡åˆ’åŸºäºå¼€æºæ¨¡å‹å‘å¸ƒä»£ç ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee965fc3063cea1099d1a788584150d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d753d21c54ebc6456937f17d3e9e71a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cb62cb28a54dd058bb39b3b2bfafbe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6f5f4cc15dc30fe00c4852e9829e5fe.jpg" align="middle">
</details>




<h2 id="SynCamMaster-Synchronizing-Multi-Camera-Video-Generation-from-Diverse-Viewpoints"><a href="#SynCamMaster-Synchronizing-Multi-Camera-Video-Generation-from-Diverse-Viewpoints" class="headerlink" title="SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse   Viewpoints"></a>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse   Viewpoints</h2><p><strong>Authors:Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang</strong></p>
<p>Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: <a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒ3Dä¸€è‡´æ€§æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚è¿™ä¸€è¿›å±•æ¿€å‘æˆ‘ä»¬ç ”ç©¶è¿™äº›æ¨¡å‹åœ¨è·¨ä¸åŒè§†è§’ç¡®ä¿åŠ¨æ€ä¸€è‡´æ€§çš„æ½œåŠ›ï¼Œè¿™å¯¹äºè™šæ‹Ÿæ‹æ‘„ç­‰åº”ç”¨æ¥è¯´æ˜¯ä¸€ä¸ªé«˜åº¦ç†æƒ³çš„åŠŸèƒ½ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ä¸“æ³¨äºå•ä¸€å¯¹è±¡çš„å¤šä¸ªè§†è§’ç”Ÿæˆç”¨äº4Dé‡å»ºï¼Œæˆ‘ä»¬çš„å…´è¶£åœ¨äºä»ä»»æ„è§†è§’ç”Ÿæˆå¼€æ”¾ä¸–ç•Œè§†é¢‘ï¼Œç»“åˆé‡‡ç”¨è‡ªç”±åº¦ä¸ºå…­çš„è§†è§’æ‘„åƒå¤´å§¿æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæ’éšç©çš„æ¨¡å—ï¼Œå®ƒå¯å¢å¼ºæ–‡æœ¬è‡³è§†é¢‘çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¤šæ‘„åƒå¤´è§†é¢‘ç”Ÿæˆçš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿ä¸åŒè§†è§’é—´çš„å†…å®¹ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè§†è§’åŒæ­¥æ¨¡å—ä»¥ä¿æŒè¿™äº›è§†è§’ä¹‹é—´å¤–è§‚å’Œå‡ ä½•çš„ä¸€è‡´æ€§ã€‚è€ƒè™‘åˆ°é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨å¤šæ‘„åƒå¤´å›¾åƒå’Œå•ç›®è§†é¢‘æ¥è¡¥å……ç”±è™šå¹»å¼•æ“æ¸²æŸ“çš„å¤šæ‘„åƒå¤´è§†é¢‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ç”¨äº†æœ‰è¶£çš„æ‰©å±•åŠŸèƒ½ï¼Œå¦‚ä»æ–°é¢–è§†è§’é‡æ–°æ¸²æŸ“è§†é¢‘ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåä¸ºSynCamVideo-Datasetçš„å¤šè§†è§’åŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07760v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒ3Dä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿæ¢è®¨äº†è¯¥æ¨¡å‹åœ¨è™šæ‹Ÿæ‹æ‘„ç­‰åº”ç”¨ä¸­ç¡®ä¿ä»ä¸åŒè§†è§’åŠ¨æ€ä¸€è‡´æ€§çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å³æ’å³ç”¨æ¨¡å—ï¼Œå¯ä»¥å¢å¼ºé¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å¤šç›¸æœºè§†é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œç¡®ä¿ä¸åŒè§†è§’çš„å†…å®¹ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†å¤šè§†è§’åŒæ­¥æ¨¡å—ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ¡ˆæ¥åˆ©ç”¨å¤šç›¸æœºå›¾åƒå’Œå•ç›®è§†é¢‘æ¥è¡¥å……è™šå¹»å¼•æ“æ¸²æŸ“çš„å¤šç›¸æœºè§†é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•è¿˜å¯ä»¥è¿›è¡Œæœ‰è¶£çš„æ‰©å±•ï¼Œä¾‹å¦‚ä»æ–°é¢–çš„è§†è§’é‡æ–°æ¸²æŸ“è§†é¢‘ã€‚åŒæ—¶å‘å¸ƒäº†ä¸€ä¸ªåä¸ºSynCamVideo-Datasetçš„å¤šè§†è§’åŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒ3Dä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå…³æ³¨æ¨¡å‹åœ¨è™šæ‹Ÿæ‹æ‘„ä¸­ç¡®ä¿ä¸åŒè§†è§’åŠ¨æ€ä¸€è‡´æ€§çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å³æ’å³ç”¨æ¨¡å—ï¼Œå¢å¼ºé¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å¤šç›¸æœºè§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†å¤šè§†è§’åŒæ­¥æ¨¡å—ä»¥ç»´æŒä¸åŒè§†è§’çš„å¤–è§‚å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤šç›¸æœºå›¾åƒå’Œå•ç›®è§†é¢‘æ¥è¡¥å……è™šå¹»å¼•æ“æ¸²æŸ“çš„å¤šç›¸æœºè§†é¢‘ã€‚</li>
<li>æ–¹æ³•æ”¯æŒä»æ–°é¢–è§†è§’é‡æ–°æ¸²æŸ“è§†é¢‘ï¼Œæä¾›äº†æ›´å¤§çš„åˆ›ä½œè‡ªç”±åº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cedadb2a6a524961cb37855fe9be841.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b31ef600e0a14b462d2dcfbf2e1e57bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aef164b104eed0ad6a622de188acbf28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85fa1fa7ccd65095a4fbc200417c521a.jpg" align="middle">
</details>




<h2 id="TraSCE-Trajectory-Steering-for-Concept-Erasure"><a href="#TraSCE-Trajectory-Steering-for-Concept-Erasure" class="headerlink" title="TraSCE: Trajectory Steering for Concept Erasure"></a>TraSCE: Trajectory Steering for Concept Erasure</h2><p><strong>Authors:Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, Yuki Mitsufuji</strong></p>
<p>Recent advancements in text-to-image diffusion models have brought them to the public spotlight, becoming widely accessible and embraced by everyday users. However, these models have been shown to generate harmful content such as not-safe-for-work (NSFW) images. While approaches have been proposed to erase such abstract concepts from the models, jail-breaking techniques have succeeded in bypassing such safety measures. In this paper, we propose TraSCE, an approach to guide the diffusion trajectory away from generating harmful content. Our approach is based on negative prompting, but as we show in this paper, conventional negative prompting is not a complete solution and can easily be bypassed in some corner cases. To address this issue, we first propose a modification of conventional negative prompting. Furthermore, we introduce a localized loss-based guidance that enhances the modified negative prompting technique by steering the diffusion trajectory. We demonstrate that our proposed method achieves state-of-the-art results on various benchmarks in removing harmful content including ones proposed by red teams; and erasing artistic styles and objects. Our proposed approach does not require any training, weight modifications, or training data (both image or prompt), making it easier for model owners to erase new concepts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä½¿å…¶å—åˆ°å…¬ä¼—å…³æ³¨ï¼Œå¹¶é€æ¸è¢«æ—¥å¸¸ç”¨æˆ·å¹¿æ³›æ¥å—å’Œä½¿ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å·²è¢«è¯æ˜å¯ä»¥ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œå¦‚ä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„å›¾åƒã€‚è™½ç„¶å·²æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­æ¶ˆé™¤è¿™äº›æŠ½è±¡æ¦‚å¿µï¼Œä½†ç ´è§£æŠ€æœ¯å·²æˆåŠŸç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºTraSCEæ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ‰©æ•£è½¨è¿¹è¿œç¦»ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè´Ÿæç¤ºï¼Œä½†æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ‰€å±•ç¤ºçš„ï¼Œä¼ ç»Ÿçš„è´Ÿæç¤ºå¹¶ä¸æ„æˆå®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œåœ¨æŸäº›ç‰¹å®šæƒ…å†µä¸‹å®¹æ˜“è¢«ç»•è¿‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºå¯¹å¸¸è§„è´Ÿæç¤ºçš„ä¿®æ”¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ‰©æ•£è½¨è¿¹æ¥å¢å¼ºæ”¹è¿›åçš„è´Ÿæç¤ºæŠ€æœ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬çº¢é˜Ÿæå‡ºçš„å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„æœ‰å®³å†…å®¹ï¼›ä»¥åŠæ¶ˆé™¤è‰ºæœ¯é£æ ¼å’Œç‰©ä½“ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸éœ€è¦ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼ˆæ— è®ºæ˜¯å›¾åƒè¿˜æ˜¯æç¤ºï¼‰ï¼Œè¿™ä½¿å¾—æ¨¡å‹æ‰€æœ‰è€…æ›´å®¹æ˜“æ¶ˆé™¤æ–°æ¦‚å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07658v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿‘æœŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å…¶ç”Ÿæˆä¸é€‚å½“å†…å®¹ï¼ˆå¦‚ä¸é€‚å®œå·¥ä½œåœºåˆçš„å›¾åƒï¼‰çš„é—®é¢˜å¼•å‘æ‹…å¿§ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­ç§»é™¤è¿™äº›æŠ½è±¡æ¦‚å¿µï¼Œä½†å­˜åœ¨ç»•è¿‡å®‰å…¨æªæ–½çš„è¶Šç‹±æŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºTraSCEæ–¹æ³•ï¼Œé€šè¿‡è´Ÿæç¤ºå’Œå¼•å¯¼æ‰©æ•£è½¨è¿¹ï¼Œé¿å…ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚TraSCEæ–¹æ³•æ”¹è¿›äº†ä¼ ç»Ÿè´Ÿæç¤ºï¼Œå¹¶å¼•å…¥åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼ï¼Œæ”¹è¿›åçš„æŠ€æœ¯èƒ½æ›´æœ‰æ•ˆåœ°å¼•å¯¼æ‰©æ•£è½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹ã€æ¶ˆé™¤è‰ºæœ¯é£æ ¼å’Œç‰©ä½“æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼Œä¾¿äºæ¨¡å‹æ‰€æœ‰è€…ç§»é™¤æ–°æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ç”Ÿæˆä¸é€‚å½“å†…å®¹çš„é—®é¢˜å¼•å‘æ‹…å¿§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­ç§»é™¤æœ‰å®³å†…å®¹ï¼Œä½†å­˜åœ¨ç»•è¿‡å®‰å…¨æªæ–½çš„æŠ€æœ¯ã€‚</li>
<li>TraSCEæ–¹æ³•åŸºäºè´Ÿæç¤ºå’Œæ”¹è¿›çš„æ‰©æ•£è½¨è¿¹æŒ‡å¯¼ï¼Œé¿å…ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚</li>
<li>TraSCEæ–¹æ³•æ”¹è¿›äº†ä¼ ç»Ÿè´Ÿæç¤ºï¼Œå¹¶å¼•å…¥å±€éƒ¨æŸå¤±æŒ‡å¯¼æœºåˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜TraSCEæ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>TraSCEæ–¹æ³•æ— éœ€ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼Œæ˜“äºå®æ–½æ–°æ¦‚å¿µç§»é™¤ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90e08ded28c54362a9bc6e958d7de136.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9977d55951a2f9b136992f265976828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-040c9e01c46ba7f7a7281eaea5b5ea91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29cc281a6f072cd35671ca8d9ac80956.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b0d9fe0eb315dd15a2b6ef43b9f4539.jpg" align="middle">
</details>




<h2 id="Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model"><a href="#Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model" class="headerlink" title="Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model"></a>Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Jianfeng Guo, Feng Yang, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Motion artifacts present in magnetic resonance imaging (MRI) can seriously interfere with clinical diagnosis. Removing motion artifacts is a straightforward solution and has been extensively studied. However, paired data are still heavily relied on in recent works and the perturbations in k-space (frequency domain) are not well considered, which limits their applications in the clinical field. To address these issues, we propose a novel unsupervised purification method which leverages pixel-frequency information of noisy MRI images to guide a pre-trained diffusion model to recover clean MRI images. Specifically, considering that motion artifacts are mainly concentrated in high-frequency components in k-space, we utilize the low-frequency components as the guide to ensure correct tissue textures. Additionally, given that high-frequency and pixel information are helpful for recovering shape and detail textures, we design alternate complementary masks to simultaneously destroy the artifact structure and exploit useful information. Quantitative experiments are performed on datasets from different tissues and show that our method achieves superior performance on several metrics. Qualitative evaluations with radiologists also show that our method provides better clinical feedback. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD">https://github.com/medcx/PFAD</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šä¸¥é‡å¹²æ‰°ä¸´åºŠè¯Šæ–­ã€‚å»é™¤è¿åŠ¨ä¼ªå½±æ˜¯ä¸€ç§ç›´æ¥è§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶ä»ç„¶ä¸¥é‡ä¾èµ–äºé…å¯¹æ•°æ®ï¼Œè€Œkç©ºé—´ï¼ˆé¢‘ç‡åŸŸï¼‰ä¸­çš„æ‰°åŠ¨å¹¶æœªå¾—åˆ°å¾ˆå¥½çš„è€ƒè™‘ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠé¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¸¦å™ªå£°çš„MRIå›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´çš„MRIå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œè€ƒè™‘åˆ°è¿åŠ¨ä¼ªå½±ä¸»è¦é›†ä¸­åœ¨kç©ºé—´çš„é«˜é¢‘æˆåˆ†ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„çº¹ç†ç»„ç»‡ã€‚æ­¤å¤–ï¼Œé‰´äºé«˜é¢‘å’Œåƒç´ ä¿¡æ¯æœ‰åŠ©äºæ¢å¤å½¢çŠ¶å’Œç»†èŠ‚çº¹ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†äº¤æ›¿çš„äº’è¡¥æ©è†œæ¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶åˆ©ç”¨æœ‰ç”¨ä¿¡æ¯ã€‚åœ¨ä¸åŒç»„ç»‡æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„å®šæ€§è¯„ä¼°ä¹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æ›´å¥½çš„ä¸´åºŠåé¦ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/medcx/PFADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07590v2">PDF</a> 12 pages, 8 figures, AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å™ªå£°ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯å¼•å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´çš„MRIå›¾åƒï¼Œä»¥è§£å†³MRIä¸­çš„è¿åŠ¨ä¼ªå½±é—®é¢˜ã€‚è¯¥æ–¹æ³•è€ƒè™‘è¿åŠ¨ä¼ªå½±ä¸»è¦é›†ä¸­åœ¨kç©ºé—´çš„é«˜é¢‘æˆåˆ†ä¸Šï¼Œåˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ä»¥ç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ã€‚åŒæ—¶ï¼Œè®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œåŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æå–æœ‰ç”¨ä¿¡æ¯ï¼Œä»¥æé«˜å›¾åƒæ¢å¤è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè·å¾—æ”¾å°„ç§‘åŒ»å¸ˆçš„ç§¯æè¯„ä»·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šå¹²æ‰°ä¸´åºŠè¯Šæ–­ï¼Œå»é™¤ä¼ªå½±æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šä¾èµ–é…å¯¹æ•°æ®ï¼Œå¯¹kç©ºé—´é¢‘ç‡æ‰°åŠ¨è€ƒè™‘ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠçš„åº”ç”¨ã€‚</li>
<li>æ–°å‹æ— ç›‘ç£å‡€åŒ–æ–¹æ³•åˆ©ç”¨åƒç´ é¢‘ç‡ä¿¡æ¯å¼•å¯¼é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´MRIå›¾åƒã€‚</li>
<li>æ–¹æ³•é’ˆå¯¹è¿åŠ¨ä¼ªå½±ä¸»è¦é›†ä¸­åœ¨kç©ºé—´é«˜é¢‘æˆåˆ†çš„é—®é¢˜ï¼Œä½¿ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ã€‚</li>
<li>è®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œä»¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å’Œæå–æœ‰ç”¨ä¿¡æ¯ã€‚</li>
<li>å®éªŒåœ¨å¤šä¸ªæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œè·å¾—æ”¾å°„ç§‘åŒ»å¸ˆçš„ç§¯æè¯„ä»·ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0ff7c13591035b978b06d36f1f533ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f99b9342819d5c3237e084dec6be6f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cfd4272ea20d1f177b432a0446e8119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47ef4c87738d300c706dc2cbf4506649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8186300dbbe1c884fae5bbebd2c36e3a.jpg" align="middle">
</details>




<h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, BjÃ¶rn Ommer</strong></p>
<p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks. </p>
<blockquote>
<p>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæœ‰ä¸¤ç§èŒƒå¼åœ¨å„ç§åº”ç”¨ä¸­å—åˆ°äº†å…³æ³¨ï¼šåŸºäºä¸‹ä¸€ä¸ªé›†åˆé¢„æµ‹çš„Masked Generative Modelså’ŒåŸºäºä¸‹ä¸€ä¸ªå™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆä¾‹å¦‚æ‰©æ•£æ¨¡å‹ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹æ¥è¿æ¥å®ƒä»¬ï¼Œå¹¶æ¢ç´¢å®ƒä»¬åœ¨è§†è§‰é¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€çš„è®¾è®¡ç©ºé—´ä¸­å¯¹ä¸¤ç§ç±»å‹çš„æ¨¡å‹è¿›è¡Œäº†é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ã€æ¸©åº¦ã€æŒ‡å¯¼å¼ºåº¦ç­‰ï¼Œä»¥ä¸€ç§å¯æ‰©å±•çš„æ–¹å¼ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†å‰²ï¼‰é‡æ–°å®šä½ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»é®æ©è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œå„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…è®­ç»ƒä¸€æ¬¡æ¥å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå®ç°çµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰ä¸Šè¿°æ¢ç´¢éƒ½å¼•é¢†æˆ‘ä»¬æ„å»ºäº†åä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚ImageNet256ã€MS COCOå’Œè§†é¢‘æ•°æ®é›†FaceForensicsï¼‰ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡åŸºäºç¦»æ•£çŠ¶æ€çš„æ–¹æ³•çš„å…ˆè¿›æ°´å¹³ã€‚æ€»çš„æ¥è¯´ï¼Œé€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œæˆ‘ä»¬å¯ä»¥æ¶èµ·Masked Generativeå’ŒNon-autoregressive Diffusionæ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œä»¥åŠç”Ÿæˆä»»åŠ¡å’Œåˆ¤åˆ«ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v2">PDF</a> Technical Report (WIP), Project Page(code, model, dataset):   <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸¤ç§èŒƒå¼ï¼šåŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„Masked Generativeæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼Œä¾‹å¦‚æ‰©æ•£æ¨¡å‹ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹å°†ä¸¤è€…è”ç³»èµ·æ¥ï¼Œå¹¶åœ¨è§†è§‰é¢†åŸŸæ¢ç´¢å…¶å¯æ‰©å±•æ€§ã€‚é€šè¿‡å¯¹ä¸¤ç§æ¨¡å‹çš„ç»Ÿä¸€è®¾è®¡ç©ºé—´è¿›è¡Œé€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ã€æ¸©åº¦ã€æŒ‡å¯¼å¼ºåº¦ç­‰ï¼Œå¹¶å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»é®æ©è¿‡ç¨‹ã€‚è¿™å¯ç”¨äº†å„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…è®­ç»ƒä¸€æ¬¡æ¥å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œçµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰è¿™äº›æ¢ç´¢éƒ½å¼•é¢†äº†ä¸€ä¸ªåä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ImageNet256ã€MS COCOå’ŒFaceForensicsè§†é¢‘æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æˆ–å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œå¯ä»¥æ¡¥æ¥Masked Generativeå’ŒNon-autoregressive Diffusionæ¨¡å‹ï¼Œä»¥åŠç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å·¥ä½œä¸­æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸¤ç§èŒƒå¼ï¼šMasked Generativeæ¨¡å‹å’ŒNon-Autoregressive Modelsï¼ˆä¾‹å¦‚æ‰©æ•£æ¨¡å‹ï¼‰ã€‚</li>
<li>ä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹å°†è¿™ä¸¤ç§èŒƒå¼è¿æ¥èµ·æ¥ï¼Œå¹¶åœ¨è§†è§‰é¢†åŸŸæ¢ç´¢å…¶å¯æ‰©å±•æ€§ã€‚</li>
<li>åœ¨ç»Ÿä¸€è®¾è®¡ç©ºé—´ä¸­å¯¹ä¸¤ç§æ¨¡å‹è¿›è¡Œé€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ç­‰ã€‚</li>
<li>å°†åˆ¤åˆ«ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»é®æ©è¿‡ç¨‹ï¼Œå®ç°äº†çµæ´»çš„é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†åä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œæ¡¥æ¥äº†ä¸åŒç±»å‹çš„ç”Ÿæˆæ¨¡å‹å’Œä»»åŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-84482f8d037dfd8fca4b36a654af2c7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7bd916bdace6c59e807a319127cb89ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3c9e8e97bdb09e0544dbb86f3d0601f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c35d03a36efcc9698532982aad9a05f.jpg" align="middle">
</details>




<h2 id="ContRail-A-Framework-for-Realistic-Railway-Image-Synthesis-using-ControlNet"><a href="#ContRail-A-Framework-for-Realistic-Railway-Image-Synthesis-using-ControlNet" class="headerlink" title="ContRail: A Framework for Realistic Railway Image Synthesis using   ControlNet"></a>ContRail: A Framework for Realistic Railway Image Synthesis using   ControlNet</h2><p><strong>Authors:Andrei-Robert Alexandrescu, Razvan-Gabriel Petec, Alexandru Manole, Laura-Silvia Diosan</strong></p>
<p>Deep Learning became an ubiquitous paradigm due to its extraordinary effectiveness and applicability in numerous domains. However, the approach suffers from the high demand of data required to achieve the potential of this type of model. An ever-increasing sub-field of Artificial Intelligence, Image Synthesis, aims to address this limitation through the design of intelligent models capable of creating original and realistic images, endeavour which could drastically reduce the need for real data. The Stable Diffusion generation paradigm recently propelled state-of-the-art approaches to exceed all previous benchmarks. In this work, we propose the ContRail framework based on the novel Stable Diffusion model ControlNet, which we empower through a multi-modal conditioning method. We experiment with the task of synthetic railway image generation, where we improve the performance in rail-specific tasks, such as rail semantic segmentation by enriching the dataset with realistic synthetic images. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å› å…¶åœ¨ä¼—å¤šé¢†åŸŸçš„å“è¶Šæ•ˆæœå’Œé€‚ç”¨æ€§è€Œæˆä¸ºæ— å¤„ä¸åœ¨çš„èŒƒä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®æ¥å®ç°è¯¥ç±»å‹æ¨¡å‹çš„æ½œåŠ›ã€‚äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ—¥ç›Šå¢é•¿çš„å­é¢†åŸŸâ€”â€”å›¾åƒåˆæˆï¼Œæ—¨åœ¨é€šè¿‡è®¾è®¡èƒ½å¤Ÿåˆ›å»ºåŸåˆ›å’Œé€¼çœŸå›¾åƒçš„æ™ºèƒ½æ¨¡å‹æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä»è€Œå¤§å¹…å‡å°‘å¯¹çœŸå®æ•°æ®çš„éœ€æ±‚ã€‚æœ€è¿‘ï¼ŒStable Diffusionç”ŸæˆèŒƒå¼æ¨åŠ¨äº†æœ€å…ˆè¿›çš„æ–¹æ³•è¶…è¶Šäº†æ‰€æœ‰ä¹‹å‰çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ–°å‹Stable Diffusionæ¨¡å‹ControlNetçš„ContRailæ¡†æ¶ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€æ¡ä»¶æ–¹æ³•å¯¹å…¶è¿›è¡Œèµ‹èƒ½ã€‚æˆ‘ä»¬å°è¯•é“è·¯å›¾åƒåˆæˆä»»åŠ¡ï¼Œé€šè¿‡ç”¨é€¼çœŸçš„åˆæˆå›¾åƒä¸°å¯Œæ•°æ®é›†ï¼Œæ”¹è¿›é“è·¯ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é“è·¯è¯­ä¹‰åˆ†å‰²ï¼‰çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06742v2">PDF</a> 9 pages, 5 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ å› å…¶è·¨å¤šä¸ªé¢†åŸŸçš„å‡ºè‰²æ•ˆæœå’Œé€‚ç”¨æ€§è€Œæˆä¸ºæ— å¤„ä¸åœ¨çš„èŒƒä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯¹æ•°æ®çš„éœ€æ±‚æé«˜ï¼Œé™åˆ¶äº†å…¶æ½œåŠ›ã€‚å›¾åƒåˆæˆä½œä¸ºäººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ—¥ç›Šå¢é•¿çš„å­é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡è®¾è®¡èƒ½å¤Ÿåˆ›å»ºåŸå§‹å’Œé€¼çœŸå›¾åƒçš„æ™ºèƒ½æ¨¡å‹æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚æœ€è¿‘çš„Stable Diffusionç”ŸæˆèŒƒå¼æ¨åŠ¨æœ€å…ˆè¿›çš„æŠ€æœ¯è¶…è¶Šäº†æ‰€æœ‰å…ˆå‰çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºæ–°å‹çš„Stable Diffusionæ¨¡å‹ControlNetæå‡ºContRailæ¡†æ¶ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ¡ä»¶æ–¹æ³•å¢å¼ºå…¶èƒ½åŠ›ã€‚æˆ‘ä»¬å°è¯•é“è·¯å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡ä¸°å¯Œæ•°æ®é›†ä¸é€¼çœŸçš„åˆæˆå›¾åƒï¼Œæé«˜äº†é“è·¯ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é“è·¯è¯­ä¹‰åˆ†å‰²ï¼‰çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å·²æˆä¸ºå¤šä¸ªé¢†åŸŸçš„æ ‡å‡†èŒƒä¾‹ï¼Œä½†å…¶å¯¹æ•°æ®çš„é«˜éœ€æ±‚é™åˆ¶äº†å…¶æ½œåŠ›ã€‚</li>
<li>å›¾åƒåˆæˆå­é¢†åŸŸæ—¨åœ¨é€šè¿‡æ™ºèƒ½æ¨¡å‹åˆ›å»ºé€¼çœŸå›¾åƒæ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>Stable Diffusionç”ŸæˆèŒƒå¼å·²æˆä¸ºæœ€å…ˆè¿›çš„å›¾åƒç”ŸæˆæŠ€æœ¯ã€‚</li>
<li>ContRailæ¡†æ¶åŸºäºControlNetæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¡ä»¶æ–¹æ³•å¢å¼ºæ€§èƒ½ã€‚</li>
<li>åœ¨é“è·¯å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒContRailæ¡†æ¶é€šè¿‡ä¸°å¯Œæ•°æ®é›†ä¸åˆæˆå›¾åƒæé«˜äº†é“è·¯ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é“è·¯è¯­ä¹‰åˆ†å‰²ï¼‰çš„æ€§èƒ½ã€‚</li>
<li>ContRailæ¡†æ¶å¯èƒ½æœ‰åŠ©äºå‡å°‘çœŸå®æ•°æ®çš„éœ€æ±‚ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé€‚ç”¨æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-adb625fd18a5cadb69b96ca71f2d670d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-282f3655f025832fd0b8255d6c97dd81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ccdb569fc2c3bd7448fa2740bc75912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0988c5696c2b634669b5ff8560f75c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53710ab7cf073314c03f5c6f8e24e722.jpg" align="middle">
</details>




<h2 id="Open-Source-Acceleration-of-Stable-Diffusion-cpp"><a href="#Open-Source-Acceleration-of-Stable-Diffusion-cpp" class="headerlink" title="Open-Source Acceleration of Stable-Diffusion.cpp"></a>Open-Source Acceleration of Stable-Diffusion.cpp</h2><p><strong>Authors:Jingxu Ng, Cheng Lv, Pu Zhao, Wei Niu, Juyi Lin, Minzhou Pan, Yun Liang, Yanzhi Wang</strong></p>
<p>Stable diffusion plays a crucial role in generating high-quality images. However, image generation is time-consuming and memory-intensive. To address this, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate the diffusion models. Although it is lightweight, the current implementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference latency and massive memory usage. To address this, in this work, we present an optimized version of Sdcpp leveraging the Winograd algorithm to accelerate 2D convolution operations, which is the primary bottleneck in the pipeline. By analyzing both dependent and independent computation graphs, we exploit the deviceâ€™s locality and parallelism to achieve substantial performance improvements. Our framework delivers correct end-to-end results across various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and SDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for individual convolutional layers and an inference speedup up to 4.79x for the overall image generation process, compared with the original Sdcpp on M1 pro. Homepage: <a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a> </p>
<blockquote>
<p>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œå›¾åƒç”Ÿæˆæ˜¯è€—æ—¶ä¸”å†…å­˜å¯†é›†å‹çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œstable-diffusion.cppï¼ˆSdcppï¼‰ä½œä¸ºä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†æ¡†æ¶åº”è¿è€Œç”Ÿï¼Œä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚è™½ç„¶å®ƒå¾ˆè½»ä¾¿ï¼Œä½†Sdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å¹¶ä¸ç†æƒ³ï¼Œå­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜å’Œå†…å­˜ä½¿ç”¨é‡å¤§çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­æå‡ºäº†ä¸€ä¸ªä¼˜åŒ–ç‰ˆçš„Sdcppï¼Œå®ƒåˆ©ç”¨Winogradç®—æ³•åŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè¿™æ˜¯ç®¡é“ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚é€šè¿‡åˆ†ææœ‰å‘å’Œæ— å‘è®¡ç®—å›¾ï¼Œæˆ‘ä»¬åˆ©ç”¨è®¾å¤‡çš„å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­éƒ½èƒ½æä¾›æ­£ç¡®çš„ç«¯åˆ°ç«¯ç»“æœï¼ŒåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹Sdcppåœ¨M1 Proä¸Šçš„è¡¨ç°ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•ä¸ªå·ç§¯å±‚ä¸Šå®ç°äº†æœ€é«˜è¾¾2.76å€çš„é€Ÿåº¦æå‡ï¼Œæ•´ä½“å›¾åƒç”Ÿæˆè¿‡ç¨‹å®ç°äº†æœ€é«˜è¾¾4.79å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05781v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½†å›¾åƒç”Ÿæˆè€—æ—¶ä¸”å ç”¨å¤§é‡å†…å­˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå‡ºç°äº†stable-diffusion.cppï¼ˆSdcppï¼‰è¿™ä¸€é«˜æ•ˆæ¨ç†æ¡†æ¶æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼ŒSdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å¹¶ä¸ç†æƒ³ï¼Œå­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜å’Œå†…å­˜ä½¿ç”¨é‡å¤§ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨Winogradç®—æ³•ä¼˜åŒ–Sdcppç‰ˆæœ¬ï¼ŒåŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè¿™æ˜¯ç®¡é“ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚é€šè¿‡åˆ†ææœ‰ä¾èµ–æ€§å’Œæ— ä¾èµ–æ€§çš„è®¡ç®—å›¾ï¼Œæˆ‘ä»¬åˆ©ç”¨è®¾å¤‡çš„å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶å¯åœ¨å„ç§ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼ˆåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboï¼‰ä¸­æä¾›æ­£ç¡®çš„ç«¯åˆ°ç«¯ç»“æœã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸åŸå§‹Sdcppç›¸æ¯”ï¼Œå•ä¸ªå·ç§¯å±‚çš„é€Ÿåº¦æå‡å¯è¾¾2.76å€ï¼Œæ•´ä½“å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ¨ç†é€Ÿåº¦æå‡å¯è¾¾4.79å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­å¾ˆé‡è¦ï¼Œä½†å­˜åœ¨æ—¶é—´å’Œå†…å­˜æ¶ˆè€—å¤§çš„é—®é¢˜ã€‚</li>
<li>stable-diffusion.cppï¼ˆSdcppï¼‰æ¡†æ¶æ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>Sdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>åˆ©ç”¨Winogradç®—æ³•ä¼˜åŒ–Sdcppï¼ŒåŠ é€Ÿ2Då·ç§¯æ“ä½œã€‚</li>
<li>é€šè¿‡åˆ†æè®¡ç®—å›¾ï¼Œåˆ©ç”¨è®¾å¤‡å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§æå‡æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–çš„æ¡†æ¶é€‚ç”¨äºå¤šç§ç¨³å®šæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ä¼˜åŒ–æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦å’Œå›¾åƒç”Ÿæˆæ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54c9fe172a83be87335a9ebbdb49b5c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7668f911018f7c5da7612ca3bc0e829f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01b444daef29cdf097fb454de4f91021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc717995bf69c5de91eae7f5cbe3947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c26c7d226f869f76c1a7c3096970423.jpg" align="middle">
</details>




<h2 id="BudgetFusion-Perceptually-Guided-Adaptive-Diffusion-Models"><a href="#BudgetFusion-Perceptually-Guided-Adaptive-Diffusion-Models" class="headerlink" title="BudgetFusion: Perceptually-Guided Adaptive Diffusion Models"></a>BudgetFusion: Perceptually-Guided Adaptive Diffusion Models</h2><p><strong>Authors:Qinchan Li, Kenneth Chen, Changyue Su, Qi Sun</strong></p>
<p>Diffusion models have shown unprecedented success in the task of text-to-image generation. While these models are capable of generating high-quality and realistic images, the complexity of sequential denoising has raised societal concerns regarding high computational demands and energy consumption. In response, various efforts have been made to improve inference efficiency. However, most of the existing efforts have taken a fixed approach with neural network simplification or text prompt optimization. Are the quality improvements from all denoising computations equally perceivable to humans? We observed that images from different text prompts may require different computational efforts given the desired content. The observation motivates us to present BudgetFusion, a novel model that suggests the most perceptually efficient number of diffusion steps before a diffusion model starts to generate an image. This is achieved by predicting multi-level perceptual metrics relative to diffusion steps. With the popular Stable Diffusion as an example, we conduct both numerical analyses and user studies. Our experiments show that BudgetFusion saves up to five seconds per prompt without compromising perceptual similarity. We hope this work can initiate efforts toward answering a core question: how much do humans perceptually gain from images created by a generative model, per watt of energy? </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚è™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œé€¼çœŸçš„å›¾åƒï¼Œä½†åºåˆ—å»å™ªçš„å¤æ‚æ€§å¼•å‘äº†ç¤¾ä¼šå¯¹æé«˜è®¡ç®—æ•ˆç‡å’Œé™ä½èƒ½æºæ¶ˆè€—çš„å…³æ³¨ã€‚ä½œä¸ºå›åº”ï¼Œäººä»¬å·²ç»åšå‡ºäº†å„ç§åŠªåŠ›æé«˜æ¨ç†æ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°åŠªåŠ›éƒ½é‡‡å–äº†å›ºå®šçš„æ–¹æ³•ï¼Œå¦‚ç¥ç»ç½‘ç»œç®€åŒ–æˆ–æ–‡æœ¬æç¤ºä¼˜åŒ–ã€‚æ¥è‡ªæ‰€æœ‰å»å™ªè®¡ç®—çš„è´¨é‡æå‡æ˜¯å¦å¯¹äººç±»çš„æ„ŸçŸ¥å…·æœ‰åŒç­‰çš„è®¤çŸ¥æ•ˆæœï¼Ÿæˆ‘ä»¬å‘ç°ï¼Œæ ¹æ®æ‰€éœ€å†…å®¹ï¼Œæ¥è‡ªä¸åŒæ–‡æœ¬æç¤ºçš„å›¾åƒå¯èƒ½éœ€è¦ä¸åŒçš„è®¡ç®—å·¥ä½œé‡ã€‚è¿™ä¸€è§‚å¯Ÿä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†BudgetFusionæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ‰©æ•£æ¨¡å‹å¼€å§‹ç”Ÿæˆå›¾åƒä¹‹å‰ï¼Œé¢„æµ‹å‡ºæ„ŸçŸ¥æ•ˆç‡æœ€é«˜çš„æ‰©æ•£æ­¥éª¤æ•°é‡ã€‚è¿™æ˜¯é€šè¿‡é¢„æµ‹ä¸æ‰©æ•£æ­¥éª¤ç›¸å…³çš„å¤šçº§æ„ŸçŸ¥æŒ‡æ ‡æ¥å®ç°çš„ã€‚ä»¥æµè¡Œçš„Stable Diffusionä¸ºä¾‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ•°å€¼åˆ†æå’Œç”¨æˆ·ç ”ç©¶ã€‚å®éªŒè¡¨æ˜ï¼ŒBudgetFusionèƒ½åœ¨ä¸æŸå®³æ„ŸçŸ¥ç›¸ä¼¼æ€§çš„æƒ…å†µä¸‹ï¼Œä¸ºæ¯ä¸ªæç¤ºèŠ‚çœå¤šè¾¾äº”ç§’çš„æ—¶é—´ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿå¼•å‘å¯¹æ ¸å¿ƒé—®é¢˜çš„æ€è€ƒï¼šäººç±»ä»æ¯ç“¦ç‰¹èƒ½é‡æ‰€ç”Ÿæˆçš„å›¾åƒä¸­è·å¾—å¤šå°‘æ„ŸçŸ¥æ”¶ç›Šï¼Ÿ</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05780v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºå…¶å¤æ‚çš„å»å™ªè¿‡ç¨‹ï¼Œç¤¾ä¼šå¯¹é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—è¡¨ç¤ºæ‹…å¿§ã€‚ä¸ºæé«˜æ¨ç†æ•ˆç‡ï¼Œå·²åšå‡ºå¤šç§åŠªåŠ›ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é€šè¿‡ç®€åŒ–ç¥ç»ç½‘ç»œæˆ–ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥å®ç°ã€‚äººç±»æ˜¯å¦å¯¹æ‰€æœ‰çš„å»å™ªè®¡ç®—æ”¹è¿›æœ‰åŒç­‰çš„æ„ŸçŸ¥æ•ˆæœï¼Ÿç ”ç©¶å‘ç°ï¼Œä¸åŒçš„æ–‡æœ¬æç¤ºå¯èƒ½å› æ‰€éœ€å†…å®¹ä¸åŒè€Œéœ€è¦ä¸åŒçš„è®¡ç®—é‡ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†BudgetFusionæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å»ºè®®åœ¨æ‰©æ•£æ¨¡å‹å¼€å§‹ç”Ÿæˆå›¾åƒä¹‹å‰ï¼Œä»¥æœ€ç¬¦åˆäººç±»æ„ŸçŸ¥æ•ˆç‡çš„æ–¹å¼æ‰§è¡Œä¸åŒæ•°é‡çš„æ‰©æ•£æ­¥éª¤ã€‚è¿™æ˜¯é€šè¿‡é¢„æµ‹ä¸æ‰©æ•£æ­¥éª¤ç›¸å…³çš„å¤šçº§æ„ŸçŸ¥åº¦é‡æ¥å®ç°çš„ã€‚ä»¥æµè¡Œçš„Stable Diffusionä¸ºä¾‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ•°å€¼åˆ†æå’Œç”¨æˆ·ç ”ç©¶ã€‚å®éªŒè¡¨æ˜ï¼ŒBudgetFusionå¯ä»¥åœ¨ä¸æŸå®³æ„ŸçŸ¥ç›¸ä¼¼æ€§çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæç¤ºèŠ‚çœå¤šè¾¾äº”ç§’æ—¶é—´ã€‚å¸Œæœ›é€šè¿‡è¿™é¡¹å·¥ä½œèƒ½å¼•å‘å›ç­”ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜çš„åŠªåŠ›ï¼šäººç±»ä»ç”Ÿæˆæ¨¡å‹åˆ›å»ºçš„å›¾åƒä¸­æ„ŸçŸ¥åˆ°çš„æ”¶ç›Šä¸å…¶æ¶ˆè€—çš„æ¯ç“¦èƒ½é‡ç›¸æ¯”æ˜¯å¤šå°‘ï¼Ÿ</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—è¾ƒé«˜ã€‚</li>
<li>æé«˜æ¨ç†æ•ˆç‡æ˜¯å½“å‰çš„ç ”ç©¶é‡ç‚¹ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç¥ç»ç½‘ç»œç®€åŒ–å’Œæ–‡æœ¬æç¤ºä¼˜åŒ–ä¸Šã€‚</li>
<li>äººç±»å¯¹ä¸åŒå»å™ªè®¡ç®—æ”¹è¿›çš„æ„ŸçŸ¥æ•ˆæœå¯èƒ½ä¸åŒï¼Œä¸åŒæ–‡æœ¬æç¤ºå¯èƒ½éœ€è¦ä¸åŒçš„è®¡ç®—é‡ã€‚</li>
<li>BudgetFusionæ¨¡å‹å»ºè®®æœ€ç¬¦åˆäººç±»æ„ŸçŸ¥æ•ˆç‡çš„å»å™ªæ­¥éª¤æ•°é‡ï¼Œä»¥æé«˜å›¾åƒç”Ÿæˆæ•ˆç‡ã€‚</li>
<li>BudgetFusionä»¥Stable Diffusionä¸ºä¾‹ï¼Œé€šè¿‡æ•°å€¼åˆ†æå’Œç”¨æˆ·ç ”ç©¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>BudgetFusionå¯èŠ‚çœå¤§é‡æ—¶é—´ï¼ŒåŒæ—¶ä¸æŸå®³å›¾åƒçš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62dd551fde641da35ac8bd62ae792887.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d98bbdcfecab39556914f20b9d119057.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c02379e08453839fbac5c683c142ad5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a0a4b59a78298e531cffcecf101070.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c93c9d5af7bcc548ea49315deedd0a0.jpg" align="middle">
</details>




<h2 id="Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images"><a href="#Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images" class="headerlink" title="Hidden in the Noise: Two-Stage Robust Watermarking for Images"></a>Hidden in the Noise: Two-Stage Robust Watermarking for Images</h2><p><strong>Authors:Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</strong></p>
<p>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion modelâ€™s initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆå™¨çš„è´¨é‡ä¸æ–­æé«˜ï¼Œæ·±åº¦ä¼ªé€ æŠ€æœ¯æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°å…è®¸è´Ÿè´£ä»»çš„æ¨¡å‹æ‰€æœ‰è€…æ£€æµ‹å’Œæ ‡è®°å…¶AIç”Ÿæˆçš„å†…å®¹ï¼Œä»è€Œå‡è½»æ½œåœ¨çš„ä¼¤å®³ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ°´å°åµŒå…¥æŠ€æœ¯ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»çš„å½±å“ã€‚è¿™ç§è„†å¼±æ€§éƒ¨åˆ†æ˜¯å› ä¸ºæ°´å°ä¼šæ‰­æ›²ç”Ÿæˆçš„å›¾åƒçš„åˆ†å¸ƒï¼Œä»è€Œæ— æ„ä¸­æ³„éœ²æœ‰å…³æ°´å°æŠ€æœ¯çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°çš„æ— å¤±çœŸæ°´å°åµŒå…¥æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ£€æµ‹æ°´å°éœ€è¦å¯¹æ¯”å›¾åƒé‡å»ºçš„åˆå§‹å™ªå£°ä¸æ‰€æœ‰ä¹‹å‰ä½¿ç”¨çš„åˆå§‹å™ªå£°ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆæ£€æµ‹çš„ä¸¤é˜¶æ®µæ°´å°æ¡†æ¶ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼ä¸åˆå§‹å™ªå£°ç›¸ç»“åˆï¼ŒåµŒå…¥æœ‰å…³æˆ‘ä»¬æ‰€ä½¿ç”¨çš„åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚å¯¹äºæ£€æµ‹ï¼Œæˆ‘ä»¬ï¼ˆiï¼‰æ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œï¼ˆiiï¼‰åœ¨ç»™å®šç»„å†…æœç´¢å¯èƒ½ä¸æˆ‘ä»¬çš„å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¿™ç§æ°´å°æ–¹æ³•è¾¾åˆ°äº†å¯¹æŠ—å¤§é‡æ”»å‡»çš„ä¼ªé€ å’Œç§»é™¤çš„æœ€æ–°ç¨³å¥æ€§æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04653v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£å›¾åƒç”Ÿæˆå™¨è´¨é‡ä¸æ–­æå‡ï¼Œæ·±åº¦ä¼ªé€ æŠ€æœ¯å¼•å‘äº†ç¤¾ä¼šçƒ­è®®ã€‚å›¾åƒæ°´å°æŠ€æœ¯èƒ½è®©æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶AIç”Ÿæˆå†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡æ³¨ï¼Œä»è€Œå‡è½»æ½œåœ¨å±å®³ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ°´å°æ–¹æ³•ä»æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— å¤±çœŸå›¾åƒæ°´å°æ–¹æ³•ï¼Œä½†æ£€æµ‹æ°´å°éœ€è¦å¯¹æ¯”å›¾åƒé‡å»ºçš„åˆå§‹å™ªå£°ä¸æ‰€æœ‰å·²ä½¿ç”¨çš„åˆå§‹å™ªå£°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ°´å°æ£€æµ‹æ¡†æ¶ä»¥æé«˜æ•ˆç‡ã€‚ç”Ÿæˆé˜¶æ®µæˆ‘ä»¬é€šè¿‡åµŒå…¥ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼å¢å¼ºåˆå§‹å™ªå£°ä»¥æºå¸¦ä¿¡æ¯ï¼Œæ£€æµ‹é˜¶æ®µåˆ™é¦–å…ˆæ‰¾åˆ°ç›¸å…³å™ªå£°ç»„å¹¶åœ¨è¯¥ç»„å†…æœç´¢åŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¯¥æ–¹æ³•åœ¨æ°´å°é˜²ä¼ªé€ å’Œé˜²ç§»é™¤æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆå™¨è´¨é‡æå‡å¼•å‘æ·±åº¦ä¼ªé€ æŠ€æœ¯ç¤¾ä¼šå…³æ³¨ã€‚</li>
<li>å›¾åƒæ°´å°æŠ€æœ¯å…è®¸æ¨¡å‹æ‰€æœ‰è€…æ ‡æ³¨AIç”Ÿæˆå†…å®¹ä»¥å‡è½»æ½œåœ¨å±å®³ã€‚</li>
<li>å½“å‰æ°´å°æ–¹æ³•æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— å¤±çœŸå›¾åƒæ°´å°æ–¹æ³•ã€‚</li>
<li>æ£€æµ‹æ°´å°éœ€è¦å¯¹æ¯”å›¾åƒé‡å»ºçš„åˆå§‹å™ªå£°ä¸æ‰€æœ‰å·²ä½¿ç”¨çš„åˆå§‹å™ªå£°ï¼Œå­˜åœ¨æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºä¸¤é˜¶æ®µæ°´å°æ£€æµ‹æ¡†æ¶ä»¥æé«˜æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28e32e7c0b9e5d8e4a894413678d703d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-386598134522fd952f67c0570a8317c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cb61476f17701a921b96d15e5003ab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe0e04f2a8d7e7051c13900ddba61f73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d15147d40d07fa34ccfd53c158d89ad.jpg" align="middle">
</details>




<h2 id="Taming-Diffusion-Prior-for-Image-Super-Resolution-with-Domain-Shift-SDEs"><a href="#Taming-Diffusion-Prior-for-Image-Super-Resolution-with-Domain-Shift-SDEs" class="headerlink" title="Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs"></a>Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs</h2><p><strong>Authors:Qinpeng Cui, Yixuan Liu, Xinyi Zhang, Qiqi Bao, Qingmin Liao, Li Wang, Tian Lu, Zicheng Liu, Zhongdao Wang, Emad Barsoum</strong></p>
<p>Diffusion-based image super-resolution (SR) models have attracted substantial interest due to their powerful image restoration capabilities. However, prevailing diffusion models often struggle to strike an optimal balance between efficiency and performance. Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency. In this paper, we present DoSSR, a Domain Shift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images. At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models. This integration not only improves the use of diffusion prior but also boosts inference efficiency. Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoS-SDEs. This advancement leads to the fast and customized solvers that further enhance sampling efficiency. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on synthetic and real-world datasets, while notably requiring only 5 sampling steps. Compared to previous diffusion prior based methods, our approach achieves a remarkable speedup of 5-7 times, demonstrating its superior efficiency. Code: <a target="_blank" rel="noopener" href="https://github.com/QinpengCui/DoSSR">https://github.com/QinpengCui/DoSSR</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ¨¡å‹å› å…¶å¼ºå¤§çš„å›¾åƒæ¢å¤èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œæµè¡Œçš„æ‰©æ•£æ¨¡å‹å¾€å¾€éš¾ä»¥åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚é€šå¸¸ï¼Œå®ƒä»¬è¦ä¹ˆå¿½è§†åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›ï¼Œé™åˆ¶äº†å…¶ç”Ÿæˆèƒ½åŠ›ï¼Œè¦ä¹ˆéœ€è¦ä»éšæœºå™ªå£°å¼€å§‹å¤šæ¬¡å‰å‘ä¼ é€’ï¼Œä»è€Œå½±å“æ¨ç†æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DoSSRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŸŸè¿ç§»çš„æ‰©æ•£SRæ¨¡å‹ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡ä»¥ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒå¼€å§‹æ‰©æ•£è¿‡ç¨‹æ¥æ˜¾è‘—æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸŸè¿ç§»æ–¹ç¨‹ï¼Œå®ƒèƒ½ä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ— ç¼é›†æˆã€‚è¿™ç§é›†æˆä¸ä»…æ”¹å–„äº†æ‰©æ•£ä¼˜å…ˆçš„ä½¿ç”¨ï¼Œè¿˜æé«˜äº†æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç¦»æ•£è¿ç§»è¿‡ç¨‹è½¬å˜ä¸ºè¿ç»­å…¬å¼ï¼Œå³DoS-SDEsï¼Œè¿›ä¸€æ­¥æ¨è¿›äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚è¿™ç§è¿›å±•å¯¼è‡´äº†å¿«é€Ÿã€å®šåˆ¶çš„æ±‚è§£å™¨ï¼Œè¿›ä¸€æ­¥æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…éœ€è¦5ä¸ªé‡‡æ ·æ­¥éª¤ã€‚ä¸ä¹‹å‰åŸºäºæ‰©æ•£ä¼˜å…ˆçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†5-7å€çš„æ˜¾è‘—åŠ é€Ÿï¼Œè¯æ˜äº†å…¶å“è¶Šçš„æ•ˆç‡ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/QinpengCui/DoSSR%E3%80%82">https://github.com/QinpengCui/DoSSRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17778v2">PDF</a> This paper is accepted by NeurIPS 2024</p>
<p><strong>Summary</strong><br>     åŸŸè½¬ç§»æ‰©æ•£æ¨¡å‹ï¼ˆDoSSRï¼‰å®ç°äº†åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼Œåˆ©ç”¨äº†é¢„è®­ç»ƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å¹¶æ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œé€šè¿‡å¯¹ä½åˆ†è¾¨ç‡å›¾åƒå¯åŠ¨æ‰©æ•£è¿‡ç¨‹ã€‚æ–°æ–¹æ³•é€šè¿‡è¿ç»­åŒ–ç¦»æ•£è½¬ç§»è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…éœ€è¦5ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä¸ä¹‹å‰åŸºäºæ‰©æ•£å…ˆéªŒçš„æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†5-7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å›¾åƒæ¢å¤èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´éš¾ä»¥è¾¾åˆ°å¹³è¡¡ã€‚</li>
<li>DoSSRæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜äº†æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>DoSSRé€šè¿‡åŸŸè½¬ç§»æ–¹ç¨‹ä¸ç°æœ‰æ‰©æ•£æ¨¡å‹æ— ç¼é›†æˆï¼Œæ”¹è¿›äº†æ‰©æ•£å…ˆéªŒçš„ä½¿ç”¨å¹¶æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
<li>DoSSRå°†ç¦»æ•£è½¬ç§»è¿‡ç¨‹è½¬åŒ–ä¸ºè¿ç»­å½¢å¼ï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼ŒDoSSRåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>DoSSRä»…éœ€5ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œç›¸æ¯”ä¹‹å‰çš„æ‰©æ•£å…ˆéªŒæ–¹æ³•ï¼Œé€Ÿåº¦æé«˜äº†5-7å€ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ab597b072942a2f55a19e6e0a704aef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac555712eeffad8844e29c37a78c10cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0584b3ccb14660910e8031cb904ec4c7.jpg" align="middle">
</details>




<h2 id="DeCLIP-Decoding-CLIP-representations-for-deepfake-localization"><a href="#DeCLIP-Decoding-CLIP-representations-for-deepfake-localization" class="headerlink" title="DeCLIP: Decoding CLIP representations for deepfake localization"></a>DeCLIP: Decoding CLIP representations for deepfake localization</h2><p><strong>Authors:Stefan Smeu, Elisabeta Oneata, Dan Oneata</strong></p>
<p>Generative models can create entirely new images, but they can also partially modify real images in ways that are undetectable to the human eye. In this paper, we address the challenge of automatically detecting such local manipulations. One of the most pressing problems in deepfake detection remains the ability of models to generalize to different classes of generators. In the case of fully manipulated images, representations extracted from large self-supervised models (such as CLIP) provide a promising direction towards more robust detectors. Here, we introduce DeCLIP, a first attempt to leverage such large pretrained features for detecting local manipulations. We show that, when combined with a reasonably large convolutional decoder, pretrained self-supervised representations are able to perform localization and improve generalization capabilities over existing methods. Unlike previous work, our approach is able to perform localization on the challenging case of latent diffusion models, where the entire image is affected by the fingerprint of the generator. Moreover, we observe that this type of data, which combines local semantic information with a global fingerprint, provides more stable generalization than other categories of generative methods. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆ›å»ºå…¨æ–°çš„å›¾åƒï¼Œä½†å®ƒä»¬ä¹Ÿå¯ä»¥ä»¥äººç±»çœ¼ç›æ— æ³•å¯Ÿè§‰çš„æ–¹å¼éƒ¨åˆ†ä¿®æ”¹çœŸå®å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†è‡ªåŠ¨æ£€æµ‹æ­¤ç±»å±€éƒ¨æ“ä½œæŒ‘æˆ˜çš„é—®é¢˜ã€‚åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­ï¼Œä»ç„¶å­˜åœ¨æ¨¡å‹èƒ½å¦æ¨å¹¿åˆ°ä¸åŒç±»åˆ«ç”Ÿæˆå™¨çš„é—®é¢˜ã€‚åœ¨å®Œå…¨æ“ä½œçš„å›¾åƒæƒ…å†µä¸‹ï¼Œä»å¤§å‹è‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸­æå–çš„è¡¨ç¤ºï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„æ£€æµ‹å™¨æä¾›äº†å¾ˆæœ‰å‰æ™¯çš„æ–¹å‘ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†DeCLIPï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•åˆ©ç”¨æ­¤ç±»å¤§å‹é¢„è®­ç»ƒç‰¹å¾æ¥æ£€æµ‹å±€éƒ¨æ“ä½œã€‚æˆ‘ä»¬è¯æ˜ï¼Œå½“ä¸åˆç†çš„å·ç§¯è§£ç å™¨ç»“åˆæ—¶ï¼Œé¢„è®­ç»ƒçš„è‡ªç›‘ç£è¡¨ç¤ºèƒ½å¤Ÿæ‰§è¡Œå®šä½ä»»åŠ¡å¹¶æ”¹è¿›ç°æœ‰æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ½œæ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ä¸­æ‰§è¡Œå®šä½ä»»åŠ¡ï¼Œæ•´ä¸ªå›¾åƒå—åˆ°ç”Ÿæˆå™¨æŒ‡çº¹çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ­¤ç±»ç»“åˆäº†å±€éƒ¨è¯­ä¹‰ä¿¡æ¯å’Œå…¨å±€æŒ‡çº¹çš„æ•°æ®ï¼Œç›¸æ¯”å…¶ä»–ç±»åˆ«çš„ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ›´ä¸ºç¨³å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08849v2">PDF</a> Accepted at Winter Conference on Applications of Computer Vision   (WACV) 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDeCLIPçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æ¥æ£€æµ‹å›¾åƒä¸­çš„å±€éƒ¨æ“ä½œã€‚é€šè¿‡ç»“åˆå¤§å‹å·ç§¯è§£ç å™¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å®šä½ï¼Œå¹¶æ”¹è¿›å¯¹ç°æœ‰æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜æƒ…å†µä¸‹è¿›è¡Œå®šä½ï¼Œä¸”æ­¤ç§æ•°æ®ç±»å‹ç»“åˆäº†å±€éƒ¨è¯­ä¹‰ä¿¡æ¯å’Œå…¨å±€æŒ‡çº¹ï¼Œæä¾›äº†æ¯”å…¶ä»–ç±»åˆ«ç”Ÿæˆæ–¹æ³•æ›´ç¨³å®šçš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeCLIPæ–¹æ³•åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾è¿›è¡Œå›¾åƒå±€éƒ¨æ“ä½œçš„æ£€æµ‹ã€‚</li>
<li>é€šè¿‡ç»“åˆå·ç§¯è§£ç å™¨ï¼ŒDeCLIPèƒ½å¤Ÿå®ç°å®šä½å¹¶æ”¹è¿›æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DeCLIPæ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜æƒ…å†µä¸‹è¿›è¡Œå®šä½ã€‚</li>
<li>å±€éƒ¨æ“ä½œæ£€æµ‹éœ€è¦åŒæ—¶è€ƒè™‘å±€éƒ¨è¯­ä¹‰ä¿¡æ¯å’Œå…¨å±€æŒ‡çº¹ã€‚</li>
<li>å¤§å‹è‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å›¾åƒæ£€æµ‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ã€‚</li>
<li>DeCLIPæ–¹æ³•åœ¨å¤„ç†ç”Ÿæˆæ¨¡å‹åˆ›å»ºçš„å›¾åƒæ—¶ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-197897f62781de822354c3bd843ecdd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ddbb15164f61d7df567fdf87c01d6a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f29207e00aa799e424f969c405e6320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-564284db12e359511251f8e8a04fea49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a864b883db099cde2b6d1bace0d21000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7921baccd0154010b1f1354bd31bb17.jpg" align="middle">
</details>




<h2 id="Scalable-Autoregressive-Image-Generation-with-Mamba"><a href="#Scalable-Autoregressive-Image-Generation-with-Mamba" class="headerlink" title="Scalable Autoregressive Image Generation with Mamba"></a>Scalable Autoregressive Image Generation with Mamba</h2><p><strong>Authors:Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li</strong></p>
<p>We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mambaâ€™s core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM">https://github.com/hp-l33/AiM</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åŸºäºMambaæ¶æ„çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹AiMã€‚AiMé‡‡ç”¨Mambaè¿™ä¸€æ–°å‹çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå…·æœ‰å¯¹é•¿åºåˆ—è¿›è¡Œçº¿æ€§æ—¶é—´å¤æ‚åº¦å»ºæ¨¡çš„å“è¶Šæ€§èƒ½ï¼Œå–ä»£äº†ARå›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å¸¸ç”¨çš„Transformerï¼Œæ—¨åœ¨å®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•é€šè¿‡å¤šæ–¹å‘æ‰«æä½¿Mambaé€‚åº”å¤„ç†äºŒç»´ä¿¡å·ï¼ŒAiMåˆ™ç›´æ¥ä½¿ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†éœ€è¦å¯¹Mambaè¿›è¡Œå¤§é‡ä¿®æ”¹ä»¥å­¦ä¹ äºŒç»´ç©ºé—´è¡¨ç¤ºçš„éœ€è¦ã€‚æˆ‘ä»¬å¯¹è§†è§‰ç”Ÿæˆä»»åŠ¡è¿›è¡Œäº†ç®€å•è€Œæˆ˜ç•¥æ€§çš„ä¿®æ”¹ï¼Œå……åˆ†åˆ©ç”¨äº†Mambaçš„æ ¸å¿ƒç»“æ„ã€é«˜æ•ˆçš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æä¾›äº†å„ç§è§„æ¨¡çš„AiMæ¨¡å‹ï¼Œå‚æ•°æ•°é‡ä»148Måˆ°1.3Bä¸ç­‰ã€‚åœ¨ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬æœ€å¥½çš„AiMæ¨¡å‹å®ç°äº†FIDä¸º2.21ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰å‚æ•°æ•°é‡ç›¸å½“çš„ARæ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ç«äº‰åŠ›ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„2åˆ°10å€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hp-l33/AiMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12245v3">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºMambaæ¶æ„çš„AiMå›¾åƒç”Ÿæˆæ¨¡å‹å…·æœ‰å‡ºè‰²çš„æ€§èƒ½å’Œçº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œå¯å®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œå¿«é€Ÿæ¨ç†ã€‚é€šè¿‡ç›´æ¥åˆ©ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œæ— éœ€å¯¹Mambaè¿›è¡Œå¤§é‡ä¿®æ”¹å³å¯å­¦ä¹ äºŒç»´ç©ºé—´è¡¨ç¤ºã€‚åŒæ—¶ï¼Œæä¾›ä¸åŒè§„æ¨¡çš„AiMæ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»148Måˆ°1.3Bï¼Œæœ€ä½³æ¨¡å‹åœ¨ImageNet 256*256åŸºå‡†æµ‹è¯•ä¸­FIDä¸º2.21ï¼Œè¶…è¶Šç°æœ‰åŒç±»å‚æ•°è§„æ¨¡çš„ARæ¨¡å‹ï¼Œå¹¶ä¸æ‰©æ•£æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œæ¨ç†é€Ÿåº¦æé«˜2è‡³10å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AiMæ˜¯ä¸€ä¸ªåŸºäºMambaæ¶æ„çš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½å’Œçº¿æ€§æ—¶é—´å¤æ‚åº¦ã€‚</li>
<li>AiMé€šè¿‡ç›´æ¥åˆ©ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œæ— éœ€å¯¹Mambaè¿›è¡Œå¤§é‡ä¿®æ”¹å³å¯å¤„ç†äºŒç»´ä¿¡å·ã€‚</li>
<li>AiMæ¨¡å‹å‚æ•°èŒƒå›´ä»148Måˆ°1.3Bï¼Œå¯åœ¨ä¸åŒè§„æ¨¡ä¸‹æä¾›ã€‚</li>
<li>åœ¨ImageNet 256*256åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ€ä½³AiMæ¨¡å‹çš„FIDè¾¾åˆ°2.21ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>AiMè¶…è¶Šç°æœ‰åŒç±»å‚æ•°è§„æ¨¡çš„ARæ¨¡å‹ï¼Œæ˜¾ç¤ºå…¶æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
<li>AiMä¸æ‰©æ•£æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶æä¾›äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f12290be29c2e4f6dffc3b385d4360ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7081497a5e348adb3d9b8e58da6baf6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52b010c7b40b64ac0c995f5595fca100.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3690e88e538007c181f769678283c6e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9315d9a228dce5cde9174681dbb3a684.jpg" align="middle">
</details>




<h2 id="EvolvED-Evolutionary-Embeddings-to-Understand-the-Generation-Process-of-Diffusion-Models"><a href="#EvolvED-Evolutionary-Embeddings-to-Understand-the-Generation-Process-of-Diffusion-Models" class="headerlink" title="EvolvED: Evolutionary Embeddings to Understand the Generation Process of   Diffusion Models"></a>EvolvED: Evolutionary Embeddings to Understand the Generation Process of   Diffusion Models</h2><p><strong>Authors:Vidya Prasad, Hans van Gorp, Christina Humer, Ruud J. G. van Sloun, Anna Vilanova, Nicola Pezzotti</strong></p>
<p>Diffusion models, widely used in image generation, rely on iterative refinement to generate images from noise. Understanding this data evolution is important for model development and interpretability, yet challenging due to its high-dimensional, iterative nature. Prior works often focus on static or instance-level analyses, missing the iterative and holistic aspects of the generative path. While dimensionality reduction can visualize image evolution for few instances, it does preserve the iterative structure. To address these gaps, we introduce EvolvED, a method that presents a holistic view of the iterative generative process in diffusion models. EvolvED goes beyond instance exploration by leveraging predefined research questions to streamline generative space exploration. Tailored prompts aligned with these questions are used to extract intermediate images, preserving iterative context. Targeted feature extractors trace the evolution of key image attribute evolution, addressing the complexity of high-dimensional outputs. Central to EvolvED is a novel evolutionary embedding algorithm that encodes iterative steps while maintaining semantic relations. It enhances the visualization of data evolution by clustering semantically similar elements within each iteration with t-SNE, grouping elements by iteration, and aligning an instanceâ€™s elements across iterations. We present rectilinear and radial layouts to represent iterations and support exploration. We apply EvolvED to diffusion models like GLIDE and Stable Diffusion, demonstrating its ability to provide valuable insights into the generative process. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆï¼Œä¾èµ–äºè¿­ä»£ä¼˜åŒ–ä»å™ªå£°ä¸­ç”Ÿæˆå›¾åƒã€‚äº†è§£æ•°æ®çš„æ¼”åŒ–å¯¹äºæ¨¡å‹å‘å±•å’Œå¯è§£é‡Šæ€§å¾ˆé‡è¦ï¼Œä½†ç”±äºå…¶é«˜ç»´åº¦ã€è¿­ä»£æ€§è´¨ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ—©æœŸçš„ç ”ç©¶ç»å¸¸å…³æ³¨é™æ€æˆ–å®ä¾‹çº§åˆ«çš„åˆ†æï¼Œå¿½ç•¥äº†ç”Ÿæˆè·¯å¾„çš„è¿­ä»£å’Œæ•´ä½“æ–¹é¢ã€‚å°½ç®¡é™ç»´å¯ä»¥å¯è§†åŒ–å°‘æ•°å®ä¾‹çš„å›¾åƒæ¼”åŒ–ï¼Œä½†å®ƒå¹¶ä¸ä¿ç•™è¿­ä»£ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EvolvEDæ–¹æ³•ï¼Œå®ƒä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„è¿­ä»£ç”Ÿæˆè¿‡ç¨‹æä¾›äº†æ•´ä½“è§†å›¾ã€‚EvolvEDè¶…è¶Šäº†å®ä¾‹æ¢ç´¢ï¼Œåˆ©ç”¨é¢„å…ˆå®šä¹‰çš„ç ”ç©¶é—®é¢˜æ¥ç®€åŒ–ç”Ÿæˆç©ºé—´æ¢ç´¢ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜é‡èº«å®šåˆ¶çš„æç¤ºç”¨äºæå–ä¸­é—´å›¾åƒï¼Œä¿ç•™è¿­ä»£ä¸Šä¸‹æ–‡ã€‚ç›®æ ‡ç‰¹å¾æå–å™¨è¿½è¸ªå…³é”®å›¾åƒå±æ€§æ¼”å˜çš„å†ç¨‹ï¼Œè§£å†³é«˜ç»´åº¦è¾“å‡ºçš„å¤æ‚æ€§ã€‚EvolvEDçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹è¿›åŒ–åµŒå…¥ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç»´æŒè¯­ä¹‰å…³ç³»çš„åŒæ—¶ç¼–ç è¿­ä»£æ­¥éª¤ã€‚å®ƒé€šè¿‡t-SNEå°†æ¯ä¸ªè¿­ä»£ä¸­è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å…ƒç´ è¿›è¡Œèšç±»ï¼ŒæŒ‰è¿­ä»£å¯¹å…ƒç´ è¿›è¡Œåˆ†ç»„ï¼Œå¹¶å¯¹é½å„è¿­ä»£çš„å®ä¾‹å…ƒç´ ã€‚æˆ‘ä»¬é‡‡ç”¨ç›´çº¿å’Œå¾„å‘å¸ƒå±€æ¥è¡¨ç¤ºè¿­ä»£å¹¶æ”¯æŒæ¢ç´¢ã€‚æˆ‘ä»¬å°†EvolvEDåº”ç”¨äºGLIDEå’ŒStable Diffusionç­‰æ‰©æ•£æ¨¡å‹ï¼Œå±•ç¤ºäº†å®ƒåœ¨äº†è§£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æœ‰ä»·å€¼è§è§£çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17462v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ä»å™ªå£°ç”Ÿæˆå›¾åƒã€‚äº†è§£æ•°æ®æ¼”åŒ–å¯¹æ¨¡å‹å‘å±•å’Œå¯è§£é‡Šæ€§é‡è¦ï¼Œä½†å› å…¶é«˜ç»´ã€è¿­ä»£æ€§è´¨ï¼ŒæŒ‘æˆ˜è¾ƒå¤§ã€‚ä»¥å‰çš„ç ”ç©¶å¤šå…³æ³¨é™æ€æˆ–å®ä¾‹å±‚é¢çš„åˆ†æï¼Œå¿½ç•¥äº†ç”Ÿæˆè·¯å¾„çš„è¿­ä»£å’Œæ•´ä½“æ–¹é¢ã€‚EvolvEDæ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®¾çš„ç ”ç©¶é—®é¢˜ï¼Œå¯¹æ‰©æ•£æ¨¡å‹çš„è¿­ä»£ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œå…¨é¢è§‚å¯Ÿã€‚é€šè¿‡é’ˆå¯¹æ€§æç¤ºæå–ä¸­é—´å›¾åƒï¼Œä¿ç•™è¿­ä»£ä¸Šä¸‹æ–‡ã€‚ç›®æ ‡ç‰¹å¾æå–å™¨è¿½è¸ªå…³é”®å›¾åƒå±æ€§æ¼”å˜ï¼Œè§£å†³é«˜ç»´è¾“å‡ºçš„å¤æ‚æ€§ã€‚EvolvEDçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹æ¼”åŒ–åµŒå…¥ç®—æ³•ï¼Œå¯åœ¨ç¼–ç è¿­ä»£æ­¥éª¤çš„åŒæ—¶ç»´æŒè¯­ä¹‰å…³ç³»ã€‚è¯¥ç®—æ³•é‡‡ç”¨t-SNEèšç±»ï¼Œå°†è¯­ä¹‰ç›¸ä¼¼çš„å…ƒç´ èšé›†åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œé€šè¿‡å®ä¾‹å…ƒç´ å¯¹é½å…¶è¿­ä»£è¿‡ç¨‹ã€‚æœ¬æ–‡æå‡ºäº†ç›´çº¿å’Œå¾„å‘å¸ƒå±€æ¥ä»£è¡¨è¿­ä»£å¹¶æ”¯æŒæ¢ç´¢ã€‚åº”ç”¨EvolvEDäºGLIDEå’ŒStable Diffusionç­‰æ‰©æ•£æ¨¡å‹ï¼Œè¯æ˜å…¶èƒ½ä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–ä»å™ªå£°ç”Ÿæˆå›¾åƒï¼Œç†è§£å…¶æ•°æ®æ¼”åŒ–å¯¹æ¨¡å‹å‘å±•å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨é™æ€æˆ–å®ä¾‹å±‚é¢çš„åˆ†æï¼Œå¿½ç•¥äº†æ‰©æ•£æ¨¡å‹çš„è¿­ä»£å’Œæ•´ä½“ç”Ÿæˆè·¯å¾„ã€‚</li>
<li>EvolvEDæ–¹æ³•æä¾›å¯¹æ‰©æ•£æ¨¡å‹è¿­ä»£ç”Ÿæˆè¿‡ç¨‹çš„å…¨é¢è§‚å¯Ÿï¼Œåˆ©ç”¨é¢„è®¾ç ”ç©¶é—®é¢˜å’Œé’ˆå¯¹æ€§æç¤ºè¿›è¡Œç©ºé—´æ¢ç´¢ã€‚</li>
<li>ç›®æ ‡ç‰¹å¾æå–å™¨è¿½è¸ªå…³é”®å›¾åƒå±æ€§æ¼”å˜ï¼Œè§£å†³é«˜ç»´æ•°æ®å¤æ‚æ€§ã€‚</li>
<li>EvolvEDé‡‡ç”¨æ–°å‹æ¼”åŒ–åµŒå…¥ç®—æ³•ï¼Œç¼–ç è¿­ä»£æ­¥éª¤åŒæ—¶ç»´æŒè¯­ä¹‰å…³ç³»ï¼Œå¢å¼ºæ•°æ®æ¼”åŒ–çš„å¯è§†åŒ–ã€‚</li>
<li>åº”ç”¨EvolvEDäºGLIDEå’ŒStable Diffusionç­‰æ¨¡å‹ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75566319f5d337bb575cea1f6b3b8f4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1db2ca04ebb2f197b82a761f00091a3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3092c430d36df5f080356ba8cae3c66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d50933f392876def70c174fed3abc58f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7203375e3eb793fbb6a918817e6c17b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46e21923d0cde497725267e88773e7e4.jpg" align="middle">
</details>




<h2 id="Ctrl-X-Controlling-Structure-and-Appearance-for-Text-To-Image-Generation-Without-Guidance"><a href="#Ctrl-X-Controlling-Structure-and-Appearance-for-Text-To-Image-Generation-Without-Guidance" class="headerlink" title="Ctrl-X: Controlling Structure and Appearance for Text-To-Image   Generation Without Guidance"></a>Ctrl-X: Controlling Structure and Appearance for Text-To-Image   Generation Without Guidance</h2><p><strong>Authors:Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou</strong></p>
<p>Recent controllable generation approaches such as FreeControl and Diffusion Self-Guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model. See our project page for an overview of the results: <a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¯æ§ç”Ÿæˆæ–¹æ³•ï¼Œå¦‚FreeControlå’ŒDiffusion Self-Guidanceï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¸¦æ¥äº†ç²¾ç»†çš„æ—¶ç©ºå’Œæ§åˆ¶åŠ›ï¼Œè€Œæ— éœ€è®­ç»ƒè¾…åŠ©æ¨¡å—ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é’ˆå¯¹æ¯ç§ç±»å‹çš„å¾—åˆ†å‡½æ•°ä¼˜åŒ–æ½œåœ¨åµŒå…¥ï¼Œå¹¶å¢åŠ äº†æ‰©æ•£æ­¥éª¤ï¼Œä½¿å¾—ç”Ÿæˆè¿‡ç¨‹å˜å¾—è€—æ—¶ï¼Œå¹¶é™åˆ¶äº†å…¶çµæ´»æ€§å’Œä½¿ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†Ctrl-Xï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºT2Iæ‰©æ•£æ§åˆ¶ç»“æ„å’Œå¤–è§‚çš„ç®€å•æ¡†æ¶ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æŒ‡å¯¼ã€‚Ctrl-Xè®¾è®¡å‰é¦ˆç»“æ„æ§åˆ¶ä»¥å®ç°ä¸ç»“æ„å›¾åƒçš„ç»“æ„å¯¹é½å’Œç”¨æˆ·è¾“å…¥å›¾åƒçš„è¯­ä¹‰æ„ŸçŸ¥å¤–è§‚è½¬ç§»ï¼Œä»¥ä¿ƒè¿›å¤–è§‚è½¬ç§»ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒCtrl-Xåœ¨å„ç§æ¡ä»¶è¾“å…¥å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¸Šçš„æ€§èƒ½ä¼˜è¶Šã€‚ç‰¹åˆ«åœ°ï¼ŒCtrl-Xæ”¯æŒä½¿ç”¨ä»»æ„æ¨¡æ€çš„æ¡ä»¶å›¾åƒè¿›è¡Œæ–°é¢–çš„ç»“æ„å’Œå¤–è§‚æ§åˆ¶ï¼Œä¸ç°æœ‰ä½œå“ç›¸æ¯”å±•ç°å‡ºæ›´é«˜çš„å›¾åƒè´¨é‡å’Œå¤–è§‚è½¬ç§»æ•ˆæœï¼Œå¹¶ä¸ºä»»ä½•T2Iå’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰çš„æ‰©æ•£æ¨¡å‹æä¾›äº†å³æ—¶å³ç”¨çš„åŠŸèƒ½ã€‚æœ‰å…³ç»“æœçš„æ¦‚è¿°ï¼Œè¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07540v2">PDF</a> 22 pages, 17 figures, see project page at   <a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¯æ§ç”Ÿæˆæ–¹æ³•å¦‚FreeControlå’ŒDiffusion Self-Guidanceä¸ºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¸¦æ¥äº†ç²¾ç»†çš„æ—¶ç©ºå’Œå¤–è§‚æ§åˆ¶ï¼Œæ— éœ€è®­ç»ƒè¾…åŠ©æ¨¡å—ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é’ˆå¯¹æ¯ç§ç±»å‹çš„å¾—åˆ†å‡½æ•°ä¼˜åŒ–æ½œåœ¨åµŒå…¥ï¼Œæ‰©æ•£æ­¥éª¤è¾ƒé•¿ï¼Œä½¿å¾—ç”Ÿæˆè¿‡ç¨‹è€—æ—¶ï¼Œä¸”é™åˆ¶äº†å…¶çµæ´»æ€§å’Œä½¿ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºCtrl-Xï¼Œä¸€ä¸ªæ— éœ€é¢å¤–è®­ç»ƒæˆ–æŒ‡å¯¼çš„T2Iæ‰©æ•£æ§åˆ¶æ¡†æ¶ã€‚Ctrl-Xè®¾è®¡å‰é¦ˆç»“æ„æ§åˆ¶ä»¥å®ç°ç»“æ„å›¾åƒçš„å¯¹é½ï¼Œå¹¶è®¾è®¡è¯­ä¹‰æ„ŸçŸ¥çš„å¤–è§‚è½¬ç§»ä»¥ä¿ƒè¿›ä»ç”¨æˆ·è¾“å…¥å›¾åƒè¿›è¡Œå¤–è§‚è½¬ç§»ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒCtrl-Xåœ¨å„ç§æ¡ä»¶è¾“å…¥å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCtrl-Xæ”¯æŒå…·æœ‰ä»»æ„æ¨¡æ€æ¡ä»¶å›¾åƒçš„æ–°é¢–ç»“æ„å’Œå¤–è§‚æ§åˆ¶ï¼Œä¸ç°æœ‰ä½œå“ç›¸æ¯”å±•ç°å‡ºå“è¶Šçš„å›¾ç‰‡è´¨é‡å’Œå¤–è§‚è½¬ç§»æ•ˆæœï¼Œå¹¶ä¸”èƒ½ä¸ºä»»ä½•T2Iå’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹æä¾›å³æ—¶å³ç”¨çš„åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ctrl-Xæ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ§åˆ¶æ¡†æ¶ï¼Œå¯å®ç°ç»“æ„å’Œå¤–è§‚çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>Ctrl-Xä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–æŒ‡å¯¼ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿä¸”çµæ´»çš„æ§åˆ¶ã€‚</li>
<li>Ctrl-Xé€šè¿‡å‰é¦ˆç»“æ„æ§åˆ¶å’Œè¯­ä¹‰æ„ŸçŸ¥çš„å¤–è§‚è½¬ç§»è®¾è®¡ï¼Œå®ç°å¯¹ç»“æ„å›¾åƒçš„å¯¹é½å’Œå¤–è§‚ä»ç”¨æˆ·è¾“å…¥å›¾åƒçš„è½¬ç§»ã€‚</li>
<li>Ctrl-Xåœ¨å¤šç§æ¡ä»¶è¾“å…¥å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>Ctrl-Xæ”¯æŒå…·æœ‰ä»»æ„æ¨¡æ€æ¡ä»¶å›¾åƒçš„æ–°é¢–ç»“æ„å’Œå¤–è§‚æ§åˆ¶ã€‚</li>
<li>Ctrl-Xåœ¨å›¾åƒè´¨é‡å’Œå¤–è§‚è½¬ç§»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c7e485979e9ea6d843b17aa542d313c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b905cfd7c6b5741f6b971f6b709a8c1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69c0ca886edc147424356c0976de6413.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a110dc2f9facee3193a8ddf3a250f813.jpg" align="middle">
</details>




<h2 id="RectifID-Personalizing-Rectified-Flow-with-Anchored-Classifier-Guidance"><a href="#RectifID-Personalizing-Rectified-Flow-with-Anchored-Classifier-Guidance" class="headerlink" title="RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance"></a>RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance</h2><p><strong>Authors:Zhicheng Sun, Zhenhao Yang, Yang Jin, Haozhe Chi, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Yang Song, Kun Gai, Yadong Mu</strong></p>
<p>Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at <a target="_blank" rel="noopener" href="https://github.com/feifeiobama/RectifID">https://github.com/feifeiobama/RectifID</a>. </p>
<blockquote>
<p>å®šåˆ¶æ‰©æ•£æ¨¡å‹ä»¥ä»ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒç”Ÿæˆèº«ä»½ä¿ç•™å›¾åƒæ˜¯ä¸€ä¸ªå¼•äººå…¥èƒœçš„æ–°é—®é¢˜ã€‚æµè¡Œçš„æ–¹æ³•é€šå¸¸éœ€è¦åœ¨å¯¹ç‰¹å®šé¢†åŸŸçš„å¤§é‡å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒä»¥å®ç°èº«ä»½ä¿ç•™ï¼Œè¿™åœ¨ä¸åŒçš„ä½¿ç”¨æƒ…å†µä¸‹ç¼ºä¹çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ†ç±»å™¨æŒ‡å¯¼è¿™ä¸€æ— éœ€è®­ç»ƒçš„æŠ€æœ¯ï¼Œä½¿ç”¨ç°æœ‰åˆ†ç±»å™¨æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæœ€æ–°çš„æ ¡æ­£æµæ¡†æ¶ï¼Œæ™®é€šåˆ†ç±»å™¨æŒ‡å¯¼çš„ä¸»è¦å±€é™æ€§åœ¨äºéœ€è¦ç‰¹æ®Šåˆ†ç±»å™¨ï¼Œå¯ä»¥é€šè¿‡ç®€å•çš„å®šç‚¹è§£å†³ç­–ç•¥æ¥è§£å†³ï¼Œå…è®¸ä½¿ç”¨ç°æˆçš„å›¾åƒé‰´åˆ«å™¨è¿›è¡Œçµæ´»çš„ä¸ªæ€§åŒ–è®¾ç½®ã€‚è€Œä¸”ï¼Œå½“å…¶è§£å†³æ–¹æ¡ˆå›ºå®šåœ¨å‚è€ƒæµè½¨è¿¹ä¸Šæ—¶ï¼Œè¯æ˜å…¶è§£å†³ç¨‹åºæ˜¯ç¨³å®šçš„ï¼Œå¹¶ä¸”æœ‰æ”¶æ•›æ€§çš„ä¿è¯ã€‚æ‰€æ¨å¯¼çš„æ–¹æ³•åœ¨ä¸åŒçš„ç°æˆå›¾åƒé‰´åˆ«å™¨ä¸Šåº”ç”¨äºæ ¡æ­£æµï¼Œä¸ºäººè„¸ã€æ´»ä½“ä¸»ä½“å’ŒæŸäº›ç‰©ä½“ç”Ÿæˆäº†æœ‰åˆ©çš„ä¸ªæ€§åŒ–ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/feifeiobama/RectifID%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/feifeiobama/RectifIDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14677v4">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹æ ¹æ®ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒç”Ÿæˆèº«ä»½ä¿ç•™å›¾åƒæ˜¯ä¸€ä¸ªæ–°çš„ç ”ç©¶é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†æ— è®­ç»ƒçš„åˆ†ç±»å™¨æŒ‡å¯¼æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨ç°æœ‰çš„åˆ†ç±»å™¨æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚è¯¥ç ”ç©¶è§£å†³äº†åŸå§‹åˆ†ç±»å™¨æŒ‡å¯¼æ–¹æ³•çš„ä¸»è¦é™åˆ¶ï¼Œä½¿ç”¨ç®€å•çš„å®šç‚¹è§£å†³æ–¹æ¡ˆï¼Œå¹¶è¯æ˜å…¶åœ¨å‚è€ƒæµè½¨è¿¹é”šå®šä¸‹çš„ç¨³å®šæ€§ï¼Œå¹¶æä¾›äº†æ”¶æ•›ä¿è¯ã€‚è¯¥æ–¹æ³•åœ¨äººè„¸ã€æ´»ä½“ç”Ÿç‰©å’ŒæŸäº›ç‰©ä½“ä¸Šå®ç°äº†æœ‰åˆ©çš„ä¸ªæ€§åŒ–ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒç”Ÿæˆèº«ä»½ä¿ç•™å›¾åƒã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé¦–æ¬¡å°è¯•ä½¿ç”¨æ— è®­ç»ƒçš„åˆ†ç±»å™¨æŒ‡å¯¼æŠ€æœ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>ç ”ç©¶è§£å†³äº†åŸå§‹åˆ†ç±»å™¨æŒ‡å¯¼æ–¹æ³•çš„ä¸»è¦é™åˆ¶ï¼Œä½¿å…¶æ›´åŠ çµæ´»ã€‚</li>
<li>ç ”ç©¶è¯æ˜æ–¹æ³•åœ¨ä½¿ç”¨å‚è€ƒæµè½¨è¿¹æ—¶çš„ç¨³å®šæ€§ï¼Œå¹¶æä¾›æ”¶æ•›ä¿è¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒçš„å¯¹è±¡ï¼ˆå¦‚äººè„¸ã€æ´»ä½“ç”Ÿç‰©å’ŒæŸäº›ç‰©ä½“ï¼‰ä¸Šéƒ½å®ç°äº†è‰¯å¥½çš„ä¸ªæ€§åŒ–ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•çš„å®ç°ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šä¾›å…¬ä¼—æŸ¥é˜…å’Œä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26f9c5ae4b60b66ffa9f62575a833d44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0b616068b083bb24b4588ce378aecb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4e35066b1a9cce3f36fdaa67ae742d3.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a5ffa6182763bc6e4bde526a15db0e11.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
