<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    96 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="DMin-Scalable-Training-Data-Influence-Estimation-for-Diffusion-Models"><a href="#DMin-Scalable-Training-Data-Influence-Estimation-for-Diffusion-Models" class="headerlink" title="DMin: Scalable Training Data Influence Estimation for Diffusion Models"></a>DMin: Scalable Training Data Influence Estimation for Diffusion Models</h2><p><strong>Authors:Huawei Lin, Yingjie Lao, Weijie Zhao</strong></p>
<p>Identifying the training data samples that most influence a generated image is a critical task in understanding diffusion models, yet existing influence estimation methods are constrained to small-scale or LoRA-tuned models due to computational limitations. As diffusion models scale up, these methods become impractical. To address this challenge, we propose DMin (Diffusion Model influence), a scalable framework for estimating the influence of each training data sample on a given generated image. By leveraging efficient gradient compression and retrieval techniques, DMin reduces storage requirements from 339.39 TB to only 726 MB and retrieves the top-k most influential training samples in under 1 second, all while maintaining performance. Our empirical results demonstrate DMin is both effective in identifying influential training samples and efficient in terms of computational and storage requirements. </p>
<blockquote>
<p>è¯†åˆ«å’Œè¯„ä¼°å¯¹ç”Ÿæˆå›¾åƒå½±å“æœ€å¤§çš„è®­ç»ƒæ•°æ®æ ·æœ¬æ˜¯ç†è§£æ‰©æ•£æ¨¡å‹çš„å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œç°æœ‰çš„å½±å“è¯„ä¼°æ–¹æ³•ä»…é™äºå°è§„æ¨¡æˆ–ç»è¿‡LoRAè°ƒæ•´è¿‡çš„æ¨¡å‹ã€‚éšç€æ‰©æ•£æ¨¡å‹çš„æ‰©å±•ï¼Œè¿™äº›æ–¹æ³•å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DMinï¼ˆæ‰©æ•£æ¨¡å‹å½±å“åŠ›ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºä¼°è®¡æ¯ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç»™å®šç”Ÿæˆå›¾åƒçš„å½±å“ã€‚é€šè¿‡åˆ©ç”¨é«˜æ•ˆçš„æ¢¯åº¦å‹ç¼©å’Œæ£€ç´¢æŠ€æœ¯ï¼ŒDMinå°†å­˜å‚¨éœ€æ±‚ä»339.39TBå‡å°‘åˆ°ä»…726MBï¼Œå¹¶åœ¨ä¸åˆ°1ç§’å†…æ£€ç´¢åˆ°å‰kä¸ªæœ€å…·å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒDMinåœ¨è¯†åˆ«æœ‰å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬æ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒåŒæ—¶åœ¨è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚æ–¹é¢ä¹Ÿéå¸¸é«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08637v1">PDF</a> 14 pages, 6 figures, 8 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDMinçš„æ‰©æ•£æ¨¡å‹å½±å“ä¼°è®¡æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ¯ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨é«˜æ•ˆçš„æ¢¯åº¦å‹ç¼©å’Œæ£€ç´¢æŠ€æœ¯ï¼Œå®ç°äº†å¯¹å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„å®ç”¨å½±å“ä¼°è®¡ï¼Œåœ¨å­˜å‚¨éœ€æ±‚æ–¹é¢ä»339.39TBç¼©å‡è‡³ä»…726MBï¼Œå¹¶åœ¨ä¸åˆ°1ç§’å†…æ£€ç´¢åˆ°å‰kä¸ªæœ€å…·å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒDMinåœ¨è¯†åˆ«æœ‰å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬æ–¹é¢æ—¢æœ‰æ•ˆåˆé«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DMinæ¡†æ¶ç”¨äºè¯„ä¼°è®­ç»ƒæ•°æ®æ ·æœ¬å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å½±å“ä¼°è®¡é¢ä¸´è®¡ç®—é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>DMiné€šè¿‡æ¢¯åº¦å‹ç¼©å’Œæ£€ç´¢æŠ€æœ¯å®ç°äº†é«˜æ•ˆçš„å½±å“ä¼°è®¡ã€‚</li>
<li>DMinå°†å­˜å‚¨éœ€æ±‚ä»å¤§é‡TBçº§åˆ«é™ä½åˆ°ä»…MBçº§åˆ«ã€‚</li>
<li>DMinèƒ½åœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…æ£€ç´¢åˆ°æœ€å…·å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜DMinåœ¨è¯†åˆ«å½±å“åŠ›æ ·æœ¬æ–¹é¢æ—¢æœ‰æ•ˆåˆé«˜æ•ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b91c6c92f0168d31ec8753dab1d6e4cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8430fd7a7b4d2e453e4c528a89a370c1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43bf3c416ff96d38aeebeb435464daed.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cfd4f8873a6dfcd15ea8920862474249.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cd5f6f9b1b1c461fd1cdba2a7dfb40b0.jpg" align="middle">
</details>




<h2 id="TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person"><a href="#TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person" class="headerlink" title="TryOffAnyone: Tiled Cloth Generation from a Dressed Person"></a>TryOffAnyone: Tiled Cloth Generation from a Dressed Person</h2><p><strong>Authors:Ioannis Xarchakos, Theodoros Koukopoulos</strong></p>
<p>The fashion industry is increasingly leveraging computer vision and deep learning technologies to enhance online shopping experiences and operational efficiencies. In this paper, we address the challenge of generating high-fidelity tiled garment images essential for personalized recommendations, outfit composition, and virtual try-on systems from photos of garments worn by models. Inspired by the success of Latent Diffusion Models (LDMs) in image-to-image translation, we propose a novel approach utilizing a fine-tuned StableDiffusion model. Our method features a streamlined single-stage network design, which integrates garmentspecific masks to isolate and process target clothing items effectively. By simplifying the network architecture through selective training of transformer blocks and removing unnecessary crossattention layers, we significantly reduce computational complexity while achieving state-of-the-art performance on benchmark datasets like VITON-HD. Experimental results demonstrate the effectiveness of our approach in producing high-quality tiled garment images for both full-body and half-body inputs. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone">https://github.com/ixarchakos/try-off-anyone</a> </p>
<blockquote>
<p>æ—¶å°šäº§ä¸šæ­£è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æå‡åœ¨çº¿è´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³ä»æ¨¡ç‰¹ç©¿ç€çš„æœè£…ç…§ç‰‡ç”Ÿæˆé«˜è´¨é‡å¹³é“ºæœè£…å›¾åƒçš„æŒ‘æˆ˜ï¼Œè¿™äº›å›¾åƒå¯¹äºä¸ªæ€§åŒ–æ¨èã€æœè£…æ­é…å’Œè™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨å›¾åˆ°å›¾ç¿»è¯‘ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¾®è°ƒè¿‡çš„StableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œç»“åˆæœè£…ç‰¹å®šæ©è†œï¼Œæœ‰æ•ˆåœ°éš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°è®­ç»ƒå˜å‹å™¨å—å¹¶åˆ é™¤ä¸å¿…è¦çš„äº¤å‰æ³¨æ„å±‚ï¼Œç®€åŒ–äº†ç½‘ç»œæ¶æ„ï¼Œæˆ‘ä»¬åœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œåœ¨VITON-HDç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…¨èº«å’ŒåŠèº«è¾“å…¥çš„é«˜è´¨é‡å¹³é“ºæœè£…å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/ixarchakos/try-off-anyoneæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08573v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ—¶å°šäº§ä¸šæ­£ç§¯æè¿ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥æå‡çº¿ä¸Šè´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚æœ¬æ–‡èšç„¦ç”Ÿæˆé«˜ä¿çœŸæ‹¼æ¥è¡£ç‰©å›¾åƒçš„æŒ‘æˆ˜ï¼Œè¿™å¯¹äºä¸ªæ€§åŒ–æ¨èã€æ­é…ç»„åˆå’Œè™šæ‹Ÿè¯•è¡£ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç ”ç©¶å›¢é˜Ÿå—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨å›¾åƒç¿»è¯‘é¢†åŸŸçš„æˆåŠŸå¯å‘ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ç²¾ç»†è°ƒæ•´çš„StableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œé€šè¿‡æœè£…ç‰¹å®šæ©è†œæœ‰æ•ˆéš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚é€šè¿‡é€‰æ‹©æ€§è®­ç»ƒå˜å‹å™¨å—å¹¶å»é™¤ä¸å¿…è¦çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œç®€åŒ–ç½‘ç»œæ¶æ„ï¼Œåœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œåœ¨VITON-HDç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¨èº«ä½“å’ŒåŠèº«è¾“å…¥ä¸‹éƒ½èƒ½ç”Ÿæˆé«˜è´¨é‡çš„æ‹¼æ¥è¡£ç‰©å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ—¶å°šäº§ä¸šæ­£åœ¨èåˆè®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ä»¥æå‡åœ¨çº¿è´­ç‰©ä½“éªŒã€‚</li>
<li>ç”Ÿæˆé«˜ä¿çœŸæ‹¼æ¥è¡£ç‰©å›¾åƒæ˜¯æ—¶å°šç§‘æŠ€é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚</li>
<li>æ­¤ç ”ç©¶åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å›¾åƒç¿»è¯‘ä¸­çš„æˆåŠŸï¼Œæå‡ºä¸€ç§åŸºäºStableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•é‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œå¹¶é€šè¿‡æœè£…ç‰¹å®šæ©è†œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚</li>
<li>é€šè¿‡ç®€åŒ–ç½‘ç»œæ¶æ„ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œå®ç°äº†åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜è´¨é‡çš„æ‹¼æ¥è¡£ç‰©å›¾åƒï¼Œé€‚ç”¨äºå…¨èº«ä½“å’ŒåŠèº«è¾“å…¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4858b57b9c75d47132c65f2050bc5fe1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2856b5c51d6af52f31848166ea39e62f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5bd374d5d4a708e65208387522338bb4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3127daa555f76175132b4ef21712c878.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7748cecb9cb068dc6d44eb58141f9a77.jpg" align="middle">
</details>




<h2 id="Learning-Flow-Fields-in-Attention-for-Controllable-Person-Image-Generation"><a href="#Learning-Flow-Fields-in-Attention-for-Controllable-Person-Image-Generation" class="headerlink" title="Learning Flow Fields in Attention for Controllable Person Image   Generation"></a>Learning Flow Fields in Attention for Controllable Person Image   Generation</h2><p><strong>Authors:Zijian Zhou, Shikun Liu, Xiao Han, Haozhe Liu, Kam Woh Ng, Tian Xie, Yuren Cong, Hang Li, Mengmeng Xu, Juan-Manuel PÃ©rez-RÃºa, Aditya Patel, Tao Xiang, Miaojing Shi, Sen He</strong></p>
<p>Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the personâ€™s appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models. </p>
<blockquote>
<p>å¯æ§äººç‰©å›¾åƒç”Ÿæˆçš„ç›®æ ‡æ˜¯ä¾æ®å‚è€ƒå›¾åƒç”Ÿæˆäººç‰©å›¾åƒï¼Œå¹¶å¯¹äººç‰©çš„å¤–è§‚æˆ–å§¿åŠ¿è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚ç„¶è€Œï¼Œå°½ç®¡æ•´ä½“å›¾åƒè´¨é‡å¾ˆé«˜ï¼Œä½†ä¹‹å‰çš„æ–¹æ³•å¾€å¾€ä¼šæ‰­æ›²å‚è€ƒå›¾åƒçš„ç»†èŠ‚çº¹ç†ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›æ‰­æ›²æ˜¯ç”±äºå¯¹å‚è€ƒå›¾åƒä¸­ç›¸åº”åŒºåŸŸçš„å…³æ³¨ä¸è¶³æ‰€å¯¼è‡´çš„ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ æ³¨æ„åŠ›æµåœºï¼ˆLeffaï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜ç¡®å¼•å¯¼ç›®æ ‡æŸ¥è¯¢åœ¨æ³¨æ„åŠ›å±‚å…³æ³¨æ­£ç¡®çš„å‚è€ƒé”®ã€‚å…·ä½“è€Œè¨€ï¼Œè¿™æ˜¯é€šè¿‡åœ¨åŸºäºæ‰©æ•£çš„åŸºçº¿ä¹‹ä¸Šå®ç°æ³¨æ„åŠ›æ˜ å°„çš„æ­£åˆ™åŒ–æŸå¤±æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLeffaåœ¨æ§åˆ¶å¤–è§‚ï¼ˆè™šæ‹Ÿè¯•ç©¿ï¼‰å’Œå§¿åŠ¿ï¼ˆå§¿åŠ¿è½¬æ¢ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨ä¿æŒé«˜å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†ç»†èŠ‚çº¹ç†çš„æ‰­æ›²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜æˆ‘ä»¬çš„æŸå¤±æ¨¡å‹å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯ç”¨äºæé«˜å…¶ä»–æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08486v1">PDF</a> github: <a target="_blank" rel="noopener" href="https://github.com/franciszzj/Leffa">https://github.com/franciszzj/Leffa</a>, demo:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/franciszzj/Leffa">https://huggingface.co/spaces/franciszzj/Leffa</a>, model:   <a target="_blank" rel="noopener" href="https://huggingface.co/franciszzj/Leffa">https://huggingface.co/franciszzj/Leffa</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå‚è€ƒå›¾åƒè¿›è¡Œå¯æ§äººç‰©å›¾åƒç”Ÿæˆçš„ç›®æ ‡æ˜¯åœ¨ç»™å®šå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆäººç‰©å›¾åƒï¼Œå¹¶èƒ½å¤Ÿå¯¹äººç‰©çš„å¤–è§‚æˆ–å§¿æ€è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•å¸¸å¸¸åœ¨ä¿æŒæ•´ä½“å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œä¸¢å¤±äº†å‚è€ƒå›¾åƒä¸­çš„ç»†èŠ‚çº¹ç†ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›å¤±çœŸé—®é¢˜æºäºå¯¹å‚è€ƒå›¾åƒä¸­å¯¹åº”åŒºåŸŸçš„å…³æ³¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ æ³¨æ„åŠ›æµåœºï¼ˆLeffaï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å¯¼ç›®æ ‡æŸ¥è¯¢åœ¨æ³¨æ„åŠ›å±‚å…³æ³¨æ­£ç¡®çš„å‚è€ƒå…³é”®ä¿¡æ¯ï¼Œä»¥æé«˜å¯¹ç»†èŠ‚çš„å…³æ³¨ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™æ˜¯é€šè¿‡åœ¨åŸºäºæ‰©æ•£çš„åŸºçº¿æ¨¡å‹ä¸Šçš„æ³¨æ„åŠ›å›¾å¼•å…¥æ­£åˆ™åŒ–æŸå¤±æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLeffaåœ¨æ§åˆ¶å¤–è§‚ï¼ˆè™šæ‹Ÿè¯•ç©¿ï¼‰å’Œå§¿æ€ï¼ˆå§¿æ€è¿ç§»ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†ç»†èŠ‚å¤±çœŸï¼ŒåŒæ—¶ä¿æŒäº†é«˜å›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„æŸå¤±æ¨¡å‹å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯ä»¥ç”¨äºæé«˜å…¶ä»–æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯æ§äººç‰©å›¾åƒç”Ÿæˆæ—¨åœ¨åŸºäºå‚è€ƒå›¾åƒç”Ÿæˆäººç‰©å›¾åƒï¼Œå¹¶èƒ½å¯¹äººç‰©çš„å¤–è§‚æˆ–å§¿æ€è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸åœ¨ä¿æŒæ•´ä½“å›¾åƒè´¨é‡çš„åŒæ—¶å¿½ç•¥ç»†èŠ‚çº¹ç†ã€‚</li>
<li>ç»†èŠ‚çº¹ç†å¤±çœŸé—®é¢˜æºäºå¯¹å‚è€ƒå›¾åƒä¸­å¯¹åº”åŒºåŸŸçš„å…³æ³¨ä¸è¶³ã€‚</li>
<li>å­¦ä¹ æ³¨æ„åŠ›æµåœºï¼ˆLeffaï¼‰æ–¹æ³•é€šè¿‡å¼•å¯¼ç›®æ ‡æŸ¥è¯¢åœ¨æ³¨æ„åŠ›å±‚å…³æ³¨æ­£ç¡®çš„å‚è€ƒå…³é”®ä¿¡æ¯ï¼Œæé«˜ç»†èŠ‚å…³æ³¨åº¦ã€‚</li>
<li>Leffaé€šè¿‡åŸºäºæ‰©æ•£çš„åŸºçº¿æ¨¡å‹ä¸Šçš„æ³¨æ„åŠ›å›¾å¼•å…¥æ­£åˆ™åŒ–æŸå¤±æ¥å®ç°ã€‚</li>
<li>Leffaåœ¨æ§åˆ¶å¤–è§‚å’Œå§¿æ€æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘ç»†èŠ‚å¤±çœŸï¼ŒåŒæ—¶ä¿æŒé«˜å›¾åƒè´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-54cb028b4daea989345268fd31a8c299.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-69066777cbb33ba1d954ab23046a5510.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3f4644bc452d3ca3cef1d2cfd7b36345.jpg" align="middle">
</details>




<h2 id="InvDiff-Invariant-Guidance-for-Bias-Mitigation-in-Diffusion-Models"><a href="#InvDiff-Invariant-Guidance-for-Bias-Mitigation-in-Diffusion-Models" class="headerlink" title="InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models"></a>InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models</h2><p><strong>Authors:Min Hou, Yueying Wu, Chang Xu, Yu-Hao Huang, Chenxi Bai, Le Wu, Jiang Bian</strong></p>
<p>As one of the most successful generative models, diffusion models have demonstrated remarkable efficacy in synthesizing high-quality images. These models learn the underlying high-dimensional data distribution in an unsupervised manner. Despite their success, diffusion models are highly data-driven and prone to inheriting the imbalances and biases present in real-world data. Some studies have attempted to address these issues by designing text prompts for known biases or using bias labels to construct unbiased data. While these methods have shown improved results, real-world scenarios often contain various unknown biases, and obtaining bias labels is particularly challenging. In this paper, we emphasize the necessity of mitigating bias in pre-trained diffusion models without relying on auxiliary bias annotations. To tackle this problem, we propose a framework, InvDiff, which aims to learn invariant semantic information for diffusion guidance. Specifically, we propose identifying underlying biases in the training data and designing a novel debiasing training objective. Then, we employ a lightweight trainable module that automatically preserves invariant semantic information and uses it to guide the diffusion modelâ€™s sampling process toward unbiased outcomes simultaneously. Notably, we only need to learn a small number of parameters in the lightweight learnable module without altering the pre-trained diffusion model. Furthermore, we provide a theoretical guarantee that the implementation of InvDiff is equivalent to reducing the error upper bound of generalization. Extensive experimental results on three publicly available benchmarks demonstrate that InvDiff effectively reduces biases while maintaining the quality of image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Hundredl/InvDiff">https://github.com/Hundredl/InvDiff</a>. </p>
<blockquote>
<p>ä½œä¸ºæœ€æˆåŠŸçš„ç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ï¼Œæ‰©æ•£æ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ•ˆç‡ã€‚è¿™äº›æ¨¡å‹ä»¥æ— ç›‘ç£çš„æ–¹å¼å­¦ä¹ æ½œåœ¨çš„é«˜ç»´æ•°æ®åˆ†å¸ƒã€‚å°½ç®¡å®ƒä»¬å¾ˆæˆåŠŸï¼Œä½†æ‰©æ•£æ¨¡å‹æ˜¯é«˜åº¦æ•°æ®é©±åŠ¨çš„ï¼Œå¹¶ä¸”å®¹æ˜“ç»§æ‰¿çœŸå®æ•°æ®ä¸­çš„ä¸å¹³è¡¡å’Œåè§ã€‚ä¸€äº›ç ”ç©¶è¯•å›¾é€šè¿‡ä¸ºå·²çŸ¥åè§è®¾è®¡æ–‡æœ¬æç¤ºæˆ–ä½¿ç”¨åè§æ ‡ç­¾æ¥æ„å»ºæ— åè§æ•°æ®æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæ”¹è¿›çš„ç»“æœï¼Œä½†ç°å®ä¸–ç•Œçš„åœºæ™¯é€šå¸¸åŒ…å«å„ç§æœªçŸ¥çš„åè§ï¼Œå¹¶ä¸”è·å–åè§æ ‡ç­¾å…·æœ‰ç‰¹åˆ«æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å‡è½»åè§çš„å¿…è¦æ€§ï¼Œè€Œæ— éœ€ä¾èµ–è¾…åŠ©åè§æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºInvDiffçš„æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ æ‰©æ•£æŒ‡å¯¼çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºè¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨åè§ï¼Œå¹¶è®¾è®¡ä¸€ç§æ–°çš„å»åè®­ç»ƒç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªè½»é‡çº§çš„å¯è®­ç»ƒæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªåŠ¨ä¿ç•™ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥äº§ç”Ÿæ— åçš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨è½»é‡çº§çš„å¯è®­ç»ƒæ¨¡å—ä¸­å­¦ä¹ å°‘é‡çš„å‚æ•°ï¼Œè€Œæ— éœ€æ›´æ”¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜InvDiffçš„å®æ–½ç›¸å½“äºé™ä½äº†æ³›åŒ–çš„è¯¯å·®ä¸Šé™ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒInvDiffåœ¨å‡å°‘åè§çš„åŒæ—¶ä¿æŒäº†å›¾åƒç”Ÿæˆçš„å“è´¨ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hundredl/InvDiff">https://github.com/Hundredl/InvDiff</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08480v1">PDF</a> KDD 2025</p>
<p><strong>Summary</strong><br>    æ‰©æ•£æ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œä½†å­˜åœ¨æ•°æ®é©±åŠ¨å¸¦æ¥çš„åè§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºInvDiffçš„æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ æ‰©æ•£æŒ‡å¯¼çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥å‡è½»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„åè§ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨åè§å¹¶è®¾è®¡å»åè®­ç»ƒç›®æ ‡ï¼Œé‡‡ç”¨è½»é‡çº§å¯è®­ç»ƒæ¨¡å—è‡ªåŠ¨ä¿ç•™ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹å¾—åˆ°æ— åè§çš„ç»“æœã€‚åªéœ€åœ¨è½»é‡çº§å¯è®­ç»ƒæ¨¡å—ä¸­å­¦ä¹ å°‘é‡å‚æ•°ï¼Œæ— éœ€æ›´æ”¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInvDiffåœ¨å‡å°‘åè§çš„åŒæ—¶ä¿æŒäº†å›¾åƒç”Ÿæˆçš„å“è´¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ˜“å—åˆ°çœŸå®ä¸–ç•Œæ•°æ®ä¸­çš„åè§å½±å“ã€‚</li>
<li>å·²çŸ¥åè§çš„å¤„ç†æ–¹æ³•åŒ…æ‹¬è®¾è®¡æ–‡æœ¬æç¤ºå’Œä½¿ç”¨åè§æ ‡ç­¾æ„å»ºæ— åè§æ•°æ®ï¼Œä½†å¤„ç†æœªçŸ¥åè§å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>InvDiffæ¡†æ¶è¢«æå‡ºç”¨äºå¤„ç†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„åè§é—®é¢˜ã€‚</li>
<li>InvDiffé€šè¿‡è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çš„æ½œåœ¨åè§å¹¶è®¾è®¡å»åè®­ç»ƒç›®æ ‡æ¥å®ç°ã€‚</li>
<li>InvDiffé‡‡ç”¨è½»é‡çº§å¯è®­ç»ƒæ¨¡å—ï¼Œè‡ªåŠ¨ä¿ç•™ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e62a8822f6dfdf50634b68b964c5f1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-06d138724421f3c0520b072624a6ddbc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f56f88fec6e82de1849708de23501d90.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fcdc020a6ab8c7711e65c8d147a5fd1d.jpg" align="middle">
</details>




<h2 id="CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis"><a href="#CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis" class="headerlink" title="CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis"></a>CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis</h2><p><strong>Authors:Mu Zhang, Yunfan Liu, Yue Liu, Hongtian Yu, Qixiang Ye</strong></p>
<p>Accurately depicting real-world landscapes in remote sensing (RS) images requires precise alignment between objects and their environment. However, most existing synthesis methods for natural images prioritize foreground control, often reducing the background to plain textures. This neglects the interaction between foreground and background, which can lead to incoherence in RS scenarios. In this paper, we introduce CC-Diff, a Diffusion Model-based approach for RS image generation with enhanced Context Coherence. To capture spatial interdependence, we propose a sequential pipeline where background generation is conditioned on synthesized foreground instances. Distinct learnable queries are also employed to model both the complex background texture and its semantic relation to the foreground. Extensive experiments demonstrate that CC-Diff outperforms state-of-the-art methods in visual fidelity, semantic accuracy, and positional precision, excelling in both RS and natural image domains. CC-Diff also shows strong trainability, improving detection accuracy by 2.04 mAP on DOTA and 2.25 mAP on the COCO benchmark. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒä¸­å‡†ç¡®æç»˜çœŸå®ä¸–ç•Œçš„æ™¯è§‚è¦æ±‚ç‰©ä½“ä¸å…¶ç¯å¢ƒä¹‹é—´æœ‰ç²¾ç¡®çš„å¯¹é½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è‡ªç„¶å›¾åƒåˆæˆæ–¹æ³•ä¼˜å…ˆæ§åˆ¶å‰æ™¯ï¼Œé€šå¸¸å°†èƒŒæ™¯ç®€åŒ–ä¸ºå¹³åŸçº¹ç†ã€‚è¿™å¿½ç•¥äº†å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„äº¤äº’ï¼Œå¯èƒ½å¯¼è‡´é¥æ„Ÿåœºæ™¯ä¸­çš„ä¸è¿è´¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CC-Diffï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå…·æœ‰å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚ä¸ºäº†æ•æ‰ç©ºé—´ç›¸å…³æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¡ºåºæµç¨‹ï¼Œå…¶ä¸­èƒŒæ™¯ç”Ÿæˆæ˜¯åœ¨åˆæˆçš„å‰æ™¯å®ä¾‹æ¡ä»¶ä¸‹è¿›è¡Œçš„ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸åŒçš„å¯å­¦ä¹ æŸ¥è¯¢æ¥å¯¹å¤æ‚çš„èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨é¥æ„Ÿå’Œè‡ªç„¶å›¾åƒé¢†åŸŸå‡è¡¨ç°å‡ºè‰²ã€‚CC-Diffè¿˜æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¯è®­ç»ƒæ€§ï¼Œåœ¨DOTAä¸Šæé«˜äº†2.04 mAPçš„æ£€æµ‹ç²¾åº¦ï¼Œåœ¨COCOåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†2.25 mAPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08464v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹é¥æ„Ÿå›¾åƒä¸­çš„çœŸå®ä¸–ç•Œæ™¯è§‚å‡†ç¡®æç»˜éœ€è¦ç‰©ä½“ä¸å…¶ç¯å¢ƒä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚ç°æœ‰å¤§å¤šæ•°è‡ªç„¶å›¾åƒåˆæˆæ–¹æ³•ä¸»è¦å…³æ³¨å‰æ™¯æ§åˆ¶ï¼Œå¾€å¾€å°†èƒŒæ™¯ç®€åŒ–ä¸ºç®€å•çº¹ç†ï¼Œå¿½ç•¥äº†å‰æ™¯ä¸èƒŒæ™¯ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œå¯¼è‡´é¥æ„Ÿåœºæ™¯ä¸­çš„ä¸è¿è´¯æ€§ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒç”Ÿæˆæ–¹æ³•CC-Diffï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚é€šè¿‡æ„å»ºç©ºé—´ä¾èµ–æ€§çš„é¡ºåºæµç¨‹ï¼Œä»¥åˆæˆçš„å‰æ™¯å®ä¾‹ä¸ºæ¡ä»¶è¿›è¡ŒèƒŒæ™¯ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢æ¥æ¨¡æ‹Ÿå¤æ‚çš„èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼ŒCC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨é¥æ„Ÿå›¾åƒå’Œè‡ªç„¶å›¾åƒé¢†åŸŸå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚CC-Diffè¿˜æ˜¾ç¤ºäº†å¼ºå¤§çš„å¯è®­ç»ƒæ€§ï¼Œåœ¨DOTAå’ŒCOCOåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æé«˜äº†2.04 mAPå’Œ2.25 mAPçš„æ£€æµ‹ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒå‡†ç¡®æç»˜éœ€è¦å‰æ™¯ä¸èƒŒæ™¯çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å‰æ™¯æ§åˆ¶ï¼Œå¯¼è‡´èƒŒæ™¯å¤„ç†ä¸è¶³å’Œåœºæ™¯ä¸è¿è´¯ã€‚</li>
<li>CC-Diffæ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚</li>
<li>é€šè¿‡é¡ºåºæµç¨‹æ•æ‰ç©ºé—´ä¾èµ–æ€§ï¼Œä»¥åˆæˆçš„å‰æ™¯ä¸ºæ¡ä»¶è¿›è¡ŒèƒŒæ™¯ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨å¯å­¦ä¹ æŸ¥è¯¢æ¨¡æ‹ŸèƒŒæ™¯çº¹ç†ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»ã€‚</li>
<li>CC-Diffåœ¨è§†è§‰ã€è¯­ä¹‰å’Œå®šä½ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-39f965fde7b978afd19e7cc0ca9f8b95.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9ed910a52932186b8173136b43049cc7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0614f7a5ceff9a03b46ed443d78c6d8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0df51cf4df07c894818863e62a13b9f4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c037adc9b093b6f98a1c87b90e337a76.jpg" align="middle">
</details>




<h2 id="Grasp-Diffusion-Network-Learning-Grasp-Generators-from-Partial-Point-Clouds-with-Diffusion-Models-in-SO-3-xR3"><a href="#Grasp-Diffusion-Network-Learning-Grasp-Generators-from-Partial-Point-Clouds-with-Diffusion-Models-in-SO-3-xR3" class="headerlink" title="Grasp Diffusion Network: Learning Grasp Generators from Partial Point   Clouds with Diffusion Models in SO(3)xR3"></a>Grasp Diffusion Network: Learning Grasp Generators from Partial Point   Clouds with Diffusion Models in SO(3)xR3</h2><p><strong>Authors:Joao Carvalho, An T. Le, Philipp Jahr, Qiao Sun, Julen Urain, Dorothea Koert, Jan Peters</strong></p>
<p>Grasping objects successfully from a single-view camera is crucial in many robot manipulation tasks. An approach to solve this problem is to leverage simulation to create large datasets of pairs of objects and grasp poses, and then learn a conditional generative model that can be prompted quickly during deployment. However, the grasp pose data is highly multimodal since there are several ways to grasp an object. Hence, in this work, we learn a grasp generative model with diffusion models to sample candidate grasp poses given a partial point cloud of an object. A novel aspect of our method is to consider diffusion in the manifold space of rotations and to propose a collision-avoidance cost guidance to improve the grasp success rate during inference. To accelerate grasp sampling we use recent techniques from the diffusion literature to achieve faster inference times. We show in simulation and real-world experiments that our approach can grasp several objects from raw depth images with $90%$ success rate and benchmark it against several baselines. </p>
<blockquote>
<p>åœ¨ä¼—å¤šçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä»å•è§†è§’ç›¸æœºæˆåŠŸæŠ“å–ç‰©ä½“è‡³å…³é‡è¦ã€‚è§£å†³æ­¤é—®é¢˜çš„ä¸€ä¸ªæ–¹æ³•æ˜¯åˆ©ç”¨ä»¿çœŸæ¥åˆ›å»ºå¤§é‡çš„ç‰©ä½“å’ŒæŠ“å–å§¿æ€é…å¯¹æ•°æ®é›†ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ªæ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨éƒ¨ç½²æ—¶å¯ä»¥å¿«é€Ÿæç¤ºã€‚ç„¶è€Œï¼Œç”±äºæœ‰å¤šç§æ–¹æ³•å¯ä»¥æŠ“å–ä¸€ä¸ªç‰©ä½“ï¼ŒæŠ“å–å§¿æ€æ•°æ®æ˜¯å¤šæ¨¡æ€çš„ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ æŠ“å–ç”Ÿæˆæ¨¡å‹ï¼Œæ ¹æ®ç‰©ä½“çš„éƒ¨åˆ†ç‚¹äº‘å¯¹å€™é€‰æŠ“å–å§¿æ€è¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬æ–¹æ³•çš„ä¸€ä¸ªæ–°é¢–ä¹‹å¤„åœ¨äºè€ƒè™‘åœ¨æ—‹è½¬æµå½¢ç©ºé—´ä¸­çš„æ‰©æ•£ï¼Œå¹¶æå‡ºä¸€ç§ç¢°æ’é¿å…æˆæœ¬æŒ‡å¯¼ï¼Œä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„æŠ“å–æˆåŠŸç‡ã€‚ä¸ºäº†åŠ å¿«æŠ“å–é‡‡æ ·é€Ÿåº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ‰©æ•£æ–‡çŒ®ä¸­çš„æœ€æ–°æŠ€æœ¯æ¥å®ç°æ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­éƒ½è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä»åŸå§‹æ·±åº¦å›¾åƒä¸­æŠ“å–å¤šä¸ªç‰©ä½“ï¼ŒæˆåŠŸç‡è¾¾åˆ°90%ï¼Œå¹¶å°†å…¶ä¸å‡ ä¸ªåŸºå‡†è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥è§£å†³ä»å•è§†è§’ç›¸æœºæˆåŠŸæŠ“å–ç‰©ä½“çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å€ŸåŠ©ä»¿çœŸåˆ›å»ºå¤§é‡çš„ç‰©ä½“å’ŒæŠ“å–å§¿æ€æ•°æ®é›†ï¼Œå­¦ä¹ æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨éƒ¨ç½²æ—¶å¯å¿«é€Ÿæç¤ºã€‚ç”±äºæŠ“å–å§¿æ€æ•°æ®å…·æœ‰å¤šæ¨¡æ€æ€§ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ˜¯åœ¨ç‚¹äº‘ç©ºé—´ä¸­å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥é‡‡æ ·å€™é€‰æŠ“å–å§¿æ€ã€‚è¯¥æ–¹æ³•è€ƒè™‘äº†åœ¨æ—‹è½¬æµå½¢ç©ºé—´ä¸­çš„æ‰©æ•£ï¼Œå¹¶æå‡ºç¢°æ’é¿å…æˆæœ¬æŒ‡å¯¼æ¥æé«˜æ¨ç†é˜¶æ®µçš„æŠ“å–æˆåŠŸç‡ã€‚é€šè¿‡ä½¿ç”¨æ‰©æ•£æ–‡çŒ®ä¸­çš„æœ€æ–°æŠ€æœ¯ï¼Œæé«˜äº†é‡‡æ ·é€Ÿåº¦ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»åŸå§‹æ·±åº¦å›¾åƒä¸­æŠ“å–å¤šä¸ªç‰©ä½“ï¼ŒæˆåŠŸç‡è¾¾åˆ°90%ï¼Œå¹¶ä¸å‡ ä¸ªåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨ä»¿çœŸåˆ›å»ºå¤§é‡çš„ç‰©ä½“å’ŒæŠ“å–å§¿æ€æ•°æ®é›†ï¼Œå­¦ä¹ æ¡ä»¶ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºè§£å†³ä»å•è§†è§’ç›¸æœºæŠ“å–ç‰©ä½“çš„é—®é¢˜ã€‚</li>
<li>æŠ“å–å§¿æ€æ•°æ®å…·æœ‰å¤šæ¨¡æ€æ€§ï¼Œå› æ­¤åœ¨ç‚¹äº‘ç©ºé—´ä¸­å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œè®­ç»ƒä»¥é‡‡æ ·å€™é€‰æŠ“å–å§¿æ€ã€‚</li>
<li>è€ƒè™‘äº†åœ¨æ—‹è½¬æµå½¢ç©ºé—´ä¸­çš„æ‰©æ•£ã€‚</li>
<li>æå‡ºäº†ç¢°æ’é¿å…æˆæœ¬æŒ‡å¯¼æ¥æé«˜æ¨ç†é˜¶æ®µçš„æŠ“å–æˆåŠŸç‡ã€‚</li>
<li>ä½¿ç”¨æœ€æ–°çš„æ‰©æ•£æŠ€æœ¯æ¥åŠ é€ŸæŠ“å–é‡‡æ ·ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c17eacc64fa0c9fb52341ea65b88bc6d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c272602300f7c75c6511c67dad2747ba.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e5ac35f945bb82796595229121aaebad.jpg" align="middle">
</details>




<h2 id="Unicorn-Unified-Neural-Image-Compression-with-One-Number-Reconstruction"><a href="#Unicorn-Unified-Neural-Image-Compression-with-One-Number-Reconstruction" class="headerlink" title="Unicorn: Unified Neural Image Compression with One Number Reconstruction"></a>Unicorn: Unified Neural Image Compression with One Number Reconstruction</h2><p><strong>Authors:Qi Zheng, Haozhi Wang, Zihao Liu, Jiaming Liu, Peiye Liu, Zhijian Hao, Yanheng Lu, Dimin Niu, Jinjia Zhou, Minge Jing, Yibo Fan</strong></p>
<p>Prevalent lossy image compression schemes can be divided into: 1) explicit image compression (EIC), including traditional standards and neural end-to-end algorithms; 2) implicit image compression (IIC) based on implicit neural representations (INR). The former is encountering impasses of either leveling off bitrate reduction at a cost of tremendous complexity while the latter suffers from excessive smoothing quality as well as lengthy decoder models. In this paper, we propose an innovative paradigm, which we dub \textbf{Unicorn} (\textbf{U}nified \textbf{N}eural \textbf{I}mage \textbf{C}ompression with \textbf{O}ne \textbf{N}number \textbf{R}econstruction). By conceptualizing the images as index-image pairs and learning the inherent distribution of pairs in a subtle neural network model, Unicorn can reconstruct a visually pleasing image from a randomly generated noise with only one index number. The neural model serves as the unified decoder of images while the noises and indexes corresponds to explicit representations. As a proof of concept, we propose an effective and efficient prototype of Unicorn based on latent diffusion models with tailored model designs. Quantitive and qualitative experimental results demonstrate that our prototype achieves significant bitrates reduction compared with EIC and IIC algorithms. More impressively, benefitting from the unified decoder, our compression ratio escalates as the quantity of images increases. We envision that more advanced model designs will endow Unicorn with greater potential in image compression. We will release our codes in \url{<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree%7D">https://github.com/uniqzheng/Unicorn-Laduree}</a>. </p>
<blockquote>
<p>æµè¡Œçš„æœ‰æŸå›¾åƒå‹ç¼©æ–¹æ¡ˆå¯åˆ†ä¸ºä¸¤ç±»ï¼š1ï¼‰æ˜¾å¼å›¾åƒå‹ç¼©ï¼ˆEICï¼‰ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæ ‡å‡†å’Œç¥ç»ç«¯åˆ°ç«¯ç®—æ³•ï¼›ä»¥åŠåŸºäºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„éšå¼å›¾åƒå‹ç¼©ï¼ˆIICï¼‰ã€‚å‰è€…åœ¨å·¨å¤§çš„å¤æ‚æ€§ä¸‹é‡åˆ°æ¯”ç‰¹ç‡å‡å°‘åœæ»ä¸å‰çš„å›°å¢ƒï¼Œè€Œåè€…åˆ™é¢ä¸´è¿‡åº¦å¹³æ»‘è´¨é‡ä»¥åŠè§£ç å™¨æ¨¡å‹è¿‡é•¿çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œç‹¬è§’å…½â€ï¼ˆç»Ÿä¸€ç¥ç»å›¾åƒå‹ç¼©é‡å»ºï¼‰ã€‚é€šè¿‡å°†å›¾åƒæ¦‚å¿µåŒ–ä¸ºç´¢å¼•å›¾åƒå¯¹ï¼Œå¹¶åœ¨å¾®å¦™çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­å­¦ä¹ å¯¹ä¹‹é—´çš„å†…åœ¨åˆ†å¸ƒï¼Œç‹¬è§’å…½å¯ä»¥ä»éšæœºç”Ÿæˆçš„å™ªå£°ä¸­ä»…ç”¨å•ä¸ªç´¢å¼•å·é‡å»ºå‡ºä»¤äººæ»¡æ„çš„å›¾åƒã€‚è¯¥ç¥ç»ç½‘ç»œæ¨¡å‹ä½œä¸ºç»Ÿä¸€è§£ç å™¨ä¸ºå›¾åƒæœåŠ¡ï¼Œè€Œå™ªå£°å’Œç´¢å¼•åˆ™å¯¹åº”äºæ˜¾å¼è¡¨ç¤ºã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç‹¬è§’å…½çš„æœ‰æ•ˆé«˜æ•ˆåŸå‹ã€‚å®šåˆ¶æ¨¡å‹è®¾è®¡çš„ç»“æœå’Œå®éªŒå®šé‡ä¸å®šæ€§åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸå‹ä¸EICå’ŒIICç®—æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æ¯”ç‰¹ç‡é™ä½ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œå¾—ç›Šäºç»Ÿä¸€çš„è§£ç å™¨ï¼Œéšç€å›¾åƒæ•°é‡çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¯”ä¹Ÿåœ¨æé«˜ã€‚æˆ‘ä»¬ç›¸ä¿¡æ›´å…ˆè¿›çš„æ¨¡å‹è®¾è®¡å°†ä½¿ç‹¬è§’å…½åœ¨å›¾åƒå‹ç¼©æ–¹é¢å±•ç°å‡ºæ›´å¤§çš„æ½œåŠ›ã€‚æˆ‘ä»¬å°†ä¼šåœ¨ç½‘å€ä¸­å…¬å¼€ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree%E3%80%82">https://github.com/uniqzheng/Unicorn-Ladureeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08210v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å›¾åƒå‹ç¼©æ–¹æ³•â€”â€”Unicornï¼ˆåŸºäºå•ä¸€ç¥ç»ç½‘ç»œå›¾åƒå‹ç¼©ï¼‰ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè§†ä¸ºç´¢å¼•-å›¾åƒå¯¹ï¼Œå¹¶åœ¨å¾®å¦™çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­å­¦ä¹ è¿™äº›å¯¹ä¹‹é—´çš„å†…åœ¨åˆ†å¸ƒã€‚Unicornå¯ä»¥ä»éšæœºç”Ÿæˆçš„å™ªå£°ä¸­ä»…ç”¨å•ä¸€ç´¢å¼•æ•°å­—é‡å»ºå‡ºè§†è§‰æ„‰æ‚¦çš„å›¾åƒã€‚ç¥ç»ç½‘ç»œæ¨¡å‹å……å½“äº†ç»Ÿä¸€çš„è§£ç å™¨ï¼Œå™ªå£°å’Œç´¢å¼•åˆ™å¯¹åº”æ˜ç¡®çš„è¡¨ç¤ºå½¢å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰å›¾åƒå‹ç¼©ç®—æ³•ç›¸æ¯”ï¼ŒUnicornåŸå‹åœ¨æ¯”ç‰¹ç‡å‡å°‘æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œå¾—ç›Šäºç»Ÿä¸€çš„è§£ç å™¨ï¼Œéšç€å›¾åƒæ•°é‡çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¯”ä¹Ÿåœ¨æé«˜ã€‚æˆ‘ä»¬æœŸå¾…æœªæ¥æ›´å…ˆè¿›çš„æ¨¡å‹è®¾è®¡èƒ½ä¸ºUnicornåœ¨å›¾åƒå‹ç¼©æ–¹é¢å¸¦æ¥æ›´å¼ºå¤§çš„æ½œåŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree">https://github.com/uniqzheng/Unicorn-Laduree</a>ã€‚æ­¤æ–¹æ³•å¯èƒ½é‡å¡‘æœªæ¥å›¾åƒå‹ç¼©é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å›¾åƒå‹ç¼©åˆ†ä¸ºæ˜¾å¼å›¾åƒå‹ç¼©ï¼ˆEICï¼‰å’ŒåŸºäºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„éšå¼å›¾åƒå‹ç¼©ï¼ˆIICï¼‰ã€‚</li>
<li>å½“å‰æŠ€æœ¯é¢ä¸´æŒ‘æˆ˜ï¼šEICå¤æ‚åº¦æé«˜ä¸”æ¯”ç‰¹ç‡é™ä½å·²æ¥è¿‘ç“¶é¢ˆï¼›IICåˆ™å­˜åœ¨å›¾åƒå¹³æ»‘è¿‡åº¦å’Œè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>Unicornæå‡ºäº†ä¸€ä¸ªæ–°çš„ç¥ç»ç½‘ç»œæ¨¡å‹ç”¨äºå›¾åƒå‹ç¼©çš„æ¦‚å¿µæ¡†æ¶ï¼Œèƒ½å¤Ÿä»ä¸€ä¸ªç®€å•çš„ç´¢å¼•æ•°å­—é‡æ„é«˜è´¨é‡å›¾åƒã€‚å®ƒç”¨ä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨å¤„ç†æ‰€æœ‰å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5e9526e87e92fe4967015f82a9583a1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b8e36b788687535643c8ea55fb808f7b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b5427895e74d7b079fb9e86e3fd1cb5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3d03eb4b291b395751079d79dfdcd80f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aec2e069491f59d900ff73613a434c81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-95d33bebd679d05deea40b127573544a.jpg" align="middle">
</details>




<h2 id="Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing"><a href="#Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing" class="headerlink" title="Diffusion-Based Attention Warping for Consistent 3D Scene Editing"></a>Diffusion-Based Attention Warping for Consistent 3D Scene Editing</h2><p><strong>Authors:Eyal Gomel, Lior Wolf</strong></p>
<p>We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \url{<a target="_blank" rel="noopener" href="https://attention-warp.github.io}/">https://attention-warp.github.io}</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œ3Dåœºæ™¯ç¼–è¾‘çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ç¡®ä¿ä»ä¸åŒè§†è§’è§‚çœ‹æ—¶çš„ä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä»å•ä¸ªå‚è€ƒå›¾åƒä¸­æå–çš„æ³¨æ„åŠ›ç‰¹å¾æ¥å®šä¹‰é¢„æœŸçš„ç¼–è¾‘ã€‚é€šè¿‡å°†è¿™äº›ç‰¹å¾ä¸ä»é«˜æ–¯æ‹¼è´´æ·±åº¦ä¼°è®¡ä¸­å¾—å‡ºçš„åœºæ™¯å‡ ä½•è¿›è¡Œå¯¹é½ï¼Œå°†è¿™äº›ç‰¹å¾è·¨å¤šä¸ªè§†è§’è¿›è¡Œå˜å½¢ã€‚å°†è¿™äº›å˜å½¢ç‰¹å¾æ³¨å…¥åˆ°å…¶ä»–è§†è§’ä¸­ï¼Œå¯ä»¥å®ç°ç¼–è¾‘çš„è¿è´¯ä¼ æ’­ï¼Œåœ¨é«˜ä¿çœŸå’Œç©ºé—´å¯¹é½æ–¹é¢å®ç°3Dç©ºé—´çš„é«˜ä¿çœŸå’Œç©ºé—´å¯¹é½ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„3Dåœºæ™¯ç¼–è¾‘æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæå¤§åœ°æé«˜äº†åœºæ™¯æ“ä½œçš„èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://attention-warp.github.io/">https://attention-warp.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07984v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œ3Dåœºæ™¯ç¼–è¾‘çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å‚è€ƒå›¾åƒçš„æ³¨æ„åŠ›ç‰¹å¾æ¥å®šä¹‰é¢„æœŸçš„ç¼–è¾‘ï¼Œå¹¶å°†å…¶è·¨å¤šä¸ªè§†è§’è¿›è¡Œå¯¹é½ï¼Œä»è€Œå®ç°ç¼–è¾‘çš„ä¸€è‡´æ€§ä¼ æ’­å’Œé«˜ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜æ–¯å–·æº…æ·±åº¦ä¼°è®¡æ¥æ¨å¯¼åœºæ™¯å‡ ä½•ç»“æ„ï¼Œå¹¶å°†æ³¨æ„åŠ›ç‰¹å¾è¿›è¡Œå˜å½¢å’Œæ³¨å…¥å…¶ä»–è§†è§’ï¼Œç¡®ä¿äº†ç¼–è¾‘åœ¨3Dç©ºé—´ä¸­çš„è¿è´¯æ€§å’Œç©ºé—´å¯¹é½ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„3Dåœºæ™¯ç¼–è¾‘æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œæ˜¾è‘—æé«˜äº†åœºæ™¯æ“ä½œçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•è¿›è¡Œ3Dåœºæ™¯ç¼–è¾‘ã€‚</li>
<li>åˆ©ç”¨å‚è€ƒå›¾åƒçš„æ³¨æ„åŠ›ç‰¹å¾è¿›è¡Œç¼–è¾‘å®šä¹‰ã€‚</li>
<li>é€šè¿‡é«˜æ–¯å–·æº…æ·±åº¦ä¼°è®¡çš„åœºæ·Ÿå‡ ä½•ç»“æ„å®ç°è·¨è§†è§’çš„ä¸€è‡´æ€§ã€‚</li>
<li>å˜å½¢å’Œæ³¨å…¥æ³¨æ„åŠ›ç‰¹å¾åˆ°å…¶ä»–è§†è§’ï¼Œç¡®ä¿ç¼–è¾‘çš„è¿è´¯æ€§å’Œç©ºé—´å¯¹é½ã€‚</li>
<li>æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„3Dåœºæ™¯ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†åœºæ™¯æ“ä½œçš„èƒ½åŠ›ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šçš„è¯¦ç»†ä¿¡æ¯ï¼š<a target="_blank" rel="noopener" href="https://attention-warp.github.io./">https://attention-warp.github.ioã€‚</a></li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f8f42584dfad759e1b2aecc279277861.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5766ba7cdaf9078e0a54b1a828295c72.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e83f73a0915f2745d1dec4a301df78d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a0003e9cacf1db217bda34beeb1a59b.jpg" align="middle">
</details>




<h2 id="Non-Normal-Diffusion-Models"><a href="#Non-Normal-Diffusion-Models" class="headerlink" title="Non-Normal Diffusion Models"></a>Non-Normal Diffusion Models</h2><p><strong>Authors:Henry Li</strong></p>
<p>Diffusion models generate samples by incrementally reversing a process that turns data into noise. We show that when the step size goes to zero, the reversed process is invariant to the distribution of these increments. This reveals a previously unconsidered parameter in the design of diffusion models: the distribution of the diffusion step $\Delta x_k :&#x3D; x_{k} - x_{k + 1}$. This parameter is implicitly set by default to be normally distributed in most diffusion models. By lifting this assumption, we generalize the framework for designing diffusion models and establish an expanded class of diffusion processes with greater flexibility in the choice of loss function used during training. We demonstrate the effectiveness of these models on density estimation and generative modeling tasks on standard image datasets, and show that different choices of the distribution of $\Delta x_k$ result in qualitatively different generated samples. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥åè½¬å°†æ•°æ®è½¬åŒ–ä¸ºå™ªå£°çš„è¿‡ç¨‹æ¥ç”Ÿæˆæ ·æœ¬ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå½“æ­¥é•¿è¶‹äºé›¶æ—¶ï¼Œåè½¬è¿‡ç¨‹å¯¹è¿™äº›å¢é‡çš„åˆ†å¸ƒæ˜¯ä¸å˜çš„ã€‚è¿™æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹è®¾è®¡ä¸­ä¸€ä¸ªä»¥å‰æœªè¢«è€ƒè™‘è¿‡çš„å‚æ•°ï¼šæ‰©æ•£æ­¥é•¿Î”xk:&#x3D;xkâˆ’xk+1çš„åˆ†å¸ƒã€‚è¿™ä¸€å‚æ•°åœ¨å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹ä¸­é»˜è®¤è®¾ç½®ä¸ºæ­£æ€åˆ†å¸ƒã€‚é€šè¿‡å–æ¶ˆè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æ¨å¹¿äº†æ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ¡†æ¶ï¼Œå»ºç«‹äº†ä¸€ä¸ªå…·æœ‰æ›´å¤§çµæ´»æ€§çš„æ‰©æ•£è¿‡ç¨‹ç±»ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥é€‰æ‹©æ›´å¹¿æ³›çš„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†å›¾åƒæ•°æ®é›†ä¸Šçš„å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸­éªŒè¯äº†è¿™äº›æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜Î”xkåˆ†å¸ƒçš„ä¸åŒé€‰æ‹©ä¼šå¯¼è‡´ç”Ÿæˆçš„æ ·æœ¬åœ¨è´¨é‡ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07935v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ¸é€†è½¬å°†æ•°æ®å˜ä¸ºå™ªå£°çš„è¿‡ç¨‹æ¥ç”Ÿæˆæ ·æœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œå½“æ­¥é•¿è¶‹äºé›¶æ—¶ï¼Œé€†å‘è¿‡ç¨‹ä¸å—è¿™äº›å¢é‡åˆ†å¸ƒçš„çš„å½±å“ã€‚è¿™ä¸ºæ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ­ç¤ºäº†ä¸€ä¸ªä»¥å‰æœªè¢«è€ƒè™‘è¿‡çš„å‚æ•°ï¼Œå³æ‰©æ•£æ­¥é•¿Î”xkçš„åˆ†å¸ƒã€‚å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹é»˜è®¤å°†å…¶è®¾ä¸ºæ­£æ€åˆ†å¸ƒã€‚é€šè¿‡å–æ¶ˆè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æ¨å¹¿äº†æ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ¡†æ¶ï¼Œå»ºç«‹äº†ä¸€ä¸ªå…·æœ‰æ›´å¤§çµæ´»æ€§çš„æ‰©æ•£è¿‡ç¨‹ç±»ï¼Œåœ¨è®­ç»ƒä¸­å¯ä»¥é€‰æ‹©æ›´å¤šçš„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†å›¾åƒæ•°æ®é›†ä¸Šå±•ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†Î”xkåˆ†å¸ƒçš„ä¸åŒé€‰æ‹©ä¼šäº§ç”Ÿä¸åŒè´¨é‡çš„ç”Ÿæˆæ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡é€†å‘è¿‡ç¨‹ä»å™ªå£°ç”Ÿæˆæ ·æœ¬ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„æ­¥é•¿è¶‹äºé›¶æ—¶ï¼Œé€†å‘è¿‡ç¨‹ä¸å¢é‡åˆ†å¸ƒæ— å…³ã€‚</li>
<li>æ‰©æ•£æ­¥é•¿çš„åˆ†å¸ƒåœ¨æ¨¡å‹è®¾è®¡ä¸­æ˜¯å…³é”®å‚æ•°ã€‚</li>
<li>å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹é»˜è®¤å°†æ­¥é•¿åˆ†å¸ƒè®¾ä¸ºæ­£æ€åˆ†å¸ƒã€‚</li>
<li>å–æ¶ˆè¿™ä¸€é»˜è®¤å‡è®¾å¯ä»¥æ¨å¹¿æ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ¡†æ¶ã€‚</li>
<li>ä¸åŒçš„æ‰©æ•£æ­¥é•¿åˆ†å¸ƒé€‰æ‹©ä¼šå½±å“ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e51bcb7d6661004cce30ab4aaaf7543b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0fdee689b42dcc752a197d0f2a3aa59a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4b38949908ad2849cbbc799f48b413fd.jpg" align="middle">
</details>




<h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models on some reward functions that are either designed by experts or learned from small-scale datasets. Existing methods for finetuning diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. Inspired by recent successes in generative flow networks (GFlowNets), a class of probabilistic models that sample with the unnormalized density of a reward function, we propose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), the first GFlowNet method that leverages the rich signal in reward gradients, together with an objective called $\nabla$-DB plus its variant residual $\nabla$-DB designed for prior-preserving diffusion alignment. We show that our proposed method achieves fast yet diversity- and prior-preserving alignment of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>é€šå¸¸äººä»¬é€šè¿‡åœ¨ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ”¶é›†æ•°æ®é›†æ¥è®­ç»ƒå¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œä½†äººä»¬å¾€å¾€å¸Œæœ›å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸æŸäº›å¥–åŠ±å‡½æ•°å¯¹é½å¹¶è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›å¥–åŠ±å‡½æ•°è¦ä¹ˆæ˜¯ä¸“å®¶è®¾è®¡çš„ï¼Œè¦ä¹ˆæ˜¯ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚ç°æœ‰çš„å¾®è°ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒè¿‡ç¨‹ä¸­æ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚å—æœ€è¿‘ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æˆåŠŸçš„å¯å‘ï¼ŒGFlowNetsæ˜¯ä¸€ç±»ä»¥å¥–åŠ±å‡½æ•°çš„æœªå½’ä¸€åŒ–å¯†åº¦è¿›è¡Œé‡‡æ ·çš„æ¦‚ç‡æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„GFlowNetæ–¹æ³•ï¼Œç§°ä¸ºNabla-GFlowNetï¼ˆç®€ç§°$\nabla$-GFlowNetï¼‰ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·çš„GFlowNetæ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªç§°ä¸º$\nabla$-DBçš„ç›®æ ‡å‡½æ•°åŠå…¶ä¸ºä¿ç•™å…ˆéªŒçš„æ‰©æ•£å¯¹é½è®¾è®¡çš„æ®‹å·®$\nabla$-DBå˜ä½“ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„ç°å®å¥–åŠ±å‡½æ•°ä¸Šå¿«é€Ÿä¸”å¤šæ ·åŒ–å’Œä¿ç•™å…ˆéªŒåœ°å¯¹é½å¤§è§„æ¨¡æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹Stable Diffusionã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v1">PDF</a> Technical Report (35 pages, 31 figures)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ‰©æ•£æ¨¡å‹é€šå¸¸é€šè¿‡é’ˆå¯¹ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡æ”¶é›†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä½†äººä»¬æ›´å¸Œæœ›å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸ä¸“å®¶è®¾è®¡çš„å¥–åŠ±å‡½æ•°æˆ–ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°è¿›è¡Œå¯¹é½å’Œå¾®è°ƒã€‚ç°æœ‰å¾®è°ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒé€Ÿåº¦æ…¢ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶å—ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æœ€æ–°æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸ºNabla-GFlowNetçš„æ–°å‹GFlowNetæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸­çš„ä¸°å¯Œä¿¡å·ï¼Œå¹¶ç»“åˆä¸€ä¸ªç§°ä¸º$\nabla$-DBçš„ç›®æ ‡åŠå…¶ä¸ºä¿ç•™å…ˆéªŒæ‰©æ•£å¯¹é½è®¾è®¡çš„æ®‹å·®$\nabla$-DBå˜ä½“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯å®ç°ç¨³å®šæ‰©æ•£çš„å¿«é€Ÿã€å¤šæ ·æ€§å’Œä¿ç•™å…ˆéªŒçš„å¯¹é½ï¼Œè¿™æ˜¯ä¸€ç§å¤§å‹æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¯åœ¨ä¸åŒçš„çœŸå®å¥–åŠ±å‡½æ•°ä¸Šè¿›è¡Œåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ‰©æ•£æ¨¡å‹é€šå¸¸é€šè¿‡ä¸‹æ¸¸ä»»åŠ¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä½†å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä¸å¥–åŠ±å‡½æ•°å¯¹é½æ›´ä¸ºå¸¸è§ã€‚</li>
<li>ç°å­˜çš„å¾®è°ƒæ–¹æ³•å­˜åœ¨æ ·æœ¬å¤šæ ·æ€§ä¸è¶³ã€ç¼ºå°‘å…ˆéªŒçŸ¥è¯†ä¿ç•™å’Œæ…¢æ”¶æ•›çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨GFlowNetsæœ€æ–°æˆåŠŸä¸ºçµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸ºNabla-GFlowNetçš„æ–°å‹æ–¹æ³•ã€‚</li>
<li>Nabla-GFlowNetåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸­çš„ä¸°å¯Œä¿¡å·è¿›è¡Œå·¥ä½œã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†$\nabla$-DBåŠå…¶å˜ä½“æ®‹å·®$\nabla$-DBç›®æ ‡ï¼Œç”¨äºå®ç°ä¿ç•™å…ˆéªŒçš„æ‰©æ•£å¯¹é½ã€‚</li>
<li>Nabla-GFlowNetåœ¨å¤šç§çœŸå®å¥–åŠ±å‡½æ•°ä¸Šå®ç°äº†ç¨³å®šæ‰©æ•£çš„å¿«é€Ÿã€å¤šæ ·æ€§å’Œä¿ç•™å…ˆéªŒçš„å¯¹é½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e6ad08bb0dc78455f591fd489bf8bf14.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-21950de2b5b2e07066fd3abf8159ca34.jpg" align="middle">
</details>




<h2 id="From-Slow-Bidirectional-to-Fast-Causal-Video-Generators"><a href="#From-Slow-Bidirectional-to-Fast-Causal-Video-Generators" class="headerlink" title="From Slow Bidirectional to Fast Causal Video Generators"></a>From Slow Bidirectional to Fast Causal Video Generators</h2><p><strong>Authors:Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</strong></p>
<p>Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to a causal transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacherâ€™s ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model supports fast streaming generation of high quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future. </p>
<blockquote>
<p>å½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–æ€§ï¼Œåœ¨äº¤äº’å¼åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚å•ä¸ªå¸§çš„ç”Ÿæˆéœ€è¦æ¨¡å‹å¤„ç†æ•´ä¸ªåºåˆ—ï¼ŒåŒ…æ‹¬æœªæ¥ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡å°†é¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£å˜å‹å™¨é€‚åº”ä¸ºå› æœå˜å‹å™¨æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¯¥å› æœå˜å‹å™¨å¯ä»¥å³æ—¶ç”Ÿæˆå¸§ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿï¼Œæˆ‘ä»¬å°†åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹è’¸é¦ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚ä¸ºäº†å®ç°ç¨³å®šå’Œé«˜è´¨é‡çš„è’¸é¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ•™å¸ˆODEè½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥åŠä¸€ç§ä¸å¯¹ç§°çš„è’¸é¦ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç”¨åŒå‘æ•™å¸ˆç›‘ç£å› æœå­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ï¼Œå³ä½¿åœ¨çŸ­ç‰‡æ®µè®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é•¿æ—¶é•¿è§†é¢‘åˆæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒåœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿæµå¼ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œè¿™å¾—ç›ŠäºKVç¼“å­˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†æµå¼è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ã€å›¾åƒåˆ°è§†é¢‘ä»¥åŠé›¶æ ·æœ¬æ–¹å¼çš„åŠ¨æ€æç¤ºã€‚æœªæ¥ï¼Œæˆ‘ä»¬å°†åŸºäºå¼€æºæ¨¡å‹å‘å¸ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07772v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://causvid.github.io/">https://causvid.github.io/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–è€Œäº§ç”Ÿçš„ç“¶é¢ˆï¼Œæœ¬ç ”ç©¶é€šè¿‡å°†ä¸€ä¸ªé¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£è½¬æ¢å™¨æ”¹ç¼–ä¸ºå› æœè½¬æ¢å™¨ï¼Œå®ç°äº†å³æ—¶ç”Ÿæˆå¸§çš„æŠ€æœ¯ã€‚ä¸ºé™ä½å»¶è¿Ÿï¼Œç ”ç©¶å›¢é˜Ÿå°†åˆ†å¸ƒåŒ¹é…è’¸é¦æ³•ï¼ˆDMDï¼‰æ‰©å±•è‡³è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹ç¼©å‡ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚åŒæ—¶ï¼Œå¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆåŠä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œæœ‰æ•ˆå‡è½»è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ï¼Œå³ä¾¿åœ¨çŸ­ç‰‡æ®µè®­ç»ƒä¸‹ä¹Ÿèƒ½å®ç°é•¿æœŸè§†é¢‘åˆæˆã€‚å€ŸåŠ©KVç¼“å­˜ï¼Œè¯¥æ¨¡å‹å¯åœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜æ”¯æŒè§†é¢‘è½¬è§†é¢‘ã€å›¾ç‰‡è½¬è§†é¢‘çš„å®æ—¶æµå¼è½¬æ¢ä»¥åŠé›¶æ ·æœ¬åŠ¨æ€æç¤ºã€‚æœªæ¥ç ”ç©¶å›¢é˜Ÿå°†åŸºäºå¼€æºæ¨¡å‹å…¬å¼€ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­å­˜åœ¨åŒå‘æ³¨æ„åŠ›ä¾èµ–çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å› æœè½¬æ¢å™¨ï¼Œèƒ½å¤Ÿå³æ—¶ç”Ÿæˆå¸§ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é€šè¿‡å°†åˆ†å¸ƒåŒ¹é…è’¸é¦æ³•æ‰©å±•è‡³è§†é¢‘é¢†åŸŸï¼Œé™ä½äº†æ¨¡å‹çš„ç”Ÿæˆæ­¥éª¤ï¼Œä»è€Œå‡å°‘äº†å»¶è¿Ÿã€‚</li>
<li>å¼•å…¥äº†å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆå’Œä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œé«˜è´¨é‡è’¸é¦æ•ˆæœã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå‡è½»è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ï¼Œå®ç°é•¿æœŸè§†é¢‘åˆæˆã€‚</li>
<li>æ¨¡å‹æ”¯æŒå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶èƒ½åœ¨å•ä¸ªGPUä¸Šå®ç°æµå¼ä¼ è¾“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ee965fc3063cea1099d1a788584150d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d753d21c54ebc6456937f17d3e9e71a9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9cb62cb28a54dd058bb39b3b2bfafbe2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f6f5f4cc15dc30fe00c4852e9829e5fe.jpg" align="middle">
</details>




<h2 id="SynCamMaster-Synchronizing-Multi-Camera-Video-Generation-from-Diverse-Viewpoints"><a href="#SynCamMaster-Synchronizing-Multi-Camera-Video-Generation-from-Diverse-Viewpoints" class="headerlink" title="SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse   Viewpoints"></a>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse   Viewpoints</h2><p><strong>Authors:Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang</strong></p>
<p>Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: <a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒ3Dä¸€è‡´æ€§æ–¹é¢å±•ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚è¿™ä¸€è¿›å±•æ¿€å‘äº†æˆ‘ä»¬æ¢ç´¢è¿™äº›æ¨¡å‹åœ¨è·¨ä¸åŒè§†è§’ç¡®ä¿åŠ¨æ€ä¸€è‡´æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œè¿™å¯¹äºè™šæ‹Ÿæ‹æ‘„ç­‰åº”ç”¨æ¥è¯´æ˜¯ä¸€ä¸ªé«˜åº¦ç†æƒ³çš„åŠŸèƒ½ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•èšç„¦äºå•ä¸ªå¯¹è±¡çš„å¤šè§†è§’ç”Ÿæˆä»¥è¿›è¡Œ4Dé‡å»ºï¼Œæˆ‘ä»¬çš„å…´è¶£åœ¨äºä»ä»»æ„è§†è§’ç”Ÿæˆå¼€æ”¾ä¸–ç•Œè§†é¢‘ï¼Œå¹¶èå…¥6è‡ªç”±åº¦ç›¸æœºå§¿æ€ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯å¢å¼ºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä»¥è¿›è¡Œå¤šç›¸æœºè§†é¢‘ç”Ÿæˆï¼Œç¡®ä¿ä¸åŒè§†è§’çš„å†…å®¹ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè§†è§’åŒæ­¥æ¨¡å—æ¥ä¿æŒè¿™äº›è§†è§’çš„å¤–è§‚å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚è€ƒè™‘åˆ°é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨å¤šç›¸æœºå›¾åƒå’Œå•çœ¼è§†é¢‘æ¥è¡¥å……ç”±Unreal Engineæ¸²æŸ“çš„å¤šç›¸æœºè§†é¢‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†æœ‰è¶£çš„æ‰©å±•ï¼Œå¦‚ä»æ–°é¢–è§†è§’é‡æ–°æ¸²æŸ“è§†é¢‘ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåä¸ºSynCamVideo-Datasetçš„å¤šè§†è§’åŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/%E3%80%82">https://jianhongbai.github.io/SynCamMaster/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07760v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒ3Dä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç´¢äº†è¿™ç§æ¨¡å‹åœ¨è™šæ‹Ÿæ‹æ‘„ç­‰åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡æå‡ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å¤šç›¸æœºè§†é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œç¡®ä¿ä¸åŒè§†è§’ä¸‹çš„å†…å®¹ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†å¤šè§†è§’åŒæ­¥æ¨¡å—ï¼Œç»´æŒå„è§†è§’ä¸‹çš„å¤–è§‚å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚é’ˆå¯¹é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤šç›¸æœºå›¾åƒå’Œå•ç›®è§†é¢‘æ¥è¡¥å……è™šå¹»å¼•æ“æ¸²æŸ“çš„å¤šç›¸æœºè§†é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ”¯æŒä»æ–°è§†è§’é‡æ–°æ¸²æŸ“è§†é¢‘ç­‰æœ‰è¶£çš„åº”ç”¨æ‰©å±•ã€‚åŒæ—¶ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†ä¸€ä¸ªåä¸ºSynCamVideo-Datasetçš„å¤šè§†è§’åŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒ3Dä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿæ‹æ‘„åº”ç”¨çš„æ½œåŠ›ï¼Œå¼ºè°ƒå¤šè§†è§’ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¢å¼ºé¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å¤šç›¸æœºè§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å¤šè§†è§’åŒæ­¥æ¨¡å—ï¼Œç»´æŒä¸åŒè§†è§’ä¸‹çš„å¤–è§‚å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>é’ˆå¯¹é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§é—®é¢˜ï¼Œé‡‡ç”¨æ··åˆè®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤šæºæ•°æ®è¿›è¡Œè¡¥å……ã€‚</li>
<li>æ”¯æŒä»æ–°è§†è§’é‡æ–°æ¸²æŸ“è§†é¢‘çš„æœ‰è¶£åº”ç”¨æ‰©å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6cedadb2a6a524961cb37855fe9be841.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b31ef600e0a14b462d2dcfbf2e1e57bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-aef164b104eed0ad6a622de188acbf28.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-85fa1fa7ccd65095a4fbc200417c521a.jpg" align="middle">
</details>




<h2 id="TraSCE-Trajectory-Steering-for-Concept-Erasure"><a href="#TraSCE-Trajectory-Steering-for-Concept-Erasure" class="headerlink" title="TraSCE: Trajectory Steering for Concept Erasure"></a>TraSCE: Trajectory Steering for Concept Erasure</h2><p><strong>Authors:Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, Yuki Mitsufuji</strong></p>
<p>Recent advancements in text-to-image diffusion models have brought them to the public spotlight, becoming widely accessible and embraced by everyday users. However, these models have been shown to generate harmful content such as not-safe-for-work (NSFW) images. While approaches have been proposed to erase such abstract concepts from the models, jail-breaking techniques have succeeded in bypassing such safety measures. In this paper, we propose TraSCE, an approach to guide the diffusion trajectory away from generating harmful content. Our approach is based on negative prompting, but as we show in this paper, conventional negative prompting is not a complete solution and can easily be bypassed in some corner cases. To address this issue, we first propose a modification of conventional negative prompting. Furthermore, we introduce a localized loss-based guidance that enhances the modified negative prompting technique by steering the diffusion trajectory. We demonstrate that our proposed method achieves state-of-the-art results on various benchmarks in removing harmful content including ones proposed by red teams; and erasing artistic styles and objects. Our proposed approach does not require any training, weight modifications, or training data (both image or prompt), making it easier for model owners to erase new concepts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä½¿å…¶å—åˆ°å…¬ä¼—å…³æ³¨ï¼Œå¹¶æˆä¸ºæ—¥å¸¸ç”¨æˆ·å¹¿æ³›å¯è®¿é—®å’Œæ¥å—çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å·²è¢«è¯æ˜å¯ä»¥ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œä¾‹å¦‚ä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„å›¾åƒã€‚è™½ç„¶å·²æœ‰æ–¹æ³•è¢«æå‡ºä»æ¨¡å‹ä¸­åˆ é™¤æ­¤ç±»æŠ½è±¡æ¦‚å¿µï¼Œä½†è¶Šç‹±æŠ€æœ¯å·²æˆåŠŸç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºTraSCEæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å¯¼æ‰©æ•£è½¨è¿¹è¿œç¦»ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè´Ÿæç¤ºï¼Œä½†æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ‰€å±•ç¤ºçš„ï¼Œä¼ ç»Ÿçš„è´Ÿæç¤ºå¹¶éå®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨æŸäº›ç‰¹æ®Šæƒ…å†µä¸‹å®¹æ˜“è¢«ç»•è¿‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºå¯¹ä¼ ç»Ÿè´Ÿæç¤ºçš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ‰©æ•£è½¨è¿¹æ¥å¢å¼ºæ”¹è¿›åçš„è´Ÿæç¤ºæŠ€æœ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒ…æ‹¬çº¢é˜Ÿæå‡ºçš„å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„æœ‰å®³å†…å®¹ï¼›ä»¥åŠæ¶ˆé™¤è‰ºæœ¯é£æ ¼å’Œç‰©ä½“ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸éœ€è¦ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼ˆæ— è®ºæ˜¯å›¾åƒè¿˜æ˜¯æç¤ºï¼‰ï¼Œè¿™ä½¿å¾—æ¨¡å‹æ‰€æœ‰è€…æ›´å®¹æ˜“æ¶ˆé™¤æ–°æ¦‚å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07658v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•å·²å¼•èµ·å…¬ä¼—å…³æ³¨ï¼Œå¹¶è¢«æ—¥å¸¸ç”¨æˆ·å¹¿æ³›æ¥å—ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¼šç”Ÿæˆä¸å®‰å…¨çš„æˆäººå†…å®¹ã€‚å°½ç®¡å·²æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­æ¶ˆé™¤æ­¤ç±»æŠ½è±¡æ¦‚å¿µï¼Œä½†è¶Šç‹±æŠ€æœ¯å·²æˆåŠŸç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚æœ¬æ–‡æå‡ºTraSCEæ–¹æ³•ï¼Œé€šè¿‡è´Ÿæç¤ºå¼•å¯¼æ‰©æ•£è½¨è¿¹ï¼Œé¿å…ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚ä½†å¦‚æœ¬æ–‡æ‰€ç¤ºï¼Œä¼ ç»Ÿè´Ÿæç¤ºå¹¶éå®Œå…¨è§£å†³æ–¹æ¡ˆï¼ŒæŸäº›æƒ…å†µä¸‹å®¹æ˜“è¢«ç»•è¿‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹å¸¸è§„è´Ÿæç¤ºè¿›è¡Œæ”¹è¿›ï¼Œå¹¶å¼•å…¥åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼ï¼Œé€šè¿‡æ§åˆ¶æ‰©æ•£è½¨è¿¹å¼ºåŒ–è´Ÿæç¤ºæŠ€æœ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹ã€æ¶ˆé™¤è‰ºæœ¯é£æ ¼å’Œç‰©ä½“æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼ˆå›¾åƒæˆ–æç¤ºï¼‰ï¼Œä½¿å¾—æ¨¡å‹ä¸»äººæ›´å®¹æ˜“æ¶ˆé™¤æ–°æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å¸¦æ¥å…¬ä¼—å…³æ³¨ï¼Œä½†ç”Ÿæˆçš„æœ‰å®³å†…å®¹æˆä¸ºé—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­æ¶ˆé™¤æœ‰å®³å†…å®¹ï¼Œä½†å­˜åœ¨è¶Šç‹±æŠ€æœ¯ç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚</li>
<li>æå‡ºTraSCEæ–¹æ³•ï¼Œé€šè¿‡è´Ÿæç¤ºå¼•å¯¼æ‰©æ•£è½¨è¿¹ï¼Œé¿å…ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚</li>
<li>ä¼ ç»Ÿè´Ÿæç¤ºå¹¶éå®Œå…¨è§£å†³æ–¹æ¡ˆï¼ŒæŸäº›æƒ…å†µä¸‹å®¹æ˜“è¢«ç»•è¿‡ã€‚</li>
<li>å¯¹å¸¸è§„è´Ÿæç¤ºè¿›è¡Œæ”¹è¿›ï¼Œå¹¶å¼•å…¥åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼å¼ºåŒ–æŠ€æœ¯ã€‚</li>
<li>TraSCEæ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-90e08ded28c54362a9bc6e958d7de136.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9977d55951a2f9b136992f265976828.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-040c9e01c46ba7f7a7281eaea5b5ea91.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-29cc281a6f072cd35671ca8d9ac80956.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4b0d9fe0eb315dd15a2b6ef43b9f4539.jpg" align="middle">
</details>




<h2 id="Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model"><a href="#Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model" class="headerlink" title="Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model"></a>Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Jianfeng Guo, Feng Yang, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Motion artifacts present in magnetic resonance imaging (MRI) can seriously interfere with clinical diagnosis. Removing motion artifacts is a straightforward solution and has been extensively studied. However, paired data are still heavily relied on in recent works and the perturbations in k-space (frequency domain) are not well considered, which limits their applications in the clinical field. To address these issues, we propose a novel unsupervised purification method which leverages pixel-frequency information of noisy MRI images to guide a pre-trained diffusion model to recover clean MRI images. Specifically, considering that motion artifacts are mainly concentrated in high-frequency components in k-space, we utilize the low-frequency components as the guide to ensure correct tissue textures. Additionally, given that high-frequency and pixel information are helpful for recovering shape and detail textures, we design alternate complementary masks to simultaneously destroy the artifact structure and exploit useful information. Quantitative experiments are performed on datasets from different tissues and show that our method achieves superior performance on several metrics. Qualitative evaluations with radiologists also show that our method provides better clinical feedback. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD">https://github.com/medcx/PFAD</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šä¸¥é‡å¹²æ‰°ä¸´åºŠè¯Šæ–­ã€‚å»é™¤è¿åŠ¨ä¼ªå½±æ˜¯ä¸€ç§ç›´æ¥è§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶ä»ç„¶ä¸¥é‡ä¾èµ–äºé…å¯¹æ•°æ®ï¼Œè€Œkç©ºé—´ï¼ˆé¢‘ç‡åŸŸï¼‰ä¸­çš„æ‰°åŠ¨å¹¶æœªå¾—åˆ°å¾ˆå¥½çš„è€ƒè™‘ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠé¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å™ªå£°MRIå›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´çš„MRIå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œè€ƒè™‘åˆ°è¿åŠ¨ä¼ªå½±ä¸»è¦é›†ä¸­åœ¨kç©ºé—´çš„é«˜é¢‘æˆåˆ†ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼æ¥ç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ã€‚æ­¤å¤–ï¼Œé‰´äºé«˜é¢‘å’Œåƒç´ ä¿¡æ¯æœ‰åŠ©äºæ¢å¤å½¢çŠ¶å’Œç»†èŠ‚çº¹ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†äº¤æ›¿çš„äº’è¡¥æ©è†œæ¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚åœ¨ä¸åŒç»„ç»‡æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„å®šæ€§è¯„ä¼°ä¹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æ›´å¥½çš„ä¸´åºŠåé¦ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/medcx/PFADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07590v2">PDF</a> 12 pages, 8 figures, AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å™ªå£°ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯å¼•å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ™°çš„MRIå›¾åƒï¼Œä»¥è§£å†³MRIä¸­çš„è¿åŠ¨ä¼ªå½±é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ï¼Œå¹¶è®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œåŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¾—åˆ°æ”¾å°„ç§‘åŒ»å¸ˆçš„ç§¯æä¸´åºŠåé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šå¹²æ‰°ä¸´åºŠè¯Šæ–­ï¼Œå»é™¤ä¼ªå½±æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šä¾èµ–é…å¯¹æ•°æ®ï¼Œå¯¹kç©ºé—´ï¼ˆé¢‘ç‡åŸŸï¼‰çš„æ‰°åŠ¨è€ƒè™‘ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠåº”ç”¨çš„å¹¿æ³›æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨åƒç´ é¢‘ç‡ä¿¡æ¯å¼•å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ™°MRIå›¾åƒã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ï¼Œç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ï¼›è®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œï¼ŒåŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®šé‡å’Œå®šæ€§è¯„ä¼°å‡éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸æ”¾å°„ç§‘åŒ»å¸ˆçš„åˆä½œè¯„ä»·æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ï¼Œè·å¾—ç§¯æåé¦ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e0ff7c13591035b978b06d36f1f533ae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f99b9342819d5c3237e084dec6be6f9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4cfd4272ea20d1f177b432a0446e8119.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-47ef4c87738d300c706dc2cbf4506649.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8186300dbbe1c884fae5bbebd2c36e3a.jpg" align="middle">
</details>




<h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, BjÃ¶rn Ommer</strong></p>
<p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks. </p>
<blockquote>
<p>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæœ‰ä¸¤ç§èŒƒå¼åœ¨å„ç§åº”ç”¨ä¸­è·å¾—å…³æ³¨ï¼šåŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„Masked Generative Modelså’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆä¾‹å¦‚æ‰©æ•£æ¨¡å‹ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹å°†å®ƒä»¬è¿æ¥èµ·æ¥ï¼Œå¹¶æ¢ç´¢å®ƒä»¬åœ¨è§†è§‰é¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»¥å¯ä¼¸ç¼©çš„æ–¹å¼ï¼Œåœ¨ç»Ÿä¸€çš„è®¾è®¡ç©ºé—´å†…å¯¹è¿™ä¸¤ç§ç±»å‹çš„æ¨¡å‹è¿›è¡Œäº†é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ã€æ¸©åº¦ã€æŒ‡å¯¼å¼ºåº¦ç­‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†å‰²ï¼‰é‡æ–°å®šä½ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šä»[MASK]æ ‡è®°è¿›è¡Œçš„å»é®æ©è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œå„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…ä¸€æ¬¡è®­ç»ƒæ¥å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œçµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰ä¸Šè¿°æ¢ç´¢éƒ½å¼•é¢†æˆ‘ä»¬æ„å»ºäº†åä¸ºDiscrete Interpolantsçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¦‚ImageNet256ã€MS COCOå’Œè§†é¢‘æ•°æ®é›†FaceForensicsã€‚æ€»ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œæˆ‘ä»¬å¯ä»¥æ¶èµ·Masked Generativeå’ŒNon-autoregressive Diffusionæ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œä»¥åŠç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v2">PDF</a> Technical Report (WIP), Project Page(code, model, dataset):   <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹è¿æ¥åŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„é®ç½©ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œå¹¶åœ¨è§†è§‰ä¸Šæ¢ç´¢å…¶å¯æ‰©å±•æ€§ã€‚è¯¥ç ”ç©¶è¿›è¡Œäº†ç»Ÿä¸€è®¾è®¡ç©ºé—´ä¸­çš„é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°è°ƒåº¦ã€æ¸©åº¦ã€å¼•å¯¼å¼ºåº¦ç­‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»é®æ©è¿‡ç¨‹ï¼Œå®ç°äº†å„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…è®­ç»ƒä¸€æ¬¡å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œçµæ´»æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰è¿™äº›æ¢ç´¢éƒ½å½’åŠŸäºåä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ImageNet256ã€MS COCOå’ŒFaceForensicsè§†é¢‘æ•°æ®é›†ä¸Šå®ç°äº†ä¸ä»¥å‰åŸºäºç¦»æ•£çŠ¶æ€çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç®€å•æ¥è¯´ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„é®ç½©è¿æ¥é®ç½©ç”Ÿæˆæ¨¡å‹å’Œéè‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¿æ¥ç”Ÿæˆä»»åŠ¡å’Œåˆ¤åˆ«ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹è¿æ¥ä¸¤ç§ç”Ÿæˆæ¨¡å‹èŒƒå¼ï¼šåŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„é®ç½©ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ã€‚</li>
<li>åœ¨ç»Ÿä¸€è®¾è®¡ç©ºé—´ä¸­å¯¹ä¸¤ç§æ¨¡å‹è¿›è¡Œäº†é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°è°ƒåº¦ç­‰å…³é”®å› ç´ ã€‚</li>
<li>å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå»é®æ©è¿‡ç¨‹ï¼Œå®ç°äº†çµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚</li>
<li>æå‡ºåä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œå®ç°äº†å„ç§è§†è§‰ä»»åŠ¡ä¸Šçš„ç«äº‰åŠ›æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„é®ç½©ï¼Œè¿æ¥äº†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«ä»»åŠ¡ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬ImageNet256ã€MS COCOå’ŒFaceForensicsæ•°æ®é›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-84482f8d037dfd8fca4b36a654af2c7d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7bd916bdace6c59e807a319127cb89ee.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c3c9e8e97bdb09e0544dbb86f3d0601f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c35d03a36efcc9698532982aad9a05f.jpg" align="middle">
</details>




<h2 id="ContRail-A-Framework-for-Realistic-Railway-Image-Synthesis-using-ControlNet"><a href="#ContRail-A-Framework-for-Realistic-Railway-Image-Synthesis-using-ControlNet" class="headerlink" title="ContRail: A Framework for Realistic Railway Image Synthesis using   ControlNet"></a>ContRail: A Framework for Realistic Railway Image Synthesis using   ControlNet</h2><p><strong>Authors:Andrei-Robert Alexandrescu, Razvan-Gabriel Petec, Alexandru Manole, Laura-Silvia Diosan</strong></p>
<p>Deep Learning became an ubiquitous paradigm due to its extraordinary effectiveness and applicability in numerous domains. However, the approach suffers from the high demand of data required to achieve the potential of this type of model. An ever-increasing sub-field of Artificial Intelligence, Image Synthesis, aims to address this limitation through the design of intelligent models capable of creating original and realistic images, endeavour which could drastically reduce the need for real data. The Stable Diffusion generation paradigm recently propelled state-of-the-art approaches to exceed all previous benchmarks. In this work, we propose the ContRail framework based on the novel Stable Diffusion model ControlNet, which we empower through a multi-modal conditioning method. We experiment with the task of synthetic railway image generation, where we improve the performance in rail-specific tasks, such as rail semantic segmentation by enriching the dataset with realistic synthetic images. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å› å…¶åœ¨ä¼—å¤šé¢†åŸŸçš„å‡ºè‰²æ•ˆæœå’Œé€‚ç”¨æ€§è€Œæˆä¸ºæ— å¤„ä¸åœ¨çš„èŒƒä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦è¾¾åˆ°æ¨¡å‹æ½œåŠ›æ‰€éœ€çš„å¤§é‡æ•°æ®ã€‚äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ—¥ç›Šå¢é•¿çš„å­é¢†åŸŸâ€”â€”å›¾åƒåˆæˆï¼Œæ—¨åœ¨é€šè¿‡è®¾è®¡èƒ½å¤Ÿåˆ›å»ºåŸå§‹å’ŒçœŸå®å›¾åƒçš„æ™ºèƒ½æ¨¡å‹æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä»è€Œå¤§å¹…å‡å°‘å¯¹çœŸå®æ•°æ®çš„éœ€æ±‚ã€‚æœ€è¿‘ï¼ŒStable Diffusionç”ŸæˆèŒƒå¼æ¨åŠ¨äº†æœ€å…ˆè¿›çš„æ–¹æ³•è¶…è¶Šäº†æ‰€æœ‰å…ˆå‰çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºæ–°é¢–çš„Stable Diffusionæ¨¡å‹ControlNetæå‡ºContRailæ¡†æ¶ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ¡ä»¶æ–¹æ³•è¿›è¡Œèµ‹èƒ½ã€‚æˆ‘ä»¬å°è¯•é“è·¯å›¾åƒåˆæˆç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡ç”¨é€¼çœŸçš„åˆæˆå›¾åƒä¸°å¯Œæ•°æ®é›†ï¼Œæé«˜é“è·¯ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é“è·¯è¯­ä¹‰åˆ†å‰²ï¼‰çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06742v2">PDF</a> 9 pages, 5 figures, 2 tables</p>
<p><strong>Summary</strong>ï¼šæ·±åº¦å­¦ä¹ å› å…¶å“è¶Šçš„æ•ˆç‡å’Œå¹¿æ³›é€‚ç”¨çš„èƒ½åŠ›æˆä¸ºäº†æ— å¤„ä¸åœ¨çš„èŒƒå¼ï¼Œä½†åœ¨éœ€è¦å®ç°å…¶æ½œåŠ›æ—¶å¯¹æ•°æ®çš„éœ€æ±‚æé«˜ã€‚å›¾åƒåˆæˆä½œä¸ºäººå·¥æ™ºèƒ½çš„ä¸€ä¸ªä¸æ–­å¢é•¿å­é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡è®¾è®¡èƒ½å¤Ÿåˆ›å»ºåŸå§‹å’Œé€¼çœŸå›¾åƒçš„æ™ºèƒ½æ¨¡å‹æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä»è€Œå¤§å¹…å‡å°‘å¯¹çœŸå®æ•°æ®çš„éœ€æ±‚ã€‚æœ¬ç ”ç©¶åŸºäºæ–°å‹ç¨³å®šæ‰©æ•£æ¨¡å‹ControlNetæå‡ºäº†ContRailæ¡†æ¶ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è°ƒèŠ‚æ–¹æ³•æå‡äº†æ€§èƒ½ã€‚é€šè¿‡é“è·¯åˆæˆå›¾åƒç”Ÿæˆçš„ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæé«˜ç‰¹å®šäºé“è·¯ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚é“è·¯è¯­ä¹‰åˆ†å‰²ã€‚ä½¿ç”¨é€¼çœŸçš„åˆæˆå›¾åƒä¸°å¯Œäº†æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ æˆä¸ºæ— å¤„ä¸åœ¨çš„èŒƒå¼ï¼Œä½†å…¶å¯¹æ•°æ®çš„éœ€æ±‚æé«˜ã€‚</li>
<li>å›¾åƒåˆæˆå­é¢†åŸŸæ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œé€šè¿‡åˆ›å»ºé€¼çœŸå›¾åƒå‡å°‘çœŸå®æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>ContRailæ¡†æ¶åŸºäºæ–°å‹ç¨³å®šæ‰©æ•£æ¨¡å‹ControlNetæ„å»ºã€‚</li>
<li>å¤šæ¨¡æ€è°ƒèŠ‚æ–¹æ³•å¢å¼ºäº†ContRailæ¡†æ¶çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒä»¥é“è·¯åˆæˆå›¾åƒç”Ÿæˆä»»åŠ¡è¿›è¡Œï¼Œè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½æå‡ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿä¸°å¯Œæ•°æ®é›†ä»¥æé«˜é“è·¯è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-adb625fd18a5cadb69b96ca71f2d670d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-282f3655f025832fd0b8255d6c97dd81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ccdb569fc2c3bd7448fa2740bc75912.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0988c5696c2b634669b5ff8560f75c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-53710ab7cf073314c03f5c6f8e24e722.jpg" align="middle">
</details>




<h2 id="Open-Source-Acceleration-of-Stable-Diffusion-cpp"><a href="#Open-Source-Acceleration-of-Stable-Diffusion-cpp" class="headerlink" title="Open-Source Acceleration of Stable-Diffusion.cpp"></a>Open-Source Acceleration of Stable-Diffusion.cpp</h2><p><strong>Authors:Jingxu Ng, Cheng Lv, Pu Zhao, Wei Niu, Juyi Lin, Minzhou Pan, Yun Liang, Yanzhi Wang</strong></p>
<p>Stable diffusion plays a crucial role in generating high-quality images. However, image generation is time-consuming and memory-intensive. To address this, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate the diffusion models. Although it is lightweight, the current implementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference latency and massive memory usage. To address this, in this work, we present an optimized version of Sdcpp leveraging the Winograd algorithm to accelerate 2D convolution operations, which is the primary bottleneck in the pipeline. By analyzing both dependent and independent computation graphs, we exploit the deviceâ€™s locality and parallelism to achieve substantial performance improvements. Our framework delivers correct end-to-end results across various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and SDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for individual convolutional layers and an inference speedup up to 4.79x for the overall image generation process, compared with the original Sdcpp on M1 pro. Homepage: <a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a> </p>
<blockquote>
<p>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œå›¾åƒç”Ÿæˆæ˜¯è€—æ—¶çš„ä¸”éœ€è¦å¤§é‡å†…å­˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œstable-diffusion.cppï¼ˆSdcppï¼‰ä½œä¸ºä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†æ¡†æ¶åº”è¿è€Œç”Ÿï¼Œä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚è™½ç„¶å®ƒå¾ˆè½»ä¾¿ï¼Œä½†Sdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å¹¶ä¸ç†æƒ³ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„æ¨ç†å»¶è¿Ÿå’Œå·¨å¤§çš„å†…å­˜ä½¿ç”¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼˜åŒ–ç‰ˆçš„Sdcppï¼Œåˆ©ç”¨Winogradç®—æ³•åŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè¿™æ˜¯ç®¡é“ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚é€šè¿‡åˆ†ææœ‰ä¾èµ–å’Œæ— ä¾èµ–çš„è®¡ç®—å›¾ï¼Œæˆ‘ä»¬åˆ©ç”¨è®¾å¤‡çš„å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§æ¥å®ç°æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­éƒ½èƒ½æä¾›æ­£ç¡®çš„ç«¯åˆ°ç«¯ç»“æœï¼ŒåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹Sdcppåœ¨M1 proä¸Šçš„è¡¨ç°ç›¸æ¯”ï¼Œå•ä¸ªå·ç§¯å±‚çš„é€Ÿåº¦æé«˜äº†2.76å€ï¼Œæ•´ä½“å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ¨ç†é€Ÿåº¦æé«˜äº†4.79å€ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05781v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œä½†å›¾åƒç”Ÿæˆè€—æ—¶ä¸”å ç”¨å¤§é‡å†…å­˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå‡ºç°äº†stable-diffusion.cppï¼ˆSdcppï¼‰è¿™ä¸€é«˜æ•ˆæ¨ç†æ¡†æ¶æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼ŒSdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å­˜åœ¨ç¼ºé™·ï¼Œå­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜å’Œå†…å­˜ä½¿ç”¨é‡å¤§ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨Winogradç®—æ³•ä¼˜åŒ–Sdcppçš„ç‰ˆæœ¬æ¥åŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè¿™æ˜¯ç®¡é“ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚é€šè¿‡åˆ†ææœ‰ä¾èµ–å’Œæ— ä¾èµ–çš„è®¡ç®—å›¾ï¼Œæˆ‘ä»¬åˆ©ç”¨è®¾å¤‡çš„å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šç§ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼ˆåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboï¼‰ä¸­å‡èƒ½æä¾›æ­£ç¡®çš„ç«¯åˆ°ç«¯ç»“æœã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸åŸå§‹Sdcppç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨M1 proä¸Šé’ˆå¯¹å•ä¸ªå·ç§¯å±‚å®ç°äº†æœ€é«˜2.76å€çš„åŠ é€Ÿï¼Œæ•´ä½“å›¾åƒç”Ÿæˆè¿‡ç¨‹å®ç°äº†æœ€é«˜4.79å€çš„æ¨ç†åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­å¾ˆé‡è¦ï¼Œä½†å­˜åœ¨è€—æ—¶å’Œå†…å­˜å ç”¨å¤§çš„é—®é¢˜ã€‚</li>
<li>stable-diffusion.cppï¼ˆSdcppï¼‰æ¡†æ¶æ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Sdcppä¸­çš„ggml_conv_2dç®—å­å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼ŒåŒ…æ‹¬é«˜å»¶è¿Ÿå’Œå¤§é‡å†…å­˜ä½¿ç”¨ã€‚</li>
<li>åˆ©ç”¨Winogradç®—æ³•ä¼˜åŒ–Sdcppï¼ŒåŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè§£å†³æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>é€šè¿‡åˆ†æè®¡ç®—å›¾ï¼Œåˆ©ç”¨è®¾å¤‡å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§æå‡æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–çš„æ¡†æ¶é€‚ç”¨äºå¤šç§ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸åŸå§‹Sdcppç›¸æ¯”ï¼Œæ–°æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-54c9fe172a83be87335a9ebbdb49b5c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7668f911018f7c5da7612ca3bc0e829f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-01b444daef29cdf097fb454de4f91021.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8cc717995bf69c5de91eae7f5cbe3947.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3c26c7d226f869f76c1a7c3096970423.jpg" align="middle">
</details>




<h2 id="BudgetFusion-Perceptually-Guided-Adaptive-Diffusion-Models"><a href="#BudgetFusion-Perceptually-Guided-Adaptive-Diffusion-Models" class="headerlink" title="BudgetFusion: Perceptually-Guided Adaptive Diffusion Models"></a>BudgetFusion: Perceptually-Guided Adaptive Diffusion Models</h2><p><strong>Authors:Qinchan Li, Kenneth Chen, Changyue Su, Qi Sun</strong></p>
<p>Diffusion models have shown unprecedented success in the task of text-to-image generation. While these models are capable of generating high-quality and realistic images, the complexity of sequential denoising has raised societal concerns regarding high computational demands and energy consumption. In response, various efforts have been made to improve inference efficiency. However, most of the existing efforts have taken a fixed approach with neural network simplification or text prompt optimization. Are the quality improvements from all denoising computations equally perceivable to humans? We observed that images from different text prompts may require different computational efforts given the desired content. The observation motivates us to present BudgetFusion, a novel model that suggests the most perceptually efficient number of diffusion steps before a diffusion model starts to generate an image. This is achieved by predicting multi-level perceptual metrics relative to diffusion steps. With the popular Stable Diffusion as an example, we conduct both numerical analyses and user studies. Our experiments show that BudgetFusion saves up to five seconds per prompt without compromising perceptual similarity. We hope this work can initiate efforts toward answering a core question: how much do humans perceptually gain from images created by a generative model, per watt of energy? </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚è™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œé€¼çœŸçš„å›¾åƒï¼Œä½†åºåˆ—å»å™ªçš„å¤æ‚æ€§å¼•å‘äº†ç¤¾ä¼šå¯¹é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—çš„å…³æ³¨ã€‚ä½œä¸ºå›åº”ï¼Œå·²ç»åšå‡ºäº†å„ç§åŠªåŠ›æé«˜æ¨ç†æ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°åŠªåŠ›éƒ½é‡‡å–äº†å›ºå®šçš„æ–¹æ³•ï¼Œå¦‚ç®€åŒ–ç¥ç»ç½‘ç»œæˆ–ä¼˜åŒ–æ–‡æœ¬æç¤ºã€‚æ‰€æœ‰å»å™ªè®¡ç®—çš„è´¨é‡æ”¹è¿›å¯¹äººç±»æ¥è¯´éƒ½æ˜¯åŒç­‰å¯æ„ŸçŸ¥çš„å—ï¼Ÿæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ ¹æ®ä¸åŒçš„æ‰€éœ€å†…å®¹ï¼Œæ¥è‡ªä¸åŒæ–‡æœ¬æç¤ºçš„å›¾åƒå¯èƒ½éœ€è¦ä¸åŒçš„è®¡ç®—æŠ•å…¥ã€‚è¿™ä¸€è§‚å¯Ÿä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†BudgetFusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œå®ƒå»ºè®®åœ¨æ‰©æ•£æ¨¡å‹å¼€å§‹ç”Ÿæˆå›¾åƒä¹‹å‰ï¼Œè¿›è¡Œæœ€ç¬¦åˆæ„ŸçŸ¥æ•ˆç‡è¦æ±‚çš„æ‰©æ•£æ­¥éª¤æ•°é‡ã€‚è¿™æ˜¯é€šè¿‡é¢„æµ‹ä¸æ‰©æ•£æ­¥éª¤ç›¸å…³çš„å¤šçº§æ„ŸçŸ¥æŒ‡æ ‡æ¥å®ç°çš„ã€‚ä»¥æµè¡Œçš„Stable Diffusionä¸ºä¾‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ•°å€¼åˆ†æå’Œç”¨æˆ·ç ”ç©¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒBudgetFusionå¯ä»¥åœ¨ä¸æŸå®³æ„ŸçŸ¥ç›¸ä¼¼æ€§çš„æƒ…å†µä¸‹ï¼Œä¸ºæ¯ä¸ªæç¤ºèŠ‚çœå¤šè¾¾äº”ç§’çš„æ—¶é—´ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¼•å‘å¯¹æ ¸å¿ƒé—®é¢˜çš„å›ç­”ï¼šäººç±»ä»ç”Ÿæˆæ¨¡å‹åˆ›å»ºçš„å›¾åƒä¸­æ„ŸçŸ¥åˆ°çš„ä»·å€¼ï¼Œä¸æ¯ç“¦ç‰¹èƒ½é‡æ¶ˆè€—ä¹‹é—´æœ‰ä½•å…³ç³»ï¼Ÿ</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05780v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºå…¶å¤æ‚çš„å»å™ªè¿‡ç¨‹ï¼Œç¤¾ä¼šå¯¹é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—è¡¨ç¤ºæ‹…å¿§ã€‚ä¸ºæé«˜æ¨ç†æ•ˆç‡ï¼Œå·²åšå‡ºå¤šç§åŠªåŠ›ã€‚ä½†ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨ç¥ç»ç½‘ç»œç®€åŒ–æˆ–æ–‡æœ¬æç¤ºä¼˜åŒ–ç­‰å›ºå®šæ–¹å¼ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸åŒæ–‡æœ¬æç¤ºç”Ÿæˆçš„å›¾åƒå¯èƒ½éœ€è¦ä¸åŒçš„è®¡ç®—é‡æ¥ç”Ÿæˆæ‰€éœ€å†…å®¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†BudgetFusionæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å»ºè®®æ‰©æ•£æ¨¡å‹åœ¨å¼€å§‹ç”Ÿæˆå›¾åƒä¹‹å‰è¿›è¡Œæœ€æ„ŸçŸ¥æ•ˆç‡æœ€é«˜çš„æ‰©æ•£æ­¥éª¤æ•°ã€‚è¿™æ˜¯é€šè¿‡é¢„æµ‹ä¸æ‰©æ•£æ­¥éª¤ç›¸å…³çš„å¤šçº§æ„ŸçŸ¥æŒ‡æ ‡æ¥å®ç°çš„ã€‚ä»¥æµè¡Œçš„Stable Diffusionä¸ºä¾‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ•°å€¼åˆ†æå’Œç”¨æˆ·ç ”ç©¶ã€‚å®éªŒè¡¨æ˜ï¼ŒBudgetFusionåœ¨ä¸å½±å“æ„ŸçŸ¥ç›¸ä¼¼æ€§çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæç¤ºå¯èŠ‚çœå¤šè¾¾äº”ç§’é’Ÿã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¼•å‘å¯¹äººç±»ä»ç”Ÿæˆæ¨¡å‹ä¸­åˆ›å»ºçš„å›¾åƒæ¯æ¶ˆè€—ä¸€ç“¦èƒ½é‡æ‰€èƒ½æ„ŸçŸ¥åˆ°çš„æ”¶ç›Šçš„æ ¸å¿ƒé—®é¢˜çš„æ€è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸï¼Œä½†è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—å—åˆ°å…³æ³¨ã€‚</li>
<li>æé«˜æ¨ç†æ•ˆç‡çš„åŠªåŠ›å¤šæ•°é‡‡ç”¨å›ºå®šæ–¹æ³•ï¼Œå¦‚ç¥ç»ç½‘ç»œç®€åŒ–å’Œæ–‡æœ¬æç¤ºä¼˜åŒ–ã€‚</li>
<li>ä¸åŒæ–‡æœ¬æç¤ºç”Ÿæˆçš„å›¾åƒå¯èƒ½éœ€è¦ä¸åŒçš„è®¡ç®—é‡æ¥ç”Ÿæˆæ‰€éœ€å†…å®¹ã€‚</li>
<li>BudgetFusionæ¨¡å‹å»ºè®®è¿›è¡Œæœ€æ„ŸçŸ¥æ•ˆç‡æœ€é«˜çš„æ‰©æ•£æ­¥éª¤æ•°ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>BudgetFusioné€šè¿‡é¢„æµ‹ä¸æ‰©æ•£æ­¥éª¤ç›¸å…³çš„å¤šçº§æ„ŸçŸ¥æŒ‡æ ‡æ¥å®ç°é«˜æ•ˆç”Ÿæˆã€‚</li>
<li>ä»¥Stable Diffusionä¸ºä¾‹çš„å®éªŒè¡¨æ˜ï¼ŒBudgetFusionèƒ½æ˜¾è‘—èŠ‚çœæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒæ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-62dd551fde641da35ac8bd62ae792887.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d98bbdcfecab39556914f20b9d119057.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8c02379e08453839fbac5c683c142ad5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8a0a4b59a78298e531cffcecf101070.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9c93c9d5af7bcc548ea49315deedd0a0.jpg" align="middle">
</details>




<h2 id="Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images"><a href="#Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images" class="headerlink" title="Hidden in the Noise: Two-Stage Robust Watermarking for Images"></a>Hidden in the Noise: Two-Stage Robust Watermarking for Images</h2><p><strong>Authors:Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</strong></p>
<p>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion modelâ€™s initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆå™¨çš„è´¨é‡ä¸æ–­æé«˜ï¼Œæ·±åº¦ä¼ªé€ æŠ€æœ¯æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°å…è®¸æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶ç”Ÿæˆçš„AIå†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡æ³¨ï¼Œä»è€Œå‡è½»å…¶é€ æˆçš„æŸå®³ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„å›¾åƒæ°´å°æ–¹æ³•ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»çš„å½±å“ã€‚è¿™ç§è„†å¼±æ€§éƒ¨åˆ†æ˜¯å› ä¸ºæ°´å°ä¼šæ‰­æ›²ç”Ÿæˆçš„å›¾åƒçš„åˆ†å¸ƒï¼Œä»è€Œæ— æ„ä¸­æ³„éœ²æœ‰å…³æ°´å°æŠ€æœ¯çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— å¤±çœŸå›¾åƒæ°´å°æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ£€æµ‹æ°´å°éœ€è¦æ¯”è¾ƒå›¾åƒçš„é‡å»ºåˆå§‹å™ªå£°ä¸æ‰€æœ‰ä¹‹å‰ä½¿ç”¨çš„åˆå§‹å™ªå£°ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆæ£€æµ‹çš„ä¸¤é˜¶æ®µæ°´å°æ¡†æ¶ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼ä¸åˆå§‹å™ªå£°ç›¸ç»“åˆï¼ŒåµŒå…¥æœ‰å…³æˆ‘ä»¬æ‰€ç”¨åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚å¯¹äºæ£€æµ‹ï¼Œæˆ‘ä»¬ï¼ˆiï¼‰æ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œï¼ˆiiï¼‰åœ¨ç»™å®šç»„å†…æœç´¢å¯èƒ½ä¸æˆ‘ä»¬çš„å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¿™ç§æ°´å°æ–¹æ³•å®ç°äº†å¯¹ä¸€ç³»åˆ—æ”»å‡»çš„é«˜åº¦ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬ä¼ªé€ å’Œç§»é™¤æ”»å‡»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04653v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£å›¾åƒç”Ÿæˆå™¨è´¨é‡ä¸æ–­æå‡ï¼Œæ·±ä¼ªæŠ€æœ¯æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°å¯ä»¥å¸®åŠ©æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶AIç”Ÿæˆçš„å†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡æ³¨ï¼Œå‡è½»æ½œåœ¨çš„å±å®³ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„å›¾åƒæ°´å°æ–¹æ³•ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªé€ å’Œåˆ é™¤æ”»å‡»çš„å½±å“ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— ç•¸å˜æ°´å°æ–¹æ³•ã€‚æœ¬æ–‡ä¸­çš„æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒçš„åˆå§‹å™ªå£°å¤åŸæ¯”å¯¹å·¥ä½œä»¥åŠé˜²æ­¢åŸºäºæ­¤äº§ç”Ÿçš„å¸¸è§å¨èƒå¦‚å¹²æ‰°æ··æ·†åˆå§‹å™ªå£°é…ç½®çš„æ–¹æ³•å®ç°å’Œæ”¹è¿›åº”ç”¨ç­‰æ–¹é¢çš„å¤„ç†æªæ–½ç­‰ç»†èŠ‚å†…å®¹ï¼Œå¹¶æå‡ºäº†ä¸¤ä¸ªé˜¶æ®µçš„è§£å†³æ–¹æ¡ˆä»¥å®ç°é«˜æ•ˆçš„æ£€æµ‹è¿‡ç¨‹ã€‚æ­¤æ¡†æ¶æ—¢ä¼˜åŒ–äº†åˆå§‹å™ªå£°çš„ç®¡ç†ä½¿ç”¨æ•ˆç‡åˆå¢å¼ºäº†æ£€æµ‹æµç¨‹çš„ç²¾ç¡®åº¦ä¸æ•ˆç‡ã€‚è¯¥æ–¹æ³•é’ˆå¯¹ä¼ªé€ å’Œæ”»å‡»çš„è€å—åº¦æœ‰æ˜æ˜¾æé«˜ã€‚è¯¥é¡¹ç ”ç©¶çš„åˆ›æ–°æ€§ä½“ç°åœ¨ä¼˜åŒ–äº†ä¸¤é˜¶æ®µæ¡†æ¶çš„è¿è¡Œæ•ˆæœä½¿å…¶åœ¨å¤„ç†è¿‡ç¨‹çš„è¡¨ç°è¾ƒä¸ºå“è¶Šä¸”å…·æœ‰ä¼˜å¼‚æ£€æµ‹ç²¾åº¦ä¸é«˜æ•ˆç‡ç­‰ä¼˜ç‚¹ï¼Œç›¸è¾ƒäºç°æœ‰çš„æŠ€æœ¯è¡¨ç°å¤„äºé¢†å…ˆåœ°ä½ã€‚åœ¨å®éªŒä¸­å…¶æ˜¾è‘—çš„ä¼˜åŠ¿æ˜¾è‘—å¢å¼ºï¼Œä½“ç°äº†æ–¹æ³•çš„ç¨³å®šæ€§å’Œå…ˆè¿›æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨å‚…é‡Œå¶æ¨¡å¼å¢å¼ºåˆå§‹å™ªå£°åµŒå…¥ä¿¡æ¯å¹¶ç®€åŒ–æ£€æµ‹è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å¯¹æ”»å‡»å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶æä¾›äº†ä¸€ç§åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆä»¥åº”å¯¹å›¾åƒæ°´å°æŠ€æœ¯çš„æŒ‘æˆ˜ï¼Œæé«˜äº†æ°´å°çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆå™¨è´¨é‡çš„æå‡å¼•å‘æ·±ä¼ªæŠ€æœ¯è®¨è®ºï¼Œæ°´å°æŠ€æœ¯æˆä¸ºå…³é”®è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰å›¾åƒæ°´å°æŠ€æœ¯é¢ä¸´ä¼ªé€ å’Œåˆ é™¤æ”»å‡»çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— ç•¸å˜æ°´å°æ–¹æ³•åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åµŒå…¥å‚…é‡Œå¶æ¨¡å¼ä¼˜åŒ–åˆå§‹å™ªå£°çš„ä½¿ç”¨ä¸æ£€æµ‹æ•ˆç‡ã€‚</li>
<li>ä¸¤é˜¶æ®µæ¡†æ¶å®ç°é«˜æ•ˆæ£€æµ‹è¿‡ç¨‹ï¼Œæé«˜æ°´å°å¯¹æ”»å‡»çš„é²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯è¡¨ç°é¢†å…ˆï¼Œå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-28e32e7c0b9e5d8e4a894413678d703d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-386598134522fd952f67c0570a8317c6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5cb61476f17701a921b96d15e5003ab5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fe0e04f2a8d7e7051c13900ddba61f73.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d15147d40d07fa34ccfd53c158d89ad.jpg" align="middle">
</details>




<h2 id="Taming-Diffusion-Prior-for-Image-Super-Resolution-with-Domain-Shift-SDEs"><a href="#Taming-Diffusion-Prior-for-Image-Super-Resolution-with-Domain-Shift-SDEs" class="headerlink" title="Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs"></a>Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs</h2><p><strong>Authors:Qinpeng Cui, Yixuan Liu, Xinyi Zhang, Qiqi Bao, Qingmin Liao, Li Wang, Tian Lu, Zicheng Liu, Zhongdao Wang, Emad Barsoum</strong></p>
<p>Diffusion-based image super-resolution (SR) models have attracted substantial interest due to their powerful image restoration capabilities. However, prevailing diffusion models often struggle to strike an optimal balance between efficiency and performance. Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency. In this paper, we present DoSSR, a Domain Shift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images. At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models. This integration not only improves the use of diffusion prior but also boosts inference efficiency. Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoS-SDEs. This advancement leads to the fast and customized solvers that further enhance sampling efficiency. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on synthetic and real-world datasets, while notably requiring only 5 sampling steps. Compared to previous diffusion prior based methods, our approach achieves a remarkable speedup of 5-7 times, demonstrating its superior efficiency. Code: <a target="_blank" rel="noopener" href="https://github.com/QinpengCui/DoSSR">https://github.com/QinpengCui/DoSSR</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ¨¡å‹å› å…¶å¼ºå¤§çš„å›¾åƒæ¢å¤èƒ½åŠ›è€Œå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæµè¡Œçš„æ‰©æ•£æ¨¡å‹é€šå¸¸éš¾ä»¥åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šå¸¸ï¼Œå®ƒä»¬è¦ä¹ˆå¿½ç•¥äº†ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›ï¼Œé™åˆ¶äº†å…¶ç”Ÿæˆèƒ½åŠ›ï¼Œè¦ä¹ˆéœ€è¦ä»éšæœºå™ªå£°å¼€å§‹è¿›è¡Œå¤šæ¬¡å‰å‘ä¼ é€’ï¼Œä»è€Œå½±å“æ¨ç†æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DoSSRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŸŸè¿ç§»æ‰©æ•£çš„SRæ¨¡å‹ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡ä»¥ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒå¼€å§‹æ‰©æ•£è¿‡ç¨‹æ¥æ˜¾è‘—æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸŸè¿ç§»æ–¹ç¨‹ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚è¿™ç§é›†æˆä¸ä»…æ”¹å–„äº†æ‰©æ•£å…ˆéªŒçš„ä½¿ç”¨ï¼Œè¿˜æé«˜äº†æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç¦»æ•£è¿ç§»è¿‡ç¨‹è½¬å˜ä¸ºè¿ç»­å…¬å¼ï¼ˆç§°ä¸ºDoS-SDEsï¼‰æ¥æ¨è¿›æˆ‘ä»¬çš„æ–¹æ³•ã€‚è¿™ä¸€è¿›å±•å¯¼è‡´äº†å¿«é€Ÿå’Œè‡ªå®šä¹‰æ±‚è§£å™¨ï¼Œè¿›ä¸€æ­¥æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œè€Œä¸”ä»…éœ€5ä¸ªé‡‡æ ·æ­¥éª¤ã€‚ä¸ä»¥å‰çš„åŸºäºæ‰©æ•£å…ˆéªŒçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†5-7å€çš„æ˜¾è‘—åŠ é€Ÿï¼Œè¯æ˜äº†å…¶å“è¶Šçš„æ•ˆç‡ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/QinpengCui/DoSSR">https://github.com/QinpengCui/DoSSR</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17778v2">PDF</a> This paper is accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ¨¡å‹å› å…¶å¼ºå¤§çš„å›¾åƒæ¢å¤èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´éš¾ä»¥è¾¾åˆ°å¹³è¡¡ã€‚æœ¬æ–‡æå‡ºçš„DoSSRæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡ä»ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒå¼€å§‹æ‰©æ•£è¿‡ç¨‹æ¥æ˜¾è‘—æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŸŸè½¬ç§»æ–¹ç¨‹ï¼Œå®ƒèƒ½æ— ç¼åœ°èå…¥ç°æœ‰æ‰©æ•£æ¨¡å‹ï¼Œä¸ä»…æ”¹å–„äº†æ‰©æ•£å…ˆéªŒçš„ä½¿ç”¨ï¼Œè¿˜æé«˜äº†æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç¦»æ•£è½¬ç§»è¿‡ç¨‹è½¬å˜ä¸ºè¿ç»­å½¢å¼ï¼Œç§°ä¸ºDoS-SDEsï¼Œè¿›ä¸€æ­¥æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…éœ€5ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä¸ä¹‹å‰çš„æ‰©æ•£å…ˆéªŒæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†5-7å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸå—åˆ°å…³æ³¨ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´é¢ä¸´å¹³è¡¡æŒ‘æˆ˜ã€‚</li>
<li>DoSSRæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>DoSSRé€šè¿‡ä»ä½åˆ†è¾¨ç‡å›¾åƒå¼€å§‹æ‰©æ•£è¿‡ç¨‹æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>åŸŸè½¬ç§»æ–¹ç¨‹æ— ç¼èå…¥ç°æœ‰æ‰©æ•£æ¨¡å‹ï¼Œæ”¹å–„æ‰©æ•£å…ˆéªŒçš„ä½¿ç”¨å¹¶æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>DoS-SDEså°†ç¦»æ•£è½¬ç§»è¿‡ç¨‹è½¬å˜ä¸ºè¿ç»­å½¢å¼ï¼Œè¿›ä¸€æ­¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ab597b072942a2f55a19e6e0a704aef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ac555712eeffad8844e29c37a78c10cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0584b3ccb14660910e8031cb904ec4c7.jpg" align="middle">
</details>




<h2 id="DeCLIP-Decoding-CLIP-representations-for-deepfake-localization"><a href="#DeCLIP-Decoding-CLIP-representations-for-deepfake-localization" class="headerlink" title="DeCLIP: Decoding CLIP representations for deepfake localization"></a>DeCLIP: Decoding CLIP representations for deepfake localization</h2><p><strong>Authors:Stefan Smeu, Elisabeta Oneata, Dan Oneata</strong></p>
<p>Generative models can create entirely new images, but they can also partially modify real images in ways that are undetectable to the human eye. In this paper, we address the challenge of automatically detecting such local manipulations. One of the most pressing problems in deepfake detection remains the ability of models to generalize to different classes of generators. In the case of fully manipulated images, representations extracted from large self-supervised models (such as CLIP) provide a promising direction towards more robust detectors. Here, we introduce DeCLIP, a first attempt to leverage such large pretrained features for detecting local manipulations. We show that, when combined with a reasonably large convolutional decoder, pretrained self-supervised representations are able to perform localization and improve generalization capabilities over existing methods. Unlike previous work, our approach is able to perform localization on the challenging case of latent diffusion models, where the entire image is affected by the fingerprint of the generator. Moreover, we observe that this type of data, which combines local semantic information with a global fingerprint, provides more stable generalization than other categories of generative methods. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆ›å»ºå…¨æ–°çš„å›¾åƒï¼Œä½†å®ƒä»¬ä¹Ÿå¯ä»¥ä»¥äººç±»çœ¼ç›æ— æ³•å¯Ÿè§‰çš„æ–¹å¼éƒ¨åˆ†ä¿®æ”¹çœŸå®å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†è‡ªåŠ¨æ£€æµ‹æ­¤ç±»å±€éƒ¨æ“ä½œæŒ‘æˆ˜çš„é—®é¢˜ã€‚åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­ï¼Œä»ç„¶å­˜åœ¨æ¨¡å‹èƒ½å¦æ¨å¹¿åˆ°ä¸åŒç±»åˆ«çš„ç”Ÿæˆå™¨çš„é—®é¢˜ã€‚å¯¹äºå®Œå…¨æ“ä½œçš„å›¾åƒï¼Œä»å¤§å‹è‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸­æå–çš„è¡¨ç¤ºå¯¹äºå®ç°æ›´ç¨³å¥çš„æ£€æµ‹å™¨æä¾›äº†å……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†DeCLIPï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•åˆ©ç”¨æ­¤ç±»å¤§å‹é¢„è®­ç»ƒç‰¹å¾æ¥æ£€æµ‹å±€éƒ¨æ“ä½œã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“ä¸åˆç†çš„å·ç§¯è§£ç å™¨ç»“åˆæ—¶ï¼Œé¢„è®­ç»ƒçš„è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºèƒ½å¤Ÿæ‰§è¡Œå®šä½å¹¶æ”¹å–„ç°æœ‰æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å¤æ‚æƒ…å†µä¸‹è¿›è¡Œå®šä½ï¼Œå…¶ä¸­æ•´ä¸ªå›¾åƒå—åˆ°ç”Ÿæˆå™¨çš„æŒ‡çº¹å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ç§ç»“åˆäº†å±€éƒ¨è¯­ä¹‰ä¿¡æ¯å’Œå…¨å±€æŒ‡çº¹çš„æ•°æ®ç±»å‹æä¾›äº†æ¯”å…¶ä»–ç±»åˆ«ç”Ÿæˆæ–¹æ³•æ›´ç¨³å®šçš„æ³›åŒ–æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08849v2">PDF</a> Accepted at Winter Conference on Applications of Computer Vision   (WACV) 2025</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾æ£€æµ‹å›¾åƒå±€éƒ¨ä¿®æ”¹çš„æ–¹æ³•ï¼Œåä¸ºDeCLIPã€‚é€šè¿‡ç»“åˆé¢„è®­ç»ƒè‡ªç›‘ç£è¡¨ç¤ºå’Œå·ç§¯è§£ç å™¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å¯¹å±€éƒ¨æ“ä½œçš„å®šä½ï¼Œå¹¶æé«˜äº†å¯¹å„ç±»ç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•èƒ½åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜æƒ…å†µä¸‹è¿›è¡Œå®šä½ï¼Œå¹¶èƒ½æ›´ç¨³å®šåœ°æ³›åŒ–åˆ°å…¶ä»–ç±»åˆ«çš„ç”Ÿæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆ›å»ºå…¨æ–°å›¾åƒï¼Œä¹Ÿèƒ½å¯¹çœŸå®å›¾åƒè¿›è¡Œå±€éƒ¨ä¿®æ”¹ï¼Œè¿™äº›ä¿®æ”¹å¯¹äººç±»éš¾ä»¥å¯Ÿè§‰ã€‚</li>
<li>DeCLIPæ–¹æ³•åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾æ¥æ£€æµ‹å›¾åƒçš„å±€éƒ¨æ“ä½œã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒè‡ªç›‘ç£è¡¨ç¤ºå’Œå·ç§¯è§£ç å™¨ï¼ŒDeCLIPèƒ½å®šä½å±€éƒ¨æ“ä½œå¹¶æé«˜å¯¹å„ç±»ç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DeCLIPæ–¹æ³•åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å®šä½ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>å±€éƒ¨è¯­ä¹‰ä¿¡æ¯å’Œå…¨å±€æŒ‡çº¹çš„ç»“åˆæä¾›äº†æ›´ç¨³å®šçš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„è¡¨ç¤ºä¸ºæ›´ç¨³å¥çš„æ£€æµ‹å™¨æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-197897f62781de822354c3bd843ecdd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5ddbb15164f61d7df567fdf87c01d6a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4f29207e00aa799e424f969c405e6320.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-564284db12e359511251f8e8a04fea49.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a864b883db099cde2b6d1bace0d21000.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a7921baccd0154010b1f1354bd31bb17.jpg" align="middle">
</details>




<h2 id="Scalable-Autoregressive-Image-Generation-with-Mamba"><a href="#Scalable-Autoregressive-Image-Generation-with-Mamba" class="headerlink" title="Scalable Autoregressive Image Generation with Mamba"></a>Scalable Autoregressive Image Generation with Mamba</h2><p><strong>Authors:Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li</strong></p>
<p>We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mambaâ€™s core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM">https://github.com/hp-l33/AiM</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åŸºäºMambaæ¶æ„çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹AiMã€‚AiMé‡‡ç”¨Mambaè¿™ä¸€æ–°å‹çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œä»¥å…¶å¯¹é•¿åºåˆ—å»ºæ¨¡çš„å‡ºè‰²æ€§èƒ½ä»¥åŠçº¿æ€§æ—¶é—´å¤æ‚åº¦ä¸ºç‰¹ç‚¹ï¼Œæ—¨åœ¨å–ä»£è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å¸¸ç”¨çš„Transformerï¼Œä»¥å®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•é‡‡ç”¨Mambaé€šè¿‡å¤šæ–¹å‘æ‰«æå¤„ç†äºŒç»´ä¿¡å·ï¼ŒAiMç›´æ¥é‡‡ç”¨ä¸‹ä¸€ä»£é¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†éœ€è¦å¯¹Mambaè¿›è¡Œå¤§é‡ä¿®æ”¹ä»¥å­¦ä¹ äºŒç»´ç©ºé—´è¡¨ç¤ºçš„éœ€è¦ã€‚é€šè¿‡å¯¹è§†è§‰ç”Ÿæˆä»»åŠ¡è¿›è¡Œç®€å•è€Œæœ‰é’ˆå¯¹æ€§çš„ä¿®æ”¹ï¼Œæˆ‘ä»¬ä¿ç•™äº†Mambaçš„æ ¸å¿ƒç»“æ„ï¼Œå……åˆ†åˆ©ç”¨äº†å…¶é«˜æ•ˆçš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸åŒè§„æ¨¡çš„AiMæ¨¡å‹ï¼Œå‚æ•°æ•°é‡ä»148Måˆ°1.3Bä¸ç­‰ã€‚åœ¨ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æœ€å¥½çš„AiMæ¨¡å‹å®ç°äº†FIDä¸º2.21ï¼Œè¶…è¿‡äº†æ‰€æœ‰ç°æœ‰å‚æ•°ç›¸è¿‘çš„è‡ªå›å½’æ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæ˜¯æ‰©æ•£æ¨¡å‹çš„2åˆ°10å€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hp-l33/AiMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12245v3">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬ä»‹ç»äº†åŸºäºMambaæ¶æ„çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹AiMã€‚AiMåˆ©ç”¨å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦ã€é€‚ç”¨äºé•¿åºåˆ—å»ºæ¨¡çš„æ–°çŠ¶æ€ç©ºé—´æ¨¡å‹Mambaï¼Œæ›¿ä»£ARå›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å¸¸ç”¨çš„Transformerï¼Œæ—¨åœ¨å®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚AiMç›´æ¥é‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œæ— éœ€å¯¹Mambaè¿›è¡Œå¤§é‡ä¿®æ”¹ä»¥é€‚åº”äºŒç»´ä¿¡å·ã€‚æˆ‘ä»¬åœ¨å„ç§è§„æ¨¡çš„ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸Šï¼Œæœ€å¥½çš„AiMæ¨¡å‹å®ç°äº†FIDä¸º2.21ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰å‚æ•°ç›¸è¿‘çš„ARæ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—å¿«äºæ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AiMæ˜¯ä¸€ä¸ªåŸºäºMambaæ¶æ„çš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>Mambaæ˜¯ä¸€ä¸ªæ–°çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œé€‚ç”¨äºé•¿åºåˆ—å»ºæ¨¡ã€‚</li>
<li>AiMåˆ©ç”¨Mambaæ›¿ä»£Transformerï¼Œå®ç°æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>AiMé‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œæ— éœ€å¯¹Mambaè¿›è¡Œå¤§é‡ä¿®æ”¹ä»¥é€‚åº”äºŒç»´ä¿¡å·ã€‚</li>
<li>AiMåœ¨å„ç§è§„æ¨¡çš„ImageNet1K 256*256åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæœ€ä½³æ¨¡å‹çš„FIDä¸º2.21ã€‚</li>
<li>AiMåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å‚æ•°ç›¸è¿‘çš„ç°æœ‰è‡ªå›å½’æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f12290be29c2e4f6dffc3b385d4360ef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7081497a5e348adb3d9b8e58da6baf6c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-52b010c7b40b64ac0c995f5595fca100.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3690e88e538007c181f769678283c6e4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9315d9a228dce5cde9174681dbb3a684.jpg" align="middle">
</details>




<h2 id="EvolvED-Evolutionary-Embeddings-to-Understand-the-Generation-Process-of-Diffusion-Models"><a href="#EvolvED-Evolutionary-Embeddings-to-Understand-the-Generation-Process-of-Diffusion-Models" class="headerlink" title="EvolvED: Evolutionary Embeddings to Understand the Generation Process of   Diffusion Models"></a>EvolvED: Evolutionary Embeddings to Understand the Generation Process of   Diffusion Models</h2><p><strong>Authors:Vidya Prasad, Hans van Gorp, Christina Humer, Ruud J. G. van Sloun, Anna Vilanova, Nicola Pezzotti</strong></p>
<p>Diffusion models, widely used in image generation, rely on iterative refinement to generate images from noise. Understanding this data evolution is important for model development and interpretability, yet challenging due to its high-dimensional, iterative nature. Prior works often focus on static or instance-level analyses, missing the iterative and holistic aspects of the generative path. While dimensionality reduction can visualize image evolution for few instances, it does preserve the iterative structure. To address these gaps, we introduce EvolvED, a method that presents a holistic view of the iterative generative process in diffusion models. EvolvED goes beyond instance exploration by leveraging predefined research questions to streamline generative space exploration. Tailored prompts aligned with these questions are used to extract intermediate images, preserving iterative context. Targeted feature extractors trace the evolution of key image attribute evolution, addressing the complexity of high-dimensional outputs. Central to EvolvED is a novel evolutionary embedding algorithm that encodes iterative steps while maintaining semantic relations. It enhances the visualization of data evolution by clustering semantically similar elements within each iteration with t-SNE, grouping elements by iteration, and aligning an instanceâ€™s elements across iterations. We present rectilinear and radial layouts to represent iterations and support exploration. We apply EvolvED to diffusion models like GLIDE and Stable Diffusion, demonstrating its ability to provide valuable insights into the generative process. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆï¼Œä¾èµ–äºè¿­ä»£ç»†åŒ–ä»å™ªå£°ä¸­ç”Ÿæˆå›¾åƒã€‚äº†è§£æ•°æ®çš„æ¼”å˜å¯¹äºæ¨¡å‹å‘å±•å’Œå¯è§£é‡Šæ€§å¾ˆé‡è¦ï¼Œä½†ç”±äºå…¶é«˜ç»´åº¦ã€è¿­ä»£æ€§è´¨ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä»¥å‰çš„å·¥ä½œç»å¸¸å…³æ³¨é™æ€æˆ–å®ä¾‹çº§åˆ«çš„åˆ†æï¼Œå¿½ç•¥äº†ç”Ÿæˆè·¯å¾„çš„è¿­ä»£å’Œæ•´ä½“æ–¹é¢ã€‚è™½ç„¶é™ç»´å¯ä»¥ä¸ºå°‘æ•°å®ä¾‹å¯è§†åŒ–å›¾åƒæ¼”å˜ï¼Œä½†å®ƒå¹¶ä¸ä¿ç•™è¿­ä»£ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EvolvEDï¼Œä¸€ç§å‘ˆç°æ‰©æ•£æ¨¡å‹ä¸­è¿­ä»£ç”Ÿæˆè¿‡ç¨‹æ•´ä½“è§†å›¾çš„æ–¹æ³•ã€‚EvolvEDé€šè¿‡åˆ©ç”¨é¢„å…ˆè®¾å®šçš„ç ”ç©¶é—®é¢˜æ¥ç®€åŒ–ç”Ÿæˆç©ºé—´æ¢ç´¢ï¼Œè¶…è¶Šäº†å®ä¾‹æ¢ç´¢ã€‚æ ¹æ®è¿™äº›é—®é¢˜å®šåˆ¶çš„æç¤ºç”¨äºæå–ä¸­é—´å›¾åƒï¼Œä¿ç•™è¿­ä»£ä¸Šä¸‹æ–‡ã€‚æœ‰é’ˆå¯¹æ€§çš„ç‰¹å¾æå–å™¨è¿½è¸ªå…³é”®å›¾åƒå±æ€§æ¼”å˜çš„è¿‡ç¨‹ï¼Œè§£å†³é«˜ç»´åº¦è¾“å‡ºçš„å¤æ‚æ€§ã€‚EvolvEDçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹è¿›åŒ–åµŒå…¥ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç¼–ç è¿­ä»£æ­¥éª¤çš„åŒæ—¶ä¿æŒè¯­ä¹‰å…³ç³»ã€‚å®ƒé€šè¿‡t-SNEå°†æ¯ä¸ªè¿­ä»£ä¸­è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å…ƒç´ è¿›è¡Œèšç±»ï¼ŒæŒ‰è¿­ä»£å¯¹å…ƒç´ è¿›è¡Œåˆ†ç»„ï¼Œå¹¶è·¨è¿­ä»£å¯¹é½å®ä¾‹çš„å…ƒç´ ï¼Œä»è€Œå¢å¼ºäº†æ•°æ®æ¼”å˜çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬æä¾›äº†ç›´çº¿å’Œå¾„å‘å¸ƒå±€æ¥è¡¨ç¤ºè¿­ä»£å¹¶æ”¯æŒæ¢ç´¢ã€‚æˆ‘ä»¬å°†EvolvEDåº”ç”¨äºGLIDEå’ŒStable Diffusionç­‰æ‰©æ•£æ¨¡å‹ï¼Œå±•ç¤ºäº†å®ƒä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›æœ‰ä»·å€¼è§è§£çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17462v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä¾é è¿­ä»£ä¼˜åŒ–ä»å™ªå£°ç”Ÿæˆå›¾åƒã€‚äº†è§£æ•°æ®æ¼”åŒ–å¯¹æ¨¡å‹å‘å±•å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ï¼Œä½†ç”±äºå…¶é«˜ç»´è¿­ä»£ç‰¹æ€§ï¼Œå­˜åœ¨æŒ‘æˆ˜ã€‚æ­¤å‰çš„ç ”ç©¶å¾€å¾€ä¾§é‡äºé™æ€æˆ–å®ä¾‹çº§åˆ«çš„åˆ†æï¼Œå¿½è§†äº†ç”Ÿæˆè·¯å¾„çš„è¿­ä»£å’Œæ•´ä½“æ–¹é¢ã€‚æœ¬æ–‡æå‡ºEvolvEDæ–¹æ³•ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„è¿­ä»£ç”Ÿæˆè¿‡ç¨‹æä¾›å…¨é¢è§†è§’ã€‚EvolvEDä¸ä»…æ¢ç´¢å®ä¾‹ï¼Œè¿˜åˆ©ç”¨é¢„è®¾çš„ç ”ç©¶é—®é¢˜æ¥å¼•å¯¼ç”Ÿæˆç©ºé—´æ¢ç´¢ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æç¤ºæ¥æå–ä¸­é—´å›¾åƒï¼Œä¿æŒè¿­ä»£ä¸Šä¸‹æ–‡ã€‚ç›®æ ‡ç‰¹å¾æå–å™¨è¿½è¸ªå…³é”®å›¾åƒå±æ€§æ¼”å˜ï¼Œè§£å†³é«˜ç»´è¾“å‡ºçš„å¤æ‚æ€§ã€‚EvolvEDçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹è¿›åŒ–åµŒå…¥ç®—æ³•ï¼Œå¯åœ¨ä¿æŒè¯­ä¹‰å…³ç³»çš„åŒæ—¶ç¼–ç è¿­ä»£æ­¥éª¤ã€‚é€šè¿‡t-SNEå¯¹è¯­ä¹‰ç›¸ä¼¼å…ƒç´ è¿›è¡Œèšç±»ï¼ŒæŒ‰è¿­ä»£åˆ†ç»„å…ƒç´ ï¼Œå¹¶åœ¨è¿­ä»£ä¹‹é—´å¯¹é½å®ä¾‹å…ƒç´ ã€‚é‡‡ç”¨ç›´è§’åæ ‡å’Œå¾„å‘å¸ƒå±€è¡¨ç¤ºè¿­ä»£å¹¶æ”¯æŒæ¢ç´¢ã€‚æˆ‘ä»¬å°†EvolvEDåº”ç”¨äºGLIDEå’ŒStable Diffusionç­‰æ‰©æ•£æ¨¡å‹ï¼Œå±•ç¤ºå…¶å¯¹ç”Ÿæˆè¿‡ç¨‹æä¾›æœ‰ä»·å€¼æ´å¯Ÿçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¾é è¿­ä»£ä¼˜åŒ–ä»å™ªå£°ç”Ÿæˆå›¾åƒï¼Œç†è§£æ•°æ®æ¼”åŒ–å¯¹æ¨¡å‹å‘å±•å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶å¤šä¾§é‡äºé™æ€æˆ–å®ä¾‹çº§åˆ«çš„åˆ†æï¼Œå¿½è§†äº†æ‰©æ•£æ¨¡å‹çš„è¿­ä»£å’Œæ•´ä½“æ–¹é¢ã€‚</li>
<li>EvolvEDæ–¹æ³•ä¸ºæ‰©æ•£æ¨¡å‹çš„è¿­ä»£ç”Ÿæˆè¿‡ç¨‹æä¾›å…¨é¢è§†è§’ï¼ŒåŒ…æ‹¬åˆ©ç”¨é¢„è®¾ç ”ç©¶é—®é¢˜å’Œæœ‰é’ˆå¯¹æ€§çš„æç¤ºè¿›è¡Œæ¢ç´¢ã€‚</li>
<li>ç›®æ ‡ç‰¹å¾æå–å™¨èƒ½å¤Ÿè¿½è¸ªå…³é”®å›¾åƒå±æ€§æ¼”å˜ã€‚</li>
<li>æ–°å‹è¿›åŒ–åµŒå…¥ç®—æ³•æ˜¯EvolvEDçš„æ ¸å¿ƒï¼Œå¯åœ¨ä¿æŒè¯­ä¹‰å…³ç³»çš„åŒæ—¶ç¼–ç è¿­ä»£æ­¥éª¤ã€‚</li>
<li>é€šè¿‡t-SNEå¯¹è¯­ä¹‰ç›¸ä¼¼å…ƒç´ è¿›è¡Œèšç±»ï¼ŒæŒ‰è¿­ä»£åˆ†ç»„å…ƒç´ ï¼Œå±•ç¤ºæ•°æ®æ¼”åŒ–çš„å¯è§†åŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-75566319f5d337bb575cea1f6b3b8f4c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1db2ca04ebb2f197b82a761f00091a3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a3092c430d36df5f080356ba8cae3c66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d50933f392876def70c174fed3abc58f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7203375e3eb793fbb6a918817e6c17b4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46e21923d0cde497725267e88773e7e4.jpg" align="middle">
</details>




<h2 id="Ctrl-X-Controlling-Structure-and-Appearance-for-Text-To-Image-Generation-Without-Guidance"><a href="#Ctrl-X-Controlling-Structure-and-Appearance-for-Text-To-Image-Generation-Without-Guidance" class="headerlink" title="Ctrl-X: Controlling Structure and Appearance for Text-To-Image   Generation Without Guidance"></a>Ctrl-X: Controlling Structure and Appearance for Text-To-Image   Generation Without Guidance</h2><p><strong>Authors:Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou</strong></p>
<p>Recent controllable generation approaches such as FreeControl and Diffusion Self-Guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model. See our project page for an overview of the results: <a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¯æ§ç”Ÿæˆæ–¹æ³•ï¼Œå¦‚FreeControlå’ŒDiffusion Self-Guidanceï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹å¸¦æ¥äº†ç²¾ç»†çš„ç©ºé—´å’Œå¤–è§‚æ§åˆ¶ï¼Œè€Œæ— éœ€è®­ç»ƒè¾…åŠ©æ¨¡å—ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é’ˆå¯¹æ¯ç§ç±»å‹çš„åˆ†æ•°å‡½æ•°ä¼˜åŒ–æ½œåœ¨åµŒå…¥ï¼Œæ‰©æ•£æ­¥éª¤è¾ƒé•¿ï¼Œä½¿å¾—ç”Ÿæˆè¿‡ç¨‹è€—æ—¶ï¼Œå¹¶é™åˆ¶äº†å…¶çµæ´»æ€§å’Œä½¿ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†Ctrl-Xï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºT2Iæ‰©æ•£æ§åˆ¶ç»“æ„å’Œå¤–è§‚çš„ç®€å•æ¡†æ¶ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æŒ‡å¯¼ã€‚Ctrl-Xè®¾è®¡å‰é¦ˆç»“æ„æ§åˆ¶ä»¥å®ç°ä¸ç»“æ„å›¾åƒçš„ç»“æ„å¯¹é½ï¼Œå¹¶è®¾è®¡è¯­ä¹‰æ„ŸçŸ¥å¤–è§‚ä¼ è¾“ä»¥ä¿ƒè¿›ä»ç”¨æˆ·è¾“å…¥å›¾åƒè¿›è¡Œå¤–è§‚ä¼ è¾“ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒCtrl-Xåœ¨å„ç§æ¡ä»¶è¾“å…¥å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¸Šçš„æ€§èƒ½ä¼˜è¶Šã€‚ç‰¹åˆ«åœ°ï¼ŒCtrl-Xæ”¯æŒå…·æœ‰ä»»æ„æ¡ä»¶å›¾åƒï¼ˆä»»ä½•æ¨¡å¼ï¼‰çš„æ–°ç»“æ„å’Œå¤–è§‚æ§åˆ¶ï¼Œä¸ç°æœ‰ä½œå“ç›¸æ¯”å±•ç°å‡ºä¼˜è¶Šçš„å›¾åƒè´¨é‡å’Œå¤–è§‚ä¼ è¾“èƒ½åŠ›ï¼Œå¹¶ä¸ºä»»ä½•T2Iå’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰çš„æ‰©æ•£æ¨¡å‹æä¾›äº†å³æ—¶æ’å…¥å’Œæ’­æ”¾åŠŸèƒ½ã€‚æƒ³äº†è§£æ›´å¤šå®éªŒç»“æœï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07540v2">PDF</a> 22 pages, 17 figures, see project page at   <a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¯æ§ç”Ÿæˆæ–¹æ³•å¦‚FreeControlå’ŒDiffusion Self-Guidanceä¸ºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¸¦æ¥äº†ç²¾ç»†çš„æ—¶ç©ºå’Œå¤–è§‚æ§åˆ¶ï¼Œæ— éœ€è®­ç»ƒè¾…åŠ©æ¨¡å—ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é’ˆå¯¹æ¯ç§ç±»å‹çš„åˆ†æ•°å‡½æ•°ä¼˜åŒ–æ½œåœ¨åµŒå…¥ï¼Œå¹¶å¢åŠ äº†æ‰©æ•£æ­¥éª¤ï¼Œä½¿å¾—ç”Ÿæˆè¿‡ç¨‹è€—æ—¶ï¼Œå¹¶é™åˆ¶äº†å…¶çµæ´»æ€§å’Œä½¿ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºCtrl-Xï¼Œä¸€ä¸ªç”¨äºT2Iæ‰©æ•£æ§åˆ¶ç»“æ„å’Œå¤–è§‚çš„ç®€å•æ¡†æ¶ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æŒ‡å¯¼ã€‚Ctrl-Xè®¾è®¡å‰é¦ˆç»“æ„æ§åˆ¶ä»¥å®ç°ç»“æ„å›¾åƒçš„ç»“æ„å¯¹é½å’Œç”¨æˆ·è¾“å…¥å›¾åƒçš„è¯­ä¹‰æ„ŸçŸ¥å¤–è§‚ä¼ è¾“ï¼Œä»¥ä¿ƒè¿›å¤–è§‚çš„ä¼ è¾“ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒCtrl-Xåœ¨å„ç§æ¡ä»¶è¾“å…¥å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¸Šå…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«åœ°ï¼ŒCtrl-Xæ”¯æŒå…·æœ‰ä»»æ„æ¨¡æ€æ¡ä»¶å›¾åƒçš„æ–°é¢–ç»“æ„å’Œå¤–è§‚æ§åˆ¶ï¼Œä¸ç°æœ‰ä½œå“ç›¸æ¯”å±•ç°å‡ºå“è¶Šçš„å›¾åƒè´¨é‡å’Œå¤–è§‚è½¬ç§»æ•ˆæœï¼Œå¹¶ä¸ºä»»ä½•T2Iå’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹æä¾›å³æ—¶å³ç”¨çš„åŠŸèƒ½ã€‚æœ‰å…³ç»“æœçš„æ¦‚è¿°ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x%E3%80%82">https://genforce.github.io/ctrl-xã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ctrl-Xæ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å¯æ§ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>å®ƒå®ç°äº†ç»“æ„æ§åˆ¶å’Œå¤–è§‚æ§åˆ¶ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æŒ‡å¯¼ã€‚</li>
<li>Ctrl-Xé€šè¿‡å‰é¦ˆç»“æ„æ§åˆ¶å®ç°ç»“æ„å¯¹é½å’Œè¯­ä¹‰æ„ŸçŸ¥çš„å¤–è§‚ä¼ è¾“ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCtrl-Xåœ¨ç»“æ„å’Œå¤–è§‚æ§åˆ¶æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Ctrl-Xæ”¯æŒä»»æ„æ¨¡æ€æ¡ä»¶å›¾åƒçš„æ–°é¢–ç»“æ„å’Œå¤–è§‚æ§åˆ¶ã€‚</li>
<li>Ctrl-Xæä¾›äº†é«˜å›¾åƒè´¨é‡å’Œå¤–è§‚è½¬ç§»æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1c7e485979e9ea6d843b17aa542d313c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b905cfd7c6b5741f6b971f6b709a8c1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-69c0ca886edc147424356c0976de6413.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a110dc2f9facee3193a8ddf3a250f813.jpg" align="middle">
</details>




<h2 id="RectifID-Personalizing-Rectified-Flow-with-Anchored-Classifier-Guidance"><a href="#RectifID-Personalizing-Rectified-Flow-with-Anchored-Classifier-Guidance" class="headerlink" title="RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance"></a>RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance</h2><p><strong>Authors:Zhicheng Sun, Zhenhao Yang, Yang Jin, Haozhe Chi, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Yang Song, Kun Gai, Yadong Mu</strong></p>
<p>Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at <a target="_blank" rel="noopener" href="https://github.com/feifeiobama/RectifID">https://github.com/feifeiobama/RectifID</a>. </p>
<blockquote>
<p>å®šåˆ¶æ‰©æ•£æ¨¡å‹ä»¥ä»ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒç”Ÿæˆèº«ä»½ä¿ç•™å›¾åƒæ˜¯ä¸€ä¸ªå¼•äººå…¥èƒœçš„æ–°é—®é¢˜ã€‚æµè¡Œçš„æ–¹æ³•é€šå¸¸éœ€è¦åœ¨å¯¹ç‰¹å®šé¢†åŸŸçš„å¤§é‡å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒä»¥å®ç°èº«ä»½ä¿ç•™ï¼Œè¿™åœ¨ä¸åŒçš„ä½¿ç”¨æƒ…å†µä¸‹ç¼ºä¹çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ†ç±»å™¨æŒ‡å¯¼è¿™ä¸€æ— éœ€è®­ç»ƒçš„æŠ€æœ¯ï¼Œä½¿ç”¨ç°æœ‰åˆ†ç±»å™¨æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæœ€æ–°çš„æ ¡æ­£æµæ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡ç®€å•çš„å®šç‚¹è§£å†³æ–¹æ¡ˆè§£å†³æ™®é€šåˆ†ç±»å™¨æŒ‡å¯¼çš„ä¸»è¦å±€é™æ€§ï¼Œå³éœ€è¦ç‰¹æ®Šåˆ†ç±»å™¨çš„é—®é¢˜ï¼Œä»è€Œå®ç°ä½¿ç”¨ç°æˆçš„å›¾åƒé‰´åˆ«å™¨è¿›è¡Œçµæ´»çš„ä¸ªæ€§åŒ–è®¾ç½®ã€‚è€Œä¸”ï¼Œå½“å…¶è§£å†³æ–¹æ¡ˆé”šå®šåˆ°å‚è€ƒæµè½¨è¿¹æ—¶ï¼Œå…¶è§£å†³è¿‡ç¨‹æ˜¯ç¨³å®šçš„ï¼Œå¹¶ä¸”æœ‰æ”¶æ•›ä¿è¯ã€‚æ‰€æ¨å¯¼çš„æ–¹æ³•åœ¨ä¸åŒçš„ç°æˆå›¾åƒé‰´åˆ«å™¨ä¸Šå®ç°äº†æ ¡æ­£æµï¼Œä¸ºäººè„¸ã€æ´»ç”Ÿç”Ÿä¸»ä½“å’ŒæŸäº›ç‰©ä½“ç”Ÿæˆäº†æœ‰åˆ©çš„ä¸ªæ€§åŒ–ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/feifeiobama/RectifID%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/feifeiobama/RectifIDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14677v4">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹æ ¹æ®ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒç”Ÿæˆèº«ä»½ä¿ç•™å›¾åƒæ˜¯ä¸€ä¸ªæ–°å…´çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬é‡‡å–äº†åˆ†ç±»å™¨å¼•å¯¼çš„è®­ç»ƒå‰æŠ€æœ¯æ¥ä¸ªæ€§åŒ–ç”Ÿæˆå›¾åƒï¼Œè¿™ä¸éœ€è¦ç‰¹å®šçš„åˆ†ç±»å™¨å³å¯æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚æœ¬ç ”ç©¶è¡¨æ˜åˆ©ç”¨ç°æœ‰å›¾åƒé‰´åˆ«å™¨è¿›è¡Œä¸ªæ€§åŒ–ç”Ÿæˆå…·æœ‰çµæ´»æ€§å’Œç¨³å®šæ€§ï¼Œå¹¶å·²åœ¨äººè„¸ã€æ´»ä½“ç‰©ä½“å’ŒæŸäº›å¯¹è±¡ä¸Šå–å¾—äº†ä¼˜åŠ¿ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶èšç„¦äºå¦‚ä½•åœ¨æ‰©æ•£æ¨¡å‹ä¸­è‡ªå®šä¹‰ç”Ÿæˆèº«ä»½ä¿ç•™çš„å›¾åƒã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡ç‰¹å®šé¢†åŸŸçš„å›¾åƒè¿›è¡Œè®­ç»ƒä»¥å®ç°èº«ä»½ä¿ç•™ï¼Œç¼ºä¹çµæ´»æ€§ã€‚</li>
<li>ç ”ç©¶è€…é‡‡ç”¨è®­ç»ƒå‰çš„åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ— éœ€ç‰¹å®šåˆ†ç±»å™¨å³å¯æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰å›¾åƒé‰´åˆ«å™¨è¿›è¡Œä¸ªæ€§åŒ–ç”Ÿæˆåœ¨è§£å†³ç¨‹åºä¸Šç›¸å¯¹ç¨³å®šã€‚æ­¤æ–¹æ³•æä¾›äº†ä¸€ä¸ªç®€å•ä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆæ¥é€‚åº”ä¸åŒçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚é€šè¿‡å¼•ç”¨ä¸€ä¸ªç®€å•çš„å›ºå®šç‚¹è§£å†³æ–¹æ¡ˆè§£å†³äº†æ™®é€šåˆ†ç±»å™¨å¼•å¯¼çš„ä¸»è¦å±€é™æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26f9c5ae4b60b66ffa9f62575a833d44.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a0b616068b083bb24b4588ce378aecb7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f4e35066b1a9cce3f36fdaa67ae742d3.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  GN-FRGeneralizable Neural Radiance Fields for Flare Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
