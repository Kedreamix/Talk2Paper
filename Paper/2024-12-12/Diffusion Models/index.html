<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    23.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    96 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="DMin-Scalable-Training-Data-Influence-Estimation-for-Diffusion-Models"><a href="#DMin-Scalable-Training-Data-Influence-Estimation-for-Diffusion-Models" class="headerlink" title="DMin: Scalable Training Data Influence Estimation for Diffusion Models"></a>DMin: Scalable Training Data Influence Estimation for Diffusion Models</h2><p><strong>Authors:Huawei Lin, Yingjie Lao, Weijie Zhao</strong></p>
<p>Identifying the training data samples that most influence a generated image is a critical task in understanding diffusion models, yet existing influence estimation methods are constrained to small-scale or LoRA-tuned models due to computational limitations. As diffusion models scale up, these methods become impractical. To address this challenge, we propose DMin (Diffusion Model influence), a scalable framework for estimating the influence of each training data sample on a given generated image. By leveraging efficient gradient compression and retrieval techniques, DMin reduces storage requirements from 339.39 TB to only 726 MB and retrieves the top-k most influential training samples in under 1 second, all while maintaining performance. Our empirical results demonstrate DMin is both effective in identifying influential training samples and efficient in terms of computational and storage requirements. </p>
<blockquote>
<p>识别和评估对生成图像影响最大的训练数据样本是理解扩散模型的关键任务。然而，由于计算限制，现有的影响评估方法仅限于小规模或经过LoRA调整过的模型。随着扩散模型的扩展，这些方法变得不切实际。为了解决这一挑战，我们提出了DMin（扩散模型影响力）框架，这是一个可扩展的框架，用于估计每个训练数据样本对给定生成图像的影响。通过利用高效的梯度压缩和检索技术，DMin将存储需求从339.39TB减少到仅726MB，并在不到1秒内检索到前k个最具影响力的训练样本，同时保持性能。我们的实证结果表明，DMin在识别有影响力的训练样本方面非常有效，同时在计算和存储需求方面也非常高效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08637v1">PDF</a> 14 pages, 6 figures, 8 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为DMin的扩散模型影响估计框架，用于评估每个训练数据样本对生成图像的影响。该框架通过利用高效的梯度压缩和检索技术，实现了对大规模扩散模型的实用影响估计，在存储需求方面从339.39TB缩减至仅726MB，并在不到1秒内检索到前k个最具影响力的训练样本。实证结果表明，DMin在识别有影响力的训练样本方面既有效又高效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DMin框架用于评估训练数据样本对生成图像的影响。</li>
<li>扩散模型影响估计面临计算限制的挑战。</li>
<li>DMin通过梯度压缩和检索技术实现了高效的影响估计。</li>
<li>DMin将存储需求从大量TB级别降低到仅MB级别。</li>
<li>DMin能在不到一秒的时间内检索到最具影响力的训练样本。</li>
<li>实证结果表明DMin在识别影响力样本方面既有效又高效。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b91c6c92f0168d31ec8753dab1d6e4cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8430fd7a7b4d2e453e4c528a89a370c1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43bf3c416ff96d38aeebeb435464daed.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cfd4f8873a6dfcd15ea8920862474249.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cd5f6f9b1b1c461fd1cdba2a7dfb40b0.jpg" align="middle">
</details>




<h2 id="TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person"><a href="#TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person" class="headerlink" title="TryOffAnyone: Tiled Cloth Generation from a Dressed Person"></a>TryOffAnyone: Tiled Cloth Generation from a Dressed Person</h2><p><strong>Authors:Ioannis Xarchakos, Theodoros Koukopoulos</strong></p>
<p>The fashion industry is increasingly leveraging computer vision and deep learning technologies to enhance online shopping experiences and operational efficiencies. In this paper, we address the challenge of generating high-fidelity tiled garment images essential for personalized recommendations, outfit composition, and virtual try-on systems from photos of garments worn by models. Inspired by the success of Latent Diffusion Models (LDMs) in image-to-image translation, we propose a novel approach utilizing a fine-tuned StableDiffusion model. Our method features a streamlined single-stage network design, which integrates garmentspecific masks to isolate and process target clothing items effectively. By simplifying the network architecture through selective training of transformer blocks and removing unnecessary crossattention layers, we significantly reduce computational complexity while achieving state-of-the-art performance on benchmark datasets like VITON-HD. Experimental results demonstrate the effectiveness of our approach in producing high-quality tiled garment images for both full-body and half-body inputs. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone">https://github.com/ixarchakos/try-off-anyone</a> </p>
<blockquote>
<p>时尚产业正越来越多地利用计算机视觉和深度学习技术，以提升在线购物体验和运营效率。本文旨在解决从模特穿着的服装照片生成高质量平铺服装图像的挑战，这些图像对于个性化推荐、服装搭配和虚拟试穿系统至关重要。受潜在扩散模型（Latent Diffusion Models，简称LDMs）在图到图翻译中的成功的启发，我们提出了一种利用微调过的StableDiffusion模型的新方法。我们的方法采用简洁的单阶段网络设计，结合服装特定掩膜，有效地隔离和处理目标服装项目。通过有选择地训练变压器块并删除不必要的交叉注意层，简化了网络架构，我们在降低计算复杂性的同时，在VITON-HD等基准数据集上实现了最先进的性能。实验结果证明了我们的方法在生成全身和半身输入的高质量平铺服装图像方面的有效性。代码和模型可在[<a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/ixarchakos/try-off-anyone找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08573v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>时尚产业正积极运用计算机视觉和深度学习技术来提升线上购物体验和运营效率。本文聚焦生成高保真拼接衣物图像的挑战，这对于个性化推荐、搭配组合和虚拟试衣系统至关重要。研究团队受潜在扩散模型（Latent Diffusion Models，简称LDMs）在图像翻译领域的成功启发，提出了一种利用精细调整的StableDiffusion模型的新方法。该方法采用简洁的单阶段网络设计，通过服装特定掩膜有效隔离和处理目标服装项目。通过选择性训练变压器块并去除不必要的交叉注意力层，简化网络架构，在降低计算复杂性的同时，在VITON-HD等基准数据集上实现了卓越的性能。实验结果表明，该方法在全身体和半身输入下都能生成高质量的拼接衣物图像。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>时尚产业正在融合计算机视觉和深度学习技术以提升在线购物体验。</li>
<li>生成高保真拼接衣物图像是时尚科技领域的一个重要挑战。</li>
<li>此研究利用潜在扩散模型（LDMs）在图像翻译中的成功，提出一种基于StableDiffusion模型的新方法。</li>
<li>新方法采用简洁的单阶段网络设计，并通过服装特定掩膜处理目标服装项目。</li>
<li>通过简化网络架构，该方法在降低计算复杂性的同时，实现了在基准数据集上的卓越性能。</li>
<li>实验证明该方法能生成高质量的拼接衣物图像，适用于全身体和半身输入。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4858b57b9c75d47132c65f2050bc5fe1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2856b5c51d6af52f31848166ea39e62f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5bd374d5d4a708e65208387522338bb4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3127daa555f76175132b4ef21712c878.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7748cecb9cb068dc6d44eb58141f9a77.jpg" align="middle">
</details>




<h2 id="Learning-Flow-Fields-in-Attention-for-Controllable-Person-Image-Generation"><a href="#Learning-Flow-Fields-in-Attention-for-Controllable-Person-Image-Generation" class="headerlink" title="Learning Flow Fields in Attention for Controllable Person Image   Generation"></a>Learning Flow Fields in Attention for Controllable Person Image   Generation</h2><p><strong>Authors:Zijian Zhou, Shikun Liu, Xiao Han, Haozhe Liu, Kam Woh Ng, Tian Xie, Yuren Cong, Hang Li, Mengmeng Xu, Juan-Manuel Pérez-Rúa, Aditya Patel, Tao Xiang, Miaojing Shi, Sen He</strong></p>
<p>Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the person’s appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models. </p>
<blockquote>
<p>可控人物图像生成的目标是依据参考图像生成人物图像，并对人物的外观或姿势进行精确控制。然而，尽管整体图像质量很高，但之前的方法往往会扭曲参考图像的细节纹理。我们认为这些扭曲是由于对参考图像中相应区域的关注不足所导致的。</p>
</blockquote>
<p>为了解决这一问题，我们提出了学习注意力流场（Leffa），在训练过程中明确引导目标查询在注意力层关注正确的参考键。具体而言，这是通过在基于扩散的基线之上实现注意力映射的正则化损失来实现的。我们的广泛实验表明，Leffa在控制外观（虚拟试穿）和姿势（姿势转换）方面达到了最新技术水平，在保持高图像质量的同时，显著减少了细节纹理的扭曲。此外，我们还证明我们的损失模型具有模型无关性，可用于提高其他扩散模型的性能。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08486v1">PDF</a> github: <a target="_blank" rel="noopener" href="https://github.com/franciszzj/Leffa">https://github.com/franciszzj/Leffa</a>, demo:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/franciszzj/Leffa">https://huggingface.co/spaces/franciszzj/Leffa</a>, model:   <a target="_blank" rel="noopener" href="https://huggingface.co/franciszzj/Leffa">https://huggingface.co/franciszzj/Leffa</a></p>
<p><strong>Summary</strong></p>
<p>基于参考图像进行可控人物图像生成的目标是在给定参考图像的情况下生成人物图像，并能够对人物的外观或姿态进行精确控制。然而，先前的方法常常在保持整体图像质量的同时，丢失了参考图像中的细节纹理。我们认为这些失真问题源于对参考图像中对应区域的关注不足。为解决这一问题，我们提出了学习注意力流场（Leffa）的方法，通过在训练过程中引导目标查询在注意力层关注正确的参考关键信息，以提高对细节的关注。具体来说，这是通过在基于扩散的基线模型上的注意力图引入正则化损失来实现的。我们的广泛实验表明，Leffa在控制外观（虚拟试穿）和姿态（姿态迁移）方面达到了最先进的性能，显著减少了细节失真，同时保持了高图像质量。此外，我们还展示了我们的损失模型具有模型无关性，可以用于提高其他扩散模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>可控人物图像生成旨在基于参考图像生成人物图像，并能对人物的外观或姿态进行精确控制。</li>
<li>现有方法常在保持整体图像质量的同时忽略细节纹理。</li>
<li>细节纹理失真问题源于对参考图像中对应区域的关注不足。</li>
<li>学习注意力流场（Leffa）方法通过引导目标查询在注意力层关注正确的参考关键信息，提高细节关注度。</li>
<li>Leffa通过基于扩散的基线模型上的注意力图引入正则化损失来实现。</li>
<li>Leffa在控制外观和姿态方面表现优异，显著减少细节失真，同时保持高图像质量。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-54cb028b4daea989345268fd31a8c299.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-69066777cbb33ba1d954ab23046a5510.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3f4644bc452d3ca3cef1d2cfd7b36345.jpg" align="middle">
</details>




<h2 id="InvDiff-Invariant-Guidance-for-Bias-Mitigation-in-Diffusion-Models"><a href="#InvDiff-Invariant-Guidance-for-Bias-Mitigation-in-Diffusion-Models" class="headerlink" title="InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models"></a>InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models</h2><p><strong>Authors:Min Hou, Yueying Wu, Chang Xu, Yu-Hao Huang, Chenxi Bai, Le Wu, Jiang Bian</strong></p>
<p>As one of the most successful generative models, diffusion models have demonstrated remarkable efficacy in synthesizing high-quality images. These models learn the underlying high-dimensional data distribution in an unsupervised manner. Despite their success, diffusion models are highly data-driven and prone to inheriting the imbalances and biases present in real-world data. Some studies have attempted to address these issues by designing text prompts for known biases or using bias labels to construct unbiased data. While these methods have shown improved results, real-world scenarios often contain various unknown biases, and obtaining bias labels is particularly challenging. In this paper, we emphasize the necessity of mitigating bias in pre-trained diffusion models without relying on auxiliary bias annotations. To tackle this problem, we propose a framework, InvDiff, which aims to learn invariant semantic information for diffusion guidance. Specifically, we propose identifying underlying biases in the training data and designing a novel debiasing training objective. Then, we employ a lightweight trainable module that automatically preserves invariant semantic information and uses it to guide the diffusion model’s sampling process toward unbiased outcomes simultaneously. Notably, we only need to learn a small number of parameters in the lightweight learnable module without altering the pre-trained diffusion model. Furthermore, we provide a theoretical guarantee that the implementation of InvDiff is equivalent to reducing the error upper bound of generalization. Extensive experimental results on three publicly available benchmarks demonstrate that InvDiff effectively reduces biases while maintaining the quality of image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Hundredl/InvDiff">https://github.com/Hundredl/InvDiff</a>. </p>
<blockquote>
<p>作为最成功的生成模型之一，扩散模型在合成高质量图像方面表现出了显著的效率。这些模型以无监督的方式学习潜在的高维数据分布。尽管它们很成功，但扩散模型是高度数据驱动的，并且容易继承真实数据中的不平衡和偏见。一些研究试图通过为已知偏见设计文本提示或使用偏见标签来构建无偏见数据来解决这些问题。虽然这些方法已经显示出改进的结果，但现实世界的场景通常包含各种未知的偏见，并且获取偏见标签具有特别挑战性。在本文中，我们强调了在预训练的扩散模型中减轻偏见的必要性，而无需依赖辅助偏见注释。为了解决这一问题，我们提出了一种名为InvDiff的框架，旨在学习扩散指导的不变语义信息。具体来说，我们提出识别训练数据中的潜在偏见，并设计一种新的去偏训练目标。然后，我们采用一个轻量级的可训练模块，该模块可以自动保留不变语义信息，并将其用于指导扩散模型的采样过程，以产生无偏的结果。值得注意的是，我们只需要在轻量级的可训练模块中学习少量的参数，而无需更改预训练的扩散模型。此外，我们提供了理论保证，证明InvDiff的实施相当于降低了泛化的误差上限。在三个公开可用基准测试上的广泛实验结果表明，InvDiff在减少偏见的同时保持了图像生成的品质。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Hundredl/InvDiff">https://github.com/Hundredl/InvDiff</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08480v1">PDF</a> KDD 2025</p>
<p><strong>Summary</strong><br>    扩散模型在合成高质量图像方面表现出显著效果，但存在数据驱动带来的偏见问题。本文提出一种名为InvDiff的框架，旨在学习扩散指导的不变语义信息，以减轻预训练扩散模型中的偏见。该框架通过识别训练数据中的潜在偏见并设计去偏训练目标，采用轻量级可训练模块自动保留不变语义信息，引导扩散模型的采样过程得到无偏见的结果。只需在轻量级可训练模块中学习少量参数，无需更改预训练的扩散模型。实验结果表明，InvDiff在减少偏见的同时保持了图像生成的品质。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在合成高质量图像方面表现出卓越性能。</li>
<li>扩散模型易受到真实世界数据中的偏见影响。</li>
<li>已知偏见的处理方法包括设计文本提示和使用偏见标签构建无偏见数据，但处理未知偏见具有挑战性。</li>
<li>InvDiff框架被提出用于处理预训练扩散模型中的偏见问题。</li>
<li>InvDiff通过识别训练数据中的潜在偏见并设计去偏训练目标来实现。</li>
<li>InvDiff采用轻量级可训练模块，自动保留不变语义信息，并用于指导扩散模型的采样过程。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e62a8822f6dfdf50634b68b964c5f1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-06d138724421f3c0520b072624a6ddbc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f56f88fec6e82de1849708de23501d90.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fcdc020a6ab8c7711e65c8d147a5fd1d.jpg" align="middle">
</details>




<h2 id="CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis"><a href="#CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis" class="headerlink" title="CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis"></a>CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis</h2><p><strong>Authors:Mu Zhang, Yunfan Liu, Yue Liu, Hongtian Yu, Qixiang Ye</strong></p>
<p>Accurately depicting real-world landscapes in remote sensing (RS) images requires precise alignment between objects and their environment. However, most existing synthesis methods for natural images prioritize foreground control, often reducing the background to plain textures. This neglects the interaction between foreground and background, which can lead to incoherence in RS scenarios. In this paper, we introduce CC-Diff, a Diffusion Model-based approach for RS image generation with enhanced Context Coherence. To capture spatial interdependence, we propose a sequential pipeline where background generation is conditioned on synthesized foreground instances. Distinct learnable queries are also employed to model both the complex background texture and its semantic relation to the foreground. Extensive experiments demonstrate that CC-Diff outperforms state-of-the-art methods in visual fidelity, semantic accuracy, and positional precision, excelling in both RS and natural image domains. CC-Diff also shows strong trainability, improving detection accuracy by 2.04 mAP on DOTA and 2.25 mAP on the COCO benchmark. </p>
<blockquote>
<p>在遥感（RS）图像中准确描绘真实世界的景观要求物体与其环境之间有精确的对齐。然而，大多数现有的自然图像合成方法优先控制前景，通常将背景简化为平原纹理。这忽略了前景和背景之间的交互，可能导致遥感场景中的不连贯性。在本文中，我们介绍了CC-Diff，这是一种基于扩散模型的遥感图像生成方法，具有增强的上下文一致性。为了捕捉空间相关性，我们提出了一个顺序流程，其中背景生成是在合成的前景实例条件下进行的。我们还使用不同的可学习查询来对复杂的背景纹理及其与前景的语义关系进行建模。大量实验表明，CC-Diff在视觉保真度、语义准确性和定位精度方面优于最新方法，在遥感和自然图像领域均表现出色。CC-Diff还显示出强大的可训练性，在DOTA上提高了2.04 mAP的检测精度，在COCO基准测试上提高了2.25 mAP。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08464v1">PDF</a> </p>
<p><strong>Summary</strong><br>远程遥感图像中的真实世界景观准确描绘需要物体与其环境之间的精确对齐。现有大多数自然图像合成方法主要关注前景控制，往往将背景简化为简单纹理，忽略了前景与背景之间的交互作用，导致遥感场景中的不连贯性。本文提出了基于扩散模型的遥感图像生成方法CC-Diff，增强了上下文连贯性。通过构建空间依赖性的顺序流程，以合成的前景实例为条件进行背景生成。此外，还采用可学习的查询来模拟复杂的背景纹理及其与前景的语义关系。实验表明，CC-Diff在视觉保真度、语义准确性和定位精度方面均优于现有方法，在遥感图像和自然图像领域均表现出卓越性能。CC-Diff还显示了强大的可训练性，在DOTA和COCO基准测试中分别提高了2.04 mAP和2.25 mAP的检测精度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感图像准确描绘需要前景与背景的精确对齐。</li>
<li>现有方法主要关注前景控制，导致背景处理不足和场景不连贯。</li>
<li>CC-Diff方法基于扩散模型，增强了上下文连贯性。</li>
<li>通过顺序流程捕捉空间依赖性，以合成的前景为条件进行背景生成。</li>
<li>采用可学习查询模拟背景纹理与前景的语义关系。</li>
<li>CC-Diff在视觉、语义和定位精度方面优于现有方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-39f965fde7b978afd19e7cc0ca9f8b95.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9ed910a52932186b8173136b43049cc7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0614f7a5ceff9a03b46ed443d78c6d8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0df51cf4df07c894818863e62a13b9f4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c037adc9b093b6f98a1c87b90e337a76.jpg" align="middle">
</details>




<h2 id="Grasp-Diffusion-Network-Learning-Grasp-Generators-from-Partial-Point-Clouds-with-Diffusion-Models-in-SO-3-xR3"><a href="#Grasp-Diffusion-Network-Learning-Grasp-Generators-from-Partial-Point-Clouds-with-Diffusion-Models-in-SO-3-xR3" class="headerlink" title="Grasp Diffusion Network: Learning Grasp Generators from Partial Point   Clouds with Diffusion Models in SO(3)xR3"></a>Grasp Diffusion Network: Learning Grasp Generators from Partial Point   Clouds with Diffusion Models in SO(3)xR3</h2><p><strong>Authors:Joao Carvalho, An T. Le, Philipp Jahr, Qiao Sun, Julen Urain, Dorothea Koert, Jan Peters</strong></p>
<p>Grasping objects successfully from a single-view camera is crucial in many robot manipulation tasks. An approach to solve this problem is to leverage simulation to create large datasets of pairs of objects and grasp poses, and then learn a conditional generative model that can be prompted quickly during deployment. However, the grasp pose data is highly multimodal since there are several ways to grasp an object. Hence, in this work, we learn a grasp generative model with diffusion models to sample candidate grasp poses given a partial point cloud of an object. A novel aspect of our method is to consider diffusion in the manifold space of rotations and to propose a collision-avoidance cost guidance to improve the grasp success rate during inference. To accelerate grasp sampling we use recent techniques from the diffusion literature to achieve faster inference times. We show in simulation and real-world experiments that our approach can grasp several objects from raw depth images with $90%$ success rate and benchmark it against several baselines. </p>
<blockquote>
<p>在众多的机器人操作任务中，从单视角相机成功抓取物体至关重要。解决此问题的一个方法是利用仿真来创建大量的物体和抓取姿态配对数据集，然后学习一个条件生成模型，在部署时可以快速提示。然而，由于有多种方法可以抓取一个物体，抓取姿态数据是多模态的。因此，在这项工作中，我们使用扩散模型来学习抓取生成模型，根据物体的部分点云对候选抓取姿态进行采样。我们方法的一个新颖之处在于考虑在旋转流形空间中的扩散，并提出一种碰撞避免成本指导，以提高推理过程中的抓取成功率。为了加快抓取采样速度，我们采用了扩散文献中的最新技术来实现更快的推理时间。我们在模拟和真实世界的实验中都证明了我们的方法可以从原始深度图像中抓取多个物体，成功率达到90%，并将其与几个基准进行了比较。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种利用扩散模型来解决从单视角相机成功抓取物体的问题。该方法借助仿真创建大量的物体和抓取姿态数据集，学习条件生成模型，在部署时可快速提示。由于抓取姿态数据具有多模态性，本文的方法是在点云空间中对扩散模型进行训练，以采样候选抓取姿态。该方法考虑了在旋转流形空间中的扩散，并提出碰撞避免成本指导来提高推理阶段的抓取成功率。通过使用扩散文献中的最新技术，提高了采样速度。在模拟和真实实验中，该方法能够从原始深度图像中抓取多个物体，成功率达到90%，并与几个基线进行了比较。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用仿真创建大量的物体和抓取姿态数据集，学习条件生成模型。</li>
<li>扩散模型被用于解决从单视角相机抓取物体的问题。</li>
<li>抓取姿态数据具有多模态性，因此在点云空间中对扩散模型进行训练以采样候选抓取姿态。</li>
<li>考虑了在旋转流形空间中的扩散。</li>
<li>提出了碰撞避免成本指导来提高推理阶段的抓取成功率。</li>
<li>使用最新的扩散技术来加速抓取采样。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c17eacc64fa0c9fb52341ea65b88bc6d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c272602300f7c75c6511c67dad2747ba.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e5ac35f945bb82796595229121aaebad.jpg" align="middle">
</details>




<h2 id="Unicorn-Unified-Neural-Image-Compression-with-One-Number-Reconstruction"><a href="#Unicorn-Unified-Neural-Image-Compression-with-One-Number-Reconstruction" class="headerlink" title="Unicorn: Unified Neural Image Compression with One Number Reconstruction"></a>Unicorn: Unified Neural Image Compression with One Number Reconstruction</h2><p><strong>Authors:Qi Zheng, Haozhi Wang, Zihao Liu, Jiaming Liu, Peiye Liu, Zhijian Hao, Yanheng Lu, Dimin Niu, Jinjia Zhou, Minge Jing, Yibo Fan</strong></p>
<p>Prevalent lossy image compression schemes can be divided into: 1) explicit image compression (EIC), including traditional standards and neural end-to-end algorithms; 2) implicit image compression (IIC) based on implicit neural representations (INR). The former is encountering impasses of either leveling off bitrate reduction at a cost of tremendous complexity while the latter suffers from excessive smoothing quality as well as lengthy decoder models. In this paper, we propose an innovative paradigm, which we dub \textbf{Unicorn} (\textbf{U}nified \textbf{N}eural \textbf{I}mage \textbf{C}ompression with \textbf{O}ne \textbf{N}number \textbf{R}econstruction). By conceptualizing the images as index-image pairs and learning the inherent distribution of pairs in a subtle neural network model, Unicorn can reconstruct a visually pleasing image from a randomly generated noise with only one index number. The neural model serves as the unified decoder of images while the noises and indexes corresponds to explicit representations. As a proof of concept, we propose an effective and efficient prototype of Unicorn based on latent diffusion models with tailored model designs. Quantitive and qualitative experimental results demonstrate that our prototype achieves significant bitrates reduction compared with EIC and IIC algorithms. More impressively, benefitting from the unified decoder, our compression ratio escalates as the quantity of images increases. We envision that more advanced model designs will endow Unicorn with greater potential in image compression. We will release our codes in \url{<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree%7D">https://github.com/uniqzheng/Unicorn-Laduree}</a>. </p>
<blockquote>
<p>流行的有损图像压缩方案可分为两类：1）显式图像压缩（EIC），包括传统标准和神经端到端算法；以及基于隐神经表示（INR）的隐式图像压缩（IIC）。前者在巨大的复杂性下遇到比特率减少停滞不前的困境，而后者则面临过度平滑质量以及解码器模型过长的问题。在本文中，我们提出了一种创新的方法，我们称之为“独角兽”（统一神经图像压缩重建）。通过将图像概念化为索引图像对，并在微妙的神经网络模型中学习对之间的内在分布，独角兽可以从随机生成的噪声中仅用单个索引号重建出令人满意的图像。该神经网络模型作为统一解码器为图像服务，而噪声和索引则对应于显式表示。作为概念验证，我们提出了基于潜在扩散模型的独角兽的有效高效原型。定制模型设计的结果和实验定量与定性分析表明，我们的原型与EIC和IIC算法相比实现了显著的比特率降低。更令人印象深刻的是，得益于统一的解码器，随着图像数量的增加，我们的压缩比也在提高。我们相信更先进的模型设计将使独角兽在图像压缩方面展现出更大的潜力。我们将会在网址中公开代码：<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree%E3%80%82">https://github.com/uniqzheng/Unicorn-Laduree。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08210v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了一种创新的图像压缩方法——Unicorn（基于单一神经网络图像压缩）。该方法将图像视为索引-图像对，并在微妙的神经网络模型中学习这些对之间的内在分布。Unicorn可以从随机生成的噪声中仅用单一索引数字重建出视觉愉悦的图像。神经网络模型充当了统一的解码器，噪声和索引则对应明确的表示形式。实验结果显示，与现有图像压缩算法相比，Unicorn原型在比特率减少方面表现优异。更令人印象深刻的是，得益于统一的解码器，随着图像数量的增加，我们的压缩比也在提高。我们期待未来更先进的模型设计能为Unicorn在图像压缩方面带来更强大的潜力。代码地址：<a target="_blank" rel="noopener" href="https://github.com/uniqzheng/Unicorn-Laduree">https://github.com/uniqzheng/Unicorn-Laduree</a>。此方法可能重塑未来图像压缩领域的发展。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>图像压缩分为显式图像压缩（EIC）和基于隐神经表示（INR）的隐式图像压缩（IIC）。</li>
<li>当前技术面临挑战：EIC复杂度极高且比特率降低已接近瓶颈；IIC则存在图像平滑过度和质量下降的问题。</li>
<li>Unicorn提出了一个新的神经网络模型用于图像压缩的概念框架，能够从一个简单的索引数字重构高质量图像。它用一个统一的解码器处理所有图像。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5e9526e87e92fe4967015f82a9583a1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b8e36b788687535643c8ea55fb808f7b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b5427895e74d7b079fb9e86e3fd1cb5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3d03eb4b291b395751079d79dfdcd80f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aec2e069491f59d900ff73613a434c81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-95d33bebd679d05deea40b127573544a.jpg" align="middle">
</details>




<h2 id="Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing"><a href="#Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing" class="headerlink" title="Diffusion-Based Attention Warping for Consistent 3D Scene Editing"></a>Diffusion-Based Attention Warping for Consistent 3D Scene Editing</h2><p><strong>Authors:Eyal Gomel, Lior Wolf</strong></p>
<p>We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \url{<a target="_blank" rel="noopener" href="https://attention-warp.github.io}/">https://attention-warp.github.io}</a> </p>
<blockquote>
<p>我们提出了一种利用扩散模型进行3D场景编辑的新方法，旨在确保从不同视角观看时的一致性和逼真性。我们的方法利用从单个参考图像中提取的注意力特征来定义预期的编辑。通过将这些特征与从高斯拼贴深度估计中得出的场景几何进行对齐，将这些特征跨多个视角进行变形。将这些变形特征注入到其他视角中，可以实现编辑的连贯传播，在高保真和空间对齐方面实现3D空间的高保真和空间对齐。广泛评估表明，我们的方法在生成多样化的3D场景编辑方面非常有效，与现有方法相比，极大地提高了场景操作的能力。项目页面：<a target="_blank" rel="noopener" href="https://attention-warp.github.io/">https://attention-warp.github.io</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07984v1">PDF</a> </p>
<p><strong>Summary</strong>：我们提出了一种使用扩散模型进行3D场景编辑的新方法，通过利用参考图像的注意力特征来定义预期的编辑，并将其跨多个视角进行对齐，从而实现编辑的一致性传播和高保真度。该方法通过高斯喷溅深度估计来推导场景几何结构，并将注意力特征进行变形和注入其他视角，确保了编辑在3D空间中的连贯性和空间对齐。大量评估表明，我们的方法在生成多样化的3D场景编辑方面非常有效，显著提高了场景操作的能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了一种基于扩散模型的新方法进行3D场景编辑。</li>
<li>利用参考图像的注意力特征进行编辑定义。</li>
<li>通过高斯喷溅深度估计的场淟几何结构实现跨视角的一致性。</li>
<li>变形和注入注意力特征到其他视角，确保编辑的连贯性和空间对齐。</li>
<li>方法在生成多样化的3D场景编辑方面表现出色。</li>
<li>与现有方法相比，显著提高了场景操作的能力。</li>
<li>项目页面提供了更多的详细信息：<a target="_blank" rel="noopener" href="https://attention-warp.github.io./">https://attention-warp.github.io。</a></li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f8f42584dfad759e1b2aecc279277861.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5766ba7cdaf9078e0a54b1a828295c72.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e83f73a0915f2745d1dec4a301df78d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a0003e9cacf1db217bda34beeb1a59b.jpg" align="middle">
</details>




<h2 id="Non-Normal-Diffusion-Models"><a href="#Non-Normal-Diffusion-Models" class="headerlink" title="Non-Normal Diffusion Models"></a>Non-Normal Diffusion Models</h2><p><strong>Authors:Henry Li</strong></p>
<p>Diffusion models generate samples by incrementally reversing a process that turns data into noise. We show that when the step size goes to zero, the reversed process is invariant to the distribution of these increments. This reveals a previously unconsidered parameter in the design of diffusion models: the distribution of the diffusion step $\Delta x_k :&#x3D; x_{k} - x_{k + 1}$. This parameter is implicitly set by default to be normally distributed in most diffusion models. By lifting this assumption, we generalize the framework for designing diffusion models and establish an expanded class of diffusion processes with greater flexibility in the choice of loss function used during training. We demonstrate the effectiveness of these models on density estimation and generative modeling tasks on standard image datasets, and show that different choices of the distribution of $\Delta x_k$ result in qualitatively different generated samples. </p>
<blockquote>
<p>扩散模型通过逐步反转将数据转化为噪声的过程来生成样本。我们证明，当步长趋于零时，反转过程对这些增量的分布是不变的。这揭示了扩散模型设计中一个以前未被考虑过的参数：扩散步长Δxk:&#x3D;xk−xk+1的分布。这一参数在大多数扩散模型中默认设置为正态分布。通过取消这一假设，我们推广了扩散模型的设计框架，建立了一个具有更大灵活性的扩散过程类，在训练过程中可以选择更广泛的损失函数。我们在标准图像数据集上的密度估计和生成建模任务中验证了这些模型的有效性，并证明Δxk分布的不同选择会导致生成的样本在质量上存在显著差异。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07935v1">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型通过逐渐逆转将数据变为噪声的过程来生成样本。研究发现，当步长趋于零时，逆向过程不受这些增量分布的的影响。这为扩散模型的设计揭示了一个以前未被考虑过的参数，即扩散步长Δxk的分布。大多数扩散模型默认将其设为正态分布。通过取消这一假设，我们推广了扩散模型的设计框架，建立了一个具有更大灵活性的扩散过程类，在训练中可以选择更多的损失函数。我们在标准图像数据集上展示了这些模型在密度估计和生成建模任务上的有效性，并证明了Δxk分布的不同选择会产生不同质量的生成样本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过逆向过程从噪声生成样本。</li>
<li>扩散模型的步长趋于零时，逆向过程与增量分布无关。</li>
<li>扩散步长的分布在模型设计中是关键参数。</li>
<li>大多数扩散模型默认将步长分布设为正态分布。</li>
<li>取消这一默认假设可以推广扩散模型的设计框架。</li>
<li>不同的扩散步长分布选择会影响生成样本的质量。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e51bcb7d6661004cce30ab4aaaf7543b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0fdee689b42dcc752a197d0f2a3aa59a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4b38949908ad2849cbbc799f48b413fd.jpg" align="middle">
</details>




<h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models on some reward functions that are either designed by experts or learned from small-scale datasets. Existing methods for finetuning diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. Inspired by recent successes in generative flow networks (GFlowNets), a class of probabilistic models that sample with the unnormalized density of a reward function, we propose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), the first GFlowNet method that leverages the rich signal in reward gradients, together with an objective called $\nabla$-DB plus its variant residual $\nabla$-DB designed for prior-preserving diffusion alignment. We show that our proposed method achieves fast yet diversity- and prior-preserving alignment of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>通常人们通过在目标下游任务上收集数据集来训练大型扩散模型，但人们往往希望将预训练的扩散模型与某些奖励函数对齐并进行微调，这些奖励函数要么是专家设计的，要么是从小规模数据集中学习得到的。现有的微调扩散模型的方法通常存在生成样本缺乏多样性、缺乏先验知识保留以及微调过程中收敛速度慢的问题。受最近生成流网络（GFlowNets）成功的启发，GFlowNets是一类以奖励函数的未归一化密度进行采样的概率模型。我们提出了一种新的GFlowNet方法，称为Nabla-GFlowNet（简称$\nabla$-GFlowNet），它是第一个利用奖励梯度丰富信号的GFlowNet方法，以及一个称为$\nabla$-DB的目标函数及其为保留先验的扩散对齐设计的残差$\nabla$-DB变体。我们显示，我们提出的方法能够在不同的现实奖励函数上快速且多样化和保留先验地对齐大规模文本条件图像扩散模型Stable Diffusion。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v1">PDF</a> Technical Report (35 pages, 31 figures)</p>
<p><strong>Summary</strong></p>
<p>大型扩散模型通常通过针对目标下游任务收集数据集进行训练，但人们更希望将预训练的扩散模型与专家设计的奖励函数或从小规模数据集中学习到的奖励函数进行对齐和微调。现有微调扩散模型的方法通常存在生成样本缺乏多样性、缺乏先验知识保留以及微调速度慢等问题。本研究受生成流网络（GFlowNets）最新成功的启发，提出了一种名为Nabla-GFlowNet的新型GFlowNet方法，该方法利用奖励梯度中的丰富信号，并结合一个称为$\nabla$-DB的目标及其为保留先验扩散对齐设计的残差$\nabla$-DB变体。研究结果表明，该方法可实现稳定扩散的快速、多样性和保留先验的对齐，这是一种大型文本条件图像扩散模型，可在不同的真实奖励函数上进行应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型扩散模型通常通过下游任务数据集进行训练，但微调预训练模型与奖励函数对齐更为常见。</li>
<li>现存的微调方法存在样本多样性不足、缺少先验知识保留和慢收敛的问题。</li>
<li>研究采用GFlowNets最新成功为灵感，提出了一种名为Nabla-GFlowNet的新型方法。</li>
<li>Nabla-GFlowNet利用奖励梯度中的丰富信号进行工作。</li>
<li>研究引入了$\nabla$-DB及其变体残差$\nabla$-DB目标，用于实现保留先验的扩散对齐。</li>
<li>Nabla-GFlowNet在多种真实奖励函数上实现了稳定扩散的快速、多样性和保留先验的对齐。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e6ad08bb0dc78455f591fd489bf8bf14.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-21950de2b5b2e07066fd3abf8159ca34.jpg" align="middle">
</details>




<h2 id="From-Slow-Bidirectional-to-Fast-Causal-Video-Generators"><a href="#From-Slow-Bidirectional-to-Fast-Causal-Video-Generators" class="headerlink" title="From Slow Bidirectional to Fast Causal Video Generators"></a>From Slow Bidirectional to Fast Causal Video Generators</h2><p><strong>Authors:Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</strong></p>
<p>Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to a causal transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher’s ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model supports fast streaming generation of high quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future. </p>
<blockquote>
<p>当前的视频扩散模型在生成质量方面表现出色，但由于双向注意力依赖性，在交互式应用中表现不佳。单个帧的生成需要模型处理整个序列，包括未来信息。我们通过将预训练的双向扩散变压器适应为因果变压器来解决这一限制，该因果变压器可以即时生成帧。为了进一步降低延迟，我们将分布匹配蒸馏（DMD）扩展到视频领域，将50步扩散模型蒸馏为4步生成器。为了实现稳定和高质量的蒸馏，我们引入了基于教师ODE轨迹的学生初始化方案，以及一种不对称的蒸馏策略，该策略用双向教师监督因果学生模型。这种方法有效地减轻了自回归生成中的误差累积，即使在短片段训练的情况下也能实现长时长视频合成。我们的模型支持在单个GPU上以9.4 FPS的速度快速流式生成高质量视频，这得益于KV缓存。我们的方法还实现了流式视频到视频的翻译、图像到视频以及零样本方式的动态提示。未来，我们将基于开源模型发布代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07772v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://causvid.github.io/">https://causvid.github.io/</a></p>
<p><strong>Summary</strong><br>     针对当前视频扩散模型在交互式应用中由于双向注意力依赖而产生的瓶颈，本研究通过将一个预训练的双向扩散转换器改编为因果转换器，实现了即时生成帧的技术。为降低延迟，研究团队将分布匹配蒸馏法（DMD）扩展至视频领域，将50步扩散模型缩减为4步生成器。同时，引入基于教师常微分方程轨迹的学生初始化方案及不对称蒸馏策略，有效减轻自回归生成中的误差累积，即便在短片段训练下也能实现长期视频合成。借助KV缓存，该模型可在单个GPU上以9.4 FPS的速度快速生成高质量视频。此外，该模型还支持视频转视频、图片转视频的实时流式转换以及零样本动态提示。未来研究团队将基于开源模型公开代码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前视频扩散模型在交互式应用中存在双向注意力依赖的问题。</li>
<li>研究提出了一种因果转换器，能够即时生成帧，解决了这一问题。</li>
<li>通过将分布匹配蒸馏法扩展至视频领域，降低了模型的生成步骤，从而减少了延迟。</li>
<li>引入了学生初始化方案和不对称蒸馏策略，提高了模型的稳定性和高质量蒸馏效果。</li>
<li>模型能够有效减轻自回归生成中的误差累积，实现长期视频合成。</li>
<li>模型支持快速生成高质量视频，并能在单个GPU上实现流式传输。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ee965fc3063cea1099d1a788584150d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d753d21c54ebc6456937f17d3e9e71a9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9cb62cb28a54dd058bb39b3b2bfafbe2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f6f5f4cc15dc30fe00c4852e9829e5fe.jpg" align="middle">
</details>




<h2 id="SynCamMaster-Synchronizing-Multi-Camera-Video-Generation-from-Diverse-Viewpoints"><a href="#SynCamMaster-Synchronizing-Multi-Camera-Video-Generation-from-Diverse-Viewpoints" class="headerlink" title="SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse   Viewpoints"></a>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse   Viewpoints</h2><p><strong>Authors:Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang</strong></p>
<p>Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: <a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a>. </p>
<blockquote>
<p>近期视频扩散模型的进展在模拟真实世界动态和保持3D一致性方面展现出了卓越的能力。这一进展激发了我们探索这些模型在跨不同视角确保动态一致性方面的潜力，这对于虚拟拍摄等应用来说是一个高度理想的功能。不同于现有方法聚焦于单个对象的多视角生成以进行4D重建，我们的兴趣在于从任意视角生成开放世界视频，并融入6自由度相机姿态。为了实现这一目标，我们提出了一种即插即用的模块，该模块可增强预训练的文本到视频模型以进行多相机视频生成，确保不同视角的内容一致性。具体来说，我们引入了一个多视角同步模块来保持这些视角的外观和几何一致性。考虑到高质量训练数据的稀缺性，我们设计了一种混合训练方案，该方案利用多相机图像和单眼视频来补充由Unreal Engine渲染的多相机视频。此外，我们的方法还实现了有趣的扩展，如从新颖视角重新渲染视频。我们还发布了一个名为SynCamVideo-Dataset的多视角同步视频数据集。项目页面：<a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/%E3%80%82">https://jianhongbai.github.io/SynCamMaster/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07760v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jianhongbai.github.io/SynCamMaster/">https://jianhongbai.github.io/SynCamMaster/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了视频扩散模型的新进展，该模型能够在模拟真实世界动态和保持3D一致性方面表现出卓越的能力。研究团队探索了这种模型在虚拟拍摄等应用中的潜力，通过提出一个即插即用的模块，增强了预训练文本到视频模型的多相机视频生成能力，确保不同视角下的内容一致性。该研究引入了多视角同步模块，维持各视角下的外观和几何一致性。针对高质量训练数据的稀缺性，研究团队设计了一种混合训练方案，利用多相机图像和单目视频来补充虚幻引擎渲染的多相机视频。此外，该方法还支持从新视角重新渲染视频等有趣的应用扩展。同时，研究团队还发布了一个名为SynCamVideo-Dataset的多视角同步视频数据集。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频扩散模型在模拟真实世界动态和保持3D一致性方面表现出卓越的能力。</li>
<li>研究探索了视频扩散模型在虚拟拍摄应用的潜力，强调多视角一致性的重要性。</li>
<li>提出一个即插即用的模块，增强预训练文本到视频模型的多相机视频生成能力。</li>
<li>引入多视角同步模块，维持不同视角下的外观和几何一致性。</li>
<li>针对高质量训练数据的稀缺性问题，采用混合训练方案，利用多源数据进行补充。</li>
<li>支持从新视角重新渲染视频的有趣应用扩展。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6cedadb2a6a524961cb37855fe9be841.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b31ef600e0a14b462d2dcfbf2e1e57bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-aef164b104eed0ad6a622de188acbf28.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-85fa1fa7ccd65095a4fbc200417c521a.jpg" align="middle">
</details>




<h2 id="TraSCE-Trajectory-Steering-for-Concept-Erasure"><a href="#TraSCE-Trajectory-Steering-for-Concept-Erasure" class="headerlink" title="TraSCE: Trajectory Steering for Concept Erasure"></a>TraSCE: Trajectory Steering for Concept Erasure</h2><p><strong>Authors:Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, Yuki Mitsufuji</strong></p>
<p>Recent advancements in text-to-image diffusion models have brought them to the public spotlight, becoming widely accessible and embraced by everyday users. However, these models have been shown to generate harmful content such as not-safe-for-work (NSFW) images. While approaches have been proposed to erase such abstract concepts from the models, jail-breaking techniques have succeeded in bypassing such safety measures. In this paper, we propose TraSCE, an approach to guide the diffusion trajectory away from generating harmful content. Our approach is based on negative prompting, but as we show in this paper, conventional negative prompting is not a complete solution and can easily be bypassed in some corner cases. To address this issue, we first propose a modification of conventional negative prompting. Furthermore, we introduce a localized loss-based guidance that enhances the modified negative prompting technique by steering the diffusion trajectory. We demonstrate that our proposed method achieves state-of-the-art results on various benchmarks in removing harmful content including ones proposed by red teams; and erasing artistic styles and objects. Our proposed approach does not require any training, weight modifications, or training data (both image or prompt), making it easier for model owners to erase new concepts. </p>
<blockquote>
<p>文本到图像扩散模型的最新进展使其受到公众关注，并成为日常用户广泛可访问和接受的技术。然而，这些模型已被证明可以生成有害内容，例如不适合工作场合（NSFW）的图像。虽然已有方法被提出从模型中删除此类抽象概念，但越狱技术已成功绕过这些安全措施。在本文中，我们提出TraSCE方法，它通过引导扩散轨迹远离生成有害内容。我们的方法基于负提示，但正如我们在本文中所展示的，传统的负提示并非完整的解决方案，在某些特殊情况下容易被绕过。为了解决这一问题，我们首先提出对传统负提示的改进。此外，我们还引入了一种基于局部损失的指导方法，通过引导扩散轨迹来增强改进后的负提示技术。我们证明，我们的方法在去除有害内容方面达到了最先进的结果，包括红队提出的各种基准测试中的有害内容；以及消除艺术风格和物体。我们提出的方法不需要任何训练、权重修改或训练数据（无论是图像还是提示），这使得模型所有者更容易消除新概念。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07658v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像扩散模型的新进展已引起公众关注，并被日常用户广泛接受。然而，这些模型会生成不安全的成人内容。尽管已有方法试图从模型中消除此类抽象概念，但越狱技术已成功绕过这些安全措施。本文提出TraSCE方法，通过负提示引导扩散轨迹，避免生成有害内容。但如本文所示，传统负提示并非完全解决方案，某些情况下容易被绕过。因此，我们首先对常规负提示进行改进，并引入基于局部损失的指导，通过控制扩散轨迹强化负提示技术。实验证明，该方法在去除有害内容、消除艺术风格和物体方面达到最新水平，且无需任何训练、权重修改或训练数据（图像或提示），使得模型主人更容易消除新概念。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像扩散模型的进步带来公众关注，但生成的有害内容成为问题。</li>
<li>现有方法试图从模型中消除有害内容，但存在越狱技术绕过这些安全措施。</li>
<li>提出TraSCE方法，通过负提示引导扩散轨迹，避免生成有害内容。</li>
<li>传统负提示并非完全解决方案，某些情况下容易被绕过。</li>
<li>对常规负提示进行改进，并引入基于局部损失的指导强化技术。</li>
<li>TraSCE方法在去除有害内容方面达到最新水平。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-90e08ded28c54362a9bc6e958d7de136.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9977d55951a2f9b136992f265976828.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-040c9e01c46ba7f7a7281eaea5b5ea91.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-29cc281a6f072cd35671ca8d9ac80956.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4b0d9fe0eb315dd15a2b6ef43b9f4539.jpg" align="middle">
</details>




<h2 id="Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model"><a href="#Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model" class="headerlink" title="Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model"></a>Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Jianfeng Guo, Feng Yang, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Motion artifacts present in magnetic resonance imaging (MRI) can seriously interfere with clinical diagnosis. Removing motion artifacts is a straightforward solution and has been extensively studied. However, paired data are still heavily relied on in recent works and the perturbations in k-space (frequency domain) are not well considered, which limits their applications in the clinical field. To address these issues, we propose a novel unsupervised purification method which leverages pixel-frequency information of noisy MRI images to guide a pre-trained diffusion model to recover clean MRI images. Specifically, considering that motion artifacts are mainly concentrated in high-frequency components in k-space, we utilize the low-frequency components as the guide to ensure correct tissue textures. Additionally, given that high-frequency and pixel information are helpful for recovering shape and detail textures, we design alternate complementary masks to simultaneously destroy the artifact structure and exploit useful information. Quantitative experiments are performed on datasets from different tissues and show that our method achieves superior performance on several metrics. Qualitative evaluations with radiologists also show that our method provides better clinical feedback. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD">https://github.com/medcx/PFAD</a>. </p>
<blockquote>
<p>磁共振成像（MRI）中的运动伪影会严重干扰临床诊断。去除运动伪影是一种直接解决方案，已经得到了广泛的研究。然而，最近的研究仍然严重依赖于配对数据，而k空间（频率域）中的扰动并未得到很好的考虑，这限制了其在临床领域的应用。为了解决这些问题，我们提出了一种新型的无监督净化方法，该方法利用噪声MRI图像的像素频率信息来指导预训练的扩散模型恢复清洁的MRI图像。具体来说，考虑到运动伪影主要集中在k空间的高频成分中，我们利用低频成分作为指导来确保正确的组织纹理。此外，鉴于高频和像素信息有助于恢复形状和细节纹理，我们设计了交替的互补掩膜来同时破坏伪影结构并挖掘有用信息。在不同组织数据集上进行的定量实验表明，我们的方法在多个指标上实现了卓越的性能。与放射科医生进行的定性评估也表明，我们的方法提供了更好的临床反馈。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/medcx/PFAD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07590v2">PDF</a> 12 pages, 8 figures, AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的无监督净化方法，利用噪声磁共振成像（MRI）图像的像素频率信息引导预训练的扩散模型恢复清晰的MRI图像，以解决MRI中的运动伪影问题。该方法利用低频成分作为指导确保正确的组织纹理，并设计交替互补掩膜同时破坏伪影结构并挖掘有用信息。实验证明该方法在多个指标上表现优异，并得到放射科医师的积极临床反馈。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>磁共振成像（MRI）中的运动伪影会干扰临床诊断，去除伪影是一个亟待解决的问题。</li>
<li>现有方法多依赖配对数据，对k空间（频率域）的扰动考虑不足，限制了其在临床应用的广泛性。</li>
<li>本文提出一种新型无监督净化方法，利用像素频率信息引导预训练的扩散模型恢复清晰MRI图像。</li>
<li>方法利用低频成分作为指导，确保正确的组织纹理；设计交替互补掩膜，同时破坏伪影结构并挖掘有用信息。</li>
<li>实验证明该方法在多个数据集上表现优异，定量和定性评估均验证其有效性。</li>
<li>与放射科医师的合作评价显示，该方法在临床应用中表现良好，获得积极反馈。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e0ff7c13591035b978b06d36f1f533ae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f99b9342819d5c3237e084dec6be6f9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4cfd4272ea20d1f177b432a0446e8119.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-47ef4c87738d300c706dc2cbf4506649.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8186300dbbe1c884fae5bbebd2c36e3a.jpg" align="middle">
</details>




<h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, Björn Ommer</strong></p>
<p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks. </p>
<blockquote>
<p>在生成模型中，有两种范式在各种应用中获得关注：基于下一步预测的Masked Generative Models和基于下一步噪声预测的非自回归模型（例如扩散模型）。在这项工作中，我们提出使用离散状态模型将它们连接起来，并探索它们在视觉领域的可扩展性。首先，我们以可伸缩的方式，在统一的设计空间内对这两种类型的模型进行了逐步分析，包括时间步独立性、噪声时间表、温度、指导强度等。其次，我们将典型的判别任务（例如图像分割）重新定位为离散状态模型上从[MASK]标记进行的去遮掩过程。这使得我们能够执行各种采样过程，包括通过仅一次训练来对联合分布进行灵活的条件采样。所有上述探索都引领我们构建了名为Discrete Interpolants的框架，该框架使我们能够在各种基准测试中达到或接近最新技术水平，如ImageNet256、MS COCO和视频数据集FaceForensics。总之，通过利用离散状态模型中的[MASK]，我们可以架起Masked Generative和Non-autoregressive Diffusion模型之间的桥梁，以及生成和判别任务之间的桥梁。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v2">PDF</a> Technical Report (WIP), Project Page(code, model, dataset):   <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p>
<p><strong>Summary</strong>：本研究提出了利用离散状态模型连接基于下一步预测的遮罩生成模型和基于下一步噪声预测的非自回归模型（如扩散模型），并在视觉上探索其可扩展性。该研究进行了统一设计空间中的逐步分析，包括时间步独立性、噪声调度、温度、引导强度等。此外，研究将典型的判别任务重新定义为离散状态模型上的去遮掩过程，实现了各种采样过程，包括通过仅训练一次对联合分布进行灵活条件采样。所有这些探索都归功于名为“离散插值”的框架，该框架在ImageNet256、MS COCO和FaceForensics视频数据集上实现了与以前基于离散状态的方法相比具有竞争力的性能。简单来说，本研究利用离散状态模型中的遮罩连接遮罩生成模型和非自回归扩散模型，并连接生成任务和判别任务。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>研究利用离散状态模型连接两种生成模型范式：基于下一步预测的遮罩生成模型和基于下一步噪声预测的非自回归模型（如扩散模型）。</li>
<li>在统一设计空间中对两种模型进行了逐步分析，包括时间步独立性、噪声调度等关键因素。</li>
<li>将典型的判别任务重新定义为去遮掩过程，实现了灵活的条件采样。</li>
<li>提出名为“离散插值”的框架，实现了各种视觉任务上的竞争力性能。</li>
<li>通过利用离散状态模型中的遮罩，连接了生成模型和判别任务。</li>
<li>框架在多个基准测试中表现优秀，包括ImageNet256、MS COCO和FaceForensics数据集。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-84482f8d037dfd8fca4b36a654af2c7d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7bd916bdace6c59e807a319127cb89ee.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c3c9e8e97bdb09e0544dbb86f3d0601f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c35d03a36efcc9698532982aad9a05f.jpg" align="middle">
</details>




<h2 id="ContRail-A-Framework-for-Realistic-Railway-Image-Synthesis-using-ControlNet"><a href="#ContRail-A-Framework-for-Realistic-Railway-Image-Synthesis-using-ControlNet" class="headerlink" title="ContRail: A Framework for Realistic Railway Image Synthesis using   ControlNet"></a>ContRail: A Framework for Realistic Railway Image Synthesis using   ControlNet</h2><p><strong>Authors:Andrei-Robert Alexandrescu, Razvan-Gabriel Petec, Alexandru Manole, Laura-Silvia Diosan</strong></p>
<p>Deep Learning became an ubiquitous paradigm due to its extraordinary effectiveness and applicability in numerous domains. However, the approach suffers from the high demand of data required to achieve the potential of this type of model. An ever-increasing sub-field of Artificial Intelligence, Image Synthesis, aims to address this limitation through the design of intelligent models capable of creating original and realistic images, endeavour which could drastically reduce the need for real data. The Stable Diffusion generation paradigm recently propelled state-of-the-art approaches to exceed all previous benchmarks. In this work, we propose the ContRail framework based on the novel Stable Diffusion model ControlNet, which we empower through a multi-modal conditioning method. We experiment with the task of synthetic railway image generation, where we improve the performance in rail-specific tasks, such as rail semantic segmentation by enriching the dataset with realistic synthetic images. </p>
<blockquote>
<p>深度学习因其在众多领域的出色效果和适用性而成为无处不在的范例。然而，这种方法需要达到模型潜力所需的大量数据。人工智能的一个日益增长的子领域——图像合成，旨在通过设计能够创建原始和真实图像的智能模型来解决这一局限性，从而大幅减少对真实数据的需求。最近，Stable Diffusion生成范式推动了最先进的方法超越了所有先前的基准测试。在这项工作中，我们基于新颖的Stable Diffusion模型ControlNet提出ContRail框架，并通过多模态条件方法进行赋能。我们尝试铁路图像合成生成任务，通过用逼真的合成图像丰富数据集，提高铁路特定任务（如铁路语义分割）的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06742v2">PDF</a> 9 pages, 5 figures, 2 tables</p>
<p><strong>Summary</strong>：深度学习因其卓越的效率和广泛适用的能力成为了无处不在的范式，但在需要实现其潜力时对数据的需求极高。图像合成作为人工智能的一个不断增长子领域，旨在通过设计能够创建原始和逼真图像的智能模型来解决这一局限性，从而大幅减少对真实数据的需求。本研究基于新型稳定扩散模型ControlNet提出了ContRail框架，并通过多模态调节方法提升了性能。通过铁路合成图像生成的任务实验表明，我们的方法能够提高特定于铁路任务的性能，例如铁路语义分割。使用逼真的合成图像丰富了数据集。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度学习成为无处不在的范式，但其对数据的需求极高。</li>
<li>图像合成子领域旨在解决这一局限性，通过创建逼真图像减少真实数据的需求。</li>
<li>ContRail框架基于新型稳定扩散模型ControlNet构建。</li>
<li>多模态调节方法增强了ContRail框架的性能。</li>
<li>实验以铁路合成图像生成任务进行，表现出良好性能提升。</li>
<li>方法能够丰富数据集以提高铁路语义分割等任务的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-adb625fd18a5cadb69b96ca71f2d670d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-282f3655f025832fd0b8255d6c97dd81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ccdb569fc2c3bd7448fa2740bc75912.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0988c5696c2b634669b5ff8560f75c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-53710ab7cf073314c03f5c6f8e24e722.jpg" align="middle">
</details>




<h2 id="Open-Source-Acceleration-of-Stable-Diffusion-cpp"><a href="#Open-Source-Acceleration-of-Stable-Diffusion-cpp" class="headerlink" title="Open-Source Acceleration of Stable-Diffusion.cpp"></a>Open-Source Acceleration of Stable-Diffusion.cpp</h2><p><strong>Authors:Jingxu Ng, Cheng Lv, Pu Zhao, Wei Niu, Juyi Lin, Minzhou Pan, Yun Liang, Yanzhi Wang</strong></p>
<p>Stable diffusion plays a crucial role in generating high-quality images. However, image generation is time-consuming and memory-intensive. To address this, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate the diffusion models. Although it is lightweight, the current implementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference latency and massive memory usage. To address this, in this work, we present an optimized version of Sdcpp leveraging the Winograd algorithm to accelerate 2D convolution operations, which is the primary bottleneck in the pipeline. By analyzing both dependent and independent computation graphs, we exploit the device’s locality and parallelism to achieve substantial performance improvements. Our framework delivers correct end-to-end results across various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and SDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for individual convolutional layers and an inference speedup up to 4.79x for the overall image generation process, compared with the original Sdcpp on M1 pro. Homepage: <a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a> </p>
<blockquote>
<p>稳定扩散在生成高质量图像中扮演着至关重要的角色。然而，图像生成是耗时的且需要大量内存。为了解决这一问题，stable-diffusion.cpp（Sdcpp）作为一个高效的推理框架应运而生，以加速扩散模型。虽然它很轻便，但Sdcpp中ggml_conv_2d算子的当前实现并不理想，表现出较高的推理延迟和巨大的内存使用。针对这一问题，在这项工作中，我们提出了一种优化版的Sdcpp，利用Winograd算法加速2D卷积操作，这是管道中的主要瓶颈。通过分析有依赖和无依赖的计算图，我们利用设备的局部性和并行性来实现显著的性能改进。我们的框架在各种稳定扩散模型中都能提供正确的端到端结果，包括SDv1.4、v1.5、v2.1、SDXL和SDXL-Turbo。我们的评估结果表明，与原始Sdcpp在M1 pro上的表现相比，单个卷积层的速度提高了2.76倍，整体图像生成过程的推理速度提高了4.79倍。更多信息请访问：<a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05781v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>稳定扩散在生成高质量图像中扮演关键角色，但图像生成耗时且占用大量内存。为解决这个问题，出现了stable-diffusion.cpp（Sdcpp）这一高效推理框架来加速扩散模型。然而，Sdcpp中ggml_conv_2d算子的当前实现存在缺陷，存在推理延迟高和内存使用量大等问题。本研究中，我们提出了一种利用Winograd算法优化Sdcpp的版本来加速2D卷积操作，这是管道中的主要瓶颈。通过分析有依赖和无依赖的计算图，我们利用设备的局部性和并行性实现了显著的性能提升。我们的框架在多种稳定扩散模型（包括SDv1.4、v1.5、v2.1、SDXL和SDXL-Turbo）中均能提供正确的端到端结果。评估结果显示，与原始Sdcpp相比，我们的方法在M1 pro上针对单个卷积层实现了最高2.76倍的加速，整体图像生成过程实现了最高4.79倍的推理加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稳定扩散在生成高质量图像中很重要，但存在耗时和内存占用大的问题。</li>
<li>stable-diffusion.cpp（Sdcpp）框架旨在加速扩散模型。</li>
<li>Sdcpp中的ggml_conv_2d算子存在性能问题，包括高延迟和大量内存使用。</li>
<li>利用Winograd算法优化Sdcpp，加速2D卷积操作，解决性能瓶颈。</li>
<li>通过分析计算图，利用设备局部性和并行性提升性能。</li>
<li>优化的框架适用于多种稳定扩散模型，包括SDv1.4、v1.5、v2.1、SDXL和SDXL-Turbo。</li>
<li>评估结果显示，与原始Sdcpp相比，新方法实现了显著的性能提升。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-54c9fe172a83be87335a9ebbdb49b5c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7668f911018f7c5da7612ca3bc0e829f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-01b444daef29cdf097fb454de4f91021.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8cc717995bf69c5de91eae7f5cbe3947.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3c26c7d226f869f76c1a7c3096970423.jpg" align="middle">
</details>




<h2 id="BudgetFusion-Perceptually-Guided-Adaptive-Diffusion-Models"><a href="#BudgetFusion-Perceptually-Guided-Adaptive-Diffusion-Models" class="headerlink" title="BudgetFusion: Perceptually-Guided Adaptive Diffusion Models"></a>BudgetFusion: Perceptually-Guided Adaptive Diffusion Models</h2><p><strong>Authors:Qinchan Li, Kenneth Chen, Changyue Su, Qi Sun</strong></p>
<p>Diffusion models have shown unprecedented success in the task of text-to-image generation. While these models are capable of generating high-quality and realistic images, the complexity of sequential denoising has raised societal concerns regarding high computational demands and energy consumption. In response, various efforts have been made to improve inference efficiency. However, most of the existing efforts have taken a fixed approach with neural network simplification or text prompt optimization. Are the quality improvements from all denoising computations equally perceivable to humans? We observed that images from different text prompts may require different computational efforts given the desired content. The observation motivates us to present BudgetFusion, a novel model that suggests the most perceptually efficient number of diffusion steps before a diffusion model starts to generate an image. This is achieved by predicting multi-level perceptual metrics relative to diffusion steps. With the popular Stable Diffusion as an example, we conduct both numerical analyses and user studies. Our experiments show that BudgetFusion saves up to five seconds per prompt without compromising perceptual similarity. We hope this work can initiate efforts toward answering a core question: how much do humans perceptually gain from images created by a generative model, per watt of energy? </p>
<blockquote>
<p>扩散模型在文本到图像生成任务中取得了前所未有的成功。虽然这些模型能够生成高质量和逼真的图像，但序列去噪的复杂性引发了社会对高计算需求和能源消耗的关注。作为回应，已经做出了各种努力提高推理效率。然而，现有的大多数努力都采取了固定的方法，如简化神经网络或优化文本提示。所有去噪计算的质量改进对人类来说都是同等可感知的吗？我们观察到，根据不同的所需内容，来自不同文本提示的图像可能需要不同的计算投入。这一观察促使我们提出了BudgetFusion，这是一种新型模型，它建议在扩散模型开始生成图像之前，进行最符合感知效率要求的扩散步骤数量。这是通过预测与扩散步骤相关的多级感知指标来实现的。以流行的Stable Diffusion为例，我们进行了数值分析和用户研究。我们的实验表明，BudgetFusion可以在不损害感知相似性的情况下，为每个提示节省多达五秒的时间。我们希望这项工作能引发对核心问题的回答：人类从生成模型创建的图像中感知到的价值，与每瓦特能量消耗之间有何关系？</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05780v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在文本到图像生成任务中取得了前所未有的成功。然而，由于其复杂的去噪过程，社会对高计算需求和能源消耗表示担忧。为提高推理效率，已做出多种努力。但现有方法大多采用神经网络简化或文本提示优化等固定方式。我们观察到，不同文本提示生成的图像可能需要不同的计算量来生成所需内容。因此，我们提出了BudgetFusion模型，该模型建议扩散模型在开始生成图像之前进行最感知效率最高的扩散步骤数。这是通过预测与扩散步骤相关的多级感知指标来实现的。以流行的Stable Diffusion为例，我们进行了数值分析和用户研究。实验表明，BudgetFusion在不影响感知相似性的情况下，每个提示可节省多达五秒钟。我们希望这项工作能引发对人类从生成模型中创建的图像每消耗一瓦能量所能感知到的收益的核心问题的思考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本到图像生成中表现出显著成功，但计算需求和能源消耗受到关注。</li>
<li>提高推理效率的努力多数采用固定方法，如神经网络简化和文本提示优化。</li>
<li>不同文本提示生成的图像可能需要不同的计算量来生成所需内容。</li>
<li>BudgetFusion模型建议进行最感知效率最高的扩散步骤数，以实现更高效的图像生成。</li>
<li>BudgetFusion通过预测与扩散步骤相关的多级感知指标来实现高效生成。</li>
<li>以Stable Diffusion为例的实验表明，BudgetFusion能显著节省时间，同时保持感知相似性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-62dd551fde641da35ac8bd62ae792887.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d98bbdcfecab39556914f20b9d119057.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8c02379e08453839fbac5c683c142ad5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8a0a4b59a78298e531cffcecf101070.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9c93c9d5af7bcc548ea49315deedd0a0.jpg" align="middle">
</details>




<h2 id="Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images"><a href="#Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images" class="headerlink" title="Hidden in the Noise: Two-Stage Robust Watermarking for Images"></a>Hidden in the Noise: Two-Stage Robust Watermarking for Images</h2><p><strong>Authors:Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</strong></p>
<p>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model’s initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </p>
<blockquote>
<p>随着图像生成器的质量不断提高，深度伪造技术成为社会热议的话题。图像水印允许模型所有者对其生成的AI内容进行检测和标注，从而减轻其造成的损害。然而，当前最先进的图像水印方法仍然容易受到伪造和移除攻击的影响。这种脆弱性部分是因为水印会扭曲生成的图像的分布，从而无意中泄露有关水印技术的信息。在这项工作中，我们首先展示了一种基于扩散模型初始噪声的无失真图像水印方法。然而，检测水印需要比较图像的重建初始噪声与所有之前使用的初始噪声。为了缓解这些问题，我们提出了一种用于高效检测的两阶段水印框架。在生成过程中，我们通过将生成的傅里叶模式与初始噪声相结合，嵌入有关我们所用初始噪声组的信息。对于检测，我们（i）检索相关的噪声组，（ii）在给定组内搜索可能与我们的图像匹配的初始噪声。这种水印方法实现了对一系列攻击的高度稳健性，包括伪造和移除攻击。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04653v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>新一代图像生成器质量不断提升，深伪技术成为社会热议的话题。图像水印可以帮助模型所有者对其AI生成的内容进行检测和标注，减轻潜在的危害。然而，当前最先进的图像水印方法仍然容易受到伪造和删除攻击的影响。为了克服这一问题，我们提出了一种基于扩散模型初始噪声的无畸变水印方法。本文中的检测方法主要关注图像的初始噪声复原比对工作以及防止基于此产生的常见威胁如干扰混淆初始噪声配置的方法实现和改进应用等方面的处理措施等细节内容，并提出了两个阶段的解决方案以实现高效的检测过程。此框架既优化了初始噪声的管理使用效率又增强了检测流程的精确度与效率。该方法针对伪造和攻击的耐受度有明显提高。该项研究的创新性体现在优化了两阶段框架的运行效果使其在处理过程的表现较为卓越且具有优异检测精度与高效率等优点，相较于现有的技术表现处于领先地位。在实验中其显著的优势显著增强，体现了方法的稳定性和先进性。我们利用傅里叶模式增强初始噪声嵌入信息并简化检测过程。这种方法对攻击具有较强的鲁棒性。总体而言，该研究提供了一种创新性的解决方案以应对图像水印技术的挑战，提高了水印的鲁棒性和安全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成器质量的提升引发深伪技术讨论，水印技术成为关键解决方案。</li>
<li>当前图像水印技术面临伪造和删除攻击的挑战。</li>
<li>提出基于扩散模型初始噪声的无畸变水印方法应对上述挑战。</li>
<li>通过嵌入傅里叶模式优化初始噪声的使用与检测效率。</li>
<li>两阶段框架实现高效检测过程，提高水印对攻击的鲁棒性。</li>
<li>该方法相较于现有技术表现领先，展现出显著优势。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-28e32e7c0b9e5d8e4a894413678d703d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-386598134522fd952f67c0570a8317c6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5cb61476f17701a921b96d15e5003ab5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fe0e04f2a8d7e7051c13900ddba61f73.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d15147d40d07fa34ccfd53c158d89ad.jpg" align="middle">
</details>




<h2 id="Taming-Diffusion-Prior-for-Image-Super-Resolution-with-Domain-Shift-SDEs"><a href="#Taming-Diffusion-Prior-for-Image-Super-Resolution-with-Domain-Shift-SDEs" class="headerlink" title="Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs"></a>Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs</h2><p><strong>Authors:Qinpeng Cui, Yixuan Liu, Xinyi Zhang, Qiqi Bao, Qingmin Liao, Li Wang, Tian Lu, Zicheng Liu, Zhongdao Wang, Emad Barsoum</strong></p>
<p>Diffusion-based image super-resolution (SR) models have attracted substantial interest due to their powerful image restoration capabilities. However, prevailing diffusion models often struggle to strike an optimal balance between efficiency and performance. Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency. In this paper, we present DoSSR, a Domain Shift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images. At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models. This integration not only improves the use of diffusion prior but also boosts inference efficiency. Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoS-SDEs. This advancement leads to the fast and customized solvers that further enhance sampling efficiency. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on synthetic and real-world datasets, while notably requiring only 5 sampling steps. Compared to previous diffusion prior based methods, our approach achieves a remarkable speedup of 5-7 times, demonstrating its superior efficiency. Code: <a target="_blank" rel="noopener" href="https://github.com/QinpengCui/DoSSR">https://github.com/QinpengCui/DoSSR</a>. </p>
<blockquote>
<p>基于扩散的图像超分辨率（SR）模型因其强大的图像恢复能力而引起了广泛的关注。然而，流行的扩散模型通常难以在效率和性能之间达到最佳平衡。通常，它们要么忽略了现有预训练模型的潜力，限制了其生成能力，要么需要从随机噪声开始进行多次前向传递，从而影响推理效率。在本文中，我们提出了DoSSR，这是一种基于域迁移扩散的SR模型，它利用预训练扩散模型的生成能力，同时通过以低分辨率（LR）图像开始扩散过程来显著提高效率。我们的方法的核心是一个域迁移方程，它可以无缝地集成到现有的扩散模型中。这种集成不仅改善了扩散先验的使用，还提高了推理效率。此外，我们通过将离散迁移过程转变为连续公式（称为DoS-SDEs）来推进我们的方法。这一进展导致了快速和自定义求解器，进一步提高了采样效率。经验结果表明，我们提出的方法在合成和真实世界数据集上达到了最新性能，而且仅需5个采样步骤。与以前的基于扩散先验的方法相比，我们的方法实现了5-7倍的显著加速，证明了其卓越的效率。代码地址：<a target="_blank" rel="noopener" href="https://github.com/QinpengCui/DoSSR">https://github.com/QinpengCui/DoSSR</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17778v2">PDF</a> This paper is accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>基于扩散的图像超分辨率（SR）模型因其强大的图像恢复能力而受到广泛关注。然而，现有的扩散模型在效率和性能之间难以达到平衡。本文提出的DoSSR模型利用预训练扩散模型的生成能力，并通过从低分辨率（LR）图像开始扩散过程来显著提高效率。该方法的核心是域转移方程，它能无缝地融入现有扩散模型，不仅改善了扩散先验的使用，还提高了推理效率。此外，我们将离散转移过程转变为连续形式，称为DoS-SDEs，进一步提高了采样效率。实证结果表明，所提方法在合成和真实世界数据集上取得了最先进的性能，仅需5个采样步骤，与之前的扩散先验方法相比，实现了5-7倍的加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像超分辨率领域受到关注。</li>
<li>当前扩散模型在效率和性能之间面临平衡挑战。</li>
<li>DoSSR模型利用预训练扩散模型的生成能力。</li>
<li>DoSSR通过从低分辨率图像开始扩散过程来提高效率。</li>
<li>域转移方程无缝融入现有扩散模型，改善扩散先验的使用并提高推理效率。</li>
<li>DoS-SDEs将离散转移过程转变为连续形式，进一步提高采样效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ab597b072942a2f55a19e6e0a704aef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ac555712eeffad8844e29c37a78c10cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0584b3ccb14660910e8031cb904ec4c7.jpg" align="middle">
</details>




<h2 id="DeCLIP-Decoding-CLIP-representations-for-deepfake-localization"><a href="#DeCLIP-Decoding-CLIP-representations-for-deepfake-localization" class="headerlink" title="DeCLIP: Decoding CLIP representations for deepfake localization"></a>DeCLIP: Decoding CLIP representations for deepfake localization</h2><p><strong>Authors:Stefan Smeu, Elisabeta Oneata, Dan Oneata</strong></p>
<p>Generative models can create entirely new images, but they can also partially modify real images in ways that are undetectable to the human eye. In this paper, we address the challenge of automatically detecting such local manipulations. One of the most pressing problems in deepfake detection remains the ability of models to generalize to different classes of generators. In the case of fully manipulated images, representations extracted from large self-supervised models (such as CLIP) provide a promising direction towards more robust detectors. Here, we introduce DeCLIP, a first attempt to leverage such large pretrained features for detecting local manipulations. We show that, when combined with a reasonably large convolutional decoder, pretrained self-supervised representations are able to perform localization and improve generalization capabilities over existing methods. Unlike previous work, our approach is able to perform localization on the challenging case of latent diffusion models, where the entire image is affected by the fingerprint of the generator. Moreover, we observe that this type of data, which combines local semantic information with a global fingerprint, provides more stable generalization than other categories of generative methods. </p>
<blockquote>
<p>生成模型可以创建全新的图像，但它们也可以以人类眼睛无法察觉的方式部分修改真实图像。在本文中，我们解决了自动检测此类局部操作挑战的问题。在深度伪造检测中，仍然存在模型能否推广到不同类别的生成器的问题。对于完全操作的图像，从大型自监督模型（如CLIP）中提取的表示对于实现更稳健的检测器提供了充满希望的方向。在这里，我们介绍了DeCLIP，这是首次尝试利用此类大型预训练特征来检测局部操作。我们表明，当与合理的卷积解码器结合时，预训练的自我监督表示能够执行定位并改善现有方法的泛化能力。与以前的工作不同，我们的方法能够在潜在扩散模型的复杂情况下进行定位，其中整个图像受到生成器的指纹影响。此外，我们观察到这种结合了局部语义信息和全局指纹的数据类型提供了比其他类别生成方法更稳定的泛化性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08849v2">PDF</a> Accepted at Winter Conference on Applications of Computer Vision   (WACV) 2025</p>
<p><strong>Summary</strong><br>    本文介绍了利用大型预训练模型特征检测图像局部修改的方法，名为DeCLIP。通过结合预训练自监督表示和卷积解码器，该方法能够实现对局部操作的定位，并提高了对各类生成器的泛化能力。该方法能在潜在扩散模型的挑战情况下进行定位，并能更稳定地泛化到其他类别的生成方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型可以创建全新图像，也能对真实图像进行局部修改，这些修改对人类难以察觉。</li>
<li>DeCLIP方法利用大型预训练模型特征来检测图像的局部操作。</li>
<li>结合预训练自监督表示和卷积解码器，DeCLIP能定位局部操作并提高对各类生成器的泛化能力。</li>
<li>DeCLIP方法在潜在扩散模型的定位上具有优势。</li>
<li>局部语义信息和全局指纹的结合提供了更稳定的泛化性能。</li>
<li>利用大型自监督模型（如CLIP）的表示为更稳健的检测器提供了方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-197897f62781de822354c3bd843ecdd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5ddbb15164f61d7df567fdf87c01d6a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4f29207e00aa799e424f969c405e6320.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-564284db12e359511251f8e8a04fea49.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a864b883db099cde2b6d1bace0d21000.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a7921baccd0154010b1f1354bd31bb17.jpg" align="middle">
</details>




<h2 id="Scalable-Autoregressive-Image-Generation-with-Mamba"><a href="#Scalable-Autoregressive-Image-Generation-with-Mamba" class="headerlink" title="Scalable Autoregressive Image Generation with Mamba"></a>Scalable Autoregressive Image Generation with Mamba</h2><p><strong>Authors:Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li</strong></p>
<p>We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mamba’s core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM">https://github.com/hp-l33/AiM</a> </p>
<blockquote>
<p>我们介绍了基于Mamba架构的自回归（AR）图像生成模型AiM。AiM采用Mamba这一新型状态空间模型，以其对长序列建模的出色性能以及线性时间复杂度为特点，旨在取代自回归图像生成模型中常用的Transformer，以实现更高的生成质量和更快的推理速度。与现有方法不同，这些方法采用Mamba通过多方向扫描处理二维信号，AiM直接采用下一代预测范式进行自回归图像生成。这种方法避免了需要对Mamba进行大量修改以学习二维空间表示的需要。通过对视觉生成任务进行简单而有针对性的修改，我们保留了Mamba的核心结构，充分利用了其高效的长序列建模能力和可扩展性。我们提供了不同规模的AiM模型，参数数量从148M到1.3B不等。在ImageNet1K 256*256基准测试中，我们最好的AiM模型实现了FID为2.21，超过了所有现有参数相近的自回归模型，并在推理速度上表现出显著的优势，是扩散模型的2到10倍。代码可在<a target="_blank" rel="noopener" href="https://github.com/hp-l33/AiM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hp-l33/AiM找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12245v3">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong><br>     我们介绍了基于Mamba架构的自回归（AR）图像生成模型AiM。AiM利用具有线性时间复杂度、适用于长序列建模的新状态空间模型Mamba，替代AR图像生成模型中常用的Transformer，旨在实现更高的生成质量和更快的推理速度。AiM直接采用下一个令牌预测范式进行自回归图像生成，无需对Mamba进行大量修改以适应二维信号。我们在各种规模的ImageNet1K 256*256基准测试上，最好的AiM模型实现了FID为2.21，超越了所有现有参数相近的AR模型，并在推理速度上显著快于扩散模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AiM是一个基于Mamba架构的自回归图像生成模型。</li>
<li>Mamba是一个新的状态空间模型，具有线性时间复杂度，适用于长序列建模。</li>
<li>AiM利用Mamba替代Transformer，实现更高的生成质量和更快的推理速度。</li>
<li>AiM采用下一个令牌预测范式进行自回归图像生成，无需对Mamba进行大量修改以适应二维信号。</li>
<li>AiM在各种规模的ImageNet1K 256*256基准测试上表现优异，最佳模型的FID为2.21。</li>
<li>AiM在性能上超越了参数相近的现有自回归模型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d054cf158cdc646867dab396f77b531a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f12290be29c2e4f6dffc3b385d4360ef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7081497a5e348adb3d9b8e58da6baf6c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-52b010c7b40b64ac0c995f5595fca100.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3690e88e538007c181f769678283c6e4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9315d9a228dce5cde9174681dbb3a684.jpg" align="middle">
</details>




<h2 id="EvolvED-Evolutionary-Embeddings-to-Understand-the-Generation-Process-of-Diffusion-Models"><a href="#EvolvED-Evolutionary-Embeddings-to-Understand-the-Generation-Process-of-Diffusion-Models" class="headerlink" title="EvolvED: Evolutionary Embeddings to Understand the Generation Process of   Diffusion Models"></a>EvolvED: Evolutionary Embeddings to Understand the Generation Process of   Diffusion Models</h2><p><strong>Authors:Vidya Prasad, Hans van Gorp, Christina Humer, Ruud J. G. van Sloun, Anna Vilanova, Nicola Pezzotti</strong></p>
<p>Diffusion models, widely used in image generation, rely on iterative refinement to generate images from noise. Understanding this data evolution is important for model development and interpretability, yet challenging due to its high-dimensional, iterative nature. Prior works often focus on static or instance-level analyses, missing the iterative and holistic aspects of the generative path. While dimensionality reduction can visualize image evolution for few instances, it does preserve the iterative structure. To address these gaps, we introduce EvolvED, a method that presents a holistic view of the iterative generative process in diffusion models. EvolvED goes beyond instance exploration by leveraging predefined research questions to streamline generative space exploration. Tailored prompts aligned with these questions are used to extract intermediate images, preserving iterative context. Targeted feature extractors trace the evolution of key image attribute evolution, addressing the complexity of high-dimensional outputs. Central to EvolvED is a novel evolutionary embedding algorithm that encodes iterative steps while maintaining semantic relations. It enhances the visualization of data evolution by clustering semantically similar elements within each iteration with t-SNE, grouping elements by iteration, and aligning an instance’s elements across iterations. We present rectilinear and radial layouts to represent iterations and support exploration. We apply EvolvED to diffusion models like GLIDE and Stable Diffusion, demonstrating its ability to provide valuable insights into the generative process. </p>
<blockquote>
<p>扩散模型广泛应用于图像生成，依赖于迭代细化从噪声中生成图像。了解数据的演变对于模型发展和可解释性很重要，但由于其高维度、迭代性质，这具有挑战性。以前的工作经常关注静态或实例级别的分析，忽略了生成路径的迭代和整体方面。虽然降维可以为少数实例可视化图像演变，但它并不保留迭代结构。为了解决这些差距，我们引入了EvolvED，一种呈现扩散模型中迭代生成过程整体视图的方法。EvolvED通过利用预先设定的研究问题来简化生成空间探索，超越了实例探索。根据这些问题定制的提示用于提取中间图像，保留迭代上下文。有针对性的特征提取器追踪关键图像属性演变的过程，解决高维度输出的复杂性。EvolvED的核心是一种新型进化嵌入算法，该算法在编码迭代步骤的同时保持语义关系。它通过t-SNE将每个迭代中语义上相似的元素进行聚类，按迭代对元素进行分组，并跨迭代对齐实例的元素，从而增强了数据演变的可视化。我们提供了直线和径向布局来表示迭代并支持探索。我们将EvolvED应用于GLIDE和Stable Diffusion等扩散模型，展示了它为生成过程提供有价值见解的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17462v2">PDF</a> </p>
<p><strong>Summary</strong>：扩散模型在图像生成领域应用广泛，依靠迭代优化从噪声生成图像。了解数据演化对模型发展和可解释性至关重要，但由于其高维迭代特性，存在挑战。此前的研究往往侧重于静态或实例级别的分析，忽视了生成路径的迭代和整体方面。本文提出EvolvED方法，为扩散模型的迭代生成过程提供全面视角。EvolvED不仅探索实例，还利用预设的研究问题来引导生成空间探索。通过有针对性的提示来提取中间图像，保持迭代上下文。目标特征提取器追踪关键图像属性演变，解决高维输出的复杂性。EvolvED的核心是一种新型进化嵌入算法，可在保持语义关系的同时编码迭代步骤。通过t-SNE对语义相似元素进行聚类，按迭代分组元素，并在迭代之间对齐实例元素。采用直角坐标和径向布局表示迭代并支持探索。我们将EvolvED应用于GLIDE和Stable Diffusion等扩散模型，展示其对生成过程提供有价值洞察的能力。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>扩散模型依靠迭代优化从噪声生成图像，理解数据演化对模型发展和可解释性至关重要。</li>
<li>此前的研究多侧重于静态或实例级别的分析，忽视了扩散模型的迭代和整体方面。</li>
<li>EvolvED方法为扩散模型的迭代生成过程提供全面视角，包括利用预设研究问题和有针对性的提示进行探索。</li>
<li>目标特征提取器能够追踪关键图像属性演变。</li>
<li>新型进化嵌入算法是EvolvED的核心，可在保持语义关系的同时编码迭代步骤。</li>
<li>通过t-SNE对语义相似元素进行聚类，按迭代分组元素，展示数据演化的可视化。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-75566319f5d337bb575cea1f6b3b8f4c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1db2ca04ebb2f197b82a761f00091a3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a3092c430d36df5f080356ba8cae3c66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d50933f392876def70c174fed3abc58f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7203375e3eb793fbb6a918817e6c17b4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46e21923d0cde497725267e88773e7e4.jpg" align="middle">
</details>




<h2 id="Ctrl-X-Controlling-Structure-and-Appearance-for-Text-To-Image-Generation-Without-Guidance"><a href="#Ctrl-X-Controlling-Structure-and-Appearance-for-Text-To-Image-Generation-Without-Guidance" class="headerlink" title="Ctrl-X: Controlling Structure and Appearance for Text-To-Image   Generation Without Guidance"></a>Ctrl-X: Controlling Structure and Appearance for Text-To-Image   Generation Without Guidance</h2><p><strong>Authors:Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou</strong></p>
<p>Recent controllable generation approaches such as FreeControl and Diffusion Self-Guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model. See our project page for an overview of the results: <a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a> </p>
<blockquote>
<p>最近的可控生成方法，如FreeControl和Diffusion Self-Guidance，为文本到图像（T2I）的扩散模型带来了精细的空间和外观控制，而无需训练辅助模块。然而，这些方法针对每种类型的分数函数优化潜在嵌入，扩散步骤较长，使得生成过程耗时，并限制了其灵活性和使用。本研究提出了Ctrl-X，这是一个用于T2I扩散控制结构和外观的简单框架，无需额外的训练或指导。Ctrl-X设计前馈结构控制以实现与结构图像的结构对齐，并设计语义感知外观传输以促进从用户输入图像进行外观传输。广泛的定性和定量实验表明，Ctrl-X在各种条件输入和模型检查点上的性能优越。特别地，Ctrl-X支持具有任意条件图像（任何模式）的新结构和外观控制，与现有作品相比展现出优越的图像质量和外观传输能力，并为任何T2I和文本到视频（T2V）的扩散模型提供了即时插入和播放功能。想了解更多实验结果，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07540v2">PDF</a> 22 pages, 17 figures, see project page at   <a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x">https://genforce.github.io/ctrl-x</a></p>
<p><strong>Summary</strong></p>
<p>近期可控生成方法如FreeControl和Diffusion Self-Guidance为文本到图像（T2I）扩散模型带来了精细的时空和外观控制，无需训练辅助模块。然而，这些方法针对每种类型的分数函数优化潜在嵌入，并增加了扩散步骤，使得生成过程耗时，并限制了其灵活性和使用。本研究提出Ctrl-X，一个用于T2I扩散控制结构和外观的简单框架，无需额外的训练或指导。Ctrl-X设计前馈结构控制以实现结构图像的结构对齐和用户输入图像的语义感知外观传输，以促进外观的传输。广泛的定性和定量实验表明，Ctrl-X在各种条件输入和模型检查点上具有卓越的性能。特别地，Ctrl-X支持具有任意模态条件图像的新颖结构和外观控制，与现有作品相比展现出卓越的图像质量和外观转移效果，并为任何T2I和文本到视频（T2V）扩散模型提供即时即用的功能。有关结果的概述，请参阅我们的项目页面：<a target="_blank" rel="noopener" href="https://genforce.github.io/ctrl-x%E3%80%82">https://genforce.github.io/ctrl-x。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ctrl-X是一个用于文本到图像（T2I）扩散模型的可控生成框架。</li>
<li>它实现了结构控制和外观控制，无需额外的训练或指导。</li>
<li>Ctrl-X通过前馈结构控制实现结构对齐和语义感知的外观传输。</li>
<li>与现有方法相比，Ctrl-X在结构和外观控制方面表现出卓越性能。</li>
<li>Ctrl-X支持任意模态条件图像的新颖结构和外观控制。</li>
<li>Ctrl-X提供了高图像质量和外观转移效果。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1c7e485979e9ea6d843b17aa542d313c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b905cfd7c6b5741f6b971f6b709a8c1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-69c0ca886edc147424356c0976de6413.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a110dc2f9facee3193a8ddf3a250f813.jpg" align="middle">
</details>




<h2 id="RectifID-Personalizing-Rectified-Flow-with-Anchored-Classifier-Guidance"><a href="#RectifID-Personalizing-Rectified-Flow-with-Anchored-Classifier-Guidance" class="headerlink" title="RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance"></a>RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance</h2><p><strong>Authors:Zhicheng Sun, Zhenhao Yang, Yang Jin, Haozhe Chi, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Yang Song, Kun Gai, Yadong Mu</strong></p>
<p>Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at <a target="_blank" rel="noopener" href="https://github.com/feifeiobama/RectifID">https://github.com/feifeiobama/RectifID</a>. </p>
<blockquote>
<p>定制扩散模型以从用户提供的参考图像生成身份保留图像是一个引人入胜的新问题。流行的方法通常需要在对特定领域的大量图像上进行训练以实现身份保留，这在不同的使用情况下缺乏灵活性。为了解决这一问题，我们利用分类器指导这一无需训练的技术，使用现有分类器来引导扩散模型进行个性化图像生成。我们的研究表明，基于最新的校正流框架，可以通过简单的定点解决方案解决普通分类器指导的主要局限性，即需要特殊分类器的问题，从而实现使用现成的图像鉴别器进行灵活的个性化设置。而且，当其解决方案锚定到参考流轨迹时，其解决过程是稳定的，并且有收敛保证。所推导的方法在不同的现成图像鉴别器上实现了校正流，为人脸、活生生主体和某些物体生成了有利的个性化结果。代码可在<a target="_blank" rel="noopener" href="https://github.com/feifeiobama/RectifID%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/feifeiobama/RectifID找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14677v4">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong><br>扩散模型根据用户提供的参考图像生成身份保留图像是一个新兴的问题。为了解决这个问题，研究者们采取了分类器引导的训练前技术来个性化生成图像，这不需要特定的分类器即可指导扩散模型。本研究表明利用现有图像鉴别器进行个性化生成具有灵活性和稳定性，并已在人脸、活体物体和某些对象上取得了优势结果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究聚焦于如何在扩散模型中自定义生成身份保留的图像。</li>
<li>传统方法需要大量特定领域的图像进行训练以实现身份保留，缺乏灵活性。</li>
<li>研究者采用训练前的分类器引导技术来解决这个问题，无需特定分类器即可指导扩散模型。</li>
<li>利用现有图像鉴别器进行个性化生成在解决程序上相对稳定。此方法提供了一个简单且灵活的解决方案来适应不同的使用案例。通过引用一个简单的固定点解决方案解决了普通分类器引导的主要局限性。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26f9c5ae4b60b66ffa9f62575a833d44.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a0b616068b083bb24b4588ce378aecb7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f4e35066b1a9cce3f36fdaa67ae742d3.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-12  GN-FRGeneralizable Neural Radiance Fields for Flare Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
