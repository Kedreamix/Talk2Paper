<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œä»£ç ï¼‰å’Œè¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨è¯­è¨€å»ºæ¨¡ï¼ˆLatentLMï¼‰ï¼Œå®ƒæ— ç¼é›†æˆäº†è¿ç»­æ•°æ®å’Œç¦»æ•£æ•°æ®ï¼Œä½¿ç”¨å› æœTransformerã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è¿ç»­æ•°æ®è¡¨ç¤ºä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œæ‰©æ•£æ¥è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†Ïƒ-VAEï¼Œä»¥è§£å†³æ–¹å·®å´©æºƒçš„æŒ‘æˆ˜ï¼Œè¿™å¯¹äºè‡ªå›å½’å»ºæ¨¡è‡³å…³é‡è¦ã€‚å¤§é‡å®éªŒè¯æ˜äº†LatentLMåœ¨å„ç§æ¨¡æ€ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒLatentLMåœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢éƒ½è¶…è¶Šäº†Diffusion Transformersã€‚å½“é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼ŒLatentLMæä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£ï¼Œç»Ÿä¸€äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰©å¤§è®­ç»ƒä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒLatentLMä¸Transfusionå’Œå‘é‡é‡åŒ–æ¨¡å‹ç›¸æ¯”ï¼Œå–å¾—äº†æœ‰åˆ©çš„æ€§èƒ½ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ–¹é¢ï¼ŒLatentLMåœ¨è¯­éŸ³ç›¸ä¼¼æ€§å’Œç¨³å¥æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„VALL-E 2æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦10å€æ›´å°‘çš„è§£ç æ­¥éª¤ã€‚ç»“æœè¯æ˜LatentLMæ˜¯ä¸€ç§é«˜åº¦æœ‰æ•ˆå’Œå¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯æ¨åŠ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Latent Language Modelingï¼ˆLatentLMï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆè¿ç»­æ•°æ®å’Œç¦»æ•£æ•°æ®ã€‚å®ƒä½¿ç”¨å› æœTransformerå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¡¨ç¤ºè¿ç»­æ•°æ®ä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªæ ‡è®°æ‰©æ•£è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚ä¸ºè§£å†³è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ï¼Œå¼€å‘å‡ºäº†Ïƒ-VAEã€‚å®éªŒè¡¨æ˜ï¼ŒLatentLMåœ¨å¤šç§æ¨¡æ€ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå›¾åƒç”Ÿæˆæ–¹é¢è¶…è¿‡äº†Diffusion Transformersï¼Œåœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æ€»çš„æ¥è¯´ï¼ŒLatentLMæ˜¯ä¸€ç§é«˜æ•ˆå¯ä¼¸ç¼©çš„å¤šæ¨¡æ€å»ºæ¨¡æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Language Modelingï¼ˆLatentLMï¼‰å¯ä»¥æ— ç¼é›†æˆè¿ç»­æ•°æ®å’Œç¦»æ•£æ•°æ®ã€‚</li>
<li>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç”¨äºå°†è¿ç»­æ•°æ®è¡¨ç¤ºä¸ºæ½œåœ¨å‘é‡ã€‚</li>
<li>å¼•å…¥ä¸‹ä¸€ä¸ªæ ‡è®°æ‰©æ•£è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚</li>
<li>Ïƒ-VAEè§£å†³äº†è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ã€‚</li>
<li>LatentLMåœ¨å¤šç§æ¨¡æ€ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå›¾åƒç”Ÿæˆä¼˜äºDiffusion Transformersã€‚</li>
<li>é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„LatentLMæä¾›äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£æ¥å£ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Zero-Shot-Mono-to-Binaural-Speech-Synthesis"><a href="#Zero-Shot-Mono-to-Binaural-Speech-Synthesis" class="headerlink" title="Zero-Shot Mono-to-Binaural Speech Synthesis"></a>Zero-Shot Mono-to-Binaural Speech Synthesis</h2><p><strong>Authors:Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn, Eliya Nachmani</strong></p>
<p>We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ZeroBASæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå•å£°é“éŸ³é¢‘å½•éŸ³å’Œä½ç½®ä¿¡æ¯åˆæˆåŒå£°é“éŸ³é¢‘çš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œæ— éœ€å¯¹ä»»ä½•åŒå£°é“æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å‘è¡¨çš„æ— è®­ç»ƒåŒå£°é“éŸ³é¢‘åˆæˆçš„ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºæºä½ç½®çš„å‚æ•°åŒ–å‡ ä½•æ—¶é—´æ‰­æ›²å’ŒæŒ¯å¹…ç¼©æ”¾è¶³ä»¥è¿›è¡Œåˆæ­¥çš„å¬è§‰åˆæˆï¼Œè¿™å¯ä»¥é€šè¿‡åå¤åº”ç”¨é¢„è®­ç»ƒçš„é™å™ªç¼–ç å™¨æ¥ä¼˜åŒ–æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§æ–¹æ³•èƒ½åœ¨ä¸åŒçš„æˆ¿é—´æ¡ä»¶ä¸‹å®ç°æ³›åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„æ•°æ®é›†TUT Mono-to-Binauralæ¥è¡¡é‡è¿™ä¸€ç‚¹ï¼Œä»¥è¯„ä¼°æœ€å…ˆè¿›çš„å•å£°é“åˆ°åŒå£°é“åˆæˆæ–¹æ³•åœ¨æœªè§æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•ä¸æ ‡å‡†å•å£°é“åˆ°åŒå£°é“æ•°æ®é›†ä¸Šçš„æœ‰ç›‘ç£æ–¹æ³•çš„æ€§èƒ½ç›¸å½“ï¼Œç”šè‡³åœ¨æˆ‘ä»¬çš„ç¦»åˆ†å¸ƒTUT Mono-to-Binauralæ•°æ®é›†ä¸Šè¶…è¿‡äº†å®ƒä»¬ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†é¢„è®­ç»ƒçš„ç”ŸæˆéŸ³é¢‘æ¨¡å‹å’Œé›¶è®­ç»ƒå­¦ä¹ çš„æ½œåŠ›ï¼Œä¸ºç¨³å¥çš„åŒå£°é“éŸ³é¢‘åˆæˆæä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒä»»ä½•åŒè€³æ•°æ®ï¼Œå³å¯ä»å•å£°é“éŸ³é¢‘è®°å½•å’Œä½ç½®ä¿¡æ¯åˆæˆåŒè€³éŸ³é¢‘ã€‚è¿™æ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒçš„åŒå£°é“éŸ³é¢‘åˆæˆæ–¹æ³•ã€‚é€šè¿‡åŸºäºæºä½ç½®çš„å‚æ•°åŒ–å‡ ä½•æ—¶é—´æ‰­æ›²å’ŒæŒ¯å¹…ç¼©æ”¾ï¼Œå¯ä»¥å¾—åˆ°åˆæ­¥çš„åŒè€³åˆæˆï¼Œå†é€šè¿‡é¢„è®­ç»ƒçš„é™å™ªç¼–è§£ç å™¨è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•åœ¨è·¨æˆ¿é—´æ¡ä»¶ä¸‹çš„è¡¨ç°å¾—åˆ°äº†è¯„ä¼°ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TUT Mono-to-Binauralä»¥è¯„ä¼°æœ€æ–°å•å£°é“åˆ°åŒå£°é“çš„åˆæˆæ–¹æ³•åœ¨æœªè§è¿‡çš„æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ä¸æ ‡å‡†å•å£°é“åˆ°åŒå£°é“æ•°æ®é›†ä¸Šçš„ç›‘ç£æ–¹æ³•çš„æ€§èƒ½ç›¸å½“ï¼Œç”šè‡³åœ¨æˆ‘ä»¬çš„è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„TUT Mono-to-Binauralæ•°æ®é›†ä¸Šè¶…è¿‡äº†å®ƒä»¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºç¥ç»æ–¹æ³•çš„ZeroBASï¼Œèƒ½å¤Ÿä»å•å£°é“éŸ³é¢‘å’Œä½ç½®ä¿¡æ¯é›¶æ ·æœ¬åˆæˆåŒè€³éŸ³é¢‘ã€‚</li>
<li>é€šè¿‡å‚æ•°åŒ–çš„å‡ ä½•æ—¶é—´æ‰­æ›²å’ŒæŒ¯å¹…ç¼©æ”¾è¿›è¡Œåˆæ­¥çš„åŒè€³åˆæˆã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„é™å™ªç¼–è§£ç å™¨è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜åˆæˆè´¨é‡ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®é›†TUT Mono-to-Binauralä»¥è¯„ä¼°åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>é›¶æ ·æœ¬æ–¹æ³•çš„è¡¨ç°ä¸æ ‡å‡†æ•°æ®é›†ä¸Šçš„ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</li>
<li>åœ¨æ–°æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fc16144bf45b94da94e40c11b920a95f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d0fbdc199ca731fc2359afadc2dc804e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5639e22ca8041f88b59072e33ed30687.jpg" align="middle">
</details>




<h2 id="A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings"><a href="#A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings" class="headerlink" title="A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings"></a>A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings</h2><p><strong>Authors:Anindita Mondal, Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Anil Kumar Vuppala, Chiranjeevi Yarra</strong></p>
<p>Automatic detection of prominence at the word and syllable-levels is critical for building computer-assisted language learning systems. It has been shown that prosody embeddings learned by the current state-of-the-art (SOTA) text-to-speech (TTS) systems could generate word- and syllable-level prominence in the synthesized speech as natural as in native speech. To understand the effectiveness of prosody embeddings from TTS for prominence detection under nonnative context, a comparative analysis is conducted on the embeddings extracted from native and non-native speech considering the prominence-related embeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2. These embeddings are extracted under two conditions considering: 1) only text, 2) both speech and text. For the first condition, the embeddings are extracted directly from the TTS inference mode, whereas for the second condition, we propose to extract from the TTS under training mode. Experiments are conducted on native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For experimentation, word-level prominence locations are manually annotated for both corpora. The highest relative improvement on word &amp; syllable-level prominence detection accuracies with the TTS embeddings are found to be 13.7% &amp; 5.9% and 16.2% &amp; 6.9% compared to those with the heuristic-based features and self-supervised Wav2Vec-2.0 representations, respectively. </p>
<blockquote>
<p>è‡ªåŠ¨æ£€æµ‹å•è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºæ€§å¯¹äºæ„å»ºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå­¦ä¹ çš„éŸµå¾‹åµŒå…¥å¯ä»¥ç”Ÿæˆä¸åŸç”Ÿè¯­éŸ³ä¸€æ ·è‡ªç„¶çš„å•è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºæ€§ã€‚ä¸ºäº†äº†è§£TTSçš„éŸµå¾‹åµŒå…¥åœ¨éæœ¬åœŸè¯­å¢ƒä¸‹çš„çªå‡ºæ€§æ£€æµ‹æ•ˆæœï¼Œå¯¹æ¥è‡ªåä¸ºFastSpeech2çš„å…ˆè¿›TTSçš„åµŒå…¥è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œè€ƒè™‘äº†ä¸çªå‡ºæ€§ç›¸å…³çš„åµŒå…¥ï¼šæŒç»­æ—¶é—´ã€èƒ½é‡å’ŒéŸ³è°ƒã€‚è¿™äº›åµŒå…¥æ˜¯åœ¨ä¸¤ç§æ¡ä»¶ä¸‹æå–çš„ï¼š1ï¼‰åªæœ‰æ–‡æœ¬ï¼Œ2ï¼‰è¯­éŸ³å’Œæ–‡æœ¬éƒ½åŒ…å«ã€‚å¯¹äºç¬¬ä¸€ç§æƒ…å†µï¼ŒåµŒå…¥ç›´æ¥ä»TTSæ¨ç†æ¨¡å¼ä¸­æå–ï¼Œè€Œå¯¹äºç¬¬äºŒç§æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºåœ¨TTSè®­ç»ƒæ¨¡å¼ä¸‹æå–ã€‚å®éªŒåœ¨æœ¬åœ°è¯­éŸ³è¯­æ–™åº“Tatoebaå’Œéæœ¬åœ°è¯­éŸ³è¯­æ–™åº“ISLEä¸Šè¿›è¡Œã€‚ä¸ºäº†å®éªŒï¼Œä¸¤ä¸ªè¯­æ–™åº“çš„å•è¯çº§åˆ«çªå‡ºä½ç½®éƒ½è¿›è¡Œäº†æ‰‹åŠ¨æ³¨é‡Šã€‚ä¸åŸºäºå¯å‘å¼ç‰¹å¾å’Œè‡ªç›‘ç£çš„Wav2Vec-2.0è¡¨ç¤ºç›¸æ¯”ï¼Œä½¿ç”¨TTSåµŒå…¥çš„å•è¯å’ŒéŸ³èŠ‚çº§åˆ«çªå‡ºæ€§æ£€æµ‹ç²¾åº¦çš„ç›¸å¯¹æé«˜æœ€é«˜åˆ†åˆ«ä¸º13.7%å’Œ5.9%ï¼Œä»¥åŠ16.2%å’Œ6.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08283v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è‡ªåŠ¨æ£€æµ‹è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„é‡è¦æ€§ï¼Œè¿™å¯¹äºæ„å»ºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå­¦ä¹ åˆ°çš„éŸµå¾‹åµŒå…¥å¯ä»¥ç”Ÿæˆä¸è‡ªç„¶è¯­éŸ³ä¸€æ ·è‡ªç„¶çš„è¯å’ŒéŸ³èŠ‚çº§åˆ«çªå‡ºã€‚ä¸ºäº†äº†è§£åœ¨éæœ¬åœ°èƒŒæ™¯ä¸‹TTSéŸµå¾‹åµŒå…¥åœ¨çªå‡ºæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¯¹éæœ¬åœ°å’Œæœ¬åœ°è¯­éŸ³çš„åµŒå…¥è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œè€ƒè™‘äº†æŒç»­æ—¶é—´ã€èƒ½é‡å’ŒéŸ³è°ƒç­‰çªå‡ºç›¸å…³çš„åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å¯å‘å¼ç‰¹å¾å’Œè‡ªç›‘ç£Wav2Vec-2.0è¡¨ç¤ºç›¸æ¯”ï¼Œä½¿ç”¨TTSåµŒå…¥çš„è¯å’ŒéŸ³èŠ‚çº§åˆ«çªå‡ºæ£€æµ‹å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†æœ€é«˜è¾¾13.7%å’Œ6.9%ã€‚æ€»ç»“èµ·æ¥ï¼Œè¯¥æ–‡æœ¬æ¢è®¨äº†ä½¿ç”¨å…ˆè¿›çš„TTSç³»ç»Ÿæ£€æµ‹è¯­éŸ³ä¸­çš„è¯å’ŒéŸ³èŠ‚çº§åˆ«çªå‡ºçš„æ–¹æ³•å’Œæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„å…³é”®è§è§£åˆ—è¡¨ï¼š</p>
<ol>
<li>è‡ªåŠ¨æ£€æµ‹è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºå¯¹äºæ„å»ºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„éŸµå¾‹åµŒå…¥å¯ä»¥ç”Ÿæˆä¸è‡ªç„¶è¯­éŸ³ä¸€æ ·è‡ªç„¶çš„è¯å’ŒéŸ³èŠ‚çº§åˆ«çªå‡ºã€‚</li>
<li>ä¸ºäº†æ¯”è¾ƒåœ¨éæœ¬åœ°èƒŒæ™¯ä¸‹TTSéŸµå¾‹åµŒå…¥åœ¨çªå‡ºæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¯¹éæœ¬åœ°å’Œæœ¬åœ°è¯­éŸ³çš„åµŒå…¥è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</li>
<li>è€ƒè™‘äº†æŒç»­æ—¶é—´ã€èƒ½é‡å’ŒéŸ³è°ƒç­‰çªå‡ºç›¸å…³çš„åµŒå…¥å› ç´ ã€‚</li>
<li>å®éªŒåœ¨æœ¬åœ°è¯­éŸ³è¯­æ–™åº“Tatoebaå’Œéæœ¬åœ°è¯­éŸ³è¯­æ–™åº“ISLEä¸Šè¿›è¡Œã€‚</li>
<li>æ‰‹åŠ¨æ³¨é‡Šäº†ä¸¤ç§è¯­æ–™åº“çš„è¯çº§åˆ«çªå‡ºçš„ä½ç½®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cddeb908c5ada709a4912e18669893fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5338a274123eec473a1867ddacc01306.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56dc19cd9feb26e4ff74f63dad0098a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-15260501556eca45828eccdcf14f7450.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1eb686b114ea5a3fc74636b3e2d428e2.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>åŸºäºå¤§æ¨¡å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå¯¹æ•°æ®æœ‰ç€æé«˜çš„éœ€æ±‚ã€‚è¿‘æœŸåŸºäºå¤§æ¨¡å‹çš„TTSå·¥ä½œé€šå¸¸é‡‡ç”¨å¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹æ¥è·å¾—é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›å¤æ‚æµç¨‹éœ€è¦åœ¨æ¯ä¸ªé˜¶æ®µéƒ½æœ‰å‡ºè‰²çš„æ¨¡å‹ï¼ˆä¾‹å¦‚è¯­éŸ³é™å™ªã€è¯­éŸ³å¢å¼ºã€è¯´è¯äººåˆ†éŸ³å’Œæ ‡ç‚¹æ¨¡å‹ï¼‰ï¼Œè€Œè¿™äº›æ¨¡å‹æœ¬èº«ä¹Ÿéœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¾ˆå°‘å¼€æºã€‚å³ä½¿ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨ä»¥åŠæ ‡ç‚¹ç¬¦å·ä¸å®é™…è¯­éŸ³åœé¡¿ä¹‹é—´çš„ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œä¸¥æ ¼çš„è¿‡æ»¤ç­–ç•¥é€šå¸¸åªèƒ½ä¿ç•™åŸå§‹æ•°æ®çš„10-30%ï¼Œæå¤§åœ°é˜»ç¢äº†æ•°æ®æ‰©å±•å·¥ä½œã€‚</p>
</blockquote>
<p>åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡äº†ä¸€ä¸ªç®€åŒ–è€Œé«˜æ•ˆTTSæ•°æ®å¤„ç†æµç¨‹ï¼Œè¯¥æµç¨‹åœ¨ä¿æŒæ•°æ®è´¨é‡çš„åŒæ—¶å¤§å¹…é™ä½äº†æ•°æ®è·å–æˆæœ¬ï¼Œå®ç°äº†è¶…è¿‡50%çš„æ•°æ®ä¿ç•™ç‡ã€‚é™¤äº†æ•°æ®æ‰©å±•çš„æŒ‘æˆ˜å¤–ï¼ŒåŸºäºå¤§æ¨¡å‹çš„TTSç³»ç»Ÿçš„éƒ¨ç½²æˆæœ¬ä¹Ÿé«˜äºä¼ ç»Ÿæ–¹æ³•ã€‚å½“å‰çš„ç³»ç»Ÿé€šå¸¸ä»…å°†å¤§æ¨¡å‹ç”¨äºæ–‡æœ¬åˆ°ä»£å¸çš„ç”Ÿæˆï¼Œä½†éœ€è¦é¢å¤–çš„æ¨¡å‹ï¼ˆå¦‚æµåŒ¹é…æ¨¡å‹ï¼‰è¿›è¡Œä»£å¸åˆ°æ³¢å½¢ç”Ÿæˆï¼Œè¿™äº›æ¨¡å‹ä¸èƒ½ç”±å¤§æ¨¡å‹æ¨ç†å¼•æ“ç›´æ¥æ‰§è¡Œï¼Œè¿›ä¸€æ­¥å¢åŠ äº†éƒ¨ç½²çš„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†å¤§æ¨¡å‹å’Œæµç»„ä»¶ä¸­çš„å†—ä½™æ¨¡å—ï¼Œå¹¶ç”¨å¤§æ¨¡å‹æ¶æ„æ›¿ä»£æµæ¨¡å‹çš„ä¸»å¹²ã€‚åŸºäºè¿™ç§ç®€åŒ–çš„æµä¸»å¹²ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæµå¼å’Œéæµå¼æ¨ç†çš„ç»Ÿä¸€æ¶æ„ï¼Œå¤§å¤§é™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨åŒä¸€æ•°æ®è¿›è¡ŒTTSå’Œè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡è®­ç»ƒçš„å¯èƒ½æ€§ï¼Œè¿™å¾—ç›Šäºç®€åŒ–çš„æµç¨‹å’ŒS3Tokenizeré™ä½äº†å¯¹TTSè®­ç»ƒæ•°æ®çš„è´¨é‡è¦æ±‚ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMæ¨¡å‹å¯¹æ•°æ®éœ€æ±‚é‡å¤§ï¼Œä¼ ç»Ÿæ•°æ®é¢„å¤„ç†æµç¨‹å¤æ‚ä¸”æ•°æ®ä¿ç•™ç‡ä½ã€‚æœ¬æ–‡åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡ç®€åŒ–æœ‰æ•ˆçš„TTSæ•°æ®å¤„ç†æµç¨‹ï¼Œæé«˜æ•°æ®ä¿ç•™ç‡å¹¶é™ä½æˆæœ¬ã€‚åŒæ—¶ç®€åŒ–LLMå’Œæµç»„ä»¶ä¸­çš„å†—ä½™æ¨¡å—ï¼Œæå‡ºç»Ÿä¸€æ¶æ„è¿›è¡Œæµå’Œéæµæ¨ç†ï¼Œè¿›ä¸€æ­¥é™ä½éƒ¨ç½²æˆæœ¬ã€‚å¹¶ä¸”æ¢ç´¢ç”¨åŒä¸€æ•°æ®é›†ç»Ÿä¸€TTSå’ŒASRä»»åŠ¡çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMæ¨¡å‹éœ€è¦å¤§é‡æ•°æ®ï¼Œä¼ ç»ŸTTSæ•°æ®é¢„å¤„ç†æµç¨‹å¤æ‚ä¸”æ•°æ®æŸå¤±ä¸¥é‡ã€‚</li>
<li>S3Tokenizerè¢«ç”¨æ¥è®¾è®¡ä¸€ä¸ªç®€åŒ–è€Œæœ‰æ•ˆçš„TTSæ•°æ®å¤„ç†æµç¨‹ï¼Œæé«˜æ•°æ®ä¿ç•™ç‡å¹¶é™ä½æˆæœ¬ã€‚</li>
<li>é€šè¿‡ç®€åŒ–LLMå’Œæµç»„ä»¶ä¸­çš„å†—ä½™æ¨¡å—ï¼Œæå‡ºç»Ÿä¸€æ¶æ„ä»¥æ”¯æŒæµå’Œéæµæ¨ç†ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚</li>
<li>ç®€åŒ–åçš„TTSæµç¨‹å’ŒS3Tokenizerä½¿å¾—TTSè®­ç»ƒæ•°æ®çš„è´¨é‡è¦æ±‚é™ä½ï¼Œä¸ºç»Ÿä¸€TTSå’ŒASRä»»åŠ¡è®­ç»ƒæä¾›äº†å¯èƒ½æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶è§£å†³äº†èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨å’Œæ ‡ç‚¹ä¸å®é™…è¯­éŸ³åœé¡¿ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨LLMæ¶æ„å¤„ç†æµå¼å’Œéæµå¼æ¨ç†ï¼Œæé«˜äº†ç³»ç»Ÿçš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆäººå·¥æ™ºèƒ½å› å…¶ç›¸è¾ƒäºå…¶ä»–ç”ŸæˆæŠ€æœ¯ï¼ˆå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼‰çš„å“è¶Šæ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶å®ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸»æµçš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿä¸»è¦åœ¨è°±ç©ºé—´ä¸­å°†è¾“å‡ºæ˜ å°„åˆ°æ¢…å°”é¢‘è°±ï¼Œç”±äºæ¢…å°”é¢‘è°±çš„ç¨€ç–æ€§ï¼Œå¯¼è‡´è®¡ç®—è´Ÿè½½è¾ƒé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†LatentSpeechï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚LatentSpeechä½¿ç”¨æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå°†ç›®æ ‡ç»´åº¦é™ä½åˆ°æ¢…å°”é¢‘è°±æ‰€éœ€ç»´åº¦çš„5%ï¼Œç®€åŒ–äº†æ–‡æœ¬åˆ°è¯­éŸ³ç¼–ç å™¨åŠvocoderçš„å¤„ç†è¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™é¡¹ç ”ç©¶æ ‡å¿—ç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯ä¸­çš„é¦–æ¬¡é›†æˆï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLatentSpeechåœ¨å•è¯é”™è¯¯ç‡æ–¹é¢æé«˜äº†25%ï¼Œæ¢…å°”å€’è°±å¤±çœŸæ–¹é¢æé«˜äº†24%ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè¿™ä¸¤é¡¹æŒ‡æ ‡åˆ†åˆ«è¿›ä¸€æ­¥æé«˜äº†49.5%å’Œ26%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LatentSpeechåœ¨æ¨åŠ¨æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åŸºäºæ‰©æ•£çš„ç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨è¯­éŸ³ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ä¸»æµè¯­éŸ³åˆæˆç³»ç»Ÿå­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ³•LatentSpeechï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œé™ä½äº†ç›®æ ‡ç»´åº¦ï¼Œç®€åŒ–äº†TTSç¼–ç å™¨å’Œvocoderçš„å¤„ç†ï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentSpeechåœ¨è¯é”™è¯¯ç‡å’Œæ¢…å°”å€’è°±å¤±çœŸæ–¹é¢ç›¸æ¯”ç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨è¯­éŸ³ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å—åˆ°å…³æ³¨ã€‚</li>
<li>ä¸»æµè¯­éŸ³åˆæˆç³»ç»Ÿå­˜åœ¨é«˜è®¡ç®—è´Ÿè½½é—®é¢˜ã€‚</li>
<li>LatentSpeechæ˜¯ä¸€ç§æ–°çš„æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>LatentSpeeché€šè¿‡é™ä½ç›®æ ‡ç»´åº¦ç®€åŒ–äº†TTSç¼–ç å™¨å’Œvocoderçš„å¤„ç†ã€‚</li>
<li>LatentSpeechå®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰æ¨¡å‹ï¼Œå…¶åœ¨è¯é”™è¯¯ç‡å’Œæ¢…å°”å€’è°±å¤±çœŸæ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
<li>LatentSpeechçš„å¼•å…¥å¢å¼ºäº†è¯­éŸ³ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats"><a href="#Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats" class="headerlink" title="Sampling from Boltzmann densities with physics informed low-rank formats"></a>Sampling from Boltzmann densities with physics informed low-rank formats</h2><p><strong>Authors:Paul Hagemann, Janina SchÃ¼tte, David Sommer, Martin Eigel, Gabriele Steidl</strong></p>
<p>Our method proposes the efficient generation of samples from an unnormalized Boltzmann density by solving the underlying continuity equation in the low-rank tensor train (TT) format. It is based on the annealing path commonly used in MCMC literature, which is given by the linear interpolation in the space of energies. Inspired by Sequential Monte Carlo, we alternate between deterministic time steps from the TT representation of the flow field and stochastic steps, which include Langevin and resampling steps. These adjust the relative weights of the different modes of the target distribution and anneal to the correct path distribution. We showcase the efficiency of our method on multiple numerical examples. </p>
<blockquote>
<p>æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è§£å†³ä½ç§©å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰æ ¼å¼ä¸­çš„åŸºç¡€è¿ç»­æ€§æ–¹ç¨‹ï¼Œæœ‰æ•ˆåœ°ä»æœªæ ‡å‡†åŒ–çš„ç»å°”å…¹æ›¼å¯†åº¦ä¸­ç”Ÿæˆæ ·æœ¬ã€‚å®ƒåŸºäºMCMCæ–‡çŒ®ä¸­å¸¸ç”¨çš„é€€ç«è·¯å¾„ï¼Œç”±èƒ½é‡ç©ºé—´ä¸­çš„çº¿æ€§æ’å€¼ç»™å‡ºã€‚å—åºè´¯è’™ç‰¹å¡ç½—çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨æµåœºçš„TTè¡¨ç¤ºçš„ç¡®å®šæ€§æ—¶é—´æ­¥é•¿å’ŒåŒ…æ‹¬æœ—ä¹‹ä¸‡å’Œé‡é‡‡æ ·æ­¥éª¤çš„éšæœºæ­¥éª¤ä¹‹é—´è¿›è¡Œäº¤æ›¿ã€‚è¿™äº›ä¼šè°ƒæ•´ç›®æ ‡åˆ†å¸ƒçš„å„æ¨¡æ€çš„ç›¸å¯¹æƒé‡ï¼Œå¹¶é€€ç«åˆ°æ­£ç¡®çš„è·¯å¾„åˆ†å¸ƒã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å€¼ç¤ºä¾‹ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07637v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä½ç§©å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰æ ¼å¼çš„æœªå½’ä¸€åŒ–ç»å°”å…¹æ›¼å¯†åº¦æ ·æœ¬é«˜æ•ˆç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºMCMCæ–‡çŒ®ä¸­å¸¸ç”¨çš„é€€ç«è·¯å¾„ï¼Œé€šè¿‡èƒ½é‡ç©ºé—´ä¸­çš„çº¿æ€§æ’å€¼ç»™å‡ºã€‚è¯¥æ–¹æ³•å—åˆ°åºè´¯è’™ç‰¹å¡ç½—çš„å¯å‘ï¼Œåœ¨æµåœºçš„TTè¡¨ç¤ºä¸­äº¤æ›¿è¿›è¡Œç¡®å®šæ€§æ—¶é—´æ­¥é•¿å’Œéšæœºæ­¥éª¤ï¼ŒåŒ…æ‹¬æœ—æ ¼æ–‡é‡é‡‡æ ·æ­¥éª¤ã€‚è¿™äº›æ­¥éª¤è°ƒæ•´ç›®æ ‡åˆ†å¸ƒçš„å„æ¨¡æ€ç›¸å¯¹æƒé‡ï¼Œå¹¶é€€ç«åˆ°æ­£ç¡®çš„è·¯å¾„åˆ†å¸ƒã€‚é€šè¿‡å¤šä¸ªæ•°å€¼ä¾‹å­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºä½ç§©å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰æ ¼å¼çš„é«˜æ•ˆç”Ÿæˆæœªå½’ä¸€åŒ–ç»å°”å…¹æ›¼å¯†åº¦æ ·æœ¬çš„æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åŸºäºé€€ç«è·¯å¾„ï¼Œé€šè¿‡èƒ½é‡ç©ºé—´ä¸­çš„çº¿æ€§æ’å€¼å®ç°ã€‚</li>
<li>å—åˆ°äº†åºè´¯è’™ç‰¹å¡ç½—çš„å¯å‘ï¼Œç»“åˆäº†ç¡®å®šæ€§æ—¶é—´æ­¥é•¿å’Œéšæœºæ­¥éª¤ã€‚</li>
<li>å…¶ä¸­åŒ…æ‹¬æœ—æ ¼æ–‡é‡é‡‡æ ·æ­¥éª¤ï¼Œç”¨äºè°ƒæ•´ç›®æ ‡åˆ†å¸ƒçš„å„æ¨¡æ€ç›¸å¯¹æƒé‡ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å¤šä¸ªæ•°å€¼ä¾‹å­éªŒè¯ï¼Œè¯æ˜äº†å…¶æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cbaf87fb6994743aa883b5db79f16f19.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c51d55d3f34411e1abfb2c58e4b59726.jpg" align="middle">
</details>




<h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹æ€§å¯¹äºé€‚åº”ç‰¹å®šçš„è¯­å¢ƒå’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿè‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œé‚£å°±æ˜¯å¯¹åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥çš„é‡è§†ç¨‹åº¦æ­£åœ¨ä¸æ–­å¢é•¿ï¼Œè¿™å¯èƒ½ä¸ºæ¸¸æˆå’Œè™šæ‹Ÿç°å®æä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå®ƒå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è°ƒæ•´åˆæˆçš„æ¢…å°”é¢‘è°±å›¾ä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ‰€æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸å½±å“è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œæ ‡å¿—ç€è¯­å¢ƒæ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> ç´¢å¼•è¯-è¯­éŸ³åˆæˆã€åœºæ™¯æç¤ºã€ç©ºé—´æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v2">PDF</a> The paper is missing some information</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å®ƒç»“åˆäº†è§†è§‰åœºæ™¯æç¤ºåˆ°åˆæˆæµç¨‹ä¸­ï¼Œæ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨äº†å›å£°åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯æ¥è°ƒæ•´åˆæˆçš„æ¢…å°”é¢‘è°±å›¾ï¼Œä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿å›å£°æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚è¯¥ç ”ç©¶å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸”ä¸æŸå®³è¯­éŸ³çš„è‡ªç„¶æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼ºè°ƒäº†æ§åˆ¶è¯­éŸ³åˆæˆé£æ ¼å’Œç‰¹ç‚¹çš„é‡è¦æ€§ï¼Œä»¥é€‚åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚ã€‚</li>
<li>ä»¥å‰çš„TTSç ”ç©¶ä¸»è¦å…³æ³¨æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ï¼Œä½†å¿½è§†äº†åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥ã€‚</li>
<li>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€TTSæ–¹æ³•ï¼ˆI2TTSï¼‰ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆæµç¨‹ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>I2TTSé‡‡ç”¨äº†ä¸€ç§å›å£°åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨å¢å¼ºæ²‰æµ¸å¼ä½“éªŒå¹¶ç¡®ä¿å›å£°æ¡ä»¶ä¸åœºæ™¯åŒ¹é…ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒI2TTSæ¨¡å‹èƒ½åœ¨ä¸æŸå®³è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­éŸ³åˆæˆé¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>é¡¹ç›®æ¼”ç¤ºé¡µé¢æä¾›äº†è¿›ä¸€æ­¥çš„äº†è§£å’Œæ¢ç´¢ã€‚<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">é“¾æ¥</a></li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-66e6ccdd517a50e9ee5dddf9637b47e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4689ec2cdbacd48202667ffe499ad66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cf7fa3b4564f4f1f3a720aedb6cba728.jpg" align="middle">
</details>




<h2 id="Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection"><a href="#Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection" class="headerlink" title="Mitigating Unauthorized Speech Synthesis for Voice Protection"></a>Mitigating Unauthorized Speech Synthesis for Voice Protection</h2><p><strong>Authors:Zhisheng Zhang, Qianyi Yang, Derui Wang, Pengyang Huang, Yuxin Cao, Kai Ye, Jie Hao</strong></p>
<p>With just a few speech samples, it is possible to perfectly replicate a speakerâ€™s voice in recent years, while malicious voice exploitation (e.g., telecom fraud for illegal financial gain) has brought huge hazards in our daily lives. Therefore, it is crucial to protect publicly accessible speech data that contains sensitive information, such as personal voiceprints. Most previous defense methods have focused on spoofing speaker verification systems in timbre similarity but the synthesized deepfake speech is still of high quality. In response to the rising hazards, we devise an effective, transferable, and robust proactive protection technology named Pivotal Objective Perturbation (POP) that applies imperceptible error-minimizing noises on original speech samples to prevent them from being effectively learned for text-to-speech (TTS) synthesis models so that high-quality deepfake speeches cannot be generated. We conduct extensive experiments on state-of-the-art (SOTA) TTS models utilizing objective and subjective metrics to comprehensively evaluate our proposed method. The experimental results demonstrate outstanding effectiveness and transferability across various models. Compared to the speech unclarity score of 21.94% from voice synthesizers trained on samples without protection, POP-protected samples significantly increase it to 127.31%. Moreover, our method shows robustness against noise reduction and data augmentation techniques, thereby greatly reducing potential hazards. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåªéœ€å°‘é‡çš„è¯­éŸ³æ ·æœ¬ï¼Œå°±èƒ½å¤Ÿå®Œç¾å¤åˆ¶ä¸€ä¸ªäººçš„å£°éŸ³ï¼Œè€Œæ¶æ„å£°éŸ³æ»¥ç”¨ï¼ˆä¾‹å¦‚ç”µä¿¡æ¬ºè¯ˆä»¥è·å–éæ³•ç»æµåˆ©ç›Šï¼‰åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­å¸¦æ¥äº†å·¨å¤§çš„å±å®³ã€‚å› æ­¤ï¼Œä¿æŠ¤åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ä¸ªäººå£°éŸ³ç‰¹å¾ï¼‰çš„å…¬å…±å¯è®¿é—®è¯­éŸ³æ•°æ®è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°å…ˆå‰çš„é˜²å¾¡æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¬ºéª—è¯­éŸ³éªŒè¯ç³»ç»Ÿçš„éŸ³è‰²ç›¸ä¼¼æ€§ä¸Šï¼Œä½†åˆæˆçš„æ·±åº¦ä¼ªé€ è¯­éŸ³ä»ç„¶å…·æœ‰å¾ˆé«˜çš„è´¨é‡ã€‚ä¸ºäº†åº”å¯¹æ—¥ç›Šä¸¥é‡çš„é£é™©ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆã€å¯è¿ç§»å’Œç¨³å¥çš„ä¸»åŠ¨ä¿æŠ¤æŠ€æœ¯ï¼Œç§°ä¸ºå…³é”®ç›®æ ‡æ‰°åŠ¨ï¼ˆPOPï¼‰ï¼Œå®ƒé€šè¿‡åœ¨åŸå§‹è¯­éŸ³æ ·æœ¬ä¸Šåº”ç”¨å‡ ä¹æ— æ³•å¯Ÿè§‰çš„é”™è¯¯æœ€å°åŒ–å™ªå£°ï¼Œé˜²æ­¢å®ƒä»¬è¢«æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ï¼Œä»è€Œæ— æ³•ç”Ÿæˆé«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ è¯­éŸ³ã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡çš„æœ€å…ˆè¿›TTSæ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå…¨é¢è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜åœ¨ä¸åŒæ¨¡å‹ä¸­çš„å“è¶Šæœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚ç›¸è¾ƒäºæ— ä¿æŠ¤æ ·æœ¬è®­ç»ƒçš„è¯­éŸ³åˆæˆå™¨21.94%çš„è¯­éŸ³æ¸…æ™°åº¦å¾—åˆ†ï¼ŒPOPä¿æŠ¤çš„æ ·æœ¬å°†å…¶æ˜¾è‘—æé«˜è‡³127.31%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºå¯¹æŠ—é™å™ªå’Œæ•°æ®å¢å¼ºæŠ€æœ¯çš„ç¨³å¥æ€§ï¼Œä»è€Œæå¤§åœ°é™ä½äº†æ½œåœ¨é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20742v1">PDF</a> Accepted to ACM CCS Workshop (LAMPS) 2024</p>
<p><strong>æ‘˜è¦</strong><br>è¿‘å‡ å¹´é€šè¿‡æå°‘é‡çš„è¯­éŸ³æ ·æœ¬ä¾¿èƒ½å®Œç¾æ¨¡ä»¿è¯´è¯äººçš„å£°éŸ³ï¼ŒåŒæ—¶æ¶æ„è¯­éŸ³æ»¥ç”¨ï¼ˆä¾‹å¦‚ç”µä¿¡è¯ˆéª—ä»¥è·å–éæ³•ç»æµåˆ©ç›Šï¼‰ç»™æ—¥å¸¸ç”Ÿæ´»å¸¦æ¥äº†å·¨å¤§çš„å±å®³ã€‚å› æ­¤ï¼Œä¿æŠ¤å«æœ‰æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ä¸ªäººå£°çº¹ï¼‰çš„å…¬å¼€è¯­éŸ³æ•°æ®è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¾§é‡äºåœ¨éŸ³è‰²ç›¸ä¼¼æ€§ä¸Šæ¬ºéª—è¯´è¯è€…éªŒè¯ç³»ç»Ÿï¼Œä½†åˆæˆçš„æ·±åº¦ä¼ªé€ è¯­éŸ³ä»ç„¶å…·æœ‰å¾ˆé«˜çš„è´¨é‡ã€‚ä¸ºäº†åº”å¯¹æ—¥ç›Šå¢é•¿çš„å¨èƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆã€å¯è¿ç§»ä¸”ç¨³å¥çš„ä¸»åŠ¨ä¿æŠ¤æŠ€æœ¯â€”â€”å…³é”®ç›®æ ‡æ‰°åŠ¨ï¼ˆPOPï¼‰ï¼Œå®ƒé€šè¿‡å‘åŸå§‹è¯­éŸ³æ ·æœ¬åº”ç”¨å‡ ä¹æ— æ³•å¯Ÿè§‰çš„æœ€å°è¯¯å·®å™ªå£°ï¼Œé˜²æ­¢å®ƒä»¬è¢«ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹çš„æœ‰æ•ˆå­¦ä¹ ï¼Œä»è€Œé˜²æ­¢ç”Ÿæˆé«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ è¯­éŸ³ã€‚æˆ‘ä»¬åœ¨æœ€å…ˆè¿›çš„TTSæ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œé‡‡ç”¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡å…¨é¢è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§æ¨¡å‹ä¸­çš„å‡ºè‰²æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚ä¸æœªç»ä¿æŠ¤çš„æ ·æœ¬ç›¸æ¯”ï¼Œå—ä¿æŠ¤çš„æ ·æœ¬ä½¿è¯­éŸ³æ¸…æ™°åº¦å¾—åˆ†ä»åˆæˆå™¨è®­ç»ƒçš„è¯­éŸ³æ¸…æ™°åº¦å¾—åˆ†çš„ä½åˆ†æå‡åˆ°äº†ä¸€ä¸ªæ›´é«˜çš„å¾—åˆ†æ°´å¹³ï¼ˆPOPä¿æŠ¤çš„æ ·æœ¬ç›¸è¾ƒäº21.94%æ˜¾è‘—æé«˜åˆ°æé«˜åˆ°äº†æ— é˜²æŠ¤æ°´å¹³è¾¾åˆ°çš„å®Œæ•´è¯„ä»·æŒ‡æ ‡ä»¥ä¸Šè‡³ç”šè‡³è¶…å‡ºè¶…å‡ºä¸€å®šçš„åŸºæœ¬è®­ç»ƒçŠ¶æ€æé«˜è‡³æ°´å¹³å€¼å³çš„æŒ‡æ•°æ°´å¹³è¾¾é«˜è‡³åˆ†æ•°æé«˜åˆ°å¾—åˆ†æå‡è¾¾åˆ°äº†æŒ‡æ•°çº§åˆ«å¢åŠ åˆ°äº†ç›¸è¾ƒäºå£°ç å™¨ä»¥æ— æ¡ä»¶å»ºæ¨¡éšæœºè‡ªç„¶ç”Ÿæˆçš„å¹³å‡æ°´å¹³çš„è‡³å°‘è‡³å£°å­¦æ„ŸçŸ¥è¯„çº§æå‡äº†ä¸¤å€å¤šå€ï¼ˆç”±å£°éŸ³æ¸…æ™°åº¦åˆ†æ•°è¡¨ç¤ºï¼‰å¹¶ä¸”èƒ½å¤Ÿåœ¨å¯¹æŠ—å™ªå£°è¿˜åŸå’Œæ•°æ®å¢å¼ºæŠ€æœ¯æ—¶æ˜¾ç¤ºå‡ºç¨³å¥æ€§ã€‚è¿™æå¤§åœ°å‡å°‘äº†æ½œåœ¨çš„é£é™©ã€‚æ€»çš„æ¥è¯´æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆçš„é˜²å¾¡æ‰‹æ®µä¿æŠ¤å…¬å¼€è¯­éŸ³æ•°æ®å…å—æ”»å‡»è€…çš„åˆ©ç”¨ä»è€Œæœ‰æ•ˆæŠµå¾¡è¯­éŸ³è¯ˆéª—ç­‰å¨èƒç»´æŠ¤ä¿¡æ¯å®‰å…¨å’Œä¸ªäººéšç§ã€‚è¿™é¡¹æŠ€æœ¯çš„æå‡ºæ— ç–‘å¯¹é˜²èŒƒæœªæ¥çš„æ¶æ„æ”»å‡»å…·æœ‰å·¨å¤§çš„æ½œåœ¨ä»·å€¼ç‰¹åˆ«æ˜¯åœ¨é˜²æ­¢ä½¿ç”¨TTSåˆæˆæŠ€æœ¯è¿›è¡Œè¯ˆéª—ç­‰æ–¹é¢æœ‰ç€ä¸¾è¶³è½»é‡çš„æ„ä¹‰å€¼å¾—æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¸å…³æ³¨ä¸ºä¿¡æ¯å®‰å…¨é˜²æŠ¤è´¡çŒ®åŠ›é‡æä¾›äº†å®è´µçš„å€Ÿé‰´ã€‚å¦å¤–è¯¥æŠ€æœ¯è¿˜å¯å¹¿æ³›åº”ç”¨äºè¯­éŸ³è¯†åˆ«è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¿›ä¸€æ­¥æ‹“å±•å…¶åº”ç”¨åœºæ™¯å…·æœ‰é‡è¦çš„ç°å®æ„ä¹‰å’Œå®ç”¨ä»·å€¼ã€‚è¯¥è®ºæ–‡ä¸ºæ„å»ºæ›´åŠ å®‰å…¨å¯é çš„æ™ºèƒ½è¯­éŸ³ç”Ÿæ€ç³»ç»Ÿæä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒä¹Ÿä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®å’Œå¯ç¤ºä»·å€¼å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„é‡è¦ä»·å€¼ã€‚<strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ä»…ä½¿ç”¨å°‘é‡è¯­éŸ³æ ·æœ¬å³å¯å®Œç¾å¤åˆ¶è¯´è¯äººçš„å£°éŸ³ï¼Œè¿™å¼•å‘äº†æ¶æ„è¯­éŸ³æ»¥ç”¨çš„é£é™©ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºPOPçš„æœ‰æ•ˆã€å¯è¿ç§»å’Œç¨³å¥çš„ä¸»åŠ¨ä¿æŠ¤æŠ€æœ¯ï¼Œé€šè¿‡å‘åŸå§‹è¯­éŸ³æ ·æœ¬æ·»åŠ å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å™ªå£°æ¥ä¿æŠ¤å®ƒä»¬å…å—æ”»å‡»è€…çš„åˆ©ç”¨ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPOPæ–¹æ³•åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„TTSæ¨¡å‹ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚</li>
<li>POPæŠ€æœ¯æ˜¾è‘—æé«˜äº†è¯­éŸ³æ¸…æ™°åº¦å¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºå¯¹æŠ—å™ªå£°è¿˜åŸå’Œæ•°æ®å¢å¼ºæŠ€æœ¯çš„ç¨³å¥æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9cc75591dbb4786702c6ed9b92008756.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c50bf08a392d1274b1e3ff5afe161f71.jpg" align="middle">
</details>




<h2 id="Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation"><a href="#Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation" class="headerlink" title="Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation"></a>Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation</h2><p><strong>Authors:Maohao Shen, Shun Zhang, Jilong Wu, Zhiping Xiu, Ehab AlBadawy, Yiting Lu, Mike Seltzer, Qing He</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llamaâ€™s competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œå¯åœ¨å¤šç§åŸºäºæ–‡æœ¬çš„ä»»åŠ¡ä¸Šå®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºæ–‡æœ¬çš„LLMæ‰©å±•åˆ°è¯­éŸ³ç”Ÿæˆä»»åŠ¡ä»å¤„äºæ¢ç´¢é˜¶æ®µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±ç²¾ç»†è°ƒæ•´çš„Llamaæ¨¡å‹é©±åŠ¨çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œåä¸ºTTS-Llamaï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„è¯­éŸ³åˆæˆæ€§èƒ½ã€‚åŸºäºTTS-Llamaï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†MoLE-Llamaï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬å’Œè¯­éŸ³å¤šæ¨¡æ€LLMï¼Œé€šè¿‡çº¯ç²¹çš„åæœŸèåˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å’Œæ··åˆä¸“å®¶æ¶æ„å¼€å‘è€Œæˆã€‚å¹¿æ³›çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒMoLE-Llamaåœ¨çº¯æ–‡æœ¬é—®ç­”ï¼ˆQAï¼‰å’ŒTTSä»»åŠ¡ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œå‡è½»äº†ä»»ä¸€æ¨¡æ€ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬åœ¨è¯­éŸ³å¤–é—®ç­”ä»»åŠ¡ä¸­è¿›ä¸€æ­¥æ¢ç´¢äº†MoLE-Llamaï¼Œè¯æ˜äº†å…¶ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿåœ¨è¿›è¡Œè¯­éŸ³ç”Ÿæˆçš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„çªç ´æ€§è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºç²¾ç»†è°ƒæ•´çš„Llamaæ¨¡å‹çš„TTSç³»ç»Ÿï¼Œåä¸ºTTS-Llamaï¼Œè¯¥ç³»ç»Ÿå®ç°äº†å…ˆè¿›çš„è¯­éŸ³åˆæˆæ€§èƒ½ã€‚åœ¨TTS-Llamaçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†MoLE-Llamaï¼Œè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬å’Œè¯­éŸ³å¤šæ¨¡æ€LLMï¼Œé€šè¿‡çº¯ç²¹çš„åæœŸèåˆå‚æ•°é«˜æ•ˆç²¾ç»†è°ƒæ•´ï¼ˆPEFTï¼‰å’Œæ··åˆä¸“å®¶æ¶æ„å¼€å‘ã€‚MoLE-Llamaåœ¨çº¯æ–‡æœ¬é—®ç­”ï¼ˆQAï¼‰å’ŒTTSä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå‡è½»äº†ä»»ä¸€æ¨¡æ€çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æœ€åï¼Œåœ¨æ–‡æœ¬å…¥è¯­éŸ³å‡ºé—®ç­”ä»»åŠ¡ä¸­è¿›ä¸€æ­¥æ¢ç´¢äº†MoLE-Llamaçš„æ½œåŠ›ï¼Œè¯æ˜äº†å…¶ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿåœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸè¡¨ç°çªå‡ºï¼Œå°¤å…¶åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†åŸºäºLlamaæ¨¡å‹çš„TTSç³»ç»ŸTTS-Llamaï¼Œå®ç°äº†å…ˆè¿›çš„è¯­éŸ³åˆæˆæ€§èƒ½ã€‚</li>
<li>åœ¨TTS-LlamaåŸºç¡€ä¸Šè¿›ä¸€æ­¥å¼€å‘äº†MoLE-Llamaï¼Œä¸€ä¸ªæ–‡æœ¬å’Œè¯­éŸ³å¤šæ¨¡æ€LLMã€‚</li>
<li>MoLE-Llamaé€šè¿‡å‚æ•°é«˜æ•ˆç²¾ç»†è°ƒæ•´å’Œæ··åˆä¸“å®¶æ¶æ„å®ç°ä¼˜ç§€æ€§èƒ½ã€‚</li>
<li>MoLE-Llamaåœ¨çº¯æ–‡æœ¬é—®ç­”å’ŒTTSä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>MoLE-Llamaåœ¨æ–‡æœ¬å…¥è¯­éŸ³å‡ºé—®ç­”ä»»åŠ¡ä¸­å±•ç¤ºäº†å·¨å¤§æ½œåŠ›ï¼Œå¯ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†MoLE-Llamaåœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ab288d5e910dba80ad1170150f3378af.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d1d6f26c2831e3dbd99fcf1e01e379ac.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-154078eb091a83b8bcaebdf9c65b4d09.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-18398ad78f9636c12358fd611c1222b8.jpg" align="middle">
</details>




<h2 id="Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis"><a href="#Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis" class="headerlink" title="Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis"></a>Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis</h2><p><strong>Authors:Suparna De, Ionut Bostan, Nishanth Sastry</strong></p>
<p>Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakersâ€™ emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶æ¦‚è¿°äº†ç›²äººæˆ–è§†éšœã€ä½å­¦å†äººç¾¤åœ¨ä½¿ç”¨ç¤¾äº¤ç½‘ç»œæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°½ç®¡å­˜åœ¨å•è°ƒæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å±å¹•é˜…è¯»å™¨ç­‰è¾…åŠ©æŠ€æœ¯ä»¥åŠè¡¨æƒ…ç¬¦å·ç­‰è§†è§‰å…ƒç´ çš„éŸ³é¢‘æè¿°ç­‰ä¾¿åˆ©æŠ€æœ¯ã€‚æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆä¼ ç»Ÿä¸Šä¾èµ–äºäººç±»è¾“å…¥çš„é¢„æœŸæƒ…æ„Ÿä»¥åŠè¦åˆæˆçš„æ–‡æœ¬ï¼Œè¿˜é¢ä¸´æ•°æ®ç®€åŒ–ï¼ˆå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼‰å’ŒæŒç»­æ—¶é—´ä¸å‡†ç¡®ç­‰é¢å¤–æŒ‘æˆ˜ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æƒ…æ„Ÿçš„è¡¨ç°ã€‚åœ¨çœŸå®é€šä¿¡ä¸­ï¼ŒéŸ³ç´ çš„æŒç»­æ—¶é—´å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºåŒä¸€å¥è¯çš„å‘éŸ³æ–¹å¼å¯èƒ½ä¼šå› è¯´è¯è€…çš„æƒ…æ„ŸçŠ¶æ€æˆ–å£éŸ³è€Œå¼‚ï¼ˆè¿™è¢«ç§°ä¸ºæ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆçš„ä¸€åˆ°å¤šé—®é¢˜ï¼‰ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªå…ˆè¿›çš„è¯­éŸ³åˆæˆç³»ç»Ÿæ¥åº”å¯¹è¿™ç§ä¸å¯é¢„æµ‹æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä»æ–‡æœ¬è¾“å…¥ä¸­æ¨æ–­å‡ºè¡¨è¾¾çš„æƒ…æ„Ÿï¼ŒåˆæˆéŸ³é¢‘ä¾§é‡äºæƒ…æ„Ÿå’Œè¯´è¯äººç‰¹å¾ä»¥å®ç°è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ï¼Œé›†æˆäº†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯ç”¨äºå®æ—¶åº”ç”¨ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸æœ€æ–°TTSæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œè¿˜å±•ç¤ºäº†å…·æœ‰ç«äº‰åŠ›çš„æ¨ç†æ—¶é—´æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºé€‚åˆå®æ—¶è¾…åŠ©åº”ç”¨ç¨‹åºçš„ç†æƒ³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code> æœ€è¿‘ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æœ‰å•è°ƒæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å±å¹•é˜…è¯»å™¨å’ŒéŸ³é¢‘å™è¿°ç­‰è¾…åŠ©æŠ€æœ¯ï¼Œä½†ç›²äººæˆ–è§†è§‰éšœç¢è€…ä»¥åŠä½æ–‡åŒ–ç¨‹åº¦çš„äººåœ¨ç¤¾äº¤ç½‘ç»œä¸Šä»é¢ä¸´äº¤äº’æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆä¾èµ–äºé¢„æœŸçš„æ–‡æœ¬è¾“å…¥ï¼Œä½†åœ¨ç®€åŒ–æ•°æ®å’ŒæŒç»­æ—¶é—´ä¸å‡†ç¡®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æƒ…æ„Ÿçš„æ¸²æŸ“èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­å¢ƒæ„ŸçŸ¥æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»æ–‡æœ¬è¾“å…¥ä¸­æ¨æ–­å‡ºè¡¨è¾¾çš„æƒ…æ„Ÿï¼Œå¹¶åˆæˆéŸ³é¢‘ä»¥çªå‡ºæƒ…æ„Ÿå’Œè¯´è¯äººçš„ç‰¹å¾ï¼Œå®ç°è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚æ­¤å¤–ï¼Œä¸æœ€æ–°çš„TTSæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨æ¨ç†æ—¶é—´æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚åˆç”¨äºå®æ—¶è®¿é—®æ€§åº”ç”¨ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›²äººæˆ–è§†è§‰éšœç¢è€…ä»¥åŠä½æ–‡åŒ–ç¨‹åº¦çš„äººåœ¨ç¤¾äº¤ç½‘ç»œä¸Šä»ç„¶é¢ä¸´äº¤äº’æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆä¾èµ–äºé¢„æœŸçš„æ–‡æœ¬è¾“å…¥å’Œé¢„æœŸçš„è¯­éŸ³åˆæˆã€‚</li>
<li>æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆé¢ä¸´ç®€åŒ–æ•°æ®å’ŒæŒç»­æ—¶é—´ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå¯¼è‡´ç¼ºä¹æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­å¢ƒæ„ŸçŸ¥æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç³»ç»Ÿï¼Œä»æ–‡æœ¬è¾“å…¥ä¸­æ¨æ–­æƒ…æ„Ÿå¹¶åˆæˆéŸ³é¢‘ä»¥çªå‡ºæƒ…æ„Ÿå’Œè¯´è¯äººçš„ç‰¹å¾ã€‚</li>
<li>è¯¥ç³»ç»Ÿç»“åˆäº†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œå®ç°è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚</li>
<li>ç³»ç»Ÿå…·æœ‰é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ€§èƒ½ï¼Œé€‚åˆç”¨äºå®æ—¶åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-954c4baed564d5fc17bf475672a0c733.jpg" align="middle">
</details>




<h2 id="Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model"><a href="#Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model" class="headerlink" title="Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model"></a>Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model</h2><p><strong>Authors:Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, Yike Guo, Wei Xue</strong></p>
<p>Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: <a target="_blank" rel="noopener" href="https://x-codec-audio.github.io/">https://x-codec-audio.github.io</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zhenye234/xcodec">https://github.com/zhenye234/xcodec</a>) </p>
<blockquote>
<p>æœ€è¿‘éŸ³é¢‘ç”Ÿæˆé¢†åŸŸçš„è¿›å±•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚å½“å‰å…³äºéŸ³é¢‘LLMçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¢å¼ºéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¶æ„å’Œè§„æ¨¡ï¼Œåˆ©ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼Œä»¥åŠé€šå¸¸ä½¿ç”¨éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆå¦‚EnCodecï¼‰è¿›è¡ŒéŸ³é¢‘æ ‡è®°ã€‚ç„¶è€Œï¼Œè¿™äº›ç¼–è§£ç å™¨æœ€åˆæ˜¯ä¸ºéŸ³é¢‘å‹ç¼©è€Œè®¾è®¡çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨éŸ³é¢‘LLMçš„ä¸Šä¸‹æ–‡ä¸­æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰éŸ³é¢‘LLMç¼–è§£ç å™¨çš„ç¼ºç‚¹ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨ç»´æŒç”ŸæˆéŸ³é¢‘çš„è¯­ä¹‰å®Œæ•´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œç°æœ‰çš„æ–¹æ³•å¦‚VALL-Eï¼Œæ ¹æ®æ–‡æœ¬è½¬å½•æ¥ç”Ÿæˆå£°å­¦æ ‡è®°ï¼Œä½†ç”±äºå£°å­¦æ ‡è®°çš„è¯­ä¹‰è¯¯è§£ï¼Œç»å¸¸å‡ºç°å†…å®¹ä¸å‡†ç¡®å’Œè¾ƒé«˜çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¯¼è‡´å•è¯è·³è¿‡å’Œé”™è¯¯ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºX-Codecã€‚X-Codecåœ¨æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰é˜¶æ®µä¹‹å‰èå…¥äº†é¢„è®­ç»ƒè¯­ä¹‰ç¼–ç å™¨çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åœ¨RVQä¹‹åå¼•å…¥äº†è¯­ä¹‰é‡å»ºæŸå¤±ã€‚é€šè¿‡å¢å¼ºç¼–è§£ç å™¨çš„è¯­ä¹‰èƒ½åŠ›ï¼ŒX-Codecåœ¨è¯­éŸ³åˆæˆä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼Œå¹¶å°†è¿™äº›å¥½å¤„æ‰©å±•åˆ°äº†éè¯­éŸ³åº”ç”¨ï¼ŒåŒ…æ‹¬éŸ³ä¹å’Œå£°éŸ³ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ã€éŸ³ä¹å»¶ç»­å’Œæ–‡æœ¬åˆ°å£°éŸ³çš„ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œæ•´åˆè¯­ä¹‰ä¿¡æ¯æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä¸­çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºï¼ˆDemo: <a target="_blank" rel="noopener" href="https://x-codec-audio.github.io/">https://x-codec-audio.github.io</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zhenye234/xcodec%EF%BC%89%E5%8F%AF%E4%BE%9B%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/zhenye234/xcodecï¼‰å¯ä¾›ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17175v3">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éŸ³é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰éŸ³é¢‘LLMç¼–ç åœ¨è¯­ä¹‰å®Œæ•´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç ”ç©¶æå‡ºX-Codecï¼Œé€šè¿‡å¼•å…¥é¢„è®­ç»ƒè¯­ä¹‰ç¼–ç å™¨å¹¶å¼•å…¥è¯­ä¹‰é‡å»ºæŸå¤±ï¼Œæé«˜ç¼–ç å™¨çš„è¯­ä¹‰èƒ½åŠ›ï¼Œæ˜¾è‘—å‡å°‘è¯­éŸ³åˆæˆä»»åŠ¡çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¹¶æ‰©å±•è¿™äº›ä¼˜åŠ¿è‡³éè¯­éŸ³åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨äº†éŸ³é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ã€‚</li>
<li>å½“å‰éŸ³é¢‘LLMç¼–ç åœ¨ç»´æŒè¯­ä¹‰å®Œæ•´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>X-Codecæ—¨åœ¨è§£å†³ç°æœ‰éŸ³é¢‘LLMç¼–ç çš„çŸ­æ¿ã€‚</li>
<li>X-Codecé€šè¿‡å¼•å…¥é¢„è®­ç»ƒè¯­ä¹‰ç¼–ç å™¨å’Œè¯­ä¹‰é‡å»ºæŸå¤±æé«˜ç¼–ç å™¨æ€§èƒ½ã€‚</li>
<li>X-Codecæ˜¾è‘—å‡å°‘è¯­éŸ³åˆæˆä»»åŠ¡çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>X-Codecçš„ä¼˜åŠ¿ä¸ä»…é™äºè¯­éŸ³åº”ç”¨ï¼Œä¹Ÿé€‚ç”¨äºéŸ³ä¹å’Œå£°æ•ˆç”Ÿæˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-788cc4d6126dce014156c652e21e827a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8188aaf4247a31c6edf4e05d85eafb07.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-beeaa767369c1a338c22f952cb659424.jpg" align="middle">
</details>




<h2 id="Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning"><a href="#Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning" class="headerlink" title="Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning"></a>Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning</h2><p><strong>Authors:Shuai Wang, Zhengyang Chen, Kong Aik Lee, Yanmin Qian, Haizhou Li</strong></p>
<p>Speaker individuality information is among the most critical elements within speech signals. By thoroughly and accurately modeling this information, it can be utilized in various intelligent speech applications, such as speaker recognition, speaker diarization, speech synthesis, and target speaker extraction. In this overview, we present a comprehensive review of neural approaches to speaker representation learning from both theoretical and practical perspectives. Theoretically, we discuss speaker encoders ranging from supervised to self-supervised learning algorithms, standalone models to large pretrained models, pure speaker embedding learning to joint optimization with downstream tasks, and efforts toward interpretability. Practically, we systematically examine approaches for robustness and effectiveness, introduce and compare various open-source toolkits in the field. Through the systematic and comprehensive review of the relevant literature, research activities, and resources, we provide a clear reference for researchers in the speaker characterization and modeling field, as well as for those who wish to apply speaker modeling techniques to specific downstream tasks. </p>
<blockquote>
<p>è¯´è¯äººçš„ä¸ªæ€§ä¿¡æ¯æ˜¯è¯­éŸ³ä¿¡å·ä¸­æœ€å…³é”®çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚é€šè¿‡å…¨é¢å‡†ç¡®åœ°å¯¹æ­¤ç±»ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ï¼Œå¯ä»¥å°†å…¶åº”ç”¨äºå„ç§æ™ºèƒ½è¯­éŸ³åº”ç”¨ä¸­ï¼Œä¾‹å¦‚è¯´è¯äººè¯†åˆ«ã€è¯´è¯äººæ—¥è®°åŒ–ã€è¯­éŸ³åˆæˆå’Œç›®æ ‡è¯´è¯äººæå–ç­‰ã€‚åœ¨æœ¬æ¬¡æ¦‚è¿°ä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºå’Œå®è·µä¸¤ä¸ªè§’åº¦å¯¹ç¥ç»æ–¹æ³•è¿›è¡Œå…¨é¢çš„è¯´è¯äººè¡¨å¾å­¦ä¹ ç»¼è¿°ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è®¨è®ºäº†ä»ç›‘ç£åˆ°è‡ªæˆ‘ç›‘ç£å­¦ä¹ ç®—æ³•çš„å„ç§è¯´è¯äººç¼–ç å™¨ï¼Œä»ç‹¬ç«‹æ¨¡å‹åˆ°å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»çº¯è¯´è¯äººåµŒå…¥å­¦ä¹ åˆ°ä¸ä¸‹æ¸¸ä»»åŠ¡çš„è”åˆä¼˜åŒ–ï¼Œä»¥åŠå‘è§£é‡Šæ€§çš„åŠªåŠ›ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ£€éªŒäº†è¿™äº›æ–¹æ³•çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä»‹ç»äº†è¯¥é¢†åŸŸçš„å„ç§å¼€æºå·¥å…·åŒ…å¹¶è¿›è¡Œäº†æ¯”è¾ƒã€‚é€šè¿‡å¯¹ç›¸å…³æ–‡çŒ®ã€ç ”ç©¶æ´»åŠ¨å’Œèµ„æºçš„ç³»ç»Ÿå’Œå…¨é¢çš„å›é¡¾ï¼Œæˆ‘ä»¬ä¸ºè¯­éŸ³ç‰¹å¾è¡¨å¾å’Œå»ºæ¨¡é¢†åŸŸçš„ç ”ç©¶äººå‘˜ä»¥åŠå¸Œæœ›å°†è¯´è¯äººå»ºæ¨¡æŠ€æœ¯åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„äººå‘˜æä¾›äº†æ¸…æ™°çš„å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15188v2">PDF</a> Accepted to TASLP</p>
<p><strong>Summary</strong><br>è¯­éŸ³ä¿¡å·ä¸­ï¼Œè¯´è¯äººçš„ä¸ªæ€§åŒ–ä¿¡æ¯æ˜¯æœ€å…³é”®è¦ç´ ä¹‹ä¸€ã€‚é€šè¿‡å…¨é¢å‡†ç¡®åœ°å»ºæ¨¡è¿™ä¸€ä¿¡æ¯ï¼Œå¯ä»¥å°†å…¶åº”ç”¨äºå„ç§æ™ºèƒ½è¯­éŸ³åº”ç”¨ä¸­ï¼Œå¦‚è¯´è¯äººè¯†åˆ«ã€è¯´è¯äººæ—¥è®°åŒ–ã€è¯­éŸ³åˆæˆå’Œç›®æ ‡è¯´è¯äººæå–ç­‰ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†ä»ç†è®ºå’Œå®ç”¨è§’åº¦å¯¹è¯´è¯äººè¡¨å¾å­¦ä¹ çš„ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è®¨è®ºäº†ä»ç›‘ç£åˆ°è‡ªæˆ‘ç›‘ç£çš„å­¦ä¹ ç®—æ³•ã€ç‹¬ç«‹æ¨¡å‹åˆ°å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ã€çº¯è¯´è¯äººåµŒå…¥å­¦ä¹ åˆ°ä¸ä¸‹æ¸¸ä»»åŠ¡çš„è”åˆä¼˜åŒ–ç­‰çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œå¹¶æœç€å¯è§£é‡Šæ€§çš„æ–¹å‘åšå‡ºåŠªåŠ›ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ£€éªŒäº†é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä»‹ç»å¹¶æ¯”è¾ƒäº†è¯¥é¢†åŸŸçš„å„ç§å¼€æºå·¥å…·åŒ…ã€‚é€šè¿‡ç³»ç»Ÿåœ°å…¨é¢å›é¡¾ç›¸å…³æ–‡çŒ®ã€ç ”ç©¶æ´»åŠ¨å’Œèµ„æºï¼Œä¸ºè¯´è¯äººè¡¨å¾å’Œå»ºæ¨¡é¢†åŸŸçš„ç ”ç©¶äººå‘˜ä»¥åŠå¸Œæœ›å°†è¯´è¯äººå»ºæ¨¡æŠ€æœ¯åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„äººå‘˜æä¾›äº†æ¸…æ™°çš„å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººçš„ä¸ªæ€§åŒ–ä¿¡æ¯æ˜¯è¯­éŸ³ä¿¡å·ä¸­æœ€å…³é”®çš„éƒ¨åˆ†ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ–¹æ³•è¢«ç”¨äºå»ºæ¨¡è¯´è¯äººçš„è¡¨å¾ï¼Œå¹¶åº”ç”¨äºå¤šç§æ™ºèƒ½è¯­éŸ³åº”ç”¨ã€‚</li>
<li>ä»ç†è®ºè§’åº¦ï¼Œæ–‡ç« è®¨è®ºäº†å¤šç§æ¼”è®²è€…ç¼–ç å™¨ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œä»¥åŠçº¯è¯´è¯äººåµŒå…¥å­¦ä¹ ä¸ä¸‹æ¸¸ä»»åŠ¡çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>æ–‡ç« è¿˜ä»‹ç»äº†æœç€å¯è§£é‡Šæ€§çš„åŠªåŠ›ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯ç†è§£æ€§ã€‚</li>
<li>ä»å®ç”¨è§’åº¦ï¼Œæ–‡ç« ç³»ç»Ÿåœ°æ£€éªŒäº†æ–¹æ³•çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>æ–‡ç« æä¾›äº†å„ç§å¼€æºå·¥å…·åŒ…çš„ä»‹ç»å’Œæ¯”è¾ƒï¼Œä»¥æ–¹ä¾¿ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-865221afda42c7c4c88832efc9e0749b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f86e39c2559437ccd8459e78536e3049.jpg" align="middle">
</details>




<h2 id="TTSDS-â€“-Text-to-Speech-Distribution-Score"><a href="#TTSDS-â€“-Text-to-Speech-Distribution-Score" class="headerlink" title="TTSDS â€“ Text-to-Speech Distribution Score"></a>TTSDS â€“ Text-to-Speech Distribution Score</h2><p><strong>Authors:Christoph Minixhofer, OndÅ™ej Klejch, Peter Bell</strong></p>
<p>Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period. </p>
<blockquote>
<p>è¿‘å¹´æ¥å‡ºç‰ˆçš„è®¸å¤šæ–‡å­—è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆçš„éŸ³é¢‘æ¥è¿‘çœŸå®è¯­éŸ³ã€‚ç„¶è€Œï¼Œéšç€æ–°æ¶æ„ã€æ–¹æ³•å’Œæ•°æ®é›†çš„å‡ºç°ï¼Œéœ€è¦é‡æ–°å®¡è§†TTSçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥ç†è§£æ‰€è·å¾—çš„ç»“æœã€‚æˆ‘ä»¬æè®®ä»å¤šä¸ªå› ç´ ç»¼åˆè¯„ä¼°åˆæˆè¯­éŸ³çš„è´¨é‡ï¼Œå¦‚è¯­è°ƒã€è¯´è¯äººèº«ä»½å’Œæ¸…æ™°åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è·å–æ¯ä¸ªå› ç´ çš„å…³è”é¡¹ï¼Œå¹¶æµ‹é‡å®ƒä»¬ä¸çœŸå®è¯­éŸ³æ•°æ®é›†å’Œå™ªå£°æ•°æ®é›†çš„å·®è·æ¥è¯„ä¼°åˆæˆè¯­éŸ³ä¸çœŸå®è¯­éŸ³çš„åŒ¹é…ç¨‹åº¦ã€‚æˆ‘ä»¬å¯¹2008å¹´è‡³2024å¹´é—´å¼€å‘çš„35ä¸ªTTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„åˆ†æ•°ï¼ˆä½œä¸ºå„å› ç´ çš„æ— æƒé‡å¹³å‡å€¼ï¼‰ä¸æ¯ä¸ªæ—¶æœŸçš„äººå·¥è¯„ä»·ç»“æœé«˜åº¦ç›¸å…³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12707v3">PDF</a> SLT 2024</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæœ‰è®¸å¤šæ–°å‘å¸ƒçš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆçš„éŸ³é¢‘å·²æ¥è¿‘çœŸå®è¯­éŸ³ã€‚ä¸ºæ›´å¥½åœ°ç†è§£æ–°æ¶æ„ã€æ–¹æ³•å’Œæ•°æ®é›†æ‰€å¾—ç»“æœï¼Œéœ€è¦é‡æ–°å®¡è§†TTSçš„è¯„ä¼°æ–¹å¼ã€‚æœ¬æ–‡æè®®ä»è¯­è°ƒã€è¯´è¯äººèº«ä»½å’Œæ¸…æ™°åº¦ç­‰å¤šä¸ªå› ç´ æ¥è¯„ä¼°åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚é€šè¿‡è·å–æ¯ä¸ªå› ç´ çš„å…³è”å¹¶æµ‹é‡å…¶ä¸çœŸå®è¯­éŸ³æ•°æ®é›†å’Œå™ªå£°æ•°æ®é›†çš„å·®å¼‚æ¥è¯„ä¼°åˆæˆè¯­éŸ³å¯¹çœŸå®è¯­éŸ³çš„æ¨¡æ‹Ÿç¨‹åº¦ã€‚ä½œè€…å¯¹2008å¹´è‡³2024å¹´é—´å¼€å‘çš„35ä¸ªTTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘ç°æœ¬æ–‡è®¡ç®—çš„æ— æƒé‡å¹³å‡å› ç´ å¾—åˆ†ä¸å„ä¸ªæ—¶æœŸçš„äººç±»è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSç³»ç»Ÿç”Ÿæˆçš„éŸ³é¢‘å·²æ¥è¿‘çœŸå®è¯­éŸ³ï¼Œéœ€è¦é‡æ–°å®¡è§†å…¶è¯„ä¼°æ–¹å¼ã€‚</li>
<li>æå‡ºä»è¯­è°ƒã€è¯´è¯äººèº«ä»½å’Œæ¸…æ™°åº¦ç­‰å¤šä¸ªå› ç´ æ¥è¯„ä¼°åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚</li>
<li>é€šè¿‡æµ‹é‡ä¸çœŸå®è¯­éŸ³æ•°æ®é›†å’Œå™ªå£°æ•°æ®é›†çš„å·®å¼‚æ¥è¯„ä¼°åˆæˆè¯­éŸ³å¯¹çœŸå®è¯­éŸ³çš„æ¨¡æ‹Ÿç¨‹åº¦ã€‚</li>
<li>å¯¹å¤šä¸ªTTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å‘ç°æ— æƒé‡å¹³å‡å› ç´ å¾—åˆ†ä¸å„ä¸ªæ—¶æœŸçš„äººç±»è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</li>
<li>è¯¥è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿä¸ºTTSç³»ç»Ÿçš„è¿›æ­¥æä¾›æœ‰åŠ›çš„è¯„ä¼°å·¥å…·ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5d7824eb7f71b474fd46332370f442de.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20f7ffc0f4e39850306419bd41ad9476.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2f17fae8c43b1b2b890c4c7493d4ca4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-27bb06ebfa2bbd737eb3089e0df3d026.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89e6fbc1e8abf28dba60aa35c2760d78.jpg" align="middle">
</details>




<h2 id="Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers"><a href="#Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers" class="headerlink" title="Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers"></a>Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers</h2><p><strong>Authors:Krzysztof Choromanski, Arijit Sehanobish, Somnath Basu Roy Chowdhury, Han Lin, Avinava Dubey, Tamas Sarlos, Snigdha Chaturvedi</strong></p>
<p>We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular low displacement rank) for integrating tensor fields defined on weighted trees. Several applications of the resulting fast tree-field integrators (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) Topological Transformers (TTs) (Choromanski et al., 2022) for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as three extra learnable parameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains. Importantly, most of FTFIs are exact methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide 5.7-13x speedups. We also provide an extensive theoretical analysis of our methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç±»æ–°çš„å¿«é€Ÿå¤šé¡¹å¼å¯¹æ•°çº¿æ€§ç®—æ³•ï¼Œè¯¥ç®—æ³•åŸºäºç»“æ„çŸ©é˜µç†è®ºï¼ˆå°¤å…¶æ˜¯ä½ä½ç§»ç§©ï¼‰ï¼Œç”¨äºé›†æˆå®šä¹‰åœ¨åŠ æƒæ ‘ä¸Šçš„å¼ é‡åœºã€‚æ‰€å¾—åˆ°çš„å¿«é€Ÿæ ‘åœºç§¯åˆ†å™¨ï¼ˆFTFIsï¼‰çš„å‡ ä¸ªåº”ç”¨åŒ…æ‹¬ï¼šï¼ˆaï¼‰å›¾åº¦é‡å’Œæ ‘åº¦é‡çš„è¿‘ä¼¼ï¼Œï¼ˆbï¼‰å›¾åˆ†ç±»ï¼Œï¼ˆcï¼‰ç½‘æ ¼å»ºæ¨¡ï¼Œä»¥åŠæœ€åï¼ˆdï¼‰å›¾åƒæ‹“æ‰‘è½¬æ¢å™¨ï¼ˆTTsï¼‰ï¼ˆChoromanskiç­‰äººï¼Œ2022å¹´ï¼‰ã€‚å¯¹äºæ‹“æ‰‘è½¬æ¢å™¨ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰è¾ƒå°‘é¢å¤–å¯å­¦ä¹ å‚æ•°ï¼ˆæ¯ä¸ªè½¬æ¢å™¨å±‚ä»…æœ‰ä¸‰ä¸ªï¼‰çš„æ–°å‹ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRPEï¼‰å±è”½æœºåˆ¶ï¼Œè¿™å¯¼è‡´ç²¾åº¦æé«˜äº†1.0-1.5%ã€‚é‡è¦çš„æ˜¯ï¼Œå¤§å¤šæ•°FTFIsæ˜¯ç²¾ç¡®æ–¹æ³•ï¼Œå› æ­¤åœ¨æ•°å€¼ä¸Šä¸å®ƒä»¬çš„æš´åŠ›å¯¹åº”æ–¹æ³•ç›¸å½“ã€‚å½“åº”ç”¨äºå…·æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„å›¾æ—¶ï¼Œè¿™äº›ç²¾ç¡®ç®—æ³•æä¾›äº†5.7-13å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å¯¹æˆ‘ä»¬æ–¹æ³•çš„å¹¿æ³›ç†è®ºåˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15881v2">PDF</a> NeurIPS 2024</p>
<p><strong>æ‘˜è¦</strong><br>     åŸºäºç»“æ„åŒ–çŸ©é˜µç†è®ºï¼ˆç‰¹åˆ«æ˜¯ä½ä½ç§»ç§©ï¼‰æå‡ºäº†ä¸€ç±»æ–°çš„å¿«é€Ÿå¤šé¡¹å¼å¯¹æ•°çº¿æ€§ç®—æ³•ï¼Œç”¨äºæ•´åˆå®šä¹‰åœ¨åŠ æƒæ ‘ä¸Šçš„å¼ é‡åœºã€‚å¿«é€Ÿæ ‘åœºç§¯åˆ†å™¨ï¼ˆFTFIsï¼‰çš„å‡ ä¸ªåº”ç”¨åŒ…æ‹¬ï¼šï¼ˆaï¼‰å›¾åº¦é‡ä¸æ ‘åº¦é‡çš„è¿‘ä¼¼ï¼Œï¼ˆbï¼‰å›¾åˆ†ç±»ï¼Œï¼ˆcï¼‰ç½‘æ ¼å»ºæ¨¡ï¼Œä»¥åŠï¼ˆdï¼‰å›¾åƒæ‹“æ‰‘è½¬æ¢å™¨ï¼ˆTTsï¼‰ã€‚å¯¹äºæ‹“æ‰‘è½¬æ¢å™¨ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æ¯å±‚ä»…ä¸‰ä¸ªé¢å¤–å¯å­¦ä¹ å‚æ•°çš„æ–°å‹ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRPEï¼‰å±è”½æœºåˆ¶ï¼Œå¸¦æ¥äº†1.0-1.5%+çš„å‡†ç¡®ç‡æå‡ã€‚é‡è¦çš„æ˜¯ï¼Œå¤§å¤šæ•°FTFIsæ˜¯ç²¾ç¡®æ–¹æ³•ï¼Œå› æ­¤åœ¨æ•°å€¼ä¸Šç­‰åŒäºå…¶æš´åŠ›è®¡ç®—å¯¹åº”æ–¹æ³•ã€‚å½“åº”ç”¨äºå…·æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„å›¾æ—¶ï¼Œè¿™äº›ç²¾ç¡®ç®—æ³•æä¾›äº†5.7-13å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬è¿˜å¯¹æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„ç†è®ºåˆ†æã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºç»“æ„åŒ–çŸ©é˜µç†è®ºçš„æ–°å‹å¿«é€Ÿå¤šé¡¹å¼å¯¹æ•°çº¿æ€§ç®—æ³•ï¼Œç”¨äºåŠ æƒæ ‘ä¸Šçš„å¼ é‡åœºæ•´åˆã€‚</li>
<li>æå‡ºäº†å¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åº¦é‡è¿‘ä¼¼ã€å›¾åˆ†ç±»ã€ç½‘æ ¼å»ºæ¨¡ä»¥åŠå›¾åƒæ‹“æ‰‘è½¬æ¢å™¨ã€‚</li>
<li>å¯¹äºæ‹“æ‰‘è½¬æ¢å™¨ï¼Œå¼•å…¥äº†ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRPEï¼‰å±è”½æœºåˆ¶ï¼Œæé«˜äº†å‡†ç¡®ç‡ã€‚</li>
<li>å¤§å¤šæ•°FTFIsæ˜¯ç²¾ç¡®æ–¹æ³•ï¼Œå¯æä¾›æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
<li>æ–¹æ³•åœ¨ç†è®ºåˆ†æä¸­å¾—åˆ°äº†å¹¿æ³›éªŒè¯ã€‚</li>
<li>FTFIsåœ¨å¤„ç†å¤§è§„æ¨¡å›¾æ•°æ®æ—¶å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-631340e9e632a5f32dd7eabce23fea29.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3fc5affe036c5a7c010539a04991aed1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bad831467d9c313e96be11881ef19e14.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥çš„å„ç§å¨èƒï¼Œå¯¹åˆæˆè¯­éŸ³çš„å¯é è¿½è¸ªéœ€æ±‚æ„ˆå‘è¿«åˆ‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•æ˜¯åœ¨ç”ŸæˆéŸ³é¢‘åå•ç‹¬æ·»åŠ æ°´å°ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢å½±å“äº†è¯­éŸ³è´¨é‡ï¼Œåˆå½±å“äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨ç¨³å¥æ€§å’Œçµæ´»æ€§æ–¹é¢ä¹Ÿå­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TraceableSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹TTSæ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ï¼Œæé«˜äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¸§çº§æ°´å°å°åˆ¶å’Œæå–æŠ€æœ¯ï¼Œæé«˜äº†å¯¹æŠ—é‡æ–°æ‹¼æ¥æ”»å‡»çš„ç¨³å¥æ€§å’Œæ“ä½œçš„æ—¶æ•ˆæ€§çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨ä¸æ˜“å¯Ÿè§‰æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠµæŠ—é‡æ–°æ‹¼æ¥æ”»å‡»æ–¹é¢ï¼Œè¡¨ç°ä¼˜äºVALL-Eæˆ–HiFicodecå•ç‹¬ä½¿ç”¨WavMarkçš„å¼ºåŸºçº¿ã€‚å®ƒè¿˜å¯ä»¥åº”ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjzser/TraceableSpeechæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯è¿›æ­¥çš„å¨èƒåŠå…¶å¯é è¿½è¸ªçš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é‡‡å–åœ¨è¯­éŸ³ç”Ÿæˆåå•ç‹¬æ·»åŠ æ°´å°çš„æ–¹å¼ï¼Œè¿™ä¼šå½±å“è¯­éŸ³è´¨é‡å’Œæ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TraceableSpeechè¿™ä¸€æ–°å‹çš„TTSæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿç›´æ¥ç”Ÿæˆå¸¦æœ‰æ°´å°çš„è¯­éŸ³ï¼Œæé«˜äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†å¸§çº§åˆ«çš„æ°´å°å°åˆ»å’Œæå–æŠ€æœ¯ï¼Œæé«˜äº†å¯¹æŠ—é‡æ–°æ‹¼æ¥æ”»å‡»çš„é²æ£’æ€§å’Œæ“ä½œçš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨ä¸æ˜“å¯Ÿè§‰æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠµæŠ—é‡æ–°æ‹¼æ¥æ”»å‡»æ–¹é¢ä¼˜äºä½¿ç”¨WavMarkçš„VALL-Eæˆ–HiFicodecç­‰å¼ºå¤§åŸºçº¿æ¨¡å‹ï¼Œä¸”é€‚ç”¨äºå„ç§é•¿åº¦çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSæŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥äº†ä¸€ç³»åˆ—å¨èƒï¼Œéœ€è¦å¯é è¿½è¸ªåˆæˆè¯­éŸ³ã€‚</li>
<li>å½“å‰æ·»åŠ æ°´å°çš„æ–¹æ³•ä¼šå½±å“è¯­éŸ³è´¨é‡å’Œæ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚</li>
<li>TraceableSpeechæ˜¯ä¸€ç§æ–°å‹çš„TTSæ¨¡å‹ï¼Œèƒ½ç›´æ¥ç”Ÿæˆå¸¦æœ‰æ°´å°çš„è¯­éŸ³ã€‚</li>
<li>TraceableSpeechæé«˜äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚</li>
<li>é€šè¿‡å¸§çº§åˆ«çš„æ°´å°å°åˆ»å’Œæå–æŠ€æœ¯ï¼Œå®ç°äº†å¯¹æŠ—é‡æ–°æ‹¼æ¥æ”»å‡»çš„é²æ£’æ€§å’Œæ“ä½œçš„çµæ´»æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis"><a href="#Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis" class="headerlink" title="Style Mixture of Experts for Expressive Text-To-Speech Synthesis"></a>Style Mixture of Experts for Expressive Text-To-Speech Synthesis</h2><p><strong>Authors:Ahad Jawaid, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman</strong></p>
<p>Recent advances in style transfer text-to-speech (TTS) have improved the expressiveness of synthesized speech. However, encoding stylistic information (e.g., timbre, emotion, and prosody) from diverse and unseen reference speech remains a challenge. This paper introduces StyleMoE, an approach that addresses the issue of learning averaged style representations in the style encoder by creating style experts that learn from subsets of data. The proposed method replaces the style encoder in a TTS framework with a Mixture of Experts (MoE) layer. The style experts specialize by learning from subsets of reference speech routed to them by the gating network, enabling them to handle different aspects of the style space. As a result, StyleMoE improves the style coverage of the style encoder for style transfer TTS. Our experiments, both objective and subjective, demonstrate improved style transfer for diverse and unseen reference speech. The proposed method enhances the performance of existing state-of-the-art style transfer TTS models and represents the first study of style MoE in TTS. </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„é£æ ¼è¿ç§»æ–¹é¢æœ€è¿‘çš„è¿›å±•æé«˜äº†åˆæˆè¯­éŸ³çš„è¡¨ç°åŠ›ã€‚ç„¶è€Œï¼Œä»å¤šæ ·ä¸”æœªè§çš„å‚è€ƒè¯­éŸ³ä¸­ç¼–ç é£æ ¼ä¿¡æ¯ï¼ˆå¦‚éŸ³è´¨ã€æƒ…æ„Ÿå’Œè¯­è°ƒï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†StyleMoEï¼Œä¸€ç§é€šè¿‡åˆ›å»ºä»æ•°æ®å­é›†å­¦ä¹ çš„é£æ ¼ä¸“å®¶æ¥è§£å†³é£æ ¼ç¼–ç å™¨ä¸­å­¦ä¹ å¹³å‡é£æ ¼è¡¨ç¤ºé—®é¢˜çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å±‚æ›¿æ¢TTSæ¡†æ¶ä¸­çš„é£æ ¼ç¼–ç å™¨ã€‚é£æ ¼ä¸“å®¶é€šè¿‡ä»è·¯ç”±åˆ°å®ƒä»¬çš„å‚è€ƒè¯­éŸ³å­é›†è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†é£æ ¼ç©ºé—´çš„ä¸åŒæ–¹é¢ã€‚å› æ­¤ï¼ŒStyleMoEæé«˜äº†é£æ ¼ç¼–ç å™¨çš„é£æ ¼è¦†ç›–èƒ½åŠ›ï¼Œé€‚ç”¨äºé£æ ¼è¿ç§»TTSã€‚æˆ‘ä»¬çš„å®¢è§‚å’Œä¸»è§‚å®éªŒéƒ½è¯æ˜ï¼Œå¯¹äºå¤šæ ·ä¸”æœªè§çš„å‚è€ƒè¯­éŸ³ï¼Œé£æ ¼è¿ç§»å¾—åˆ°äº†æ”¹å–„ã€‚æ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†ç°æœ‰æœ€å…ˆè¿›çš„é£æ ¼è¿ç§»TTSæ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ä¸”æ˜¯TTSä¸­é£æ ¼MoEçš„é¦–æ¬¡ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03637v2">PDF</a> Published in Audio Imagination: NeurIPS 2024 Workshop</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¿‘æœŸæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„é£æ ¼è¿ç§»æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œæé«˜äº†åˆæˆè¯­éŸ³çš„è¡¨è¾¾æ€§ã€‚ç„¶è€Œï¼Œä»å¤šæ ·ä¸”æœªè§çš„å‚è€ƒè¯­éŸ³ä¸­ç¼–ç é£æ ¼ä¿¡æ¯ï¼ˆå¦‚éŸ³è‰²ã€æƒ…æ„Ÿå’Œè¯­è°ƒï¼‰ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†StyleMoEæ–¹æ³•ï¼Œé€šè¿‡åˆ›å»ºé£æ ¼ä¸“å®¶æ¥è§£å†³é£æ ¼ç¼–ç å™¨ä¸­å­¦ä¹ å¹³å‡é£æ ¼è¡¨ç¤ºçš„é—®é¢˜ã€‚é£æ ¼ä¸“å®¶é€šè¿‡ä»è·¯ç”±åˆ°å®ƒä»¬çš„å‚è€ƒè¯­éŸ³å­é›†å­¦ä¹ ï¼Œä»¥å¤„ç†é£æ ¼ç©ºé—´çš„ä¸åŒæ–¹é¢ã€‚å› æ­¤ï¼ŒStyleMoEæé«˜äº†é£æ ¼ç¼–ç å™¨çš„é£æ ¼è¦†ç›–èƒ½åŠ›ï¼Œæ”¹è¿›äº†TTSçš„é£æ ¼è¿ç§»ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼Œå¯¹äºå¤šæ ·ä¸”æœªè§çš„å‚è€ƒè¯­éŸ³ï¼ŒStyleMoEåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸Šéƒ½è¡¨ç°å‡ºæ”¹è¿›çš„é£æ ¼è¿ç§»æ•ˆæœã€‚è¯¥ç ”ç©¶æ˜¯TTSä¸­é¦–æ¬¡ç ”ç©¶é£æ ¼MoEã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>StyleMoEæ–¹æ³•è§£å†³äº†åœ¨TTSé£æ ¼è¿ç§»ä¸­ä»å¤šæ ·ä¸”æœªè§çš„å‚è€ƒè¯­éŸ³ç¼–ç é£æ ¼ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>StyleMoEé€šè¿‡åˆ›å»ºé£æ ¼ä¸“å®¶æ¥æ”¹è¿›é£æ ¼ç¼–ç å™¨ï¼Œè¿™äº›ä¸“å®¶é€šè¿‡ä»è·¯ç”±åˆ°å®ƒä»¬çš„å‚è€ƒè¯­éŸ³å­é›†å­¦ä¹ æ¥ä¸“ä¸šåŒ–å¤„ç†ä¸åŒçš„é£æ ¼æ–¹é¢ã€‚</li>
<li>StyleMoEæé«˜äº†é£æ ¼ç¼–ç å™¨çš„é£æ ¼è¦†ç›–èƒ½åŠ›ï¼Œä»è€Œæ”¹è¿›äº†TTSçš„é£æ ¼è¿ç§»æ•ˆæœã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒStyleMoEåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸Šéƒ½æ˜¾ç¤ºå‡ºæ”¹è¿›çš„é£æ ¼è¿ç§»æ•ˆæœï¼Œå¯¹äºå¤šæ ·ä¸”æœªè§çš„å‚è€ƒè¯­éŸ³å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>StyleMoEæ–¹æ³•å¢å¼ºäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„TTSé£æ ¼è¿ç§»æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡åœ¨TTSä¸­ç ”ç©¶ä½¿ç”¨é£æ ¼MoEçš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8071a305826f1eb16863c0591528d7a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6d6716de85f2a3d1185b60e764b04a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0f2d3c9d4d3c06e0b1f2bb0a2e078b5.jpg" align="middle">
</details>




<h2 id="DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models"><a href="#DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models" class="headerlink" title="DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models"></a>DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models</h2><p><strong>Authors:Jingyi Chen, Ju-Seung Byun, Micha Elsner, Andrew Perrault</strong></p>
<p>Recent advancements in generative models have sparked a significant interest within the machine learning community. Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech. Studies such as those by Lee et al. (2023), Black et al. (2023), Wang et al. (2023), and Fan et al. (2024) illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis. However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models. In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system (Saeki et al., 2022) as a proxy loss. We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model (Mittag et al., 2021) and human preference experiments for further evaluation. Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„å‘å±•åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…´è¶£ã€‚ç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè¯­éŸ³åˆæˆæ–¹é¢å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚Leeç­‰äººï¼ˆ2023ï¼‰ã€Blackç­‰äººï¼ˆ2023ï¼‰ã€Wangç­‰äººï¼ˆ2023ï¼‰å’ŒFanç­‰äººï¼ˆ2024ï¼‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯ä»¥å¢å¼ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡å‹ä¸è¯­éŸ³åˆæˆä¸­æ‰€ç”¨æ¨¡å‹çš„ç»“æ„å·®å¼‚ï¼Œå°šä¸æ¸…æ¥šRLHFæ˜¯å¦ä¹Ÿèƒ½åŒæ ·æœ‰ç›Šäºè¯­éŸ³åˆæˆæ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†RLHFå®é™…åº”ç”¨åˆ°åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­ï¼Œåˆ©ç”¨ç”±UTokyo-SaruLabçš„MOSé¢„æµ‹ç³»ç»Ÿï¼ˆSaekiç­‰äººï¼Œ2022ï¼‰é¢„æµ‹çš„å‡å€¼æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ä½œä¸ºä»£ç†æŸå¤±ã€‚æˆ‘ä»¬ä»‹ç»äº†æ‰©æ•£æ¨¡å‹æŸå¤±å¼•å¯¼çš„RLç­–ç•¥ä¼˜åŒ–ï¼ˆDLPOï¼‰ï¼Œå¹¶å°†å…¶ä¸å…¶ä»–RLHFæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œåˆ©ç”¨NISQAè¯­éŸ³è´¨é‡å’Œè‡ªç„¶åº¦è¯„ä¼°æ¨¡å‹ï¼ˆMittagç­‰äººï¼Œ2021ï¼‰ä»¥åŠäººç±»åå¥½å®éªŒè¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒRLHFå¯ä»¥å¢å¼ºåŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”DLPOèƒ½æ›´å¥½åœ°æ”¹å–„æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªç„¶å’Œé«˜è´¨é‡çš„è¯­éŸ³éŸ³é¢‘çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14632v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå›¾åƒå’Œè¯­éŸ³æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯è¿›ä¸€æ­¥æå‡å…¶å›¾åƒåˆæˆæ•ˆæœã€‚æœ¬æ–‡æ¢ç´¢äº†RLHFåœ¨åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„å®é™…åº”ç”¨ï¼Œåˆ©ç”¨ç”±ä¸œäº¬å¤§å­¦SaruLabå¼€å‘çš„MOSé¢„æµ‹ç³»ç»Ÿä½œä¸ºä»£ç†æŸå¤±ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†æ‰©æ•£æ¨¡å‹æŸå¤±å¼•å¯¼RLç­–ç•¥ä¼˜åŒ–ï¼ˆDLPOï¼‰ï¼Œå¹¶ä¸å…¶ä»–RLHFæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œé‡‡ç”¨NISQAè¯­éŸ³è´¨é‡å’Œè‡ªç„¶åº¦è¯„ä¼°æ¨¡å‹ä»¥åŠäººç±»åå¥½å®éªŒè¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºRLHFå¯ä»¥æå‡åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè€ŒDLPOå¯ä»¥æ›´å¥½åœ°æ”¹å–„ç”Ÿæˆè‡ªç„¶ä¸”é«˜è´¨é‡çš„è¯­éŸ³éŸ³é¢‘çš„æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå›¾åƒå’Œè¯­éŸ³æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯å¯ä»¥æå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†RLHFåœ¨åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨ã€‚</li>
<li>ä½¿ç”¨ä¸œäº¬å¤§å­¦SaruLabå¼€å‘çš„MOSé¢„æµ‹ç³»ç»Ÿä½œä¸ºä»£ç†æŸå¤±ã€‚</li>
<li>å¼•å…¥æ‰©æ•£æ¨¡å‹æŸå¤±å¼•å¯¼RLç­–ç•¥ä¼˜åŒ–ï¼ˆDLPOï¼‰ã€‚</li>
<li>DLPOä¸å…¶ä»–RLHFæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œé‡‡ç”¨NISQAè¯­éŸ³è´¨é‡å’Œè‡ªç„¶åº¦è¯„ä¼°æ¨¡å‹è¿›è¡Œè¯„æµ‹ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºRLHFå’ŒDLPOå¯ä»¥æ”¹è¿›åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œç”Ÿæˆæ›´è‡ªç„¶ã€é«˜è´¨é‡çš„è¯­éŸ³éŸ³é¢‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c14030629a7cc9a613b4afe409c46451.jpg" align="middle">
</details>




<h2 id="VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization"><a href="#VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization" class="headerlink" title="VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization"></a>VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization</h2><p><strong>Authors:Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai</strong></p>
<p>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the <a target="_blank" rel="noopener" href="https://vimtextspotter.github.io/">https://VimTextSpotter.github.io</a>. </p>
<blockquote>
<p>æ–‡æœ¬è¯†åˆ«æ˜¯ä¸€é¡¹ä»å›¾åƒæˆ–è§†é¢‘åºåˆ—ä¸­æå–æ–‡æœ¬ä¿¡æ¯çš„ä»»åŠ¡ï¼Œé¢ä¸´ç€è·¨åŸŸé€‚åº”çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ä»å›¾åƒåˆ°å›¾åƒå’Œä»å›¾åƒåˆ°è§†é¢‘çš„æ³›åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºVimTSï¼Œå®ƒé€šè¿‡å®ç°ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ›´å¥½ååŒä½œç”¨ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§PromptæŸ¥è¯¢ç”Ÿæˆæ¨¡å—å’Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ï¼Œæœ‰æ•ˆåœ°å°†åŸå§‹å•ä»»åŠ¡æ¨¡å‹è½¬æ¢ä¸ºé€‚åˆå›¾åƒå’Œè§†é¢‘åœºæ™¯çš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œå¹¶ä¸”åªéœ€æå°‘çš„é¢å¤–å‚æ•°ã€‚PromptæŸ¥è¯¢ç”Ÿæˆæ¨¡å—ä¿ƒè¿›äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ˜¾å¼äº¤äº’ï¼Œè€Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨åˆ™å¸®åŠ©æ¨¡å‹åŠ¨æ€å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„é€‚å½“ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»¥æ›´ä½çš„æˆæœ¬ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œæˆ‘ä»¬åˆ©ç”¨å†…å®¹å˜å½¢åœºï¼ˆCoDeFï¼‰ç®—æ³•åˆ›å»ºäº†ä¸€ä¸ªåˆæˆè§†é¢‘æ–‡æœ¬æ•°æ®é›†ï¼ˆVTD-368kï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•çº¦2.6%ï¼Œå¦‚TTåˆ°IC15ã€CTW1500åˆ°TTå’ŒTTåˆ°CTW1500ç­‰ã€‚å¯¹äºè§†é¢‘çº§åˆ«çš„è·¨åŸŸé€‚åº”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ICDAR2015è§†é¢‘å’ŒDSText v2ä¸Šçš„å¹³å‡MOTAæŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†ä¹‹å‰çš„ç«¯åˆ°ç«¯è§†é¢‘è¯†åˆ«æ–¹æ³•çº¦5.5%ï¼Œè€Œä»…ä½¿ç”¨å›¾åƒçº§æ•°æ®ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯æ˜ï¼Œç°æœ‰çš„å¤§å‹å¤šåª’ä½“æ¨¡å‹åœ¨ç”Ÿæˆè·¨åŸŸåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸æˆ‘ä»¬çš„VimTSæ¨¡å‹ç›¸æ¯”ï¼Œåè€…æ‰€éœ€çš„å‚æ•°å’Œæ•°æ®é‡æ˜¾è‘—å‡å°‘ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://vimtextspotter.github.ioä¸Šæä¾›./">https://VimTextSpotter.github.ioä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19652v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•VimTSï¼Œé€šè¿‡å®ç°ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ›´å¥½ååŒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ–¹æ³•åŒ…æ‹¬ç”Ÿæˆæç¤ºæŸ¥è¯¢æ¨¡å—å’Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ï¼Œå°†åŸå§‹å•ä»»åŠ¡æ¨¡å‹æœ‰æ•ˆåœ°è½¬æ¢ä¸ºé€‚åˆå›¾åƒå’Œè§†é¢‘åœºæ™¯çš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œä¸”é¢å¤–å‚æ•°æœ€å°‘ã€‚è¿˜æå‡ºä¸€ä¸ªé€šè¿‡åˆ©ç”¨å†…å®¹å˜å½¢åœºç®—æ³•åˆæˆçš„è§†é¢‘æ–‡æœ¬æ•°æ®é›†ï¼ˆVTD-368kï¼‰ï¼Œä½¿æ¨¡å‹èƒ½ä»¥è¾ƒä½æˆæœ¬å­¦ä¹ æ—¶é—´åºåˆ—ä¿¡æ¯ã€‚åœ¨è·¨åŸŸé€‚åº”çš„æ–‡æœ¬å›¾åƒä»»åŠ¡ä¸­ï¼ŒVimTSæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•å¹³å‡2.6%ã€‚å¯¹äºè§†é¢‘çº§åˆ«çš„è·¨åŸŸé€‚åº”ï¼ŒVimTSæ–¹æ³•ç”šè‡³åœ¨ICDAR2015è§†é¢‘å’ŒDSText v2ä¸Šå¹³å‡è¶…å‡ºä¹‹å‰çš„ç«¯åˆ°ç«¯è§†é¢‘è¯†åˆ«æ–¹æ³•5.5%ã€‚ä¸ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼ŒVimTSåœ¨è·¨åŸŸåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œä¸”æ‰€éœ€å‚æ•°å’Œæ•°æ®æ›´å°‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VimTSæ–¹æ³•é€šè¿‡ç”Ÿæˆæç¤ºæŸ¥è¯¢æ¨¡å—å’Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ï¼Œå®ç°äº†å•ä»»åŠ¡æ¨¡å‹åˆ°å¤šä»»åŠ¡æ¨¡å‹çš„è½¬æ¢ï¼Œé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘åœºæ™¯ã€‚</li>
<li>VimTSæ–¹æ³•é€šè¿‡åˆæˆè§†é¢‘æ–‡æœ¬æ•°æ®é›†ï¼ˆVTD-368kï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä½æˆæœ¬åœ°å­¦ä¹ æ—¶é—´åºåˆ—ä¿¡æ¯ã€‚</li>
<li>VimTSæ–¹æ³•åœ¨å¤šä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬ä¸åŒæ–‡æœ¬å’Œå›¾åƒåŸŸçš„é€‚åº”ã€‚</li>
<li>åœ¨è§†é¢‘çº§åˆ«çš„è·¨åŸŸé€‚åº”ä¸­ï¼ŒVimTSæ–¹æ³•æ˜¾è‘—ä¼˜äºä¹‹å‰çš„ç«¯åˆ°ç«¯è§†é¢‘è¯†åˆ«æ–¹æ³•ã€‚</li>
<li>ä¸ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼ŒVimTSåœ¨è·¨åŸŸåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œä¸”å‚æ•°å’Œæ•°æ®éœ€æ±‚æ›´å°‘ã€‚</li>
<li>VimTSæ–¹æ³•é€šè¿‡å®ç°ä»»åŠ¡é—´çš„æ›´å¥½ååŒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f6e7e98614248d3c744cdf84eac7f35e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20ddf5ee8d45f8d89b38acbfb080a543.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef497ce9045e2a79784ed651cb501142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a70a938a0b3b4ad66f5e15abd6215c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-732e80b5850a49baa56fae267715864a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9f08976df42514822cf5c9d5706cf73a.jpg" align="middle">
</details>




<h2 id="Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting"><a href="#Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting" class="headerlink" title="Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting"></a>Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting</h2><p><strong>Authors:Haolin Chen, Philip N. Garner</strong></p>
<p>We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained modelâ€™s inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸»è¦å—åˆ°æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„é€‚åº”æ€§çš„é©±åŠ¨ï¼›ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºæ›´é€šç”¨çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯æ‰§è¡Œæ­¤ç±»é€‚åº”æ€§çš„é€‚å½“æ¡†æ¶ã€‚ç„¶è€Œï¼Œç¾éš¾æ€§é—å¿˜ä»ç„¶æ˜¯PEFTçš„ä¸€ä¸ªé—®é¢˜ï¼Œä¼šæŸå®³é¢„è®­ç»ƒæ¨¡å‹çš„å›ºæœ‰èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåªè¦ç²¾ç»†è°ƒæ•´å±‚çš„å‚æ•°å˜åŒ–å¯ä»¥å¾®åˆ†è®¡ç®—ï¼Œç°æœ‰çš„è´å¶æ–¯å­¦ä¹ æŠ€æœ¯å°±å¯ä»¥åº”ç”¨äºPEFTä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ä¸€ç³»åˆ—è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³åˆæˆä»»åŠ¡çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å·²å»ºç«‹çš„æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ï¼ŒåŒ…æ‹¬å¯¹è§’çº¿å’Œå…‹è‹¥å†…å…‹å› å­åˆ†è§£æ–¹æ³•ï¼Œä»¥æ­£åˆ™åŒ–PEFTçš„ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬åœ¨é¢„è®­ç»ƒçŸ¥è¯†ä¿ç•™æ–¹é¢çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå…‹æœç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶ä¸é™ä½å¾®è°ƒæ€§èƒ½ï¼Œå¹¶ä¸”ä½¿ç”¨å…‹è‹¥å†…å…‹å› å­åˆ†è§£è¿‘ä¼¼æ¯”ä½¿ç”¨å¯¹è§’çº¿è¿‘ä¼¼æ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.12220v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä¸»è¦æ¢è®¨äº†æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´é—®é¢˜ï¼Œå¹¶æå‡ºåˆ©ç”¨æ›´é€šç”¨çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ¡†æ¶è¿›è¡Œæ­¤ç±»è°ƒæ•´ã€‚ç„¶è€Œï¼ŒPEFTå­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¼šæŸå®³é¢„è®­ç»ƒæ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ã€‚ä½œè€…å±•ç¤ºäº†ç°æœ‰çš„è´å¶æ–¯å­¦ä¹ æŠ€æœ¯å¯ä»¥åº”ç”¨äºPEFTä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œåªè¦èƒ½å¤Ÿè®¡ç®—å¾®è°ƒå±‚çš„å‚æ•°å˜åŒ–æ˜¯å¯å¾®åˆ†çš„ã€‚é€šè¿‡å®éªŒï¼Œä½œè€…åˆ©ç”¨Laplaceè¿‘ä¼¼æ–¹æ³•ï¼ŒåŒ…æ‹¬å¯¹è§’çº¿å’ŒKroneckeråˆ†è§£æ³•ï¼Œå¯¹PEFTè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶ä¸ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ä½œè€…çš„æ–¹æ³•èƒ½å¤Ÿå…‹æœç¾éš¾æ€§é—å¿˜è€Œä¸æŸå®³å¾®è°ƒæ€§èƒ½ï¼Œä¸”ä½¿ç”¨Kroneckeråˆ†è§£æ³•æ¯”ä½¿ç”¨å¯¹è§’çº¿æ³•æ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä¸»è¦å…³æ³¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯ä¸€ä¸ªé€‚ç”¨äºæ­¤ç±»è°ƒæ•´çš„é€šç”¨æ¡†æ¶ã€‚</li>
<li>PEFTå­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³å®ƒä¼šæŸå®³é¢„è®­ç»ƒæ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ã€‚</li>
<li>ç°æœ‰çš„è´å¶æ–¯å­¦ä¹ æŠ€æœ¯å¯ä»¥åº”ç”¨äºPEFTä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>åªè¦å¾®è°ƒå±‚çš„å‚æ•°å˜åŒ–å¯å¾®åˆ†ï¼Œå°±èƒ½åº”ç”¨è´å¶æ–¯å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>å®éªŒåˆ©ç”¨Laplaceè¿‘ä¼¼æ–¹æ³•å¯¹PEFTè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶ä¸ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œæ¯”è¾ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-91d17a7c8f1b6a0625fecb03c05952c4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1d6417acfdbfc3b9b7a1ef71e8297497.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3793a3389ce1451686ef4ed95b62ef38.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-035ba61f5cf002bdc4724210bc479027.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-111842636e0d3e93e65cd94d73b190bc.jpg" align="middle">
</details>




<h2 id="Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation"><a href="#Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation" class="headerlink" title="Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation"></a>Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation</h2><p><strong>Authors:Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Soyoon Kim, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Jung-Woo Ha, Sungroh Yoon, Kang Min Yoo</strong></p>
<p>Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm">https://github.com/naver-ai/usdm</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶å·¥ä½œæ˜¾ç¤ºï¼Œåœ¨æ‰©å¤§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä»¥ç›´æ¥ç†è§£å’Œåˆæˆè¯­éŸ³æ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„å»ºæ¨¡å£è¯­å¯¹è¯çš„ç­–ç•¥ä»ç„¶éš¾ä»¥æ‰æ‘¸ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„è°ƒæŸ¥ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„è¯­éŸ³æ–‡æœ¬LLMæ¡†æ¶ï¼Œå³ç»Ÿä¸€å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆUSDMï¼‰ï¼Œæ—¨åœ¨ç”Ÿæˆä¸ç»™å®šè¾“å…¥è¯­éŸ³ç›¸å…³çš„è¿è´¯å£è¯­å“åº”ï¼Œè€Œæ— éœ€ä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚æˆ‘ä»¬å·²ç»éªŒè¯äº†åŒ…å«ä¸»è¦å«æœ‰è¯­ä¹‰ä¿¡æ¯çš„è¯­éŸ³ç¬¦å·ä¸­çš„è¯­è°ƒï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªåŸºç¡€æ¥æ„å»ºä¸€ä¸ªæ³¨å…¥è¯­è°ƒçš„è¯­éŸ³æ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­éŸ³æ–‡æœ¬é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæé«˜äº†è·¨æ¨¡å¼è¯­ä¹‰çš„æ•è·èƒ½åŠ›ã€‚ä¸ºäº†æ„å»ºUSDMï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šæ­¥éª¤å£è¯­å¯¹è¯æ¨¡æ¿å¯¹è¯­éŸ³æ–‡æœ¬æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡æ¿åˆºæ¿€äº†ä¸‹å±‚LLMæ‰€å±•ç°çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨DailyTalkæ•°æ®é›†ä¸Šçš„è‡ªåŠ¨å’Œäººç±»è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç”Ÿæˆäº†è‡ªç„¶æµç•…çš„å£è¯­å“åº”ï¼Œè¶…è¶Šäº†å…ˆå‰çš„å’Œçº§è”çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/naver-ai/usdmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.05706v3">PDF</a> NeurIPS 2024, Project Page: <a target="_blank" rel="noopener" href="https://unifiedsdm.github.io/">https://unifiedsdm.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆUSDMï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ç†è§£å’Œåˆæˆè¯­éŸ³ï¼Œç”Ÿæˆä¸è¾“å…¥è¯­éŸ³ç›¸å…³çš„è¿è´¯å£è¯­å“åº”ï¼Œå…·æœ‰è‡ªç„¶å‘ç”Ÿçš„éŸµå¾‹ç‰¹å¾ï¼Œæ— éœ€ä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚é€šè¿‡æ„å»ºåŒ…å«ä¸»è¦è¯­ä¹‰ä¿¡æ¯çš„éŸµå¾‹è¯­éŸ³ä»¤ç‰Œï¼Œæå‡ºä¸€ç§é€šç”¨çš„è¯­éŸ³æ–‡æœ¬é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œå¢å¼ºè·¨æ¨¡æ€è¯­ä¹‰çš„æ•è·ã€‚åœ¨å£è¯­å¯¹è¯æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œé€šè¿‡å¤šæ­¥å£è¯­å¯¹è¯æ¨¡æ¿åˆºæ¿€åº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨DailyTalkæ•°æ®é›†ä¸Šçš„è‡ªåŠ¨å’Œäººç±»è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è‡ªç„¶å£è¯­å“åº”è¶…è¿‡å…ˆå‰å’Œçº§è”åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç›´æ¥ç†è§£å’Œåˆæˆè¯­éŸ³æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆUSDMï¼‰ï¼Œèƒ½ç”Ÿæˆä¸è¾“å…¥è¯­éŸ³ç›¸å…³çš„è¿è´¯å£è¯­å“åº”ã€‚</li>
<li>USDMä¸éœ€è¦ä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚</li>
<li>USDMé€šè¿‡åœ¨åŒ…å«ä¸»è¦è¯­ä¹‰ä¿¡æ¯çš„è¯­éŸ³ä»¤ç‰Œä¸­èå…¥éŸµå¾‹ï¼Œæ„å»ºäº†ä¸€ä¸ªéŸµå¾‹è¯­éŸ³æ–‡æœ¬æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­éŸ³æ–‡æœ¬é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€è¯­ä¹‰çš„æ•è·ã€‚</li>
<li>æ¨¡å‹åœ¨å£è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡å¤šæ­¥å£è¯­å¯¹è¯æ¨¡æ¿æ¿€å‘LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6798c63bc786ca1de2c782f0be72cea6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-91dc65923495bfd0c6671bc48d3bac96.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-df6d275e2fd773757f0bfd6c831df15e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c49f6c367b897312213c20c2022c1d95.jpg" align="middle">
</details>




<h2 id="Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer"><a href="#Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer" class="headerlink" title="Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer"></a>Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer</h2><p><strong>Authors:Peter Ochieng</strong></p>
<p>Diffusion based vocoders have been criticised for being slow due to the many steps required during sampling. Moreover, the modelâ€™s loss function that is popularly implemented is designed such that the target is the original input $x_0$ or error $\epsilon_0$. For early time steps of the reverse process, this results in large prediction errors, which can lead to speech distortions and increase the learning time. We propose a setup where the targets are the different outputs of forward process time steps with a goal to reduce the magnitude of prediction errors and reduce the training time. We use the different layers of a neural network (NN) to perform denoising by training them to learn to generate representations similar to the noised outputs in the forward process of the diffusion. The NN layers learn to progressively denoise the input in the reverse process until finally the final layer estimates the clean speech. To avoid 1:1 mapping between layers of the neural network and the forward process steps, we define a skip parameter $\tau&gt;1$ such that an NN layer is trained to cumulatively remove the noise injected in the $\tau$ steps in the forward process. This significantly reduces the number of data distribution recovery steps and, consequently, the time to generate speech. We show through extensive evaluation that the proposed technique generates high-fidelity speech in competitive time that outperforms current state-of-the-art tools. The proposed technique is also able to generalize well to unseen speech. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„vocoderç”±äºé‡‡æ ·è¿‡ç¨‹ä¸­éœ€è¦å¤šä¸ªæ­¥éª¤è€Œé€Ÿåº¦è¾ƒæ…¢ï¼Œå—åˆ°äº†æ‰¹è¯„ã€‚æ­¤å¤–ï¼Œç›®å‰å¹¿æ³›å®ç°çš„æ¨¡å‹çš„æŸå¤±å‡½æ•°æ˜¯é’ˆå¯¹åŸå§‹è¾“å…¥x_0æˆ–è¯¯å·®Îµ_0è®¾è®¡çš„ã€‚åœ¨åå‘è¿‡ç¨‹çš„å‰æœŸæ­¥éª¤ä¸­ï¼Œè¿™ä¼šå¯¼è‡´è¾ƒå¤§çš„é¢„æµ‹è¯¯å·®ï¼Œå¯èƒ½å¯¼è‡´è¯­éŸ³å¤±çœŸå¹¶å¢åŠ å­¦ä¹ æ—¶é—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¾ç½®ï¼Œç›®æ ‡ä¸ºæ­£å‘è¿‡ç¨‹æ—¶é—´æ­¥éª¤çš„ä¸åŒè¾“å‡ºï¼Œæ—¨åœ¨å‡å°‘é¢„æµ‹è¯¯å·®çš„å¤§å°å¹¶ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆNNï¼‰çš„ä¸åŒå±‚æ‰§è¡Œå»å™ªï¼Œé€šè¿‡è®­ç»ƒå®ƒä»¬å­¦ä¹ ç”Ÿæˆä¸æ‰©æ•£æ­£å‘è¿‡ç¨‹ä¸­çš„å™ªå£°è¾“å‡ºç›¸ä¼¼çš„è¡¨ç¤ºã€‚ç¥ç»ç½‘ç»œå±‚å­¦ä¹ åœ¨åå‘è¿‡ç¨‹ä¸­é€æ­¥å¯¹è¾“å…¥è¿›è¡Œå»å™ªï¼Œç›´åˆ°æœ€ç»ˆå±‚ä¼°è®¡å‡ºå¹²å‡€çš„è¯­éŸ³ã€‚ä¸ºäº†é¿å…ç¥ç»ç½‘ç»œå±‚ä¸æ­£å‘è¿‡ç¨‹æ­¥éª¤ä¹‹é—´çš„1:1æ˜ å°„ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè·³è¿‡å‚æ•°Ï„&gt; 1ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå±‚è¢«è®­ç»ƒç´¯ç§¯åœ°å»é™¤åœ¨æ­£å‘è¿‡ç¨‹çš„Ï„æ­¥ä¸­æ³¨å…¥çš„å™ªå£°ã€‚è¿™æ˜¾è‘—å‡å°‘äº†æ•°æ®åˆ†å¸ƒæ¢å¤æ­¥éª¤çš„æ•°é‡ï¼Œå› æ­¤ä¹Ÿç¼©çŸ­äº†ç”Ÿæˆè¯­éŸ³çš„æ—¶é—´ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æ‰€æå‡ºçš„æŠ€æœ¯èƒ½å¤Ÿåœ¨ç«äº‰æ—¶é—´å†…ç”Ÿæˆé«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼Œå¹¶ä¸”ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å·¥å…·ã€‚æ‰€æå‡ºçš„æŠ€æœ¯ä¹Ÿèƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯­éŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09652v3">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„vocoderæ”¹è¿›æ–¹æ¡ˆï¼Œé€šè¿‡è°ƒæ•´ç›®æ ‡è®¾å®šå’Œç¥ç»ç½‘ç»œå±‚çš„è®¾è®¡ï¼Œå‡å°‘äº†é¢„æµ‹è¯¯å·®å’Œè®­ç»ƒæ—¶é—´ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€é«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼Œå¹¶åœ¨ç«äº‰æ—¶é—´å†…è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£vocoderå› é‡‡æ ·è¿‡ç¨‹ä¸­æ­¥éª¤ç¹å¤šè€Œå—åˆ°é€Ÿåº¦æ…¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æµè¡Œçš„æ¨¡å‹æŸå¤±å‡½æ•°è®¾è®¡å¯¼è‡´æ—©æœŸåå‘è¿‡ç¨‹ä¸­çš„é¢„æµ‹è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½é€ æˆè¯­éŸ³å¤±çœŸå’Œå­¦ä¹ æ—¶é—´å¢åŠ ã€‚</li>
<li>æå‡ºä»¥æ‰©æ•£è¿‡ç¨‹å‰å‘æ­¥éª¤çš„ä¸åŒè¾“å‡ºæ¥è®¾å®šç›®æ ‡ï¼Œä»¥å‡å°é¢„æµ‹è¯¯å·®å’Œç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚</li>
<li>ä½¿ç”¨ç¥ç»ç½‘ç»œçš„ä¸åŒå±‚è¿›è¡Œå»å™ªè®­ç»ƒï¼Œå­¦ä¹ ç”Ÿæˆä¸æ‰©æ•£å‰å‘è¿‡ç¨‹ä¸­çš„å™ªå£°è¾“å‡ºç›¸ä¼¼çš„è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥è·³è·ƒå‚æ•°Ï„&gt;1ï¼Œä½¿ç¥ç»ç½‘ç»œå±‚ç´¯ç§¯å­¦ä¹ åœ¨Ï„æ­¥ä¸­æ³¨å…¥çš„å™ªå£°çš„å‰å‘è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘æ•°æ®åˆ†å¸ƒæ¢å¤æ­¥éª¤å’Œç”Ÿæˆè¯­éŸ³çš„æ—¶é—´ã€‚</li>
<li>ç»å¹¿æ³›è¯„ä¼°ï¼Œè¯¥æŠ€æœ¯åœ¨ç«äº‰æ—¶é—´å†…ç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c25b2ace83cc3057ff3a08e435a27f4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e96c9504cca1fdb2836c0820b9d1871.jpg" align="middle">
</details>




<h2 id="Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds"><a href="#Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds" class="headerlink" title="Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds"></a>Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds</h2><p><strong>Authors:Androniki Dimitriou, Daniel G. Figueroa, Bryan Zaldivar</strong></p>
<p>We apply state-of-the-art, likelihood-free statistical inference (machine-learning-based) techniques for reconstructing the spectral shape of a gravitational wave background (GWB). We focus on the reconstruction of an arbitrarily shaped signal by the LISA detector, but the method can be easily extended to either template-dependent signals, or to other detectors, as long as a characterisation of the instrumental noise is available. As proof of the technique, we quantify the ability of LISA to reconstruct signals of arbitrary spectral shape (${\it blind}$ reconstruction), considering a diversity of frequency profiles, and including astrophysical backgrounds in some cases. As a teaser of how the method can reconstruct signals characterised by a parameter-dependent template (${\it template}$ reconstruction), we present a dedicated study for power-law signals. While our technique has several advantages with respect to traditional MCMC methods, we validate it with the latter for concrete cases. This work opens the door for both fast and accurate Bayesian parameter estimation of GWBs, with essentially no computational overhead during the inference step. Our set of tools are integrated into the package ${\tt GWBackFinder}$, which is publicly available in <a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder">https://github.com/AndronikiDimitriou/GWBackFinder</a>. </p>
<blockquote>
<p>æˆ‘ä»¬é‡‡ç”¨æœ€å…ˆè¿›çš„æ— ä¼¼ç„¶ç»Ÿè®¡æ¨æ–­ï¼ˆåŸºäºæœºå™¨å­¦ä¹ ï¼‰æŠ€æœ¯æ¥é‡å»ºå¼•åŠ›æ³¢èƒŒæ™¯ï¼ˆGWBï¼‰çš„é¢‘è°±å½¢çŠ¶ã€‚æˆ‘ä»¬ä¸“æ³¨äºé€šè¿‡LISAæ¢æµ‹å™¨é‡å»ºä»»æ„å½¢çŠ¶çš„ä¿¡å·ï¼Œä½†åªè¦æœ‰ä»ªå™¨å™ªå£°çš„ç‰¹å¾ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°æ¨¡æ¿ä¾èµ–çš„ä¿¡å·æˆ–å…¶ä»–æ¢æµ‹å™¨ã€‚ä½œä¸ºè¯¥æŠ€æœ¯çš„è¯æ˜ï¼Œæˆ‘ä»¬é‡åŒ–äº†LISAåœ¨å¤šç§é¢‘ç‡åˆ†å¸ƒä¸‹é‡å»ºä»»æ„é¢‘è°±å½¢çŠ¶ä¿¡å·çš„èƒ½åŠ›ï¼ˆç›²é‡å»ºï¼‰ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹è¿˜åŒ…æ‹¬å¤©æ–‡èƒŒæ™¯ã€‚ä½œä¸ºè¯¥æ–¹æ³•å¦‚ä½•é‡å»ºå‚æ•°ä¾èµ–æ¨¡æ¿ä¿¡å·çš„ä¸€ä¸ªæç¤ºï¼ˆæ¨¡æ¿é‡å»ºï¼‰ï¼Œæˆ‘ä»¬å¯¹å¹‚å¾‹ä¿¡å·è¿›è¡Œäº†ä¸“é¡¹ç ”ç©¶ã€‚è™½ç„¶æˆ‘ä»¬çš„æŠ€æœ¯ä¸ä¼ ç»Ÿçš„MCMCæ–¹æ³•ç›¸æ¯”å…·æœ‰è®¸å¤šä¼˜åŠ¿ï¼Œä½†å¯¹äºå…·ä½“æ¡ˆä¾‹ï¼Œæˆ‘ä»¬ä»ä½¿ç”¨åè€…å¯¹å…¶è¿›è¡ŒéªŒè¯ã€‚è¿™é¡¹å·¥ä½œä¸ºGWBçš„å¿«é€Ÿå’Œç²¾ç¡®è´å¶æ–¯å‚æ•°ä¼°è®¡æ‰“å¼€äº†å¤§é—¨ï¼Œåœ¨æ¨ç†æ­¥éª¤ä¸­å‡ ä¹ä¸éœ€è¦è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„å·¥å…·é›†å·²é›†æˆåˆ°GWBackFinderè½¯ä»¶åŒ…ä¸­ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AndronikiDimitriou/GWBackFinderå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08430v3">PDF</a> Published in JCAP. 29 pages plus appendices and references, 12   figures</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨æœ€å…ˆè¿›çš„æ— éœ€ä¼¼ç„¶å‡½æ•°çš„ç»Ÿè®¡æ¨æ–­ï¼ˆåŸºäºæœºå™¨å­¦ä¹ ï¼‰æŠ€æœ¯ï¼Œå¯¹å¼•åŠ›æ³¢èƒŒæ™¯ï¼ˆGWBï¼‰çš„é¢‘è°±å½¢çŠ¶è¿›è¡Œé‡å»ºã€‚é‡ç‚¹ç ”ç©¶LISAæ¢æµ‹å™¨å¯¹ä»»æ„å½¢çŠ¶ä¿¡å·çš„é‡å»ºï¼Œä½†è¯¥æ–¹æ³•å¯è½»æ¾æ‰©å±•åˆ°ä¾èµ–äºæ¨¡æ¿çš„ä¿¡å·æˆ–å…¶ä»–æ¢æµ‹å™¨ï¼Œåªè¦æä¾›ä»ªå™¨å™ªå£°çš„ç‰¹å¾å³å¯ã€‚ä½œä¸ºä¸­å›½è¯æ˜è¯¥æŠ€æœ¯èƒ½åŠ›çš„å±•ç¤ºï¼Œæˆ‘ä»¬é‡åŒ–äº†LISAå¯¹ä»»æ„é¢‘è°±å½¢çŠ¶ä¿¡å·çš„é‡å»ºèƒ½åŠ›ï¼ˆç›²é‡å»ºï¼‰ï¼Œå¹¶è€ƒè™‘äº†å¤šç§é¢‘ç‡åˆ†å¸ƒï¼Œåœ¨æŸäº›æƒ…å†µä¸‹è¿˜åŒ…æ‹¬å¤©æ–‡èƒŒæ™¯ã€‚ä½œä¸ºè¯¥æ–¹æ³•çš„æ¨¡æ¿é‡å»ºæ¦‚å¿µçš„é¢„è§ˆï¼Œæˆ‘ä»¬å¯¹å¹‚å¾‹ä¿¡å·è¿›è¡Œäº†ä¸“é¡¹ç ”ç©¶ã€‚è™½ç„¶æˆ‘ä»¬çš„æŠ€æœ¯ä¸ä¼ ç»Ÿçš„MCMCæ–¹æ³•ç›¸æ¯”å…·æœ‰å¤šä¸ªä¼˜åŠ¿ï¼Œä½†æˆ‘ä»¬ä»å°†å…¶ç”¨äºå…·ä½“æ¡ˆä¾‹è¿›è¡ŒéªŒè¯ã€‚è¿™ä¸ºå¿«é€Ÿå‡†ç¡®çš„GWBè´å¶æ–¯å‚æ•°ä¼°è®¡æ‰“å¼€äº†å¤§é—¨ï¼Œæ¨ç†æ­¥éª¤å‡ ä¹æ²¡æœ‰è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„å·¥å…·é›†å·²é›†æˆåˆ°GWBackFinderè½¯ä»¶åŒ…ä¸­ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder">https://github.com/AndronikiDimitriou/GWBackFinder</a>å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯é‡å»ºå¼•åŠ›æ³¢èƒŒæ™¯ï¼ˆGWBï¼‰çš„é¢‘è°±å½¢çŠ¶ã€‚</li>
<li>é‡ç‚¹å…³æ³¨LISAæ¢æµ‹å™¨å¯¹ä»»æ„å½¢çŠ¶ä¿¡å·çš„é‡å»ºèƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºä¾èµ–äºæ¨¡æ¿çš„ä¿¡å·æˆ–å…¶ä»–æ¢æµ‹å™¨ã€‚</li>
<li>å±•ç¤ºäº†ç›²é‡å»ºèƒ½åŠ›ï¼Œå³å¯¹ä»»æ„é¢‘è°±å½¢çŠ¶ä¿¡å·çš„é‡å»ºï¼Œå¹¶è€ƒè™‘äº†å¤šç§é¢‘ç‡åˆ†å¸ƒå’Œå¤©æ–‡èƒŒæ™¯ã€‚</li>
<li>é€šè¿‡å¹‚å¾‹ä¿¡å·è¿›è¡Œæ¨¡æ¿é‡å»ºçš„é¢„è§ˆã€‚</li>
<li>ä¸ä¼ ç»ŸMCMCæ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶é€šè¿‡å…·ä½“æ¡ˆä¾‹è¿›è¡ŒéªŒè¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b71214c325a1d33137c11a8085cf040a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f111152cd7161ef71d176cd5aef745f9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;æœ¬ç¯‡
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
