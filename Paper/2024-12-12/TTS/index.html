<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>多模态生成模型需要一种统一的方法来处理离散数据（如文本和代码）和连续数据（如图像、音频和视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），它利用因果Transformer无缝集成连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入下一个令牌扩散来进行这些向量的自回归生成。此外，我们开发出了σ-VAE，以解决方差崩溃的挑战，这对于自回归建模至关重要。大量实验表明，LatentLM在多种模态中的应用都很有效。在图像生成方面，LatentLM在性能和可扩展性方面都超越了Diffusion Transformers。当集成到多模态大型语言模型中时，LatentLM提供了一个通用接口，统一了多模态生成和理解。实验结果表明，在扩大训练令牌规模方面，LatentLM与Transfusion和向量量化模型相比取得了有利的表现。在文本到语音合成方面，LatentLM在说话人相似性和稳健性方面超越了最先进的VALL-E 2模型，同时需要10倍更少的解码步骤。这些结果证明了LatentLM在推进大型多模态模型方面是一种高效且可扩展的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Latent Language Modeling（LatentLM）是一种多模态生成模型，能处理离散数据和连续数据。它通过变分自编码器（VAE）表示连续数据为潜在向量，并引入next-token扩散进行这些向量的自回归生成。此外，LatentLM解决了方差消失问题并提升了模型性能。实验表明，LatentLM在各种模态上表现出优越效果，超越Diffusion Transformers与Transfusion等模型。在文本到语音合成方面，LatentLM以较少的解码步骤达到或超越了现有技术。因此，LatentLM是一种高效、可扩展的多模态模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Language Modeling (LatentLM) 能统一处理离散和连续数据。</li>
<li>使用变分自编码器（VAE）表示连续数据为潜在向量。</li>
<li>Next-token扩散用于自回归生成潜在向量。</li>
<li>LatentLM解决了方差消失问题以提高模型性能。</li>
<li>在各种模态上，LatentLM表现优越，超越其他模型如Diffusion Transformers和Transfusion等。</li>
<li>在文本到语音合成方面，LatentLM效果显著，与现有技术相比需要更少的解码步骤。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Zero-Shot-Mono-to-Binaural-Speech-Synthesis"><a href="#Zero-Shot-Mono-to-Binaural-Speech-Synthesis" class="headerlink" title="Zero-Shot Mono-to-Binaural Speech Synthesis"></a>Zero-Shot Mono-to-Binaural Speech Synthesis</h2><p><strong>Authors:Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn, Eliya Nachmani</strong></p>
<p>We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis. </p>
<blockquote>
<p>我们提出了ZeroBAS方法，这是一种从单声道音频录制和位置信息合成双声道音频的神经方法，无需对任何双声道数据进行训练。据我们所知，这是首次发布的从零开始神经方法用于单声道到双声道的音频合成。具体来说，我们展示了基于源位置的无需参数几何时间扭曲和振幅缩放足以获得初始双声道合成，可以通过迭代应用预训练的降噪编解码器进行细化。此外，我们发现这导致了跨房间条件的泛化，我们通过引入一个新的数据集TUT Mono-to-Binaural来衡量这一点，以评估最先进单声道到双声道合成方法在未见条件上的表现。我们的零样本方法与标准单声道到双声道数据集上的有监督方法在感知上相当，甚至在我们的离群TUT Mono-to-Binaural数据集上超过了它们。我们的结果突出了预训练的生成音频模型和零样本学习在解锁稳健的双声道音频合成方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>零基音频合成方法（ZeroBAS）是一种无需训练即可从单声道音频录制和位置信息合成双声道音频的神经方法。这是首次发表的无训练双声道音频合成的神经方法。通过基于源位置的参数化几何时间扭曲和振幅缩放，可以实现初步的双声道合成，并通过迭代应用预训练的降噪编解码器进行改进。此方法可在不同房间条件下进行推广，并通过引入新的数据集TUT Mono-to-Binaural来评估其性能。零基方法在标准单声道到双声道数据集上的表现与有监督方法相当，甚至在我们的TUT Mono-to-Binaural数据集上表现更佳。结果突显了预训练生成音频模型和零样本学习的潜力，可解锁稳健的双声道音频合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZeroBAS是一种无需训练的双声道音频合成方法，可从单声道音频和位置信息合成双声道音频。</li>
<li>参数化的几何时间扭曲和振幅缩放是实现初步双声道合成的关键步骤。</li>
<li>通过迭代应用预训练的降噪编解码器，可以改进初步合成的双声道音频质量。</li>
<li>该方法可在不同房间条件下进行推广。</li>
<li>零基方法在标准数据集上的表现与有监督方法相当。</li>
<li>在新的数据集TUT Mono-to-Binaural上，零基方法的性能更佳，突显了其稳健性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fc16144bf45b94da94e40c11b920a95f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0fbdc199ca731fc2359afadc2dc804e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5639e22ca8041f88b59072e33ed30687.jpg" align="middle">
</details>




<h2 id="A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings"><a href="#A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings" class="headerlink" title="A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings"></a>A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings</h2><p><strong>Authors:Anindita Mondal, Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Anil Kumar Vuppala, Chiranjeevi Yarra</strong></p>
<p>Automatic detection of prominence at the word and syllable-levels is critical for building computer-assisted language learning systems. It has been shown that prosody embeddings learned by the current state-of-the-art (SOTA) text-to-speech (TTS) systems could generate word- and syllable-level prominence in the synthesized speech as natural as in native speech. To understand the effectiveness of prosody embeddings from TTS for prominence detection under nonnative context, a comparative analysis is conducted on the embeddings extracted from native and non-native speech considering the prominence-related embeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2. These embeddings are extracted under two conditions considering: 1) only text, 2) both speech and text. For the first condition, the embeddings are extracted directly from the TTS inference mode, whereas for the second condition, we propose to extract from the TTS under training mode. Experiments are conducted on native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For experimentation, word-level prominence locations are manually annotated for both corpora. The highest relative improvement on word &amp; syllable-level prominence detection accuracies with the TTS embeddings are found to be 13.7% &amp; 5.9% and 16.2% &amp; 6.9% compared to those with the heuristic-based features and self-supervised Wav2Vec-2.0 representations, respectively. </p>
<blockquote>
<p>自动检测单词和音节级别的突出性对于构建计算机辅助语言学习系统至关重要。研究表明，当前最先进的文本转语音（TTS）系统所学习的韵律嵌入可以在合成语音中产生与原生语音一样自然的单词和音节级别的突出性。为了了解TTS韵律嵌入在非母语环境下的突出性检测效果，对来自名为FastSpeech2的先进TTS系统提取的嵌入进行了比较分析，考虑了与突出性相关的嵌入：持续时间、能量和音调。这些嵌入是在两种条件下提取的：1）只有文本，2）语音和文本都有。对于第一种情况，嵌入直接从TTS推理模式中提取，而对于第二种情况，我们提出在TTS训练模式下提取。实验在母语语料库Tatoeba和非母语语料库ISLE上进行。为了实验，两个语料库的单词级别突出位置都进行了手动注释。与基于启发式特征和自监督Wav2Vec-2.0表示相比，使用TTS嵌入在单词和音节级别的突出性检测准确率上获得了最高相对改进，分别为13.7%和5.9%，以及16.2%和6.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08283v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于当前最新技术（SOTA）文本转语音（TTS）系统学习到的韵律嵌入能够在合成语音中生成自然的词和音节级别的突出表达。在非母语环境下，对TTS中用于突出检测的有效性进行了对比分析，从母语和非母语语音中提取的嵌入考虑了与突出相关的嵌入因素：持续时间、能量和音调。这些嵌入是在两种情况下提取的：仅文本和语音与文本结合。对于第一种情况，直接从TTS推理模式中提取嵌入；对于第二种情况，建议在TTS训练模式下提取。实验在母语语料库Tatoeba和非母语语料库ISLE上进行。实验中，手动标注了两组语料库中词级的突出位置。相较于启发式特征和自监督的Wav2Vec-2.0表示方法，使用TTS嵌入后，词和音节级别的突出检测准确率相对提高了最高达16.2%和6.9%。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>文本转语音系统对韵律嵌入的理解非常重要，可生成自然的词和音节级别的突出表达。</li>
<li>对比分析了在自然和非母语环境下的韵律嵌入对于重要性检测的有效性。</li>
<li>实验考虑了在两种情境下提取嵌入：仅基于文本和同时考虑语音与文本。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cddeb908c5ada709a4912e18669893fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5338a274123eec473a1867ddacc01306.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56dc19cd9feb26e4ff74f63dad0098a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15260501556eca45828eccdcf14f7450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb686b114ea5a3fc74636b3e2d428e2.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>基于大模型的文本转语音（TTS）系统对数据有着极高的需求。近期基于大模型的TTS工作通常采用复杂的数据处理流程以获得高质量的训练数据。这些高级流程需要在每个阶段都有优秀的模型（例如语音降噪、语音增强、说话人识别和标点模型），而这些模型本身也需要高质量的训练数据并且很少开源。即使使用最先进的模型，仍存在一些问题，例如背景噪声去除不完全以及标点符号与实际语音停顿之间的不匹配。此外，严格的过滤策略通常只能保留原始数据的10-30%，极大地阻碍了数据扩展的努力。</p>
</blockquote>
<p>在这项工作中，我们利用噪声鲁棒的音频标记器（S3Tokenizer）设计了一个简化而有效的TTS数据处理流程，该流程在保持数据质量的同时，大大降低了数据获取成本，实现了超过50%的数据保留率。除了数据扩展的挑战外，基于大模型的TTS系统的部署成本也高于传统方法。当前的系统通常仅使用大模型进行文本到标记的生成，但需要额外的模型（如流程匹配模型）来进行标记到波形生成的转换，这些转换无法由大模型推理引擎直接执行，进一步增加了部署的复杂性。为了应对这些挑战，我们消除了大模型和流程组件中的冗余模块，并用大模型架构替代流程模型的主干。基于这种简化的流程主干，我们提出了流式和非流式推理的统一架构，大大降低了部署成本。最后，我们借助简化的流程和S3Tokenizer，探索了使用相同数据进行TTS和语音识别（ASR）任务训练的可行性，降低了TTS训练数据的质量要求。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>摘要</strong><br>基于LLM的TTS数据处理方法的新改进。通过采用噪声鲁棒的音频分词器（S3Tokenizer），简化了数据获取流程，提高了数据保留率。同时，通过优化模型架构，减少了部署成本，并探索了统一TTS和ASR任务的可行性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM-based TTS系统对数据质量有较高要求，需复杂的数据处理流程和高质量的训练数据。</li>
<li>S3Tokenizer的应用使得TTS数据处理流程简化，同时保持数据质量，并提高数据保留率至50%以上。</li>
<li>LLM-based TTS系统的部署成本较高，需要通过优化模型架构来降低。</li>
<li>提出了一种统一的架构，支持流式和非流式推理，降低了部署成本。</li>
<li>简化的数据处理流程和S3Tokenizer使得TTS和ASR任务可以使用相同的数据进行训练。</li>
<li>仍存在背景噪声去除不完全和标点与语音停顿对齐问题，需要进一步完善。</li>
<li>未来的研究可以进一步探索如何优化LLM-based TTS系统的效率和性能，以应对大规模数据和复杂场景的挑战。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>基于扩散的生成人工智能（AI）因其相较于其他生成技术（如生成对抗网络和变分自编码器）的卓越性能而受到广泛关注。虽然它在计算机视觉和自然语言处理等领域取得了显著进展，但在语音生成方面的应用仍然被较少探索。主流的文本到语音（TTS）系统主要在频谱空间中把输出映射到梅尔频谱（Mel-Spectrograms），由于梅尔频谱的稀疏性，导致计算负载较高。为了解决这些限制，我们提出了LatentSpeech，这是一种利用潜在扩散模型的新型TTS生成方法。LatentSpeech通过使用潜在嵌入作为中间表示，将目标维度降低到梅尔频谱所需的5%，简化了TTS编码器与vocoder的处理过程，实现了高效高质量的语音生成。这项研究标志着潜在扩散模型在TTS中的首次集成，提高了生成语音的准确性和自然度。在基准数据集上的实验结果表明，与现有模型相比，LatentSpeech在词错误率上提高了25%，梅尔倒谱失真提高了24%，随着训练数据的增加，这两项指标分别进一步提高到49.5%和26%。这些发现凸显了LatentSpeech在推动TTS技术前沿方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>Summary</strong><br>     基于扩散的生成式AI在语音识别领域受到关注，但仍存在应用局限性。主流文本转语音系统主要映射到Mel频谱图，导致高计算负载。本研究提出LatentSpeech，一种利用潜在扩散模型的新型TTS生成方法，通过潜在嵌入作为中间表示，降低目标维度至Mel频谱图的5%，简化TTS编码器和vocoder的处理流程，实现高效高质量的语音生成。LatentSpeech集成于TTS中提高了生成语音的准确性和自然度，在基准数据集上的实验结果表明其较现有模型有所改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散式生成AI在多个领域表现优越，但在语音生成领域的应用仍待探索。</li>
<li>主流TTS系统主要映射到Mel频谱图，导致高计算负载。</li>
<li>LatentSpeech是一种新型TTS生成方法，利用潜在扩散模型，降低目标维度至Mel频谱图的5%。</li>
<li>LatentSpeech简化了TTS编码器和vocoder的处理流程，提高了语音生成的效率。</li>
<li>LatentSpeech集成了扩散模型在TTS中，提高了生成语音的准确性和自然度。</li>
<li>实验结果表明，相较于现有模型，LatentSpeech在Word Error Rate上有25%的改进，在Mel Cepstral Distortion上有24%的改进。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats"><a href="#Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats" class="headerlink" title="Sampling from Boltzmann densities with physics informed low-rank formats"></a>Sampling from Boltzmann densities with physics informed low-rank formats</h2><p><strong>Authors:Paul Hagemann, Janina Schütte, David Sommer, Martin Eigel, Gabriele Steidl</strong></p>
<p>Our method proposes the efficient generation of samples from an unnormalized Boltzmann density by solving the underlying continuity equation in the low-rank tensor train (TT) format. It is based on the annealing path commonly used in MCMC literature, which is given by the linear interpolation in the space of energies. Inspired by Sequential Monte Carlo, we alternate between deterministic time steps from the TT representation of the flow field and stochastic steps, which include Langevin and resampling steps. These adjust the relative weights of the different modes of the target distribution and anneal to the correct path distribution. We showcase the efficiency of our method on multiple numerical examples. </p>
<blockquote>
<p>我们的方法通过解决低秩张量列车（TT）格式中的基础连续性方程，有效地从未标准化的玻尔兹曼密度中生成样本。它基于MCMC文献中常用的退火路径，由能量空间中的线性插值给出。受序贯蒙特卡罗的启发，我们在确定性时间步长和流动场TT表示的随机步骤之间进行交替切换，随机步骤包括朗格文（Langevin）和重采样步骤。这些步骤调整了目标分布的各模态的相对权重，并退火到正确的路径分布。我们在多个数值示例中展示了该方法的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07637v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出一种基于低秩张量列车（TT）格式的未归一化玻尔兹曼密度样本高效生成方法。该方法基于MCMC文献中常用的退火路径，通过能量空间中的线性插值给出。该方法受到序贯蒙特卡罗的启发，在流场的TT表示中交替进行确定性时间步长和随机步骤，包括朗格文重采样步骤。这些步骤调整目标分布的各模态相对权重，并退火到正确的路径分布。通过多个数值例子展示了该方法的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于低秩张量列车（TT）格式的高效生成未归一化玻尔兹曼密度样本的方法。</li>
<li>方法结合了退火路径和线性插值技术，在能量空间中寻找最佳样本。</li>
<li>受到序贯蒙特卡罗的启发，该方法交替使用确定性时间步长和随机步骤。</li>
<li>确定性时间步长基于流场的TT表示，而随机步骤包括朗格文重采样步骤。</li>
<li>通过调整目标分布的各模态相对权重，实现了更精确的样本生成。</li>
<li>该方法通过退火过程达到正确的路径分布，提高了样本生成的效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cbaf87fb6994743aa883b5db79f16f19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c51d55d3f34411e1abfb2c58e4b59726.jpg" align="middle">
</details>




<h2 id="Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection"><a href="#Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection" class="headerlink" title="Mitigating Unauthorized Speech Synthesis for Voice Protection"></a>Mitigating Unauthorized Speech Synthesis for Voice Protection</h2><p><strong>Authors:Zhisheng Zhang, Qianyi Yang, Derui Wang, Pengyang Huang, Yuxin Cao, Kai Ye, Jie Hao</strong></p>
<p>With just a few speech samples, it is possible to perfectly replicate a speaker’s voice in recent years, while malicious voice exploitation (e.g., telecom fraud for illegal financial gain) has brought huge hazards in our daily lives. Therefore, it is crucial to protect publicly accessible speech data that contains sensitive information, such as personal voiceprints. Most previous defense methods have focused on spoofing speaker verification systems in timbre similarity but the synthesized deepfake speech is still of high quality. In response to the rising hazards, we devise an effective, transferable, and robust proactive protection technology named Pivotal Objective Perturbation (POP) that applies imperceptible error-minimizing noises on original speech samples to prevent them from being effectively learned for text-to-speech (TTS) synthesis models so that high-quality deepfake speeches cannot be generated. We conduct extensive experiments on state-of-the-art (SOTA) TTS models utilizing objective and subjective metrics to comprehensively evaluate our proposed method. The experimental results demonstrate outstanding effectiveness and transferability across various models. Compared to the speech unclarity score of 21.94% from voice synthesizers trained on samples without protection, POP-protected samples significantly increase it to 127.31%. Moreover, our method shows robustness against noise reduction and data augmentation techniques, thereby greatly reducing potential hazards. </p>
<blockquote>
<p>近年来，只需少量的语音样本，就能够完美复制一个人的声音，而恶意声音滥用（例如电信欺诈以获取非法财务收益）给我们日常生活带来了巨大的危害。因此，保护含有敏感信息（如个人语音特征）的公开语音数据至关重要。虽然大多数先前的方法侧重于欺骗说话者验证系统的音色相似性，但合成的深度伪造语音仍然具有很高的质量。为了应对日益增长的威胁，我们设计了一种有效、可迁移且稳健的主动保护技术，称为关键目标扰动（POP）。该技术对原始语音样本应用几乎无法察觉的错误最小化噪声，防止它们被有效学习用于文本到语音（TTS）合成模型，从而无法生成高质量的深度伪造语音。我们对采用客观和主观指标的先进TTS模型进行了广泛实验，全面评估了我们提出的方法。实验结果表明，该方法在各种模型中的有效性和可迁移性都很出色。与未经保护的样本训练的语音合成器的语音清晰度得分为21.94%相比，POP保护的样本得分显著提高到127.31%。此外，我们的方法显示出对降噪和数据增强技术的稳健性，从而大大减少了潜在风险。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20742v1">PDF</a> Accepted to ACM CCS Workshop (LAMPS) 2024</p>
<p><strong>摘要</strong><br>近期出现能通过少量语音样本完美复制发言人声音的技术，但同时也带来了恶意语音滥用（如电信诈骗）的巨大风险。因此，保护含有敏感信息的公开语音数据至关重要，如个人声纹。为应对风险，我们开发了一种有效、可迁移且稳健的主动保护技术——关键目标扰动（POP），通过在原始语音样本上添加几乎无法察觉的最小误差噪声，防止它们被用于文本到语音（TTS）合成模型的训练，从而防止高质量深度伪造语音的生成。我们对最先进的TTS模型进行了大量实验，用客观和主观指标全面评估了我们的方法。实验结果显示其在不同模型中的出色效果和可迁移性。相较于未经保护的样本训练的语音合成器，其语音清晰度只有21.94%，使用POP保护的样本将其显著提高到127.31%。此外，我们的方法显示出对抗降噪和数据增强技术的稳健性，大大降低了潜在风险。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>仅需少量语音样本即可完美复制发言人声音，同时出现恶意语音滥用风险。</li>
<li>保护公开语音数据中的敏感信息至关重要。</li>
<li>提出一种名为关键目标扰动（POP）的主动保护技术，通过添加几乎无法察觉的噪声防止语音样本被用于TTS合成。</li>
<li>POP技术在各种TTS模型中表现出卓越的有效性和可迁移性。</li>
<li>与未经保护的样本相比，POP保护的样本训练的语音合成器语音清晰度显著提高。</li>
<li>POP技术对抗降噪和数据增强技术具有稳健性。</li>
<li>POP技术有助于大大降低因深度伪造语音带来的潜在风险。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9cc75591dbb4786702c6ed9b92008756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c50bf08a392d1274b1e3ff5afe161f71.jpg" align="middle">
</details>




<h2 id="Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation"><a href="#Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation" class="headerlink" title="Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation"></a>Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation</h2><p><strong>Authors:Maohao Shen, Shun Zhang, Jilong Wu, Zhiping Xiu, Ehab AlBadawy, Yiting Lu, Mike Seltzer, Qing He</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama’s competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation. </p>
<blockquote>
<p>大型语言模型（LLM）已经凭借在各种文本相关任务中的卓越表现彻底改变了自然语言处理（NLP）的格局。然而，将文本主导的大型语言模型扩展到语音生成任务仍然有待探索。在这项工作中，我们引入了一种由精细调整的Llama模型驱动的文本到语音（TTS）系统，名为TTS-Llama，该系统实现了最先进的语音合成性能。基于TTS-Llama，我们进一步提出了MoLE-Llama，这是一个文本和语音多模态的大型语言模型，通过纯粹的后期融合参数高效微调（PEFT）和混合专家架构开发。大量的实证结果表明，MoLE-Llama在纯文本问答（QA）和TTS任务上的表现具有竞争力，减轻了任一模态中的灾难性遗忘问题。最后，我们在文本-语音问答任务中进一步探索了MoLE-Llama，证明了其作为多模态对话系统的巨大潜力，能够进行语音生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的文本转语音（TTS）系统取得了显著进展。本研究引入了一种基于精细调整Llama模型的TTS系统（名为TTS-Llama），实现了先进的语音合成性能。进一步构建的MoLE-Llama文本与语音混合模态LLM模型，通过纯粹的晚期融合参数效率微调（PEFT）和混合专家架构发展而来。该模型在纯文本问答（QA）和TTS任务上表现出竞争力，减轻了模态灾难遗忘问题。在文本语音问答任务中的探索表明，其作为多模态对话系统的潜力巨大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在文本转语音（TTS）系统的应用取得了显著进展。</li>
<li>TTS-Llama系统基于精细调整的Llama模型，实现了先进的语音合成性能。</li>
<li>MoLE-Llama是文本与语音混合模态LLM模型，通过PEFT和混合专家架构发展而来。</li>
<li>MoLE-Llama在纯文本问答（QA）和TTS任务上表现出竞争力。</li>
<li>MoLE-Llama可以减轻模态灾难遗忘问题。</li>
<li>MoLE-Llama在文本语音问答任务中的探索展现出其作为多模态对话系统的潜力。</li>
<li>该研究为TTS领域提供了新方向，结合了LLM和语音技术，具有广泛的应用前景。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab288d5e910dba80ad1170150f3378af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1d6f26c2831e3dbd99fcf1e01e379ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-154078eb091a83b8bcaebdf9c65b4d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18398ad78f9636c12358fd611c1222b8.jpg" align="middle">
</details>




<h2 id="Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis"><a href="#Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis" class="headerlink" title="Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis"></a>Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis</h2><p><strong>Authors:Suparna De, Ionut Bostan, Nishanth Sastry</strong></p>
<p>Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers’ emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications. </p>
<blockquote>
<p>尽管有单音调文本到语音（TTS）屏幕阅读器等技术和表情符号的视觉元素音频叙述等辅助技术，但最近的研究概述了盲人或视障以及识字较少的人在社交网络中遇到的交互访问挑战。情感语音生成传统上依赖于人类输入的预期情感以及文本合成，还存在数据简化（导致信息丢失）和持续时间不准确等额外挑战，导致缺乏表达情感的表现。在现实生活中的通信中，由于说话人的情绪状态或口音的不同，音素的持续时间可能会有所不同（这被称为文本到语音生成的一对多问题）。因此，需要一个先进的语音合成系统来应对这种不可预测性。我们提出了一种端到端的上下文感知文本到语音（TTS）合成系统，该系统从文本输入中推导出表达的情感，并合成音频，专注于情感和说话者特征以实现自然和富有表现力的语音，集成先进的自然语言处理（NLP）和语音合成技术用于实时应用。当与最新TTS模型进行基准测试时，我们的系统还展示了具有竞争力的推理时间性能，使其适合用于实时访问性应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19199v1">PDF</a> </p>
<p><strong>Summary</strong><br>     近期研究表明，尽管有单调文本转语音（TTS）屏幕阅读器和音频叙述等辅助技术，但盲人或视障、低学历人群在使用社交网络时仍面临无障碍挑战。情感语音生成传统上依赖于预期情感和文本输入，存在数据简化导致信息丢失和持续时间不准确等问题，导致情感表达渲染不足。针对真实沟通中音素持续时间的不确定性问题，我们提出了一种端到端的语境感知文本转语音（TTS）合成系统，该系统从文本输入中推导出表达的情感，合成专注于情感和说话人特征的音频，以实现自然和富有表现力的语音。整合先进的自然语言处理（NLP）和语音合成技术，适用于实时应用。此外，我们的系统在推理时间性能上表现出竞争力，使其成为适合实时无障碍应用的理想选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>盲人或视障、低学历人群在使用社交网络时仍面临无障碍挑战。</li>
<li>传统情感语音生成存在数据简化导致的信息丢失和持续时间不准确的问题。</li>
<li>真实沟通中，音素持续时间因说话人的情感状态和口音而有所不同。</li>
<li>提出一种端到端的语境感知TTS合成系统，从文本输入中推导出表达的情感。</li>
<li>系统能合成专注于情感和说话人特征的音频，实现自然和富有表现力的语音。</li>
<li>系统整合了先进的NLP和语音合成技术，适用于实时应用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-954c4baed564d5fc17bf475672a0c733.jpg" align="middle">
</details>




<h2 id="Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model"><a href="#Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model" class="headerlink" title="Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model"></a>Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model</h2><p><strong>Authors:Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, Yike Guo, Wei Xue</strong></p>
<p>Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: <a target="_blank" rel="noopener" href="https://x-codec-audio.github.io/">https://x-codec-audio.github.io</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zhenye234/xcodec">https://github.com/zhenye234/xcodec</a>) </p>
<blockquote>
<p>音频生成领域的最新进展在很大程度上得益于大型语言模型（LLM）的能力。目前关于音频LLM的研究主要集中在增强音频语言模型的架构和规模、利用更大的数据集以及一般采用音频编解码器（如EnCodec）进行音频标记化。然而，这些编解码器最初是为音频压缩而设计的，这可能导致在音频LLM的上下文中性能不佳。我们的研究旨在解决当前音频LLM编解码器的不足，特别是它们在维持生成音频的语义完整性方面所面临的挑战。例如，现有的方法如VALL-E，根据文本转录来条件化声学标记生成，由于声学标记的语义误解，经常导致内容不准确和升高的词错误率（WER），从而产生跳词和错误。为了克服这些问题，我们提出了一种简单而有效的方法，称为X-Codec。X-Codec在残差向量量化（RVQ）阶段之前融入了预训练语义编码器的语义特征，并在RVQ之后引入了语义重建损失。通过增强编解码器的语义能力，X-Codec在语音合成任务中显著降低了WER，并将这些优势扩展到了非语音应用，包括音乐和声音生成。我们在文本到语音、音乐延续和文本到声音的任务实验表明，融入语义信息能大幅提升语言模型在音频生成中的整体性能。我们的代码和演示（演示：<a href="https://x-codec-audio.github.io；代码：https://github.com/zhenye234/xcodec）已经可用。">https://x-codec-audio.github.io；代码：https://github.com/zhenye234/xcodec）已经可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17175v3">PDF</a> </p>
<p><strong>Summary</strong><br>     近期音频生成技术的进展得益于大型语言模型（LLM）的能力。研究集中在增强音频语言模型的架构和规模、利用更大的数据集上，但现有音频LLM的编解码器如EnCodec，是为音频压缩设计的，在音频LLM语境下的表现可能不够理想。针对此问题，提出X-Codec编解码器，融入预训练语义编码器的语义特征，降低词错误率（WER），提升语音合成等任务的性能，并扩展至非语音应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）推动音频生成技术的最新进展。</li>
<li>现有音频LLM编解码器主要关注架构和规模的增强以及大数据集的利用。</li>
<li>EnCodec等编解码器最初是为音频压缩设计的，在音频LLM语境下可能存在性能不足的问题。</li>
<li>X-Codec旨在解决现有音频LLM编解码器的短板，特别是在保持生成音频的语义完整性方面。</li>
<li>现有方法如VALL-E在文本转录条件下生成声学令牌时，由于声学令牌的语义误解，会出现内容不准确和较高的词错误率（WER）。</li>
<li>X-Codec通过融入预训练语义编码器的语义特征，在降低WER方面表现出色，提升语音合成任务的性能。</li>
<li>X-Codec的优势不仅限于语音合成，还扩展至音乐和声效生成等非语音应用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-788cc4d6126dce014156c652e21e827a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8188aaf4247a31c6edf4e05d85eafb07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-beeaa767369c1a338c22f952cb659424.jpg" align="middle">
</details>




<h2 id="Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning"><a href="#Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning" class="headerlink" title="Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning"></a>Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning</h2><p><strong>Authors:Shuai Wang, Zhengyang Chen, Kong Aik Lee, Yanmin Qian, Haizhou Li</strong></p>
<p>Speaker individuality information is among the most critical elements within speech signals. By thoroughly and accurately modeling this information, it can be utilized in various intelligent speech applications, such as speaker recognition, speaker diarization, speech synthesis, and target speaker extraction. In this overview, we present a comprehensive review of neural approaches to speaker representation learning from both theoretical and practical perspectives. Theoretically, we discuss speaker encoders ranging from supervised to self-supervised learning algorithms, standalone models to large pretrained models, pure speaker embedding learning to joint optimization with downstream tasks, and efforts toward interpretability. Practically, we systematically examine approaches for robustness and effectiveness, introduce and compare various open-source toolkits in the field. Through the systematic and comprehensive review of the relevant literature, research activities, and resources, we provide a clear reference for researchers in the speaker characterization and modeling field, as well as for those who wish to apply speaker modeling techniques to specific downstream tasks. </p>
<blockquote>
<p>说话人的个体信息是语音信号中最关键的部分。通过对这部分信息进行全面准确的建模，可以将其应用于各种智能语音应用中，如说话人识别、说话人日记化、语音合成和目标说话人提取等。在本次概述中，我们从理论和实际两个角度对神经方法进行全面的评述，以学习说话人的表征。理论上，我们讨论了从监督到自我监督学习算法的说话人编码器，从独立模型到大型预训练模型，从纯粹的说话人嵌入学习到与下游任务的联合优化，以及向可解释性的努力。实际上，我们系统地检验了鲁棒性和有效性的方法，介绍并比较了该领域的各种开源工具包。通过对相关文献、研究活动和资源的系统和全面回顾，我们为语音特征刻画和建模领域的研究人员，以及那些希望将说话人建模技术应用于特定下游任务的人员提供了清晰的参考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15188v2">PDF</a> Accepted to TASLP</p>
<p><strong>Summary</strong><br>基于神经网络的方法对说话人特征学习进行了全面深入的探讨，从理论角度探讨了各种编码器的性能和应用范围，从实际应用角度进行了模型的鲁棒性和有效性分析，并介绍了相关的开源工具包。该综述为说话人表征和建模领域的研究人员以及希望将说话人建模技术应用于特定下游任务的人员提供了清晰的参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>说话人个体信息在语音信号中占据核心地位，可用于智能语音应用如说话人识别、说话人分时段化、语音合成和目标说话人提取。</li>
<li>神经网络方法被广泛应用于说话人特征学习，包括监督学习和自监督学习算法，独立模型与大型预训练模型等。</li>
<li>说话人嵌入学习不仅关注模型性能，还努力与下游任务联合优化，提升模型的可解释性。</li>
<li>在实际应用中，说话人特征学习模型需要具备鲁棒性和有效性。</li>
<li>综述详细介绍了各种开源工具包，为研究人员提供了丰富的资源。</li>
<li>该综述为说话人表征和建模领域的研究人员提供了清晰的参考和指导。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-865221afda42c7c4c88832efc9e0749b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f86e39c2559437ccd8459e78536e3049.jpg" align="middle">
</details>




<h2 id="TTSDS-–-Text-to-Speech-Distribution-Score"><a href="#TTSDS-–-Text-to-Speech-Distribution-Score" class="headerlink" title="TTSDS – Text-to-Speech Distribution Score"></a>TTSDS – Text-to-Speech Distribution Score</h2><p><strong>Authors:Christoph Minixhofer, Ondřej Klejch, Peter Bell</strong></p>
<p>Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period. </p>
<blockquote>
<p>许多最近发布的文本到语音（TTS）系统产生的音频接近真实语音。然而，随着新架构、方法和数据集的出现，需要重新审视TTS的评估方法，以便理解所获得的实验结果。我们提议从多个因素结合评估合成语音的质量，如语调、说话人身份和清晰度等。我们的方法通过获取每个因素的关联项，并测量它们与真实语音数据集和噪声数据集的差异来评估合成语音对真实语音的模拟程度。我们对2008年至2024年间开发的35个TTS系统进行了基准测试，并展示了我们的得分作为未加权因素平均值与每个时期的人类评估结果存在很强的相关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12707v3">PDF</a> SLT 2024</p>
<p><strong>Summary</strong></p>
<p>近期发布的许多文本转语音（TTS）系统生成的音频已接近真实语音。然而，随着新架构、方法和数据集的出现，需要重新审视TTS的评估方法。本文提出一种评估合成语音质量的方法，综合考虑语调、说话人身份和清晰度等多个因素。该方法通过获取每个因素的相关指标，测量其与真实语音数据集和噪声数据集的差异来评估合成语音对真实语音的模拟程度。作者对2008年至2024年间开发的35个TTS系统进行了基准测试，证明其计算的无权重平均因素得分与各个时期的人类评估结果高度相关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS系统生成的音频已接近真实语音，需要重新审视其评估方法。</li>
<li>评估合成语音质量应综合考虑语调、说话人身份和清晰度等多个因素。</li>
<li>通过测量与真实语音数据集和噪声数据集的差异来评估合成语音的模拟程度。</li>
<li>本文对过去多年开发的TTS系统进行了基准测试。</li>
<li>测试结果表明，综合各因素的平均得分与人类评估结果高度相关。</li>
<li>此评估方法能够为TTS系统的进步和发展提供有力的衡量标准。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d7824eb7f71b474fd46332370f442de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f7ffc0f4e39850306419bd41ad9476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f17fae8c43b1b2b890c4c7493d4ca4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27bb06ebfa2bbd737eb3089e0df3d026.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89e6fbc1e8abf28dba60aa35c2760d78.jpg" align="middle">
</details>




<h2 id="Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers"><a href="#Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers" class="headerlink" title="Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers"></a>Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers</h2><p><strong>Authors:Krzysztof Choromanski, Arijit Sehanobish, Somnath Basu Roy Chowdhury, Han Lin, Avinava Dubey, Tamas Sarlos, Snigdha Chaturvedi</strong></p>
<p>We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular low displacement rank) for integrating tensor fields defined on weighted trees. Several applications of the resulting fast tree-field integrators (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) Topological Transformers (TTs) (Choromanski et al., 2022) for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as three extra learnable parameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains. Importantly, most of FTFIs are exact methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide 5.7-13x speedups. We also provide an extensive theoretical analysis of our methods. </p>
<blockquote>
<p>我们基于结构化矩阵理论（尤其是低位移秩）提出了一种新的快速多项式对数线性算法，用于对定义在加权树上的张量场进行积分。展示了由此产生的快速树场积分器（FTFIs）的几个应用，包括（a）树度量逼近图度量，（b）图分类，（c）网格建模，以及最后（d）用于图像的拓扑转换器（TTs）（Choromanski等人，2022年）。针对拓扑转换器，我们提出了具有三个额外可学习参数的新型相对位置编码（RPE）掩码机制，带来了1.0-1.5%的准确率提升。重要的是，大多数FTFIs是精确方法，因此在数值上与暴力求解等效。当应用于具有数千个节点的图时，这些精确算法可提供5.7-13倍的加速。我们还对所用方法进行了广泛的理论分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15881v2">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong><br>     基于结构化矩阵理论（特别是低位移秩）提出的新型快速多项式对数线性算法，用于整合定义在加权树上的张量场。该算法包含多种应用场景，如近似图指标与树指标、图分类、网格建模和图像拓扑变换器等。针对拓扑变换器，我们提出了具有更少可学习参数的新相对位置编码掩码机制，带来了额外的准确性提升。此外，大多数FTFIs算法是精确方法，在应用于具有数千个节点的图形时，提供了高达13倍的加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于结构化矩阵理论的新型快速多项式对数线性算法用于整合张量场。</li>
<li>FTFIs算法适用于多种应用场景，包括图与树指标的近似、图分类、网格建模和图像拓扑变换器。</li>
<li>针对拓扑变换器，提出了具有更少可学习参数的新相对位置编码掩码机制。</li>
<li>新机制带来了额外的准确性提升。</li>
<li>大多数FTFIs算法是精确的，等价于暴力计算方法。</li>
<li>在应用于具有数千个节点的图形时，FTFIs算法提供了显著的加速效果。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-631340e9e632a5f32dd7eabce23fea29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fc5affe036c5a7c010539a04991aed1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad831467d9c313e96be11881ef19e14.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>随着文本转语音（TTS）的进步带来的各种威胁，对合成语音的可靠追踪需求日益迫切。然而，当前的任务处理方法是在生成音频后单独添加水印，这一过程既影响了语音质量，也影响了水印的不易察觉性。此外，这些方法在稳健性和灵活性方面也存在局限。为了解决这些问题，我们提出了TraceableSpeech，这是一种新型TTS模型，能够直接生成带水印的语音，提高了水印的不易察觉性和语音质量。此外，我们设计了帧级的水印印制和提取技术，提高了对抗重新拼接攻击的稳健性和操作的时序灵活性。实验结果表明，TraceableSpeech在不易察觉性、语音质量和抵抗重新拼接攻击方面，优于VALL-E或HiFicodec等单独使用WavMark的强基线。它还可以应用于各种时长的语音。代码可在<a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjzser/TraceableSpeech找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>文本到语音（TTS）的进步带来了多种威胁，需要进行可靠的追踪合成语音。然而，现有的追踪方法通过在生成后单独添加水印到音频中，这种方法既影响语音质量又影响水印的不可察觉性。为解决这些问题，我们提出了TraceableSpeech，这是一种新型的TTS模型，可直接生成带水印的语音，提高了水印的不可察觉性和语音质量。我们还设计了帧级的水印印入和提取方法，提高了对抗拼接攻击的稳健性和操作的时效性。实验结果表明，TraceableSpeech在不可察觉性、语音质量和抗拼接攻击方面优于使用WavMark的VALL-E或HiFicodec等强基线模型，且适用于各种时长的语音。相关代码已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS的进步带来了追踪合成语音的必要性，因为存在多种潜在威胁。</li>
<li>现有追踪方法通过生成后添加水印到音频中，存在语音质量和水印不可察觉性的问题。</li>
<li>TraceableSpeech是一种新型的TTS模型，能直接生成带水印的语音，提高水印的不可察觉性和语音质量。</li>
<li>TraceableSpeech实现了帧级的水印印入和提取，提高了对抗拼接攻击的稳健性。</li>
<li>TraceableSpeech在不可察觉性、语音质量和抗拼接攻击方面优于现有基线模型。</li>
<li>TraceableSpeech适用于各种时长的语音。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis"><a href="#Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis" class="headerlink" title="Style Mixture of Experts for Expressive Text-To-Speech Synthesis"></a>Style Mixture of Experts for Expressive Text-To-Speech Synthesis</h2><p><strong>Authors:Ahad Jawaid, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman</strong></p>
<p>Recent advances in style transfer text-to-speech (TTS) have improved the expressiveness of synthesized speech. However, encoding stylistic information (e.g., timbre, emotion, and prosody) from diverse and unseen reference speech remains a challenge. This paper introduces StyleMoE, an approach that addresses the issue of learning averaged style representations in the style encoder by creating style experts that learn from subsets of data. The proposed method replaces the style encoder in a TTS framework with a Mixture of Experts (MoE) layer. The style experts specialize by learning from subsets of reference speech routed to them by the gating network, enabling them to handle different aspects of the style space. As a result, StyleMoE improves the style coverage of the style encoder for style transfer TTS. Our experiments, both objective and subjective, demonstrate improved style transfer for diverse and unseen reference speech. The proposed method enhances the performance of existing state-of-the-art style transfer TTS models and represents the first study of style MoE in TTS. </p>
<blockquote>
<p>在文本到语音（TTS）的风格迁移方面最近的进展提高了合成语音的表现力。然而，从多样且未知的参考语音中编码风格信息（如音质、情感和语调）仍然是一个挑战。本文介绍了StyleMoE方法，通过创建从数据子集学习的风格专家来解决风格编码器中学习平均风格表示的问题。所提出的方法用混合专家（MoE）层替换了TTS框架中的风格编码器。风格专家通过从路由到它们的参考语音子集学习来专业化，使它们能够处理风格空间的不同方面。因此，StyleMoE提高了风格编码器在风格迁移TTS中的风格覆盖率。我们的客观和主观实验都证明了在多样且未知的参考语音中风格迁移的改进。所提出的方法提高了现有最先进的风格迁移TTS模型的性能，并且是TTS中风格MoE的首次研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03637v2">PDF</a> Published in Audio Imagination: NeurIPS 2024 Workshop</p>
<p><strong>Summary</strong></p>
<p>近期文本转语音（TTS）的风格迁移技术取得了进展，提高了合成语音的表达性。然而，从多样且未知的参考语音中编码风格信息（如音质、情感和语调）仍是一个挑战。本文提出了StyleMoE方法，通过创建风格专家来解决风格编码器中学习平均风格表示的问题。该方法用专家混合（MoE）层替换TTS框架中的风格编码器。风格专家通过从路由到它们的参考语音子集学习专业知识，能够处理风格空间的不同方面。因此，StyleMoE提高了风格编码器的风格覆盖能力，改善了多样且未知参考语音的风格迁移。实验证明，该方法在主观和客观上都提高了风格迁移的效果，并增强了现有最先进的风格迁移TTS模型的性能。这是MoE在TTS中的首次研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期TTS的风格迁移技术提高了合成语音的表达性。</li>
<li>从多样且未知的参考语音中编码风格信息是一个挑战。</li>
<li>StyleMoE方法通过创建风格专家来解决这一挑战。</li>
<li>风格专家通过从路由到它们的参考语音子集学习专业知识。</li>
<li>StyleMoE提高了风格编码器的风格覆盖能力。</li>
<li>实验证明StyleMoE在主观和客观上都提高了风格迁移的效果。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8071a305826f1eb16863c0591528d7a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6d6716de85f2a3d1185b60e764b04a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f2d3c9d4d3c06e0b1f2bb0a2e078b5.jpg" align="middle">
</details>




<h2 id="DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models"><a href="#DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models" class="headerlink" title="DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models"></a>DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models</h2><p><strong>Authors:Jingyi Chen, Ju-Seung Byun, Micha Elsner, Andrew Perrault</strong></p>
<p>Recent advancements in generative models have sparked a significant interest within the machine learning community. Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech. Studies such as those by Lee et al. (2023), Black et al. (2023), Wang et al. (2023), and Fan et al. (2024) illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis. However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models. In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system (Saeki et al., 2022) as a proxy loss. We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model (Mittag et al., 2021) and human preference experiments for further evaluation. Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios. </p>
<blockquote>
<p>近年来，生成模型的新进展在机器学习领域引起了极大的兴趣。特别是扩散模型在图像和语音合成方面表现出了显著的能力。Lee等人（2023年）、Black等人（2023年）、Wang等人（2023年）和Fan等人（2024年）的研究表明，强化学习与人类反馈（RLHF）可以增强扩散模型在图像合成方面的性能。然而，由于这些模型与语音合成中所用模型的结构差异，尚不清楚RLHF是否也能同样有益于语音合成模型。在本文中，我们探索了RLHF在基于扩散的文本到语音合成中的实际应用，利用东京大学SaruLab的MOS预测系统所预测的均值意见分（MOS）作为代理损失。我们介绍了扩散模型损失引导RL策略优化（DLPO），并将其与其他RLHF方法进行比较，进一步采用NISQA语音质量和自然度评估模型以及人类偏好实验进行评估。我们的结果表明，RLHF可以增强基于扩散的文本到语音合成模型的性能，而且DLPO能更好地改善扩散模型生成自然、高质量语音音频的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14632v2">PDF</a> </p>
<p><strong>Summary</strong><br>     近期生成模型进展显著，特别是扩散模型在图像和语音合成方面表现出卓越能力。研究表明强化学习与人类反馈（RLHF）能提升扩散模型在图像合成方面的表现。本文探索了RLHF在基于扩散的文本到语音合成中的实际应用，采用东京大学SaruLab的MOS预测系统作为代理损失。通过对比其他RLHF方法和采用NISQA语音质量和自然度评估模型的人类偏好实验，发现RLHF能提升基于扩散的文本到语音合成模型的性能，特别是DLPO方法在生成自然高质量语音方面表现更佳。</p>
<p><strong>Key Takeaways</strong><br>     1. 生成模型领域出现新的重要进展，扩散模型在图像和语音合成上具备出色性能。<br>     2. 强化学习与人类反馈（RLHF）可以提升扩散模型的表现，尤其是在图像合成方面已有研究证实。<br>     3. 本文探索了RLHF在基于扩散的文本到语音合成中的应用。<br>     4. 采用东京大学SaruLab的MOS预测系统作为代理损失进行模型优化。<br>     5. 对比了不同的RLHF方法和采用NISQA语音评估模型的人类偏好实验。<br>     6. 结果显示RLHF能提升基于扩散的文本到语音合成模型的性能。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c14030629a7cc9a613b4afe409c46451.jpg" align="middle">
</details>




<h2 id="VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization"><a href="#VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization" class="headerlink" title="VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization"></a>VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization</h2><p><strong>Authors:Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai</strong></p>
<p>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the <a target="_blank" rel="noopener" href="https://vimtextspotter.github.io/">https://VimTextSpotter.github.io</a>. </p>
<blockquote>
<p>文本识别涉及从图像或视频序列中提取文本信息，在跨域适应方面面临挑战，如图像到图像和图像到视频的推广。在本文中，我们介绍了一种新方法，称为VimTS。它通过实现不同任务之间的更好协同作用，增强了模型的推广能力。一般来说，我们提出了一个提示查询生成模块和任务感知适配器，有效地将原始的单任务模型转换为适合图像和视频场景的多任务模型，并且增加了极少的参数。提示查询生成模块促进了不同任务之间的显式交互，而任务感知适配器帮助模型动态学习每个任务的适当特征。此外，为了以更低的成本使模型能够学习时间序列信息，我们利用内容变形场（CoDeF）算法提出了一个合成视频文本数据集（VTD-368k）。值得注意的是，我们的方法在六个跨域基准测试上的平均表现优于最新方法约2.6%，如TT到IC15、CTW1500到TT和TT到CTW1500等。对于视频级别的跨域适应，我们的方法甚至在ICDAR2015视频和DSText v2上超越了之前的端到端视频识别方法，在MOTA指标上平均提高了约5.5%，并且仅使用图像级别的数据。我们还进一步证明了现有的大型多模态模型在生成跨域场景文本识别方面存在局限性，相比之下，我们的VimTS模型需要更少的参数和数据。代码和数据集将在VimTextSpotter.github.io上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19652v4">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种新的方法VimTS，通过实现不同任务之间的更好协同作用，提高了模型的泛化能力。为此，本文提出了Prompt Queries生成模块和任务感知适配器，以将原始单任务模型有效地转换为适合图像和视频场景的多任务模型，且只需极少量的额外参数。Prompt Queries生成模块促进了不同任务之间的显式交互，而任务感知适配器帮助模型动态学习每个任务的适当特征。此外，为了以较低的成本使模型能够学习时间序列信息，本文利用内容变形场算法提出了一个合成视频文本数据集（VTD-368k）。在六个跨域基准测试中，如TT到IC15等，本文的方法优于当前最先进的方法平均提高了2.6%。对于视频级别的跨域适应，本文的方法在ICDAR2015视频和DSText v2上甚至超过了之前的端到端视频识别方法，在MOTA指标上平均提高了5.5%，仅使用图像级别的数据。此外，本文还指出了现有大型多模态模型在跨域场景文本识别方面的局限性，与VimTS模型相比，其需要更少的参数和数据。相关代码和数据集将在<a target="_blank" rel="noopener" href="https://vimtextspotter.github.io上提供./">https://VimTextSpotter.github.io上提供。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>VimTS方法通过实现不同任务之间的协同作用，增强了模型的泛化能力。</li>
<li>提出了Prompt Queries生成模块和任务感知适配器，将单任务模型转换为多任务模型，适用于图像和视频场景，且参数增加少。</li>
<li>Prompt Queries生成模块促进任务间的交互，而任务感知适配器使模型能动态学习每个任务的特征。</li>
<li>利用内容变形场算法提出了合成视频文本数据集VTD-368k，用于学习时间序列信息。</li>
<li>VimTS方法在多个跨域基准测试中表现优越，平均提高2.6%。</li>
<li>在视频级别的跨域适应方面，VimTS方法显著超越了现有方法，平均提高5.5%。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f6e7e98614248d3c744cdf84eac7f35e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20ddf5ee8d45f8d89b38acbfb080a543.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef497ce9045e2a79784ed651cb501142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a70a938a0b3b4ad66f5e15abd6215c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-732e80b5850a49baa56fae267715864a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f08976df42514822cf5c9d5706cf73a.jpg" align="middle">
</details>




<h2 id="Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting"><a href="#Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting" class="headerlink" title="Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting"></a>Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting</h2><p><strong>Authors:Haolin Chen, Philip N. Garner</strong></p>
<p>We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model’s inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones. </p>
<blockquote>
<p>我们主要受到文本到语音合成模型的适应性的激励；然而，我们认为更通用的参数有效微调（PEFT）是执行此类适应性的适当框架。然而，灾难性遗忘仍然是PEFT的一个问题，会损害预训练模型的固有功能。我们证明，只要可以计算微调层的参数变化是可微分的，现有的贝叶斯学习技术就可以应用于PEFT以防止灾难性遗忘。在一系列语言建模和语音合成任务的实验中，我们利用建立的拉普拉斯近似方法，包括对角线和克罗内克因子分解方法，对低秩适应（LoRA）进行正则化PEFT，并比较它们在预训练知识保留方面的性能。结果表明，我们的方法能够克服灾难性遗忘，同时不会降低微调性能，并且使用克罗内克因子分解近似比使用对角线近似能更好地保留预训练知识。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.12220v3">PDF</a> </p>
<p><strong>Summary</strong><br>     本文主要探讨了文本到语音合成模型的适应性调整问题，并提出利用更通用的参数高效微调（PEFT）框架进行此类调整。然而，PEFT存在灾难性遗忘问题，会损害预训练模型的固有功能。作者展示了现有的贝叶斯学习技术可以应用于PEFT以防止灾难性遗忘，只要能够计算微调层的参数差分转移。作者在一系列语言建模和语音合成任务实验中使用Laplace近似方法（包括对角线和Kronecker分解方法）对PEFT进行正则化，并与低秩适配（LoRA）比较其保存预训练知识的效果。结果表明，该方法能克服灾难性遗忘，同时不影响微调性能，且使用Kronecker分解近似比使用对角线方法更好地保留预训练知识。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章主要讨论文本到语音合成模型的适应性调整问题。</li>
<li>参数高效微调（PEFT）是一个适当的框架来进行此类调整。</li>
<li>PEFT存在灾难性遗忘问题，损害预训练模型的固有功能。</li>
<li>现有的贝叶斯学习技术可以防止PEFT中的灾难性遗忘，前提是能计算微调层的参数差分转移。</li>
<li>作者使用Laplace近似方法（包括对角线和Kronecker分解）来正则化PEFT，并与低秩适配（LoRA）比较。</li>
<li>实验结果显示，使用Kronecker分解近似可以更好地保留预训练知识。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-91d17a7c8f1b6a0625fecb03c05952c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6417acfdbfc3b9b7a1ef71e8297497.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3793a3389ce1451686ef4ed95b62ef38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-035ba61f5cf002bdc4724210bc479027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-111842636e0d3e93e65cd94d73b190bc.jpg" align="middle">
</details>




<h2 id="Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation"><a href="#Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation" class="headerlink" title="Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation"></a>Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation</h2><p><strong>Authors:Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Soyoon Kim, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Jung-Woo Ha, Sungroh Yoon, Kang Min Yoo</strong></p>
<p>Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm">https://github.com/naver-ai/usdm</a>. </p>
<blockquote>
<p>近期的研究工作显示，在扩大大型语言模型（LLM）的能力以直接理解和合成语音方面取得了有前景的结果。然而，基于LLM的建模口语对话的策略仍然难以捉摸，需要进一步的调查。本文介绍了一个全面的语音文本LLM框架，即统一口语对话模型（USDM），旨在生成与给定输入语音相关的连贯口语响应，而无需依赖自动语音识别（ASR）或文本到语音（TTS）系统。我们已经验证了包含主要含有语义信息的语音符号中的语调，并使用这个基础来构建一个注入语调的语音文本模型。此外，我们提出了一种通用的语音文本预训练方案，该方案提高了跨模式语义的捕获能力.为了构建USDM，我们使用多步骤口语对话模板对语音文本模型进行微调，该模板刺激底层LLM所展现的推理能力。在DailyTalk数据集上的自动和人类评估表明，我们的方法有效地生成了自然的口语响应，超越了先前的和级联的基准测试。我们的代码和检查点位于<a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm%E3%80%82">https://github.com/naver-ai/usdm。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.05706v3">PDF</a> NeurIPS 2024, Project Page: <a target="_blank" rel="noopener" href="https://unifiedsdm.github.io/">https://unifiedsdm.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种统一的口语对话模型（USDM），能够直接理解和合成语音，生成与输入语音相关的连贯口语响应，具有自然发生的韵律特征，无需依赖自动语音识别（ASR）或文本到语音（TTS）系统。通过构建包含主要语义信息的韵律语音令牌，该模型实现了韵律注入的语音文本模型。此外，还提出了一种通用的语音文本预训练方案，提高了跨模态语义的捕获能力。通过在日常对话数据集上的自动和人类评估表明，该方法生成的响应自然流畅，超越了之前的级联基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在直接理解和合成语音方面展现出有前景的结果，但仍需进一步探索建模口语对话的策略。</li>
<li>USDM模型旨在生成与输入语音相关的连贯口语响应，并具备自然韵律特征。</li>
<li>USDM模型通过包含主要语义信息的韵律语音令牌实现韵律注入。</li>
<li>提出了一种通用的语音文本预训练方案，增强跨模态语义捕获。</li>
<li>通过多步口语对话模板对USDM进行微调，刺激LLM的推理能力。</li>
<li>在日常对话数据集上的评估显示，USDM生成的响应自然流畅，超越级联基线方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6798c63bc786ca1de2c782f0be72cea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dc65923495bfd0c6671bc48d3bac96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df6d275e2fd773757f0bfd6c831df15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49f6c367b897312213c20c2022c1d95.jpg" align="middle">
</details>




<h2 id="Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer"><a href="#Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer" class="headerlink" title="Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer"></a>Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer</h2><p><strong>Authors:Peter Ochieng</strong></p>
<p>Diffusion based vocoders have been criticised for being slow due to the many steps required during sampling. Moreover, the model’s loss function that is popularly implemented is designed such that the target is the original input $x_0$ or error $\epsilon_0$. For early time steps of the reverse process, this results in large prediction errors, which can lead to speech distortions and increase the learning time. We propose a setup where the targets are the different outputs of forward process time steps with a goal to reduce the magnitude of prediction errors and reduce the training time. We use the different layers of a neural network (NN) to perform denoising by training them to learn to generate representations similar to the noised outputs in the forward process of the diffusion. The NN layers learn to progressively denoise the input in the reverse process until finally the final layer estimates the clean speech. To avoid 1:1 mapping between layers of the neural network and the forward process steps, we define a skip parameter $\tau&gt;1$ such that an NN layer is trained to cumulatively remove the noise injected in the $\tau$ steps in the forward process. This significantly reduces the number of data distribution recovery steps and, consequently, the time to generate speech. We show through extensive evaluation that the proposed technique generates high-fidelity speech in competitive time that outperforms current state-of-the-art tools. The proposed technique is also able to generalize well to unseen speech. </p>
<blockquote>
<p>基于扩散的vocoder由于采样过程中需要多个步骤，所以被批评速度较慢。此外，流行的实现中的模型的损失函数的设计目标是原始输入x_0或误差ε_0。在反向过程的早期步骤中，这会导致较大的预测误差，可能导致语音失真并增加学习时间。我们提出了一种设置，目标为正向过程时间步骤的不同输出，旨在减小预测误差的幅度并缩短训练时间。我们使用神经网络（NN）的不同层执行去噪，通过训练它们学习生成与扩散正向过程中的噪声输出相似的表示。NN层学习在反向过程中逐步对输入进行去噪，直到最终层估计出干净语音。为了避免神经网络层与正向过程步骤之间的1：1映射，我们定义了一个跳过参数τ&gt;1，使得NN层被训练累积地去除在正向过程的τ步中注入的噪声。这显著减少了数据分布恢复步骤的数量，因此也缩短了生成语音的时间。我们通过广泛评估证明，所提出的技术在竞争时间内生成了高保真度的语音，优于当前最先进的工具。所提出的技术也能很好地推广未见过的语音。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09652v3">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于扩散的vocoder改进方案，通过调整目标设定和神经网络层的设计，减少了预测误差和训练时间，生成了高质量、高保真度的语音，同时具有良好的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散vocoder因采样过程中步骤繁多而速度较慢，本文提出改进方案。</li>
<li>传统模型损失函数设计以原始输入或误差为目标，导致早期时间步骤反向过程中的预测误差较大，本文改变目标设定为前向过程时间步骤的输出，以减少预测误差和学习时间。</li>
<li>利用神经网络的不同层进行去噪训练，学习生成与扩散前向过程中的噪声输出相似的表示。</li>
<li>神经网络层逐层去噪，最终层估计清洁语音。</li>
<li>引入跳过参数τ&gt;1，使神经网络层累计去除在τ步中注入的噪声，减少数据分布恢复步骤和时间。</li>
<li>改进方案生成的高保真语音在竞争时间内表现出优异性能，优于现有最先进的工具。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c25b2ace83cc3057ff3a08e435a27f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e96c9504cca1fdb2836c0820b9d1871.jpg" align="middle">
</details>




<h2 id="Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds"><a href="#Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds" class="headerlink" title="Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds"></a>Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds</h2><p><strong>Authors:Androniki Dimitriou, Daniel G. Figueroa, Bryan Zaldivar</strong></p>
<p>We apply state-of-the-art, likelihood-free statistical inference (machine-learning-based) techniques for reconstructing the spectral shape of a gravitational wave background (GWB). We focus on the reconstruction of an arbitrarily shaped signal by the LISA detector, but the method can be easily extended to either template-dependent signals, or to other detectors, as long as a characterisation of the instrumental noise is available. As proof of the technique, we quantify the ability of LISA to reconstruct signals of arbitrary spectral shape (${\it blind}$ reconstruction), considering a diversity of frequency profiles, and including astrophysical backgrounds in some cases. As a teaser of how the method can reconstruct signals characterised by a parameter-dependent template (${\it template}$ reconstruction), we present a dedicated study for power-law signals. While our technique has several advantages with respect to traditional MCMC methods, we validate it with the latter for concrete cases. This work opens the door for both fast and accurate Bayesian parameter estimation of GWBs, with essentially no computational overhead during the inference step. Our set of tools are integrated into the package ${\tt GWBackFinder}$, which is publicly available in <a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder">https://github.com/AndronikiDimitriou/GWBackFinder</a>. </p>
<blockquote>
<p>我们采用最先进的无可能性统计推断（基于机器学习）技术来重建引力波背景（GWB）的频谱形状。我们专注于利用LISA探测器对任意形状的信号进行重建，但只要有仪器噪声的特征，该方法可轻易扩展到模板依赖的信号或其他探测器。作为该技术的证明，我们量化了LISA重建任意频谱形状信号的能力（盲重建），考虑了各种频率分布，并在某些情况下包括天文背景。作为该方法可以重建参数依赖模板信号的一个提示（模板重建），我们为幂律信号进行了专项研究。虽然我们的技术相对于传统的MCMC方法有许多优势，但对于具体案例，我们还是用后者对其进行了验证。这项工作为GWBs的快速和精确贝叶斯参数估计打开了大门，在推断步骤中几乎没有计算开销。我们的工具集已集成到GWBackFinder软件包中，可在<a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AndronikiDimitriou/GWBackFinder公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08430v3">PDF</a> Published in JCAP. 29 pages plus appendices and references, 12   figures</p>
<p><strong>Summary</strong></p>
<p>基于最新技术，采用无需似然函数的统计推断（基于机器学习）方法，对引力波背景（GWB）的频谱形状进行重建。研究重点为LISA探测器对任意形状信号的重建，该方法可轻易扩展至模板依赖信号或其他探测器，只要具备仪器噪声特征即可。对中国技术进行了量化评估，展示了其对任意频谱形状信号的重建能力，包括多种频率分布，并在某些情况下考虑天文背景。还为参数依赖模板的信号重建提供了专项研究，重点研究了幂律信号。该技术相对于传统的MCMC方法具有多个优势，并在具体案例中进行验证。本研究为快速准确的GWB贝叶斯参数估计打开了大门，推理步骤几乎无计算开销。相关工具集已集成到公开可用的GWBackFinder软件包中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>采用前沿的机器学习技术，进行无需似然函数的统计推断，以重建引力波背景的频谱形状。</li>
<li>研究的焦点是LISA探测器对任意形状信号的重建能力。</li>
<li>该方法可灵活扩展到模板依赖的信号和其他探测器，前提是有仪器噪声的特征描述。</li>
<li>技术验证包括对任意频谱形状信号的重建能力，涵盖了多种频率分布，并考虑了天文背景。</li>
<li>提供了参数依赖模板的信号重建的专项研究，以幂律信号为例。</li>
<li>与传统MCMC方法相比，该技术具有显著优势，并在具体案例中得到验证。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b71214c325a1d33137c11a8085cf040a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f111152cd7161ef71d176cd5aef745f9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2024-12-12  CoPrUS Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
