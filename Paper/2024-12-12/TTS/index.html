<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œä»£ç ï¼‰å’Œè¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨è¯­è¨€å»ºæ¨¡ï¼ˆLatentLMï¼‰ï¼Œå®ƒåˆ©ç”¨å› æœTransformeræ— ç¼é›†æˆè¿ç»­å’Œç¦»æ•£æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è¿ç»­æ•°æ®è¡¨ç¤ºä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œæ‰©æ•£æ¥è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†Ïƒ-VAEï¼Œä»¥è§£å†³æ–¹å·®å´©æºƒçš„æŒ‘æˆ˜ï¼Œè¿™å¯¹äºè‡ªå›å½’å»ºæ¨¡è‡³å…³é‡è¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLatentLMåœ¨å¤šç§æ¨¡æ€ä¸­çš„åº”ç”¨éƒ½å¾ˆæœ‰æ•ˆã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒLatentLMåœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢éƒ½è¶…è¶Šäº†Diffusion Transformersã€‚å½“é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼ŒLatentLMæä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£ï¼Œç»Ÿä¸€äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰©å¤§è®­ç»ƒä»¤ç‰Œè§„æ¨¡æ–¹é¢ï¼ŒLatentLMä¸Transfusionå’Œå‘é‡é‡åŒ–æ¨¡å‹ç›¸æ¯”å–å¾—äº†æœ‰åˆ©çš„è¡¨ç°ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ–¹é¢ï¼ŒLatentLMåœ¨è¯´è¯äººç›¸ä¼¼æ€§å’Œç¨³å¥æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„VALL-E 2æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦10å€æ›´å°‘çš„è§£ç æ­¥éª¤ã€‚è¿™äº›ç»“æœè¯æ˜äº†LatentLMåœ¨æ¨è¿›å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ–¹é¢æ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Latent Language Modelingï¼ˆLatentLMï¼‰æ˜¯ä¸€ç§å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤„ç†ç¦»æ•£æ•°æ®å’Œè¿ç»­æ•°æ®ã€‚å®ƒé€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¡¨ç¤ºè¿ç»­æ•°æ®ä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶å¼•å…¥next-tokenæ‰©æ•£è¿›è¡Œè¿™äº›å‘é‡çš„è‡ªå›å½’ç”Ÿæˆã€‚æ­¤å¤–ï¼ŒLatentLMè§£å†³äº†æ–¹å·®æ¶ˆå¤±é—®é¢˜å¹¶æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒLatentLMåœ¨å„ç§æ¨¡æ€ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ•ˆæœï¼Œè¶…è¶ŠDiffusion Transformersä¸Transfusionç­‰æ¨¡å‹ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ–¹é¢ï¼ŒLatentLMä»¥è¾ƒå°‘çš„è§£ç æ­¥éª¤è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚å› æ­¤ï¼ŒLatentLMæ˜¯ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Language Modeling (LatentLM) èƒ½ç»Ÿä¸€å¤„ç†ç¦»æ•£å’Œè¿ç»­æ•°æ®ã€‚</li>
<li>ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¡¨ç¤ºè¿ç»­æ•°æ®ä¸ºæ½œåœ¨å‘é‡ã€‚</li>
<li>Next-tokenæ‰©æ•£ç”¨äºè‡ªå›å½’ç”Ÿæˆæ½œåœ¨å‘é‡ã€‚</li>
<li>LatentLMè§£å†³äº†æ–¹å·®æ¶ˆå¤±é—®é¢˜ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å„ç§æ¨¡æ€ä¸Šï¼ŒLatentLMè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šå…¶ä»–æ¨¡å‹å¦‚Diffusion Transformerså’ŒTransfusionç­‰ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ–¹é¢ï¼ŒLatentLMæ•ˆæœæ˜¾è‘—ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”éœ€è¦æ›´å°‘çš„è§£ç æ­¥éª¤ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Zero-Shot-Mono-to-Binaural-Speech-Synthesis"><a href="#Zero-Shot-Mono-to-Binaural-Speech-Synthesis" class="headerlink" title="Zero-Shot Mono-to-Binaural Speech Synthesis"></a>Zero-Shot Mono-to-Binaural Speech Synthesis</h2><p><strong>Authors:Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn, Eliya Nachmani</strong></p>
<p>We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ZeroBASæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»å•å£°é“éŸ³é¢‘å½•åˆ¶å’Œä½ç½®ä¿¡æ¯åˆæˆåŒå£°é“éŸ³é¢‘çš„ç¥ç»æ–¹æ³•ï¼Œæ— éœ€å¯¹ä»»ä½•åŒå£°é“æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å‘å¸ƒçš„ä»é›¶å¼€å§‹ç¥ç»æ–¹æ³•ç”¨äºå•å£°é“åˆ°åŒå£°é“çš„éŸ³é¢‘åˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºæºä½ç½®çš„æ— éœ€å‚æ•°å‡ ä½•æ—¶é—´æ‰­æ›²å’ŒæŒ¯å¹…ç¼©æ”¾è¶³ä»¥è·å¾—åˆå§‹åŒå£°é“åˆæˆï¼Œå¯ä»¥é€šè¿‡è¿­ä»£åº”ç”¨é¢„è®­ç»ƒçš„é™å™ªç¼–è§£ç å™¨è¿›è¡Œç»†åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¿™å¯¼è‡´äº†è·¨æˆ¿é—´æ¡ä»¶çš„æ³›åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„æ•°æ®é›†TUT Mono-to-Binauralæ¥è¡¡é‡è¿™ä¸€ç‚¹ï¼Œä»¥è¯„ä¼°æœ€å…ˆè¿›å•å£°é“åˆ°åŒå£°é“åˆæˆæ–¹æ³•åœ¨æœªè§æ¡ä»¶ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„é›¶æ ·æœ¬æ–¹æ³•ä¸æ ‡å‡†å•å£°é“åˆ°åŒå£°é“æ•°æ®é›†ä¸Šçš„æœ‰ç›‘ç£æ–¹æ³•åœ¨æ„ŸçŸ¥ä¸Šç›¸å½“ï¼Œç”šè‡³åœ¨æˆ‘ä»¬çš„ç¦»ç¾¤TUT Mono-to-Binauralæ•°æ®é›†ä¸Šè¶…è¿‡äº†å®ƒä»¬ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†é¢„è®­ç»ƒçš„ç”ŸæˆéŸ³é¢‘æ¨¡å‹å’Œé›¶æ ·æœ¬å­¦ä¹ åœ¨è§£é”ç¨³å¥çš„åŒå£°é“éŸ³é¢‘åˆæˆæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é›¶åŸºéŸ³é¢‘åˆæˆæ–¹æ³•ï¼ˆZeroBASï¼‰æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯ä»å•å£°é“éŸ³é¢‘å½•åˆ¶å’Œä½ç½®ä¿¡æ¯åˆæˆåŒå£°é“éŸ³é¢‘çš„ç¥ç»æ–¹æ³•ã€‚è¿™æ˜¯é¦–æ¬¡å‘è¡¨çš„æ— è®­ç»ƒåŒå£°é“éŸ³é¢‘åˆæˆçš„ç¥ç»æ–¹æ³•ã€‚é€šè¿‡åŸºäºæºä½ç½®çš„å‚æ•°åŒ–å‡ ä½•æ—¶é—´æ‰­æ›²å’ŒæŒ¯å¹…ç¼©æ”¾ï¼Œå¯ä»¥å®ç°åˆæ­¥çš„åŒå£°é“åˆæˆï¼Œå¹¶é€šè¿‡è¿­ä»£åº”ç”¨é¢„è®­ç»ƒçš„é™å™ªç¼–è§£ç å™¨è¿›è¡Œæ”¹è¿›ã€‚æ­¤æ–¹æ³•å¯åœ¨ä¸åŒæˆ¿é—´æ¡ä»¶ä¸‹è¿›è¡Œæ¨å¹¿ï¼Œå¹¶é€šè¿‡å¼•å…¥æ–°çš„æ•°æ®é›†TUT Mono-to-Binauralæ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚é›¶åŸºæ–¹æ³•åœ¨æ ‡å‡†å•å£°é“åˆ°åŒå£°é“æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸æœ‰ç›‘ç£æ–¹æ³•ç›¸å½“ï¼Œç”šè‡³åœ¨æˆ‘ä»¬çš„TUT Mono-to-Binauralæ•°æ®é›†ä¸Šè¡¨ç°æ›´ä½³ã€‚ç»“æœçªæ˜¾äº†é¢„è®­ç»ƒç”ŸæˆéŸ³é¢‘æ¨¡å‹å’Œé›¶æ ·æœ¬å­¦ä¹ çš„æ½œåŠ›ï¼Œå¯è§£é”ç¨³å¥çš„åŒå£°é“éŸ³é¢‘åˆæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZeroBASæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„åŒå£°é“éŸ³é¢‘åˆæˆæ–¹æ³•ï¼Œå¯ä»å•å£°é“éŸ³é¢‘å’Œä½ç½®ä¿¡æ¯åˆæˆåŒå£°é“éŸ³é¢‘ã€‚</li>
<li>å‚æ•°åŒ–çš„å‡ ä½•æ—¶é—´æ‰­æ›²å’ŒæŒ¯å¹…ç¼©æ”¾æ˜¯å®ç°åˆæ­¥åŒå£°é“åˆæˆçš„å…³é”®æ­¥éª¤ã€‚</li>
<li>é€šè¿‡è¿­ä»£åº”ç”¨é¢„è®­ç»ƒçš„é™å™ªç¼–è§£ç å™¨ï¼Œå¯ä»¥æ”¹è¿›åˆæ­¥åˆæˆçš„åŒå£°é“éŸ³é¢‘è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨ä¸åŒæˆ¿é—´æ¡ä»¶ä¸‹è¿›è¡Œæ¨å¹¿ã€‚</li>
<li>é›¶åŸºæ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸æœ‰ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</li>
<li>åœ¨æ–°çš„æ•°æ®é›†TUT Mono-to-Binauralä¸Šï¼Œé›¶åŸºæ–¹æ³•çš„æ€§èƒ½æ›´ä½³ï¼Œçªæ˜¾äº†å…¶ç¨³å¥æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc16144bf45b94da94e40c11b920a95f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0fbdc199ca731fc2359afadc2dc804e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5639e22ca8041f88b59072e33ed30687.jpg" align="middle">
</details>




<h2 id="A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings"><a href="#A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings" class="headerlink" title="A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings"></a>A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings</h2><p><strong>Authors:Anindita Mondal, Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Anil Kumar Vuppala, Chiranjeevi Yarra</strong></p>
<p>Automatic detection of prominence at the word and syllable-levels is critical for building computer-assisted language learning systems. It has been shown that prosody embeddings learned by the current state-of-the-art (SOTA) text-to-speech (TTS) systems could generate word- and syllable-level prominence in the synthesized speech as natural as in native speech. To understand the effectiveness of prosody embeddings from TTS for prominence detection under nonnative context, a comparative analysis is conducted on the embeddings extracted from native and non-native speech considering the prominence-related embeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2. These embeddings are extracted under two conditions considering: 1) only text, 2) both speech and text. For the first condition, the embeddings are extracted directly from the TTS inference mode, whereas for the second condition, we propose to extract from the TTS under training mode. Experiments are conducted on native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For experimentation, word-level prominence locations are manually annotated for both corpora. The highest relative improvement on word &amp; syllable-level prominence detection accuracies with the TTS embeddings are found to be 13.7% &amp; 5.9% and 16.2% &amp; 6.9% compared to those with the heuristic-based features and self-supervised Wav2Vec-2.0 representations, respectively. </p>
<blockquote>
<p>è‡ªåŠ¨æ£€æµ‹å•è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºæ€§å¯¹äºæ„å»ºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿæ‰€å­¦ä¹ çš„éŸµå¾‹åµŒå…¥å¯ä»¥åœ¨åˆæˆè¯­éŸ³ä¸­äº§ç”Ÿä¸åŸç”Ÿè¯­éŸ³ä¸€æ ·è‡ªç„¶çš„å•è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºæ€§ã€‚ä¸ºäº†äº†è§£TTSéŸµå¾‹åµŒå…¥åœ¨éæ¯è¯­ç¯å¢ƒä¸‹çš„çªå‡ºæ€§æ£€æµ‹æ•ˆæœï¼Œå¯¹æ¥è‡ªåä¸ºFastSpeech2çš„å…ˆè¿›TTSç³»ç»Ÿæå–çš„åµŒå…¥è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œè€ƒè™‘äº†ä¸çªå‡ºæ€§ç›¸å…³çš„åµŒå…¥ï¼šæŒç»­æ—¶é—´ã€èƒ½é‡å’ŒéŸ³è°ƒã€‚è¿™äº›åµŒå…¥æ˜¯åœ¨ä¸¤ç§æ¡ä»¶ä¸‹æå–çš„ï¼š1ï¼‰åªæœ‰æ–‡æœ¬ï¼Œ2ï¼‰è¯­éŸ³å’Œæ–‡æœ¬éƒ½æœ‰ã€‚å¯¹äºç¬¬ä¸€ç§æƒ…å†µï¼ŒåµŒå…¥ç›´æ¥ä»TTSæ¨ç†æ¨¡å¼ä¸­æå–ï¼Œè€Œå¯¹äºç¬¬äºŒç§æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºåœ¨TTSè®­ç»ƒæ¨¡å¼ä¸‹æå–ã€‚å®éªŒåœ¨æ¯è¯­è¯­æ–™åº“Tatoebaå’Œéæ¯è¯­è¯­æ–™åº“ISLEä¸Šè¿›è¡Œã€‚ä¸ºäº†å®éªŒï¼Œä¸¤ä¸ªè¯­æ–™åº“çš„å•è¯çº§åˆ«çªå‡ºä½ç½®éƒ½è¿›è¡Œäº†æ‰‹åŠ¨æ³¨é‡Šã€‚ä¸åŸºäºå¯å‘å¼ç‰¹å¾å’Œè‡ªç›‘ç£Wav2Vec-2.0è¡¨ç¤ºç›¸æ¯”ï¼Œä½¿ç”¨TTSåµŒå…¥åœ¨å•è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºæ€§æ£€æµ‹å‡†ç¡®ç‡ä¸Šè·å¾—äº†æœ€é«˜ç›¸å¯¹æ”¹è¿›ï¼Œåˆ†åˆ«ä¸º13.7%å’Œ5.9%ï¼Œä»¥åŠ16.2%å’Œ6.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08283v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå½“å‰æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå­¦ä¹ åˆ°çš„éŸµå¾‹åµŒå…¥èƒ½å¤Ÿåœ¨åˆæˆè¯­éŸ³ä¸­ç”Ÿæˆè‡ªç„¶çš„è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºè¡¨è¾¾ã€‚åœ¨éæ¯è¯­ç¯å¢ƒä¸‹ï¼Œå¯¹TTSä¸­ç”¨äºçªå‡ºæ£€æµ‹çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†å¯¹æ¯”åˆ†æï¼Œä»æ¯è¯­å’Œéæ¯è¯­è¯­éŸ³ä¸­æå–çš„åµŒå…¥è€ƒè™‘äº†ä¸çªå‡ºç›¸å…³çš„åµŒå…¥å› ç´ ï¼šæŒç»­æ—¶é—´ã€èƒ½é‡å’ŒéŸ³è°ƒã€‚è¿™äº›åµŒå…¥æ˜¯åœ¨ä¸¤ç§æƒ…å†µä¸‹æå–çš„ï¼šä»…æ–‡æœ¬å’Œè¯­éŸ³ä¸æ–‡æœ¬ç»“åˆã€‚å¯¹äºç¬¬ä¸€ç§æƒ…å†µï¼Œç›´æ¥ä»TTSæ¨ç†æ¨¡å¼ä¸­æå–åµŒå…¥ï¼›å¯¹äºç¬¬äºŒç§æƒ…å†µï¼Œå»ºè®®åœ¨TTSè®­ç»ƒæ¨¡å¼ä¸‹æå–ã€‚å®éªŒåœ¨æ¯è¯­è¯­æ–™åº“Tatoebaå’Œéæ¯è¯­è¯­æ–™åº“ISLEä¸Šè¿›è¡Œã€‚å®éªŒä¸­ï¼Œæ‰‹åŠ¨æ ‡æ³¨äº†ä¸¤ç»„è¯­æ–™åº“ä¸­è¯çº§çš„çªå‡ºä½ç½®ã€‚ç›¸è¾ƒäºå¯å‘å¼ç‰¹å¾å’Œè‡ªç›‘ç£çš„Wav2Vec-2.0è¡¨ç¤ºæ–¹æ³•ï¼Œä½¿ç”¨TTSåµŒå…¥åï¼Œè¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºæ£€æµ‹å‡†ç¡®ç‡ç›¸å¯¹æé«˜äº†æœ€é«˜è¾¾16.2%å’Œ6.9%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿå¯¹éŸµå¾‹åµŒå…¥çš„ç†è§£éå¸¸é‡è¦ï¼Œå¯ç”Ÿæˆè‡ªç„¶çš„è¯å’ŒéŸ³èŠ‚çº§åˆ«çš„çªå‡ºè¡¨è¾¾ã€‚</li>
<li>å¯¹æ¯”åˆ†æäº†åœ¨è‡ªç„¶å’Œéæ¯è¯­ç¯å¢ƒä¸‹çš„éŸµå¾‹åµŒå…¥å¯¹äºé‡è¦æ€§æ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒè€ƒè™‘äº†åœ¨ä¸¤ç§æƒ…å¢ƒä¸‹æå–åµŒå…¥ï¼šä»…åŸºäºæ–‡æœ¬å’ŒåŒæ—¶è€ƒè™‘è¯­éŸ³ä¸æ–‡æœ¬ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cddeb908c5ada709a4912e18669893fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5338a274123eec473a1867ddacc01306.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56dc19cd9feb26e4ff74f63dad0098a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15260501556eca45828eccdcf14f7450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb686b114ea5a3fc74636b3e2d428e2.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>åŸºäºå¤§æ¨¡å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå¯¹æ•°æ®æœ‰ç€æé«˜çš„éœ€æ±‚ã€‚è¿‘æœŸåŸºäºå¤§æ¨¡å‹çš„TTSå·¥ä½œé€šå¸¸é‡‡ç”¨å¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹ä»¥è·å¾—é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›é«˜çº§æµç¨‹éœ€è¦åœ¨æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ä¼˜ç§€çš„æ¨¡å‹ï¼ˆä¾‹å¦‚è¯­éŸ³é™å™ªã€è¯­éŸ³å¢å¼ºã€è¯´è¯äººè¯†åˆ«å’Œæ ‡ç‚¹æ¨¡å‹ï¼‰ï¼Œè€Œè¿™äº›æ¨¡å‹æœ¬èº«ä¹Ÿéœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å¹¶ä¸”å¾ˆå°‘å¼€æºã€‚å³ä½¿ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨ä»¥åŠæ ‡ç‚¹ç¬¦å·ä¸å®é™…è¯­éŸ³åœé¡¿ä¹‹é—´çš„ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œä¸¥æ ¼çš„è¿‡æ»¤ç­–ç•¥é€šå¸¸åªèƒ½ä¿ç•™åŸå§‹æ•°æ®çš„10-30%ï¼Œæå¤§åœ°é˜»ç¢äº†æ•°æ®æ‰©å±•çš„åŠªåŠ›ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘æ ‡è®°å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡äº†ä¸€ä¸ªç®€åŒ–è€Œæœ‰æ•ˆçš„TTSæ•°æ®å¤„ç†æµç¨‹ï¼Œè¯¥æµç¨‹åœ¨ä¿æŒæ•°æ®è´¨é‡çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†æ•°æ®è·å–æˆæœ¬ï¼Œå®ç°äº†è¶…è¿‡50%çš„æ•°æ®ä¿ç•™ç‡ã€‚é™¤äº†æ•°æ®æ‰©å±•çš„æŒ‘æˆ˜å¤–ï¼ŒåŸºäºå¤§æ¨¡å‹çš„TTSç³»ç»Ÿçš„éƒ¨ç½²æˆæœ¬ä¹Ÿé«˜äºä¼ ç»Ÿæ–¹æ³•ã€‚å½“å‰çš„ç³»ç»Ÿé€šå¸¸ä»…ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°æ ‡è®°çš„ç”Ÿæˆï¼Œä½†éœ€è¦é¢å¤–çš„æ¨¡å‹ï¼ˆå¦‚æµç¨‹åŒ¹é…æ¨¡å‹ï¼‰æ¥è¿›è¡Œæ ‡è®°åˆ°æ³¢å½¢ç”Ÿæˆçš„è½¬æ¢ï¼Œè¿™äº›è½¬æ¢æ— æ³•ç”±å¤§æ¨¡å‹æ¨ç†å¼•æ“ç›´æ¥æ‰§è¡Œï¼Œè¿›ä¸€æ­¥å¢åŠ äº†éƒ¨ç½²çš„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†å¤§æ¨¡å‹å’Œæµç¨‹ç»„ä»¶ä¸­çš„å†—ä½™æ¨¡å—ï¼Œå¹¶ç”¨å¤§æ¨¡å‹æ¶æ„æ›¿ä»£æµç¨‹æ¨¡å‹çš„ä¸»å¹²ã€‚åŸºäºè¿™ç§ç®€åŒ–çš„æµç¨‹ä¸»å¹²ï¼Œæˆ‘ä»¬æå‡ºäº†æµå¼å’Œéæµå¼æ¨ç†çš„ç»Ÿä¸€æ¶æ„ï¼Œå¤§å¤§é™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬å€ŸåŠ©ç®€åŒ–çš„æµç¨‹å’ŒS3Tokenizerï¼Œæ¢ç´¢äº†ä½¿ç”¨ç›¸åŒæ•°æ®è¿›è¡ŒTTSå’Œè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡è®­ç»ƒçš„å¯è¡Œæ€§ï¼Œé™ä½äº†TTSè®­ç»ƒæ•°æ®çš„è´¨é‡è¦æ±‚ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºLLMçš„TTSæ•°æ®å¤„ç†æ–¹æ³•çš„æ–°æ”¹è¿›ã€‚é€šè¿‡é‡‡ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰ï¼Œç®€åŒ–äº†æ•°æ®è·å–æµç¨‹ï¼Œæé«˜äº†æ•°æ®ä¿ç•™ç‡ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¼˜åŒ–æ¨¡å‹æ¶æ„ï¼Œå‡å°‘äº†éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æ¢ç´¢äº†ç»Ÿä¸€TTSå’ŒASRä»»åŠ¡çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLM-based TTSç³»ç»Ÿå¯¹æ•°æ®è´¨é‡æœ‰è¾ƒé«˜è¦æ±‚ï¼Œéœ€å¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>S3Tokenizerçš„åº”ç”¨ä½¿å¾—TTSæ•°æ®å¤„ç†æµç¨‹ç®€åŒ–ï¼ŒåŒæ—¶ä¿æŒæ•°æ®è´¨é‡ï¼Œå¹¶æé«˜æ•°æ®ä¿ç•™ç‡è‡³50%ä»¥ä¸Šã€‚</li>
<li>LLM-based TTSç³»ç»Ÿçš„éƒ¨ç½²æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦é€šè¿‡ä¼˜åŒ–æ¨¡å‹æ¶æ„æ¥é™ä½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¶æ„ï¼Œæ”¯æŒæµå¼å’Œéæµå¼æ¨ç†ï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚</li>
<li>ç®€åŒ–çš„æ•°æ®å¤„ç†æµç¨‹å’ŒS3Tokenizerä½¿å¾—TTSå’ŒASRä»»åŠ¡å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ä»å­˜åœ¨èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨å’Œæ ‡ç‚¹ä¸è¯­éŸ³åœé¡¿å¯¹é½é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„ã€‚</li>
<li>æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•ä¼˜åŒ–LLM-based TTSç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œä»¥åº”å¯¹å¤§è§„æ¨¡æ•°æ®å’Œå¤æ‚åœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å› å…¶ç›¸è¾ƒäºå…¶ä»–ç”ŸæˆæŠ€æœ¯ï¼ˆå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼‰çš„å“è¶Šæ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶å®ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸»æµçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸»è¦åœ¨é¢‘è°±ç©ºé—´ä¸­æŠŠè¾“å‡ºæ˜ å°„åˆ°æ¢…å°”é¢‘è°±ï¼ˆMel-Spectrogramsï¼‰ï¼Œç”±äºæ¢…å°”é¢‘è°±çš„ç¨€ç–æ€§ï¼Œå¯¼è‡´è®¡ç®—è´Ÿè½½è¾ƒé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†LatentSpeechï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹TTSç”Ÿæˆæ–¹æ³•ã€‚LatentSpeeché€šè¿‡ä½¿ç”¨æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå°†ç›®æ ‡ç»´åº¦é™ä½åˆ°æ¢…å°”é¢‘è°±æ‰€éœ€çš„5%ï¼Œç®€åŒ–äº†TTSç¼–ç å™¨ä¸vocoderçš„å¤„ç†è¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™é¡¹ç ”ç©¶æ ‡å¿—ç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨TTSä¸­çš„é¦–æ¬¡é›†æˆï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLatentSpeechåœ¨è¯é”™è¯¯ç‡ä¸Šæé«˜äº†25%ï¼Œæ¢…å°”å€’è°±å¤±çœŸæé«˜äº†24%ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè¿™ä¸¤é¡¹æŒ‡æ ‡åˆ†åˆ«è¿›ä¸€æ­¥æé«˜åˆ°49.5%å’Œ26%ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†LatentSpeechåœ¨æ¨åŠ¨TTSæŠ€æœ¯å‰æ²¿æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼AIåœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œä½†ä»å­˜åœ¨åº”ç”¨å±€é™æ€§ã€‚ä¸»æµæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿä¸»è¦æ˜ å°„åˆ°Melé¢‘è°±å›¾ï¼Œå¯¼è‡´é«˜è®¡ç®—è´Ÿè½½ã€‚æœ¬ç ”ç©¶æå‡ºLatentSpeechï¼Œä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹TTSç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œé™ä½ç›®æ ‡ç»´åº¦è‡³Melé¢‘è°±å›¾çš„5%ï¼Œç®€åŒ–TTSç¼–ç å™¨å’Œvocoderçš„å¤„ç†æµç¨‹ï¼Œå®ç°é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚LatentSpeeché›†æˆäºTTSä¸­æé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶è¾ƒç°æœ‰æ¨¡å‹æœ‰æ‰€æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å¼ç”ŸæˆAIåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜è¶Šï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚</li>
<li>ä¸»æµTTSç³»ç»Ÿä¸»è¦æ˜ å°„åˆ°Melé¢‘è°±å›¾ï¼Œå¯¼è‡´é«˜è®¡ç®—è´Ÿè½½ã€‚</li>
<li>LatentSpeechæ˜¯ä¸€ç§æ–°å‹TTSç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé™ä½ç›®æ ‡ç»´åº¦è‡³Melé¢‘è°±å›¾çš„5%ã€‚</li>
<li>LatentSpeechç®€åŒ–äº†TTSç¼–ç å™¨å’Œvocoderçš„å¤„ç†æµç¨‹ï¼Œæé«˜äº†è¯­éŸ³ç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
<li>LatentSpeeché›†æˆäº†æ‰©æ•£æ¨¡å‹åœ¨TTSä¸­ï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡å‹ï¼ŒLatentSpeechåœ¨Word Error Rateä¸Šæœ‰25%çš„æ”¹è¿›ï¼Œåœ¨Mel Cepstral Distortionä¸Šæœ‰24%çš„æ”¹è¿›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats"><a href="#Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats" class="headerlink" title="Sampling from Boltzmann densities with physics informed low-rank formats"></a>Sampling from Boltzmann densities with physics informed low-rank formats</h2><p><strong>Authors:Paul Hagemann, Janina SchÃ¼tte, David Sommer, Martin Eigel, Gabriele Steidl</strong></p>
<p>Our method proposes the efficient generation of samples from an unnormalized Boltzmann density by solving the underlying continuity equation in the low-rank tensor train (TT) format. It is based on the annealing path commonly used in MCMC literature, which is given by the linear interpolation in the space of energies. Inspired by Sequential Monte Carlo, we alternate between deterministic time steps from the TT representation of the flow field and stochastic steps, which include Langevin and resampling steps. These adjust the relative weights of the different modes of the target distribution and anneal to the correct path distribution. We showcase the efficiency of our method on multiple numerical examples. </p>
<blockquote>
<p>æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è§£å†³ä½ç§©å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰æ ¼å¼ä¸­çš„åŸºç¡€è¿ç»­æ€§æ–¹ç¨‹ï¼Œæœ‰æ•ˆåœ°ä»æœªæ ‡å‡†åŒ–çš„ç»å°”å…¹æ›¼å¯†åº¦ä¸­ç”Ÿæˆæ ·æœ¬ã€‚å®ƒåŸºäºMCMCæ–‡çŒ®ä¸­å¸¸ç”¨çš„é€€ç«è·¯å¾„ï¼Œç”±èƒ½é‡ç©ºé—´ä¸­çš„çº¿æ€§æ’å€¼ç»™å‡ºã€‚å—åºè´¯è’™ç‰¹å¡ç½—çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨ç¡®å®šæ€§æ—¶é—´æ­¥é•¿å’ŒæµåŠ¨åœºTTè¡¨ç¤ºçš„éšæœºæ­¥éª¤ä¹‹é—´è¿›è¡Œäº¤æ›¿åˆ‡æ¢ï¼Œéšæœºæ­¥éª¤åŒ…æ‹¬æœ—æ ¼æ–‡ï¼ˆLangevinï¼‰å’Œé‡é‡‡æ ·æ­¥éª¤ã€‚è¿™äº›æ­¥éª¤è°ƒæ•´äº†ç›®æ ‡åˆ†å¸ƒçš„å„æ¨¡æ€çš„ç›¸å¯¹æƒé‡ï¼Œå¹¶é€€ç«åˆ°æ­£ç¡®çš„è·¯å¾„åˆ†å¸ƒã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å€¼ç¤ºä¾‹ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07637v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä½ç§©å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰æ ¼å¼çš„æœªå½’ä¸€åŒ–ç»å°”å…¹æ›¼å¯†åº¦æ ·æœ¬é«˜æ•ˆç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºMCMCæ–‡çŒ®ä¸­å¸¸ç”¨çš„é€€ç«è·¯å¾„ï¼Œé€šè¿‡èƒ½é‡ç©ºé—´ä¸­çš„çº¿æ€§æ’å€¼ç»™å‡ºã€‚è¯¥æ–¹æ³•å—åˆ°åºè´¯è’™ç‰¹å¡ç½—çš„å¯å‘ï¼Œåœ¨æµåœºçš„TTè¡¨ç¤ºä¸­äº¤æ›¿è¿›è¡Œç¡®å®šæ€§æ—¶é—´æ­¥é•¿å’Œéšæœºæ­¥éª¤ï¼ŒåŒ…æ‹¬æœ—æ ¼æ–‡é‡é‡‡æ ·æ­¥éª¤ã€‚è¿™äº›æ­¥éª¤è°ƒæ•´ç›®æ ‡åˆ†å¸ƒçš„å„æ¨¡æ€ç›¸å¯¹æƒé‡ï¼Œå¹¶é€€ç«åˆ°æ­£ç¡®çš„è·¯å¾„åˆ†å¸ƒã€‚é€šè¿‡å¤šä¸ªæ•°å€¼ä¾‹å­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºä½ç§©å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰æ ¼å¼çš„é«˜æ•ˆç”Ÿæˆæœªå½’ä¸€åŒ–ç»å°”å…¹æ›¼å¯†åº¦æ ·æœ¬çš„æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†é€€ç«è·¯å¾„å’Œçº¿æ€§æ’å€¼æŠ€æœ¯ï¼Œåœ¨èƒ½é‡ç©ºé—´ä¸­å¯»æ‰¾æœ€ä½³æ ·æœ¬ã€‚</li>
<li>å—åˆ°åºè´¯è’™ç‰¹å¡ç½—çš„å¯å‘ï¼Œè¯¥æ–¹æ³•äº¤æ›¿ä½¿ç”¨ç¡®å®šæ€§æ—¶é—´æ­¥é•¿å’Œéšæœºæ­¥éª¤ã€‚</li>
<li>ç¡®å®šæ€§æ—¶é—´æ­¥é•¿åŸºäºæµåœºçš„TTè¡¨ç¤ºï¼Œè€Œéšæœºæ­¥éª¤åŒ…æ‹¬æœ—æ ¼æ–‡é‡é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>é€šè¿‡è°ƒæ•´ç›®æ ‡åˆ†å¸ƒçš„å„æ¨¡æ€ç›¸å¯¹æƒé‡ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„æ ·æœ¬ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é€€ç«è¿‡ç¨‹è¾¾åˆ°æ­£ç¡®çš„è·¯å¾„åˆ†å¸ƒï¼Œæé«˜äº†æ ·æœ¬ç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbaf87fb6994743aa883b5db79f16f19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c51d55d3f34411e1abfb2c58e4b59726.jpg" align="middle">
</details>




<h2 id="Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection"><a href="#Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection" class="headerlink" title="Mitigating Unauthorized Speech Synthesis for Voice Protection"></a>Mitigating Unauthorized Speech Synthesis for Voice Protection</h2><p><strong>Authors:Zhisheng Zhang, Qianyi Yang, Derui Wang, Pengyang Huang, Yuxin Cao, Kai Ye, Jie Hao</strong></p>
<p>With just a few speech samples, it is possible to perfectly replicate a speakerâ€™s voice in recent years, while malicious voice exploitation (e.g., telecom fraud for illegal financial gain) has brought huge hazards in our daily lives. Therefore, it is crucial to protect publicly accessible speech data that contains sensitive information, such as personal voiceprints. Most previous defense methods have focused on spoofing speaker verification systems in timbre similarity but the synthesized deepfake speech is still of high quality. In response to the rising hazards, we devise an effective, transferable, and robust proactive protection technology named Pivotal Objective Perturbation (POP) that applies imperceptible error-minimizing noises on original speech samples to prevent them from being effectively learned for text-to-speech (TTS) synthesis models so that high-quality deepfake speeches cannot be generated. We conduct extensive experiments on state-of-the-art (SOTA) TTS models utilizing objective and subjective metrics to comprehensively evaluate our proposed method. The experimental results demonstrate outstanding effectiveness and transferability across various models. Compared to the speech unclarity score of 21.94% from voice synthesizers trained on samples without protection, POP-protected samples significantly increase it to 127.31%. Moreover, our method shows robustness against noise reduction and data augmentation techniques, thereby greatly reducing potential hazards. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåªéœ€å°‘é‡çš„è¯­éŸ³æ ·æœ¬ï¼Œå°±èƒ½å¤Ÿå®Œç¾å¤åˆ¶ä¸€ä¸ªäººçš„å£°éŸ³ï¼Œè€Œæ¶æ„å£°éŸ³æ»¥ç”¨ï¼ˆä¾‹å¦‚ç”µä¿¡æ¬ºè¯ˆä»¥è·å–éæ³•è´¢åŠ¡æ”¶ç›Šï¼‰ç»™æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»å¸¦æ¥äº†å·¨å¤§çš„å±å®³ã€‚å› æ­¤ï¼Œä¿æŠ¤å«æœ‰æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ä¸ªäººè¯­éŸ³ç‰¹å¾ï¼‰çš„å…¬å¼€è¯­éŸ³æ•°æ®è‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¾§é‡äºæ¬ºéª—è¯´è¯è€…éªŒè¯ç³»ç»Ÿçš„éŸ³è‰²ç›¸ä¼¼æ€§ï¼Œä½†åˆæˆçš„æ·±åº¦ä¼ªé€ è¯­éŸ³ä»ç„¶å…·æœ‰å¾ˆé«˜çš„è´¨é‡ã€‚ä¸ºäº†åº”å¯¹æ—¥ç›Šå¢é•¿çš„å¨èƒï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆã€å¯è¿ç§»ä¸”ç¨³å¥çš„ä¸»åŠ¨ä¿æŠ¤æŠ€æœ¯ï¼Œç§°ä¸ºå…³é”®ç›®æ ‡æ‰°åŠ¨ï¼ˆPOPï¼‰ã€‚è¯¥æŠ€æœ¯å¯¹åŸå§‹è¯­éŸ³æ ·æœ¬åº”ç”¨å‡ ä¹æ— æ³•å¯Ÿè§‰çš„é”™è¯¯æœ€å°åŒ–å™ªå£°ï¼Œé˜²æ­¢å®ƒä»¬è¢«æœ‰æ•ˆå­¦ä¹ ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹ï¼Œä»è€Œæ— æ³•ç”Ÿæˆé«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ è¯­éŸ³ã€‚æˆ‘ä»¬å¯¹é‡‡ç”¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡çš„å…ˆè¿›TTSæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå…¨é¢è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§éƒ½å¾ˆå‡ºè‰²ã€‚ä¸æœªç»ä¿æŠ¤çš„æ ·æœ¬è®­ç»ƒçš„è¯­éŸ³åˆæˆå™¨çš„è¯­éŸ³æ¸…æ™°åº¦å¾—åˆ†ä¸º21.94%ç›¸æ¯”ï¼ŒPOPä¿æŠ¤çš„æ ·æœ¬å¾—åˆ†æ˜¾è‘—æé«˜åˆ°127.31%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºå¯¹é™å™ªå’Œæ•°æ®å¢å¼ºæŠ€æœ¯çš„ç¨³å¥æ€§ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†æ½œåœ¨é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20742v1">PDF</a> Accepted to ACM CCS Workshop (LAMPS) 2024</p>
<p><strong>æ‘˜è¦</strong><br>è¿‘æœŸå‡ºç°èƒ½é€šè¿‡å°‘é‡è¯­éŸ³æ ·æœ¬å®Œç¾å¤åˆ¶å‘è¨€äººå£°éŸ³çš„æŠ€æœ¯ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ¶æ„è¯­éŸ³æ»¥ç”¨ï¼ˆå¦‚ç”µä¿¡è¯ˆéª—ï¼‰çš„å·¨å¤§é£é™©ã€‚å› æ­¤ï¼Œä¿æŠ¤å«æœ‰æ•æ„Ÿä¿¡æ¯çš„å…¬å¼€è¯­éŸ³æ•°æ®è‡³å…³é‡è¦ï¼Œå¦‚ä¸ªäººå£°çº¹ã€‚ä¸ºåº”å¯¹é£é™©ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆã€å¯è¿ç§»ä¸”ç¨³å¥çš„ä¸»åŠ¨ä¿æŠ¤æŠ€æœ¯â€”â€”å…³é”®ç›®æ ‡æ‰°åŠ¨ï¼ˆPOPï¼‰ï¼Œé€šè¿‡åœ¨åŸå§‹è¯­éŸ³æ ·æœ¬ä¸Šæ·»åŠ å‡ ä¹æ— æ³•å¯Ÿè§‰çš„æœ€å°è¯¯å·®å™ªå£°ï¼Œé˜²æ­¢å®ƒä»¬è¢«ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹çš„è®­ç»ƒï¼Œä»è€Œé˜²æ­¢é«˜è´¨é‡æ·±åº¦ä¼ªé€ è¯­éŸ³çš„ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„TTSæ¨¡å‹è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç”¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡å…¨é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨ä¸åŒæ¨¡å‹ä¸­çš„å‡ºè‰²æ•ˆæœå’Œå¯è¿ç§»æ€§ã€‚ç›¸è¾ƒäºæœªç»ä¿æŠ¤çš„æ ·æœ¬è®­ç»ƒçš„è¯­éŸ³åˆæˆå™¨ï¼Œå…¶è¯­éŸ³æ¸…æ™°åº¦åªæœ‰21.94%ï¼Œä½¿ç”¨POPä¿æŠ¤çš„æ ·æœ¬å°†å…¶æ˜¾è‘—æé«˜åˆ°127.31%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºå¯¹æŠ—é™å™ªå’Œæ•°æ®å¢å¼ºæŠ€æœ¯çš„ç¨³å¥æ€§ï¼Œå¤§å¤§é™ä½äº†æ½œåœ¨é£é™©ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»…éœ€å°‘é‡è¯­éŸ³æ ·æœ¬å³å¯å®Œç¾å¤åˆ¶å‘è¨€äººå£°éŸ³ï¼ŒåŒæ—¶å‡ºç°æ¶æ„è¯­éŸ³æ»¥ç”¨é£é™©ã€‚</li>
<li>ä¿æŠ¤å…¬å¼€è¯­éŸ³æ•°æ®ä¸­çš„æ•æ„Ÿä¿¡æ¯è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºå…³é”®ç›®æ ‡æ‰°åŠ¨ï¼ˆPOPï¼‰çš„ä¸»åŠ¨ä¿æŠ¤æŠ€æœ¯ï¼Œé€šè¿‡æ·»åŠ å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å™ªå£°é˜²æ­¢è¯­éŸ³æ ·æœ¬è¢«ç”¨äºTTSåˆæˆã€‚</li>
<li>POPæŠ€æœ¯åœ¨å„ç§TTSæ¨¡å‹ä¸­è¡¨ç°å‡ºå“è¶Šçš„æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚</li>
<li>ä¸æœªç»ä¿æŠ¤çš„æ ·æœ¬ç›¸æ¯”ï¼ŒPOPä¿æŠ¤çš„æ ·æœ¬è®­ç»ƒçš„è¯­éŸ³åˆæˆå™¨è¯­éŸ³æ¸…æ™°åº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>POPæŠ€æœ¯å¯¹æŠ—é™å™ªå’Œæ•°æ®å¢å¼ºæŠ€æœ¯å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>POPæŠ€æœ¯æœ‰åŠ©äºå¤§å¤§é™ä½å› æ·±åº¦ä¼ªé€ è¯­éŸ³å¸¦æ¥çš„æ½œåœ¨é£é™©ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9cc75591dbb4786702c6ed9b92008756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c50bf08a392d1274b1e3ff5afe161f71.jpg" align="middle">
</details>




<h2 id="Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation"><a href="#Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation" class="headerlink" title="Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation"></a>Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation</h2><p><strong>Authors:Maohao Shen, Shun Zhang, Jilong Wu, Zhiping Xiu, Ehab AlBadawy, Yiting Lu, Mike Seltzer, Qing He</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llamaâ€™s competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å‡­å€Ÿåœ¨å„ç§æ–‡æœ¬ç›¸å…³ä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„æ ¼å±€ã€‚ç„¶è€Œï¼Œå°†æ–‡æœ¬ä¸»å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°è¯­éŸ³ç”Ÿæˆä»»åŠ¡ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±ç²¾ç»†è°ƒæ•´çš„Llamaæ¨¡å‹é©±åŠ¨çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œåä¸ºTTS-Llamaï¼Œè¯¥ç³»ç»Ÿå®ç°äº†æœ€å…ˆè¿›çš„è¯­éŸ³åˆæˆæ€§èƒ½ã€‚åŸºäºTTS-Llamaï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†MoLE-Llamaï¼Œè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬å’Œè¯­éŸ³å¤šæ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡çº¯ç²¹çš„åæœŸèåˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å’Œæ··åˆä¸“å®¶æ¶æ„å¼€å‘ã€‚å¤§é‡çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒMoLE-Llamaåœ¨çº¯æ–‡æœ¬é—®ç­”ï¼ˆQAï¼‰å’ŒTTSä»»åŠ¡ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œå‡è½»äº†ä»»ä¸€æ¨¡æ€ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬-è¯­éŸ³é—®ç­”ä»»åŠ¡ä¸­è¿›ä¸€æ­¥æ¢ç´¢äº†MoLE-Llamaï¼Œè¯æ˜äº†å…¶ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œè¯­éŸ³ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºç²¾ç»†è°ƒæ•´Llamaæ¨¡å‹çš„TTSç³»ç»Ÿï¼ˆåä¸ºTTS-Llamaï¼‰ï¼Œå®ç°äº†å…ˆè¿›çš„è¯­éŸ³åˆæˆæ€§èƒ½ã€‚è¿›ä¸€æ­¥æ„å»ºçš„MoLE-Llamaæ–‡æœ¬ä¸è¯­éŸ³æ··åˆæ¨¡æ€LLMæ¨¡å‹ï¼Œé€šè¿‡çº¯ç²¹çš„æ™šæœŸèåˆå‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰å’Œæ··åˆä¸“å®¶æ¶æ„å‘å±•è€Œæ¥ã€‚è¯¥æ¨¡å‹åœ¨çº¯æ–‡æœ¬é—®ç­”ï¼ˆQAï¼‰å’ŒTTSä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå‡è½»äº†æ¨¡æ€ç¾éš¾é—å¿˜é—®é¢˜ã€‚åœ¨æ–‡æœ¬è¯­éŸ³é—®ç­”ä»»åŠ¡ä¸­çš„æ¢ç´¢è¡¨æ˜ï¼Œå…¶ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„æ½œåŠ›å·¨å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>TTS-Llamaç³»ç»ŸåŸºäºç²¾ç»†è°ƒæ•´çš„Llamaæ¨¡å‹ï¼Œå®ç°äº†å…ˆè¿›çš„è¯­éŸ³åˆæˆæ€§èƒ½ã€‚</li>
<li>MoLE-Llamaæ˜¯æ–‡æœ¬ä¸è¯­éŸ³æ··åˆæ¨¡æ€LLMæ¨¡å‹ï¼Œé€šè¿‡PEFTå’Œæ··åˆä¸“å®¶æ¶æ„å‘å±•è€Œæ¥ã€‚</li>
<li>MoLE-Llamaåœ¨çº¯æ–‡æœ¬é—®ç­”ï¼ˆQAï¼‰å’ŒTTSä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>MoLE-Llamaå¯ä»¥å‡è½»æ¨¡æ€ç¾éš¾é—å¿˜é—®é¢˜ã€‚</li>
<li>MoLE-Llamaåœ¨æ–‡æœ¬è¯­éŸ³é—®ç­”ä»»åŠ¡ä¸­çš„æ¢ç´¢å±•ç°å‡ºå…¶ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºTTSé¢†åŸŸæä¾›äº†æ–°æ–¹å‘ï¼Œç»“åˆäº†LLMå’Œè¯­éŸ³æŠ€æœ¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab288d5e910dba80ad1170150f3378af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1d6f26c2831e3dbd99fcf1e01e379ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-154078eb091a83b8bcaebdf9c65b4d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18398ad78f9636c12358fd611c1222b8.jpg" align="middle">
</details>




<h2 id="Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis"><a href="#Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis" class="headerlink" title="Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis"></a>Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis</h2><p><strong>Authors:Suparna De, Ionut Bostan, Nishanth Sastry</strong></p>
<p>Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakersâ€™ emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications. </p>
<blockquote>
<p>å°½ç®¡æœ‰å•éŸ³è°ƒæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å±å¹•é˜…è¯»å™¨ç­‰æŠ€æœ¯å’Œè¡¨æƒ…ç¬¦å·çš„è§†è§‰å…ƒç´ éŸ³é¢‘å™è¿°ç­‰è¾…åŠ©æŠ€æœ¯ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶æ¦‚è¿°äº†ç›²äººæˆ–è§†éšœä»¥åŠè¯†å­—è¾ƒå°‘çš„äººåœ¨ç¤¾äº¤ç½‘ç»œä¸­é‡åˆ°çš„äº¤äº’è®¿é—®æŒ‘æˆ˜ã€‚æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆä¼ ç»Ÿä¸Šä¾èµ–äºäººç±»è¾“å…¥çš„é¢„æœŸæƒ…æ„Ÿä»¥åŠæ–‡æœ¬åˆæˆï¼Œè¿˜å­˜åœ¨æ•°æ®ç®€åŒ–ï¼ˆå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼‰å’ŒæŒç»­æ—¶é—´ä¸å‡†ç¡®ç­‰é¢å¤–æŒ‘æˆ˜ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æƒ…æ„Ÿçš„è¡¨ç°ã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­çš„é€šä¿¡ä¸­ï¼Œç”±äºè¯´è¯äººçš„æƒ…ç»ªçŠ¶æ€æˆ–å£éŸ³çš„ä¸åŒï¼ŒéŸ³ç´ çš„æŒç»­æ—¶é—´å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼ˆè¿™è¢«ç§°ä¸ºæ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆçš„ä¸€å¯¹å¤šé—®é¢˜ï¼‰ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªå…ˆè¿›çš„è¯­éŸ³åˆæˆç³»ç»Ÿæ¥åº”å¯¹è¿™ç§ä¸å¯é¢„æµ‹æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä»æ–‡æœ¬è¾“å…¥ä¸­æ¨å¯¼å‡ºè¡¨è¾¾çš„æƒ…æ„Ÿï¼Œå¹¶åˆæˆéŸ³é¢‘ï¼Œä¸“æ³¨äºæƒ…æ„Ÿå’Œè¯´è¯è€…ç‰¹å¾ä»¥å®ç°è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ï¼Œé›†æˆå…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯ç”¨äºå®æ—¶åº”ç”¨ã€‚å½“ä¸æœ€æ–°TTSæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿè¿˜å±•ç¤ºäº†å…·æœ‰ç«äº‰åŠ›çš„æ¨ç†æ—¶é—´æ€§èƒ½ï¼Œä½¿å…¶é€‚åˆç”¨äºå®æ—¶è®¿é—®æ€§åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19199v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æœ‰å•è°ƒæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å±å¹•é˜…è¯»å™¨å’ŒéŸ³é¢‘å™è¿°ç­‰è¾…åŠ©æŠ€æœ¯ï¼Œä½†ç›²äººæˆ–è§†éšœã€ä½å­¦å†äººç¾¤åœ¨ä½¿ç”¨ç¤¾äº¤ç½‘ç»œæ—¶ä»é¢ä¸´æ— éšœç¢æŒ‘æˆ˜ã€‚æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆä¼ ç»Ÿä¸Šä¾èµ–äºé¢„æœŸæƒ…æ„Ÿå’Œæ–‡æœ¬è¾“å…¥ï¼Œå­˜åœ¨æ•°æ®ç®€åŒ–å¯¼è‡´ä¿¡æ¯ä¸¢å¤±å’ŒæŒç»­æ—¶é—´ä¸å‡†ç¡®ç­‰é—®é¢˜ï¼Œå¯¼è‡´æƒ…æ„Ÿè¡¨è¾¾æ¸²æŸ“ä¸è¶³ã€‚é’ˆå¯¹çœŸå®æ²Ÿé€šä¸­éŸ³ç´ æŒç»­æ—¶é—´çš„ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­å¢ƒæ„ŸçŸ¥æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä»æ–‡æœ¬è¾“å…¥ä¸­æ¨å¯¼å‡ºè¡¨è¾¾çš„æƒ…æ„Ÿï¼Œåˆæˆä¸“æ³¨äºæƒ…æ„Ÿå’Œè¯´è¯äººç‰¹å¾çš„éŸ³é¢‘ï¼Œä»¥å®ç°è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚æ•´åˆå…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æ¨ç†æ—¶é—´æ€§èƒ½ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä½¿å…¶æˆä¸ºé€‚åˆå®æ—¶æ— éšœç¢åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›²äººæˆ–è§†éšœã€ä½å­¦å†äººç¾¤åœ¨ä½¿ç”¨ç¤¾äº¤ç½‘ç»œæ—¶ä»é¢ä¸´æ— éšœç¢æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆå­˜åœ¨æ•°æ®ç®€åŒ–å¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±å’ŒæŒç»­æ—¶é—´ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>çœŸå®æ²Ÿé€šä¸­ï¼ŒéŸ³ç´ æŒç»­æ—¶é—´å› è¯´è¯äººçš„æƒ…æ„ŸçŠ¶æ€å’Œå£éŸ³è€Œæœ‰æ‰€ä¸åŒã€‚</li>
<li>æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„è¯­å¢ƒæ„ŸçŸ¥TTSåˆæˆç³»ç»Ÿï¼Œä»æ–‡æœ¬è¾“å…¥ä¸­æ¨å¯¼å‡ºè¡¨è¾¾çš„æƒ…æ„Ÿã€‚</li>
<li>ç³»ç»Ÿèƒ½åˆæˆä¸“æ³¨äºæƒ…æ„Ÿå’Œè¯´è¯äººç‰¹å¾çš„éŸ³é¢‘ï¼Œå®ç°è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚</li>
<li>ç³»ç»Ÿæ•´åˆäº†å…ˆè¿›çš„NLPå’Œè¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-954c4baed564d5fc17bf475672a0c733.jpg" align="middle">
</details>




<h2 id="Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model"><a href="#Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model" class="headerlink" title="Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model"></a>Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model</h2><p><strong>Authors:Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, Yike Guo, Wei Xue</strong></p>
<p>Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: <a target="_blank" rel="noopener" href="https://x-codec-audio.github.io/">https://x-codec-audio.github.io</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zhenye234/xcodec">https://github.com/zhenye234/xcodec</a>) </p>
<blockquote>
<p>éŸ³é¢‘ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚ç›®å‰å…³äºéŸ³é¢‘LLMçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¢å¼ºéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¶æ„å’Œè§„æ¨¡ã€åˆ©ç”¨æ›´å¤§çš„æ•°æ®é›†ä»¥åŠä¸€èˆ¬é‡‡ç”¨éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆå¦‚EnCodecï¼‰è¿›è¡ŒéŸ³é¢‘æ ‡è®°åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›ç¼–è§£ç å™¨æœ€åˆæ˜¯ä¸ºéŸ³é¢‘å‹ç¼©è€Œè®¾è®¡çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨éŸ³é¢‘LLMçš„ä¸Šä¸‹æ–‡ä¸­æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰éŸ³é¢‘LLMç¼–è§£ç å™¨çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨ç»´æŒç”ŸæˆéŸ³é¢‘çš„è¯­ä¹‰å®Œæ•´æ€§æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œç°æœ‰çš„æ–¹æ³•å¦‚VALL-Eï¼Œæ ¹æ®æ–‡æœ¬è½¬å½•æ¥æ¡ä»¶åŒ–å£°å­¦æ ‡è®°ç”Ÿæˆï¼Œç”±äºå£°å­¦æ ‡è®°çš„è¯­ä¹‰è¯¯è§£ï¼Œç»å¸¸å¯¼è‡´å†…å®¹ä¸å‡†ç¡®å’Œå‡é«˜çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œä»è€Œäº§ç”Ÿè·³è¯å’Œé”™è¯¯ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºX-Codecã€‚X-Codecåœ¨æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰é˜¶æ®µä¹‹å‰èå…¥äº†é¢„è®­ç»ƒè¯­ä¹‰ç¼–ç å™¨çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åœ¨RVQä¹‹åå¼•å…¥äº†è¯­ä¹‰é‡å»ºæŸå¤±ã€‚é€šè¿‡å¢å¼ºç¼–è§£ç å™¨çš„è¯­ä¹‰èƒ½åŠ›ï¼ŒX-Codecåœ¨è¯­éŸ³åˆæˆä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†WERï¼Œå¹¶å°†è¿™äº›ä¼˜åŠ¿æ‰©å±•åˆ°äº†éè¯­éŸ³åº”ç”¨ï¼ŒåŒ…æ‹¬éŸ³ä¹å’Œå£°éŸ³ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ã€éŸ³ä¹å»¶ç»­å’Œæ–‡æœ¬åˆ°å£°éŸ³çš„ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œèå…¥è¯­ä¹‰ä¿¡æ¯èƒ½å¤§å¹…æå‡è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä¸­çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºï¼ˆæ¼”ç¤ºï¼š<a href="https://x-codec-audio.github.ioï¼›ä»£ç ï¼šhttps://github.com/zhenye234/xcodecï¼‰å·²ç»å¯ç”¨ã€‚">https://x-codec-audio.github.ioï¼›ä»£ç ï¼šhttps://github.com/zhenye234/xcodecï¼‰å·²ç»å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17175v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸéŸ³é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚ç ”ç©¶é›†ä¸­åœ¨å¢å¼ºéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¶æ„å’Œè§„æ¨¡ã€åˆ©ç”¨æ›´å¤§çš„æ•°æ®é›†ä¸Šï¼Œä½†ç°æœ‰éŸ³é¢‘LLMçš„ç¼–è§£ç å™¨å¦‚EnCodecï¼Œæ˜¯ä¸ºéŸ³é¢‘å‹ç¼©è®¾è®¡çš„ï¼Œåœ¨éŸ³é¢‘LLMè¯­å¢ƒä¸‹çš„è¡¨ç°å¯èƒ½ä¸å¤Ÿç†æƒ³ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºX-Codecç¼–è§£ç å™¨ï¼Œèå…¥é¢„è®­ç»ƒè¯­ä¹‰ç¼–ç å™¨çš„è¯­ä¹‰ç‰¹å¾ï¼Œé™ä½è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œæå‡è¯­éŸ³åˆæˆç­‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶æ‰©å±•è‡³éè¯­éŸ³åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨éŸ³é¢‘ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>ç°æœ‰éŸ³é¢‘LLMç¼–è§£ç å™¨ä¸»è¦å…³æ³¨æ¶æ„å’Œè§„æ¨¡çš„å¢å¼ºä»¥åŠå¤§æ•°æ®é›†çš„åˆ©ç”¨ã€‚</li>
<li>EnCodecç­‰ç¼–è§£ç å™¨æœ€åˆæ˜¯ä¸ºéŸ³é¢‘å‹ç¼©è®¾è®¡çš„ï¼Œåœ¨éŸ³é¢‘LLMè¯­å¢ƒä¸‹å¯èƒ½å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>X-Codecæ—¨åœ¨è§£å†³ç°æœ‰éŸ³é¢‘LLMç¼–è§£ç å™¨çš„çŸ­æ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒç”ŸæˆéŸ³é¢‘çš„è¯­ä¹‰å®Œæ•´æ€§æ–¹é¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚VALL-Eåœ¨æ–‡æœ¬è½¬å½•æ¡ä»¶ä¸‹ç”Ÿæˆå£°å­¦ä»¤ç‰Œæ—¶ï¼Œç”±äºå£°å­¦ä»¤ç‰Œçš„è¯­ä¹‰è¯¯è§£ï¼Œä¼šå‡ºç°å†…å®¹ä¸å‡†ç¡®å’Œè¾ƒé«˜çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>X-Codecé€šè¿‡èå…¥é¢„è®­ç»ƒè¯­ä¹‰ç¼–ç å™¨çš„è¯­ä¹‰ç‰¹å¾ï¼Œåœ¨é™ä½WERæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæå‡è¯­éŸ³åˆæˆä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>X-Codecçš„ä¼˜åŠ¿ä¸ä»…é™äºè¯­éŸ³åˆæˆï¼Œè¿˜æ‰©å±•è‡³éŸ³ä¹å’Œå£°æ•ˆç”Ÿæˆç­‰éè¯­éŸ³åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-788cc4d6126dce014156c652e21e827a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8188aaf4247a31c6edf4e05d85eafb07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-beeaa767369c1a338c22f952cb659424.jpg" align="middle">
</details>




<h2 id="Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning"><a href="#Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning" class="headerlink" title="Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning"></a>Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning</h2><p><strong>Authors:Shuai Wang, Zhengyang Chen, Kong Aik Lee, Yanmin Qian, Haizhou Li</strong></p>
<p>Speaker individuality information is among the most critical elements within speech signals. By thoroughly and accurately modeling this information, it can be utilized in various intelligent speech applications, such as speaker recognition, speaker diarization, speech synthesis, and target speaker extraction. In this overview, we present a comprehensive review of neural approaches to speaker representation learning from both theoretical and practical perspectives. Theoretically, we discuss speaker encoders ranging from supervised to self-supervised learning algorithms, standalone models to large pretrained models, pure speaker embedding learning to joint optimization with downstream tasks, and efforts toward interpretability. Practically, we systematically examine approaches for robustness and effectiveness, introduce and compare various open-source toolkits in the field. Through the systematic and comprehensive review of the relevant literature, research activities, and resources, we provide a clear reference for researchers in the speaker characterization and modeling field, as well as for those who wish to apply speaker modeling techniques to specific downstream tasks. </p>
<blockquote>
<p>è¯´è¯äººçš„ä¸ªä½“ä¿¡æ¯æ˜¯è¯­éŸ³ä¿¡å·ä¸­æœ€å…³é”®çš„éƒ¨åˆ†ã€‚é€šè¿‡å¯¹è¿™éƒ¨åˆ†ä¿¡æ¯è¿›è¡Œå…¨é¢å‡†ç¡®çš„å»ºæ¨¡ï¼Œå¯ä»¥å°†å…¶åº”ç”¨äºå„ç§æ™ºèƒ½è¯­éŸ³åº”ç”¨ä¸­ï¼Œå¦‚è¯´è¯äººè¯†åˆ«ã€è¯´è¯äººæ—¥è®°åŒ–ã€è¯­éŸ³åˆæˆå’Œç›®æ ‡è¯´è¯äººæå–ç­‰ã€‚åœ¨æœ¬æ¬¡æ¦‚è¿°ä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºå’Œå®é™…ä¸¤ä¸ªè§’åº¦å¯¹ç¥ç»æ–¹æ³•è¿›è¡Œå…¨é¢çš„è¯„è¿°ï¼Œä»¥å­¦ä¹ è¯´è¯äººçš„è¡¨å¾ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è®¨è®ºäº†ä»ç›‘ç£åˆ°è‡ªæˆ‘ç›‘ç£å­¦ä¹ ç®—æ³•çš„è¯´è¯äººç¼–ç å™¨ï¼Œä»ç‹¬ç«‹æ¨¡å‹åˆ°å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»çº¯ç²¹çš„è¯´è¯äººåµŒå…¥å­¦ä¹ åˆ°ä¸ä¸‹æ¸¸ä»»åŠ¡çš„è”åˆä¼˜åŒ–ï¼Œä»¥åŠå‘å¯è§£é‡Šæ€§çš„åŠªåŠ›ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ£€éªŒäº†é²æ£’æ€§å’Œæœ‰æ•ˆæ€§çš„æ–¹æ³•ï¼Œä»‹ç»å¹¶æ¯”è¾ƒäº†è¯¥é¢†åŸŸçš„å„ç§å¼€æºå·¥å…·åŒ…ã€‚é€šè¿‡å¯¹ç›¸å…³æ–‡çŒ®ã€ç ”ç©¶æ´»åŠ¨å’Œèµ„æºçš„ç³»ç»Ÿå’Œå…¨é¢å›é¡¾ï¼Œæˆ‘ä»¬ä¸ºè¯­éŸ³ç‰¹å¾åˆ»ç”»å’Œå»ºæ¨¡é¢†åŸŸçš„ç ”ç©¶äººå‘˜ï¼Œä»¥åŠé‚£äº›å¸Œæœ›å°†è¯´è¯äººå»ºæ¨¡æŠ€æœ¯åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„äººå‘˜æä¾›äº†æ¸…æ™°çš„å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15188v2">PDF</a> Accepted to TASLP</p>
<p><strong>Summary</strong><br>åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•å¯¹è¯´è¯äººç‰¹å¾å­¦ä¹ è¿›è¡Œäº†å…¨é¢æ·±å…¥çš„æ¢è®¨ï¼Œä»ç†è®ºè§’åº¦æ¢è®¨äº†å„ç§ç¼–ç å™¨çš„æ€§èƒ½å’Œåº”ç”¨èŒƒå›´ï¼Œä»å®é™…åº”ç”¨è§’åº¦è¿›è¡Œäº†æ¨¡å‹çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§åˆ†æï¼Œå¹¶ä»‹ç»äº†ç›¸å…³çš„å¼€æºå·¥å…·åŒ…ã€‚è¯¥ç»¼è¿°ä¸ºè¯´è¯äººè¡¨å¾å’Œå»ºæ¨¡é¢†åŸŸçš„ç ”ç©¶äººå‘˜ä»¥åŠå¸Œæœ›å°†è¯´è¯äººå»ºæ¨¡æŠ€æœ¯åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„äººå‘˜æä¾›äº†æ¸…æ™°çš„å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººä¸ªä½“ä¿¡æ¯åœ¨è¯­éŸ³ä¿¡å·ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œå¯ç”¨äºæ™ºèƒ½è¯­éŸ³åº”ç”¨å¦‚è¯´è¯äººè¯†åˆ«ã€è¯´è¯äººåˆ†æ—¶æ®µåŒ–ã€è¯­éŸ³åˆæˆå’Œç›®æ ‡è¯´è¯äººæå–ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ–¹æ³•è¢«å¹¿æ³›åº”ç”¨äºè¯´è¯äººç‰¹å¾å­¦ä¹ ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œç‹¬ç«‹æ¨¡å‹ä¸å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ç­‰ã€‚</li>
<li>è¯´è¯äººåµŒå…¥å­¦ä¹ ä¸ä»…å…³æ³¨æ¨¡å‹æ€§èƒ½ï¼Œè¿˜åŠªåŠ›ä¸ä¸‹æ¸¸ä»»åŠ¡è”åˆä¼˜åŒ–ï¼Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯´è¯äººç‰¹å¾å­¦ä¹ æ¨¡å‹éœ€è¦å…·å¤‡é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>ç»¼è¿°è¯¦ç»†ä»‹ç»äº†å„ç§å¼€æºå·¥å…·åŒ…ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚</li>
<li>è¯¥ç»¼è¿°ä¸ºè¯´è¯äººè¡¨å¾å’Œå»ºæ¨¡é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†æ¸…æ™°çš„å‚è€ƒå’ŒæŒ‡å¯¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-865221afda42c7c4c88832efc9e0749b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f86e39c2559437ccd8459e78536e3049.jpg" align="middle">
</details>




<h2 id="TTSDS-â€“-Text-to-Speech-Distribution-Score"><a href="#TTSDS-â€“-Text-to-Speech-Distribution-Score" class="headerlink" title="TTSDS â€“ Text-to-Speech Distribution Score"></a>TTSDS â€“ Text-to-Speech Distribution Score</h2><p><strong>Authors:Christoph Minixhofer, OndÅ™ej Klejch, Peter Bell</strong></p>
<p>Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period. </p>
<blockquote>
<p>è®¸å¤šæœ€è¿‘å‘å¸ƒçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿäº§ç”Ÿçš„éŸ³é¢‘æ¥è¿‘çœŸå®è¯­éŸ³ã€‚ç„¶è€Œï¼Œéšç€æ–°æ¶æ„ã€æ–¹æ³•å’Œæ•°æ®é›†çš„å‡ºç°ï¼Œéœ€è¦é‡æ–°å®¡è§†TTSçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥ä¾¿ç†è§£æ‰€è·å¾—çš„å®éªŒç»“æœã€‚æˆ‘ä»¬æè®®ä»å¤šä¸ªå› ç´ ç»“åˆè¯„ä¼°åˆæˆè¯­éŸ³çš„è´¨é‡ï¼Œå¦‚è¯­è°ƒã€è¯´è¯äººèº«ä»½å’Œæ¸…æ™°åº¦ç­‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è·å–æ¯ä¸ªå› ç´ çš„å…³è”é¡¹ï¼Œå¹¶æµ‹é‡å®ƒä»¬ä¸çœŸå®è¯­éŸ³æ•°æ®é›†å’Œå™ªå£°æ•°æ®é›†çš„å·®å¼‚æ¥è¯„ä¼°åˆæˆè¯­éŸ³å¯¹çœŸå®è¯­éŸ³çš„æ¨¡æ‹Ÿç¨‹åº¦ã€‚æˆ‘ä»¬å¯¹2008å¹´è‡³2024å¹´é—´å¼€å‘çš„35ä¸ªTTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„å¾—åˆ†ä½œä¸ºæœªåŠ æƒå› ç´ å¹³å‡å€¼ä¸æ¯ä¸ªæ—¶æœŸçš„äººç±»è¯„ä¼°ç»“æœå­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12707v3">PDF</a> SLT 2024</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå‘å¸ƒçš„è®¸å¤šæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆçš„éŸ³é¢‘å·²æ¥è¿‘çœŸå®è¯­éŸ³ã€‚ç„¶è€Œï¼Œéšç€æ–°æ¶æ„ã€æ–¹æ³•å’Œæ•°æ®é›†çš„å‡ºç°ï¼Œéœ€è¦é‡æ–°å®¡è§†TTSçš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºä¸€ç§è¯„ä¼°åˆæˆè¯­éŸ³è´¨é‡çš„æ–¹æ³•ï¼Œç»¼åˆè€ƒè™‘è¯­è°ƒã€è¯´è¯äººèº«ä»½å’Œæ¸…æ™°åº¦ç­‰å¤šä¸ªå› ç´ ã€‚è¯¥æ–¹æ³•é€šè¿‡è·å–æ¯ä¸ªå› ç´ çš„ç›¸å…³æŒ‡æ ‡ï¼Œæµ‹é‡å…¶ä¸çœŸå®è¯­éŸ³æ•°æ®é›†å’Œå™ªå£°æ•°æ®é›†çš„å·®å¼‚æ¥è¯„ä¼°åˆæˆè¯­éŸ³å¯¹çœŸå®è¯­éŸ³çš„æ¨¡æ‹Ÿç¨‹åº¦ã€‚ä½œè€…å¯¹2008å¹´è‡³2024å¹´é—´å¼€å‘çš„35ä¸ªTTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜å…¶è®¡ç®—çš„æ— æƒé‡å¹³å‡å› ç´ å¾—åˆ†ä¸å„ä¸ªæ—¶æœŸçš„äººç±»è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSç³»ç»Ÿç”Ÿæˆçš„éŸ³é¢‘å·²æ¥è¿‘çœŸå®è¯­éŸ³ï¼Œéœ€è¦é‡æ–°å®¡è§†å…¶è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>è¯„ä¼°åˆæˆè¯­éŸ³è´¨é‡åº”ç»¼åˆè€ƒè™‘è¯­è°ƒã€è¯´è¯äººèº«ä»½å’Œæ¸…æ™°åº¦ç­‰å¤šä¸ªå› ç´ ã€‚</li>
<li>é€šè¿‡æµ‹é‡ä¸çœŸå®è¯­éŸ³æ•°æ®é›†å’Œå™ªå£°æ•°æ®é›†çš„å·®å¼‚æ¥è¯„ä¼°åˆæˆè¯­éŸ³çš„æ¨¡æ‹Ÿç¨‹åº¦ã€‚</li>
<li>æœ¬æ–‡å¯¹è¿‡å»å¤šå¹´å¼€å‘çš„TTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œç»¼åˆå„å› ç´ çš„å¹³å‡å¾—åˆ†ä¸äººç±»è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</li>
<li>æ­¤è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿä¸ºTTSç³»ç»Ÿçš„è¿›æ­¥å’Œå‘å±•æä¾›æœ‰åŠ›çš„è¡¡é‡æ ‡å‡†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d7824eb7f71b474fd46332370f442de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f7ffc0f4e39850306419bd41ad9476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f17fae8c43b1b2b890c4c7493d4ca4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27bb06ebfa2bbd737eb3089e0df3d026.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89e6fbc1e8abf28dba60aa35c2760d78.jpg" align="middle">
</details>




<h2 id="Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers"><a href="#Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers" class="headerlink" title="Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers"></a>Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers</h2><p><strong>Authors:Krzysztof Choromanski, Arijit Sehanobish, Somnath Basu Roy Chowdhury, Han Lin, Avinava Dubey, Tamas Sarlos, Snigdha Chaturvedi</strong></p>
<p>We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular low displacement rank) for integrating tensor fields defined on weighted trees. Several applications of the resulting fast tree-field integrators (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) Topological Transformers (TTs) (Choromanski et al., 2022) for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as three extra learnable parameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains. Importantly, most of FTFIs are exact methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide 5.7-13x speedups. We also provide an extensive theoretical analysis of our methods. </p>
<blockquote>
<p>æˆ‘ä»¬åŸºäºç»“æ„åŒ–çŸ©é˜µç†è®ºï¼ˆå°¤å…¶æ˜¯ä½ä½ç§»ç§©ï¼‰æå‡ºäº†ä¸€ç§æ–°çš„å¿«é€Ÿå¤šé¡¹å¼å¯¹æ•°çº¿æ€§ç®—æ³•ï¼Œç”¨äºå¯¹å®šä¹‰åœ¨åŠ æƒæ ‘ä¸Šçš„å¼ é‡åœºè¿›è¡Œç§¯åˆ†ã€‚å±•ç¤ºäº†ç”±æ­¤äº§ç”Ÿçš„å¿«é€Ÿæ ‘åœºç§¯åˆ†å™¨ï¼ˆFTFIsï¼‰çš„å‡ ä¸ªåº”ç”¨ï¼ŒåŒ…æ‹¬ï¼ˆaï¼‰æ ‘åº¦é‡é€¼è¿‘å›¾åº¦é‡ï¼Œï¼ˆbï¼‰å›¾åˆ†ç±»ï¼Œï¼ˆcï¼‰ç½‘æ ¼å»ºæ¨¡ï¼Œä»¥åŠæœ€åï¼ˆdï¼‰ç”¨äºå›¾åƒçš„æ‹“æ‰‘è½¬æ¢å™¨ï¼ˆTTsï¼‰ï¼ˆChoromanskiç­‰äººï¼Œ2022å¹´ï¼‰ã€‚é’ˆå¯¹æ‹“æ‰‘è½¬æ¢å™¨ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰ä¸‰ä¸ªé¢å¤–å¯å­¦ä¹ å‚æ•°çš„æ–°å‹ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRPEï¼‰æ©ç æœºåˆ¶ï¼Œå¸¦æ¥äº†1.0-1.5%çš„å‡†ç¡®ç‡æå‡ã€‚é‡è¦çš„æ˜¯ï¼Œå¤§å¤šæ•°FTFIsæ˜¯ç²¾ç¡®æ–¹æ³•ï¼Œå› æ­¤åœ¨æ•°å€¼ä¸Šä¸æš´åŠ›æ±‚è§£ç­‰æ•ˆã€‚å½“åº”ç”¨äºå…·æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„å›¾æ—¶ï¼Œè¿™äº›ç²¾ç¡®ç®—æ³•å¯æä¾›5.7-13å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬è¿˜å¯¹æ‰€ç”¨æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„ç†è®ºåˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15881v2">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong><br>     åŸºäºç»“æ„åŒ–çŸ©é˜µç†è®ºï¼ˆç‰¹åˆ«æ˜¯ä½ä½ç§»ç§©ï¼‰æå‡ºçš„æ–°å‹å¿«é€Ÿå¤šé¡¹å¼å¯¹æ•°çº¿æ€§ç®—æ³•ï¼Œç”¨äºæ•´åˆå®šä¹‰åœ¨åŠ æƒæ ‘ä¸Šçš„å¼ é‡åœºã€‚è¯¥ç®—æ³•åŒ…å«å¤šç§åº”ç”¨åœºæ™¯ï¼Œå¦‚è¿‘ä¼¼å›¾æŒ‡æ ‡ä¸æ ‘æŒ‡æ ‡ã€å›¾åˆ†ç±»ã€ç½‘æ ¼å»ºæ¨¡å’Œå›¾åƒæ‹“æ‰‘å˜æ¢å™¨ç­‰ã€‚é’ˆå¯¹æ‹“æ‰‘å˜æ¢å™¨ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æ›´å°‘å¯å­¦ä¹ å‚æ•°çš„æ–°ç›¸å¯¹ä½ç½®ç¼–ç æ©ç æœºåˆ¶ï¼Œå¸¦æ¥äº†é¢å¤–çš„å‡†ç¡®æ€§æå‡ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°FTFIsç®—æ³•æ˜¯ç²¾ç¡®æ–¹æ³•ï¼Œåœ¨åº”ç”¨äºå…·æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„å›¾å½¢æ—¶ï¼Œæä¾›äº†é«˜è¾¾13å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºç»“æ„åŒ–çŸ©é˜µç†è®ºçš„æ–°å‹å¿«é€Ÿå¤šé¡¹å¼å¯¹æ•°çº¿æ€§ç®—æ³•ç”¨äºæ•´åˆå¼ é‡åœºã€‚</li>
<li>FTFIsç®—æ³•é€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬å›¾ä¸æ ‘æŒ‡æ ‡çš„è¿‘ä¼¼ã€å›¾åˆ†ç±»ã€ç½‘æ ¼å»ºæ¨¡å’Œå›¾åƒæ‹“æ‰‘å˜æ¢å™¨ã€‚</li>
<li>é’ˆå¯¹æ‹“æ‰‘å˜æ¢å™¨ï¼Œæå‡ºäº†å…·æœ‰æ›´å°‘å¯å­¦ä¹ å‚æ•°çš„æ–°ç›¸å¯¹ä½ç½®ç¼–ç æ©ç æœºåˆ¶ã€‚</li>
<li>æ–°æœºåˆ¶å¸¦æ¥äº†é¢å¤–çš„å‡†ç¡®æ€§æå‡ã€‚</li>
<li>å¤§å¤šæ•°FTFIsç®—æ³•æ˜¯ç²¾ç¡®çš„ï¼Œç­‰ä»·äºæš´åŠ›è®¡ç®—æ–¹æ³•ã€‚</li>
<li>åœ¨åº”ç”¨äºå…·æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„å›¾å½¢æ—¶ï¼ŒFTFIsç®—æ³•æä¾›äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-631340e9e632a5f32dd7eabce23fea29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fc5affe036c5a7c010539a04991aed1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad831467d9c313e96be11881ef19e14.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰çš„è¿›æ­¥å¸¦æ¥çš„å„ç§å¨èƒï¼Œå¯¹åˆæˆè¯­éŸ³çš„å¯é è¿½è¸ªéœ€æ±‚æ—¥ç›Šè¿«åˆ‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»»åŠ¡å¤„ç†æ–¹æ³•æ˜¯åœ¨ç”ŸæˆéŸ³é¢‘åå•ç‹¬æ·»åŠ æ°´å°ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢å½±å“äº†è¯­éŸ³è´¨é‡ï¼Œä¹Ÿå½±å“äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨ç¨³å¥æ€§å’Œçµæ´»æ€§æ–¹é¢ä¹Ÿå­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TraceableSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹TTSæ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ï¼Œæé«˜äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¸§çº§çš„æ°´å°å°åˆ¶å’Œæå–æŠ€æœ¯ï¼Œæé«˜äº†å¯¹æŠ—é‡æ–°æ‹¼æ¥æ”»å‡»çš„ç¨³å¥æ€§å’Œæ“ä½œçš„æ—¶åºçµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨ä¸æ˜“å¯Ÿè§‰æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠµæŠ—é‡æ–°æ‹¼æ¥æ”»å‡»æ–¹é¢ï¼Œä¼˜äºVALL-Eæˆ–HiFicodecç­‰å•ç‹¬ä½¿ç”¨WavMarkçš„å¼ºåŸºçº¿ã€‚å®ƒè¿˜å¯ä»¥åº”ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjzser/TraceableSpeechæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„è¿›æ­¥å¸¦æ¥äº†å¤šç§å¨èƒï¼Œéœ€è¦è¿›è¡Œå¯é çš„è¿½è¸ªåˆæˆè¯­éŸ³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¿½è¸ªæ–¹æ³•é€šè¿‡åœ¨ç”Ÿæˆåå•ç‹¬æ·»åŠ æ°´å°åˆ°éŸ³é¢‘ä¸­ï¼Œè¿™ç§æ–¹æ³•æ—¢å½±å“è¯­éŸ³è´¨é‡åˆå½±å“æ°´å°çš„ä¸å¯å¯Ÿè§‰æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TraceableSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„TTSæ¨¡å‹ï¼Œå¯ç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ï¼Œæé«˜äº†æ°´å°çš„ä¸å¯å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†å¸§çº§çš„æ°´å°å°å…¥å’Œæå–æ–¹æ³•ï¼Œæé«˜äº†å¯¹æŠ—æ‹¼æ¥æ”»å‡»çš„ç¨³å¥æ€§å’Œæ“ä½œçš„æ—¶æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨ä¸å¯å¯Ÿè§‰æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠ—æ‹¼æ¥æ”»å‡»æ–¹é¢ä¼˜äºä½¿ç”¨WavMarkçš„VALL-Eæˆ–HiFicodecç­‰å¼ºåŸºçº¿æ¨¡å‹ï¼Œä¸”é€‚ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSçš„è¿›æ­¥å¸¦æ¥äº†è¿½è¸ªåˆæˆè¯­éŸ³çš„å¿…è¦æ€§ï¼Œå› ä¸ºå­˜åœ¨å¤šç§æ½œåœ¨å¨èƒã€‚</li>
<li>ç°æœ‰è¿½è¸ªæ–¹æ³•é€šè¿‡ç”Ÿæˆåæ·»åŠ æ°´å°åˆ°éŸ³é¢‘ä¸­ï¼Œå­˜åœ¨è¯­éŸ³è´¨é‡å’Œæ°´å°ä¸å¯å¯Ÿè§‰æ€§çš„é—®é¢˜ã€‚</li>
<li>TraceableSpeechæ˜¯ä¸€ç§æ–°å‹çš„TTSæ¨¡å‹ï¼Œèƒ½ç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ï¼Œæé«˜æ°´å°çš„ä¸å¯å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚</li>
<li>TraceableSpeechå®ç°äº†å¸§çº§çš„æ°´å°å°å…¥å’Œæå–ï¼Œæé«˜äº†å¯¹æŠ—æ‹¼æ¥æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>TraceableSpeechåœ¨ä¸å¯å¯Ÿè§‰æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠ—æ‹¼æ¥æ”»å‡»æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚</li>
<li>TraceableSpeeché€‚ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis"><a href="#Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis" class="headerlink" title="Style Mixture of Experts for Expressive Text-To-Speech Synthesis"></a>Style Mixture of Experts for Expressive Text-To-Speech Synthesis</h2><p><strong>Authors:Ahad Jawaid, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman</strong></p>
<p>Recent advances in style transfer text-to-speech (TTS) have improved the expressiveness of synthesized speech. However, encoding stylistic information (e.g., timbre, emotion, and prosody) from diverse and unseen reference speech remains a challenge. This paper introduces StyleMoE, an approach that addresses the issue of learning averaged style representations in the style encoder by creating style experts that learn from subsets of data. The proposed method replaces the style encoder in a TTS framework with a Mixture of Experts (MoE) layer. The style experts specialize by learning from subsets of reference speech routed to them by the gating network, enabling them to handle different aspects of the style space. As a result, StyleMoE improves the style coverage of the style encoder for style transfer TTS. Our experiments, both objective and subjective, demonstrate improved style transfer for diverse and unseen reference speech. The proposed method enhances the performance of existing state-of-the-art style transfer TTS models and represents the first study of style MoE in TTS. </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„é£æ ¼è¿ç§»æ–¹é¢æœ€è¿‘çš„è¿›å±•æé«˜äº†åˆæˆè¯­éŸ³çš„è¡¨ç°åŠ›ã€‚ç„¶è€Œï¼Œä»å¤šæ ·ä¸”æœªçŸ¥çš„å‚è€ƒè¯­éŸ³ä¸­ç¼–ç é£æ ¼ä¿¡æ¯ï¼ˆå¦‚éŸ³è´¨ã€æƒ…æ„Ÿå’Œè¯­è°ƒï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†StyleMoEæ–¹æ³•ï¼Œé€šè¿‡åˆ›å»ºä»æ•°æ®å­é›†å­¦ä¹ çš„é£æ ¼ä¸“å®¶æ¥è§£å†³é£æ ¼ç¼–ç å™¨ä¸­å­¦ä¹ å¹³å‡é£æ ¼è¡¨ç¤ºçš„é—®é¢˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å±‚æ›¿æ¢äº†TTSæ¡†æ¶ä¸­çš„é£æ ¼ç¼–ç å™¨ã€‚é£æ ¼ä¸“å®¶é€šè¿‡ä»è·¯ç”±åˆ°å®ƒä»¬çš„å‚è€ƒè¯­éŸ³å­é›†å­¦ä¹ æ¥ä¸“ä¸šåŒ–ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¤„ç†é£æ ¼ç©ºé—´çš„ä¸åŒæ–¹é¢ã€‚å› æ­¤ï¼ŒStyleMoEæé«˜äº†é£æ ¼ç¼–ç å™¨åœ¨é£æ ¼è¿ç§»TTSä¸­çš„é£æ ¼è¦†ç›–ç‡ã€‚æˆ‘ä»¬çš„å®¢è§‚å’Œä¸»è§‚å®éªŒéƒ½è¯æ˜äº†åœ¨å¤šæ ·ä¸”æœªçŸ¥çš„å‚è€ƒè¯­éŸ³ä¸­é£æ ¼è¿ç§»çš„æ”¹è¿›ã€‚æ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†ç°æœ‰æœ€å…ˆè¿›çš„é£æ ¼è¿ç§»TTSæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯TTSä¸­é£æ ¼MoEçš„é¦–æ¬¡ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03637v2">PDF</a> Published in Audio Imagination: NeurIPS 2024 Workshop</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰çš„é£æ ¼è¿ç§»æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œæé«˜äº†åˆæˆè¯­éŸ³çš„è¡¨è¾¾æ€§ã€‚ç„¶è€Œï¼Œä»å¤šæ ·ä¸”æœªçŸ¥çš„å‚è€ƒè¯­éŸ³ä¸­ç¼–ç é£æ ¼ä¿¡æ¯ï¼ˆå¦‚éŸ³è´¨ã€æƒ…æ„Ÿå’Œè¯­è°ƒï¼‰ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†StyleMoEæ–¹æ³•ï¼Œé€šè¿‡åˆ›å»ºé£æ ¼ä¸“å®¶æ¥è§£å†³é£æ ¼ç¼–ç å™¨ä¸­å­¦ä¹ å¹³å‡é£æ ¼è¡¨ç¤ºçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å±‚æ›¿æ¢TTSæ¡†æ¶ä¸­çš„é£æ ¼ç¼–ç å™¨ã€‚é£æ ¼ä¸“å®¶é€šè¿‡ä»è·¯ç”±åˆ°å®ƒä»¬çš„å‚è€ƒè¯­éŸ³å­é›†å­¦ä¹ ä¸“ä¸šçŸ¥è¯†ï¼Œèƒ½å¤Ÿå¤„ç†é£æ ¼ç©ºé—´çš„ä¸åŒæ–¹é¢ã€‚å› æ­¤ï¼ŒStyleMoEæé«˜äº†é£æ ¼ç¼–ç å™¨çš„é£æ ¼è¦†ç›–èƒ½åŠ›ï¼Œæ”¹å–„äº†å¤šæ ·ä¸”æœªçŸ¥å‚è€ƒè¯­éŸ³çš„é£æ ¼è¿ç§»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»è§‚å’Œå®¢è§‚ä¸Šéƒ½æé«˜äº†é£æ ¼è¿ç§»çš„æ•ˆæœï¼Œå¹¶å¢å¼ºäº†ç°æœ‰æœ€å…ˆè¿›çš„é£æ ¼è¿ç§»TTSæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™æ˜¯MoEåœ¨TTSä¸­çš„é¦–æ¬¡ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸTTSçš„é£æ ¼è¿ç§»æŠ€æœ¯æé«˜äº†åˆæˆè¯­éŸ³çš„è¡¨è¾¾æ€§ã€‚</li>
<li>ä»å¤šæ ·ä¸”æœªçŸ¥çš„å‚è€ƒè¯­éŸ³ä¸­ç¼–ç é£æ ¼ä¿¡æ¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>StyleMoEæ–¹æ³•é€šè¿‡åˆ›å»ºé£æ ¼ä¸“å®¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>é£æ ¼ä¸“å®¶é€šè¿‡ä»è·¯ç”±åˆ°å®ƒä»¬çš„å‚è€ƒè¯­éŸ³å­é›†å­¦ä¹ ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>StyleMoEæé«˜äº†é£æ ¼ç¼–ç å™¨çš„é£æ ¼è¦†ç›–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜StyleMoEåœ¨ä¸»è§‚å’Œå®¢è§‚ä¸Šéƒ½æé«˜äº†é£æ ¼è¿ç§»çš„æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8071a305826f1eb16863c0591528d7a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6d6716de85f2a3d1185b60e764b04a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f2d3c9d4d3c06e0b1f2bb0a2e078b5.jpg" align="middle">
</details>




<h2 id="DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models"><a href="#DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models" class="headerlink" title="DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models"></a>DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models</h2><p><strong>Authors:Jingyi Chen, Ju-Seung Byun, Micha Elsner, Andrew Perrault</strong></p>
<p>Recent advancements in generative models have sparked a significant interest within the machine learning community. Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech. Studies such as those by Lee et al. (2023), Black et al. (2023), Wang et al. (2023), and Fan et al. (2024) illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis. However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models. In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system (Saeki et al., 2022) as a proxy loss. We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model (Mittag et al., 2021) and human preference experiments for further evaluation. Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆæ¨¡å‹çš„æ–°è¿›å±•åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…´è¶£ã€‚ç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè¯­éŸ³åˆæˆæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚Leeç­‰äººï¼ˆ2023å¹´ï¼‰ã€Blackç­‰äººï¼ˆ2023å¹´ï¼‰ã€Wangç­‰äººï¼ˆ2023å¹´ï¼‰å’ŒFanç­‰äººï¼ˆ2024å¹´ï¼‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯ä»¥å¢å¼ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡å‹ä¸è¯­éŸ³åˆæˆä¸­æ‰€ç”¨æ¨¡å‹çš„ç»“æ„å·®å¼‚ï¼Œå°šä¸æ¸…æ¥šRLHFæ˜¯å¦ä¹Ÿèƒ½åŒæ ·æœ‰ç›Šäºè¯­éŸ³åˆæˆæ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†RLHFåœ¨åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„å®é™…åº”ç”¨ï¼Œåˆ©ç”¨ä¸œäº¬å¤§å­¦SaruLabçš„MOSé¢„æµ‹ç³»ç»Ÿæ‰€é¢„æµ‹çš„å‡å€¼æ„è§åˆ†ï¼ˆMOSï¼‰ä½œä¸ºä»£ç†æŸå¤±ã€‚æˆ‘ä»¬ä»‹ç»äº†æ‰©æ•£æ¨¡å‹æŸå¤±å¼•å¯¼RLç­–ç•¥ä¼˜åŒ–ï¼ˆDLPOï¼‰ï¼Œå¹¶å°†å…¶ä¸å…¶ä»–RLHFæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¿›ä¸€æ­¥é‡‡ç”¨NISQAè¯­éŸ³è´¨é‡å’Œè‡ªç„¶åº¦è¯„ä¼°æ¨¡å‹ä»¥åŠäººç±»åå¥½å®éªŒè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒRLHFå¯ä»¥å¢å¼ºåŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”DLPOèƒ½æ›´å¥½åœ°æ”¹å–„æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªç„¶ã€é«˜è´¨é‡è¯­éŸ³éŸ³é¢‘çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14632v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸç”Ÿæˆæ¨¡å‹è¿›å±•æ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè¯­éŸ³åˆæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰èƒ½æå‡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢çš„è¡¨ç°ã€‚æœ¬æ–‡æ¢ç´¢äº†RLHFåœ¨åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„å®é™…åº”ç”¨ï¼Œé‡‡ç”¨ä¸œäº¬å¤§å­¦SaruLabçš„MOSé¢„æµ‹ç³»ç»Ÿä½œä¸ºä»£ç†æŸå¤±ã€‚é€šè¿‡å¯¹æ¯”å…¶ä»–RLHFæ–¹æ³•å’Œé‡‡ç”¨NISQAè¯­éŸ³è´¨é‡å’Œè‡ªç„¶åº¦è¯„ä¼°æ¨¡å‹çš„äººç±»åå¥½å®éªŒï¼Œå‘ç°RLHFèƒ½æå‡åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯DLPOæ–¹æ³•åœ¨ç”Ÿæˆè‡ªç„¶é«˜è´¨é‡è¯­éŸ³æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. ç”Ÿæˆæ¨¡å‹é¢†åŸŸå‡ºç°æ–°çš„é‡è¦è¿›å±•ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè¯­éŸ³åˆæˆä¸Šå…·å¤‡å‡ºè‰²æ€§èƒ½ã€‚<br>     2. å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯ä»¥æå‡æ‰©æ•£æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒåˆæˆæ–¹é¢å·²æœ‰ç ”ç©¶è¯å®ã€‚<br>     3. æœ¬æ–‡æ¢ç´¢äº†RLHFåœ¨åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨ã€‚<br>     4. é‡‡ç”¨ä¸œäº¬å¤§å­¦SaruLabçš„MOSé¢„æµ‹ç³»ç»Ÿä½œä¸ºä»£ç†æŸå¤±è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚<br>     5. å¯¹æ¯”äº†ä¸åŒçš„RLHFæ–¹æ³•å’Œé‡‡ç”¨NISQAè¯­éŸ³è¯„ä¼°æ¨¡å‹çš„äººç±»åå¥½å®éªŒã€‚<br>     6. ç»“æœæ˜¾ç¤ºRLHFèƒ½æå‡åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c14030629a7cc9a613b4afe409c46451.jpg" align="middle">
</details>




<h2 id="VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization"><a href="#VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization" class="headerlink" title="VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization"></a>VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization</h2><p><strong>Authors:Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai</strong></p>
<p>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the <a target="_blank" rel="noopener" href="https://vimtextspotter.github.io/">https://VimTextSpotter.github.io</a>. </p>
<blockquote>
<p>æ–‡æœ¬è¯†åˆ«æ¶‰åŠä»å›¾åƒæˆ–è§†é¢‘åºåˆ—ä¸­æå–æ–‡æœ¬ä¿¡æ¯ï¼Œåœ¨è·¨åŸŸé€‚åº”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å›¾åƒåˆ°å›¾åƒå’Œå›¾åƒåˆ°è§†é¢‘çš„æ¨å¹¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºVimTSã€‚å®ƒé€šè¿‡å®ç°ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ›´å¥½ååŒä½œç”¨ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨å¹¿èƒ½åŠ›ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæç¤ºæŸ¥è¯¢ç”Ÿæˆæ¨¡å—å’Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ï¼Œæœ‰æ•ˆåœ°å°†åŸå§‹çš„å•ä»»åŠ¡æ¨¡å‹è½¬æ¢ä¸ºé€‚åˆå›¾åƒå’Œè§†é¢‘åœºæ™¯çš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œå¹¶ä¸”å¢åŠ äº†æå°‘çš„å‚æ•°ã€‚æç¤ºæŸ¥è¯¢ç”Ÿæˆæ¨¡å—ä¿ƒè¿›äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ˜¾å¼äº¤äº’ï¼Œè€Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨å¸®åŠ©æ¨¡å‹åŠ¨æ€å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„é€‚å½“ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»¥æ›´ä½çš„æˆæœ¬ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œæˆ‘ä»¬åˆ©ç”¨å†…å®¹å˜å½¢åœºï¼ˆCoDeFï¼‰ç®—æ³•æå‡ºäº†ä¸€ä¸ªåˆæˆè§†é¢‘æ–‡æœ¬æ•°æ®é›†ï¼ˆVTD-368kï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•çº¦2.6%ï¼Œå¦‚TTåˆ°IC15ã€CTW1500åˆ°TTå’ŒTTåˆ°CTW1500ç­‰ã€‚å¯¹äºè§†é¢‘çº§åˆ«çš„è·¨åŸŸé€‚åº”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”šè‡³åœ¨ICDAR2015è§†é¢‘å’ŒDSText v2ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„ç«¯åˆ°ç«¯è§†é¢‘è¯†åˆ«æ–¹æ³•ï¼Œåœ¨MOTAæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†çº¦5.5%ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å›¾åƒçº§åˆ«çš„æ•°æ®ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯æ˜äº†ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆè·¨åŸŸåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„VimTSæ¨¡å‹éœ€è¦æ›´å°‘çš„å‚æ•°å’Œæ•°æ®ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨VimTextSpotter.github.ioä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19652v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•VimTSï¼Œé€šè¿‡å®ç°ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ›´å¥½ååŒä½œç”¨ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Prompt Queriesç”Ÿæˆæ¨¡å—å’Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ï¼Œä»¥å°†åŸå§‹å•ä»»åŠ¡æ¨¡å‹æœ‰æ•ˆåœ°è½¬æ¢ä¸ºé€‚åˆå›¾åƒå’Œè§†é¢‘åœºæ™¯çš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œä¸”åªéœ€æå°‘é‡çš„é¢å¤–å‚æ•°ã€‚Prompt Queriesç”Ÿæˆæ¨¡å—ä¿ƒè¿›äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ˜¾å¼äº¤äº’ï¼Œè€Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨å¸®åŠ©æ¨¡å‹åŠ¨æ€å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„é€‚å½“ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»¥è¾ƒä½çš„æˆæœ¬ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œæœ¬æ–‡åˆ©ç”¨å†…å®¹å˜å½¢åœºç®—æ³•æå‡ºäº†ä¸€ä¸ªåˆæˆè§†é¢‘æ–‡æœ¬æ•°æ®é›†ï¼ˆVTD-368kï¼‰ã€‚åœ¨å…­ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚TTåˆ°IC15ç­‰ï¼Œæœ¬æ–‡çš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•å¹³å‡æé«˜äº†2.6%ã€‚å¯¹äºè§†é¢‘çº§åˆ«çš„è·¨åŸŸé€‚åº”ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨ICDAR2015è§†é¢‘å’ŒDSText v2ä¸Šç”šè‡³è¶…è¿‡äº†ä¹‹å‰çš„ç«¯åˆ°ç«¯è§†é¢‘è¯†åˆ«æ–¹æ³•ï¼Œåœ¨MOTAæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†5.5%ï¼Œä»…ä½¿ç”¨å›¾åƒçº§åˆ«çš„æ•°æ®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è·¨åŸŸåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹é¢çš„å±€é™æ€§ï¼Œä¸VimTSæ¨¡å‹ç›¸æ¯”ï¼Œå…¶éœ€è¦æ›´å°‘çš„å‚æ•°å’Œæ•°æ®ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://vimtextspotter.github.ioä¸Šæä¾›./">https://VimTextSpotter.github.ioä¸Šæä¾›ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VimTSæ–¹æ³•é€šè¿‡å®ç°ä¸åŒä»»åŠ¡ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†Prompt Queriesç”Ÿæˆæ¨¡å—å’Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ï¼Œå°†å•ä»»åŠ¡æ¨¡å‹è½¬æ¢ä¸ºå¤šä»»åŠ¡æ¨¡å‹ï¼Œé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘åœºæ™¯ï¼Œä¸”å‚æ•°å¢åŠ å°‘ã€‚</li>
<li>Prompt Queriesç”Ÿæˆæ¨¡å—ä¿ƒè¿›ä»»åŠ¡é—´çš„äº¤äº’ï¼Œè€Œä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨ä½¿æ¨¡å‹èƒ½åŠ¨æ€å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨å†…å®¹å˜å½¢åœºç®—æ³•æå‡ºäº†åˆæˆè§†é¢‘æ–‡æœ¬æ•°æ®é›†VTD-368kï¼Œç”¨äºå­¦ä¹ æ—¶é—´åºåˆ—ä¿¡æ¯ã€‚</li>
<li>VimTSæ–¹æ³•åœ¨å¤šä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹³å‡æé«˜2.6%ã€‚</li>
<li>åœ¨è§†é¢‘çº§åˆ«çš„è·¨åŸŸé€‚åº”æ–¹é¢ï¼ŒVimTSæ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æé«˜5.5%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6e7e98614248d3c744cdf84eac7f35e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20ddf5ee8d45f8d89b38acbfb080a543.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef497ce9045e2a79784ed651cb501142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a70a938a0b3b4ad66f5e15abd6215c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-732e80b5850a49baa56fae267715864a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f08976df42514822cf5c9d5706cf73a.jpg" align="middle">
</details>




<h2 id="Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting"><a href="#Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting" class="headerlink" title="Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting"></a>Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting</h2><p><strong>Authors:Haolin Chen, Philip N. Garner</strong></p>
<p>We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained modelâ€™s inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸»è¦å—åˆ°æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„é€‚åº”æ€§çš„æ¿€åŠ±ï¼›ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºæ›´é€šç”¨çš„å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯æ‰§è¡Œæ­¤ç±»é€‚åº”æ€§çš„é€‚å½“æ¡†æ¶ã€‚ç„¶è€Œï¼Œç¾éš¾æ€§é—å¿˜ä»ç„¶æ˜¯PEFTçš„ä¸€ä¸ªé—®é¢˜ï¼Œä¼šæŸå®³é¢„è®­ç»ƒæ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåªè¦å¯ä»¥è®¡ç®—å¾®è°ƒå±‚çš„å‚æ•°å˜åŒ–æ˜¯å¯å¾®åˆ†çš„ï¼Œç°æœ‰çš„è´å¶æ–¯å­¦ä¹ æŠ€æœ¯å°±å¯ä»¥åº”ç”¨äºPEFTä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ä¸€ç³»åˆ—è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³åˆæˆä»»åŠ¡çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å»ºç«‹çš„æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼æ–¹æ³•ï¼ŒåŒ…æ‹¬å¯¹è§’çº¿å’Œå…‹ç½—å†…å…‹å› å­åˆ†è§£æ–¹æ³•ï¼Œå¯¹ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œæ­£åˆ™åŒ–PEFTï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬åœ¨é¢„è®­ç»ƒçŸ¥è¯†ä¿ç•™æ–¹é¢çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå…‹æœç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶ä¸ä¼šé™ä½å¾®è°ƒæ€§èƒ½ï¼Œå¹¶ä¸”ä½¿ç”¨å…‹ç½—å†…å…‹å› å­åˆ†è§£è¿‘ä¼¼æ¯”ä½¿ç”¨å¯¹è§’çº¿è¿‘ä¼¼èƒ½æ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.12220v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä¸»è¦æ¢è®¨äº†æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´é—®é¢˜ï¼Œå¹¶æå‡ºåˆ©ç”¨æ›´é€šç”¨çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ¡†æ¶è¿›è¡Œæ­¤ç±»è°ƒæ•´ã€‚ç„¶è€Œï¼ŒPEFTå­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¼šæŸå®³é¢„è®­ç»ƒæ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ã€‚ä½œè€…å±•ç¤ºäº†ç°æœ‰çš„è´å¶æ–¯å­¦ä¹ æŠ€æœ¯å¯ä»¥åº”ç”¨äºPEFTä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œåªè¦èƒ½å¤Ÿè®¡ç®—å¾®è°ƒå±‚çš„å‚æ•°å·®åˆ†è½¬ç§»ã€‚ä½œè€…åœ¨ä¸€ç³»åˆ—è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³åˆæˆä»»åŠ¡å®éªŒä¸­ä½¿ç”¨Laplaceè¿‘ä¼¼æ–¹æ³•ï¼ˆåŒ…æ‹¬å¯¹è§’çº¿å’ŒKroneckeråˆ†è§£æ–¹æ³•ï¼‰å¯¹PEFTè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶ä¸ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¯”è¾ƒå…¶ä¿å­˜é¢„è®­ç»ƒçŸ¥è¯†çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å…‹æœç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶ä¸å½±å“å¾®è°ƒæ€§èƒ½ï¼Œä¸”ä½¿ç”¨Kroneckeråˆ†è§£è¿‘ä¼¼æ¯”ä½¿ç”¨å¯¹è§’çº¿æ–¹æ³•æ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä¸»è¦è®¨è®ºæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´é—®é¢˜ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯ä¸€ä¸ªé€‚å½“çš„æ¡†æ¶æ¥è¿›è¡Œæ­¤ç±»è°ƒæ•´ã€‚</li>
<li>PEFTå­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼ŒæŸå®³é¢„è®­ç»ƒæ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ã€‚</li>
<li>ç°æœ‰çš„è´å¶æ–¯å­¦ä¹ æŠ€æœ¯å¯ä»¥é˜²æ­¢PEFTä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Œå‰ææ˜¯èƒ½è®¡ç®—å¾®è°ƒå±‚çš„å‚æ•°å·®åˆ†è½¬ç§»ã€‚</li>
<li>ä½œè€…ä½¿ç”¨Laplaceè¿‘ä¼¼æ–¹æ³•ï¼ˆåŒ…æ‹¬å¯¹è§’çº¿å’ŒKroneckeråˆ†è§£ï¼‰æ¥æ­£åˆ™åŒ–PEFTï¼Œå¹¶ä¸ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¯”è¾ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Kroneckeråˆ†è§£è¿‘ä¼¼å¯ä»¥æ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91d17a7c8f1b6a0625fecb03c05952c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6417acfdbfc3b9b7a1ef71e8297497.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3793a3389ce1451686ef4ed95b62ef38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-035ba61f5cf002bdc4724210bc479027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-111842636e0d3e93e65cd94d73b190bc.jpg" align="middle">
</details>




<h2 id="Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation"><a href="#Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation" class="headerlink" title="Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation"></a>Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation</h2><p><strong>Authors:Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Soyoon Kim, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Jung-Woo Ha, Sungroh Yoon, Kang Min Yoo</strong></p>
<p>Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm">https://github.com/naver-ai/usdm</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶å·¥ä½œæ˜¾ç¤ºï¼Œåœ¨æ‰©å¤§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä»¥ç›´æ¥ç†è§£å’Œåˆæˆè¯­éŸ³æ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„å»ºæ¨¡å£è¯­å¯¹è¯çš„ç­–ç•¥ä»ç„¶éš¾ä»¥æ‰æ‘¸ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„è°ƒæŸ¥ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„è¯­éŸ³æ–‡æœ¬LLMæ¡†æ¶ï¼Œå³ç»Ÿä¸€å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆUSDMï¼‰ï¼Œæ—¨åœ¨ç”Ÿæˆä¸ç»™å®šè¾“å…¥è¯­éŸ³ç›¸å…³çš„è¿è´¯å£è¯­å“åº”ï¼Œè€Œæ— éœ€ä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚æˆ‘ä»¬å·²ç»éªŒè¯äº†åŒ…å«ä¸»è¦å«æœ‰è¯­ä¹‰ä¿¡æ¯çš„è¯­éŸ³ç¬¦å·ä¸­çš„è¯­è°ƒï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªåŸºç¡€æ¥æ„å»ºä¸€ä¸ªæ³¨å…¥è¯­è°ƒçš„è¯­éŸ³æ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­éŸ³æ–‡æœ¬é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæé«˜äº†è·¨æ¨¡å¼è¯­ä¹‰çš„æ•è·èƒ½åŠ›.ä¸ºäº†æ„å»ºUSDMï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šæ­¥éª¤å£è¯­å¯¹è¯æ¨¡æ¿å¯¹è¯­éŸ³æ–‡æœ¬æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡æ¿åˆºæ¿€åº•å±‚LLMæ‰€å±•ç°çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨DailyTalkæ•°æ®é›†ä¸Šçš„è‡ªåŠ¨å’Œäººç±»è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç”Ÿæˆäº†è‡ªç„¶çš„å£è¯­å“åº”ï¼Œè¶…è¶Šäº†å…ˆå‰çš„å’Œçº§è”çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ£€æŸ¥ç‚¹ä½äº<a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm%E3%80%82">https://github.com/naver-ai/usdmã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.05706v3">PDF</a> NeurIPS 2024, Project Page: <a target="_blank" rel="noopener" href="https://unifiedsdm.github.io/">https://unifiedsdm.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆUSDMï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ç†è§£å’Œåˆæˆè¯­éŸ³ï¼Œç”Ÿæˆä¸è¾“å…¥è¯­éŸ³ç›¸å…³çš„è¿è´¯å£è¯­å“åº”ï¼Œå…·æœ‰è‡ªç„¶å‘ç”Ÿçš„éŸµå¾‹ç‰¹å¾ï¼Œæ— éœ€ä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚é€šè¿‡æ„å»ºåŒ…å«ä¸»è¦è¯­ä¹‰ä¿¡æ¯çš„éŸµå¾‹è¯­éŸ³ä»¤ç‰Œï¼Œè¯¥æ¨¡å‹å®ç°äº†éŸµå¾‹æ³¨å…¥çš„è¯­éŸ³æ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­éŸ³æ–‡æœ¬é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œæé«˜äº†è·¨æ¨¡æ€è¯­ä¹‰çš„æ•è·èƒ½åŠ›ã€‚é€šè¿‡åœ¨æ—¥å¸¸å¯¹è¯æ•°æ®é›†ä¸Šçš„è‡ªåŠ¨å’Œäººç±»è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å“åº”è‡ªç„¶æµç•…ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„çº§è”åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç›´æ¥ç†è§£å’Œåˆæˆè¯­éŸ³æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æ¢ç´¢å»ºæ¨¡å£è¯­å¯¹è¯çš„ç­–ç•¥ã€‚</li>
<li>USDMæ¨¡å‹æ—¨åœ¨ç”Ÿæˆä¸è¾“å…¥è¯­éŸ³ç›¸å…³çš„è¿è´¯å£è¯­å“åº”ï¼Œå¹¶å…·å¤‡è‡ªç„¶éŸµå¾‹ç‰¹å¾ã€‚</li>
<li>USDMæ¨¡å‹é€šè¿‡åŒ…å«ä¸»è¦è¯­ä¹‰ä¿¡æ¯çš„éŸµå¾‹è¯­éŸ³ä»¤ç‰Œå®ç°éŸµå¾‹æ³¨å…¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­éŸ³æ–‡æœ¬é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œå¢å¼ºè·¨æ¨¡æ€è¯­ä¹‰æ•è·ã€‚</li>
<li>é€šè¿‡å¤šæ­¥å£è¯­å¯¹è¯æ¨¡æ¿å¯¹USDMè¿›è¡Œå¾®è°ƒï¼Œåˆºæ¿€LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨æ—¥å¸¸å¯¹è¯æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒUSDMç”Ÿæˆçš„å“åº”è‡ªç„¶æµç•…ï¼Œè¶…è¶Šçº§è”åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6798c63bc786ca1de2c782f0be72cea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dc65923495bfd0c6671bc48d3bac96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df6d275e2fd773757f0bfd6c831df15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49f6c367b897312213c20c2022c1d95.jpg" align="middle">
</details>




<h2 id="Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer"><a href="#Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer" class="headerlink" title="Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer"></a>Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer</h2><p><strong>Authors:Peter Ochieng</strong></p>
<p>Diffusion based vocoders have been criticised for being slow due to the many steps required during sampling. Moreover, the modelâ€™s loss function that is popularly implemented is designed such that the target is the original input $x_0$ or error $\epsilon_0$. For early time steps of the reverse process, this results in large prediction errors, which can lead to speech distortions and increase the learning time. We propose a setup where the targets are the different outputs of forward process time steps with a goal to reduce the magnitude of prediction errors and reduce the training time. We use the different layers of a neural network (NN) to perform denoising by training them to learn to generate representations similar to the noised outputs in the forward process of the diffusion. The NN layers learn to progressively denoise the input in the reverse process until finally the final layer estimates the clean speech. To avoid 1:1 mapping between layers of the neural network and the forward process steps, we define a skip parameter $\tau&gt;1$ such that an NN layer is trained to cumulatively remove the noise injected in the $\tau$ steps in the forward process. This significantly reduces the number of data distribution recovery steps and, consequently, the time to generate speech. We show through extensive evaluation that the proposed technique generates high-fidelity speech in competitive time that outperforms current state-of-the-art tools. The proposed technique is also able to generalize well to unseen speech. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„vocoderç”±äºé‡‡æ ·è¿‡ç¨‹ä¸­éœ€è¦å¤šä¸ªæ­¥éª¤ï¼Œæ‰€ä»¥è¢«æ‰¹è¯„é€Ÿåº¦è¾ƒæ…¢ã€‚æ­¤å¤–ï¼Œæµè¡Œçš„å®ç°ä¸­çš„æ¨¡å‹çš„æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯åŸå§‹è¾“å…¥x_0æˆ–è¯¯å·®Îµ_0ã€‚åœ¨åå‘è¿‡ç¨‹çš„æ—©æœŸæ­¥éª¤ä¸­ï¼Œè¿™ä¼šå¯¼è‡´è¾ƒå¤§çš„é¢„æµ‹è¯¯å·®ï¼Œå¯èƒ½å¯¼è‡´è¯­éŸ³å¤±çœŸå¹¶å¢åŠ å­¦ä¹ æ—¶é—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¾ç½®ï¼Œç›®æ ‡ä¸ºæ­£å‘è¿‡ç¨‹æ—¶é—´æ­¥éª¤çš„ä¸åŒè¾“å‡ºï¼Œæ—¨åœ¨å‡å°é¢„æµ‹è¯¯å·®çš„å¹…åº¦å¹¶ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆNNï¼‰çš„ä¸åŒå±‚æ‰§è¡Œå»å™ªï¼Œé€šè¿‡è®­ç»ƒå®ƒä»¬å­¦ä¹ ç”Ÿæˆä¸æ‰©æ•£æ­£å‘è¿‡ç¨‹ä¸­çš„å™ªå£°è¾“å‡ºç›¸ä¼¼çš„è¡¨ç¤ºã€‚NNå±‚å­¦ä¹ åœ¨åå‘è¿‡ç¨‹ä¸­é€æ­¥å¯¹è¾“å…¥è¿›è¡Œå»å™ªï¼Œç›´åˆ°æœ€ç»ˆå±‚ä¼°è®¡å‡ºå¹²å‡€è¯­éŸ³ã€‚ä¸ºäº†é¿å…ç¥ç»ç½‘ç»œå±‚ä¸æ­£å‘è¿‡ç¨‹æ­¥éª¤ä¹‹é—´çš„1ï¼š1æ˜ å°„ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªè·³è¿‡å‚æ•°Ï„&gt;1ï¼Œä½¿å¾—NNå±‚è¢«è®­ç»ƒç´¯ç§¯åœ°å»é™¤åœ¨æ­£å‘è¿‡ç¨‹çš„Ï„æ­¥ä¸­æ³¨å…¥çš„å™ªå£°ã€‚è¿™æ˜¾è‘—å‡å°‘äº†æ•°æ®åˆ†å¸ƒæ¢å¤æ­¥éª¤çš„æ•°é‡ï¼Œå› æ­¤ä¹Ÿç¼©çŸ­äº†ç”Ÿæˆè¯­éŸ³çš„æ—¶é—´ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›è¯„ä¼°è¯æ˜ï¼Œæ‰€æå‡ºçš„æŠ€æœ¯åœ¨ç«äº‰æ—¶é—´å†…ç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å·¥å…·ã€‚æ‰€æå‡ºçš„æŠ€æœ¯ä¹Ÿèƒ½å¾ˆå¥½åœ°æ¨å¹¿æœªè§è¿‡çš„è¯­éŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09652v3">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„vocoderæ”¹è¿›æ–¹æ¡ˆï¼Œé€šè¿‡è°ƒæ•´ç›®æ ‡è®¾å®šå’Œç¥ç»ç½‘ç»œå±‚çš„è®¾è®¡ï¼Œå‡å°‘äº†é¢„æµ‹è¯¯å·®å’Œè®­ç»ƒæ—¶é—´ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€é«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼ŒåŒæ—¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£vocoderå› é‡‡æ ·è¿‡ç¨‹ä¸­æ­¥éª¤ç¹å¤šè€Œé€Ÿåº¦è¾ƒæ…¢ï¼Œæœ¬æ–‡æå‡ºæ”¹è¿›æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹æŸå¤±å‡½æ•°è®¾è®¡ä»¥åŸå§‹è¾“å…¥æˆ–è¯¯å·®ä¸ºç›®æ ‡ï¼Œå¯¼è‡´æ—©æœŸæ—¶é—´æ­¥éª¤åå‘è¿‡ç¨‹ä¸­çš„é¢„æµ‹è¯¯å·®è¾ƒå¤§ï¼Œæœ¬æ–‡æ”¹å˜ç›®æ ‡è®¾å®šä¸ºå‰å‘è¿‡ç¨‹æ—¶é—´æ­¥éª¤çš„è¾“å‡ºï¼Œä»¥å‡å°‘é¢„æµ‹è¯¯å·®å’Œå­¦ä¹ æ—¶é—´ã€‚</li>
<li>åˆ©ç”¨ç¥ç»ç½‘ç»œçš„ä¸åŒå±‚è¿›è¡Œå»å™ªè®­ç»ƒï¼Œå­¦ä¹ ç”Ÿæˆä¸æ‰©æ•£å‰å‘è¿‡ç¨‹ä¸­çš„å™ªå£°è¾“å‡ºç›¸ä¼¼çš„è¡¨ç¤ºã€‚</li>
<li>ç¥ç»ç½‘ç»œå±‚é€å±‚å»å™ªï¼Œæœ€ç»ˆå±‚ä¼°è®¡æ¸…æ´è¯­éŸ³ã€‚</li>
<li>å¼•å…¥è·³è¿‡å‚æ•°Ï„&gt;1ï¼Œä½¿ç¥ç»ç½‘ç»œå±‚ç´¯è®¡å»é™¤åœ¨Ï„æ­¥ä¸­æ³¨å…¥çš„å™ªå£°ï¼Œå‡å°‘æ•°æ®åˆ†å¸ƒæ¢å¤æ­¥éª¤å’Œæ—¶é—´ã€‚</li>
<li>æ”¹è¿›æ–¹æ¡ˆç”Ÿæˆçš„é«˜ä¿çœŸè¯­éŸ³åœ¨ç«äº‰æ—¶é—´å†…è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å·¥å…·ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c25b2ace83cc3057ff3a08e435a27f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e96c9504cca1fdb2836c0820b9d1871.jpg" align="middle">
</details>




<h2 id="Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds"><a href="#Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds" class="headerlink" title="Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds"></a>Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds</h2><p><strong>Authors:Androniki Dimitriou, Daniel G. Figueroa, Bryan Zaldivar</strong></p>
<p>We apply state-of-the-art, likelihood-free statistical inference (machine-learning-based) techniques for reconstructing the spectral shape of a gravitational wave background (GWB). We focus on the reconstruction of an arbitrarily shaped signal by the LISA detector, but the method can be easily extended to either template-dependent signals, or to other detectors, as long as a characterisation of the instrumental noise is available. As proof of the technique, we quantify the ability of LISA to reconstruct signals of arbitrary spectral shape (${\it blind}$ reconstruction), considering a diversity of frequency profiles, and including astrophysical backgrounds in some cases. As a teaser of how the method can reconstruct signals characterised by a parameter-dependent template (${\it template}$ reconstruction), we present a dedicated study for power-law signals. While our technique has several advantages with respect to traditional MCMC methods, we validate it with the latter for concrete cases. This work opens the door for both fast and accurate Bayesian parameter estimation of GWBs, with essentially no computational overhead during the inference step. Our set of tools are integrated into the package ${\tt GWBackFinder}$, which is publicly available in <a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder">https://github.com/AndronikiDimitriou/GWBackFinder</a>. </p>
<blockquote>
<p>æˆ‘ä»¬é‡‡ç”¨æœ€å…ˆè¿›çš„æ— å¯èƒ½æ€§ç»Ÿè®¡æ¨æ–­ï¼ˆåŸºäºæœºå™¨å­¦ä¹ ï¼‰æŠ€æœ¯æ¥é‡å»ºå¼•åŠ›æ³¢èƒŒæ™¯ï¼ˆGWBï¼‰çš„é¢‘è°±å½¢çŠ¶ã€‚æˆ‘ä»¬ä¸“æ³¨äºåˆ©ç”¨LISAæ¢æµ‹å™¨å¯¹ä»»æ„å½¢çŠ¶çš„ä¿¡å·è¿›è¡Œé‡å»ºï¼Œä½†åªè¦æœ‰ä»ªå™¨å™ªå£°çš„ç‰¹å¾ï¼Œè¯¥æ–¹æ³•å¯è½»æ˜“æ‰©å±•åˆ°æ¨¡æ¿ä¾èµ–çš„ä¿¡å·æˆ–å…¶ä»–æ¢æµ‹å™¨ã€‚ä½œä¸ºè¯¥æŠ€æœ¯çš„è¯æ˜ï¼Œæˆ‘ä»¬é‡åŒ–äº†LISAé‡å»ºä»»æ„é¢‘è°±å½¢çŠ¶ä¿¡å·çš„èƒ½åŠ›ï¼ˆç›²é‡å»ºï¼‰ï¼Œè€ƒè™‘äº†å„ç§é¢‘ç‡åˆ†å¸ƒï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹åŒ…æ‹¬å¤©æ–‡èƒŒæ™¯ã€‚ä½œä¸ºè¯¥æ–¹æ³•å¯ä»¥é‡å»ºå‚æ•°ä¾èµ–æ¨¡æ¿ä¿¡å·çš„ä¸€ä¸ªæç¤ºï¼ˆæ¨¡æ¿é‡å»ºï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¹‚å¾‹ä¿¡å·è¿›è¡Œäº†ä¸“é¡¹ç ”ç©¶ã€‚è™½ç„¶æˆ‘ä»¬çš„æŠ€æœ¯ç›¸å¯¹äºä¼ ç»Ÿçš„MCMCæ–¹æ³•æœ‰è®¸å¤šä¼˜åŠ¿ï¼Œä½†å¯¹äºå…·ä½“æ¡ˆä¾‹ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”¨åè€…å¯¹å…¶è¿›è¡Œäº†éªŒè¯ã€‚è¿™é¡¹å·¥ä½œä¸ºGWBsçš„å¿«é€Ÿå’Œç²¾ç¡®è´å¶æ–¯å‚æ•°ä¼°è®¡æ‰“å¼€äº†å¤§é—¨ï¼Œåœ¨æ¨æ–­æ­¥éª¤ä¸­å‡ ä¹æ²¡æœ‰è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„å·¥å…·é›†å·²é›†æˆåˆ°GWBackFinderè½¯ä»¶åŒ…ä¸­ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AndronikiDimitriou/GWBackFinderå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08430v3">PDF</a> Published in JCAP. 29 pages plus appendices and references, 12   figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°æŠ€æœ¯ï¼Œé‡‡ç”¨æ— éœ€ä¼¼ç„¶å‡½æ•°çš„ç»Ÿè®¡æ¨æ–­ï¼ˆåŸºäºæœºå™¨å­¦ä¹ ï¼‰æ–¹æ³•ï¼Œå¯¹å¼•åŠ›æ³¢èƒŒæ™¯ï¼ˆGWBï¼‰çš„é¢‘è°±å½¢çŠ¶è¿›è¡Œé‡å»ºã€‚ç ”ç©¶é‡ç‚¹ä¸ºLISAæ¢æµ‹å™¨å¯¹ä»»æ„å½¢çŠ¶ä¿¡å·çš„é‡å»ºï¼Œè¯¥æ–¹æ³•å¯è½»æ˜“æ‰©å±•è‡³æ¨¡æ¿ä¾èµ–ä¿¡å·æˆ–å…¶ä»–æ¢æµ‹å™¨ï¼Œåªè¦å…·å¤‡ä»ªå™¨å™ªå£°ç‰¹å¾å³å¯ã€‚å¯¹ä¸­å›½æŠ€æœ¯è¿›è¡Œäº†é‡åŒ–è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶å¯¹ä»»æ„é¢‘è°±å½¢çŠ¶ä¿¡å·çš„é‡å»ºèƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šç§é¢‘ç‡åˆ†å¸ƒï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è€ƒè™‘å¤©æ–‡èƒŒæ™¯ã€‚è¿˜ä¸ºå‚æ•°ä¾èµ–æ¨¡æ¿çš„ä¿¡å·é‡å»ºæä¾›äº†ä¸“é¡¹ç ”ç©¶ï¼Œé‡ç‚¹ç ”ç©¶äº†å¹‚å¾‹ä¿¡å·ã€‚è¯¥æŠ€æœ¯ç›¸å¯¹äºä¼ ç»Ÿçš„MCMCæ–¹æ³•å…·æœ‰å¤šä¸ªä¼˜åŠ¿ï¼Œå¹¶åœ¨å…·ä½“æ¡ˆä¾‹ä¸­è¿›è¡ŒéªŒè¯ã€‚æœ¬ç ”ç©¶ä¸ºå¿«é€Ÿå‡†ç¡®çš„GWBè´å¶æ–¯å‚æ•°ä¼°è®¡æ‰“å¼€äº†å¤§é—¨ï¼Œæ¨ç†æ­¥éª¤å‡ ä¹æ— è®¡ç®—å¼€é”€ã€‚ç›¸å…³å·¥å…·é›†å·²é›†æˆåˆ°å…¬å¼€å¯ç”¨çš„GWBackFinderè½¯ä»¶åŒ…ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‡ç”¨å‰æ²¿çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œè¿›è¡Œæ— éœ€ä¼¼ç„¶å‡½æ•°çš„ç»Ÿè®¡æ¨æ–­ï¼Œä»¥é‡å»ºå¼•åŠ›æ³¢èƒŒæ™¯çš„é¢‘è°±å½¢çŠ¶ã€‚</li>
<li>ç ”ç©¶çš„ç„¦ç‚¹æ˜¯LISAæ¢æµ‹å™¨å¯¹ä»»æ„å½¢çŠ¶ä¿¡å·çš„é‡å»ºèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å¯çµæ´»æ‰©å±•åˆ°æ¨¡æ¿ä¾èµ–çš„ä¿¡å·å’Œå…¶ä»–æ¢æµ‹å™¨ï¼Œå‰ææ˜¯æœ‰ä»ªå™¨å™ªå£°çš„ç‰¹å¾æè¿°ã€‚</li>
<li>æŠ€æœ¯éªŒè¯åŒ…æ‹¬å¯¹ä»»æ„é¢‘è°±å½¢çŠ¶ä¿¡å·çš„é‡å»ºèƒ½åŠ›ï¼Œæ¶µç›–äº†å¤šç§é¢‘ç‡åˆ†å¸ƒï¼Œå¹¶è€ƒè™‘äº†å¤©æ–‡èƒŒæ™¯ã€‚</li>
<li>æä¾›äº†å‚æ•°ä¾èµ–æ¨¡æ¿çš„ä¿¡å·é‡å»ºçš„ä¸“é¡¹ç ”ç©¶ï¼Œä»¥å¹‚å¾‹ä¿¡å·ä¸ºä¾‹ã€‚</li>
<li>ä¸ä¼ ç»ŸMCMCæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æŠ€æœ¯å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨å…·ä½“æ¡ˆä¾‹ä¸­å¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b71214c325a1d33137c11a8085cf040a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f111152cd7161ef71d176cd5aef745f9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  CoPrUS Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
