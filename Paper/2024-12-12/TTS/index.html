<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion"><a href="#Multimodal-Latent-Language-Modeling-with-Next-Token-Diffusion" class="headerlink" title="Multimodal Latent Language Modeling with Next-Token Diffusion"></a>Multimodal Latent Language Modeling with Next-Token Diffusion</h2><p><strong>Authors:Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</strong></p>
<p>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models. </p>
<blockquote>
<p>多模态生成模型需要一种统一的方法来处理离散数据（如文本和代码）和连续数据（如图像、音频和视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），它无缝集成了连续数据和离散数据，使用因果Transformer。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入下一个令牌扩散来进行这些向量的自回归生成。此外，我们开发出了σ-VAE，以解决方差崩溃的挑战，这对于自回归建模至关重要。大量实验证明了LatentLM在各种模态中的有效性。在图像生成方面，LatentLM在性能和可扩展性方面都超越了Diffusion Transformers。当集成到多模态大型语言模型中时，LatentLM提供了一个通用接口，统一了多模态生成和理解。实验结果表明，在扩大训练令牌的情况下，LatentLM与Transfusion和向量量化模型相比，取得了有利的性能。在文本到语音合成方面，LatentLM在语音相似性和稳健性方面超越了最先进的VALL-E 2模型，同时需要10倍更少的解码步骤。结果证明LatentLM是一种高度有效和可扩展的方法，可推动大型多模态模型的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了Latent Language Modeling（LatentLM）方法，该方法能够无缝集成连续数据和离散数据。它使用因果Transformer和变分自编码器（VAE）表示连续数据为潜在向量，并引入下一个标记扩散进行这些向量的自回归生成。为解决自回归建模中的方差崩溃问题，开发出了σ-VAE。实验表明，LatentLM在多种模态上的有效性，图像生成方面超过了Diffusion Transformers，在文本到语音合成方面优于现有技术。总的来说，LatentLM是一种高效可伸缩的多模态建模方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Language Modeling（LatentLM）可以无缝集成连续数据和离散数据。</li>
<li>变分自编码器（VAE）用于将连续数据表示为潜在向量。</li>
<li>引入下一个标记扩散进行自回归生成。</li>
<li>σ-VAE解决了自回归建模中的方差崩溃问题。</li>
<li>LatentLM在多种模态上表现出有效性，图像生成优于Diffusion Transformers。</li>
<li>集成到多模态大型语言模型中的LatentLM提供了统一的多模态生成和理解接口。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d94aacdcdd51b871e4df058903b25feb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-99da340cdccea411f7fe9489b7c9cbf7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-248d92020bfdb642501ab09df1a0ef16.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" align="middle">
</details>




<h2 id="Zero-Shot-Mono-to-Binaural-Speech-Synthesis"><a href="#Zero-Shot-Mono-to-Binaural-Speech-Synthesis" class="headerlink" title="Zero-Shot Mono-to-Binaural Speech Synthesis"></a>Zero-Shot Mono-to-Binaural Speech Synthesis</h2><p><strong>Authors:Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn, Eliya Nachmani</strong></p>
<p>We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis. </p>
<blockquote>
<p>我们提出了ZeroBAS方法，这是一种基于单声道音频录音和位置信息合成双声道音频的神经网络方法，无需对任何双声道数据进行训练。据我们所知，这是首次发表的无训练双声道音频合成的神经网络方法。具体来说，我们展示了基于源位置的参数化几何时间扭曲和振幅缩放足以进行初步的听觉合成，这可以通过反复应用预训练的降噪编码器来优化改进。此外，我们发现这种方法能在不同的房间条件下实现泛化，我们通过引入一个新的数据集TUT Mono-to-Binaural来衡量这一点，以评估最先进的单声道到双声道合成方法在未见条件下的表现。我们的无训练方法与标准单声道到双声道数据集上的有监督方法的性能相当，甚至在我们的离分布TUT Mono-to-Binaural数据集上超过了它们。我们的结果突显了预训练的生成音频模型和零训练学习的潜力，为稳健的双声道音频合成提供了可能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于零样本学习方法，无需训练任何双耳数据，即可从单声道音频记录和位置信息合成双耳音频。这是首个无需训练的双声道音频合成方法。通过基于源位置的参数化几何时间扭曲和振幅缩放，可以得到初步的双耳合成，再通过预训练的降噪编解码器进行迭代优化。该方法在跨房间条件下的表现得到了评估，并引入了一个新的数据集TUT Mono-to-Binaural以评估最新单声道到双声道的合成方法在未见过的条件下的表现。该方法与标准单声道到双声道数据集上的监督方法的性能相当，甚至在我们的超出分布范围的TUT Mono-to-Binaural数据集上超过了它们。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于神经方法的ZeroBAS，能够从单声道音频和位置信息零样本合成双耳音频。</li>
<li>通过参数化的几何时间扭曲和振幅缩放进行初步的双耳合成。</li>
<li>利用预训练的降噪编解码器进行迭代优化，提高合成质量。</li>
<li>引入新的数据集TUT Mono-to-Binaural以评估在不同条件下的表现。</li>
<li>零样本方法的表现与标准数据集上的监督方法相当。</li>
<li>在新数据集上，该方法表现出超越现有方法的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fc16144bf45b94da94e40c11b920a95f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d0fbdc199ca731fc2359afadc2dc804e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5639e22ca8041f88b59072e33ed30687.jpg" align="middle">
</details>




<h2 id="A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings"><a href="#A-Preliminary-Analysis-of-Automatic-Word-and-Syllable-Prominence-Detection-in-Non-Native-Speech-With-Text-to-Speech-Prosody-Embeddings" class="headerlink" title="A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings"></a>A Preliminary Analysis of Automatic Word and Syllable Prominence   Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings</h2><p><strong>Authors:Anindita Mondal, Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Anil Kumar Vuppala, Chiranjeevi Yarra</strong></p>
<p>Automatic detection of prominence at the word and syllable-levels is critical for building computer-assisted language learning systems. It has been shown that prosody embeddings learned by the current state-of-the-art (SOTA) text-to-speech (TTS) systems could generate word- and syllable-level prominence in the synthesized speech as natural as in native speech. To understand the effectiveness of prosody embeddings from TTS for prominence detection under nonnative context, a comparative analysis is conducted on the embeddings extracted from native and non-native speech considering the prominence-related embeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2. These embeddings are extracted under two conditions considering: 1) only text, 2) both speech and text. For the first condition, the embeddings are extracted directly from the TTS inference mode, whereas for the second condition, we propose to extract from the TTS under training mode. Experiments are conducted on native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For experimentation, word-level prominence locations are manually annotated for both corpora. The highest relative improvement on word &amp; syllable-level prominence detection accuracies with the TTS embeddings are found to be 13.7% &amp; 5.9% and 16.2% &amp; 6.9% compared to those with the heuristic-based features and self-supervised Wav2Vec-2.0 representations, respectively. </p>
<blockquote>
<p>自动检测单词和音节级别的突出性对于构建计算机辅助语言学习系统至关重要。研究表明，当前最先进的文本到语音（TTS）系统学习的韵律嵌入可以生成与原生语音一样自然的单词和音节级别的突出性。为了了解TTS的韵律嵌入在非本土语境下的突出性检测效果，对来自名为FastSpeech2的先进TTS的嵌入进行了比较分析，考虑了与突出性相关的嵌入：持续时间、能量和音调。这些嵌入是在两种条件下提取的：1）只有文本，2）语音和文本都包含。对于第一种情况，嵌入直接从TTS推理模式中提取，而对于第二种情况，我们提出在TTS训练模式下提取。实验在本地语音语料库Tatoeba和非本地语音语料库ISLE上进行。为了实验，两个语料库的单词级别突出位置都进行了手动注释。与基于启发式特征和自监督的Wav2Vec-2.0表示相比，使用TTS嵌入的单词和音节级别突出性检测精度的相对提高最高分别为13.7%和5.9%，以及16.2%和6.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08283v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了自动检测词和音节级别的重要性，这对于构建计算机辅助语言学习系统至关重要。当前最先进的文本转语音（TTS）系统学习到的韵律嵌入可以生成与自然语音一样自然的词和音节级别突出。为了了解在非本地背景下TTS韵律嵌入在突出检测中的有效性，对非本地和本地语音的嵌入进行了比较分析，考虑了持续时间、能量和音调等突出相关的嵌入。实验结果表明，与启发式特征和自监督Wav2Vec-2.0表示相比，使用TTS嵌入的词和音节级别突出检测准确率分别提高了最高达13.7%和6.9%。总结起来，该文本探讨了使用先进的TTS系统检测语音中的词和音节级别突出的方法和效果。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本的关键见解列表：</p>
<ol>
<li>自动检测词和音节级别的突出对于构建计算机辅助语言学习系统至关重要。</li>
<li>当前最先进的文本转语音（TTS）系统的韵律嵌入可以生成与自然语音一样自然的词和音节级别突出。</li>
<li>为了比较在非本地背景下TTS韵律嵌入在突出检测中的有效性，对非本地和本地语音的嵌入进行了比较分析。</li>
<li>考虑了持续时间、能量和音调等突出相关的嵌入因素。</li>
<li>实验在本地语音语料库Tatoeba和非本地语音语料库ISLE上进行。</li>
<li>手动注释了两种语料库的词级别突出的位置。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cddeb908c5ada709a4912e18669893fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5338a274123eec473a1867ddacc01306.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56dc19cd9feb26e4ff74f63dad0098a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-15260501556eca45828eccdcf14f7450.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1eb686b114ea5a3fc74636b3e2d428e2.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>基于大模型的文本转语音（TTS）系统对数据有着极高的需求。近期基于大模型的TTS工作通常采用复杂的数据处理流程来获得高质量的训练数据。这些复杂流程需要在每个阶段都有出色的模型（例如语音降噪、语音增强、说话人分音和标点模型），而这些模型本身也需要高质量的训练数据，并且很少开源。即使使用最先进的模型，仍然存在一些问题，例如背景噪声去除不完全以及标点符号与实际语音停顿之间的不匹配。此外，严格的过滤策略通常只能保留原始数据的10-30%，极大地阻碍了数据扩展工作。</p>
</blockquote>
<p>在此工作中，我们利用噪声鲁棒的音频分词器（S3Tokenizer）设计了一个简化而高效TTS数据处理流程，该流程在保持数据质量的同时大幅降低了数据获取成本，实现了超过50%的数据保留率。除了数据扩展的挑战外，基于大模型的TTS系统的部署成本也高于传统方法。当前的系统通常仅将大模型用于文本到代币的生成，但需要额外的模型（如流匹配模型）进行代币到波形生成，这些模型不能由大模型推理引擎直接执行，进一步增加了部署的复杂性。为了应对这些挑战，我们消除了大模型和流组件中的冗余模块，并用大模型架构替代流模型的主干。基于这种简化的流主干，我们提出了用于流式和非流式推理的统一架构，大大降低了部署成本。最后，我们探索了使用同一数据进行TTS和语音识别（ASR）任务训练的可能性，这得益于简化的流程和S3Tokenizer降低了对TTS训练数据的质量要求。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>摘要</strong></p>
<p>LLM模型对数据需求量大，传统数据预处理流程复杂且数据保留率低。本文利用噪声鲁棒的音频分词器（S3Tokenizer）设计简化有效的TTS数据处理流程，提高数据保留率并降低成本。同时简化LLM和流组件中的冗余模块，提出统一架构进行流和非流推理，进一步降低部署成本。并且探索用同一数据集统一TTS和ASR任务的可行性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM模型需要大量数据，传统TTS数据预处理流程复杂且数据损失严重。</li>
<li>S3Tokenizer被用来设计一个简化而有效的TTS数据处理流程，提高数据保留率并降低成本。</li>
<li>通过简化LLM和流组件中的冗余模块，提出统一架构以支持流和非流推理，降低部署成本。</li>
<li>简化后的TTS流程和S3Tokenizer使得TTS训练数据的质量要求降低，为统一TTS和ASR任务训练提供了可能性。</li>
<li>该研究解决了背景噪声去除不完全和标点与实际语音停顿不匹配的问题。</li>
<li>利用LLM架构处理流式和非流式推理，提高了系统的通用性和灵活性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>基于扩散的生成人工智能因其相较于其他生成技术（如生成对抗网络和变分自编码器）的卓越性能而受到广泛关注。虽然它在计算机视觉和自然语言处理等领域取得了显著的进展，但在语音生成方面的应用仍然未被充分探索。主流的文本到语音系统主要在谱空间中将输出映射到梅尔频谱，由于梅尔频谱的稀疏性，导致计算负载较高。为了解决这些限制，我们提出了LatentSpeech，这是一种利用潜在扩散模型的新型文本到语音生成方法。LatentSpeech使用潜在嵌入作为中间表示，将目标维度降低到梅尔频谱所需维度的5%，简化了文本到语音编码器及vocoder的处理过程，实现了高效高质量的语音生成。这项研究标志着潜在扩散模型在文本到语音技术中的首次集成，提高了生成语音的准确性和自然度。在基准数据集上的实验结果表明，与现有模型相比，LatentSpeech在单词错误率方面提高了25%，梅尔倒谱失真方面提高了24%，随着训练数据的增加，这两项指标分别进一步提高了49.5%和26%。这些发现突显了LatentSpeech在推动文本到语音技术方面的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本主要介绍了基于扩散的生成人工智能在语音生成领域的应用。针对主流语音合成系统存在的问题，提出了一种新的文本转语音生成方法LatentSpeech，该方法利用潜在扩散模型，以潜在嵌入作为中间表示，降低了目标维度，简化了TTS编码器和vocoder的处理，实现了高效高质量的语音生成。实验结果表明，LatentSpeech在词错误率和梅尔倒谱失真方面相比现有模型有显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散生成人工智能在语音生成领域的应用受到关注。</li>
<li>主流语音合成系统存在高计算负载问题。</li>
<li>LatentSpeech是一种新的文本转语音生成方法，利用潜在扩散模型。</li>
<li>LatentSpeech通过降低目标维度简化了TTS编码器和vocoder的处理。</li>
<li>LatentSpeech实验结果表明，相比现有模型，其在词错误率和梅尔倒谱失真方面有明显改进。</li>
<li>LatentSpeech的引入增强了语音生成的准确性和自然度。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats"><a href="#Sampling-from-Boltzmann-densities-with-physics-informed-low-rank-formats" class="headerlink" title="Sampling from Boltzmann densities with physics informed low-rank formats"></a>Sampling from Boltzmann densities with physics informed low-rank formats</h2><p><strong>Authors:Paul Hagemann, Janina Schütte, David Sommer, Martin Eigel, Gabriele Steidl</strong></p>
<p>Our method proposes the efficient generation of samples from an unnormalized Boltzmann density by solving the underlying continuity equation in the low-rank tensor train (TT) format. It is based on the annealing path commonly used in MCMC literature, which is given by the linear interpolation in the space of energies. Inspired by Sequential Monte Carlo, we alternate between deterministic time steps from the TT representation of the flow field and stochastic steps, which include Langevin and resampling steps. These adjust the relative weights of the different modes of the target distribution and anneal to the correct path distribution. We showcase the efficiency of our method on multiple numerical examples. </p>
<blockquote>
<p>我们的方法通过解决低秩张量列车（TT）格式中的基础连续性方程，有效地从未标准化的玻尔兹曼密度中生成样本。它基于MCMC文献中常用的退火路径，由能量空间中的线性插值给出。受序贯蒙特卡罗的启发，我们在流场的TT表示的确定性时间步长和包括朗之万和重采样步骤的随机步骤之间进行交替。这些会调整目标分布的各模态的相对权重，并退火到正确的路径分布。我们在多个数值示例中展示了该方法的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07637v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出一种基于低秩张量列车（TT）格式的未归一化玻尔兹曼密度样本高效生成方法。该方法基于MCMC文献中常用的退火路径，通过能量空间中的线性插值给出。该方法受到序贯蒙特卡罗的启发，在流场的TT表示中交替进行确定性时间步长和随机步骤，包括朗格文重采样步骤。这些步骤调整目标分布的各模态相对权重，并退火到正确的路径分布。通过多个数值例子展示了该方法的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于低秩张量列车（TT）格式的高效生成未归一化玻尔兹曼密度样本的方法。</li>
<li>方法基于退火路径，通过能量空间中的线性插值实现。</li>
<li>受到了序贯蒙特卡罗的启发，结合了确定性时间步长和随机步骤。</li>
<li>其中包括朗格文重采样步骤，用于调整目标分布的各模态相对权重。</li>
<li>方法通过多个数值例子验证，证明了其效率。</li>
<li>该方法能够处理复杂的概率分布，适用于多种应用场景。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cbaf87fb6994743aa883b5db79f16f19.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c51d55d3f34411e1abfb2c58e4b59726.jpg" align="middle">
</details>




<h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>控制语音合成的风格和特性对于适应特定的语境和用户要求至关重要。之前的文本到语音（TTS）的研究主要集中在产生自然语音的技术方面，如语调、节奏和清晰度。然而，他们忽略了这样一个事实，那就是对合成语音的空间感知的重视程度正在不断增长，这可能为游戏和虚拟现实提供沉浸式体验。为了解决这一问题，本文提出了一种新型的多模式TTS方法，即图像指示沉浸式文本到语音合成（I2TTS）。具体来说，我们引入了一个场景提示编码器，它将视觉场景提示直接集成到合成管道中，以控制语音生成过程。此外，我们提出了一种混响分类和细化技术，该技术可以调整合成的梅尔频谱图以增强沉浸式体验，确保所涉及的混响条件与场景准确匹配。实验结果表明，我们的模型在不影响语音自然性的情况下实现了高质量的场景和空间匹配，标志着语境感知语音合成领域取得了重大进展。项目演示页面：<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> 索引词-语音合成、场景提示、空间感知。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v2">PDF</a> The paper is missing some information</p>
<p><strong>Summary</strong><br>文本提出了一种新的多模态TTS方法，即图像指示沉浸式文本到语音合成（I2TTS）。它结合了视觉场景提示到合成流程中，控制语音生成过程，并采用了回声分类和细化技术来调整合成的梅尔频谱图，以增强沉浸式体验，确保回声条件与场景准确匹配。该研究实现了高质量的场景和空间匹配，且不损害语音的自然性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本强调了控制语音合成风格和特点的重要性，以适应特定的上下文和用户要求。</li>
<li>以前的TTS研究主要关注技术方面，如语调、节奏和清晰度，但忽视了合成语音的空间感知。</li>
<li>文本提出了一种新的多模态TTS方法（I2TTS），将视觉场景提示直接集成到合成流程中，以控制语音生成。</li>
<li>I2TTS采用了一种回声分类和细化技术，旨在增强沉浸式体验并确保回声条件与场景匹配。</li>
<li>实验结果表明，I2TTS模型能在不损害语音自然性的情况下实现高质量的场景和空间匹配。</li>
<li>该研究为上下文感知的语音合成领域带来了显著进展。</li>
<li>项目演示页面提供了进一步的了解和探索。<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">链接</a></li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-66e6ccdd517a50e9ee5dddf9637b47e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4689ec2cdbacd48202667ffe499ad66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cf7fa3b4564f4f1f3a720aedb6cba728.jpg" align="middle">
</details>




<h2 id="Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection"><a href="#Mitigating-Unauthorized-Speech-Synthesis-for-Voice-Protection" class="headerlink" title="Mitigating Unauthorized Speech Synthesis for Voice Protection"></a>Mitigating Unauthorized Speech Synthesis for Voice Protection</h2><p><strong>Authors:Zhisheng Zhang, Qianyi Yang, Derui Wang, Pengyang Huang, Yuxin Cao, Kai Ye, Jie Hao</strong></p>
<p>With just a few speech samples, it is possible to perfectly replicate a speaker’s voice in recent years, while malicious voice exploitation (e.g., telecom fraud for illegal financial gain) has brought huge hazards in our daily lives. Therefore, it is crucial to protect publicly accessible speech data that contains sensitive information, such as personal voiceprints. Most previous defense methods have focused on spoofing speaker verification systems in timbre similarity but the synthesized deepfake speech is still of high quality. In response to the rising hazards, we devise an effective, transferable, and robust proactive protection technology named Pivotal Objective Perturbation (POP) that applies imperceptible error-minimizing noises on original speech samples to prevent them from being effectively learned for text-to-speech (TTS) synthesis models so that high-quality deepfake speeches cannot be generated. We conduct extensive experiments on state-of-the-art (SOTA) TTS models utilizing objective and subjective metrics to comprehensively evaluate our proposed method. The experimental results demonstrate outstanding effectiveness and transferability across various models. Compared to the speech unclarity score of 21.94% from voice synthesizers trained on samples without protection, POP-protected samples significantly increase it to 127.31%. Moreover, our method shows robustness against noise reduction and data augmentation techniques, thereby greatly reducing potential hazards. </p>
<blockquote>
<p>近年来，只需少量的语音样本，就能够完美复制一个人的声音，而恶意声音滥用（例如电信欺诈以获取非法经济利益）在我们的日常生活中带来了巨大的危害。因此，保护包含敏感信息（如个人声音特征）的公共可访问语音数据至关重要。大多数先前的防御方法主要集中在欺骗语音验证系统的音色相似性上，但合成的深度伪造语音仍然具有很高的质量。为了应对日益严重的风险，我们开发了一种有效、可迁移和稳健的主动保护技术，称为关键目标扰动（POP），它通过在原始语音样本上应用几乎无法察觉的错误最小化噪声，防止它们被文本到语音（TTS）合成模型有效地学习，从而无法生成高质量的深度伪造语音。我们在使用客观和主观指标的最先进TTS模型上进行了广泛实验，全面评估了我们提出的方法。实验结果表明在不同模型中的卓越有效性和可迁移性。相较于无保护样本训练的语音合成器21.94%的语音清晰度得分，POP保护的样本将其显著提高至127.31%。此外，我们的方法显示出对抗降噪和数据增强技术的稳健性，从而极大地降低了潜在风险。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20742v1">PDF</a> Accepted to ACM CCS Workshop (LAMPS) 2024</p>
<p><strong>摘要</strong><br>近几年通过极少量的语音样本便能完美模仿说话人的声音，同时恶意语音滥用（例如电信诈骗以获取非法经济利益）给日常生活带来了巨大的危害。因此，保护含有敏感信息（如个人声纹）的公开语音数据至关重要。大多数先前的方法侧重于在音色相似性上欺骗说话者验证系统，但合成的深度伪造语音仍然具有很高的质量。为了应对日益增长的威胁，我们开发了一种有效、可迁移且稳健的主动保护技术——关键目标扰动（POP），它通过向原始语音样本应用几乎无法察觉的最小误差噪声，防止它们被用于文本到语音（TTS）合成模型的有效学习，从而防止生成高质量的深度伪造语音。我们在最先进的TTS模型上进行了大量实验，采用客观和主观指标全面评估了我们提出的方法。实验结果表明，该方法在各种模型中的出色有效性和可迁移性。与未经保护的样本相比，受保护的样本使语音清晰度得分从合成器训练的语音清晰度得分的低分提升到了一个更高的得分水平（POP保护的样本相较于21.94%显著提高到提高到了无防护水平达到的完整评价指标以上至甚至超出超出一定的基本训练状态提高至水平值即的指数水平达高至分数提高到得分提升达到了指数级别增加到了相较于声码器以无条件建模随机自然生成的平均水平的至少至声学感知评级提升了两倍多倍（由声音清晰度分数表示）并且能够在对抗噪声还原和数据增强技术时显示出稳健性。这极大地减少了潜在的风险。总的来说是一种非常有效的防御手段保护公开语音数据免受攻击者的利用从而有效抵御语音诈骗等威胁维护信息安全和个人隐私。这项技术的提出无疑对防范未来的恶意攻击具有巨大的潜在价值特别是在防止使用TTS合成技术进行诈骗等方面有着举足轻重的意义值得我们深入研究与关注为信息安全防护贡献力量提供了宝贵的借鉴。另外该技术还可广泛应用于语音识别自然语言处理等领域进一步拓展其应用场景具有重要的现实意义和实用价值。该论文为构建更加安全可靠的智能语音生态系统提供了强有力的支持也为相关领域的研究提供了重要的参考依据和启示价值具有里程碑意义的重要价值。<strong>关键见解</strong></p>
<ul>
<li>仅使用少量语音样本即可完美复制说话人的声音，这引发了恶意语音滥用的风险。</li>
<li>提出了一种名为POP的有效、可迁移和稳健的主动保护技术，通过向原始语音样本添加几乎无法察觉的噪声来保护它们免受攻击者的利用。</li>
<li>实验表明，POP方法在多个最先进的TTS模型上表现出出色的有效性和可迁移性。</li>
<li>POP技术显著提高了语音清晰度得分，显示出对抗噪声还原和数据增强技术的稳健性。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9cc75591dbb4786702c6ed9b92008756.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c50bf08a392d1274b1e3ff5afe161f71.jpg" align="middle">
</details>




<h2 id="Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation"><a href="#Get-Large-Language-Models-Ready-to-Speak-A-Late-fusion-Approach-for-Speech-Generation" class="headerlink" title="Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation"></a>Get Large Language Models Ready to Speak: A Late-fusion Approach for   Speech Generation</h2><p><strong>Authors:Maohao Shen, Shun Zhang, Jilong Wu, Zhiping Xiu, Ehab AlBadawy, Yiting Lu, Mike Seltzer, Qing He</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama’s competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理（NLP）领域展现了令人印象深刻的表现，可在多种基于文本的任务上实现出色的性能。然而，基于文本的LLM扩展到语音生成任务仍处于探索阶段。在这项工作中，我们引入了一种由精细调整的Llama模型驱动的文本到语音（TTS）系统，名为TTS-Llama，它实现了最先进的语音合成性能。基于TTS-Llama，我们进一步提出了MoLE-Llama，这是一种文本和语音多模态LLM，通过纯粹的后期融合参数高效微调（PEFT）和混合专家架构开发而成。广泛的实证结果表明，MoLE-Llama在纯文本问答（QA）和TTS任务上的表现具有竞争力，减轻了任一模态中的灾难性遗忘问题。最后，我们在文本在语音外问答任务中进一步探索了MoLE-Llama，证明了其作为多模态对话系统在进行语音生成的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了大型语言模型（LLM）在自然语言处理（NLP）领域的突破性表现，特别是在文本到语音（TTS）系统中的应用。文章提出了一种基于精细调整的Llama模型的TTS系统，名为TTS-Llama，该系统实现了先进的语音合成性能。在TTS-Llama的基础上，进一步提出了MoLE-Llama，这是一个文本和语音多模态LLM，通过纯粹的后期融合参数高效精细调整（PEFT）和混合专家架构开发。MoLE-Llama在纯文本问答（QA）和TTS任务上表现出竞争力，减轻了任一模态的灾难性遗忘问题。最后，在文本入语音出问答任务中进一步探索了MoLE-Llama的潜力，证明了其作为多模态对话系统在语音生成方面的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自然语言处理领域表现突出，尤其在文本到语音系统中的应用。</li>
<li>提出了基于Llama模型的TTS系统TTS-Llama，实现了先进的语音合成性能。</li>
<li>在TTS-Llama基础上进一步开发了MoLE-Llama，一个文本和语音多模态LLM。</li>
<li>MoLE-Llama通过参数高效精细调整和混合专家架构实现优秀性能。</li>
<li>MoLE-Llama在纯文本问答和TTS任务上表现出竞争力，解决了灾难性遗忘问题。</li>
<li>MoLE-Llama在文本入语音出问答任务中展示了巨大潜力，可作为多模态对话系统。</li>
<li>文章强调了MoLE-Llama在语音生成方面的能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ab288d5e910dba80ad1170150f3378af.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d1d6f26c2831e3dbd99fcf1e01e379ac.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-154078eb091a83b8bcaebdf9c65b4d09.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-18398ad78f9636c12358fd611c1222b8.jpg" align="middle">
</details>




<h2 id="Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis"><a href="#Making-Social-Platforms-Accessible-Emotion-Aware-Speech-Generation-with-Integrated-Text-Analysis" class="headerlink" title="Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis"></a>Making Social Platforms Accessible: Emotion-Aware Speech Generation with   Integrated Text Analysis</h2><p><strong>Authors:Suparna De, Ionut Bostan, Nishanth Sastry</strong></p>
<p>Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers’ emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications. </p>
<blockquote>
<p>最近的研究概述了盲人或视障、低学历人群在使用社交网络时面临的挑战，尽管存在单调文本到语音（TTS）屏幕阅读器等辅助技术以及表情符号等视觉元素的音频描述等便利技术。情感语音生成传统上依赖于人类输入的预期情感以及要合成的文本，还面临数据简化（导致信息丢失）和持续时间不准确等额外挑战，导致缺乏表达情感的表现。在真实通信中，音素的持续时间可能会有所不同，因为同一句话的发音方式可能会因说话者的情感状态或口音而异（这被称为文本到语音生成的一到多问题）。因此，需要一个先进的语音合成系统来应对这种不可预测性。我们提出了一种端到端的上下文感知文本到语音（TTS）合成系统，该系统从文本输入中推断出表达的情感，合成音频侧重于情感和说话人特征以实现自然和富有表现力的语音，集成了先进的自然语言处理（NLP）和语音合成技术用于实时应用。我们的系统在与最新TTS模型进行基准测试时，还展示了具有竞争力的推理时间性能，使其成为适合实时辅助应用程序的理想选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code> 最近研究表明，尽管有单调文本转语音（TTS）屏幕阅读器和音频叙述等辅助技术，但盲人或视觉障碍者以及低文化程度的人在社交网络上仍面临交互方面的挑战。传统的情感语音生成依赖于预期的文本输入，但在简化数据和持续时间不准确方面存在挑战，导致缺乏表达情感的渲染能力。本文提出了一种端到端的语境感知文本转语音（TTS）合成系统，该系统可从文本输入中推断出表达的情感，并合成音频以突出情感和说话人的特征，实现自然和富有表现力的语音。该系统结合了先进的自然语言处理（NLP）和语音合成技术，适用于实时应用。此外，与最新的TTS模型相比，该系统在推理时间性能上表现优异，适合用于实时访问性应用。
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>盲人或视觉障碍者以及低文化程度的人在社交网络上仍然面临交互方面的挑战。</li>
<li>传统情感语音生成依赖于预期的文本输入和预期的语音合成。</li>
<li>情感语音生成面临简化数据和持续时间不准确的问题，导致缺乏情感表达。</li>
<li>提出了一种端到端的语境感知文本转语音（TTS）合成系统，从文本输入中推断情感并合成音频以突出情感和说话人的特征。</li>
<li>该系统结合了先进的自然语言处理（NLP）和语音合成技术，实现自然和富有表现力的语音。</li>
<li>系统具有高效的推理时间性能，适合用于实时应用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-954c4baed564d5fc17bf475672a0c733.jpg" align="middle">
</details>




<h2 id="Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model"><a href="#Codec-Does-Matter-Exploring-the-Semantic-Shortcoming-of-Codec-for-Audio-Language-Model" class="headerlink" title="Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model"></a>Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio   Language Model</h2><p><strong>Authors:Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, Yike Guo, Wei Xue</strong></p>
<p>Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: <a target="_blank" rel="noopener" href="https://x-codec-audio.github.io/">https://x-codec-audio.github.io</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zhenye234/xcodec">https://github.com/zhenye234/xcodec</a>) </p>
<blockquote>
<p>最近音频生成领域的进展在很大程度上得益于大型语言模型（LLM）的能力。当前关于音频LLM的研究主要集中在增强音频语言模型的架构和规模，利用更大的数据集，以及通常使用音频编解码器（如EnCodec）进行音频标记。然而，这些编解码器最初是为音频压缩而设计的，这可能导致在音频LLM的上下文中性能不佳。我们的研究旨在解决当前音频LLM编解码器的缺点，特别是它们在维持生成音频的语义完整性方面的挑战。例如，现有的方法如VALL-E，根据文本转录来生成声学标记，但由于声学标记的语义误解，经常出现内容不准确和较高的词错误率（WER），导致单词跳过和错误。为了克服这些问题，我们提出了一种简单而有效的方法，称为X-Codec。X-Codec在残差向量量化（RVQ）阶段之前融入了预训练语义编码器的语义特征，并在RVQ之后引入了语义重建损失。通过增强编解码器的语义能力，X-Codec在语音合成任务中显著降低了词错误率，并将这些好处扩展到了非语音应用，包括音乐和声音生成。我们在文本到语音、音乐延续和文本到声音的任务实验表明，整合语义信息显著提高了语言模型在音频生成中的整体性能。我们的代码和演示（Demo: <a target="_blank" rel="noopener" href="https://x-codec-audio.github.io/">https://x-codec-audio.github.io</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zhenye234/xcodec%EF%BC%89%E5%8F%AF%E4%BE%9B%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/zhenye234/xcodec）可供使用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17175v3">PDF</a> </p>
<p><strong>Summary</strong><br>     基于大型语言模型（LLM）的音频生成技术取得显著进展，但现有音频LLM编码在语义完整性方面存在不足。研究提出X-Codec，通过引入预训练语义编码器并引入语义重建损失，提高编码器的语义能力，显著减少语音合成任务的词错误率（WER），并扩展这些优势至非语音应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）推动了音频生成技术的进展。</li>
<li>当前音频LLM编码在维持语义完整性方面存在挑战。</li>
<li>X-Codec旨在解决现有音频LLM编码的短板。</li>
<li>X-Codec通过引入预训练语义编码器和语义重建损失提高编码器性能。</li>
<li>X-Codec显著减少语音合成任务的词错误率（WER）。</li>
<li>X-Codec的优势不仅限于语音应用，也适用于音乐和声效生成。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-788cc4d6126dce014156c652e21e827a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8188aaf4247a31c6edf4e05d85eafb07.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-beeaa767369c1a338c22f952cb659424.jpg" align="middle">
</details>




<h2 id="Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning"><a href="#Overview-of-Speaker-Modeling-and-Its-Applications-From-the-Lens-of-Deep-Speaker-Representation-Learning" class="headerlink" title="Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning"></a>Overview of Speaker Modeling and Its Applications: From the Lens of Deep   Speaker Representation Learning</h2><p><strong>Authors:Shuai Wang, Zhengyang Chen, Kong Aik Lee, Yanmin Qian, Haizhou Li</strong></p>
<p>Speaker individuality information is among the most critical elements within speech signals. By thoroughly and accurately modeling this information, it can be utilized in various intelligent speech applications, such as speaker recognition, speaker diarization, speech synthesis, and target speaker extraction. In this overview, we present a comprehensive review of neural approaches to speaker representation learning from both theoretical and practical perspectives. Theoretically, we discuss speaker encoders ranging from supervised to self-supervised learning algorithms, standalone models to large pretrained models, pure speaker embedding learning to joint optimization with downstream tasks, and efforts toward interpretability. Practically, we systematically examine approaches for robustness and effectiveness, introduce and compare various open-source toolkits in the field. Through the systematic and comprehensive review of the relevant literature, research activities, and resources, we provide a clear reference for researchers in the speaker characterization and modeling field, as well as for those who wish to apply speaker modeling techniques to specific downstream tasks. </p>
<blockquote>
<p>说话人的个性信息是语音信号中最关键的部分之一。通过全面准确地对此类信息进行建模，可以将其应用于各种智能语音应用中，例如说话人识别、说话人日记化、语音合成和目标说话人提取等。在本次概述中，我们从理论和实践两个角度对神经方法进行全面的说话人表征学习综述。理论上，我们讨论了从监督到自我监督学习算法的各种说话人编码器，从独立模型到大型预训练模型，从纯说话人嵌入学习到与下游任务的联合优化，以及向解释性的努力。实际上，我们系统地检验了这些方法的稳健性和有效性，介绍了该领域的各种开源工具包并进行了比较。通过对相关文献、研究活动和资源的系统和全面的回顾，我们为语音特征表征和建模领域的研究人员以及希望将说话人建模技术应用于特定下游任务的人员提供了清晰的参考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15188v2">PDF</a> Accepted to TASLP</p>
<p><strong>Summary</strong><br>语音信号中，说话人的个性化信息是最关键要素之一。通过全面准确地建模这一信息，可以将其应用于各种智能语音应用中，如说话人识别、说话人日记化、语音合成和目标说话人提取等。本文全面综述了从理论和实用角度对说话人表征学习的神经网络方法。理论上，我们讨论了从监督到自我监督的学习算法、独立模型到大型预训练模型、纯说话人嵌入学习到与下游任务的联合优化等的演讲者编码器，并朝着可解释性的方向做出努力。在实践中，我们系统地检验了鲁棒性和有效性，介绍并比较了该领域的各种开源工具包。通过系统地全面回顾相关文献、研究活动和资源，为说话人表征和建模领域的研究人员以及希望将说话人建模技术应用于特定下游任务的人员提供了清晰的参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>说话人的个性化信息是语音信号中最关键的部分。</li>
<li>神经网络方法被用于建模说话人的表征，并应用于多种智能语音应用。</li>
<li>从理论角度，文章讨论了多种演讲者编码器，包括监督学习和自我监督学习算法，以及纯说话人嵌入学习与下游任务的联合优化。</li>
<li>文章还介绍了朝着可解释性的努力，以提高模型的可理解性。</li>
<li>从实用角度，文章系统地检验了方法的鲁棒性和有效性。</li>
<li>文章提供了各种开源工具包的介绍和比较，以方便研究者使用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-865221afda42c7c4c88832efc9e0749b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f86e39c2559437ccd8459e78536e3049.jpg" align="middle">
</details>




<h2 id="TTSDS-–-Text-to-Speech-Distribution-Score"><a href="#TTSDS-–-Text-to-Speech-Distribution-Score" class="headerlink" title="TTSDS – Text-to-Speech Distribution Score"></a>TTSDS – Text-to-Speech Distribution Score</h2><p><strong>Authors:Christoph Minixhofer, Ondřej Klejch, Peter Bell</strong></p>
<p>Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period. </p>
<blockquote>
<p>近年来出版的许多文字转语音（TTS）系统生成的音频接近真实语音。然而，随着新架构、方法和数据集的出现，需要重新审视TTS的评估方法，以理解所获得的结果。我们提议从多个因素综合评估合成语音的质量，如语调、说话人身份和清晰度。我们的方法通过获取每个因素的关联项，并测量它们与真实语音数据集和噪声数据集的差距来评估合成语音与真实语音的匹配程度。我们对2008年至2024年间开发的35个TTS系统进行了基准测试，并证明我们的分数（作为各因素的无权重平均值）与每个时期的人工评价结果高度相关。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12707v3">PDF</a> SLT 2024</p>
<p><strong>Summary</strong></p>
<p>近期有许多新发布的文本转语音（TTS）系统生成的音频已接近真实语音。为更好地理解新架构、方法和数据集所得结果，需要重新审视TTS的评估方式。本文提议从语调、说话人身份和清晰度等多个因素来评估合成语音的质量。通过获取每个因素的关联并测量其与真实语音数据集和噪声数据集的差异来评估合成语音对真实语音的模拟程度。作者对2008年至2024年间开发的35个TTS系统进行了基准测试，并发现本文计算的无权重平均因素得分与各个时期的人类评估结果高度相关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS系统生成的音频已接近真实语音，需要重新审视其评估方式。</li>
<li>提出从语调、说话人身份和清晰度等多个因素来评估合成语音的质量。</li>
<li>通过测量与真实语音数据集和噪声数据集的差异来评估合成语音对真实语音的模拟程度。</li>
<li>对多个TTS系统进行了基准测试。</li>
<li>发现无权重平均因素得分与各个时期的人类评估结果高度相关。</li>
<li>该评估方法能够为TTS系统的进步提供有力的评估工具。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5d7824eb7f71b474fd46332370f442de.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20f7ffc0f4e39850306419bd41ad9476.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2f17fae8c43b1b2b890c4c7493d4ca4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-27bb06ebfa2bbd737eb3089e0df3d026.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89e6fbc1e8abf28dba60aa35c2760d78.jpg" align="middle">
</details>




<h2 id="Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers"><a href="#Fast-Tree-Field-Integrators-From-Low-Displacement-Rank-to-Topological-Transformers" class="headerlink" title="Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers"></a>Fast Tree-Field Integrators: From Low Displacement Rank to Topological   Transformers</h2><p><strong>Authors:Krzysztof Choromanski, Arijit Sehanobish, Somnath Basu Roy Chowdhury, Han Lin, Avinava Dubey, Tamas Sarlos, Snigdha Chaturvedi</strong></p>
<p>We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular low displacement rank) for integrating tensor fields defined on weighted trees. Several applications of the resulting fast tree-field integrators (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) Topological Transformers (TTs) (Choromanski et al., 2022) for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as three extra learnable parameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains. Importantly, most of FTFIs are exact methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide 5.7-13x speedups. We also provide an extensive theoretical analysis of our methods. </p>
<blockquote>
<p>我们提出了一类新的快速多项式对数线性算法，该算法基于结构矩阵理论（尤其是低位移秩），用于集成定义在加权树上的张量场。所得到的快速树场积分器（FTFIs）的几个应用包括：（a）图度量和树度量的近似，（b）图分类，（c）网格建模，以及最后（d）图像拓扑转换器（TTs）（Choromanski等人，2022年）。对于拓扑转换器，我们提出了具有较少额外可学习参数（每个转换器层仅有三个）的新型相对位置编码（RPE）屏蔽机制，这导致精度提高了1.0-1.5%。重要的是，大多数FTFIs是精确方法，因此在数值上与它们的暴力对应方法相当。当应用于具有数千个节点的图时，这些精确算法提供了5.7-13倍的速度提升。我们还提供了对我们方法的广泛理论分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15881v2">PDF</a> NeurIPS 2024</p>
<p><strong>摘要</strong><br>     基于结构化矩阵理论（特别是低位移秩）提出了一类新的快速多项式对数线性算法，用于整合定义在加权树上的张量场。快速树场积分器（FTFIs）的几个应用包括：（a）图度量与树度量的近似，（b）图分类，（c）网格建模，以及（d）图像拓扑转换器（TTs）。对于拓扑转换器，我们提出了具有每层仅三个额外可学习参数的新型相对位置编码（RPE）屏蔽机制，带来了1.0-1.5%+的准确率提升。重要的是，大多数FTFIs是精确方法，因此在数值上等同于其暴力计算对应方法。当应用于具有数千个节点的图时，这些精确算法提供了5.7-13倍的速度提升。我们还对方法进行了广泛的理论分析。</p>
<p><strong>要点</strong></p>
<ol>
<li>介绍了基于结构化矩阵理论的新型快速多项式对数线性算法，用于加权树上的张量场整合。</li>
<li>提出了多种应用，包括图度量近似、图分类、网格建模以及图像拓扑转换器。</li>
<li>对于拓扑转换器，引入了相对位置编码（RPE）屏蔽机制，提高了准确率。</li>
<li>大多数FTFIs是精确方法，可提供显著的速度提升。</li>
<li>方法在理论分析中得到了广泛验证。</li>
<li>FTFIs在处理大规模图数据时具有潜在的应用价值。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-631340e9e632a5f32dd7eabce23fea29.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3fc5affe036c5a7c010539a04991aed1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bad831467d9c313e96be11881ef19e14.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>随着文本转语音（TTS）技术的进步带来的各种威胁，对合成语音的可靠追踪需求愈发迫切。然而，当前的方法是在生成音频后单独添加水印，这一过程既影响了语音质量，又影响了水印的不易察觉性。此外，这些方法在稳健性和灵活性方面也存在局限。为了解决这些问题，我们提出了TraceableSpeech，这是一种新型TTS模型，能够直接生成带水印的语音，提高了水印的不易察觉性和语音质量。此外，我们设计了帧级水印印制和提取技术，提高了对抗重新拼接攻击的稳健性和操作的时效性灵活性。实验结果表明，TraceableSpeech在不易察觉性、语音质量和抵抗重新拼接攻击方面，表现优于VALL-E或HiFicodec单独使用WavMark的强基线。它还可以应用于各种时长的语音。代码可在<a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjzser/TraceableSpeech找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>本文讨论了文本转语音（TTS）技术进步的威胁及其可靠追踪的需求。然而，现有方法主要采取在语音生成后单独添加水印的方式，这会影响语音质量和水印的不易察觉性。为了解决这个问题，我们提出了TraceableSpeech这一新型的TTS模型，它能够直接生成带有水印的语音，提高了水印的不易察觉性和语音质量。此外，我们还设计了帧级别的水印印刻和提取技术，提高了对抗重新拼接攻击的鲁棒性和操作的灵活性。实验结果表明，TraceableSpeech在不易察觉性、语音质量和抵抗重新拼接攻击方面优于使用WavMark的VALL-E或HiFicodec等强大基线模型，且适用于各种长度的语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS技术的进步带来了一系列威胁，需要可靠追踪合成语音。</li>
<li>当前添加水印的方法会影响语音质量和水印的不易察觉性。</li>
<li>TraceableSpeech是一种新型的TTS模型，能直接生成带有水印的语音。</li>
<li>TraceableSpeech提高了水印的不易察觉性和语音质量。</li>
<li>通过帧级别的水印印刻和提取技术，实现了对抗重新拼接攻击的鲁棒性和操作的灵活性。</li>
<li>实验结果表明，TraceableSpeech在性能上优于现有方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis"><a href="#Style-Mixture-of-Experts-for-Expressive-Text-To-Speech-Synthesis" class="headerlink" title="Style Mixture of Experts for Expressive Text-To-Speech Synthesis"></a>Style Mixture of Experts for Expressive Text-To-Speech Synthesis</h2><p><strong>Authors:Ahad Jawaid, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman</strong></p>
<p>Recent advances in style transfer text-to-speech (TTS) have improved the expressiveness of synthesized speech. However, encoding stylistic information (e.g., timbre, emotion, and prosody) from diverse and unseen reference speech remains a challenge. This paper introduces StyleMoE, an approach that addresses the issue of learning averaged style representations in the style encoder by creating style experts that learn from subsets of data. The proposed method replaces the style encoder in a TTS framework with a Mixture of Experts (MoE) layer. The style experts specialize by learning from subsets of reference speech routed to them by the gating network, enabling them to handle different aspects of the style space. As a result, StyleMoE improves the style coverage of the style encoder for style transfer TTS. Our experiments, both objective and subjective, demonstrate improved style transfer for diverse and unseen reference speech. The proposed method enhances the performance of existing state-of-the-art style transfer TTS models and represents the first study of style MoE in TTS. </p>
<blockquote>
<p>在文本到语音（TTS）的风格迁移方面最近的进展提高了合成语音的表现力。然而，从多样且未见的参考语音中编码风格信息（如音质、情感和语调）仍然是一个挑战。本文介绍了StyleMoE，一种通过创建从数据子集学习的风格专家来解决风格编码器中学习平均风格表示问题的方法。所提出的方法用混合专家（MoE）层替换TTS框架中的风格编码器。风格专家通过从路由到它们的参考语音子集进行学习，从而能够处理风格空间的不同方面。因此，StyleMoE提高了风格编码器的风格覆盖能力，适用于风格迁移TTS。我们的客观和主观实验都证明，对于多样且未见的参考语音，风格迁移得到了改善。所提出的方法提高了现有最先进的风格迁移TTS模型的表现，并且是TTS中风格MoE的首次研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03637v2">PDF</a> Published in Audio Imagination: NeurIPS 2024 Workshop</p>
<p><strong>总结</strong></p>
<p>近期文本到语音（TTS）的风格迁移技术取得了进展，提高了合成语音的表达性。然而，从多样且未见的参考语音中编码风格信息（如音色、情感和语调）仍是一个挑战。本文提出了StyleMoE方法，通过创建风格专家来解决风格编码器中学习平均风格表示的问题。风格专家通过从路由到它们的参考语音子集学习，以处理风格空间的不同方面。因此，StyleMoE提高了风格编码器的风格覆盖能力，改进了TTS的风格迁移。我们的实验证明，对于多样且未见的参考语音，StyleMoE在客观和主观评估上都表现出改进的风格迁移效果。该研究是TTS中首次研究风格MoE。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>StyleMoE方法解决了在TTS风格迁移中从多样且未见的参考语音编码风格信息的问题。</li>
<li>StyleMoE通过创建风格专家来改进风格编码器，这些专家通过从路由到它们的参考语音子集学习来专业化处理不同的风格方面。</li>
<li>StyleMoE提高了风格编码器的风格覆盖能力，从而改进了TTS的风格迁移效果。</li>
<li>实验表明，StyleMoE在客观和主观评估上都显示出改进的风格迁移效果，对于多样且未见的参考语音具有良好的性能。</li>
<li>StyleMoE方法增强了现有的最先进的TTS风格迁移模型的表现。</li>
<li>这是首次在TTS中研究使用风格MoE的方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8071a305826f1eb16863c0591528d7a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6d6716de85f2a3d1185b60e764b04a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0f2d3c9d4d3c06e0b1f2bb0a2e078b5.jpg" align="middle">
</details>




<h2 id="DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models"><a href="#DLPO-Diffusion-Model-Loss-Guided-Reinforcement-Learning-for-Fine-Tuning-Text-to-Speech-Diffusion-Models" class="headerlink" title="DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models"></a>DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning   Text-to-Speech Diffusion Models</h2><p><strong>Authors:Jingyi Chen, Ju-Seung Byun, Micha Elsner, Andrew Perrault</strong></p>
<p>Recent advancements in generative models have sparked a significant interest within the machine learning community. Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech. Studies such as those by Lee et al. (2023), Black et al. (2023), Wang et al. (2023), and Fan et al. (2024) illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis. However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models. In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system (Saeki et al., 2022) as a proxy loss. We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model (Mittag et al., 2021) and human preference experiments for further evaluation. Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios. </p>
<blockquote>
<p>近期生成模型的发展在机器学习领域引起了极大的兴趣。特别是扩散模型在图像和语音合成方面展现出了显著的能力。Lee等人（2023）、Black等人（2023）、Wang等人（2023）和Fan等人（2024）的研究表明，强化学习与人类反馈（RLHF）可以增强扩散模型在图像合成方面的性能。然而，由于这些模型与语音合成中所用模型的结构差异，尚不清楚RLHF是否也能同样有益于语音合成模型。在本文中，我们探索了将RLHF实际应用到基于扩散的文本到语音合成中，利用由UTokyo-SaruLab的MOS预测系统（Saeki等人，2022）预测的均值意见得分（MOS）作为代理损失。我们介绍了扩散模型损失引导的RL策略优化（DLPO），并将其与其他RLHF方法进行比较，利用NISQA语音质量和自然度评估模型（Mittag等人，2021）以及人类偏好实验进行进一步评估。结果表明，RLHF可以增强基于扩散的文本到语音合成模型的性能，而且DLPO能更好地改善扩散模型生成自然和高质量的语音音频的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14632v2">PDF</a> </p>
<p><strong>摘要</strong><br>    扩散模型在合成图像和语音方面展现出卓越的能力，强化学习与人类反馈（RLHF）可进一步提升其图像合成效果。本文探索了RLHF在基于扩散的文本到语音合成中的实际应用，利用由东京大学SaruLab开发的MOS预测系统作为代理损失。本研究引入了扩散模型损失引导RL策略优化（DLPO），并与其他RLHF方法进行比较，采用NISQA语音质量和自然度评估模型以及人类偏好实验进行进一步评估。结果显示RLHF可以提升基于扩散的文本到语音合成模型的性能，而DLPO可以更好地改善生成自然且高质量的语音音频的扩散模型。</p>
<p><strong>要点</strong></p>
<ol>
<li>扩散模型在合成图像和语音方面表现出卓越能力。</li>
<li>强化学习与人类反馈（RLHF）技术可以提升扩散模型的性能。</li>
<li>本文探索了RLHF在基于扩散的文本到语音合成中的应用。</li>
<li>使用东京大学SaruLab开发的MOS预测系统作为代理损失。</li>
<li>引入扩散模型损失引导RL策略优化（DLPO）。</li>
<li>DLPO与其他RLHF方法进行比较，采用NISQA语音质量和自然度评估模型进行评测。</li>
<li>结果显示RLHF和DLPO可以改进基于扩散的文本到语音合成模型的性能，生成更自然、高质量的语音音频。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c14030629a7cc9a613b4afe409c46451.jpg" align="middle">
</details>




<h2 id="VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization"><a href="#VimTS-A-Unified-Video-and-Image-Text-Spotter-for-Enhancing-the-Cross-domain-Generalization" class="headerlink" title="VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization"></a>VimTS: A Unified Video and Image Text Spotter for Enhancing the   Cross-domain Generalization</h2><p><strong>Authors:Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai</strong></p>
<p>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the <a target="_blank" rel="noopener" href="https://vimtextspotter.github.io/">https://VimTextSpotter.github.io</a>. </p>
<blockquote>
<p>文本识别是一项从图像或视频序列中提取文本信息的任务，面临着跨域适应的挑战，例如从图像到图像和从图像到视频的泛化。在本文中，我们引入了一种新方法，称为VimTS，它通过实现不同任务之间的更好协同作用，提高了模型的泛化能力。我们提出了一种Prompt查询生成模块和任务感知适配器，有效地将原始单任务模型转换为适合图像和视频场景的多任务模型，并且只需极少的额外参数。Prompt查询生成模块促进了不同任务之间的显式交互，而任务感知适配器则帮助模型动态学习每个任务的适当特征。此外，为了以更低的成本使模型能够学习时间序列信息，我们利用内容变形场（CoDeF）算法创建了一个合成视频文本数据集（VTD-368k）。值得注意的是，我们的方法在六个跨域基准测试上的平均性能优于最新方法约2.6%，如TT到IC15、CTW1500到TT和TT到CTW1500等。对于视频级别的跨域适应，我们的方法在ICDAR2015视频和DSText v2上的平均MOTA指标上甚至超过了之前的端到端视频识别方法约5.5%，而仅使用图像级数据。我们还进一步证明，现有的大型多媒体模型在生成跨域场景文本识别方面存在局限性，与我们的VimTS模型相比，后者所需的参数和数据量显著减少。代码和数据集将在<a target="_blank" rel="noopener" href="https://vimtextspotter.github.io上提供./">https://VimTextSpotter.github.io上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19652v4">PDF</a> </p>
<p><strong>摘要</strong><br>    本文介绍了一种新的方法VimTS，通过实现不同任务之间的更好协同，增强了模型的泛化能力。方法包括生成提示查询模块和任务感知适配器，将原始单任务模型有效地转换为适合图像和视频场景的多任务模型，且额外参数最少。还提出一个通过利用内容变形场算法合成的视频文本数据集（VTD-368k），使模型能以较低成本学习时间序列信息。在跨域适应的文本图像任务中，VimTS方法在多个基准测试中表现出卓越性能，优于现有技术方法平均2.6%。对于视频级别的跨域适应，VimTS方法甚至在ICDAR2015视频和DSText v2上平均超出之前的端到端视频识别方法5.5%。与现有的大型多模态模型相比，VimTS在跨域场景文本识别方面展现出优势，且所需参数和数据更少。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VimTS方法通过生成提示查询模块和任务感知适配器，实现了单任务模型到多任务模型的转换，适用于图像和视频场景。</li>
<li>VimTS方法通过合成视频文本数据集（VTD-368k），使模型能够低成本地学习时间序列信息。</li>
<li>VimTS方法在多个跨域基准测试中表现出卓越性能，包括不同文本和图像域的适应。</li>
<li>在视频级别的跨域适应中，VimTS方法显著优于之前的端到端视频识别方法。</li>
<li>与现有大型多模态模型相比，VimTS在跨域场景文本识别方面展现出优势，且参数和数据需求更少。</li>
<li>VimTS方法通过实现任务间的更好协同，增强了模型的泛化能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f6e7e98614248d3c744cdf84eac7f35e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20ddf5ee8d45f8d89b38acbfb080a543.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef497ce9045e2a79784ed651cb501142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a70a938a0b3b4ad66f5e15abd6215c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-732e80b5850a49baa56fae267715864a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9f08976df42514822cf5c9d5706cf73a.jpg" align="middle">
</details>




<h2 id="Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting"><a href="#Bayesian-Parameter-Efficient-Fine-Tuning-for-Overcoming-Catastrophic-Forgetting" class="headerlink" title="Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting"></a>Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic   Forgetting</h2><p><strong>Authors:Haolin Chen, Philip N. Garner</strong></p>
<p>We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model’s inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones. </p>
<blockquote>
<p>我们主要受到文本到语音合成模型的适应性的驱动；然而，我们认为更通用的参数高效微调（PEFT）是执行此类适应性的适当框架。然而，灾难性遗忘仍然是PEFT的一个问题，会损害预训练模型的固有能力。我们证明，只要精细调整层的参数变化可以微分计算，现有的贝叶斯学习技术就可以应用于PEFT以防止灾难性遗忘。在一系列语言建模和语音合成任务的实验中，我们利用已建立的拉普拉斯近似，包括对角线和克若内克因子分解方法，以正则化PEFT的低阶适应（LoRA），并比较它们在预训练知识保留方面的性能。结果表明，我们的方法能够克服灾难性遗忘，同时不降低微调性能，并且使用克若内克因子分解近似比使用对角线近似更好地保留预训练知识。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.12220v3">PDF</a> </p>
<p><strong>Summary</strong><br>     本文主要探讨了文本到语音合成模型的适应性调整问题，并提出利用更通用的参数高效微调（PEFT）框架进行此类调整。然而，PEFT存在灾难性遗忘问题，会损害预训练模型的固有功能。作者展示了现有的贝叶斯学习技术可以应用于PEFT以防止灾难性遗忘，只要能够计算微调层的参数变化是可微分的。通过实验，作者利用Laplace近似方法，包括对角线和Kronecker分解法，对PEFT进行正则化，并与低秩适应（LoRA）进行比较，结果表明作者的方法能够克服灾难性遗忘而不损害微调性能，且使用Kronecker分解法比使用对角线法更好地保留预训练知识。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章主要关注文本到语音合成模型的适应性调整。</li>
<li>参数高效微调（PEFT）是一个适用于此类调整的通用框架。</li>
<li>PEFT存在灾难性遗忘问题，即它会损害预训练模型的固有功能。</li>
<li>现有的贝叶斯学习技术可以应用于PEFT以防止灾难性遗忘。</li>
<li>只要微调层的参数变化可微分，就能应用贝叶斯学习技术。</li>
<li>实验利用Laplace近似方法对PEFT进行正则化，并与低秩适应（LoRA）进行比较。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-91d17a7c8f1b6a0625fecb03c05952c4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1d6417acfdbfc3b9b7a1ef71e8297497.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3793a3389ce1451686ef4ed95b62ef38.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-035ba61f5cf002bdc4724210bc479027.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-111842636e0d3e93e65cd94d73b190bc.jpg" align="middle">
</details>




<h2 id="Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation"><a href="#Paralinguistics-Aware-Speech-Empowered-Large-Language-Models-for-Natural-Conversation" class="headerlink" title="Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation"></a>Paralinguistics-Aware Speech-Empowered Large Language Models for Natural   Conversation</h2><p><strong>Authors:Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Soyoon Kim, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Jung-Woo Ha, Sungroh Yoon, Kang Min Yoo</strong></p>
<p>Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm">https://github.com/naver-ai/usdm</a>. </p>
<blockquote>
<p>近期的研究工作显示，在扩大大型语言模型（LLM）的能力以直接理解和合成语音方面取得了有前景的结果。然而，基于LLM的建模口语对话的策略仍然难以捉摸，需要进一步的调查。本文介绍了一个全面的语音文本LLM框架，即统一口语对话模型（USDM），旨在生成与给定输入语音相关的连贯口语响应，而无需依赖自动语音识别（ASR）或文本到语音（TTS）系统。我们已经验证了包含主要含有语义信息的语音符号中的语调，并使用这个基础来构建一个注入语调的语音文本模型。此外，我们提出了一种通用的语音文本预训练方案，该方案提高了跨模式语义的捕获能力。为了构建USDM，我们使用多步骤口语对话模板对语音文本模型进行微调，该模板刺激了下层LLM所展现的推理能力。在DailyTalk数据集上的自动和人类评估表明，我们的方法有效地生成了自然流畅的口语响应，超越了先前的和级联的基线。我们的代码和检查点可在<a target="_blank" rel="noopener" href="https://github.com/naver-ai/usdm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/naver-ai/usdm找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.05706v3">PDF</a> NeurIPS 2024, Project Page: <a target="_blank" rel="noopener" href="https://unifiedsdm.github.io/">https://unifiedsdm.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种统一的口语对话模型（USDM），能够直接理解和合成语音，生成与输入语音相关的连贯口语响应，具有自然发生的韵律特征，无需依赖自动语音识别（ASR）或文本到语音（TTS）系统。通过构建包含主要语义信息的韵律语音令牌，提出一种通用的语音文本预训练方案，增强跨模态语义的捕获。在口语对话数据上微调模型，通过多步口语对话模板刺激底层大型语言模型的推理能力。在DailyTalk数据集上的自动和人类评估表明，该方法生成的自然口语响应超过先前和级联基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在直接理解和合成语音方面展现出有前景的结果。</li>
<li>论文提出了一种统一的口语对话模型（USDM），能生成与输入语音相关的连贯口语响应。</li>
<li>USDM不需要依赖自动语音识别（ASR）或文本到语音（TTS）系统。</li>
<li>USDM通过在包含主要语义信息的语音令牌中融入韵律，构建了一个韵律语音文本模型。</li>
<li>提出了一种通用的语音文本预训练方案，以增强跨模态语义的捕获。</li>
<li>模型在口语对话数据上进行微调，通过多步口语对话模板激发LLM的推理能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6798c63bc786ca1de2c782f0be72cea6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-91dc65923495bfd0c6671bc48d3bac96.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-df6d275e2fd773757f0bfd6c831df15e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c49f6c367b897312213c20c2022c1d95.jpg" align="middle">
</details>




<h2 id="Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer"><a href="#Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer" class="headerlink" title="Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer"></a>Speeding Up Speech Synthesis In Diffusion Models By Reducing Data   Distribution Recovery Steps Via Content Transfer</h2><p><strong>Authors:Peter Ochieng</strong></p>
<p>Diffusion based vocoders have been criticised for being slow due to the many steps required during sampling. Moreover, the model’s loss function that is popularly implemented is designed such that the target is the original input $x_0$ or error $\epsilon_0$. For early time steps of the reverse process, this results in large prediction errors, which can lead to speech distortions and increase the learning time. We propose a setup where the targets are the different outputs of forward process time steps with a goal to reduce the magnitude of prediction errors and reduce the training time. We use the different layers of a neural network (NN) to perform denoising by training them to learn to generate representations similar to the noised outputs in the forward process of the diffusion. The NN layers learn to progressively denoise the input in the reverse process until finally the final layer estimates the clean speech. To avoid 1:1 mapping between layers of the neural network and the forward process steps, we define a skip parameter $\tau&gt;1$ such that an NN layer is trained to cumulatively remove the noise injected in the $\tau$ steps in the forward process. This significantly reduces the number of data distribution recovery steps and, consequently, the time to generate speech. We show through extensive evaluation that the proposed technique generates high-fidelity speech in competitive time that outperforms current state-of-the-art tools. The proposed technique is also able to generalize well to unseen speech. </p>
<blockquote>
<p>基于扩散的vocoder由于采样过程中需要多个步骤而速度较慢，受到了批评。此外，目前广泛实现的模型的损失函数是针对原始输入x_0或误差ε_0设计的。在反向过程的前期步骤中，这会导致较大的预测误差，可能导致语音失真并增加学习时间。我们提出了一种设置，目标为正向过程时间步骤的不同输出，旨在减少预测误差的大小并缩短训练时间。我们使用神经网络（NN）的不同层执行去噪，通过训练它们学习生成与扩散正向过程中的噪声输出相似的表示。神经网络层学习在反向过程中逐步对输入进行去噪，直到最终层估计出干净的语音。为了避免神经网络层与正向过程步骤之间的1:1映射，我们定义一个跳过参数τ&gt; 1，使得神经网络层被训练累积地去除在正向过程的τ步中注入的噪声。这显著减少了数据分布恢复步骤的数量，因此也缩短了生成语音的时间。通过广泛评估，我们证明所提出的技术能够在竞争时间内生成高保真度的语音，并且优于当前最先进的工具。所提出的技术也能够很好地推广到未见过的语音。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09652v3">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于扩散的vocoder改进方案，通过调整目标设定和神经网络层的设计，减少了预测误差和训练时间，生成了高质量、高保真度的语音，并在竞争时间内超越了当前最先进的工具。此外，该技术还能很好地泛化到未见过的语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散vocoder因采样过程中步骤繁多而受到速度慢的挑战。</li>
<li>当前流行的模型损失函数设计导致早期反向过程中的预测误差较大，可能造成语音失真和学习时间增加。</li>
<li>提出以扩散过程前向步骤的不同输出来设定目标，以减小预测误差和缩短训练时间。</li>
<li>使用神经网络的不同层进行去噪训练，学习生成与扩散前向过程中的噪声输出相似的表示。</li>
<li>引入跳跃参数τ&gt;1，使神经网络层累积学习在τ步中注入的噪声的前向过程，从而减少数据分布恢复步骤和生成语音的时间。</li>
<li>经广泛评估，该技术在竞争时间内生成了高保真度的语音，超越了当前最先进的技术。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c25b2ace83cc3057ff3a08e435a27f4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e96c9504cca1fdb2836c0820b9d1871.jpg" align="middle">
</details>




<h2 id="Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds"><a href="#Fast-Likelihood-free-Reconstruction-of-Gravitational-Wave-Backgrounds" class="headerlink" title="Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds"></a>Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds</h2><p><strong>Authors:Androniki Dimitriou, Daniel G. Figueroa, Bryan Zaldivar</strong></p>
<p>We apply state-of-the-art, likelihood-free statistical inference (machine-learning-based) techniques for reconstructing the spectral shape of a gravitational wave background (GWB). We focus on the reconstruction of an arbitrarily shaped signal by the LISA detector, but the method can be easily extended to either template-dependent signals, or to other detectors, as long as a characterisation of the instrumental noise is available. As proof of the technique, we quantify the ability of LISA to reconstruct signals of arbitrary spectral shape (${\it blind}$ reconstruction), considering a diversity of frequency profiles, and including astrophysical backgrounds in some cases. As a teaser of how the method can reconstruct signals characterised by a parameter-dependent template (${\it template}$ reconstruction), we present a dedicated study for power-law signals. While our technique has several advantages with respect to traditional MCMC methods, we validate it with the latter for concrete cases. This work opens the door for both fast and accurate Bayesian parameter estimation of GWBs, with essentially no computational overhead during the inference step. Our set of tools are integrated into the package ${\tt GWBackFinder}$, which is publicly available in <a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder">https://github.com/AndronikiDimitriou/GWBackFinder</a>. </p>
<blockquote>
<p>我们采用最先进的无似然统计推断（基于机器学习）技术来重建引力波背景（GWB）的频谱形状。我们专注于通过LISA探测器重建任意形状的信号，但只要有仪器噪声的特征，该方法可以很容易地扩展到模板依赖的信号或其他探测器。作为该技术的证明，我们量化了LISA在多种频率分布下重建任意频谱形状信号的能力（盲重建），在某些情况下还包括天文背景。作为该方法如何重建参数依赖模板信号的一个提示（模板重建），我们对幂律信号进行了专项研究。虽然我们的技术与传统的MCMC方法相比具有许多优势，但对于具体案例，我们仍使用后者对其进行验证。这项工作为GWB的快速和精确贝叶斯参数估计打开了大门，在推理步骤中几乎不需要计算开销。我们的工具集已集成到GWBackFinder软件包中，可在<a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AndronikiDimitriou/GWBackFinder公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08430v3">PDF</a> Published in JCAP. 29 pages plus appendices and references, 12   figures</p>
<p><strong>Summary</strong><br>     利用最先进的无需似然函数的统计推断（基于机器学习）技术，对引力波背景（GWB）的频谱形状进行重建。重点研究LISA探测器对任意形状信号的重建，但该方法可轻松扩展到依赖于模板的信号或其他探测器，只要提供仪器噪声的特征即可。作为中国证明该技术能力的展示，我们量化了LISA对任意频谱形状信号的重建能力（盲重建），并考虑了多种频率分布，在某些情况下还包括天文背景。作为该方法的模板重建概念的预览，我们对幂律信号进行了专项研究。虽然我们的技术与传统的MCMC方法相比具有多个优势，但我们仍将其用于具体案例进行验证。这为快速准确的GWB贝叶斯参数估计打开了大门，推理步骤几乎没有计算开销。我们的工具集已集成到GWBackFinder软件包中，可在<a target="_blank" rel="noopener" href="https://github.com/AndronikiDimitriou/GWBackFinder">https://github.com/AndronikiDimitriou/GWBackFinder</a>公开访问。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用机器学习技术重建引力波背景（GWB）的频谱形状。</li>
<li>重点关注LISA探测器对任意形状信号的重建能力。</li>
<li>方法具有通用性，可应用于依赖于模板的信号或其他探测器。</li>
<li>展示了盲重建能力，即对任意频谱形状信号的重建，并考虑了多种频率分布和天文背景。</li>
<li>通过幂律信号进行模板重建的预览。</li>
<li>与传统MCMC方法相比具有优势，并通过具体案例进行验证。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b71214c325a1d33137c11a8085cf040a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f111152cd7161ef71d176cd5aef745f9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5a525d3e4cea6bd0abe6e892d77d4dd2.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
