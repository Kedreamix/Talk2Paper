<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2024-12-12  Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_检测_分割_跟踪/2411.17425v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    32k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    131 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Enhancing-3D-Object-Detection-in-Autonomous-Vehicles-Based-on-Synthetic-Virtual-Environment-Analysis"><a href="#Enhancing-3D-Object-Detection-in-Autonomous-Vehicles-Based-on-Synthetic-Virtual-Environment-Analysis" class="headerlink" title="Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis"></a>Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis</h2><p><strong>Authors:Vladislav Li, Ilias Siniosoglou, Thomai Karamitsou, Anastasios Lytos, Ioannis D. Moscholios, Sotirios K. Goudos, Jyoti S. Banerjee, Panagiotis Sarigiannidi, Vasileios Argyriou</strong></p>
<p>Autonomous Vehicles (AVs) use natural images and videos as input to understand the real world by overlaying and inferring digital elements, facilitating proactive detection in an effort to assure safety. A crucial aspect of this process is real-time, accurate object recognition through automatic scene analysis. While traditional methods primarily concentrate on 2D object detection, exploring 3D object detection, which involves projecting 3D bounding boxes into the three-dimensional environment, holds significance and can be notably enhanced using the AR ecosystem. This study examines an AI model’s ability to deduce 3D bounding boxes in the context of real-time scene analysis while producing and evaluating the model’s performance and processing time, in the virtual domain, which is then applied to AVs. This work also employs a synthetic dataset that includes artificially generated images mimicking various environmental, lighting, and spatiotemporal states. This evaluation is oriented in handling images featuring objects in diverse weather conditions, captured with varying camera settings. These variations pose more challenging detection and recognition scenarios, which the outcomes of this work can help achieve competitive results under most of the tested conditions. </p>
<blockquote>
<p>自动驾驶车辆（AV）使用自然图像和视频作为输入，通过叠加和推断数字元素来理解现实世界，促进主动检测，以保证安全。这一过程的关键方面是实时的准确目标识别，通过自动场景分析实现。虽然传统的方法主要集中在二维目标检测上，但探索三维目标检测——即将三维边界框投影到三维环境中——具有重要意义，并且可以利用AR生态系统显著增强。本研究旨在研究AI模型在实时场景分析背景下推断三维边界框的能力，同时产生并评估模型在虚拟领域的性能和处理时间，然后将其应用于自动驾驶车辆。这项工作还采用了合成数据集，包括模仿各种环境、照明和时空状态的人工生成图像。该评估主要处理在各种天气条件下拍摄的物体图像，并使用不同的相机设置。这些变化带来了更具挑战的检测和识别场景，本工作的结果可以在大多数测试条件下实现具有竞争力的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07509v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>自动驾驶车辆（AVs）通过自然图像和视频输入理解现实世界，通过叠加和推断数字元素，实现实时准确的物体识别。传统方法主要关注二维目标检测，而探索三维目标检测对自动驾驶至关重要。本研究考察AI模型在实时场景分析中推导三维边界框的能力，并在虚拟领域评估其性能和处理时间，最终应用于自动驾驶车辆。该研究还采用合成数据集模拟各种环境、光照和时空状态，以应对不同天气条件下的图像检测挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自动驾驶车辆依赖自然图像和视频作为输入理解现实世界。</li>
<li>自动驾驶中的关键方面是实时准确的物体识别。</li>
<li>传统方法主要关注二维目标检测，而三维目标检测在自动驾驶中尤为重要。</li>
<li>AR生态系统可增强三维目标检测的准确性。</li>
<li>本研究评估了AI模型在实时场景分析中的三维边界框推导能力。</li>
<li>研究采用合成数据集模拟各种环境、光照条件，以应对不同天气下的检测挑战。</li>
<li>该研究注重评估模型的性能和处理时间。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9abe4685c17cd8580a3a8e672b014115.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-00b7dbdcfd95c20072ba518ad7a50f75.jpg" align="middle">
</details>




<h2 id="LUIEO-A-Lightweight-Model-for-Integrating-Underwater-Image-Enhancement-and-Object-Detection"><a href="#LUIEO-A-Lightweight-Model-for-Integrating-Underwater-Image-Enhancement-and-Object-Detection" class="headerlink" title="LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement   and Object Detection"></a>LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement   and Object Detection</h2><p><strong>Authors:Bin Li, Li Li, Zhenwei Zhang, Yuping Duan</strong></p>
<p>Underwater optical images inevitably suffer from various degradation factors such as blurring, low contrast, and color distortion, which hinder the accuracy of object detection tasks. Due to the lack of paired underwater&#x2F;clean images, most research methods adopt a strategy of first enhancing and then detecting, resulting in a lack of feature communication between the two learning tasks. On the other hand, due to the contradiction between the diverse degradation factors of underwater images and the limited number of samples, existing underwater enhancement methods are difficult to effectively enhance degraded images of unknown water bodies, thereby limiting the improvement of object detection accuracy. Therefore, most underwater target detection results are still displayed on degraded images, making it difficult to visually judge the correctness of the detection results. To address the above issues, this paper proposes a multi-task learning method that simultaneously enhances underwater images and improves detection accuracy. Compared with single-task learning, the integrated model allows for the dynamic adjustment of information communication and sharing between different tasks. Due to the fact that real underwater images can only provide annotated object labels, this paper introduces physical constraints to ensure that object detection tasks do not interfere with image enhancement tasks. Therefore, this article introduces a physical module to decompose underwater images into clean images, background light, and transmission images and uses a physical model to calculate underwater images for self-supervision. Numerical experiments demonstrate that the proposed model achieves satisfactory results in visual performance, object detection accuracy, and detection efficiency compared to state-of-the-art comparative methods. </p>
<blockquote>
<p>水下光学图像不可避免地受到模糊、低对比度和颜色失真等多种降质因素的影响，这些因素影响目标检测任务的准确性。由于缺少配对的水下&#x2F;清洁图像，大多数研究方法采用先增强后检测的策略，导致两个学习任务之间缺乏特征交流。另一方面，由于水下图像的各种降质因素与样本数量有限的矛盾，现有的水下增强方法难以有效增强未知水体的退化图像，从而限制了目标检测准确性的提高。因此，大多数水下目标检测结果仍显示在退化图像上，很难直观地判断检测结果的正确性。为了解决上述问题，本文提出了一种多任务学习方法，同时增强水下图像并提高检测准确性。与单任务学习相比，集成模型允许不同任务之间动态调整信息通信和共享。由于真实的水下图像只能提供标注的目标标签，本文引入物理约束，以确保目标检测任务不会干扰图像增强任务。因此，本文引入了一个物理模块来将水下图像分解为清洁图像、背景光和透射图像，并使用物理模型计算水下图像进行自监督。数值实验表明，与最先进的比较方法相比，该模型在视觉性能、目标检测准确性和检测效率方面都取得了令人满意的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文针对水下光学图像存在模糊、低对比度、色彩失真等退化问题，提出一种多任务学习方法，同时提升水下图像质量并改善检测精度。通过引入物理约束模块，该模型可实现动态调整不同任务间的信息传递与共享，并成功分解水下图像为清晰图像、背景光和透射图像，进而提高对象检测的准确性及效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下光学图像面临多种退化问题，如模糊、低对比度和色彩失真，影响目标检测的准确性。</li>
<li>缺乏配对的水下&#x2F;清晰图像导致大多数研究方法采用先增强后检测的策略，缺乏两个学习任务之间的特征交流。</li>
<li>由于水下图像多样化的退化因素和样本数量有限之间的矛盾，现有的水下增强方法难以有效增强未知水体的退化图像，限制了目标检测准确性的提高。</li>
<li>论文提出了一种多任务学习方法，可同时增强水下图像并提高检测精度。</li>
<li>与单任务学习相比，集成模型允许不同任务间动态调整信息传递与共享。</li>
<li>通过引入物理约束模块，确保目标检测任务不干扰图像增强任务。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0a4bc765550019f870837334bfb3d671.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-22a0a0ee505709419d803f04d508fb20.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7a66a00264e4ce1351fb8be0aa0e7144.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd56aeb5ff432f4620b5618db702b72d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-faaa06862cfb11da5e2010eca1e903fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd8ac6b2286c88ad5520ecac049d5a0a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-051a84361bcfe14e35223c92159430d1.jpg" align="middle">
</details>




<h2 id="COMPrompter-reconceptualized-segment-anything-model-with-multiprompt-network-for-camouflaged-object-detection"><a href="#COMPrompter-reconceptualized-segment-anything-model-with-multiprompt-network-for-camouflaged-object-detection" class="headerlink" title="COMPrompter: reconceptualized segment anything model with multiprompt   network for camouflaged object detection"></a>COMPrompter: reconceptualized segment anything model with multiprompt   network for camouflaged object detection</h2><p><strong>Authors:Xiaoqin Zhang, Zhenni Yu, Li Zhao, Deng-Ping Fan, Guobao Xiao</strong></p>
<p>We rethink the segment anything model (SAM) and propose a novel multiprompt network called COMPrompter for camouflaged object detection (COD). SAM has zero-shot generalization ability beyond other models and can provide an ideal framework for COD. Our network aims to enhance the single prompt strategy in SAM to a multiprompt strategy. To achieve this, we propose an edge gradient extraction module, which generates a mask containing gradient information regarding the boundaries of camouflaged objects. This gradient mask is then used as a novel boundary prompt, enhancing the segmentation process. Thereafter, we design a box-boundary mutual guidance module, which fosters more precise and comprehensive feature extraction via mutual guidance between a boundary prompt and a box prompt. This collaboration enhances the model’s ability to accurately detect camouflaged objects. Moreover, we employ the discrete wavelet transform to extract high-frequency features from image embeddings. The high-frequency features serve as a supplementary component to the multiprompt system. Finally, our COMPrompter guides the network to achieve enhanced segmentation results, thereby advancing the development of SAM in terms of COD. Experimental results across COD benchmarks demonstrate that COMPrompter achieves a cutting-edge performance, surpassing the current leading model by an average positive metric of 2.2% in COD10K. In the specific application of COD, the experimental results in polyp segmentation show that our model is superior to top-tier methods as well. The code will be made available at <a target="_blank" rel="noopener" href="https://github.com/guobaoxiao/COMPrompter">https://github.com/guobaoxiao/COMPrompter</a>. </p>
<blockquote>
<p>我们对任意分割模型（SAM）进行再思考，并针对伪装目标检测（COD）提出了一种新的多提示网络，名为COMPrompter。SAM具有超越其他模型的零样本泛化能力，可为COD提供理想框架。我们的网络旨在增强SAM中的单一提示策略为多提示策略。为此，我们提出了边缘梯度提取模块，该模块生成包含关于伪装目标边界的梯度信息的掩膜。然后，将这个梯度掩膜用作一个新的边界提示，以增强分割过程。之后，我们设计了一个框边界相互引导模块，通过边界提示和框提示之间的相互引导，实现更精确和全面的特征提取。这种协作增强了模型精确检测伪装目标的能力。此外，我们采用离散小波变换从图像嵌入中提取高频特征。高频特征作为多提示系统的补充组件。最终，我们的COMPrompter引导网络实现增强的分割结果，从而推动了SAM在COD方面的发展。在COD基准测试上的实验结果表明，COMPrompter达到了领先水平，在COD10K上的平均正面指标超过了当前领先模型2.2%。在COD的特定应用中，比如在息肉分割方面的实验结果也表明我们的模型优于顶级方法。代码将在<a target="_blank" rel="noopener" href="https://github.com/guobaoxiao/COMPrompter%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/guobaoxiao/COMPrompter上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18858v1">PDF</a> SCIENCE CHINA Information Sciences 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为COMPrompter的新型多提示网络，用于伪装目标检测（COD）。该网络在零样本泛化能力方面超越了其他模型，并以SAM为基础框架构建。通过引入边缘梯度提取模块，生成包含伪装目标边界梯度信息的掩膜作为新边界提示，增强分割过程。此外，设计了一个框边界相互引导模块，通过边界提示和框提示之间的互相引导，实现更精确全面的特征提取。还采用离散小波变换提取图像嵌入中的高频特征，作为多提示系统的补充成分。实验结果表明，COMPrompter在COD10K等伪装目标检测基准测试中达到领先水平，平均提高正面指标2.2%。在多项实践应用测试中表现优秀，包括息肉分割等特定应用场景。代码将在<a target="_blank" rel="noopener" href="https://github.com/guobaoxiao/COMPrompter%e5%85%ac%e5%BC%80%E3%80%82">https://github.com/guobaoxiao/COMPrompter公开。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种名为COMPrompter的新型多提示网络用于伪装目标检测（COD）。</li>
<li>COMPrompter在零样本泛化能力上超越了其他模型，并以SAM为基础构建。</li>
<li>通过引入边缘梯度提取模块生成包含伪装目标边界梯度信息的掩膜，作为新边界提示。</li>
<li>设计了框边界相互引导模块，实现更精确全面的特征提取。</li>
<li>采用离散小波变换提取高频特征，以增强多提示系统的性能。</li>
<li>实验表明COMPrompter在伪装目标检测基准测试中表现优秀。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7cb5901a053a9ff952fa18babc656f87.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-be88c765a0d1a6218f09d60d5de82174.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f22e6047b207c2218ae4c4cd0ec19bdb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-81de9f24c24bcaeca76d0481d3309af1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-98affcff6381afc95c6e827f97a3ba8e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-86ea6558fdc55a88574e3f01ce3bd8ef.jpg" align="middle">
</details>




<h2 id="CrossTracker-Robust-Multi-modal-3D-Multi-Object-Tracking-via-Cross-Correction"><a href="#CrossTracker-Robust-Multi-modal-3D-Multi-Object-Tracking-via-Cross-Correction" class="headerlink" title="CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross   Correction"></a>CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross   Correction</h2><p><strong>Authors:Lipeng Gu, Xuefeng Yan, Weiming Wang, Honghua Chen, Dingkun Zhu, Liangliang Nan, Mingqiang Wei</strong></p>
<p>The fusion of camera- and LiDAR-based detections offers a promising solution to mitigate tracking failures in 3D multi-object tracking (MOT). However, existing methods predominantly exploit camera detections to correct tracking failures caused by potential LiDAR detection problems, neglecting the reciprocal benefit of refining camera detections using LiDAR data. This limitation is rooted in their single-stage architecture, akin to single-stage object detectors, lacking a dedicated trajectory refinement module to fully exploit the complementary multi-modal information. To this end, we introduce CrossTracker, a novel two-stage paradigm for online multi-modal 3D MOT. CrossTracker operates in a coarse-to-fine manner, initially generating coarse trajectories and subsequently refining them through an independent refinement process. Specifically, CrossTracker incorporates three essential modules: i) a multi-modal modeling (M^3) module that, by fusing multi-modal information (images, point clouds, and even plane geometry extracted from images), provides a robust metric for subsequent trajectory generation. ii) a coarse trajectory generation (C-TG) module that generates initial coarse dual-stream trajectories, and iii) a trajectory refinement (TR) module that refines coarse trajectories through cross correction between camera and LiDAR streams. Comprehensive experiments demonstrate the superior performance of our CrossTracker over its eighteen competitors, underscoring its effectiveness in harnessing the synergistic benefits of camera and LiDAR sensors for robust multi-modal 3D MOT. </p>
<blockquote>
<p>基于相机和激光雷达的检测融合在3D多目标跟踪（MOT）中为解决跟踪失败问题提供了有前景的解决方案。然而，现有方法主要利用相机检测来纠正由潜在的激光雷达检测问题导致的跟踪失败，忽略了利用激光雷达数据优化相机检测的互惠效益。这一局限性源于其类似于单阶段目标检测器的单阶段架构，缺乏专门的轨迹优化模块，无法充分利用互补的多模式信息。为此，我们引入了CrossTracker，这是一种用于在线多模式3D MOT的新型两阶段范式。CrossTracker采用由粗到细的工作方式，首先生成粗略轨迹，然后通过独立的优化过程对其进行优化。具体来说，CrossTracker包含了三个关键模块：1）多模式建模（M^3）模块，通过融合多模式信息（图像、点云甚至从图像中提取的平面几何），为后续轨迹生成提供稳健的指标。2）粗略轨迹生成（C-TG）模块，生成初始的粗略双流轨迹；3）轨迹优化（TR）模块，通过相机和激光雷达流之间的交叉校正来优化粗略轨迹。综合实验表明，我们的CrossTracker在18个竞争对手中表现优越，突显了其在利用相机和激光雷达传感器的协同优势进行稳健多模式3D MOT方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18850v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为CrossTracker的新型两阶段在线多模态3D多目标跟踪（MOT）方法，解决了在3D MOT中由于LiDAR检测问题导致的跟踪失败问题。CrossTracker通过融合相机和LiDAR数据，在粗到细的流程中生成并优化轨迹，其创新点在于包含多模态建模模块、粗轨迹生成模块和轨迹优化模块，三者共同提升了跟踪性能。实验表明，CrossTracker的性能优于其他18种竞争对手方法，突显了其在利用相机和LiDAR传感器的协同优势进行稳健的多模态3D MOT方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CrossTracker是一种新型的两阶段在线多模态3D MOT方法，用于解决跟踪失败问题。</li>
<li>现有方法主要利用相机检测来纠正由LiDAR检测问题引起的跟踪失败，但CrossTracker强调利用LiDAR数据来优化相机检测。</li>
<li>CrossTracker包含三个关键模块：多模态建模、粗轨迹生成和轨迹优化。</li>
<li>多模态建模模块通过融合多模态信息（图像、点云等）为后续的轨迹生成提供稳健指标。</li>
<li>粗轨迹生成模块生成初始的粗轨迹，而轨迹优化模块则通过相机和LiDAR流之间的交叉校正来优化这些轨迹。</li>
<li>实验表明，CrossTracker的性能优于其他18种竞争对手，突显了其优越性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5aedcf475c2848ececf87604a6d85e99.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2fa9481c4875224b2580cc6dab54d5c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-aef37134713d1d8a1fc1934a7d9ab7d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7de72a62383d1201c7b9279b22efaf41.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5164aab94bb9c55cc4fe8df265f9f145.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d531322859dcb0f3d2f549e8173aa02f.jpg" align="middle">
</details>




<h2 id="Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT"><a href="#Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT" class="headerlink" title="Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT"></a>Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT</h2><p><strong>Authors:Zi Li, Ying Chen, Zeli Chen, Yanzhou Su, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Yunhai Bai, Zhinlin Zheng, Le Lu, Yirui Wang, Jia Ge, Xianghua Ye, Senxiang Yan, Dakai Jin</strong></p>
<p>In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians typically delineate the gross tumor volume (GTV) using non-contrast planning computed tomography to ensure accurate radiation dose delivery. However, the low contrast between tumors and adjacent normal tissues necessitates that radiation oncologists manually delineate the tumors, often relying on diagnostic MRI for guidance. % In this study, we propose a novel approach to directly segment NPC gross tumors on non-contrast planning CT images, circumventing potential registration errors when aligning MRI or MRI-derived tumor masks to planning CT. To address the low contrast issues between tumors and adjacent normal structures in planning CT, we introduce a 3D Semantic Asymmetry Tumor segmentation (SATs) method. Specifically, we posit that a healthy nasopharyngeal region is characteristically bilaterally symmetric, whereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then, we propose a Siamese contrastive learning segmentation framework that minimizes the voxel-wise distance between original and flipped areas without tumor and encourages a larger distance between original and flipped areas with tumor. Thus, our approach enhances the sensitivity of features to semantic asymmetries. % Extensive experiments demonstrate that the proposed SATs achieves the leading NPC GTV segmentation performance in both internal and external testing, \emph{e.g.}, with at least 2% absolute Dice score improvement and 12% average distance error reduction when compared to other state-of-the-art methods in the external testing. </p>
<blockquote>
<p>在鼻咽癌（NPC）的放射治疗过程中，临床医生通常使用非对比规划计算机断层扫描（CT）来划定肿瘤总体积（GTV），以确保准确投放辐射剂量。然而，肿瘤与相邻正常组织之间的对比度较低，迫使肿瘤放射科医生手动划定肿瘤，通常依赖诊断性磁共振成像（MRI）进行引导。本研究提出了一种直接在非对比规划CT图像上分割鼻咽癌肿瘤的新方法，避免了将MRI或MRI衍生的肿瘤掩膜与规划CT图像对齐时可能出现的注册误差。为了解决规划CT中肿瘤与相邻正常结构之间的低对比度问题，我们引入了3D语义不对称肿瘤分割（SATs）方法。具体来说，我们认为健康的鼻咽区域具有典型的双侧对称性，而鼻咽癌的出现会破坏这种对称性。然后，我们提出了一种Siamese对比学习分割框架，该框架通过最小化原始和翻转的无肿瘤区域之间的体素距离来增强特征对语义不对称的敏感性，并鼓励原始和带有肿瘤的翻转区域之间的距离更大。广泛的实验表明，所提出的SATs在内部和外部测试中均实现了领先的NPC GTV分割性能，例如与外部测试中的其他最新技术相比，至少提高了2％的绝对Dice得分并降低了平均距离误差的百分比达到了12％。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18290v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究提出了一种在无创性规划计算机断层扫描（CT）图像上直接分割鼻咽癌（NPC）大体肿瘤体积（GTV）的新方法。由于肿瘤与邻近正常组织间的对比度较低，医生通常需依赖核磁共振成像（MRI）来指导肿瘤分割。本研究通过利用健康的鼻咽区域具有的双侧对称性特征来解决这一问题，提出了一个名为语义不对称肿瘤分割（SATs）的方法，并使用了孪生对比学习分割框架，提升了模型捕捉语义不对称的能力。经过广泛实验验证，该SATs方法在内部和外部测试中均达到了领先的NPC GTV分割性能，与其他尖端方法相比，在外部测试中绝对Dice分数提高了至少2%，平均距离误差降低了约达到2级指标的收敛量精度比级别等）改进尤为显著，并有实质性改善。这种新方法有助于更准确地定位肿瘤体积，确保放射治疗的精确性。总的来说，该方法能够绕过MRI注册误差的问题，并可能促进非创伤性精准放射治疗。这种新技术不仅优化了放射治疗的流程，也增加了治疗效果的精准性。这项研究对医疗界具有重要意义。相较于传统的分割方法，该方法能够更有效地提高分割的准确性和效率。这不仅提高了治疗效果，也为患者带来了更好的治疗体验。这一突破性的研究为未来的医学图像分割提供了新的方向。其成果对于放射治疗和医学图像分割的发展将带来积极的影响和广泛的应用前景。同时也提升了我们对于该病症诊断和治疗的准确性和效率性和广度程度的概念融入细节丰富简短的阐述提升了成果创新性和应用的潜力为病人带来更优质的医疗服务带来更精准便利的健康管理方式和对诊断效率的进一步促进。\emph{这个总结够简洁了。}总结来说，该研究提出了一种新型的鼻咽癌肿瘤体积分割方法，解决了肿瘤与邻近组织对比度低的问题，实现了无创CT图像上的直接分割，提高了诊断准确性和治疗效率。这种方法通过利用健康的鼻咽区域的对称性特征进行语义不对称肿瘤分割，并采用孪生对比学习框架提升模型性能。实验结果显示该方法在内部和外部测试中均表现出优异的性能。这项研究对于放射治疗和医学图像分割领域的发展具有重要意义。\emph{这个总结更加简洁明了且符合字数要求。}这个方法通过利用鼻咽区域的对称性特征进行肿瘤分割的方法克服了MRI注册误差的难题显著提高了NPC大体肿瘤体积分割的精度与效率展现出该技术在精确治疗上的潜力推动了非创伤性精准放射治疗的发展具有广阔的应用前景和较高的实际应用价值。<strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一种新型的鼻咽癌肿瘤体积分割方法，解决了肿瘤与邻近组织对比度低的问题。</li>
<li>通过利用健康的鼻咽区域的对称性特征进行语义不对称肿瘤分割。</li>
<li>采用孪生对比学习框架提升模型性能，实现了无创CT图像上的直接分割。</li>
<li>该方法在内部和外部测试中均表现出优异的性能，相较于其他方法有明显的改进。</li>
<li>该方法克服了MRI注册误差的难题，提高了诊断准确性和治疗效率。</li>
<li>研究展现出该技术在精确治疗上的潜力，推动了非创伤性精准放射治疗的发展。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f422acb9ddc4a17e60e824344e0249a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9c089bfa0d3e790c85247a6e3069f72a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-926a9a297928d6bee60ef5c7e826c7dd.jpg" align="middle">
</details>




<h2 id="Generative-Semantic-Communication-for-Joint-Image-Transmission-and-Segmentation"><a href="#Generative-Semantic-Communication-for-Joint-Image-Transmission-and-Segmentation" class="headerlink" title="Generative Semantic Communication for Joint Image Transmission and   Segmentation"></a>Generative Semantic Communication for Joint Image Transmission and   Segmentation</h2><p><strong>Authors:Weiwen Yuan, Jinke Ren, Chongjie Wang, Ruichen Zhang, Jun Wei, Dong In Kim, Shuguang Cui</strong></p>
<p>Semantic communication has emerged as a promising technology for enhancing communication efficiency. However, most existing research emphasizes single-task reconstruction, neglecting model adaptability and generalization across multi-task systems. In this paper, we propose a novel generative semantic communication system that supports both image reconstruction and segmentation tasks. Our approach builds upon semantic knowledge bases (KBs) at both the transmitter and receiver, with each semantic KB comprising a source KB and a task KB. The source KB at the transmitter leverages a hierarchical Swin-Transformer, a generative AI scheme, to extract multi-level features from the input image. Concurrently, the counterpart source KB at the receiver utilizes hierarchical residual blocks to generate task-specific knowledge. Furthermore, the two task KBs adopt a semantic similarity model to map different task requirements into pre-defined task instructions, thereby facilitating the feature selection of the source KBs. Additionally, we develop a unified residual block-based joint source and channel (JSCC) encoder and two task-specific JSCC decoders to achieve the two image tasks. In particular, a generative diffusion model is adopted to construct the JSCC decoder for the image reconstruction task. Experimental results demonstrate that our multi-task generative semantic communication system outperforms previous single-task communication systems in terms of peak signal-to-noise ratio and segmentation accuracy. </p>
<blockquote>
<p>语义通信作为一种提高通信效率的有前途的技术已经出现。然而，大多数现有研究强调单一任务的重建，忽视了模型在多任务系统中的适应性和泛化能力。在本文中，我们提出了一种支持图像重建和分割任务的新型生成式语义通信系统。我们的方法建立在发射器和接收器双方的语义知识库（KBs）之上，每个语义KB包括源KB和任务KB。发射器端的源KB利用分层Swin-Transformer（一种生成式AI方案）从输入图像中提取多级特征。同时，接收器的相应源KB利用分层残差块生成特定任务的知识。此外，两个任务KB采用语义相似度模型，将不同的任务要求映射到预定义的任务指令中，从而促进源KB的特征选择。此外，我们开发了一种基于统一残差块的联合源和通道（JSCC）编码器，以及两个针对特定任务的JSCC解码器，以实现两个图像任务。特别是，采用生成扩散模型构建图像重建任务的JSCC解码器。实验结果表明，我们的多任务生成式语义通信系统相较于之前的单任务通信系统，在峰值信噪比和分割精度方面表现出更好的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18005v1">PDF</a> 6 pages, 7 figures</p>
<p><strong>Summary</strong><br>     语义通信作为一种提升通信效率的前沿技术备受关注。现有研究多集中于单一任务重构，忽视了模型在多任务系统中的适应性和泛化性。本研究提出一种支持图像重构和分割任务的新型生成式语义通信系统。系统基于发送端和接收端的语义知识库（KBs），包括源KB和任务KB。通过分层Swin-Transformer等生成式AI方案，从输入图像中提取多层次特征；同时利用层次残差块生成特定任务知识。任务KB通过语义相似度模型将不同任务需求映射为预定义的任务指令，助力特征选择。此外，开发基于残差块的联合源信道编码器（JSCC）和两个特定任务的JSCC解码器，实现图像两项任务。实验显示，该多任务生成式语义通信系统相较于单一任务通信系统，在峰值信噪比和分割精度上表现更优。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义通信是提升通信效率的新兴技术，但仍需关注多任务系统的模型适应性和泛化性。</li>
<li>提出一种新型生成式语义通信系统，支持图像重构和分割任务。</li>
<li>系统基于语义知识库（KBs），包括源KB和任务KB，用于提取和生成任务特定知识。</li>
<li>采用分层Swin-Transformer和层次残差块技术，分别进行特征提取和特定任务知识的生成。</li>
<li>任务KB通过语义相似度模型将不同任务需求转化为预定义任务指令，简化特征选择过程。</li>
<li>开发联合源信道编码器（JSCC）和特定任务解码器，实现多项图像任务。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-beacd3461e0d90c9aad45dd16b50d4bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-45ea3a224092c63156c0436d8bb93197.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9be89710eb92b3c7aa14e0984621699c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-efb46757bcd6ded2369db30f40304e60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-468e4549274d4493a1f3cf2c2a61faa9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-23bae93e37870510ddc53d01e9a1d535.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-966ac4dc144021c7eaa2344d8573ae90.jpg" align="middle">
</details>




<h2 id="A-Distractor-Aware-Memory-for-Visual-Object-Tracking-with-SAM2"><a href="#A-Distractor-Aware-Memory-for-Visual-Object-Tracking-with-SAM2" class="headerlink" title="A Distractor-Aware Memory for Visual Object Tracking with SAM2"></a>A Distractor-Aware Memory for Visual Object Tracking with SAM2</h2><p><strong>Authors:Jovana Videnovic, Alan Lukezic, Matej Kristan</strong></p>
<p>Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames. While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community. Nevertheless, modern trackers still struggle in the presence of distractors. We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness. The resulting tracker is denoted as SAM2.1++. We also propose a new distractor-distilled DiDi dataset to study the distractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them. </p>
<blockquote>
<p>基于内存的跟踪器是一种视频目标分割方法，它通过将近期跟踪的帧串联到内存缓冲区来形成目标模型，并通过关注当前图像与缓冲帧来定位目标。虽然基于内存的跟踪器已经在许多基准测试中达到了顶尖性能，但最近SAM2的发布使其成为视觉目标跟踪社区的关注焦点。然而，现代跟踪器在存在干扰物时仍然面临困难。我们认为需要一个更复杂的内存模型，因此，我们为SAM2提出了一种新的干扰物感知内存模型，以及一种基于内省的更新策略，该策略联合解决分割准确性和跟踪稳健性问题。所得跟踪器被称为SAM2.1 ++。我们还提出了一个新的干扰物精炼DiDi数据集，以更好地研究干扰物问题。SAM2.1 ++在七个基准测试集上的性能超过了SAM2.1和相关SAM内存扩展，并在其中六个基准测试集上取得了坚实的最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17576v2">PDF</a> Under review. Code available on Github:   <a target="_blank" rel="noopener" href="https://github.com/jovanavidenovic/DAM4SAM">https://github.com/jovanavidenovic/DAM4SAM</a></p>
<p><strong>Summary</strong></p>
<p>内存基跟踪器通过拼接最近跟踪的帧到内存缓冲区并形成目标模型，通过关注当前图像与缓冲帧来定位目标。尽管在许多基准测试中已经取得了顶尖性能，但现代跟踪器在存在干扰物时仍面临挑战。为此，我们提出了一个更为复杂的记忆模型，用于SAM2的干扰物感知记忆模型，以及一个基于自省策略的更新策略，旨在同时解决分割精度和跟踪鲁棒性问题。新推出的跟踪器SAM2.1++在七个基准测试中表现优异，并在其中六个上创造了新的最佳记录。同时，我们还推出了干扰物蒸馏DiDi数据集，以更好地研究干扰物问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>内存基跟踪器通过拼接最近跟踪的帧来形成目标模型，并定位目标。</li>
<li>SAM2是近年来视频对象跟踪领域的一个突出方法。</li>
<li>现代跟踪器在存在干扰物时面临挑战。</li>
<li>提出了一个更为复杂的记忆模型，用于SAM2的干扰物感知记忆模型。</li>
<li>新推出的SAM2.1++跟踪器在多个基准测试中表现优异，并在其中大部分上创造了新的最佳记录。</li>
<li>同时推出了干扰物蒸馏DiDi数据集，以研究干扰物问题。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bd4db80d39f7c70a45983fd86497eed8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-91603742efec4b0dda214b22a16bb972.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3030961b168801bccf29622be2c80ef7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a2b99f61dd49e9fa41979dee97105fe4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43f465177f25aa7798513b2a3cedabad.jpg" align="middle">
</details>




<h2 id="Self-supervised-Video-Instance-Segmentation-Can-Boost-Geographic-Entity-Alignment-in-Historical-Maps"><a href="#Self-supervised-Video-Instance-Segmentation-Can-Boost-Geographic-Entity-Alignment-in-Historical-Maps" class="headerlink" title="Self-supervised Video Instance Segmentation Can Boost Geographic Entity   Alignment in Historical Maps"></a>Self-supervised Video Instance Segmentation Can Boost Geographic Entity   Alignment in Historical Maps</h2><p><strong>Authors:Xue Xia, Randall Balestriero, Tao Zhang, Lorenz Hurni</strong></p>
<p>Tracking geographic entities from historical maps, such as buildings, offers valuable insights into cultural heritage, urbanization patterns, environmental changes, and various historical research endeavors. However, linking these entities across diverse maps remains a persistent challenge for researchers. Traditionally, this has been addressed through a two-step process: detecting entities within individual maps and then associating them via a heuristic-based post-processing step. In this paper, we propose a novel approach that combines segmentation and association of geographic entities in historical maps using video instance segmentation (VIS). This method significantly streamlines geographic entity alignment and enhances automation. However, acquiring high-quality, video-format training data for VIS models is prohibitively expensive, especially for historical maps that often contain hundreds or thousands of geographic entities. To mitigate this challenge, we explore self-supervised learning (SSL) techniques to enhance VIS performance on historical maps. We evaluate the performance of VIS models under different pretraining configurations and introduce a novel method for generating synthetic videos from unlabeled historical map images for pretraining. Our proposed self-supervised VIS method substantially reduces the need for manual annotation. Experimental results demonstrate the superiority of the proposed self-supervised VIS approach, achieving a 24.9% improvement in AP and a 0.23 increase in F1 score compared to the model trained from scratch. </p>
<blockquote>
<p>从历史地图中追踪地理实体，如建筑等，为文化遗产、城市化模式、环境变化以及各种历史研究提供了宝贵的见解。然而，如何在不同的地图中连接这些实体一直是研究人员面临的一个持续挑战。传统上，这通过两步过程来解决：首先在单个地图内检测实体，然后通过基于启发式的后处理步骤将它们关联起来。在本文中，我们提出了一种结合历史地图中的地理实体分割和关联的新方法，使用视频实例分割（VIS）。该方法极大地简化了地理实体对齐过程，提高了自动化程度。然而，获取高质量的视频格式训练数据对于VIS模型来说非常昂贵，尤其是历史地图，往往包含数百或数千个地理实体。为了缓解这一挑战，我们探索了自监督学习（SSL）技术，以提高VIS在历史地图上的性能。我们评估了VIS模型在不同预训练配置下的性能，并引入了一种从无标签的历史地图图像生成合成视频的新方法，用于预训练。我们提出的自监督VIS方法大大减少了对手动注释的需求。实验结果表明，所提出的自监督VIS方法具有优越性，与从头开始训练的模型相比，AP提高了24.9%，F1分数增加了0.23。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17425v1">PDF</a> </p>
<p><strong>Summary</strong><br>本文介绍了如何利用视频实例分割技术实现对历史地图中的地理实体进行分割与追踪，以此提升实体对齐流程的自动化程度。为应对高质量视频格式训练数据难以获取的问题，文章采用自监督学习技术来提升模型性能，并提出一种从非标记历史地图图像生成合成视频的方法，以大幅减少人工标注的需求。实验证明，该自监督视频实例分割方法相较于基础模型表现更优，AP值提升24.9%，F1分数增加0.23。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>历史地图中的地理实体追踪对于研究文化遗产、城市化模式、环境变化和历史研究至关重要。</li>
<li>传统的实体链接方法需通过两步流程完成，而新方法结合视频实例分割技术简化了流程。</li>
<li>自监督学习技术用于提升视频实例分割模型在历史地图上的性能。</li>
<li>提出一种生成合成视频的方法，利用非标记历史地图图像进行预训练，减少人工标注需求。</li>
<li>实验结果显示，新方法的性能显著优于基础模型。</li>
<li>新方法能够在实现地理实体对齐自动化的同时，降低人力成本。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a7a83ef339bc649055b4bd1f01c49340.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e5d89ba3c043cd3c4bfd8049d075987d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-11656420d73b820a77145cac102c9849.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6b7b0cfdca462dcc434646f365792554.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-758b616cd5fed999a635a8a1f019019d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-012da1c6a9ed84ab263df2ab4b9ed8b5.jpg" align="middle">
</details>




<h2 id="MRIFE-A-Mask-Recovering-and-Interactive-Feature-Enhancing-Semantic-Segmentation-Network-For-Relic-Landslide-Detection"><a href="#MRIFE-A-Mask-Recovering-and-Interactive-Feature-Enhancing-Semantic-Segmentation-Network-For-Relic-Landslide-Detection" class="headerlink" title="MRIFE: A Mask-Recovering and Interactive-Feature-Enhancing Semantic   Segmentation Network For Relic Landslide Detection"></a>MRIFE: A Mask-Recovering and Interactive-Feature-Enhancing Semantic   Segmentation Network For Relic Landslide Detection</h2><p><strong>Authors:Juefei He, Yuexing Peng, Wei Li, Junchuan Yu, Daqing Ge, Wei Xiang</strong></p>
<p>Relic landslide, formed over a long period, possess the potential for reactivation, making them a hazardous geological phenomenon. While reliable relic landslide detection benefits the effective monitoring and prevention of landslide disaster, semantic segmentation using high-resolution remote sensing images for relic landslides faces many challenges, including the object visual blur problem, due to the changes of appearance caused by prolonged natural evolution and human activities, and the small-sized dataset problem, due to difficulty in recognizing and labelling the samples. To address these challenges, a semantic segmentation model, termed mask-recovering and interactive-feature-enhancing (MRIFE), is proposed for more efficient feature extraction and separation. Specifically, a contrastive learning and mask reconstruction method with locally significant feature enhancement is proposed to improve the ability to distinguish between the target and background and represent landslide semantic features. Meanwhile, a dual-branch interactive feature enhancement architecture is used to enrich the extracted features and address the issue of visual ambiguity. Self-distillation learning is introduced to leverage the feature diversity both within and between samples for contrastive learning, improving sample utilization, accelerating model convergence, and effectively addressing the problem of the small-sized dataset. The proposed MRIFE is evaluated on a real relic landslide dataset, and experimental results show that it greatly improves the performance of relic landslide detection. For the semantic segmentation task, compared to the baseline, the precision increases from 0.4226 to 0.5347, the mean intersection over union (IoU) increases from 0.6405 to 0.6680, the landslide IoU increases from 0.3381 to 0.3934, and the F1-score increases from 0.5054 to 0.5646. </p>
<blockquote>
<p>古滑坡经过长期形成，存在再次活动的可能，是一种危险的地质现象。可靠的古滑坡检测对有效监测和预防滑坡灾害具有积极意义。然而，使用高分辨率遥感图像对古滑坡进行语义分割面临诸多挑战，其中包括由于长期自然演变和人为活动导致的外观变化造成的目标视觉模糊问题，以及由于识别和标记样本困难而导致的小样本集问题。为了解决这些挑战，提出了一种名为掩膜恢复和交互特征增强（MRIFE）的语义分割模型，以更有效地提取和分离特征。具体地说，提出了一种对比学习和掩膜重建方法，该方法具有局部重要特征增强的能力，以提高目标和背景之间的区分能力，并表现滑坡语义特征。同时，使用双分支交互特征增强架构来丰富提取的特征并解决视觉模糊的问题。引入自蒸馏学习来利用样本内的特征多样性，提高样本利用率，加速模型收敛，有效解决小样本集的问题。所提出的MRIFE方法在真实的古滑坡数据集上进行了评估，实验结果表明，它大大提高了古滑坡检测的性能。对于语义分割任务，与基线相比，精确度从0.4226提高到0.5347，平均交并比（IoU）从0.6405提高到0.6680，滑坡IoU从0.3381提高到0.3934，F1分数从0.5054提高到0.5646。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用高分辨率遥感图像进行遗迹滑坡语义分割所面临的挑战，包括目标视觉模糊问题和数据集较小的问题。为此，提出了一种名为MRIFE的语义分割模型，该模型采用对比学习和掩膜重建方法，结合局部重要特征增强技术来解决目标辨识问题，并使用双分支交互特征增强架构来丰富提取的特征并解决视觉模糊问题。此外，引入了自蒸馏学习来提高样本利用率和模型收敛速度。实验结果表明，MRIFE模型在遗迹滑坡检测方面的性能得到了显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>遗迹滑坡具有重新活动的潜力，对其检测有助于有效监控和预防滑坡灾害。</li>
<li>遗迹滑坡语义分割面临目标视觉模糊和小规模数据集两大挑战。</li>
<li>MRIFE模型通过对比学习和掩膜重建方法提高目标辨识能力。</li>
<li>局部重要特征增强和双分支交互特征增强架构用于丰富特征和解决视觉模糊问题。</li>
<li>自蒸馏学习提高了样本利用率和模型收敛速度。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20cd246940870b0a748006b3dfa131e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d083ef86b0c6219ddf6f3c775affd251.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-691742291c9d78786a889d30aeb25fdd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-740b817e4d0c7ef202a32b9a8e1064cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-110eb60633812d529d373823a789fd37.jpg" align="middle">
</details>




<h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits models’ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>开放词汇语义分割（OVSS）随着最近的视觉语言模型（VLMs）的发展而进步，通过各种学习方案实现了超出预定类别的分割。值得注意的是，无训练方法为处理未见数据提供了可扩展、易于部署的解决方案，这是OVSS的关键目标。然而，仍存在一个关键问题：在基于任意查询提示的OVSS挑战环境中，对复杂对象进行分割时，缺乏对象级别的上下文考虑。这种疏忽限制了模型将语义上一致的元素分组到对象内部并准确地将它们映射到用户定义的任意类别中的能力。在这项工作中，我们引入了一种克服这一局限的新方法，该方法通过在图像内部结合对象级别的上下文知识。具体来说，我们的模型通过将从视觉基础模型蒸馏出的光谱驱动特征融入视觉编码器的注意力机制中，增强了对象内部的一致性，使得语义上连贯的组件能够形成单个对象掩码。此外，我们还通过零样本对象存在概率对文本嵌入进行细化，以确保与图像中表示的特定对象的准确对齐。通过利用对象级别的上下文知识，我们提出的方法在多个数据集上实现了最先进的性能，并具有较强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于开放词汇语义分割（OVSS）的最新研究进展，特别是在使用视觉语言模型（VLMs）的开放场景下的新策略。虽然存在挑战，如无预设类别的分割以及考虑对象级别上下文的重要性等，但通过融入对象级别的上下文知识并采用特定的技术手段如蒸馏频谱驱动特征到视觉编码器的注意力机制等，本研究提出了一种克服这些挑战的新方法，并实现了跨多个数据集的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>开放词汇语义分割（OVSS）已经随着最新的视觉语言模型（VLMs）的进步而发展。</li>
<li>训练自由的方法对于处理未见过的数据提供了可扩展和易于部署的解决方案。</li>
<li>当前存在的问题是缺乏对象级别的上下文考虑，这在基于任意查询提示的OVSS环境中进行复杂对象的分割时是一个关键限制。</li>
<li>本文介绍了一种新的方法，通过融入对象级别的上下文知识来克服这一限制。</li>
<li>通过将频谱驱动特征蒸馏到视觉编码器的注意力机制中，提高了对象内部的一致性。</li>
<li>通过精炼文本嵌入与零样本对象存在概率的对齐，确保了与图像中特定对象的准确对齐。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f7b6fbeb2826a7dad8df4b062d3486a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffccfcbd45ef2a94b19be0bce7c72be4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bbe7a6fdd528b4861885240dd1defd77.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-90fa9c3ab36b67f06e6c20c1253b7353.jpg" align="middle">
</details>




<h2 id="SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models"><a href="#SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models" class="headerlink" title="SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models"></a>SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models</h2><p><strong>Authors:Harsh Goel, Sai Shankar Narasimhan, Oguzhan Akcin, Sandeep Chinchali</strong></p>
<p>In recent years, significant progress has been made in collecting large-scale datasets to improve segmentation and autonomous driving models. These large-scale datasets are often dominated by common environmental conditions such as “Clear and Day” weather, leading to decreased performance in under-represented conditions like “Rainy and Night”. To address this issue, we introduce SynDiff-AD, a novel data augmentation pipeline that leverages diffusion models (DMs) to generate realistic images for such subgroups. SynDiff-AD uses ControlNet-a DM that guides data generation conditioned on semantic maps-along with a novel prompting scheme that generates subgroup-specific, semantically dense prompts. By augmenting datasets with SynDiff-AD, we improve the performance of segmentation models like Mask2Former and SegFormer by up to 1.2% and 2.3% on the Waymo dataset, and up to 1.4% and 0.7% on the DeepDrive dataset, respectively. Additionally, we demonstrate that our SynDiff-AD pipeline enhances the driving performance of end-to-end autonomous driving models, like AIM-2D and AIM-BEV, by up to 20% across diverse environmental conditions in the CARLA autonomous driving simulator, providing a more robust model. </p>
<blockquote>
<p>近年来，在收集大规模数据集以改进分割和自动驾驶模型方面取得了重大进展。这些大规模数据集通常以“晴朗和白天”等常见环境条件为主，导致在代表性不足的条件（如“雨天和夜间”）下性能下降。为了解决这个问题，我们引入了SynDiff-AD，这是一种新的数据增强流程，它利用扩散模型（DM）生成此类子组的逼真图像。SynDiff-AD使用ControlNet（一种基于语义地图引导数据生成的DM）以及一种新型提示方案，该方案可以生成针对子组的语义密集提示。通过SynDiff-AD增强数据集，我们提高了Mask2Former和SegFormer等分割模型在Waymo数据集上的性能，分别提高了1.2%和2.3%，以及在DeepDrive数据集上的性能，分别提高了1.4%和0.7%。此外，我们还证明了我们的SynDiff-AD流程可以提高端到端的自动驾驶模型（如AIM-2D和AIM-BEV）在CARLA自动驾驶模拟器中不同环境条件下的驾驶性能，最多可提高20%，为更稳健的模型提供了支持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16776v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>大型数据集在提升分割和自动驾驶模型方面取得了显著进展，但现有数据集往往偏向常见环境，如晴朗白天，对于雨天夜晚等较少代表的环境性能较差。为此，提出SynDiff-AD这一新型数据增强管道，利用扩散模型生成针对这些子群体的真实图像。通过ControlNet DM和新的提示方案，对Waymo和DeepDrive数据集上的分割模型性能提升达1.2%-2.3%。同时，在CARLA自动驾驶模拟器中的端到端自动驾驶模型性能提升达20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型数据集在提升分割和自动驾驶模型方面取得显著进展。</li>
<li>现有数据集偏向常见环境，如晴朗白天，对于其他环境性能较差。</li>
<li>SynDiff-AD是一种新型数据增强管道，用于生成针对子群体的真实图像。</li>
<li>利用ControlNet DM和新的提示方案进行数据增强。</li>
<li>对Waymo和DeepDrive数据集上的分割模型性能有显著提升。</li>
<li>在CARLA自动驾驶模拟器中，对端到端的自动驾驶模型性能提升显著。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9beebce3088f2766cf5c9cbb0026de11.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c6ad64077fb21b241a397267f7fc9529.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e84d37a2fd77b480f20f9f04e6564859.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-af7c7747251904e413a7cc77f60381ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f64383ea48c4966ab8d076cfd8a8262.jpg" align="middle">
</details>




<h2 id="CutS3D-Cutting-Semantics-in-3D-for-2D-Unsupervised-Instance-Segmentation"><a href="#CutS3D-Cutting-Semantics-in-3D-for-2D-Unsupervised-Instance-Segmentation" class="headerlink" title="CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance   Segmentation"></a>CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance   Segmentation</h2><p><strong>Authors:Leon Sick, Dominik Engel, Sebastian Hartwig, Pedro Hermosilla, Timo Ropinski</strong></p>
<p>Traditionally, algorithms that learn to segment object instances in 2D images have heavily relied on large amounts of human-annotated data. Only recently, novel approaches have emerged tackling this problem in an unsupervised fashion. Generally, these approaches first generate pseudo-masks and then train a class-agnostic detector. While such methods deliver the current state of the art, they often fail to correctly separate instances overlapping in 2D image space since only semantics are considered. To tackle this issue, we instead propose to cut the semantic masks in 3D to obtain the final 2D instances by utilizing a point cloud representation of the scene. Furthermore, we derive a Spatial Importance function, which we use to resharpen the semantics along the 3D borders of instances. Nevertheless, these pseudo-masks are still subject to mask ambiguity. To address this issue, we further propose to augment the training of a class-agnostic detector with three Spatial Confidence components aiming to isolate a clean learning signal. With these contributions, our approach outperforms competing methods across multiple standard benchmarks for unsupervised instance segmentation and object detection. </p>
<blockquote>
<p>传统上，学习在2D图像中分割对象实例的算法主要依赖于大量的人工注释数据。最近，才出现了一些新的方法以无监督的方式解决这个问题。一般来说，这些方法首先生成伪掩膜，然后训练一个类别无关的探测器。虽然这些方法达到了目前的技术水平，但由于它们只考虑语义，因此往往无法正确分离在2D图像空间中重叠的实例。为了解决这一问题，我们提出了一种方法，即在3D中对语义掩膜进行切割，利用场景的点云表示来获得最终的2D实例。此外，我们推导出了一个空间重要性函数，我们用它来锐化实例的3D边界处的语义。然而，这些伪掩膜仍然存在掩膜模糊的问题。为了解决这个问题，我们进一步提出对类别无关的探测器训练进行增强，加入三个空间置信度组件，旨在产生一个清晰的信号。通过以上的贡献，我们的方法在多个标准基准测试上实现了对无监督实例分割和对象检测的竞争方法的超越。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16319v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>传统上，二维图像中的对象实例分割算法严重依赖于大量人工标注数据。近期出现了新型的无监督方法来解决这一问题，通常首先生成伪掩膜并训练类无关的探测器。这些方法虽达到了当前最佳水平，但在处理二维图像空间中重叠的实例时常常无法正确分离，因为它们只考虑语义信息。为了解决这个问题，我们提出了一个基于三维场景点云表示的方法，通过将语义掩膜切割成最终的二维实例来实现改进。此外，我们还提出了一种空间重要性函数，用于在实例的三维边界上重塑语义信息。然而，这些伪掩膜仍然存在掩膜模糊的问题。为了解决这个问题，我们进一步提出了三种空间置信度的增强组件，以强化类无关的探测器的训练过程。凭借这些改进，我们的方法在多标准无监督实例分割和对象检测基准测试中均优于竞争对手的方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>传统实例分割算法依赖大量人工标注数据。</li>
<li>近期出现了无监督的实例分割方法，主要通过生成伪掩膜并训练类无关的探测器来实现。</li>
<li>当前最佳方法在处理重叠实例时存在缺陷，因为它们仅考虑语义信息。</li>
<li>提出了一种基于三维场景点云表示的方法来解决这一问题，通过切割语义掩膜生成二维实例。</li>
<li>利用空间重要性函数在实例的三维边界上重塑语义信息。</li>
<li>伪掩膜存在模糊问题，因此提出了三种空间置信度增强组件以改进类无关探测器的训练过程。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-54499c9a4c35a84eaad384562a274908.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-553622c5b379011fa82ea96f493d4f9f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6f6005b81f396ba65c4c1f93662c5e5b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7c8757a997c2c4f8c96266d71eada972.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f734059cfeaa8ab9fe9de6f1b82cbf81.jpg" align="middle">
</details>




<h2 id="A-Performance-Increment-Strategy-for-Semantic-Segmentation-of-Low-Resolution-Images-from-Damaged-Roads"><a href="#A-Performance-Increment-Strategy-for-Semantic-Segmentation-of-Low-Resolution-Images-from-Damaged-Roads" class="headerlink" title="A Performance Increment Strategy for Semantic Segmentation of   Low-Resolution Images from Damaged Roads"></a>A Performance Increment Strategy for Semantic Segmentation of   Low-Resolution Images from Damaged Roads</h2><p><strong>Authors:Rafael S. Toledo, Cristiano S. Oliveira, Vitor H. T. Oliveira, Eric A. Antonelo, Aldo von Wangenheim</strong></p>
<p>Autonomous driving needs good roads, but 85% of Brazilian roads have damages that deep learning models may not regard as most semantic segmentation datasets for autonomous driving are high-resolution images of well-maintained urban roads. A representative dataset for emerging countries consists of low-resolution images of poorly maintained roads and includes labels of damage classes; in this scenario, three challenges arise: objects with few pixels, objects with undefined shapes, and highly underrepresented classes. To tackle these challenges, this work proposes the Performance Increment Strategy for Semantic Segmentation (PISSS) as a methodology of 14 training experiments to boost performance. With PISSS, we reached state-of-the-art results of 79.8 and 68.8 mIoU on the Road Traversing Knowledge (RTK) and Technik Autonomer Systeme 500 (TAS500) test sets, respectively. Furthermore, we also offer an analysis of DeepLabV3+ pitfalls for small object segmentation. </p>
<blockquote>
<p>自动驾驶需要良好的道路，但巴西有85%的道路存在损伤，这可能超出深度学习模型的考虑范围，因为大多数自动驾驶的语义分割数据集都是高分辨率的良好维护的城市道路图像。对于新兴国家而言，一个典型的数据集包含低分辨率的维护不良的道路图像，并包括损伤类别的标签；在此场景中，会出现三个挑战：像素少的物体、形状不确定的物体以及高度欠代表的类别。为了应对这些挑战，这项工作提出了语义分割性能提升策略（PISSS）作为一种包含14个训练实验的方法来提高性能。通过PISSS，我们在Road Traversing Knowledge（RTK）和Technik Autonomer Systeme 500（TAS500）测试集上达到了最先进的79.8和68.8 mIoU结果。此外，我们还对DeepLabV3+在小目标分割中的陷阱进行了分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文指出，自主驾驶需要良好的道路环境，但巴西约有85%的道路存在损坏情况。现有的语义分割数据集主要为高分辨率的良好维护的城市道路图像，而新兴国家的情况不同，需要使用低分辨率的、维护不良的道路图像数据集，并包含损伤类别标签。这引发了三大挑战：像素少的物体、形状不确定的物体和高度欠代表的类别。为应对这些挑战，本文提出了语义分割性能提升策略（PISSS），通过14个训练实验提升性能。在路遍历知识（RTK）和技术自主系统500（TAS500）测试集上分别达到了最先进的79.8和68.8 mIoU结果。此外，本文还对DeepLabV3+在小目标分割中的缺陷进行了分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>巴西约85%的道路存在损坏情况，对自主驾驶构成挑战。</li>
<li>现有的语义分割数据集主要关注良好维护的城市道路图像，但新兴国家需要低分辨率的、维护不良的道路图像数据集。</li>
<li>应对新兴国家道路情况的三大挑战包括：像素少的物体、形状不确定的物体和高度欠代表的类别。</li>
<li>提出了一种新的语义分割策略PISSS，通过14个训练实验提升性能。</li>
<li>在RTK和TAS500测试集上达到了先进的mIoU结果。</li>
<li>PISSS策略可能对解决其他类似场景下的语义分割问题具有借鉴意义。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-efbba0e878aefa8b8bd633287c5ae0b5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-41116cee64da7331e02fc1a98ea5e558.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8028ee57fa6dcc885c54769960f71137.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-58876456223de5bb75ec48e7f2a9a96f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d392fc10978b009abbdf160912c20465.jpg" align="middle">
</details>




<h2 id="Instance-Aware-Generalized-Referring-Expression-Segmentation"><a href="#Instance-Aware-Generalized-Referring-Expression-Segmentation" class="headerlink" title="Instance-Aware Generalized Referring Expression Segmentation"></a>Instance-Aware Generalized Referring Expression Segmentation</h2><p><strong>Authors:E-Ro Nguyen, Hieu Le, Dimitris Samaras, Michael Ryoo</strong></p>
<p>Recent works on Generalized Referring Expression Segmentation (GRES) struggle with handling complex expressions referring to multiple distinct objects. This is because these methods typically employ an end-to-end foreground-background segmentation and lack a mechanism to explicitly differentiate and associate different object instances to the text query. To this end, we propose InstAlign, a method that incorporates object-level reasoning into the segmentation process. Our model leverages both text and image inputs to extract a set of object-level tokens that capture both the semantic information in the input prompt and the objects within the image. By modeling the text-object alignment via instance-level supervision, each token uniquely represents an object segment in the image, while also aligning with relevant semantic information from the text. Extensive experiments on the gRefCOCO and Ref-ZOM benchmarks demonstrate that our method significantly advances state-of-the-art performance, setting a new standard for precise and flexible GRES. </p>
<blockquote>
<p>关于广义引用表达式分割（GRES）的最新研究在处理引用多个不同对象的复杂表达式时遇到了困难。这是因为这些方法通常采用端到端的前景背景分割，并缺乏一种机制来明确区分和关联不同的对象实例与文本查询。为此，我们提出了InstAlign方法，它将对象级别的推理融入分割过程中。我们的模型利用文本和图像输入来提取一组对象级别的标记，这些标记既能捕获输入提示中的语义信息，又能反映图像中的对象。通过对文本对象进行实例级监督的对齐建模，每个标记唯一地代表图像中的一个对象段，同时与文本中的相关语义信息对齐。在gRefCOCO和Ref-ZOM基准测试上的大量实验表明，我们的方法显著提高了最新技术的性能，为精确灵活的GRES设定了新的标准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15087v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为InstAlign的方法，用于处理广义指代表达式分割（GRES）中的复杂问题。该方法结合了文本和图像输入，通过对象级别的推理机制，提取出能够捕捉输入提示语义信息和图像内对象的对象级令牌集。通过实例级别的监督建模文本与对象的对齐关系，每个令牌都能唯一代表图像中的一个对象段，同时与文本中的相关语义信息对齐。在gRefCOCO和Ref-ZOM基准测试上的实验表明，该方法显著提高了最新技术的性能，为精确灵活的GRES设定了新的标准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>广义指代表达式分割（GRES）在处理指代多个不同对象的复杂表达式时面临挑战。</li>
<li>现有方法通常采用端到前背景的分割方式，无法明确区分和关联不同的对象实例。</li>
<li>InstAlign方法结合文本和图像输入，提取对象级别的令牌集，捕捉输入提示的语义信息和图像中的对象。</li>
<li>通过实例级别的监督建模文本与对象的对齐，每个令牌能唯一代表图像中的对象段。</li>
<li>方法在gRefCOCO和Ref-ZOM基准测试上表现出显著性能提升。</li>
<li>InstAlign为精确灵活的GRES设定了新的标准。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7a8e9ba96c7fe8ba0e15f9a8470d8cb3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c1a1abe20722b5cb51215c7e09470ebc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b10866630c8fbebd7241e05c5af73638.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-49fcd1f6c365540a9ae2672c05b1c856.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-01b240c4e6a8647080a7e1c40667c0a2.jpg" align="middle">
</details>




<h2 id="Effective-SAM-Combination-for-Open-Vocabulary-Semantic-Segmentation"><a href="#Effective-SAM-Combination-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Effective SAM Combination for Open-Vocabulary Semantic Segmentation"></a>Effective SAM Combination for Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Minhyeok Lee, Suhwan Cho, Jungho Lee, Sunghun Yang, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee</strong></p>
<p>Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAM’s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions. </p>
<blockquote>
<p>开放词汇语义分割旨在给无限范围的类别中的图像分配像素级标签。传统方法通过顺序连接强大的掩膜提议生成器（如任何物体分割模型SAM）和预训练的视觉语言模型（如CLIP）来解决这个问题。但这些两阶段的方法通常面临高计算成本和内存效率低的问题。在本文中，我们提出了ESC-Net，这是一种新型的一阶段开放词汇分割模型，它利用SAM解码器块在有效的推理框架中进行类别无关的分割。通过将根据图像文本相关性生成的伪提示嵌入到SAM的可提示分割框架中，ESC-Net实现了精细的空间聚合，以获得准确的掩膜预测。ESC-Net在包括ADE20K、PASCAL-VOC和PASCAL-Context在内的标准基准测试中实现了卓越的性能，在效率和准确性方面均优于以前的方法。全面的消融研究进一步证明了其在各种具有挑战性的条件下的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14723v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文提出了一种新型的开放词汇语义分割模型ESC-Net，它结合了SAM解码器块进行类不可知的分割，在一个高效的推理框架中实现图像像素级的标签分配。ESC-Net通过嵌入伪提示（伪提示是由图像文本相关性生成的）到SAM的可提示分割框架中，实现了精细的空间聚合，从而获得准确的掩膜预测。在ADE20K、PASCAL-VOC和PASCAL-Context等标准数据集上，ESC-Net表现出卓越的性能，在效率和准确性方面都超过了之前的方法。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>开放词汇语义分割的目标是为图像中的每个像素分配标签，涵盖无限类别的范围。</li>
<li>传统方法通常使用两阶段的方法，结合强大的掩膜提案生成器（如SAM）和预训练的视觉语言模型（如CLIP）。</li>
<li>ESC-Net是一个新型的一阶段开放词汇分割模型，利用SAM解码器块进行类不可知的分割。</li>
<li>ESC-Net通过将伪提示嵌入SAM的提示分割框架中，实现精细的空间聚合和准确的掩膜预测。</li>
<li>ESC-Net在多个标准数据集上实现了卓越的性能，包括ADE20K、PASCAL-VOC和PASCAL-Context。</li>
<li>ESC-Net在效率和准确性方面都超过了之前的方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0290ce7a2cb759fddbe791631cdbbf2f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-31cd9e3384d6a84abdf267d8636798eb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6d18dbab2b8ee6417990e3d46117d89.jpg" align="middle">
</details>




<h2 id="DINO-X-A-Unified-Vision-Model-for-Open-World-Object-Detection-and-Understanding"><a href="#DINO-X-A-Unified-Vision-Model-for-Open-World-Object-Detection-and-Understanding" class="headerlink" title="DINO-X: A Unified Vision Model for Open-World Object Detection and   Understanding"></a>DINO-X: A Unified Vision Model for Open-World Object Detection and   Understanding</h2><p><strong>Authors:Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, Lei Zhang</strong></p>
<p>In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model’s core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model’s open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects. </p>
<blockquote>
<p>本文介绍了IDEA Research开发的统一对象级视觉模型DINO-X，它是目前为止具有最佳开放世界目标检测性能的模型。DINO-X采用与Grounding DINO 1.5相同的基于Transformer的编码器-解码器架构，追求对象级别的表示，以实现开放世界对象的理解。为了简化长尾目标检测，DINO-X扩展了其输入选项，支持文本提示、视觉提示和自定义提示。通过这样灵活的提示选项，我们开发了一种通用对象提示，支持无提示的开放世界检测，使得在不需要用户提供任何提示的情况下，可以在图像中检测任何东西。为了提高模型的核心定位能力，我们构建了一个大规模数据集，称为Grounding-100M，包含超过1亿个高质量定位样本，以提升模型的开放词汇检测性能。在如此大规模的定位数据集上进行预训练，得到了基础的对象级别表示，使DINO-X能够集成多个感知头，同时支持多个对象感知和理解任务，包括检测、分割、姿态估计、对象描述、基于对象的问答等。实验结果表明DINO-X性能卓越。具体来说，DINO-X Pro模型在COCO、LVIS-minival和LVIS-val零样本目标检测基准测试上的成绩分别为56.0 AP、59.8 AP和52.4 AP。值得注意的是，它在LVIS-minival和LVIS-val基准测试的罕见类别上的得分分别为63.3 AP和56.5 AP，比之前的最佳性能提高了5.8 AP和5.0 AP。这一结果突显了其在识别长尾对象方面的显著改进能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14347v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文介绍了由IDEA Research研发的对象级视觉模型DINO-X，该模型具备目前最佳的开放世界目标检测性能。DINO-X采用与Grounding DINO 1.5相同的Transformer-based编码器-解码器架构，追求对象级表示以实现对开放世界目标的感知理解。它支持文本提示、视觉提示和自定义提示等灵活的提示选项，可支持无提示的开放世界检测任务，无需用户提供任何提示即可检测图像中的任何对象。此外，该模型构建了一个大规模的高品质定位数据集Grounding-100M，以提高模型的开放词汇检测性能。经过在大规模定位数据集上的预训练后，DINO-X奠定了对象级表示的基础，可以集成多个感知头来同时支持多个对象感知和理解任务。实验结果表明DINO-X具有卓越的性能。特别是DINO-X Pro模型在COCO、LVIS-minival和LVIS-val零样本目标检测基准测试上分别取得了56.0 AP、59.8 AP和52.4 AP的成绩。在LVIS-minival和LVIS-val的稀有类别上分别取得了63.3 AP和56.5 AP的成绩，较之前的最优性能分别提高了5.8 AP和5.0 AP，突显了其在识别长尾对象方面的显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DINO-X是IDEA Research开发的对象级视觉模型，具有最佳的开放世界目标检测性能。</li>
<li>DINO-X采用与Grounding DINO 1.5相同的Transformer架构，追求对象级表示。</li>
<li>支持文本、视觉和自定义提示，可实现无提示的开放世界检测任务。</li>
<li>构建了一个大规模的高品质定位数据集Grounding-100M，提高模型开放词汇检测性能。</li>
<li>DINO-X通过预训练奠定了对象级表示基础，支持多种对象感知和理解任务。</li>
<li>DINO-X Pro模型在多个基准测试中表现出卓越性能，尤其在识别长尾对象方面有较大改进。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5bc885768aabd0215ecc31021b3bd190.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1a71407f0bbe0e18a7be64e2b6be4f72.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-be6397aad0e25f4645cae060e4e0cdc6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-98b13dfa6922d37353ac97289264d998.jpg" align="middle">
</details>




<h2 id="DIS-Mine-Instance-Segmentation-for-Disaster-Awareness-in-Poor-Light-Condition-in-Underground-Mines"><a href="#DIS-Mine-Instance-Segmentation-for-Disaster-Awareness-in-Poor-Light-Condition-in-Underground-Mines" class="headerlink" title="DIS-Mine: Instance Segmentation for Disaster-Awareness in Poor-Light   Condition in Underground Mines"></a>DIS-Mine: Instance Segmentation for Disaster-Awareness in Poor-Light   Condition in Underground Mines</h2><p><strong>Authors:Mizanur Rahman Jewel, Mohamed Elmahallawy, Sanjay Madria, Samuel Frimpong</strong></p>
<p>Detecting disasters in underground mining, such as explosions and structural damage, has been a persistent challenge over the years. This problem is compounded for first responders, who often have no clear information about the extent or nature of the damage within the mine. The poor-light or even total darkness inside the mines makes rescue efforts incredibly difficult, leading to a tragic loss of life. In this paper, we propose a novel instance segmentation method called DIS-Mine, specifically designed to identify disaster-affected areas within underground mines under low-light or poor visibility conditions, aiding first responders in rescue efforts. DIS-Mine is capable of detecting objects in images, even in complete darkness, by addressing challenges such as high noise, color distortions, and reduced contrast. The key innovations of DIS-Mine are built upon four core components: i) Image brightness improvement, ii) Instance segmentation with SAM integration, iii) Mask R-CNN-based segmentation, and iv) Mask alignment with feature matching. On top of that, we have collected real-world images from an experimental underground mine, introducing a new dataset named ImageMine, specifically gathered in low-visibility conditions. This dataset serves to validate the performance of DIS-Mine in realistic, challenging environments. Our comprehensive experiments on the ImageMine dataset, as well as on various other datasets demonstrate that DIS-Mine achieves a superior F1 score of 86.0% and mIoU of 72.0%, outperforming state-of-the-art instance segmentation methods, with at least 15x improvement and up to 80% higher precision in object detection. </p>
<blockquote>
<p>检测地下采矿中的灾害，如爆炸和结构破坏，一直是多年来的一个持续挑战。对于经常没有关于矿内损害程度或性质的明确信息的一线救援人员来说，这个问题更为严重。矿内光线昏暗甚至完全黑暗，使得救援工作极为困难，并导致生命悲剧。在本文中，我们提出了一种新型的实例分割方法，称为DIS-Mine，专门设计用于在地下矿场光线不足或能见度差的情况下识别受灾区域，以协助一线救援人员进行救援工作。DIS-Mine能够在图像中检测物体，甚至在完全黑暗的环境中也能应对高噪声、色彩失真和对比度降低等挑战。DIS-Mine的关键创新点基于四个核心组件：i）改善图像亮度，ii）结合SAM的实例分割，iii）基于Mask R-CNN的分割，以及iv）特征匹配的掩膜对齐。除此之外，我们从实验性地下矿场收集了真实世界的图像，并引入了一个名为ImageMine的新数据集，该数据集是在低能见度条件下专门收集的。该数据集用于验证DIS-Mine在真实且具有挑战性的环境中的性能。我们在ImageMine数据集以及其他多个数据集上的综合实验表明，DIS-Mine的F1分数达到了86.0%，mIoU达到了72.0%，优于最先进的实例分割方法，在物体检测方面至少有15倍的改进，最高精度提高了80%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13544v1">PDF</a> </p>
<p><strong>Summary</strong>：针对地下矿山灾难检测问题，如爆炸和结构性破坏，在低光照或无光照环境下识别受灾区域是一大挑战。本文提出了一种名为DIS-Mine的实例分割方法，该方法通过改善图像亮度、集成SAM、基于Mask R-CNN的分割和特征匹配等技术，有效检测地下矿山中的灾害区域。此外，还收集了一个名为ImageMine的新数据集，用于验证DIS-Mine在真实挑战环境中的性能。实验表明，DIS-Mine的F1分数达到86.0%，mIoU为72.0%，优于其他先进的实例分割方法，在目标检测方面至少提高了15倍，精度提高了高达80%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>地下矿山灾难检测面临低光照或无光照环境的挑战。</li>
<li>DIS-Mine是一种针对地下矿山灾难检测的实例分割方法，能在恶劣环境下识别受灾区域。</li>
<li>DIS-Mine的关键创新包括图像亮度改善、SAM集成、基于Mask R-CNN的分割和特征匹配等技术。</li>
<li>ImageMine数据集用于验证DIS-Mine在真实环境下的性能。</li>
<li>实验表明，DIS-Mine在F1分数和mIoU方面表现优异，优于其他实例分割方法。</li>
<li>DIS-Mine在目标检测方面有明显的性能提升，至少提高了15倍，精度提高了高达80%。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6764bc1684f78bd770ec03fcc25a1548.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc55044b29d960619784ae0ab190c769.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6cd2b3a5a236fd43ce93b5df92bc6746.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-09a2e22a81211b632cddf77063e7e18f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f7d0d4e398c497f473489f7aa1f352f1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-10114466f3156d6df6ae09d46de3e48e.jpg" align="middle">
</details>




<h2 id="Zero-Shot-Automatic-Annotation-and-Instance-Segmentation-using-LLM-Generated-Datasets-Eliminating-Field-Imaging-and-Manual-Annotation-for-Deep-Learning-Model-Development"><a href="#Zero-Shot-Automatic-Annotation-and-Instance-Segmentation-using-LLM-Generated-Datasets-Eliminating-Field-Imaging-and-Manual-Annotation-for-Deep-Learning-Model-Development" class="headerlink" title="Zero-Shot Automatic Annotation and Instance Segmentation using   LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for   Deep Learning Model Development"></a>Zero-Shot Automatic Annotation and Instance Segmentation using   LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for   Deep Learning Model Development</h2><p><strong>Authors:Ranjan Sapkota, Achyut Paudel, Manoj Karkee</strong></p>
<p>Currently, deep learning-based instance segmentation for various applications (e.g., Agriculture) is predominantly performed using a labor-intensive process involving extensive field data collection using sophisticated sensors, followed by careful manual annotation of images, presenting significant logistical and financial challenges to researchers and organizations. The process also slows down the model development and training process. In this study, we presented a novel method for deep learning-based instance segmentation of apples in commercial orchards that eliminates the need for labor-intensive field data collection and manual annotation. Utilizing a Large Language Model (LLM), we synthetically generated orchard images and automatically annotated them using the Segment Anything Model (SAM) integrated with a YOLO11 base model. This method significantly reduces reliance on physical sensors and manual data processing, presenting a major advancement in “Agricultural AI”. The synthetic, auto-annotated dataset was used to train the YOLO11 model for Apple instance segmentation, which was then validated on real orchard images. The results showed that the automatically generated annotations achieved a Dice Coefficient of 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask annotations. All YOLO11 configurations, trained solely on these synthetic datasets with automated annotations, accurately recognized and delineated apples, highlighting the method’s efficacy. Specifically, the YOLO11m-seg configuration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on test images collected from a commercial orchard. Additionally, the YOLO11l-seg configuration outperformed other models in validation on 40 LLM-generated images, achieving the highest mask precision and mAP@50 metrics.   Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM </p>
<blockquote>
<p>当前，基于深度学习的实例分割在各种应用（例如农业）中主要使用劳动密集型过程，涉及使用复杂传感器进行广泛现场数据采集，随后对图像进行仔细的手动标注，这给研究者和组织带来了重大的后勤和财务挑战，也减缓了模型和训练过程的发展。在这项研究中，我们提出了一种基于深度学习的商业果园苹果实例分割的新方法，该方法消除了对劳动密集型现场数据采集和手动注释的需求。我们利用大型语言模型（LLM）合成生成果园图像，并使用与YOLOv11基础模型集成的Segment Anything Model（SAM）进行自动标注。这种方法显著减少了对物理传感器和手动数据处理的依赖，是“农业人工智能”中的一项重大进展。合成的自动标注数据集用于训练YOLOv11苹果实例分割模型，然后在真实的果园图像上进行验证。结果表明，自动生成的注释实现了0.9513的Dice系数和0.9303的IoU，验证了掩膜注释的准确性和重叠度。所有仅在这些合成数据集上进行训练的YOLOv11配置，都能准确识别和划分苹果，这突出了该方法的有效性。特别是YOLOv11m-seg配置在来自商业果园的测试图像上达到了0.902的掩膜精度和0.833的mAP@50指标。此外，YOLOv11l-seg配置在40张LLM生成的图像验证中表现优于其他模型，达到了最高的掩膜精度和mAP@50指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11285v1">PDF</a> </p>
<p><strong>Summary</strong><br>该研究提出了一种基于深度学习和大型语言模型（LLM）的苹果实例分割新方法，适用于商业果园。该方法通过合成图像生成和自动标注技术，减少了对传统传感器和手动数据处理的依赖，为农业AI领域带来了重大进展。训练后的YOLO11模型在真实果园图像上的表现良好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一种新的深度学习方法进行苹果实例分割，应用于商业果园。</li>
<li>利用大型语言模型（LLM）合成果园图像，并自动进行标注。</li>
<li>方法减少了对传统传感器和手动数据处理的依赖。</li>
<li>自动标注的合成数据集用于训练YOLO11模型，在真实果园图像上验证，表现出良好的性能。</li>
<li>YOLO11模型的改进配置（YOLO11m-seg和YOLO11l-seg）在测试图像上实现了高精确度和高mAP@50指标。</li>
<li>该方法显著提高了苹果实例分割的效率和准确性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-11ab0cab69ed14f80746314532e3223f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1f5db900e8a911f086756e6cc7dab803.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4c31d0c8c265caa2a7bc605af0f973d6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-63b83791d5ab043c9c80f866aff71cba.jpg" align="middle">
</details>




<h2 id="Horticultural-Temporal-Fruit-Monitoring-via-3D-Instance-Segmentation-and-Re-Identification-using-Point-Clouds"><a href="#Horticultural-Temporal-Fruit-Monitoring-via-3D-Instance-Segmentation-and-Re-Identification-using-Point-Clouds" class="headerlink" title="Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and   Re-Identification using Point Clouds"></a>Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and   Re-Identification using Point Clouds</h2><p><strong>Authors:Daniel Fusaro, Federico Magistri, Jens Behley, Alberto Pretto, Cyrill Stachniss</strong></p>
<p>Robotic fruit monitoring is a key step toward automated agricultural production systems. Robots can significantly enhance plant and temporal fruit monitoring by providing precise, high-throughput assessments that overcome the limitations of traditional manual methods. Fruit monitoring is a challenging task due to the significant variation in size, shape, orientation, and occlusion of fruits. Also, fruits may be harvested or newly grown between recording sessions. Most methods are 2D image-based and they lack the 3D structure, depth, and spatial information, which represent key aspects of fruit monitoring. 3D colored point clouds, instead, can offer this information but they introduce challenges such as their sparsity and irregularity. In this paper, we present a novel approach for temporal fruit monitoring that addresses point clouds collected in a greenhouse over time. Our method segments fruits using a learning-based instance segmentation approach directly on the point cloud. Each segmented fruit is processed by a 3D sparse convolutional neural network to extract descriptors, which are used in an attention-based matching network to associate fruits with their instances from previous data collections. Experimental results on a real dataset of strawberries demonstrate that our approach outperforms other methods for fruits re-identification over time, allowing for precise temporal fruit monitoring in real and complex scenarios. </p>
<blockquote>
<p>机器人水果监测是朝着自动化农业生产系统发展的一个重要步骤。机器人可以通过提供精确、高通量的评估，克服传统手动方法的局限性，从而显著增强植物和临时水果监测。水果监测是一项具有挑战性的任务，因为水果的大小、形状、方向和遮挡存在很大差异。此外，在录制期间可能会有水果被采摘或新长出来。大多数方法都是基于2D图像的，它们缺乏3D结构、深度和空间信息，这些都是水果监测的关键方面。相反，3D彩色点云可以提供这些信息，但它们也带来了稀疏性和不规则性的挑战。在本文中，我们提出了一种用于临时水果监测的新方法，该方法解决了在温室中随时间收集的点云问题。我们的方法直接在点云上使用基于学习的实例分割方法进行水果分割。每个分割出来的水果都会通过3D稀疏卷积神经网络进行处理，以提取描述符，这些描述符将用于基于注意力的匹配网络，将水果与以前的数据采集中的实例进行关联。在草莓的实际数据集上的实验结果表明，我们的方法在长时间水果再识别方面优于其他方法，允许在真实和复杂场景中进行精确的临时水果监测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07799v1">PDF</a> Submitted to IEEE Robotics and Automation Letters</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于点云的时空水果监测新方法，该方法直接在点云上进行学习型的实例分割，并通过3D稀疏卷积神经网络处理每个分割的水果，以提取描述符号。这些描述符号用于基于注意力的匹配网络，与以前的数据收集中的水果实例进行关联。在草莓的实际数据集上的实验结果表明，该方法在复杂的现实场景中，对水果进行再识别和精确的时间监测方面具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人水果监测是自动化农业生产系统的关键步骤，可克服传统手动方法的局限性。</li>
<li>水果监测具有挑战性，因为水果的大小、形状、方向和遮挡有很大的变化，且可能在记录期间被采摘或新长出来。</li>
<li>大多数方法基于2D图像，缺乏关键的3D结构和空间信息。</li>
<li>点云技术能够提供必要的3D结构和空间信息，但存在稀疏性和不规则性问题。</li>
<li>本文提出了一种基于点云的时空水果监测新方法，直接在点云上进行实例分割。</li>
<li>使用3D稀疏卷积神经网络处理分割的水果，并通过注意力机制进行匹配网络关联。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf335d218ffa93fd9752e49c9079c6e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7f625b74aa88af265230f64463dbf44c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-62ddf7fac5d4d62a95922014e91b3f1f.jpg" align="middle">
</details>




<h2 id="GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection"></a>GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection</h2><p><strong>Authors:Jiyul Ham, Yonggon Jung, Jun-Geol Baek</strong></p>
<p>Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting anomalous patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP">https://github.com/YUL-git/GlocalCLIP</a>. </p>
<blockquote>
<p>零样本异常检测（ZSAD）在目标数据集中检测异常模式时，无需使用训练样本尤为重要。特别是在目标域与训练数据之间存在分布差异或由于访问受限而导致数据稀缺的场景中。尽管最近预训练的视觉语言模型在各种视觉任务中表现出强大的零样本性能，但它们主要关注类别语义的学习，这使得它们直接应用于ZSAD具有挑战性。针对这一场景，我们提出了GlocalCLIP方法，该方法独特地分离全局和局部提示并进行联合优化。这一方法使得对象无关的局部语义提示可以有效地捕捉一般正常和异常模式，而不依赖于图像中的特定对象。我们通过利用文本编码器的深度文本提示调整来完善文本提示，进行更精确的调整。在视觉编码器方面，我们应用V-V注意力层来捕捉详细的局部图像特征。最后，我们引入了局部对比学习，以提高全局和局部提示的互补学习，有效检测各种领域的异常模式。GlocalCLIP在ZSAD中的泛化性能在来自工业和医疗领域的15个真实世界数据集上得到了验证，与现有方法相比，实现了卓越的性能。代码将发布在<a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP%E3%80%82">https://github.com/YUL-git/GlocalCLIP。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06071v3">PDF</a> 29 pages, 36 figures</p>
<p><strong>Summary</strong><br>零样本异常检测（ZSAD）对于在目标数据集中检测异常模式至关重要，尤其适用于目标域与训练数据存在分布差异或由于访问限制导致数据稀缺的场景。针对预训练的视觉语言模型在零样本场景下的挑战，提出了GlocalCLIP方法。该方法通过分离全局和局部提示并进行联合优化，能够捕捉图像中的一般正常和异常模式。通过深文本提示调整和V-V注意力层，提高了精度和局部图像特征的捕捉。引入全局对比学习，有效检测不同领域的异常模式。在15个真实世界数据集上的实验表明，GlocalCLIP在ZSAD中的性能优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ZSAD对于在没有训练样本的情况下检测目标数据集中的异常模式至关重要。</li>
<li>预训练的视觉语言模型在零样本场景下面临挑战，难以直接应用于ZSAD。</li>
<li>GlocalCLIP通过分离全局和局部提示并进行联合优化，解决这一问题。</li>
<li>GlocalCLIP能够捕捉图像中的一般正常和异常模式，而不依赖于特定对象。</li>
<li>通过深文本提示调整和V-V注意力层，提高了GlocalCLIP的精度和局部图像特征捕捉能力。</li>
<li>引入全局对比学习，提高异常模式的检测能力。</li>
<li>GlocalCLIP在多个真实世界数据集上的性能优于现有方法。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26e62baf3d484faf017e96a0933d2b89.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1125ab6e3ab5117d2b7066fa7f664de1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2f8c17425f03137645e1037af7d759b4.jpg" align="middle">
</details>




<h2 id="CRT-Fusion-Camera-Radar-Temporal-Fusion-Using-Motion-Information-for-3D-Object-Detection"><a href="#CRT-Fusion-Camera-Radar-Temporal-Fusion-Using-Motion-Information-for-3D-Object-Detection" class="headerlink" title="CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for   3D Object Detection"></a>CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for   3D Object Detection</h2><p><strong>Authors:Jisong Kim, Minjae Seong, Jun Won Choi</strong></p>
<p>Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the bird’s-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and bird’s-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection. </p>
<blockquote>
<p>准确且稳定的3D物体检测是自动驾驶汽车和机器人技术中的关键组成部分。虽然最近的雷达-相机融合方法通过融合鸟瞰图（BEV）表示中的信息取得了显著进展，但它们往往难以有效地捕捉动态物体的运动，导致在真实场景中的性能受限。针对这一挑战，我们在本文中介绍了CRT-Fusion，这是一个将时间信息融入雷达-相机融合的新型框架。我们的方法包括三个关键模块：多视图融合（MVF）、运动特征估计器（MFE）和运动引导时间融合（MGTF）。MVF模块融合雷达和图像特征，在相机视图和鸟瞰图中生成更精确的统一BEV表示。MFE模块同时执行两个任务：逐像素速度信息估计和BEV分割。基于MFE模块获得的速度和占用分数图，MGTF模块以递归方式对齐并融合多个时间戳的特征图。通过考虑动态物体的运动，CRT-Fusion可以生成稳定的BEV特征图，从而提高检测准确性和稳健性。在具有挑战性的nuScenes数据集上的广泛评估表明，CRT-Fusion在基于雷达-相机的3D物体检测方面达到了最新技术水平。我们的方法在NDS方面较之前最佳方法提高了+1.7%，同时在mAP方面也超过了领先方法+1.4%。这两个指标的大幅提升展示了我们的融合策略在提高3D物体检测的可靠性和准确性方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03013v3">PDF</a> Accepted at NeurIPS2024</p>
<p><strong>Summary</strong></p>
<p>雷达与相机融合技术对于实现自主车辆和机器人的三维物体检测至关重要。近期雷达-相机融合方法已在鸟瞰图信息融合方面取得显著进展，但在捕捉动态物体运动方面仍存在挑战。本文提出CRT-Fusion框架，通过整合时间信息来解决这一问题。该框架包括三个关键模块：多视角融合、运动特征估计器和运动引导时间融合。实验表明，CRT-Fusion在nuScenes数据集上实现了基于雷达-相机的三维物体检测的卓越性能，显著优于其他方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>雷达与相机融合技术对于自主车辆和机器人的三维物体检测至关重要。</li>
<li>近期雷达-相机融合方法在鸟瞰图信息融合方面取得进展，但捕捉动态物体运动方面存在挑战。</li>
<li>CRT-Fusion框架通过整合时间信息来解决这一挑战，包括多视角融合、运动特征估计器和运动引导时间融合三个关键模块。</li>
<li>CRT-Fusion在nuScenes数据集上实现了卓越性能，显著优于其他方法。</li>
<li>CRT-Fusion能生成更精确的统一鸟瞰图表示，通过考虑动态物体的运动，提高检测准确性和稳健性。</li>
<li>在NDS和mAP两个指标上，CRT-Fusion均实现了对之前最佳方法的显著超越。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-87f4cdb6a0088c13dd0cc5e7c10e70a8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6b3ad89deada39b999855110c997390d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7609be08a13017629b14fe29e506d7a1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6b029a5122f8eb9b58425a375ed0e63.jpg" align="middle">
</details>




<h2 id="On-the-Black-box-Explainability-of-Object-Detection-Models-for-Safe-and-Trustworthy-Industrial-Applications"><a href="#On-the-Black-box-Explainability-of-Object-Detection-Models-for-Safe-and-Trustworthy-Industrial-Applications" class="headerlink" title="On the Black-box Explainability of Object Detection Models for Safe and   Trustworthy Industrial Applications"></a>On the Black-box Explainability of Object Detection Models for Safe and   Trustworthy Industrial Applications</h2><p><strong>Authors:Alain Andres, Aitor Martinez-Seras, Ibai Laña, Javier Del Ser</strong></p>
<p>In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic explainability methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP) technique based on segmentation-based masks to generate explanations. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in a scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used. </p>
<blockquote>
<p>在人机交互领域，人工智能已成为加速数据建模任务的强大工具。物体检测方法已取得了显著成果，并广泛应用于自动驾驶、视频监控等重要领域。然而，其在高风险应用中的采用，即那些可能导致严重后果的应用中，仍然受到限制。可解释人工智能方法旨在解决这一问题，但许多现有技术都是针对特定模型的分类任务，使得它们在物体检测方面的效果较差，非专业人士难以解释。在这项工作中，我们专注于物体检测模型的模型无关可解释性方法，并提出了D-MFPP，这是基于分割掩膜技术的Morphological Fragmental Perturbation Pyramid（MFPP）技术的扩展，用于生成解释。此外，我们引入了D-Deletion，这是一个结合了忠实性和定位的新指标，专门为物体检测器的独特需求而设计。我们在现实世界中的工业数据集和机器人数据集上评估了这些方法，研究了掩膜数量、模型大小和图像分辨率等参数对解释质量的影响。我们的实验使用了单阶段物体检测模型，应用于两个安全关键的机器人环境：一是人机共享的工作空间，安全至关重要；二是电池组件的组装区域，由于高风险组件的存在，安全至关重要。我们的研究结果表明，D-Deletion在场景中同一类的多个元素出现时，能有效衡量解释的性能；而D-MFPP在掩膜数量较少时提供了一个有前途的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00818v2">PDF</a> 14 pages, 10 figures, 6 tables</p>
<p><strong>摘要</strong><br>人工智能已应用于人机互动领域的数据建模任务中并展现强大的推动力。目标检测技术在自动驾驶和视频监控等重要领域应用广泛，效果突出。然而，在存在高风险的领域中应用目标检测时，由于错误可能导致严重后果，其应用仍然受限。为解决此问题，人们提出了可解释的人工智能方法，但许多现有技术具有模型特定性，主要为分类任务设计，对于目标检测效果较差且难以被非专业人士解读。本研究关注模型无关的可解释性方法，针对目标检测模型提出D-MFPP方法，基于分割掩膜扩展形态学片段扰动金字塔技术生成解释。此外，我们提出D-Deletion指标，结合忠诚度和定位度，专为目标检测器需求定制。我们在现实世界的工业及机器人数据集上评估这些方法，研究掩膜数量、模型大小和图像分辨率等参数对解释质量的影响。实验将单阶段目标检测模型应用于两个安全关键的机器人环境：一是人机共享工作空间，安全至关重要；二是电池组件装配区，因高风险部件的存在安全至关重要。实验结果表明，D-Deletion能有效衡量场景中存在同类别多个元素时的解释性能表现，而D-MFPP在掩膜数量较少时提供了有前景的替代方案。</p>
<p><strong>要点解析</strong></p>
<ol>
<li>人工智能在人机互动领域的数据建模任务中发挥了重要作用。</li>
<li>目标检测技术广泛应用于自动驾驶和视频监控等领域且效果显著。</li>
<li>在高风险领域应用目标检测时存在挑战，错误可能导致严重后果。</li>
<li>当前的可解释人工智能方法多具有模型特定性，对于目标检测任务的解释效果不佳且难以被非专业人士理解。</li>
<li>本研究提出了针对目标检测模型的模型无关解释方法D-MFPP和新的评估指标D-Deletion。</li>
<li>D-MFPP基于分割掩膜扩展形态学片段扰动金字塔技术生成解释。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ab8a4a024def8d48f315e182c66098e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1193c8fbfe78ee919beac27e5ad03b95.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f9946c0b55f3fa4b4fd4cf309b113ede.jpg" align="middle">
</details>




<h2 id="SiamSeg-Self-Training-with-Contrastive-Learning-for-Unsupervised-Domain-Adaptation-Semantic-Segmentation-in-Remote-Sensing"><a href="#SiamSeg-Self-Training-with-Contrastive-Learning-for-Unsupervised-Domain-Adaptation-Semantic-Segmentation-in-Remote-Sensing" class="headerlink" title="SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain   Adaptation Semantic Segmentation in Remote Sensing"></a>SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain   Adaptation Semantic Segmentation in Remote Sensing</h2><p><strong>Authors:Bin Wang, Fei Deng, Shuang Wang, Wen Luo, Zhixuan Zhang, Peifan Jiang</strong></p>
<p>Semantic segmentation of remote sensing (RS) images is a challenging yet essential task with broad applications. While deep learning, particularly supervised learning with large-scale labeled datasets, has significantly advanced this field, the acquisition of high-quality labeled data remains costly and time-intensive. Unsupervised domain adaptation (UDA) provides a promising alternative by enabling models to learn from unlabeled target domain data while leveraging labeled source domain data. Recent self-training (ST) approaches employing pseudo-label generation have shown potential in mitigating domain discrepancies. However, the application of ST to RS image segmentation remains underexplored. Factors such as variations in ground sampling distance, imaging equipment, and geographic diversity exacerbate domain shifts, limiting model performance across domains. In that case, existing ST methods, due to significant domain shifts in cross-domain RS images, often underperform. To address these challenges, we propose integrating contrastive learning into UDA, enhancing the model’s ability to capture semantic information in the target domain by maximizing the similarity between augmented views of the same image. This additional supervision improves the model’s representational capacity and segmentation performance in the target domain. Extensive experiments conducted on RS datasets, including Potsdam, Vaihingen, and LoveDA, demonstrate that our method, SimSeg, outperforms existing approaches, achieving state-of-the-art results. Visualization and quantitative analyses further validate SimSeg’s superior ability to learn from the target domain. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg">https://github.com/woldier/SiamSeg</a>. </p>
<blockquote>
<p>遥感（RS）图像的语义分割是一项具有广泛应用挑战性的但必不可少的任务。深度学习，特别是使用大规模标记数据集进行监督学习，已经大大推动了这一领域的发展，但是获取高质量标记数据仍然成本高昂且耗时。无监督域自适应（UDA）提供了一种有前途的替代方案，使模型能够从无标签的目标域数据中学习，同时利用有标签的源域数据。最近采用伪标签生成的自训练（ST）方法显示出减少域差异潜力。然而，将ST应用于RS图像分割仍处于探索阶段。地面采样距离、成像设备和地理多样性的变化等因素加剧了域偏移，限制了模型在不同域之间的性能。在这种情况下，由于跨域RS图像的域偏移较大，现有的ST方法往往表现不佳。为了应对这些挑战，我们提出将对比学习整合到UDA中，通过最大化同一图像增强视图之间的相似性，增强模型在目标域中捕获语义信息的能力。这种额外的监督提高了模型的表示能力和目标域的分割性能。在包括Potsdam、Vaihingen和LoveDA的RS数据集上进行的广泛实验表明，我们的SimSeg方法优于现有方法，达到了最先进的成果。可视化及定量分析进一步验证了SimSeg在目标域中的学习能力优越性。代码公开在<a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg%E3%80%82">https://github.com/woldier/SiamSeg。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13471v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了遥感图像语义分割的挑战，包括跨域数据的高成本标注和领域差异问题。提出了一种结合对比学习和无监督域自适应的方法，通过最大化同一图像增强视图之间的相似性，增强模型在目标领域捕捉语义信息的能力，进而提高分割性能。在多个遥感数据集上的实验表明，该方法优于现有方法，达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感图像语义分割是挑战性和重要的任务，具有广泛的应用。</li>
<li>深度学习和大规模标注数据集对遥感图像分割有显著的推动作用，但标注成本高昂且耗时。</li>
<li>无监督域自适应（UDA）提供了一种利用未标注目标域数据和标注源域数据的方法。</li>
<li>自训练（ST）方法通过伪标签生成具有缓解领域差异潜力。</li>
<li>在遥感图像分割中，由于跨域的领域偏移，现有的自训练方法通常表现不佳。</li>
<li>提出的结合对比学习和无监督域自适应的方法通过最大化同一图像的增强视图之间的相似性，提高了模型在目标领域的语义捕捉能力和分割性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-16b9c605aaae9f9eba57c4095c57a82a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0441e5239e8a25f01953cf4967e11891.jpg" align="middle">
</details>




<h2 id="Adapting-Vision-Language-Model-with-Fine-grained-Semantics-for-Open-Vocabulary-Segmentation"><a href="#Adapting-Vision-Language-Model-with-Fine-grained-Semantics-for-Open-Vocabulary-Segmentation" class="headerlink" title="Adapting Vision-Language Model with Fine-grained Semantics for   Open-Vocabulary Segmentation"></a>Adapting Vision-Language Model with Fine-grained Semantics for   Open-Vocabulary Segmentation</h2><p><strong>Authors:Yong Xien Chng, Xuchong Qiu, Yizeng Han, Kai Ding, Wan Ding, Gao Huang</strong></p>
<p>Despite extensive research, open-vocabulary segmentation methods still struggle to generalize across diverse domains. To reduce the computational cost of adapting Vision-Language Models (VLMs) while preserving their pre-trained knowledge, most methods freeze the VLMs for mask classification and train only the mask generator. However, our comprehensive analysis reveals a surprising insight: open-vocabulary segmentation is primarily bottlenecked by mask classification, not mask generation. This discovery prompts us to rethink the existing paradigm and explore an alternative approach. Instead of freezing the VLM, we propose to freeze the pre-trained mask generator and focus on optimizing the mask classifier. Building on the observation that VLMs pre-trained on global-pooled image-text features often fail to capture fine-grained semantics necessary for effective mask classification, we propose a novel Fine-grained Semantic Adaptation (FISA) method to address this limitation. FISA enhances the extracted visual features with fine-grained semantic awareness by explicitly integrating this crucial semantic information early in the visual encoding process. As our method strategically optimizes only a small portion of the VLM’s parameters, it enjoys the efficiency of adapting to new data distributions while largely preserving the valuable VLM pre-trained knowledge. Extensive ablation studies confirm the superiority of our approach. Notably, FISA achieves new state-of-the-art results across multiple representative benchmarks, improving performance by up to +1.0 PQ and +3.0 mIoU and reduces training costs by nearly 5x compared to previous best methods. Our code and data will be made public. </p>
<blockquote>
<p>尽管进行了广泛的研究，开放词汇分割方法仍然难以在不同领域进行推广。为了降低适应视觉语言模型（VLM）的计算成本，同时保留其预训练知识，大多数方法都会冻结VLM进行掩膜分类，并且只训练掩膜生成器。然而，我们的综合分析揭示了一个令人惊讶的见解：开放词汇分割主要受到掩膜分类的制约，而不是掩膜生成。这一发现促使我们重新思考现有的范式，并探索一种替代方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16278v2">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的开放词汇分割方法，重点在于优化预训练的掩膜分类器，而非传统的掩膜生成器。研究发现，开放词汇分割的瓶颈在于掩膜分类而非生成。为解决预训练的视觉语言模型（VLM）在全局池化图像文本特征上捕捉不到精细语义的问题，提出一种名为Fine-grained Semantic Adaptation（FISA）的新方法。该方法通过早期明确整合关键语义信息，增强了提取的视觉特征的精细语义感知能力。FISA方法仅在VLM的部分参数上进行优化，既适应了新的数据分布，又保留了大部分预训练知识。实验表明，FISA在多个代表性基准测试上取得了最新的一流结果，性能提升显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开放词汇分割方法的瓶颈在于掩膜分类而非生成。</li>
<li>现有的掩膜生成器冻结方法可能无法充分利用预训练的视觉语言模型（VLM）的知识。</li>
<li>FISA方法专注于优化掩膜分类器，以改善在全局池化图像文本特征上捕捉不到精细语义的问题。</li>
<li>FISA方法通过早期整合关键语义信息，增强了视觉特征的精细语义感知能力。</li>
<li>FISA方法仅在VLM的部分参数上进行优化，以适应新数据分布并保留大部分预训练知识。</li>
<li>FISA在多个基准测试上取得了最新的一流结果，性能提升显著。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-21ed2044cc4b9ba56ee3e2620cb7c73c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-47db8f9dc5c9817210eca4b992830339.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-706901fed520947b5cbaf6c62b10c250.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8688bde5f1bb544f5638ac2b177c1922.jpg" align="middle">
</details>




<h2 id="PolarBEVDet-Exploring-Polar-Representation-for-Multi-View-3D-Object-Detection-in-Bird’s-Eye-View"><a href="#PolarBEVDet-Exploring-Polar-Representation-for-Multi-View-3D-Object-Detection-in-Bird’s-Eye-View" class="headerlink" title="PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object   Detection in Bird’s-Eye-View"></a>PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object   Detection in Bird’s-Eye-View</h2><p><strong>Authors:Zichen Yu, Quanli Liu, Wei Wang, Liyong Zhang, Xiaoguang Zhao</strong></p>
<p>Recently, LSS-based multi-view 3D object detection provides an economical and deployment-friendly solution for autonomous driving. However, all the existing LSS-based methods transform multi-view image features into a Cartesian Bird’s-Eye-View(BEV) representation, which does not take into account the non-uniform image information distribution and hardly exploits the view symmetry. In this paper, in order to adapt the image information distribution and preserve the view symmetry by regular convolution, we propose to employ the polar BEV representation to substitute the Cartesian BEV representation. To achieve this, we elaborately tailor three modules: a polar view transformer to generate the polar BEV representation, a polar temporal fusion module for fusing historical polar BEV features and a polar detection head to predict the polar-parameterized representation of the object. In addition, we design a 2D auxiliary detection head and a spatial attention enhancement module to improve the quality of feature extraction in perspective view and BEV, respectively. Finally, we integrate the above improvements into a novel multi-view 3D object detector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves the superior performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Yzichen/PolarBEVDet.git">https://github.com/Yzichen/PolarBEVDet.git</a>.(This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible) </p>
<blockquote>
<p>最近，基于LSS的多视角3D对象检测为自动驾驶提供了经济且易于部署的解决方案。然而，现有的所有基于LSS的方法都将多视角图像特征转换为笛卡尔鸟瞰（BEV）表示，这种方法没有考虑到图像信息分布的不均匀性，且难以利用视图对称性。在本文中，为了适应图像信息分布并保留常规卷积的视图对称性，我们提出使用极坐标BEV表示来替代笛卡尔BEV表示。为此，我们精心设计了三个模块：极视变换器，用于生成极坐标BEV表示；极时间融合模块，用于融合历史极BEV特征；以及极检测头，用于预测对象的极参数表示。此外，我们设计了一个2D辅助检测头和空间注意力增强模块，分别提高了透视图和BEV的特征提取质量。最后，我们将上述改进整合到新型多视角3D对象检测器PolarBEVDet中。在nuScenes上的实验表明，PolarBEVDet实现了卓越的性能。代码可在[<a target="_blank" rel="noopener" href="https://github.com/Yzichen/PolarBEVDet.git%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82%EF%BC%88%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E5%B7%B2%E6%8F%90%E4%BA%A4%E8%87%B3IEEE%E5%8F%AF%E8%83%BD%E8%BF%9B%E8%A1%8C%E5%8F%91%E8%A1%A8%E3%80%82%E7%89%88%E6%9D%83%E5%8F%AF%E8%83%BD%E5%9C%A8%E6%B2%A1%E6%9C%89%E9%80%9A%E7%9F%A5%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E8%BF%9B%E8%A1%8C%E8%BD%AC%E7%A7%BB%EF%BC%8C%E6%AD%A4%E5%90%8E%E6%AD%A4%E7%89%88%E6%9C%AC%E5%8F%AF%E8%83%BD%E4%B8%8D%E5%86%8D%E5%8F%AF%E7%94%A8%EF%BC%89]">https://github.com/Yzichen/PolarBEVDet.git上获取。（这项工作已提交至IEEE可能进行发表。版权可能在没有通知的情况下进行转移，此后此版本可能不再可用）]</a>(<a target="_blank" rel="noopener" href="https://github.com/Yzichen/PolarBEVDet.git%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82%EF%BC%88%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E5%B7%B2%E6%8F%90%E4%BA%A4%E8%87%BA%E5%A4%84%E5%8F%AF%E8%83%BD%E5%87%BA%E7%89%88%E7%89%A9%E8%A1%A8%E4%B8%8E%E8%B7%A8%OFDM%E5%85%83%E7%BA%A6%E5%AE%9A,%E6%98%AF%E5%90%A6%E9%80%9A%E7%9F%A5%E5%90%8E%E7%89%88%E6%9D%83%E5%8F%AF%E8%83%BD%E8%BD%AC%E7%A7%BB%E5%90%8E%E6%AD%A4%E7%89%88%E6%9C%AC%E5%8F%AF%E8%83%BD%E4%B8%8D%E5%86%8D%E5%8F%AF%E7%94%A8%EF%BC%89]">https://github.com/Yzichen/PolarBEVDet.git%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82%EF%BC%88%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E5%B7%B2%E6%8F%90%E4%BA%A4%E8%87%BA处可能出版物表与跨%OFDM元约定,是否通知后版权可能转移后此版本可能不再可用）]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16200v3">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>基于LSS的多视角3D对象检测为自动驾驶提供了经济且易于部署的解决方案。但现有方法忽略图像信息分布的非均匀性，难以利用视角对称性。本文提出使用极坐标BEV表示替代笛卡尔坐标系BEV表示，并设计了三个模块：极视图转换器、极时间融合模块和极检测头，以预测对象的极参数表示。此外，还设计了二维辅助检测头和空间注意力增强模块，以提高透视图和俯视图特征提取的质量。最终将上述改进整合到新型多视角3D对象检测器PolarBEVDet中，在nuScenes上的实验表明其性能卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LSS-based multi-view 3D object detection is an economical and deployment-friendly solution for autonomous driving.</li>
<li>现有LSS方法使用笛卡尔坐标系BEV表示，忽略了图像信息的非均匀分布和视角对称性。</li>
<li>本文提出使用极坐标BEV表示，并设计了三个关键模块以适应图像信息分布并保留视角对称性。</li>
<li>极视图转换器、极时间融合模块和极检测头共同构成新型多视角3D对象检测器PolarBEVDet。</li>
<li>通过nuScenes实验验证了PolarBEVDet的卓越性能。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-eb4a5fcb6dc69cc5707ee5e7f059dc76.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dcaae24e0a9ff42338b169bac8cd9673.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-887236fea5c85b8d657364f98e0a6cef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-99a68bec8511867c6e3ccf99638c12bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7e3b362b22feb1f0b33fa1f55299679e.jpg" align="middle">
</details>




<h2 id="A-Review-of-Human-Object-Interaction-Detection"><a href="#A-Review-of-Human-Object-Interaction-Detection" class="headerlink" title="A Review of Human-Object Interaction Detection"></a>A Review of Human-Object Interaction Detection</h2><p><strong>Authors:Yuxiao Wang, Qiwei Xiong, Yu Lei, Weiying Xue, Qi Liu, Zhenao Wei</strong></p>
<p>Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summarizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detection are introduced. Furthermore, starting with two-stage methods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored. </p>
<blockquote>
<p>人机交互（HOI）检测在高层次视觉理解中扮演着关键角色，有助于对人类活动进行深度理解。具体而言，HOI检测旨在定位图像或视频中涉及交互的人类和物体，并分类它们之间的特定交互。此任务的成功受到几个关键因素的影响，包括人类和物体实例的准确定位，以及物体类别和交互关系的正确分类。本文系统地总结和讨论了基于图像的HOI检测的近期工作。首先，介绍了HOI关系检测所涉及的主流数据集。此外，本文从两阶段方法和端到端单阶段检测方法入手，全面讨论了基于图像的HOI检测的当前发展，分析了这两种方法的优缺点。另外，还讨论了零样本学习、弱监督学习以及大规模语言模型在HOI检测中的应用。最后，概述了HOI检测当前的挑战，并探讨了潜在的研究方向和未来趋势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10641v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文总结了基于图像的人与物体交互（HOI）检测领域的最新进展。文章介绍了HOI检测的主流数据集，详细讨论了包括两阶段方法和端到端的一阶段检测方案在内的当前图像HOI检测方法，并分析了它们的优缺点。此外，文章还探讨了零样本学习、弱监督学习以及大规模语言模型在HOI检测中的应用，并指出了当前HOI检测面临的挑战以及潜在的研究方向和未来趋势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人与物体交互（HOI）检测在高级视觉理解中扮演重要角色，旨在识别图像或视频中的人和物体间的交互，并对它们之间的特定交互进行分类。</li>
<li>主流数据集涉及HOI关系检测被介绍。</li>
<li>当前图像HOI检测方法包括两阶段方法和端到端的一阶段检测方案，它们的优缺点进行了分析。</li>
<li>零样本学习、弱监督学习以及大规模语言模型在HOI检测中的应用被探讨。</li>
<li>HOI检测的成功依赖于准确的人与物体实例定位，以及物体类别和交互关系的正确分类。</li>
<li>当前HOI检测面临的挑战包括技术难题和实际应用中的限制。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-82a16e25685dec1bdd8dd9d3a5d1aaa5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-24c75a4834c705e220c4be1624d57c18.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-471e4dd4bc2692edcf61916f32d134b6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fd4dc8f1567690797e9bae7fafdf2f4.jpg" align="middle">
</details>




<h2 id="PADetBench-Towards-Benchmarking-Physical-Attacks-against-Object-Detection"><a href="#PADetBench-Towards-Benchmarking-Physical-Attacks-against-Object-Detection" class="headerlink" title="PADetBench: Towards Benchmarking Physical Attacks against Object   Detection"></a>PADetBench: Towards Benchmarking Physical Attacks against Object   Detection</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Lap-Pui Chau, Shaohui Mei</strong></p>
<p>Physical attacks against object detection have gained increasing attention due to their significant practical implications. However, conducting physical experiments is extremely time-consuming and labor-intensive. Moreover, physical dynamics and cross-domain transformation are challenging to strictly regulate in the real world, leading to unaligned evaluation and comparison, severely hindering the development of physically robust models. To accommodate these challenges, we explore utilizing realistic simulation to thoroughly and rigorously benchmark physical attacks with fairness under controlled physical dynamics and cross-domain transformation. This resolves the problem of capturing identical adversarial images that cannot be achieved in the real world. Our benchmark includes 20 physical attack methods, 48 object detectors, comprehensive physical dynamics, and evaluation metrics. We also provide end-to-end pipelines for dataset generation, detection, evaluation, and further analysis. In addition, we perform 8064 groups of evaluation based on our benchmark, which includes both overall evaluation and further detailed ablation studies for controlled physical dynamics. Through these experiments, we provide in-depth analyses of physical attack performance and physical adversarial robustness, draw valuable observations, and discuss potential directions for future research.   Codebase: <a target="_blank" rel="noopener" href="https://github.com/JiaweiLian/Benchmarking_Physical_Attack">https://github.com/JiaweiLian/Benchmarking_Physical_Attack</a> </p>
<blockquote>
<p>针对对象检测的实体攻击因其在实际应用中的重大含义而日益受到关注。然而，进行物理实验非常耗时且劳力密集。此外，在现实世界中对物理动态和跨域转换进行严格监管具有挑战性，导致评估与比较无法对齐，严重阻碍了物理稳健型模型的发展。为了应对这些挑战，我们探索利用逼真的模拟，在受控的物理动态和跨域转换下，公平、全面、严格地评估物理攻击。这解决了在现实世界无法捕获相同对抗图像的问题。我们的基准测试包括20种物理攻击方法、48种对象检测器、全面的物理动态和评估指标。我们还为数据集生成、检测、评估和进一步分析提供了端到端的管道。此外，我们基于基准测试进行了8064组评估，包括整体评估和受控物理动态的进一步详细消融研究。通过这些实验，我们对物理攻击性能和物理对抗稳健性进行了深入分析，观察并总结了有价值的见解，并讨论了未来研究的发展方向。相关代码库地址为：<a target="_blank" rel="noopener" href="https://github.com/JiaweiLian/Benchmarking_Physical_Attack">https://github.com/JiaweiLian/Benchmarking_Physical_Attack</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09181v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对物理攻击对物体检测的影响，研究界日益关注其实践意义。然而，物理实验耗时耗力，且现实世界中物理动态和跨域转换的调控具有挑战性，导致评估与对比的不统一，阻碍了物理鲁棒性模型的发展。本研究利用逼真的模拟技术，在控制物理动态和跨域转换的条件下，公平地评估物理攻击方法。该模拟解决了现实中无法捕捉相同对抗图像的问题。研究构建了包含多种物理攻击方法、目标检测器、物理动态和评估指标的基准测试平台，并提供数据集生成、检测、评估和进一步分析的端到端管道。基于该基准测试进行了大量实验，深入分析了物理攻击性能和物理对抗稳健性，为未来的研究提供了潜在方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>物理攻击对物体检测的影响受到广泛关注，其实践意义日益重要。</li>
<li>现实世界的物理实验存在时间成本和劳动力成本高昂的问题。</li>
<li>物理动态和跨域转换的调控在现实中具有挑战性，影响评估与对比的统一性。</li>
<li>利用逼真模拟技术可以公平评估物理攻击方法，解决现实中无法捕捉相同对抗图像的问题。</li>
<li>研究构建了包含多种物理攻击方法、目标检测器等的基准测试平台。</li>
<li>研究提供了数据集生成、检测、评估和进一步分析的端到端管道。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b778d9b94e2074d79e9fcd5078ab6286.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2a336d85c76d7921601b3bc861f8e17.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9eb6b48781d406806b9dfab84faccf9e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-59f738ab1333907cf5ad54b50a356f70.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dd6c6bd1e84625607e986fd8ca6245b1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4b90ea974d2b504963190f16e920e65.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f4d2eaaf52c7778c8252d83b797b429.jpg" align="middle">
</details>




<h2 id="Adaptive-Patch-Contrast-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Adaptive-Patch-Contrast-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation"></a>Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Jimin Xiao, Fei Ma, Renrong Ouyang</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to its cost-effectiveness. The typical framework involves using image-level labels as training data to generate pixel-level pseudo-labels with refinements. Recently, methods based on Vision Transformers (ViT) have demonstrated superior capabilities in generating reliable pseudo-labels, particularly in recognizing complete object regions, compared to CNN methods. However, current ViT-based approaches have some limitations in the use of patch embeddings, being prone to being dominated by certain abnormal patches, as well as many multi-stage methods being time-consuming and lengthy in training, thus lacking efficiency. Therefore, in this paper, we introduce a novel ViT-based WSSS method named \textit{Adaptive Patch Contrast} (APC) that significantly enhances patch embedding learning for improved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP) layer to address the limitations of previous max pooling selection methods. Additionally, we propose a Patch Contrastive Learning (PCL) to enhance patch embeddings, thereby further improving the final results. Furthermore, we improve upon the existing multi-stage training framework without CAM by transforming it into an end-to-end single-stage training approach, thereby enhancing training efficiency. The experimental results show that our approach is effective and efficient, outperforming other state-of-the-art WSSS methods on the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training duration. </p>
<blockquote>
<p>使用仅图像级标签的弱监督语义分割（WSSS）因其成本效益而备受关注。典型的框架是使用图像级标签作为训练数据，通过改进生成像素级伪标签。最近，基于视觉转换器（ViT）的方法在生成可靠的伪标签方面表现出了卓越的能力，特别是在识别完整对象区域方面，相较于CNN方法。然而，当前的ViT方法在使用补丁嵌入时存在一些局限性，容易受某些异常补丁的主导，并且许多多阶段方法在训练和推理上耗时较长，因此缺乏效率。因此，本文介绍了一种新型的基于ViT的WSSS方法，名为自适应补丁对比（APC），它显著提高了补丁嵌入学习，增强了分割效果。APC利用自适应K池化（AKP）层来解决先前最大池化选择方法的局限性。此外，我们提出了补丁对比学习（PCL）以增强补丁嵌入，从而进一步提高最终结果。此外，我们改进了现有的多阶段训练框架，通过将其转变为端到端的单阶段训练方式，提高了训练效率。实验结果表明，我们的方法有效且高效，在PASCAL VOC 2012和MS COCO 2014数据集上超越了其他最先进的WSSS方法，并且在较短的训练时间内达到了良好效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10649v2">PDF</a> Accepted by the EAAI Journal</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种基于Vision Transformer（ViT）的弱监督语义分割（WSSS）新方法——自适应补丁对比（APC）。该方法通过改进补丁嵌入学习和提出自适应K池化（AKP）层以及补丁对比学习（PCL）技术，提高了分割效果。此外，它还将现有的多阶段训练框架改进为端到端的单阶段训练，提高了训练效率。实验结果表明，该方法在PASCAL VOC 2012和MS COCO 2014数据集上表现优异，且训练时间短。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>WSSS利用图像级标签作为训练数据生成像素级伪标签，近期基于Vision Transformer（ViT）的方法在生成可靠伪标签方面表现出卓越的能力。</li>
<li>当前ViT方法在使用补丁嵌入时存在局限性，容易受到异常补丁的影响。</li>
<li>提出了一种新的ViT-based WSSS方法——自适应补丁对比（APC），通过改进补丁嵌入学习提高分割效果。</li>
<li>APC利用自适应K池化（AKP）层解决之前最大池化选择方法的局限性。</li>
<li>引入了补丁对比学习（PCL）来进一步增强补丁嵌入。</li>
<li>将现有的多阶段训练框架改进为端到端的单阶段训练，提高了训练效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-37c3ec3384e6ddd1aeaa3c8b2c22510e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c4b55aacee3db09fe4f73359438a7ced.jpg" align="middle">
</details>




<h2 id="BiCo-Fusion-Bidirectional-Complementary-LiDAR-Camera-Fusion-for-Semantic-and-Spatial-Aware-3D-Object-Detection"><a href="#BiCo-Fusion-Bidirectional-Complementary-LiDAR-Camera-Fusion-for-Semantic-and-Spatial-Aware-3D-Object-Detection" class="headerlink" title="BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for   Semantic- and Spatial-Aware 3D Object Detection"></a>BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for   Semantic- and Spatial-Aware 3D Object Detection</h2><p><strong>Authors:Yang Song, Lin Wang</strong></p>
<p>3D object detection is an important task that has been widely applied in autonomous driving. To perform this task, a new trend is to fuse multi-modal inputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these two modalities by unifying them in the same 3D space. However, during direct fusion in a unified space, the drawbacks of both modalities (LiDAR features struggle with detailed semantic information and the camera lacks accurate 3D spatial information) are also preserved, diluting semantic and spatial awareness of the final unified representation. To address the issue, this letter proposes a novel bidirectional complementary LiDAR-camera fusion framework, called BiCo-Fusion that can achieve robust semantic- and spatial-aware 3D object detection. The key insight is to fuse LiDAR and camera features in a bidirectional complementary way to enhance the semantic awareness of the LiDAR and the 3D spatial awareness of the camera. The enhanced features from both modalities are then adaptively fused to build a semantic- and spatial-aware unified representation. Specifically, we introduce Pre-Fusion consisting of a Voxel Enhancement Module (VEM) to enhance the semantic awareness of voxel features from 2D camera features and Image Enhancement Module (IEM) to enhance the 3D spatial awareness of camera features from 3D voxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse the enhanced features from the last stage to build a unified representation. Extensive experiments demonstrate the superiority of our BiCo-Fusion against the prior arts. Project page: <a target="_blank" rel="noopener" href="https://t-ys.github.io/BiCo-Fusion/">https://t-ys.github.io/BiCo-Fusion/</a>. </p>
<blockquote>
<p>三维物体检测是一项在自动驾驶中广泛应用的重要任务。为了完成这项任务，目前的一个新趋势是融合多模态输入，即激光雷达和摄像机。在这种趋势下，最近的方法通过将这两种模态统一到同一三维空间中进行融合。然而，在统一空间中进行直接融合时，两种模态的缺点（激光雷达特征在详细语义信息方面的挣扎以及摄像机缺乏准确的3D空间信息）也被保留下来，稀释了最终统一表示的语义和空间感知能力。针对这一问题，本文提出了一种双向互补激光雷达-摄像机融合框架，称为BiCo-Fusion，可实现稳健的语义和空间感知三维物体检测。关键在于以双向互补的方式融合激光雷达和摄像机特征，以增强激光雷达的语义感知能力和摄像机的三维空间感知能力。然后，增强后的来自两个模态的特征被自适应融合以构建具有语义和空间感知的统一表示。具体来说，我们引入了Pre-Fusion，包括一个体素增强模块（VEM）以增强二维摄像机特征的体素特征的语义感知能力，以及图像增强模块（IEM）以增强三维体素特征的摄像机特征的三维空间感知能力。然后我们引入了Unified Fusion（U-Fusion）来自适应融合上一阶段的增强特征以构建统一表示。大量实验证明我们的BiCo-Fusion相较于先前技术具有优越性。项目页面：<a target="_blank" rel="noopener" href="https://t-ys.github.io/BiCo-Fusion/">https://t-ys.github.io/BiCo-Fusion/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.19048v2">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在自动驾驶中的三维物体检测任务中，融合激光雷达和相机两种模态输入的新趋势。针对直接融合带来的语义和空间信息缺失问题，提出了一种双向互补的激光雷达-相机融合框架BiCo-Fusion，通过增强激光雷达的语义信息和相机的三维空间信息，实现稳健的三维物体检测。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D物体检测在自动驾驶中至关重要，多模态融合（尤其是激光雷达和相机）成为当前研究趋势。</li>
<li>直接在统一的三维空间中进行多模态融合会保留两种模态的缺点，导致语义和空间信息的损失。</li>
<li>BiCo-Fusion框架通过双向互补融合激光雷达和相机特征，提高语义和空间感知能力。</li>
<li>框架包括预融合阶段的voxel增强模块和图像增强模块，以及统一融合阶段的自适应特征融合。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0132297f5baaa19fa50be4c3f8feb13f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4188fa509b6075fd7dc64fa22982b02f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f720f55c719eac59d999b7eec56eaf14.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fd6bde5ecb548fa05046c878a5b7215d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-523fec1f380049e3ab375d94b1aa0063.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1f6684274aa763b8958d930e7c05db26.jpg" align="middle">
</details>




<h2 id="SemFlow-Binding-Semantic-Segmentation-and-Image-Synthesis-via-Rectified-Flow"><a href="#SemFlow-Binding-Semantic-Segmentation-and-Image-Synthesis-via-Rectified-Flow" class="headerlink" title="SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified   Flow"></a>SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified   Flow</h2><p><strong>Authors:Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, Ming-Hsuan Yang</strong></p>
<p>Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation. While existing methods consider them as two distinct tasks, we propose a unified framework (SemFlow) and model them as a pair of reverse problems. Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks. As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly. For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results. For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories. Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks. We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision. </p>
<blockquote>
<p>语义分割和语义图像合成是视觉感知和生成中的两个代表性任务。虽然现有方法将它们视为两个独立的任务，但我们提出了一个统一的框架（SemFlow）并将它们建模为一对反向问题。具体来说，受正则流理论的启发，我们训练一个常微分方程（ODE）模型，在真实图像和语义蒙版分布之间进行传输。由于训练对象是对称的，属于两个分布（即图像和语义蒙版）的样本可以很容易地进行可逆转换。对于语义分割，我们的方法解决了扩散输出随机性和分割结果唯一性之间的矛盾。对于图像合成，我们提出了一种有限扰动方法，以提高生成结果的多样性，而不改变语义类别。实验表明，我们的SemFlow在语义分割和语义图像合成任务上取得了具有竞争力的结果。我们希望这个简单的框架能激励人们重新思考低级别和高级别视觉的统一性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20282v2">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong>：我们提出了一种统一框架（SemFlow），将语义分割和语义图像合成视为一对反向问题。通过训练普通微分方程（ODE）模型，在真实图像和语义掩膜分布之间实现传输。这种对称性使得两个分布之间的样本，即图像和语义掩膜，可以容易地进行可逆转换。这一方法解决了语义分割中扩散输出随机性与分割结果唯一性的矛盾，同时为图像合成提出了一种有限扰动方法，提高了生成结果的多样性而不改变语义类别。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>提出了一种统一框架SemFlow，将语义分割和语义图像合成视为反向问题。</li>
<li>利用普通微分方程（ODE）模型在真实图像和语义掩膜分布间实现传输。</li>
<li>训练对象具有对称性，使得图像和语义掩膜之间的转换可逆。</li>
<li>解决语义分割中扩散输出随机性与分割结果唯一性的矛盾。</li>
<li>在图像合成中采用有限扰动方法，提高生成结果的多样性。</li>
<li>实验显示SemFlow在语义分割和语义图像合成任务上表现有竞争力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-354ba4524def0ee54e46d8ec2a845dd1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0a9467b5b908c5e22a9fd3b93479c472.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ccd0d6a3db4aeb572f14215970e62c75.jpg" align="middle">
</details>




<h2 id="Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models"><a href="#Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models" class="headerlink" title="Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models"></a>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models</h2><p><strong>Authors:Zhaozheng Chen, Qianru Sun</strong></p>
<p>The rapid development of deep learning has driven significant progress in image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this journal, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area. </p>
<blockquote>
<p>深度学习技术的快速发展推动了图像语义分割在计算机视觉领域中的重大进步。语义分割算法通常依赖于像素级标签（即对象掩膜）的可用性，这些标签标注昂贵、耗费时间并且需要大量人工操作。弱监督语义分割（WSSS）是一种有效的避免这种标注的解决方案。它仅利用部分或不完整的注释，并为全监督语义分割提供了成本效益更高的替代方案。在本期刊中，我们的关注点是使用图像级标签的WSSS，这是最具有挑战性的形式。我们的工作分为两部分。首先，我们对传统方法进行了全面的综述，主要集中在主要研究会议提出的这些方法上。我们根据他们的操作方法将它们分为四类：像素级、图像级、跨图像和外部数据。其次，我们探讨了视觉基础模型在WSSS背景下的适用性，如分段任何模型（SAM）。我们在两个有趣的场景中仔细审查了SAM：文本提示和零样本学习。我们深入探讨了将视觉基础模型用于WSSS的潜力和挑战，为未来在这个激动人心的研究领域中的发展提供了便利。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13026v2">PDF</a> Accepted to ACM Computing Surveys</p>
<p><strong>Summary</strong></p>
<p>深度学习的发展极大地推动了图像语义分割在计算机视觉领域中的进步。然而，语义分割算法通常依赖于像素级标签（即对象掩膜），这些标签的获取成本高昂且耗时。弱监督语义分割（WSSS）是一种有效的解决方案，它仅利用部分或不完整的注释，为全监督语义分割提供了经济高效的替代方案。本文关注于使用图像级标签的WSSS，这是最具挑战性的形式。本文首先全面回顾了传统方法，主要聚焦于顶级研究会议提出的方法，并根据它们的工作方式将其分为四类：像素级、图像级、跨图像和外部数据。其次，我们探讨了视觉基础模型（如Segment Anything Model，SAM）在WSSS中的应用。我们详细研究了SAM在两个有趣的场景：文本提示和零样本学习中的适用性。我们深入剖析了使用视觉基础模型进行WSSS的潜力和挑战，为未来该领域的发展提供了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习推动了图像语义分割的进步。</li>
<li>语义分割算法依赖像素级标签，但获取成本高昂且耗时。</li>
<li>弱监督语义分割（WSSS）是解决这一问题的一种有效方法，仅利用部分或不完整的注释。</li>
<li>使用图像级标签的WSSS最具挑战性。</li>
<li>传统方法被分类为像素级、图像级、跨图像和外部数据四类。</li>
<li>Segment Anything Model（SAM）在WSSS中展现出潜力，特别是在文本提示和零样本学习方面。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5de8bbf35b6e19bb15d97b70d1365142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6f366106e2aa1a029e0793160273fb8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c826195ccfa445054e06431b7d6c4bdc.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_人脸相关/2401.09006v2/page_3_1.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2024-12-12  Local Features Meet Stochastic Anonymization Revolutionizing   Privacy-Preserving Face Recognition for Black-Box Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Vision Transformer/2403.08271v2/page_3_0.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2024-12-12  EOV-Seg Efficient Open-Vocabulary Panoptic Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
