<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a5ffa6182763bc6e4bde526a15db0e11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    34.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    141 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Enhancing-3D-Object-Detection-in-Autonomous-Vehicles-Based-on-Synthetic-Virtual-Environment-Analysis"><a href="#Enhancing-3D-Object-Detection-in-Autonomous-Vehicles-Based-on-Synthetic-Virtual-Environment-Analysis" class="headerlink" title="Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis"></a>Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis</h2><p><strong>Authors:Vladislav Li, Ilias Siniosoglou, Thomai Karamitsou, Anastasios Lytos, Ioannis D. Moscholios, Sotirios K. Goudos, Jyoti S. Banerjee, Panagiotis Sarigiannidi, Vasileios Argyriou</strong></p>
<p>Autonomous Vehicles (AVs) use natural images and videos as input to understand the real world by overlaying and inferring digital elements, facilitating proactive detection in an effort to assure safety. A crucial aspect of this process is real-time, accurate object recognition through automatic scene analysis. While traditional methods primarily concentrate on 2D object detection, exploring 3D object detection, which involves projecting 3D bounding boxes into the three-dimensional environment, holds significance and can be notably enhanced using the AR ecosystem. This study examines an AI modelâ€™s ability to deduce 3D bounding boxes in the context of real-time scene analysis while producing and evaluating the modelâ€™s performance and processing time, in the virtual domain, which is then applied to AVs. This work also employs a synthetic dataset that includes artificially generated images mimicking various environmental, lighting, and spatiotemporal states. This evaluation is oriented in handling images featuring objects in diverse weather conditions, captured with varying camera settings. These variations pose more challenging detection and recognition scenarios, which the outcomes of this work can help achieve competitive results under most of the tested conditions. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVï¼‰ä½¿ç”¨è‡ªç„¶å›¾åƒå’Œè§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å åŠ å’Œæ¨æ–­æ•°å­—å…ƒç´ æ¥ç†è§£ç°å®ä¸–ç•Œï¼Œä¿ƒè¿›ä¸»åŠ¨æ£€æµ‹ï¼Œä»¥ç¡®ä¿å®‰å…¨ã€‚æ­¤è¿‡ç¨‹çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯å®æ—¶ã€å‡†ç¡®çš„ç‰©ä½“è¯†åˆ«ï¼Œé€šè¿‡è‡ªåŠ¨åœºæ™¯åˆ†æå®ç°ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨2Dç›®æ ‡æ£€æµ‹ä¸Šï¼Œä½†æ¢ç´¢3Dç›®æ ‡æ£€æµ‹å…·æœ‰é‡è¦æ„ä¹‰ï¼Œæ¶‰åŠå°†3Dè¾¹ç•Œæ¡†æŠ•å½±åˆ°ä¸‰ç»´ç¯å¢ƒä¸­ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨ARç”Ÿæ€ç³»ç»Ÿæ˜¾è‘—å¢å¼ºã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç ”ç©¶AIæ¨¡å‹åœ¨å®æ—¶åœºæ™¯åˆ†æèƒŒæ™¯ä¸‹æ¨æ–­3Dè¾¹ç•Œæ¡†çš„èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨è™šæ‹ŸåŸŸä¸­ç”Ÿæˆå¹¶è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œå¤„ç†æ—¶é—´ï¼Œç„¶åå°†å…¶åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚è¿™é¡¹å·¥ä½œè¿˜é‡‡ç”¨åˆæˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿå„ç§ç¯å¢ƒã€ç…§æ˜å’Œæ—¶ç©ºçŠ¶æ€çš„äººå·¥ç”Ÿæˆå›¾åƒã€‚è¯¥è¯„ä¼°ä¾§é‡äºå¤„ç†åœ¨ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹æ‹æ‘„çš„åŒ…å«ç‰©ä½“çš„å›¾åƒï¼Œä½¿ç”¨ä¸åŒçš„ç›¸æœºè®¾ç½®ã€‚è¿™äº›å˜åŒ–å¸¦æ¥äº†æ›´å…·æŒ‘æˆ˜çš„æ£€æµ‹å’Œè¯†åˆ«åœºæ™¯ï¼Œæœ¬å·¥ä½œçš„ç»“æœå¯ä»¥åœ¨å¤§å¤šæ•°æµ‹è¯•æ¡ä»¶ä¸‹å®ç°æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07509v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨é©¾é©¶è½¦è¾†åˆ©ç”¨è‡ªç„¶å›¾åƒå’Œè§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å åŠ å’Œæ¨æ–­æ•°å­—å…ƒç´ ç†è§£ç°å®ä¸–ç•Œï¼Œä»¥å®ç°ä¸»åŠ¨æ£€æµ‹å¹¶ç¡®ä¿å®‰å…¨ã€‚å…³é”®è¿‡ç¨‹åœ¨äºé€šè¿‡è‡ªåŠ¨åœºæ™¯åˆ†æè¿›è¡Œå®æ—¶ã€ç²¾ç¡®çš„ç›®æ ‡è¯†åˆ«ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å®æ—¶åœºæ™¯åˆ†æä¸­æ¨æ–­ä¸‰ç»´è¾¹ç•Œæ¡†çš„èƒ½åŠ›ï¼Œå¹¶åœ¨è™šæ‹Ÿé¢†åŸŸäº§ç”Ÿå¹¶è¯„ä¼°äº†æ¨¡å‹çš„æ€§èƒ½å’Œè¿è¡Œæ—¶é—´ï¼Œç„¶åå°†å…¶åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚è¯¥ç ”ç©¶è¿˜é‡‡ç”¨äº†åˆæˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿå„ç§ç¯å¢ƒã€å…‰ç…§å’Œæ—¶ç©ºçŠ¶æ€çš„åˆæˆå›¾åƒã€‚æ­¤è¯„ä¼°ä¸»è¦å¤„ç†åœ¨ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹æ‹æ‘„çš„ç‰©ä½“å›¾åƒï¼Œé‡‡ç”¨å„ç§ç›¸æœºè®¾ç½®ã€‚è¿™äº›å˜åŒ–å¸¦æ¥äº†æ›´å…·æŒ‘æˆ˜æ€§çš„æ£€æµ‹å’Œè¯†åˆ«åœºæ™¯ï¼Œè€Œè¿™é¡¹å·¥ä½œçš„æˆæœå¯ä»¥åœ¨å¤§å¤šæ•°æµ‹è¯•æ¡ä»¶ä¸‹å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰ä¾èµ–è‡ªç„¶å›¾åƒå’Œè§†é¢‘ä½œä¸ºè¾“å…¥æ¥ç†è§£ç°å®ä¸–ç•Œï¼Œå¹¶é‡‡ç”¨æ•°å­—å…ƒç´ å åŠ è¿›è¡Œå®æ—¶æ£€æµ‹ä»¥ç¡®ä¿å®‰å…¨ã€‚</li>
<li>å®æ—¶ã€ç²¾ç¡®çš„ç›®æ ‡è¯†åˆ«æ˜¯è‡ªåŠ¨é©¾é©¶ä¸­çš„å…³é”®è¿‡ç¨‹ï¼Œä¸»è¦é€šè¿‡è‡ªåŠ¨åœºæ™¯åˆ†æå®ç°ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨äºŒç»´ç›®æ ‡æ£€æµ‹ï¼Œè€Œä¸‰ç»´ç›®æ ‡æ£€æµ‹ï¼ˆæ¶‰åŠåœ¨ä¸‰ç»´ç¯å¢ƒä¸­æŠ•å½±ä¸‰ç»´è¾¹ç•Œæ¡†ï¼‰å¯¹è‡ªåŠ¨é©¾é©¶å°¤ä¸ºé‡è¦ã€‚</li>
<li>å¢å¼ºç°å®ï¼ˆARï¼‰ç”Ÿæ€ç³»ç»Ÿå¯ä»¥æ˜¾è‘—å¢å¼ºä¸‰ç»´ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†AIæ¨¡å‹åœ¨å®æ—¶åœºæ™¯åˆ†æä¸­æ¨æ–­ä¸‰ç»´è¾¹ç•Œæ¡†çš„èƒ½åŠ›ï¼Œå¹¶åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¯„ä¼°äº†å…¶æ€§èƒ½å’Œè¿è¡Œæ—¶é—´ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†åˆæˆæ•°æ®é›†ï¼Œæ¨¡æ‹Ÿäº†å„ç§ç¯å¢ƒã€å…‰ç…§å’Œæ—¶ç©ºæ¡ä»¶ä¸‹çš„å›¾åƒï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å„ç§å¤©æ°”å’Œç›¸æœºè®¾ç½®ä¸‹çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9abe4685c17cd8580a3a8e672b014115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b7dbdcfd95c20072ba518ad7a50f75.jpg" align="middle">
</details>




<h2 id="LUIEO-A-Lightweight-Model-for-Integrating-Underwater-Image-Enhancement-and-Object-Detection"><a href="#LUIEO-A-Lightweight-Model-for-Integrating-Underwater-Image-Enhancement-and-Object-Detection" class="headerlink" title="LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement   and Object Detection"></a>LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement   and Object Detection</h2><p><strong>Authors:Bin Li, Li Li, Zhenwei Zhang, Yuping Duan</strong></p>
<p>Underwater optical images inevitably suffer from various degradation factors such as blurring, low contrast, and color distortion, which hinder the accuracy of object detection tasks. Due to the lack of paired underwater&#x2F;clean images, most research methods adopt a strategy of first enhancing and then detecting, resulting in a lack of feature communication between the two learning tasks. On the other hand, due to the contradiction between the diverse degradation factors of underwater images and the limited number of samples, existing underwater enhancement methods are difficult to effectively enhance degraded images of unknown water bodies, thereby limiting the improvement of object detection accuracy. Therefore, most underwater target detection results are still displayed on degraded images, making it difficult to visually judge the correctness of the detection results. To address the above issues, this paper proposes a multi-task learning method that simultaneously enhances underwater images and improves detection accuracy. Compared with single-task learning, the integrated model allows for the dynamic adjustment of information communication and sharing between different tasks. Due to the fact that real underwater images can only provide annotated object labels, this paper introduces physical constraints to ensure that object detection tasks do not interfere with image enhancement tasks. Therefore, this article introduces a physical module to decompose underwater images into clean images, background light, and transmission images and uses a physical model to calculate underwater images for self-supervision. Numerical experiments demonstrate that the proposed model achieves satisfactory results in visual performance, object detection accuracy, and detection efficiency compared to state-of-the-art comparative methods. </p>
<blockquote>
<p>æ°´ä¸‹å…‰å­¦å›¾åƒä¸å¯é¿å…åœ°å—åˆ°æ¨¡ç³Šã€ä½å¯¹æ¯”åº¦å’Œé¢œè‰²å¤±çœŸç­‰å¤šç§é™è´¨å› ç´ çš„å½±å“ï¼Œè¿™äº›å› ç´ å½±å“ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚ç”±äºç¼ºå°‘é…å¯¹çš„æ°´ä¸‹&#x2F;æ¸…æ´å›¾åƒï¼Œå¤§å¤šæ•°ç ”ç©¶æ–¹æ³•é‡‡ç”¨å…ˆå¢å¼ºåæ£€æµ‹çš„ç­–ç•¥ï¼Œå¯¼è‡´ä¸¤ä¸ªå­¦ä¹ ä»»åŠ¡ä¹‹é—´ç¼ºä¹ç‰¹å¾äº¤æµã€‚å¦ä¸€æ–¹é¢ï¼Œç”±äºæ°´ä¸‹å›¾åƒçš„å„ç§é™è´¨å› ç´ ä¸æ ·æœ¬æ•°é‡æœ‰é™çš„çŸ›ç›¾ï¼Œç°æœ‰çš„æ°´ä¸‹å¢å¼ºæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¢å¼ºæœªçŸ¥æ°´ä½“çš„é€€åŒ–å›¾åƒï¼Œä»è€Œé™åˆ¶äº†ç›®æ ‡æ£€æµ‹å‡†ç¡®æ€§çš„æé«˜ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°æ°´ä¸‹ç›®æ ‡æ£€æµ‹ç»“æœä»æ˜¾ç¤ºåœ¨é€€åŒ–å›¾åƒä¸Šï¼Œå¾ˆéš¾ç›´è§‚åœ°åˆ¤æ–­æ£€æµ‹ç»“æœçš„æ­£ç¡®æ€§ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶å¢å¼ºæ°´ä¸‹å›¾åƒå¹¶æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚ä¸å•ä»»åŠ¡å­¦ä¹ ç›¸æ¯”ï¼Œé›†æˆæ¨¡å‹å…è®¸ä¸åŒä»»åŠ¡ä¹‹é—´åŠ¨æ€è°ƒæ•´ä¿¡æ¯äº¤æµå’Œå…±äº«ã€‚ç”±äºçœŸå®æ°´ä¸‹å›¾åƒåªèƒ½æä¾›æ³¨é‡Šçš„ç›®æ ‡æ ‡ç­¾ï¼Œæœ¬æ–‡å¼•å…¥ç‰©ç†çº¦æŸä»¥ç¡®ä¿ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸ä¼šå¹²æ‰°å›¾åƒå¢å¼ºä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªç‰©ç†æ¨¡å—æ¥å°†æ°´ä¸‹å›¾åƒåˆ†è§£ä¸ºæ¸…æ´å›¾åƒã€èƒŒæ™¯å…‰å’Œé€å°„å›¾åƒï¼Œå¹¶ä½¿ç”¨ç‰©ç†æ¨¡å‹è®¡ç®—æ°´ä¸‹å›¾åƒè¿›è¡Œè‡ªç›‘ç£ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¯”è¾ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨è§†è§‰æ€§èƒ½ã€ç›®æ ‡æ£€æµ‹å‡†ç¡®æ€§å’Œæ£€æµ‹æ•ˆç‡æ–¹é¢å–å¾—äº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æŒ‡å‡ºæ°´ä¸‹å…‰å­¦å›¾åƒå­˜åœ¨æ¨¡ç³Šã€ä½å¯¹æ¯”åº¦å’Œè‰²å½©å¤±çœŸç­‰é€€åŒ–é—®é¢˜ï¼Œå½±å“ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ç”±äºæ²¡æœ‰é…å¯¹çš„æ°´ä¸‹&#x2F;æ¸…æ´å›¾åƒï¼Œå¤§å¤šæ•°ç ”ç©¶æ–¹æ³•é‡‡ç”¨å…ˆå¢å¼ºåæ£€æµ‹çš„ç­–ç•¥ï¼Œå¯¼è‡´ä¸¤ä¸ªå­¦ä¹ ä»»åŠ¡ä¹‹é—´ç¼ºä¹ç‰¹å¾äº¤æµã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œå¯åŒæ—¶å¢å¼ºæ°´ä¸‹å›¾åƒå¹¶æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚ä¸å•ä»»åŠ¡å­¦ä¹ ç›¸æ¯”ï¼Œé›†æˆæ¨¡å‹å¯å®ç°ä¸åŒä»»åŠ¡é—´ä¿¡æ¯é€šä¿¡å’Œå…±äº«çš„åŠ¨æ€è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥ç‰©ç†çº¦æŸæ¥ç¡®ä¿ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸ä¸å›¾åƒå¢å¼ºä»»åŠ¡ç›¸äº’å¹²æ‰°ï¼Œå¹¶é‡‡ç”¨ç‰©ç†æ¨¡å—å¯¹æ°´ä¸‹å›¾åƒè¿›è¡Œåˆ†è§£ä¸ºæ¸…æ´å›¾åƒã€èƒŒæ™¯å…‰å’Œé€å°„å›¾åƒï¼Œå¹¶ä½¿ç”¨ç‰©ç†æ¨¡å‹è®¡ç®—æ°´ä¸‹å›¾åƒè¿›è¡Œè‡ªç›‘ç£ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è§†è§‰æ€§èƒ½ã€ç›®æ ‡æ£€æµ‹å‡†ç¡®æ€§å’Œæ£€æµ‹æ•ˆç‡æ–¹é¢å‡å–å¾—ä»¤äººæ»¡æ„çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å…‰å­¦å›¾åƒå­˜åœ¨å¤šç§é€€åŒ–å› ç´ ï¼Œå½±å“ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç¼ºä¹é…å¯¹çš„æ°´ä¸‹&#x2F;æ¸…æ´å›¾åƒä½¿å¾—å…ˆå¢å¼ºåæ£€æµ‹çš„ç­–ç•¥å¯¼è‡´ç‰¹å¾äº¤æµä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œå¯åŒæ—¶å¢å¼ºæ°´ä¸‹å›¾åƒå¹¶æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ä¸å•ä»»åŠ¡å­¦ä¹ ç›¸æ¯”ï¼Œé›†æˆæ¨¡å‹å¯å®ç°ä¿¡æ¯äº¤æµå’Œå…±äº«çš„åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>ä¸ºç¡®ä¿ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸ä¸å›¾åƒå¢å¼ºä»»åŠ¡ç›¸äº’å¹²æ‰°ï¼Œå¼•å…¥ç‰©ç†çº¦æŸã€‚</li>
<li>ä½¿ç”¨ç‰©ç†æ¨¡å—å¯¹æ°´ä¸‹å›¾åƒè¿›è¡Œåˆ†è§£ï¼Œå¹¶é‡‡ç”¨ç‰©ç†æ¨¡å‹è¿›è¡Œè‡ªç›‘ç£è®¡ç®—ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0a4bc765550019f870837334bfb3d671.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a66a00264e4ce1351fb8be0aa0e7144.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd56aeb5ff432f4620b5618db702b72d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-faaa06862cfb11da5e2010eca1e903fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd8ac6b2286c88ad5520ecac049d5a0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-051a84361bcfe14e35223c92159430d1.jpg" align="middle">
</details>




<h2 id="COMPrompter-reconceptualized-segment-anything-model-with-multiprompt-network-for-camouflaged-object-detection"><a href="#COMPrompter-reconceptualized-segment-anything-model-with-multiprompt-network-for-camouflaged-object-detection" class="headerlink" title="COMPrompter: reconceptualized segment anything model with multiprompt   network for camouflaged object detection"></a>COMPrompter: reconceptualized segment anything model with multiprompt   network for camouflaged object detection</h2><p><strong>Authors:Xiaoqin Zhang, Zhenni Yu, Li Zhao, Deng-Ping Fan, Guobao Xiao</strong></p>
<p>We rethink the segment anything model (SAM) and propose a novel multiprompt network called COMPrompter for camouflaged object detection (COD). SAM has zero-shot generalization ability beyond other models and can provide an ideal framework for COD. Our network aims to enhance the single prompt strategy in SAM to a multiprompt strategy. To achieve this, we propose an edge gradient extraction module, which generates a mask containing gradient information regarding the boundaries of camouflaged objects. This gradient mask is then used as a novel boundary prompt, enhancing the segmentation process. Thereafter, we design a box-boundary mutual guidance module, which fosters more precise and comprehensive feature extraction via mutual guidance between a boundary prompt and a box prompt. This collaboration enhances the modelâ€™s ability to accurately detect camouflaged objects. Moreover, we employ the discrete wavelet transform to extract high-frequency features from image embeddings. The high-frequency features serve as a supplementary component to the multiprompt system. Finally, our COMPrompter guides the network to achieve enhanced segmentation results, thereby advancing the development of SAM in terms of COD. Experimental results across COD benchmarks demonstrate that COMPrompter achieves a cutting-edge performance, surpassing the current leading model by an average positive metric of 2.2% in COD10K. In the specific application of COD, the experimental results in polyp segmentation show that our model is superior to top-tier methods as well. The code will be made available at <a target="_blank" rel="noopener" href="https://github.com/guobaoxiao/COMPrompter">https://github.com/guobaoxiao/COMPrompter</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰è¿›è¡Œäº†é‡æ–°æ€è€ƒï¼Œå¹¶é’ˆå¯¹ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰æå‡ºäº†ä¸€ç§æ–°çš„å¤šæç¤ºç½‘ç»œï¼Œåä¸ºCOMPrompterã€‚SAMå…·æœ‰è¶…è¶Šå…¶ä»–æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä¸ºCODæä¾›ç†æƒ³æ¡†æ¶ã€‚æˆ‘ä»¬çš„ç½‘ç»œæ—¨åœ¨å¢å¼ºSAMä¸­çš„å•æç¤ºç­–ç•¥ä¸ºå¤šæç¤ºç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è¾¹ç¼˜æ¢¯åº¦æå–æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”ŸæˆåŒ…å«å…³äºä¼ªè£…ç›®æ ‡è¾¹ç•Œçš„æ¢¯åº¦ä¿¡æ¯çš„æ©è†œã€‚ç„¶åï¼Œå°†è¿™ä¸ªæ¢¯åº¦æ©è†œç”¨ä½œä¸€ä¸ªæ–°çš„è¾¹ç•Œæç¤ºï¼Œä»¥å¢å¼ºåˆ†å‰²è¿‡ç¨‹ã€‚ä¹‹åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç›’å­è¾¹ç•Œç›¸äº’å¼•å¯¼æ¨¡å—ï¼Œé€šè¿‡è¾¹ç•Œæç¤ºå’Œç›’å­æç¤ºä¹‹é—´çš„ç›¸äº’å¼•å¯¼ï¼Œå®ç°æ›´ç²¾ç¡®å’Œå…¨é¢çš„ç‰¹å¾æå–ã€‚è¿™ç§åä½œå¢å¼ºäº†æ¨¡å‹ç²¾ç¡®æ£€æµ‹ä¼ªè£…ç›®æ ‡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¦»æ•£å°æ³¢å˜æ¢ä»å›¾åƒåµŒå…¥ä¸­æå–é«˜é¢‘ç‰¹å¾ã€‚é«˜é¢‘ç‰¹å¾ä½œä¸ºå¤šæç¤ºç³»ç»Ÿçš„è¡¥å……æˆåˆ†ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„COMPrompterå¼•å¯¼ç½‘ç»œå®ç°å¢å¼ºçš„åˆ†å‰²ç»“æœï¼Œä»è€Œä¿ƒè¿›äº†SAMåœ¨CODæ–¹é¢çš„å‘å±•ã€‚åœ¨CODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCOMPrompterè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œåœ¨COD10Kä¸Šçš„å¹³å‡æ­£é¢æŒ‡æ ‡è¶…è¶Šäº†å½“å‰é¢†å…ˆæ¨¡å‹2.2%ã€‚åœ¨CODçš„ç‰¹å®šåº”ç”¨â€”â€”æ¯è‚‰åˆ†å‰²çš„å®éªŒç»“æœä¹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºé¡¶çº§æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/guobaoxiao/COMPrompter%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/guobaoxiao/COMPrompterä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18858v1">PDF</a> SCIENCE CHINA Information Sciences 2024</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¤šæç¤ºç½‘ç»œï¼Œåä¸ºCOMPrompterï¼Œç”¨äºä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰ã€‚è¯¥æ–¹æ³•åœ¨åˆ†æ®µä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆSAMï¼‰çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œé€šè¿‡å¼•å…¥è¾¹ç¼˜æ¢¯åº¦æå–æ¨¡å—å’Œé«˜é¢‘ç‰¹å¾æå–æ–¹æ³•ï¼Œæé«˜äº†ä¼ªè£…ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚COMPrompteråœ¨å¤šä¸ªCODåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾ƒç°æœ‰é¢†å…ˆæ¨¡å‹å¹³å‡æé«˜äº†2.2%çš„é˜³æ€§æŒ‡æ ‡ã€‚åŒæ—¶ï¼Œåœ¨ç‰¹å®šåº”ç”¨å¦‚æ¯è‚‰åˆ†å‰²ä¸­ï¼Œè¯¥æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>COMPrompteræ˜¯ä¸€ç§åŸºäºSAMçš„å¤šæç¤ºç½‘ç»œï¼Œç”¨äºä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>å¼•å…¥è¾¹ç¼˜æ¢¯åº¦æå–æ¨¡å—ï¼Œç”ŸæˆåŒ…å«ç›®æ ‡è¾¹ç•Œæ¢¯åº¦ä¿¡æ¯çš„æ©è†œï¼Œä½œä¸ºæ–°çš„è¾¹ç•Œæç¤ºï¼Œå¢å¼ºåˆ†å‰²è¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡äº†ç›’è¾¹ç•Œç›¸äº’å¼•å¯¼æ¨¡å—ï¼Œé€šè¿‡è¾¹ç•Œæç¤ºå’Œç›’æç¤ºä¹‹é—´çš„ç›¸äº’å¼•å¯¼ï¼Œå®ç°æ›´ç²¾ç¡®å’Œå…¨é¢çš„ç‰¹å¾æå–ã€‚</li>
<li>é‡‡ç”¨ç¦»æ•£å°æ³¢å˜æ¢æå–å›¾åƒåµŒå…¥ä¸­çš„é«˜é¢‘ç‰¹å¾ï¼Œä½œä¸ºå¤šæç¤ºç³»ç»Ÿçš„è¡¥å……ã€‚</li>
<li>COMPrompteræé«˜äº†ä¼ªè£…ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cb5901a053a9ff952fa18babc656f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be88c765a0d1a6218f09d60d5de82174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22e6047b207c2218ae4c4cd0ec19bdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81de9f24c24bcaeca76d0481d3309af1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98affcff6381afc95c6e827f97a3ba8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86ea6558fdc55a88574e3f01ce3bd8ef.jpg" align="middle">
</details>




<h2 id="CrossTracker-Robust-Multi-modal-3D-Multi-Object-Tracking-via-Cross-Correction"><a href="#CrossTracker-Robust-Multi-modal-3D-Multi-Object-Tracking-via-Cross-Correction" class="headerlink" title="CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross   Correction"></a>CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross   Correction</h2><p><strong>Authors:Lipeng Gu, Xuefeng Yan, Weiming Wang, Honghua Chen, Dingkun Zhu, Liangliang Nan, Mingqiang Wei</strong></p>
<p>The fusion of camera- and LiDAR-based detections offers a promising solution to mitigate tracking failures in 3D multi-object tracking (MOT). However, existing methods predominantly exploit camera detections to correct tracking failures caused by potential LiDAR detection problems, neglecting the reciprocal benefit of refining camera detections using LiDAR data. This limitation is rooted in their single-stage architecture, akin to single-stage object detectors, lacking a dedicated trajectory refinement module to fully exploit the complementary multi-modal information. To this end, we introduce CrossTracker, a novel two-stage paradigm for online multi-modal 3D MOT. CrossTracker operates in a coarse-to-fine manner, initially generating coarse trajectories and subsequently refining them through an independent refinement process. Specifically, CrossTracker incorporates three essential modules: i) a multi-modal modeling (M^3) module that, by fusing multi-modal information (images, point clouds, and even plane geometry extracted from images), provides a robust metric for subsequent trajectory generation. ii) a coarse trajectory generation (C-TG) module that generates initial coarse dual-stream trajectories, and iii) a trajectory refinement (TR) module that refines coarse trajectories through cross correction between camera and LiDAR streams. Comprehensive experiments demonstrate the superior performance of our CrossTracker over its eighteen competitors, underscoring its effectiveness in harnessing the synergistic benefits of camera and LiDAR sensors for robust multi-modal 3D MOT. </p>
<blockquote>
<p>èåˆç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ£€æµ‹ä¸ºè§£å†³ä¸‰ç»´å¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰ä¸­çš„è·Ÿè¸ªå¤±è´¥é—®é¢˜æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨ç›¸æœºæ£€æµ‹æ¥çº æ­£ç”±æ½œåœ¨çš„æ¿€å…‰é›·è¾¾æ£€æµ‹é—®é¢˜å¼•èµ·çš„è·Ÿè¸ªå¤±è´¥ï¼Œå¿½ç•¥äº†åˆ©ç”¨æ¿€å…‰é›·è¾¾æ•°æ®ä¼˜åŒ–ç›¸æœºæ£€æµ‹çš„äº’æƒ æ•ˆç›Šã€‚è¿™ä¸€å±€é™æ€§æºäºå…¶ç±»ä¼¼äºå•é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨çš„å•ä¸€é˜¶æ®µæ¶æ„ï¼Œç¼ºä¹ä¸“é—¨çš„è½¨è¿¹ä¼˜åŒ–æ¨¡å—ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨äº’è¡¥çš„å¤šæ¨¡å¼ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CrossTrackerï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåœ¨çº¿å¤šæ¨¡å¼ä¸‰ç»´MOTçš„æ–°å‹ä¸¤é˜¶æ®µèŒƒå¼ã€‚CrossTrackeræŒ‰ç”±ç²—åˆ°ç»†çš„æ–¹å¼è¿è¡Œï¼Œé¦–å…ˆç”Ÿæˆç²—ç•¥è½¨è¿¹ï¼Œç„¶åé€šè¿‡ç‹¬ç«‹çš„ä¼˜åŒ–è¿‡ç¨‹å¯¹å…¶è¿›è¡Œç»†åŒ–ã€‚å…·ä½“æ¥è¯´ï¼ŒCrossTrackeråŒ…å«äº†ä¸‰ä¸ªåŸºæœ¬æ¨¡å—ï¼šiï¼‰å¤šæ¨¡å¼å»ºæ¨¡ï¼ˆM^3ï¼‰æ¨¡å—ï¼Œå®ƒé€šè¿‡èåˆå¤šæ¨¡å¼ä¿¡æ¯ï¼ˆå›¾åƒã€ç‚¹äº‘ä»¥åŠç”šè‡³ä»å›¾åƒä¸­æå–çš„å¹³é¢å‡ ä½•ï¼‰ï¼Œä¸ºåç»­è½¨è¿¹ç”Ÿæˆæä¾›ç¨³å¥çš„åº¦é‡æ ‡å‡†ã€‚iiï¼‰ç²—ç•¥è½¨è¿¹ç”Ÿæˆï¼ˆC-TGï¼‰æ¨¡å—ï¼Œç”¨äºç”Ÿæˆåˆå§‹çš„ç²—ç•¥åŒæµè½¨è¿¹ï¼›iiiï¼‰è½¨è¿¹ä¼˜åŒ–ï¼ˆTRï¼‰æ¨¡å—ï¼Œé€šè¿‡ç›¸æœºå’Œæ¿€å…‰é›·è¾¾æµä¹‹é—´çš„äº¤å‰æ ¡æ­£æ¥ä¼˜åŒ–ç²—ç•¥è½¨è¿¹ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CrossTrackeråœ¨18ä¸ªç«äº‰å¯¹æ‰‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨åˆ©ç”¨ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨çš„ååŒä¼˜åŠ¿è¿›è¡Œç¨³å¥å¤šæ¨¡å¼ä¸‰ç»´MOTæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18850v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨æ¨¡æ€è¿½è¸ªï¼ˆCrossTrackerï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åœ¨çº¿å¤šæ¨¡æ€ä¸‰ç»´å¤šç›®æ ‡è¿½è¸ªï¼ˆ3D MOTï¼‰çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚å®ƒé€šè¿‡èåˆç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ•°æ®ï¼Œæ—¨åœ¨è§£å†³è¿½è¸ªå¤±è´¥çš„é—®é¢˜ã€‚CrossTrackeré€šè¿‡ç²—åˆ°ç»†çš„æµç¨‹æ“ä½œï¼Œé¦–å…ˆç”Ÿæˆç²—ç•¥è½¨è¿¹ï¼Œç„¶åé€šè¿‡ç‹¬ç«‹çš„ç»†åŒ–è¿‡ç¨‹è¿›è¡Œæ”¹è¿›ã€‚å®éªŒè¡¨æ˜ï¼ŒCrossTrackeråœ¨åå…«ä¸ªç«äº‰å¯¹æ‰‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆå®ç°äº†ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨çš„ååŒä¼˜åŠ¿ï¼Œä¸ºç¨³å¥çš„å¤šæ¨¡æ€3D MOTæä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨æ¨¡æ€è¿½è¸ªæŠ€æœ¯é€šè¿‡èåˆç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ£€æµ‹ï¼Œå¼¥è¡¥äº†å•ä¸€ä¼ æ„Ÿå™¨è¿½è¸ªçš„ç¼ºé™·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç›¸æœºæ£€æµ‹æ¥çº æ­£ç”±æ½œåœ¨æ¿€å…‰é›·è¾¾æ£€æµ‹é—®é¢˜å¼•èµ·çš„è·Ÿè¸ªå¤±è´¥ï¼Œè€ŒCrossTrackeråˆ©ç”¨åŒå‘ä¼˜åŠ¿ï¼Œå³ä½¿ç”¨æ¿€å…‰é›·è¾¾æ•°æ®æ¥ä¼˜åŒ–ç›¸æœºæ£€æµ‹ã€‚</li>
<li>CrossTrackerå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä»¥åœ¨çº¿å¤šæ¨¡æ€3D MOTä¸ºç›®æ ‡ã€‚</li>
<li>è¯¥æŠ€æœ¯é€šè¿‡ç²—åˆ°ç»†çš„æµç¨‹æ“ä½œï¼Œé¦–å…ˆç”Ÿæˆç²—ç•¥è½¨è¿¹ï¼Œç„¶åé€šè¿‡ç‹¬ç«‹çš„ç»†åŒ–è¿‡ç¨‹è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>CrossTrackeråŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå¤šæ¨¡æ€å»ºæ¨¡ï¼ˆM^3ï¼‰ã€ç²—è½¨è¿¹ç”Ÿæˆï¼ˆC-TGï¼‰å’Œè½¨è¿¹ç»†åŒ–ï¼ˆTRï¼‰ã€‚</li>
<li>M^3æ¨¡å—é€šè¿‡èåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå›¾åƒã€ç‚¹äº‘ç”šè‡³ä»å›¾åƒä¸­æå–çš„å¹³é¢å‡ ä½•ï¼‰ï¼Œä¸ºåç»­è½¨è¿¹ç”Ÿæˆæä¾›ç¨³å¥æŒ‡æ ‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5aedcf475c2848ececf87604a6d85e99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fa9481c4875224b2580cc6dab54d5c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aef37134713d1d8a1fc1934a7d9ab7d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de72a62383d1201c7b9279b22efaf41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5164aab94bb9c55cc4fe8df265f9f145.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d531322859dcb0f3d2f549e8173aa02f.jpg" align="middle">
</details>




<h2 id="Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT"><a href="#Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT" class="headerlink" title="Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT"></a>Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT</h2><p><strong>Authors:Zi Li, Ying Chen, Zeli Chen, Yanzhou Su, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Yunhai Bai, Zhinlin Zheng, Le Lu, Yirui Wang, Jia Ge, Xianghua Ye, Senxiang Yan, Dakai Jin</strong></p>
<p>In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians typically delineate the gross tumor volume (GTV) using non-contrast planning computed tomography to ensure accurate radiation dose delivery. However, the low contrast between tumors and adjacent normal tissues necessitates that radiation oncologists manually delineate the tumors, often relying on diagnostic MRI for guidance. % In this study, we propose a novel approach to directly segment NPC gross tumors on non-contrast planning CT images, circumventing potential registration errors when aligning MRI or MRI-derived tumor masks to planning CT. To address the low contrast issues between tumors and adjacent normal structures in planning CT, we introduce a 3D Semantic Asymmetry Tumor segmentation (SATs) method. Specifically, we posit that a healthy nasopharyngeal region is characteristically bilaterally symmetric, whereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then, we propose a Siamese contrastive learning segmentation framework that minimizes the voxel-wise distance between original and flipped areas without tumor and encourages a larger distance between original and flipped areas with tumor. Thus, our approach enhances the sensitivity of features to semantic asymmetries. % Extensive experiments demonstrate that the proposed SATs achieves the leading NPC GTV segmentation performance in both internal and external testing, \emph{e.g.}, with at least 2% absolute Dice score improvement and 12% average distance error reduction when compared to other state-of-the-art methods in the external testing. </p>
<blockquote>
<p>åœ¨é¼»å’½ç™Œï¼ˆNPCï¼‰çš„æ”¾å°„æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œä¸´åºŠåŒ»ç”Ÿé€šå¸¸ä½¿ç”¨éå¯¹æ¯”è®¡åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆplanning CTï¼‰æ¥åˆ’å®šå¤§ä½“è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVï¼‰ï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„è¾å°„å‰‚é‡ä¼ é€’ã€‚ç„¶è€Œï¼Œè‚¿ç˜¤ä¸ç›¸é‚»æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¿«ä½¿æ”¾ç–—ç§‘åŒ»ç”Ÿæ‰‹åŠ¨åˆ’å®šè‚¿ç˜¤ï¼Œé€šå¸¸ä¾èµ–è¯Šæ–­ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œå¼•å¯¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨æ— éœ€å¯¹æ¯”å‰‚çš„éå¯¹æ¯”è®¡åˆ’CTå›¾åƒä¸Šç›´æ¥åˆ†å‰²é¼»å’½ç™Œå¤§ä½“è‚¿ç˜¤çš„æ–°æ–¹æ³•ï¼Œé¿å…äº†å°†MRIæˆ–åŸºäºMRIçš„è‚¿ç˜¤æ©è†œä¸è®¡åˆ’CTå¯¹é½æ—¶å¯èƒ½å‡ºç°çš„æ³¨å†Œè¯¯å·®ã€‚ä¸ºäº†è§£å†³åœ¨è®¡åˆ’CTä¸­è‚¿ç˜¤ä¸ç›¸é‚»æ­£å¸¸ç»“æ„ä¹‹é—´çš„ä½å¯¹æ¯”åº¦é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç»´è¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¤ä¸ºå¥åº·çš„å’½è…”åŒºåŸŸå…·æœ‰å…¸å‹çš„åŒä¾§å¯¹ç§°æ€§ï¼Œè€Œé¼»å’½ç™Œçš„å‡ºç°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–åŸå§‹å’Œç¿»è½¬åŒºåŸŸï¼ˆæ— è‚¿ç˜¤ï¼‰ä¹‹é—´çš„ä½“ç´ è·ç¦»ä»¥åŠé¼“åŠ±åŸå§‹å’Œç¿»è½¬åŒºåŸŸï¼ˆæœ‰è‚¿ç˜¤ï¼‰ä¹‹é—´çš„è·ç¦»æ¥å¢å¼ºç‰¹å¾å¯¹è¯­ä¹‰ä¸å¯¹ç§°çš„æ•æ„Ÿæ€§ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SATsåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„NPC GTVåˆ†å‰²æ€§èƒ½ï¼Œä¾‹å¦‚ä¸å¤–éƒ¨æµ‹è¯•ä¸­çš„å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè‡³å°‘æé«˜äº†2ï¼…çš„ç»å¯¹Diceå¾—åˆ†å¹¶å‡å°‘äº†12ï¼…çš„å¹³å‡è·ç¦»è¯¯å·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18290v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåœ¨é¼»å’½ç™Œæ”¾å°„æ²»ç–—é¢†åŸŸï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³åŸºäºéå¯¹æ¯”å‰‚è§„åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç›´æ¥åˆ†å‰²é¼»å’½ç™Œè‚¿ç˜¤ä½“ç§¯ï¼ˆGTVï¼‰ã€‚é€šè¿‡åˆ©ç”¨å¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰åŒä¾§å¯¹ç§°æ€§çš„ç‰¹å¾ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ä¸‰ç»´è¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•æ¥è§£å†³è‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦ä½çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç¼©å°æ— è‚¿ç˜¤åŒºåŸŸçš„åŸå§‹å’Œç¿»è½¬åŒºåŸŸä¹‹é—´çš„åƒç´ è·ç¦»ï¼Œå¹¶é¼“åŠ±æœ‰è‚¿ç˜¤åŒºåŸŸçš„åŸå§‹å’Œç¿»è½¬åŒºåŸŸä¹‹é—´çš„è·ç¦»æ›´å¤§ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œæœ¬ç ”ç©¶æå‡ºçš„SATsæ–¹æ³•åœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„NPC GTVåˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºéå¯¹æ¯”å‰‚è§„åˆ’CTç›´æ¥åˆ†å‰²é¼»å’½ç™Œè‚¿ç˜¤ä½“ç§¯çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ä¸‰ç»´è¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•æ¥è§£å†³è‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦ä½çš„é—®é¢˜ã€‚</li>
<li>SATsæ–¹æ³•åˆ©ç”¨äº†å¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰çš„åŒä¾§å¯¹ç§°æ€§ç‰¹å¾æ¥åŒºåˆ†è‚¿ç˜¤å’Œæ­£å¸¸ç»„ç»‡ã€‚</li>
<li>ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶æ¥å¢å¼ºç‰¹å¾çš„æ•æ„Ÿæ€§ã€‚</li>
<li>SATsæ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¦‚åœ¨å¤–éƒ¨æµ‹è¯•ä¸­å®ç°äº†è‡³å°‘2%çš„ç»å¯¹Diceå¾—åˆ†æé«˜å’Œå¹³å‡è·ç¦»è¯¯å·®å‡å°‘12%ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f422acb9ddc4a17e60e824344e0249a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c089bfa0d3e790c85247a6e3069f72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-926a9a297928d6bee60ef5c7e826c7dd.jpg" align="middle">
</details>




<h2 id="Generative-Semantic-Communication-for-Joint-Image-Transmission-and-Segmentation"><a href="#Generative-Semantic-Communication-for-Joint-Image-Transmission-and-Segmentation" class="headerlink" title="Generative Semantic Communication for Joint Image Transmission and   Segmentation"></a>Generative Semantic Communication for Joint Image Transmission and   Segmentation</h2><p><strong>Authors:Weiwen Yuan, Jinke Ren, Chongjie Wang, Ruichen Zhang, Jun Wei, Dong In Kim, Shuguang Cui</strong></p>
<p>Semantic communication has emerged as a promising technology for enhancing communication efficiency. However, most existing research emphasizes single-task reconstruction, neglecting model adaptability and generalization across multi-task systems. In this paper, we propose a novel generative semantic communication system that supports both image reconstruction and segmentation tasks. Our approach builds upon semantic knowledge bases (KBs) at both the transmitter and receiver, with each semantic KB comprising a source KB and a task KB. The source KB at the transmitter leverages a hierarchical Swin-Transformer, a generative AI scheme, to extract multi-level features from the input image. Concurrently, the counterpart source KB at the receiver utilizes hierarchical residual blocks to generate task-specific knowledge. Furthermore, the two task KBs adopt a semantic similarity model to map different task requirements into pre-defined task instructions, thereby facilitating the feature selection of the source KBs. Additionally, we develop a unified residual block-based joint source and channel (JSCC) encoder and two task-specific JSCC decoders to achieve the two image tasks. In particular, a generative diffusion model is adopted to construct the JSCC decoder for the image reconstruction task. Experimental results demonstrate that our multi-task generative semantic communication system outperforms previous single-task communication systems in terms of peak signal-to-noise ratio and segmentation accuracy. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡ä½œä¸ºä¸€ç§æé«˜é€šä¿¡æ•ˆç‡çš„æœ‰å‰é€”çš„æŠ€æœ¯å·²ç»å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶å¼ºè°ƒå•ä»»åŠ¡é‡å»ºï¼Œå¿½ç•¥äº†æ¨¡å‹åœ¨å¤šä»»åŠ¡ç³»ç»Ÿä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¯æŒå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡çš„æ–°å‹ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºå‘å°„å™¨å’Œæ¥æ”¶å™¨åŒæ–¹çš„è¯­ä¹‰çŸ¥è¯†åº“ï¼ˆKBsï¼‰ï¼Œæ¯ä¸ªè¯­ä¹‰çŸ¥è¯†åº“åŒ…æ‹¬æºçŸ¥è¯†åº“å’Œä»»åŠ¡çŸ¥è¯†åº“ã€‚å‘å°„å™¨ç«¯çš„æºçŸ¥è¯†åº“åˆ©ç”¨åˆ†å±‚Swin-Transformerï¼ˆä¸€ç§ç”Ÿæˆäººå·¥æ™ºèƒ½æ–¹æ¡ˆï¼‰ä»è¾“å…¥å›¾åƒä¸­æå–å¤šå±‚æ¬¡ç‰¹å¾ã€‚åŒæ—¶ï¼Œæ¥æ”¶å™¨ç«¯çš„å¯¹åº”æºçŸ¥è¯†åº“åˆ©ç”¨åˆ†å±‚æ®‹å·®å—ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªä»»åŠ¡çŸ¥è¯†åº“é‡‡ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§æ¨¡å‹ï¼Œå°†ä¸åŒçš„ä»»åŠ¡è¦æ±‚æ˜ å°„åˆ°é¢„å®šä¹‰çš„ä»»åŠ¡æŒ‡ä»¤ä¸­ï¼Œä»è€Œä¿ƒè¿›æºçŸ¥è¯†åº“çš„ç‰¹å¾é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºç»Ÿä¸€æ®‹å·®å—çš„è”åˆæºä¿¡é“ï¼ˆJSCCï¼‰ç¼–ç å™¨ï¼Œä»¥åŠä¸¤ä¸ªé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„JSCCè§£ç å™¨ï¼Œä»¥å®ç°ä¸¤ä¸ªå›¾åƒä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ„å»ºå›¾åƒé‡å»ºä»»åŠ¡çš„JSCCè§£ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šä»»åŠ¡ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿç›¸å¯¹äºä¹‹å‰çš„å•ä»»åŠ¡é€šä¿¡ç³»ç»Ÿåœ¨å³°å€¼ä¿¡å™ªæ¯”å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18005v1">PDF</a> 6 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¯æŒå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡çš„å¤šä»»åŠ¡ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿã€‚è¯¥ç³»ç»ŸåŸºäºå‘é€å™¨å’Œæ¥æ”¶å™¨ç«¯çš„è¯­ä¹‰çŸ¥è¯†åº“ï¼Œåˆ©ç”¨ç”Ÿæˆäººå·¥æ™ºèƒ½æ–¹æ¡ˆæå–è¾“å…¥å›¾åƒçš„å¤šå±‚æ¬¡ç‰¹å¾ï¼Œå¹¶å®ç°ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„ç”Ÿæˆã€‚é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§æ¨¡å‹æ˜ å°„ä¸åŒä»»åŠ¡è¦æ±‚ï¼Œå¹¶å¼€å‘ç»Ÿä¸€çš„è”åˆæºä¿¡é“ç¼–ç å™¨ä¸ä¸¤ä¸ªä»»åŠ¡ç‰¹å®šè§£ç å™¨ï¼Œå®ç°å›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å³°å€¼ä¿¡å™ªæ¯”å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢ä¼˜äºä»¥å‰çš„å•ä»»åŠ¡é€šä¿¡ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰é€šä¿¡æŠ€æœ¯èƒ½æé«˜é€šä¿¡æ•ˆç‡ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•ä»»åŠ¡é‡å»ºï¼Œå¿½è§†äº†æ¨¡å‹åœ¨å¤šä»»åŠ¡ç³»ç»Ÿä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆè¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œæ”¯æŒå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ç³»ç»Ÿåˆ©ç”¨è¯­ä¹‰çŸ¥è¯†åº“æ¥æå–è¾“å…¥å›¾åƒçš„å¤šå±‚æ¬¡ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆä»»åŠ¡ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§æ¨¡å‹æ˜ å°„ä¸åŒä»»åŠ¡è¦æ±‚ï¼Œä¿ƒè¿›ç‰¹å¾é€‰æ‹©ã€‚</li>
<li>å¼€å‘äº†è”åˆæºå’Œä¿¡é“ç¼–ç å™¨åŠä¸¤ä¸ªä»»åŠ¡ç‰¹å®šè§£ç å™¨ï¼Œç”¨äºå®ç°ä¸¤ä¸ªå›¾åƒä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ„å»ºå›¾åƒé‡å»ºä»»åŠ¡çš„è§£ç å™¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beacd3461e0d90c9aad45dd16b50d4bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45ea3a224092c63156c0436d8bb93197.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9be89710eb92b3c7aa14e0984621699c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efb46757bcd6ded2369db30f40304e60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-468e4549274d4493a1f3cf2c2a61faa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23bae93e37870510ddc53d01e9a1d535.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-966ac4dc144021c7eaa2344d8573ae90.jpg" align="middle">
</details>




<h2 id="A-Distractor-Aware-Memory-for-Visual-Object-Tracking-with-SAM2"><a href="#A-Distractor-Aware-Memory-for-Visual-Object-Tracking-with-SAM2" class="headerlink" title="A Distractor-Aware Memory for Visual Object Tracking with SAM2"></a>A Distractor-Aware Memory for Visual Object Tracking with SAM2</h2><p><strong>Authors:Jovana Videnovic, Alan Lukezic, Matej Kristan</strong></p>
<p>Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames. While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community. Nevertheless, modern trackers still struggle in the presence of distractors. We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness. The resulting tracker is denoted as SAM2.1++. We also propose a new distractor-distilled DiDi dataset to study the distractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them. </p>
<blockquote>
<p>åŸºäºå†…å­˜çš„è·Ÿè¸ªå™¨æ˜¯ä¸€ç§è§†é¢‘ç›®æ ‡åˆ†å‰²æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†è¿‘æ¥è·Ÿè¸ªçš„å¸§æ‹¼æ¥åˆ°å†…å­˜ç¼“å†²åŒºä¸­å¹¶å½¢æˆç›®æ ‡æ¨¡å‹ï¼Œç„¶åé€šè¿‡å…³æ³¨å½“å‰å›¾åƒä¸ç¼“å†²å¸§æ¥å®šä½ç›®æ ‡ã€‚å°½ç®¡åŸºäºå†…å­˜çš„è·Ÿè¸ªå™¨å·²åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†é¡¶å°–æ€§èƒ½ï¼Œä½†SAM2çš„æœ€è¿‘å‘å¸ƒè¿˜æ˜¯ä½¿å…¶æˆä¸ºè§†è§‰ç›®æ ‡è·Ÿè¸ªç¤¾åŒºçš„å…³æ³¨ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œåœ¨ç°ä»£è·Ÿè¸ªå™¨é¢å‰ï¼Œå¹²æ‰°ç‰©çš„å­˜åœ¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¤ä¸ºéœ€è¦ä¸€ä¸ªæ›´å¤æ‚çš„å†…å­˜æ¨¡å‹ï¼Œå› æ­¤ä¸ºSAM2æå‡ºäº†ä¸€ä¸ªæ–°çš„å¹²æ‰°ç‰©æ„ŸçŸ¥å†…å­˜æ¨¡å‹ï¼Œä»¥åŠä¸€ç§åŸºäºè‡ªçœç­–ç•¥çš„æ›´æ–°ç­–ç•¥ï¼Œå…±åŒè§£å†³åˆ†å‰²ç²¾åº¦å’Œè·Ÿè¸ªç¨³å¥æ€§é—®é¢˜ã€‚æ‰€å¾—çš„è·Ÿè¸ªå™¨è¢«æ ‡è®°ä¸ºSAM2.1 ++ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å¹²æ‰°ç‰©ç²¾ç‚¼æ•°æ®é›†DiDiï¼Œä»¥æ›´å¥½åœ°ç ”ç©¶å¹²æ‰°ç‰©é—®é¢˜ã€‚SAM2.1 ++ åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†SAM2.1å’Œç›¸å…³çš„SAMå†…å­˜æ‰©å±•ï¼Œå¹¶åœ¨å…¶ä¸­å…­ä¸ªä¸Šåˆ›ä¸‹äº†æœ€æ–°çš„å…ˆè¿›æŠ€æœ¯æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17576v2">PDF</a> Under review. Code available on Github:   <a target="_blank" rel="noopener" href="https://github.com/jovanavidenovic/DAM4SAM">https://github.com/jovanavidenovic/DAM4SAM</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå†…å­˜çš„è·Ÿè¸ªå™¨æ˜¯ä¸€ç§è§†é¢‘ç›®æ ‡åˆ†å‰²æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿æ¥æœ€è¿‘è·Ÿè¸ªçš„å¸§åˆ°å†…å­˜ç¼“å†²åŒºæ¥å½¢æˆç›®æ ‡æ¨¡å‹ï¼Œå¹¶é€šè¿‡å…³æ³¨å½“å‰å›¾åƒå’Œç¼“å†²å¸§æ¥å®šä½ç›®æ ‡ã€‚å°½ç®¡å·²ç»åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¡¶å°–æ€§èƒ½ï¼Œä½†æœ€è¿‘SAM2çš„å‘å¸ƒä½¿åŸºäºå†…å­˜çš„è·Ÿè¸ªå™¨æˆä¸ºè§†è§‰ç›®æ ‡è·Ÿè¸ªç¤¾åŒºçš„å…³æ³¨ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œç°ä»£è·Ÿè¸ªå™¨åœ¨å­˜åœ¨å¹²æ‰°ç‰©æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¹²æ‰°ç‰©æ„ŸçŸ¥å†…å­˜æ¨¡å‹ç”¨äºSAM2ï¼Œä»¥åŠä¸€ç§åŸºäºå†…çœçš„æ›´æ–°ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³åˆ†å‰²ç²¾åº¦å’Œè·Ÿè¸ªç¨³å¥æ€§é—®é¢˜ã€‚æ‰€å¾—è·Ÿè¸ªå™¨ç§°ä¸ºSAM2.1++ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„å¹²æ‰°ç‰©è’¸é¦DiDiæ•°æ®é›†ï¼Œä»¥æ›´å¥½åœ°ç ”ç©¶å¹²æ‰°ç‰©é—®é¢˜ã€‚SAM2.1++åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†SAM2.1å’Œç›¸å…³SAMå†…å­˜æ‰©å±•ï¼Œå¹¶åœ¨å…¶ä¸­å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå†…å­˜çš„è·Ÿè¸ªå™¨é€šè¿‡è¿æ¥æœ€è¿‘è·Ÿè¸ªçš„å¸§åˆ°å†…å­˜ç¼“å†²åŒºå½¢æˆç›®æ ‡æ¨¡å‹ï¼Œå¹¶å®šä½ç›®æ ‡ã€‚</li>
<li>SAM2çš„å‘å¸ƒä½¿åŸºäºå†…å­˜çš„è·Ÿè¸ªå™¨æˆä¸ºè§†è§‰ç›®æ ‡è·Ÿè¸ªçš„ç„¦ç‚¹ã€‚</li>
<li>ç°ä»£è·Ÿè¸ªå™¨åœ¨å­˜åœ¨å¹²æ‰°ç‰©æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¹²æ‰°ç‰©æ„ŸçŸ¥å†…å­˜æ¨¡å‹ç”¨äºSAM2ï¼Œè§£å†³åˆ†å‰²ç²¾åº¦å’Œè·Ÿè¸ªç¨³å¥æ€§é—®é¢˜ã€‚</li>
<li>SAM2.1++æ€§èƒ½è¶…è¶Šäº†SAM2.1å’Œç›¸å…³SAMå†…å­˜æ‰©å±•ã€‚</li>
<li>SAM2.1++åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd4db80d39f7c70a45983fd86497eed8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91603742efec4b0dda214b22a16bb972.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3030961b168801bccf29622be2c80ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2b99f61dd49e9fa41979dee97105fe4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43f465177f25aa7798513b2a3cedabad.jpg" align="middle">
</details>




<h2 id="Self-supervised-Video-Instance-Segmentation-Can-Boost-Geographic-Entity-Alignment-in-Historical-Maps"><a href="#Self-supervised-Video-Instance-Segmentation-Can-Boost-Geographic-Entity-Alignment-in-Historical-Maps" class="headerlink" title="Self-supervised Video Instance Segmentation Can Boost Geographic Entity   Alignment in Historical Maps"></a>Self-supervised Video Instance Segmentation Can Boost Geographic Entity   Alignment in Historical Maps</h2><p><strong>Authors:Xue Xia, Randall Balestriero, Tao Zhang, Lorenz Hurni</strong></p>
<p>Tracking geographic entities from historical maps, such as buildings, offers valuable insights into cultural heritage, urbanization patterns, environmental changes, and various historical research endeavors. However, linking these entities across diverse maps remains a persistent challenge for researchers. Traditionally, this has been addressed through a two-step process: detecting entities within individual maps and then associating them via a heuristic-based post-processing step. In this paper, we propose a novel approach that combines segmentation and association of geographic entities in historical maps using video instance segmentation (VIS). This method significantly streamlines geographic entity alignment and enhances automation. However, acquiring high-quality, video-format training data for VIS models is prohibitively expensive, especially for historical maps that often contain hundreds or thousands of geographic entities. To mitigate this challenge, we explore self-supervised learning (SSL) techniques to enhance VIS performance on historical maps. We evaluate the performance of VIS models under different pretraining configurations and introduce a novel method for generating synthetic videos from unlabeled historical map images for pretraining. Our proposed self-supervised VIS method substantially reduces the need for manual annotation. Experimental results demonstrate the superiority of the proposed self-supervised VIS approach, achieving a 24.9% improvement in AP and a 0.23 increase in F1 score compared to the model trained from scratch. </p>
<blockquote>
<p>ä»å†å²åœ°å›¾ä¸­è¿½è¸ªåœ°ç†å®ä½“ï¼Œå¦‚å»ºç­‘ç­‰ï¼Œä¸ºæ–‡åŒ–é—äº§ã€åŸå¸‚åŒ–æ¨¡å¼ã€ç¯å¢ƒå˜åŒ–ä»¥åŠå„ç§å†å²ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨ä¸åŒçš„åœ°å›¾ä¹‹é—´å°†è¿™äº›å®ä½“å…³è”èµ·æ¥ä¸€ç›´æ˜¯ç ”ç©¶äººå‘˜é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™é€šè¿‡ä¸¤æ­¥è¿‡ç¨‹æ¥è§£å†³ï¼šå…ˆåœ¨å•ä¸ªåœ°å›¾å†…æ£€æµ‹å®ä½“ï¼Œç„¶åé€šè¿‡åŸºäºå¯å‘å¼çš„åå¤„ç†æ­¥éª¤å°†å®ƒä»¬å…³è”èµ·æ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å†å²åœ°å›¾ä¸­çš„åœ°ç†å®ä½“åˆ†å‰²å’Œå…³è”ï¼Œä½¿ç”¨è§†é¢‘å®ä¾‹åˆ†å‰²ï¼ˆVISï¼‰æŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•æå¤§åœ°ç®€åŒ–äº†åœ°ç†å®ä½“çš„å¯¹é½è¿‡ç¨‹ï¼Œæé«˜äº†è‡ªåŠ¨åŒ–ç¨‹åº¦ã€‚ç„¶è€Œï¼Œä¸ºVISæ¨¡å‹è·å–é«˜è´¨é‡çš„è§†é¢‘æ ¼å¼è®­ç»ƒæ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œå°¤å…¶æ˜¯å†å²åœ°å›¾é€šå¸¸åŒ…å«æ•°ç™¾æˆ–æ•°åƒä¸ªåœ°ç†å®ä½“ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æŠ€æœ¯ï¼Œä»¥æé«˜å†å²åœ°å›¾ä¸Šçš„VISæ€§èƒ½ã€‚æˆ‘ä»¬è¯„ä¼°äº†VISæ¨¡å‹åœ¨ä¸åŒé¢„è®­ç»ƒé…ç½®ä¸‹çš„æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ä»æ— æ ‡ç­¾çš„å†å²åœ°å›¾å›¾åƒç”Ÿæˆåˆæˆè§†é¢‘çš„æ–°æ–¹æ³•ï¼Œç”¨äºé¢„è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºçš„è‡ªç›‘ç£VISæ–¹æ³•å¤§å¤§é™ä½äº†å¯¹æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„è‡ªç›‘ç£VISæ–¹æ³•åœ¨APä¸Šæé«˜äº†24.9%ï¼ŒF1åˆ†æ•°æé«˜äº†0.23ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17425v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨è§†é¢‘å®ä¾‹åˆ†å‰²æŠ€æœ¯ç»“åˆå†å²åœ°å›¾ä¸­çš„åœ°ç†å®ä½“åˆ†å‰²ä¸å…³è”ï¼Œä¸ºæ–‡åŒ–ç»§æ‰¿ã€åŸå¸‚åŒ–æ¨¡å¼ã€ç¯å¢ƒå˜åŒ–ç­‰é¢†åŸŸæä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚é’ˆå¯¹å†å²åœ°å›¾ä¸­å®ä½“å¯¹é½çš„éš¾é¢˜ï¼Œæå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯æé«˜è§†é¢‘å®ä¾‹åˆ†å‰²æ€§èƒ½ï¼Œå‡å°‘äººå·¥æ ‡æ³¨éœ€æ±‚ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å†å²åœ°å›¾ä¸­çš„åœ°ç†å®ä½“è·Ÿè¸ªå¯¹äºæ–‡åŒ–ç ”ç©¶ã€åŸå¸‚è§„åˆ’ç­‰é¢†åŸŸå…·æœ‰ä»·å€¼ã€‚</li>
<li>ä¼ ç»Ÿçš„åœ°ç†å®ä½“å…³è”æ–¹æ³•éœ€è¦ä¸¤æ­¥å¤„ç†ï¼Œå­˜åœ¨æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>è§†é¢‘å®ä¾‹åˆ†å‰²æŠ€æœ¯è¢«åº”ç”¨äºå†å²åœ°å›¾çš„åœ°ç†å®ä½“åˆ†å‰²ä¸å…³è”ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>è·å–é«˜è´¨é‡çš„è§†é¢‘æ ¼å¼è®­ç»ƒæ•°æ®å¯¹äºVISæ¨¡å‹æ¥è¯´æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯è§£å†³VISæ¨¡å‹è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¯¹ä¸åŒé¢„è®­ç»ƒé…ç½®è¿›è¡Œè¯„ä¼°ï¼Œæå‡ºäº†æ–°çš„æ–¹æ³•ç”Ÿæˆåˆæˆè§†é¢‘ç”¨äºé¢„è®­ç»ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7a83ef339bc649055b4bd1f01c49340.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5d89ba3c043cd3c4bfd8049d075987d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11656420d73b820a77145cac102c9849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b7b0cfdca462dcc434646f365792554.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-758b616cd5fed999a635a8a1f019019d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-012da1c6a9ed84ab263df2ab4b9ed8b5.jpg" align="middle">
</details>




<h2 id="MRIFE-A-Mask-Recovering-and-Interactive-Feature-Enhancing-Semantic-Segmentation-Network-For-Relic-Landslide-Detection"><a href="#MRIFE-A-Mask-Recovering-and-Interactive-Feature-Enhancing-Semantic-Segmentation-Network-For-Relic-Landslide-Detection" class="headerlink" title="MRIFE: A Mask-Recovering and Interactive-Feature-Enhancing Semantic   Segmentation Network For Relic Landslide Detection"></a>MRIFE: A Mask-Recovering and Interactive-Feature-Enhancing Semantic   Segmentation Network For Relic Landslide Detection</h2><p><strong>Authors:Juefei He, Yuexing Peng, Wei Li, Junchuan Yu, Daqing Ge, Wei Xiang</strong></p>
<p>Relic landslide, formed over a long period, possess the potential for reactivation, making them a hazardous geological phenomenon. While reliable relic landslide detection benefits the effective monitoring and prevention of landslide disaster, semantic segmentation using high-resolution remote sensing images for relic landslides faces many challenges, including the object visual blur problem, due to the changes of appearance caused by prolonged natural evolution and human activities, and the small-sized dataset problem, due to difficulty in recognizing and labelling the samples. To address these challenges, a semantic segmentation model, termed mask-recovering and interactive-feature-enhancing (MRIFE), is proposed for more efficient feature extraction and separation. Specifically, a contrastive learning and mask reconstruction method with locally significant feature enhancement is proposed to improve the ability to distinguish between the target and background and represent landslide semantic features. Meanwhile, a dual-branch interactive feature enhancement architecture is used to enrich the extracted features and address the issue of visual ambiguity. Self-distillation learning is introduced to leverage the feature diversity both within and between samples for contrastive learning, improving sample utilization, accelerating model convergence, and effectively addressing the problem of the small-sized dataset. The proposed MRIFE is evaluated on a real relic landslide dataset, and experimental results show that it greatly improves the performance of relic landslide detection. For the semantic segmentation task, compared to the baseline, the precision increases from 0.4226 to 0.5347, the mean intersection over union (IoU) increases from 0.6405 to 0.6680, the landslide IoU increases from 0.3381 to 0.3934, and the F1-score increases from 0.5054 to 0.5646. </p>
<blockquote>
<p>é—è¿¹æ»‘å¡ç»å†äº†é•¿æœŸå½¢æˆè¿‡ç¨‹ï¼Œå…·æœ‰é‡æ–°æ´»åŠ¨çš„æ½œåŠ›ï¼Œæˆä¸ºä¸€ç§å±é™©çš„åœ°è´¨ç°è±¡ã€‚å¯é çš„é—è¿¹æ»‘å¡æ£€æµ‹æœ‰åŠ©äºæœ‰æ•ˆç›‘æµ‹å’Œé¢„é˜²æ»‘å¡ç¾å®³ï¼Œç„¶è€Œï¼Œä½¿ç”¨é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå¯¹é—è¿¹æ»‘å¡è¿›è¡Œè¯­ä¹‰åˆ†å‰²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå…¶ä¸­åŒ…æ‹¬ç”±äºé•¿æœŸè‡ªç„¶æ¼”å˜å’Œäººç±»æ´»åŠ¨å¯¼è‡´çš„å¤–è§‚å˜åŒ–å¼•èµ·çš„ç›®æ ‡è§†è§‰æ¨¡ç³Šé—®é¢˜ï¼Œä»¥åŠç”±äºè¯†åˆ«å’Œæ ‡è®°æ ·æœ¬å›°éš¾è€Œå¯¼è‡´çš„å°è§„æ¨¡æ•°æ®é›†é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºæ©è†œæ¢å¤å’Œäº¤äº’ç‰¹å¾å¢å¼ºï¼ˆMRIFEï¼‰çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æå–å’Œåˆ†ç¦»ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæå‡ºäº†ä¸€ç§å¯¹æ¯”å­¦ä¹ å’Œæ©è†œé‡å»ºæ–¹æ³•ï¼Œå…·æœ‰å±€éƒ¨é‡è¦ç‰¹å¾å¢å¼ºçš„èƒ½åŠ›ï¼Œä»¥æé«˜åŒºåˆ†ç›®æ ‡å’ŒèƒŒæ™¯çš„èƒ½åŠ›ï¼Œå¹¶è¡¨ç°æ»‘å¡è¯­ä¹‰ç‰¹å¾ã€‚åŒæ—¶ï¼Œä½¿ç”¨åŒåˆ†æ”¯äº¤äº’ç‰¹å¾å¢å¼ºæ¶æ„æ¥ä¸°å¯Œæå–çš„ç‰¹å¾å¹¶è§£å†³è§†è§‰æ¨¡ç³Šé—®é¢˜ã€‚å¼•å…¥äº†è‡ªè’¸é¦å­¦ä¹ ï¼Œä»¥åˆ©ç”¨æ ·æœ¬å†…çš„ç‰¹å¾å¤šæ ·æ€§è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡ï¼ŒåŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œæœ‰æ•ˆè§£å†³å°è§„æ¨¡æ•°æ®é›†çš„é—®é¢˜ã€‚æå‡ºçš„MRIFEæ–¹æ³•åœ¨çœŸå®çš„é—è¿¹æ»‘å¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒå¤§å¤§æé«˜äº†é—è¿¹æ»‘å¡æ£€æµ‹çš„æ€§èƒ½ã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œç²¾ç¡®åº¦ä»0.4226æé«˜åˆ°0.5347ï¼Œå¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ä»0.6405æé«˜åˆ°0.6680ï¼Œæ»‘å¡IoUä»0.3381æé«˜åˆ°0.3934ï¼ŒF1åˆ†æ•°ä»0.5054æé«˜åˆ°0.5646ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé•¿æœŸå½¢æˆçš„é—è¿¹æ»‘å¡å…·æœ‰é‡æ–°æ¿€æ´»çš„æ½œåŠ›ï¼Œå› æ­¤å¯¹å…¶è¿›è¡Œå¯é çš„æ£€æµ‹å¯¹äºæœ‰æ•ˆçš„ç¾å®³ç›‘æµ‹å’Œé¢„é˜²è‡³å…³é‡è¦ã€‚é¢å¯¹é—è¿¹æ»‘å¡åœ¨è§†è§‰ä¸Šçš„æ¨¡ç³Šæ€§ä»¥åŠå°æ ·æœ¬æ•°æ®é›†å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºMRIFEçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œæ©è†œé‡å»ºæ–¹æ³•æé«˜ç›®æ ‡èƒŒæ™¯åŒºåˆ†èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨åŒåˆ†æ”¯äº¤äº’ç‰¹å¾å¢å¼ºæ¶æ„æ¥ä¸°å¯Œæå–çš„ç‰¹å¾å¹¶è§£å†³è§†è§‰æ¨¡ç³Šé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªè’¸é¦å­¦ä¹ æ¥æé«˜æ ·æœ¬åˆ©ç”¨ç‡å’Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦ã€‚åœ¨çœŸå®é—è¿¹æ»‘å¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMRIFEæ¨¡å‹å¤§å¤§æé«˜äº†é—è¿¹æ»‘å¡æ£€æµ‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é—è¿¹æ»‘å¡å…·æœ‰é‡æ–°æ¿€æ´»çš„æ½œåŠ›ï¼Œå¯é çš„æ£€æµ‹å¯¹äºç¾å®³ç›‘æµ‹å’Œé¢„é˜²è‡³å…³é‡è¦ã€‚</li>
<li>é—è¿¹æ»‘å¡åœ¨è§†è§‰ä¸Šå­˜åœ¨æ¨¡ç³Šé—®é¢˜ï¼Œå°æ ·æœ¬æ•°æ®é›†ä¹Ÿæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†MRIFEè¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œæ©è†œé‡å»ºæé«˜ç›®æ ‡èƒŒæ™¯åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>åŒåˆ†æ”¯äº¤äº’ç‰¹å¾å¢å¼ºæ¶æ„ç”¨äºä¸°å¯Œæå–çš„ç‰¹å¾å¹¶è§£å†³è§†è§‰æ¨¡ç³Šã€‚</li>
<li>è‡ªè’¸é¦å­¦ä¹ æé«˜æ ·æœ¬åˆ©ç”¨ç‡å’Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>MRIFEæ¨¡å‹åœ¨çœŸå®é—è¿¹æ»‘å¡æ•°æ®é›†ä¸Šçš„å®éªŒæ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20cd246940870b0a748006b3dfa131e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d083ef86b0c6219ddf6f3c775affd251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-691742291c9d78786a889d30aeb25fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-740b817e4d0c7ef202a32b9a8e1064cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-110eb60633812d529d373823a789fd37.jpg" align="middle">
</details>




<h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits modelsâ€™ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éšç€æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œé€šè¿‡å„ç§å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…å‡ºé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®­ç»ƒæ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼šåœ¨åŸºäºä»»æ„æŸ¥è¯¢æç¤ºçš„OVSSçš„å¤æ‚ç¯å¢ƒä¸­ï¼Œå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚è¿™ä¸€ç–å¿½é™åˆ¶äº†æ¨¡å‹åœ¨å¯¹è±¡å†…ç»„åˆè¯­ä¹‰ä¸€è‡´å…ƒç´ çš„èƒ½åŠ›ï¼Œå¹¶å‡†ç¡®åœ°å°†å®ƒä»¬æ˜ å°„åˆ°ç”¨æˆ·å®šä¹‰çš„ä»»æ„ç±»åˆ«ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é€šè¿‡ç»“åˆå›¾åƒä¸­çš„å¯¹è±¡çº§ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥å…‹æœè¿™ä¸€é™åˆ¶çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†ä»è§†è§‰åŸºç¡€æ¨¡å‹æç‚¼çš„è°±é©±åŠ¨ç‰¹å¾è’¸é¦åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„è¿è´¯æ€§ï¼Œä½¿å¾—è¯­ä¹‰ä¸€è‡´çš„ç»„ä»¶èƒ½å¤Ÿå½¢æˆå•ä¸ªå¯¹è±¡æ©ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨æ¦‚ç‡å¯¹æ–‡æœ¬åµŒå…¥è¿›è¡Œäº†ç²¾ç‚¼ï¼Œä»¥ç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰åœ¨æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ”¯æŒä¸‹å¾—åˆ°äº†å‘å±•ï¼Œå¯ä»¥é€šè¿‡ä¸åŒçš„å­¦ä¹ æ–¹æ¡ˆå®ç°è¶…å‡ºé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³å¤æ‚å¯¹è±¡åœ¨OVSSç¯å¢ƒä¸­çš„åˆ†å‰²é—®é¢˜ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„è¯­ä¹‰åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰å…è®¸è¶…è¶Šé¢„å®šç±»åˆ«çš„åˆ†å‰²ï¼Œé€šè¿‡ä¸åŒçš„å­¦ä¹ æ–¹æ¡ˆå®ç°æ›´å¹¿æ³›çš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>è®­ç»ƒå…è´¹çš„æ–¹æ³•ä¸ºå¤„ç†æœªè§è¿‡çš„æ•°æ®æä¾›äº†å¯æ‰©å±•å’Œæ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¯OVSSçš„å…³é”®ç›®æ ‡ä¹‹ä¸€ã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ï¼Œåœ¨åŸºäºä»»æ„æŸ¥è¯¢æç¤ºçš„OVSSç¯å¢ƒä¸­åˆ†å‰²å¤æ‚å¯¹è±¡æ—¶å­˜åœ¨é™åˆ¶ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡å¼•å…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æç‚¼è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿è¯­ä¹‰ä¸Šä¸€è‡´çš„åˆ†é‡å½¢æˆå•ä¸ªå¯¹è±¡æ©è†œã€‚</li>
<li>é€šè¿‡æ”¹è¿›æ–‡æœ¬åµŒå…¥å’Œé›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨å¯èƒ½æ€§ï¼Œç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f7b6fbeb2826a7dad8df4b062d3486a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffccfcbd45ef2a94b19be0bce7c72be4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbe7a6fdd528b4861885240dd1defd77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90fa9c3ab36b67f06e6c20c1253b7353.jpg" align="middle">
</details>




<h2 id="SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models"><a href="#SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models" class="headerlink" title="SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models"></a>SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models</h2><p><strong>Authors:Harsh Goel, Sai Shankar Narasimhan, Oguzhan Akcin, Sandeep Chinchali</strong></p>
<p>In recent years, significant progress has been made in collecting large-scale datasets to improve segmentation and autonomous driving models. These large-scale datasets are often dominated by common environmental conditions such as â€œClear and Dayâ€ weather, leading to decreased performance in under-represented conditions like â€œRainy and Nightâ€. To address this issue, we introduce SynDiff-AD, a novel data augmentation pipeline that leverages diffusion models (DMs) to generate realistic images for such subgroups. SynDiff-AD uses ControlNet-a DM that guides data generation conditioned on semantic maps-along with a novel prompting scheme that generates subgroup-specific, semantically dense prompts. By augmenting datasets with SynDiff-AD, we improve the performance of segmentation models like Mask2Former and SegFormer by up to 1.2% and 2.3% on the Waymo dataset, and up to 1.4% and 0.7% on the DeepDrive dataset, respectively. Additionally, we demonstrate that our SynDiff-AD pipeline enhances the driving performance of end-to-end autonomous driving models, like AIM-2D and AIM-BEV, by up to 20% across diverse environmental conditions in the CARLA autonomous driving simulator, providing a more robust model. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåœ¨æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ä»¥æ”¹è¿›åˆ†å‰²å’Œè‡ªåŠ¨é©¾é©¶æ¨¡å‹æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚è¿™äº›å¤§è§„æ¨¡æ•°æ®é›†é€šå¸¸ä»¥â€œæ™´æœ—å’Œç™½å¤©â€ç­‰å¸¸è§ç¯å¢ƒæ¡ä»¶ä¸ºä¸»ï¼Œå¯¼è‡´åœ¨ä»£è¡¨æ€§ä¸è¶³çš„æ¡ä»¶ï¼ˆå¦‚â€œé›¨å¤©å¤œé—´â€ï¼‰ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SynDiff-ADï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ•°æ®å¢å¼ºç®¡é“ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç”Ÿæˆæ­¤ç±»å­ç»„çš„ç°å®å›¾åƒã€‚SynDiff-ADä½¿ç”¨ControlNetï¼ˆä¸€ç§åŸºäºè¯­ä¹‰åœ°å›¾å¼•å¯¼æ•°æ®ç”Ÿæˆçš„DMï¼‰ï¼Œä»¥åŠä¸€ç§æ–°å‹æç¤ºæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥ç”Ÿæˆå­ç»„ç‰¹å®šçš„è¯­ä¹‰å¯†é›†æç¤ºã€‚é€šè¿‡SynDiff-ADå¢å¼ºæ•°æ®é›†ï¼Œæˆ‘ä»¬æ”¹è¿›äº†Mask2Formerå’ŒSegFormerç­‰åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨Waymoæ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜äº†1.2%å’Œ2.3%ï¼Œåœ¨DeepDriveæ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜äº†1.4%å’Œ0.7%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„SynDiff-ADç®¡é“åœ¨CARLAè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­çš„å¤šç§ç¯å¢ƒæ¡ä»¶ä¸‹ï¼Œæé«˜äº†ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹ï¼ˆå¦‚AIM-2Då’ŒAIM-BEVï¼‰çš„é©¾é©¶æ€§èƒ½ï¼Œæœ€é«˜æé«˜äº†20%ï¼Œä¸ºæ›´ç¨³å¥çš„æ¨¡å‹æä¾›äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16776v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†åœ¨è‡ªåŠ¨é©¾é©¶æ¨¡å‹ä¸­çš„ä½¿ç”¨é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•SynDiff-ADã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ç‰¹å®šç¯å¢ƒæ¡ä»¶ä¸‹çš„çœŸå®å›¾åƒï¼Œè§£å†³äº†å› å¤©æ°”æ¡ä»¶å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡SynDiff-ADæ–¹æ³•çš„æ•°æ®å¢å¼ºï¼Œå¯ä»¥æå‡åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨CARLAè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­æé«˜äº†ç«¯åˆ°ç«¯çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„é©¾é©¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¤§è§„æ¨¡æ•°æ®é›†æé«˜äº†åˆ†å‰²å’Œè‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¸¸è§ç¯å¢ƒæ¡ä»¶ä¸‹çš„æ•°æ®é›†å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨ç‰¹å®šç¯å¢ƒä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>SynDiff-ADæ˜¯ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆçœŸå®å›¾åƒã€‚</li>
<li>SynDiff-ADå¯ä»¥è§£å†³ç‰¹å®šç¯å¢ƒæ¡ä»¶ä¸‹çš„æ¨¡å‹æ€§èƒ½é—®é¢˜ã€‚</li>
<li>é€šè¿‡SynDiff-ADçš„æ•°æ®å¢å¼ºï¼Œå¯ä»¥æé«˜åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨CARLAè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­ï¼ŒSynDiff-ADæé«˜äº†ç«¯åˆ°ç«¯çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„é©¾é©¶æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9beebce3088f2766cf5c9cbb0026de11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ad64077fb21b241a397267f7fc9529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e84d37a2fd77b480f20f9f04e6564859.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af7c7747251904e413a7cc77f60381ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f64383ea48c4966ab8d076cfd8a8262.jpg" align="middle">
</details>




<h2 id="CutS3D-Cutting-Semantics-in-3D-for-2D-Unsupervised-Instance-Segmentation"><a href="#CutS3D-Cutting-Semantics-in-3D-for-2D-Unsupervised-Instance-Segmentation" class="headerlink" title="CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance   Segmentation"></a>CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance   Segmentation</h2><p><strong>Authors:Leon Sick, Dominik Engel, Sebastian Hartwig, Pedro Hermosilla, Timo Ropinski</strong></p>
<p>Traditionally, algorithms that learn to segment object instances in 2D images have heavily relied on large amounts of human-annotated data. Only recently, novel approaches have emerged tackling this problem in an unsupervised fashion. Generally, these approaches first generate pseudo-masks and then train a class-agnostic detector. While such methods deliver the current state of the art, they often fail to correctly separate instances overlapping in 2D image space since only semantics are considered. To tackle this issue, we instead propose to cut the semantic masks in 3D to obtain the final 2D instances by utilizing a point cloud representation of the scene. Furthermore, we derive a Spatial Importance function, which we use to resharpen the semantics along the 3D borders of instances. Nevertheless, these pseudo-masks are still subject to mask ambiguity. To address this issue, we further propose to augment the training of a class-agnostic detector with three Spatial Confidence components aiming to isolate a clean learning signal. With these contributions, our approach outperforms competing methods across multiple standard benchmarks for unsupervised instance segmentation and object detection. </p>
<blockquote>
<p>ä¼ ç»Ÿä¸Šï¼Œå­¦ä¹ åœ¨2Då›¾åƒä¸­åˆ†å‰²å¯¹è±¡å®ä¾‹çš„ç®—æ³•ä¸¥é‡ä¾èµ–äºå¤§é‡çš„äººå·¥æ ‡æ³¨æ•°æ®ã€‚ä»…æœ€è¿‘ï¼Œæ–°å…´çš„æ–¹æ³•å¼€å§‹ä»¥æ— ç›‘ç£çš„æ–¹å¼å¤„ç†è¿™ä¸€é—®é¢˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™äº›æ–¹æ³•é¦–å…ˆç”Ÿæˆä¼ªæ©è†œï¼Œç„¶åè®­ç»ƒä¸€ç§ç±»æ— å…³çš„æ¢æµ‹å™¨ã€‚è™½ç„¶è¿™äº›æ–¹æ³•è¾¾åˆ°äº†å½“å‰çš„æœ€ä½³æ°´å¹³ï¼Œä½†ç”±äºåªè€ƒè™‘è¯­ä¹‰ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•æ­£ç¡®åˆ†ç¦»åœ¨2Då›¾åƒç©ºé—´ä¸­é‡å çš„å®ä¾‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨3Dç©ºé—´ä¸­å¯¹è¯­ä¹‰æ©è†œè¿›è¡Œåˆ‡å‰²çš„æ–¹æ³•ï¼Œåˆ©ç”¨åœºæ™¯çš„ç‚¹äº‘è¡¨ç¤ºæ¥è·å¾—æœ€ç»ˆçš„2Då®ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªç©ºé—´é‡è¦æ€§å‡½æ•°ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥é”åŒ–å®ä¾‹çš„3Dè¾¹ç•Œå¤„çš„è¯­ä¹‰ã€‚ç„¶è€Œï¼Œè¿™äº›ä¼ªæ©è†œä»ç„¶å­˜åœ¨æ©è†œæ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºåœ¨ç±»æ— å…³çš„æ¢æµ‹å™¨è®­ç»ƒä¸­å¢åŠ ä¸‰ä¸ªç©ºé—´ç½®ä¿¡åº¦åˆ†é‡ï¼Œæ—¨åœ¨äº§ç”Ÿä¸€ä¸ªæ¸…æ™°çš„å­¦ä¹ ä¿¡å·ã€‚é€šè¿‡ä»¥ä¸Šçš„è´¡çŒ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªäººå·¥åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹çš„åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16319v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸæ–°å…´çš„æ— ç›‘ç£æ–¹æ³•å¼€å§‹å°è¯•è§£å†³äºŒç»´å›¾åƒä¸­çš„å®ä¾‹åˆ†å‰²é—®é¢˜ï¼Œå®ƒä»¬å…ˆç”Ÿæˆä¼ªæ©è†œå†è®­ç»ƒç±»åˆ«æ— å…³çš„æ¢æµ‹å™¨ä»¥è¾¾åˆ°å½“å‰æœ€ä¼˜æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åˆ†ç¦»é‡å å®ä¾‹æ—¶å­˜åœ¨ç¼ºé™·ï¼Œä»…è€ƒè™‘è¯­ä¹‰è€Œæœªå……åˆ†åˆ©ç”¨ä¸‰ç»´ç©ºé—´ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ç‚¹äº‘è¡¨ç¤ºåœºæ™¯ï¼Œåœ¨ä¸‰ç»´ç©ºé—´è¿›è¡Œè¯­ä¹‰æ©è†œçš„åˆ‡å‰²ä»¥è·å¾—æœ€ç»ˆçš„äºŒç»´å®ä¾‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥ç©ºé—´é‡è¦æ€§å‡½æ•°ä»¥å¼ºåŒ–ä¸‰ç»´è¾¹ç•Œå¤„çš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºè§£å†³ä¼ªæ©è†œå­˜åœ¨çš„æ¨¡ç³Šé—®é¢˜ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ·»åŠ ä¸‰ä¸ªç©ºé—´ç½®ä¿¡åº¦ç»„ä»¶æ¥å¢å¼ºç±»åˆ«æ— å…³æ¢æµ‹å™¨çš„è®­ç»ƒæ•ˆæœã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ— ç›‘ç£æ–¹æ³•å¼€å§‹åº”ç”¨äºäºŒç»´å›¾åƒå®ä¾‹åˆ†å‰²é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè€Œæ–°å…´æ–¹æ³•é€šè¿‡ç”Ÿæˆä¼ªæ©è†œè®­ç»ƒç±»åˆ«æ— å…³æ¢æµ‹å™¨ä»¥è¾¾æˆæœ€ä¼˜æ•ˆæœã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†é‡å å®ä¾‹æ—¶å­˜åœ¨å›°éš¾ï¼Œä»…è€ƒè™‘è¯­ä¹‰æœªå……åˆ†åˆ©ç”¨ä¸‰ç»´ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>æå‡ºåˆ©ç”¨ç‚¹äº‘è¡¨ç¤ºåœºæ™¯è¿›è¡Œä¸‰ç»´è¯­ä¹‰æ©è†œåˆ‡å‰²ä»¥è·å¾—äºŒç»´å®ä¾‹ã€‚</li>
<li>å¼•å…¥ç©ºé—´é‡è¦æ€§å‡½æ•°å¼ºåŒ–ä¸‰ç»´è¾¹ç•Œå¤„çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ä¼ªæ©è†œå­˜åœ¨æ¨¡ç³Šé—®é¢˜ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æ·»åŠ äº†ä¸‰ä¸ªç©ºé—´ç½®ä¿¡åº¦ç»„ä»¶ä»¥å¢å¼ºæ¢æµ‹å™¨è®­ç»ƒæ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54499c9a4c35a84eaad384562a274908.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-553622c5b379011fa82ea96f493d4f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6005b81f396ba65c4c1f93662c5e5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c8757a997c2c4f8c96266d71eada972.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f734059cfeaa8ab9fe9de6f1b82cbf81.jpg" align="middle">
</details>




<h2 id="A-Performance-Increment-Strategy-for-Semantic-Segmentation-of-Low-Resolution-Images-from-Damaged-Roads"><a href="#A-Performance-Increment-Strategy-for-Semantic-Segmentation-of-Low-Resolution-Images-from-Damaged-Roads" class="headerlink" title="A Performance Increment Strategy for Semantic Segmentation of   Low-Resolution Images from Damaged Roads"></a>A Performance Increment Strategy for Semantic Segmentation of   Low-Resolution Images from Damaged Roads</h2><p><strong>Authors:Rafael S. Toledo, Cristiano S. Oliveira, Vitor H. T. Oliveira, Eric A. Antonelo, Aldo von Wangenheim</strong></p>
<p>Autonomous driving needs good roads, but 85% of Brazilian roads have damages that deep learning models may not regard as most semantic segmentation datasets for autonomous driving are high-resolution images of well-maintained urban roads. A representative dataset for emerging countries consists of low-resolution images of poorly maintained roads and includes labels of damage classes; in this scenario, three challenges arise: objects with few pixels, objects with undefined shapes, and highly underrepresented classes. To tackle these challenges, this work proposes the Performance Increment Strategy for Semantic Segmentation (PISSS) as a methodology of 14 training experiments to boost performance. With PISSS, we reached state-of-the-art results of 79.8 and 68.8 mIoU on the Road Traversing Knowledge (RTK) and Technik Autonomer Systeme 500 (TAS500) test sets, respectively. Furthermore, we also offer an analysis of DeepLabV3+ pitfalls for small object segmentation. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶éœ€è¦è‰¯å¥½çš„é“è·¯ï¼Œä½†å·´è¥¿æœ‰85%çš„é“è·¯å­˜åœ¨æŸåï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¯èƒ½æ— æ³•è¯†åˆ«ï¼Œå› ä¸ºå¤§å¤šæ•°è‡ªåŠ¨é©¾é©¶çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†éƒ½æ˜¯é«˜åˆ†è¾¨ç‡çš„è‰¯å¥½ç»´æŠ¤çš„åŸå¸‚é“è·¯å›¾åƒã€‚å¯¹äºæ–°å…´å›½å®¶è€Œè¨€ï¼Œä¸€ä¸ªå…¸å‹çš„æ•°æ®é›†åŒ…å«ä½åˆ†è¾¨ç‡çš„ç ´æŸé“è·¯å›¾åƒï¼Œå¹¶åŒ…æ‹¬æŸåç±»åˆ«çš„æ ‡ç­¾ï¼›åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡ºç°äº†ä¸‰ä¸ªæŒ‘æˆ˜ï¼šåƒç´ å°‘çš„ç‰©ä½“ã€å½¢çŠ¶ä¸ç¡®å®šçš„ç‰©ä½“ä»¥åŠé«˜åº¦æ¬ ä»£è¡¨çš„ç±»åˆ«ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†è¯­ä¹‰åˆ†å‰²æ€§èƒ½æå‡ç­–ç•¥ï¼ˆPISSSï¼‰ä½œä¸ºä¸€ç§åŒ…å«14ä¸ªè®­ç»ƒå®éªŒçš„æ–¹æ³•æ¥æé«˜æ€§èƒ½ã€‚é€šè¿‡PISSSï¼Œæˆ‘ä»¬åœ¨Road Traversing Knowledgeï¼ˆRTKï¼‰å’ŒTechnik Autonomer Systeme 500ï¼ˆTAS500ï¼‰æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„79.8%å’Œ68.8%çš„mIoUç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹DeepLabV3+åœ¨å°å‹å¯¹è±¡åˆ†å‰²ä¸­çš„é™·é˜±è¿›è¡Œäº†åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œè‡ªä¸»é©¾é©¶éœ€è¦è‰¯å¥½çš„é“è·¯ç¯å¢ƒï¼Œä½†å·´è¥¿çº¦æœ‰85%çš„é“è·¯å­˜åœ¨æŸåæƒ…å†µã€‚ç°æœ‰çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†å¤šä¸ºé«˜åˆ†è¾¨ç‡çš„è‰¯å¥½åŸå¸‚é“è·¯å›¾åƒï¼Œä¸é€‚ç”¨äºæ­¤åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†é’ˆå¯¹æ–°å…´å›½å®¶é“è·¯æƒ…å†µçš„ä»£è¡¨æ€§æ•°æ®é›†ï¼Œå¹¶æŒ‡å‡ºäº†å…¶ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šåƒç´ å°‘çš„ç‰©ä½“ã€å½¢çŠ¶ä¸ç¡®å®šçš„ç‰©ä½“ä»¥åŠé«˜åº¦æœªè¢«å……åˆ†ä»£è¡¨çš„ç±»åˆ«ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è¯­ä¹‰åˆ†å‰²æ€§èƒ½æå‡ç­–ç•¥ï¼ˆPISSSï¼‰ï¼Œé€šè¿‡14ä¸ªè®­ç»ƒå®éªŒæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨RTKå’ŒTAS500æµ‹è¯•é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†79.8å’Œ68.8 mIoUçš„å…ˆè¿›æ°´å¹³ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿå¯¹DeepLabV3+åœ¨å°ç›®æ ‡åˆ†å‰²ä¸­çš„é™·é˜±è¿›è¡Œäº†åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å·´è¥¿å¤§éƒ¨åˆ†é“è·¯å­˜åœ¨æŸåæƒ…å†µï¼ˆå æ¯”çº¦ä¸º85%ï¼‰ï¼Œå¯¹è‡ªä¸»é©¾é©¶æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç°å­˜çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸»è¦å…³æ³¨è‰¯å¥½ç»´æŠ¤çš„åŸå¸‚é“è·¯çš„é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä¸é€‚ç”¨äºæ–°å…´å›½å®¶çš„é“è·¯çŠ¶å†µã€‚</li>
<li>é’ˆå¯¹æ–°å…´å›½å®¶é“è·¯çŠ¶å†µï¼Œå­˜åœ¨ä¸‰å¤§æŒ‘æˆ˜ï¼šåƒç´ å°‘çš„ç‰©ä½“ã€å½¢çŠ¶ä¸ç¡®å®šçš„ç‰©ä½“ä»¥åŠé«˜åº¦æœªè¢«å……åˆ†ä»£è¡¨çš„ç±»åˆ«ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†è¯­ä¹‰åˆ†å‰²æ€§èƒ½æå‡ç­–ç•¥ï¼ˆPISSSï¼‰ï¼Œå¹¶é€šè¿‡è®­ç»ƒå®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨RTKå’ŒTAS500æµ‹è¯•é›†ä¸Šï¼Œä½¿ç”¨PISSSç­–ç•¥è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼ŒmIoUåˆ†åˆ«ä¸º79.8å’Œ68.8ã€‚</li>
<li>æ–‡ç« è¿˜æ¢è®¨äº†DeepLabV3+åœ¨å°ç›®æ ‡åˆ†å‰²ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-efbba0e878aefa8b8bd633287c5ae0b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41116cee64da7331e02fc1a98ea5e558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8028ee57fa6dcc885c54769960f71137.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58876456223de5bb75ec48e7f2a9a96f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d392fc10978b009abbdf160912c20465.jpg" align="middle">
</details>




<h2 id="Instance-Aware-Generalized-Referring-Expression-Segmentation"><a href="#Instance-Aware-Generalized-Referring-Expression-Segmentation" class="headerlink" title="Instance-Aware Generalized Referring Expression Segmentation"></a>Instance-Aware Generalized Referring Expression Segmentation</h2><p><strong>Authors:E-Ro Nguyen, Hieu Le, Dimitris Samaras, Michael Ryoo</strong></p>
<p>Recent works on Generalized Referring Expression Segmentation (GRES) struggle with handling complex expressions referring to multiple distinct objects. This is because these methods typically employ an end-to-end foreground-background segmentation and lack a mechanism to explicitly differentiate and associate different object instances to the text query. To this end, we propose InstAlign, a method that incorporates object-level reasoning into the segmentation process. Our model leverages both text and image inputs to extract a set of object-level tokens that capture both the semantic information in the input prompt and the objects within the image. By modeling the text-object alignment via instance-level supervision, each token uniquely represents an object segment in the image, while also aligning with relevant semantic information from the text. Extensive experiments on the gRefCOCO and Ref-ZOM benchmarks demonstrate that our method significantly advances state-of-the-art performance, setting a new standard for precise and flexible GRES. </p>
<blockquote>
<p>å…³äºå¹¿ä¹‰å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰çš„æœ€æ–°ç ”ç©¶åœ¨å¤„ç†å¼•ç”¨å¤šä¸ªä¸åŒå¯¹è±¡çš„å¤æ‚è¡¨è¾¾å¼æ—¶é‡åˆ°äº†å›°éš¾ã€‚è¿™æ˜¯å› ä¸ºè¿™äº›æ–¹æ³•é€šå¸¸é‡‡ç”¨ç«¯åˆ°ç«¯çš„å‰æ™¯èƒŒæ™¯åˆ†å‰²ï¼Œå¹¶ä¸”ç¼ºä¹ä¸€ç§æœºåˆ¶æ¥æ˜ç¡®åŒºåˆ†å’Œå…³è”ä¸åŒçš„å¯¹è±¡å®ä¾‹åˆ°æ–‡æœ¬æŸ¥è¯¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†InstAlignæ–¹æ³•ï¼Œå®ƒå°†å¯¹è±¡çº§åˆ«çš„æ¨ç†èå…¥åˆ°åˆ†å‰²è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨æ–‡æœ¬å’Œå›¾åƒè¾“å…¥æ¥æå–ä¸€ç»„å¯¹è±¡çº§åˆ«çš„æ ‡è®°ï¼Œè¿™äº›æ ‡è®°æ—¢æ•è·è¾“å…¥æç¤ºä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¹Ÿæ•è·å›¾åƒä¸­çš„å¯¹è±¡ã€‚é€šè¿‡å¯¹æ–‡æœ¬å¯¹è±¡å¯¹é½è¿›è¡Œå®ä¾‹çº§ç›‘ç£å»ºæ¨¡ï¼Œæ¯ä¸ªæ ‡è®°å”¯ä¸€åœ°ä»£è¡¨å›¾åƒä¸­çš„ä¸€ä¸ªå¯¹è±¡æ®µï¼ŒåŒæ—¶ä¸æ–‡æœ¬ä¸­çš„ç›¸å…³è¯­ä¹‰ä¿¡æ¯å¯¹é½ã€‚åœ¨gRefCOCOå’ŒRef-ZOMåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ï¼Œä¸ºç²¾ç¡®çµæ´»çš„GRESè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15087v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºInstAlignçš„æ–¹æ³•ï¼Œç”¨äºå¤„ç†å¹¿ä¹‰æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰ä¸­çš„å¤æ‚é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œæå–å¯¹è±¡çº§æ ‡è®°ï¼Œä»¥æ•æ‰è¾“å…¥æç¤ºä¸­çš„è¯­ä¹‰ä¿¡æ¯å’Œå›¾åƒä¸­çš„å¯¹è±¡ã€‚é€šè¿‡å®ä¾‹çº§ç›‘ç£å»ºæ¨¡æ–‡æœ¬å¯¹è±¡å¯¹é½ï¼Œæ¯ä¸ªæ ‡è®°å”¯ä¸€ä»£è¡¨å›¾åƒä¸­çš„ä¸€ä¸ªå¯¹è±¡æ®µï¼Œå¹¶ä¸æ–‡æœ¬ä¸­çš„ç›¸å…³è¯­ä¹‰ä¿¡æ¯å¯¹é½ã€‚åœ¨gRefCOCOå’ŒRef-ZOMåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æœ€æ–°æŠ€æœ¯çš„åŸºç¡€ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºç²¾ç¡®çµæ´»çš„GRESè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹¿ä¹‰æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰åœ¨å¤„ç†å¤æ‚è¡¨è¾¾å¼æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›è¡¨è¾¾å¼å¯èƒ½æŒ‡ä»£å¤šä¸ªä¸åŒå¯¹è±¡ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç«¯åˆ°å‰çš„å‰å°åå°åˆ†å‰²æ–¹å¼ï¼Œç¼ºä¹æ˜ç¡®åŒºåˆ†å’Œå…³è”ä¸åŒå¯¹è±¡å®ä¾‹çš„æœºåˆ¶ã€‚</li>
<li>InstAlignæ–¹æ³•ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œæå–å¯¹è±¡çº§æ ‡è®°ä»¥æ•æ‰è¯­ä¹‰ä¿¡æ¯å’Œå›¾åƒä¸­çš„å¯¹è±¡ã€‚</li>
<li>é€šè¿‡å®ä¾‹çº§ç›‘ç£å»ºæ¨¡æ–‡æœ¬ä¸å¯¹è±¡çš„å¯¹é½å…³ç³»ã€‚</li>
<li>æ¯ä¸ªæ ‡è®°å”¯ä¸€ä»£è¡¨å›¾åƒä¸­çš„ä¸€ä¸ªå¯¹è±¡æ®µï¼Œå¹¶ä¸æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯å¯¹é½ã€‚</li>
<li>åœ¨gRefCOCOå’ŒRef-ZOMåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒInstAlignæ–¹æ³•æ˜¾è‘—æé«˜äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a8e9ba96c7fe8ba0e15f9a8470d8cb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a1abe20722b5cb51215c7e09470ebc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b10866630c8fbebd7241e05c5af73638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49fcd1f6c365540a9ae2672c05b1c856.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01b240c4e6a8647080a7e1c40667c0a2.jpg" align="middle">
</details>




<h2 id="Effective-SAM-Combination-for-Open-Vocabulary-Semantic-Segmentation"><a href="#Effective-SAM-Combination-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Effective SAM Combination for Open-Vocabulary Semantic Segmentation"></a>Effective SAM Combination for Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Minhyeok Lee, Suhwan Cho, Jungho Lee, Sunghun Yang, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee</strong></p>
<p>Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAMâ€™s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ç»™æ— é™èŒƒå›´çš„ç±»åˆ«ä¸­çš„å›¾åƒåˆ†é…åƒç´ çº§æ ‡ç­¾ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡é¡ºåºè¿æ¥å¼ºå¤§çš„æ©è†œææ¡ˆç”Ÿæˆå™¨ï¼ˆå¦‚Anythingåˆ†æ®µæ¨¡å‹ï¼ˆSAMï¼‰ï¼‰å’Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½†è¿™äº›ä¸¤é˜¶æ®µæ–¹æ³•å¾€å¾€å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€å†…å­˜æ•ˆç‡ä½çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ESC-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸€é˜¶æ®µå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨SAMè§£ç å™¨å—åœ¨æœ‰æ•ˆçš„æ¨ç†æ¡†æ¶å†…è¿›è¡Œç±»åˆ«æ— å…³çš„åˆ†å‰²ã€‚é€šè¿‡å°†ç”±å›¾åƒæ–‡æœ¬ç›¸å…³æ€§ç”Ÿæˆçš„ä¼ªæç¤ºåµŒå…¥åˆ°SAMçš„å¯æç¤ºåˆ†å‰²æ¡†æ¶ä¸­ï¼ŒESC-Netå®ç°äº†ç²¾ç»†çš„ç©ºé—´èšåˆï¼Œä»¥è·å¾—å‡†ç¡®çš„æ©è†œé¢„æµ‹ã€‚ESC-Netåœ¨åŒ…æ‹¬ADE20Kã€PASCAL-VOCå’ŒPASCAL-Contextåœ¨å†…çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨å…·æœ‰æŒ‘æˆ˜çš„æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14723v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸€ç«™å¼å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹ESC-Netã€‚ç»“åˆSegment Anything Modelï¼ˆSAMï¼‰è§£ç å™¨å—ï¼Œè¯¥æ¨¡å‹å¯åœ¨é«˜æ•ˆæ¨ç†æ¡†æ¶å†…è¿›è¡Œç±»åˆ«æ— å…³çš„åˆ†å‰²ã€‚é€šè¿‡åµŒå…¥ç”±å›¾åƒæ–‡æœ¬ç›¸å…³æ€§ç”Ÿæˆçš„ä¼ªæç¤ºï¼ŒESC-Netå®ç°äº†ç²¾ç»†çš„ç©ºé—´èšåˆï¼Œç”¨äºå‡†ç¡®çš„æ©è†œé¢„æµ‹ã€‚åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒESC-Netè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ADE20Kã€PASCAL-VOCå’ŒPASCAL-Contextï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„ç›®æ ‡æ˜¯ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…æ ‡ç­¾ï¼Œæ¶µç›–æ— é™ç±»åˆ«èŒƒå›´ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬å¼ºå¤§çš„æ©è†œææ¡ˆç”Ÿæˆå™¨ï¼ˆå¦‚SAMï¼‰å’Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ã€‚</li>
<li>ä¸¤é˜¶æ®µæ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œå†…å­˜æ•ˆç‡ä½çš„é—®é¢˜ã€‚</li>
<li>ESC-Netæ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸€ç«™å¼å¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ï¼Œåˆ©ç”¨SAMè§£ç å™¨å—åœ¨é«˜æ•ˆæ¨ç†æ¡†æ¶å†…è¿›è¡Œç±»åˆ«æ— å…³çš„åˆ†å‰²ã€‚</li>
<li>ESC-Neté€šè¿‡åµŒå…¥ä¼ªæç¤ºï¼Œå®ç°ç²¾ç»†çš„ç©ºé—´èšåˆï¼Œç”¨äºå‡†ç¡®çš„æ©è†œé¢„æµ‹ã€‚</li>
<li>ESC-Netåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬ADE20Kã€PASCAL-VOCå’ŒPASCAL-Contextã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0290ce7a2cb759fddbe791631cdbbf2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31cd9e3384d6a84abdf267d8636798eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6d18dbab2b8ee6417990e3d46117d89.jpg" align="middle">
</details>




<h2 id="DINO-X-A-Unified-Vision-Model-for-Open-World-Object-Detection-and-Understanding"><a href="#DINO-X-A-Unified-Vision-Model-for-Open-World-Object-Detection-and-Understanding" class="headerlink" title="DINO-X: A Unified Vision Model for Open-World Object Detection and   Understanding"></a>DINO-X: A Unified Vision Model for Open-World Object Detection and   Understanding</h2><p><strong>Authors:Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, Lei Zhang</strong></p>
<p>In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the modelâ€™s core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the modelâ€™s open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç”±IDEA Researchå¼€å‘çš„DINO-Xï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è§†è§‰æ¨¡å‹ï¼Œå…·æœ‰è¿„ä»Šä¸ºæ­¢æœ€ä½³çš„å¼€æ”¾ä¸–ç•Œå¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚DINO-Xé‡‡ç”¨ä¸Grounding DINO 1.5ç›¸åŒçš„åŸºäºTransformerçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¿½æ±‚å¼€æ”¾ä¸–ç•Œå¯¹è±¡ç†è§£çš„å¯¹è±¡çº§è¡¨ç¤ºã€‚ä¸ºäº†ç®€åŒ–é•¿å°¾å¯¹è±¡æ£€æµ‹ï¼ŒDINO-Xæ‰©å±•äº†å…¶è¾“å…¥é€‰é¡¹ï¼Œæ”¯æŒæ–‡æœ¬æç¤ºã€è§†è§‰æç¤ºå’Œè‡ªå®šä¹‰æç¤ºã€‚é€šè¿‡è¿™æ ·çµæ´»çš„æç¤ºé€‰é¡¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€šç”¨å¯¹è±¡æç¤ºï¼Œæ”¯æŒæ— æç¤ºçš„å¼€æ”¾ä¸–ç•Œæ£€æµ‹ï¼Œä½¿å¾—åœ¨å›¾åƒä¸­æ£€æµ‹ä»»ä½•å¯¹è±¡æˆä¸ºå¯èƒ½ï¼Œè€Œæ— éœ€ç”¨æˆ·æä¾›ä»»ä½•æç¤ºã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ ¸å¿ƒå®šä½èƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œç§°ä¸ºGrounding-100Mï¼ŒåŒ…å«è¶…è¿‡1äº¿ä¸ªé«˜è´¨é‡å®šä½æ ·æœ¬ï¼Œä»¥æå‡æ¨¡å‹çš„å¼€æ”¾è¯æ±‡æ£€æµ‹æ€§èƒ½ã€‚åœ¨å¦‚æ­¤å¤§è§„æ¨¡çš„å®šä½æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¾—åˆ°äº†ä¸€ä¸ªåŸºç¡€çš„å¯¹è±¡çº§è¡¨ç¤ºï¼Œä½¿DINO-Xèƒ½å¤Ÿé›†æˆå¤šä¸ªæ„ŸçŸ¥å¤´ï¼ŒåŒæ—¶æ”¯æŒå¤šä¸ªå¯¹è±¡æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ£€æµ‹ã€åˆ†å‰²ã€å§¿æ€ä¼°è®¡ã€å¯¹è±¡æè¿°ã€åŸºäºå¯¹è±¡çš„é—®ç­”ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜DINO-Xçš„æ€§èƒ½ä¼˜è¶Šã€‚å…·ä½“æ¥è¯´ï¼ŒDINO-X Proæ¨¡å‹åœ¨COCOã€LVIS-minivalå’ŒLVIS-valé›¶æ ·æœ¬å¯¹è±¡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šçš„APå€¼åˆ†åˆ«è¾¾åˆ°56.0ã€59.8å’Œ52.4ã€‚ç‰¹åˆ«æ˜¯åœ¨LVIS-minivalå’ŒLVIS-valåŸºå‡†çš„ç¨€æœ‰ç±»åˆ«ä¸Šï¼Œå…¶APå€¼è¾¾åˆ°63.3å’Œ56.5ï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³æ€§èƒ½æé«˜äº†5.8 APå’Œ5.0 APã€‚è¿™ä¸€ç»“æœå‡¸æ˜¾äº†å…¶åœ¨è¯†åˆ«é•¿å°¾å¯¹è±¡æ–¹é¢çš„æ˜¾è‘—æ”¹å–„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14347v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”±IDEA Researchå¼€å‘çš„DINO-Xç»Ÿä¸€å¯¹è±¡çº§è§†è§‰æ¨¡å‹ï¼Œå…·æœ‰æœ€ä½³å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚DINO-Xé‡‡ç”¨ä¸Grounding DINO 1.5ç›¸åŒçš„Transformerç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¿½æ±‚å¯¹è±¡çº§è¡¨ç¤ºä»¥å®ç°å¼€æ”¾ä¸–ç•Œå¯¹è±¡ç†è§£ã€‚å®ƒé€šè¿‡æ‰©å±•è¾“å…¥é€‰é¡¹ä»¥æ”¯æŒæ–‡æœ¬æç¤ºã€è§†è§‰æç¤ºå’Œè‡ªå®šä¹‰æç¤ºï¼Œç®€åŒ–äº†é•¿å°¾ç›®æ ‡æ£€æµ‹ã€‚æ­¤å¤–ï¼ŒDINO-Xå…·æœ‰é€šç”¨å¯¹è±¡æç¤ºï¼Œæ”¯æŒæ— æç¤ºå¼€æ”¾ä¸–ç•Œæ£€æµ‹ï¼Œå¯åœ¨ä¸éœ€è¦ç”¨æˆ·æä¾›ä»»ä½•æç¤ºçš„æƒ…å†µä¸‹æ£€æµ‹å›¾åƒä¸­çš„ä»»ä½•å†…å®¹ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ¥åœ°æ•°æ®é›†Grounding-100Mï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ ¸å¿ƒæ¥åœ°èƒ½åŠ›ï¼Œå¹¶æé«˜äº†å¼€æ”¾è¯æ±‡æ£€æµ‹æ€§èƒ½ã€‚é¢„è®­ç»ƒè¿™æ ·çš„å¤§å‹æ¥åœ°æ•°æ®é›†ä¸ºDINO-Xæä¾›äº†åŸºç¡€çš„å¯¹è±¡çº§è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿé›†æˆå¤šä¸ªæ„ŸçŸ¥å¤´ï¼ŒåŒæ—¶æ”¯æŒå¤šä¸ªå¯¹è±¡æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜DINO-Xæ€§èƒ½å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DINO-Xæ˜¯IDEA Researchå¼€å‘çš„ç»Ÿä¸€å¯¹è±¡çº§è§†è§‰æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>DINO-Xé‡‡ç”¨ä¸Grounding DINO 1.5ç›¸åŒçš„Transformeræ¶æ„ã€‚</li>
<li>DINO-Xæ”¯æŒå¤šç§æç¤ºæ–¹å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’Œè‡ªå®šä¹‰æç¤ºï¼Œç®€åŒ–äº†é•¿å°¾ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>é€šç”¨å¯¹è±¡æç¤ºæ”¯æŒæ— æç¤ºå¼€æ”¾ä¸–ç•Œæ£€æµ‹ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ¥åœ°æ•°æ®é›†Grounding-100Mæé«˜æ¨¡å‹æ ¸å¿ƒæ¥åœ°èƒ½åŠ›å’Œå¼€æ”¾è¯æ±‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒå¤§å‹æ¥åœ°æ•°æ®é›†ä¸ºDINO-Xæä¾›åŸºç¡€å¯¹è±¡çº§è¡¨ç¤ºã€‚</li>
<li>DINO-Xèƒ½åŒæ—¶æ”¯æŒå¤šä¸ªå¯¹è±¡æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ£€æµ‹ã€åˆ†å‰²ã€å§¿æ€ä¼°è®¡ç­‰ï¼Œä¸”å®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½å“è¶Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5bc885768aabd0215ecc31021b3bd190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a71407f0bbe0e18a7be64e2b6be4f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be6397aad0e25f4645cae060e4e0cdc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98b13dfa6922d37353ac97289264d998.jpg" align="middle">
</details>




<h2 id="DIS-Mine-Instance-Segmentation-for-Disaster-Awareness-in-Poor-Light-Condition-in-Underground-Mines"><a href="#DIS-Mine-Instance-Segmentation-for-Disaster-Awareness-in-Poor-Light-Condition-in-Underground-Mines" class="headerlink" title="DIS-Mine: Instance Segmentation for Disaster-Awareness in Poor-Light   Condition in Underground Mines"></a>DIS-Mine: Instance Segmentation for Disaster-Awareness in Poor-Light   Condition in Underground Mines</h2><p><strong>Authors:Mizanur Rahman Jewel, Mohamed Elmahallawy, Sanjay Madria, Samuel Frimpong</strong></p>
<p>Detecting disasters in underground mining, such as explosions and structural damage, has been a persistent challenge over the years. This problem is compounded for first responders, who often have no clear information about the extent or nature of the damage within the mine. The poor-light or even total darkness inside the mines makes rescue efforts incredibly difficult, leading to a tragic loss of life. In this paper, we propose a novel instance segmentation method called DIS-Mine, specifically designed to identify disaster-affected areas within underground mines under low-light or poor visibility conditions, aiding first responders in rescue efforts. DIS-Mine is capable of detecting objects in images, even in complete darkness, by addressing challenges such as high noise, color distortions, and reduced contrast. The key innovations of DIS-Mine are built upon four core components: i) Image brightness improvement, ii) Instance segmentation with SAM integration, iii) Mask R-CNN-based segmentation, and iv) Mask alignment with feature matching. On top of that, we have collected real-world images from an experimental underground mine, introducing a new dataset named ImageMine, specifically gathered in low-visibility conditions. This dataset serves to validate the performance of DIS-Mine in realistic, challenging environments. Our comprehensive experiments on the ImageMine dataset, as well as on various other datasets demonstrate that DIS-Mine achieves a superior F1 score of 86.0% and mIoU of 72.0%, outperforming state-of-the-art instance segmentation methods, with at least 15x improvement and up to 80% higher precision in object detection. </p>
<blockquote>
<p>æ£€æµ‹åœ°ä¸‹é‡‡çŸ¿ä¸­çš„ç¾å®³ï¼Œå¦‚çˆ†ç‚¸å’Œç»“æ„ç ´åï¼Œä¸€ç›´æ˜¯å¤šå¹´æ¥çš„ä¸€ä¸ªæŒç»­æŒ‘æˆ˜ã€‚å¯¹äºç»å¸¸æ²¡æœ‰å…³äºçŸ¿å†…æŸå®³ç¨‹åº¦æˆ–æ€§è´¨çš„æ˜ç¡®ä¿¡æ¯çš„ä¸€çº¿æ•‘æ´äººå‘˜æ¥è¯´ï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºä¸¥é‡ã€‚çŸ¿å†…å…‰çº¿æ˜æš—ç”šè‡³å®Œå…¨é»‘æš—ï¼Œä½¿å¾—æ•‘æ´å·¥ä½œæä¸ºå›°éš¾ï¼Œå¹¶å¯¼è‡´ç”Ÿå‘½æ‚²å‰§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œç§°ä¸ºDIS-Mineï¼Œä¸“é—¨ç”¨äºåœ¨å…‰çº¿ä¸è¶³æˆ–èƒ½è§åº¦å·®çš„æ¡ä»¶ä¸‹è¯†åˆ«åœ°ä¸‹çŸ¿è—ä¸­çš„å—ç¾åŒºåŸŸï¼Œä»¥ååŠ©ä¸€çº¿æ•‘æ´äººå‘˜è¿›è¡Œæ•‘æ´å·¥ä½œã€‚DIS-Mineèƒ½å¤Ÿåœ¨å›¾åƒä¸­æ£€æµ‹ç‰©ä½“ï¼Œç”šè‡³åœ¨å®Œå…¨é»‘æš—ä¸­ä¹Ÿèƒ½å¦‚æ­¤ï¼Œé€šè¿‡è§£å†³é«˜å™ªå£°ã€è‰²å½©å¤±çœŸå’Œå¯¹æ¯”åº¦é™ä½ç­‰æŒ‘æˆ˜ã€‚DIS-Mineçš„å…³é”®åˆ›æ–°ç‚¹åŸºäºå››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šiï¼‰å›¾åƒäº®åº¦æ”¹è¿›ï¼Œiiï¼‰å¸¦SAMé›†æˆçš„å®ä¾‹åˆ†å‰²ï¼Œiiiï¼‰åŸºäºMask R-CNNçš„åˆ†å‰²ï¼Œä»¥åŠivï¼‰ä¸ç‰¹å¾åŒ¹é…çš„æ©è†œå¯¹é½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä»å®éªŒæ€§åœ°ä¸‹çŸ¿åœºæ”¶é›†äº†ç°å®ä¸–ç•Œå›¾åƒï¼Œå¼•å…¥äº†ä¸€ä¸ªåä¸ºImageMineçš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯åœ¨ä½èƒ½è§åº¦æ¡ä»¶ä¸‹ä¸“é—¨æ”¶é›†çš„ã€‚è¯¥æ•°æ®é›†ç”¨äºéªŒè¯DIS-Mineåœ¨çœŸå®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ImageMineæ•°æ®é›†ä»¥åŠå…¶ä»–å„ä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDIS-Mineçš„F1åˆ†æ•°è¾¾åˆ°86.0%ï¼ŒmIoUè¾¾åˆ°72.0%ï¼Œä¼˜äºæœ€å…ˆè¿›å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œåœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è‡³å°‘æœ‰15å€çš„æ”¹è¿›ï¼Œæœ€é«˜å¯è¾¾80%çš„ç²¾ç¡®åº¦æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13544v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåœ°ä¸‹çŸ¿å±±ç¾éš¾æ£€æµ‹ï¼Œå¦‚çˆ†ç‚¸å’Œç»“æ„æŸåï¼Œä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å®ä¾‹åˆ†å‰²æ–¹æ³•DIS-Mineï¼Œèƒ½åœ¨ä½å…‰ç…§æˆ–ä½å¯è§æ€§æ¡ä»¶ä¸‹è¯†åˆ«çŸ¿å±±å†…çš„å—ç¾åŒºåŸŸï¼Œæœ‰åŠ©äºæ•‘æ´äººå‘˜å¼€å±•æ•‘æ´å·¥ä½œã€‚è¯¥æ–¹æ³•è§£å†³äº†å›¾åƒä¸­çš„é«˜å™ªå£°ã€è‰²å½©å¤±çœŸå’Œå¯¹æ¯”åº¦é™ä½ç­‰é—®é¢˜ï¼Œç”±å›¾åƒäº®åº¦æ”¹è¿›ã€å®ä¾‹åˆ†å‰²ä¸SAMé›†æˆç­‰å››ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆã€‚æ­¤å¤–ï¼Œè¿˜æ”¶é›†äº†ä¸€ä¸ªåä¸ºImageMineçš„æ–°æ•°æ®é›†æ¥éªŒè¯DIS-Mineåœ¨çœŸå®ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒDIS-Mineåœ¨ImageMineå’Œå…¶ä»–æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œå®ç°äº†F1åˆ†æ•°ä¸º86.0%ï¼ŒmIoUä¸º72.0%ï¼Œåœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢çš„ç²¾åº¦è‡³å°‘æé«˜äº†15å€ï¼Œæœ€é«˜æé«˜äº†80%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åœ°ä¸‹çŸ¿å±±ç¾éš¾æ£€æµ‹æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å…‰ç…§æˆ–ä½å¯è§æ€§æ¡ä»¶ä¸‹ã€‚</li>
<li>DIS-Mineæ˜¯ä¸€ç§æ–°å‹çš„å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åœ°ä¸‹çŸ¿å±±ç¾éš¾æ£€æµ‹ä¸­çš„éš¾é¢˜ã€‚</li>
<li>DIS-Mineå…·æœ‰åœ¨æ¶åŠ£ç¯å¢ƒä¸‹æ£€æµ‹å›¾åƒä¸­ç‰©ä½“çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é«˜å™ªå£°ã€è‰²å½©å¤±çœŸå’Œå¯¹æ¯”åº¦é™ä½çš„ç¯å¢ƒã€‚</li>
<li>DIS-MineåŒ…æ‹¬å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå›¾åƒäº®åº¦æ”¹è¿›ã€å®ä¾‹åˆ†å‰²ä¸SAMé›†æˆç­‰ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„æ•°æ®é›†ImageMineæ¥éªŒè¯DIS-Mineçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDIS-Mineåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å®ä¾‹åˆ†å‰²æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6764bc1684f78bd770ec03fcc25a1548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc55044b29d960619784ae0ab190c769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cd2b3a5a236fd43ce93b5df92bc6746.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09a2e22a81211b632cddf77063e7e18f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7d0d4e398c497f473489f7aa1f352f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10114466f3156d6df6ae09d46de3e48e.jpg" align="middle">
</details>




<h2 id="Zero-Shot-Automatic-Annotation-and-Instance-Segmentation-using-LLM-Generated-Datasets-Eliminating-Field-Imaging-and-Manual-Annotation-for-Deep-Learning-Model-Development"><a href="#Zero-Shot-Automatic-Annotation-and-Instance-Segmentation-using-LLM-Generated-Datasets-Eliminating-Field-Imaging-and-Manual-Annotation-for-Deep-Learning-Model-Development" class="headerlink" title="Zero-Shot Automatic Annotation and Instance Segmentation using   LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for   Deep Learning Model Development"></a>Zero-Shot Automatic Annotation and Instance Segmentation using   LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for   Deep Learning Model Development</h2><p><strong>Authors:Ranjan Sapkota, Achyut Paudel, Manoj Karkee</strong></p>
<p>Currently, deep learning-based instance segmentation for various applications (e.g., Agriculture) is predominantly performed using a labor-intensive process involving extensive field data collection using sophisticated sensors, followed by careful manual annotation of images, presenting significant logistical and financial challenges to researchers and organizations. The process also slows down the model development and training process. In this study, we presented a novel method for deep learning-based instance segmentation of apples in commercial orchards that eliminates the need for labor-intensive field data collection and manual annotation. Utilizing a Large Language Model (LLM), we synthetically generated orchard images and automatically annotated them using the Segment Anything Model (SAM) integrated with a YOLO11 base model. This method significantly reduces reliance on physical sensors and manual data processing, presenting a major advancement in â€œAgricultural AIâ€. The synthetic, auto-annotated dataset was used to train the YOLO11 model for Apple instance segmentation, which was then validated on real orchard images. The results showed that the automatically generated annotations achieved a Dice Coefficient of 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask annotations. All YOLO11 configurations, trained solely on these synthetic datasets with automated annotations, accurately recognized and delineated apples, highlighting the methodâ€™s efficacy. Specifically, the YOLO11m-seg configuration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on test images collected from a commercial orchard. Additionally, the YOLO11l-seg configuration outperformed other models in validation on 40 LLM-generated images, achieving the highest mask precision and mAP@50 metrics.   Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM </p>
<blockquote>
<p>ç›®å‰ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å„ç§åº”ç”¨ï¼ˆä¾‹å¦‚å†œä¸šï¼‰ä¸­çš„å®ä¾‹åˆ†å‰²ä¸»è¦é‡‡ç”¨åŠ³åŠ¨å¯†é›†å‹çš„æµç¨‹ï¼Œæ¶‰åŠä½¿ç”¨ç²¾å¯†ä¼ æ„Ÿå™¨è¿›è¡Œå¤§é‡çš„ç”°é—´æ•°æ®æ”¶é›†ï¼Œä»¥åŠå¯¹å›¾åƒè¿›è¡Œä»”ç»†çš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œè¿™ç»™ç ”ç©¶è€…å’Œç»„ç»‡å¸¦æ¥äº†é‡å¤§çš„åå‹¤å’Œè´¢åŠ¡æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¹Ÿå‡ç¼“äº†æ¨¡å‹å’Œè®­ç»ƒçš„å¼€å‘è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å•†ä¸šæœå›­è‹¹æœå®ä¾‹åˆ†å‰²çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€è¿›è¡ŒåŠ³åŠ¨å¯†é›†å‹çš„ç”°é—´æ•°æ®æ”¶é›†å’Œæ‰‹åŠ¨æ ‡æ³¨ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆæˆç”Ÿæˆæœå›­å›¾åƒï¼Œå¹¶ä½¿ç”¨ä¸YOLO11åŸºç¡€æ¨¡å‹é›†æˆçš„Segment Anything Modelï¼ˆSAMï¼‰è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¯¹ç‰©ç†ä¼ æ„Ÿå™¨å’Œæ‰‹åŠ¨æ•°æ®å¤„ç†çš„ä¾èµ–ï¼Œæ˜¯â€œå†œä¸šäººå·¥æ™ºèƒ½â€é¢†åŸŸçš„ä¸€é¡¹é‡å¤§è¿›å±•ã€‚åˆæˆçš„è‡ªåŠ¨æ ‡æ³¨æ•°æ®é›†ç”¨äºè®­ç»ƒYOLO11è‹¹æœå®ä¾‹åˆ†å‰²æ¨¡å‹ï¼Œå¹¶åœ¨çœŸå®çš„æœå›­å›¾åƒä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œè‡ªåŠ¨ç”Ÿæˆçš„æ ‡æ³¨è¾¾åˆ°äº†Diceç³»æ•°ä¸º0.9513å’ŒIoUä¸º0.9303ï¼ŒéªŒè¯äº†æ©è†œæ ‡æ³¨çš„å‡†ç¡®æ€§å’Œé‡å åº¦ã€‚æ‰€æœ‰ä»…åœ¨è‡ªåŠ¨æ ‡æ³¨çš„åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„YOLO11é…ç½®ï¼Œéƒ½èƒ½å‡†ç¡®è¯†åˆ«å’Œåˆ’åˆ†è‹¹æœï¼Œçªæ˜¾äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯YOLO11m-segé…ç½®åœ¨æ¥è‡ªå•†ä¸šæœå›­çš„æµ‹è¯•å›¾åƒä¸Šè¾¾åˆ°äº†0.902çš„æ©è†œç²¾åº¦å’Œ0.833çš„mAP@50æŒ‡æ ‡ã€‚æ­¤å¤–ï¼ŒYOLO11l-segé…ç½®åœ¨40å¼ LLMç”Ÿæˆçš„å›¾åƒéªŒè¯ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè·å¾—äº†æœ€é«˜çš„æ©è†œç²¾åº¦å’ŒmAP@50æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11285v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‹¹æœå®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆæˆå›¾åƒè‡ªåŠ¨ç”Ÿæˆå’Œè‡ªåŠ¨æ ‡æ³¨æŠ€æœ¯ï¼Œå¤§å¹…å‡å°‘äº†äººåŠ›å¯†é›†å‹çš„å®åœ°æ•°æ®æ”¶é›†ä¸æ‰‹åŠ¨æ ‡æ³¨å·¥ä½œã€‚ç ”ç©¶åˆ©ç”¨Segment Anything Model (SAM)ä¸YOLO11åŸºç¡€æ¨¡å‹ï¼Œå®ç°å¯¹è‹¹æœå®ä¾‹çš„è‡ªåŠ¨åˆ†å‰²ï¼Œå¹¶åœ¨å•†ä¸šæœå›­çš„å®æ‹å›¾åƒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¯¹ä¼ ç»Ÿç‰©ç†ä¼ æ„Ÿå™¨å’Œæ‰‹åŠ¨æ•°æ®å¤„ç†çš„ä¾èµ–ï¼Œæ˜¯å†œä¸šäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€å¤§è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è‹¹æœå®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œå®ç°äº†å›¾åƒçš„è‡ªåŠ¨æ ‡æ³¨å’Œåˆ†å‰²ã€‚</li>
<li>æ–¹æ³•é€šè¿‡åˆæˆå›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œå¤§å¹…å‡å°‘äº†å®åœ°æ•°æ®æ”¶é›†å’Œæ‰‹åŠ¨æ ‡æ³¨çš„å·¥ä½œé‡ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨äº†Segment Anything Model (SAM)ä¸YOLO11æ¨¡å‹è¿›è¡Œè‹¹æœå®ä¾‹çš„è‡ªåŠ¨åˆ†å‰²ã€‚</li>
<li>è‡ªåŠ¨åŒ–ç”Ÿæˆçš„æ³¨é‡Šåœ¨çœŸå®æœå›­å›¾åƒä¸Šå–å¾—äº†è¾ƒé«˜çš„åˆ†å‰²æ•ˆæœï¼ŒDiceç³»æ•°ä¸º0.9513ï¼ŒIoUä¸º0.9303ã€‚</li>
<li>YOLO11æ¨¡å‹çš„ä¸åŒé…ç½®åœ¨ä»…ä½¿ç”¨åˆæˆæ•°æ®é›†å’Œè‡ªåŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œå‡èƒ½å‡†ç¡®è¯†åˆ«å’Œåˆ†å‰²è‹¹æœã€‚</li>
<li>YOLO11m-segé…ç½®åœ¨æµ‹è¯•å›¾åƒä¸Šå–å¾—äº†è¾ƒé«˜çš„æ©è†œç²¾åº¦å’ŒmAP@50æŒ‡æ ‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11ab0cab69ed14f80746314532e3223f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f5db900e8a911f086756e6cc7dab803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c31d0c8c265caa2a7bc605af0f973d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63b83791d5ab043c9c80f866aff71cba.jpg" align="middle">
</details>




<h2 id="Horticultural-Temporal-Fruit-Monitoring-via-3D-Instance-Segmentation-and-Re-Identification-using-Point-Clouds"><a href="#Horticultural-Temporal-Fruit-Monitoring-via-3D-Instance-Segmentation-and-Re-Identification-using-Point-Clouds" class="headerlink" title="Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and   Re-Identification using Point Clouds"></a>Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and   Re-Identification using Point Clouds</h2><p><strong>Authors:Daniel Fusaro, Federico Magistri, Jens Behley, Alberto Pretto, Cyrill Stachniss</strong></p>
<p>Robotic fruit monitoring is a key step toward automated agricultural production systems. Robots can significantly enhance plant and temporal fruit monitoring by providing precise, high-throughput assessments that overcome the limitations of traditional manual methods. Fruit monitoring is a challenging task due to the significant variation in size, shape, orientation, and occlusion of fruits. Also, fruits may be harvested or newly grown between recording sessions. Most methods are 2D image-based and they lack the 3D structure, depth, and spatial information, which represent key aspects of fruit monitoring. 3D colored point clouds, instead, can offer this information but they introduce challenges such as their sparsity and irregularity. In this paper, we present a novel approach for temporal fruit monitoring that addresses point clouds collected in a greenhouse over time. Our method segments fruits using a learning-based instance segmentation approach directly on the point cloud. Each segmented fruit is processed by a 3D sparse convolutional neural network to extract descriptors, which are used in an attention-based matching network to associate fruits with their instances from previous data collections. Experimental results on a real dataset of strawberries demonstrate that our approach outperforms other methods for fruits re-identification over time, allowing for precise temporal fruit monitoring in real and complex scenarios. </p>
<blockquote>
<p>æœºå™¨äººæ°´æœç›‘æµ‹æ˜¯å‘è‡ªåŠ¨åŒ–å†œä¸šç”Ÿäº§ç³»ç»Ÿè¿ˆè¿›çš„å…³é”®ä¸€æ­¥ã€‚æœºå™¨äººé€šè¿‡æä¾›ç²¾ç¡®ã€é«˜é€šé‡çš„è¯„ä¼°ï¼Œèƒ½å¤Ÿæå¤§åœ°å¢å¼ºæ¤ç‰©å’Œä¸´æ—¶æ°´æœç›‘æµ‹ï¼Œå…‹æœä¼ ç»Ÿäººå·¥æ–¹æ³•çš„å±€é™æ€§ã€‚æ°´æœç›‘æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæ°´æœçš„å¤§å°ã€å½¢çŠ¶ã€æ–¹å‘å’Œé®æŒ¡å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨è®°å½•æœŸé—´å¯èƒ½ä¼šæœ‰æ°´æœè¢«é‡‡æ‘˜æˆ–æ–°é•¿å‡ºæ¥ã€‚å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯åŸºäº2Då›¾åƒçš„ï¼Œç¼ºä¹3Dç»“æ„ã€æ·±åº¦å’Œç©ºé—´ä¿¡æ¯ï¼Œè¿™äº›æ­£æ˜¯æ°´æœç›‘æµ‹çš„å…³é”®æ–¹é¢ã€‚ç›¸åï¼Œ3Då½©è‰²ç‚¹äº‘å¯ä»¥æä¾›è¿™äº›ä¿¡æ¯ï¼Œä½†å®ƒä»¬å¸¦æ¥äº†ç¨€ç–æ€§å’Œä¸è§„åˆ™æ€§ç­‰æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºä¸´æ—¶æ°´æœç›‘æµ‹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¤„ç†åœ¨æ¸©å®¤ä¸­éšæ—¶é—´æ”¶é›†çš„ç‚¹äº‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥åœ¨ç‚¹äº‘ä¸Šä½¿ç”¨åŸºäºå­¦ä¹ çš„å®ä¾‹åˆ†å‰²æ–¹æ³•è¿›è¡Œæ°´æœåˆ†å‰²ã€‚æ¯ä¸ªåˆ†å‰²å‡ºæ¥çš„æ°´æœéƒ½ä¼šé€šè¿‡3Dç¨€ç–å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä»¥æå–æè¿°ç¬¦ï¼Œè¿™äº›æè¿°ç¬¦ç”¨äºåŸºäºæ³¨æ„åŠ›çš„åŒ¹é…ç½‘ç»œï¼Œå°†æ°´æœä¸ä»¥å‰çš„æ•°æ®æ”¶é›†ä¸­çš„å®ä¾‹è¿›è¡Œå…³è”ã€‚åœ¨è‰è“çš„å®é™…æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿æ—¶é—´æ°´æœå†è¯†åˆ«æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…è®¸åœ¨çœŸå®å’Œå¤æ‚åœºæ™¯ä¸­è¿›è¡Œç²¾ç¡®çš„ä¸´æ—¶æ°´æœç›‘æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07799v1">PDF</a> Submitted to IEEE Robotics and Automation Letters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç‚¹äº‘çš„æ—¶ç©ºæ°´æœç›‘æµ‹æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨ç‚¹äº‘ä¸Šè¿›è¡Œå­¦ä¹ å‹çš„å®ä¾‹åˆ†å‰²ï¼Œå¹¶é€šè¿‡3Dç¨€ç–å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†æ¯ä¸ªåˆ†å‰²çš„æ°´æœï¼Œä»¥æå–æè¿°ç¬¦å·ã€‚è¿™äº›æè¿°ç¬¦å·ç”¨äºåŸºäºæ³¨æ„åŠ›çš„åŒ¹é…ç½‘ç»œï¼Œå°†ä¸ä»¥å‰çš„æ•°æ®æ”¶é›†ä¸­çš„æ°´æœå®ä¾‹è¿›è¡Œå…³è”ã€‚åœ¨è‰è“çš„çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„ç°å®åœºæ™¯ä¸­ï¼Œå¯¹æ°´æœçš„å†è¯†åˆ«èƒ½åŠ›ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¯å®ç°ç²¾ç¡®çš„æ—¶ç©ºæ°´æœç›‘æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´æœç›‘æµ‹æ˜¯å†œä¸šè‡ªåŠ¨åŒ–ç”Ÿäº§ç³»ç»Ÿä¸­çš„å…³é”®æ­¥éª¤ï¼Œæœºå™¨äººå¯ä»¥å…‹æœä¼ ç»Ÿæ‰‹åŠ¨æ–¹æ³•çš„å±€é™æ€§ï¼Œæä¾›ç²¾ç¡®ã€é«˜é€šé‡çš„è¯„ä¼°ç»“æœã€‚</li>
<li>æ°´æœç›‘æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ°´æœçš„å¤§å°ã€å½¢çŠ¶ã€æ–¹å‘å’Œé®æŒ¡æœ‰å¾ˆå¤§çš„å˜åŒ–ï¼Œä¸”å¯èƒ½åœ¨è®°å½•æœŸé—´è¢«é‡‡æ‘˜æˆ–æ–°é•¿å‡ºæ¥ã€‚</li>
<li>å¤§å¤šæ•°æ–¹æ³•åŸºäºäºŒç»´å›¾åƒï¼Œç¼ºä¹ä¸‰ç»´ç»“æ„å’Œæ·±åº¦ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>ç‚¹äº‘æŠ€æœ¯å¯ä»¥å…‹æœäºŒç»´å›¾åƒçš„é™åˆ¶ï¼Œæä¾›ä¸‰ç»´ç»“æ„ä¿¡æ¯ï¼Œä½†ç‚¹äº‘çš„ç¨€ç–æ€§å’Œä¸è§„åˆ™æ€§å¼•å…¥æ–°çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‚¹äº‘çš„æ°´æœç›‘æµ‹æ–°æ–¹æ³•ï¼Œé€šè¿‡å®ä¾‹åˆ†å‰²å’Œå·ç§¯ç¥ç»ç½‘ç»œå¤„ç†æ°´æœï¼Œå®ç°ç²¾ç¡®ç›‘æµ‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„ç°å®åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿè¯†åˆ«éšæ—¶é—´å˜åŒ–çš„æ°´æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf335d218ffa93fd9752e49c9079c6e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62ddf7fac5d4d62a95922014e91b3f1f.jpg" align="middle">
</details>




<h2 id="GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection"></a>GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection</h2><p><strong>Authors:Jiyul Ham, Yonggon Jung, Jun-Geol Baek</strong></p>
<p>Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting anomalous patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP">https://github.com/YUL-git/GlocalCLIP</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰å¯¹äºåœ¨æ— éœ€è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹æ£€æµ‹ç›®æ ‡æ•°æ®é›†ä¸­çš„å¼‚å¸¸æ¨¡å¼è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡åŸŸä¸è®­ç»ƒæ•°æ®ä¹‹é—´å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–ç”±äºè®¿é—®å—é™è€Œå¯¼è‡´æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚å°½ç®¡æœ€è¿‘é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨ç±»åˆ«è¯­ä¹‰çš„å­¦ä¹ ï¼Œè¿™ä½¿å¾—å®ƒä»¬ç›´æ¥åº”ç”¨äºZSADå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºäº†GlocalCLIPæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç‹¬ç‰¹åœ°åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºå¹¶è¿›è¡Œè”åˆä¼˜åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä½¿å¾—å¯¹è±¡æ— å…³çš„å±€éƒ¨è¯­ä¹‰æç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰é€šç”¨æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ï¼Œè€Œæ— éœ€ä¾èµ–å›¾åƒä¸­çš„ç‰¹å®šå¯¹è±¡ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´æ¥å®Œå–„æ–‡æœ¬æç¤ºï¼Œä»¥å®ç°æ›´ç²¾ç¡®çš„è°ƒæ•´ã€‚åœ¨è§†è§‰ç¼–ç å™¨æ–¹é¢ï¼Œæˆ‘ä»¬åº”ç”¨V-Væ³¨æ„åŠ›å±‚æ¥æ•æ‰è¯¦ç»†çš„å±€éƒ¨å›¾åƒç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†å±€éƒ¨å¯¹æ¯”å­¦ä¹ ï¼Œä»¥æé«˜å…¨å±€å’Œå±€éƒ¨æç¤ºçš„äº’è¡¥å­¦ä¹ ï¼Œæœ‰æ•ˆæ£€æµ‹ä¸åŒé¢†åŸŸçš„å¼‚å¸¸æ¨¡å¼ã€‚GlocalCLIPåœ¨ZSADä¸­çš„æ³›åŒ–æ€§èƒ½åœ¨15ä¸ªæ¥è‡ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP%E3%80%82">https://github.com/YUL-git/GlocalCLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06071v3">PDF</a> 29 pages, 36 figures</p>
<p><strong>Summary</strong><br>é’ˆå¯¹é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯å½“ç›®æ ‡æ•°æ®é›†ä¸è®­ç»ƒæ•°æ®å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–æ•°æ®ç¨€ç¼ºæ—¶ï¼Œæå‡ºä¸€ç§åä¸ºGlocalCLIPçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºå¹¶è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œæœ‰æ•ˆæ•æ‰æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´å’ŒV-Væ³¨æ„åŠ›å±‚å¢å¼ºå­¦ä¹ æ€§èƒ½ã€‚æœ€åé€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†GlocalCLIPåœ¨ä¸åŒé¢†åŸŸçš„å®é™…æ•°æ®é›†ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ZSADåœ¨å¤„ç†å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–æ•°æ®ç¨€ç¼ºçš„é—®é¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>GlocalCLIPæ–¹æ³•é€šè¿‡åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºè¿›è¡Œä¼˜åŒ–ï¼Œèƒ½å¤Ÿæ•æ‰æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ã€‚</li>
<li>æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´æé«˜äº†ç²¾åº¦ã€‚</li>
<li>V-Væ³¨æ„åŠ›å±‚æœ‰åŠ©äºæ•æ‰å±€éƒ¨å›¾åƒç‰¹å¾ã€‚</li>
<li>GlocalCLIPå¼•å…¥å¯¹æ¯”å­¦ä¹ ä»¥å¢å¼ºå…¨å±€å’Œå±€éƒ¨æç¤ºçš„äº’è¡¥å­¦ä¹ ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†GlocalCLIPçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26e62baf3d484faf017e96a0933d2b89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1125ab6e3ab5117d2b7066fa7f664de1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f8c17425f03137645e1037af7d759b4.jpg" align="middle">
</details>




<h2 id="CRT-Fusion-Camera-Radar-Temporal-Fusion-Using-Motion-Information-for-3D-Object-Detection"><a href="#CRT-Fusion-Camera-Radar-Temporal-Fusion-Using-Motion-Information-for-3D-Object-Detection" class="headerlink" title="CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for   3D Object Detection"></a>CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for   3D Object Detection</h2><p><strong>Authors:Jisong Kim, Minjae Seong, Jun Won Choi</strong></p>
<p>Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the birdâ€™s-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and birdâ€™s-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection. </p>
<blockquote>
<p>å‡†ç¡®ä¸”ç¨³å®šçš„3Dç‰©ä½“æ£€æµ‹æ˜¯è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚è™½ç„¶æœ€è¿‘çš„é›·è¾¾-ç›¸æœºèåˆæ–¹æ³•åœ¨é¸Ÿç°å›¾ï¼ˆBEVï¼‰è¡¨ç¤ºçš„ä¿¡æ¯èåˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°æ•æ‰åŠ¨æ€ç‰©ä½“çš„è¿åŠ¨ï¼Œå¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸­çš„æ€§èƒ½å—é™ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ä»‹ç»äº†CRT-Fusionï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ—¶é—´ä¿¡æ¯èå…¥é›·è¾¾-ç›¸æœºèåˆçš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå¤šè§†å›¾èåˆï¼ˆMVFï¼‰ã€è¿åŠ¨ç‰¹å¾ä¼°è®¡å™¨ï¼ˆMFEï¼‰å’Œè¿åŠ¨å¼•å¯¼æ—¶é—´èåˆï¼ˆMGTFï¼‰ã€‚MVFæ¨¡å—åœ¨ç›¸æœºè§†å›¾å’Œé¸Ÿç°å›¾ä¸­èåˆé›·è¾¾å’Œå›¾åƒç‰¹å¾ï¼Œä»è€Œç”Ÿæˆæ›´ç²¾ç¡®çš„ç»Ÿä¸€BEVè¡¨ç¤ºã€‚MFEæ¨¡å—åŒæ—¶æ‰§è¡Œä¸¤ä¸ªä»»åŠ¡ï¼šåƒç´ çº§é€Ÿåº¦ä¿¡æ¯ä¼°è®¡å’ŒBEVåˆ†å‰²ã€‚åŸºäºMFEæ¨¡å—è·å¾—çš„é€Ÿåº¦å’Œå ç”¨åˆ†æ•°å›¾ï¼ŒMGTFæ¨¡å—ä»¥é€’å½’æ–¹å¼å¯¹é½å’Œèåˆå¤šä¸ªæ—¶é—´æˆ³çš„ç‰¹å¾å›¾ã€‚é€šè¿‡è€ƒè™‘åŠ¨æ€ç‰©ä½“çš„è¿åŠ¨ï¼ŒCRT-Fusionå¯ä»¥ç”Ÿæˆç¨³å®šçš„BEVç‰¹å¾å›¾ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„nuScenesæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCRT-Fusionåœ¨åŸºäºé›·è¾¾-ç›¸æœºçš„3Dç‰©ä½“æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨NDSæ–¹é¢è¾ƒä¹‹å‰æœ€ä½³æ–¹æ³•æé«˜äº†+1.7%ï¼ŒåŒæ—¶åœ¨mAPæ–¹é¢ä¹Ÿè¶…è¶Šäº†é¢†å…ˆæ–¹æ³•+1.4%ã€‚è¿™ä¸¤ä¸ªæŒ‡æ ‡çš„å¤§å¹…æå‡å±•ç¤ºäº†æˆ‘ä»¬çš„èåˆç­–ç•¥åœ¨æé«˜3Dç‰©ä½“æ£€æµ‹çš„å¯é æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03013v3">PDF</a> Accepted at NeurIPS2024</p>
<p><strong>Summary</strong></p>
<p>é›·è¾¾ä¸ç›¸æœºèåˆæŠ€æœ¯åœ¨ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨å¯¹åŠ¨æ€ç‰©ä½“è¿åŠ¨æ•æ‰ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºçš„CRT-Fusionæ¡†æ¶é€šè¿‡é›†æˆæ—¶é—´ä¿¡æ¯ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚å®ƒåŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå¤šè§†è§’èåˆï¼ˆMVFï¼‰ã€è¿åŠ¨ç‰¹å¾ä¼°è®¡å™¨ï¼ˆMFEï¼‰å’Œè¿åŠ¨å¼•å¯¼æ—¶é—´èåˆï¼ˆMGTFï¼‰ã€‚CRT-Fusionåœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå…¶åœ¨é›·è¾¾-ç›¸æœºåŸºçš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CRT-Fusionæ¡†æ¶é€šè¿‡æ•´åˆæ—¶é—´ä¿¡æ¯ï¼Œæé«˜äº†é›·è¾¾ä¸ç›¸æœºèåˆæŠ€æœ¯çš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>MVFæ¨¡å—åœ¨ç›¸æœºè§†è§’å’Œé¸Ÿç°è§†è§’èåˆé›·è¾¾å’Œå›¾åƒç‰¹å¾ï¼Œç”Ÿæˆæ›´ç²¾ç¡®çš„ç»Ÿä¸€é¸Ÿç°è§†å›¾è¡¨ç¤ºã€‚</li>
<li>MFEæ¨¡å—åŒæ—¶æ‰§è¡Œåƒç´ çº§é€Ÿåº¦ä¿¡æ¯ä¼°è®¡å’Œé¸Ÿç°è§†å›¾åˆ†å‰²ä¸¤ä¸ªä»»åŠ¡ã€‚</li>
<li>MGTFæ¨¡å—åŸºäºé€Ÿåº¦å’Œå ç”¨åˆ†æ•°å›¾ï¼Œå¯¹è·¨å¤šä¸ªæ—¶é—´æˆ³çš„ç‰¹å¾å›¾è¿›è¡Œå¯¹é½å’Œèåˆã€‚</li>
<li>CRT-Fusionåœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨NDSå’ŒmAPæŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶å¢å¼ºäº†ä¸‰ç»´ç‰©ä½“æ£€æµ‹çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87f4cdb6a0088c13dd0cc5e7c10e70a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b3ad89deada39b999855110c997390d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7609be08a13017629b14fe29e506d7a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b029a5122f8eb9b58425a375ed0e63.jpg" align="middle">
</details>




<h2 id="On-the-Black-box-Explainability-of-Object-Detection-Models-for-Safe-and-Trustworthy-Industrial-Applications"><a href="#On-the-Black-box-Explainability-of-Object-Detection-Models-for-Safe-and-Trustworthy-Industrial-Applications" class="headerlink" title="On the Black-box Explainability of Object Detection Models for Safe and   Trustworthy Industrial Applications"></a>On the Black-box Explainability of Object Detection Models for Safe and   Trustworthy Industrial Applications</h2><p><strong>Authors:Alain Andres, Aitor Martinez-Seras, Ibai LaÃ±a, Javier Del Ser</strong></p>
<p>In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic explainability methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP) technique based on segmentation-based masks to generate explanations. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in a scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used. </p>
<blockquote>
<p>åœ¨äººæœºäº’åŠ¨é¢†åŸŸï¼Œäººå·¥æ™ºèƒ½å·²æˆä¸ºåŠ é€Ÿæ•°æ®å»ºæ¨¡ä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚ç‰©ä½“æ£€æµ‹æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹¶å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶å’Œè§†é¢‘ç›‘æ§ç­‰é‡è¦é¢†åŸŸã€‚ç„¶è€Œï¼Œå…¶åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„é‡‡ç”¨ä»ç„¶æœ‰é™ï¼Œå› ä¸ºé”™è¯¯å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡åæœã€‚å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è®¸å¤šç°æœ‰æŠ€æœ¯éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹äºç‰©ä½“æ£€æµ‹è€Œè¨€æ•ˆæœè¾ƒå·®ï¼Œä¸”éä¸“ä¸šäººå‘˜éš¾ä»¥è§£é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºé’ˆå¯¹ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æ¨¡å‹æ— å…³å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå¹¶æå‡ºD-MFPPï¼Œå®ƒæ˜¯åŸºäºåˆ†å‰²æ©è†œæ‰©å±•çš„å½¢æ€å­¦ç‰‡æ®µæ‰°åŠ¨é‡‘å­—å¡”ï¼ˆMFPPï¼‰æŠ€æœ¯ï¼Œç”¨äºç”Ÿæˆè§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†D-Deletionï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¿ å®æ€§å’Œå®šä½æ€§çš„æ–°å‹æŒ‡æ ‡ï¼Œä¸“é—¨ä¸ºç‰©ä½“æ£€æµ‹å™¨çš„ç‹¬ç‰¹éœ€æ±‚è€Œè®¾è®¡ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„å·¥ä¸šå’Œæœºå™¨äººæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™äº›æ–¹æ³•ï¼Œç ”ç©¶äº†æ©è†œæ•°é‡ã€æ¨¡å‹å¤§å°å’Œå›¾åƒåˆ†è¾¨ç‡ç­‰å‚æ•°å¯¹è§£é‡Šè´¨é‡çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒä½¿ç”¨äº†åº”ç”¨äºä¸¤ä¸ªå®‰å…¨å…³é”®çš„æœºå™¨äººç¯å¢ƒçš„å•é˜¶æ®µç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼šä¸€æ˜¯å¯¹äººçš„å®‰å…¨è‡³å…³é‡è¦çš„å…±äº«äººæœºå·¥ä½œç©ºé—´ï¼›äºŒæ˜¯ç”µæ± ç»„ä»¶è£…é…åŒºï¼Œç”±äºé«˜é£é™©ç»„ä»¶çš„å­˜åœ¨ï¼Œå®‰å…¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“åœºæ™¯ä¸­å‡ºç°åŒä¸€ç±»çš„å¤šä¸ªå…ƒç´ æ—¶ï¼ŒD-Deletionæœ‰æ•ˆåœ°è¡¡é‡äº†è§£é‡Šçš„æ€§èƒ½ï¼Œè€ŒD-MFPPåœ¨ä½¿ç”¨çš„æ©è†œæ•°é‡è¾ƒå°‘æ—¶æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00818v2">PDF</a> 14 pages, 10 figures, 6 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨äººæœºäº’åŠ¨é¢†åŸŸï¼Œäººå·¥æ™ºèƒ½å·²æˆä¸ºåŠ é€Ÿæ•°æ®å»ºæ¨¡ä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚å¯¹è±¡æ£€æµ‹æ–¹æ³•å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶å’Œè§†é¢‘ç›‘æ§ç­‰é‡è¦é¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„é‡‡ç”¨ä»å—åˆ°é™åˆ¶ï¼Œå› ä¸ºä¸€æ—¦å‡ºç°é”™è¯¯å¯èƒ½ä¼šå¸¦æ¥ä¸¥é‡åæœã€‚å¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è®¸å¤šç°æœ‰æŠ€æœ¯éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹äºå¯¹è±¡æ£€æµ‹çš„åº”ç”¨æ•ˆæœè¾ƒå·®ï¼Œå¹¶ä¸”éš¾ä»¥è¢«éä¸“ä¸šäººå£«è§£è¯»ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºæ¨¡å‹ä¸å¯çŸ¥çš„å¯è§£é‡Šæ€§æ–¹æ³•ç”¨äºå¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œå¹¶æå‡ºD-MFPPï¼Œå®ƒæ˜¯åŸºäºåˆ†å‰²æ©è†œæŠ€æœ¯çš„Morphological Fragmental Perturbation Pyramidï¼ˆMFPPï¼‰æŠ€æœ¯çš„æ‰©å±•ï¼Œç”¨äºç”Ÿæˆè§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†D-Deletionï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¿ å®æ€§å’Œå®šä½çš„æ–°æŒ‡æ ‡ï¼Œä¸“ä¸ºæ»¡è¶³å¯¹è±¡æ£€æµ‹å™¨çš„ç‹¬ç‰¹éœ€æ±‚è€Œè®¾è®¡ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„å·¥ä¸šç•Œå’Œæœºå™¨äººæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™äº›æ–¹æ³•ï¼Œç ”ç©¶äº†æ©è†œæ•°é‡ã€æ¨¡å‹å¤§å°å’Œå›¾åƒåˆ†è¾¨ç‡ç­‰å‚æ•°å¯¹è§£é‡Šè´¨é‡çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒä½¿ç”¨äº†å•é˜¶æ®µå¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œå¹¶åº”ç”¨äºä¸¤ä¸ªå®‰å…¨è‡³å…³é‡è¦çš„æœºå™¨äººç¯å¢ƒï¼šä¸€æ˜¯å¯¹äººæœºå™¨äººå…±äº«å·¥ä½œç©ºé—´å†…çš„è£…é…ç”µæ± ç»„ä»¶åŒºåŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“åœºæ™¯ä¸­åŒä¸€ç±»çš„å…ƒç´ å‡ºç°å¤šæ¬¡æ—¶ï¼ŒD-Deletionèƒ½æœ‰æ•ˆè¡¡é‡è§£é‡Šçš„æ€§èƒ½ï¼Œè€ŒD-MFPPåœ¨æ©è†œæ•°é‡è¾ƒå°‘æ—¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨æ•°æ®å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„åŠ é€Ÿèƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¯¹è±¡æ£€æµ‹é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>å¯¹è±¡æ£€æµ‹æ–¹æ³•åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„é‡‡ç”¨å—åˆ°é™åˆ¶ï¼Œå› ä¸ºé”™è¯¯å¯èƒ½å¯¼è‡´ä¸¥é‡åæœã€‚</li>
<li>å¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½æ–¹æ³•åœ¨å¯¹è±¡æ£€æµ‹é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æŠ€æœ¯å¤šé’ˆå¯¹ç‰¹å®šæ¨¡å‹ï¼Œéš¾ä»¥è¢«éä¸“ä¸šäººå£«è§£è¯»ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºD-MFPPå’ŒD-Deletionæ–¹æ³•ï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆè§£é‡Šå’Œè¯„ä¼°å¯¹è±¡æ£€æµ‹æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒD-Deletionèƒ½æœ‰æ•ˆè¡¡é‡åœ¨åœºæ™¯ä¸­åŒä¸€ç±»å…ƒç´ å‡ºç°å¤šæ¬¡æ—¶çš„è§£é‡Šæ€§èƒ½ã€‚</li>
<li>D-MFPPåœ¨è¾ƒå°‘æ©è†œä½¿ç”¨æ—¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ab8a4a024def8d48f315e182c66098e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1193c8fbfe78ee919beac27e5ad03b95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9946c0b55f3fa4b4fd4cf309b113ede.jpg" align="middle">
</details>




<h2 id="MV-Adapter-Enhancing-Underwater-Instance-Segmentation-via-Adaptive-Channel-Attention"><a href="#MV-Adapter-Enhancing-Underwater-Instance-Segmentation-via-Adaptive-Channel-Attention" class="headerlink" title="MV-Adapter: Enhancing Underwater Instance Segmentation via Adaptive   Channel Attention"></a>MV-Adapter: Enhancing Underwater Instance Segmentation via Adaptive   Channel Attention</h2><p><strong>Authors:Lianjun Liu</strong></p>
<p>Underwater instance segmentation is a fundamental and critical step in various underwater vision tasks. However, the decline in image quality caused by complex underwater environments presents significant challenges to existing segmentation models. While the state-of-the-art USIS-SAM model has demonstrated impressive performance, it struggles to effectively adapt to feature variations across different channels in addressing issues such as light attenuation, color distortion, and complex backgrounds. This limitation hampers its segmentation performance in challenging underwater scenarios. To address these issues, we propose the MarineVision Adapter (MV-Adapter). This module introduces an adaptive channel attention mechanism that enables the model to dynamically adjust the feature weights of each channel based on the characteristics of underwater images. By adaptively weighting features, the model can effectively handle challenges such as light attenuation, color shifts, and complex backgrounds. Experimental results show that integrating the MV-Adapter module into the USIS-SAM network architecture further improves the modelâ€™s overall performance, especially in high-precision segmentation tasks. On the USIS10K dataset, the module achieves improvements in key metrics such as mAP, AP50, and AP75 compared to competitive baseline models. </p>
<blockquote>
<p>æ°´ä¸‹å®ä¾‹åˆ†å‰²æ˜¯å„ç§æ°´ä¸‹è§†è§‰ä»»åŠ¡ä¸­çš„ä¸€é¡¹åŸºæœ¬ä¸”å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œç”±äºæ°´ä¸‹ç¯å¢ƒçš„å¤æ‚æ€§å¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™ï¼Œç»™ç°æœ‰çš„åˆ†å‰²æ¨¡å‹å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶ç›®å‰æœ€å…ˆè¿›çš„USIS-SAMæ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†åœ¨å¤„ç†å…‰è¡°å‡ã€è‰²å½©å¤±çœŸå’Œå¤æ‚èƒŒæ™¯ç­‰é—®é¢˜æ—¶ï¼Œå®ƒåœ¨é€‚åº”ä¸åŒé€šé“çš„ç‰¹å¾å˜åŒ–æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚è¿™ä¸€å±€é™æ€§é˜»ç¢äº†å®ƒåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ°´ä¸‹åœºæ™¯ä¸­çš„åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MarineVision Adapterï¼ˆMV-Adapterï¼‰ã€‚è¯¥æ¨¡å—å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ°´ä¸‹å›¾åƒçš„ç‰¹æ€§åŠ¨æ€è°ƒæ•´æ¯ä¸ªé€šé“çš„ç‰¹å¾æƒé‡ã€‚é€šè¿‡è‡ªé€‚åº”åœ°åŠ æƒç‰¹å¾ï¼Œè¯¥æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°åº”å¯¹å…‰è¡°å‡ã€è‰²å½©åç§»å’Œå¤æ‚èƒŒæ™¯ç­‰æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†MV-Adapteræ¨¡å—é›†æˆåˆ°USIS-SAMç½‘ç»œæ¶æ„ä¸­ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç²¾åº¦åˆ†å‰²ä»»åŠ¡ä¸­ã€‚åœ¨USIS10Kæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å—åœ¨ä¸»è¦æŒ‡æ ‡mAPã€AP50å’ŒAP75ä¸Šç›¸å¯¹äºç«äº‰åŸºå‡†æ¨¡å‹å–å¾—äº†æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00472v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ°´ä¸‹å®ä¾‹åˆ†å‰²æ˜¯æ°´ä¸‹è§†è§‰ä»»åŠ¡ä¸­çš„åŸºç¡€ä¸”å…³é”®çš„ä¸€æ­¥ã€‚å¤æ‚çš„æ°´ä¸‹ç¯å¢ƒå¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™ç»™ç°æœ‰çš„åˆ†å‰²æ¨¡å‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚è™½ç„¶ç›®å‰æœ€å…ˆè¿›çš„USIS-SAMæ¨¡å‹è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†åœ¨å¤„ç†å…‰è¡°å‡ã€è‰²å½©å¤±çœŸå’Œå¤æ‚èƒŒæ™¯ç­‰é—®é¢˜æ—¶ï¼Œå®ƒéš¾ä»¥é€‚åº”ä¸åŒé€šé“çš„ç‰¹å¾å˜åŒ–ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MarineVision Adapterï¼ˆMV-Adapterï¼‰æ¨¡å—ã€‚è¯¥æ¨¡å—å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ°´ä¸‹å›¾åƒçš„ç‰¹æ€§åŠ¨æ€è°ƒæ•´æ¯ä¸ªé€šé“çš„ç‰¹å¾æƒé‡ã€‚é€šè¿‡è‡ªé€‚åº”åœ°åŠ æƒç‰¹å¾ï¼Œæ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°åº”å¯¹å…‰è¡°å‡ã€è‰²å½©åç§»å’Œå¤æ‚èƒŒæ™¯ç­‰æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†MV-Adapteræ¨¡å—é›†æˆåˆ°USIS-SAMç½‘ç»œæ¶æ„ä¸­ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç²¾åº¦åˆ†å‰²ä»»åŠ¡ä¸­ã€‚åœ¨USIS10Kæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å—ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å…³é”®æŒ‡æ ‡mAPã€AP50å’ŒAP75ä¸Šå–å¾—äº†æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å®ä¾‹åˆ†å‰²æ˜¯æ°´ä¸‹è§†è§‰ä»»åŠ¡çš„åŸºç¡€ä¸”å…³é”®æ­¥éª¤ï¼Œä½†å¤æ‚çš„æ°´ä¸‹ç¯å¢ƒå¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™ç»™ç°æœ‰æ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>USIS-SAMæ¨¡å‹è™½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨é€‚åº”ä¸åŒé€šé“ç‰¹å¾å˜åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥åº”å¯¹å…‰è¡°å‡ã€è‰²å½©å¤±çœŸå’Œå¤æ‚èƒŒæ™¯ç­‰é—®é¢˜ã€‚</li>
<li>MarineVision Adapterï¼ˆMV-Adapterï¼‰æ¨¡å—é€šè¿‡å¼•å…¥è‡ªé€‚åº”é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½æ ¹æ®æ°´ä¸‹å›¾åƒç‰¹æ€§åŠ¨æ€è°ƒæ•´é€šé“ç‰¹å¾æƒé‡ã€‚</li>
<li>MV-Adapteræ¨¡å—é›†æˆåˆ°USIC-SAMç½‘ç»œæ¶æ„ä¸­ï¼Œæé«˜äº†æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç²¾åº¦åˆ†å‰²ä»»åŠ¡ä¸­ã€‚</li>
<li>åœ¨USIS10Kæ•°æ®é›†ä¸Šï¼ŒMV-Adapteræ¨¡å—åœ¨å…³é”®æŒ‡æ ‡ä¸Šæ”¹è¿›äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¦‚mAPã€AP50å’ŒAP75ã€‚</li>
<li>MV-Adapteræ¨¡å—çš„å‡ºç°æ˜¯ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹åœ¨åº”å¯¹æ°´ä¸‹å›¾åƒæŒ‘æˆ˜æ—¶çš„ä¸è¶³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4c2a1ce99b0664ec0bdb2098fc74664.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d5ef547594058228141b24e587cddb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be5f214ab1cd19945830583cf1dac4fc.jpg" align="middle">
</details>




<h2 id="NT-VOT211-A-Large-Scale-Benchmark-for-Night-time-Visual-Object-Tracking"><a href="#NT-VOT211-A-Large-Scale-Benchmark-for-Night-time-Visual-Object-Tracking" class="headerlink" title="NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking"></a>NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</h2><p><strong>Authors:Yu Liu, Arif Mahmood, Muhammad Haris Khan</strong></p>
<p>Many current visual object tracking benchmarks such as OTB100, NfS, UAV123, LaSOT, and GOT-10K, predominantly contain day-time scenarios while the challenges posed by the night-time has been less investigated. It is primarily because of the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. To this end, this paper presents NT-VOT211, a new benchmark tailored for evaluating visual object tracking algorithms in the challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NT-VOT211, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual object tracking, particularly in environments with suboptimal lighting. Besides, a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results is made publicly available. We believe that our NT-VOT211 benchmark will not only be instrumental in facilitating field deployment of VOT algorithms, but will also help VOT enhancements and it will unlock new real-world tracking applications. Our dataset and other assets can be found at: {<a target="_blank" rel="noopener" href="https://github.com/LiuYuML/NV-VOT211">https://github.com/LiuYuML/NV-VOT211</a>. </p>
<blockquote>
<p>å½“å‰è®¸å¤šè§†è§‰ç›®æ ‡è·Ÿè¸ªåŸºå‡†æµ‹è¯•ï¼Œå¦‚OTB100ã€NfSã€UAV123ã€LaSOTå’ŒGOT-10Kç­‰ï¼Œä¸»è¦åŒ…å«æ—¥é—´åœºæ™¯ï¼Œè€Œå¤œé—´åœºæ™¯æ‰€å¸¦æ¥çš„æŒ‘æˆ˜å´ç ”ç©¶è¾ƒå°‘ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¤§è§„æ¨¡ã€æ ‡æ³¨è‰¯å¥½çš„å¤œé—´åŸºå‡†æµ‹è¯•ï¼Œä»¥ä¸¥æ ¼è¯„ä¼°è·Ÿè¸ªç®—æ³•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†NV-VOT211ï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤œé—´æ¡ä»¶ä¸‹è§†è§‰ç›®æ ‡è·Ÿè¸ªç®—æ³•çš„æ€§èƒ½ã€‚NV-VOT211åŒ…å«211ä¸ªå¤šæ ·åŒ–çš„è§†é¢‘ï¼Œæä¾›211,000ä¸ªæ ‡æ³¨è‰¯å¥½çš„å¸§ï¼ŒåŒ…æ‹¬8ä¸ªå±æ€§ï¼Œå¦‚ç›¸æœºè¿åŠ¨ã€å˜å½¢ã€å¿«é€Ÿè¿åŠ¨ã€è¿åŠ¨æ¨¡ç³Šã€å¾®å°ç›®æ ‡ã€å¹²æ‰°ç‰©ã€é®æŒ¡å’Œè§†é‡å¤–ç­‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤œé—´è·Ÿè¸ªåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§£å†³å¤œé—´è·Ÿè¸ªåœºæ™¯æ‰€ç‰¹æœ‰çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚èƒ½è§åº¦ä¸è‰¯ã€å›¾åƒæ¨¡ç³Šå’Œå¹²æ‰°ç‰©ç­‰ã€‚é€šè¿‡å¯¹42ç§ä¸åŒçš„è·Ÿè¸ªç®—æ³•åœ¨NV-VOT211ä¸Šçš„ç»“æœè¿›è¡Œå…¨é¢åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›ç®—æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œçªæ˜¾äº†è§†è§‰ç›®æ ‡è·Ÿè¸ªåœ¨å…‰çº¿ä¸è¶³ç¯å¢ƒä¸­çš„å¢å¼ºæœºä¼šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ’è¡Œæ¦œï¼Œå±•ç¤ºæ€§èƒ½æ’åã€æ ‡æ³¨å·¥å…·ã€ç»¼åˆå…ƒä¿¡æ¯å’Œæ‰€æœ‰å¿…è¦ä»£ç ä»¥ä¾›ç»“æœå¤ç°ï¼Œç°å·²å…¬å¼€å¯ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„NV-VOT211åŸºå‡†æµ‹è¯•ä¸ä»…å°†æœ‰åŠ©äºä¿ƒè¿›VOTç®—æ³•çš„å®åœ°éƒ¨ç½²ï¼Œè¿˜å°†æœ‰åŠ©äºVOTçš„æ”¹è¿›ï¼Œå¹¶å°†è§£é”æ–°çš„ç°å®ä¸–ç•Œè·Ÿè¸ªåº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œå…¶ä»–èµ„äº§å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/LiuYuML/NV-VOT211%E3%80%82">https://github.com/LiuYuML/NV-VOT211ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20421v1">PDF</a> Oral Acceptance at the Asian Conference on Computer Vision (ACCV)   2024, Hanoi, Vietnam</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹å¤œé—´è§†è§‰ç›®æ ‡è·Ÿè¸ªçš„æ–°åŸºå‡†æ•°æ®é›†NT-VOT211ã€‚è¯¥æ•°æ®é›†åŒ…å«211ä¸ªè§†é¢‘å’Œè¶…è¿‡21ä¸‡ä¸ªæ ‡æ³¨è‰¯å¥½çš„å¸§ï¼Œæ¶µç›–8ç§å±æ€§ï¼Œæ˜¯ç›®å‰æœ€å¤§çš„ä¸“é—¨ç”¨äºåº”å¯¹å¤œé—´è·Ÿè¸ªæŒ‘æˆ˜çš„åŸºå‡†æ•°æ®é›†ã€‚é€šè¿‡å¯¹42ç§ä¸åŒçš„è·Ÿè¸ªç®—æ³•åœ¨NT-VOT211ä¸Šçš„ç»“æœè¿›è¡Œåˆ†æï¼Œæ­ç¤ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼Œå¼ºè°ƒäº†æ¬¡ä¼˜å…‰ç…§ç¯å¢ƒä¸‹è§†è§‰ç›®æ ‡è·Ÿè¸ªçš„æ”¹è¿›æœºä¼šã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†æ€§èƒ½æ’è¡Œæ¦œã€æ ‡æ³¨å·¥å…·ã€ç»¼åˆå…ƒä¿¡æ¯å’Œæ‰€æœ‰å¿…è¦ä»£ç ä¾›å…¬ä¼—ä½¿ç”¨ã€‚NT-VOT211æœ‰æœ›æ¨åŠ¨è§†è§‰ç›®æ ‡è·Ÿè¸ªç®—æ³•çš„éƒ¨ç½²ï¼Œä¿ƒè¿›ç®—æ³•æ”¹è¿›ï¼Œå¹¶æ¨åŠ¨æ–°çš„å®é™…åº”ç”¨å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰è§†è§‰ç›®æ ‡è·Ÿè¸ªåŸºå‡†æ•°æ®é›†ä¸»è¦å…³æ³¨ç™½å¤©åœºæ™¯ï¼Œå¤œé—´æŒ‘æˆ˜çš„ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡ã€æ ‡æ³¨è‰¯å¥½çš„å¤œé—´åŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°è·Ÿè¸ªç®—æ³•ã€‚</li>
<li>NT-VOT211æ˜¯ä¸€ä¸ªæ–°çš„å¤œé—´è§†è§‰ç›®æ ‡è·Ÿè¸ªåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„è§†é¢‘å’Œæ ‡æ³¨è‰¯å¥½çš„å¸§ã€‚</li>
<li>NT-VOT211ä¸“é—¨é’ˆå¯¹å¤œé—´è·Ÿè¸ªçš„æŒ‘æˆ˜ï¼Œå¦‚ä¸è‰¯èƒ½è§åº¦ã€å›¾åƒæ¨¡ç³Šå’Œå¹²æ‰°ç‰©ç­‰ã€‚</li>
<li>å¯¹42ç§è·Ÿè¸ªç®—æ³•åœ¨NT-VOT211ä¸Šçš„ç»“æœåˆ†ææ­ç¤ºäº†ç®—æ³•çš„ä¼˜ç¼ºç‚¹å’Œæ”¹è¿›æœºä¼šã€‚</li>
<li>NT-VOT211æä¾›äº†æ€§èƒ½æ’è¡Œæ¦œã€æ ‡æ³¨å·¥å…·ç­‰å…¬å…±èµ„æºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-663b5e62d8f384dbe08784fc175a1681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5971e95bd3c82abea1be7919ada3070.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5ffa6182763bc6e4bde526a15db0e11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-947d8d35eb7e88436aeedeb7b55b0af4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91da41d418476a33d9703c3156a0f63a.jpg" align="middle">
</details>




<h2 id="Comparing-YOLO11-and-YOLOv8-for-instance-segmentation-of-occluded-and-non-occluded-immature-green-fruits-in-complex-orchard-environment"><a href="#Comparing-YOLO11-and-YOLOv8-for-instance-segmentation-of-occluded-and-non-occluded-immature-green-fruits-in-complex-orchard-environment" class="headerlink" title="Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and   non-occluded immature green fruits in complex orchard environment"></a>Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and   non-occluded immature green fruits in complex orchard environment</h2><p><strong>Authors:Ranjan Sapkota, Manoj Karkee</strong></p>
<p>This study conducted a comprehensive performance evaluation on YOLO11 and YOLOv8, the latest in the â€œYou Only Look Onceâ€ (YOLO) series, focusing on their instance segmentation capabilities for immature green apples in orchard environments. YOLO11n-seg achieved the highest mask precision across all categories with a notable score of 0.831, highlighting its effectiveness in fruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and occluded fruitlet segmentation with scores of 0.851 and 0.829, respectively. Additionally, YOLO11x-seg led in mask recall for all categories, achieving a score of 0.815, with YOLO11m-seg performing best for non-occluded immature green fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with 0.800. In terms of mean average precision at a 50% intersection over union (mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores for both box and mask segmentation, at 0.876 and 0.860 for the â€œAllâ€ class and 0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg and YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at 0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the advancements in YOLO11, YOLOv8n surpassed its counterparts in image processing speed, with an impressive inference speed of 3.3 milliseconds, compared to the fastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability for real-time agricultural applications related to complex green fruit environments. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¯¹æœ€æ–°çš„â€œYou Only Look Onceâ€ï¼ˆYOLOï¼‰ç³»åˆ—ä¸­çš„YOLO11å’ŒYOLOv8è¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬åœ¨æœå›­ç¯å¢ƒä¸­å¯¹æœªæˆç†Ÿç»¿è‹¹æœçš„å®ä¾‹åˆ†å‰²èƒ½åŠ›ã€‚YOLO11n-segåœ¨æ‰€æœ‰ç±»åˆ«ä¸­è·å¾—äº†æœ€é«˜çš„æ©è†œç²¾åº¦ï¼Œè¾¾åˆ°äº†0.831çš„æ˜¾è‘—åˆ†æ•°ï¼Œçªæ˜¾å…¶åœ¨æ°´æœæ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚YOLO11m-segå’ŒYOLO11l-segåœ¨éé®æŒ¡å’Œé®æŒ¡çš„å¹¼æœåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåˆ†åˆ«è·å¾—äº†0.851å’Œ0.829çš„åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒYOLO11x-segåœ¨æ‰€æœ‰ç±»åˆ«çš„æ©è†œå¬å›ç‡æ–¹é¢ååˆ—å‰èŒ…ï¼Œå¾—åˆ†ä¸º0.815ï¼Œå…¶ä¸­YOLO11m-segåœ¨éé®æŒ¡çš„æœªæˆç†Ÿç»¿è‹¹ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†ä¸º0.858ï¼Œè€ŒYOLOv8x-segåœ¨é®æŒ¡ç±»åˆ«ä¸­é¢†å…ˆï¼Œå¾—åˆ†ä¸º0.800ã€‚åœ¨50%äº¤é›†ä¸Šçš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@50ï¼‰æ–¹é¢ï¼ŒYOLO11m-segè¡¨ç°ä¸€ç›´è¾ƒå¥½ï¼Œåœ¨â€œå…¨éƒ¨â€ç±»åˆ«ä¸­ï¼Œæ¡†å’Œæ©è†œåˆ†å‰²çš„æœ€é«˜åˆ†æ•°åˆ†åˆ«ä¸º0.876å’Œ0.860ï¼Œåœ¨æœªé®æŒ¡çš„æœªæˆç†Ÿæœç±»ä¸­åˆ†åˆ«ä¸º0.908å’Œ0.909ã€‚YOLO11l-segå’ŒYOLOv8l-segåœ¨é®æŒ¡çš„æœªæˆç†Ÿæœç±»ä¸­å…±äº«äº†æœ€é«˜çš„æ¡†mAP@50ï¼Œå¾—åˆ†ä¸º0.847ï¼Œè€ŒYOLO11m-segè·å¾—äº†æœ€é«˜çš„æ©è†œmAP@50ï¼Œå¾—åˆ†ä¸º0.810ã€‚å°½ç®¡YOLO11æœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨å›¾åƒå¤„ç†é€Ÿåº¦æ–¹é¢ï¼ŒYOLOv8nè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ï¼Œå…¶æ¨ç†é€Ÿåº¦è¾¾åˆ°äº†æƒŠäººçš„3.3æ¯«ç§’ï¼Œè€Œæœ€å¿«çš„YOLO11ç³»åˆ—æ¨¡å‹ä¸º4.8æ¯«ç§’ï¼Œè¿™è¡¨æ˜YOLOv8néå¸¸é€‚åˆä¸å¤æ‚ç»¿è‰²æ°´æœç¯å¢ƒç›¸å…³çš„å®æ—¶å†œä¸šåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19869v2">PDF</a> 16 Pages, 10 Figures, 3 Tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶å¯¹YOLOç³»åˆ—çš„æœ€æ–°ç‰ˆæœ¬YOLO11å’ŒYOLOv8è¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½è¯„ä¼°ï¼Œé‡ç‚¹è€ƒå¯Ÿå®ƒä»¬åœ¨æœå›­ç¯å¢ƒä¸­å¯¹æœªæˆç†Ÿç»¿è‹¹æœçš„å®ä¾‹åˆ†å‰²èƒ½åŠ›ã€‚YOLO11ç³»åˆ—åœ¨å„ç±»åˆ«ä¸­å‡å–å¾—äº†æœ€é«˜çš„æ©è†œç²¾åº¦ï¼Œå…¶ä¸­YOLO11n-segä»¥0.831çš„æ˜¾è‘—åˆ†æ•°è„±é¢–è€Œå‡ºï¼Œå‡¸æ˜¾å…¶åœ¨æœå®æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚YOLO11m-segå’ŒYOLO11l-segåœ¨éé®æŒ¡å’Œé®æŒ¡æœå®åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåˆ†æ•°åˆ†åˆ«ä¸º0.851å’Œ0.829ã€‚æ­¤å¤–ï¼ŒYOLO11x-segåœ¨æ‰€æœ‰ç±»åˆ«ä¸­é¢†å…ˆï¼Œåœ¨æ©è†œå¬å›æ–¹é¢å¾—åˆ†ä¸º0.815ã€‚åœ¨å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@50ï¼‰æ–¹é¢ï¼ŒYOLO11m-segè¡¨ç°å“è¶Šï¼Œåœ¨â€œæ‰€æœ‰â€ç±»åˆ«å’Œéé®æŒ¡æœªæˆç†Ÿæœå®ä¸­åˆ†åˆ«è·å¾—0.876å’Œ0.860çš„é«˜åˆ†ã€‚å°½ç®¡YOLO1åœ¨å¤šä¸ªè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†YOLOv8nåœ¨å›¾åƒå¤„ç†é€Ÿåº¦æ–¹é¢æ›´èƒœä¸€ç­¹ï¼Œå…¶æ¨ç†é€Ÿåº¦ä¸º3.3æ¯«ç§’ï¼Œä¸é€Ÿåº¦æœ€å¿«çš„YOLOiç³»åˆ—æ¨¡å‹ç›¸æ¯”æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½¿å…¶é€‚ç”¨äºå¤æ‚çš„ç»¿è‰²æ°´æœç¯å¢ƒçš„å®æ—¶å†œä¸šåº”ç”¨ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>YOLOç³»åˆ—çš„æœ€æ–°æ¨¡å‹YOLO11å’ŒYOLOv8åœ¨æœå®æ£€æµ‹é¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>YOLO11ç³»åˆ—åœ¨æœå®æ©è†œç²¾åº¦æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«åœ¨å„ç±»åˆ«ä¸­YOLO1ln-segå–å¾—äº†æ˜¾è‘—æˆç»©ã€‚</li>
<li>YOLO1lm-segå’ŒYOLOlxl-segåœ¨éé®æŒ¡å’Œé®æŒ¡æ¡ä»¶ä¸‹çš„æœå®åˆ†å‰²è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>YOLOllç³»åˆ—åœ¨æ©è†œå¬å›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­YOLOllx-segåœ¨æ‰€æœ‰ç±»åˆ«ä¸­é¢†å…ˆã€‚</li>
<li>åœ¨å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@50ï¼‰è¯„ä¼°ä¸­ï¼ŒYOLOllm-segè·å¾—æœ€é«˜åˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡YOLOllå…·æœ‰è®¸å¤šä¼˜åŠ¿ï¼Œä½†YOLOv8nåœ¨å›¾åƒå¤„ç†é€Ÿåº¦æ–¹é¢æ›´èƒœä¸€ç­¹ï¼Œå…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4de4197d0c9cb004ac68ed6a5a013291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6572e68814f4babce6ff6cd18039a9bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f143418053ff8b59dccf1aee28995d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf331f02deec539e8f4fd6a7db8de6a.jpg" align="middle">
</details>




<h2 id="SiamSeg-Self-Training-with-Contrastive-Learning-for-Unsupervised-Domain-Adaptation-Semantic-Segmentation-in-Remote-Sensing"><a href="#SiamSeg-Self-Training-with-Contrastive-Learning-for-Unsupervised-Domain-Adaptation-Semantic-Segmentation-in-Remote-Sensing" class="headerlink" title="SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain   Adaptation Semantic Segmentation in Remote Sensing"></a>SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain   Adaptation Semantic Segmentation in Remote Sensing</h2><p><strong>Authors:Bin Wang, Fei Deng, Shuang Wang, Wen Luo, Zhixuan Zhang, Peifan Jiang</strong></p>
<p>Semantic segmentation of remote sensing (RS) images is a challenging yet essential task with broad applications. While deep learning, particularly supervised learning with large-scale labeled datasets, has significantly advanced this field, the acquisition of high-quality labeled data remains costly and time-intensive. Unsupervised domain adaptation (UDA) provides a promising alternative by enabling models to learn from unlabeled target domain data while leveraging labeled source domain data. Recent self-training (ST) approaches employing pseudo-label generation have shown potential in mitigating domain discrepancies. However, the application of ST to RS image segmentation remains underexplored. Factors such as variations in ground sampling distance, imaging equipment, and geographic diversity exacerbate domain shifts, limiting model performance across domains. In that case, existing ST methods, due to significant domain shifts in cross-domain RS images, often underperform. To address these challenges, we propose integrating contrastive learning into UDA, enhancing the modelâ€™s ability to capture semantic information in the target domain by maximizing the similarity between augmented views of the same image. This additional supervision improves the modelâ€™s representational capacity and segmentation performance in the target domain. Extensive experiments conducted on RS datasets, including Potsdam, Vaihingen, and LoveDA, demonstrate that our method, SimSeg, outperforms existing approaches, achieving state-of-the-art results. Visualization and quantitative analyses further validate SimSegâ€™s superior ability to learn from the target domain. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg">https://github.com/woldier/SiamSeg</a>. </p>
<blockquote>
<p>é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰å¹¿æ³›åº”ç”¨æŒ‘æˆ˜æ€§çš„ä¸”å¿…ä¸å¯å°‘çš„ä»»åŠ¡ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œå·²ç»æ˜¾è‘—æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œä½†è·å–é«˜è´¨é‡æ ‡è®°æ•°æ®ä»ç„¶æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ ‡è®°çš„æºåŸŸæ•°æ®çš„åŒæ—¶ä»æœªæ ‡è®°çš„ç›®æ ‡åŸŸæ•°æ®è¿›è¡Œå­¦ä¹ ã€‚æœ€è¿‘é‡‡ç”¨ä¼ªæ ‡ç­¾ç”Ÿæˆçš„è‡ªè®­ç»ƒï¼ˆSTï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºåœ¨å‡å°‘åŸŸå·®å¼‚æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†STåº”ç”¨äºé¥æ„Ÿå›¾åƒåˆ†å‰²ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚è¯¸å¦‚åœ°é¢é‡‡æ ·è·ç¦»ã€æˆåƒè®¾å¤‡å’Œåœ°ç†å¤šæ ·æ€§çš„å˜åŒ–ç­‰å› ç´ åŠ å‰§äº†åŸŸåç§»ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸åŒåŸŸä¹‹é—´çš„æ€§èƒ½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè·¨åŸŸé¥æ„Ÿå›¾åƒä¸­çš„åŸŸåç§»è¾ƒå¤§ï¼Œç°æœ‰çš„STæ–¹æ³•å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå°†å¯¹æ¯”å­¦ä¹ æ•´åˆåˆ°UDAä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„å¢å¼ºè§†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¢å¼ºæ¨¡å‹åœ¨ç›®æ ‡åŸŸä¸­æ•è·è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¿™ç§é¢å¤–çš„ç›‘ç£æé«˜äº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’Œç›®æ ‡åŸŸçš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨åŒ…æ‹¬Potsdamã€Vaihingenå’ŒLoveDAçš„é¥æ„Ÿæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SimSegæ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚å¯è§†åŒ–å’Œå®šé‡åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†SimSegä»ç›®æ ‡åŸŸå­¦ä¹ çš„é«˜çº§èƒ½åŠ›ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg%E3%80%82">https://github.com/woldier/SiamSegã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13471v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è·¨åŸŸæ•°æ®çš„é«˜æˆæœ¬æ ‡æ³¨å’Œé¢†åŸŸå·®å¼‚é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ— ç›‘ç£åŸŸè‡ªé€‚åº”çš„æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„ä¸åŒè§†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¢å¼ºæ¨¡å‹åœ¨ç›®æ ‡åŸŸæ•æ‰è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä½†å…·æœ‰å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ ‡æ³¨é«˜è´¨é‡æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æä¾›äº†ä¸€ç§åˆ©ç”¨æœªæ ‡è®°ç›®æ ‡åŸŸæ•°æ®å’Œæ ‡è®°æºåŸŸæ•°æ®çš„æ¨¡å‹å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>è‡ªè®­ç»ƒï¼ˆSTï¼‰æ–¹æ³•é€šè¿‡ä¼ªæ ‡ç­¾ç”Ÿæˆå…·æœ‰å‡è½»åŸŸå·®å¼‚æ½œåŠ›ï¼Œä½†åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­åº”ç”¨æœ‰é™ã€‚</li>
<li>é¢†åŸŸæ¼‚ç§»é—®é¢˜åœ¨é¥æ„Ÿå›¾åƒä¸­å°¤ä¸ºä¸¥é‡ï¼Œç°æœ‰è‡ªè®­ç»ƒæ–¹æ³•å¾€å¾€å› è·¨åŸŸé¥æ„Ÿå›¾åƒçš„æ˜¾è‘—é¢†åŸŸæ¼‚ç§»è€Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>ç»“åˆå¯¹æ¯”å­¦ä¹ å¯ä»¥å¢å¼ºæ¨¡å‹åœ¨ç›®æ ‡åŸŸæ•æ‰è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ï¼Œé€šè¿‡æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„ä¸åŒè§†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥æé«˜æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’Œåˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16b9c605aaae9f9eba57c4095c57a82a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0441e5239e8a25f01953cf4967e11891.jpg" align="middle">
</details>




<h2 id="Adapting-Vision-Language-Model-with-Fine-grained-Semantics-for-Open-Vocabulary-Segmentation"><a href="#Adapting-Vision-Language-Model-with-Fine-grained-Semantics-for-Open-Vocabulary-Segmentation" class="headerlink" title="Adapting Vision-Language Model with Fine-grained Semantics for   Open-Vocabulary Segmentation"></a>Adapting Vision-Language Model with Fine-grained Semantics for   Open-Vocabulary Segmentation</h2><p><strong>Authors:Yong Xien Chng, Xuchong Qiu, Yizeng Han, Kai Ding, Wan Ding, Gao Huang</strong></p>
<p>Despite extensive research, open-vocabulary segmentation methods still struggle to generalize across diverse domains. To reduce the computational cost of adapting Vision-Language Models (VLMs) while preserving their pre-trained knowledge, most methods freeze the VLMs for mask classification and train only the mask generator. However, our comprehensive analysis reveals a surprising insight: open-vocabulary segmentation is primarily bottlenecked by mask classification, not mask generation. This discovery prompts us to rethink the existing paradigm and explore an alternative approach. Instead of freezing the VLM, we propose to freeze the pre-trained mask generator and focus on optimizing the mask classifier. Building on the observation that VLMs pre-trained on global-pooled image-text features often fail to capture fine-grained semantics necessary for effective mask classification, we propose a novel Fine-grained Semantic Adaptation (FISA) method to address this limitation. FISA enhances the extracted visual features with fine-grained semantic awareness by explicitly integrating this crucial semantic information early in the visual encoding process. As our method strategically optimizes only a small portion of the VLMâ€™s parameters, it enjoys the efficiency of adapting to new data distributions while largely preserving the valuable VLM pre-trained knowledge. Extensive ablation studies confirm the superiority of our approach. Notably, FISA achieves new state-of-the-art results across multiple representative benchmarks, improving performance by up to +1.0 PQ and +3.0 mIoU and reduces training costs by nearly 5x compared to previous best methods. Our code and data will be made public. </p>
<blockquote>
<p>å°½ç®¡è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ä»ç„¶éš¾ä»¥åœ¨ä¸åŒé¢†åŸŸè¿›è¡Œæ¨å¹¿ã€‚ä¸ºäº†é™ä½é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™å…¶é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¼šå†»ç»“VLMè¿›è¡Œæ©è†œåˆ†ç±»ï¼Œå¹¶ä¸”åªè®­ç»ƒæ©è†œç”Ÿæˆå™¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»¼åˆåˆ†ææ­ç¤ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„è§è§£ï¼šå¼€æ”¾è¯æ±‡åˆ†å‰²ä¸»è¦å—åˆ°æ©è†œåˆ†ç±»çš„ç“¶é¢ˆé™åˆ¶ï¼Œè€Œä¸æ˜¯æ©è†œç”Ÿæˆã€‚è¿™ä¸€å‘ç°ä¿ƒä½¿æˆ‘ä»¬é‡æ–°æ€è€ƒç°æœ‰çš„èŒƒå¼ï¼Œå¹¶æ¢ç´¢ä¸€ç§æ›¿ä»£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16278v2">PDF</a> 13 pages, 10 figures</p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬æå‡ºäº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å¯¹å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•çš„ç“¶é¢ˆä¸åœ¨äºé¢å…·ç”Ÿæˆï¼Œè€Œåœ¨äºé¢å…·åˆ†ç±»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå†»ç»“é¢„è®­ç»ƒçš„é¢å…·ç”Ÿæˆå™¨å¹¶ä¸“æ³¨äºä¼˜åŒ–é¢å…·åˆ†ç±»å™¨ã€‚ä¸ºæ­¤å¼•å…¥äº†ä¸€ç§åä¸ºâ€œç²¾ç»†è¯­ä¹‰é€‚åº”â€ï¼ˆFISAï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ—©æœŸåœ¨è§†è§‰ç¼–ç è¿‡ç¨‹ä¸­æ˜ç¡®é›†æˆå…³é”®è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºæå–çš„è§†è§‰ç‰¹å¾çš„ç²¾ç»†è¯­ä¹‰æ„è¯†ã€‚ç”±äºè¯¥æ–¹æ³•ä»…ä¼˜åŒ–VLMä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå› æ­¤åœ¨é€‚åº”æ–°æ•°æ®åˆ†å¸ƒæ—¶ä¿æŒäº†é«˜æ•ˆæ€§ï¼ŒåŒæ—¶ä¿ç•™äº†å®è´µçš„VLMé¢„è®­ç»ƒçŸ¥è¯†ã€‚ç»è¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œè¯å®äº†è¯¥æ–¹æ³•åœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸Šçš„ä¼˜è¶Šæ€§ã€‚å°¤å…¶æ˜¯FISAç›¸è¾ƒäºä»¥å‰æœ€ä½³æ–¹æ³•å–å¾—äº†+1.0 PQå’Œ+3.0 mIoUçš„æ€§èƒ½æå‡ï¼Œå¹¶å°†è®­ç»ƒæˆæœ¬é™ä½äº†è¿‘5å€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•çš„ç“¶é¢ˆåœ¨äºé¢å…·åˆ†ç±»è€Œéé¢å…·ç”Ÿæˆã€‚</li>
<li>æå‡ºå†»ç»“é¢„è®­ç»ƒçš„é¢å…·ç”Ÿæˆå™¨å¹¶ä¼˜åŒ–é¢å…·åˆ†ç±»å™¨çš„ç­–ç•¥ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºâ€œç²¾ç»†è¯­ä¹‰é€‚åº”â€ï¼ˆFISAï¼‰çš„æ–¹æ³•ï¼Œå¢å¼ºäº†è§†è§‰ç‰¹å¾çš„ç²¾ç»†è¯­ä¹‰æ„è¯†ã€‚</li>
<li>FISAé€šè¿‡æ—©æœŸé›†æˆå…³é”®è¯­ä¹‰ä¿¡æ¯åœ¨è§†è§‰ç¼–ç è¿‡ç¨‹ä¸­æé«˜äº†æ€§èƒ½ã€‚</li>
<li>FISAæ–¹æ³•ä»…ä¼˜åŒ–VLMä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶é€‚åº”æ–°æ•°æ®åˆ†å¸ƒã€‚</li>
<li>ç»è¿‡æ¶ˆèç ”ç©¶è¯å®FISAæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21ed2044cc4b9ba56ee3e2620cb7c73c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47db8f9dc5c9817210eca4b992830339.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-706901fed520947b5cbaf6c62b10c250.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8688bde5f1bb544f5638ac2b177c1922.jpg" align="middle">
</details>




<h2 id="PolarBEVDet-Exploring-Polar-Representation-for-Multi-View-3D-Object-Detection-in-Birdâ€™s-Eye-View"><a href="#PolarBEVDet-Exploring-Polar-Representation-for-Multi-View-3D-Object-Detection-in-Birdâ€™s-Eye-View" class="headerlink" title="PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object   Detection in Birdâ€™s-Eye-View"></a>PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object   Detection in Birdâ€™s-Eye-View</h2><p><strong>Authors:Zichen Yu, Quanli Liu, Wei Wang, Liyong Zhang, Xiaoguang Zhao</strong></p>
<p>Recently, LSS-based multi-view 3D object detection provides an economical and deployment-friendly solution for autonomous driving. However, all the existing LSS-based methods transform multi-view image features into a Cartesian Birdâ€™s-Eye-View(BEV) representation, which does not take into account the non-uniform image information distribution and hardly exploits the view symmetry. In this paper, in order to adapt the image information distribution and preserve the view symmetry by regular convolution, we propose to employ the polar BEV representation to substitute the Cartesian BEV representation. To achieve this, we elaborately tailor three modules: a polar view transformer to generate the polar BEV representation, a polar temporal fusion module for fusing historical polar BEV features and a polar detection head to predict the polar-parameterized representation of the object. In addition, we design a 2D auxiliary detection head and a spatial attention enhancement module to improve the quality of feature extraction in perspective view and BEV, respectively. Finally, we integrate the above improvements into a novel multi-view 3D object detector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves the superior performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Yzichen/PolarBEVDet.git">https://github.com/Yzichen/PolarBEVDet.git</a>.(This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible) </p>
<blockquote>
<p>è¿‘æœŸï¼ŒåŸºäºLSSçš„å¤šè§†è§’3Dç›®æ ‡æ£€æµ‹ä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€ç§ç»æµä¸”æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰€æœ‰åŸºäºLSSçš„æ–¹æ³•éƒ½å°†å¤šè§†è§’å›¾åƒç‰¹å¾è½¬æ¢ä¸ºç¬›å¡å°”é¸Ÿç°ï¼ˆBEVï¼‰è¡¨ç¤ºï¼Œè¿™ç§æ–¹æ³•æ²¡æœ‰è€ƒè™‘åˆ°å›¾åƒä¿¡æ¯åˆ†å¸ƒçš„ä¸å‡åŒ€æ€§ï¼Œä¸”å‡ ä¹ä¸èƒ½åˆ©ç”¨è§†è§’å¯¹ç§°æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä¸ºäº†é€‚åº”å›¾åƒä¿¡æ¯åˆ†å¸ƒå¹¶ä¿ç•™å¸¸è§„å·ç§¯çš„è§†å›¾å¯¹ç§°æ€§ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨æåæ ‡BEVè¡¨ç¤ºæ¥æ›¿ä»£ç¬›å¡å°”BEVè¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä¸‰ä¸ªæ¨¡å—ï¼šæè§†å›¾å˜æ¢å™¨ï¼Œç”¨äºç”Ÿæˆæåæ ‡BEVè¡¨ç¤ºï¼›ææ—¶é—´èåˆæ¨¡å—ï¼Œç”¨äºèåˆå†å²æBEVç‰¹å¾ï¼›ä»¥åŠææ£€æµ‹å¤´ï¼Œç”¨äºé¢„æµ‹å¯¹è±¡çš„æå‚æ•°è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ª2Dè¾…åŠ©æ£€æµ‹å¤´å’Œç©ºé—´æ³¨æ„åŠ›å¢å¼ºæ¨¡å—ï¼Œåˆ†åˆ«æé«˜äº†é€è§†å›¾å’ŒBEVçš„ç‰¹å¾æå–è´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸Šè¿°æ”¹è¿›æ•´åˆåˆ°æ–°å‹å¤šè§†è§’3Dç›®æ ‡æ£€æµ‹å™¨PolarBEVDetä¸­ã€‚åœ¨nuScenesä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPolarBEVDetè¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Yzichen/PolarBEVDet.git%E3%80%82%EF%BC%88%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E5%B7%B2%E6%8F%90%E4%BA%A4%E8%87%B3IEEE%E4%BB%A5%E5%BE%85%E5%8F%91%E8%A1%A8%E3%80%82%E5%9C%A8%E7%89%88%E6%9D%83%E8%BD%AC%E8%AE%A9%E9%80%9A%E7%9F%A5%E5%90%8E%EF%BC%8C%E6%AD%A4%E7%89%88%E6%9C%AC%E5%8F%AF%E8%83%BD%E4%B8%8D%E5%86%8D%E5%8F%AF%E7%94%A8%EF%BC%89">https://github.com/Yzichen/PolarBEVDet.gitã€‚ï¼ˆè¿™é¡¹å·¥ä½œå·²æäº¤è‡³IEEEä»¥å¾…å‘è¡¨ã€‚åœ¨ç‰ˆæƒè½¬è®©é€šçŸ¥åï¼Œæ­¤ç‰ˆæœ¬å¯èƒ½ä¸å†å¯ç”¨ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16200v3">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLSSçš„å¤šè§†è§’3Då¯¹è±¡æ£€æµ‹ä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†ç»æµä¸”æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚ä½†ç°æœ‰æ–¹æ³•å°†å¤šè§†è§’å›¾åƒç‰¹å¾è½¬æ¢ä¸ºç¬›å¡å°”é¸Ÿç°å›¾è¡¨ç¤ºï¼Œå¿½ç•¥äº†éå‡åŒ€å›¾åƒä¿¡æ¯åˆ†å¸ƒï¼Œéš¾ä»¥åˆ©ç”¨è§†è§’å¯¹ç§°æ€§ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨æåæ ‡é¸Ÿç°å›¾è¡¨ç¤ºæ›¿ä»£ç¬›å¡å°”é¸Ÿç°å›¾è¡¨ç¤ºï¼Œä¸ºæ­¤ç²¾å¿ƒè®¾è®¡ä¸‰ä¸ªæ¨¡å—ï¼šæè§†è§’å˜æ¢å™¨ç”Ÿæˆæåæ ‡é¸Ÿç°å›¾è¡¨ç¤ºã€ææ—¶åºèåˆæ¨¡å—èåˆå†å²æåæ ‡é¸Ÿç°å›¾ç‰¹å¾ã€ææ£€æµ‹å¤´é¢„æµ‹å¯¹è±¡çš„æå‚æ•°è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªäºŒç»´è¾…åŠ©æ£€æµ‹å¤´å’Œç©ºé—´æ³¨æ„åŠ›å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜é€è§†å›¾å’Œé¸Ÿç°å›¾ç‰¹å¾æå–çš„è´¨é‡ã€‚æœ€ç»ˆå°†ä¸Šè¿°æ”¹è¿›æ•´åˆåˆ°æ–°å‹å¤šè§†è§’ä¸‰ç»´ç›®æ ‡æ£€æµ‹å™¨PolarBEVDetä¸­ï¼Œåœ¨nuScenesä¸Šçš„å®éªŒè¡¨æ˜å…¶æ€§èƒ½å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LSSä¸ºåŸºç¡€çš„å¤šè§†è§’3Dç›®æ ‡æ£€æµ‹ä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ç¬›å¡å°”é¸Ÿç°å›¾è¡¨ç¤ºå­˜åœ¨ä¿¡æ¯åˆ†å¸ƒä¸å‡å’Œè§†è§’å¯¹ç§°æ€§åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥æåæ ‡é¸Ÿç°å›¾è¡¨ç¤ºæ³•ï¼Œä»¥æ›´å¥½åœ°é€‚åº”å›¾åƒä¿¡æ¯åˆ†å¸ƒå¹¶ä¿ç•™è§†è§’å¯¹ç§°æ€§ã€‚</li>
<li>åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæè§†è§’å˜æ¢å™¨ã€ææ—¶åºèåˆæ¨¡å—å’Œææ£€æµ‹å¤´ã€‚</li>
<li>è®¾è®¡äºŒç»´è¾…åŠ©æ£€æµ‹å¤´å’Œç©ºé—´æ³¨æ„åŠ›å¢å¼ºæ¨¡å—æå‡ç‰¹å¾æå–è´¨é‡ã€‚</li>
<li>æ–°å‹æ£€æµ‹å™¨PolarBEVDeté›†æˆäº†ä¸Šè¿°æ”¹è¿›ï¼Œå¹¶åœ¨nuScenesä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb4a5fcb6dc69cc5707ee5e7f059dc76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcaae24e0a9ff42338b169bac8cd9673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887236fea5c85b8d657364f98e0a6cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a68bec8511867c6e3ccf99638c12bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e3b362b22feb1f0b33fa1f55299679e.jpg" align="middle">
</details>




<h2 id="A-Review-of-Human-Object-Interaction-Detection"><a href="#A-Review-of-Human-Object-Interaction-Detection" class="headerlink" title="A Review of Human-Object Interaction Detection"></a>A Review of Human-Object Interaction Detection</h2><p><strong>Authors:Yuxiao Wang, Qiwei Xiong, Yu Lei, Weiying Xue, Qi Liu, Zhenao Wei</strong></p>
<p>Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summarizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detection are introduced. Furthermore, starting with two-stage methods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored. </p>
<blockquote>
<p>äººæœºäº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹åœ¨é«˜å±‚æ¬¡è§†è§‰ç†è§£ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œæœ‰åŠ©äºå¯¹äººç±»æ´»åŠ¨è¿›è¡Œæ·±åº¦å­¦ä¹ ç†è§£ã€‚å…·ä½“è€Œè¨€ï¼ŒHOIæ£€æµ‹çš„ç›®æ ‡æ˜¯å®šä½å›¾åƒæˆ–è§†é¢‘ä¸­å‚ä¸äº¤äº’çš„äººå’Œç‰©ä½“ï¼Œå¹¶åˆ†ç±»å®ƒä»¬ä¹‹é—´çš„ç‰¹å®šäº¤äº’ã€‚è¯¥ä»»åŠ¡çš„æˆåŠŸå—åˆ°å‡ ä¸ªå…³é”®å› ç´ çš„å½±å“ï¼ŒåŒ…æ‹¬äººå’Œç‰©ä½“å®ä¾‹çš„å‡†ç¡®å®šä½ï¼Œä»¥åŠç‰©ä½“ç±»åˆ«å’Œäº¤äº’å…³ç³»çš„æ­£ç¡®åˆ†ç±»ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ€»ç»“å’Œè®¨è®ºäº†åŸºäºå›¾åƒçš„HOIæ£€æµ‹çš„è¿‘æœŸå·¥ä½œã€‚é¦–å…ˆï¼Œä»‹ç»äº†HOIå…³ç³»æ£€æµ‹æ¶‰åŠçš„ä¸»æµæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ç»¼åˆè®¨è®ºäº†åŸºäºå›¾åƒçš„HOIæ£€æµ‹çš„å½“å‰å‘å±•ï¼Œä»ä¸¤é˜¶æ®µæ–¹æ³•å’Œç«¯åˆ°ç«¯å•é˜¶æ®µæ£€æµ‹æ–¹æ³•å¼€å§‹ï¼Œåˆ†æäº†è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚å¦å¤–ï¼Œè¿˜è®¨è®ºäº†é›¶æ ·æœ¬å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ ä»¥åŠå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨HOIæ£€æµ‹ä¸­çš„åº”ç”¨ã€‚æœ€åï¼Œæ¦‚è¿°äº†HOIæ£€æµ‹å½“å‰çš„æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†æ½œåœ¨çš„ç ”ç©¶æ–¹å‘å’Œæœªæ¥è¶‹åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10641v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºåŸºäºå›¾åƒçš„HOIæ£€æµ‹çš„ç ”ç©¶æ€»ç»“ã€‚æ–‡ç« ä»‹ç»äº†ä¸»æµæ•°æ®é›†ï¼Œæ¢è®¨äº†ä»ä¸¤é˜¶æ®µæ–¹æ³•åˆ°ç«¯åˆ°ç«¯çš„ä¸€é˜¶æ®µæ£€æµ‹æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œå¹¶åˆ†æäº†ä¸¤è€…ä¼˜ç¼ºç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜è®¨è®ºäº†é›¶æ ·æœ¬å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨HOIæ£€æµ‹ä¸­çš„åº”ç”¨ã€‚æœ€åï¼Œæ–‡ç« æŒ‡å‡ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæ½œåœ¨çš„ç ”ç©¶æ–¹å‘åŠæœªæ¥è¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HOIæ£€æµ‹åœ¨é«˜å±‚æ¬¡è§†è§‰ç†è§£ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ—¨åœ¨å®šä½å›¾åƒæˆ–è§†é¢‘ä¸­äººç±»ä¸ç‰©ä½“é—´çš„äº¤äº’ï¼Œå¹¶åˆ†ç±»å®ƒä»¬ä¹‹é—´çš„ç‰¹å®šäº¤äº’ã€‚</li>
<li>ä¸»æµæ•°æ®é›†åœ¨HOIå…³ç³»æ£€æµ‹ä¸­çš„åº”ç”¨è¢«ä»‹ç»ã€‚</li>
<li>å½“å‰å›¾åƒåŸºäºHOIæ£€æµ‹çš„æ–¹æ³•åŒ…æ‹¬ä¸¤é˜¶æ®µæ–¹æ³•å’Œç«¯åˆ°ç«¯çš„ä¸€é˜¶æ®µæ£€æµ‹æ–¹æ³•ï¼Œä¸¤è€…å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>é›¶æ ·æœ¬å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨HOIæ£€æµ‹ä¸­çš„åº”ç”¨å¾—åˆ°æ¢è®¨ã€‚</li>
<li>HOIæ£€æµ‹å½“å‰é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚å‡†ç¡®çš„äººç±»å’Œç‰©ä½“å®ä¾‹å®šä½ã€ç‰©ä½“ç±»åˆ«å’Œäº¤äº’å…³ç³»çš„æ­£ç¡®åˆ†ç±»ç­‰ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†æœªæ¥HOIæ£€æµ‹çš„ç ”ç©¶æ–¹å‘å’Œè¶‹åŠ¿ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ–¹æ³•çš„æ”¹è¿›ã€æ–°æ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†çš„æ¢ç´¢ç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-82a16e25685dec1bdd8dd9d3a5d1aaa5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24c75a4834c705e220c4be1624d57c18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-471e4dd4bc2692edcf61916f32d134b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd4dc8f1567690797e9bae7fafdf2f4.jpg" align="middle">
</details>




<h2 id="PADetBench-Towards-Benchmarking-Physical-Attacks-against-Object-Detection"><a href="#PADetBench-Towards-Benchmarking-Physical-Attacks-against-Object-Detection" class="headerlink" title="PADetBench: Towards Benchmarking Physical Attacks against Object   Detection"></a>PADetBench: Towards Benchmarking Physical Attacks against Object   Detection</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Lap-Pui Chau, Shaohui Mei</strong></p>
<p>Physical attacks against object detection have gained increasing attention due to their significant practical implications. However, conducting physical experiments is extremely time-consuming and labor-intensive. Moreover, physical dynamics and cross-domain transformation are challenging to strictly regulate in the real world, leading to unaligned evaluation and comparison, severely hindering the development of physically robust models. To accommodate these challenges, we explore utilizing realistic simulation to thoroughly and rigorously benchmark physical attacks with fairness under controlled physical dynamics and cross-domain transformation. This resolves the problem of capturing identical adversarial images that cannot be achieved in the real world. Our benchmark includes 20 physical attack methods, 48 object detectors, comprehensive physical dynamics, and evaluation metrics. We also provide end-to-end pipelines for dataset generation, detection, evaluation, and further analysis. In addition, we perform 8064 groups of evaluation based on our benchmark, which includes both overall evaluation and further detailed ablation studies for controlled physical dynamics. Through these experiments, we provide in-depth analyses of physical attack performance and physical adversarial robustness, draw valuable observations, and discuss potential directions for future research.   Codebase: <a target="_blank" rel="noopener" href="https://github.com/JiaweiLian/Benchmarking_Physical_Attack">https://github.com/JiaweiLian/Benchmarking_Physical_Attack</a> </p>
<blockquote>
<p>é’ˆå¯¹ç›®æ ‡æ£€æµ‹çš„ç‰©ç†æ”»å‡»å› å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é‡è¦æ„ä¹‰è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿›è¡Œç‰©ç†å®éªŒæå…¶è€—è´¹æ—¶é—´å’ŒäººåŠ›ã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œç‰©ç†åŠ¨æ€å’Œè·¨åŸŸè½¬æ¢çš„ä¸¥æ ¼è°ƒæ§å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯¼è‡´è¯„ä¼°ä¸æ¯”è¾ƒçš„ä¸å¯¹é½ï¼Œä¸¥é‡é˜»ç¢ç‰©ç†ç¨³å¥å‹æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¢ç´¢åˆ©ç”¨ç°å®ä»¿çœŸæ¥åœ¨å—æ§çš„ç‰©ç†åŠ¨æ€å’Œè·¨åŸŸè½¬æ¢ä¸‹ï¼Œå…¬å¹³ã€å…¨é¢ã€ä¸¥æ ¼åœ°è¯„ä¼°ç‰©ç†æ”»å‡»ã€‚è¿™è§£å†³äº†åœ¨çœŸå®ä¸–ç•Œä¸­æ— æ³•æ•è·ç›¸åŒå¯¹æŠ—å›¾åƒçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬20ç§ç‰©ç†æ”»å‡»æ–¹æ³•ã€48ç§ç›®æ ‡æ£€æµ‹å™¨ã€å…¨é¢çš„ç‰©ç†åŠ¨æ€å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç”¨äºæ•°æ®é›†ç”Ÿæˆã€æ£€æµ‹ã€è¯„ä¼°å’Œè¿›ä¸€æ­¥åˆ†æçš„ç«¯åˆ°ç«¯ç®¡é“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºåŸºå‡†æµ‹è¯•è¿›è¡Œäº†8064ç»„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ•´ä½“è¯„ä¼°å’Œå—æ§ç‰©ç†åŠ¨æ€çš„è¿›ä¸€æ­¥è¯¦ç»†æ¶ˆèç ”ç©¶ã€‚é€šè¿‡è¿™äº›å®éªŒï¼Œæˆ‘ä»¬å¯¹ç‰©ç†æ”»å‡»æ€§èƒ½å’Œç‰©ç†å¯¹æŠ—ç¨³å¥æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œè§‚å¯Ÿå¹¶è®¨è®ºäº†æœªæ¥ç ”ç©¶æ½œåœ¨æ–¹å‘ã€‚ä»£ç åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/JiaweiLian/Benchmarking_Physical_Attack">https://github.com/JiaweiLian/Benchmarking_Physical_Attack</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09181v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ç‰©ç†æ”»å‡»ä¸‹çš„ç›®æ ‡æ£€æµ‹æŒ‘æˆ˜ï¼Œé€šè¿‡åˆ©ç”¨ç°å®ä»¿çœŸæŠ€æœ¯æ¥å»ºç«‹åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹Ÿç‰©ç†æ”»å‡»å¹¶æ§åˆ¶ç‰©ç†åŠ¨æ€å’Œè·¨åŸŸè½¬æ¢ï¼Œä»¥å…¬å¹³åœ°è¯„ä¼°æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„é²æ£’æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å¤šç§ç‰©ç†æ”»å‡»æ–¹æ³•å’Œç›®æ ‡æ£€æµ‹å™¨ï¼Œæä¾›æ•°æ®é›†ç”Ÿæˆã€æ£€æµ‹ã€è¯„ä¼°å’Œè¿›ä¸€æ­¥åˆ†æçš„ç«¯åˆ°ç«¯æµç¨‹ã€‚é€šè¿‡å®éªŒï¼Œå¯¹ç‰©ç†æ”»å‡»æ€§èƒ½å’Œç‰©ç†å¯¹æŠ—ç¨³å¥æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ½œåœ¨æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†æ”»å‡»å¯¹ç›®æ ‡æ£€æµ‹çš„æŒ‘æˆ˜åœ¨äºç°å®ä¸–ç•Œä¸­çš„æ—¶é—´æ¶ˆè€—å’ŒåŠ³åŠ¨åŠ›éœ€æ±‚å¤§ï¼Œä¸”ç‰©ç†åŠ¨æ€å’Œè·¨åŸŸè½¬æ¢éš¾ä»¥ä¸¥æ ¼è°ƒæ§ã€‚</li>
<li>åˆ©ç”¨ç°å®ä»¿çœŸæŠ€æœ¯å»ºç«‹åŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨¡æ‹Ÿç‰©ç†æ”»å‡»å¹¶æ§åˆ¶ç‰©ç†åŠ¨æ€å’Œè·¨åŸŸè½¬æ¢ï¼Œä»è€Œå®ç°å…¬å¹³è¯„ä¼°ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…å«å¤šç§ç‰©ç†æ”»å‡»æ–¹æ³•å’Œç›®æ ‡æ£€æµ‹å™¨ï¼Œæä¾›å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>æä¾›æ•°æ®é›†ç”Ÿæˆã€æ£€æµ‹ã€è¯„ä¼°å’Œè¿›ä¸€æ­¥åˆ†æçš„ç«¯åˆ°ç«¯æµç¨‹ã€‚</li>
<li>é€šè¿‡å®éªŒå¯¹ç‰©ç†æ”»å‡»æ€§èƒ½å’Œç‰©ç†å¯¹æŠ—ç¨³å¥æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„åŸºå‡†æµ‹è¯•æœ‰åŠ©äºæ·±å…¥äº†è§£ç‰©ç†æ”»å‡»çš„å½±å“å¹¶æ¨åŠ¨ç›¸å…³ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b778d9b94e2074d79e9fcd5078ab6286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2a336d85c76d7921601b3bc861f8e17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9eb6b48781d406806b9dfab84faccf9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f738ab1333907cf5ad54b50a356f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6c6bd1e84625607e986fd8ca6245b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4b90ea974d2b504963190f16e920e65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f4d2eaaf52c7778c8252d83b797b429.jpg" align="middle">
</details>




<h2 id="Adaptive-Patch-Contrast-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Adaptive-Patch-Contrast-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation"></a>Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Jimin Xiao, Fei Ma, Renrong Ouyang</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to its cost-effectiveness. The typical framework involves using image-level labels as training data to generate pixel-level pseudo-labels with refinements. Recently, methods based on Vision Transformers (ViT) have demonstrated superior capabilities in generating reliable pseudo-labels, particularly in recognizing complete object regions, compared to CNN methods. However, current ViT-based approaches have some limitations in the use of patch embeddings, being prone to being dominated by certain abnormal patches, as well as many multi-stage methods being time-consuming and lengthy in training, thus lacking efficiency. Therefore, in this paper, we introduce a novel ViT-based WSSS method named \textit{Adaptive Patch Contrast} (APC) that significantly enhances patch embedding learning for improved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP) layer to address the limitations of previous max pooling selection methods. Additionally, we propose a Patch Contrastive Learning (PCL) to enhance patch embeddings, thereby further improving the final results. Furthermore, we improve upon the existing multi-stage training framework without CAM by transforming it into an end-to-end single-stage training approach, thereby enhancing training efficiency. The experimental results show that our approach is effective and efficient, outperforming other state-of-the-art WSSS methods on the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training duration. </p>
<blockquote>
<p>ä½¿ç”¨ä»…å›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å› å…¶æˆæœ¬æ•ˆç›Šè€Œå¤‡å—å…³æ³¨ã€‚å…¸å‹çš„æ¡†æ¶æ˜¯ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡ä¸æ–­ä¼˜åŒ–æ¥ç”Ÿæˆåƒç´ çº§ä¼ªæ ‡ç­¾ã€‚æœ€è¿‘ï¼ŒåŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„æ–¹æ³•åœ¨ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾æ–¹é¢è¡¨ç°å‡ºäº†å‡ºè‰²çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å®Œæ•´çš„å¯¹è±¡åŒºåŸŸæ–¹é¢ä¼˜äºCNNæ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„ViTæ–¹æ³•åœ¨ä½¿ç”¨è¡¥ä¸åµŒå…¥æ—¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå®¹æ˜“å—åˆ°æŸäº›å¼‚å¸¸è¡¥ä¸çš„ä¸»å¯¼ï¼Œè€Œä¸”è®¸å¤šå¤šé˜¶æ®µæ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†ä¸Šè€—æ—¶è¾ƒé•¿ï¼Œç¼ºä¹æ•ˆç‡ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºViTçš„WSSSæ–¹æ³•ï¼Œåä¸ºè‡ªé€‚åº”è¡¥ä¸å¯¹æ¯”ï¼ˆAPCï¼‰ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†è¡¥ä¸åµŒå…¥å­¦ä¹ ï¼Œæé«˜äº†åˆ†å‰²æ•ˆæœã€‚APCåˆ©ç”¨è‡ªé€‚åº”Kæ± åŒ–ï¼ˆAKPï¼‰å±‚è§£å†³äº†ä»¥å‰æœ€å¤§æ± åŒ–é€‰æ‹©æ–¹æ³•çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è¡¥ä¸å¯¹æ¯”å­¦ä¹ ï¼ˆPCLï¼‰æ¥å¢å¼ºè¡¥ä¸åµŒå…¥ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æœ€ç»ˆç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¹è¿›äº†ç°æœ‰çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå°†å…¶è½¬å˜ä¸ºç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆä¸”é«˜æ•ˆï¼Œåœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„WSSSæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´å†…å–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10649v2">PDF</a> Accepted by the EAAI Journal</p>
<p><strong>Summary</strong></p>
<p>åŸºäºVision Transformerï¼ˆViTï¼‰çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ–¹æ³•é€šè¿‡ä½¿ç”¨å›¾åƒçº§åˆ«çš„æ ‡ç­¾ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾æ¥è¯†åˆ«å®Œæ•´çš„å¯¹è±¡åŒºåŸŸï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰ViTæ–¹æ³•å­˜åœ¨å±€é™ï¼Œå¦‚æ˜“å—å¼‚å¸¸è¡¥ä¸ä¸»å¯¼ä»¥åŠå¤šé˜¶æ®µæ–¹æ³•è®­ç»ƒæ—¶é—´é•¿ã€æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œè‡ªé€‚åº”è¡¥ä¸å¯¹æ¯”â€ï¼ˆAPCï¼‰çš„æ–°å‹ViT-based WSSSæ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”Kæ± åŒ–ï¼ˆAKPï¼‰å±‚å’Œè¡¥ä¸å¯¹æ¯”å­¦ä¹ ï¼ˆPCLï¼‰å¢å¼ºè¡¥ä¸åµŒå…¥å­¦ä¹ ï¼Œæé«˜åˆ†å‰²æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¹è¿›äº†ç°æœ‰çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå°†å…¶è½¬å˜ä¸ºç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ—¶é—´çŸ­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSSåˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ç”Ÿæˆåƒç´ çº§ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬æ•ˆç›Šæ˜¾è‘—ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰åœ¨ç”Ÿæˆå¯é ä¼ªæ ‡ç­¾æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å®Œæ•´å¯¹è±¡åŒºåŸŸæ–¹é¢ã€‚</li>
<li>å½“å‰ViTæ–¹æ³•å­˜åœ¨å±€é™ï¼Œæ˜“å—åˆ°å¼‚å¸¸è¡¥ä¸ä¸»å¯¼ä»¥åŠè®­ç»ƒæ—¶é—´é•¿ã€æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„â€œè‡ªé€‚åº”è¡¥ä¸å¯¹æ¯”â€ï¼ˆAPCï¼‰æ–¹æ³•é€šè¿‡è‡ªé€‚åº”Kæ± åŒ–ï¼ˆAKPï¼‰å±‚å’Œè¡¥ä¸å¯¹æ¯”å­¦ä¹ ï¼ˆPCLï¼‰å¢å¼ºè¡¥ä¸åµŒå…¥å­¦ä¹ ï¼Œæé«˜åˆ†å‰²æ•ˆæœã€‚</li>
<li>APCæ”¹è¿›äº†ç°æœ‰çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAPCæ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37c3ec3384e6ddd1aeaa3c8b2c22510e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4b55aacee3db09fe4f73359438a7ced.jpg" align="middle">
</details>




<h2 id="BiCo-Fusion-Bidirectional-Complementary-LiDAR-Camera-Fusion-for-Semantic-and-Spatial-Aware-3D-Object-Detection"><a href="#BiCo-Fusion-Bidirectional-Complementary-LiDAR-Camera-Fusion-for-Semantic-and-Spatial-Aware-3D-Object-Detection" class="headerlink" title="BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for   Semantic- and Spatial-Aware 3D Object Detection"></a>BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for   Semantic- and Spatial-Aware 3D Object Detection</h2><p><strong>Authors:Yang Song, Lin Wang</strong></p>
<p>3D object detection is an important task that has been widely applied in autonomous driving. To perform this task, a new trend is to fuse multi-modal inputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these two modalities by unifying them in the same 3D space. However, during direct fusion in a unified space, the drawbacks of both modalities (LiDAR features struggle with detailed semantic information and the camera lacks accurate 3D spatial information) are also preserved, diluting semantic and spatial awareness of the final unified representation. To address the issue, this letter proposes a novel bidirectional complementary LiDAR-camera fusion framework, called BiCo-Fusion that can achieve robust semantic- and spatial-aware 3D object detection. The key insight is to fuse LiDAR and camera features in a bidirectional complementary way to enhance the semantic awareness of the LiDAR and the 3D spatial awareness of the camera. The enhanced features from both modalities are then adaptively fused to build a semantic- and spatial-aware unified representation. Specifically, we introduce Pre-Fusion consisting of a Voxel Enhancement Module (VEM) to enhance the semantic awareness of voxel features from 2D camera features and Image Enhancement Module (IEM) to enhance the 3D spatial awareness of camera features from 3D voxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse the enhanced features from the last stage to build a unified representation. Extensive experiments demonstrate the superiority of our BiCo-Fusion against the prior arts. Project page: <a target="_blank" rel="noopener" href="https://t-ys.github.io/BiCo-Fusion/">https://t-ys.github.io/BiCo-Fusion/</a>. </p>
<blockquote>
<p>ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ˜¯ä¸€é¡¹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¹¿æ³›åº”ç”¨çš„é‡è¦ä»»åŠ¡ã€‚ä¸ºäº†å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œç›®å‰çš„ä¸€ä¸ªæ–°è¶‹åŠ¿æ˜¯èåˆå¤šæ¨¡æ€è¾“å…¥ï¼Œå³æ¿€å…‰é›·è¾¾å’Œæ‘„åƒæœºã€‚åœ¨è¿™ç§è¶‹åŠ¿ä¸‹ï¼Œæœ€è¿‘çš„æ–¹æ³•é€šè¿‡å°†è¿™ä¸¤ç§æ¨¡æ€ç»Ÿä¸€åˆ°åŒä¸€ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œèåˆã€‚ç„¶è€Œï¼Œåœ¨ç»Ÿä¸€ç©ºé—´ä¸­è¿›è¡Œç›´æ¥èåˆæ—¶ï¼Œä¸¤ç§æ¨¡æ€çš„ç¼ºç‚¹ï¼ˆæ¿€å…‰é›·è¾¾ç‰¹å¾éš¾ä»¥è·å–è¯¦ç»†çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œæ‘„åƒæœºç¼ºä¹å‡†ç¡®çš„ä¸‰ç»´ç©ºé—´ä¿¡æ¯ï¼‰ä¹Ÿä¼šè¢«ä¿ç•™ä¸‹æ¥ï¼Œå¯¼è‡´æœ€ç»ˆç»Ÿä¸€è¡¨ç¤ºçš„è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å‡å¼±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.19048v2">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†ä¸‰ç»´ç‰©ä½“æ£€æµ‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰èåˆæ¿€å…‰é›·è¾¾å’Œç›¸æœºè¿›è¡Œå¤šæ¨¡æ€è¾“å…¥çš„æ–°è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œç›´æ¥åœ¨åŒä¸€ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œèåˆä¼šå¯¼è‡´ä¸¤ç§æ¨¡æ€çš„ç¼ºç‚¹ï¼ˆæ¿€å…‰é›·è¾¾ç‰¹å¾ç¼ºä¹è¯¦ç»†è¯­ä¹‰ä¿¡æ¯ï¼Œç›¸æœºç¼ºä¹å‡†ç¡®çš„3Dç©ºé—´ä¿¡æ¯ï¼‰éƒ½è¢«ä¿ç•™ä¸‹æ¥ï¼Œå½±å“æœ€ç»ˆç»Ÿä¸€è¡¨ç¤ºçš„è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒå‘äº’è¡¥çš„æ¿€å…‰é›·è¾¾-ç›¸æœºèåˆæ¡†æ¶â€”â€”BiCo-Fusionï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°ç¨³å¥çš„è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚é€šè¿‡åŒå‘äº’è¡¥çš„æ–¹å¼èåˆæ¿€å…‰é›·è¾¾å’Œç›¸æœºç‰¹å¾ï¼Œå¢å¼ºæ¿€å…‰é›·è¾¾çš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›å’Œç›¸æœºçš„ä¸‰ç»´ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶åè‡ªé€‚åº”åœ°èåˆæ¥è‡ªä¸¤ä¸ªæ¨¡æ€çš„å¢å¼ºç‰¹å¾ï¼Œæ„å»ºå…·æœ‰è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥çš„ç»Ÿä¸€è¡¨ç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3Dç‰©ä½“æ£€æµ‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>å½“å‰è¶‹åŠ¿æ˜¯èåˆå¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚æ¿€å…‰é›·è¾¾å’Œç›¸æœºï¼‰è¿›è¡Œä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚</li>
<li>ç›´æ¥åœ¨ç»Ÿä¸€ç©ºé—´èåˆä¸¤ç§æ¨¡æ€ä¼šå¯¼è‡´è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥çš„ç¼ºé™·ã€‚</li>
<li>åŒå‘äº’è¡¥çš„æ¿€å…‰é›·è¾¾-ç›¸æœºèåˆæ¡†æ¶â€”â€”BiCo-Fusionè¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>BiCo-Fusionå¢å¼ºäº†æ¿€å…‰é›·è¾¾çš„è¯­ä¹‰æ„ŸçŸ¥å’Œç›¸æœºçš„ä¸‰ç»´ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”èåˆå¢å¼ºç‰¹å¾ï¼Œæ„å»ºå…·æœ‰è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥çš„ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0132297f5baaa19fa50be4c3f8feb13f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4188fa509b6075fd7dc64fa22982b02f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f720f55c719eac59d999b7eec56eaf14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd6bde5ecb548fa05046c878a5b7215d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-523fec1f380049e3ab375d94b1aa0063.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f6684274aa763b8958d930e7c05db26.jpg" align="middle">
</details>




<h2 id="SemFlow-Binding-Semantic-Segmentation-and-Image-Synthesis-via-Rectified-Flow"><a href="#SemFlow-Binding-Semantic-Segmentation-and-Image-Synthesis-via-Rectified-Flow" class="headerlink" title="SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified   Flow"></a>SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified   Flow</h2><p><strong>Authors:Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, Ming-Hsuan Yang</strong></p>
<p>Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation. While existing methods consider them as two distinct tasks, we propose a unified framework (SemFlow) and model them as a pair of reverse problems. Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks. As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly. For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results. For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories. Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks. We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²å’Œè¯­ä¹‰å›¾åƒåˆæˆæ˜¯è§†è§‰æ„ŸçŸ¥å’Œç”Ÿæˆä¸­çš„ä¸¤ä¸ªä»£è¡¨æ€§ä»»åŠ¡ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•å°†å®ƒä»¬è§†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼ˆSemFlowï¼‰å¹¶å°†å®ƒä»¬å»ºæ¨¡ä¸ºä¸€å¯¹åå‘é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå—æ­£åˆ™æµç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹ï¼Œåœ¨çœŸå®å›¾åƒå’Œè¯­ä¹‰è’™ç‰ˆåˆ†å¸ƒä¹‹é—´è¿›è¡Œä¼ è¾“ã€‚ç”±äºè®­ç»ƒå¯¹è±¡æ˜¯å¯¹ç§°çš„ï¼Œå±äºä¸¤ä¸ªåˆ†å¸ƒï¼ˆå³å›¾åƒå’Œè¯­ä¹‰è’™ç‰ˆï¼‰çš„æ ·æœ¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è¿›è¡Œå¯é€†è½¬æ¢ã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†æ‰©æ•£è¾“å‡ºéšæœºæ€§å’Œåˆ†å‰²ç»“æœå”¯ä¸€æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚å¯¹äºå›¾åƒåˆæˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰é™æ‰°åŠ¨æ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ï¼Œè€Œä¸æ”¹å˜è¯­ä¹‰ç±»åˆ«ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SemFlowåœ¨è¯­ä¹‰åˆ†å‰²å’Œè¯­ä¹‰å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªç®€å•çš„æ¡†æ¶èƒ½æ¿€åŠ±äººä»¬é‡æ–°æ€è€ƒä½çº§åˆ«å’Œé«˜çº§åˆ«è§†è§‰ä»»åŠ¡çš„ç»Ÿä¸€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20282v2">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼ˆSemFlowï¼‰ï¼Œå°†è¯­ä¹‰åˆ†å‰²å’Œè¯­ä¹‰å›¾åƒåˆæˆè§†ä¸ºä¸€å¯¹åå‘é—®é¢˜ã€‚åˆ©ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹å®ç°çœŸå®å›¾åƒå’Œè¯­ä¹‰æ©è†œä¹‹é—´çš„è½¬æ¢ï¼Œå®ç°å¯é€†è½¬æ¢ã€‚è§£å†³æ‰©æ•£è¾“å‡ºéšæœºæ€§ä¸åˆ†å‰²ç»“æœå”¯ä¸€æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œæå‡ºæœ‰é™æ‰°åŠ¨æ–¹æ³•æé«˜ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ã€‚åœ¨è¯­ä¹‰åˆ†å‰²å’Œè¯­ä¹‰å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºç»Ÿä¸€æ¡†æ¶SemFlowï¼Œå°†è¯­ä¹‰åˆ†å‰²å’Œè¯­ä¹‰å›¾åƒåˆæˆè§†ä¸ºåå‘é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹å®ç°çœŸå®å›¾åƒå’Œè¯­ä¹‰æ©è†œä¹‹é—´çš„è½¬æ¢ã€‚</li>
<li>è§£å†³æ‰©æ•£è¾“å‡ºéšæœºæ€§ä¸åˆ†å‰²ç»“æœå”¯ä¸€æ€§çš„çŸ›ç›¾ã€‚</li>
<li>æå‡ºæœ‰é™æ‰°åŠ¨æ–¹æ³•ï¼Œæé«˜ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ã€‚</li>
<li>åœ¨è¯­ä¹‰åˆ†å‰²å’Œè¯­ä¹‰å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
<li>å¸Œæœ›æ­¤ç®€å•æ¡†æ¶èƒ½æ¿€åŠ±äººä»¬é‡æ–°æ€è€ƒä½çº§åˆ«å’Œé«˜çº§åˆ«è§†è§‰ä»»åŠ¡çš„ç»Ÿä¸€ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-354ba4524def0ee54e46d8ec2a845dd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a9467b5b908c5e22a9fd3b93479c472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd0d6a3db4aeb572f14215970e62c75.jpg" align="middle">
</details>




<h2 id="Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models"><a href="#Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models" class="headerlink" title="Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models"></a>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models</h2><p><strong>Authors:Zhaozheng Chen, Qianru Sun</strong></p>
<p>The rapid development of deep learning has driven significant progress in image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this journal, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„é‡å¤§è¿›æ­¥ã€‚è¯­ä¹‰åˆ†å‰²ç®—æ³•é€šå¸¸ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼ˆå³å¯¹è±¡æ©è†œï¼‰çš„å¯ç”¨æ€§ï¼Œè€Œè¿™äº›æ ‡ç­¾çš„è·å–æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”åŠ³åŠ¨å¯†é›†ã€‚å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯è§£å†³é¿å…æ­¤ç±»æ ‡æ³¨çš„æœ‰æ•ˆæ–¹æ³•ã€‚å®ƒä»…åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´çš„æ³¨é‡Šï¼Œä¸ºå®Œå…¨ç›‘ç£çš„è¯­ä¹‰åˆ†å‰²æä¾›äº†æˆæœ¬æ•ˆç›Šæ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨æœ¬æœŸåˆŠä¸­ï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯æœ‰å›¾åƒçº§æ ‡ç­¾çš„WSSSï¼Œè¿™æ˜¯WSSSä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚æˆ‘ä»¬çš„å·¥ä½œåˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œä¸»è¦é›†ä¸­åœ¨ä¸»è¦ç ”ç©¶ä¼šè®®ä¸Šå‘è¡¨çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„æ–¹æ³•æ“ä½œä½ç½®å°†å®ƒä»¬åˆ†ä¸ºå››ç±»ï¼šåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢è®¨äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ä¸‡ç‰©åˆ†å‰²æ¨¡å‹SAMï¼‰åœ¨WSSSèƒŒæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬å¯¹SAMåœ¨ä¸¤ä¸ªæœ‰è¶£çš„åœºæ™¯ï¼šæ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ è¿›è¡Œäº†å®¡æŸ¥ã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å°†è§†è§‰åŸºç¡€æ¨¡å‹ç”¨äºWSSSçš„æ½œåŠ›å’ŒæŒ‘æˆ˜ï¼Œæœ‰åŠ©äºè¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„ç ”ç©¶é¢†åŸŸæœªæ¥çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13026v2">PDF</a> Accepted to ACM Computing Surveys</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ çš„å‘å±•æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚è¯­ä¹‰åˆ†å‰²ç®—æ³•é€šå¸¸ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼ˆå³å¯¹è±¡æ©è†œï¼‰ï¼Œè¿™äº›æ ‡ç­¾çš„è·å–æˆæœ¬é«˜ã€è€—æ—¶é•¿ä¸”åŠ³åŠ›å¯†é›†ã€‚å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯é¿å…è¿™ç§æ ‡æ³¨çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå®ƒä»…åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´çš„æ³¨é‡Šï¼Œä¸ºå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²æä¾›äº†æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨å…·æœ‰å›¾åƒçº§æ ‡ç­¾çš„WSSSï¼Œè¿™æ˜¯WSSSä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚æ–‡ç« é¦–å…ˆå…¨é¢å›é¡¾äº†ä¼ ç»Ÿæ–¹æ³•ï¼Œä¸»è¦å…³æ³¨é¡¶çº§ç ”ç©¶ä¼šè®®æå‡ºçš„æ–¹æ³•ï¼Œå°†å®ƒä»¬åˆ†ä¸ºå››ç±»ï¼šåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®ã€‚å…¶æ¬¡ï¼Œè°ƒæŸ¥äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Segment Anything Modelï¼ŒSAMï¼‰åœ¨WSSSä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æäº†SAMåœ¨ä¸¤ä¸ªæœ‰è¶£åœºæ™¯ï¼šæ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚ä¸ºè§†è§‰åŸºç¡€æ¨¡å‹åœ¨WSSSä¸­çš„åº”ç”¨æä¾›äº†è§è§£å’ŒæŒ‘æˆ˜ï¼Œæœ‰åŠ©äºè¯¥é¢†åŸŸçš„æœªæ¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²çš„è¿›æ­¥ã€‚</li>
<li>è¯­ä¹‰åˆ†å‰²ç®—æ³•ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼Œä½†è¿™äº›æ ‡ç­¾è·å–æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚</li>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯é¿å…å¯†é›†æ ‡æ³¨çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>WSSSçš„ç ”ç©¶é‡ç‚¹åœ¨äºå¦‚ä½•åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ï¼Œè¿™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚</li>
<li>æ–‡ç« å›é¡¾äº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶å°†å…¶åˆ†ä¸ºåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®å››ç±»ã€‚</li>
<li>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMï¼‰åœ¨WSSSä¸­çš„åº”ç”¨è¢«è¯¦ç»†è°ƒæŸ¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5de8bbf35b6e19bb15d97b70d1365142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f366106e2aa1a029e0793160273fb8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c826195ccfa445054e06431b7d6c4bdc.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8e62a8822f6dfdf50634b68b964c5f1a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  DMin Scalable Training Data Influence Estimation for Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-000e979692aa4c8d8f144717029a4813.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  GN-FRGeneralizable Neural Radiance Fields for Flare Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15821.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
