<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="åŒ»å­¦å›¾åƒ"><meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>åŒ»å­¦å›¾åƒ | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>é¦–é¡µ</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>æ ‡ç­¾</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>åˆ†ç±»</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>å½’æ¡£</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> é¦–é¡µ</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> æ ‡ç­¾</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> åˆ†ç±»</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> å½’æ¡£</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://pic1.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"><span class="chip bg-color">åŒ»å­¦å›¾åƒ</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">åŒ»å­¦å›¾åƒ</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp; 2024-12-12</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp; 2024-12-12</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> æ–‡ç« å­—æ•°:&nbsp;&nbsp; 28k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> é˜…è¯»æ—¶é•¿:&nbsp;&nbsp; 116 åˆ†</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p><p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the modelâ€™s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models.</p><blockquote><p>å›¾åƒåå°„æ¶ˆé™¤å¯¹äºæ¢å¤å›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚æ‰­æ›²çš„å›¾åƒä¼šå¯¹ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å•å¹…å›¾åƒè¿›è¡Œå›¾åƒåå°„æ¶ˆé™¤çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æ²¡æœ‰å…³æ³¨æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§å¯ä»¥æ¨å¹¿åˆ°å›¾åƒåˆ°å›¾åƒé—®é¢˜çš„æ–°è®­ç»ƒæŠ€æœ¯ï¼Œè¾“å…¥å’Œè¾“å‡ºçš„æ€§è´¨ç›¸ä¼¼ã€‚è¿™ç§æŠ€æœ¯ä½“ç°åœ¨æˆ‘ä»¬çš„å¤šæ­¥æŸå¤±æœºåˆ¶ä¸­ï¼Œåœ¨åå°„å»é™¤ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨Pix2Pix GANåˆæˆäº†ä¸€ç§é«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹å­¦ä¹ æ›´å¥½åå°„å»é™¤æ¨¡å¼çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„èŒƒå›´æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å…¶ç¼ºä¹åå°„æ·±åº¦ä¼°è®¡çš„å±æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SIR^2åŸºå‡†å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåå°„å»é™¤æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒå³å¯å®Œæˆã€‚ç ”ç©¶é‡ç‚¹ä¸åœ¨äºæ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§å¯æ¨å¹¿è‡³åŒç±»å›¾åƒè½¬æ¢é—®é¢˜çš„æ–°å‹è®­ç»ƒæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ä½“ç°åœ¨å¤šæ­¥éª¤æŸå¤±æœºåˆ¶ä¸­ï¼Œåœ¨åå°„å»é™¤ä»»åŠ¡ä¸­è¯æ˜æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œé€šè¿‡Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚è¿˜åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒæ·±åº¦ä¼°è®¡ä¸­æå–çš„æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œå‘æŒ¥å…¶ç¼ºä¹åå°„æ·±åº¦ä¼°è®¡çš„ç‰¹æ€§ã€‚è¯¥æ–¹æ³•åœ¨SIR^2åŸºå‡†æµ‹è¯•å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒåå°„å»é™¤æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒå³å¯å®ç°ã€‚</li><li>ç ”ç©¶çš„é‡ç‚¹ä¸åœ¨äºæ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§æ–°å‹è®­ç»ƒæŠ€æœ¯ï¼Œé€‚ç”¨äºåŒç±»å›¾åƒè½¬æ¢é—®é¢˜ã€‚</li><li>é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶å®ç°æœ‰æ•ˆåå°„å»é™¤ã€‚</li><li>åˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li><li>åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒæ·±åº¦ä¼°è®¡ä¸­æå–çš„æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‚</li><li>è¯¥æ–¹æ³•åœ¨SIR^2åŸºå‡†æµ‹è¯•å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b7cd56e05b1afded563a61c2fe8e3211.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-47b9cb26824414e39d3a063f2d4843c9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fdb27ae3046b3ba4acd7d4cddaf1cb0b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-964e226e5e8922d8bf51ab0b2e812c6d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2cd2af283292b21629cca6cea92528de.jpg" align="middle"></details><h2 id="Annotation-Efficient-Task-Guidance-for-Medical-Segment-Anything"><a href="#Annotation-Efficient-Task-Guidance-for-Medical-Segment-Anything" class="headerlink" title="Annotation-Efficient Task Guidance for Medical Segment Anything"></a>Annotation-Efficient Task Guidance for Medical Segment Anything</h2><p><strong>Authors:Tyler Ward, Abdullah-Al-Zubaer Imran</strong></p><p>Medical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose SAM-Mix, a novel multitask learning framework for medical image segmentation that uses class activation maps produced by an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the SAM framework. Experimental evaluations on the public LiTS dataset confirm the effectiveness of SAM-Mix for simultaneous classification and segmentation of the liver from abdominal computed tomography (CT) scans. When trained for 90% fewer epochs on only 50 labeled 2D slices, representing just 0.04% of the available labeled training data, SAM-Mix achieves a Dice improvement of 5.1% over the best baseline model. The generalization results for SAM-Mix are even more impressive, with the same model configuration yielding a 25.4% Dice improvement on a cross-domain segmentation task. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tbwa233/SAM-Mix">https://github.com/tbwa233/SAM-Mix</a>.</p><blockquote><p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ä¼ ç»Ÿçš„å…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œé€šå¸¸é€šè¿‡æ‰‹åŠ¨æ³¨é‡Šè·å¾—ï¼Œè¿™å¯èƒ½ä¼šæ˜¯ä¸€ä¸ªæˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”æ˜“å‡ºé”™çš„è¿‡ç¨‹ã€‚è¿™æ˜¾ç¤ºäº†å¯¹å‡†ç¡®ã€è‡ªåŠ¨å’Œæ³¨é‡Šé«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•çš„éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºSAM-Mixï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå®ƒä½¿ç”¨è¾…åŠ©åˆ†ç±»å™¨ç”Ÿæˆçš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ï¼Œè¯¥åˆ†æ”¯åŸºäºSAMæ¡†æ¶ã€‚åœ¨å…¬å…±LiTSæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯å®äº†SAM-Mixåœ¨è…¹éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTæ‰«æï¼‰ä¸­åŒæ—¶è¿›è¡Œè‚è„åˆ†ç±»å’Œåˆ†å‰²çš„æœ‰æ•ˆæ€§ã€‚ä»…åœ¨50ä¸ªæ ‡è®°çš„2Dåˆ‡ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç›¸å½“äºç°æœ‰å¯ç”¨æ ‡è®°è®­ç»ƒæ•°æ®çš„0.04%ï¼ŒSAM-Mixåœ¨æœ€ä½³åŸºçº¿æ¨¡å‹çš„åŸºç¡€ä¸Šå®ç°äº†5.1%çš„Diceç³»æ•°æå‡ã€‚SAM-Mixçš„æ³›åŒ–ç»“æœæ›´ä»¤äººå°è±¡æ·±åˆ»ï¼Œç›¸åŒçš„æ¨¡å‹é…ç½®åœ¨è·¨åŸŸåˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†25.4%çš„Diceç³»æ•°æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tbwa233/SAM-Mix%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tbwa233/SAM-Mixä¸Šè·å–ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08575v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•SAM-Mixï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è¾…åŠ©åˆ†ç±»å™¨äº§ç”Ÿçš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨å…¬å…±LiTSæ•°æ®é›†ä¸Šï¼ŒSAM-Mixåœ¨è‚è„åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ ‡æ³¨æ•°æ®å³å¯è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶åœ¨è·¨åŸŸåˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li><p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚</p></li><li><p>ä¼ ç»Ÿå®Œå…¨ç›‘ç£çš„åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œè¿˜å®¹æ˜“å‡ºé”™ã€‚</p></li><li><p>SAM-Mixæ˜¯ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨è¾…åŠ©åˆ†ç±»å™¨äº§ç”Ÿçš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼é¢„æµ‹ã€‚</p></li><li><p>åœ¨LiTSæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒSAM-Mixåœ¨è‚è„åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p></li><li><p>SAM-Mixåœ¨ä»…ä½¿ç”¨æå°‘æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°è‰¯å¥½æ€§èƒ½ï¼Œè®­ç»ƒå‘¨æœŸå‡å°‘90%ï¼Œä¸”Diceç³»æ•°ç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹æé«˜äº†5.1%ã€‚</p></li><li><p>SAM-Mixçš„æ³›åŒ–èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œåœ¨åŒä¸€æ¨¡å‹é…ç½®ä¸‹ï¼Œåœ¨è·¨åŸŸåˆ†å‰²ä»»åŠ¡ä¸­Diceç³»æ•°æé«˜äº†25.4%ã€‚</p><pre><code>          HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p></li></ul><h2 id="ConDSeg-A-General-Medical-Image-Segmentation-Framework-via-Contrast-Driven-Feature-Enhancement"><a href="#ConDSeg-A-General-Medical-Image-Segmentation-Framework-via-Contrast-Driven-Feature-Enhancement" class="headerlink" title="ConDSeg: A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement"></a>ConDSeg: A General Medical Image Segmentation Framework via Contrast-Driven Feature Enhancement</h2><p><strong>Authors:Mengqi Lei, Haochen Wu, Xinhua Lv, Xin Wang</strong></p><p>Medical image segmentation plays an important role in clinical decision making, treatment planning, and disease tracking. However, it still faces two major challenges. On the one hand, there is often a &#96;&#96;soft boundaryâ€™â€™ between foreground and background in medical images, with poor illumination and low contrast further reducing the distinguishability of foreground and background within the image. On the other hand, co-occurrence phenomena are widespread in medical images, and learning these features is misleading to the modelâ€™s judgment. To address these challenges, we propose a general framework called Contrast-Driven Medical Image Segmentation (ConDSeg). First, we develop a contrastive training strategy called Consistency Reinforcement. It is designed to improve the encoderâ€™s robustness in various illumination and contrast scenarios, enabling the model to extract high-quality features even in adverse environments. Second, we introduce a Semantic Information Decoupling module, which is able to decouple features from the encoder into foreground, background, and uncertainty regions, gradually acquiring the ability to reduce uncertainty during training. The Contrast-Driven Feature Aggregation module then contrasts the foreground and background features to guide multi-level feature fusion and key feature enhancement, further distinguishing the entities to be segmented. We also propose a Size-Aware Decoder to solve the scale singularity of the decoder. It accurately locate entities of different sizes in the image, thus avoiding erroneous learning of co-occurrence features. Extensive experiments on five medical image datasets across three scenarios demonstrate the state-of-the-art performance of our method, proving its advanced nature and general applicability to various medical image segmentation scenarios. Our released code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg%7D">https://github.com/Mengqi-Lei/ConDSeg}</a>.</p><blockquote><p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå†³ç­–ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…è¿½è¸ªä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼ŒåŒ»å­¦å›¾åƒä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´é€šå¸¸å­˜åœ¨â€œè½¯è¾¹ç•Œâ€ï¼Œè€Œç…§æ˜ä¸è‰¯å’Œå¯¹æ¯”åº¦ä½è¿›ä¸€æ­¥é™ä½äº†å›¾åƒä¸­å‰æ™¯å’ŒèƒŒæ™¯çš„è¾¨åˆ«èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŒ»å­¦å›¾åƒä¸­çš„å…±ç°ç°è±¡æ™®éå­˜åœ¨ï¼Œå­¦ä¹ è¿™äº›ç‰¹å¾ä¼šå¯¹æ¨¡å‹çš„åˆ¤æ–­äº§ç”Ÿè¯¯å¯¼ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„æ¡†æ¶ï¼Œç§°ä¸ºContrast-Driven Medical Image Segmentationï¼ˆConDSegï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºConsistency Reinforcementçš„å¯¹æ¯”è®­ç»ƒç­–ç•¥ã€‚å®ƒæ—¨åœ¨æé«˜ç¼–ç å™¨åœ¨å„ç§ç…§æ˜å’Œå¯¹æ¯”åº¦åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œä½¿æ¨¡å‹å³ä½¿åœ¨æ¶åŠ£ç¯å¢ƒä¸­ä¹Ÿèƒ½æå–é«˜è´¨é‡çš„ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªSemantic Information Decouplingæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°†ç¼–ç å™¨çš„ç‰¹å¾è§£è€¦ä¸ºå‰æ™¯ã€èƒŒæ™¯å’Œä¸ç¡®å®šåŒºåŸŸï¼Œé€æ¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—å‡å°‘ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚ç„¶åï¼ŒContrast-Driven Feature Aggregationæ¨¡å—å¯¹æ¯”å‰æ™¯å’ŒèƒŒæ™¯ç‰¹å¾ï¼Œå¼•å¯¼å¤šçº§åˆ«ç‰¹å¾èåˆå’Œå…³é”®ç‰¹å¾å¢å¼ºï¼Œè¿›ä¸€æ­¥åŒºåˆ†è¦åˆ†å‰²çš„å®ä½“ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§Size-Aware Decoderï¼Œä»¥è§£å†³è§£ç å™¨çš„å°ºåº¦å•ä¸€æ€§é—®é¢˜ã€‚å®ƒå‡†ç¡®åœ°å®šä½å›¾åƒä¸­ä¸åŒå¤§å°çš„å®ä½“ï¼Œä»è€Œé¿å…äº†å…±ç°ç‰¹å¾çš„é”™è¯¯å­¦ä¹ ã€‚åœ¨ä¸‰ç§åœºæ™¯ä¸‹çš„äº”ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ä¸­çš„å…ˆè¿›æ€§å’Œé€šç”¨é€‚ç”¨æ€§ã€‚æˆ‘ä»¬å‘å¸ƒçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Mengqi-Lei/ConDSegè·å–ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08345v1">PDF</a> This paper has been accepted by AAAI-2025</p><p><strong>Summary</strong></p><p>åŒ»ç–—å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå†³ç­–ã€æ²»ç–—è®¡åˆ’ä»¥åŠç–¾ç—…è¿½è¸ªä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä½†é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸€æ˜¯åŒ»å­¦å›¾åƒä¸­å‰æ™¯ä¸èƒŒæ™¯ä¹‹é—´å¸¸å­˜åœ¨â€œè½¯è¾¹ç•Œâ€ï¼ŒåŠ ä¹‹ç…§æ˜ä¸è‰¯ã€å¯¹æ¯”åº¦ä½ï¼Œä½¿å¾—å‰æ™¯ä¸èƒŒæ™¯çš„è¾¨è¯†åº¦é™ä½ã€‚äºŒæ˜¯åŒ»å­¦å›¾åƒä¸­æ™®éå­˜åœ¨å…±ç°ç°è±¡ï¼Œå¯¹æ¨¡å‹åˆ¤æ–­é€ æˆè¯¯å¯¼ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶â€”â€”Contrast-Driven Medical Image Segmentation (ConDSeg)ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¯”è®­ç»ƒç­–ç•¥å’Œæå‡ç¼–ç å™¨ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿåœ¨æ¶åŠ£ç¯å¢ƒä¸‹æå–é«˜è´¨é‡ç‰¹å¾ï¼›å¼•å…¥è¯­ä¹‰ä¿¡æ¯è§£è€¦æ¨¡å—ï¼Œé€æ­¥å‡å°‘ä¸ç¡®å®šæ€§ï¼›é‡‡ç”¨å¯¹æ¯”é©±åŠ¨ç‰¹å¾èšåˆæ¨¡å—ï¼Œå¼•å¯¼å¤šçº§åˆ«ç‰¹å¾èåˆå’Œå…³é”®ç‰¹å¾å¢å¼ºï¼Œè¿›ä¸€æ­¥åŒºåˆ†å¾…åˆ†å‰²å®ä½“ï¼›å¹¶æå‡ºå°ºå¯¸æ„ŸçŸ¥è§£ç å™¨ï¼Œè§£å†³è§£ç å™¨å°ºåº¦å•ä¸€æ€§é—®é¢˜ï¼Œå‡†ç¡®å®šä½å›¾åƒä¸­ä¸åŒå¤§å°çš„å®ä½“ï¼Œé¿å…é”™è¯¯å­¦ä¹ å…±ç°ç‰¹å¾ã€‚åœ¨äº”ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„å‰æ²¿æ€§èƒ½ï¼Œä¸”é€‚ç”¨äºå¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨[<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg]%E3%80%82">https://github.com/Mengqi-Lei/ConDSeg]ã€‚</a></p><p><strong>Key Takeaways</strong></p><ol><li><p>åŒ»ç–—å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å­˜åœ¨å‰æ™¯ä¸èƒŒæ™¯è¾¨è¯†å›°éš¾ã€å…±ç°ç°è±¡ç­‰æŒ‘æˆ˜ã€‚</p></li><li><p>æå‡ºäº†Contrast-Driven Medical Image Segmentation (ConDSeg)æ¡†æ¶ï¼Œé€šè¿‡ä¸€ç³»åˆ—æ¨¡å—è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</p></li><li><p>å¼•å…¥å¯¹æ¯”è®­ç»ƒç­–ç•¥å’Œæå‡ç¼–ç å™¨ç¨³å¥æ€§ï¼Œä»¥é€‚åº”ä¸åŒç…§æ˜å’Œå¯¹æ¯”åº¦åœºæ™¯ã€‚</p></li><li><p>è¯­ä¹‰ä¿¡æ¯è§£è€¦æ¨¡å—èƒ½å¤Ÿé€æ­¥å‡å°‘ä¸ç¡®å®šæ€§ï¼Œå¯¹æ¯”é©±åŠ¨ç‰¹å¾èšåˆæ¨¡å—åŒºåˆ†å¾…åˆ†å‰²å®ä½“ã€‚</p></li><li><p>å°ºå¯¸æ„ŸçŸ¥è§£ç å™¨è§£å†³è§£ç å™¨å°ºåº¦å•ä¸€æ€§é—®é¢˜ï¼Œå‡†ç¡®å®šä½ä¸åŒå¤§å°å®ä½“ã€‚</p></li><li><p>åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„å‰æ²¿æ€§èƒ½ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Lightweight-Method-for-Interactive-3D-Medical-Image-Segmentation-with-Multi-Round-Result-Fusion"><a href="#Lightweight-Method-for-Interactive-3D-Medical-Image-Segmentation-with-Multi-Round-Result-Fusion" class="headerlink" title="Lightweight Method for Interactive 3D Medical Image Segmentation with   Multi-Round Result Fusion"></a>Lightweight Method for Interactive 3D Medical Image Segmentation with Multi-Round Result Fusion</h2><p><strong>Authors:Bingzhi Shen, Lufan Chang, Siqi Chen, Shuxiang Guo, Hao Liu</strong></p><p>In medical imaging, precise annotation of lesions or organs is often required. However, 3D volumetric images typically consist of hundreds or thousands of slices, making the annotation process extremely time-consuming and laborious. Recently, the Segment Anything Model (SAM) has drawn widespread attention due to its remarkable zero-shot generalization capabilities in interactive segmentation. While researchers have explored adapting SAM for medical applications, such as using SAM adapters or constructing 3D SAM models, a key question remains: Can traditional CNN networks achieve the same strong zero-shot generalization in this task? In this paper, we propose the Lightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a novel approach demonstrating the potential of compact CNN-based models. Built upon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D prompt mask from user hints. This mask is then propagated through the 3D sequence via the Memory Module. To refine and stabilize results during interaction, the Multi-Round Result Fusion (MRF) Module selects and merges optimal masks from multiple rounds. Our extensive experiments across multiple datasets and modalities demonstrate LIM-Netâ€™s competitive performance. It exhibits stronger generalization to unseen data compared to SAM-based models, with competitive accuracy while requiring fewer interactions. Notably, LIM-Netâ€™s lightweight design offers significant advantages in deployment and inference efficiency, with low GPU memory consumption suitable for resource-constrained environments. These promising results demonstrate LIM-Net can serve as a strong baseline, complementing and contrasting with popular SAM models to further boost effective interactive medical image segmentation. The code will be released at \url{<a target="_blank" rel="noopener" href="https://github.com/goodtime-123/LIM-Net%7D">https://github.com/goodtime-123/LIM-Net}</a>.</p><blockquote><p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå¯¹ç—…å˜æˆ–å™¨å®˜è¿›è¡Œç²¾ç¡®æ ‡æ³¨æ˜¯ç»å¸¸éœ€è¦çš„ã€‚ç„¶è€Œï¼Œ3Dä½“ç§¯å›¾åƒé€šå¸¸ç”±æ•°ç™¾æˆ–æ•°åƒä¸ªåˆ‡ç‰‡ç»„æˆï¼Œä½¿å¾—æ ‡æ³¨è¿‡ç¨‹æä¸ºè€—æ—¶ä¸”ç¹çã€‚æœ€è¿‘ï¼Œç”±äºå…¶åœ¨äº¤äº’å¼åˆ†å‰²ä¸­çš„å‡ºè‰²é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶ç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†å°†SAMç”¨äºåŒ»å­¦åº”ç”¨ï¼Œä¾‹å¦‚ä½¿ç”¨SAMé€‚é…å™¨æˆ–æ„å»º3D SAMæ¨¡å‹ï¼Œä½†ä¸€ä¸ªå…³é”®é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šä¼ ç»ŸCNNç½‘ç»œèƒ½å¦åœ¨æ­¤ä»»åŠ¡ä¸­å®ç°åŒæ ·çš„å¼ºå¤§é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„è½»å‹äº¤äº’å¼ç½‘ç»œï¼ˆLIM-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å±•ç¤ºç´§å‡‘CNNæ¨¡å‹æ½œåŠ›çš„æ–°å‹æ–¹æ³•ã€‚åŸºäº2D CNNéª¨å¹²ç½‘ï¼ŒLIM-Neté€šè¿‡ç”Ÿæˆç”¨æˆ·æç¤ºçš„2Dæç¤ºæ©è†œæ¥å¯åŠ¨åˆ†å‰²ã€‚ç„¶åï¼Œè¯¥æ©è†œé€šè¿‡å†…å­˜æ¨¡å—ä¼ æ’­åˆ°æ•´ä¸ª3Dåºåˆ—ã€‚ä¸ºäº†åœ¨äº¤äº’è¿‡ç¨‹ä¸­ä¼˜åŒ–å’Œç¨³å®šç»“æœï¼Œå¤šè½®ç»“æœèåˆï¼ˆMRFï¼‰æ¨¡å—ä»å¤šè½®ä¸­é€‰æ‹©å¹¶åˆå¹¶æœ€ä½³æ©è†œã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡æ€ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†LIM-Netçš„ç«äº‰æ€§èƒ½ã€‚ä¸åŸºäºSAMçš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨æœªè§æ•°æ®ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰ç›¸å½“çš„å‡†ç¡®æ€§ä½†éœ€è¦æ›´å°‘çš„äº¤äº’ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLIM-Netçš„è½»é‡çº§è®¾è®¡åœ¨éƒ¨ç½²å’Œæ¨ç†æ•ˆç‡æ–¹é¢æä¾›äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½GPUå†…å­˜æ¶ˆè€—ä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒçš„ç†æƒ³é€‰æ‹©ã€‚è¿™äº›ä»¤äººé¼“èˆçš„ç»“æœè¡¨æ˜ï¼ŒLIM-Netå¯ä»¥ä½œä¸ºå¼ºå¤§çš„åŸºçº¿ï¼Œä¸æµè¡Œçš„SAMæ¨¡å‹ç›¸è¾…ç›¸æˆï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æœ‰æ•ˆçš„äº¤äº’å¼åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚ä»£ç å°†åœ¨\url{<a target="_blank" rel="noopener" href="https://github.com/goodtime-12KFRPMGQQBYBFGCYBFLMLVNPDXXBFHXNXFYFGBRO4OZYGJQ/%7D%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/goodtime-12KFRPMGQQBYBFGCYBFLMLVNPDXXBFHXNXFYFGBRO4OZYGJQ/}ä¸Šå‘å¸ƒã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08315v1">PDF</a></p><p><strong>æ‘˜è¦</strong></p><p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå¯¹ç—…å˜æˆ–å™¨å®˜è¿›è¡Œç²¾ç¡®æ ‡æ³¨æ˜¯éå¸¸å¿…è¦çš„ã€‚ç„¶è€Œï¼Œç”±äºä¸‰ç»´ä½“ç§¯å›¾åƒé€šå¸¸åŒ…å«æ•°ç™¾æˆ–æ•°åƒä¸ªåˆ‡ç‰‡ï¼Œä½¿å¾—æ ‡æ³¨è¿‡ç¨‹éå¸¸è€—æ—¶ä¸”ç¹çã€‚æœ€è¿‘ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰å› å…¶å‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è€Œåœ¨äº¤äº’å¼åˆ†å‰²ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡ç ”ç©¶è€…å·²ç»å°è¯•å°†SAMé€‚åº”äºåŒ»å­¦åº”ç”¨ï¼Œå¦‚ä½¿ç”¨SAMé€‚é…å™¨æˆ–æ„å»ºä¸‰ç»´SAMæ¨¡å‹ï¼Œä½†ä»æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä¼ ç»Ÿçš„CNNç½‘ç»œæ˜¯å¦èƒ½åœ¨è¯¥ä»»åŠ¡ä¸­å®ç°åŒæ ·çš„å¼ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Ÿæœ¬æ–‡æå‡ºäº†ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„è½»å‹äº¤äº’å¼ç½‘ç»œï¼ˆLIM-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å±•ç¤ºç´§å‡‘CNNæ¨¡å‹æ½œåŠ›æ–°æ–¹æ³•ã€‚LIM-Netå»ºç«‹åœ¨äºŒç»´CNNçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ç”¨æˆ·æç¤ºç”ŸæˆäºŒç»´æç¤ºæ©è†œæ¥å¯åŠ¨åˆ†å‰²ã€‚è¯¥æ©è†œç„¶åé€šè¿‡å†…å­˜æ¨¡å—ä¼ æ’­åˆ°æ•´ä¸ªä¸‰ç»´åºåˆ—ã€‚ä¸ºäº†åœ¨äº¤äº’è¿‡ç¨‹ä¸­ä¼˜åŒ–å’Œç¨³å®šç»“æœï¼Œå¤šè½®ç»“æœèåˆï¼ˆMRFï¼‰æ¨¡å—ä¼šé€‰æ‹©å¹¶åˆå¹¶æ¥è‡ªå¤šè½®çš„ä¼˜è´¨æ©è†œã€‚æˆ‘ä»¬çš„å®éªŒåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡æ€ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„éªŒè¯ï¼Œè¯æ˜äº†LIM-Netçš„ç«äº‰åŠ›ã€‚ä¸SAMæ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨æœªè§æ•°æ®ä¸Šå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰ç«äº‰æ€§çš„å‡†ç¡®æ€§åŒæ—¶éœ€è¦æ›´å°‘çš„äº¤äº’ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLIM-Netçš„è½»é‡çº§è®¾è®¡åœ¨éƒ¨ç½²å’Œæ¨ç†æ•ˆç‡æ–¹é¢æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä½GPUå†…å­˜æ¶ˆè€—ä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒçš„ç†æƒ³é€‰æ‹©ã€‚è¿™äº›ä»¤äººé¼“èˆçš„ç»“æœè¡¨æ˜ï¼ŒLIM-Netå¯ä»¥ä½œä¸ºå¼ºæœ‰åŠ›çš„åŸºçº¿ï¼Œä¸æµè¡Œçš„SAMæ¨¡å‹ç›¸è¾…ç›¸æˆï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº¤äº’å¼åŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/goodtime-123/LIM-Net">https://github.com/goodtime-123/LIM-Net</a>å‘å¸ƒã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>LIM-Netæ˜¯ä¸€ç§æ–°å‹çš„äº¤äº’å¼ç½‘ç»œï¼Œç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</p></li><li><p>å®ƒå»ºç«‹åœ¨äºŒç»´CNNçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ç”ŸæˆäºŒç»´æç¤ºæ©è†œæ¥å¯åŠ¨åˆ†å‰²è¿‡ç¨‹ã€‚</p></li><li><p>LIM-Neté€šè¿‡å†…å­˜æ¨¡å—å°†æ©è†œä¼ æ’­åˆ°æ•´ä¸ªä¸‰ç»´åºåˆ—ã€‚</p></li><li><p>å¤šè½®ç»“æœèåˆæ¨¡å—ç”¨äºåœ¨äº¤äº’è¿‡ç¨‹ä¸­ä¼˜åŒ–å’Œç¨³å®šç»“æœã€‚</p></li><li><p>LIM-Netåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚</p></li><li><p>ä¸SAMæ¨¡å‹ç›¸æ¯”ï¼ŒLIM-Netéœ€è¦æ›´å°‘çš„äº¤äº’å°±èƒ½è¾¾åˆ°ç«äº‰æ€§çš„å‡†ç¡®æ€§ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Unified-HT-CNNs-Architecture-Transfer-Learning-for-Segmenting-Diverse-Brain-Tumors-in-MRI-from-Gliomas-to-Pediatric-Tumors"><a href="#Unified-HT-CNNs-Architecture-Transfer-Learning-for-Segmenting-Diverse-Brain-Tumors-in-MRI-from-Gliomas-to-Pediatric-Tumors" class="headerlink" title="Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse   Brain Tumors in MRI from Gliomas to Pediatric Tumors"></a>Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors</h2><p><strong>Authors:Ramy A. Zeineldin, Franziska Mathis-Ullrich</strong></p><p>Accurate segmentation of brain tumors from 3D multimodal MRI is vital for diagnosis and treatment planning across diverse brain tumors. This paper addresses the challenges posed by the BraTS 2023, presenting a unified transfer learning approach that applies to a broader spectrum of brain tumors. We introduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural Networks optimized through transfer learning for varied brain tumor segmentation. This method captures spatial and contextual details from MRI data, fine-tuned on diverse datasets representing common tumor types. Through transfer learning, HT-CNNs utilize the learned representations from one task to improve generalization in another, harnessing the power of pre-trained models on large datasets and fine-tuning them on specific tumor types. We preprocess diverse datasets from multiple international distributions, ensuring representativeness for the most common brain tumors. Our rigorous evaluation employs standardized quantitative metrics across all tumor types, ensuring robustness and generalizability. The proposed ensemble model achieves superior segmentation results across the BraTS validation datasets over the previous winning methods. Comprehensive quantitative evaluations using the DSC and HD95 demonstrate the effectiveness of our approach. Qualitative segmentation predictions further validate the high-quality outputs produced by our model. Our findings underscore the potential of transfer learning and ensemble approaches in medical image segmentation, indicating a substantial enhancement in clinical decision-making and patient care. Despite facing challenges related to post-processing and domain gaps, our study sets a new precedent for future research for brain tumor segmentation. The docker image for the code and models has been made publicly available, <a target="_blank" rel="noopener" href="https://hub.docker.com/r/razeineldin/ht-cnns">https://hub.docker.com/r/razeineldin/ht-cnns</a>.</p><blockquote><p>å¯¹3Då¤šæ¨¡æ€MRIä¸­çš„è„‘è‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºå„ç§è„‘è‚¿ç˜¤çš„è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æœ¬æ–‡é’ˆå¯¹BraTS 2023æå‡ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¿ç§»å­¦ä¹ çš„æ–¹æ³•ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„è„‘è‚¿ç˜¤è°±ã€‚æˆ‘ä»¬å¼•å…¥äº†HT-CNNsï¼Œè¿™æ˜¯ä¸€ç§æ··åˆTransformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„é›†æˆæ–¹æ³•ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ é’ˆå¯¹å„ç§è„‘è‚¿ç˜¤åˆ†å‰²è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ•è·MRIæ•°æ®ä¸­çš„ç©ºé—´å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå¹¶åœ¨ä»£è¡¨å¸¸è§è‚¿ç˜¤ç±»å‹çš„å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒHT-CNNsåˆ©ç”¨ä¸€ä¸ªä»»åŠ¡ä¸­å­¦åˆ°çš„è¡¨ç¤ºæ¥æé«˜å¦ä¸€ä¸ªä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå……åˆ†åˆ©ç”¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹çš„åŠ›é‡ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šè‚¿ç˜¤ç±»å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¯¹æ¥è‡ªå¤šä¸ªå›½é™…åˆ†å¸ƒçš„å¤šç§æ•°æ®é›†è¿›è¡Œäº†é¢„å¤„ç†ï¼Œä»¥ç¡®ä¿å¯¹æœ€å¸¸è§çš„è„‘è‚¿ç˜¤çš„ä»£è¡¨æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æ‰€æœ‰è‚¿ç˜¤ç±»å‹çš„æ ‡å‡†åŒ–å®šé‡æŒ‡æ ‡è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œä»¥ç¡®ä¿å…¶ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚åœ¨BraTSéªŒè¯æ•°æ®é›†ä¸Šï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„é›†æˆæ¨¡å‹å®ç°äº†æ›´å¥½çš„åˆ†å‰²ç»“æœã€‚ä½¿ç”¨DSCå’ŒHD95çš„å®šé‡è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®šæ€§çš„åˆ†å‰²é¢„æµ‹è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹äº§ç”Ÿçš„é«˜è´¨é‡è¾“å‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ï¼Œè¿™å¯èƒ½ä¼šæå¤§åœ°æé«˜ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†çš„è´¨é‡ã€‚å°½ç®¡é¢ä¸´ç€ä¸åå¤„ç†å’Œé¢†åŸŸå·®è·ç›¸å…³çš„æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæœªæ¥çš„è„‘è‚¿ç˜¤åˆ†å‰²ç ”ç©¶æ ‘ç«‹äº†æ–°çš„å…ˆä¾‹ã€‚ä»£ç çš„dockeré•œåƒå’Œæ¨¡å‹å·²ç»å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://hub.docker.com/r/razeineldin/ht-cnns%E4%B8%8A%E3%80%82">https://hub.docker.com/r/razeineldin/ht-cnnsä¸Šã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08240v1">PDF</a> Accepted in the Computer Assisted Radiology and Surgery (CARS 2024) Conference</p><p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¿ç§»å­¦ä¹ çš„ç»Ÿä¸€æ–¹æ³•ï¼Œç”¨äºä»3Då¤šæ¨¡æ€MRIä¸­å‡†ç¡®åˆ†å‰²å¤šç§ç±»å‹çš„è„‘è‚¿ç˜¤ã€‚é€šè¿‡æ··åˆTransformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆHT-CNNsï¼‰çš„é›†æˆæ¨¡å‹ï¼Œç»“åˆè¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°ç©ºé—´ä¸ä¸Šä¸‹æ–‡ç»†èŠ‚çš„æ•æ‰ã€‚è¯¥æ¨¡å‹åœ¨å¤šç§è„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„å¤„ç†å’Œæ ‡å‡†åŒ–è¯„ä¼°ï¼Œå…¬å¼€ä»£ç å’Œæ¨¡å‹Dockeré•œåƒå¯ä¾›ä¸‹è½½ã€‚ç ”ç©¶è¡¨æ˜è¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•èƒ½æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç²¾åº¦ï¼Œæœ‰æœ›æ”¹å–„ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šç§è„‘è‚¿ç˜¤åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨è¿ç§»å­¦ä¹ æ–¹æ³•åº”å¯¹ã€‚</p></li><li><p>å¼•å…¥HT-CNNsæ¨¡å‹ï¼Œç»“åˆTransformerå’Œå·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</p></li><li><p>æ¨¡å‹èƒ½å¤Ÿä»MRIæ•°æ®ä¸­æ•æ‰ç©ºé—´ä¸ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå¹¶é’ˆå¯¹ä¸åŒè‚¿ç˜¤ç±»å‹è¿›è¡Œå¾®è°ƒã€‚</p></li><li><p>é€šè¿‡è¿ç§»å­¦ä¹ ï¼Œæ¨¡å‹åˆ©ç”¨ä¸€ç§ä»»åŠ¡çš„è¡¨ç¤ºå­¦ä¹ æ¥æ”¹å–„å¦ä¸€ç§ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p></li><li><p>è®ºæ–‡é‡‡ç”¨å¤šä¸ªå›½é™…åˆ†å¸ƒçš„æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å’Œè¯„ä¼°ï¼Œç¡®ä¿æ¨¡å‹çš„ä»£è¡¨æ€§å’Œé²æ£’æ€§ã€‚</p></li><li><p>ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®ºæ–‡ä¸­æå‡ºçš„æ¨¡å‹åœ¨BraTSéªŒè¯æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²ç»“æœã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM"><a href="#Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM" class="headerlink" title="Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM"></a>Detection of extended X-ray emission around the PeVatron microquasar V4641 Sgr with XRISM</h2><p><strong>Authors:Hiromasa Suzuki, Naomi Tsuji, Yoshiaki Kanemaru, Megumi Shidatsu, Laura Olivera-Nieto, Samar Safi-Harb, Shigeo S. Kimura, Eduardo de la Fuente, Sabrina Casanova, Kaya Mori, Xiaojie Wang, Sei Kato, Dai Tateishi, Hideki Uchiyama, Takaaki Tanaka, Hiroyuki Uchida, Shun Inoue, Dezhi Huang, Marianne Lemoine-Goumard, Daiki Miura, Shoji Ogawa, Shogo B. Kobayashi, Chris Done, Maxime Parra, MarÃ­a DÃ­az Trigo, Teo MuÃ±oz-Darias, Montserrat Armas Padilla, Ryota Tomaru, Yoshihiro Ueda</strong></p><p>A recent report on the detection of very-high-energy gamma rays from V4641 Sagittarii (V4641 Sgr) up to <del>0.8 peta-electronvolt has made it the second confirmed â€œPeVatronâ€ microquasar. Here we report on the observation of V4641 Sgr with X-Ray Imaging and Spectroscopy Mission (XRISM) in September 2024. Thanks to the large field of view and low background, the CCD imager Xtend successfully detected for the first time X-ray extended emission around V4641 Sgr with a significance of &gt; 4.5 sigma and &gt; 10 sigma based on our imaging and spectral analysis, respectively. The spatial extent is estimated to have a radius of $7 \pm 3$ arcmin ($13 \pm 5$ pc at a distance of 6.2 kpc) assuming a Gaussian-like radial distribution, which suggests that the particle acceleration site is within ~10 pc of the microquasar. If the X-ray morphology traces the diffusion of accelerated electrons, this spatial extent can be explained by either an enhanced magnetic field (</del>80 uG) or a suppressed diffusion coefficient (~$10^{27}$ cm$^2$ s$^{-1}$ at 100 TeV). The integrated X-ray flux, (4-6)$\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ (2-10 keV), would require a magnetic field strength higher than the galactic mean (&gt; 8 uG) if the diffuse X-ray emission originates from synchrotron radiation and the gamma-ray emission is predominantly hadronic. If the X-rays are of thermal origin, the measured extension, temperature, and plasma density can be explained by a jet with a luminosity of ~$2\times 10^{39}$ erg s$^{-1}$, which is comparable to the Eddington luminosity of this system.</p><blockquote><p>æœ€è¿‘ä¸€ä»½å…³äºä»V4641äººé©¬åº§ï¼ˆV4641 Sgrï¼‰æ£€æµ‹åˆ°è¶…é«˜èƒ½ä¼½é©¬å°„çº¿çš„æŠ¥å‘Šï¼Œèƒ½é‡é«˜è¾¾<del>0.8æ‹ç”µå­ä¼ç‰¹ï¼Œä½¿å…¶æˆä¸ºç¬¬äºŒä¸ªç¡®è®¤çš„â€œæ‹ç”µå­ä¼ç‰¹åŠ é€Ÿå™¨â€å¾®ç±»æ˜Ÿã€‚è¿™é‡Œæˆ‘ä»¬æŠ¥å‘Šäº†2024å¹´9æœˆä½¿ç”¨Xå°„çº¿æˆåƒå’Œå…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰å¯¹V4641 Sgrçš„è§‚å¯Ÿç»“æœã€‚ç”±äºXRISMå…·æœ‰å¤§è§†åœºå’Œä½èƒŒæ™¯çš„ç‰¹ç‚¹ï¼ŒCCDæˆåƒä»ªXtendé¦–æ¬¡æˆåŠŸæ£€æµ‹åˆ°V4641 Sgrå‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„ï¼Œå…¶æ˜¾è‘—æ€§å¤§äº4.5 sigmaå’ŒåŸºäºæˆ‘ä»¬çš„æˆåƒå’Œå…‰è°±åˆ†æçš„å¤§äº10 sigmaã€‚å…¶ç©ºé—´èŒƒå›´åœ¨å‡è®¾é«˜æ–¯å¾„å‘åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œä¼°è®¡åŠå¾„ä¸º$7 \pm 3$è§’åˆ†ï¼ˆåœ¨è·ç¦»6.2 kpcçš„æƒ…å†µä¸‹ä¸º$13 \pm 5$ pcï¼‰ã€‚è¿™è¡¨æ˜ç²’å­åŠ é€Ÿéƒ¨ä½ä½äºå¾®ç±»æ˜Ÿé™„è¿‘çº¦10 pcå¤„ã€‚å¦‚æœXå°„çº¿çš„å½¢æ€è¿½è¸ªäº†åŠ é€Ÿç”µå­çš„æ‰©æ•£ï¼Œé‚£ä¹ˆè¿™ä¸ªç©ºé—´èŒƒå›´å¯ä»¥ç”±å¢å¼ºçš„ç£åœºï¼ˆçº¦80 uGï¼‰æˆ–æŠ‘åˆ¶çš„æ‰©æ•£ç³»æ•°ï¼ˆåœ¨100 TeVæ—¶çº¦ä¸º$10^{27}$ cm$^2$ s$^{-1}$ï¼‰æ¥è§£é‡Šã€‚Xå°„çº¿çš„ç§¯åˆ†æµé‡ä¸ºï¼ˆ4-6ï¼‰Ã— 10$^{-12}$ erg s$^{-1}$ cm$^{-2}$ï¼ˆ2-10 keVï¼‰ï¼Œå¦‚æœæ¼«å°„Xå°„çº¿å‘å°„æ¥è‡ªåŒæ­¥è¾å°„ä¸”ä¼½é©¬å°„çº¿å‘å°„ä¸»è¦æ˜¯å¼ºå­è¿‡ç¨‹ï¼Œé‚£ä¹ˆè¿™å°†è¦æ±‚ç£åœºå¼ºåº¦é«˜äºé“¶æ²³ç³»çš„å¹³å‡å€¼ï¼ˆ&gt; 8 uGï¼‰ã€‚å¦‚æœXå°„çº¿æ˜¯çƒ­èµ·æºçš„ï¼Œæ‰€æµ‹å¾—çš„æ‰©å±•èŒƒå›´ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“å¯†åº¦å¯ä»¥ç”¨äº®åº¦çº¦ä¸º</del> $2\times 10^{39}$ erg s$^{-1}$çš„å–·æµæ¥è§£é‡Šï¼Œè¿™ä¸è¯¥ç³»ç»Ÿçš„çˆ±ä¸é¡¿äº®åº¦ç›¸å½“ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08089v1">PDF</a> 9 pages, 5 figures, accepted for publication in ApJL</p><p><strong>Summary</strong><br>V4641 Sagittariiï¼ˆV4641 Sgrï¼‰é€šè¿‡XRISMè§‚æµ‹å‘ç°Xå°„çº¿æ‰©å±•å‘å°„ï¼Œç©ºé—´èŒƒå›´æš—ç¤ºç²’å­åŠ é€Ÿä½ç‚¹è·ç¦»å¾®ç±»æ˜Ÿä½“çº¦10ç§’å·®è·ç¦»ã€‚ç ”ç©¶å‘ç°ï¼Œç£åœºçš„å¢å¼ºæˆ–æ‰©æ•£ç³»æ•°çš„æŠ‘åˆ¶éƒ½èƒ½è§£é‡Šè¿™ä¸€ç©ºé—´èŒƒå›´ï¼Œä¸”Xå°„çº¿å½¢æ€å¯èƒ½è¿½è¸ªåŠ é€Ÿç”µå­çš„æ‰©æ•£è·¯å¾„ã€‚æ­¤å¤–ï¼Œæ¢è®¨äº†åŒæ­¥è¾å°„å’Œè´¨å­ä¸»å¯¼çš„ä¼½é©¬å°„çº¿æ’æ”¾çš„æƒ…å¢ƒï¼Œè‹¥åŒæ­¥è¾å°„å’Œä¼½é©¬å°„çº¿ä¸»è¦ä¸ºçƒ­èµ·æºï¼Œåˆ™å¯ä»¥è§£é‡Šæµ‹é‡å¾—åˆ°çš„å»¶ä¼¸ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“å¯†åº¦æ˜¯ç”±å°„æµäº§ç”Ÿï¼Œå…¶å…‰åº¦ä¸ç³»ç»Ÿçš„çˆ±ä¸é¡¿å…‰åº¦ç›¸å½“ã€‚è¯¥æŠ¥å‘Šç¡®å®šäº†V4641 Sgræ˜¯ä¸€ä¸ªæ–°çš„PeVatronå¾®ç±»æ˜Ÿä½“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li><p>V4641 Sagittariiï¼ˆV4641 Sgrï¼‰è¢«ç¡®è®¤ä¸ºç¬¬äºŒä¸ªå·²çŸ¥çš„PeVatronå¾®ç±»æ˜Ÿä½“ã€‚</p></li><li><p>XRISMæˆåŠŸæ£€æµ‹åˆ°V4641 Sgrå‘¨å›´çš„Xå°„çº¿æ‰©å±•å‘å°„ï¼Œæ˜¾ç¤ºå‡ºå…¶æ˜¾è‘—çš„è¾å°„ç‰¹å¾ã€‚</p></li><li><p>è§‚æµ‹ç»“æœæ˜¾ç¤ºç²’å­åŠ é€Ÿä½ç‚¹è·ç¦»å¾®ç±»æ˜Ÿä½“çº¦10ç§’å·®è·ç¦»ã€‚</p></li><li><p>Xå°„çº¿å½¢æ€å¯èƒ½è¿½è¸ªåŠ é€Ÿç”µå­çš„æ‰©æ•£è·¯å¾„ã€‚</p></li><li><p>æŠ¥å‘Šæ¢è®¨äº†åŒæ­¥è¾å°„å’Œè´¨å­ä¸»å¯¼çš„ä¼½é©¬å°„çº¿æ’æ”¾çš„ä¸åŒè§£é‡Šã€‚</p></li><li><p>è‹¥Xå°„çº¿ä¸ºåŒæ­¥è¾å°„èµ·æºï¼Œåˆ™è¦æ±‚ç£åœºå¼ºåº¦é«˜äºé“¶æ²³ç³»å¹³å‡å€¼ã€‚</p><pre><code>          HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p></li></ul><h2 id="How-to-select-slices-for-annotation-to-train-best-performing-deep-learning-segmentation-models-for-cross-sectional-medical-images"><a href="#How-to-select-slices-for-annotation-to-train-best-performing-deep-learning-segmentation-models-for-cross-sectional-medical-images" class="headerlink" title="How to select slices for annotation to train best-performing deep   learning segmentation models for cross-sectional medical images?"></a>How to select slices for annotation to train best-performing deep learning segmentation models for cross-sectional medical images?</h2><p><strong>Authors:Yixin Zhang, Kevin Kramer, Maciej A. Mazurowski</strong></p><p>Automated segmentation of medical images highly depends on the availability of accurate manual image annotations. Such annotations are very time-consuming and costly to generate, and often require specialized expertise, particularly for cross-sectional images which contain many slices for each patient. It is crucial to ensure the best use of annotation resources. In this paper, we systematically answer the question of how to select slices of cross-sectional medical images in order to maximize performance of the resulting deep learning segmentation models. We conducted experiments on 4 medical imaging segmentation tasks with varying annotation budgets, numbers of annotated cases, numbers of annotated slices per volume, slice selection techniques, and mask interpolations. We found that: 1) It is almost always preferable to annotate fewer slices per volume and more volumes given an annotation budget. 2) Selecting slices for annotation by unsupervised active learning (UAL) is not superior to selecting slices randomly or at fixed intervals, provided that each volume is allocated the same number of annotated slices. 3) Interpolating masks between annotated slices rarely enhances model performance, with exceptions of some specific configuration for 3D models.</p><blockquote><p>åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå‡†ç¡®çš„æ‰‹åŠ¨å›¾åƒæ³¨é‡Šçš„å¯ç”¨æ€§ã€‚è¿™äº›æ³¨é‡Šçš„ç”Ÿæˆéå¸¸è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ï¼Œé€šå¸¸éœ€è¦ä¸“ä¸šç»éªŒï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŒ…å«å¤šä¸ªåˆ‡ç‰‡çš„æ¨ªæˆªé¢å›¾åƒã€‚ç¡®ä¿æœ€ä½³ä½¿ç”¨æ³¨é‡Šèµ„æºè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å›ç­”äº†å¦‚ä½•é€‰æ‹©æ¨ªæˆªé¢åŒ»å­¦å›¾åƒçš„åˆ‡ç‰‡ï¼Œä»¥æœ€å¤§åŒ–æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨4ä¸ªåŒ»å­¦å½±åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œæ¶‰åŠä¸åŒçš„æ³¨é‡Šé¢„ç®—ã€æ³¨é‡Šæ¡ˆä¾‹æ•°é‡ã€æ¯ä¸ªå·ä¸­æ³¨é‡Šçš„åˆ‡ç‰‡æ•°é‡ã€åˆ‡ç‰‡é€‰æ‹©æŠ€æœ¯å’Œæ©è†œæ’å€¼ã€‚æˆ‘ä»¬å‘ç°ï¼š1ï¼‰åœ¨ç»™å®šæ³¨é‡Šé¢„ç®—çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹æ€»æ˜¯å»ºè®®æ¯ä¸ªå·æ³¨é‡Šè¾ƒå°‘çš„åˆ‡ç‰‡å¹¶å¢åŠ æ›´å¤šçš„å·ã€‚2ï¼‰é€šè¿‡æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ ï¼ˆUALï¼‰é€‰æ‹©åˆ‡ç‰‡è¿›è¡Œæ³¨é‡Šå¹¶ä¸ä¼˜äºéšæœºé€‰æ‹©åˆ‡ç‰‡æˆ–åœ¨å›ºå®šé—´éš”é€‰æ‹©åˆ‡ç‰‡ï¼Œåªè¦æ¯ä¸ªå·åˆ†é…ç›¸åŒæ•°é‡çš„æ³¨é‡Šåˆ‡ç‰‡å³å¯ã€‚3ï¼‰åœ¨æ ‡æ³¨åˆ‡ç‰‡ä¹‹é—´è¿›è¡Œæ©è†œæ’å€¼å¾ˆå°‘èƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å¯¹äºæŸäº›ç‰¹å®šé…ç½®çš„3Dæ¨¡å‹å¯èƒ½ä¼šæœ‰ä¾‹å¤–ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08081v1">PDF</a></p><p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè‡ªåŠ¨åˆ†å‰²å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå‡†ç¡®çš„æ‰‹åŠ¨å›¾åƒæ ‡æ³¨ã€‚ç”±äºéœ€è¦æ¶ˆè€—å¤§é‡æ—¶é—´å’Œæˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤šä¸ªåˆ‡ç‰‡çš„æ¨ªæˆªé¢å›¾åƒä¸Šéœ€è¦ä¸“ä¸šæ ‡æ³¨äººå‘˜ï¼Œå› æ­¤åˆç†åˆ©ç”¨æ ‡æ³¨èµ„æºè‡³å…³é‡è¦ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†å¦‚ä½•é€‰æ‹©æ¨ªæˆªé¢åŒ»å­¦å›¾åƒçš„åˆ‡ç‰‡ï¼Œä»¥æœ€å¤§åŒ–æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å®éªŒï¼Œå‘ç°ï¼šåœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œå»ºè®®æ¯ä¸ªä½“ç§¯æ ‡æ³¨çš„åˆ‡ç‰‡æ•°é‡å°‘ä½†æ ‡æ³¨çš„ä½“ç§¯æ•°é‡å¤šï¼›é‡‡ç”¨æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ çš„åˆ‡ç‰‡æ ‡æ³¨æ–¹å¼å¹¶ä¸ä¼˜äºéšæœºæˆ–å›ºå®šé—´éš”çš„åˆ‡ç‰‡é€‰æ‹©æ–¹å¼ï¼Œåªè¦æ¯ä¸ªä½“ç§¯åˆ†é…çš„æ ‡æ³¨åˆ‡ç‰‡æ•°é‡ç›¸åŒï¼›åœ¨æ ‡æ³¨åˆ‡ç‰‡ä¹‹é—´æ’å€¼ç”Ÿæˆæ©è†œå¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æœ‰é™ï¼Œä½†å¯¹äºæŸäº›ç‰¹å®šé…ç½®çš„3Dæ¨¡å‹å¯èƒ½ä¼šæœ‰ä¾‹å¤–ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>åœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œåº”ä¼˜å…ˆè€ƒè™‘å¢åŠ æ ‡æ³¨ä½“ç§¯çš„æ•°é‡è€Œéæ¯ä¸ªä½“ç§¯ä¸­æ ‡æ³¨çš„åˆ‡ç‰‡æ•°é‡ã€‚</p></li><li><p>é‡‡ç”¨æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ çš„åˆ‡ç‰‡æ ‡æ³¨æ–¹å¼æœªå¿…ä¼˜è¶Šï¼Œå…³é”®åœ¨äºç¡®ä¿æ¯ä¸ªä½“ç§¯è·å¾—ç›¸åŒæ•°é‡çš„æ ‡æ³¨åˆ‡ç‰‡ã€‚</p></li><li><p>é€šè¿‡æ’å€¼ç”Ÿæˆæ©è†œå¯¹æ¨¡å‹æ€§èƒ½çš„æå‡å¹¶ä¸æ˜¾è‘—ï¼Œä»…åœ¨ç‰¹å®šé…ç½®çš„3Dæ¨¡å‹ä¸­å¯èƒ½æœ‰æ‰€åŠ©ç›Šã€‚</p></li><li><p>æ¨ªæˆªé¢å›¾åƒä¸­çš„å¤šåˆ‡ç‰‡ç‰¹æ€§å¯¹æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚</p></li><li><p>å®éªŒè¡¨æ˜ï¼Œä¸åŒæ ‡æ³¨é¢„ç®—ã€æ ‡æ³¨æ¡ˆä¾‹æ•°é‡ã€æ¯ä½“ç§¯æ ‡æ³¨åˆ‡ç‰‡æ•°é‡ã€åˆ‡ç‰‡é€‰æ‹©æŠ€æœ¯ä»¥åŠæ©è†œæ’å€¼æ–¹æ³•éƒ½ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚</p></li><li><p>é€‰æ‹©åˆ‡ç‰‡è¿›è¡Œæ ‡æ³¨æ—¶ï¼Œé™¤äº†è€ƒè™‘å›¾åƒæœ¬èº«çš„ç‰¹å¾å¤–ï¼Œè¿˜éœ€ç»¼åˆè€ƒè™‘å„ç§å®éªŒå› ç´ ä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="BSAFusion-A-Bidirectional-Stepwise-Feature-Alignment-Network-for-Unaligned-Medical-Image-Fusion"><a href="#BSAFusion-A-Bidirectional-Stepwise-Feature-Alignment-Network-for-Unaligned-Medical-Image-Fusion" class="headerlink" title="BSAFusion: A Bidirectional Stepwise Feature Alignment Network for   Unaligned Medical Image Fusion"></a>BSAFusion: A Bidirectional Stepwise Feature Alignment Network for Unaligned Medical Image Fusion</h2><p><strong>Authors:Huafeng Li, Dayong Su, Qing Cai, Yafei Zhang</strong></p><p>If unaligned multimodal medical images can be simultaneously aligned and fused using a single-stage approach within a unified processing framework, it will not only achieve mutual promotion of dual tasks but also help reduce the complexity of the model. However, the design of this model faces the challenge of incompatible requirements for feature fusion and alignment; specifically, feature alignment requires consistency among corresponding features, whereas feature fusion requires the features to be complementary to each other. To address this challenge, this paper proposes an unaligned medical image fusion method called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F) strategy. To reduce the negative impact of modality differences on cross-modal feature matching, we incorporate the Modal Discrepancy-Free Feature Representation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature Representation Head (MFRH) to integrate the global information of the input image. By injecting the information contained in MFRH of the current image into other modality images, it effectively reduces the impact of modality differences on feature alignment while preserving the complementary information carried by different images. In terms of feature alignment, BSFA-F employs a bidirectional stepwise alignment deformation field prediction strategy based on the path independence of vector displacement between two points. This strategy solves the problem of large spans and inaccurate deformation field prediction in single-step alignment. Finally, Multi-Modal Feature Fusion block achieves the fusion of aligned features. The experimental results across multiple datasets demonstrate the effectiveness of our method. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/slrl123/BSAFusion">https://github.com/slrl123/BSAFusion</a>.</p><blockquote><p>å¦‚æœèƒ½å¤Ÿåœ¨ç»Ÿä¸€å¤„ç†æ¡†æ¶å†…é‡‡ç”¨å•é˜¶æ®µæ–¹æ³•åŒæ—¶å¯¹éå¯¹é½çš„å¤šæ¨¡å¼åŒ»å­¦å›¾åƒè¿›è¡Œå¯¹é½å’Œèåˆï¼Œä¸ä»…å¯ä»¥å®ç°åŒé‡ä»»åŠ¡çš„ç›¸äº’ä¿ƒè¿›ï¼Œè¿˜æœ‰åŠ©äºé™ä½æ¨¡å‹çš„å¤æ‚æ€§ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹çš„è®¾è®¡é¢ä¸´ç€ç‰¹å¾èåˆä¸å¯¹é½è¦æ±‚ä¸å…¼å®¹çš„æŒ‘æˆ˜ï¼›å…·ä½“è€Œè¨€ï¼Œç‰¹å¾å¯¹é½è¦æ±‚ç›¸åº”ç‰¹å¾ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè€Œç‰¹å¾èåˆåˆ™è¦æ±‚ç‰¹å¾å½¼æ­¤äº’è¡¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºåŒå‘é€æ­¥ç‰¹å¾å¯¹é½ä¸èåˆï¼ˆBSFA-Fï¼‰ç­–ç•¥çš„éå¯¹é½åŒ»å­¦å›¾åƒèåˆæ–¹æ³•ã€‚ä¸ºäº†å‡å°‘æ¨¡æ€å·®å¼‚å¯¹è·¨æ¨¡æ€ç‰¹å¾åŒ¹é…çš„è´Ÿé¢å½±å“ï¼Œæˆ‘ä»¬å°†æ¨¡æ€å·®å¼‚æ— å…³ç‰¹å¾è¡¨ç¤ºï¼ˆMDF-FRï¼‰æ–¹æ³•çº³å…¥BSFA-Fã€‚MDF-FRåˆ©ç”¨æ¨¡æ€ç‰¹å¾è¡¨ç¤ºå¤´ï¼ˆMFRHï¼‰æ¥æ•´åˆè¾“å…¥å›¾åƒçš„å…¨å±€ä¿¡æ¯ã€‚é€šè¿‡å°†å½“å‰å›¾åƒçš„MFRHä¸­æ‰€åŒ…å«çš„ä¿¡æ¯æ³¨å…¥åˆ°å…¶ä»–æ¨¡æ€å›¾åƒä¸­ï¼Œåœ¨ä¿ç•™ä¸åŒå›¾åƒæ‰€æºå¸¦çš„äº’è¡¥ä¿¡æ¯çš„åŒæ—¶ï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨¡æ€å·®å¼‚å¯¹ç‰¹å¾å¯¹é½çš„å½±å“ã€‚åœ¨ç‰¹å¾å¯¹é½æ–¹é¢ï¼ŒBSFA-Fé‡‡ç”¨äº†ä¸€ç§åŸºäºä¸¤ç‚¹é—´çŸ¢é‡ä½ç§»è·¯å¾„ç‹¬ç«‹æ€§çš„åŒå‘é€æ­¥å¯¹é½å˜å½¢åœºé¢„æµ‹ç­–ç•¥ã€‚è¯¥ç­–ç•¥è§£å†³äº†å•æ­¥å¯¹é½ä¸­å­˜åœ¨çš„å¤§è·¨åº¦å’Œä¸å‡†ç¡®çš„å˜å½¢åœºé¢„æµ‹é—®é¢˜ã€‚æœ€åï¼Œå¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—å®ç°äº†å¯¹é½ç‰¹å¾çš„èåˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/slrl123/BSAFusion%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/slrl123/BSAFusionè·å–ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08050v1">PDF</a> Accepted by AAAI2025</p><p><strong>Summary</strong><br>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆæ˜¯åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ ¸å¿ƒè¯¾é¢˜ã€‚ä¸ºäº†æ”¹å–„å…ˆå‰çš„æ–¹æ³•ä¸­çš„é™åˆ¶å’ŒæŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†åŒå‘åˆ†æ­¥ç‰¹å¾å¯¹é½å’Œèåˆï¼ˆBSFA-Fï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡åŒå‘åˆ†æ­¥å¯¹é½ç­–ç•¥è§£å†³æ¨¡æ€å·®å¼‚å¸¦æ¥çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—å®ç°ç‰¹å¾èåˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ›´å¤šç»†èŠ‚å¯é€šè¿‡è®¿é—®ä»£ç åº“æ·±å…¥äº†è§£ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li><p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒå¯¹é½é—®é¢˜ï¼Œæå‡è¯Šç–—æ•ˆç‡ã€‚</p></li><li><p>é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºç‰¹å¾èåˆä¸å¯¹é½ä¹‹é—´çš„ä¸å…¼å®¹è¦æ±‚ï¼Œè¦æ±‚å¯¹åº”ç‰¹å¾çš„ä¸€è‡´æ€§ä»¥åŠå¯¹ç‰¹å¾çš„äº’è¡¥æ€§éœ€æ±‚ã€‚</p></li><li><p>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„èåˆç­–ç•¥ï¼Œå³åŒå‘åˆ†æ­¥ç‰¹å¾å¯¹é½å’Œèåˆï¼ˆBSFA-Fï¼‰ã€‚</p></li><li><p>è¯¥ç­–ç•¥ä½¿ç”¨æ¨¡æ€å·®å¼‚æ— å…³çš„ç‰¹å¾è¡¨ç¤ºï¼ˆMDF-FRï¼‰æ–¹æ³•ï¼Œä»¥å‡å°ä¸åŒæ¨¡æ€é—´çš„å·®å¼‚å¯¹ç‰¹å¾åŒ¹é…çš„è´Ÿé¢å½±å“ã€‚</p></li><li><p>é€šè¿‡æ•´åˆè¾“å…¥å›¾åƒçš„å…¨å±€ä¿¡æ¯æ¥åˆ›å»ºæ¨¡æ€ç‰¹å¾è¡¨ç¤ºå¤´ï¼ˆMFRHï¼‰ï¼Œè¿›è€Œæå‡ç‰¹å¾å¯¹é½çš„å‡†ç¡®æ€§å¹¶ä¿ç•™ä¸åŒå›¾åƒçš„äº’è¡¥ä¿¡æ¯ã€‚</p><pre><code>          HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p></li></ul><h2 id="Graph-convolutional-networks-enable-fast-hemorrhagic-stroke-monitoring-with-electrical-impedance-tomography"><a href="#Graph-convolutional-networks-enable-fast-hemorrhagic-stroke-monitoring-with-electrical-impedance-tomography" class="headerlink" title="Graph convolutional networks enable fast hemorrhagic stroke monitoring   with electrical impedance tomography"></a>Graph convolutional networks enable fast hemorrhagic stroke monitoring with electrical impedance tomography</h2><p><strong>Authors:J. Toivanen, V. Kolehmainen, A. Paldanius, A. HÃ¤nninen, A. Hauptmann, S. J. Hamilton</strong></p><p>Objective: To develop a fast image reconstruction method for stroke monitoring with electrical impedance tomography with image quality comparable to computationally expensive nonlinear model-based methods. Methods: A post-processing approach with graph convolutional networks is employed. Utilizing the flexibility of the graph setting, a graph U-net is trained on linear difference reconstructions from 2D simulated stroke data and applied to fully 3D images from realistic simulated and experimental data. An additional network, trained on 3D vs. 2D images, is also considered for comparison. Results: Post-processing the linear difference reconstructions through the graph U-net significantly improved the image quality, resulting in images comparable to, or better than, the time-intensive nonlinear reconstruction method (a few minutes vs. several hours). Conclusion: Pairing a fast reconstruction method, such as linear difference imaging, with post-processing through a graph U-net provided significant improvements, at a negligible computational cost. Training in the graph framework vs classic pixel-based setting (CNN) allowed the ability to train on 2D cross-sectional images and process 3D volumes providing a nearly 50x savings in data simulation costs with no noticeable loss in quality. Significance: The proposed approach of post-processing a linear difference reconstruction with the graph U-net could be a feasible approach for on-line monitoring of hemorrhagic stroke.</p><blockquote><p>ç›®æ ‡ï¼šå¼€å‘ä¸€ç§åŸºäºç”µé˜»æŠ—æˆåƒæŠ€æœ¯çš„å¿«é€Ÿå›¾åƒé‡å»ºæ–¹æ³•ï¼Œç”¨äºä¸­é£ç›‘æµ‹ï¼Œå…¶å›¾åƒè´¨é‡å¯ä¸è®¡ç®—å¯†é›†å‹çš„éçº¿æ€§æ¨¡å‹æ–¹æ³•ç›¸åª²ç¾ã€‚æ–¹æ³•ï¼šé‡‡ç”¨å›¾å·ç§¯ç½‘ç»œçš„åå¤„ç†æ–¹æ³•ã€‚åˆ©ç”¨å›¾è®¾ç½®çš„çµæ´»æ€§ï¼Œå¯¹æ¥è‡ªäºŒç»´æ¨¡æ‹Ÿä¸­é£æ•°æ®çš„çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œå›¾U-netè®­ç»ƒï¼Œå¹¶åº”ç”¨äºæ¥è‡ªçœŸå®æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®çš„å®Œå…¨ä¸‰ç»´å›¾åƒã€‚ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œè¿˜è€ƒè™‘äº†åŸºäºä¸‰ç»´ä¸äºŒç»´å›¾åƒè®­ç»ƒçš„é™„åŠ ç½‘ç»œã€‚ç»“æœï¼šé€šè¿‡å›¾U-netå¯¹çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œåå¤„ç†æ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ï¼Œäº§ç”Ÿäº†ä¸è€—æ—¶éçº¿æ€§é‡å»ºæ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½çš„å›¾åƒï¼ˆå‡ åˆ†é’Ÿä¸æ•°å°æ—¶ï¼‰ã€‚ç»“è®ºï¼šå°†å¿«é€Ÿé‡å»ºæ–¹æ³•ï¼ˆå¦‚çº¿æ€§å·®åˆ†æˆåƒï¼‰ä¸é€šè¿‡å›¾U-netè¿›è¡Œåå¤„ç†ç›¸ç»“åˆï¼Œåœ¨å¯å¿½ç•¥çš„è®¡ç®—æˆæœ¬ä¸‹å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸ç»å…¸åƒç´ åŸºç¡€è®¾ç½®ï¼ˆCNNï¼‰ç›¸æ¯”ï¼Œåœ¨å›¾å½¢æ¡†æ¶ä¸­è¿›è¡Œè®­ç»ƒèƒ½å¤Ÿåœ¨äºŒç»´æ¨ªæˆªé¢å›¾åƒä¸Šè®­ç»ƒå¹¶åœ¨ä¸‰ç»´ä½“ç§¯ä¸Šè¿›è¡Œå¤„ç†ï¼Œä»è€Œåœ¨ä¸å½±å“è´¨é‡çš„æƒ…å†µä¸‹å®ç°äº†è¿‘50å€çš„æ•°æ®æ¨¡æ‹Ÿæˆæœ¬èŠ‚çº¦ã€‚æ„ä¹‰ï¼šæ‰€æå‡ºçš„åˆ©ç”¨å›¾U-netå¯¹çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œåå¤„ç†çš„æ–¹æ³•å¯èƒ½æˆä¸ºåœ¨çº¿ç›‘æµ‹å‡ºè¡€æ€§ä¸­é£çš„å¯è¡Œæ–¹æ³•ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07888v1">PDF</a> 11 pages, 8 figures</p><p><strong>Summary</strong><br>é‡‡ç”¨å›¾å·ç§¯ç½‘ç»œåå¤„ç†çš„æ–¹æ³•ï¼Œç»“åˆçº¿æ€§å·®åˆ†é‡å»ºæŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿä¸”å›¾åƒè´¨é‡è¾ƒé«˜çš„ä¸­é£ç›‘æµ‹å›¾åƒé‡å»ºã€‚é€šè¿‡è®­ç»ƒå›¾U-netç½‘ç»œå¤„ç†äºŒç»´æ¨¡æ‹Ÿä¸­é£æ•°æ®ï¼Œå¹¶åº”ç”¨äºå…¨ä¸‰ç»´å›¾åƒï¼Œæ˜¾è‘—æé«˜å›¾åƒè´¨é‡ï¼Œä¸è€—æ—¶è¾ƒé•¿çš„éçº¿æ€§é‡å»ºæ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>ç ”ç©¶ç›®æ ‡ï¼šå¼€å‘ä¸€ç§å¿«é€Ÿå›¾åƒé‡å»ºæ–¹æ³•ï¼Œç”¨äºä¸­é£ç›‘æµ‹çš„ç”µé˜»æŠ—å±‚ææˆåƒï¼Œå…¶å›¾åƒè´¨é‡å¯ä¸è®¡ç®—æ˜‚è´µçš„éçº¿æ€§æ¨¡å‹æ–¹æ³•ç›¸ï¿½ï¿½ åª²ç¾ã€‚</p></li><li><p>æ–¹æ³•ï¼šé‡‡ç”¨å…·æœ‰å›¾è®¾ç½®çµæ´»æ€§çš„å›¾å·ç§¯ç½‘ç»œåå¤„ç†ç­–ç•¥ã€‚è®­ç»ƒå›¾U-netç½‘ç»œå¤„ç†äºŒç»´æ¨¡æ‹Ÿä¸­é£æ•°æ®å¹¶åº”ç”¨äºå…¨ä¸‰ç»´å›¾åƒã€‚</p></li><li><p>ç»“æœï¼šé€šè¿‡å›¾U-netåå¤„ç†çº¿æ€§å·®åˆ†é‡å»ºï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ï¼Œå¾—åˆ°çš„å›¾åƒè´¨é‡å¯ä¸æˆ–ä¼˜äºè€—æ—¶è¾ƒé•¿çš„éçº¿æ€§é‡å»ºæ–¹æ³•ã€‚</p></li><li><p>ç»“è®ºï¼šå°†å¿«é€Ÿé‡å»ºæ–¹æ³•ï¼ˆå¦‚çº¿æ€§å·®åˆ†æˆåƒï¼‰ä¸å›¾U-netåå¤„ç†ç›¸ç»“åˆï¼Œåœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ã€‚</p></li><li><p>è®­ç»ƒæ–¹å¼ï¼šåœ¨å›¾å½¢æ¡†æ¶ä¸­çš„è®­ç»ƒç›¸å¯¹äºä¼ ç»Ÿçš„åƒç´ åŸºç¡€è®¾ç½®ï¼ˆCNNï¼‰ï¼Œèƒ½å¤Ÿåœ¨äºŒç»´æ¨ªæˆªé¢å›¾åƒä¸Šè®­ç»ƒå¹¶å¤„ç†ä¸‰ç»´ä½“ç§¯ï¼ŒèŠ‚çœäº†è¿‘50å€çš„æ•°æ®æ¨¡æ‹Ÿæˆæœ¬ï¼Œä¸”è´¨é‡æ— æ˜æ˜¾æŸå¤±ã€‚</p></li><li><p>åˆ›æ–°æ€§ï¼šæå‡ºçš„å¯¹çº¿æ€§å·®åˆ†é‡å»ºè¿›è¡Œå›¾U-netåå¤„ç†çš„æ–¹æ³•ï¼Œå¯èƒ½æ˜¯å¯¹åœ¨çº¿ç›‘æµ‹å‡ºè¡€æ€§ä¸­é£çš„ä¸€ç§å¯è¡Œæ–¹æ³•ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder"><a href="#XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder" class="headerlink" title="XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder"></a>XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder</h2><p><strong>Authors:Shenghao Zhu, Yifei Chen, Shuo Jiang, Weihong Chen, Chang Liu, Yuanhan Wang, Xu Chen, Yifan Ke, Feiwei Qin, Zhu Zhu, Changmiao Wang</strong></p><p>Neurogliomas are among the most aggressive forms of cancer, presenting considerable challenges in both treatment and monitoring due to their unpredictable biological behavior. Magnetic resonance imaging (MRI) is currently the preferred method for diagnosing and monitoring gliomas. However, the lack of specific imaging techniques often compromises the accuracy of tumor segmentation during the imaging process. To address this issue, we introduce the XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder framework with the Vision XLSTM module to reconstruct missing MRI modalities. By deeply fusing spatial and temporal features, it enhances tumor segmentation performance. The key innovation of our approach is the Self-Attention Variational Encoder (SAVE) module, which improves the integration of modal features. Additionally, it optimizes the interaction of features between segmentation and reconstruction tasks through the Squeeze-Fusion-Excitation Cross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset demonstrate that our model significantly outperforms existing advanced methods in handling cases where modalities are missing. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED">https://github.com/Quanato607/XLSTM-HVED</a>.</p><blockquote><p>ç¥ç»èƒ¶è´¨ç˜¤æ˜¯æœ€å…·ä¾µè¢­æ€§çš„ç™Œç—‡å½¢å¼ä¹‹ä¸€ï¼Œç”±äºå…¶ä¸å¯é¢„æµ‹çš„ç”Ÿç‰©è¡Œä¸ºï¼Œä¸ºæ²»ç–—å’Œç›‘æµ‹å¸¦æ¥äº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ç›®å‰ï¼Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯è¯Šæ–­å’Œæ²»ç–—èƒ¶è´¨ç˜¤çš„é¦–é€‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç¼ºä¹ç‰¹å®šçš„æˆåƒæŠ€æœ¯å¾€å¾€ä¼šå½±å“æˆåƒè¿‡ç¨‹ä¸­è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†XLSTM-HVEDæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼‚æ¨¡å¼ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’ŒVision XLSTMæ¨¡å—ï¼Œä»¥é‡å»ºç¼ºå¤±çš„MRIæ¨¡å¼ã€‚é€šè¿‡æ·±åº¦èåˆç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæé«˜äº†è‚¿ç˜¤åˆ†å‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºè‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—ï¼Œå®ƒæ”¹è¿›äº†æ¨¡å¼ç‰¹å¾çš„èåˆã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡æŒ¤å‹-èåˆ-å…´å¥‹äº¤å‰æ„è¯†ï¼ˆSFECAï¼‰æ¨¡å—ä¼˜åŒ–äº†åˆ†å‰²å’Œé‡å»ºä»»åŠ¡ä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚æˆ‘ä»¬ä½¿ç”¨BraTS 2024æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡å¼çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰é«˜çº§æ–¹æ³•ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Quanato607/XLSTM-HVEDä¸­æ‰¾åˆ°ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07804v1">PDF</a> 5 pages, 2 figures</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ç¥ç»èƒ¶è´¨ç˜¤åœ¨æ²»ç–—å’Œç›‘æµ‹æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å…¶ä¸å¯é¢„æµ‹çš„ç”Ÿç‰©è¡Œä¸ºã€‚ä¸ºè§£å†³ç£å…±æŒ¯æˆåƒåœ¨è¯Šæ–­ä¸ç›‘æµ‹ç¥ç»èƒ¶è´¨ç˜¤æ—¶ç¼ºä¹ç‰¹å®šæˆåƒæŠ€æœ¯è€Œå¯¼è‡´è‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§å—é™çš„é—®é¢˜ï¼Œå¼•å…¥äº†XLSTM-HVEDæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆå¼‚æ„æ¨¡æ€ç¼–ç å™¨è§£ç å™¨æ¡†æ¶ä¸Vision XLSTMæ¨¡å—ï¼Œé‡å»ºç¼ºå¤±çš„MRIæ¨¡æ€ï¼Œé€šè¿‡æ·±åº¦èåˆæ—¶ç©ºç‰¹å¾æå‡è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºè‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—ï¼Œå®ƒæ”¹è¿›äº†æ¨¡æ€ç‰¹å¾çš„èåˆã€‚åŒæ—¶ï¼Œé€šè¿‡Squeeze-Fusion-Excitation Cross Awarenessï¼ˆSFECAï¼‰æ¨¡å—ä¼˜åŒ–åˆ†å‰²ä¸é‡å»ºä»»åŠ¡ä¹‹é—´ç‰¹å¾çš„äº¤äº’ã€‚ä½¿ç”¨BraTS 2024æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰é«˜çº§æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>ç¥ç»èƒ¶è´¨ç˜¤æ˜¯æå…·ä¾µç•¥æ€§çš„ç™Œç—‡ï¼Œå…¶æ²»ç–—ä¸ç›‘æµ‹é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”±äºå…¶ç”Ÿç‰©è¡Œä¸ºçš„ä¸å¯é¢„æµ‹æ€§ã€‚</p></li><li><p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ç›®å‰è¯Šæ–­ä¸ç›‘æµ‹ç¥ç»èƒ¶è´¨ç˜¤çš„é¦–é€‰æ–¹æ³•ï¼Œä½†ç¼ºä¹ç‰¹å®šçš„æˆåƒæŠ€æœ¯ä¼šå½±å“è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</p></li><li><p>å¼•å…¥çš„XLSTM-HVEDæ¨¡å‹é€šè¿‡ç»“åˆå¼‚æ„æ¨¡æ€ç¼–ç å™¨è§£ç å™¨æ¡†æ¶ä¸Vision XLSTMæ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</p></li><li><p>æ¨¡å‹é‡‡ç”¨è‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—æ¥æå‡æ¨¡æ€ç‰¹å¾çš„èåˆæ•ˆæœã€‚</p></li><li><p>SFECAæ¨¡å—ç”¨äºä¼˜åŒ–åˆ†å‰²ä¸é‡å»ºä»»åŠ¡ä¹‹é—´ç‰¹å¾çš„äº¤äº’ã€‚</p></li><li><p>ä½¿ç”¨BraTS 2024æ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µæ—¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Image-Retrieval-with-Intra-Sweep-Representation-Learning-for-Neck-Ultrasound-Scanning-Guidance"><a href="#Image-Retrieval-with-Intra-Sweep-Representation-Learning-for-Neck-Ultrasound-Scanning-Guidance" class="headerlink" title="Image Retrieval with Intra-Sweep Representation Learning for Neck   Ultrasound Scanning Guidance"></a>Image Retrieval with Intra-Sweep Representation Learning for Neck Ultrasound Scanning Guidance</h2><p><strong>Authors:Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E. Salcudean</strong></p><p>Purpose: Intraoperative ultrasound (US) can enhance real-time visualization in transoral robotic surgery. The surgeon creates a mental map with a pre-operative scan. Then, a surgical assistant performs freehand US scanning during the surgery while the surgeon operates at the remote surgical console. Communicating the target scanning plane in the surgeonâ€™s mental map is difficult. Automatic image retrieval can help match intraoperative images to preoperative scans, guiding the assistant to adjust the US probe toward the target plane. Methods: We propose a self-supervised contrastive learning approach to match intraoperative US views to a preoperative image database. We introduce a novel contrastive learning strategy that leverages intra-sweep similarity and US probe location to improve feature encoding. Additionally, our model incorporates a flexible threshold to reject unsatisfactory matches. Results: Our method achieves 92.30% retrieval accuracy on simulated data and outperforms state-of-the-art temporal-based contrastive learning approaches. Our ablation study demonstrates that using probe location in the optimization goal improves image representation, suggesting that semantic information can be extracted from probe location. We also present our approach on real patient data to show the feasibility of the proposed US probe localization system despite tissue deformation from tongue retraction. Conclusion: Our contrastive learning method, which utilizes intra-sweep similarity and US probe location, enhances US image representation learning. We also demonstrate the feasibility of using our image retrieval method to provide neck US localization on real patient US after tongue retraction.</p><blockquote><p>ç›®çš„ï¼šæœ¯ä¸­è¶…å£°ï¼ˆUSï¼‰èƒ½å¤Ÿå¢å¼ºåœ¨ç»å£æœºå™¨äººæ‰‹æœ¯ä¸­çš„å®æ—¶å¯è§†åŒ–æ•ˆæœã€‚å¤–ç§‘åŒ»ç”Ÿé€šè¿‡æœ¯å‰æ‰«æåˆ›å»ºä¸€ä¸ªå¿ƒç†åœ°å›¾ã€‚ç„¶åï¼Œæ‰‹æœ¯åŠ©ç†åœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­è¿›è¡Œè‡ªç”±æ‰‹è¶…å£°æ‰«æï¼Œè€Œå¤–ç§‘åŒ»ç”Ÿåˆ™åœ¨è¿œç¨‹æ‰‹æœ¯å°ä¸Šè¿›è¡Œæ“ä½œã€‚åœ¨å¤–ç§‘åŒ»ç”Ÿçš„å¿ƒä¸­ä¼ è¾¾ç›®æ ‡æ‰«æå¹³é¢æ˜¯å›°éš¾çš„ã€‚è‡ªåŠ¨å›¾åƒæ£€ç´¢å¯ä»¥å¸®åŠ©å°†æœ¯ä¸­å›¾åƒä¸æœ¯å‰æ‰«æè¿›è¡ŒåŒ¹é…ï¼Œä»è€ŒæŒ‡å¯¼åŠ©ç†è°ƒæ•´è¶…å£°æ¢å¤´æœå‘ç›®æ ‡å¹³é¢ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•æ¥åŒ¹é…æœ¯ä¸­è¶…å£°è§†å›¾åˆ°æœ¯å‰å›¾åƒæ•°æ®åº“ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æ‰«æå†…ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®æ¥æé«˜ç‰¹å¾ç¼–ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨çµæ´»çš„é˜ˆå€¼æ¥æ‹’ç»ä¸æ»¡æ„çš„åŒ¹é…ç»“æœã€‚ç»“æœï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šå®ç°äº†92.30%çš„æ£€ç´¢ç²¾åº¦ï¼Œå¹¶è¶…è¶Šäº†åŸºäºæ—¶é—´çš„å¯¹æ¯”å­¦ä¹ å‰æ²¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯æ˜ï¼Œåœ¨ä¼˜åŒ–ç›®æ ‡ä¸­ä½¿ç”¨æ¢å¤´ä½ç½®å¯ä»¥æ”¹å–„å›¾åƒè¡¨ç¤ºï¼Œè¿™è¡¨æ˜å¯ä»¥ä»æ¢å¤´ä½ç½®æå–è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨çœŸå®æ‚£è€…æ•°æ®ä¸Šåº”ç”¨æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»¥å±•ç¤ºå°½ç®¡èˆŒé€€ç¼©å¯¼è‡´ç»„ç»‡å˜å½¢ï¼Œæ‰€æå‡ºçš„è¶…å£°æ¢å¤´å®šä½ç³»ç»Ÿä»ç„¶å¯è¡Œã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•åˆ©ç”¨æ‰«æå†…ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®ï¼Œå¢å¼ºäº†è¶…å£°å›¾åƒè¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†ä½¿ç”¨æˆ‘ä»¬çš„å›¾åƒæ£€ç´¢æ–¹æ³•åœ¨çœŸå®æ‚£è€…è¶…å£°ä¸­è¿›è¡Œé¢ˆéƒ¨è¶…å£°å®šä½æ˜¯å¯è¡Œçš„ï¼Œå°½ç®¡å­˜åœ¨èˆŒé€€ç¼©çš„æƒ…å†µã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07741v1">PDF</a> 12 pages, 5 figures</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç»å£æœºå™¨äººæ‰‹æœ¯ä¸­ï¼Œåˆ©ç”¨æœ¯ä¸­è¶…å£°ï¼ˆUSï¼‰å¢å¼ºå®æ—¶å¯è§†åŒ–æŠ€æœ¯çš„è¿‡ç¨‹ã€‚ä¸ºæé«˜æ‰‹æœ¯æ•ˆç‡ï¼ŒåŒ»ç”Ÿåœ¨æœ¯å‰è¿›è¡Œæ‰«æä»¥å»ºç«‹å¿ƒç†åœ°å›¾ï¼Œç„¶åç”±æ‰‹æœ¯åŠ©ç†åœ¨æœ¯ä¸­æ‰§è¡Œè‡ªç”±æ‰‹è¶…å£°æ‰«æã€‚ä¸ºæé«˜åŠ©ç†è°ƒæ•´è¶…å£°æ¢å¤´æŒ‡å‘ç›®æ ‡å¹³é¢çš„å‡†ç¡®æ€§ï¼Œæ–‡ç« æå‡ºä¸€ç§è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ¥åŒ¹é…æœ¯ä¸­è¶…å£°è§†å›¾ä¸æœ¯å‰å›¾åƒæ•°æ®åº“ã€‚æ–°æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°å‹å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æ‰«æŸ¥å†…çš„ç›¸ä¼¼æ€§ä»¥åŠè¶…å£°æ¢å¤´ä½ç½®æ¥æ”¹å–„ç‰¹å¾ç¼–ç ï¼Œå¹¶é€šè¿‡çµæ´»çš„é˜ˆå€¼æ‹’ç»ä¸æ»¡æ„çš„åŒ¹é…ã€‚æ¨¡æ‹Ÿæ•°æ®æ˜¾ç¤ºæ–°æ–¹æ³•è¾¾åˆ°92.3%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¼˜äºåŸºäºæ—¶é—´çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†å°†æ¢å¤´ä½ç½®çº³å…¥ä¼˜åŒ–ç›®æ ‡èƒ½å¤Ÿæé«˜å›¾åƒè¡¨ç°çš„ç ”ç©¶ç»“æœã€‚åœ¨çœŸå®æ‚£è€…æ•°æ®ä¸­ï¼Œå³ä¾¿å­˜åœ¨èˆŒåç¼©å¯¼è‡´çš„ç»„ç»‡å˜å½¢é—®é¢˜ï¼Œæ–°æ–¹æ³•çš„å¯è¡Œæ€§ä¾ç„¶å¾—ä»¥è¯æ˜ã€‚æ€»ä½“æ¥è¯´ï¼Œæ–‡ç« åˆ›æ–°åœ°åˆ©ç”¨äº†å¯¹æ¯”å­¦ä¹ æ–¹æ³•æé«˜è¶…å£°å›¾åƒå­¦ä¹ çš„æ•ˆæœï¼Œä¸ºé¢ˆéƒ¨è¶…å£°å®šä½æä¾›äº†æ–°çš„åº”ç”¨å‰æ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æœ¯ä¸­è¶…å£°åœ¨ç»å£æœºå™¨äººæ‰‹æœ¯ä¸­èµ·åˆ°å¢å¼ºå®æ—¶å¯è§†åŒ–çš„ä½œç”¨ã€‚</li><li>åŒ»ç”Ÿé€šè¿‡æœ¯å‰æ‰«æå»ºç«‹å¿ƒç†åœ°å›¾ï¼Œæ‰‹æœ¯åŠ©ç†æ‰§è¡Œè‡ªç”±æ‰‹è¶…å£°æ‰«æã€‚</li><li>æå‡ºä¸€ç§è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ¥åŒ¹é…æœ¯ä¸­è¶…å£°è§†å›¾ä¸æœ¯å‰å›¾åƒæ•°æ®åº“ã€‚</li><li>åˆ©ç”¨æ‰«æŸ¥å†…çš„ç›¸ä¼¼æ€§å’Œè¶…å£°æ¢å¤´ä½ç½®æ”¹å–„ç‰¹å¾ç¼–ç ï¼Œæé«˜å›¾åƒæ£€ç´¢å‡†ç¡®ç‡ã€‚</li><li>æ¨¡æ‹Ÿæ•°æ®æ˜¾ç¤ºæ–°æ–¹æ³•è¾¾åˆ°92.3%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¼˜äºåŸºäºæ—¶é—´çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚</li><li>çº³å…¥æ¢å¤´ä½ç½®ä¿¡æ¯èƒ½å¤Ÿæé«˜å›¾åƒè¡¨ç°ï¼Œè¯æ˜äº†è¯­ä¹‰ä¿¡æ¯å¯ä»¥ä»æ¢å¤´ä½ç½®ä¸­æå–ã€‚</li></ol><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><h2 id="SKIPNet-Spatial-Attention-Skip-Connections-for-Enhanced-Brain-Tumor-Classification"><a href="#SKIPNet-Spatial-Attention-Skip-Connections-for-Enhanced-Brain-Tumor-Classification" class="headerlink" title="SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor   Classification"></a>SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor Classification</h2><p><strong>Authors:Khush Mendiratta, Shweta Singh, Pratik Chattopadhyay</strong></p><p>Early detection of brain tumors through magnetic resonance imaging (MRI) is essential for timely treatment, yet access to diagnostic facilities remains limited in remote areas. Gliomas, the most common primary brain tumors, arise from the carcinogenesis of glial cells in the brain and spinal cord, with glioblastoma patients having a median survival time of less than 14 months. MRI serves as a non-invasive and effective method for tumor detection, but manual segmentation of brain MRI scans has traditionally been a labor-intensive task for neuroradiologists. Recent advancements in computer-aided design (CAD), machine learning (ML), and deep learning (DL) offer promising solutions for automating this process. This study proposes an automated deep learning model for brain tumor detection and classification using MRI data. The model, incorporating spatial attention, achieved 96.90% accuracy, enhancing the aggregation of contextual information for better pattern recognition. Experimental results demonstrate that the proposed approach outperforms baseline models, highlighting its robustness and potential for advancing automated MRI-based brain tumor analysis.</p><blockquote><p>æ—©æœŸé€šè¿‡ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ£€æµ‹è„‘è‚¿ç˜¤å¯¹äºåŠæ—¶æ²»ç–—è‡³å…³é‡è¦ï¼Œä½†åœ¨åè¿œåœ°åŒºï¼Œè¯Šæ–­è®¾æ–½çš„è·å–ä»ç„¶æœ‰é™ã€‚èƒ¶è´¨ç˜¤æ˜¯æœ€å¸¸è§çš„åŸå‘æ€§è„‘è‚¿ç˜¤ï¼Œèµ·æºäºè„‘å’Œè„Šé«“èƒ¶è´¨ç»†èƒçš„ç™Œå˜ï¼Œèƒ¶è´¨æ¯ç»†èƒç˜¤æ‚£è€…çš„ä¸­ä½ç”Ÿå­˜æ—¶é—´ä¸åˆ°14ä¸ªæœˆã€‚MRIä½œä¸ºä¸€ç§éä¾µå…¥æ€§çš„æœ‰æ•ˆè‚¿ç˜¤æ£€æµ‹æ–¹æ³•ï¼Œä½†æ‰‹åŠ¨åˆ†å‰²è„‘éƒ¨MRIæ‰«æå›¾åƒä¸€ç›´æ˜¯ç¥ç»æ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡ã€‚è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ã€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨MRIæ•°æ®è‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»è„‘è‚¿ç˜¤çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¾¾åˆ°äº†96.90%çš„å‡†ç¡®ç‡ï¼Œé€šè¿‡å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯çš„èšåˆï¼Œæé«˜äº†æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œçªæ˜¾äº†å…¶ç¨³å¥æ€§å’Œåœ¨æ¨è¿›åŸºäºMRIçš„è‡ªåŠ¨åŒ–è„‘è‚¿ç˜¤åˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07736v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»è„‘è‚¿ç˜¤çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶é‡‡ç”¨åŒ…å«ç©ºé—´æ³¨æ„åŠ›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡å¯¹MRIæ•°æ®çš„åˆ†æï¼Œå®ç°äº†é«˜è¾¾96.90%çš„å‡†ç¡®ç‡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å®Œæˆè‚¿ç˜¤æ£€æµ‹ä»»åŠ¡ï¼Œæé«˜äº†è¯Šæ–­æ•ˆç‡ï¼Œæœ‰æœ›æ¨åŠ¨åŸºäºMRIçš„è„‘è‚¿ç˜¤åˆ†æè¿›æ­¥ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>æ—©æœŸé€šè¿‡ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ£€æµ‹è„‘è‚¿ç˜¤å¯¹åŠæ—¶æ²»ç–—è‡³å…³é‡è¦ã€‚</p></li><li><p>èƒ¶è´¨ç˜¤æ˜¯æœ€å¸¸è§çš„åŸå‘æ€§è„‘è‚¿ç˜¤ï¼Œèµ·æºäºå¤§è„‘å’Œè„Šé«“çš„èƒ¶è´¨ç»†èƒç™Œå˜ã€‚</p></li><li><p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ã€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„è¿›æ­¥ä¸ºè‡ªåŠ¨åŒ–æ£€æµ‹MRIä¸­çš„è„‘è‚¿ç˜¤æä¾›äº†å¯èƒ½æ€§ã€‚</p></li><li><p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆç©ºé—´æ³¨æ„åŠ›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç”¨äºè‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»è„‘è‚¿ç˜¤ã€‚</p></li><li><p>è¯¥æ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°äº†96.90%ï¼Œä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨è„‘è‚¿ç˜¤åˆ†æä¸­çš„ç¨³å¥æ€§å’Œæ½œåŠ›ã€‚</p></li><li><p>é€šè¿‡æ¨¡å‹å®éªŒéªŒè¯è¡¨æ˜å…¶åœ¨èšé›†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€æ¨¡å¼è¯†åˆ«æ–¹é¢æœ‰æ˜¾è‘—æå‡æ•ˆæœã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model"><a href="#Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model" class="headerlink" title="Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model"></a>Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks and Diffusion Model</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Jianfeng Guo, Feng Yang, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p><p>Motion artifacts present in magnetic resonance imaging (MRI) can seriously interfere with clinical diagnosis. Removing motion artifacts is a straightforward solution and has been extensively studied. However, paired data are still heavily relied on in recent works and the perturbations in k-space (frequency domain) are not well considered, which limits their applications in the clinical field. To address these issues, we propose a novel unsupervised purification method which leverages pixel-frequency information of noisy MRI images to guide a pre-trained diffusion model to recover clean MRI images. Specifically, considering that motion artifacts are mainly concentrated in high-frequency components in k-space, we utilize the low-frequency components as the guide to ensure correct tissue textures. Additionally, given that high-frequency and pixel information are helpful for recovering shape and detail textures, we design alternate complementary masks to simultaneously destroy the artifact structure and exploit useful information. Quantitative experiments are performed on datasets from different tissues and show that our method achieves superior performance on several metrics. Qualitative evaluations with radiologists also show that our method provides better clinical feedback. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD">https://github.com/medcx/PFAD</a>.</p><blockquote><p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šä¸¥é‡å¹²æ‰°ä¸´åºŠè¯Šæ–­ã€‚å»é™¤è¿åŠ¨ä¼ªå½±æ˜¯ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶ä»ç„¶ä¸¥é‡ä¾èµ–äºé…å¯¹æ•°æ®ï¼Œè€Œkç©ºé—´ï¼ˆé¢‘ç‡åŸŸï¼‰ä¸­çš„æ‰°åŠ¨å¹¶æœªå¾—åˆ°å¾ˆå¥½çš„è€ƒè™‘ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠé¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¸¦å™ªå£°çš„MRIå›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´çš„MRIå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œè€ƒè™‘åˆ°è¿åŠ¨ä¼ªå½±ä¸»è¦é›†ä¸­åœ¨kç©ºé—´çš„é«˜é¢‘æˆåˆ†ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„çº¹ç†ç»„ç»‡ã€‚æ­¤å¤–ï¼Œé‰´äºé«˜é¢‘å’Œåƒç´ ä¿¡æ¯æœ‰åŠ©äºæ¢å¤å½¢çŠ¶å’Œç»†èŠ‚çº¹ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†äº¤æ›¿çš„äº’è¡¥æ©è†œæ¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚åœ¨ä¸åŒç»„ç»‡æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„å®šæ€§è¯„ä¼°ä¹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æ›´å¥½çš„ä¸´åºŠåé¦ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/medcx/PFADæ‰¾åˆ°ã€‚]</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07590v2">PDF</a> 12 pages, 8 figures, AAAI 2025</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å™ªå£°MRIå›¾åƒçš„åƒç´ é¢‘ç‡ä¿¡æ¯å¼•å¯¼é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¢å¤æ¸…æ´MRIå›¾åƒï¼Œä»¥è§£å†³ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨ä¼ªå½±é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨kç©ºé—´ä¸­çš„ä½é¢‘æˆåˆ†ä½œä¸ºæŒ‡å¯¼ï¼Œç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ï¼ŒåŒæ—¶è®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œæ¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¾—åˆ°æ”¾å°„ç§‘çš„å®šæ€§è¯„ä¼°è®¤å¯ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šå¹²æ‰°ä¸´åºŠè¯Šæ–­ï¼Œå»é™¤ä¼ªå½±æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</p></li><li><p>å½“å‰ç ”ç©¶ä»ä¾èµ–é…å¯¹æ•°æ®ï¼Œä¸”å¿½ç•¥äº†kç©ºé—´ä¸­çš„æ‰°åŠ¨ï¼Œé™åˆ¶äº†ä¸´åºŠåº”ç”¨ã€‚</p></li><li><p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ— ç›‘ç£å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨åƒç´ é¢‘ç‡ä¿¡æ¯å»é™¤MRIå›¾åƒä¸­çš„è¿åŠ¨ä¼ªå½±ã€‚</p></li><li><p>æ–¹æ³•é›†ä¸­åœ¨åˆ©ç”¨kç©ºé—´ä¸­çš„é«˜é¢‘å’Œä½é¢‘ä¿¡æ¯æ¥æ¢å¤å›¾åƒï¼Œç¡®ä¿æ­£ç¡®çš„ç»„ç»‡çº¹ç†ã€‚</p></li><li><p>è®¾è®¡äº¤æ›¿äº’è¡¥æ©è†œä»¥åŒæ—¶ç ´åä¼ªå½±ç»“æ„å¹¶æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€‚</p></li><li><p>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p></li></ol><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07754v1">PDF</a></p><h2 id="CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings"><a href="#CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings" class="headerlink" title="CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings"></a>CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings</h2><p><strong>Authors:Jiazuo Mu, Fuyi Yang, Yanshun Zhang, Junxiong Zhang, Yongjian Luo, Lan Xu, Yujiao Shi, Jingyi Yu, Yingliang Zhang</strong></p><p>We introduce CADSpotting, an efficient method for panoptic symbol spotting in large-scale architectural CAD drawings. Existing approaches struggle with the diversity of symbols, scale variations, and overlapping elements in CAD designs. CADSpotting overcomes these challenges by representing each primitive with dense points instead of a single primitive point, described by essential attributes like coordinates and color. Building upon a unified 3D point cloud model for joint semantic, instance, and panoptic segmentation, CADSpotting learns robust feature representations. To enable accurate segmentation in large, complex drawings, we further propose a novel Sliding Window Aggregation (SWA) technique, combining weighted voting and Non-Maximum Suppression (NMS). Moreover, we introduce a large-scale CAD dataset named LS-CAD to support our experiments. Each floorplan in LS-CAD has an average coverage of 1,000 square meter(versus 100 square meter in the existing dataset), providing a valuable benchmark for symbol spotting research. Experimental results on FloorPlanCAD and LS-CAD datasets demonstrate that CADSpotting outperforms existing methods, showcasing its robustness and scalability for real-world CAD applications.</p><blockquote><p>æˆ‘ä»¬ä»‹ç»äº†CADSpottingï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§å‹å»ºç­‘CADå›¾çº¸ä¸­è¿›è¡Œå…¨æ™¯ç¬¦å·è¯†åˆ«çš„é«˜æ•ˆæ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†CADè®¾è®¡ä¸­çš„ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œå…ƒç´ é‡å æ–¹é¢å­˜åœ¨å›°éš¾ã€‚CADSpottingé€šè¿‡ç”¨å¯†é›†ç‚¹è¡¨ç¤ºæ¯ä¸ªåŸºæœ¬å…ƒç´ è€Œä¸æ˜¯å•ä¸ªåŸºæœ¬ç‚¹æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œè¿™äº›å¯†é›†ç‚¹ç”±åæ ‡å’Œé¢œè‰²ç­‰åŸºæœ¬å±æ€§æè¿°ã€‚åŸºäºç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹è¿›è¡Œè”åˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ï¼ŒCADSpottingå­¦ä¹ é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†åœ¨å¤§è§„æ¨¡å¤æ‚å›¾çº¸ä¸Šå®ç°ç²¾ç¡®åˆ†å‰²ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†åŠ æƒæŠ•ç¥¨å’Œéæœ€å¤§æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡CADæ•°æ®é›†LS-CADæ¥æ”¯æŒæˆ‘ä»¬çš„å®éªŒã€‚LS-CADä¸­çš„æ¯ä¸ªå¹³é¢å›¾å¹³å‡è¦†ç›–é¢ç§¯ä¸º1000å¹³æ–¹ç±³ï¼ˆè€Œç°æœ‰æ•°æ®é›†ä¸­çš„é¢ç§¯ä¸º100å¹³æ–¹ç±³ï¼‰ï¼Œä¸ºç¬¦å·è¯†åˆ«ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åŸºå‡†ã€‚åœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCADSpottingä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œçš„CADåº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07377v2">PDF</a> 16pages, 12 figures, Project web-page: <a target="_blank" rel="noopener" href="https://dgeneai.github.io/cadspotting-pages/">https://dgeneai.github.io/cadspotting-pages/</a></p><p><strong>Summary</strong></p><p>CADSpottingæ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå¤§å‹å»ºç­‘CADå›¾çº¸ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œå…ƒç´ é‡å ç­‰æŒ‘æˆ˜æ—¶çš„å›°éš¾ï¼ŒCADSpottingé€šè¿‡å¯†é›†çš„ç‚¹è¡¨ç¤ºæ¯ä¸ªåŸºæœ¬å…ƒç´ ï¼Œç»“åˆåæ ‡å’Œé¢œè‰²ç­‰å…³é”®å±æ€§è¿›è¡Œæè¿°ï¼Œå®ç°äº†æ›´ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚åŒæ—¶ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹è¿›è¡Œè”åˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ï¼Œå¹¶å¼•å…¥Sliding Window Aggregationï¼ˆSWAï¼‰æŠ€æœ¯æé«˜å¤§å‹å¤æ‚å›¾çº¸çš„åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå»ºç«‹äº†å¤§è§„æ¨¡CADæ•°æ®é›†LS-CADï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCADSpottingåœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºå…¶åœ¨çœŸå®CADåº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>CADSpottingæ˜¯ä¸€ç§ç”¨äºå¤§å‹å»ºç­‘CADå›¾çº¸ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«çš„é«˜æ•ˆæ–¹æ³•ã€‚</p></li><li><p>ç°æœ‰æ–¹æ³•åœ¨ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œå…ƒç´ é‡å æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</p></li><li><p>CADSpottingé€šè¿‡å¯†é›†çš„ç‚¹è¡¨ç¤ºæ¯ä¸ªåŸºæœ¬å…ƒç´ ï¼Œç»“åˆåæ ‡å’Œé¢œè‰²ç­‰å±æ€§è¿›è¡Œæè¿°ã€‚</p></li><li><p>é‡‡ç”¨ç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹è¿›è¡Œè”åˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ã€‚</p></li><li><p>å¼•å…¥Sliding Window Aggregationï¼ˆSWAï¼‰æŠ€æœ¯æé«˜å¤§å‹å¤æ‚å›¾çº¸çš„åˆ†å‰²ç²¾åº¦ã€‚</p></li><li><p>å»ºç«‹äº†å¤§è§„æ¨¡CADæ•°æ®é›†LS-CADï¼Œä¸ºç¬¦å·è¯†åˆ«ç ”ç©¶æä¾›åŸºå‡†æµ‹è¯•ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="A-Generative-Victim-Model-for-Segmentation"><a href="#A-Generative-Victim-Model-for-Segmentation" class="headerlink" title="A Generative Victim Model for Segmentation"></a>A Generative Victim Model for Segmentation</h2><p><strong>Authors:Aixuan Li, Jing Zhang, Jiawei Shi, Yiran Zhong, Yuchao Dai</strong></p><p>We find that the well-trained victim models (VMs), against which the attacks are generated, serve as fundamental prerequisites for adversarial attacks, i.e. a segmentation VM is needed to generate attacks for segmentation. In this context, the victim model is assumed to be robust to achieve effective adversarial perturbation generation. Instead of focusing on improving the robustness of the task-specific victim models, we shift our attention to image generation. From an image generation perspective, we derive a novel VM for segmentation, aiming to generate adversarial perturbations for segmentation tasks without requiring models explicitly designed for image segmentation. Our approach to adversarial attack generation diverges from conventional white-box or black-box attacks, offering a fresh outlook on adversarial attack strategies. Experiments show that our attack method is able to generate effective adversarial attacks with good transferability.</p><blockquote><p>æˆ‘ä»¬å‘ç°ï¼Œé’ˆå¯¹ç”Ÿæˆçš„æ”»å‡»ï¼Œè®­ç»ƒè‰¯å¥½çš„ç›®æ ‡æ¨¡å‹ï¼ˆVMsï¼‰æ˜¯å¯¹æŠ—æ”»å‡»çš„åŸºæœ¬å‰æã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºåˆ†å‰²ä»»åŠ¡ï¼Œéœ€è¦åˆ†å‰²VMsæ¥ç”Ÿæˆæ”»å‡»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾ç›®æ ‡æ¨¡å‹æ˜¯å¥å£®çš„ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¯¹æŠ—æ€§æ‰°åŠ¨ç”Ÿæˆã€‚æˆ‘ä»¬ä¸å†ä¸“æ³¨äºæé«˜ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹å¥å£®æ€§ï¼Œè€Œæ˜¯å°†æ³¨æ„åŠ›è½¬å‘å›¾åƒç”Ÿæˆã€‚ä»å›¾åƒç”Ÿæˆçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ä¸ºåˆ†å‰²è®¾è®¡äº†ä¸€ç§æ–°å‹VMsï¼Œæ—¨åœ¨é’ˆå¯¹åˆ†å‰²ä»»åŠ¡ç”Ÿæˆå¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œè€Œæ— éœ€ä¸“é—¨è®¾è®¡ç”¨äºå›¾åƒåˆ†å‰²çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹å¯¹æŠ—æ€§æ”»å‡»ç”Ÿæˆçš„æ–¹æ³•ä¸åŒäºä¼ ç»Ÿçš„ç™½ç›’æˆ–é»‘ç›’æ”»å‡»ï¼Œä¸ºå¯¹æŠ—æ€§æ”»å‡»ç­–ç•¥æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”»å‡»æ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰æœ‰æ•ˆè¿ç§»æ€§çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07274v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¯¹æŠ—æ”»å‡»ç”Ÿæˆæ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒè‰¯å¥½çš„å—å®³è€…æ¨¡å‹ï¼ˆVMsï¼‰æ˜¯ç”Ÿæˆå¯¹æŠ—æ”»å‡»çš„åŸºç¡€å‰æï¼Œä½†æé«˜ç‰¹å®šä»»åŠ¡çš„å—å®³è€…æ¨¡å‹é²æ£’æ€§å¹¶éæœ€ä¼˜ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿä»å›¾åƒç”Ÿæˆè§’åº¦å‡ºå‘ï¼Œæ¨å‡ºæ–°çš„åˆ†å‰²VMï¼Œæ—¨åœ¨ç”Ÿæˆé’ˆå¯¹åˆ†å‰²ä»»åŠ¡çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œæ— éœ€ä¸“é—¨è®¾è®¡å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚è¯¥ç ”ç©¶æå‡ºçš„æ”»å‡»ç”Ÿæˆç­–ç•¥ä¸åŒäºä¼ ç»Ÿçš„ç™½ç›’æˆ–é»‘ç›’æ”»å‡»ï¼Œä¸ºå¯¹æŠ—æ”»å‡»ç­–ç•¥æä¾›äº†å…¨æ–°è§†è§’ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¼˜å¼‚è¿ç§»æ€§çš„æœ‰æ•ˆå¯¹æŠ—æ”»å‡»ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å—å®³è€…æ¨¡å‹ï¼ˆVMsï¼‰åœ¨ç”Ÿæˆå¯¹æŠ—æ”»å‡»æ–¹é¢èµ·åŸºç¡€ä½œç”¨ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰éœ€è¦ç›¸åº”çš„VMæ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚</li><li>ç ”ç©¶ç„¦ç‚¹ä»æé«˜ç‰¹å®šä»»åŠ¡VMçš„é²æ£’æ€§è½¬å‘å›¾åƒç”Ÿæˆã€‚</li><li>æå‡ºæ–°çš„VMç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œèƒ½åœ¨æ— éœ€ä¸“é—¨è®¾è®¡å›¾åƒåˆ†å‰²æ¨¡å‹çš„æƒ…å†µä¸‹ç”Ÿæˆå¯¹æŠ—æ‰°åŠ¨ã€‚</li><li>è¯¥ç ”ç©¶æå‡ºçš„æ”»å‡»ç”Ÿæˆç­–ç•¥ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œä¸ºå¯¹æŠ—æ”»å‡»ç­–ç•¥æä¾›æ–°çš„è§†è§’ã€‚</li><li>æ–°æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ”»å‡»å…·æœ‰æœ‰æ•ˆæ€§å’Œè¿ç§»æ€§ã€‚</li><li>è¯¥ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒé¢†åŸŸçš„å¯¹æŠ—æ”»å‡»æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li></ol><p><strong>Authors:Sejong Yang, Seoung Wug Oh, Yang Zhou, Seon Joo Kim</strong></p><h2 id="MPSI-Mamba-enhancement-model-for-pixel-wise-sequential-interaction-Image-Super-Resolution"><a href="#MPSI-Mamba-enhancement-model-for-pixel-wise-sequential-interaction-Image-Super-Resolution" class="headerlink" title="MPSI: Mamba enhancement model for pixel-wise sequential interaction   Image Super-Resolution"></a>MPSI: Mamba enhancement model for pixel-wise sequential interaction Image Super-Resolution</h2><p><strong>Authors:Yuchun He, Yuhan He</strong></p><p>Single image super-resolution (SR) has long posed a challenge in the field of computer vision. While the advent of deep learning has led to the emergence of numerous methods aimed at tackling this persistent issue, the current methodologies still encounter challenges in modeling long sequence information, leading to limitations in effectively capturing the global pixel interactions. To tackle this challenge and achieve superior SR outcomes, we propose the Mamba pixel-wise sequential interaction network (MPSI), aimed at enhancing the establishment of long-range connections of information, particularly focusing on pixel-wise sequential interaction. We propose the Channel-Mamba Block (CMB) to capture comprehensive pixel interaction information by effectively modeling long sequence information. Moreover, in the existing SR methodologies, there persists the issue of the neglect of features extracted by preceding layers, leading to the loss of valuable feature information. While certain existing models strive to preserve these features, they frequently encounter difficulty in establishing connections across all layers. To overcome this limitation, MPSI introduces the Mamba channel recursion module (MCRM), which maximizes the retention of valuable feature information from early layers, thereby facilitating the acquisition of pixel sequence interaction information from multiple-level layers. Through extensive experimentation, we demonstrate that MPSI outperforms existing super-resolution methods in terms of image reconstruction results, attaining state-of-the-art performance.</p><blockquote><p>å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ çš„å‡ºç°å¯¼è‡´äº†ä¼—å¤šæ–¹æ³•çš„å‡ºç°ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æŒä¹…æ€§é—®é¢˜ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨å»ºæ¨¡é•¿åºåˆ—ä¿¡æ¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´åœ¨æœ‰æ•ˆæ•è·å…¨å±€åƒç´ äº¤äº’æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜å¹¶å®ç°æ›´ä¼˜è¶Šçš„è¶…åˆ†è¾¨ç‡ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†Mambaåƒç´ çº§é¡ºåºäº¤äº’ç½‘ç»œï¼ˆMPSIï¼‰ï¼Œæ—¨åœ¨å¢å¼ºé•¿è·ç¦»ä¿¡æ¯è¿æ¥çš„å»ºç«‹ï¼Œç‰¹åˆ«å…³æ³¨åƒç´ çº§çš„é¡ºåºäº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†é€šé“æ¯å·´å—ï¼ˆCMBï¼‰ï¼Œé€šè¿‡æœ‰æ•ˆåœ°å¯¹é•¿åºåˆ—ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ï¼Œæ¥æ•æ‰å…¨é¢çš„åƒç´ äº¤äº’ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œåœ¨ç°æœ‰çš„SRæ–¹æ³•ä¸­ï¼Œä»ç„¶å­˜åœ¨å¿½è§†å…ˆå‰å±‚æ¬¡æå–çš„ç‰¹å¾çš„é—®é¢˜ï¼Œå¯¼è‡´æœ‰ä»·å€¼çš„ç‰¹å¾ä¿¡æ¯ä¸¢å¤±ã€‚è™½ç„¶ä¸€äº›ç°æœ‰æ¨¡å‹åŠªåŠ›ä¿ç•™è¿™äº›ç‰¹å¾ï¼Œä½†å®ƒä»¬ç»å¸¸éš¾ä»¥åœ¨æ‰€æœ‰å±‚æ¬¡ä¹‹é—´å»ºç«‹è”ç³»ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼ŒMPSIå¼•å…¥äº†Mambaé€šé“é€’å½’æ¨¡å—ï¼ˆMCRMï¼‰ï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™æ—©æœŸå±‚æ¬¡çš„å®è´µç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œä¾¿äºä»å¤šå±‚æ¬¡è·å–åƒç´ åºåˆ—äº¤äº’ä¿¡æ¯ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜MPSIåœ¨å›¾åƒé‡å»ºç»“æœæ–¹é¢ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07222v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMambaåƒç´ çº§åºåˆ—äº¤äº’ç½‘ç»œï¼ˆMPSIï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥Channel-Mamba Blockï¼ˆCMBï¼‰å’ŒMambaé€šé“é€’å½’æ¨¡å—ï¼ˆMCRMï¼‰æ¥å¢å¼ºé•¿æœŸä¿¡æ¯è¿æ¥ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰å…¨å±€åƒç´ äº¤äº’ï¼Œä»è€Œå®ç°äº†å…ˆè¿›çš„SRæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>MPSIç½‘ç»œæ—¨åœ¨è§£å†³å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸­çš„é•¿æœŸä¿¡æ¯å»ºæ¨¡æŒ‘æˆ˜ã€‚</p></li><li><p>å¼•å…¥Channel-Mamba Blockï¼ˆCMBï¼‰ä»¥æ•æ‰å…¨é¢çš„åƒç´ äº¤äº’ä¿¡æ¯ã€‚</p></li><li><p>ç°æœ‰SRæ–¹æ³•å¿½è§†äº†å…ˆå‰å±‚æ¬¡æå–çš„ç‰¹å¾ï¼Œå¯¼è‡´ç‰¹å¾ä¿¡æ¯ä¸¢å¤±ã€‚</p></li><li><p>MPSIé€šè¿‡å¼•å…¥Mambaé€šé“é€’å½’æ¨¡å—ï¼ˆMCRMï¼‰æ¥å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œä¿ç•™æ—©æœŸå±‚æ¬¡çš„æœ‰ä»·å€¼ç‰¹å¾ä¿¡æ¯ã€‚</p></li><li><p>MPSIæ–¹æ³•èƒ½å¤Ÿä»å¤šå±‚æ¬¡è·å–åƒç´ åºåˆ—äº¤äº’ä¿¡æ¯ã€‚</p></li><li><p>å®éªŒè¯æ˜ï¼ŒMPSIåœ¨å›¾åƒé‡å»ºç»“æœä¸Šä¼˜äºç°æœ‰è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Integrating-MedCLIP-and-Cross-Modal-Fusion-for-Automatic-Radiology-Report-Generation"><a href="#Integrating-MedCLIP-and-Cross-Modal-Fusion-for-Automatic-Radiology-Report-Generation" class="headerlink" title="Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology   Report Generation"></a>Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology Report Generation</h2><p><strong>Authors:Qianhao Han, Junyi Liu, Zengchang Qin, Zheng Zheng</strong></p><p>Automating radiology report generation can significantly reduce the workload of radiologists and enhance the accuracy, consistency, and efficiency of clinical documentation.We propose a novel cross-modal framework that uses MedCLIP as both a vision extractor and a retrieval mechanism to improve the process of medical report generation.By extracting retrieved report features and image features through an attention-based extract module, and integrating them with a fusion module, our method improves the coherence and clinical relevance of generated reports.Experimental results on the widely used IU-Xray dataset demonstrate the effectiveness of our approach, showing improvements over commonly used methods in both report quality and relevance.Additionally, ablation studies provide further validation of the framework, highlighting the importance of accurate report retrieval and feature integration in generating comprehensive medical reports.</p><blockquote><p>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¯ä»¥æ˜¾è‘—å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œæé«˜ä¸´åºŠæ–‡æ¡£çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ¡†æ¶ï¼Œä½¿ç”¨MedCLIPä½œä¸ºè§†è§‰æå–å™¨å’Œæ£€ç´¢æœºåˆ¶ï¼Œä»¥æ”¹è¿›åŒ»ç–—æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡æ³¨æ„åŠ›åŸºç¡€ä¸Šçš„æå–æ¨¡å—æå–æ£€ç´¢æŠ¥å‘Šç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸èåˆæ¨¡å—ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„ä¸€è‡´æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„IU-Xrayæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æŠ¥å‘Šè´¨é‡å’Œç›¸å…³æ€§æ–¹é¢éƒ½ä¼˜äºå¸¸ç”¨æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†å‡†ç¡®æŠ¥å‘Šæ£€ç´¢å’Œç‰¹å¾èåˆåœ¨ç”Ÿæˆç»¼åˆåŒ»ç–—æŠ¥å‘Šä¸­çš„é‡è¦æ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07141v1">PDF</a> Accepted in IEEE Big Data 2024</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºä¸€ç§ä½¿ç”¨MedCLIPä½œä¸ºè§†è§‰æå–å™¨å’Œæ£€ç´¢æœºåˆ¶çš„è·¨æ¨¡æ€æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›åŒ»å­¦æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡æå–æ£€ç´¢æŠ¥å‘Šç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œå¹¶ä½¿ç”¨èåˆæ¨¡å—è¿›è¡Œæ•´åˆï¼Œè¯¥æ¡†æ¶æé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„è¿è´¯æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚åœ¨å¹¿ä¸ºäººçŸ¥çš„IU-Xrayæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºå¸¸ç”¨æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æŠ¥å‘Šè´¨é‡å’Œç›¸å…³æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¡†æ¶çš„é‡è¦æ€§ï¼Œå¼ºè°ƒå‡†ç¡®æŠ¥å‘Šæ£€ç´¢å’Œç‰¹å¾æ•´åˆåœ¨ç”Ÿæˆå…¨é¢çš„åŒ»å­¦æŠ¥å‘Šä¸­çš„é‡è¦æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆèƒ½æ˜¾è‘—å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œæé«˜ä¸´åºŠæ–‡æ¡£è®°å½•çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ•ˆç‡ã€‚</p></li><li><p>æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨æ¨¡æ€æ¡†æ¶ï¼Œä½¿ç”¨MedCLIPä½œä¸ºè§†è§‰æå–å™¨å’Œæ£€ç´¢æœºåˆ¶ï¼Œæ”¹å–„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæµç¨‹ã€‚</p></li><li><p>é€šè¿‡æå–å’Œæ•´åˆæ£€ç´¢æŠ¥å‘Šç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œæé«˜äº†æŠ¥å‘Šçš„è¿è´¯æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p></li><li><p>åœ¨IU-Xrayæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æŠ¥å‘Šè´¨é‡å’Œç›¸å…³æ€§æ–¹é¢ä¼˜äºå¸¸è§æ–¹æ³•ã€‚</p></li><li><p>æ¶ˆèç ”ç©¶éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒå‡†ç¡®æŠ¥å‘Šæ£€ç´¢å’Œç‰¹å¾æ•´åˆçš„é‡è¦æ€§ã€‚</p></li><li><p>è¯¥æ–¹æ³•æœ‰åŠ©äºæå‡åŒ»å­¦æŠ¥å‘Šçš„å…¨é¢æ€§å’Œè´¨é‡ï¼Œè¿›è€Œæå‡ä¸´åºŠå†³ç­–çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Enhancing-LLMs-for-Impression-Generation-in-Radiology-Reports-through-a-Multi-Agent-System"><a href="#Enhancing-LLMs-for-Impression-Generation-in-Radiology-Reports-through-a-Multi-Agent-System" class="headerlink" title="Enhancing LLMs for Impression Generation in Radiology Reports through a   Multi-Agent System"></a>Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System</h2><p><strong>Authors:Fang Zeng, Zhiliang Lyu, Quanzheng Li, Xiang Li</strong></p><p>This study introduces â€œRadCouncil,â€ a multi-agent Large Language Model (LLM) framework designed to enhance the generation of impressions in radiology reports from the finding section. RadCouncil comprises three specialized agents: 1) a â€œRetrievalâ€ Agent that identifies and retrieves similar reports from a vector database, 2) a â€œRadiologistâ€ Agent that generates impressions based on the finding section of the given report plus the exemplar reports retrieved by the Retrieval Agent, and 3) a â€œReviewerâ€ Agent that evaluates the generated impressions and provides feedback. The performance of RadCouncil was evaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and qualitative criteria assessed by GPT-4, using chest X-ray as a case study. Experiment results show improvements in RadCouncil over the single-agent approach across multiple dimensions, including diagnostic accuracy, stylistic concordance, and clarity. This study highlights the potential of utilizing multiple interacting LLM agents, each with a dedicated task, to enhance performance in specialized medical tasks and the development of more robust and adaptable healthcare AI solutions.</p><blockquote><p>æœ¬ç ”ç©¶ä»‹ç»äº†â€RadCouncilâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ ¹æ®å‘ç°éƒ¨åˆ†ç”Ÿæˆæ”¾å°„æŠ¥å‘Šçš„å°è±¡ã€‚RadCouncilåŒ…å«ä¸‰ä¸ªä¸“ä¸šä»£ç†ï¼š1ï¼‰â€œæ£€ç´¢â€ä»£ç†ï¼Œç”¨äºä»å‘é‡æ•°æ®åº“ä¸­è¯†åˆ«å’Œæ£€ç´¢ç±»ä¼¼çš„æŠ¥å‘Šï¼›2ï¼‰â€œæ”¾å°„ç§‘åŒ»ç”Ÿâ€ä»£ç†ï¼Œæ ¹æ®ç»™å®šæŠ¥å‘Šçš„å‘ç°éƒ¨åˆ†ä»¥åŠæ£€ç´¢åˆ°çš„ç¤ºä¾‹æŠ¥å‘Šç”Ÿæˆå°è±¡ï¼›3ï¼‰â€œå®¡æ ¸å‘˜â€ä»£ç†ï¼Œè¯„ä¼°ç”Ÿæˆçš„å°è±¡å¹¶æä¾›åé¦ˆã€‚ä½¿ç”¨èƒ¸éƒ¨Xå°„çº¿ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œé€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆBLEUã€ROUGEã€BERTScoreï¼‰å’ŒGPT-4è¯„ä¼°çš„å®šæ€§æ ‡å‡†å¯¹RadCouncilçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRadCouncilåœ¨å¤šæ–¹é¢çš„è¡¨ç°ä¼˜äºå•ä»£ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬è¯Šæ–­å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨å¤šä¸ªç›¸äº’äº¤äº’çš„LLMä»£ç†çš„æ½œåŠ›ï¼Œæ¯ä¸ªä»£ç†éƒ½æœ‰ç‰¹å®šçš„ä»»åŠ¡ï¼Œä»¥æé«˜åœ¨ä¸“é¡¹åŒ»ç–—ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶å¼€å‘æ›´å¼ºå¤§ã€æ›´é€‚åº”çš„åŒ»ç–—ä¿å¥äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06828v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬ç ”ç©¶ä»‹ç»äº†åä¸ºRadCouncilçš„å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æé«˜æ”¾å°„æŠ¥å‘Šå°è±¡ç”Ÿæˆèƒ½åŠ›ã€‚RadCouncilåŒ…å«ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šæ£€ç´¢æ™ºèƒ½ä½“è´Ÿè´£ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸ä¼¼æŠ¥å‘Šï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“åŸºäºç»™å®šæŠ¥å‘Šçš„å‘ç°éƒ¨åˆ†åŠæ£€ç´¢åˆ°çš„ç¤ºä¾‹æŠ¥å‘Šç”Ÿæˆå°è±¡ï¼Œè¯„å®¡æ™ºèƒ½ä½“åˆ™è¯„ä¼°ç”Ÿæˆçš„å°è±¡å¹¶æä¾›åé¦ˆã€‚æœ¬ç ”ç©¶é€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆBLEUã€ROUGEã€BERTScoreï¼‰å’Œå®šæ€§æ ‡å‡†ï¼ˆä»¥GPT-4è¯„ä¼°èƒ¸éƒ¨Xå…‰ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼‰å¯¹RadCouncilè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºå•æ™ºèƒ½ä½“æ–¹æ³•ï¼ŒRadCouncilåœ¨å¤šä¸ªç»´åº¦ä¸Šæœ‰æ‰€æå‡ï¼ŒåŒ…æ‹¬è¯Šæ–­å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†åˆ©ç”¨å¤šä¸ªäº¤äº’LLMæ™ºèƒ½ä½“çš„æ½œåŠ›ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰ç‰¹å®šçš„ä»»åŠ¡ï¼Œä»¥æé«˜åœ¨ç‰¹å®šåŒ»ç–—ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§ã€é€‚åº”æ€§æ›´å¼ºçš„åŒ»ç–—äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆå¥ å®šäº†åŸºç¡€ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>RadCouncilæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›æ”¾å°„æŠ¥å‘Šå°è±¡ç”Ÿæˆã€‚</p></li><li><p>RadCouncilåŒ…å«ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šæ£€ç´¢æ™ºèƒ½ä½“ã€æ”¾å°„ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“å’Œè¯„å®¡æ™ºèƒ½ä½“ã€‚</p></li><li><p>æ£€ç´¢æ™ºèƒ½ä½“è´Ÿè´£ä»æ•°æ®åº“æ£€ç´¢ç›¸ä¼¼æŠ¥å‘Šã€‚</p></li><li><p>æ”¾å°„ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“åŸºäºå‘ç°éƒ¨åˆ†å’Œæ£€ç´¢åˆ°çš„æŠ¥å‘Šç”Ÿæˆå°è±¡ã€‚</p></li><li><p>è¯„å®¡æ™ºèƒ½ä½“è¯„ä¼°ç”Ÿæˆçš„å°è±¡å¹¶æä¾›åé¦ˆã€‚</p></li><li><p>å®éªŒç»“æœè¡¨æ˜RadCouncilåœ¨è¯Šæ–­å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦ç­‰æ–¹é¢ä¼˜äºå•æ™ºèƒ½ä½“æ–¹æ³•ã€‚</p></li></ol><p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»å› å…¶å¹¿æ³›çš„åº”ç”¨é¢†åŸŸè€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡æœ€è¿‘åœ¨å®ç°é€¼çœŸçš„å”‡éƒ¨è¿åŠ¨æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•æ— æ³•æ•æ‰é€šè¿‡è¯­éŸ³ä¼ è¾¾çš„å¾®å¦™æƒ…ç»ªåŸºè°ƒï¼Œå¹¶äº§ç”Ÿå•è°ƒçš„é¢éƒ¨è¿åŠ¨ã€‚è¿™äº›é™åˆ¶å¯¼è‡´é¢éƒ¨åŠ¨ç”»ç”Ÿç¡¬ä¸”é‡å¤ï¼Œé™ä½äº†ç”¨æˆ·å‚ä¸åº¦å¹¶é˜»ç¢äº†å…¶é€‚ç”¨æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DEEPTalkï¼Œè¿™æ˜¯ä¸€ç§ä»è¯­éŸ³è¾“å…¥ç›´æ¥ç”Ÿæˆå¤šæ ·ä¸”æƒ…æ„Ÿä¸°å¯Œçš„3Dé¢éƒ¨è¡¨æƒ…çš„æ–°æ–¹æ³•ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒDEEï¼ˆåŠ¨æ€æƒ…ç»ªåµŒå…¥ï¼‰ï¼Œå®ƒé‡‡ç”¨æ¦‚ç‡å¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œä¸ºè¯­éŸ³å’Œé¢éƒ¨è¿åŠ¨æ‰“é€ è”åˆæƒ…ç»ªåµŒå…¥ç©ºé—´ã€‚è¿™ä¸ªæ¦‚ç‡æ¡†æ¶æ•æ‰äº†ä»è¯­éŸ³å’Œé¢éƒ¨è¿åŠ¨è§£é‡Šæƒ…ç»ªçš„çš„ä¸ç¡®å®šæ€§ï¼Œä½¿å¾—èƒ½å¤Ÿä»å…¶å¤šç»´ç©ºé—´ä¸­å¾—å‡ºæƒ…ç»ªå‘é‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç”ŸæˆåŠ¨æ€é¢éƒ¨è¿åŠ¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†TH-VQVAEï¼ˆæ—¶åºåˆ†å±‚VQ-VAEï¼‰ä½œä¸ºè¡¨è¾¾æ€§å¼ºä¸”ç¨³å¥çš„è¿åŠ¨å…ˆéªŒï¼Œä»¥å…‹æœVAEå’ŒVQ-VAEçš„å±€é™æ€§ã€‚åˆ©ç”¨è¿™äº›å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬å¼€å‘äº†DEEPTalkï¼Œä¸€ç§éè‡ªå›å½’åœ°é¢„æµ‹ä»£ç æœ¬ç´¢å¼•ä»¥åˆ›å»ºåŠ¨æ€é¢éƒ¨è¿åŠ¨çš„è¯´è¯äººç”Ÿæˆå™¨ï¼Œå¹¶ç»“åˆäº†æ–°é¢–çš„æƒ…ç»ªä¸€è‡´æ€§æŸå¤±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ›å»ºå¤šæ ·ã€æƒ…æ„Ÿä¸°å¯Œçš„è¯´è¯é¢å­”æ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®çš„å”‡åŒæ­¥ã€‚æºä»£ç å°†å¾ˆå¿«å…¬å¼€ã€‚</p><h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, BjÃ¶rn Ommer</strong></p><p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks.</p><p>{0}. åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä¸¤ç§èŒƒå¼åœ¨å„ç§åº”ç”¨ä¸­å—åˆ°äº†å…³æ³¨ï¼šåŸºäºä¸‹ä¸€ä¸ªé›†åˆé¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€ä¸ªå™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼Œä¾‹å¦‚æ‰©æ•£æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹æ¥è¿æ¥å®ƒä»¬ï¼Œå¹¶æ¢ç´¢å®ƒä»¬åœ¨è§†è§‰é¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€çš„è®¾è®¡ç©ºé—´ä¸­å¯¹ä¸¤ç§ç±»å‹çš„æ¨¡å‹è¿›è¡Œäº†é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ã€æ¸©åº¦ã€å¼•å¯¼å¼ºåº¦ç­‰ï¼Œå¹¶ä»¥å¯æ‰©å±•çš„æ–¹å¼è¿›è¡Œäº†è®¨è®ºã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†å‰²ï¼‰é‡æ–°å®šä¹‰ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„[MASK]æ ‡è®°çš„å»æ©ç è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œå„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…è®­ç»ƒä¸€æ¬¡æ¥å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡çš„çµæ´»æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰ä¸Šè¿°æ¢ç´¢éƒ½å¼•é¢†æˆ‘ä»¬æ„å»ºäº†åä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–ä¿æŒä¸ä»¥å‰åŸºäºç¦»æ•£çŠ¶æ€çš„æ–¹æ³•ç›¸æ¯”çš„é¢†å…ˆæ°´å¹³ï¼Œå¦‚ImageNet256ã€MS COCOå’Œè§†é¢‘æ•°æ®é›†FaceForensicsã€‚æ€»ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œæˆ‘ä»¬å¯ä»¥æ¶èµ·æ©ç ç”Ÿæˆå’Œéè‡ªå›å½’æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œä»¥åŠç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚</p><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v2">PDF</a> Technical Report (WIP), Project Page(code, model, dataset): <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p><p><strong>Summary</strong><br>æœ¬ç ”ç©¶æå‡ºä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹è¿æ¥åŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€æ­¥å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œå¹¶åœ¨è§†è§‰é¢†åŸŸæ¢ç´¢å…¶å¯æ‰©å±•æ€§ã€‚é€šè¿‡ç»Ÿä¸€è®¾è®¡ç©ºé—´ä¸­çš„åˆ†æ­¥åˆ†æï¼Œç ”ç©¶å®ç°äº†ä¸¤ç§æ¨¡å‹çš„è·¨ç±»å‹å¯æ‰©å±•æ€§ï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°è°ƒåº¦ã€æ¸©åº¦ã€æŒ‡å¯¼å¼ºåº¦ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰è½¬åŒ–ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»æ©ç è¿‡ç¨‹ï¼Œå®ç°äº†å¤šç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä¸€æ¬¡è®­ç»ƒå³å¯å®ç°çš„çµæ´»æ¡ä»¶é‡‡æ ·æ¥æ¨¡æ‹Ÿè”åˆåˆ†å¸ƒã€‚æœ€ç»ˆæ„å»ºäº†åä¸ºç¦»æ•£æ’å€¼çš„æ–°æ¡†æ¶ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å…ˆè¿›æˆ–å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½æ°´å¹³ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li><p>ç ”ç©¶æå‡ºäº†ç¦»æ•£çŠ¶æ€æ¨¡å‹è¿æ¥ä¸¤ç§ç”Ÿæˆæ¨¡å‹èŒƒå¼ï¼šåŸºäºä¸‹ä¸€æ­¥é¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºå™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ã€‚</p></li><li><p>åœ¨è§†è§‰é¢†åŸŸè¿›è¡Œäº†ç»Ÿä¸€è®¾è®¡ç©ºé—´ä¸­çš„åˆ†æ­¥åˆ†æï¼Œå®ç°äº†ä¸¤ç§æ¨¡å‹çš„è·¨ç±»å‹å¯æ‰©å±•æ€§ã€‚</p></li><li><p>å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡è½¬åŒ–ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»æ©ç è¿‡ç¨‹ï¼Œå®ç°äº†å¤šç§é‡‡æ ·è¿‡ç¨‹ã€‚</p></li><li><p>é€šè¿‡çµæ´»çš„æ¡ä»¶é‡‡æ ·ï¼Œä¸€æ¬¡è®­ç»ƒå³å¯æ¨¡æ‹Ÿè”åˆåˆ†å¸ƒã€‚</p></li><li><p>æ„å»ºçš„æ–°æ¡†æ¶â€œç¦»æ•£æ’å€¼â€åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å…ˆè¿›æˆ–å…·æœ‰ç«äº‰åŠ›ã€‚</p><pre><code>          HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p></li></ul><h2 id="Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation"><a href="#Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation" class="headerlink" title="Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation"></a>Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing Image Segmentation</h2><p><strong>Authors:Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao</strong></p><p>Fine-grained remote sensing image segmentation is essential for accurately identifying detailed objects in remote sensing images. Recently, vision transformer models (VTMs) pre-trained on large-scale datasets have demonstrated strong zero-shot generalization. However, directly applying them to specific tasks may lead to domain shift. We introduce a novel end-to-end learning paradigm combining knowledge guidance with domain refinement to enhance performance. We present two key components: the Feature Alignment Module (FAM) and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based backbone with those from the pretrained VTMâ€™s encoder using channel transformation and spatial interpolation, and transfers knowledge via KL divergence and L2 normalization constraint. FMM further adapts the knowledge to the specific domain to address domain shift. We also introduce a fine-grained grass segmentation dataset and demonstrate, through experiments on two datasets, that our method achieves a significant improvement of 2.57 mIoU on the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the potential of combining knowledge transfer and domain adaptation to overcome domain-related challenges and data limitations. The project page is available at <a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/">https://xavierjiezou.github.io/KTDA/</a>.</p><blockquote><p>ç²¾ç»†ç²’åº¦é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†ç‰©ä½“è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨æ¨¡å‹ï¼ˆVTMï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å…¶åº”ç”¨äºç‰¹å®šä»»åŠ¡å¯èƒ½ä¼šå¯¼è‡´åŸŸåç§»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’ŒåŸŸç²¾ç‚¼çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡å¯¹é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼ï¼Œå°†å¯¹åŸºäºCNNçš„éª¨å¹²ç½‘æå–çš„ç‰¹å¾ä¸é¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚FMMè¿›ä¸€æ­¥å°†çŸ¥è¯†é€‚åº”åˆ°ç‰¹å®šé¢†åŸŸï¼Œä»¥è§£å†³åŸŸåç§»é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç²¾ç»†ç²’åº¦è‰åœ°åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‰åœ°æ•°æ®é›†ä¸Šå®ç°äº†2.57 mIoUçš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šå®ç°äº†3.73 mIoUçš„æ”¹è¿›ã€‚ç»“æœçªå‡ºäº†ç»“åˆçŸ¥è¯†è½¬ç§»å’ŒåŸŸé€‚åº”çš„æ½œåŠ›ï¼Œå¯ä»¥å…‹æœä¸åŸŸç›¸å…³çš„æŒ‘æˆ˜å’Œæ•°æ®é™åˆ¶ã€‚é¡¹ç›®é¡µé¢å¯é€šè¿‡[<a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/]%E8%AE%BF%E9%97%AE%E3%80%82">https://xavierjiezou.github.io/KTDA/]è®¿é—®ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06664v2">PDF</a> 6 pages, 3 figures, 6 tables</p><p><strong>Summary</strong></p><p>é’ˆå¯¹é¥æ„Ÿå›¾åƒä¸­çš„ç²¾ç»†ç›®æ ‡è¯†åˆ«ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’ŒåŸŸç²¾åŒ–çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ã€‚é€šè¿‡ç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰æ¥å®ç°ï¼Œæœ‰æ•ˆæé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘äº†é¢†åŸŸåç§»é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ç²¾ç»†è‰åœ°åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è‰åœ°æ•°æ®é›†ä¸Šçš„mIoUæé«˜äº†2.57ï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šæé«˜äº†3.73ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>ä»‹ç»äº†é¥æ„Ÿå›¾åƒä¸­ç²¾ç»†ç›®æ ‡è¯†åˆ«çš„å…³é”®æŒ‘æˆ˜å’Œé‡è¦æ€§ã€‚</p></li><li><p>æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œç»“åˆäº†çŸ¥è¯†å¼•å¯¼å’ŒåŸŸç²¾åŒ–ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</p></li><li><p>å¼•å…¥äº†ç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ä½œä¸ºè¯¥èŒƒå¼çš„æ ¸å¿ƒç»„ä»¶ã€‚</p></li><li><p>FAMé€šè¿‡å¯¹é½CNNä¸»å¹²å’Œé¢„è®­ç»ƒè§†è§‰è½¬æ¢æ¨¡å‹ï¼ˆVTMï¼‰çš„ç¼–ç å™¨ä¸­çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸæ¥ä¼ é€’çŸ¥è¯†ã€‚</p></li><li><p>FMMè¿›ä¸€æ­¥è°ƒæ•´çŸ¥è¯†ä»¥é€‚åº”ç‰¹å®šé¢†åŸŸï¼Œè§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚</p></li><li><p>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾ç»†è‰åœ°åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒmIoUæé«˜äº†2.57ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation"><a href="#Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation" class="headerlink" title="Semantic Consistency-Based Uncertainty Quantification for Factuality in   Radiology Report Generation"></a>Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation</h2><p><strong>Authors:Chenyu Wang, Weichao Zhou, Shantanu Ghosh, Kayhan Batmanghelich, Wenchao Li</strong></p><p>Radiology report generation (RRG) has shown great potential in assisting radiologists by automating the labor-intensive task of report writing. While recent advancements have improved the quality and coherence of generated reports, ensuring their factual correctness remains a critical challenge. Although generative medical Vision Large Language Models (VLLMs) have been proposed to address this issue, these models are prone to hallucinations and can produce inaccurate diagnostic information. To address these concerns, we introduce a novel Semantic Consistency-Based Uncertainty Quantification framework that provides both report-level and sentence-level uncertainties. Unlike existing approaches, our method does not require modifications to the underlying model or access to its inner state, such as output token logits, thus serving as a plug-and-play module that can be seamlessly integrated with state-of-the-art models. Extensive experiments demonstrate the efficacy of our method in detecting hallucinations and enhancing the factual accuracy of automatically generated radiology reports. By abstaining from high-uncertainty reports, our approach improves factuality scores by $10$%, achieved by rejecting $20$% of reports using the Radialog model on the MIMIC-CXR dataset. Furthermore, sentence-level uncertainty flags the lowest-precision sentence in each report with an $82.9$% success rate.</p><blockquote><p>åŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰åœ¨ååŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè‡ªåŠ¨åŒ–ä¹¦å†™æŠ¥å‘Šè¿™ä¸€åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶è¿‘æœŸçš„å‘å±•å·²ç»æé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„è´¨é‡å’Œè¿è´¯æ€§ï¼Œä½†ç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚å°½ç®¡å·²ç»æå‡ºäº†ç”Ÿæˆå¼åŒ»å­¦å½±åƒè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è¿™äº›æ¨¡å‹å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å‡†ç¡®çš„è¯Šæ–­ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºè¯­ä¹‰ä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œä¿®æ”¹ï¼Œä¹Ÿä¸éœ€è¦è®¿é—®å…¶å†…éƒ¨çŠ¶æ€ï¼Œå¦‚è¾“å‡ºä»¤ç‰Œå¯¹æ•°å‡ ç‡ï¼Œä»è€Œå¯ä»¥ä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œæ— ç¼é›†æˆåˆ°æœ€å…ˆè¿›çš„æ¨¡å‹ä¸­ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ£€æµ‹å¹»è§‰å’Œæé«˜è‡ªåŠ¨ç”Ÿæˆçš„åŒ»å­¦å½±åƒæŠ¥å‘Šçš„å‡†ç¡®æ€§æ–¹é¢éå¸¸æœ‰æ•ˆã€‚é€šè¿‡é¿å…é«˜ä¸ç¡®å®šæ€§çš„æŠ¥å‘Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä½¿ç”¨Radialogæ¨¡å‹å°†æŠ¥å‘Šæ‹’ç»ç‡æé«˜äº†çº¦ç™¾åˆ†ä¹‹äºŒåï¼Œå¹¶åœ¨è¿™ç§æƒ…å†µä¸‹æé«˜äº†ç™¾åˆ†ä¹‹åçš„äº‹å®å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§è¿˜èƒ½æ ‡è®°æ¯ä¸ªæŠ¥å‘Šä¸­æœ€ä¸å‡†ç¡®çš„å¥å­ï¼ŒæˆåŠŸç‡ä¸ºç™¾åˆ†ä¹‹å…«åäºŒç‚¹ä¹ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04606v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è¯­ä¹‰ä¸€è‡´æ€§ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œç”¨äºæé«˜æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹ç°æœ‰æ¨¡å‹çš„å‰æä¸‹ï¼Œæä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ï¼Œæœ‰æ•ˆæ£€æµ‹å¹¶å¢å¼ºè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜æŠ¥å‘Šçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿæ ‡è®°å‡ºæŠ¥å‘Šä¸­çš„ä½ç²¾åº¦å¥å­ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰åœ¨ååŠ©æ”¾å°„ç§‘åŒ»ç”Ÿæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</p></li><li><p>ç”Ÿæˆå¼åŒ»ç–—è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰è™½ç„¶è¢«æå‡ºç”¨äºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å®¹æ˜“å‡ºç°ä¿¡æ¯é”™è¯¯ã€‚</p></li><li><p>ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è¯­ä¹‰ä¸€è‡´æ€§ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ã€‚</p></li><li><p>è¯¥æ–¹æ³•æ— éœ€ä¿®æ”¹ç°æœ‰æ¨¡å‹æˆ–è®¿é—®å…¶å†…éƒ¨çŠ¶æ€ï¼Œå¯æ— ç¼é›†æˆåˆ°æœ€æ–°æ¨¡å‹ä¸­ã€‚</p></li><li><p>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ£€æµ‹å¹¶å¢å¼ºè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚</p></li><li><p>é€šè¿‡æ‹’ç»é«˜ä¸ç¡®å®šæ€§çš„æŠ¥å‘Šï¼Œä½¿ç”¨Radialogæ¨¡å‹åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„äº‹å®æ€§å¾—åˆ†æé«˜äº†10%ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images"><a href="#Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images" class="headerlink" title="Mask of truth: model sensitivity to unexpected regions of medical images"></a>Mask of truth: model sensitivity to unexpected regions of medical images</h2><p><strong>Authors:ThÃ©o Sourget, Michelle Hestbek-MÃ¸ller, Amelia JimÃ©nez-SÃ¡nchez, Jack Junchi Xu, Veronika Cheplygina</strong></p><p>The development of larger models for medical image analysis has led to increased performance. However, it also affected our ability to explain and validate model decisions. Models can use non-relevant parts of images, also called spurious correlations or shortcuts, to obtain high performance on benchmark datasets but fail in real-world scenarios. In this work, we challenge the capacity of convolutional neural networks (CNN) to classify chest X-rays and eye fundus images while masking out clinically relevant parts of the image. We show that all models trained on the PadChest dataset, irrespective of the masking strategy, are able to obtain an Area Under the Curve (AUC) above random. Moreover, the models trained on full images obtain good performance on images without the region of interest (ROI), even superior to the one obtained on images only containing the ROI. We also reveal a possible spurious correlation in the Chaksu dataset while the performances are more aligned with the expectation of an unbiased model. We go beyond the performance analysis with the usage of the explainability method SHAP and the analysis of embeddings. We asked a radiology resident to interpret chest X-rays under different masking to complement our findings with clinical knowledge. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> and <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a></p><blockquote><p>åŒ»å­¦å›¾åƒåˆ†æå¤§å‹æ¨¡å‹çš„å‘å±•æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå½±å“äº†æˆ‘ä»¬è§£é‡Šå’ŒéªŒè¯æ¨¡å‹å†³ç­–çš„èƒ½åŠ›ã€‚æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨å›¾åƒçš„éå…³é”®éƒ¨åˆ†ï¼Œä¹Ÿç§°ä¸ºå¶ç„¶å…³è”æˆ–æ·å¾„ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè·å¾—é«˜æ€§èƒ½ï¼Œä½†åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å´ä¼šå¤±è´¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹èƒ¸éƒ¨Xå°„çº¿å’Œçœ¼åº•å›¾åƒçš„åˆ†ç±»èƒ½åŠ›ï¼ŒåŒæ—¶æ©ç›–äº†å›¾åƒä¸­çš„ä¸´åºŠå…³é”®éƒ¨åˆ†ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œåœ¨PadChestæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ï¼Œæ— è®ºé‡‡ç”¨ä½•ç§æ©ç›–ç­–ç•¥ï¼Œéƒ½èƒ½è·å¾—è¶…è¿‡éšæœºçš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å®Œæ•´å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ²¡æœ‰æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å›¾åƒä¸Šä¹Ÿèƒ½è·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œç”šè‡³ä¼˜äºä»…åœ¨åŒ…å«ROIçš„å›¾åƒä¸Šè·å¾—çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†Chaksuæ•°æ®é›†ä¸­å¯èƒ½çš„å¶ç„¶å…³è”ï¼ŒåŒæ—¶æ€§èƒ½æ›´ç¬¦åˆæ— åè§æ¨¡å‹çš„é¢„æœŸã€‚é™¤äº†æ€§èƒ½åˆ†æï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†SHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†æã€‚æˆ‘ä»¬è¯·ä¸€ä½æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨ä¸åŒæ©ç›–æƒ…å†µä¸‹è§£é‡Šèƒ¸éƒ¨Xå°„çº¿ï¼Œä»¥è¡¥å……æˆ‘ä»¬çš„ä¸´åºŠçŸ¥è¯†å‘ç°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a>ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04030v2">PDF</a></p><p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹å‘å±•æé«˜æ€§èƒ½ï¼Œä½†éš¾ä»¥è§£é‡Šå’ŒéªŒè¯å†³ç­–ã€‚æ¨¡å‹å¯èƒ½åˆ©ç”¨å›¾åƒçš„éå…³é”®éƒ¨åˆ†è·å¾—é«˜åŸºå‡†æ•°æ®é›†æ€§èƒ½ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­å¤±è´¥ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹èƒ¸éƒ¨Xå…‰ç‰‡å’Œçœ¼åº•å›¾åƒçš„åˆ†ç±»èƒ½åŠ›ï¼ŒåŒæ—¶æ©ç›–äº†ä¸´åºŠä¸Šé‡è¦çš„å›¾åƒéƒ¨åˆ†ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œæ— è®ºæ©ç›–ç­–ç•¥å¦‚ä½•ï¼Œåœ¨PadChestæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹éƒ½èƒ½è·å¾—è¶…è¿‡éšæœºçš„AUCå€¼ã€‚æ­¤å¤–ï¼Œåœ¨ä¸å«æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ€§èƒ½ç”šè‡³ä¼˜äºä»…åŒ…å«ROIçš„å›¾åƒã€‚æœ¬ç ”ç©¶è¿˜æ­ç¤ºäº†Chaksuæ•°æ®é›†ä¸­çš„æ½œåœ¨å¶ç„¶å…³è”ï¼Œä¸”æ¨¡å‹æ€§èƒ½æ›´ç¬¦åˆæ— åè§æ¨¡å‹çš„é¢„æœŸã€‚é™¤äº†æ€§èƒ½åˆ†æï¼Œæœ¬ç ”ç©¶è¿˜ä½¿ç”¨äº†SHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†æã€‚ä¸€åæ”¾å°„ç§‘ä½é™¢åŒ»å¸ˆè¢«é‚€è¯·å¯¹èƒ¸éƒ¨Xå…‰ç‰‡è¿›è¡Œä¸åŒæ©ç›–è§£è¯»ï¼Œä»¥ç»“åˆä¸´åºŠçŸ¥è¯†è¡¥å……ç ”ç©¶ç»“è®ºã€‚</p><p><strong>Key Takeaways</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œè§£é‡Šå’ŒéªŒè¯æ¨¡å‹å†³ç­–çš„å›°éš¾å¢åŠ ã€‚</span><br><span class="line">2. æ¨¡å‹å¯èƒ½åˆ©ç”¨éå…³é”®å›¾åƒéƒ¨åˆ†ï¼ˆå³å¶ç„¶å…³è”æˆ–æ·å¾„ï¼‰è·å¾—é«˜åŸºå‡†æ•°æ®é›†æ€§èƒ½ã€‚</span><br><span class="line">3. åœ¨æ©ç›–ä¸´åºŠç›¸å…³å›¾åƒéƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œå·ç§¯ç¥ç»ç½‘ç»œå¯¹èƒ¸éƒ¨Xå…‰ç‰‡å’Œçœ¼åº•å›¾åƒçš„åˆ†ç±»èƒ½åŠ›å—åˆ°æŒ‘æˆ˜ã€‚</span><br><span class="line">4. åœ¨PadChestæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ— è®ºæ©ç›–ç­–ç•¥å¦‚ä½•ï¼Œéƒ½èƒ½è·å¾—è¶…è¿‡éšæœºçš„AUCå€¼ã€‚</span><br><span class="line">5. æ¨¡å‹åœ¨ä¸å«ROIçš„å›¾åƒä¸Šçš„æ€§èƒ½ä¼˜äºä»…åŒ…å«ROIçš„å›¾åƒã€‚</span><br><span class="line">6. æ­ç¤ºäº†Chaksuæ•°æ®é›†ä¸­çš„æ½œåœ¨å¶ç„¶å…³è”ï¼Œæ¨¡å‹æ€§èƒ½æ›´ç¬¦åˆé¢„æœŸçš„æ— åè§æ¨¡å‹è¡¨ç°ã€‚</span><br><span class="line">7. é™¤äº†æ€§èƒ½åˆ†æï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†SHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†æï¼Œå¹¶é‚€è¯·æ”¾å°„ç§‘ä½é™¢åŒ»å¸ˆå¯¹æ©ç›–çš„èƒ¸éƒ¨Xå…‰ç‰‡è¿›è¡Œè§£è¯»ï¼Œç»“åˆä¸´åºŠçŸ¥è¯†æ¥è¡¥å……ç ”ç©¶ç»“è®ºã€‚</span><br></pre></td></tr></table></figure><pre><code>            HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching"><a href="#INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching" class="headerlink" title="INRetouch: Context Aware Implicit Neural Representation for Photography   Retouching"></a>INRetouch: Context Aware Implicit Neural Representation for Photography Retouching</h2><p><strong>Authors:Omar Elezabi, Marcos V. Conde, Zongwei Wu, Radu Timofte</strong></p><p>Professional photo editing remains challenging, requiring extensive knowledge of imaging pipelines and significant expertise. With the ubiquity of smartphone photography, there is an increasing demand for accessible yet sophisticated image editing solutions. While recent deep learning approaches, particularly style transfer methods, have attempted to automate this process, they often struggle with output fidelity, editing control, and complex retouching capabilities. We propose a novel retouch transfer approach that learns from professional edits through before-after image pairs, enabling precise replication of complex editing operations. To facilitate this research direction, we introduce a comprehensive Photo Retouching Dataset comprising 100,000 high-quality images edited using over 170 professional Adobe Lightroom presets. We develop a context-aware Implicit Neural Representation that learns to apply edits adaptively based on image content and context, requiring no pretraining and capable of learning from a single example. Our method extracts implicit transformations from reference edits and adaptively applies them to new images. Through extensive evaluation, we demonstrate that our approach not only surpasses existing methods in photo retouching but also enhances performance in related image reconstruction tasks like Gamut Mapping and Raw Reconstruction. By bridging the gap between professional editing capabilities and automated solutions, our work presents a significant step toward making sophisticated photo editing more accessible while maintaining high-fidelity results. Check the Project Page at <a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch">https://omaralezaby.github.io/inretouch</a> for more Results and information about Code and Dataset availability.</p><blockquote><p>ä¸“ä¸šç…§ç‰‡ç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ·±å…¥äº†è§£æˆåƒç®¡é“å’Œä¸“ä¸šçŸ¥è¯†ã€‚éšç€æ™ºèƒ½æ‰‹æœºæ‘„å½±çš„æ™®åŠï¼Œå¯¹ä¾¿æ·è€Œé«˜çº§çš„å›¾åƒç¼–è¾‘è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚è™½ç„¶æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é£æ ¼è¿ç§»æ–¹æ³•ï¼Œå·²ç»å°è¯•è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼Œä½†å®ƒä»¬åœ¨è¾“å‡ºä¿çœŸåº¦ã€ç¼–è¾‘æ§åˆ¶å’Œå¤æ‚ä¿®é¥°åŠŸèƒ½æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¿®é¥°è¿ç§»æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸“ä¸šç¼–è¾‘å‰åçš„å›¾åƒå¯¹æ¥å­¦ä¹ ï¼Œèƒ½å¤Ÿç²¾ç¡®å¤åˆ¶å¤æ‚çš„ç¼–è¾‘æ“ä½œã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸ªç ”ç©¶æ–¹å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„ç…§ç‰‡ä¿®é¥°æ•°æ®é›†ï¼ŒåŒ…å«ä½¿ç”¨è¶…è¿‡170ä¸ªä¸“ä¸šAdobe Lightroomé¢„è®¾ç¼–è¾‘çš„10ä¸‡å¼ é«˜è´¨é‡å›¾åƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒå¯ä»¥æ ¹æ®å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡å­¦ä¹ è‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘æ“ä½œï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒï¼Œå¹¶ä¸”èƒ½å¤Ÿä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»å‚è€ƒç¼–è¾‘ä¸­æå–éšå¼è½¬æ¢å¹¶è‡ªé€‚åº”åœ°åº”ç”¨äºæ–°å›¾åƒã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨ç…§ç‰‡ä¿®é¥°æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”åœ¨ç›¸å…³å›¾åƒé‡å»ºä»»åŠ¡ï¼ˆå¦‚è‰²åŸŸæ˜ å°„å’ŒåŸå§‹é‡å»ºï¼‰æ–¹é¢çš„æ€§èƒ½ä¹Ÿå¾—åˆ°äº†æé«˜ã€‚é€šè¿‡ç¼©å°ä¸“ä¸šç¼–è¾‘èƒ½åŠ›å’Œè‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯åœ¨ä½¿é«˜çº§ç…§ç‰‡ç¼–è¾‘æ›´åŠ ä¾¿æ·çš„åŒæ—¶ä¿æŒé«˜ä¿çœŸç»“æœæ–¹é¢è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚æœ‰å…³æ›´å¤šç»“æœå’Œå…³äºä»£ç å’Œæ•°æ®é›†å¯ç”¨æ€§çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch%E6%9F%A5%E7%9C%8B%E9%A1%B9%E7%9B%AE%E9%A1%B5%E9%9D%A2%E3%80%82">https://omaralezaby.github.io/inretouchæŸ¥çœ‹é¡¹ç›®é¡µé¢ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03848v2">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„ç…§ç‰‡ä¿®é¥°è½¬æ¢æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ å’Œå¤åˆ¶ä¸“ä¸šç¼–è¾‘çš„æ“ä½œï¼Œå®ç°å¯¹å¤æ‚ç¼–è¾‘æ“ä½œçš„ç²¾ç¡®å¤åˆ¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ä¸ªåŒ…å«10ä¸‡å¼ é«˜è´¨é‡å›¾ç‰‡çš„ç»¼åˆç…§ç‰‡ä¿®é¥°æ•°æ®é›†ï¼Œä½¿ç”¨è¶…è¿‡170ç§Adobe Lightroomé¢„è®¾è¿›è¡Œç¼–è¾‘ã€‚ä»–ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡çš„ç¯å¢ƒæ„ŸçŸ¥éšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘æ“ä½œï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒï¼Œå¹¶èƒ½ä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ã€‚è¯¥æ–¹æ³•ä»å‚è€ƒç¼–è¾‘ä¸­æå–éšå¼è½¬æ¢å¹¶è‡ªé€‚åº”åœ°åº”ç”¨äºæ–°å›¾åƒï¼Œåœ¨ç…§ç‰‡ä¿®é¥°å’Œç›¸å…³å›¾åƒé‡å»ºä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>æå‡ºäº†ä¸€ç§æ–°çš„ç…§ç‰‡ä¿®é¥°è½¬æ¢æ–¹æ³•ï¼Œå¯ç²¾ç¡®å¤åˆ¶ä¸“ä¸šç¼–è¾‘çš„æ“ä½œã€‚</p></li><li><p>å¼•å…¥äº†åŒ…å«10ä¸‡å¼ é«˜è´¨é‡å›¾ç‰‡çš„ç»¼åˆç…§ç‰‡ä¿®é¥°æ•°æ®é›†ï¼Œä½¿ç”¨Adobe Lightroomé¢„è®¾è¿›è¡Œç¼–è¾‘ã€‚</p></li><li><p>å¼€å‘äº†ä¸€ç§ç¯å¢ƒæ„ŸçŸ¥éšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½è‡ªé€‚åº”åœ°åº”ç”¨ç¼–è¾‘æ“ä½œï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒã€‚</p></li><li><p>æ–¹æ³•èƒ½ä»å•ä¸ªç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œæå–å‚è€ƒç¼–è¾‘çš„éšå¼è½¬æ¢å¹¶åº”ç”¨äºæ–°å›¾åƒã€‚</p></li><li><p>åœ¨ç…§ç‰‡ä¿®é¥°å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p></li><li><p>æ¶èµ·äº†ä¸“ä¸šç¼–è¾‘èƒ½åŠ›å’Œè‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä¹‹é—´çš„æ¡¥æ¢ï¼Œä½¿é«˜çº§ç…§ç‰‡ç¼–è¾‘æ›´åŠ æ˜“äºè®¿é—®åŒæ—¶ä¿æŒé«˜ä¿çœŸç»“æœã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis"><a href="#INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis" class="headerlink" title="INSIGHT: Explainable Weakly-Supervised Medical Image Analysis"></a>INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</h2><p><strong>Authors:Wenbo Zhang, Junyu Chen, Christopher Kanan</strong></p><p>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: <a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/">https://zhangdylan83.github.io/ewsmia/</a></p><blockquote><p>ç”±äºä½“ç§¯æ‰«æå’Œå…¨å¹»ç¯ç‰‡ç—…ç†å›¾åƒï¼ˆWSIï¼‰çš„å°ºå¯¸è¾ƒå¤§ï¼Œé€šå¸¸é€šè¿‡å¯¹å±€éƒ¨åŒºåŸŸæå–åµŒå…¥ç„¶åè¿›è¡Œèšåˆé¢„æµ‹æ¥å¤„ç†ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•éœ€è¦äº‹åå¯è§†åŒ–æŠ€æœ¯ï¼ˆä¾‹å¦‚Grad-CAMï¼‰ï¼Œå¹¶ä¸”å¾€å¾€æ— æ³•å®šä½å°è€Œä¸´åºŠä¸Šé‡è¦çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†INSIGHTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£èšåˆå™¨ï¼Œå®ƒå°†çƒ­å›¾ç”Ÿæˆä½œä¸ºå½’çº³åç½®ã€‚ä»é¢„è®­ç»ƒçš„ç‰¹å¾å›¾å¼€å§‹ï¼ŒINSIGHTä½¿ç”¨å…·æœ‰å°å·ç§¯æ ¸çš„æ£€æµ‹æ¨¡å—æ¥æ•è·ç»†èŠ‚ï¼Œå¹¶ä½¿ç”¨å…·æœ‰æ›´å¤§æ„Ÿå—é‡çš„ä¸Šä¸‹æ–‡æ¨¡å—æ¥æŠ‘åˆ¶å±€éƒ¨è¯¯æŠ¥ã€‚ç”Ÿæˆçš„å†…éƒ¨çƒ­å›¾çªå‡ºäº†è¯Šæ–­ç›¸å…³çš„åŒºåŸŸã€‚åœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒINSIGHTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœå’Œé«˜æ€§èƒ½çš„å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ•ˆæœã€‚é¡¹ç›®ç½‘ç«™å’Œä»£ç å¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/]">https://zhangdylan83.github.io/ewsmia/]</a> æŸ¥çœ‹ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02012v2">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†å¤„ç†å¤§å‹åŒ»å­¦å›¾åƒçš„æ–°æ–¹æ³•â€”â€”INSIGHTã€‚å®ƒé€šè¿‡å±€éƒ¨åŒºåŸŸåµŒå…¥æå–å’Œèšåˆå™¨é¢„æµ‹ï¼Œè§£å†³ä½“ç§¯æ‰«æå’Œå…¨å¹»ç¯ç‰‡ç—…ç†å›¾åƒï¼ˆWSIsï¼‰çš„å¤„ç†é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éœ€è¦äº‹åå¯è§†åŒ–æŠ€æœ¯ï¼Œå¹¶ä¸”éš¾ä»¥å®šä½å°ä½†ä¸´åºŠé‡è¦çš„ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼ŒINSIGHTé‡‡ç”¨å¼±ç›‘ç£èšåˆå™¨ï¼Œç»“åˆçƒ­å›¾ç”Ÿæˆä½œä¸ºå½’çº³åç½®ã€‚é€šè¿‡æ£€æµ‹æ¨¡å—æ•æ‰ç²¾ç»†ç»†èŠ‚å’Œä¸Šä¸‹æ–‡æ¨¡å—æŠ‘åˆ¶å±€éƒ¨è¯¯æŠ¥ï¼Œæé«˜è¯Šæ–­ç›¸å…³åŒºåŸŸçš„å¯è§æ€§ã€‚åœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒINSIGHTå®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœå’Œé«˜å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>INSIGHTæ˜¯ä¸€ç§å¤„ç†å¤§å‹åŒ»å­¦å›¾åƒï¼ˆå¦‚ä½“ç§¯æ‰«æå’ŒWSIsï¼‰çš„æ–°æ–¹æ³•ã€‚</p></li><li><p>ç°æœ‰æ–¹æ³•éœ€è¦å¤æ‚çš„äº‹åå¯è§†åŒ–æŠ€æœ¯ï¼Œéš¾ä»¥å®šä½å…³é”®ç»†èŠ‚ã€‚</p></li><li><p>INSIGHTé‡‡ç”¨å¼±ç›‘ç£èšåˆå™¨ï¼Œç»“åˆçƒ­å›¾ç”Ÿæˆä»¥æé«˜è¯Šæ–­ç›¸å…³åŒºåŸŸçš„å¯è§æ€§ã€‚</p></li><li><p>INSIGHTåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šæ£€æµ‹æ¨¡å—ç”¨äºæ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œä¸Šä¸‹æ–‡æ¨¡å—ç”¨äºæŠ‘åˆ¶å±€éƒ¨è¯¯æŠ¥ã€‚</p></li><li><p>INSIGHTåœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»æ€§èƒ½ã€‚</p></li><li><p>INSIGHTè¿˜è¡¨ç°å‡ºé«˜å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Evaluating-Automated-Radiology-Report-Quality-through-Fine-Grained-Phrasal-Grounding-of-Clinical-Findings"><a href="#Evaluating-Automated-Radiology-Report-Quality-through-Fine-Grained-Phrasal-Grounding-of-Clinical-Findings" class="headerlink" title="Evaluating Automated Radiology Report Quality through Fine-Grained   Phrasal Grounding of Clinical Findings"></a>Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings</h2><p><strong>Authors:Razi Mahmood, Pingkun Yan, Diego Machado Reyes, Ge Wang, Mannudeep K. Kalra, Parisa Kaviani, Joy T. Wu, Tanveer Syeda-Mahmood</strong></p><p>Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, and severity of a large number of clinical findings. We then performed phrasal grounding to localize their associated anatomical regions on chest radiograph images. The textual and visual measures are then combined to rate the quality of the generated reports. We present results that compare this evaluation metric with other textual metrics on a gold standard dataset derived from the MIMIC collection and show its robustness and sensitivity to factual errors.</p><blockquote><p>æœ€è¿‘ï¼Œå·²ç»å¼€å‘äº†ä¸€äº›è¯„ä¼°æŒ‡æ ‡ï¼Œä»…ä½¿ç”¨æ–‡æœ¬ä¿¡æ¯ï¼Œé€šè¿‡è¯æ±‡ã€è¯­ä¹‰æˆ–ä¸´åºŠå‘½åå®ä½“è¯†åˆ«æ–¹æ³•ï¼Œè‡ªåŠ¨è¯„ä¼°èƒ¸éƒ¨æ”¾å°„å­¦æŠ¥å‘Šä¸­ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ¥å‘Šçš„è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„æŠ¥å‘Šè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé¦–å…ˆæå–ç²¾ç»†çš„æ£€æŸ¥ç»“æœæ¨¡å¼ï¼Œæ•æ‰å¤§é‡ä¸´åºŠæ£€æŸ¥ç»“æœçš„ä½ç½®ã€ä¾§åˆ«å’Œä¸¥é‡ç¨‹åº¦ã€‚ç„¶åæˆ‘ä»¬å¯¹çŸ­è¯­è¿›è¡Œå®šä½ï¼Œä»¥ç¡®å®šå…¶åœ¨èƒ¸éƒ¨æ”¾å°„å›¾åƒä¸Šçš„ç›¸å…³è§£å‰–åŒºåŸŸã€‚ç„¶åå°†æ–‡æœ¬å’Œè§†è§‰åº¦é‡ç»“åˆèµ·æ¥ï¼Œå¯¹ç”Ÿæˆçš„æŠ¥å‘Šè¿›è¡Œè´¨é‡è¯„åˆ†ã€‚æˆ‘ä»¬åœ¨é»„é‡‘æ ‡å‡†æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†è¯¥è¯„ä¼°æŒ‡æ ‡ä¸å…¶ä»–æ–‡æœ¬æŒ‡æ ‡çš„è¡¨ç°ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªMIMICæ•°æ®é›†ï¼Œå±•ç¤ºäº†å®ƒå¯¹äº‹å®é”™è¯¯çš„ç¨³å¥æ€§å’Œæ•æ„Ÿæ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01031v2">PDF</a></p><p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ–‡æœ¬ä¿¡æ¯è¯„ä¼°ç”Ÿæˆå¼AIæŠ¥å‘Šè´¨é‡çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ç²¾ç»†çš„ç—…å˜æ¨¡å¼ï¼Œæ•æ‰å¤§é‡ä¸´åºŠå‘ç°çš„éƒ¨ä½ã€å•ä¾§æ€§å’Œä¸¥é‡ç¨‹åº¦ï¼Œå¹¶åœ¨æ”¾å°„å½±åƒå›¾åƒä¸Šå®šä½ç›¸å…³çš„è§£å‰–åŒºåŸŸã€‚ç»“åˆæ–‡æœ¬å’Œè§†è§‰æµ‹é‡æ¥è¯„ä¼°æŠ¥å‘Šè´¨é‡ï¼Œå¹¶åœ¨MIMICæ•°æ®é›†çš„é‡‘æ ‡å‡†å­é›†ä¸ŠéªŒè¯äº†å…¶ä¸å…¶ä»–æ–‡æœ¬è¯„ä¼°æŒ‡æ ‡çš„ç¨³å¥æ€§å’Œå¯¹äº‹å®é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>è¯¥è®ºæ–‡è‡´åŠ›äºåˆ©ç”¨ç”Ÿæˆå¼AIå¯¹èƒ¸éƒ¨æ”¾å°„æŠ¥å‘Šçš„è‡ªåŠ¨è´¨é‡è¯„ä¼°ã€‚</p></li><li><p>é€šè¿‡æå–ç²¾ç»†ç—…å˜æ¨¡å¼æ•æ‰ä¸´åºŠå‘ç°çš„ä¿¡æ¯ã€‚</p></li><li><p>æ–¹æ³•ç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰æµ‹é‡æ¥å…¨é¢è¯„ä¼°æŠ¥å‘Šè´¨é‡ã€‚</p></li><li><p>ç ”ç©¶ä½¿ç”¨äº†MIMICæ•°æ®é›†çš„é‡‘æ ‡å‡†å­é›†è¿›è¡Œå®éªŒéªŒè¯ã€‚</p></li><li><p>è¯¥æ–¹æ³•ä¸ä»…è€ƒè™‘äº†æŠ¥å‘Šçš„æ–‡å­—å†…å®¹ï¼Œè¿˜ç»“åˆäº†å½±åƒå›¾åƒè¿›è¡Œphrasal groundingï¼Œä»¥å®šä½ç›¸å…³çš„è§£å‰–åŒºåŸŸã€‚</p></li><li><p>ä¸å…¶ä»–æ–‡æœ¬è¯„ä¼°æŒ‡æ ‡ç›¸æ¯”ï¼Œè¯¥è¯„ä»·æ¨¡å‹å±•ç°å‡ºäº†ç¨³å¥æ€§å’Œå¯¹äº‹å®é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy"><a href="#Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy" class="headerlink" title="Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy"></a>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy</h2><p><strong>Authors:Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</strong></p><p>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large GIE dataset, called EndoExtend24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of GIE medical image diagnosis. Specifically, the EVA-02 model, which is based on the ViT architecture and trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762 and a balanced accuracy of 37.1% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched EndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.</p><blockquote><p>è§†é¢‘èƒ¶å›Šå†…é•œæŠ€æœ¯ä¸ºèƒƒè‚ é“å†…çª¥é•œï¼ˆGIEï¼‰è¯Šæ–­æä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„æ–¹æ³•æ¥æ•æ‰èƒƒè‚ é“çš„è¯¦ç»†å›¾åƒï¼Œä»è€Œå®ç°äº†æ—©æœŸç–¾ç—…çš„æ£€æµ‹ã€‚ç„¶è€Œï¼Œå…¶åœ¨æˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æµ·é‡å›¾åƒé™åˆ¶äº†å…¶æ½œåŠ›ï¼Œæˆåƒè¿‡ç¨‹å¯èƒ½éœ€è¦6-8å°æ—¶ï¼Œå¹¶å¯èƒ½äº§ç”Ÿé«˜è¾¾100ä¸‡å¼ å›¾åƒï¼Œå› æ­¤éœ€è¦è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æã€‚æ­¤å¤–ï¼Œè¿™äº›å›¾åƒçš„å·®å¼‚æ€§ï¼ŒåŠ ä¸Šä¸“å®¶æ ‡æ³¨çš„éœ€æ±‚ä»¥åŠå¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œåˆ¶çº¦äº†å½“å‰åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤§å‹GIEæ•°æ®é›†EndoExtend24ï¼Œå®ƒæ˜¯é€šè¿‡åˆå¹¶åä¸ªç°æœ‰çš„å…¬å…±å’Œç§æœ‰æ•°æ®é›†åˆ›å»ºçš„ï¼Œç¡®ä¿äº†è·¨åˆ†å‰²çš„æ‚£è€…æ•°æ®å®Œæ•´æ€§ã€‚EndoExtend24åŒ…å«è¶…è¿‡22ä¸‡å¼ æ ‡è®°å›¾åƒä»¥åŠåŠ¨æ€ç±»æ˜ å°„ï¼Œå…è®¸åœ¨å…·æœ‰ä¸åŒæ ‡è®°ç²’åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œç»Ÿä¸€è®­ç»ƒï¼Œæ”¯æŒå¤šè¾¾123ç§ä¸åŒçš„ç—…ç†å‘ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨åŸºäºè‡ªç›‘ç£çš„é€šç”¨å›¾åƒæ•°æ®åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥é€‚åº”GIEåŒ»å­¦å›¾åƒè¯Šæ–­çš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒEVA-02æ¨¡å‹åŸºäºViTæ¶æ„æ„å»ºï¼Œåœ¨ImageNet-22kæ•°æ®é›†ä¸Šè¿›è¡Œé®ç½©å›¾åƒå»ºæ¨¡è®­ç»ƒï¼ˆä½¿ç”¨EVA-CLIPä½œä¸ºé®ç½©æ•™å¸ˆæ¨¡å‹ï¼‰ï¼Œç„¶ååœ¨EndoExtend24æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥å®ç°åŸŸé€‚åº”ï¼Œå¹¶æœ€ç»ˆåœ¨Capsule Endoscopy 2024æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œåœ¨Capsule Endoscopy 2024æŒ‘æˆ˜èµ›ä¸­è·å¾—äº†ç¬¬ä¸‰åã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†å®è§‚AUCä¸º0.762å’Œå¹³è¡¡ç²¾åº¦ä¸º37.1%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æˆ‘ä»¬åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’Œä¸°å¯Œçš„EndoExtend24æ•°æ®é›†åœ¨æ¨è¿›èƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21302v4">PDF</a></p><p><strong>Summary</strong><br>è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…é•œï¼ˆGIEï¼‰è¯Šæ–­æä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„æˆåƒæ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰èƒƒè‚ é“çš„è¯¦ç»†å›¾åƒï¼Œå®ç°æ—©æœŸç–¾ç—…æ£€æµ‹ã€‚ç„¶è€Œï¼Œç”±äºæˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œä»¥åŠå›¾åƒé—´çš„å·®å¼‚æ€§å’Œç¼ºä¹å¤§å‹é«˜è´¨é‡æ ‡ç­¾æ•°æ®é›†ï¼Œé™åˆ¶äº†å…¶åˆ†ææ¨¡å‹çš„æ•ˆèƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†EndoExtend24å¤§å‹GIEæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡22.6ä¸‡å¼ æ ‡è®°å›¾åƒï¼Œå¹¶æ”¯æŒå¤šè¾¾123ç§ä¸åŒç—…ç†å‘ç°çš„ç»Ÿä¸€è®­ç»ƒã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºåˆ©ç”¨åŸºäºé€šç”¨å›¾åƒæ•°æ®çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œä»¥é€‚åº”GIEåŒ»å­¦å›¾åƒè¯Šæ–­ä»»åŠ¡ã€‚æœ€ç»ˆï¼Œæœ¬ç ”ç©¶å¼€å‘çš„æ¨¡å‹åœ¨Capsule Endoscopy 2024 Challengeä¸­å–å¾—äº†ç¬¬ä¸‰åçš„å¥½æˆç»©ï¼ŒéªŒè¯äº†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’ŒEndoExtend24æ•°æ®é›†çš„ä¼˜è¶Šæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…é•œè¯Šæ–­æä¾›äº†éä¾µå…¥æ€§çš„è¯¦ç»†æˆåƒæ–¹æ³•ï¼Œæœ‰åŠ©äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚</p></li><li><p>å·¨å¤§çš„å›¾åƒæ•°é‡å’Œå·®å¼‚æ€§å¯¹åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚</p></li><li><p>EndoExtend24æ•°æ®é›†é€šè¿‡åˆå¹¶å¤šä¸ªå…¬å…±å’Œç§æœ‰æ•°æ®é›†ï¼Œæä¾›äº†å¤§é‡æ ‡è®°å›¾åƒä»¥æ”¯æŒå¹¿æ³›çš„ç—…ç†å‘ç°ã€‚</p></li><li><p>åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæœ‰åŠ©äºå°†é€šç”¨å›¾åƒæ•°æ®é¢„è®­ç»ƒçš„æ¨¡å‹é€‚åº”åˆ°GIEåŒ»å­¦å›¾åƒè¯Šæ–­ä»»åŠ¡ã€‚</p></li><li><p>EVA-02æ¨¡å‹åŸºäºViTæ¶æ„ï¼Œåˆ©ç”¨ImageNet-22kè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡åŸŸè‡ªé€‚åº”åœ¨EndoExtend24æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒã€‚</p></li><li><p>EVA-02æ¨¡å‹åœ¨Capsule Endoscopy 2024 Challengeæµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè·å¾—ç¬¬ä¸‰åã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p><h2 id="The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI"><a href="#The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI" class="headerlink" title="The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI"></a>The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain Metastasis Segmentation on Pre-treatment MRI</h2><p><strong>Authors:Ahmed W. Moawad, Anastasia Janas, Ujjwal Baid, Divya Ramakrishnan, Rachit Saluja, Nader Ashraf, Nazanin Maleki, Leon Jekel, Nikolay Yordanov, Pascal Fehringer, Athanasios Gkampenis, Raisa Amiruddin, Amirreza Manteghinejad, Maruf Adewole, Jake Albrecht, Udunna Anazodo, Sanjay Aneja, Syed Muhammad Anwar, Timothy Bergquist, Veronica Chiang, Verena Chung, Gian Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Nastaran Khalili, Keyvan Farahani, Juan Eugenio Iglesias, Zhifan Jiang, Elaine Johanson, Anahita Fathi Kazerooni, Florian Kofler, Kiril Krantchev, Dominic LaBella, Koen Van Leemput, Hongwei Bran Li, Marius George Linguraru, Xinyang Liu, Zeke Meier, Bjoern H Menze, Harrison Moy, Klara Osenberg, Marie Piraud, Zachary Reitman, Russell Takeshi Shinohara, Chunhao Wang, Benedikt Wiestler, Walter Wiggins, Umber Shafique, Klara Willms, Arman Avesta, Khaled Bousabarah, Satrajit Chakrabarty, Nicolo Gennaro, Wolfgang Holler, Manpreet Kaur, Pamela LaMontagne, MingDe Lin, Jan Lost, Daniel S. Marcus, Ryan Maresca, Sarah Merkaj, Gabriel Cassinelli Pedersen, Marc von Reppert, Aristeidis Sotiras, Oleg Teytelboym, Niklas Tillmans, Malte Westerhoff, Ayda Youssef, Devon Godfrey, Scott Floyd, Andreas Rauschecker, Javier Villanueva-Meyer, Irada Pfluger, Jaeyoung Cho, Martin Bendszus, Gianluca Brugnara, Justin Cramer, Gloria J. Guzman Perez-Carillo, Derek R. Johnson, Anthony Kam, Benjamin Yin Ming Kwan, Lillian Lai, Neil U. Lall, Fatima Memon, Mark Krycia, Satya Narayana Patro, Bojan Petrovic, Tiffany Y. So, Gerard Thompson, Lei Wu, E. Brooke Schrickel, Anu Bansal, Frederik Barkhof, Cristina Besada, Sammy Chu, Jason Druzgal, Alexandru Dusoi, Luciano Farage, Fabricio Feltrin, Amy Fong, Steve H. Fung, R. Ian Gray, Ichiro Ikuta, Michael Iv, Alida A. Postma, Amit Mahajan, David Joyner, Chase Krumpelman, Laurent Letourneau-Guillon, Christie M. Lincoln, Mate E. Maros, Elka Miller, Fanny Moron, Esther A. Nimchinsky, Ozkan Ozsarlak, Uresh Patel, Saurabh Rohatgi, Atin Saha, Anousheh Sayah, Eric D. Schwartz, Robert Shih, Mark S. Shiroishi, Juan E. Small, Manoj Tanwar, Jewels Valerie, Brent D. Weinberg, Matthew L. White, Robert Young, Vahe M. Zohrabian, Aynur Azizova, Melanie Maria Theresa Bruseler, Mohanad Ghonim, Mohamed Ghonim, Abdullah Okar, Luca Pasquini, Yasaman Sharifi, Gagandeep Singh, Nico Sollmann, Theodora Soumala, Mahsa Taherzadeh, Philipp Vollmuth, Martha Foltyn-Dumitru, Ajay Malhotra, Aly H. Abayazeed, Francesco Dellepiane, Philipp Lohmann, Victor M. Perez-Garcia, Hesham Elhalawani, Maria Correia de Verdier, Sanaria Al-Rubaiey, Rui Duarte Armindo, Kholod Ashraf, Moamen M. Asla, Mohamed Badawy, Jeroen Bisschop, Nima Broomand Lomer, Jan Bukatz, Jim Chen, Petra Cimflova, Felix Corr, Alexis Crawley, Lisa Deptula, Tasneem Elakhdar, Islam H. Shawali, Shahriar Faghani, Alexandra Frick, Vaibhav Gulati, Muhammad Ammar Haider, Fatima Hierro, Rasmus Holmboe Dahl, Sarah Maria Jacobs, Kuang-chun Jim Hsieh, Sedat G. Kandemirli, Katharina Kersting, Laura Kida, Sofia Kollia, Ioannis Koukoulithras, Xiao Li, Ahmed Abouelatta, Aya Mansour, Ruxandra-Catrinel Maria-Zamfirescu, Marcela Marsiglia, Yohana Sarahi Mateo-Camacho, Mark McArthur, Olivia McDonnell, Maire McHugh, Mana Moassefi, Samah Mostafa Morsi, Alexander Munteanu, Khanak K. Nandolia, Syed Raza Naqvi, Yalda Nikanpour, Mostafa Alnoury, Abdullah Mohamed Aly Nouh, Francesca Pappafava, Markand D. Patel, Samantha Petrucci, Eric Rawie, Scott Raymond, Borna Roohani, Sadeq Sabouhi, Laura M. Sanchez-Garcia, Zoe Shaked, Pokhraj P. Suthar, Talissa Altes, Edvin Isufi, Yaseen Dhemesh, Jaime Gass, Jonathan Thacker, Abdul Rahman Tarabishy, Benjamin Turner, Sebastiano Vacca, George K. Vilanilam, Daniel Warren, David Weiss, Fikadu Worede, Sara Yousry, Wondwossen Lerebo, Alejandro Aristizabal, Alexandros Karargyris, Hasan Kassem, Sarthak Pati, Micah Sheller, Katherine E. Link, Evan Calabrese, Nourel hoda Tahon, Ayman Nada, Yuri S. Velichko, Spyridon Bakas, Jeffrey D. Rudie, Mariam Aboian</strong></p><p>The translation of AI-generated brain metastases (BM) segmentation into clinical practice relies heavily on diverse, high-quality annotated medical imaging datasets. The BraTS-METS 2023 challenge has gained momentum for testing and benchmarking algorithms using rigorously annotated internationally compiled real-world datasets. This study presents the results of the segmentation challenge and characterizes the challenging cases that impacted the performance of the winning algorithms. Untreated brain metastases on standard anatomic MRI sequences (T1, T2, FLAIR, T1PG) from eight contributed international datasets were annotated in stepwise method: published UNET algorithms, student, neuroradiologist, final approver neuroradiologist. Segmentations were ranked based on lesion-wise Dice and Hausdorff distance (HD95) scores. False positives (FP) and false negatives (FN) were rigorously penalized, receiving a score of 0 for Dice and a fixed penalty of 374 for HD95. Eight datasets comprising 1303 studies were annotated, with 402 studies (3076 lesions) released on Synapse as publicly available datasets to challenge competitors. Additionally, 31 studies (139 lesions) were held out for validation, and 59 studies (218 lesions) were used for testing. Segmentation accuracy was measured as rank across subjects, with the winning team achieving a LesionWise mean score of 7.9. Common errors among the leading teams included false negatives for small lesions and misregistration of masks in space.The BraTS-METS 2023 challenge successfully curated well-annotated, diverse datasets and identified common errors, facilitating the translation of BM segmentation across varied clinical environments and providing personalized volumetric reports to patients undergoing BM treatment.</p><blockquote><p>å°†AIç”Ÿæˆçš„è„‘è½¬ç§»ï¼ˆBMï¼‰åˆ†æ®µç¿»è¯‘åº”ç”¨äºä¸´åºŠå®è·µï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤šæ ·åŒ–ã€é«˜è´¨é‡æ ‡æ³¨çš„åŒ»å­¦å½±åƒæ•°æ®é›†ã€‚BraTS-METS 2023æŒ‘æˆ˜èµ›é€šè¿‡ä½¿ç”¨ä¸¥æ ¼æ ‡æ³¨çš„å›½é™…æ±‡ç¼–çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæµ‹è¯•å¹¶è¯„ä¼°ç®—æ³•æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†åˆ†å‰²æŒ‘æˆ˜çš„ç»“æœï¼Œå¹¶å¯¹å½±å“è·èƒœç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§ç—…ä¾‹è¿›è¡Œäº†ç‰¹å¾æè¿°ã€‚åœ¨æ ‡å‡†çš„è§£å‰–MRIåºåˆ—ï¼ˆT1ã€T2ã€FLAIRã€T1PGï¼‰ä¸Šï¼Œå¯¹æœªæ²»ç–—çš„è„‘è½¬ç§»ç—…ç¶é‡‡ç”¨é€æ­¥æ–¹æ³•è¿›è¡Œæ ‡æ³¨ï¼šå·²å‘å¸ƒçš„UNETç®—æ³•ã€å­¦ç”Ÿã€ç¥ç»æ”¾å°„å­¦å®¶ã€æœ€ç»ˆå®¡æ‰¹çš„ç¥ç»æ”¾å°„å­¦å®¶ã€‚åˆ†å‰²æ’ååŸºäºç—…ç¶çº§åˆ«çš„Diceå’ŒHausdorffè·ç¦»ï¼ˆHD95ï¼‰å¾—åˆ†ã€‚å‡é˜³æ€§ï¼ˆFPï¼‰å’Œå‡é˜´æ€§ï¼ˆFNï¼‰å—åˆ°ä¸¥æ ¼æƒ©ç½šï¼ŒDiceå¾—åˆ†ä¸º0ï¼ŒHD95çš„å›ºå®šæƒ©ç½šä¸º374ã€‚å…±æ ‡æ³¨äº†åŒ…å«1303é¡¹ç ”ç©¶çš„å…«ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­402é¡¹ç ”ç©¶ï¼ˆå«3076ä¸ªç—…ç¶ï¼‰åœ¨Synapseä¸Šä½œä¸ºå…¬å¼€æ•°æ®é›†å‘å¸ƒï¼Œä»¥æŒ‘æˆ˜ç«äº‰å¯¹æ‰‹ã€‚æ­¤å¤–ï¼Œè¿˜ç•™å‡º31é¡¹ç ”ç©¶ï¼ˆå«139ä¸ªç—…ç¶ï¼‰ç”¨äºéªŒè¯ï¼Œå¦æœ‰59é¡¹ç ”ç©¶ï¼ˆå«218ä¸ªç—…ç¶ï¼‰ç”¨äºæµ‹è¯•ã€‚åˆ†å‰²ç²¾åº¦æŒ‰å—è¯•è€…æ’åï¼Œå† å†›å›¢é˜Ÿçš„LesionWiseå¹³å‡å¾—åˆ†ä¸º7.9ã€‚é¢†å…ˆå›¢é˜Ÿå¸¸è§çš„é”™è¯¯åŒ…æ‹¬å°ç—…ç¶çš„å‡é˜´æ€§ä»¥åŠå£ç½©åœ¨ç©ºé—´ä¸Šçš„é”™ä½ã€‚BraTS-METS 2023æŒ‘æˆ˜èµ›æˆåŠŸæ•´ç†å‡ºæ ‡æ³¨è‰¯å¥½ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œå¹¶è¯†åˆ«å‡ºå¸¸è§é”™è¯¯ï¼Œä¿ƒè¿›äº†BMåˆ†æ®µçš„ä¸´åºŠç¿»è¯‘ï¼Œå¹¶ä¸ºæ¥å—BMæ²»ç–—çš„ç—…äººæä¾›ä¸ªæ€§åŒ–çš„ä½“ç§¯æŠ¥å‘Šã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.00838v3">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†BraTS-METS 2023æŒ‘æˆ˜èµ›çš„ç»“æœå’Œç‰¹æ€§ï¼Œè¯¥æŒ‘æˆ˜èµ›ä½¿ç”¨ä¸¥æ ¼æ³¨é‡Šçš„å›½é™…çœŸå®ä¸–ç•Œæ•°æ®é›†æµ‹è¯•å’Œè¯„ä¼°ç®—æ³•åœ¨è„‘è½¬ç§»ç˜¤åˆ†å‰²ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘å¸ƒäº†ä¸€ç³»åˆ—å…¬å¼€æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªå›½é™…æ•°æ®é›†ï¼Œç”¨äºæŒ‘æˆ˜ç«äº‰è€…ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†ï¼Œè¯†åˆ«å‡ºé¢†å…ˆå›¢é˜Ÿå¸¸è§çš„é”™è¯¯ï¼Œå¦‚å°ç—…ç¶çš„å‡é˜´æ€§ä»¥åŠç©ºé—´å†…é®ç½©çš„è¯¯é…å‡†ã€‚æ­¤æŒ‘æˆ˜æˆåŠŸä¿ƒè¿›äº†è„‘è½¬ç§»ç˜¤åˆ†å‰²åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„è½¬åŒ–ï¼Œå¹¶ä¸ºæ¥å—è„‘è½¬ç§»ç˜¤æ²»ç–—çš„æ‚£è€…æä¾›ä¸ªæ€§åŒ–ä½“ç§¯æŠ¥å‘Šã€‚</p><p><strong>Key Takeaways</strong></p><ol><li><p>BraTS-METS 2023æŒ‘æˆ˜èµ›ä½¿ç”¨ä¸¥æ ¼æ³¨é‡Šçš„å›½é™…çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæ¨åŠ¨AIåœ¨è„‘è½¬ç§»ç˜¤åˆ†å‰²æ–¹é¢çš„åº”ç”¨ã€‚</p></li><li><p>ç ”ç©¶æ¶‰åŠå¤šä¸ªå›½é™…æ•°æ®é›†ï¼ŒåŒ…æ‹¬æœªå¤„ç†è¿‡çš„è„‘è½¬ç§»ç˜¤æ ‡å‡†è§£å‰–MRIåºåˆ—æ•°æ®ã€‚</p></li><li><p>åˆ†å‰²ç»“æœçš„è¯„ä¼°åŸºäºç—…ç¶çº§åˆ«çš„Diceå’ŒHausdorffè·ç¦»å¾—åˆ†ï¼Œä¸¥æ ¼æƒ©ç½šå‡é˜³æ€§å’Œå‡é˜´æ€§ç»“æœã€‚</p></li><li><p>å‘å¸ƒå…¬å¼€æ•°æ®é›†ä»¥æŒ‘æˆ˜å…¶ä»–ç«äº‰è€…ï¼ŒåŒæ—¶è¿›è¡ŒéªŒè¯å’Œæµ‹è¯•æ•°æ®é›†çš„åˆ’åˆ†ã€‚</p></li><li><p>é¢†å…ˆçš„å›¢é˜Ÿåœ¨åˆ†å‰²è¿‡ç¨‹ä¸­å­˜åœ¨çš„å¸¸è§é”™è¯¯åŒ…æ‹¬å°ç—…ç¶çš„é—æ¼å’Œé®ç½©çš„ç©ºé—´è¯¯é…å‡†ã€‚</p></li><li><p>BraTS-METS 2023æŒ‘æˆ˜èµ›æˆåŠŸè¯†åˆ«å‡ºç®—æ³•åœ¨åˆ†å‰²æ–¹é¢çš„å¸¸è§æŒ‘æˆ˜ï¼Œæœ‰åŠ©äºç®—æ³•çš„ä¼˜åŒ–å’Œæ”¹è¿›ã€‚</p><pre><code>         HTML
</code></pre><p>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹<br>â€‹</p></li></ol><p></p><p>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">æ–‡ç« ä½œè€…:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">æ–‡ç« é“¾æ¥:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">ç‰ˆæƒå£°æ˜:</i></span> <span class="reprint-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"><span class="chip bg-color">åŒ»å­¦å›¾åƒ</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;ä¸Šä¸€ç¯‡</div><div class="card"><a href="/Talk2Paper/Paper/2024-12-12/LLM/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" class="responsive-img" alt="LLM"> <span class="card-title">LLM</span></div></a><div class="card-content article-content"><div class="summary block-with-text">LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12 Generative Semantic Communication Architectures, Technologies, and Applications</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-12</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/LLM/" class="post-category">LLM</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-12-10/TTS/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS"> <span class="card-title">TTS</span></div></a><div class="card-content article-content"><div class="summary block-with-text">TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11 Towards Controllable Speech Synthesis in the Era of Large Language Models A Survey</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-11</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/TTS/" class="post-category">TTS</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/TTS/"><span class="chip bg-color">TTS</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span class="white-color">4896.5k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="æœ¬ç«™å·²è¿è¡Œ "+c+" å¤©",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="æœ¬ç«™å·²è¿è¡Œ "+l+" å¹´ "+c+" å¤©",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">çŸ¥</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;æœç´¢</span> <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ",clearTimeout(st)):(document.title="Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>