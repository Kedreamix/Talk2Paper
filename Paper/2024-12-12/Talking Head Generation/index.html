<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="医学图像"><meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>医学图像 | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>归档</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://pic1.zhimg.com/v2-31152d53eb7db9c15f8ded97030b07e2.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">医学图像</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"><span class="chip bg-color">医学图像</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">医学图像</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> 发布日期:&nbsp;&nbsp; 2024-12-12</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> 更新日期:&nbsp;&nbsp; 2024-12-12</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> 文章字数:&nbsp;&nbsp; 28k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> 阅读时长:&nbsp;&nbsp; 116 分</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> 阅读次数:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p><p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the model’s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models.</p><blockquote><p>图像反射消除对于恢复图像质量至关重要。扭曲的图像会对目标检测和图像分割等任务产生负面影响。在本文中，我们提出了一种使用单幅图像进行图像反射消除的新方法。我们没有关注模型架构，而是引入了一种可以推广到图像到图像问题的新训练技术，输入和输出的性质相似。这种技术体现在我们的多步损失机制中，在反射去除任务中已被证明是有效的。此外，我们通过使用Pix2Pix GAN合成了一种高质量的非线性合成数据集RefGAN，解决了反射去除训练数据的稀缺问题。该数据集显著提高了模型学习更好反射去除模式的能力。我们还利用从环境图像的深度估计中提取的范围深度图作为辅助特征，利用其缺乏反射深度估计的属性。我们的方法在SIR^2基准和其他真实世界数据集上表现出了卓越的性能，超越了其他最先进模型，证明了其有效性。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p><p><strong>Summary</strong></p><p>本文提出了一种新的图像反射去除方法，使用单张图像即可完成。研究重点不在于模型架构，而是引入了一种可推广至同类图像转换问题的新型训练技术。该技术体现在多步骤损失机制中，在反射去除任务中证明有效。此外，通过Pix2Pix GAN合成了一个高质量的非线性合成数据集RefGAN，解决了反射去除训练数据不足的问题。还利用从环境图像深度估计中提取的深度图作为辅助特征，发挥其缺乏反射深度估计的特性。该方法在SIR^2基准测试和其他真实世界数据集上表现出卓越性能，超越了其他最先进模型。</p><p><strong>Key Takeaways</strong></p><ol><li>引入了一种新的图像反射去除方法，使用单张图像即可实现。</li><li>研究的重点不在于模型架构，而是引入了一种新型训练技术，适用于同类图像转换问题。</li><li>通过多步骤损失机制实现有效反射去除。</li><li>利用Pix2Pix GAN合成了一个高质量的非线性合成数据集RefGAN，解决训练数据不足的问题。</li><li>利用从环境图像深度估计中提取的深度图作为辅助特征。</li><li>该方法在SIR^2基准测试和其他真实世界数据集上表现出卓越性能。</li></ol><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b7cd56e05b1afded563a61c2fe8e3211.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-47b9cb26824414e39d3a063f2d4843c9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fdb27ae3046b3ba4acd7d4cddaf1cb0b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-964e226e5e8922d8bf51ab0b2e812c6d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2cd2af283292b21629cca6cea92528de.jpg" align="middle"></details><h2 id="Annotation-Efficient-Task-Guidance-for-Medical-Segment-Anything"><a href="#Annotation-Efficient-Task-Guidance-for-Medical-Segment-Anything" class="headerlink" title="Annotation-Efficient Task Guidance for Medical Segment Anything"></a>Annotation-Efficient Task Guidance for Medical Segment Anything</h2><p><strong>Authors:Tyler Ward, Abdullah-Al-Zubaer Imran</strong></p><p>Medical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose SAM-Mix, a novel multitask learning framework for medical image segmentation that uses class activation maps produced by an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the SAM framework. Experimental evaluations on the public LiTS dataset confirm the effectiveness of SAM-Mix for simultaneous classification and segmentation of the liver from abdominal computed tomography (CT) scans. When trained for 90% fewer epochs on only 50 labeled 2D slices, representing just 0.04% of the available labeled training data, SAM-Mix achieves a Dice improvement of 5.1% over the best baseline model. The generalization results for SAM-Mix are even more impressive, with the same model configuration yielding a 25.4% Dice improvement on a cross-domain segmentation task. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tbwa233/SAM-Mix">https://github.com/tbwa233/SAM-Mix</a>.</p><blockquote><p>医学图像分割是成像工作流程中的关键任务，影响许多基于图像的决定。传统的全监督分割模型依赖于大量的标记训练数据，通常通过手动注释获得，这可能会是一个成本高昂、耗时且易出错的过程。这显示了对准确、自动和注释高效的训练方法的需求。我们提出SAM-Mix，这是一种新型的医学图像分割多任务学习框架，它使用辅助分类器生成的类激活图来指导半监督分割分支的预测，该分支基于SAM框架。在公共LiTS数据集上的实验评估证实了SAM-Mix在腹部计算机断层扫描（CT扫描）中同时进行肝脏分类和分割的有效性。仅在50个标记的2D切片上进行训练，相当于现有可用标记训练数据的0.04%，SAM-Mix在最佳基线模型的基础上实现了5.1%的Dice系数提升。SAM-Mix的泛化结果更令人印象深刻，相同的模型配置在跨域分割任务上实现了25.4%的Dice系数提升。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/tbwa233/SAM-Mix%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tbwa233/SAM-Mix上获取。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08575v1">PDF</a></p><p><strong>Summary</strong></p><p>本文提出了一种基于多任务学习框架的医学图像分割方法SAM-Mix，该方法使用辅助分类器产生的类激活图来指导半监督分割分支的预测。实验评估表明，在公共LiTS数据集上，SAM-Mix在肝脏分类和分割任务中表现出色，使用较少的标注数据即可达到良好的性能，并在跨域分割任务中实现了显著的改进。</p><p><strong>Key Takeaways</strong></p><ul><li><p>医学图像分割是成像工作流程中的关键任务，影响许多基于图像的决定。</p></li><li><p>传统完全监督的分割模型依赖于大量手动标注的训练数据，这一过程既昂贵又耗时，还容易出错。</p></li><li><p>SAM-Mix是一种新型的医学图像分割多任务学习框架，使用辅助分类器产生的类激活图来指导预测。</p></li><li><p>在LiTS数据集上的实验评估显示，SAM-Mix在肝脏分类和分割任务上表现出卓越性能。</p></li><li><p>SAM-Mix在仅使用极少标注数据的情况下即可达到良好性能，训练周期减少90%，且Dice系数相较于最佳基线模型提高了5.1%。</p></li><li><p>SAM-Mix的泛化能力令人印象深刻，在同一模型配置下，在跨域分割任务中Dice系数提高了25.4%。</p><pre><code>          HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​<br>点此查看论文截图</p></li></ul><h2 id="ConDSeg-A-General-Medical-Image-Segmentation-Framework-via-Contrast-Driven-Feature-Enhancement"><a href="#ConDSeg-A-General-Medical-Image-Segmentation-Framework-via-Contrast-Driven-Feature-Enhancement" class="headerlink" title="ConDSeg: A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement"></a>ConDSeg: A General Medical Image Segmentation Framework via Contrast-Driven Feature Enhancement</h2><p><strong>Authors:Mengqi Lei, Haochen Wu, Xinhua Lv, Xin Wang</strong></p><p>Medical image segmentation plays an important role in clinical decision making, treatment planning, and disease tracking. However, it still faces two major challenges. On the one hand, there is often a &#96;&#96;soft boundary’’ between foreground and background in medical images, with poor illumination and low contrast further reducing the distinguishability of foreground and background within the image. On the other hand, co-occurrence phenomena are widespread in medical images, and learning these features is misleading to the model’s judgment. To address these challenges, we propose a general framework called Contrast-Driven Medical Image Segmentation (ConDSeg). First, we develop a contrastive training strategy called Consistency Reinforcement. It is designed to improve the encoder’s robustness in various illumination and contrast scenarios, enabling the model to extract high-quality features even in adverse environments. Second, we introduce a Semantic Information Decoupling module, which is able to decouple features from the encoder into foreground, background, and uncertainty regions, gradually acquiring the ability to reduce uncertainty during training. The Contrast-Driven Feature Aggregation module then contrasts the foreground and background features to guide multi-level feature fusion and key feature enhancement, further distinguishing the entities to be segmented. We also propose a Size-Aware Decoder to solve the scale singularity of the decoder. It accurately locate entities of different sizes in the image, thus avoiding erroneous learning of co-occurrence features. Extensive experiments on five medical image datasets across three scenarios demonstrate the state-of-the-art performance of our method, proving its advanced nature and general applicability to various medical image segmentation scenarios. Our released code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg%7D">https://github.com/Mengqi-Lei/ConDSeg}</a>.</p><blockquote><p>医学图像分割在临床决策、治疗规划和疾病追踪中发挥着重要作用。然而，它仍然面临两大挑战。一方面，医学图像中的前景和背景之间通常存在“软边界”，而照明不良和对比度低进一步降低了图像中前景和背景的辨别能力。另一方面，医学图像中的共现现象普遍存在，学习这些特征会对模型的判断产生误导。为了解决这些挑战，我们提出了一种通用的框架，称为Contrast-Driven Medical Image Segmentation（ConDSeg）。首先，我们开发了一种名为Consistency Reinforcement的对比训练策略。它旨在提高编码器在各种照明和对比度场景中的稳健性，使模型即使在恶劣环境中也能提取高质量的特征。其次，我们引入了一个Semantic Information Decoupling模块，该模块能够将编码器的特征解耦为前景、背景和不确定区域，逐渐在训练过程中获得减少不确定性的能力。然后，Contrast-Driven Feature Aggregation模块对比前景和背景特征，引导多级别特征融合和关键特征增强，进一步区分要分割的实体。我们还提出了一种Size-Aware Decoder，以解决解码器的尺度单一性问题。它准确地定位图像中不同大小的实体，从而避免了共现特征的错误学习。在三种场景下的五个医学图像数据集上的大量实验表明，我们的方法达到了最先进的性能，证明了其在各种医学图像分割场景中的先进性和通用适用性。我们发布的代码可在<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Mengqi-Lei/ConDSeg获取。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08345v1">PDF</a> This paper has been accepted by AAAI-2025</p><p><strong>Summary</strong></p><p>医疗图像分割在临床决策、治疗计划以及疾病追踪中扮演着重要角色，但面临两大挑战。一是医学图像中前景与背景之间常存在“软边界”，加之照明不良、对比度低，使得前景与背景的辨识度降低。二是医学图像中普遍存在共现现象，对模型判断造成误导。为应对这些挑战，提出了一种通用框架——Contrast-Driven Medical Image Segmentation (ConDSeg)。该框架通过对比训练策略和提升编码器稳健性，能够在恶劣环境下提取高质量特征；引入语义信息解耦模块，逐步减少不确定性；采用对比驱动特征聚合模块，引导多级别特征融合和关键特征增强，进一步区分待分割实体；并提出尺寸感知解码器，解决解码器尺度单一性问题，准确定位图像中不同大小的实体，避免错误学习共现特征。在五个医学图像数据集上的实验证明了该方法的前沿性能，且适用于多种医学图像分割场景。相关代码已发布在[<a target="_blank" rel="noopener" href="https://github.com/Mengqi-Lei/ConDSeg]%E3%80%82">https://github.com/Mengqi-Lei/ConDSeg]。</a></p><p><strong>Key Takeaways</strong></p><ol><li><p>医疗图像分割在临床应用中具有重要意义，但存在前景与背景辨识困难、共现现象等挑战。</p></li><li><p>提出了Contrast-Driven Medical Image Segmentation (ConDSeg)框架，通过一系列模块解决上述挑战。</p></li><li><p>引入对比训练策略和提升编码器稳健性，以适应不同照明和对比度场景。</p></li><li><p>语义信息解耦模块能够逐步减少不确定性，对比驱动特征聚合模块区分待分割实体。</p></li><li><p>尺寸感知解码器解决解码器尺度单一性问题，准确定位不同大小实体。</p></li><li><p>在多个医学图像数据集上的实验证明了该方法的前沿性能。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Lightweight-Method-for-Interactive-3D-Medical-Image-Segmentation-with-Multi-Round-Result-Fusion"><a href="#Lightweight-Method-for-Interactive-3D-Medical-Image-Segmentation-with-Multi-Round-Result-Fusion" class="headerlink" title="Lightweight Method for Interactive 3D Medical Image Segmentation with   Multi-Round Result Fusion"></a>Lightweight Method for Interactive 3D Medical Image Segmentation with Multi-Round Result Fusion</h2><p><strong>Authors:Bingzhi Shen, Lufan Chang, Siqi Chen, Shuxiang Guo, Hao Liu</strong></p><p>In medical imaging, precise annotation of lesions or organs is often required. However, 3D volumetric images typically consist of hundreds or thousands of slices, making the annotation process extremely time-consuming and laborious. Recently, the Segment Anything Model (SAM) has drawn widespread attention due to its remarkable zero-shot generalization capabilities in interactive segmentation. While researchers have explored adapting SAM for medical applications, such as using SAM adapters or constructing 3D SAM models, a key question remains: Can traditional CNN networks achieve the same strong zero-shot generalization in this task? In this paper, we propose the Lightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a novel approach demonstrating the potential of compact CNN-based models. Built upon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D prompt mask from user hints. This mask is then propagated through the 3D sequence via the Memory Module. To refine and stabilize results during interaction, the Multi-Round Result Fusion (MRF) Module selects and merges optimal masks from multiple rounds. Our extensive experiments across multiple datasets and modalities demonstrate LIM-Net’s competitive performance. It exhibits stronger generalization to unseen data compared to SAM-based models, with competitive accuracy while requiring fewer interactions. Notably, LIM-Net’s lightweight design offers significant advantages in deployment and inference efficiency, with low GPU memory consumption suitable for resource-constrained environments. These promising results demonstrate LIM-Net can serve as a strong baseline, complementing and contrasting with popular SAM models to further boost effective interactive medical image segmentation. The code will be released at \url{<a target="_blank" rel="noopener" href="https://github.com/goodtime-123/LIM-Net%7D">https://github.com/goodtime-123/LIM-Net}</a>.</p><blockquote><p>在医学成像领域，对病变或器官进行精确标注是经常需要的。然而，3D体积图像通常由数百或数千个切片组成，使得标注过程极为耗时且繁琐。最近，由于其在交互式分割中的出色零样本泛化能力，Segment Anything Model（SAM）引起了广泛关注。虽然研究人员已经探索了将SAM用于医学应用，例如使用SAM适配器或构建3D SAM模型，但一个关键问题仍然存在：传统CNN网络能否在此任务中实现同样的强大零样本泛化能力？在本文中，我们提出了用于3D医学图像分割的轻型交互式网络（LIM-Net），这是一种展示紧凑CNN模型潜力的新型方法。基于2D CNN骨干网，LIM-Net通过生成用户提示的2D提示掩膜来启动分割。然后，该掩膜通过内存模块传播到整个3D序列。为了在交互过程中优化和稳定结果，多轮结果融合（MRF）模块从多轮中选择并合并最佳掩膜。我们在多个数据集和模态上进行的广泛实验证明了LIM-Net的竞争性能。与基于SAM的模型相比，它在未见数据上表现出更强的泛化能力，具有相当的准确性但需要更少的交互。值得注意的是，LIM-Net的轻量级设计在部署和推理效率方面提供了显著优势，低GPU内存消耗使其成为资源受限环境的理想选择。这些令人鼓舞的结果表明，LIM-Net可以作为强大的基线，与流行的SAM模型相辅相成，进一步推动有效的交互式医学图像分割。代码将在\url{<a target="_blank" rel="noopener" href="https://github.com/goodtime-12KFRPMGQQBYBFGCYBFLMLVNPDXXBFHXNXFYFGBRO4OZYGJQ/%7D%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/goodtime-12KFRPMGQQBYBFGCYBFLMLVNPDXXBFHXNXFYFGBRO4OZYGJQ/}上发布。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08315v1">PDF</a></p><p><strong>摘要</strong></p><p>在医学成像领域，对病变或器官进行精确标注是非常必要的。然而，由于三维体积图像通常包含数百或数千个切片，使得标注过程非常耗时且繁琐。最近，Segment Anything Model（SAM）因其出色的零样本泛化能力而在交互式分割中受到广泛关注。尽管研究者已经尝试将SAM适应于医学应用，如使用SAM适配器或构建三维SAM模型，但仍有一个关键问题：传统的CNN网络是否能在该任务中实现同样的强零样本泛化能力？本文提出了用于三维医学图像分割的轻型交互式网络（LIM-Net），这是一种展示紧凑CNN模型潜力新方法。LIM-Net建立在二维CNN的基础上，通过用户提示生成二维提示掩膜来启动分割。该掩膜然后通过内存模块传播到整个三维序列。为了在交互过程中优化和稳定结果，多轮结果融合（MRF）模块会选择并合并来自多轮的优质掩膜。我们的实验在多个数据集和模态上进行了广泛的验证，证明了LIM-Net的竞争力。与SAM模型相比，它在未见数据上展现出更强的泛化能力，具有竞争性的准确性同时需要更少的交互。值得注意的是，LIM-Net的轻量级设计在部署和推理效率方面提供了显著的优势，低GPU内存消耗使其成为资源受限环境的理想选择。这些令人鼓舞的结果表明，LIM-Net可以作为强有力的基线，与流行的SAM模型相辅相成，进一步推动交互式医学图像分割的有效性。代码将在<a target="_blank" rel="noopener" href="https://github.com/goodtime-123/LIM-Net">https://github.com/goodtime-123/LIM-Net</a>发布。</p><p><strong>Key Takeaways</strong></p><ol><li><p>LIM-Net是一种新型的交互式网络，用于三维医学图像分割。</p></li><li><p>它建立在二维CNN的基础上，通过生成二维提示掩膜来启动分割过程。</p></li><li><p>LIM-Net通过内存模块将掩膜传播到整个三维序列。</p></li><li><p>多轮结果融合模块用于在交互过程中优化和稳定结果。</p></li><li><p>LIM-Net在多个数据集上的表现具有竞争力，特别是在未见数据的泛化能力方面。</p></li><li><p>与SAM模型相比，LIM-Net需要更少的交互就能达到竞争性的准确性。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Unified-HT-CNNs-Architecture-Transfer-Learning-for-Segmenting-Diverse-Brain-Tumors-in-MRI-from-Gliomas-to-Pediatric-Tumors"><a href="#Unified-HT-CNNs-Architecture-Transfer-Learning-for-Segmenting-Diverse-Brain-Tumors-in-MRI-from-Gliomas-to-Pediatric-Tumors" class="headerlink" title="Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse   Brain Tumors in MRI from Gliomas to Pediatric Tumors"></a>Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors</h2><p><strong>Authors:Ramy A. Zeineldin, Franziska Mathis-Ullrich</strong></p><p>Accurate segmentation of brain tumors from 3D multimodal MRI is vital for diagnosis and treatment planning across diverse brain tumors. This paper addresses the challenges posed by the BraTS 2023, presenting a unified transfer learning approach that applies to a broader spectrum of brain tumors. We introduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural Networks optimized through transfer learning for varied brain tumor segmentation. This method captures spatial and contextual details from MRI data, fine-tuned on diverse datasets representing common tumor types. Through transfer learning, HT-CNNs utilize the learned representations from one task to improve generalization in another, harnessing the power of pre-trained models on large datasets and fine-tuning them on specific tumor types. We preprocess diverse datasets from multiple international distributions, ensuring representativeness for the most common brain tumors. Our rigorous evaluation employs standardized quantitative metrics across all tumor types, ensuring robustness and generalizability. The proposed ensemble model achieves superior segmentation results across the BraTS validation datasets over the previous winning methods. Comprehensive quantitative evaluations using the DSC and HD95 demonstrate the effectiveness of our approach. Qualitative segmentation predictions further validate the high-quality outputs produced by our model. Our findings underscore the potential of transfer learning and ensemble approaches in medical image segmentation, indicating a substantial enhancement in clinical decision-making and patient care. Despite facing challenges related to post-processing and domain gaps, our study sets a new precedent for future research for brain tumor segmentation. The docker image for the code and models has been made publicly available, <a target="_blank" rel="noopener" href="https://hub.docker.com/r/razeineldin/ht-cnns">https://hub.docker.com/r/razeineldin/ht-cnns</a>.</p><blockquote><p>对3D多模态MRI中的脑肿瘤进行精确分割对于各种脑肿瘤的诊断和治疗计划至关重要。本文针对BraTS 2023提出的挑战，提出了一种统一的迁移学习的方法，适用于更广泛的脑肿瘤谱。我们引入了HT-CNNs，这是一种混合Transformer和卷积神经网络（CNN）的集成方法，通过迁移学习针对各种脑肿瘤分割进行优化。这种方法能够捕获MRI数据中的空间和上下文细节，并在代表常见肿瘤类型的各种数据集上进行微调。通过迁移学习，HT-CNNs利用一个任务中学到的表示来提高另一个任务的泛化能力，充分利用大型数据集上预训练模型的力量，并针对特定肿瘤类型进行微调。我们对来自多个国际分布的多种数据集进行了预处理，以确保对最常见的脑肿瘤的代表性。我们采用所有肿瘤类型的标准化定量指标进行了严格评估，以确保其稳健性和通用性。在BraTS验证数据集上，与以前的方法相比，所提出的集成模型实现了更好的分割结果。使用DSC和HD95的定量评估证明了我们方法的有效性。定性的分割预测进一步验证了我们的模型产生的高质量输出。我们的研究强调了迁移学习和集成方法在医学图像分割中的潜力，这可能会极大地提高临床决策和患者护理的质量。尽管面临着与后处理和领域差距相关的挑战，但我们的研究为未来的脑肿瘤分割研究树立了新的先例。代码的docker镜像和模型已经公开发布在<a target="_blank" rel="noopener" href="https://hub.docker.com/r/razeineldin/ht-cnns%E4%B8%8A%E3%80%82">https://hub.docker.com/r/razeineldin/ht-cnns上。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08240v1">PDF</a> Accepted in the Computer Assisted Radiology and Surgery (CARS 2024) Conference</p><p><strong>Summary</strong><br>本文提出一种基于迁移学习的统一方法，用于从3D多模态MRI中准确分割多种类型的脑肿瘤。通过混合Transformer和卷积神经网络（HT-CNNs）的集成模型，结合迁移学习技术，实现空间与上下文细节的捕捉。该模型在多种脑肿瘤数据集上进行预处理和标准化评估，公开代码和模型Docker镜像可供下载。研究表明迁移学习和集成方法能提高医学图像分割的精度，有望改善临床决策和患者护理。</p><p><strong>Key Takeaways</strong></p><ol><li><p>论文提出了一种针对多种脑肿瘤分割的挑战，采用迁移学习方法应对。</p></li><li><p>引入HT-CNNs模型，结合Transformer和卷积神经网络进行图像分割。</p></li><li><p>模型能够从MRI数据中捕捉空间与上下文细节，并针对不同肿瘤类型进行微调。</p></li><li><p>通过迁移学习，模型利用一种任务的表示学习来改善另一种任务中的泛化能力。</p></li><li><p>论文采用多个国际分布的数据集进行预处理和评估，确保模型的代表性和鲁棒性。</p></li><li><p>与之前的方法相比，论文中提出的模型在BraTS验证数据集上实现了优越的分割结果。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM"><a href="#Detection-of-extended-X-ray-emission-around-the-PeVatron-microquasar-V4641-Sgr-with-XRISM" class="headerlink" title="Detection of extended X-ray emission around the PeVatron microquasar   V4641 Sgr with XRISM"></a>Detection of extended X-ray emission around the PeVatron microquasar V4641 Sgr with XRISM</h2><p><strong>Authors:Hiromasa Suzuki, Naomi Tsuji, Yoshiaki Kanemaru, Megumi Shidatsu, Laura Olivera-Nieto, Samar Safi-Harb, Shigeo S. Kimura, Eduardo de la Fuente, Sabrina Casanova, Kaya Mori, Xiaojie Wang, Sei Kato, Dai Tateishi, Hideki Uchiyama, Takaaki Tanaka, Hiroyuki Uchida, Shun Inoue, Dezhi Huang, Marianne Lemoine-Goumard, Daiki Miura, Shoji Ogawa, Shogo B. Kobayashi, Chris Done, Maxime Parra, María Díaz Trigo, Teo Muñoz-Darias, Montserrat Armas Padilla, Ryota Tomaru, Yoshihiro Ueda</strong></p><p>A recent report on the detection of very-high-energy gamma rays from V4641 Sagittarii (V4641 Sgr) up to <del>0.8 peta-electronvolt has made it the second confirmed “PeVatron” microquasar. Here we report on the observation of V4641 Sgr with X-Ray Imaging and Spectroscopy Mission (XRISM) in September 2024. Thanks to the large field of view and low background, the CCD imager Xtend successfully detected for the first time X-ray extended emission around V4641 Sgr with a significance of &gt; 4.5 sigma and &gt; 10 sigma based on our imaging and spectral analysis, respectively. The spatial extent is estimated to have a radius of $7 \pm 3$ arcmin ($13 \pm 5$ pc at a distance of 6.2 kpc) assuming a Gaussian-like radial distribution, which suggests that the particle acceleration site is within ~10 pc of the microquasar. If the X-ray morphology traces the diffusion of accelerated electrons, this spatial extent can be explained by either an enhanced magnetic field (</del>80 uG) or a suppressed diffusion coefficient (~$10^{27}$ cm$^2$ s$^{-1}$ at 100 TeV). The integrated X-ray flux, (4-6)$\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ (2-10 keV), would require a magnetic field strength higher than the galactic mean (&gt; 8 uG) if the diffuse X-ray emission originates from synchrotron radiation and the gamma-ray emission is predominantly hadronic. If the X-rays are of thermal origin, the measured extension, temperature, and plasma density can be explained by a jet with a luminosity of ~$2\times 10^{39}$ erg s$^{-1}$, which is comparable to the Eddington luminosity of this system.</p><blockquote><p>最近一份关于从V4641人马座（V4641 Sgr）检测到超高能伽马射线的报告，能量高达<del>0.8拍电子伏特，使其成为第二个确认的“拍电子伏特加速器”微类星。这里我们报告了2024年9月使用X射线成像和光谱任务（XRISM）对V4641 Sgr的观察结果。由于XRISM具有大视场和低背景的特点，CCD成像仪Xtend首次成功检测到V4641 Sgr周围的X射线扩展发射，其显著性大于4.5 sigma和基于我们的成像和光谱分析的大于10 sigma。其空间范围在假设高斯径向分布的情况下，估计半径为$7 \pm 3$角分（在距离6.2 kpc的情况下为$13 \pm 5$ pc）。这表明粒子加速部位位于微类星附近约10 pc处。如果X射线的形态追踪了加速电子的扩散，那么这个空间范围可以由增强的磁场（约80 uG）或抑制的扩散系数（在100 TeV时约为$10^{27}$ cm$^2$ s$^{-1}$）来解释。X射线的积分流量为（4-6）× 10$^{-12}$ erg s$^{-1}$ cm$^{-2}$（2-10 keV），如果漫射X射线发射来自同步辐射且伽马射线发射主要是强子过程，那么这将要求磁场强度高于银河系的平均值（&gt; 8 uG）。如果X射线是热起源的，所测得的扩展范围、温度和等离子体密度可以用亮度约为</del> $2\times 10^{39}$ erg s$^{-1}$的喷流来解释，这与该系统的爱丁顿亮度相当。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08089v1">PDF</a> 9 pages, 5 figures, accepted for publication in ApJL</p><p><strong>Summary</strong><br>V4641 Sagittarii（V4641 Sgr）通过XRISM观测发现X射线扩展发射，空间范围暗示粒子加速位点距离微类星体约10秒差距离。研究发现，磁场的增强或扩散系数的抑制都能解释这一空间范围，且X射线形态可能追踪加速电子的扩散路径。此外，探讨了同步辐射和质子主导的伽马射线排放的情境，若同步辐射和伽马射线主要为热起源，则可以解释测量得到的延伸、温度和等离子体密度是由射流产生，其光度与系统的爱丁顿光度相当。该报告确定了V4641 Sgr是一个新的PeVatron微类星体。</p><p><strong>Key Takeaways</strong></p><ul><li><p>V4641 Sagittarii（V4641 Sgr）被确认为第二个已知的PeVatron微类星体。</p></li><li><p>XRISM成功检测到V4641 Sgr周围的X射线扩展发射，显示出其显著的辐射特征。</p></li><li><p>观测结果显示粒子加速位点距离微类星体约10秒差距离。</p></li><li><p>X射线形态可能追踪加速电子的扩散路径。</p></li><li><p>报告探讨了同步辐射和质子主导的伽马射线排放的不同解释。</p></li><li><p>若X射线为同步辐射起源，则要求磁场强度高于银河系平均值。</p><pre><code>          HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​<br>点此查看论文截图</p></li></ul><h2 id="How-to-select-slices-for-annotation-to-train-best-performing-deep-learning-segmentation-models-for-cross-sectional-medical-images"><a href="#How-to-select-slices-for-annotation-to-train-best-performing-deep-learning-segmentation-models-for-cross-sectional-medical-images" class="headerlink" title="How to select slices for annotation to train best-performing deep   learning segmentation models for cross-sectional medical images?"></a>How to select slices for annotation to train best-performing deep learning segmentation models for cross-sectional medical images?</h2><p><strong>Authors:Yixin Zhang, Kevin Kramer, Maciej A. Mazurowski</strong></p><p>Automated segmentation of medical images highly depends on the availability of accurate manual image annotations. Such annotations are very time-consuming and costly to generate, and often require specialized expertise, particularly for cross-sectional images which contain many slices for each patient. It is crucial to ensure the best use of annotation resources. In this paper, we systematically answer the question of how to select slices of cross-sectional medical images in order to maximize performance of the resulting deep learning segmentation models. We conducted experiments on 4 medical imaging segmentation tasks with varying annotation budgets, numbers of annotated cases, numbers of annotated slices per volume, slice selection techniques, and mask interpolations. We found that: 1) It is almost always preferable to annotate fewer slices per volume and more volumes given an annotation budget. 2) Selecting slices for annotation by unsupervised active learning (UAL) is not superior to selecting slices randomly or at fixed intervals, provided that each volume is allocated the same number of annotated slices. 3) Interpolating masks between annotated slices rarely enhances model performance, with exceptions of some specific configuration for 3D models.</p><blockquote><p>医学图像自动化分割在很大程度上依赖于准确的手动图像注释的可用性。这些注释的生成非常耗时且成本高昂，通常需要专业经验，特别是对于包含多个切片的横截面图像。确保最佳使用注释资源至关重要。在本文中，我们系统地回答了如何选择横截面医学图像的切片，以最大化深度学习分割模型的性能。我们在4个医学影像分割任务上进行了实验，涉及不同的注释预算、注释案例数量、每个卷中注释的切片数量、切片选择技术和掩膜插值。我们发现：1）在给定注释预算的情况下，几乎总是建议每个卷注释较少的切片并增加更多的卷。2）通过无监督主动学习（UAL）选择切片进行注释并不优于随机选择切片或在固定间隔选择切片，只要每个卷分配相同数量的注释切片即可。3）在标注切片之间进行掩膜插值很少能提高模型性能，但对于某些特定配置的3D模型可能会有例外。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08081v1">PDF</a></p><p><strong>Summary</strong><br>医学图像自动分割很大程度上依赖于准确的手动图像标注。由于需要消耗大量时间和成本，特别是在包含多个切片的横截面图像上需要专业标注人员，因此合理利用标注资源至关重要。本文系统地探讨了如何选择横截面医学图像的切片，以最大化深度学习分割模型的性能。通过实验，发现：在有限的标注预算下，建议每个体积标注的切片数量少但标注的体积数量多；采用无监督主动学习的切片标注方式并不优于随机或固定间隔的切片选择方式，只要每个体积分配的标注切片数量相同；在标注切片之间插值生成掩膜对模型性能的提升有限，但对于某些特定配置的3D模型可能会有例外。</p><p><strong>Key Takeaways</strong></p><ol><li><p>在有限的标注预算下，应优先考虑增加标注体积的数量而非每个体积中标注的切片数量。</p></li><li><p>采用无监督主动学习的切片标注方式未必优越，关键在于确保每个体积获得相同数量的标注切片。</p></li><li><p>通过插值生成掩膜对模型性能的提升并不显著，仅在特定配置的3D模型中可能有所助益。</p></li><li><p>横截面图像中的多切片特性对深度学习分割模型的性能具有重要影响。</p></li><li><p>实验表明，不同标注预算、标注案例数量、每体积标注切片数量、切片选择技术以及掩膜插值方法都会影响模型的性能。</p></li><li><p>选择切片进行标注时，除了考虑图像本身的特征外，还需综合考虑各种实验因素以达到最佳效果。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="BSAFusion-A-Bidirectional-Stepwise-Feature-Alignment-Network-for-Unaligned-Medical-Image-Fusion"><a href="#BSAFusion-A-Bidirectional-Stepwise-Feature-Alignment-Network-for-Unaligned-Medical-Image-Fusion" class="headerlink" title="BSAFusion: A Bidirectional Stepwise Feature Alignment Network for   Unaligned Medical Image Fusion"></a>BSAFusion: A Bidirectional Stepwise Feature Alignment Network for Unaligned Medical Image Fusion</h2><p><strong>Authors:Huafeng Li, Dayong Su, Qing Cai, Yafei Zhang</strong></p><p>If unaligned multimodal medical images can be simultaneously aligned and fused using a single-stage approach within a unified processing framework, it will not only achieve mutual promotion of dual tasks but also help reduce the complexity of the model. However, the design of this model faces the challenge of incompatible requirements for feature fusion and alignment; specifically, feature alignment requires consistency among corresponding features, whereas feature fusion requires the features to be complementary to each other. To address this challenge, this paper proposes an unaligned medical image fusion method called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F) strategy. To reduce the negative impact of modality differences on cross-modal feature matching, we incorporate the Modal Discrepancy-Free Feature Representation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature Representation Head (MFRH) to integrate the global information of the input image. By injecting the information contained in MFRH of the current image into other modality images, it effectively reduces the impact of modality differences on feature alignment while preserving the complementary information carried by different images. In terms of feature alignment, BSFA-F employs a bidirectional stepwise alignment deformation field prediction strategy based on the path independence of vector displacement between two points. This strategy solves the problem of large spans and inaccurate deformation field prediction in single-step alignment. Finally, Multi-Modal Feature Fusion block achieves the fusion of aligned features. The experimental results across multiple datasets demonstrate the effectiveness of our method. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/slrl123/BSAFusion">https://github.com/slrl123/BSAFusion</a>.</p><blockquote><p>如果能够在统一处理框架内采用单阶段方法同时对非对齐的多模式医学图像进行对齐和融合，不仅可以实现双重任务的相互促进，还有助于降低模型的复杂性。然而，该模型的设计面临着特征融合与对齐要求不兼容的挑战；具体而言，特征对齐要求相应特征之间的一致性，而特征融合则要求特征彼此互补。为了解决这一挑战，本文提出了一种称为双向逐步特征对齐与融合（BSFA-F）策略的非对齐医学图像融合方法。为了减少模态差异对跨模态特征匹配的负面影响，我们将模态差异无关特征表示（MDF-FR）方法纳入BSFA-F。MDF-FR利用模态特征表示头（MFRH）来整合输入图像的全局信息。通过将当前图像的MFRH中所包含的信息注入到其他模态图像中，在保留不同图像所携带的互补信息的同时，有效减少了模态差异对特征对齐的影响。在特征对齐方面，BSFA-F采用了一种基于两点间矢量位移路径独立性的双向逐步对齐变形场预测策略。该策略解决了单步对齐中存在的大跨度和不准确的变形场预测问题。最后，多模态特征融合模块实现了对齐特征的融合。在多个数据集上的实验结果证明了我们的方法的有效性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/slrl123/BSAFusion%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/slrl123/BSAFusion获取。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08050v1">PDF</a> Accepted by AAAI2025</p><p><strong>Summary</strong><br>多模态医学图像融合是医学图像领域的核心课题。为了改善先前的方法中的限制和挑战，本研究提出了双向分步特征对齐和融合（BSFA-F）策略，该策略通过双向分步对齐策略解决模态差异带来的问题，并通过多模态特征融合模块实现特征融合。实验结果证明了该方法的有效性。更多细节可通过访问代码库深入了解。</p><p><strong>Key Takeaways</strong></p><ul><li><p>多模态医学图像融合旨在解决医学图像对齐问题，提升诊疗效率。</p></li><li><p>面临的主要挑战在于特征融合与对齐之间的不兼容要求，要求对应特征的一致性以及对特征的互补性需求。</p></li><li><p>研究提出了一种新的融合策略，即双向分步特征对齐和融合（BSFA-F）。</p></li><li><p>该策略使用模态差异无关的特征表示（MDF-FR）方法，以减小不同模态间的差异对特征匹配的负面影响。</p></li><li><p>通过整合输入图像的全局信息来创建模态特征表示头（MFRH），进而提升特征对齐的准确性并保留不同图像的互补信息。</p><pre><code>          HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​<br>点此查看论文截图</p></li></ul><h2 id="Graph-convolutional-networks-enable-fast-hemorrhagic-stroke-monitoring-with-electrical-impedance-tomography"><a href="#Graph-convolutional-networks-enable-fast-hemorrhagic-stroke-monitoring-with-electrical-impedance-tomography" class="headerlink" title="Graph convolutional networks enable fast hemorrhagic stroke monitoring   with electrical impedance tomography"></a>Graph convolutional networks enable fast hemorrhagic stroke monitoring with electrical impedance tomography</h2><p><strong>Authors:J. Toivanen, V. Kolehmainen, A. Paldanius, A. Hänninen, A. Hauptmann, S. J. Hamilton</strong></p><p>Objective: To develop a fast image reconstruction method for stroke monitoring with electrical impedance tomography with image quality comparable to computationally expensive nonlinear model-based methods. Methods: A post-processing approach with graph convolutional networks is employed. Utilizing the flexibility of the graph setting, a graph U-net is trained on linear difference reconstructions from 2D simulated stroke data and applied to fully 3D images from realistic simulated and experimental data. An additional network, trained on 3D vs. 2D images, is also considered for comparison. Results: Post-processing the linear difference reconstructions through the graph U-net significantly improved the image quality, resulting in images comparable to, or better than, the time-intensive nonlinear reconstruction method (a few minutes vs. several hours). Conclusion: Pairing a fast reconstruction method, such as linear difference imaging, with post-processing through a graph U-net provided significant improvements, at a negligible computational cost. Training in the graph framework vs classic pixel-based setting (CNN) allowed the ability to train on 2D cross-sectional images and process 3D volumes providing a nearly 50x savings in data simulation costs with no noticeable loss in quality. Significance: The proposed approach of post-processing a linear difference reconstruction with the graph U-net could be a feasible approach for on-line monitoring of hemorrhagic stroke.</p><blockquote><p>目标：开发一种基于电阻抗成像技术的快速图像重建方法，用于中风监测，其图像质量可与计算密集型的非线性模型方法相媲美。方法：采用图卷积网络的后处理方法。利用图设置的灵活性，对来自二维模拟中风数据的线性差分重建进行图U-net训练，并应用于来自真实模拟和实验数据的完全三维图像。为了进行比较，还考虑了基于三维与二维图像训练的附加网络。结果：通过图U-net对线性差分重建进行后处理显著提高了图像质量，产生了与耗时非线性重建方法相当甚至更好的图像（几分钟与数小时）。结论：将快速重建方法（如线性差分成像）与通过图U-net进行后处理相结合，在可忽略的计算成本下取得了显著的改进。与经典像素基础设置（CNN）相比，在图形框架中进行训练能够在二维横截面图像上训练并在三维体积上进行处理，从而在不影响质量的情况下实现了近50倍的数据模拟成本节约。意义：所提出的利用图U-net对线性差分重建进行后处理的方法可能成为在线监测出血性中风的可行方法。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07888v1">PDF</a> 11 pages, 8 figures</p><p><strong>Summary</strong><br>采用图卷积网络后处理的方法，结合线性差分重建技术，实现了快速且图像质量较高的中风监测图像重建。通过训练图U-net网络处理二维模拟中风数据，并应用于全三维图像，显著提高图像质量，与耗时较长的非线性重建方法相比具有显著优势。</p><p><strong>Key Takeaways</strong></p><ol><li><p>研究目标：开发一种快速图像重建方法，用于中风监测的电阻抗层析成像，其图像质量可与计算昂贵的非线性模型方法相�� 媲美。</p></li><li><p>方法：采用具有图设置灵活性的图卷积网络后处理策略。训练图U-net网络处理二维模拟中风数据并应用于全三维图像。</p></li><li><p>结果：通过图U-net后处理线性差分重建，显著提高了图像质量，得到的图像质量可与或优于耗时较长的非线性重建方法。</p></li><li><p>结论：将快速重建方法（如线性差分成像）与图U-net后处理相结合，在几乎不增加计算成本的情况下，显著提高了图像质量。</p></li><li><p>训练方式：在图形框架中的训练相对于传统的像素基础设置（CNN），能够在二维横截面图像上训练并处理三维体积，节省了近50倍的数据模拟成本，且质量无明显损失。</p></li><li><p>创新性：提出的对线性差分重建进行图U-net后处理的方法，可能是对在线监测出血性中风的一种可行方法。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder"><a href="#XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder" class="headerlink" title="XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder"></a>XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder</h2><p><strong>Authors:Shenghao Zhu, Yifei Chen, Shuo Jiang, Weihong Chen, Chang Liu, Yuanhan Wang, Xu Chen, Yifan Ke, Feiwei Qin, Zhu Zhu, Changmiao Wang</strong></p><p>Neurogliomas are among the most aggressive forms of cancer, presenting considerable challenges in both treatment and monitoring due to their unpredictable biological behavior. Magnetic resonance imaging (MRI) is currently the preferred method for diagnosing and monitoring gliomas. However, the lack of specific imaging techniques often compromises the accuracy of tumor segmentation during the imaging process. To address this issue, we introduce the XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder framework with the Vision XLSTM module to reconstruct missing MRI modalities. By deeply fusing spatial and temporal features, it enhances tumor segmentation performance. The key innovation of our approach is the Self-Attention Variational Encoder (SAVE) module, which improves the integration of modal features. Additionally, it optimizes the interaction of features between segmentation and reconstruction tasks through the Squeeze-Fusion-Excitation Cross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset demonstrate that our model significantly outperforms existing advanced methods in handling cases where modalities are missing. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED">https://github.com/Quanato607/XLSTM-HVED</a>.</p><blockquote><p>神经胶质瘤是最具侵袭性的癌症形式之一，由于其不可预测的生物行为，为治疗和监测带来了相当大的挑战。目前，磁共振成像（MRI）是诊断和治疗胶质瘤的首选方法。然而，缺乏特定的成像技术往往会影响成像过程中肿瘤分割的准确性。为了解决这一问题，我们引入了XLSTM-HVED模型。该模型结合了异模式编码器-解码器框架和Vision XLSTM模块，以重建缺失的MRI模式。通过深度融合空间和时间特征，提高了肿瘤分割的性能。我们的方法的关键创新在于自注意力变分编码器（SAVE）模块，它改进了模式特征的融合。此外，它通过挤压-融合-兴奋交叉意识（SFECA）模块优化了分割和重建任务之间的特征交互。我们使用BraTS 2024数据集进行的实验表明，我们的模型在处理缺失模式的情况下显著优于现有高级方法。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Quanato607/XLSTM-HVED中找到。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07804v1">PDF</a> 5 pages, 2 figures</p><p><strong>Summary</strong></p><p>本文介绍了神经胶质瘤在治疗和监测方面面临的挑战，尤其是其不可预测的生物行为。为解决磁共振成像在诊断与监测神经胶质瘤时缺乏特定成像技术而导致肿瘤分割准确性受限的问题，引入了XLSTM-HVED模型。该模型结合异构模态编码器解码器框架与Vision XLSTM模块，重建缺失的MRI模态，通过深度融合时空特征提升肿瘤分割性能。模型的关键创新在于自注意力变分编码器（SAVE）模块，它改进了模态特征的融合。同时，通过Squeeze-Fusion-Excitation Cross Awareness（SFECA）模块优化分割与重建任务之间特征的交互。使用BraTS 2024数据集的实验表明，该模型在处理缺失模态的情况下显著优于现有高级方法。</p><p><strong>Key Takeaways</strong></p><ol><li><p>神经胶质瘤是极具侵略性的癌症，其治疗与监测面临挑战，尤其是由于其生物行为的不可预测性。</p></li><li><p>磁共振成像（MRI）是目前诊断与监测神经胶质瘤的首选方法，但缺乏特定的成像技术会影响肿瘤分割的准确性。</p></li><li><p>引入的XLSTM-HVED模型通过结合异构模态编码器解码器框架与Vision XLSTM模块来解决这一问题。</p></li><li><p>模型采用自注意力变分编码器（SAVE）模块来提升模态特征的融合效果。</p></li><li><p>SFECA模块用于优化分割与重建任务之间特征的交互。</p></li><li><p>使用BraTS 2024数据集的实验结果显示，该模型在处理缺失模态的情况时表现优于现有方法。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Image-Retrieval-with-Intra-Sweep-Representation-Learning-for-Neck-Ultrasound-Scanning-Guidance"><a href="#Image-Retrieval-with-Intra-Sweep-Representation-Learning-for-Neck-Ultrasound-Scanning-Guidance" class="headerlink" title="Image Retrieval with Intra-Sweep Representation Learning for Neck   Ultrasound Scanning Guidance"></a>Image Retrieval with Intra-Sweep Representation Learning for Neck Ultrasound Scanning Guidance</h2><p><strong>Authors:Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E. Salcudean</strong></p><p>Purpose: Intraoperative ultrasound (US) can enhance real-time visualization in transoral robotic surgery. The surgeon creates a mental map with a pre-operative scan. Then, a surgical assistant performs freehand US scanning during the surgery while the surgeon operates at the remote surgical console. Communicating the target scanning plane in the surgeon’s mental map is difficult. Automatic image retrieval can help match intraoperative images to preoperative scans, guiding the assistant to adjust the US probe toward the target plane. Methods: We propose a self-supervised contrastive learning approach to match intraoperative US views to a preoperative image database. We introduce a novel contrastive learning strategy that leverages intra-sweep similarity and US probe location to improve feature encoding. Additionally, our model incorporates a flexible threshold to reject unsatisfactory matches. Results: Our method achieves 92.30% retrieval accuracy on simulated data and outperforms state-of-the-art temporal-based contrastive learning approaches. Our ablation study demonstrates that using probe location in the optimization goal improves image representation, suggesting that semantic information can be extracted from probe location. We also present our approach on real patient data to show the feasibility of the proposed US probe localization system despite tissue deformation from tongue retraction. Conclusion: Our contrastive learning method, which utilizes intra-sweep similarity and US probe location, enhances US image representation learning. We also demonstrate the feasibility of using our image retrieval method to provide neck US localization on real patient US after tongue retraction.</p><blockquote><p>目的：术中超声（US）能够增强在经口机器人手术中的实时可视化效果。外科医生通过术前扫描创建一个心理地图。然后，手术助理在手术过程中进行自由手超声扫描，而外科医生则在远程手术台上进行操作。在外科医生的心中传达目标扫描平面是困难的。自动图像检索可以帮助将术中图像与术前扫描进行匹配，从而指导助理调整超声探头朝向目标平面。方法：我们提出了一种自监督对比学习的方法来匹配术中超声视图到术前图像数据库。我们引入了一种新的对比学习策略，利用扫描内相似性和超声探头位置来提高特征编码。此外，我们的模型采用灵活的阈值来拒绝不满意的匹配结果。结果：我们的方法在模拟数据上实现了92.30%的检索精度，并超越了基于时间的对比学习前沿方法。我们的消融研究证明，在优化目标中使用探头位置可以改善图像表示，这表明可以从探头位置提取语义信息。我们还展示了在真实患者数据上应用我们的方法，以展示尽管舌退缩导致组织变形，所提出的超声探头定位系统仍然可行。结论：我们的对比学习方法利用扫描内相似性和超声探头位置，增强了超声图像表示学习。我们还证明了使用我们的图像检索方法在真实患者超声中进行颈部超声定位是可行的，尽管存在舌退缩的情况。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07741v1">PDF</a> 12 pages, 5 figures</p><p><strong>Summary</strong></p><p>本文介绍了在经口机器人手术中，利用术中超声（US）增强实时可视化技术的过程。为提高手术效率，医生在术前进行扫描以建立心理地图，然后由手术助理在术中执行自由手超声扫描。为提高助理调整超声探头指向目标平面的准确性，文章提出一种自监督对比学习方法来匹配术中超声视图与术前图像数据库。新方法引入了一种新型对比学习策略，利用扫查内的相似性以及超声探头位置来改善特征编码，并通过灵活的阈值拒绝不满意的匹配。模拟数据显示新方法达到92.3%的检索准确率，优于基于时间的对比学习方法。此外，还展示了将探头位置纳入优化目标能够提高图像表现的研究结果。在真实患者数据中，即便存在舌后缩导致的组织变形问题，新方法的可行性依然得以证明。总体来说，文章创新地利用了对比学习方法提高超声图像学习的效果，为颈部超声定位提供了新的应用前景。</p><p><strong>Key Takeaways</strong></p><ol><li>术中超声在经口机器人手术中起到增强实时可视化的作用。</li><li>医生通过术前扫描建立心理地图，手术助理执行自由手超声扫描。</li><li>提出一种自监督对比学习方法来匹配术中超声视图与术前图像数据库。</li><li>利用扫查内的相似性和超声探头位置改善特征编码，提高图像检索准确率。</li><li>模拟数据显示新方法达到92.3%的检索准确率，优于基于时间的对比学习方法。</li><li>纳入探头位置信息能够提高图像表现，证明了语义信息可以从探头位置中提取。</li></ol><p><strong>论文及项目相关链接</strong></p><h2 id="SKIPNet-Spatial-Attention-Skip-Connections-for-Enhanced-Brain-Tumor-Classification"><a href="#SKIPNet-Spatial-Attention-Skip-Connections-for-Enhanced-Brain-Tumor-Classification" class="headerlink" title="SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor   Classification"></a>SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor Classification</h2><p><strong>Authors:Khush Mendiratta, Shweta Singh, Pratik Chattopadhyay</strong></p><p>Early detection of brain tumors through magnetic resonance imaging (MRI) is essential for timely treatment, yet access to diagnostic facilities remains limited in remote areas. Gliomas, the most common primary brain tumors, arise from the carcinogenesis of glial cells in the brain and spinal cord, with glioblastoma patients having a median survival time of less than 14 months. MRI serves as a non-invasive and effective method for tumor detection, but manual segmentation of brain MRI scans has traditionally been a labor-intensive task for neuroradiologists. Recent advancements in computer-aided design (CAD), machine learning (ML), and deep learning (DL) offer promising solutions for automating this process. This study proposes an automated deep learning model for brain tumor detection and classification using MRI data. The model, incorporating spatial attention, achieved 96.90% accuracy, enhancing the aggregation of contextual information for better pattern recognition. Experimental results demonstrate that the proposed approach outperforms baseline models, highlighting its robustness and potential for advancing automated MRI-based brain tumor analysis.</p><blockquote><p>早期通过磁共振成像（MRI）检测脑肿瘤对于及时治疗至关重要，但在偏远地区，诊断设施的获取仍然有限。胶质瘤是最常见的原发性脑肿瘤，起源于脑和脊髓胶质细胞的癌变，胶质母细胞瘤患者的中位生存时间不到14个月。MRI作为一种非侵入性的有效肿瘤检测方法，但手动分割脑部MRI扫描图像一直是神经放射科医生的一项劳动密集型任务。计算机辅助设计（CAD）、机器学习（ML）和深度学习（DL）的最新进展为自动化这一过程提供了有希望的解决方案。本研究提出了一种利用MRI数据自动检测与分类脑肿瘤的深度学习模型。该模型结合空间注意力机制，达到了96.90%的准确率，通过增强上下文信息的聚合，提高了模式识别能力。实验结果表明，该方法优于基线模型，突显了其稳健性和在推进基于MRI的自动化脑肿瘤分析方面的潜力。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07736v1">PDF</a></p><p><strong>Summary</strong></p><p>本文介绍了利用深度学习模型自动检测与分类脑肿瘤的研究。该研究采用包含空间注意力的深度学习模型，通过对MRI数据的分析，实现了高达96.90%的准确率。该模型能够自动完成肿瘤检测任务，提高了诊断效率，有望推动基于MRI的脑肿瘤分析进步。</p><p><strong>Key Takeaways</strong></p><ol><li><p>早期通过磁共振成像（MRI）检测脑肿瘤对及时治疗至关重要。</p></li><li><p>胶质瘤是最常见的原发性脑肿瘤，起源于大脑和脊髓的胶质细胞癌变。</p></li><li><p>计算机辅助设计（CAD）、机器学习（ML）和深度学习（DL）的进步为自动化检测MRI中的脑肿瘤提供了可能性。</p></li><li><p>该研究提出了一个结合空间注意力的深度学习模型用于自动检测与分类脑肿瘤。</p></li><li><p>该模型的准确率达到了96.90%，优于基线模型，展示了其在脑肿瘤分析中的稳健性和潜力。</p></li><li><p>通过模型实验验证表明其在聚集上下文信息、模式识别方面有显著提升效果。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model"><a href="#Motion-Artifact-Removal-in-Pixel-Frequency-Domain-via-Alternate-Masks-and-Diffusion-Model" class="headerlink" title="Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks   and Diffusion Model"></a>Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks and Diffusion Model</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Jianfeng Guo, Feng Yang, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p><p>Motion artifacts present in magnetic resonance imaging (MRI) can seriously interfere with clinical diagnosis. Removing motion artifacts is a straightforward solution and has been extensively studied. However, paired data are still heavily relied on in recent works and the perturbations in k-space (frequency domain) are not well considered, which limits their applications in the clinical field. To address these issues, we propose a novel unsupervised purification method which leverages pixel-frequency information of noisy MRI images to guide a pre-trained diffusion model to recover clean MRI images. Specifically, considering that motion artifacts are mainly concentrated in high-frequency components in k-space, we utilize the low-frequency components as the guide to ensure correct tissue textures. Additionally, given that high-frequency and pixel information are helpful for recovering shape and detail textures, we design alternate complementary masks to simultaneously destroy the artifact structure and exploit useful information. Quantitative experiments are performed on datasets from different tissues and show that our method achieves superior performance on several metrics. Qualitative evaluations with radiologists also show that our method provides better clinical feedback. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD">https://github.com/medcx/PFAD</a>.</p><blockquote><p>磁共振成像（MRI）中的运动伪影会严重干扰临床诊断。去除运动伪影是一种简单的解决方案，已经得到了广泛的研究。然而，最近的研究仍然严重依赖于配对数据，而k空间（频率域）中的扰动并未得到很好的考虑，这限制了其在临床领域的应用。为了解决这些问题，我们提出了一种新型的无监督净化方法，该方法利用带噪声的MRI图像的像素频率信息来指导预训练的扩散模型恢复清洁的MRI图像。具体来说，考虑到运动伪影主要集中在k空间的高频成分中，我们利用低频成分作为指导，以确保正确的纹理组织。此外，鉴于高频和像素信息有助于恢复形状和细节纹理，我们设计了交替的互补掩膜来同时破坏伪影结构并挖掘有用信息。在不同组织数据集上进行的定量实验表明，我们的方法在多个指标上实现了卓越的性能。与放射科医生进行的定性评估也表明，我们的方法提供了更好的临床反馈。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/medcx/PFAD%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/medcx/PFAD找到。]</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07590v2">PDF</a> 12 pages, 8 figures, AAAI 2025</p><p><strong>Summary</strong></p><p>本文提出一种新型无监督净化方法，利用噪声MRI图像的像素频率信息引导预训练的扩散模型恢复清洁MRI图像，以解决磁共振成像中的运动伪影问题。该方法利用k空间中的低频成分作为指导，确保正确的组织纹理，同时设计交替互补掩膜来同时破坏伪影结构并挖掘有用信息。实验证明该方法在多个指标上表现优异，并得到放射科的定性评估认可。</p><p><strong>Key Takeaways</strong></p><ol><li><p>磁共振成像中的运动伪影会干扰临床诊断，去除伪影是亟待解决的问题。</p></li><li><p>当前研究仍依赖配对数据，且忽略了k空间中的扰动，限制了临床应用。</p></li><li><p>本文提出一种新型无监督净化方法，利用像素频率信息去除MRI图像中的运动伪影。</p></li><li><p>方法集中在利用k空间中的高频和低频信息来恢复图像，确保正确的组织纹理。</p></li><li><p>设计交替互补掩膜以同时破坏伪影结构并挖掘有用信息。</p></li><li><p>实验证明该方法在多个数据集上的性能优于其他方法。</p></li></ol><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07754v1">PDF</a></p><h2 id="CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings"><a href="#CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings" class="headerlink" title="CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings"></a>CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings</h2><p><strong>Authors:Jiazuo Mu, Fuyi Yang, Yanshun Zhang, Junxiong Zhang, Yongjian Luo, Lan Xu, Yujiao Shi, Jingyi Yu, Yingliang Zhang</strong></p><p>We introduce CADSpotting, an efficient method for panoptic symbol spotting in large-scale architectural CAD drawings. Existing approaches struggle with the diversity of symbols, scale variations, and overlapping elements in CAD designs. CADSpotting overcomes these challenges by representing each primitive with dense points instead of a single primitive point, described by essential attributes like coordinates and color. Building upon a unified 3D point cloud model for joint semantic, instance, and panoptic segmentation, CADSpotting learns robust feature representations. To enable accurate segmentation in large, complex drawings, we further propose a novel Sliding Window Aggregation (SWA) technique, combining weighted voting and Non-Maximum Suppression (NMS). Moreover, we introduce a large-scale CAD dataset named LS-CAD to support our experiments. Each floorplan in LS-CAD has an average coverage of 1,000 square meter(versus 100 square meter in the existing dataset), providing a valuable benchmark for symbol spotting research. Experimental results on FloorPlanCAD and LS-CAD datasets demonstrate that CADSpotting outperforms existing methods, showcasing its robustness and scalability for real-world CAD applications.</p><blockquote><p>我们介绍了CADSpotting，这是一种在大型建筑CAD图纸中进行全景符号识别的高效方法。现有方法在处理CAD设计中的符号多样性、尺度变化和元素重叠方面存在困难。CADSpotting通过用密集点表示每个基本元素而不是单个基本点来克服这些挑战，这些密集点由坐标和颜色等基本属性描述。基于统一的3D点云模型进行联合语义、实例和全景分割，CADSpotting学习鲁棒的特征表示。为了在大规模复杂图纸上实现精确分割，我们进一步提出了一种新颖的滑动窗口聚合（SWA）技术，该技术结合了加权投票和非最大抑制（NMS）。此外，我们引入了一个大规模CAD数据集LS-CAD来支持我们的实验。LS-CAD中的每个平面图平均覆盖面积为1000平方米（而现有数据集中的面积为100平方米），为符号识别研究提供了一个有价值的基准。在FloorPlanCAD和LS-CAD数据集上的实验结果表明，CADSpotting优于现有方法，展示了其在现实世界的CAD应用中的稳健性和可扩展性。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07377v2">PDF</a> 16pages, 12 figures, Project web-page: <a target="_blank" rel="noopener" href="https://dgeneai.github.io/cadspotting-pages/">https://dgeneai.github.io/cadspotting-pages/</a></p><p><strong>Summary</strong></p><p>CADSpotting方法能够高效地进行大型建筑CAD图纸中的全景符号识别。相较于现有方法在处理符号多样性、尺度变化和元素重叠等挑战时的困难，CADSpotting通过密集的点表示每个基本元素，结合坐标和颜色等关键属性进行描述，实现了更稳健的特征表示。同时，采用统一的3D点云模型进行联合语义、实例和全景分割，并引入Sliding Window Aggregation（SWA）技术提高大型复杂图纸的分割精度。此外，建立了大规模CAD数据集LS-CAD，为相关研究提供了有价值的基准测试。实验结果证明，CADSpotting在FloorPlanCAD和LS-CAD数据集上的表现优于现有方法，展现出其在真实CAD应用中的稳健性和可扩展性。</p><p><strong>Key Takeaways</strong></p><ol><li><p>CADSpotting是一种用于大型建筑CAD图纸中的全景符号识别的高效方法。</p></li><li><p>现有方法在符号多样性、尺度变化和元素重叠方面存在挑战。</p></li><li><p>CADSpotting通过密集的点表示每个基本元素，结合坐标和颜色等属性进行描述。</p></li><li><p>采用统一的3D点云模型进行联合语义、实例和全景分割。</p></li><li><p>引入Sliding Window Aggregation（SWA）技术提高大型复杂图纸的分割精度。</p></li><li><p>建立了大规模CAD数据集LS-CAD，为符号识别研究提供基准测试。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="A-Generative-Victim-Model-for-Segmentation"><a href="#A-Generative-Victim-Model-for-Segmentation" class="headerlink" title="A Generative Victim Model for Segmentation"></a>A Generative Victim Model for Segmentation</h2><p><strong>Authors:Aixuan Li, Jing Zhang, Jiawei Shi, Yiran Zhong, Yuchao Dai</strong></p><p>We find that the well-trained victim models (VMs), against which the attacks are generated, serve as fundamental prerequisites for adversarial attacks, i.e. a segmentation VM is needed to generate attacks for segmentation. In this context, the victim model is assumed to be robust to achieve effective adversarial perturbation generation. Instead of focusing on improving the robustness of the task-specific victim models, we shift our attention to image generation. From an image generation perspective, we derive a novel VM for segmentation, aiming to generate adversarial perturbations for segmentation tasks without requiring models explicitly designed for image segmentation. Our approach to adversarial attack generation diverges from conventional white-box or black-box attacks, offering a fresh outlook on adversarial attack strategies. Experiments show that our attack method is able to generate effective adversarial attacks with good transferability.</p><blockquote><p>我们发现，针对生成的攻击，训练良好的目标模型（VMs）是对抗攻击的基本前提。也就是说，对于分割任务，需要分割VMs来生成攻击。在这种情况下，假设目标模型是健壮的，以实现有效的对抗性扰动生成。我们不再专注于提高特定任务的模型健壮性，而是将注意力转向图像生成。从图像生成的角度来看，我们为分割设计了一种新型VMs，旨在针对分割任务生成对抗性扰动，而无需专门设计用于图像分割的模型。我们对对抗性攻击生成的方法不同于传统的白盒或黑盒攻击，为对抗性攻击策略提供了全新的视角。实验表明，我们的攻击方法可以生成具有有效迁移性的对抗性攻击。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07274v1">PDF</a></p><p><strong>Summary</strong></p><p>本文探讨了针对医学图像分割的对抗攻击生成方法。研究发现，训练良好的受害者模型（VMs）是生成对抗攻击的基础前提，但提高特定任务的受害者模型鲁棒性并非最优策略。为此，研究团队从图像生成角度出发，推出新的分割VM，旨在生成针对分割任务的对抗扰动，无需专门设计图像分割模型。该研究提出的攻击生成策略不同于传统的白盒或黑盒攻击，为对抗攻击策略提供了全新视角，实验证明该方法能够生成具有优异迁移性的有效对抗攻击。</p><p><strong>Key Takeaways</strong></p><ol><li>受害者模型（VMs）在生成对抗攻击方面起基础作用，针对特定任务（如分割）需要相应的VM来生成对抗样本。</li><li>研究焦点从提高特定任务VM的鲁棒性转向图像生成。</li><li>提出新的VM用于医学图像分割，能在无需专门设计图像分割模型的情况下生成对抗扰动。</li><li>该研究提出的攻击生成策略与传统方法不同，为对抗攻击策略提供新的视角。</li><li>新方法生成的对抗攻击具有有效性和迁移性。</li><li>该研究为医学图像领域的对抗攻击提供了新的思路和方法。</li></ol><p><strong>Authors:Sejong Yang, Seoung Wug Oh, Yang Zhou, Seon Joo Kim</strong></p><h2 id="MPSI-Mamba-enhancement-model-for-pixel-wise-sequential-interaction-Image-Super-Resolution"><a href="#MPSI-Mamba-enhancement-model-for-pixel-wise-sequential-interaction-Image-Super-Resolution" class="headerlink" title="MPSI: Mamba enhancement model for pixel-wise sequential interaction   Image Super-Resolution"></a>MPSI: Mamba enhancement model for pixel-wise sequential interaction Image Super-Resolution</h2><p><strong>Authors:Yuchun He, Yuhan He</strong></p><p>Single image super-resolution (SR) has long posed a challenge in the field of computer vision. While the advent of deep learning has led to the emergence of numerous methods aimed at tackling this persistent issue, the current methodologies still encounter challenges in modeling long sequence information, leading to limitations in effectively capturing the global pixel interactions. To tackle this challenge and achieve superior SR outcomes, we propose the Mamba pixel-wise sequential interaction network (MPSI), aimed at enhancing the establishment of long-range connections of information, particularly focusing on pixel-wise sequential interaction. We propose the Channel-Mamba Block (CMB) to capture comprehensive pixel interaction information by effectively modeling long sequence information. Moreover, in the existing SR methodologies, there persists the issue of the neglect of features extracted by preceding layers, leading to the loss of valuable feature information. While certain existing models strive to preserve these features, they frequently encounter difficulty in establishing connections across all layers. To overcome this limitation, MPSI introduces the Mamba channel recursion module (MCRM), which maximizes the retention of valuable feature information from early layers, thereby facilitating the acquisition of pixel sequence interaction information from multiple-level layers. Through extensive experimentation, we demonstrate that MPSI outperforms existing super-resolution methods in terms of image reconstruction results, attaining state-of-the-art performance.</p><blockquote><p>单图像超分辨率（SR）一直是计算机视觉领域的一大挑战。虽然深度学习的出现导致了众多方法的出现，旨在解决这一持久性问题，但当前的方法在建模长序列信息方面仍面临挑战，导致在有效捕获全局像素交互方面存在局限性。为了应对这一挑战并实现更优越的超分辨率结果，我们提出了Mamba像素级顺序交互网络（MPSI），旨在增强长距离信息连接的建立，特别关注像素级的顺序交互。我们提出了通道母巴块（CMB），通过有效地对长序列信息进行建模，来捕捉全面的像素交互信息。此外，在现有的SR方法中，仍然存在忽视先前层次提取的特征的问题，导致有价值的特征信息丢失。虽然一些现有模型努力保留这些特征，但它们经常难以在所有层次之间建立联系。为了克服这一局限性，MPSI引入了Mamba通道递归模块（MCRM），最大限度地保留早期层次的宝贵特征信息，从而便于从多层次获取像素序列交互信息。通过大量实验，我们证明MPSI在图像重建结果方面优于现有的超分辨率方法，达到了最先进的性能。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07222v1">PDF</a></p><p><strong>Summary</strong></p><p>本文提出一种名为Mamba像素级序列交互网络（MPSI）的方法，旨在解决单图像超分辨率（SR）问题。该方法通过引入Channel-Mamba Block（CMB）和Mamba通道递归模块（MCRM）来增强长期信息连接，更有效地捕捉全局像素交互，从而实现了先进的SR性能。</p><p><strong>Key Takeaways</strong></p><ol><li><p>MPSI网络旨在解决单图像超分辨率（SR）中的长期信息建模挑战。</p></li><li><p>引入Channel-Mamba Block（CMB）以捕捉全面的像素交互信息。</p></li><li><p>现有SR方法忽视了先前层次提取的特征，导致特征信息丢失。</p></li><li><p>MPSI通过引入Mamba通道递归模块（MCRM）来克服这一局限性，保留早期层次的有价值特征信息。</p></li><li><p>MPSI方法能够从多层次获取像素序列交互信息。</p></li><li><p>实验证明，MPSI在图像重建结果上优于现有超分辨率方法。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Integrating-MedCLIP-and-Cross-Modal-Fusion-for-Automatic-Radiology-Report-Generation"><a href="#Integrating-MedCLIP-and-Cross-Modal-Fusion-for-Automatic-Radiology-Report-Generation" class="headerlink" title="Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology   Report Generation"></a>Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology Report Generation</h2><p><strong>Authors:Qianhao Han, Junyi Liu, Zengchang Qin, Zheng Zheng</strong></p><p>Automating radiology report generation can significantly reduce the workload of radiologists and enhance the accuracy, consistency, and efficiency of clinical documentation.We propose a novel cross-modal framework that uses MedCLIP as both a vision extractor and a retrieval mechanism to improve the process of medical report generation.By extracting retrieved report features and image features through an attention-based extract module, and integrating them with a fusion module, our method improves the coherence and clinical relevance of generated reports.Experimental results on the widely used IU-Xray dataset demonstrate the effectiveness of our approach, showing improvements over commonly used methods in both report quality and relevance.Additionally, ablation studies provide further validation of the framework, highlighting the importance of accurate report retrieval and feature integration in generating comprehensive medical reports.</p><blockquote><p>自动化放射学报告生成可以显著减少放射科医生的工作量，提高临床文档的准确性、一致性和效率。我们提出了一种新颖的跨模态框架，使用MedCLIP作为视觉提取器和检索机制，以改进医疗报告生成过程。通过注意力基础上的提取模块提取检索报告特征和图像特征，并将其与融合模块结合，我们的方法提高了生成报告的一致性和临床相关性。在广泛使用的IU-Xray数据集上的实验结果证明了我们的方法的有效性，在报告质量和相关性方面都优于常用方法。此外，消融研究进一步验证了该框架的有效性，强调了准确报告检索和特征融合在生成综合医疗报告中的重要性。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07141v1">PDF</a> Accepted in IEEE Big Data 2024</p><p><strong>Summary</strong></p><p>本文提出一种使用MedCLIP作为视觉提取器和检索机制的跨模态框架，用于改进医学报告生成过程。通过提取检索报告特征和图像特征，并使用融合模块进行整合，该框架提高了生成报告的连贯性和临床相关性。在广为人知的IU-Xray数据集上的实验结果验证了该方法的有效性，相较于常用方法，本文方法在报告质量和相关性方面表现出优越性。消融研究进一步验证了框架的重要性，强调准确报告检索和特征整合在生成全面的医学报告中的重要性。</p><p><strong>Key Takeaways</strong></p><ol><li><p>自动化放射学报告生成能显著减少放射科医生的工作量，提高临床文档记录的准确性、一致性和效率。</p></li><li><p>提出了一种新型的跨模态框架，使用MedCLIP作为视觉提取器和检索机制，改善医学报告生成流程。</p></li><li><p>通过提取和整合检索报告特征和图像特征，提高了报告的连贯性和临床相关性。</p></li><li><p>在IU-Xray数据集上的实验结果显示，该方法在报告质量和相关性方面优于常见方法。</p></li><li><p>消融研究验证了框架的有效性，强调准确报告检索和特征整合的重要性。</p></li><li><p>该方法有助于提升医学报告的全面性和质量，进而提升临床决策的效率与准确性。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Enhancing-LLMs-for-Impression-Generation-in-Radiology-Reports-through-a-Multi-Agent-System"><a href="#Enhancing-LLMs-for-Impression-Generation-in-Radiology-Reports-through-a-Multi-Agent-System" class="headerlink" title="Enhancing LLMs for Impression Generation in Radiology Reports through a   Multi-Agent System"></a>Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System</h2><p><strong>Authors:Fang Zeng, Zhiliang Lyu, Quanzheng Li, Xiang Li</strong></p><p>This study introduces “RadCouncil,” a multi-agent Large Language Model (LLM) framework designed to enhance the generation of impressions in radiology reports from the finding section. RadCouncil comprises three specialized agents: 1) a “Retrieval” Agent that identifies and retrieves similar reports from a vector database, 2) a “Radiologist” Agent that generates impressions based on the finding section of the given report plus the exemplar reports retrieved by the Retrieval Agent, and 3) a “Reviewer” Agent that evaluates the generated impressions and provides feedback. The performance of RadCouncil was evaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and qualitative criteria assessed by GPT-4, using chest X-ray as a case study. Experiment results show improvements in RadCouncil over the single-agent approach across multiple dimensions, including diagnostic accuracy, stylistic concordance, and clarity. This study highlights the potential of utilizing multiple interacting LLM agents, each with a dedicated task, to enhance performance in specialized medical tasks and the development of more robust and adaptable healthcare AI solutions.</p><blockquote><p>本研究介绍了”RadCouncil”，这是一个多代理大型语言模型（LLM）框架，旨在增强根据发现部分生成放射报告的印象。RadCouncil包含三个专业代理：1）“检索”代理，用于从向量数据库中识别和检索类似的报告；2）“放射科医生”代理，根据给定报告的发现部分以及检索到的示例报告生成印象；3）“审核员”代理，评估生成的印象并提供反馈。使用胸部X射线作为案例研究，通过定量指标（BLEU、ROUGE、BERTScore）和GPT-4评估的定性标准对RadCouncil的性能进行了评估。实验结果表明，RadCouncil在多方面的表现优于单代理方法，包括诊断准确性、风格一致性和清晰度。本研究强调了利用多个相互交互的LLM代理的潜力，每个代理都有特定的任务，以提高在专项医疗任务中的性能，并开发更强大、更适应的医疗保健人工智能解决方案。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06828v1">PDF</a></p><p><strong>Summary</strong></p><p>本研究介绍了名为RadCouncil的多智能体大型语言模型（LLM）框架，该框架旨在提高放射报告印象生成能力。RadCouncil包含三个专业智能体：检索智能体负责从向量数据库中检索相似报告，放射科医生智能体基于给定报告的发现部分及检索到的示例报告生成印象，评审智能体则评估生成的印象并提供反馈。本研究通过定量指标（BLEU、ROUGE、BERTScore）和定性标准（以GPT-4评估胸部X光为案例研究）对RadCouncil进行了性能评估。实验结果表明，相较于单智能体方法，RadCouncil在多个维度上有所提升，包括诊断准确性、风格一致性和清晰度。本研究突显了利用多个交互LLM智能体的潜力，每个智能体都有特定的任务，以提高在特定医疗任务中的性能，并为开发更强大、适应性更强的医疗人工智能解决方案奠定了基础。</p><p><strong>Key Takeaways</strong></p><ol><li><p>RadCouncil是一个多智能体LLM框架，旨在改进放射报告印象生成。</p></li><li><p>RadCouncil包含三个专业智能体：检索智能体、放射科医生智能体和评审智能体。</p></li><li><p>检索智能体负责从数据库检索相似报告。</p></li><li><p>放射科医生智能体基于发现部分和检索到的报告生成印象。</p></li><li><p>评审智能体评估生成的印象并提供反馈。</p></li><li><p>实验结果表明RadCouncil在诊断准确性、风格一致性和清晰度等方面优于单智能体方法。</p></li></ol><p>语音驱动的3D面部动画因其广泛的应用领域而备受关注。尽管最近在实现逼真的唇部运动方面取得了进展，但当前的方法无法捕捉通过语音传达的微妙情绪基调，并产生单调的面部运动。这些限制导致面部动画生硬且重复，降低了用户参与度并阻碍了其适用性。为了应对这些挑战，我们引入了DEEPTalk，这是一种从语音输入直接生成多样且情感丰富的3D面部表情的新方法。为实现这一点，我们首先训练DEE（动态情绪嵌入），它采用概率对比学习技术，为语音和面部运动打造联合情绪嵌入空间。这个概率框架捕捉了从语音和面部运动解释情绪的的不确定性，使得能够从其多维空间中得出情绪向量。此外，为了生成动态面部运动，我们设计了TH-VQVAE（时序分层VQ-VAE）作为表达性强且稳健的运动先验，以克服VAE和VQ-VAE的局限性。利用这些强大的先验知识，我们开发了DEEPTalk，一种非自回归地预测代码本索引以创建动态面部运动的说话人生成器，并结合了新颖的情绪一致性损失。在多个数据集上的广泛实验表明，我们的方法在创建多样、情感丰富的说话面孔方面非常有效，同时保持准确的唇同步。源代码将很快公开。</p><h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, Björn Ommer</strong></p><p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks.</p><p>{0}. 在生成模型中，两种范式在各种应用中受到了关注：基于下一个集合预测的掩码生成模型和基于下一个噪声预测的非自回归模型，例如扩散模型。在这项工作中，我们提出使用离散状态模型来连接它们，并探索它们在视觉领域的可扩展性。首先，我们在统一的设计空间中对两种类型的模型进行了逐步分析，包括时间步独立性、噪声时间表、温度、引导强度等，并以可扩展的方式进行了讨论。其次，我们将典型的判别任务（例如图像分割）重新定义为离散状态模型上的[MASK]标记的去掩码过程。这使得我们能够执行各种采样过程，包括通过仅训练一次来对联合分布进行建模的灵活条件采样。所有上述探索都引领我们构建了名为“离散插值”的框架，该框架使我们能够在各种基准测试中达到或保持与以前基于离散状态的方法相比的领先水平，如ImageNet256、MS COCO和视频数据集FaceForensics。总之，通过利用离散状态模型中的[MASK]，我们可以架起掩码生成和非自回归扩散模型之间的桥梁，以及生成和判别任务之间的桥梁。</p><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v2">PDF</a> Technical Report (WIP), Project Page(code, model, dataset): <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p><p><strong>Summary</strong><br>本研究提出使用离散状态模型连接基于下一步预测的掩码生成模型和基于下一步噪声预测的非自回归模型（如扩散模型），并在视觉领域探索其可扩展性。通过统一设计空间中的分步分析，研究实现了两种模型的跨类型可扩展性，包括时间步独立性、噪声调度、温度、指导强度等。此外，本研究将典型的判别任务（如图像分割）转化为离散状态模型上的去掩码过程，实现了多种采样过程，包括通过一次训练即可实现的灵活条件采样来模拟联合分布。最终构建了名为离散插值的新框架，在各种基准测试中达到了先进或具有竞争力的性能水平。</p><p><strong>Key Takeaways</strong></p><ul><li><p>研究提出了离散状态模型连接两种生成模型范式：基于下一步预测的掩码生成模型和基于噪声预测的非自回归模型（如扩散模型）。</p></li><li><p>在视觉领域进行了统一设计空间中的分步分析，实现了两种模型的跨类型可扩展性。</p></li><li><p>将典型的判别任务转化为离散状态模型上的去掩码过程，实现了多种采样过程。</p></li><li><p>通过灵活的条件采样，一次训练即可模拟联合分布。</p></li><li><p>构建的新框架“离散插值”在各种基准测试中表现先进或具有竞争力。</p><pre><code>          HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​<br>点此查看论文截图</p></li></ul><h2 id="Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation"><a href="#Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation" class="headerlink" title="Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation"></a>Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing Image Segmentation</h2><p><strong>Authors:Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao</strong></p><p>Fine-grained remote sensing image segmentation is essential for accurately identifying detailed objects in remote sensing images. Recently, vision transformer models (VTMs) pre-trained on large-scale datasets have demonstrated strong zero-shot generalization. However, directly applying them to specific tasks may lead to domain shift. We introduce a novel end-to-end learning paradigm combining knowledge guidance with domain refinement to enhance performance. We present two key components: the Feature Alignment Module (FAM) and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based backbone with those from the pretrained VTM’s encoder using channel transformation and spatial interpolation, and transfers knowledge via KL divergence and L2 normalization constraint. FMM further adapts the knowledge to the specific domain to address domain shift. We also introduce a fine-grained grass segmentation dataset and demonstrate, through experiments on two datasets, that our method achieves a significant improvement of 2.57 mIoU on the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the potential of combining knowledge transfer and domain adaptation to overcome domain-related challenges and data limitations. The project page is available at <a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/">https://xavierjiezou.github.io/KTDA/</a>.</p><blockquote><p>精细粒度遥感图像分割对于准确识别遥感图像中的详细物体至关重要。最近，在大型数据集上预训练的视觉变压器模型（VTM）表现出强大的零样本泛化能力。然而，直接将其应用于特定任务可能会导致域偏移。我们引入了一种结合知识引导和域精炼的新型端到端学习范式，以提高性能。我们提出了两个关键组件：特征对齐模块（FAM）和特征调制模块（FMM）。FAM通过对通道变换和空间插值，将对基于CNN的骨干网提取的特征与预训练VTM编码器的特征进行对齐，并通过KL散度和L2归一化约束进行知识转移。FMM进一步将知识适应到特定领域，以解决域偏移问题。我们还引入了一个精细粒度草地分割数据集，并通过两个数据集的实验证明，我们的方法在草地数据集上实现了2.57 mIoU的显著改进，在云数据集上实现了3.73 mIoU的改进。结果突出了结合知识转移和域适应的潜力，可以克服与域相关的挑战和数据限制。项目页面可通过[<a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/]%E8%AE%BF%E9%97%AE%E3%80%82">https://xavierjiezou.github.io/KTDA/]访问。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06664v2">PDF</a> 6 pages, 3 figures, 6 tables</p><p><strong>Summary</strong></p><p>针对遥感图像中的精细目标识别，本文提出了一种结合知识引导和域精化的新型端到端学习范式。通过特征对齐模块（FAM）和特征调制模块（FMM）来实现，有效提高了模型性能，减少了领域偏移问题。此外，本文还引入了精细草地分割数据集，并在两个数据集上的实验结果显示，该方法在草地数据集上的mIoU提高了2.57，在云数据集上提高了3.73。</p><p><strong>Key Takeaways</strong></p><ol><li><p>介绍了遥感图像中精细目标识别的关键挑战和重要性。</p></li><li><p>提出了一种新型端到端学习范式，结合了知识引导和域精化，以提高模型性能。</p></li><li><p>引入了特征对齐模块（FAM）和特征调制模块（FMM）作为该范式的核心组件。</p></li><li><p>FAM通过对齐CNN主干和预训练视觉转换模型（VTM）的编码器中的特征，并使用KL散度和L2归一化约束来传递知识。</p></li><li><p>FMM进一步调整知识以适应特定领域，解决领域偏移问题。</p></li><li><p>实验结果表明，该方法在精细草地分割数据集上的表现有显著改善，mIoU提高了2.57。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation"><a href="#Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation" class="headerlink" title="Semantic Consistency-Based Uncertainty Quantification for Factuality in   Radiology Report Generation"></a>Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation</h2><p><strong>Authors:Chenyu Wang, Weichao Zhou, Shantanu Ghosh, Kayhan Batmanghelich, Wenchao Li</strong></p><p>Radiology report generation (RRG) has shown great potential in assisting radiologists by automating the labor-intensive task of report writing. While recent advancements have improved the quality and coherence of generated reports, ensuring their factual correctness remains a critical challenge. Although generative medical Vision Large Language Models (VLLMs) have been proposed to address this issue, these models are prone to hallucinations and can produce inaccurate diagnostic information. To address these concerns, we introduce a novel Semantic Consistency-Based Uncertainty Quantification framework that provides both report-level and sentence-level uncertainties. Unlike existing approaches, our method does not require modifications to the underlying model or access to its inner state, such as output token logits, thus serving as a plug-and-play module that can be seamlessly integrated with state-of-the-art models. Extensive experiments demonstrate the efficacy of our method in detecting hallucinations and enhancing the factual accuracy of automatically generated radiology reports. By abstaining from high-uncertainty reports, our approach improves factuality scores by $10$%, achieved by rejecting $20$% of reports using the Radialog model on the MIMIC-CXR dataset. Furthermore, sentence-level uncertainty flags the lowest-precision sentence in each report with an $82.9$% success rate.</p><blockquote><p>医学影像报告生成（RRG）在协助放射科医生自动化书写报告这一劳动密集型任务方面展现出了巨大潜力。虽然近期的发展已经提高了生成报告的质量和连贯性，但确保报告的准确性仍是关键挑战。尽管已经提出了生成式医学影像视觉大型语言模型（VLLM）来解决这一问题，但这些模型容易出现幻觉，并可能产生不准确的诊断信息。为了解决这些担忧，我们引入了一种新型的基于语义一致性的不确定性量化框架，该框架能够提供报告级别和句子级别的不确定性。与现有方法不同的是，我们的方法不需要对底层模型进行修改，也不需要访问其内部状态，如输出令牌对数几率，从而可以作为一个即插即用的模块，无缝集成到最先进的模型中。大量实验证明，我们的方法在检测幻觉和提高自动生成的医学影像报告的准确性方面非常有效。通过避免高不确定性的报告，我们的方法在MIMIC-CXR数据集上使用Radialog模型将报告拒绝率提高了约百分之二十，并在这种情况下提高了百分之十的事实准确性。此外，句子级别的不确定性还能标记每个报告中最不准确的句子，成功率为百分之八十二点九。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04606v1">PDF</a></p><p><strong>Summary</strong></p><p>本文介绍了一种新型的语义一致性不确定性量化框架，用于提高放射学报告生成的准确性。该框架能够在不修改现有模型的前提下，提供报告级别和句子级别的不确定性，有效检测并增强自动生成报告的准确性。实验表明，该方法可以提高报告的准确性，同时能够标记出报告中的低精度句子。</p><p><strong>Key Takeaways</strong></p><ol><li><p>放射学报告生成（RRG）在协助放射科医生方面显示出巨大潜力，但确保报告的准确性仍是关键挑战。</p></li><li><p>生成式医疗视觉大型语言模型（VLLMs）虽然被提出用于解决这一问题，但容易出现信息错误。</p></li><li><p>介绍了一种新型的语义一致性不确定性量化框架，该框架提供报告级别和句子级别的不确定性。</p></li><li><p>该方法无需修改现有模型或访问其内部状态，可无缝集成到最新模型中。</p></li><li><p>实验证明，该方法能有效检测并增强自动生成报告的准确性。</p></li><li><p>通过拒绝高不确定性的报告，使用Radialog模型在MIMIC-CXR数据集上的事实性得分提高了10%。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images"><a href="#Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images" class="headerlink" title="Mask of truth: model sensitivity to unexpected regions of medical images"></a>Mask of truth: model sensitivity to unexpected regions of medical images</h2><p><strong>Authors:Théo Sourget, Michelle Hestbek-Møller, Amelia Jiménez-Sánchez, Jack Junchi Xu, Veronika Cheplygina</strong></p><p>The development of larger models for medical image analysis has led to increased performance. However, it also affected our ability to explain and validate model decisions. Models can use non-relevant parts of images, also called spurious correlations or shortcuts, to obtain high performance on benchmark datasets but fail in real-world scenarios. In this work, we challenge the capacity of convolutional neural networks (CNN) to classify chest X-rays and eye fundus images while masking out clinically relevant parts of the image. We show that all models trained on the PadChest dataset, irrespective of the masking strategy, are able to obtain an Area Under the Curve (AUC) above random. Moreover, the models trained on full images obtain good performance on images without the region of interest (ROI), even superior to the one obtained on images only containing the ROI. We also reveal a possible spurious correlation in the Chaksu dataset while the performances are more aligned with the expectation of an unbiased model. We go beyond the performance analysis with the usage of the explainability method SHAP and the analysis of embeddings. We asked a radiology resident to interpret chest X-rays under different masking to complement our findings with clinical knowledge. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> and <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a></p><blockquote><p>医学图像分析大型模型的发展提高了性能，但也影响了我们解释和验证模型决策的能力。模型可能会使用图像的非关键部分，也称为偶然关联或捷径，在基准数据集上获得高性能，但在真实世界场景中却会失败。在这项工作中，我们挑战卷积神经网络（CNN）对胸部X射线和眼底图像的分类能力，同时掩盖了图像中的临床关键部分。我们展示，在PadChest数据集上训练的所有模型，无论采用何种掩盖策略，都能获得超过随机的曲线下面积（AUC）。此外，在完整图像上训练的模型在没有感兴趣区域（ROI）的图像上也能获得良好的性能，甚至优于仅在包含ROI的图像上获得的性能。我们还揭示了Chaksu数据集中可能的偶然关联，同时性能更符合无偏见模型的预期。除了性能分析，我们还使用了SHAP解释方法和嵌入分析。我们请一位放射科医生在不同掩盖情况下解释胸部X射线，以补充我们的临床知识发现。我们的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> 和 <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a>。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04030v2">PDF</a></p><p><strong>Summary</strong><br>医学图像分析模型发展提高性能，但难以解释和验证决策。模型可能利用图像的非关键部分获得高基准数据集性能，但在真实场景中失败。本研究挑战了卷积神经网络（CNN）对胸部X光片和眼底图像的分类能力，同时掩盖了临床上重要的图像部分。研究显示，无论掩盖策略如何，在PadChest数据集上训练的模型都能获得超过随机的AUC值。此外，在不含感兴趣区域（ROI）的图像上训练的模型，性能甚至优于仅包含ROI的图像。本研究还揭示了Chaksu数据集中的潜在偶然关联，且模型性能更符合无偏见模型的预期。除了性能分析，本研究还使用了SHAP解释方法和嵌入分析。一名放射科住院医师被邀请对胸部X光片进行不同掩盖解读，以结合临床知识补充研究结论。</p><p><strong>Key Takeaways</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 医学图像分析模型性能提升的同时，解释和验证模型决策的困难增加。</span><br><span class="line">2. 模型可能利用非关键图像部分（即偶然关联或捷径）获得高基准数据集性能。</span><br><span class="line">3. 在掩盖临床相关图像部分的情况下，卷积神经网络对胸部X光片和眼底图像的分类能力受到挑战。</span><br><span class="line">4. 在PadChest数据集上训练的模型，无论掩盖策略如何，都能获得超过随机的AUC值。</span><br><span class="line">5. 模型在不含ROI的图像上的性能优于仅包含ROI的图像。</span><br><span class="line">6. 揭示了Chaksu数据集中的潜在偶然关联，模型性能更符合预期的无偏见模型表现。</span><br><span class="line">7. 除了性能分析，研究还采用了SHAP解释方法和嵌入分析，并邀请放射科住院医师对掩盖的胸部X光片进行解读，结合临床知识来补充研究结论。</span><br></pre></td></tr></table></figure><pre><code>            HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​<br>点此查看论文截图</p><h2 id="INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching"><a href="#INRetouch-Context-Aware-Implicit-Neural-Representation-for-Photography-Retouching" class="headerlink" title="INRetouch: Context Aware Implicit Neural Representation for Photography   Retouching"></a>INRetouch: Context Aware Implicit Neural Representation for Photography Retouching</h2><p><strong>Authors:Omar Elezabi, Marcos V. Conde, Zongwei Wu, Radu Timofte</strong></p><p>Professional photo editing remains challenging, requiring extensive knowledge of imaging pipelines and significant expertise. With the ubiquity of smartphone photography, there is an increasing demand for accessible yet sophisticated image editing solutions. While recent deep learning approaches, particularly style transfer methods, have attempted to automate this process, they often struggle with output fidelity, editing control, and complex retouching capabilities. We propose a novel retouch transfer approach that learns from professional edits through before-after image pairs, enabling precise replication of complex editing operations. To facilitate this research direction, we introduce a comprehensive Photo Retouching Dataset comprising 100,000 high-quality images edited using over 170 professional Adobe Lightroom presets. We develop a context-aware Implicit Neural Representation that learns to apply edits adaptively based on image content and context, requiring no pretraining and capable of learning from a single example. Our method extracts implicit transformations from reference edits and adaptively applies them to new images. Through extensive evaluation, we demonstrate that our approach not only surpasses existing methods in photo retouching but also enhances performance in related image reconstruction tasks like Gamut Mapping and Raw Reconstruction. By bridging the gap between professional editing capabilities and automated solutions, our work presents a significant step toward making sophisticated photo editing more accessible while maintaining high-fidelity results. Check the Project Page at <a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch">https://omaralezaby.github.io/inretouch</a> for more Results and information about Code and Dataset availability.</p><blockquote><p>专业照片编辑仍然具有挑战性，需要深入了解成像管道和专业知识。随着智能手机摄影的普及，对便捷而高级的图像编辑解决方案的需求不断增加。虽然最近的深度学习技术，特别是风格迁移方法，已经尝试自动化这个过程，但它们在输出保真度、编辑控制和复杂修饰功能方面经常遇到困难。我们提出了一种新颖的修饰迁移方法，它通过专业编辑前后的图像对来学习，能够精确复制复杂的编辑操作。为了推动这个研究方向，我们引入了一个全面的照片修饰数据集，包含使用超过170个专业Adobe Lightroom预设编辑的10万张高质量图像。我们开发了一种上下文感知的隐式神经表示方法，它可以根据图像内容和上下文学习自适应地应用编辑操作，无需预先训练，并且能够从单个示例中学习。我们的方法从参考编辑中提取隐式转换并自适应地应用于新图像。通过广泛评估，我们证明我们的方法不仅在照片修饰方面超越了现有方法，而且在相关图像重建任务（如色域映射和原始重建）方面的性能也得到了提高。通过缩小专业编辑能力和自动化解决方案之间的差距，我们的工作是在使高级照片编辑更加便捷的同时保持高保真结果方面迈出的重要一步。有关更多结果和关于代码和数据集可用性的信息，请访问<a target="_blank" rel="noopener" href="https://omaralezaby.github.io/inretouch%E6%9F%A5%E7%9C%8B%E9%A1%B9%E7%9B%AE%E9%A1%B5%E9%9D%A2%E3%80%82">https://omaralezaby.github.io/inretouch查看项目页面。</a></p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03848v2">PDF</a></p><p><strong>Summary</strong></p><p>本文提出一种新型的照片修饰转换方法，通过学习和复制专业编辑的操作，实现对复杂编辑操作的精确复制。为此，研究团队引入了一个包含10万张高质量图片的综合照片修饰数据集，使用超过170种Adobe Lightroom预设进行编辑。他们开发了一种基于图像内容和上下文的环境感知隐式神经表示方法，能够自适应地应用编辑操作，无需预先训练，并能从单个示例中学习。该方法从参考编辑中提取隐式转换并自适应地应用于新图像，在照片修饰和相关图像重建任务上表现出卓越性能。</p><p><strong>Key Takeaways</strong></p><ol><li><p>提出了一种新的照片修饰转换方法，可精确复制专业编辑的操作。</p></li><li><p>引入了包含10万张高质量图片的综合照片修饰数据集，使用Adobe Lightroom预设进行编辑。</p></li><li><p>开发了一种环境感知隐式神经表示方法，能自适应地应用编辑操作，无需预先训练。</p></li><li><p>方法能从单个示例中学习，提取参考编辑的隐式转换并应用于新图像。</p></li><li><p>在照片修饰和图像重建任务上表现出卓越性能，超越了现有方法。</p></li><li><p>架起了专业编辑能力和自动化解决方案之间的桥梁，使高级照片编辑更加易于访问同时保持高保真结果。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis"><a href="#INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis" class="headerlink" title="INSIGHT: Explainable Weakly-Supervised Medical Image Analysis"></a>INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</h2><p><strong>Authors:Wenbo Zhang, Junyu Chen, Christopher Kanan</strong></p><p>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: <a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/">https://zhangdylan83.github.io/ewsmia/</a></p><blockquote><p>由于体积扫描和全幻灯片病理图像（WSI）的尺寸较大，通常通过对局部区域提取嵌入然后进行聚合预测来处理。然而，当前的方法需要事后可视化技术（例如Grad-CAM），并且往往无法定位小而临床上重要的细节。为了解决这些局限性，我们引入了INSIGHT，这是一种新型的弱监督聚合器，它将热图生成作为归纳偏置。从预训练的特征图开始，INSIGHT使用具有小卷积核的检测模块来捕获细节，并使用具有更大感受野的上下文模块来抑制局部误报。生成的内部热图突出了诊断相关的区域。在CT和WSI基准测试中，INSIGHT取得了最先进的分类结果和高性能的弱标签语义分割效果。项目网站和代码可在：[<a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/]">https://zhangdylan83.github.io/ewsmia/]</a> 查看。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02012v2">PDF</a></p><p><strong>Summary</strong></p><p>本文介绍了处理大型医学图像的新方法——INSIGHT。它通过局部区域嵌入提取和聚合器预测，解决体积扫描和全幻灯片病理图像（WSIs）的处理问题。然而，现有方法需要事后可视化技术，并且难以定位小但临床重要的细节。为此，INSIGHT采用弱监督聚合器，结合热图生成作为归纳偏置。通过检测模块捕捉精细细节和上下文模块抑制局部误报，提高诊断相关区域的可见性。在CT和WSI基准测试中，INSIGHT实现了最先进的分类结果和高弱标签语义分割性能。</p><p><strong>Key Takeaways</strong></p><ol><li><p>INSIGHT是一种处理大型医学图像（如体积扫描和WSIs）的新方法。</p></li><li><p>现有方法需要复杂的事后可视化技术，难以定位关键细节。</p></li><li><p>INSIGHT采用弱监督聚合器，结合热图生成以提高诊断相关区域的可见性。</p></li><li><p>INSIGHT包含两个模块：检测模块用于捕捉精细细节，上下文模块用于抑制局部误报。</p></li><li><p>INSIGHT在CT和WSI基准测试中实现了最先进的分类性能。</p></li><li><p>INSIGHT还表现出高弱标签语义分割性能。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Evaluating-Automated-Radiology-Report-Quality-through-Fine-Grained-Phrasal-Grounding-of-Clinical-Findings"><a href="#Evaluating-Automated-Radiology-Report-Quality-through-Fine-Grained-Phrasal-Grounding-of-Clinical-Findings" class="headerlink" title="Evaluating Automated Radiology Report Quality through Fine-Grained   Phrasal Grounding of Clinical Findings"></a>Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings</h2><p><strong>Authors:Razi Mahmood, Pingkun Yan, Diego Machado Reyes, Ge Wang, Mannudeep K. Kalra, Parisa Kaviani, Joy T. Wu, Tanveer Syeda-Mahmood</strong></p><p>Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, and severity of a large number of clinical findings. We then performed phrasal grounding to localize their associated anatomical regions on chest radiograph images. The textual and visual measures are then combined to rate the quality of the generated reports. We present results that compare this evaluation metric with other textual metrics on a gold standard dataset derived from the MIMIC collection and show its robustness and sensitivity to factual errors.</p><blockquote><p>最近，已经开发了一些评估指标，仅使用文本信息，通过词汇、语义或临床命名实体识别方法，自动评估胸部放射学报告中生成式人工智能报告的质量。在本文中，我们开发了一种新的报告质量评估方法，首先提取精细的检查结果模式，捕捉大量临床检查结果的位置、侧别和严重程度。然后我们对短语进行定位，以确定其在胸部放射图像上的相关解剖区域。然后将文本和视觉度量结合起来，对生成的报告进行质量评分。我们在黄金标准数据集上比较了该评估指标与其他文本指标的表现，该数据集来自MIMIC数据集，展示了它对事实错误的稳健性和敏感性。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01031v2">PDF</a></p><p><strong>Summary</strong><br>本文提出一种基于文本信息评估生成式AI报告质量的新方法。该方法通过提取精细的病变模式，捕捉大量临床发现的部位、单侧性和严重程度，并在放射影像图像上定位相关的解剖区域。结合文本和视觉测量来评估报告质量，并在MIMIC数据集的金标准子集上验证了其与其他文本评估指标的稳健性和对事实错误的敏感性。</p><p><strong>Key Takeaways</strong></p><ol><li><p>该论文致力于利用生成式AI对胸部放射报告的自动质量评估。</p></li><li><p>通过提取精细病变模式捕捉临床发现的信息。</p></li><li><p>方法结合了文本和视觉测量来全面评估报告质量。</p></li><li><p>研究使用了MIMIC数据集的金标准子集进行实验验证。</p></li><li><p>该方法不仅考虑了报告的文字内容，还结合了影像图像进行phrasal grounding，以定位相关的解剖区域。</p></li><li><p>与其他文本评估指标相比，该评价模型展现出了稳健性和对事实错误的敏感性。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy"><a href="#Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy" class="headerlink" title="Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy"></a>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy</h2><p><strong>Authors:Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</strong></p><p>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large GIE dataset, called EndoExtend24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of GIE medical image diagnosis. Specifically, the EVA-02 model, which is based on the ViT architecture and trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762 and a balanced accuracy of 37.1% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched EndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.</p><blockquote><p>视频胶囊内镜技术为胃肠道内窥镜（GIE）诊断提供了一种非侵入性的方法来捕捉胃肠道的详细图像，从而实现了早期疾病的检测。然而，其在成像过程中产生的海量图像限制了其潜力，成像过程可能需要6-8小时，并可能产生高达100万张图像，因此需要进行自动化分析。此外，这些图像的差异性，加上专家标注的需求以及大规模高质量标注数据集的稀缺性，制约了当前医学图像分析模型的有效性。为了解决这一问题，我们引入了一个新的大型GIE数据集EndoExtend24，它是通过合并十个现有的公共和私有数据集创建的，确保了跨分割的患者数据完整性。EndoExtend24包含超过22万张标记图像以及动态类映射，允许在具有不同标记粒度的数据集上进行统一训练，支持多达123种不同的病理发现。此外，我们提出利用基于自监督的通用图像数据域自适应预训练的模型进行迁移学习的方法，以适应GIE医学图像诊断的任务。具体来说，EVA-02模型基于ViT架构构建，在ImageNet-22k数据集上进行遮罩图像建模训练（使用EVA-CLIP作为遮罩教师模型），然后在EndoExtend24数据集上进行预训练以实现域适应，并最终在Capsule Endoscopy 2024挑战赛数据集上进行训练。我们的模型表现出稳健的性能，在Capsule Endoscopy 2024挑战赛中获得了第三名。在测试集上，我们实现了宏观AUC为0.762和平衡精度为37.1%。这些结果强调了我们域自适应预训练方法和丰富的EndoExtend24数据集在推进胃肠道内窥镜诊断方面的有效性。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21302v4">PDF</a></p><p><strong>Summary</strong><br>视频胶囊内镜为胃肠道内镜（GIE）诊断提供了一种非侵入性的成像方法，能够捕捉胃肠道的详细图像，实现早期疾病检测。然而，由于成像过程中产生的图像数量庞大，以及图像间的差异性和缺乏大型高质量标签数据集，限制了其分析模型的效能。为解决这一问题，本研究引入了EndoExtend24大型GIE数据集，包含超过22.6万张标记图像，并支持多达123种不同病理发现的统一训练。同时，本研究还提出利用基于通用图像数据的自监督预训练模型进行域自适应预训练，以适应GIE医学图像诊断任务。最终，本研究开发的模型在Capsule Endoscopy 2024 Challenge中取得了第三名的好成绩，验证了域自适应预训练方法和EndoExtend24数据集的优越性。</p><p><strong>Key Takeaways</strong></p><ol><li><p>视频胶囊内镜为胃肠道内镜诊断提供了非侵入性的详细成像方法，有助于早期疾病检测。</p></li><li><p>巨大的图像数量和差异性对医学图像分析模型构成挑战。</p></li><li><p>EndoExtend24数据集通过合并多个公共和私有数据集，提供了大量标记图像以支持广泛的病理发现。</p></li><li><p>域自适应预训练有助于将通用图像数据预训练的模型适应到GIE医学图像诊断任务。</p></li><li><p>EVA-02模型基于ViT架构，利用ImageNet-22k进行自监督预训练，并通过域自适应在EndoExtend24数据集上进一步训练。</p></li><li><p>EVA-02模型在Capsule Endoscopy 2024 Challenge测试中表现优秀，获得第三名。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p><h2 id="The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI"><a href="#The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI" class="headerlink" title="The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI"></a>The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain Metastasis Segmentation on Pre-treatment MRI</h2><p><strong>Authors:Ahmed W. Moawad, Anastasia Janas, Ujjwal Baid, Divya Ramakrishnan, Rachit Saluja, Nader Ashraf, Nazanin Maleki, Leon Jekel, Nikolay Yordanov, Pascal Fehringer, Athanasios Gkampenis, Raisa Amiruddin, Amirreza Manteghinejad, Maruf Adewole, Jake Albrecht, Udunna Anazodo, Sanjay Aneja, Syed Muhammad Anwar, Timothy Bergquist, Veronica Chiang, Verena Chung, Gian Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Nastaran Khalili, Keyvan Farahani, Juan Eugenio Iglesias, Zhifan Jiang, Elaine Johanson, Anahita Fathi Kazerooni, Florian Kofler, Kiril Krantchev, Dominic LaBella, Koen Van Leemput, Hongwei Bran Li, Marius George Linguraru, Xinyang Liu, Zeke Meier, Bjoern H Menze, Harrison Moy, Klara Osenberg, Marie Piraud, Zachary Reitman, Russell Takeshi Shinohara, Chunhao Wang, Benedikt Wiestler, Walter Wiggins, Umber Shafique, Klara Willms, Arman Avesta, Khaled Bousabarah, Satrajit Chakrabarty, Nicolo Gennaro, Wolfgang Holler, Manpreet Kaur, Pamela LaMontagne, MingDe Lin, Jan Lost, Daniel S. Marcus, Ryan Maresca, Sarah Merkaj, Gabriel Cassinelli Pedersen, Marc von Reppert, Aristeidis Sotiras, Oleg Teytelboym, Niklas Tillmans, Malte Westerhoff, Ayda Youssef, Devon Godfrey, Scott Floyd, Andreas Rauschecker, Javier Villanueva-Meyer, Irada Pfluger, Jaeyoung Cho, Martin Bendszus, Gianluca Brugnara, Justin Cramer, Gloria J. Guzman Perez-Carillo, Derek R. Johnson, Anthony Kam, Benjamin Yin Ming Kwan, Lillian Lai, Neil U. Lall, Fatima Memon, Mark Krycia, Satya Narayana Patro, Bojan Petrovic, Tiffany Y. So, Gerard Thompson, Lei Wu, E. Brooke Schrickel, Anu Bansal, Frederik Barkhof, Cristina Besada, Sammy Chu, Jason Druzgal, Alexandru Dusoi, Luciano Farage, Fabricio Feltrin, Amy Fong, Steve H. Fung, R. Ian Gray, Ichiro Ikuta, Michael Iv, Alida A. Postma, Amit Mahajan, David Joyner, Chase Krumpelman, Laurent Letourneau-Guillon, Christie M. Lincoln, Mate E. Maros, Elka Miller, Fanny Moron, Esther A. Nimchinsky, Ozkan Ozsarlak, Uresh Patel, Saurabh Rohatgi, Atin Saha, Anousheh Sayah, Eric D. Schwartz, Robert Shih, Mark S. Shiroishi, Juan E. Small, Manoj Tanwar, Jewels Valerie, Brent D. Weinberg, Matthew L. White, Robert Young, Vahe M. Zohrabian, Aynur Azizova, Melanie Maria Theresa Bruseler, Mohanad Ghonim, Mohamed Ghonim, Abdullah Okar, Luca Pasquini, Yasaman Sharifi, Gagandeep Singh, Nico Sollmann, Theodora Soumala, Mahsa Taherzadeh, Philipp Vollmuth, Martha Foltyn-Dumitru, Ajay Malhotra, Aly H. Abayazeed, Francesco Dellepiane, Philipp Lohmann, Victor M. Perez-Garcia, Hesham Elhalawani, Maria Correia de Verdier, Sanaria Al-Rubaiey, Rui Duarte Armindo, Kholod Ashraf, Moamen M. Asla, Mohamed Badawy, Jeroen Bisschop, Nima Broomand Lomer, Jan Bukatz, Jim Chen, Petra Cimflova, Felix Corr, Alexis Crawley, Lisa Deptula, Tasneem Elakhdar, Islam H. Shawali, Shahriar Faghani, Alexandra Frick, Vaibhav Gulati, Muhammad Ammar Haider, Fatima Hierro, Rasmus Holmboe Dahl, Sarah Maria Jacobs, Kuang-chun Jim Hsieh, Sedat G. Kandemirli, Katharina Kersting, Laura Kida, Sofia Kollia, Ioannis Koukoulithras, Xiao Li, Ahmed Abouelatta, Aya Mansour, Ruxandra-Catrinel Maria-Zamfirescu, Marcela Marsiglia, Yohana Sarahi Mateo-Camacho, Mark McArthur, Olivia McDonnell, Maire McHugh, Mana Moassefi, Samah Mostafa Morsi, Alexander Munteanu, Khanak K. Nandolia, Syed Raza Naqvi, Yalda Nikanpour, Mostafa Alnoury, Abdullah Mohamed Aly Nouh, Francesca Pappafava, Markand D. Patel, Samantha Petrucci, Eric Rawie, Scott Raymond, Borna Roohani, Sadeq Sabouhi, Laura M. Sanchez-Garcia, Zoe Shaked, Pokhraj P. Suthar, Talissa Altes, Edvin Isufi, Yaseen Dhemesh, Jaime Gass, Jonathan Thacker, Abdul Rahman Tarabishy, Benjamin Turner, Sebastiano Vacca, George K. Vilanilam, Daniel Warren, David Weiss, Fikadu Worede, Sara Yousry, Wondwossen Lerebo, Alejandro Aristizabal, Alexandros Karargyris, Hasan Kassem, Sarthak Pati, Micah Sheller, Katherine E. Link, Evan Calabrese, Nourel hoda Tahon, Ayman Nada, Yuri S. Velichko, Spyridon Bakas, Jeffrey D. Rudie, Mariam Aboian</strong></p><p>The translation of AI-generated brain metastases (BM) segmentation into clinical practice relies heavily on diverse, high-quality annotated medical imaging datasets. The BraTS-METS 2023 challenge has gained momentum for testing and benchmarking algorithms using rigorously annotated internationally compiled real-world datasets. This study presents the results of the segmentation challenge and characterizes the challenging cases that impacted the performance of the winning algorithms. Untreated brain metastases on standard anatomic MRI sequences (T1, T2, FLAIR, T1PG) from eight contributed international datasets were annotated in stepwise method: published UNET algorithms, student, neuroradiologist, final approver neuroradiologist. Segmentations were ranked based on lesion-wise Dice and Hausdorff distance (HD95) scores. False positives (FP) and false negatives (FN) were rigorously penalized, receiving a score of 0 for Dice and a fixed penalty of 374 for HD95. Eight datasets comprising 1303 studies were annotated, with 402 studies (3076 lesions) released on Synapse as publicly available datasets to challenge competitors. Additionally, 31 studies (139 lesions) were held out for validation, and 59 studies (218 lesions) were used for testing. Segmentation accuracy was measured as rank across subjects, with the winning team achieving a LesionWise mean score of 7.9. Common errors among the leading teams included false negatives for small lesions and misregistration of masks in space.The BraTS-METS 2023 challenge successfully curated well-annotated, diverse datasets and identified common errors, facilitating the translation of BM segmentation across varied clinical environments and providing personalized volumetric reports to patients undergoing BM treatment.</p><blockquote><p>将AI生成的脑转移（BM）分段翻译应用于临床实践，很大程度上依赖于多样化、高质量标注的医学影像数据集。BraTS-METS 2023挑战赛通过使用严格标注的国际汇编真实世界数据集，测试并评估算法性能。本研究介绍了分割挑战的结果，并对影响获胜算法性能的挑战性病例进行了特征描述。在标准的解剖MRI序列（T1、T2、FLAIR、T1PG）上，对未治疗的脑转移病灶采用逐步方法进行标注：已发布的UNET算法、学生、神经放射学家、最终审批的神经放射学家。分割排名基于病灶级别的Dice和Hausdorff距离（HD95）得分。假阳性（FP）和假阴性（FN）受到严格惩罚，Dice得分为0，HD95的固定惩罚为374。共标注了包含1303项研究的八个数据集，其中402项研究（含3076个病灶）在Synapse上作为公开数据集发布，以挑战竞争对手。此外，还留出31项研究（含139个病灶）用于验证，另有59项研究（含218个病灶）用于测试。分割精度按受试者排名，冠军团队的LesionWise平均得分为7.9。领先团队常见的错误包括小病灶的假阴性以及口罩在空间上的错位。BraTS-METS 2023挑战赛成功整理出标注良好、多样化的数据集，并识别出常见错误，促进了BM分段的临床翻译，并为接受BM治疗的病人提供个性化的体积报告。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.00838v3">PDF</a></p><p><strong>Summary</strong></p><p>本文介绍了BraTS-METS 2023挑战赛的结果和特性，该挑战赛使用严格注释的国际真实世界数据集测试和评估算法在脑转移瘤分割上的表现。研究发布了一系列公开数据集，涵盖多个国际数据集，用于挑战竞争者。同时，通过严格的评估标准，识别出领先团队常见的错误，如小病灶的假阴性以及空间内遮罩的误配准。此挑战成功促进了脑转移瘤分割在临床环境中的转化，并为接受脑转移瘤治疗的患者提供个性化体积报告。</p><p><strong>Key Takeaways</strong></p><ol><li><p>BraTS-METS 2023挑战赛使用严格注释的国际真实世界数据集，推动AI在脑转移瘤分割方面的应用。</p></li><li><p>研究涉及多个国际数据集，包括未处理过的脑转移瘤标准解剖MRI序列数据。</p></li><li><p>分割结果的评估基于病灶级别的Dice和Hausdorff距离得分，严格惩罚假阳性和假阴性结果。</p></li><li><p>发布公开数据集以挑战其他竞争者，同时进行验证和测试数据集的划分。</p></li><li><p>领先的团队在分割过程中存在的常见错误包括小病灶的遗漏和遮罩的空间误配准。</p></li><li><p>BraTS-METS 2023挑战赛成功识别出算法在分割方面的常见挑战，有助于算法的优化和改进。</p><pre><code>         HTML
</code></pre><p>​<br>​<br>​<br>​<br>​<br>​</p></li></ol><p></p><p>点此查看论文截图</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明:</i></span> <span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"><span class="chip bg-color">医学图像</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;上一篇</div><div class="card"><a href="/Talk2Paper/Paper/2024-12-12/LLM/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" class="responsive-img" alt="LLM"> <span class="card-title">LLM</span></div></a><div class="card-content article-content"><div class="summary block-with-text">LLM 方向最新论文已更新，请持续关注 Update in 2024-12-12 Generative Semantic Communication Architectures, Technologies, and Applications</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-12</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/LLM/" class="post-category">LLM</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-12-10/TTS/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS"> <span class="card-title">TTS</span></div></a><div class="card-content article-content"><div class="summary block-with-text">TTS 方向最新论文已更新，请持续关注 Update in 2024-12-11 Towards Controllable Speech Synthesis in the Era of Large Language Models A Survey</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-11</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/TTS/" class="post-category">TTS</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/TTS/"><span class="chip bg-color">TTS</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">4896.5k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="本站已运行 "+c+" 天",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="本站已运行 "+l+" 年 "+c+" 天",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Σ(っ °Д °;)っ诶，页面崩溃了嘛？",clearTimeout(st)):(document.title="φ(゜▽゜*)♪咦，又好了！",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>