<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2024-12-12  Exploring What Why and How A Multifaceted Benchmark for Causation   Understanding of Video Anomaly">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ee80a22b3ad8a8487a83dd1551e22a2a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    21 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Exploring-What-Why-and-How-A-Multifaceted-Benchmark-for-Causation-Understanding-of-Video-Anomaly"><a href="#Exploring-What-Why-and-How-A-Multifaceted-Benchmark-for-Causation-Understanding-of-Video-Anomaly" class="headerlink" title="Exploring What Why and How: A Multifaceted Benchmark for Causation   Understanding of Video Anomaly"></a>Exploring What Why and How: A Multifaceted Benchmark for Causation   Understanding of Video Anomaly</h2><p><strong>Authors:Hang Du, Guoshun Nan, Jiawen Qian, Wangchenhui Wu, Wendi Deng, Hanqing Mu, Zhenyan Chen, Pengxuan Mao, Xiaofeng Tao, Jun Liu</strong></p>
<p>Recent advancements in video anomaly understanding (VAU) have opened the door to groundbreaking applications in various fields, such as traffic monitoring and industrial automation. While the current benchmarks in VAU predominantly emphasize the detection and localization of anomalies. Here, we endeavor to delve deeper into the practical aspects of VAU by addressing the essential questions: “what anomaly occurred?”, “why did it happen?”, and “how severe is this abnormal event?”. In pursuit of these answers, we introduce a comprehensive benchmark for Exploring the Causation of Video Anomalies (ECVA). Our benchmark is meticulously designed, with each video accompanied by detailed human annotations. Specifically, each instance of our ECVA involves three sets of human annotations to indicate “what”, “why” and “how” of an anomaly, including 1) anomaly type, start and end times, and event descriptions, 2) natural language explanations for the cause of an anomaly, and 3) free text reflecting the effect of the abnormality. Building upon this foundation, we propose a novel prompt-based methodology that serves as a baseline for tackling the intricate challenges posed by ECVA. We utilize “hard prompt” to guide the model to focus on the critical parts related to video anomaly segments, and “soft prompt” to establish temporal and spatial relationships within these anomaly segments. Furthermore, we propose AnomEval, a specialized evaluation metric crafted to align closely with human judgment criteria for ECVA. This metric leverages the unique features of the ECVA dataset to provide a more comprehensive and reliable assessment of various video large language models. We demonstrate the efficacy of our approach through rigorous experimental analysis and delineate possible avenues for further investigation into the comprehension of video anomaly causation. </p>
<blockquote>
<p>近期视频异常理解（VAU）的进展为各个领域的应用打开了突破性的大门，如交通监控和工业自动化。虽然当前的VAU基准测试主要强调异常的检测和定位，但在这里，我们努力深入探索VAU的实际方面，通过解决本质问题：“发生了什么异常？”、“为什么发生？”以及“这个异常事件的严重性如何？”来介绍一个用于探索视频异常因果关系的全面基准测试（ECVA）。我们的基准测试经过精心设计，每段视频都附有详细的人类注释。具体来说，我们的ECVA的每个实例都包含三组人类注释，以指示异常的“是什么”、“为什么”和“如何”，包括1）异常类型、开始和结束时间以及事件描述；2）异常原因的自然语言解释；3）反映异常影响的自由文本。在此基础上，我们提出了一种新的基于提示的方法，作为解决ECVA所带来复杂挑战的基础。我们使用“硬提示”来指导模型关注与视频异常片段相关的关键部分，并使用“软提示”在这些异常片段内建立时间和空间关系。此外，我们提出了AnomEval，这是一个专门设计的评估指标，与人类对ECVA的评判标准紧密对齐。该指标利用ECVA数据集的独特功能，为各种视频大型语言模型提供更全面和可靠的评估。我们通过严格的实验分析证明了我们的方法的有效性，并概述了进一步探索视频异常因果理解的可能途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07183v1">PDF</a> Submitted to IEEE Transactions on Pattern Analysis and Machine   Intelligence. arXiv admin note: substantial text overlap with   arXiv:2405.00181</p>
<p><strong>Summary</strong>：<br>视频异常理解（VAU）领域最新进展为交通监测、工业自动化等跨领域应用带来了突破性机会。当前基准测试主要关注异常的检测和定位，而本研究致力于挖掘视频异常事件的因果关系的基准测试（ECVA）。每个视频都伴随详细的人类标注，包括异常类型、时间、事件描述、异常原因的自然语言解释以及异常影响的自由文本。本研究提出了基于提示的方法作为解决ECVA挑战的基础，并利用硬提示和软提示建立异常段的时间和空间关系。此外，还提出了与ECVA数据集的人类判断标准紧密对齐的AnomEval评估指标，为视频大语言模型提供更全面可靠的评估。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>视频异常理解（VAU）领域正快速发展，应用领域广泛，如交通监测和工业自动化。</li>
<li>当前主要关注异常的检测和定位，但深入理解异常的因果关系至关重要。</li>
<li>引入了一个全面的基准测试（ECVA），包含详细的人类标注，涵盖异常的“是什么”，“为什么”和“严重性”。</li>
<li>提出了一种基于提示的方法来处理视频异常的复杂挑战。</li>
<li>利用硬提示和软提示指导模型关注关键部分并建立异常段的时间和空间关系。</li>
<li>提出了一种新的评估指标AnomEval，与ECVA数据集的人类判断标准紧密对齐。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-453411562f4711d88b9fea9e14fd4e28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0462cd2d237216af72980c0bed0c637d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbe327751a68532f8fffa8dd307bb27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ea20553fa2176a09b5aecfba3b6fe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1c31a0d8f9f8fc8e1eacd6162a387c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2487cbe6f86416e964de218a9069db0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b1dd8638bc3da57b76244ce05b07a63.jpg" align="middle">
</details>




<h2 id="Towards-Long-Video-Understanding-via-Fine-detailed-Video-Story-Generation"><a href="#Towards-Long-Video-Understanding-via-Fine-detailed-Video-Story-Generation" class="headerlink" title="Towards Long Video Understanding via Fine-detailed Video Story   Generation"></a>Towards Long Video Understanding via Fine-detailed Video Story   Generation</h2><p><strong>Authors:Zeng You, Zhiquan Wen, Yaofo Chen, Xin Li, Runhao Zeng, Yaowei Wang, Mingkui Tan</strong></p>
<p>Long video understanding has become a critical task in computer vision, driving advancements across numerous applications from surveillance to content retrieval. Existing video understanding methods suffer from two challenges when dealing with long video understanding: intricate long-context relationship modeling and interference from redundancy. To tackle these challenges, we introduce Fine-Detailed Video Story generation (FDVS), which interprets long videos into detailed textual representations. Specifically, to achieve fine-grained modeling of long-temporal content, we propose a Bottom-up Video Interpretation Mechanism that progressively interprets video content from clips to video. To avoid interference from redundant information in videos, we introduce a Semantic Redundancy Reduction mechanism that removes redundancy at both the visual and textual levels. Our method transforms long videos into hierarchical textual representations that contain multi-granularity information of the video. With these representations, FDVS is applicable to various tasks without any fine-tuning. We evaluate the proposed method across eight datasets spanning three tasks. The performance demonstrates the effectiveness and versatility of our method. </p>
<blockquote>
<p>长视频理解已成为计算机视觉中的一项关键任务，推动了从监控到内容检索等多个应用的发展。现有的视频理解方法在处理长视频理解时面临两个挑战：复杂的长上下文关系建模和冗余信息的干扰。为了解决这些挑战，我们引入了精细详细视频故事生成（FDVS）方法，它将长视频解释为详细的文本表示。具体来说，为了实现长时内容的精细建模，我们提出了自下而上的视频解释机制，该机制从片段到视频逐步解释视频内容。为了避免视频中冗余信息的干扰，我们引入了语义冗余减少机制，该机制在视觉和文本层面消除了冗余。我们的方法将长视频转换为层次化的文本表示形式，包含视频的多粒度信息。利用这些表示形式，FDVS可广泛应用于各种任务，无需进行微调。我们在三个任务的八个数据集上评估了所提出的方法。性能表现证明了我们的方法的有效性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06182v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>长视频理解已成为计算机视觉中的关键任务，推动了从监控到内容检索等多个应用的发展。针对长视频理解，现有方法面临复杂的长上下文关系建模和冗余信息的干扰两大挑战。为解决这些问题，我们提出了精细详细视频故事生成（FDVS）方法，将长视频转化为详细的文本表示。通过自下而上的视频解释机制和语义冗余减少机制，实现了对长视频内容的精细颗粒度建模和冗余信息的避免。该方法将长视频转换为层次化的文本表示，包含视频的多粒度信息，可广泛应用于各种任务而无需微调。实验结果表明，该方法的有效性和通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长视频理解已成为计算机视觉的关键任务，推动多个应用领域的发展。</li>
<li>现有视频理解方法面临复杂长上下文关系建模和冗余信息干扰的挑战。</li>
<li>提出的Fine-Detailed Video Story generation（FDVS）方法将长视频转化为详细的文本表示。</li>
<li>通过自下而上的视频解释机制实现长视频内容的精细颗粒度建模。</li>
<li>语义冗余减少机制避免了冗余信息的干扰。</li>
<li>FDVS方法将长视频转换为层次化的文本表示，包含视频的多粒度信息。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-222d0a53161bf6f90ce34c5c315ff5ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8758f2b167af8e48872e8b70b6998b79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f34aef7266080bbe907dc4bfcb253c0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47b6308c74b1c065442f49b4d984e132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38ae29f586ae574a9d643c6607a4924e.jpg" align="middle">
</details>




<h2 id="LinVT-Empower-Your-Image-level-Large-Language-Model-to-Understand-Videos"><a href="#LinVT-Empower-Your-Image-level-Large-Language-Model-to-Understand-Videos" class="headerlink" title="LinVT: Empower Your Image-level Large Language Model to Understand   Videos"></a>LinVT: Empower Your Image-level Large Language Model to Understand   Videos</h2><p><strong>Authors:Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, Zheng Zhao</strong></p>
<p>Large Language Models (LLMs) have been widely used in various tasks, motivating us to develop an LLM-based assistant for videos. Instead of training from scratch, we propose a module to transform arbitrary well-trained image-based LLMs into video-LLMs (after being trained on video data). To better adapt image-LLMs for processing videos, we introduce two design principles: linear transformation to preserve the original visual-language alignment and representative information condensation from redundant video content. Guided by these principles, we propose a plug-and-play Linear Video Tokenizer(LinVT), which enables existing image-LLMs to understand videos. We benchmark LinVT with six recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL, showcasing the high compatibility of LinVT. LinVT-based LLMs achieve state-of-the-art performance across various video benchmarks, illustrating the effectiveness of LinVT in multi-modal video understanding. </p>
<blockquote>
<p>大规模语言模型（LLMs）已在各种任务中得到了广泛应用，这促使我们开发基于LLM的视频助理。我们不是从头开始训练，而是提出一个模块，将任意的、基于图像的良好训练的LLMs转化为在视频数据训练后的视频LLMs。为了更好地适应图像LLMs处理视频，我们引入了两个设计原则：线性变换以保留原始视觉语言对齐和从冗余视频内容中凝练代表性信息。在这些原则的指导下，我们提出了即插即用的线性视频令牌化器（LinVT），使现有的图像LLMs能够理解视频。我们以LinVT与最近的六个视觉LLMs进行基准测试：Aquila、Blip-3、InternVL2、Mipha、Molmo和Qwen2-VL，展示了LinVT的高度兼容性。基于LinVT的LLMs在各种视频基准测试中达到了最先进的性能，证明了LinVT在多模态视频理解中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05185v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）在多个任务中的广泛应用，我们提出了一种基于LLM的视频助理。为解决从零开始训练视频LLM的难题，我们提出了一种将预训练的图像LLM转换为视频LLM的模块（该模块经过视频数据训练）。为更好地适应视频处理，我们提出了两个设计原则：保持原始视觉语言对齐的线性转换和从冗余视频内容中提取代表性信息。基于这些原则，我们提出了一种即插即用的线性视频令牌器（LinVT），使现有图像LLM能够理解视频。我们对LinVT进行了基准测试，涵盖了六种最新的视觉LLM，展示了LinVT的高度兼容性。基于LinVT的LLM在多个视频基准测试中达到了最先进的性能，证明了LinVT在多模态视频理解中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了将预训练的图像LLM转换为视频LLM的模块。</li>
<li>采用线性转换和代表性信息冷凝两个设计原则，以适应视频处理。</li>
<li>提出了一种即插即用的线性视频令牌器（LinVT），增强了现有图像LLM的视频理解能力。</li>
<li>LinVT与六种最新的视觉LLM兼容。</li>
<li>基于LinVT的LLM在多个视频基准测试中表现优异。</li>
<li>LinVT有助于实现多模态视频理解。</li>
<li>该方法通过引入线性转换和冗余信息冷凝技术，有效提高了视频理解的准确性和效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d5ea72a1663ea4365bbadd4779a64a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee80a22b3ad8a8487a83dd1551e22a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-789bafd94ff7bb5f50a3235056e3739e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232433e59b7bd1bd6ca4708e3723ce36.jpg" align="middle">
</details>




<h2 id="SAVEn-Vid-Synergistic-Audio-Visual-Integration-for-Enhanced-Understanding-in-Long-Video-Context"><a href="#SAVEn-Vid-Synergistic-Audio-Visual-Integration-for-Enhanced-Understanding-in-Long-Video-Context" class="headerlink" title="SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced   Understanding in Long Video Context"></a>SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced   Understanding in Long Video Context</h2><p><strong>Authors:Jungang Li, Sicheng Tao, Yibo Yan, Xiaojie Gu, Haodong Xu, Xu Zheng, Yuanhuiyi Lyu, Linfeng Zhang, Xuming Hu</strong></p>
<p>Endeavors have been made to explore Large Language Models for video analysis (Video-LLMs), particularly in understanding and interpreting long videos. However, existing Video-LLMs still face challenges in effectively integrating the rich and diverse audio-visual information inherent in long videos, which is crucial for comprehensive understanding. This raises the question: how can we leverage embedded audio-visual information to enhance long video understanding? Therefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video dataset comprising over 58k audio-visual instructions. (ii) From the model perspective, we propose a time-aware Audio-Visual Large Language Model (AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present AVBench, a benchmark containing 2,500 QAs designed to evaluate models on enhanced audio-visual comprehension tasks within long video, challenging their ability to handle intricate audio-visual interactions. Experiments on AVBench reveal the limitations of current AV-LLMs. Experiments also demonstrate that SAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video task (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the zero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter scale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and code will be released at <a target="_blank" rel="noopener" href="https://ljungang.github.io/SAVEn-Vid/">https://ljungang.github.io/SAVEn-Vid/</a> upon acceptance. </p>
<blockquote>
<p>在视频分析（Video-LLMs）方面，特别是在理解和解释长视频方面，已经付出了很大的努力来探索大型语言模型的应用。然而，现有的Video-LLMs仍然面临着有效整合长视频中丰富多样的视听信息的挑战，这对于全面理解来说是至关重要的。这就提出了一个问题：我们如何利用嵌入的视听信息来提高对长视频的理解能力？因此，（i）我们推出了SAVEn-Vid，这是首个包含超过58,000条视听指令的视听长视频数据集。（ii）从模型的角度来看，我们提出了时间感知视听大型语言模型（AV-LLM），名为SAVEnVideo，在SAVEn-Vid上进行了微调。（iii）此外，我们还推出了AVBench，这是一个包含2500个问答的基准测试，旨在评估模型在处理长视频中的增强视听理解任务的能力，挑战其处理复杂视听交互的能力。在AVBench上的实验揭示了当前AV-LLM的局限性。实验还表明，在零样本长视频任务（Video-MME）上，SAVEnVideo比最佳Video-LLM高出3.61%，在零样本视听任务（Music-AVQA）上，SAVEnVideo超过领先的视听LLM 1.29%。因此，在7B参数规模下，SAVEnVideo可以达到最先进的性能。我们的数据集和代码将在<a target="_blank" rel="noopener" href="https://ljungang.github.io/SAVEn-Vid/%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E5%BE%85%E5%AE%A1%E6%A0%B8%E9%80%9A%E8%BF%87%E5%90%8E%E5%8D%B3%E5%8F%AF%E4%BD%BF%E7%94%A8%E3%80%82">https://ljungang.github.io/SAVEn-Vid/上发布，待审核通过后即可使用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16213v2">PDF</a> The publication has some processing errors (language short-cuts in   synthetic data are not avoided) that invalidate some of the conclusions</p>
<p><strong>Summary</strong><br>     为视频分析引入大型语言模型（Video-LLMs）时面临挑战，尤其是理解和解释长视频时。研究团队构建了首个长音频视觉视频数据集SAVEn-Vid，并据此提出了时间感知的音频视觉大型语言模型（AV-LLM）SAVEnVideo。此外，还推出了评估模型在长时间音频视觉理解任务上的表现的基准测试AVBench。实验表明，SAVEnVideo在零样本长视频任务上优于最佳Video-LLM模型，并在音频视觉任务上达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在视频分析中的应用仍面临挑战，尤其是在处理和解释长视频时遇到的困难主要是整合丰富多样的音视频信息的问题。</li>
<li>SAVEn-Vid是首个长音频视觉视频数据集，包含超过58k的音视频指令。</li>
<li>SAVEnVideo是一种时间感知的音频视觉大型语言模型（AV-LLM），经过SAVEn-Vid数据集微调。</li>
<li>AVBench是一个基准测试平台，旨在评估模型在复杂的音视频交互处理方面的性能。</li>
<li>实验结果显示，SAVEnVideo在零样本长视频任务和音频视觉任务上的表现优于其他模型。</li>
<li>SAVEnVideo在参数规模为7B时达到了前沿水平的表现。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3c377cfa6694d981a6da59847bad6c17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae4b65d8cc7d7e26e19bbe2ee348bdc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4290c8e22ee321990e9c9af2085f06a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed4e6f694ea2f795c573ca32380556c.jpg" align="middle">
</details>




<h2 id="Video-XL-Extra-Long-Vision-Language-Model-for-Hour-Scale-Video-Understanding"><a href="#Video-XL-Extra-Long-Vision-Language-Model-for-Hour-Scale-Video-Understanding" class="headerlink" title="Video-XL: Extra-Long Vision Language Model for Hour-Scale Video   Understanding"></a>Video-XL: Extra-Long Vision Language Model for Hour-Scale Video   Understanding</h2><p><strong>Authors:Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao</strong></p>
<p>Long video understanding poses a significant challenge for current Multi-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained by their limited context lengths and the substantial costs while processing long videos. Although several existing methods attempt to reduce visual tokens, their strategies encounter severe bottleneck, restricting MLLMs’ ability to perceive fine-grained visual details. In this work, we propose Video-XL, a novel approach that leverages MLLMs’ inherent key-value (KV) sparsification capacity to condense the visual input. Specifically, we introduce a new special token, the Visual Summarization Token (VST), for each interval of the video, which summarizes the visual information within the interval as its associated KV. The VST module is trained by instruction fine-tuning, where two optimizing strategies are offered. 1.Curriculum learning, where VST learns to make small (easy) and large compression (hard) progressively. 2. Composite data curation, which integrates single-image, multi-image, and synthetic data to overcome the scarcity of long-video instruction data. The compression quality is further improved by dynamic compression, which customizes compression granularity based on the information density of different video intervals. Video-XL’s effectiveness is verified from three aspects. First, it achieves a superior long-video understanding capability, outperforming state-of-the-art models of comparable sizes across multiple popular benchmarks. Second, it effectively preserves video information, with minimal compression loss even at 16x compression ratio. Third, it realizes outstanding cost-effectiveness, enabling high-quality processing of thousands of frames on a single A100 GPU. </p>
<blockquote>
<p>长视频理解对当前的多模态大型语言模型（MLLMs）构成重大挑战。特别是，MLLMs在处理长视频时受到其有限上下文长度和巨大成本的限制。尽管有几种现有方法试图减少视觉标记，但它们的策略遇到了严重瓶颈，限制了MLLMs感知精细粒度视觉细节的能力。在这项工作中，我们提出了Video-XL，这是一种利用MLLMs固有的键值（KV）稀疏化能力来精简视觉输入的新方法。具体来说，我们为每个视频间隔引入了一个新的特殊标记，即视觉摘要标记（VST），该标记汇总间隔内的视觉信息作为其关联的KV。VST模块通过指令微调进行训练，其中提供了两种优化策略。1. 课程内容学习，VST学习逐步进行小（容易）和大压缩（困难）。2. 综合数据收集，整合单图像、多图像和合成数据，以克服长视频指令数据的稀缺性。压缩质量通过动态压缩进一步改进，动态压缩根据不同视频间隔的信息密度定制压缩粒度。Video-XL的有效性从三个方面得到了验证。首先，它实现了卓越的长视频理解能力，在多个流行基准测试中超越了同类先进模型。其次，它有效地保存了视频信息，即使在16倍压缩比下也几乎没有压缩损失。第三，它实现了出色的成本效益，能够在单个A100 GPU上实现高质量处理数千帧。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14485v4">PDF</a> </p>
<p><strong>Summary</strong><br>视频理解中，长视频对现有的多模态大型语言模型（MLLMs）构成挑战。针对MLLMs在处理长视频时的局限性，如上下文长度有限和成本高昂，本文提出了一种新的方法Video-XL。该方法利用MLLMs的固有键值（KV）稀疏化能力来精简视觉输入。具体来说，我们为每个视频间隔引入了一个新的特殊令牌——视觉摘要令牌（VST），该令牌将间隔内的视觉信息概括为其关联的KV。通过指令微调训练VST模块，提供了两种优化策略：1. 渐进式学习，使VST逐步学习进行大小不同的压缩；2. 整合单帧、多帧和合成数据以克服长视频指令数据的稀缺性的复合数据整理。Video-XL的有效性从三个方面得到了验证：一是其卓越的长视频理解能力，在多个流行基准测试中优于同类模型；二是信息保存效果好，即使在16倍压缩比下也几乎没有压缩损失；三是成本效益高，可在单个A100 GPU上实现高质量处理数千帧。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长视频理解对于现有Multi-modal Large Language Models（MLLMs）构成挑战，需要处理上下文长度有限和成本高昂的问题。</li>
<li>Video-XL利用MLLMs的键值（KV）稀疏化能力精简视觉输入，以提高处理效率。</li>
<li>Video-XL引入视觉摘要令牌（VST），为每个视频间隔提供视觉信息的摘要。</li>
<li>VST模块通过指令微调进行训练，采用渐进式学习和复合数据整理两种优化策略。</li>
<li>Video-XL在多个基准测试中表现出卓越的长视频理解能力。</li>
<li>Video-XL能有效保存视频信息，即使在较高的压缩比下也几乎没有信息损失。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-de3de7b5febe52da29fdc84adf693eab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92cd2051516a1f4764d1148b8b939444.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbf161eb64e00df51cb97b8fe91e5136.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e02ec8ea04f911b3ea89b6de2ae1d1df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02676b1f9e43b43069e76cb854603172.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cb832b722defb2458c803f27c3a4903a.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-12  SLGaussian Fast Language Gaussian Splatting in Sparse Views
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04e472c619da5bae43ce50aa91f60b42.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-12  GASP Gaussian Avatars with Synthetic Priors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26024.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
