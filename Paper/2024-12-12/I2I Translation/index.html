<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  BLADE Single-view Body Mesh Learning through Accurate Depth Estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6a7ba1e0b1885451cc6ff5247acccb5c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    24.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    102 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="BLADE-Single-view-Body-Mesh-Learning-through-Accurate-Depth-Estimation"><a href="#BLADE-Single-view-Body-Mesh-Learning-through-Accurate-Depth-Estimation" class="headerlink" title="BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation"></a>BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation</h2><p><strong>Authors:Shengze Wang, Jiefeng Li, Tianye Li, Ye Yuan, Henry Fuchs, Koki Nagano, Shalini De Mello, Michael Stengel</strong></p>
<p>Single-image human mesh recovery is a challenging task due to the ill-posed nature of simultaneous body shape, pose, and camera estimation. Existing estimators work well on images taken from afar, but they break down as the person moves close to the camera. Moreover, current methods fail to achieve both accurate 3D pose and 2D alignment at the same time. Error is mainly introduced by inaccurate perspective projection heuristically derived from orthographic parameters. To resolve this long-standing challenge, we present our method BLADE which accurately recovers perspective parameters from a single image without heuristic assumptions. We start from the inverse relationship between perspective distortion and the personâ€™s Z-translation Tz, and we show that Tz can be reliably estimated from the image. We then discuss the important role of Tz for accurate human mesh recovery estimated from close-range images. Finally, we show that, once Tz and the 3D human mesh are estimated, one can accurately recover the focal length and full 3D translation. Extensive experiments on standard benchmarks and real-world close-range images show that our method is the first to accurately recover projection parameters from a single image, and consequently attain state-of-the-art accuracy on 3D pose estimation and 2D alignment for a wide range of images. <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/amri/projects/blade/">https://research.nvidia.com/labs/amri/projects/blade/</a> </p>
<blockquote>
<p>å•å›¾åƒäººä½“ç½‘æ ¼æ¢å¤æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºåŒæ—¶è¿›è¡Œèº«ä½“å½¢çŠ¶ã€å§¿åŠ¿å’Œç›¸æœºä¼°è®¡æ˜¯ä¸é€‚å®šçš„æ€§è´¨ã€‚ç°æœ‰ä¼°è®¡å™¨åœ¨è¿œè·ç¦»æ‹æ‘„çš„å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å½“äººç‰©é è¿‘ç›¸æœºæ—¶ï¼Œå®ƒä»¬ä¼šå¤±æ•ˆã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•æ— æ³•åŒæ—¶å®ç°å‡†ç¡®çš„3Då§¿åŠ¿å’Œ2Då¯¹é½ã€‚è¯¯å·®ä¸»è¦æ˜¯ç”±é€è§†æŠ•å½±çš„ä¸å‡†ç¡®å¯å‘å¼ä»æ­£äº¤å‚æ•°è¡ç”Ÿå‡ºæ¥çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†BLADEæ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿå‡†ç¡®åœ°ä»å•å¹…å›¾åƒä¸­æ¢å¤é€è§†å‚æ•°ï¼Œæ— éœ€å¯å‘å¼å‡è®¾ã€‚æˆ‘ä»¬ä»é€è§†å¤±çœŸä¸äººçš„Zå¹³ç§»Tzä¹‹é—´çš„é€†å‘å…³ç³»å¼€å§‹ï¼Œå¹¶å±•ç¤ºå¯ä»¥ä»å›¾åƒå¯é åœ°ä¼°è®¡Tzã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†Tzå¯¹äºä»è¿‘è·ç¦»å›¾åƒå‡†ç¡®ä¼°è®¡äººä½“ç½‘æ ¼æ¢å¤çš„é‡è¦ä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºä¸€æ—¦ä¼°è®¡å‡ºTzå’Œ3Däººä½“ç½‘æ ¼ï¼Œä¾¿å¯ä»¥å‡†ç¡®æ¢å¤ç„¦è·å’Œå®Œæ•´çš„3Då¹³ç§»ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œçš„è¿‘è·ç¦»å›¾åƒä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–æ¬¡ä»å•å¹…å›¾åƒå‡†ç¡®æ¢å¤äº†æŠ•å½±å‚æ•°ï¼Œå› æ­¤åœ¨å¹¿æ³›èŒƒå›´çš„å›¾åƒä¸Šå®ç°äº†æœ€å…ˆè¿›çš„3Då§¿åŠ¿ä¼°è®¡å’Œ2Då¯¹é½ç²¾åº¦ã€‚è¯¦æƒ…è¯·å‚é˜…ï¼š[<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/amri/projects/blade/]">https://research.nvidia.com/labs/amri/projects/blade/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08640v1">PDF</a> </p>
<p><strong>Summary</strong><br>å•å›¾åƒäººä½“ç½‘æ ¼æ¢å¤æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºéœ€è¦åŒæ—¶ä¼°è®¡èº«ä½“å½¢çŠ¶ã€å§¿åŠ¿å’Œç›¸æœºå‚æ•°ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿œè·ç¦»æ‹æ‘„çš„äººä½“å›¾åƒæ—¶æ•ˆæœè¾ƒå¥½ï¼Œä½†å½“äººä½“é è¿‘ç›¸æœºæ—¶ï¼Œæ•ˆæœæ˜æ˜¾ä¸‹é™ã€‚å½“å‰æ–¹æ³•éš¾ä»¥åŒæ—¶å®ç°å‡†ç¡®çš„3Då§¿æ€å’Œ2Då¯¹é½ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•BLADEï¼Œå¯ä»¥ä»å•å¹…å›¾åƒå‡†ç¡®æ¢å¤é€è§†å‚æ•°ï¼Œæ— éœ€å¯å‘å¼å‡è®¾ã€‚æˆ‘ä»¬åˆ©ç”¨é€è§†ç•¸å˜ä¸äººçš„Zè½´å¹³ç§»Tzä¹‹é—´çš„é€†å‘å…³ç³»ï¼Œå¯é åœ°ä»å›¾åƒä¼°è®¡Tzã€‚ç„¶åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†Tzåœ¨åŸºäºè¿‘è·ç¦»å›¾åƒçš„äººä½“ç½‘æ ¼æ¢å¤ä¸­çš„é‡è¦ä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¼°è®¡Tzå’Œ3Däººä½“ç½‘æ ¼åï¼Œå¯ä»¥å‡†ç¡®æ¢å¤ç„¦è·å’Œå®Œæ•´çš„3Då¹³ç§»ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œè¿‘è·ç¦»å›¾åƒä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–æ¬¡ä»å•å¹…å›¾åƒå‡†ç¡®æ¢å¤äº†æŠ•å½±å‚æ•°ï¼Œå¹¶åœ¨3Då§¿æ€ä¼°è®¡å’Œ2Då¯¹é½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•å›¾åƒäººä½“ç½‘æ ¼æ¢å¤æ˜¯åŒæ—¶ä¼°è®¡èº«ä½“å½¢çŠ¶ã€å§¿åŠ¿å’Œç›¸æœºå‚æ•°çš„æŒ‘æˆ˜ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è¿‘è·ç¦»å›¾åƒä¸Šæ•ˆæœä¸ä½³ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥å®ç°å‡†ç¡®çš„3Då§¿æ€å’Œ2Då¯¹é½åŒæ—¶è¾¾æˆã€‚</li>
<li>BLADEæ–¹æ³•èƒ½å‡†ç¡®ä»å•å¹…å›¾åƒæ¢å¤é€è§†å‚æ•°ï¼Œæ— éœ€å¯å‘å¼å‡è®¾ã€‚</li>
<li>åˆ©ç”¨é€è§†ç•¸å˜ä¸äººçš„Zè½´å¹³ç§»Tzçš„é€†å‘å…³ç³»ä¼°è®¡Tzã€‚</li>
<li>Tzåœ¨åŸºäºè¿‘è·ç¦»å›¾åƒçš„äººä½“ç½‘æ ¼æ¢å¤ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a69ec16cb9d31154bff9a5a40c602b0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6a7ba1e0b1885451cc6ff5247acccb5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-801f314020b37461fc778d6185f8c28e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-df5c14b6510e17b5fc939d55cdb880fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c19bc26f797d6dc98712db3613c65f94.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-634dce97742c5257dec5c7e244474f3d.jpg" align="middle">
</details>




<h2 id="TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person"><a href="#TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person" class="headerlink" title="TryOffAnyone: Tiled Cloth Generation from a Dressed Person"></a>TryOffAnyone: Tiled Cloth Generation from a Dressed Person</h2><p><strong>Authors:Ioannis Xarchakos, Theodoros Koukopoulos</strong></p>
<p>The fashion industry is increasingly leveraging computer vision and deep learning technologies to enhance online shopping experiences and operational efficiencies. In this paper, we address the challenge of generating high-fidelity tiled garment images essential for personalized recommendations, outfit composition, and virtual try-on systems from photos of garments worn by models. Inspired by the success of Latent Diffusion Models (LDMs) in image-to-image translation, we propose a novel approach utilizing a fine-tuned StableDiffusion model. Our method features a streamlined single-stage network design, which integrates garmentspecific masks to isolate and process target clothing items effectively. By simplifying the network architecture through selective training of transformer blocks and removing unnecessary crossattention layers, we significantly reduce computational complexity while achieving state-of-the-art performance on benchmark datasets like VITON-HD. Experimental results demonstrate the effectiveness of our approach in producing high-quality tiled garment images for both full-body and half-body inputs. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone">https://github.com/ixarchakos/try-off-anyone</a> </p>
<blockquote>
<p>æ—¶å°šäº§ä¸šæ­£è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æå‡åœ¨çº¿è´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¢å¯¹çš„æŒ‘æˆ˜æ˜¯ä»æ¨¡ç‰¹æ‰€ç©¿æœè£…çš„ç…§ç‰‡ç”Ÿæˆé«˜è´¨é‡æ‹¼è´´æœè£…å›¾åƒï¼Œè¿™å¯¹äºä¸ªæ€§åŒ–æ¨èã€æœè£…æ­é…å’Œè™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMï¼‰åœ¨å›¾åˆ°å›¾ç¿»è¯‘ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ç²¾ç»†è°ƒæ•´è¿‡çš„StableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ç®€æ´çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œç»“åˆæœè£…ç‰¹å®šæ©è†œï¼Œæœ‰æ•ˆåœ°éš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°è®­ç»ƒå˜å‹å™¨å—å¹¶åˆ é™¤ä¸å¿…è¦çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œæˆ‘ä»¬ç®€åŒ–äº†ç½‘ç»œæ¶æ„ï¼Œåœ¨è®¡ç®—å¤æ‚åº¦æ–¹é¢å®ç°äº†æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶åœ¨VITON-HDç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¨èº«ä½“å’ŒåŠèº«è¾“å…¥æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡æ‹¼è´´æœè£…å›¾åƒæ–¹é¢éå¸¸æœ‰æ•ˆã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ixarchakos/try-off-anyoneæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08573v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ—¶å°šäº§ä¸šæ­£ç§¯æè¿ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æå‡åœ¨çº¿è´­ç‰©ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚æœ¬æ–‡å…³æ³¨ç”Ÿæˆé«˜è´¨é‡çš„åˆ†å—æœè£…å›¾åƒçš„æŒ‘æˆ˜ï¼Œè¿™å¯¹äºä¸ªæ€§åŒ–æ¨èã€æœè£…æ­é…å’Œè™šæ‹Ÿè¯•è¡£ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨å›¾åˆ°å›¾è½¬æ¢ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨fine-tuned StableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å…·æœ‰ç®€åŒ–çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œé€šè¿‡é›†æˆæœè£…ç‰¹å®šæ©è†œï¼Œæœ‰æ•ˆéš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚é€šè¿‡é€‰æ‹©æ€§è®­ç»ƒå˜å‹å™¨å—å¹¶å»é™¤ä¸å¿…è¦çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œç®€åŒ–äº†ç½‘ç»œæ¶æ„ï¼Œåœ¨VITON-HDç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶å°šäº§ä¸šè¿ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æå‡åœ¨çº¿è´­ç‰©ä½“éªŒåŠè¿è¥æ•ˆç‡ã€‚</li>
<li>ç”Ÿæˆé«˜è´¨é‡åˆ†å—æœè£…å›¾åƒå¯¹äºä¸ªæ€§åŒ–æ¨èã€æœè£…æ­é…å’Œè™šæ‹Ÿè¯•è¡£ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>å—åˆ°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼‰æˆåŠŸçš„å¯å‘ï¼Œæå‡ºä½¿ç”¨fine-tuned StableDiffusionæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•å…·æœ‰ç®€åŒ–çš„å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œé›†æˆæœè£…ç‰¹å®šæ©è†œä»¥éš”ç¦»å’Œå¤„ç†ç›®æ ‡æœè£…é¡¹ç›®ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æ€§è®­ç»ƒå’Œä¼˜åŒ–ç½‘ç»œæ¶æ„ï¼Œå®ç°äº†åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿå¤„ç†å…¨èº«å’ŒåŠèº«è¾“å…¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„åˆ†å—æœè£…å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4858b57b9c75d47132c65f2050bc5fe1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2856b5c51d6af52f31848166ea39e62f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5bd374d5d4a708e65208387522338bb4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3127daa555f76175132b4ef21712c878.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7748cecb9cb068dc6d44eb58141f9a77.jpg" align="middle">
</details>




<h2 id="Generate-Any-Scene-Evaluating-and-Improving-Text-to-Vision-Generation-with-Scene-Graph-Programming"><a href="#Generate-Any-Scene-Evaluating-and-Improving-Text-to-Vision-Generation-with-Scene-Graph-Programming" class="headerlink" title="Generate Any Scene: Evaluating and Improving Text-to-Vision Generation   with Scene Graph Programming"></a>Generate Any Scene: Evaluating and Improving Text-to-Vision Generation   with Scene Graph Programming</h2><p><strong>Authors:Ziqi Gao, Weikai Huang, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna</strong></p>
<p>DALL-E and Sora have gained attention by producing implausible images, such as â€œastronauts riding a horse in space.â€ Despite the proliferation of text-to-vision models that have inundated the internet with synthetic visuals, from images to 3D assets, current benchmarks predominantly evaluate these models on real-world scenes paired with captions. We introduce Generate Any Scene, a framework that systematically enumerates scene graphs representing a vast array of visual scenes, spanning realistic to imaginative compositions. Generate Any Scene leverages â€˜scene graph programmingâ€™, a method for dynamically constructing scene graphs of varying complexity from a structured taxonomy of visual elements. This taxonomy includes numerous objects, attributes, and relations, enabling the synthesis of an almost infinite variety of scene graphs. Using these structured representations, Generate Any Scene translates each scene graph into a caption, enabling scalable evaluation of text-to-vision models through standard metrics. We conduct extensive evaluations across multiple text-to-image, text-to-video, and text-to-3D models, presenting key findings on model performance. We find that DiT-backbone text-to-image models align more closely with input captions than UNet-backbone models. Text-to-video models struggle with balancing dynamics and consistency, while both text-to-video and text-to-3D models show notable gaps in human preference alignment. We demonstrate the effectiveness of Generate Any Scene by conducting three practical applications leveraging captions generated by Generate Any Scene: 1) a self-improving framework where models iteratively enhance their performance using generated data, 2) a distillation process to transfer specific strengths from proprietary models to open-source counterparts, and 3) improvements in content moderation by identifying and generating challenging synthetic data. </p>
<blockquote>
<p>DALL-Eå’ŒSoraé€šè¿‡ç”Ÿæˆä¸å¯ä¿¡çš„å›¾åƒï¼Œå¦‚â€œå®‡èˆªå‘˜åœ¨å¤ªç©ºä¸­éª‘é©¬â€ï¼Œå¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚å°½ç®¡æ–‡æœ¬åˆ°è§†è§‰çš„æ¨¡å‹å·²ç»äº§ç”Ÿå¤§é‡åˆæˆè§†è§‰ï¼Œä»å›¾åƒåˆ°3Dèµ„äº§ï¼Œç›®å‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦è¿˜æ˜¯é’ˆå¯¹ç°å®ä¸–ç•Œåœºæ™¯ä¸è¯´æ˜é…æ–‡çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä»‹ç»äº†Generate Any Sceneæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°æšä¸¾è¡¨ç¤ºå„ç§è§†è§‰åœºæ™¯çš„åœºæ™¯å›¾ï¼Œæ¶µç›–çœŸå®åˆ°æƒ³è±¡çš„å„ç§ç»„åˆã€‚Generate Any Sceneåˆ©ç”¨â€œåœºæ™¯å›¾ç¼–ç¨‹â€ï¼Œè¿™æ˜¯ä¸€ç§ä»è§†è§‰å…ƒç´ çš„ç»“æ„åŒ–åˆ†ç±»ä¸­åŠ¨æ€æ„å»ºåœºæ™¯å›¾çš„æ–¹æ³•ï¼Œè¿™ç§åˆ†ç±»åŒ…æ‹¬è®¸å¤šå¯¹è±¡ã€å±æ€§å’Œå…³ç³»ï¼Œèƒ½å¤Ÿå®ç°å‡ ä¹æ— é™å¤šç§åœºæ™¯å›¾çš„åˆæˆã€‚é€šè¿‡è¿™äº›ç»“æ„åŒ–è¡¨ç¤ºï¼ŒGenerate Any Sceneå°†æ¯ä¸ªåœºæ™¯å›¾ç¿»è¯‘ä¸ºè¯´æ˜é…æ–‡ï¼Œä»è€Œèƒ½å¤Ÿé€šè¿‡æ ‡å‡†æŒ‡æ ‡å¯¹æ–‡æœ¬åˆ°è§†è§‰æ¨¡å‹è¿›è¡Œå¯æ‰©å±•çš„è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œæ–‡æœ¬åˆ°3Dæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶æå‡ºäº†å…³äºæ¨¡å‹æ€§èƒ½çš„å…³é”®å‘ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸UNetä¸»å¹²æ¨¡å‹ç›¸æ¯”ï¼ŒDiTä¸»å¹²æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ›´ç´§å¯†åœ°ä¸è¾“å…¥é…æ–‡å¯¹é½ã€‚æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹åœ¨å¹³è¡¡åŠ¨æ€å’Œä¸€è‡´æ€§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œè€Œæ–‡æœ¬åˆ°è§†é¢‘å’Œæ–‡æœ¬åˆ°3Dæ¨¡å‹åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢éƒ½å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚æˆ‘ä»¬é€šè¿‡å¼€å±•ä¸‰é¡¹å®é™…åº”ç”¨ï¼Œå±•ç¤ºäº†Generate Any Sceneçš„æœ‰æ•ˆæ€§ï¼Œè¿™äº›åº”ç”¨åˆ©ç”¨Generate Any Sceneç”Ÿæˆçš„è¯´æ˜é…æ–‡ï¼š1ï¼‰ä¸€ç§è‡ªæˆ‘å®Œå–„æ¡†æ¶ï¼Œæ¨¡å‹åˆ©ç”¨ç”Ÿæˆæ•°æ®è¿­ä»£åœ°æé«˜å…¶æ€§èƒ½ï¼›2ï¼‰ä¸€ç§è’¸é¦è¿‡ç¨‹ï¼Œå°†ä¸“æœ‰æ¨¡å‹çš„ç‰¹å®šä¼˜åŠ¿è½¬ç§»åˆ°å¼€æºæ¨¡å‹ï¼›3ï¼‰é€šè¿‡è¯†åˆ«å’Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„åˆæˆæ•°æ®ï¼Œæ”¹è¿›å†…å®¹å®¡æ ¸ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸¤ä¸ªå¼•äººæ³¨ç›®çš„æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹DALL-Eå’ŒSoraï¼Œå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆè¶…ä¹æƒ³è±¡çš„å›¾åƒï¼Œå¦‚â€œå®‡èˆªå‘˜åœ¨å¤ªç©ºéª‘ä¹˜é©¬åŒ¹â€ã€‚ç„¶è€Œï¼Œå½“å‰å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸é…å¯¹æ ‡é¢˜ä¸‹çš„è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåä¸ºGenerate Any Sceneçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°æšä¸¾ä»£è¡¨å¹¿æ³›è§†è§‰åœºæ™¯çš„åœºæ™¯å›¾ï¼Œæ¶µç›–äº†ç°å®åˆ°æƒ³è±¡çš„å„ç§ç»„åˆã€‚è¯¥æ¡†æ¶é€šè¿‡åœºæ™¯å›¾ç¼–ç¨‹çš„æ–¹æ³•åŠ¨æ€æ„å»ºä¸åŒå¤æ‚åº¦çš„åœºæ™¯å›¾ï¼Œå¹¶é‡‡ç”¨ç»“æ„åŒ–è¡¨ç¤ºå½¢å¼å°†æ¯ä¸ªåœºæ™¯å›¾è½¬åŒ–ä¸ºæ ‡é¢˜ï¼Œä»è€Œå®ç°é€šè¿‡æ ‡å‡†æŒ‡æ ‡å¯¹æ–‡æœ¬è½¬è§†è§‰æ¨¡å‹çš„å¯æ‰©å±•è¯„ä¼°ã€‚ç»è¿‡å¯¹å¤šä¸ªæ–‡æœ¬è½¬å›¾åƒã€æ–‡æœ¬è½¬è§†é¢‘å’Œæ–‡æœ¬è½¬3Dæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°DiTéª¨å¹²çš„æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹æ¯”UNetéª¨å¹²æ¨¡å‹æ›´è´´è¿‘è¾“å…¥æ ‡é¢˜ã€‚æ–‡æœ¬è½¬è§†é¢‘æ¨¡å‹åœ¨å¹³è¡¡åŠ¨æ€å’Œä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œæ–‡æœ¬è½¬è§†é¢‘å’Œæ–‡æœ¬è½¬3Dæ¨¡å‹åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œæ–‡ç« å±•ç¤ºäº†Generate Any Sceneæ¡†æ¶åœ¨ä¸‰ä¸ªå®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬åˆ©ç”¨Generate Any Sceneç”Ÿæˆçš„æ ‡é¢˜è¿›è¡Œè‡ªæˆ‘æ”¹è¿›æ¡†æ¶ã€è’¸é¦è¿‡ç¨‹ä»¥åŠå†…å®¹å®¡æ ¸æ”¹è¿›ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DALL-Eå’ŒSoraç­‰æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¶…ä¹æƒ³è±¡çš„å›¾åƒï¼Œå¦‚å®‡èˆªå‘˜åœ¨å¤ªç©ºéª‘ä¹˜é©¬åŒ¹ã€‚</li>
<li>å½“å‰æ–‡æœ¬è½¬è§†è§‰æ¨¡å‹çš„è¯„ä¼°ä¸»è¦åŸºäºç°å®åœºæ™¯ä¸é…å¯¹æ ‡é¢˜ï¼Œç¼ºä¹ç³»ç»Ÿæ€§ã€‚</li>
<li>Generate Any Sceneæ¡†æ¶é€šè¿‡åœºæ™¯å›¾ç¼–ç¨‹æ–¹æ³•ï¼Œå®ç°äº†å¯¹æ–‡æœ¬è½¬è§†è§‰æ¨¡å‹çš„æ‰©å±•è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶åŒ…å«å¹¿æ³›è§†è§‰åœºæ™¯çš„åœºæ™¯å›¾ï¼Œå¹¶å…·å¤‡ç”Ÿæˆæ ‡é¢˜çš„åŠŸèƒ½ã€‚</li>
<li>è¯„ä¼°å‘ç°DiTéª¨å¹²çš„æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹æ›´è´´è¿‘è¾“å…¥æ ‡é¢˜ï¼Œè€Œæ–‡æœ¬è½¬è§†é¢‘æ¨¡å‹åœ¨å¹³è¡¡åŠ¨æ€å’Œä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Generate Any Sceneæ¡†æ¶åœ¨è‡ªæˆ‘æ”¹è¿›ã€è’¸é¦è¿‡ç¨‹å’Œå†…å®¹å®¡æ ¸æ”¹è¿›ç­‰å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c315f584131719cbf0e51ae564191f07.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8cac25681b5b0ef4a64bd955c50a4710.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-28686693057ddfe9c59d03d7f4389268.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d6ef3519244948b5ffdb9b3ad44de264.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ff8f77c54e7bd604eed9920f8606d516.jpg" align="middle">
</details>




<h2 id="AsyncDSB-Schedule-Asynchronous-Diffusion-Schrodinger-Bridge-for-Image-Inpainting"><a href="#AsyncDSB-Schedule-Asynchronous-Diffusion-Schrodinger-Bridge-for-Image-Inpainting" class="headerlink" title="AsyncDSB: Schedule-Asynchronous Diffusion SchrÃ¶dinger Bridge for Image   Inpainting"></a>AsyncDSB: Schedule-Asynchronous Diffusion SchrÃ¶dinger Bridge for Image   Inpainting</h2><p><strong>Authors:Zihao Han, Baoquan Zhang, Lisai Zhang, Shanshan Feng, Kenghong Lin, Guotao Liang, Yunming Ye, Xiaochen Qi, Guangming Ye</strong></p>
<p>Image inpainting is an important image generation task, which aims to restore corrupted image from partial visible area. Recently, diffusion Schr&quot;odinger bridge methods effectively tackle this task by modeling the translation between corrupted and target images as a diffusion Schr&quot;odinger bridge process along a noising schedule path. Although these methods have shown superior performance, in this paper, we find that 1) existing methods suffer from a schedule-restoration mismatching issue, i.e., the theoretical schedule and practical restoration processes usually exist a large discrepancy, which theoretically results in the schedule not fully leveraged for restoring images; and 2) the key reason causing such issue is that the restoration process of all pixels are actually asynchronous but existing methods set a synchronous noise schedule to them, i.e., all pixels shares the same noise schedule. To this end, we propose a schedule-Asynchronous Diffusion Schr&quot;odinger Bridge (AsyncDSB) for image inpainting. Our insight is preferentially scheduling pixels with high frequency (i.e., large gradients) and then low frequency (i.e., small gradients). Based on this insight, given a corrupted image, we first train a network to predict its gradient map in corrupted area. Then, we regard the predicted image gradient as prior and design a simple yet effective pixel-asynchronous noise schedule strategy to enhance the diffusion Schr&quot;odinger bridge. Thanks to the asynchronous schedule at pixels, the temporal interdependence of restoration process between pixels can be fully characterized for high-quality image inpainting. Experiments on real-world datasets show that our AsyncDSB achieves superior performance, especially on FID with around 3% - 14% improvement over state-of-the-art baseline methods. </p>
<blockquote>
<p>å›¾åƒè¡¥å…¨æ˜¯ä¸€é¡¹é‡è¦çš„å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨ä»éƒ¨åˆ†å¯è§åŒºåŸŸæ¢å¤å—æŸå›¾åƒã€‚æœ€è¿‘ï¼Œæ‰©æ•£SchrÃ¶dingeræ¡¥æ–¹æ³•é€šè¿‡å°†è¦ä¿®å¤çš„å—æŸå›¾åƒä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç¿»è¯‘è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªæ²¿å™ªå£°è°ƒåº¦è·¯å¾„çš„æ‰©æ•£SchrÃ¶dingeræ¡¥è¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³äº†è¿™ä¸€ä»»åŠ¡ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å·²ç»è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°1ï¼‰ç°æœ‰æ–¹æ³•å­˜åœ¨è°ƒåº¦æ¢å¤ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå³ç†è®ºä¸Šçš„è°ƒåº¦å’Œå®é™…æ¢å¤è¿‡ç¨‹é€šå¸¸å­˜åœ¨å¾ˆå¤§çš„å·®å¼‚ï¼Œè¿™ç†è®ºä¸Šå¯¼è‡´è°ƒåº¦æ²¡æœ‰å®Œå…¨ç”¨äºæ¢å¤å›¾åƒï¼›2ï¼‰é€ æˆè¿™ä¸€é—®é¢˜çš„ä¸»è¦åŸå› æ˜¯æ‰€æœ‰åƒç´ çš„æ¢å¤è¿‡ç¨‹æ˜¯å¼‚æ­¥çš„ï¼Œä½†ç°æœ‰æ–¹æ³•å´ä¸ºå®ƒä»¬è®¾ç½®äº†åŒæ­¥å™ªå£°è°ƒåº¦ï¼Œå³æ‰€æœ‰åƒç´ å…±äº«ç›¸åŒçš„å™ªå£°è°ƒåº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒè¡¥å…¨çš„å¼‚æ­¥æ‰©æ•£SchrÃ¶dingeræ¡¥ï¼ˆAsyncDSBï¼‰ã€‚æˆ‘ä»¬çš„è§è§£æ˜¯ä¼˜å…ˆè°ƒåº¦é«˜é¢‘ï¼ˆå³å¤§æ¢¯åº¦ï¼‰åƒç´ ï¼Œç„¶åæ˜¯ä½é¢‘ï¼ˆå³å°æ¢¯åº¦ï¼‰åƒç´ ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œå¯¹äºç»™å®šçš„å—æŸå›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªç½‘ç»œæ¥é¢„æµ‹å…¶æŸååŒºåŸŸçš„æ¢¯åº¦å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é¢„æµ‹çš„å›¾åƒæ¢¯åº¦è§†ä¸ºå…ˆéªŒï¼Œå¹¶è®¾è®¡ä¸€ç§ç®€å•æœ‰æ•ˆçš„åƒç´ å¼‚æ­¥å™ªå£°è°ƒåº¦ç­–ç•¥ï¼Œä»¥å¢å¼ºæ‰©æ•£SchrÃ¶dingeræ¡¥ã€‚ç”±äºåƒç´ çš„å¼‚æ­¥è°ƒåº¦ï¼Œå¯ä»¥å……åˆ†æè¿°åƒç´ ä¹‹é—´æ¢å¤è¿‡ç¨‹çš„æ—¶åºç›¸å…³æ€§ï¼Œä»è€Œå®ç°é«˜è´¨é‡å›¾åƒè¡¥å…¨ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AsyncDSBè¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨FIDæŒ‡æ ‡ä¸Šï¼Œè¾ƒæœ€æ–°åŸºçº¿æ–¹æ³•æœ‰çº¦3%-14%çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08149v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å›¾åƒä¿®å¤çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼‚æ­¥æ‰©æ•£SchrÃ¶dingeræ¡¥çš„æ–¹æ³•ï¼ˆAsyncDSBï¼‰ã€‚æ–‡ç« æŒ‡å‡ºç°æœ‰æ–¹æ³•å­˜åœ¨ç†è®ºæ¢å¤è®¡åˆ’ä¸å®é™…æ“ä½œä¸åŒ¹é…çš„é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒæ¢å¤æ•ˆæœå—é™ã€‚æ–‡ç« è¿˜æå‡ºï¼Œæ‰€æœ‰åƒç´ çš„æ¢å¤è¿‡ç¨‹å®é™…ä¸Šæ˜¯å¼‚æ­¥çš„ï¼Œä½†ç°æœ‰æ–¹æ³•è®¾ç½®äº†åŒæ­¥å™ªå£°æ—¶é—´è¡¨ï¼Œé€ æˆæ—¶é—´ä¸Šçš„å·®å¼‚é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…ä¼˜å…ˆæ¢å¤æ¢¯åº¦å¤§çš„åƒç´ åŒºåŸŸå¹¶é‡‡ç”¨æ–°çš„å™ªå£°è°ƒåº¦ç­–ç•¥ä»¥æé«˜æ‰©æ•£æ•ˆæœï¼Œä»è€Œè¾¾åˆ°é«˜è´¨é‡å›¾åƒä¿®å¤çš„ç›®çš„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAsyncDSBåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒä¿®å¤æ˜¯é‡è¦å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨ä»éƒ¨åˆ†å¯è§åŒºåŸŸæ¢å¤å—æŸå›¾åƒã€‚</li>
<li>å½“å‰æ‰©æ•£SchrÃ¶dingeræ¡¥æ–¹æ³•åœ¨ç†è®ºä¸Šåº”ç”¨äºå›¾åƒä¿®å¤å·²ç»å±•ç°å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
<li>å­˜åœ¨ç†è®ºæ¢å¤è®¡åˆ’ä¸å®é™…æ“ä½œä¸åŒ¹é…çš„é—®é¢˜ï¼Œå¯¼è‡´æ¢å¤æ•ˆæœå—é™ã€‚</li>
<li>æ¢å¤è¿‡ç¨‹ä¸­åƒç´ çš„å¼‚æ­¥æ€§è¢«å¿½è§†ï¼Œç°æœ‰æ–¹æ³•å¯¹æ‰€æœ‰åƒç´ é‡‡ç”¨åŒæ­¥å™ªå£°æ—¶é—´è¡¨ã€‚</li>
<li>æå‡ºä¼˜å…ˆæ¢å¤æ¢¯åº¦å¤§çš„åƒç´ åŒºåŸŸå¹¶é‡‡ç”¨æ–°çš„å™ªå£°è°ƒåº¦ç­–ç•¥ä»¥æé«˜æ‰©æ•£æ•ˆæœã€‚</li>
<li>AsyncDSBæ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a969c8e8209876bcfa90b006f0af7706.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ca85087f8c0e96ca21f9abba07097a2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0433f55a7750d2d746fc27d77126cd19.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5f27e03caa918a42936e95a3eb577885.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0b465462134654b48578ff50f3c7778c.jpg" align="middle">
</details>




<h2 id="From-Slow-Bidirectional-to-Fast-Causal-Video-Generators"><a href="#From-Slow-Bidirectional-to-Fast-Causal-Video-Generators" class="headerlink" title="From Slow Bidirectional to Fast Causal Video Generators"></a>From Slow Bidirectional to Fast Causal Video Generators</h2><p><strong>Authors:Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</strong></p>
<p>Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to a causal transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacherâ€™s ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model supports fast streaming generation of high quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future. </p>
<blockquote>
<p>å½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹è™½ç„¶ç”Ÿæˆè´¨é‡ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–æ€§ï¼Œåœ¨äº¤äº’å¼åº”ç”¨ä¸­è¡¨ç°å›°éš¾ã€‚ç”Ÿæˆå•ä¸ªå¸§éœ€è¦æ¨¡å‹å¤„ç†æ•´ä¸ªåºåˆ—ï¼ŒåŒ…æ‹¬æœªæ¥ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡å°†é¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£å˜å‹å™¨è°ƒæ•´ä¸ºå› æœå˜å‹å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥å› æœå˜å‹å™¨å¯ä»¥å³æ—¶ç”Ÿæˆå¸§ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿï¼Œæˆ‘ä»¬å°†åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹ç²¾ç‚¼ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚ä¸ºäº†å®ç°ç¨³å®šå’Œé«˜è´¨é‡çš„è’¸é¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ•™å¸ˆODEè½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥åŠä¸€ç§ä¸å¯¹ç§°çš„è’¸é¦ç­–ç•¥ï¼Œå³ç”¨åŒå‘æ•™å¸ˆç›‘ç£å› æœå­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ï¼Œå³ä½¿åœ¨çŸ­ç‰‡æ®µè®­ç»ƒçš„åŸºç¡€ä¸Šä¹Ÿèƒ½å®ç°é•¿æ—¶é•¿è§†é¢‘åˆæˆã€‚å¾—ç›ŠäºKVç¼“å­˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ”¯æŒæµå¼è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ã€å›¾åƒåˆ°è§†é¢‘ä»¥åŠé›¶æ ·æœ¬æ–¹å¼çš„åŠ¨æ€æç¤ºã€‚æœªæ¥ï¼Œæˆ‘ä»¬å°†åŸºäºå¼€æºæ¨¡å‹å‘å¸ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07772v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://causvid.github.io/">https://causvid.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–è€Œå­˜åœ¨çš„é™åˆ¶ã€‚é€šè¿‡å°†ä¸€ä¸ªé¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£å˜å‹å™¨æ”¹ç¼–ä¸ºå› æœå˜å‹å™¨ï¼Œå®ç°åœ¨çº¿ç”Ÿæˆå¸§ï¼Œå‡å°‘äº†ç”Ÿæˆå•å¸§æ‰€éœ€çš„å¤„ç†æ—¶é—´ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å±•åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰è‡³è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹ç®€åŒ–ä¸º4æ­¥ç”Ÿæˆå™¨ï¼Œè¿›ä¸€æ­¥é™ä½äº†å»¶è¿Ÿã€‚é€šè¿‡å¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆä»¥åŠä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œæœ‰æ•ˆç¼“è§£äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚è¯¥æ–¹æ³•æ”¯æŒåœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶å®ç°äº†è§†é¢‘åˆ°è§†é¢‘çš„æµå¼ç¿»è¯‘ã€å›¾åƒåˆ°è§†é¢‘ä»¥åŠé›¶æ ·æœ¬åŠ¨æ€æç¤ºåŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­å­˜åœ¨åŒå‘æ³¨æ„åŠ›ä¾èµ–çš„é™åˆ¶ã€‚</li>
<li>é€šè¿‡æ”¹ç¼–é¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£å˜å‹å™¨ä¸ºå› æœå˜å‹å™¨ï¼Œå®ç°äº†åœ¨çº¿ç”Ÿæˆå¸§ï¼Œå‡å°‘äº†å¤„ç†æ—¶é—´ã€‚</li>
<li>é€šè¿‡å¯¹åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰è¿›è¡Œæ‰©å±•ï¼Œç®€åŒ–äº†è§†é¢‘ç”Ÿæˆæ­¥éª¤ã€‚</li>
<li>å¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆï¼Œæé«˜äº†ç¨³å®šæ€§å’Œé«˜è´¨é‡è’¸é¦çš„æ•ˆæœã€‚</li>
<li>é‡‡ç”¨ä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œæœ‰æ•ˆç›‘ç£äº†å› æœå­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶å®ç°äº†å¤šç§æµå¼ç¿»è¯‘å’Œæç¤ºåŠŸèƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ee965fc3063cea1099d1a788584150d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d753d21c54ebc6456937f17d3e9e71a9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9cb62cb28a54dd058bb39b3b2bfafbe2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f6f5f4cc15dc30fe00c4852e9829e5fe.jpg" align="middle">
</details>




<h2 id="Paired-Wasserstein-Autoencoders-for-Conditional-Sampling"><a href="#Paired-Wasserstein-Autoencoders-for-Conditional-Sampling" class="headerlink" title="Paired Wasserstein Autoencoders for Conditional Sampling"></a>Paired Wasserstein Autoencoders for Conditional Sampling</h2><p><strong>Authors:Moritz Piening, Matthias Chung</strong></p>
<p>Wasserstein distances greatly influenced and coined various types of generative neural network models. Wasserstein autoencoders are particularly notable for their mathematical simplicity and straight-forward implementation. However, their adaptation to the conditional case displays theoretical difficulties. As a remedy, we propose the use of two paired autoencoders. Under the assumption of an optimal autoencoder pair, we leverage the pairwise independence condition of our prescribed Gaussian latent distribution to overcome this theoretical hurdle. We conduct several experiments to showcase the practical applicability of the resulting paired Wasserstein autoencoders. Here, we consider imaging tasks and enable conditional sampling for denoising, inpainting, and unsupervised image translation. Moreover, we connect our image translation model to the Monge map behind Wasserstein-2 distances. </p>
<blockquote>
<p>Wassersteinè·ç¦»å¯¹å„ç§ç”Ÿæˆç¥ç»ç½‘ç»œæ¨¡å‹äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œå¹¶å‚¬ç”Ÿäº†å¤šç§æ¨¡å‹ã€‚Wassersteinè‡ªç¼–ç å™¨å› å…¶æ•°å­¦ç®€å•æ€§å’Œç›´æ¥å®ç°æ€§è€Œå¤‡å—ç©ç›®ã€‚ç„¶è€Œï¼Œå°†å…¶é€‚åº”äºæ¡ä»¶æƒ…å†µå´å­˜åœ¨ç†è®ºä¸Šçš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨ä¸¤ä¸ªé…å¯¹è‡ªç¼–ç å™¨çš„æ–¹æ¡ˆã€‚åœ¨å‡è®¾æœ€ä¼˜è‡ªç¼–ç å™¨å¯¹çš„å‰æä¸‹ï¼Œæˆ‘ä»¬åˆ©ç”¨æŒ‡å®šçš„é«˜æ–¯æ½œåœ¨åˆ†å¸ƒçš„é…å¯¹ç‹¬ç«‹æ€§æ¡ä»¶æ¥å…‹æœè¿™ä¸€ç†è®ºéšœç¢ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥å±•ç¤ºæ‰€å¾—é…å¯¹Wassersteinè‡ªç¼–ç å™¨çš„å®é™…åº”ç”¨æ€§ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘æˆåƒä»»åŠ¡ï¼Œå¹¶é€šè¿‡é™å™ªã€ä¿®å¤å’Œæ— ç›‘ç£å›¾åƒç¿»è¯‘å®ç°æ¡ä»¶é‡‡æ ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å›¾åƒç¿»è¯‘æ¨¡å‹ä¸Wasserstein-2è·ç¦»èƒŒåçš„Mongeå›¾è¿æ¥èµ·æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07586v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†Wassersteinè·ç¦»å¯¹ç”Ÿæˆç¥ç»ç½‘ç»œæ¨¡å‹çš„å„ç§å½±å“ä»¥åŠWassersteinè‡ªç¼–ç å™¨çš„ç‰¹ç‚¹ã€‚å°½ç®¡è‡ªç¼–ç å™¨åœ¨ç†è®ºä¸Šé¢ä¸´é€‚åº”æ¡ä»¶æƒ…å†µçš„å›°éš¾ï¼Œä½†é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªé…å¯¹è‡ªç¼–ç å™¨å¹¶åˆ©ç”¨é«˜æ–¯æ½œåœ¨åˆ†å¸ƒçš„é…å¯¹ç‹¬ç«‹æ€§æ¡ä»¶ï¼Œå¯ä»¥å…‹æœè¿™äº›å›°éš¾ã€‚å®éªŒè¯æ˜ï¼Œé…å¯¹Wassersteinè‡ªç¼–ç å™¨åœ¨æˆåƒä»»åŠ¡ä¸­çš„å®ç”¨æ€§ï¼ŒåŒ…æ‹¬é™å™ªã€å›¾åƒä¿®å¤å’Œæ— ç›‘ç£å›¾åƒç¿»è¯‘ç­‰ã€‚æ­¤å¤–ï¼Œè¿˜å°†å›¾åƒç¿»è¯‘æ¨¡å‹ä¸Wasserstein-2è·ç¦»èƒŒåçš„Mongeå›¾è”ç³»èµ·æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wassersteinè·ç¦»å¯¹ç”Ÿæˆç¥ç»ç½‘ç»œæ¨¡å‹æœ‰é‡è¦å½±å“ï¼ŒåŒ…æ‹¬Wassersteinè‡ªç¼–ç å™¨ã€‚</li>
<li>Wassersteinè‡ªç¼–ç å™¨å…·æœ‰æ•°å­¦ç®€å•æ€§å’Œç›´æ¥å®ç°æ€§ã€‚</li>
<li>è‡ªç¼–ç å™¨åœ¨é€‚åº”æ¡ä»¶æƒ…å†µæ—¶é¢ä¸´ç†è®ºå›°éš¾ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªé…å¯¹è‡ªç¼–ç å™¨å’Œé«˜æ–¯æ½œåœ¨åˆ†å¸ƒçš„é…å¯¹ç‹¬ç«‹æ€§æ¡ä»¶ï¼Œå¯ä»¥å…‹æœè¿™äº›ç†è®ºå›°éš¾ã€‚</li>
<li>é…å¯¹Wassersteinè‡ªç¼–ç å™¨åœ¨æˆåƒä»»åŠ¡ä¸­å…·æœ‰å®ç”¨æ€§ï¼ŒåŒ…æ‹¬é™å™ªã€å›¾åƒä¿®å¤å’Œæ— ç›‘ç£å›¾åƒç¿»è¯‘ã€‚</li>
<li>é…å¯¹Wassersteinè‡ªç¼–ç å™¨çš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ae0d7dc17456f7fec88d84c5fc46e182.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2a65d33a3c660bd4862296994227cdad.jpg" align="middle">
</details>




<h2 id="A-Parametric-Approach-to-Adversarial-Augmentation-for-Cross-Domain-Iris-Presentation-Attack-Detection"><a href="#A-Parametric-Approach-to-Adversarial-Augmentation-for-Cross-Domain-Iris-Presentation-Attack-Detection" class="headerlink" title="A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris   Presentation Attack Detection"></a>A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris   Presentation Attack Detection</h2><p><strong>Authors:Debasmita Pal, Redwan Sony, Arun Ross</strong></p>
<p>Iris-based biometric systems are vulnerable to presentation attacks (PAs), where adversaries present physical artifacts (e.g., printed iris images, textured contact lenses) to defeat the system. This has led to the development of various presentation attack detection (PAD) algorithms, which typically perform well in intra-domain settings. However, they often struggle to generalize effectively in cross-domain scenarios, where training and testing employ different sensors, PA instruments, and datasets. In this work, we use adversarial training samples of both bonafide irides and PAs to improve the cross-domain performance of a PAD classifier. The novelty of our approach lies in leveraging transformation parameters from classical data augmentation schemes (e.g., translation, rotation) to generate adversarial samples. We achieve this through a convolutional autoencoder, ADV-GEN, that inputs original training samples along with a set of geometric and photometric transformations. The transformation parameters act as regularization variables, guiding ADV-GEN to generate adversarial samples in a constrained search space. Experiments conducted on the LivDet-Iris 2017 database, comprising four datasets, and the LivDet-Iris 2020 dataset, demonstrate the efficacy of our proposed method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD">https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD</a>. </p>
<blockquote>
<p>åŸºäºè™¹è†œçš„ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿå®¹æ˜“å—åˆ°æ”»å‡»æ¼”ç¤ºçš„å½±å“ï¼ˆPASï¼‰ã€‚å¯¹æ‰‹åˆ©ç”¨ç‰©ç†å·¥å…·ï¼ˆå¦‚æ‰“å°çš„è™¹è†œå›¾åƒã€çº¹ç†éšå½¢çœ¼é•œï¼‰å¯¹ç³»ç»Ÿè¿›è¡Œæ”»å‡»ã€‚è¿™ä¿ƒä½¿äº†å¤šç§æ¼”ç¤ºæ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ç®—æ³•çš„å‘å±•ï¼Œè¿™äº›ç®—æ³•åœ¨å†…éƒ¨é¢†åŸŸç¯å¢ƒä¸­é€šå¸¸è¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œå½“è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ä¸åŒçš„ä¼ æ„Ÿå™¨ã€æ”»å‡»ä»ªå™¨å’Œæ•°æ®é›†æ—¶ï¼Œå®ƒä»¬åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›å¾€å¾€è¾ƒå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çœŸå®è™¹è†œå’Œæ”»å‡»æ¼”ç¤ºå¯¹æŠ—è®­ç»ƒæ ·æœ¬ï¼Œä»¥æé«˜PADåˆ†ç±»å™¨åœ¨è·¨é¢†åŸŸçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ–°é¢–ä¹‹å¤„åœ¨äºåˆ©ç”¨ç»å…¸æ•°æ®å¢å¼ºæ–¹æ¡ˆï¼ˆå¦‚å¹³ç§»ã€æ—‹è½¬ï¼‰çš„è½¬æ¢å‚æ•°æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚æˆ‘ä»¬é€šè¿‡å·ç§¯è‡ªç¼–ç å™¨ADV-GENå®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥ç¼–ç å™¨è¾“å…¥åŸå§‹è®­ç»ƒæ ·æœ¬ä»¥åŠä¸€ç»„å‡ ä½•å’Œå…‰åº¦è½¬æ¢ã€‚è½¬æ¢å‚æ•°ä½œä¸ºæ­£åˆ™åŒ–å˜é‡ï¼ŒæŒ‡å¯¼ADV-GENåœ¨çº¦æŸæœç´¢ç©ºé—´ä¸­ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚åœ¨LivDet-Iris 2017æ•°æ®åº“ï¼ˆåŒ…å«å››ä¸ªæ•°æ®é›†ï¼‰å’ŒLivDet-Iris 2020æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/iPRoBe-lab/ADV-GEN-IrisPADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07199v1">PDF</a> IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV),   2025</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶è§£å†³äº†è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿåœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹çš„å‘ˆç°æ”»å‡»æ£€æµ‹é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨å¯¹æŠ—è®­ç»ƒæ ·æœ¬å’Œç»å…¸æ•°æ®å¢å¼ºæ–¹æ¡ˆçš„è½¬æ¢å‚æ•°ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå·ç§¯è‡ªç¼–ç å™¨çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜äº†PADåˆ†ç±»å™¨åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿé¢ä¸´å‘ˆç°æ”»å‡»ï¼ˆPAsï¼‰çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘æœ‰æ•ˆçš„å‘ˆç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ç®—æ³•ã€‚</li>
<li>PADç®—æ³•åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ä¸åŒä¼ æ„Ÿå™¨ã€PAä»ªå™¨å’Œæ•°æ®é›†çš„æƒ…å†µä¸‹ã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒæ ·æœ¬è¢«ç”¨äºæé«˜PADåˆ†ç±»å™¨çš„è·¨é¢†åŸŸæ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶åˆ©ç”¨ç»å…¸æ•°æ®å¢å¼ºæ–¹æ¡ˆçš„è½¬æ¢å‚æ•°æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨å·ç§¯è‡ªç¼–ç å™¨ADV-GENæ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¯¥è‡ªç¼–ç å™¨æ¥å—åŸå§‹è®­ç»ƒæ ·æœ¬ä»¥åŠä¸€ç»„å‡ ä½•å’Œå…‰åº¦è½¬æ¢ã€‚</li>
<li>è½¬æ¢å‚æ•°ä½œä¸ºæ­£åˆ™åŒ–å˜é‡ï¼ŒæŒ‡å¯¼ADV-GENåœ¨å—é™çš„æœç´¢ç©ºé—´å†…ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b11ea9e34245ae992abb61019ca85424.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c6c2cc0aeffe8b9ab96925608d0991bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-53cb9da6a411a164b0012f856f3d4b3f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a2ffba784837ee8c3d0e774b00049510.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0383b325ddc6153979ad230975efe4b6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3d73ec5db30159c31133f901134bfdc1.jpg" align="middle">
</details>




<h2 id="MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation"><a href="#MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation" class="headerlink" title="MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation"></a>MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation</h2><p><strong>Authors:Bo Li, Shaolin Zhu, Lijie Wen</strong></p>
<p>Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority. </p>
<blockquote>
<p>å›¾åƒç¿»è¯‘ï¼ˆITï¼‰åœ¨å„ä¸ªé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°å›¾åƒå†…æ–‡æœ¬å†…å®¹çš„è·¨è¯­è¨€ç¿»è¯‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢å­˜åœ¨è¯¸å¤šå±€é™ï¼Œé˜»ç¢äº†ITæ¨¡å‹çš„å¼€å‘ä¸è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MIT-10Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€å›¾åƒç¿»è¯‘å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡1000ä¸‡ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œæ¥æºäºçœŸå®ä¸–ç•Œæ•°æ®ï¼Œå¹¶ç»è¿‡äº†å¹¿æ³›çš„æ•°æ®æ¸…æ´—å’Œå¤šè¯­è¨€ç¿»è¯‘éªŒè¯ã€‚å®ƒåŒ…å«ä¸‰ç§å°ºå¯¸ã€28ä¸ªç±»åˆ«çš„84ä¸‡å¼ å›¾åƒï¼Œä»»åŠ¡éš¾åº¦åˆ†ä¸ºä¸‰ä¸ªçº§åˆ«ï¼Œä»¥åŠ14ç§è¯­è¨€çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œå¯¹ç°æœ‰æ•°æ®é›†æ¥è¯´æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨MIT-10Mä¸Šè¿›è¡Œäº†å¤§é‡çš„å®éªŒæ¥è¯„ä¼°å’Œè®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœæ¸…æ¥šåœ°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åœ¨è¯„ä¼°æ¨¡å‹åº”å¯¹ç°å®ä¸–ç•Œä¸­å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¤æ‚æ€§çš„å›¾åƒç¿»è¯‘ä»»åŠ¡çš„æ€§èƒ½æ—¶ï¼Œå…·æœ‰æ›´é«˜çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨MIT-10Mè¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½æé«˜äº†ä¸‰å€ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07147v1">PDF</a> Accepted in COLING 2025</p>
<p><strong>Summary</strong><br>     å¼•å…¥MIT-10Må¤šè¯­è¨€å›¾åƒç¿»è¯‘å¤§å‹å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡åƒä¸‡å›¾åƒæ–‡æœ¬å¯¹ï¼Œç”¨äºè§£å†³å›¾åƒç¿»è¯‘é¢†åŸŸçš„ç—›ç‚¹ã€‚è¯¥æ•°æ®é›†ç»è¿‡æ•°æ®æ¸…æ´—å’Œå¤šè¯­è¨€ç¿»è¯‘éªŒè¯ï¼ŒåŒ…å«ä¸åŒå°ºå¯¸ã€ç±»åˆ«å’Œéš¾åº¦çš„ä»»åŠ¡ï¼Œä»¥åŠå¤šç§è¯­è¨€çš„å›¾åƒæ–‡æœ¬å¯¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ•°æ®é›†åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç»è¿‡MIT-10Må¾®è°ƒçš„æ¨¡å‹æ€§èƒ½å¤§å¹…æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIT-10Mæ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šè¯­è¨€å›¾åƒç¿»è¯‘å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡åƒä¸‡çš„å›¾åƒæ–‡æœ¬å¯¹ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤šç§å°ºå¯¸ã€ç±»åˆ«å’Œéš¾åº¦çš„ä»»åŠ¡ï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„å›¾åƒç¿»è¯‘éœ€æ±‚ã€‚</li>
<li>æ•°æ®é›†ç»è¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—å’Œå¤šè¯­è¨€ç¿»è¯‘éªŒè¯ï¼Œä¿è¯æ•°æ®è´¨é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜MIT-10Måœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸Šå…·æœ‰æ›´é«˜çš„é€‚åº”æ€§ã€‚</li>
<li>ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨MIT-10Må¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>MIT-10Mçš„å¼•å…¥æœ‰åŠ©äºæ¨åŠ¨å›¾åƒç¿»è¯‘é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-af0d0f7fcb2a8ff0029db307da64e83a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-60f79ddea5e0335376407887ffb9b7fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-06c903befd571e21804bdbe9a878a09b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b02f93a7a822cc0ea3809244981ef4f0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffad01de36274cabab6343e55be2746b.jpg" align="middle">
</details>




<h2 id="Unsupervised-Variational-Translator-for-Bridging-Image-Restoration-and-High-Level-Vision-Tasks"><a href="#Unsupervised-Variational-Translator-for-Bridging-Image-Restoration-and-High-Level-Vision-Tasks" class="headerlink" title="Unsupervised Variational Translator for Bridging Image Restoration and   High-Level Vision Tasks"></a>Unsupervised Variational Translator for Bridging Image Restoration and   High-Level Vision Tasks</h2><p><strong>Authors:Jiawei Wu, Zhi Jin</strong></p>
<p>Recent research tries to extend image restoration capabilities from human perception to machine perception, thereby enhancing the performance of high-level vision tasks in degraded environments. These methods, primarily based on supervised learning, typically involve the retraining of restoration networks or high-level vision networks. However, collecting paired data in real-world scenarios and retraining large-scale models are challenge. To this end, we propose an unsupervised learning method called \textbf{Va}riational \textbf{T}ranslator (VaT), which does not require retraining existing restoration and high-level vision networks. Instead, it establishes a lightweight network that serves as an intermediate bridge between them. By variational inference, VaT approximates the joint distribution of restoration output and high-level vision input, dividing the optimization objective into preserving content and maximizing marginal likelihood associated with high-level vision tasks. By cleverly leveraging self-training paradigms, VaT achieves the above optimization objective without requiring labels. As a result, the translated images maintain a close resemblance to their original content while also demonstrating exceptional performance on high-level vision tasks. Extensive experiments in dehazing and low-light enhancement for detection and classification show the superiority of our method over other state-of-the-art unsupervised counterparts, even significantly surpassing supervised methods in some complex real-world scenarios.Code is available at <a target="_blank" rel="noopener" href="https://github.com/Fire-friend/VaT">https://github.com/Fire-friend/VaT</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å°è¯•å°†ä»äººç±»æ„ŸçŸ¥åˆ°æœºå™¨æ„ŸçŸ¥çš„å›¾åƒæ¢å¤èƒ½åŠ›è¿›è¡Œæ‰©å±•ï¼Œä»è€Œæé«˜åœ¨æ¶åŠ£ç¯å¢ƒä¸‹é«˜çº§è§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™äº›æ–¹æ³•ä¸»è¦åŸºäºæœ‰ç›‘ç£å­¦ä¹ ï¼Œé€šå¸¸æ¶‰åŠæ¢å¤ç½‘ç»œæˆ–é«˜çº§è§†è§‰ç½‘ç»œçš„å†è®­ç»ƒã€‚ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯æ”¶é›†é…å¯¹æ•°æ®å¹¶é‡æ–°è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ˜¯æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒç°æœ‰æ¢å¤å’Œé«˜çº§è§†è§‰ç½‘ç»œçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåä¸ºâ€œå˜è¯‘â€ï¼ˆVaTï¼‰ã€‚ç›¸åï¼Œå®ƒå»ºç«‹äº†ä¸€ä¸ªè½»é‡çº§çš„ç½‘ç»œï¼Œä½œä¸ºå®ƒä»¬ä¹‹é—´çš„ä¸­é—´æ¡¥æ¢ã€‚é€šè¿‡å˜åˆ†æ¨ç†ï¼ŒVaTè¿‘ä¼¼æ¢å¤è¾“å‡ºå’Œé«˜çº§è§†è§‰è¾“å…¥çš„å…±åŒåˆ†å¸ƒï¼Œå°†ä¼˜åŒ–ç›®æ ‡åˆ†ä¸ºä¿æŒå†…å®¹å’Œæœ€å¤§åŒ–ä¸é«˜çº§è§†è§‰ä»»åŠ¡ç›¸å…³çš„è¾¹ç¼˜å¯èƒ½æ€§ã€‚é€šè¿‡å·§å¦™åœ°åˆ©ç”¨è‡ªè®­ç»ƒæ¨¡å¼ï¼ŒVaTåœ¨ä¸éœ€æ ‡ç­¾çš„æƒ…å†µä¸‹å®ç°äº†ä¸Šè¿°ä¼˜åŒ–ç›®æ ‡ã€‚å› æ­¤ï¼Œç¿»è¯‘åçš„å›¾åƒä¿æŒäº†ä¸åŸå§‹å†…å®¹çš„ç´§å¯†ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶åœ¨é«˜çº§è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å»é›¾å’Œä½å…‰å¢å¼ºæ£€æµ‹ä¸åˆ†ç±»çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶å®ƒå…ˆè¿›çš„æ— ç›‘ç£æ–¹æ³•ä¹‹ä¸Šå…·æœ‰ä¼˜è¶Šæ€§ï¼Œç”šè‡³åœ¨æŸäº›å¤æ‚çš„çœŸå®åœºæ™¯ä¸­æ˜¾è‘—è¶…è¿‡äº†æœ‰ç›‘ç£æ–¹æ³•ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Fire-friend/VaT">https://github.com/Fire-friend/VaT</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08149v3">PDF</a> </p>
<p><strong>Summary</strong><br>     ç ”ç©¶è¯•å›¾é€šè¿‡å»¶é•¿æœºå™¨å¯¹äººå›¾åƒçš„å¤åŸèƒ½åŠ›æ¥æé«˜é«˜å±‚æ¬¡çš„è§†è§‰ä»»åŠ¡æ€§èƒ½ã€‚æå‡ºçš„å˜åˆ†ç¿»è¯‘å™¨ï¼ˆVaTï¼‰é€šè¿‡å®ç°ä¸éœ€è¦é‡è®­ç°æœ‰å¤åŸç½‘ç»œå’Œé«˜çº§è§†è§‰ç½‘ç»œçš„ä¸­é—´æ¡¥æ¢æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚åˆ©ç”¨å˜åˆ†æ¨ç†è¿‘ä¼¼è®¡ç®—å¤åŸè¾“å‡ºå’Œé«˜çº§è§†è§‰è¾“å…¥çš„ç»“åˆåˆ†å¸ƒï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºä¿ç•™å†…å®¹å’Œæœ€å¤§åŒ–ä¸é«˜çº§è§†è§‰ä»»åŠ¡ç›¸å…³çš„è¾¹ç¼˜æ¦‚ç‡çš„ä¼˜åŒ–ç›®æ ‡ã€‚é€šè¿‡è‡ªæˆ‘è®­ç»ƒæ¨¡å¼ï¼ŒVaTåœ¨ä¸ä¾èµ–æ ‡ç­¾çš„æƒ…å†µä¸‹å®ç°äº†ä¸Šè¿°ä¼˜åŒ–ç›®æ ‡ã€‚å®éªŒè¯æ˜ï¼Œåœ¨é™¤é›¾å’Œä½å…‰å¢å¼ºæ£€æµ‹ä¸åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ— ç›‘ç£æ–¹æ³•ï¼Œç”šè‡³åœ¨æŸäº›å¤æ‚ç°å®åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è‡´åŠ›äºå°†äººç±»æ„ŸçŸ¥çš„å›¾åƒæ¢å¤èƒ½åŠ›æ‰©å±•åˆ°æœºå™¨æ„ŸçŸ¥ï¼Œä»¥æé«˜é€€åŒ–ç¯å¢ƒä¸­é«˜çº§è§†è§‰ä»»åŠ¡çš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºå˜åˆ†ç¿»è¯‘å™¨ï¼ˆVaTï¼‰çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€é‡è®­ç°æœ‰çš„æ¢å¤å’Œé«˜çº§è§†è§‰ç½‘ç»œã€‚</li>
<li>VaTé€šè¿‡å»ºç«‹è½»é‡çº§ç½‘ç»œä½œä¸ºä¸­é—´æ¡¥æ¢ï¼Œå®ç°äº†åœ¨ä¸éœ€è¦é‡è®­çš„æƒ…å†µä¸‹æå‡å›¾åƒæ¢å¤å’Œé«˜çº§è§†è§‰ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å˜åˆ†æ¨ç†ï¼ŒVaTè¿‘ä¼¼è®¡ç®—æ¢å¤è¾“å‡ºå’Œé«˜çº§è§†è§‰è¾“å…¥çš„ç»“åˆåˆ†å¸ƒï¼Œå¹¶åˆ†è§£ä¸ºä¿ç•™å†…å®¹å’Œæœ€å¤§åŒ–ä¸é«˜çº§è§†è§‰ä»»åŠ¡ç›¸å…³çš„è¾¹ç¼˜æ¦‚ç‡çš„ä¼˜åŒ–ç›®æ ‡ã€‚</li>
<li>VaTåˆ©ç”¨è‡ªæˆ‘è®­ç»ƒæ¨¡å¼å®ç°ä¼˜åŒ–ç›®æ ‡ï¼Œæ— éœ€ä¾èµ–æ ‡ç­¾ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œåœ¨é™¤é›¾å’Œä½å…‰å¢å¼ºæ£€æµ‹ä¸åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒVaTè¡¨ç°ä¼˜è¶Šï¼Œç”šè‡³åœ¨æŸäº›å¤æ‚åœºæ™¯ä¸­è¶…è¿‡ç›‘ç£æ–¹æ³•ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a437890eacdc6cc0a6c2e50be1487fb9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2d92153b75e6744523118686295aa463.jpg" align="middle">
</details>




<h2 id="Leveraging-Pre-trained-Models-for-FF-to-FFPE-Histopathological-Image-Translation"><a href="#Leveraging-Pre-trained-Models-for-FF-to-FFPE-Histopathological-Image-Translation" class="headerlink" title="Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image   Translation"></a>Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image   Translation</h2><p><strong>Authors:Qilai Zhang, Jiawen Li, Peiran Liao, Jiali Hu, Tian Guan, Anjia Han, Yonghong He</strong></p>
<p>The two primary types of Hematoxylin and Eosin (H&amp;E) slides in histopathology are Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides offer high quality histopathological images but require a labor-intensive acquisition process. In contrast, FF slides can be prepared quickly, but the image quality is relatively poor. Our task is to translate FF images into FFPE style, thereby improving the image quality for diagnostic purposes. In this paper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological image translation using a pre-trained diffusion model. Specifically, we utilize a one-step diffusion model as the generator, which we fine-tune using LoRA adapters within an adversarial learning framework. To enable the model to effectively capture both global structural patterns and local details, we introduce a multi-scale feature fusion module that leverages two VAE encoders to extract features at different image resolutions, performing feature fusion before inputting them into the UNet. Additionally, a pre-trained vision-language model for histopathology serves as the backbone for the discriminator, enhancing model performance. Our FF-to-FFPE translation experiments on the TCGA-NSCLC dataset demonstrate that the proposed approach outperforms existing methods. The code and models are released at <a target="_blank" rel="noopener" href="https://github.com/QilaiZhang/Diffusion-FFPE">https://github.com/QilaiZhang/Diffusion-FFPE</a>. </p>
<blockquote>
<p>åœ¨ç—…ç†å­¦ä¸­ï¼Œè‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²ç»ç‰‡çš„ä¸¤ç§ä¸»è¦ç±»å‹æ˜¯ç¦å°”é©¬æ—å›ºå®šçŸ³èœ¡åŒ…åŸ‹ï¼ˆFFPEï¼‰å’Œæ–°é²œå†·å†»ï¼ˆFFï¼‰ã€‚FFPEç»ç‰‡æä¾›é«˜è´¨é‡çš„ç—…ç†å›¾åƒï¼Œä½†éœ€è¦åŠ³åŠ¨å¯†é›†å‹çš„é‡‡é›†è¿‡ç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFFç»ç‰‡å¯ä»¥å¿«é€Ÿåˆ¶å¤‡ï¼Œä½†å›¾åƒè´¨é‡ç›¸å¯¹è¾ƒå·®ã€‚æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯å°†FFå›¾åƒè½¬æ¢ä¸ºFFPEé£æ ¼ï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ï¼Œä»¥ç”¨äºè¯Šæ–­ç›®çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Diffusion-FFPEæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å®ç°FFåˆ°FFPEç—…ç†å›¾åƒç¿»è¯‘çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå™¨ï¼Œåœ¨ä¸€ä¸ªå¯¹æŠ—æ€§å­¦ä¹ æ¡†æ¶å†…ä½¿ç”¨LoRAé€‚é…å™¨å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å…¨å±€ç»“æ„æ¨¡å¼å’Œå±€éƒ¨ç»†èŠ‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ä¸¤ä¸ªVAEç¼–ç å™¨åœ¨ä¸åŒå›¾åƒåˆ†è¾¨ç‡ä¸‹æå–ç‰¹å¾ï¼Œåœ¨è¾“å…¥UNetä¹‹å‰è¿›è¡Œç‰¹å¾èåˆã€‚æ­¤å¤–ï¼Œç—…ç†å­¦ä¸­é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ¤åˆ«å™¨çš„éª¨å¹²ç½‘ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨TCGA-NSCLCæ•°æ®é›†ä¸Šè¿›è¡Œçš„FFåˆ°FFPEç¿»è¯‘å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/QilaiZhang/Diffusion-FFPE%E3%80%82">https://github.com/QilaiZhang/Diffusion-FFPEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18054v3">PDF</a> Accepted at IEEE BIBM 2024</p>
<p><strong>Summary</strong><br>     è®ºæ–‡ä»‹ç»äº†ä¸¤ç§ä¸»è¦çš„ç—…ç†åˆ‡ç‰‡ç±»å‹â€”â€”FFPEå’ŒFFï¼Œå¹¶æå‡ºä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å°†FFå›¾åƒè½¬æ¢ä¸ºFFPEé£æ ¼çš„æ–¹æ³•ï¼Œä»¥æé«˜è¯Šæ–­å›¾åƒè´¨é‡ã€‚é‡‡ç”¨å…·æœ‰å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—çš„ä¸€ç«™å¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆå™¨ï¼Œé€šè¿‡LoRAé€‚é…å™¨åœ¨å¯¹æŠ—å­¦ä¹ æ¡†æ¶ä¸­è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨TCGA-NSCLCæ•°æ®é›†ä¸Šçš„FF-to-FFPEç¿»è¯‘æ•ˆæœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸¤ç§ä¸»è¦çš„ç—…ç†åˆ‡ç‰‡ç±»å‹FFPEå’ŒFFï¼Œå¹¶å¼ºè°ƒäº†å°†FFå›¾åƒè½¬æ¢ä¸ºFFPEé£æ ¼çš„å¿…è¦æ€§ä»¥æé«˜è¯Šæ–­å›¾åƒè´¨é‡ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„å›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œç”¨äºå®ç°FFåˆ°FFPEçš„è½¬æ¢ã€‚</li>
<li>é‡‡ç”¨å…·æœ‰å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—çš„ä¸€ç«™å¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆå™¨ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨LoRAé€‚é…å™¨åœ¨å¯¹æŠ—å­¦ä¹ æ¡†æ¶ä¸­å¯¹ç”Ÿæˆå™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœã€‚</li>
<li>å¼•å…¥é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ¤åˆ«å™¨ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨TCGA-NSCLCæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨FF-to-FFPEç¿»è¯‘ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ee3ca2d0a2bc9314487c725339a6d52.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32908570bb7a726faf05b588144c6129.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-de7ef6ead762cb3b1676edb1c6664be9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e32302b30daf4186e0e8938e260e3904.jpg" align="middle">
</details>




<h2 id="SurgeMOD-Translating-image-space-tissue-motions-into-vision-based-surgical-forces"><a href="#SurgeMOD-Translating-image-space-tissue-motions-into-vision-based-surgical-forces" class="headerlink" title="SurgeMOD: Translating image-space tissue motions into vision-based   surgical forces"></a>SurgeMOD: Translating image-space tissue motions into vision-based   surgical forces</h2><p><strong>Authors:Mikel De Iturrate Reyzabal, Dionysios Malas, Shuai Wang, Sebastien Ourselin, Hongbin Liu</strong></p>
<p>We present a new approach for vision-based force estimation in Minimally Invasive Robotic Surgery based on frequency domain basis of motion of organs derived directly from video. Using internal movements generated by natural processes like breathing or the cardiac cycle, we infer the image-space basis of the motion on the frequency domain. As we are working with this representation, we discretize the problem to a limited amount of low-frequencies to build an image-space mechanical model of the environment. We use this pre-built model to define our force estimation problem as a dynamic constraint problem. We demonstrate that this method can estimate point contact forces reliably for silicone phantom and ex-vivo experiments, matching real readings from a force sensor. In addition, we perform qualitative experiments in which we synthesize coherent force textures from surgical videos over a certain region of interest selected by the user. Our method demonstrates good results for both quantitative and qualitative analysis, providing a good starting point for a purely vision-based method for surgical force estimation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘ç›´æ¥å¯¼å‡ºå™¨å®˜è¿åŠ¨é¢‘ç‡åŸŸç‰¹å¾çš„æœ€å°ä¾µå…¥å¼æœºå™¨äººæ‰‹æœ¯ä¸­çš„åŸºäºè§†è§‰çš„åŠ›ä¼°è®¡æ–°æ–¹æ³•ã€‚åˆ©ç”¨å‘¼å¸æˆ–å¿ƒè„å‘¨æœŸç­‰è‡ªç„¶è¿‡ç¨‹äº§ç”Ÿçš„å†…éƒ¨è¿åŠ¨ï¼Œæˆ‘ä»¬åœ¨é¢‘ç‡åŸŸæ¨æ–­è¿åŠ¨çš„å›¾åƒç©ºé—´åŸºç¡€ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯è¿™ç§è¡¨ç¤ºå½¢å¼ï¼Œæˆ‘ä»¬å°†é—®é¢˜ç¦»æ•£åŒ–ä¸ºæœ‰é™æ•°é‡çš„ä½é¢‘ï¼Œä»¥å»ºç«‹å›¾åƒç©ºé—´ç¯å¢ƒæœºæ¢°æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨é¢„å…ˆæ„å»ºçš„æ¨¡å‹å°†åŠ›ä¼°è®¡é—®é¢˜å®šä¹‰ä¸ºåŠ¨æ€çº¦æŸé—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¯é åœ°ä¼°è®¡ç¡…èƒ¶å¹»å½±å’Œç¦»ä½“å®éªŒä¸­çš„ç‚¹æ¥è§¦åŠ›ï¼Œä¸åŠ›ä¼ æ„Ÿå™¨çš„å®é™…è¯»æ•°ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç”¨æˆ·é€‰æ‹©çš„ç‰¹å®šæ„Ÿå…´è¶£åŒºåŸŸä»æ‰‹æœ¯è§†é¢‘ä¸­åˆæˆè¿è´¯çš„åŠ›çº¹ç†ï¼Œè¿›è¡Œäº†å®šæ€§å®éªŒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§åˆ†æä¸­å‡å–å¾—äº†è‰¯å¥½ç»“æœï¼Œä¸ºçº¯ç²¹çš„åŸºäºè§†è§‰çš„æ‰‹æœ¯åŠ›ä¼°è®¡æ–¹æ³•æä¾›äº†ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17707v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘é¢‘åŸŸè¿åŠ¨åŸºç¡€ï¼Œæå‡ºä¸€ç§ç”¨äºå¾®åˆ›æœºå™¨äººæ‰‹æœ¯ä¸­çš„åŸºäºè§†è§‰çš„åŠ›ä¼°è®¡æ–°æ–¹æ³•ã€‚åˆ©ç”¨è‡ªç„¶è¿‡ç¨‹ï¼ˆå¦‚å‘¼å¸æˆ–å¿ƒè„å‘¨æœŸï¼‰äº§ç”Ÿçš„å†…éƒ¨è¿åŠ¨ï¼Œæ¨æ–­å›¾åƒç©ºé—´çš„é¢‘åŸŸè¿åŠ¨åŸºç¡€ã€‚åœ¨æ­¤è¡¨ç¤ºå½¢å¼çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†é—®é¢˜ç¦»æ•£åŒ–ä¸ºæœ‰é™æ•°é‡çš„ä½é¢‘ï¼Œä»¥å»ºç«‹å›¾åƒç©ºé—´ç¯å¢ƒæœºæ¢°æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤é¢„æ„å»ºæ¨¡å‹å°†åŠ›ä¼°è®¡é—®é¢˜å®šä¹‰ä¸ºåŠ¨æ€çº¦æŸé—®é¢˜ã€‚è¯¥æ–¹æ³•å¯ä»¥å¯é åœ°ä¼°è®¡ç¡…èƒ¶å¹»å½±å’Œç¦»ä½“å®éªŒçš„ç‚¹æ¥è§¦åŠ›ï¼Œä¸æ¥è‡ªåŠ›ä¼ æ„Ÿå™¨çš„å®é™…è¯»æ•°ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å®šæ€§å®éªŒï¼Œåˆæˆç”¨æˆ·é€‰æ‹©çš„æ„Ÿå…´è¶£åŒºåŸŸçš„è¿è´¯åŠ›çº¹ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºå®šé‡å’Œå®šæ€§åˆ†æå‡è¡¨ç°å‡ºè‰¯å¥½ç»“æœï¼Œä¸ºçº¯ç²¹çš„åŸºäºè§†è§‰çš„æ‰‹æœ¯åŠ›ä¼°è®¡æ–¹æ³•æä¾›äº†è‰¯å¥½çš„èµ·ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºè§†é¢‘é¢‘åŸŸè¿åŠ¨åŸºç¡€çš„å¾®åˆ›æœºå™¨äººæ‰‹æœ¯ä¸­æ–°çš„è§†è§‰åŠ›ä¼°è®¡æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¿‡ç¨‹äº§ç”Ÿçš„å†…éƒ¨è¿åŠ¨æ¥æ¨æ–­å›¾åƒç©ºé—´çš„é¢‘åŸŸè¿åŠ¨åŸºç¡€ã€‚</li>
<li>å°†é—®é¢˜ç¦»æ•£åŒ–ä¸ºä½é¢‘å»ºç«‹å›¾åƒç©ºé—´ç¯å¢ƒæœºæ¢°æ¨¡å‹ã€‚</li>
<li>å°†åŠ›ä¼°è®¡é—®é¢˜å®šä¹‰ä¸ºåŠ¨æ€çº¦æŸé—®é¢˜ï¼Œå¹¶ä½¿ç”¨é¢„æ„å»ºæ¨¡å‹è¿›è¡Œè§£å†³ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿå¯é ä¼°è®¡ç¡…èƒ¶å¹»å½±å’Œç¦»ä½“å®éªŒçš„ç‚¹æ¥è§¦åŠ›ï¼Œä¸åŠ›ä¼ æ„Ÿå™¨è¯»æ•°ç›¸åŒ¹é…ã€‚</li>
<li>è¿›è¡ŒåˆæˆåŠ›çº¹ç†çš„å®šæ€§å®éªŒï¼Œä»¥åˆæˆç”¨æˆ·é€‰æ‹©çš„æ„Ÿå…´è¶£åŒºåŸŸçš„è¿è´¯åŠ›ä¿¡æ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-02c374a6f8b80512d1eeaf903a256cdd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3af6a04bad4876c2bea06d39250e8505.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3a6143a28d17003256f23dabd1c17e1a.jpg" align="middle">
</details>




<h2 id="Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions"><a href="#Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions" class="headerlink" title="Rethinking Score Distillation as a Bridge Between Image Distributions"></a>Rethinking Score Distillation as a Bridge Between Image Distributions</h2><p><strong>Authors:David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa</strong></p>
<p>Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains. Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications. In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution. Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target). We argue that current methodsâ€™ characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution. We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead. Our method can be easily applied across many domains, matching or beating the performance of specialized methods. We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real. We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors. </p>
<blockquote>
<p>å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰å·²è¢«è¯æ˜æ˜¯ä¸€ä¸ªé‡è¦å·¥å…·ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œå¯¹æ•°æ®åŒ®ä¹é¢†åŸŸä¸­çš„ä»»åŠ¡è¿›è¡Œæ“ä½œã€‚ç„¶è€Œï¼ŒSDSå­˜åœ¨ä¸€äº›ç‰¹æ€§åŒ–çš„ä¼ªè¿¹ï¼Œé™åˆ¶äº†å…¶åœ¨é€šç”¨åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†SDSåŠå…¶å˜ä½“è§†ä¸ºä»æºåˆ†å¸ƒåˆ°ç›®æ ‡åˆ†å¸ƒçš„æœ€ä¼˜æˆæœ¬ä¼ è¾“è·¯å¾„çš„è§£å†³æ–¹å¼ï¼Œä»è€Œå–å¾—äº†å¯¹SDSè¡Œä¸ºçš„ç†è§£çš„è¿›æ­¥ã€‚åœ¨è¿™ç§æ–°çš„è§£é‡Šä¸‹ï¼Œè¿™äº›æ–¹æ³•è¯•å›¾å°†æŸåçš„å›¾åƒï¼ˆæºï¼‰ä¼ è¾“åˆ°è‡ªç„¶å›¾åƒåˆ†å¸ƒï¼ˆç›®æ ‡ï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºå½“å‰æ–¹æ³•çš„ç‰¹æ€§ä¼ªè¿¹æ˜¯ç”±ï¼ˆ1ï¼‰æœ€ä¼˜è·¯å¾„çš„çº¿æ€§è¿‘ä¼¼å’Œï¼ˆ2ï¼‰æºåˆ†å¸ƒä¼°è®¡ä¸ä½³é€ æˆçš„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ ¡å‡†æºåˆ†å¸ƒçš„æ–‡æœ¬æ¡ä»¶å¯ä»¥äº§ç”Ÿé«˜è´¨é‡ç”Ÿæˆå’Œç¿»è¯‘ç»“æœï¼Œå¹¶ä¸”åªéœ€å¾ˆå°‘é¢å¤–çš„å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯è½»æ¾åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œè¾¾åˆ°æˆ–è¶…è¿‡ä¸“ç”¨æ–¹æ³•çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬åˆ°äºŒç»´ã€åŸºäºæ–‡æœ¬çš„NeRFä¼˜åŒ–ã€ç»˜ç”»åˆ°çœŸå®å›¾åƒçš„ç¿»è¯‘ã€å…‰å­¦é”™è§‰ç”Ÿæˆå’Œä¸‰ç»´è‰å›¾åˆ°ç°å®ç­‰ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶å®ç”¨æ€§ã€‚æˆ‘ä»¬å°†æ–¹æ³•ä¸ç°æœ‰çš„åˆ†æ•°è’¸é¦é‡‡æ ·æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯æ˜å®ƒèƒ½å¤Ÿç”Ÿæˆå…·æœ‰é€¼çœŸé¢œè‰²çš„é«˜é¢‘ç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09417v2">PDF</a> NeurIPS 2024. Project webpage: <a target="_blank" rel="noopener" href="https://sds-bridge.github.io/">https://sds-bridge.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åœ¨å¤„ç†æ•°æ®è´«ç˜ é¢†åŸŸä»»åŠ¡æ—¶çš„åº”ç”¨ä»·å€¼ã€‚å°½ç®¡SDSæœ‰ä¸€äº›å±€é™æ€§ï¼Œä½†å…¶ä»ç„¶è¢«ç”¨ä½œé‡è¦å·¥å…·ç”¨äºå¤§å‹æ‰©æ•£å…ˆéªŒçš„è¿ç”¨ã€‚ä¸ºäº†æå‡SDSçš„è¡¨ç°å’Œè§£å†³å…¶åœ¨ä¸€èˆ¬åº”ç”¨åœºæ™¯ä¸­çš„å›ºæœ‰å±€é™æ€§ï¼Œè¯¥æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è§’åº¦æ¥è§£å†³SDSåŠå˜ä½“çš„æœ€ä¼˜æˆæœ¬ä¼ è¾“è·¯å¾„é—®é¢˜ï¼Œé€šè¿‡å°†å¤„ç†è¿‡ç¨‹è§†ä½œæ˜¯ä»æºåˆ†å¸ƒåˆ°ç›®æ ‡åˆ†å¸ƒçš„ä¸€ä¸ªä¼ è¾“è¿‡ç¨‹ã€‚æ–°æ–¹æ³•è§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šæœ€ä¼˜è·¯å¾„çš„çº¿æ€§è¿‘ä¼¼å’Œæºåˆ†å¸ƒä¼°ç®—çš„ä¸å‡†ç¡®ã€‚é€šè¿‡æ ¡å‡†æºåˆ†å¸ƒçš„æ–‡æœ¬æ¡ä»¶ï¼Œæ–°æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶è¾¾åˆ°ä½é¢å¤–å¼€é”€çš„å®ç”¨ç¿»è¯‘æ•ˆæœã€‚å…¶åœ¨å¤šé¢†åŸŸåº”ç”¨ä¸­è¡¨ç°å‡ºäº†ä¼˜å¼‚çš„è¡¨ç°èƒ½åŠ›ï¼Œå¯ä»¥åŒ¹é…æˆ–è¶…è¶Šä¸€äº›ä¸“ä¸šé¢†åŸŸæ–¹æ³•çš„åº”ç”¨æ•ˆæœã€‚è¯¥è®ºæ–‡å±•ç¤ºçš„æ–¹æ³•è¢«ç”¨äºæ–‡æœ¬åˆ°äºŒç»´å›¾åƒè½¬æ¢ã€åŸºäºæ–‡æœ¬çš„NeRFä¼˜åŒ–ã€ç”»ä½œåˆ°ç°å®å›¾åƒçš„ç¿»è¯‘ã€å…‰å­¦å¹»è§‰ç”Ÿæˆä»¥åŠä¸‰ç»´è‰å›¾åˆ°ç°å®çš„è½¬æ¢ç­‰åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰çœŸå®è‰²å½©çš„é«˜é¢‘ç»†èŠ‚å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SDSä½œä¸ºä¸€ç§å¤„ç†æ•°æ®è´«ç˜ é¢†åŸŸä»»åŠ¡çš„é‡è¦å·¥å…·å·²è¢«è¯å®å…¶ä»·å€¼ï¼Œä½†å…¶å…·æœ‰ä¸€å®šçš„å±€é™æ€§ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡è§£å†³SDSçš„æœ€ä¼˜æˆæœ¬ä¼ è¾“è·¯å¾„é—®é¢˜æ¥æå‡å…¶è¡¨ç°å¹¶è§£å†³å…¶å±€é™æ€§ã€‚</li>
<li>æ–°æ–¹æ³•å°†å¤„ç†è¿‡ç¨‹è§†ä½œä»æºåˆ†å¸ƒåˆ°ç›®æ ‡åˆ†å¸ƒçš„ä¼ è¾“è¿‡ç¨‹ã€‚</li>
<li>å½“å‰æ–¹æ³•çš„ç‰¹å¾ç¼ºé™·æºäºæœ€ä¼˜è·¯å¾„çš„çº¿æ€§è¿‘ä¼¼å’Œæºåˆ†å¸ƒä¼°ç®—çš„ä¸å‡†ç¡®ã€‚</li>
<li>é€šè¿‡æ ¡å‡†æºåˆ†å¸ƒçš„æ–‡æœ¬æ¡ä»¶ï¼Œæ–°æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶è¾¾åˆ°å®ç”¨ç¿»è¯‘æ•ˆæœã€‚</li>
<li>æ–°æ–¹æ³•åœ¨å¤šé¢†åŸŸåº”ç”¨ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„è¡¨ç°èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°äºŒç»´å›¾åƒè½¬æ¢ã€åŸºäºæ–‡æœ¬çš„NeRFä¼˜åŒ–ç­‰åœºæ™¯ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ab688bcbe79403b9dc0a82fa87e55b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-583f9aab3efdf9acf3d30ae12a8d5845.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd2ee8b180e521f6c21caaa035b7cc5f.jpg" align="middle">
</details>




<h2 id="SPIN-Spacecraft-Imagery-for-Navigation"><a href="#SPIN-Spacecraft-Imagery-for-Navigation" class="headerlink" title="SPIN: Spacecraft Imagery for Navigation"></a>SPIN: Spacecraft Imagery for Navigation</h2><p><strong>Authors:Javier Montalvo, Juan Ignacio Bravo PÃ©rez-Villar, Ãlvaro GarcÃ­a-MartÃ­n, Pablo Carballeira, JesÃºs BescÃ³s</strong></p>
<p>The scarcity of data acquired under actual space operational conditions poses a significant challenge for developing learning-based visual navigation algorithms crucial for autonomous spacecraft navigation. This data shortage is primarily due to the prohibitive costs and inherent complexities of space operations. While existing datasets, predominantly relying on computer-simulated data, have partially addressed this gap, they present notable limitations. Firstly, these datasets often utilize proprietary image generation tools, restricting the evaluation of navigation methods in novel, unseen scenarios. Secondly, they provide limited ground-truth data, typically focusing solely on the spacecraftâ€™s translation and rotation relative to the camera. To address these limitations, we present SPIN (SPacecraft Imagery for Navigation), an open-source spacecraft image generation tool designed to support a wide range of visual navigation scenarios in space, with a particular focus on relative navigation tasks. SPIN provides multiple modalities of ground-truth data and allows researchers to employ custom 3D models of satellites, define specific camera-relative poses, and adjust settings such as camera parameters or environmental illumination conditions. We also propose a method for exploiting our tool as a data augmentation module. We validate our tool on the spacecraft pose estimation task by training with a SPIN-generated replica of SPEED+, reaching a 47% average error reduction on SPEED+ testbed data (that simulates realistic space conditions), further reducing it to a 60% error reduction when using SPIN as a data augmentation method. Both the SPIN tool (and source code) and our SPIN-generated version of SPEED+ will be publicly released upon paper acceptance on GitHub. <a target="_blank" rel="noopener" href="https://github.com/vpulab/SPIN">https://github.com/vpulab/SPIN</a> </p>
<blockquote>
<p>åœ¨çœŸå®çš„å¤ªç©ºæ“ä½œæ¡ä»¶ä¸‹è·å–çš„æ•°æ®ç¨€ç¼ºï¼Œå¯¹äºå¼€å‘åŸºäºå­¦ä¹ çš„è§†è§‰å¯¼èˆªç®—æ³•æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¿™äº›ç®—æ³•å¯¹äºè‡ªä¸»èˆªå¤©å™¨å¯¼èˆªè‡³å…³é‡è¦ã€‚è¿™ä¸€æ•°æ®çŸ­ç¼ºä¸»è¦å½’å› äºå¤ªç©ºæ“ä½œçš„é«˜æ˜‚æˆæœ¬å’Œå›ºæœ‰å¤æ‚æ€§ã€‚è™½ç„¶ç°æœ‰çš„æ•°æ®é›†ä¸»è¦ä¾èµ–äºè®¡ç®—æœºæ¨¡æ‹Ÿæ•°æ®ï¼Œå·²éƒ¨åˆ†è§£å†³äº†è¿™ä¸€å·®è·ï¼Œä½†å®ƒä»¬å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p>é¦–å…ˆï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸ä½¿ç”¨ä¸“æœ‰å›¾åƒç”Ÿæˆå·¥å…·ï¼Œé™åˆ¶äº†åœ¨æ–°é¢–ã€æœªè§è¿‡çš„åœºæ™¯ä¸­å¯¹å¯¼èˆªæ–¹æ³•çš„è¯„ä¼°ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬æä¾›çš„çœŸå®æ•°æ®æœ‰é™ï¼Œé€šå¸¸åªä¸“æ³¨äºèˆªå¤©å™¨ç›¸å¯¹äºç›¸æœºçš„å¹³ç§»å’Œæ—‹è½¬ã€‚</p>
<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SPINï¼ˆèˆªå¤©å™¨å›¾åƒå¯¼èˆªï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼€æºçš„èˆªå¤©å™¨å›¾åƒç”Ÿæˆå·¥å…·ï¼Œæ—¨åœ¨æ”¯æŒå„ç§å¤ªç©ºè§†è§‰å¯¼èˆªåœºæ™¯ï¼Œç‰¹åˆ«ä¾§é‡äºç›¸å¯¹å¯¼èˆªä»»åŠ¡ã€‚SPINæä¾›äº†å¤šç§æ¨¡å¼çš„çœŸå®æ•°æ®ï¼Œå¹¶å…è®¸ç ”ç©¶äººå‘˜ä½¿ç”¨å®šåˆ¶çš„å«æ˜Ÿ3Dæ¨¡å‹ã€å®šä¹‰ç‰¹å®šçš„ç›¸æœºç›¸å¯¹å§¿æ€ã€å¹¶è°ƒæ•´ç›¸æœºå‚æ•°æˆ–ç¯å¢ƒç…§æ˜æ¡ä»¶ç­‰è®¾ç½®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨è¯¥å·¥å…·ä½œä¸ºæ•°æ®å¢å¼ºæ¨¡å—çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨SPINç”Ÿæˆçš„SPEED+å¤åˆ¶å“å¯¹èˆªå¤©å™¨å§¿æ€ä¼°è®¡ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ¨¡æ‹ŸçœŸå®å¤ªç©ºæ¡ä»¶çš„SPEED+æµ‹è¯•å°ä¸Šå°†å¹³å‡è¯¯å·®å‡å°‘äº†47%ï¼Œåœ¨ä½¿ç”¨SPINä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•æ—¶ï¼Œè¯¯å·®è¿›ä¸€æ­¥å‡å°‘äº†60%ã€‚</p>
<p>SPINå·¥å…·ï¼ˆåŠæºä»£ç ï¼‰å’Œæˆ‘ä»¬ä½¿ç”¨SPINç”Ÿæˆçš„SPEED+ç‰ˆæœ¬å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚<a target="_blank" rel="noopener" href="https://github.com/vpulab/SPIN">https://github.com/vpulab/SPIN</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07500v3">PDF</a> </p>
<p><strong>Summary</strong><br>    æ•°æ®è·å–éš¾åº¦å¤§ï¼Œå¯¼è‡´å®é™…ç©ºé—´æ“ä½œç¯å¢ƒä¸‹çš„è§†è§‰å¯¼èˆªç®—æ³•å‘å±•å—é™ã€‚æå‡ºSPINå·¥å…·ï¼Œæ—¨åœ¨æ”¯æŒå¤šç§ç©ºé—´è§†è§‰å¯¼èˆªåœºæ™¯ï¼Œç‰¹åˆ«æ˜¯ç›¸å¯¹å¯¼èˆªä»»åŠ¡ã€‚è¯¥å·¥å…·æä¾›å¤šç§æ¨¡æ€çš„åœ°é¢çœŸå®æ•°æ®ï¼Œå…è®¸ç ”ç©¶è€…ä½¿ç”¨è‡ªå®šä¹‰å«æ˜Ÿæ¨¡å‹ï¼Œè°ƒæ•´ç›¸æœºå‚æ•°å’Œç¯å¢ƒç…§æ˜æ¡ä»¶ç­‰ã€‚åˆ©ç”¨è¯¥å·¥å…·è¿›è¡Œæ•°æ®å¢å¼ºæ–¹æ³•éªŒè¯ï¼Œè®­ç»ƒç»“æœé™ä½äº†é€Ÿåº¦è¯¯å·®ã€‚SPINå·¥å…·åŠæºç å°†å…¬å¼€åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•°æ®è·å–éš¾é¢˜å½±å“äº†åŸºäºå­¦ä¹ çš„è§†è§‰å¯¼èˆªç®—æ³•åœ¨è‡ªä¸»èˆªå¤©å™¨å¯¼èˆªé¢†åŸŸçš„å‘å±•ã€‚</li>
<li>SPINå·¥å…·æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†ä¾èµ–è®¡ç®—æœºæ¨¡æ‹Ÿæ•°æ®çš„é—®é¢˜ï¼Œæ”¯æŒå¤šç§ç©ºé—´è§†è§‰å¯¼èˆªåœºæ™¯ã€‚</li>
<li>SPINå·¥å…·æä¾›å¤šç§æ¨¡æ€çš„åœ°é¢çœŸå®æ•°æ®ï¼Œå…è®¸ç ”ç©¶è€…è‡ªå®šä¹‰å«æ˜Ÿæ¨¡å‹å’Œç¯å¢ƒè®¾ç½®ã€‚</li>
<li>åˆ©ç”¨SPINå·¥å…·è¿›è¡Œæ•°æ®å¢å¼ºèƒ½æé«˜ç®—æ³•æ€§èƒ½ï¼Œå‡å°‘é€Ÿåº¦è¯¯å·®ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e269abba545f64322d8ae2c3a78b5aa3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3e2e58772db9cdf48a54e001582c1577.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6ed0e5144e5b418f80261f10d3dffa4d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1d4271e1912f5ef34de0d1f21797e4c5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2ec7feed52ebbd38914a42c159a4f578.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-042235db958f1be0730872b1cfbe83fe.jpg" align="middle">
</details>




<h2 id="Equivariant-Machine-Learning-on-Graphs-with-Nonlinear-Spectral-Filters"><a href="#Equivariant-Machine-Learning-on-Graphs-with-Nonlinear-Spectral-Filters" class="headerlink" title="Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters"></a>Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</h2><p><strong>Authors:Ya-Wei Eileen Lin, Ronen Talmon, Ron Levie</strong></p>
<p>Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space. We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks. </p>
<blockquote>
<p>ç­‰ä»·æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§è®¾è®¡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•ï¼Œå®ƒå°Šé‡é—®é¢˜çš„å¯¹ç§°æ€§ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹å¤æ‚æ€§å¹¶æ”¹å–„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨å¹³ç§»ç­‰ä»·æ€§çš„æ‰©å±•ï¼Œè¿™æ˜¯å›¾åƒå·ç§¯ç½‘ç»œçš„åŸºç¡€ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°ä¸€èˆ¬å›¾ä¸Šã€‚ä¸å›¾åƒä¸åŒï¼Œå›¾æ²¡æœ‰åŸŸå¹³ç§»çš„è‡ªç„¶æ¦‚å¿µã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å›¾åŠŸèƒ½å¹³ç§»è§†ä¸ºå¯¹ç§°ç¾¤ï¼šä¸å›¾å¹³ç§»ç®—å­äº¤æ¢çš„é…‰ç®—å­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§å¯¹ç§°æ€§åœ¨ä¿¡å·ç©ºé—´è€Œä¸æ˜¯ç›´æ¥åœ¨ç©ºé—´ç©ºé—´ä¸­èµ·ä½œç”¨ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæ ‡å‡†è°±å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„æ¯ä¸€å±‚çº¿æ€§æ»¤æ³¢å™¨éƒ½ä¸å›¾åŠŸèƒ½å¹³ç§»äº¤æ¢ï¼Œä½†æ¿€æ´»å‡½æ•°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†å®Œå…¨ç­‰ä»·äºå›¾åŠŸèƒ½å¹³ç§»çš„éçº¿æ€§è°±æ»¤æ³¢å™¨ï¼ˆNLSFsï¼‰ï¼Œå¹¶è¯æ˜äº†å®ƒä»¬å…·æœ‰é€šç”¨é€¼è¿‘å±æ€§ã€‚æ‰€æå‡ºçš„NLSFsåŸºäºä¸€ç§å¯åœ¨å›¾ä¹‹é—´è½¬ç§»çš„æ–°è°±åŸŸå½¢å¼ã€‚æˆ‘ä»¬åœ¨èŠ‚ç‚¹å’Œå›¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†NLSFç›¸å¯¹äºç°æœ‰è°±GNNçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01249v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ç ”ç©¶äº†å¦‚ä½•åœ¨æ·±åº¦å­¦ä¹ ä¸­åº”ç”¨ç­‰ä»·å­¦ä¹ ç†è®ºæ¥è§£å†³é—®é¢˜ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ‰©å±•å›¾åƒå·ç§¯ç½‘ç»œçš„å¹³ç§»ç­‰ä»·æ€§åˆ°ä¸€èˆ¬å›¾å½¢ä¸Šã€‚è®ºæ–‡æŒ‡å‡ºï¼Œä¸åŒäºå›¾åƒæœ‰è‡ªç„¶çš„å¹³ç§»æ¦‚å¿µï¼Œå›¾å½¢æ²¡æœ‰è‡ªç„¶çš„å¹³ç§»æ¦‚å¿µï¼Œå› æ­¤è€ƒè™‘å›¾å½¢åŠŸèƒ½å¹³ç§»ä½œä¸ºå¯¹ç§°ç¾¤æ˜¯å¿…è¦çš„ã€‚æ–‡ç« æå‡ºäº†éçº¿æ€§è°±æ»¤æ³¢å™¨ï¼ˆNLSFsï¼‰ï¼Œå®ƒæ˜¯å®Œå…¨ç­‰ä»·äºå›¾å½¢åŠŸèƒ½å¹³ç§»çš„ï¼Œå¹¶ä¸”å…·æœ‰é€šç”¨é€¼è¿‘æ€§è´¨ã€‚æ­¤å¤–ï¼ŒNLSFsåŸºäºä¸€ç§å¯åœ¨ä¸åŒå›¾å½¢ä¹‹é—´ä¼ é€’çš„æ–°å½¢å¼çš„è°±åŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨èŠ‚ç‚¹å’Œå›¾å½¢åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒNLSFsçš„è¡¨ç°ä¼˜äºç°æœ‰çš„è°±å›¾ç¥ç»ç½‘ç»œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç­‰ä»·å­¦ä¹ ç†è®ºè¢«åº”ç”¨äºæ·±åº¦å­¦ä¹ ä¸­ï¼Œæ—¨åœ¨é€šè¿‡å°Šé‡é—®é¢˜çš„å¯¹ç§°æ€§æ¥è®¾è®¡å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>æ–‡ç« æ‰©å±•äº†å›¾åƒå·ç§¯ç½‘ç»œçš„å¹³ç§»ç­‰ä»·æ€§ç†è®ºåˆ°ä¸€èˆ¬å›¾å½¢ä¸Šã€‚</li>
<li>è®ºæ–‡æŒ‡å‡ºå›¾å½¢æ²¡æœ‰è‡ªç„¶çš„å¹³ç§»æ¦‚å¿µï¼Œå› æ­¤æå‡ºäº†åˆ©ç”¨å›¾å½¢åŠŸèƒ½å¹³ç§»ä½œä¸ºå¯¹ç§°ç¾¤çš„æ–°æ–¹æ³•ã€‚</li>
<li>NLSFsæ˜¯ä¸€ç§å®Œå…¨ç­‰ä»·äºå›¾å½¢åŠŸèƒ½å¹³ç§»çš„éçº¿æ€§æ»¤æ³¢å™¨ï¼Œå…·æœ‰é€šç”¨é€¼è¿‘æ€§è´¨ã€‚</li>
<li>NLSFsåŸºäºä¸€ç§æ–°å‹è°±åŸŸç»“æ„ï¼Œè¯¥ç»“æ„åœ¨ä¸åŒå›¾å½¢ä¹‹é—´å…·æœ‰å¯è¿ç§»æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒNLSFsåœ¨èŠ‚ç‚¹å’Œå›¾å½¢åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„è°±å›¾ç¥ç»ç½‘ç»œã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-003476765b4d9b263cc5914f409c4edc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d41e4cb336668d9f32ebf187289f5cfb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1ad900ee3a425d8fac42614ce4f27e05.jpg" align="middle">
</details>




<h2 id="Imitating-the-Functionality-of-Image-to-Image-Models-Using-a-Single-Example"><a href="#Imitating-the-Functionality-of-Image-to-Image-Models-Using-a-Single-Example" class="headerlink" title="Imitating the Functionality of Image-to-Image Models Using a Single   Example"></a>Imitating the Functionality of Image-to-Image Models Using a Single   Example</h2><p><strong>Authors:Nurit Spingarn-Eliezer, Tomer Michaeli</strong></p>
<p>We study the possibility of imitating the functionality of an image-to-image translation model by observing input-output pairs. We focus on cases where training the model from scratch is impossible, either because training data are unavailable or because the model architecture is unknown. This is the case, for example, with commercial models for biological applications. Since the development of these models requires large investments, their owners commonly keep them confidential, and reveal only a few input-output examples on the companyâ€™s website or in an academic paper. Surprisingly, we find that even a single example typically suffices for learning to imitate the modelâ€™s functionality, and that this can be achieved using a simple distillation approach. We present an extensive ablation study encompassing a wide variety of model architectures, datasets and tasks, to characterize the factors affecting vulnerability to functionality imitation, and provide a preliminary theoretical discussion on the reasons for this unwanted behavior. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†é€šè¿‡è§‚å¯Ÿè¾“å…¥-è¾“å‡ºå¯¹æ¥æ¨¡ä»¿å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹åŠŸèƒ½çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬ä¸“æ³¨äºé‚£äº›ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ä¸å¯èƒ½çš„æƒ…å†µï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸å¯ç”¨æˆ–æ¨¡å‹æ¶æ„æœªçŸ¥ã€‚è¿™ç§æƒ…å†µä¾‹å¦‚é€‚ç”¨äºç”Ÿç‰©åº”ç”¨ä¸­çš„å•†ä¸šæ¨¡å‹ã€‚ç”±äºå¼€å‘è¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„æŠ•èµ„ï¼Œå› æ­¤å…¶æ‰€æœ‰è€…é€šå¸¸å°†å…¶ä¿å¯†ï¼Œä»…åœ¨å…¬å¸çš„ç½‘ç«™æˆ–å­¦æœ¯è®ºæ–‡ä¸­å…¬å¼€å°‘æ•°è¾“å…¥-è¾“å‡ºç¤ºä¾‹ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯ä¸€ä¸ªä¾‹å­é€šå¸¸ä¹Ÿè¶³ä»¥å­¦ä¹ æ¨¡ä»¿æ¨¡å‹çš„åŠŸèƒ½ï¼Œè€Œä¸”å¯ä»¥ä½¿ç”¨ç®€å•çš„è’¸é¦æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæ¶µç›–äº†å„ç§æ¨¡å‹æ¶æ„ã€æ•°æ®é›†å’Œä»»åŠ¡ï¼Œä»¥åˆ»ç”»å½±å“åŠŸèƒ½æ¨¡ä»¿è„†å¼±æ€§çš„å› ç´ ï¼Œå¹¶å°±è¿™ç§ä¸è‰¯è¡Œä¸ºçš„åŸå› è¿›è¡Œäº†åˆæ­¥çš„ç†è®ºè®¨è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00828v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡è§‚æµ‹è¾“å…¥è¾“å‡ºå¯¹æ¥æ¨¡ä»¿å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹åŠŸèƒ½çš„å¯èƒ½æ€§ã€‚ç ”ç©¶é‡ç‚¹æ˜¯åœ¨æ— æ³•ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„æƒ…å†µï¼Œä¾‹å¦‚è®­ç»ƒæ•°æ®ä¸å¯ç”¨æˆ–æ¨¡å‹æ¶æ„æœªçŸ¥ã€‚ç‰¹åˆ«æ˜¯åœ¨å•†ä¸šåº”ç”¨ä¸­ï¼ŒæŸäº›æ¨¡å‹æ¶‰åŠå¤§é‡æŠ•èµ„ï¼Œå…¶æ‰€æœ‰æƒäººé€šå¸¸ä¼šä¿å¯†å¤„ç†å¹¶ä»…åœ¨å®˜ç½‘æˆ–å­¦æœ¯åˆŠç‰©ä¸Šå‘è¡¨å°‘é‡è¾“å…¥è¾“å‡ºå®ä¾‹ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯å•ä¸€ç¤ºä¾‹é€šå¸¸ä¹Ÿè¶³ä»¥æ¨¡ä»¿æ¨¡å‹çš„åŠŸèƒ½ï¼Œä¸”å¯ä»¥é€šè¿‡ç®€å•çš„è’¸é¦æ–¹æ³•å®ç°ã€‚æœ¬ç ”ç©¶è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæ¶µç›–äº†å¤šç§æ¨¡å‹æ¶æ„ã€æ•°æ®é›†å’Œä»»åŠ¡ï¼Œä»¥åˆ»ç”»å½±å“åŠŸèƒ½æ¨¡ä»¿çš„å› ç´ ï¼Œå¹¶å¯¹è¿™ç§éæœŸæœ›è¡Œä¸ºçš„åŸå› è¿›è¡Œäº†åˆæ­¥çš„ç†è®ºè®¨è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ¨¡ä»¿å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹åŠŸèƒ½çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ã€‚</li>
<li>ä¸»è¦å…³æ³¨æ— æ³•ä½¿ç”¨è®­ç»ƒæ•°æ®æˆ–æœªçŸ¥æ¨¡å‹æ¶æ„çš„åœºæ™¯ã€‚</li>
<li>ç ”ç©¶å¯¹è±¡ä¸»è¦æ˜¯å•†ä¸šåº”ç”¨ä¸­çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡æŠ•èµ„ä¸”é€šå¸¸è¢«ä¿å¯†å¤„ç†ã€‚</li>
<li>é€šè¿‡ç®€å•çš„è’¸é¦æ–¹æ³•ï¼Œå³ä½¿æ˜¯å•ä¸€çš„ç¤ºä¾‹ä¹Ÿå¯èƒ½è¶³ä»¥æ¨¡ä»¿æ¨¡å‹çš„åŠŸèƒ½ã€‚</li>
<li>è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæ¶µç›–äº†ä¸åŒçš„æ¨¡å‹æ¶æ„ã€æ•°æ®é›†å’Œä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶åˆæ­¥æ¢è®¨äº†å½±å“åŠŸèƒ½æ¨¡ä»¿çš„å› ç´ ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9f250ebdf3361a1f875c2913c1fdc7a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-996cc6c7fce145ea5b01b5ba6cb2727c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-010ecd8d09e7a166e3f0a0e78fc412ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-057358cce4f2fa1bbfe039a46c31db2f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f5b35d2cc8bd4961798620ac2ce9da52.jpg" align="middle">
</details>




<h2 id="Multi-Sensor-Diffusion-Driven-Optical-Image-Translation-for-Large-Scale-Applications"><a href="#Multi-Sensor-Diffusion-Driven-Optical-Image-Translation-for-Large-Scale-Applications" class="headerlink" title="Multi-Sensor Diffusion-Driven Optical Image Translation for Large-Scale   Applications"></a>Multi-Sensor Diffusion-Driven Optical Image Translation for Large-Scale   Applications</h2><p><strong>Authors:JoÃ£o Gabriel Vinholi, Marco Chini, Anis Amziane, Renato Machado, Danilo Silva, Patrick Matgen</strong></p>
<p>Comparing images captured by disparate sensors is a common challenge in remote sensing. This requires image translation â€“ converting imagery from one sensor domain to another while preserving the original content. Denoising Diffusion Implicit Models (DDIM) are potential state-of-the-art solutions for such domain translation due to their proven superiority in multiple image-to-image translation tasks in computer vision. However, these models struggle with reproducing radiometric features of large-scale multi-patch imagery, resulting in inconsistencies across the full image. This renders downstream tasks like Heterogeneous Change Detection impractical. To overcome these limitations, we propose a method that leverages denoising diffusion for effective multi-sensor optical image translation over large areas. Our approach super-resolves large-scale low spatial resolution images into high-resolution equivalents from disparate optical sensors, ensuring uniformity across hundreds of patches. Our contributions lie in new forward and reverse diffusion processes that address the challenges of large-scale image translation. Extensive experiments using paired Sentinel-II (10m) and Planet Dove (3m) images demonstrate that our approach provides precise domain adaptation, preserving image content while improving radiometric accuracy and feature representation. A thorough image quality assessment and comparisons with the standard DDIM framework and five other leading methods are presented. We reach a mean Learned Perceptual Image Patch Similarity (mLPIPS) of 0.1884 and a Fr&#39;echet Inception Distance (FID) of 45.64, expressively outperforming all compared methods, including DDIM, ShuffleMixer, and SwinIR. The usefulness of our approach is further demonstrated in two Heterogeneous Change Detection tasks. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿé¢†åŸŸï¼Œæ¯”è¾ƒç”±ä¸åŒä¼ æ„Ÿå™¨æ•è·çš„å›¾åƒæ˜¯ä¸€ä¸ªå¸¸è§çš„æŒ‘æˆ˜ã€‚è¿™éœ€è¦è¿›è¡Œå›¾åƒç¿»è¯‘ï¼Œå³å°†å›¾åƒä»ä¸€ä¸ªä¼ æ„Ÿå™¨é¢†åŸŸè½¬æ¢åˆ°å¦ä¸€ä¸ªä¼ æ„Ÿå™¨é¢†åŸŸï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹ã€‚å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆDDIMï¼‰æ˜¯æ­¤ç±»é¢†åŸŸç¿»è¯‘ä¸­æ½œåœ¨çš„å…ˆè¿›è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„å¤šä¸ªå›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­å·²è¯æ˜å…¶å“è¶Šæ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨é‡ç°å¤§è§„æ¨¡å¤šè¡¥ä¸å›¾åƒçš„è¾å°„ç‰¹å¾æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´æ•´ä¸ªå›¾åƒçš„ä¸ä¸€è‡´æ€§ã€‚è¿™ä½¿å¾—åƒå¼‚æ„å˜åŒ–æ£€æµ‹è¿™æ ·çš„ä¸‹æ¸¸ä»»åŠ¡å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å»å™ªæ‰©æ•£è¿›è¡Œå¤šä¼ æ„Ÿå™¨å…‰å­¦å›¾åƒæœ‰æ•ˆç¿»è¯‘çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºå¤§è§„æ¨¡åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤§è§„æ¨¡ä½ç©ºé—´åˆ†è¾¨ç‡å›¾åƒè¶…åˆ†è¾¨ç‡è½¬åŒ–ä¸ºæ¥è‡ªä¸åŒå…‰å­¦ä¼ æ„Ÿå™¨çš„é«˜åˆ†è¾¨ç‡ç­‰æ•ˆå›¾åƒï¼Œç¡®ä¿æ•°ç™¾ä¸ªè¡¥ä¸çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºæ–°çš„å‰å‘å’Œåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè§£å†³äº†å¤§è§„æ¨¡å›¾åƒç¿»è¯‘çš„æŒ‘æˆ˜ã€‚ä½¿ç”¨é…å¯¹çš„Sentinel-IIï¼ˆ10ç±³ï¼‰å’ŒPlanet Doveï¼ˆ3ç±³ï¼‰å›¾åƒè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ç²¾ç¡®çš„é¢†åŸŸé€‚åº”æ€§ï¼Œåœ¨ä¿ç•™å›¾åƒå†…å®¹çš„åŒæ—¶æé«˜äº†è¾å°„ç²¾åº¦å’Œç‰¹å¾è¡¨ç¤ºã€‚è¿›è¡Œäº†å½»åº•çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼Œå¹¶ä¸æ ‡å‡†çš„DDIMæ¡†æ¶å’Œå…¶ä»–äº”ç§é¢†å…ˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¾¾åˆ°äº†å¹³å‡å­¦ä¹ æ„ŸçŸ¥å›¾åƒè¡¥ä¸ç›¸ä¼¼æ€§ï¼ˆmLPIPSï¼‰0.1884å’ŒFrÂ´echet Inception Distanceï¼ˆFIDï¼‰45.64ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰æ¯”è¾ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬DDIMã€ShuffleMixerå’ŒSwinIRã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰ç”¨æ€§åœ¨ä¸¤é¡¹å¼‚æ„å˜åŒ–æ£€æµ‹ä»»åŠ¡ä¸­å¾—åˆ°äº†è¿›ä¸€æ­¥è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11243v4">PDF</a> This is the accepted version of the manuscript published in IEEE   Journal of Selected Topics in Applied Earth Observations and Remote Sensing   (JSTARS). Please access the final version at IEEEXplore (Open Access). DOI   10.1109&#x2F;JSTARS.2024.3506032. This technology is protected by a patent filed   on 23 december 2023 at Office Luxembourgeois de la propri&#39;et&#39;e   intellectuelle (LU505861)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é¥æ„Ÿé¢†åŸŸä¸åŒä¼ æ„Ÿå™¨å›¾åƒè½¬æ¢çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹çš„å¤§å‹å¤šä¼ æ„Ÿå™¨å…‰å­¦å›¾åƒç¿»è¯‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤§å‹å¤šæ–‘å—å›¾åƒçš„æ”¾å°„ç‰¹å¾å†ç°é—®é¢˜ï¼Œæé«˜å›¾åƒç¿»è¯‘çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡æ–°çš„å‰å‘å’Œåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°å¯¹ç™¾ä½™ä¸ªæ–‘å—å†…å›¾åƒçš„è¶…çº§è§£æï¼Œå°†å…¶ä»ä½åˆ†è¾¨ç‡è½¬åŒ–ä¸ºé«˜åˆ†è¾¨ç‡çš„ç­‰æ•ˆå›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå†…å®¹ä¿å­˜ã€æ”¾å°„ç²¾åº¦æé«˜åŠç‰¹å¾è¡¨ç°æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸åŒä¼ æ„Ÿå™¨å›¾åƒæ¯”è¾ƒåœ¨é¥æ„Ÿä¸­æ˜¯å¸¸è§æŒ‘æˆ˜ï¼Œéœ€è¦è¿›è¡Œå›¾åƒç¿»è¯‘ï¼Œå³åœ¨ä¸€ä¸ªä¼ æ„Ÿå™¨åŸŸä¸­å°†å›¾åƒè½¬æ¢ä¸ºå¦ä¸€ä¸ªä¼ æ„Ÿå™¨åŸŸçš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹ã€‚</li>
<li>å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆDDIMï¼‰åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†é¢ä¸´å¤§å‹å¤šæ–‘å—å›¾åƒçš„æ”¾å°„ç‰¹å¾å†ç°é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå»å™ªæ‰©æ•£çš„å¤§å‹å¤šä¼ æ„Ÿå™¨å…‰å­¦å›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œè§£å†³å¤§å‹å›¾åƒç¿»è¯‘ä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œå®ç°è·¨ç™¾ä½™ä¸ªæ–‘å—çš„ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡æ–°çš„å‰å‘å’Œåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œå°†ä½åˆ†è¾¨ç‡çš„å¤§å‹å›¾åƒè¶…çº§è§£æä¸ºç­‰æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå†…å®¹ä¿å­˜ã€æ”¾å°„ç²¾åº¦åŠç‰¹å¾è¡¨ç°æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬DDIMã€ShuffleMixerå’ŒSwinIRç­‰ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªå¼‚è´¨å˜åŒ–æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå®ç”¨æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cfdaac647fea9a9da3e300d824dca1d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5e066beb09802f355d490e3b5b325f04.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ef4c11e8c2fb8d018d9bc744f478600a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ee3a521043f07a16627886f2c9276e08.jpg" align="middle">
</details>




<h2 id="In-Context-Translation-Towards-Unifying-Image-Recognition-Processing-and-Generation"><a href="#In-Context-Translation-Towards-Unifying-Image-Recognition-Processing-and-Generation" class="headerlink" title="In-Context Translation: Towards Unifying Image Recognition, Processing,   and Generation"></a>In-Context Translation: Towards Unifying Image Recognition, Processing,   and Generation</h2><p><strong>Authors:Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang</strong></p>
<p>We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis). Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks. However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines. To this end, ICT introduces two designs. Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format. This turns different tasks into a general translation task between two RGB images. Secondly, it standardizes the training of different tasks into a general in-context learning, where â€œin-contextâ€ means the input comprises an example input-output pair of the target task and a query image. The learning objective is to generate the â€œmissingâ€ data paired with the query. The implicit translation process is thus between the query and the generated image. In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks. Notably, ICT performs well across three major categories of computer vision tasks, while its two competitors (Painter and PromptDiffusion) are only effective in at most two of these task categories. In addition, compared to its competitors, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡ç¿»è¯‘ï¼ˆICTï¼‰è¿™ä¸€é€šç”¨å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰è¯†åˆ«ï¼ˆä¾‹å¦‚è¯­ä¹‰åˆ†å‰²ï¼‰ã€ä½çº§å›¾åƒå¤„ç†ï¼ˆä¾‹å¦‚å»å™ªï¼‰å’Œæ¡ä»¶å›¾åƒç”Ÿæˆï¼ˆä¾‹å¦‚è¾¹ç¼˜åˆ°å›¾åƒåˆæˆï¼‰ã€‚ç”±äºç»Ÿä¸€åŒ–è®¾è®¡ï¼ŒICTæ˜¾è‘—å‡å°‘äº†è®¾è®¡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹æ‰€å›ºæœ‰çš„å½’çº³åè§ï¼Œå¹¶æœ€å¤§é™åº¦åœ°æé«˜äº†ç±»ä¼¼ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚ç„¶è€Œï¼Œç”±äºå„ç§æ•°æ®æ ¼å¼å’Œè®­ç»ƒç®¡é“ï¼Œå¤§é‡ä»»åŠ¡çš„ç»Ÿä¸€å¹¶ä¸å®¹æ˜“ã€‚ä¸ºæ­¤ï¼ŒICTå¼•å…¥äº†ä¸¤ç§è®¾è®¡ã€‚é¦–å…ˆï¼Œå®ƒå°†ä¸åŒä»»åŠ¡çš„è¾“å…¥è¾“å‡ºæ•°æ®æ ‡å‡†åŒ–ä¸ºRGBå›¾åƒå¯¹ã€‚ä¾‹å¦‚ï¼Œè¯­ä¹‰åˆ†å‰²æ•°æ®å°†RGBå›¾åƒä¸å…¶åˆ†å‰²æ©è†œé…å¯¹åœ¨åŒä¸€RGBæ ¼å¼ä¸­ã€‚è¿™å°†ä¸åŒçš„ä»»åŠ¡è½¬å˜ä¸ºä¸¤ä¸ªRGBå›¾åƒä¹‹é—´çš„é€šç”¨ç¿»è¯‘ä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œå®ƒå°†ä¸åŒä»»åŠ¡çš„è®­ç»ƒæ ‡å‡†åŒ–ä¸ºä¸€èˆ¬çš„ä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼Œå…¶ä¸­â€œä¸Šä¸‹æ–‡å†…â€æ„å‘³ç€è¾“å…¥åŒ…å«ç›®æ ‡ä»»åŠ¡çš„ç¤ºä¾‹è¾“å…¥è¾“å‡ºå¯¹å’ŒæŸ¥è¯¢å›¾åƒã€‚å­¦ä¹ ç›®æ ‡æ˜¯ç”Ÿæˆä¸æŸ¥è¯¢é…å¯¹â€œç¼ºå¤±â€çš„æ•°æ®ã€‚å› æ­¤ï¼Œéšå¼ç¿»è¯‘è¿‡ç¨‹å‘ç”Ÿåœ¨æŸ¥è¯¢å’Œç”Ÿæˆçš„å›¾åƒä¹‹é—´ã€‚åœ¨å®éªŒä¸­ï¼ŒICTç»Ÿä¸€äº†åç§è§†è§‰ä»»åŠ¡ï¼Œå¹¶åœ¨å„è‡ªçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒICTåœ¨ä¸‰ä¸ªä¸»è¦çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ç±»åˆ«ä¸­éƒ½è¡¨ç°è‰¯å¥½ï¼Œè€Œå®ƒçš„ä¸¤ä¸ªç«äº‰å¯¹æ‰‹ï¼ˆPainterå’ŒPromptDiffusionï¼‰æœ€å¤šåªèƒ½åœ¨ä¸¤ä¸ªä»»åŠ¡ç±»åˆ«ä¸­æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œä¸ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼ŒICTä»…åœ¨4ä¸ªRTX 3090 GPUä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„è®­ç»ƒæ•ˆç‡å’Œæ›´ä½çš„æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09633v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§åä¸ºIn-Context Translationï¼ˆICTï¼‰çš„é€šç”¨å­¦ä¹ æ¡†æ¶ï¼Œå®ç°è§†è§‰è¯†åˆ«ã€ä½é˜¶å›¾åƒå¤„ç†å’Œæ¡ä»¶å›¾åƒç”Ÿæˆçš„ç»Ÿä¸€ã€‚ICTé€šè¿‡æ ‡å‡†åŒ–ä»»åŠ¡è¾“å…¥&#x2F;è¾“å‡ºæ•°æ®å’Œè®­ç»ƒæµç¨‹ï¼Œå‡å°‘ç‰¹å®šä»»åŠ¡çš„å›ºæœ‰å½’çº³åè§ï¼Œå¹¶ä¼˜åŒ–ç›¸ä¼¼ä»»åŠ¡é—´çš„ç›¸äº’å¢å¼ºã€‚å®ƒå°†ä¸åŒä»»åŠ¡è½¬åŒ–ä¸ºRGBå›¾åƒå¯¹ä¹‹é—´çš„é€šç”¨ç¿»è¯‘ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”å…¶ä»–æ¨¡å‹æ›´å…·æ•ˆç‡å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICTæ˜¯ä¸€ä¸ªé€šç”¨å­¦ä¹ æ¡†æ¶ï¼Œèƒ½ç»Ÿä¸€è§†è§‰è¯†åˆ«ã€ä½é˜¶å›¾åƒå¤„ç†å’Œæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ICTé€šè¿‡æ ‡å‡†åŒ–ä»»åŠ¡è¾“å…¥&#x2F;è¾“å‡ºæ•°æ®å’Œè®­ç»ƒæµç¨‹æ¥å‡å°‘ç‰¹å®šä»»åŠ¡çš„å½’çº³åè§ã€‚</li>
<li>ICTå°†ä¸åŒä»»åŠ¡è½¬åŒ–ä¸ºRGBå›¾åƒå¯¹ä¹‹é—´çš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>ICTé‡‡ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå³è¾“å…¥åŒ…å«ç›®æ ‡ä»»åŠ¡çš„è¾“å…¥è¾“å‡ºç¤ºä¾‹å¯¹å’ŒæŸ¥è¯¢å›¾åƒã€‚</li>
<li>ICTåœ¨å¤šä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯ä¸‰å¤§ç±»åˆ«ä»»åŠ¡ã€‚</li>
<li>ICTç›¸æ¯”å…¶ä»–æ¨¡å‹ï¼ˆå¦‚Painterå’ŒPromptDiffusionï¼‰å…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-99464ec2b923496956b27481b8d471d2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aabc87a9b02f989dcf59036a740af033.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8fcfacd5c38b9f3ed43fd1cbf6c0ff74.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3b83bf71754d2658a9a2bbb4c73ee5c1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7641f5688750eabf7b67a6966681f18b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e313d821833c57ee4b726969c7b55d60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e3958cbdc15cdaf48765c9d33cdfe244.jpg" align="middle">
</details>




<h2 id="ContourDiff-Unpaired-Image-to-Image-Translation-with-Structural-Consistency-for-Medical-Imaging"><a href="#ContourDiff-Unpaired-Image-to-Image-Translation-with-Structural-Consistency-for-Medical-Imaging" class="headerlink" title="ContourDiff: Unpaired Image-to-Image Translation with Structural   Consistency for Medical Imaging"></a>ContourDiff: Unpaired Image-to-Image Translation with Structural   Consistency for Medical Imaging</h2><p><strong>Authors:Yuwen Chen, Nicholas Konz, Hanxue Gu, Haoyu Dong, Yaqian Chen, Lin Li, Jisoo Lee, Maciej A. Mazurowski</strong></p>
<p>Preserving object structure through image-to-image translation is crucial, particularly in applications such as medical imaging (e.g., CT-to-MRI translation), where downstream clinical and machine learning applications will often rely on such preservation. However, typical image-to-image translation algorithms prioritize perceptual quality with respect to output domain features over the preservation of anatomical structures. To address these challenges, we first introduce a novel metric to quantify the structural bias between domains which must be considered for proper translation. We then propose ContourDiff, a novel image-to-image translation algorithm that leverages domain-invariant anatomical contour representations of images to preserve the anatomical structures during translation. These contour representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. ContourDiff applies an input image contour representation as a constraint at every sampling step of a diffusion model trained in the output domain, ensuring anatomical content preservation for the output image. We evaluate our method on challenging lumbar spine and hip-and-thigh CT-to-MRI translation tasks, via (1) the performance of segmentation models trained on translated images applied to real MRIs, and (2) the foreground FID and KID of translated images with respect to real MRIs. Our method outperforms other unpaired image translation methods by a significant margin across almost all metrics and scenarios. Moreover, it achieves this without the need to access any input domain information during training. </p>
<blockquote>
<p>é€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¥ä¿ç•™å¯¹è±¡ç»“æ„è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—æˆåƒï¼ˆä¾‹å¦‚CTåˆ°MRIçš„ç¿»è¯‘ï¼‰ç­‰åº”ç”¨ä¸­ã€‚ä¸‹æ¸¸çš„ä¸´åºŠå’Œæœºå™¨å­¦ä¹ åº”ç”¨é€šå¸¸éƒ½ä¼šä¾èµ–è¿™ç§ä¿ç•™ã€‚ç„¶è€Œï¼Œå…¸å‹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç®—æ³•ä¼šä¼˜å…ˆè€ƒè™‘è¾“å‡ºåŸŸç‰¹å¾çš„æ„ŸçŸ¥è´¨é‡ï¼Œè€Œä¸æ˜¯ä¿ç•™è§£å‰–ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œæ¥é‡åŒ–ç¿»è¯‘æ—¶å¿…é¡»è¦è€ƒè™‘çš„åŸŸä¹‹é—´çš„ç»“æ„åå·®ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ContourDiffï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç®—æ³•ï¼Œå®ƒåˆ©ç”¨å›¾åƒçš„åŸŸä¸å˜è§£å‰–è½®å»“è¡¨ç¤ºåœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ä¿ç•™è§£å‰–ç»“æ„ã€‚è¿™äº›è½®å»“è¡¨ç¤ºæ³•å¯ä»¥ä»å›¾åƒä¸­è½»æ¾æå–ï¼Œä½†ä¸ºå®ƒä»¬çš„è§£å‰–å†…å®¹å½¢æˆäº†ç²¾ç¡®çš„ç©ºé—´çº¦æŸã€‚ContourDiffå°†è¾“å…¥å›¾åƒçš„è½®å»“è¡¨ç¤ºæ³•ä½œä¸ºæ‰©æ•£æ¨¡å‹åœ¨è¾“å‡ºåŸŸè®­ç»ƒçš„æ¯ä¸ªé‡‡æ ·æ­¥éª¤çš„çº¦æŸï¼Œç¡®ä¿è¾“å‡ºå›¾åƒçš„è§£å‰–å†…å®¹ä¿ç•™ã€‚æˆ‘ä»¬é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è…°æ¤å’Œé«‹å…³èŠ‚CTåˆ°MRIçš„ç¿»è¯‘ä»»åŠ¡æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œï¼ˆ1ï¼‰è®­ç»ƒäºç¿»è¯‘å›¾åƒä¸Šçš„åˆ†å‰²æ¨¡å‹åœ¨çœŸå®MRIä¸Šçš„åº”ç”¨æ€§èƒ½ï¼Œï¼ˆ2ï¼‰ç¿»è¯‘å›¾åƒç›¸å¯¹äºçœŸå®MRIçš„å‰æ™¯FIDå’ŒKIDã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡å’Œåœºæ™¯ä¸­ï¼Œéƒ½å¤§å¤§ä¼˜äºå…¶ä»–éé…å¯¹å›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œè€Œä¸”ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€è®¿é—®ä»»ä½•è¾“å…¥åŸŸä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10786v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦æˆåƒç­‰é¢†åŸŸä¸­çš„å›¾åƒç¿»è¯‘é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¿»è¯‘ç®—æ³•ContourDiffã€‚è¯¥ç®—æ³•é€šè¿‡åˆ©ç”¨å›¾åƒçš„åŸŸä¸å˜è§£å‰–è½®å»“è¡¨ç¤ºæ¥ä¿ç•™è§£å‰–ç»“æ„ï¼Œè§£å†³äº†ä¼ ç»Ÿå›¾åƒç¿»è¯‘ç®—æ³•åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å¿½ç•¥ç»“æ„ä¿ç•™çš„é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒè¾“å‡ºåŸŸçš„æ‰©æ•£æ¨¡å‹ï¼ŒContourDiffåœ¨é‡‡æ ·æ­¥éª¤ä¸­åº”ç”¨è¾“å…¥å›¾åƒçš„è½®å»“è¡¨ç¤ºä½œä¸ºçº¦æŸï¼Œç¡®ä¿è¾“å‡ºå›¾åƒçš„è§£å‰–å†…å®¹å¾—åˆ°ä¿ç•™ã€‚åœ¨è…°æ¤å’Œé«‹å…³èŠ‚CTåˆ°MRIçš„ç¿»è¯‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–éé…å¯¹å›¾åƒç¿»è¯‘æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç¿»è¯‘ä¸­ä¿ç•™å¯¹è±¡ç»“æ„è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒç¿»è¯‘ç®—æ³•åœ¨ä¿ç•™è§£å‰–ç»“æ„ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†æ¥é‡åŒ–åŸŸä¹‹é—´çš„ç»“æ„åå·®ï¼Œä¸ºæ­£ç¡®ç¿»è¯‘æä¾›äº†è€ƒé‡ã€‚</li>
<li>ContourDiffç®—æ³•åˆ©ç”¨å›¾åƒçš„åŸŸä¸å˜è§£å‰–è½®å»“è¡¨ç¤ºæ¥è§£å†³ç»“æ„ä¿ç•™é—®é¢˜ã€‚</li>
<li>ContourDiffé€šè¿‡è®­ç»ƒè¾“å‡ºåŸŸçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨é‡‡æ ·æ­¥éª¤ä¸­åº”ç”¨è¾“å…¥å›¾åƒè½®å»“ä½œä¸ºçº¦æŸæ¥å·¥ä½œã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è…°æ¤å’Œé«‹å…³èŠ‚CTåˆ°MRIçš„ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–éé…å¯¹å›¾åƒç¿»è¯‘æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da00fb386f746f96f3a237dc7602f46f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4b9f4eb7f30c1ec7c2892ca99f70f944.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-70e818591ae058f7b22f7740ea5b1081.jpg" align="middle">
</details>




<h2 id="Auxiliary-CycleGAN-guidance-for-Task-Aware-Domain-Translation-from-Duplex-to-Monoplex-IHC-Images"><a href="#Auxiliary-CycleGAN-guidance-for-Task-Aware-Domain-Translation-from-Duplex-to-Monoplex-IHC-Images" class="headerlink" title="Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from   Duplex to Monoplex IHC Images"></a>Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from   Duplex to Monoplex IHC Images</h2><p><strong>Authors:Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter, Shashank Saran, Marlon Rebelatto, GÃ¼nter Schmidt</strong></p>
<p>Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿä»å®¹æ˜“è·å¾—è®­ç»ƒæ¨¡å‹çš„æºå›¾åƒåŸŸç¿»è¯‘åˆ°è®­ç»ƒæœŸé—´æœªè§çš„ç›®æ ‡åŸŸã€‚è™½ç„¶å¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å·²ç»å»ºç«‹ï¼Œä½†ç›¸å…³çš„å¾ªç¯ä¸€è‡´æ€§çº¦æŸä¾èµ–äºä¸¤ä¸ªåŸŸä¹‹é—´å­˜åœ¨å¯é€†æ˜ å°„ã€‚ç„¶è€Œï¼Œå¯¹äºæŸ“è‰²å•ç”¨å’ŒåŒé‡å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æµ‹å®šæ³•ä¹‹é—´çš„å›¾åƒç¿»è¯‘è€Œè¨€ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ã€‚æˆ‘ä»¬ä¸“æ³¨äºä»åè€…åˆ°å‰è€…çš„ç¿»è¯‘ï¼Œé€šè¿‡å¼•å…¥æ–°å‹è®­ç»ƒè®¾è®¡ï¼Œåˆ©ç”¨ä¸€ç»„å…ç–«è§å…‰ï¼ˆIFï¼‰å›¾åƒä½œä¸ºéé…å¯¹å›¾åƒåŸŸï¼Œæå‡ºä¸€ç§æ›¿ä»£çº¦æŸã€‚ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„å®šé‡å’Œå®šæ€§ç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•çš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07389v2">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒåŸŸè½¬æ¢ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨Cycle GANsçš„æ”¹è¿›ä¸Šã€‚é’ˆå¯¹å…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²å›¾åƒä¸å…ç–«è§å…‰å›¾åƒä¹‹é—´çš„è½¬æ¢é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒè®¾è®¡æ–¹æ³•ï¼Œåˆ©ç”¨å…ç–«è§å…‰å›¾åƒä½œä¸ºè¾…åŠ©éé…å¯¹å›¾åƒåŸŸï¼Œæé«˜è½¬æ¢æ•ˆæœã€‚åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜è¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¯å®ç°ä»æ˜“è®­ç»ƒæ¨¡å‹æ‰€åœ¨æºåŸŸåˆ°è®­ç»ƒæœŸé—´æœªè§ç›®æ ‡åŸŸçš„ç¿»è¯‘ã€‚</li>
<li>Cycle GANsä¾èµ–ä¸¤ä¸ªåŸŸä¹‹é—´å­˜åœ¨å¯é€†æ˜ å°„ï¼Œä½†æŸäº›å›¾åƒè½¬æ¢ï¼ˆå¦‚å…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²ä¸å…ç–«è§å…‰å›¾åƒé—´è½¬æ¢ï¼‰ä¸­æ­¤æ¡ä»¶å¹¶ä¸æ»¡è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è®­ç»ƒè®¾è®¡æ–¹æ³•ï¼Œç”¨äºè§£å†³ç‰¹å®šå›¾åƒè½¬æ¢é—®é¢˜ï¼Œå³é€šè¿‡å¼•å…¥å…ç–«è§å…‰å›¾åƒä½œä¸ºè¾…åŠ©çš„éé…å¯¹å›¾åƒåŸŸè¿›è¡Œçº¦æŸã€‚</li>
<li>æ–°æ–¹æ³•åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®šé‡å’Œå®šæ€§æµ‹è¯•ï¼Œç»“æœè¯æ˜äº†å…¶ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•çš„æ•ˆæœæå‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºå¤„ç†å¤æ‚å›¾åƒè½¬æ¢é—®é¢˜å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†è·¨å­¦ç§‘èåˆï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦å›¾åƒå¤„ç†ä¸æœºå™¨å­¦ä¹ ï¼‰åœ¨è§£å†³ç‰¹å®šé—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f3e7f2c8f6b547e5cfda44f7d51f2dd1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abfadb78af9b21221c2a37c6f5448951.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffdfb6ab93b365ba5c1ce937f218ada2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-88588e91023ccf64705b7c67715db491.jpg" align="middle">
</details>




<h2 id="FDDM-Unsupervised-Medical-Image-Translation-with-a-Frequency-Decoupled-Diffusion-Model"><a href="#FDDM-Unsupervised-Medical-Image-Translation-with-a-Frequency-Decoupled-Diffusion-Model" class="headerlink" title="FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled   Diffusion Model"></a>FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled   Diffusion Model</h2><p><strong>Authors:Yunxiang Li, Hua-Chieh Shao, Xiaoxue Qian, You Zhang</strong></p>
<p>Diffusion models have demonstrated significant potential in producing high-quality images in medical image translation to aid disease diagnosis, localization, and treatment. Nevertheless, current diffusion models have limited success in achieving faithful image translations that can accurately preserve the anatomical structures of medical images, especially for unpaired datasets. The preservation of structural and anatomical details is essential to reliable medical diagnosis and treatment planning, as structural mismatches can lead to disease misidentification and treatment errors. In this study, we introduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT conversion. FDDM first obtains the anatomical information of the CT image from the MR image through an initial conversion module. This anatomical information then guides a subsequent diffusion model to generate high-quality CT images. Our diffusion model uses a dual-path reverse diffusion process for low-frequency and high-frequency information, achieving a better balance between image quality and anatomical accuracy. We extensively evaluated FDDM using public datasets for brain MR-to-CT and pelvis MR-to-CT translations, demonstrating its superior performance to other GAN-based, VAE-based, and diffusion-based models. The evaluation metrics included Frechet Inception Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). FDDM achieved the best scores on all metrics for both datasets, particularly excelling in FID, with scores of 25.9 for brain data and 29.2 for pelvis data, significantly outperforming other methods. These results demonstrate that FDDM can generate high-quality target domain images while maintaining the accuracy of translated anatomical structures. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç¿»è¯‘ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒä»¥è¾…åŠ©ç–¾ç—…è¯Šæ–­ã€å®šä½å’Œæ²»ç–—çš„æ½œåŠ›å·¨å¤§ã€‚ç„¶è€Œï¼Œå½“å‰æ‰©æ•£æ¨¡å‹åœ¨å¿ å®å›¾åƒç¿»è¯‘æ–¹é¢å–å¾—çš„æˆæ•ˆæœ‰é™ï¼Œéš¾ä»¥å‡†ç¡®ä¿ç•™åŒ»å­¦å›¾åƒçš„ç»“æ„ä¿¡æ¯ï¼Œå°¤å…¶åœ¨éé…å¯¹æ•°æ®é›†æ–¹é¢æ›´æ˜¯å¦‚æ­¤ã€‚ä¿ç•™ç»“æ„å’Œè§£å‰–ç»†èŠ‚å¯¹äºå¯é çš„åŒ»å­¦è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œç»“æ„ä¸åŒ¹é…å¯èƒ½å¯¼è‡´ç–¾ç—…è¯¯åˆ¤å’Œæ²»ç–—å¤±è¯¯ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹MRåˆ°CTçš„è½¬æ¢å¼•å…¥äº†é¢‘ç‡è§£è€¦æ‰©æ•£æ¨¡å‹ï¼ˆFDDMï¼‰ã€‚FDDMé¦–å…ˆé€šè¿‡åˆå§‹è½¬æ¢æ¨¡å—ä»MRå›¾åƒä¸­è·å–CTå›¾åƒçš„è§£å‰–ä¿¡æ¯ã€‚ç„¶åï¼Œæ­¤è§£å‰–ä¿¡æ¯å¼•å¯¼åç»­çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„CTå›¾åƒã€‚æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹é‡‡ç”¨åŒè·¯å¾„åå‘æ‰©æ•£è¿‡ç¨‹å¤„ç†ä½é¢‘å’Œé«˜é¢‘ä¿¡æ¯ï¼Œåœ¨å›¾åƒè´¨é‡å’Œè§£å‰–å‡†ç¡®æ€§ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚æˆ‘ä»¬åˆ©ç”¨å…¬å…±æ•°æ®é›†å¯¹FDDMè¿›è¡Œäº†å¹¿æ³›çš„è„‘éƒ¨MRåˆ°CTå’Œéª¨ç›†MRåˆ°CTçš„ç¿»è¯‘è¯„ä¼°ï¼Œè¯æ˜äº†å…¶ä¼˜äºå…¶ä»–åŸºäºGANã€VAEå’Œæ‰©æ•£çš„æ¨¡å‹ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å¼—é›·æ­‡ç‰¹å…¥è·ç¦»ï¼ˆFIDï¼‰ã€å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ã€‚FDDMåœ¨ä¸¤ä¸ªæ•°æ®é›†çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æœ€ä½³æˆç»©ï¼Œç‰¹åˆ«æ˜¯åœ¨FIDä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œè„‘éƒ¨æ•°æ®å¾—åˆ†ä¸º25.9ï¼Œéª¨ç›†æ•°æ®å¾—åˆ†ä¸º29.2ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFDDMå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡åŸŸå›¾åƒï¼ŒåŒæ—¶ä¿æŒç¿»è¯‘è§£å‰–ç»“æ„çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12070v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢‘ç‡è§£è€¦æ‰©æ•£æ¨¡å‹ï¼ˆFDDMï¼‰åœ¨åŒ»å­¦å›¾åƒç¿»è¯‘ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç£å…±æŒ¯ï¼ˆMRï¼‰å›¾åƒåˆ°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒçš„è½¬æ¢ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆåˆå§‹è½¬æ¢æ¨¡å—å’ŒåŒé‡è·¯å¾„åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œæé«˜äº†å›¾åƒè´¨é‡å’Œç»“æ„å‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒFDDMåœ¨MRåˆ°CTè½¬æ¢ä¸­ä¼˜äºå…¶ä»–åŸºäºGANã€VAEå’Œæ‰©æ•£çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç¿»è¯‘ä¸­äº§ç”Ÿé«˜è´¨é‡å›¾åƒæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œæœ‰åŠ©äºç–¾ç—…è¯Šæ–­ã€å®šä½å’Œæ²»ç–—çš„è¾…åŠ©ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨å¿ å®å›¾åƒç¿»è¯‘æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å‡†ç¡®ä¿ç•™åŒ»å­¦å›¾åƒçš„ç»“æ„ç»†èŠ‚ã€‚</li>
<li>FDDMæ¨¡å‹é€šè¿‡ç»“åˆåˆå§‹è½¬æ¢æ¨¡å—å’ŒåŒé‡è·¯å¾„åå‘æ‰©æ•£è¿‡ç¨‹å®ç°åŒ»å­¦å›¾åƒçš„é«˜è´¨é‡è½¬æ¢ã€‚</li>
<li>FDDMæ¨¡å‹èƒ½å¤Ÿåœ¨MRå›¾åƒåˆ°CTå›¾åƒçš„è½¬æ¢ä¸­æ›´å¥½åœ°å¹³è¡¡å›¾åƒè´¨é‡å’Œç»“æ„å‡†ç¡®æ€§ã€‚</li>
<li>FDDMæ¨¡å‹åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–åŸºäºGANã€VAEå’Œæ‰©æ•£çš„æ¨¡å‹ã€‚</li>
<li>FDDMæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬Frechet Inception Distance (FID)ã€Peak Signal-to-Noise Ratio (PSNR)å’ŒStructural Similarity Index Measure (SSIM)ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æœ€ä½³æˆç»©ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1b2e498047ac9af8e439b084063560b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ca1b8bd30c8b11ff29e405463dc84309.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e451820915d082591eefb3de75ce1914.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c994d26ec8e9340fc481d7283cf92a0e.jpg" align="middle">
</details>




<h2 id="One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN"><a href="#One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN" class="headerlink" title="One-for-All: Towards Universal Domain Translation with a Single StyleGAN"></a>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h2><p><strong>Authors:Yong Du, Jiahui Zhan, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, Shengfeng He</strong></p>
<p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the StyleGANâ€™s latent space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç¿»è¯‘æ¨¡å‹UniTranslatorï¼Œç”¨äºåœ¨è®­ç»ƒæ•°æ®æœ‰é™å’Œè§†è§‰å·®å¼‚æ˜¾è‘—çš„æƒ…å†µä¸‹ï¼Œå®ç°ä¸åŒè§†è§‰é¢†åŸŸä¹‹é—´çš„è¡¨ç¤ºè½¬æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨CLIPçš„ä¸­ç«‹åŸŸèƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„æ¨¡å—æ¥ä»æºé¢†åŸŸå’Œç›®æ ‡é¢†åŸŸçš„åµŒå…¥ä¸­æå–æŠ½è±¡ã€é¢†åŸŸæ— å…³è¯­ä¹‰ã€‚å°†è¿™äº›æŠ½è±¡è¯­ä¹‰ä¸ç›®æ ‡ç‰¹å®šè¯­ä¹‰èåˆï¼Œå¾—åˆ°CLIPç©ºé—´å†…çš„è½¬æ¢åµŒå…¥ã€‚ä¸ºäº†å¼¥CLIPå’ŒStyleGANä¹‹é—´ä¸åŒä¸–ç•Œçš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„éçº¿æ€§æ˜ å°„å™¨â€”â€”CLIP2Pæ˜ å°„å™¨ã€‚è¯¥æ¨¡å—åˆ©ç”¨CLIPåµŒå…¥è¿›è¡Œå®šåˆ¶ï¼Œä»¥è¿‘ä¼¼StyleGANæ½œåœ¨ç©ºé—´ä¸­çš„æ½œåœ¨åˆ†å¸ƒï¼Œæœ‰æ•ˆåœ°ä½œä¸ºè¿™ä¸¤ä¸ªç©ºé—´ä¹‹é—´çš„è¿æ¥å™¨ã€‚æ‰€æå‡ºçš„UniTranslatoré€šç”¨æ€§å¼ºï¼Œèƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ï¼Œå³ä½¿åœ¨è·¨ä¸åŒè§†è§‰é¢†åŸŸçš„è§†è§‰æŒ‘æˆ˜åœºæ™¯ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUniTranslatorç”Ÿæˆçš„ç¿»è¯‘ä½œå“å±•ç¤ºäº†é¢†åŸŸç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œæ”¹è¿›çš„å›¾åƒè´¨é‡ã€‚UniTranslatorè¶…è¶Šäº†ç°æœ‰é€šç”¨æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“ä¸šæ¨¡å‹ã€‚æºä»£ç å’Œè®­ç»ƒåçš„æ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14222v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç¿»è¯‘æ¨¡å‹UniTranslatorï¼Œç”¨äºåœ¨è®­ç»ƒæ•°æ®æœ‰é™å’Œè§†è§‰å·®å¼‚æ˜¾è‘—çš„æƒ…å†µä¸‹ï¼Œå®ç°ä¸åŒè§†è§‰é¢†åŸŸä¹‹é—´çš„è¡¨ç¤ºè½¬æ¢ã€‚è¯¥æ¨¡å‹åˆ©ç”¨CLIPçš„åŸŸä¸­æ€§èƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ï¼ŒåŒæ—¶ä½¿ç”¨å•ç‹¬æ¨¡å—ä»æºåŸŸå’Œç›®æ ‡åŸŸåµŒå…¥ä¸­æå–æŠ½è±¡ã€é¢†åŸŸæ— å…³çš„è¯­ä¹‰ã€‚è¿™äº›æŠ½è±¡è¯­ä¹‰ä¸ç›®æ ‡ç‰¹å®šè¯­ä¹‰èåˆï¼Œåœ¨CLIPç©ºé—´å†…ç”Ÿæˆè½¬æ¢åçš„åµŒå…¥ã€‚ä¸ºå¼¥åˆCLIPå’ŒStyleGANä¹‹é—´é¸¿æ²Ÿï¼Œå¼•å…¥äº†æ–°çš„éçº¿æ€§æ˜ å°„å™¨CLIP2Pæ˜ å°„å™¨ã€‚è¯¥æ¨¡å—åˆ©ç”¨CLIPåµŒå…¥ï¼Œæ—¨åœ¨è¿‘ä¼¼StyleGANæ½œåœ¨ç©ºé—´ä¸­çš„æ½œåœ¨åˆ†å¸ƒï¼Œä»è€Œåœ¨è¿™ä¸¤ä¸ªç©ºé—´ä¹‹é—´å‘æŒ¥è¿æ¥ä½œç”¨ã€‚UniTranslatoråŠŸèƒ½å¼ºå¤§ï¼Œå¯æ‰§è¡ŒåŒ…æ‹¬é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘åœ¨å†…çš„å„ç§ä»»åŠ¡ï¼Œå³ä½¿åœ¨è·¨ä¸åŒè§†è§‰é¢†åŸŸçš„è§†è§‰æŒ‘æˆ˜åœºæ™¯ä¸­ä¹Ÿèƒ½ç”Ÿæˆé«˜è´¨é‡ã€å…·æœ‰é¢†åŸŸç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œæ”¹è¿›çš„å›¾åƒè´¨é‡çš„ç¿»è¯‘ã€‚å®ƒè¶…è¶Šäº†ç°æœ‰é€šç”¨æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³è¶…è¿‡äº†ä¸“ä¸šæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTranslatoræ˜¯ä¸€ä¸ªç”¨äºè§†è§‰é¢†åŸŸé—´è½¬æ¢çš„æ–°å‹ç¿»è¯‘æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨CLIPçš„åŸŸä¸­æ€§èƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ã€‚</li>
<li>UniTranslatorä½¿ç”¨å•ç‹¬æ¨¡å—æå–æºåŸŸå’Œç›®æ ‡åŸŸçš„æŠ½è±¡ã€é¢†åŸŸæ— å…³çš„è¯­ä¹‰ã€‚</li>
<li>CLIP2Pæ˜ å°„å™¨çš„å¼•å…¥ï¼Œæ—¨åœ¨å¼¥åˆCLIPå’ŒStyleGANä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
<li>UniTranslatorèƒ½æ‰§è¡Œå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ã€‚</li>
<li>UniTranslatorèƒ½ç”Ÿæˆé«˜è´¨é‡ã€å…·æœ‰é¢†åŸŸç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œæ”¹è¿›çš„å›¾åƒè´¨é‡çš„ç¿»è¯‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-95e6272f85d7fa4d37d1ae29b4643ac3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7a24b3f85d591a3f1544bb1f8be5a6ea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0f2ec04d706bea45231594a7de44808b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-32e26e6d642dd74ec5491b260c7f5d69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ed3350cdfd555c1320fc865348c6003.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b5fcd9c09d6cdabfa03da9bbb7f05557.jpg" align="middle">
</details>




<h2 id="RL-I2IT-Image-to-Image-Translation-with-Deep-Reinforcement-Learning"><a href="#RL-I2IT-Image-to-Image-Translation-with-Deep-Reinforcement-Learning" class="headerlink" title="RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning"></a>RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning</h2><p><strong>Authors:Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu, Hongtu Zhu, Xin Li, Siwei Lyu</strong></p>
<p>Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dimensional action. In the RL-I2IT framework, we also employ a task-specific auxiliary learning strategy to stabilize the training process and improve the performance of the corresponding task. Experiments on several I2IT tasks demonstrate the effectiveness and robustness of the proposed method when facing high-dimensional continuous action space problems. Our implementation of the RL-I2IT framework is available at <a target="_blank" rel="noopener" href="https://github.com/Algolzw/SPAC-Deformable-Registration">https://github.com/Algolzw/SPAC-Deformable-Registration</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å¤šæ•°å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼ˆI2ITï¼‰æ–¹æ³•éƒ½æ˜¯ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹åœ¨ä¸€æ¬¡è¿è¡Œä¸­ç”Ÿæˆå›¾åƒã€‚ç„¶è€Œï¼Œè®¾è®¡è¿™æ ·çš„å•æ­¥æ¨¡å‹æ€»æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦å¤§é‡çš„å‚æ•°ï¼Œå¹¶ä¸”å®¹æ˜“é™·å…¥ç³Ÿç³•çš„å…¨å±€æœ€å°å€¼å’Œè¿‡åº¦æ‹Ÿåˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰é‡æ–°å®šä¹‰äº†I2ITä½œä¸ºä¸€ä¸ªé€æ­¥å†³ç­–é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„I2ITï¼ˆRL-I2ITï¼‰æ–°å‹æ¡†æ¶ã€‚RL-I2ITæ¡†æ¶çš„å…³é”®åŠŸèƒ½æ˜¯å°†å•ä¸€çš„å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºå…·æœ‰å°å‹æ¨¡å‹çš„å¤šä¸ªæ­¥éª¤ï¼Œä»¥é€æ­¥å°†æºå›¾åƒè¿ç»­å˜æ¢ä¸ºç›®æ ‡å›¾åƒã€‚è€ƒè™‘åˆ°åœ¨ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­å¤„ç†é«˜ç»´è¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…ƒç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†æ–°æ¦‚å¿µè®¡åˆ’åˆ°æ ‡å‡†çš„Actor-Criticæ¨¡å‹ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªæ¯”åŸå§‹å›¾åƒæ›´ä½ç»´åº¦çš„æ¦‚å¿µï¼Œå¯ä»¥ä¿ƒè¿›Actorç”Ÿæˆå¯æ§åˆ¶çš„é«˜ç»´åŠ¨ä½œã€‚åœ¨RL-I2ITæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨ç‰¹å®šä»»åŠ¡çš„è¾…åŠ©å­¦ä¹ ç­–ç•¥æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜ç›¸åº”ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨å‡ ä¸ªI2ITä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨é¢å¯¹é«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´é—®é¢˜æ—¶ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬çš„RL-I2ITæ¡†æ¶å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Algolzw/SPAC-Deformable-Registration%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Algolzw/SPAC-Deformable-Registrationä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13672v7">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼ˆI2ITï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿçš„I2ITæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚éœ€è¦å¤§é‡å‚æ•°å’Œå®¹æ˜“é™·å…¥ä¸è‰¯å…¨å±€æœ€å°å€¼å’Œè¿‡åº¦æ‹Ÿåˆã€‚æœ¬ç ”ç©¶å°†I2ITé‡æ–°æ„å»ºä¸ºåŸºäºæ­¥éª¤çš„å†³ç­–é—®é¢˜ï¼Œå¹¶æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„I2ITæ¡†æ¶ï¼ˆRL-I2ITï¼‰ã€‚é€šè¿‡åˆ†è§£å•ä¸€å­¦ä¹ è¿‡ç¨‹ä¸ºå¤šä¸ªå°æ­¥éª¤ï¼Œä½¿ç”¨è½»é‡çº§æ¨¡å‹é€æ­¥å°†æºå›¾åƒè½¬æ¢ä¸ºç›®æ ‡å›¾åƒã€‚ä¸ºè§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ¡†æ¶å¤„ç†é«˜ç»´è¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„é—®é¢˜ï¼Œå¼•å…¥å…ƒæ”¿ç­–å’Œè®¡åˆ’æ¦‚å¿µåˆ°æ ‡å‡†Actor-Criticæ¨¡å‹ä¸­ï¼Œé™ä½ç»´åº¦å¹¶ä¿ƒè¿›ç”Ÿæˆå¯è¿½è¸ªçš„é«˜ç»´åŠ¨ä½œã€‚åŒæ—¶é‡‡ç”¨ä»»åŠ¡ç‰¹å®šçš„è¾…åŠ©å­¦ä¹ ç­–ç•¥æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æ”¹è¿›ä»»åŠ¡æ€§èƒ½ã€‚åœ¨å¤šä¸ªI2ITä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å¤„ç†é«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å°†ä¼ ç»Ÿçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼ˆI2ITï¼‰æ–¹æ³•é‡æ–°æ„å»ºä¸ºåŸºäºæ­¥éª¤çš„å†³ç­–é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„I2ITæ¡†æ¶ï¼ˆRL-I2ITï¼‰ï¼Œé€šè¿‡åˆ†è§£å•ä¸€å­¦ä¹ è¿‡ç¨‹ä¸ºå¤šä¸ªå°æ­¥éª¤æ¥è§£å†³ä¼ ç»ŸI2ITé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å…ƒæ”¿ç­–å’Œè®¡åˆ’æ¦‚å¿µæ¥è§£å†³é«˜ç»´è¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„é—®é¢˜ï¼Œä½¿å¾—å¼ºåŒ–å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å›¾åƒè½¬æ¢ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨ä»»åŠ¡ç‰¹å®šçš„è¾…åŠ©å­¦ä¹ ç­–ç•¥æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†é«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶çš„å®ç°ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šå¯ä¾›å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f361861676b29cf916d3420e8181d712.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c58af038f40fea38ddc32cbf07c73153.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e6d58993ad91ecc7661f77c504c782d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e45581393a587038f09dc17aa45fc9cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-937bbf91c309ca7aedf958e5e20fa69a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-25e7d2205f30b34b480b2e864b2270c4.jpg" align="middle">
</details>




<h2 id="The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI"><a href="#The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI" class="headerlink" title="The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI"></a>The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI</h2><p><strong>Authors:Ahmed W. Moawad, Anastasia Janas, Ujjwal Baid, Divya Ramakrishnan, Rachit Saluja, Nader Ashraf, Nazanin Maleki, Leon Jekel, Nikolay Yordanov, Pascal Fehringer, Athanasios Gkampenis, Raisa Amiruddin, Amirreza Manteghinejad, Maruf Adewole, Jake Albrecht, Udunna Anazodo, Sanjay Aneja, Syed Muhammad Anwar, Timothy Bergquist, Veronica Chiang, Verena Chung, Gian Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Nastaran Khalili, Keyvan Farahani, Juan Eugenio Iglesias, Zhifan Jiang, Elaine Johanson, Anahita Fathi Kazerooni, Florian Kofler, Kiril Krantchev, Dominic LaBella, Koen Van Leemput, Hongwei Bran Li, Marius George Linguraru, Xinyang Liu, Zeke Meier, Bjoern H Menze, Harrison Moy, Klara Osenberg, Marie Piraud, Zachary Reitman, Russell Takeshi Shinohara, Chunhao Wang, Benedikt Wiestler, Walter Wiggins, Umber Shafique, Klara Willms, Arman Avesta, Khaled Bousabarah, Satrajit Chakrabarty, Nicolo Gennaro, Wolfgang Holler, Manpreet Kaur, Pamela LaMontagne, MingDe Lin, Jan Lost, Daniel S. Marcus, Ryan Maresca, Sarah Merkaj, Gabriel Cassinelli Pedersen, Marc von Reppert, Aristeidis Sotiras, Oleg Teytelboym, Niklas Tillmans, Malte Westerhoff, Ayda Youssef, Devon Godfrey, Scott Floyd, Andreas Rauschecker, Javier Villanueva-Meyer, Irada Pfluger, Jaeyoung Cho, Martin Bendszus, Gianluca Brugnara, Justin Cramer, Gloria J. Guzman Perez-Carillo, Derek R. Johnson, Anthony Kam, Benjamin Yin Ming Kwan, Lillian Lai, Neil U. Lall, Fatima Memon, Mark Krycia, Satya Narayana Patro, Bojan Petrovic, Tiffany Y. So, Gerard Thompson, Lei Wu, E. Brooke Schrickel, Anu Bansal, Frederik Barkhof, Cristina Besada, Sammy Chu, Jason Druzgal, Alexandru Dusoi, Luciano Farage, Fabricio Feltrin, Amy Fong, Steve H. Fung, R. Ian Gray, Ichiro Ikuta, Michael Iv, Alida A. Postma, Amit Mahajan, David Joyner, Chase Krumpelman, Laurent Letourneau-Guillon, Christie M. Lincoln, Mate E. Maros, Elka Miller, Fanny Moron, Esther A. Nimchinsky, Ozkan Ozsarlak, Uresh Patel, Saurabh Rohatgi, Atin Saha, Anousheh Sayah, Eric D. Schwartz, Robert Shih, Mark S. Shiroishi, Juan E. Small, Manoj Tanwar, Jewels Valerie, Brent D. Weinberg, Matthew L. White, Robert Young, Vahe M. Zohrabian, Aynur Azizova, Melanie Maria Theresa Bruseler, Mohanad Ghonim, Mohamed Ghonim, Abdullah Okar, Luca Pasquini, Yasaman Sharifi, Gagandeep Singh, Nico Sollmann, Theodora Soumala, Mahsa Taherzadeh, Philipp Vollmuth, Martha Foltyn-Dumitru, Ajay Malhotra, Aly H. Abayazeed, Francesco Dellepiane, Philipp Lohmann, Victor M. Perez-Garcia, Hesham Elhalawani, Maria Correia de Verdier, Sanaria Al-Rubaiey, Rui Duarte Armindo, Kholod Ashraf, Moamen M. Asla, Mohamed Badawy, Jeroen Bisschop, Nima Broomand Lomer, Jan Bukatz, Jim Chen, Petra Cimflova, Felix Corr, Alexis Crawley, Lisa Deptula, Tasneem Elakhdar, Islam H. Shawali, Shahriar Faghani, Alexandra Frick, Vaibhav Gulati, Muhammad Ammar Haider, Fatima Hierro, Rasmus Holmboe Dahl, Sarah Maria Jacobs, Kuang-chun Jim Hsieh, Sedat G. Kandemirli, Katharina Kersting, Laura Kida, Sofia Kollia, Ioannis Koukoulithras, Xiao Li, Ahmed Abouelatta, Aya Mansour, Ruxandra-Catrinel Maria-Zamfirescu, Marcela Marsiglia, Yohana Sarahi Mateo-Camacho, Mark McArthur, Olivia McDonnell, Maire McHugh, Mana Moassefi, Samah Mostafa Morsi, Alexander Munteanu, Khanak K. Nandolia, Syed Raza Naqvi, Yalda Nikanpour, Mostafa Alnoury, Abdullah Mohamed Aly Nouh, Francesca Pappafava, Markand D. Patel, Samantha Petrucci, Eric Rawie, Scott Raymond, Borna Roohani, Sadeq Sabouhi, Laura M. Sanchez-Garcia, Zoe Shaked, Pokhraj P. Suthar, Talissa Altes, Edvin Isufi, Yaseen Dhemesh, Jaime Gass, Jonathan Thacker, Abdul Rahman Tarabishy, Benjamin Turner, Sebastiano Vacca, George K. Vilanilam, Daniel Warren, David Weiss, Fikadu Worede, Sara Yousry, Wondwossen Lerebo, Alejandro Aristizabal, Alexandros Karargyris, Hasan Kassem, Sarthak Pati, Micah Sheller, Katherine E. Link, Evan Calabrese, Nourel hoda Tahon, Ayman Nada, Yuri S. Velichko, Spyridon Bakas, Jeffrey D. Rudie, Mariam Aboian</strong></p>
<p>The translation of AI-generated brain metastases (BM) segmentation into clinical practice relies heavily on diverse, high-quality annotated medical imaging datasets. The BraTS-METS 2023 challenge has gained momentum for testing and benchmarking algorithms using rigorously annotated internationally compiled real-world datasets. This study presents the results of the segmentation challenge and characterizes the challenging cases that impacted the performance of the winning algorithms. Untreated brain metastases on standard anatomic MRI sequences (T1, T2, FLAIR, T1PG) from eight contributed international datasets were annotated in stepwise method: published UNET algorithms, student, neuroradiologist, final approver neuroradiologist. Segmentations were ranked based on lesion-wise Dice and Hausdorff distance (HD95) scores. False positives (FP) and false negatives (FN) were rigorously penalized, receiving a score of 0 for Dice and a fixed penalty of 374 for HD95. Eight datasets comprising 1303 studies were annotated, with 402 studies (3076 lesions) released on Synapse as publicly available datasets to challenge competitors. Additionally, 31 studies (139 lesions) were held out for validation, and 59 studies (218 lesions) were used for testing. Segmentation accuracy was measured as rank across subjects, with the winning team achieving a LesionWise mean score of 7.9. Common errors among the leading teams included false negatives for small lesions and misregistration of masks in space.The BraTS-METS 2023 challenge successfully curated well-annotated, diverse datasets and identified common errors, facilitating the translation of BM segmentation across varied clinical environments and providing personalized volumetric reports to patients undergoing BM treatment. </p>
<blockquote>
<p>å°†äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è„‘è½¬ç§»ç˜¤ï¼ˆBMï¼‰åˆ†å‰²è½¬åŒ–ä¸ºä¸´åºŠå®è·µï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤šæ ·ä¸”é«˜è´¨é‡æ ‡æ³¨çš„åŒ»å­¦å½±åƒæ•°æ®é›†ã€‚BraTS-METS 2023æŒ‘æˆ˜èµ›åˆ©ç”¨ä¸¥æ ¼æ ‡æ³¨çš„å›½é™…çœŸå®æ•°æ®é›†å¯¹ç®—æ³•è¿›è¡Œæµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ï¼Œä»è€Œè·å¾—äº†åŠ¨åŠ›ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†åˆ†å‰²æŒ‘æˆ˜çš„ç»“æœï¼Œå¹¶å¯¹å½±å“è·èƒœç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§ç—…ä¾‹è¿›è¡Œäº†ç‰¹å¾æè¿°ã€‚å¯¹æ¥è‡ªå…«ä¸ªå›½é™…æ•°æ®é›†çš„æœªç»æ²»ç–—çš„è„‘è½¬ç§»ç˜¤åœ¨æ ‡å‡†è§£å‰–MRIåºåˆ—ï¼ˆT1ã€T2ã€FLAIRã€T1PGï¼‰ä¸Šé‡‡ç”¨é€æ­¥æ–¹æ³•è¿›è¡Œäº†æ ‡æ³¨ï¼šå…¬å¼€UNETç®—æ³•ã€å­¦ç”Ÿã€ç¥ç»æ”¾å°„å­¦å®¶ã€æœ€ç»ˆå®¡æ‰¹ç¥ç»æ”¾å°„å­¦å®¶ã€‚æ ¹æ®ç—…ç¶çº§åˆ«çš„Diceå’ŒHausdorffè·ç¦»ï¼ˆHD95ï¼‰å¾—åˆ†å¯¹åˆ†å‰²è¿›è¡Œæ’åã€‚å‡é˜³æ€§ï¼ˆFPï¼‰å’Œå‡é˜´æ€§ï¼ˆFNï¼‰å—åˆ°ä¸¥æ ¼æƒ©ç½šï¼ŒDiceå¾—åˆ†ä¸º0ï¼ŒHD95çš„å›ºå®šæƒ©ç½šä¸º374ã€‚å¯¹åŒ…å«1303é¡¹ç ”ç©¶çš„å…«ä¸ªæ•°æ®é›†è¿›è¡Œäº†æ ‡æ³¨ï¼Œå…¶ä¸­åœ¨Synapseä¸Šå‘å¸ƒäº†åŒ…å«å…¬ä¼—å¯ç”¨æ•°æ®é›†çš„402é¡¹ç ”ç©¶ï¼ˆæ¶‰åŠ3076ä¸ªç—…ç¶ï¼‰ä»¥æŒ‘æˆ˜ç«äº‰å¯¹æ‰‹ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰31é¡¹ç ”ç©¶ï¼ˆæ¶‰åŠ139ä¸ªç—…ç¶ï¼‰ç”¨äºéªŒè¯ï¼Œè¿˜æœ‰59é¡¹ç ”ç©¶ï¼ˆæ¶‰åŠ218ä¸ªç—…ç¶ï¼‰ç”¨äºæµ‹è¯•ã€‚åˆ†å‰²ç²¾åº¦æ˜¯é€šè¿‡ä¸åŒå—è¯•è€…ä¹‹é—´çš„æ’åæ¥è¡¡é‡çš„ï¼Œå† å†›å›¢é˜Ÿçš„ç—…ç¶çº§å¹³å‡å¾—åˆ†ä¸º7.9åˆ†ã€‚é¡¶å°–å›¢é˜Ÿå¸¸è§çš„é”™è¯¯åŒ…æ‹¬å°ç—…ç¶çš„å‡é˜´æ€§ä»¥åŠå£ç½©åœ¨ç©ºé—´ä¸Šçš„é”™ä½ã€‚BraTS-METS 2023æŒ‘æˆ˜èµ›æˆåŠŸåœ°ç²¾å¿ƒæŒ‘é€‰äº†æ ‡æ³¨è‰¯å¥½ä¸”å¤šæ ·çš„æ•°æ®é›†ï¼Œå¹¶ç¡®å®šäº†å¸¸è§é”™è¯¯ï¼Œä¿ƒè¿›äº†BMåˆ†å‰²åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„è½¬åŒ–ï¼Œå¹¶ä¸ºæ¥å—BMæ²»ç–—çš„ç—…äººæä¾›ä¸ªæ€§åŒ–çš„ä½“ç§¯æŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.00838v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦é¢†åŸŸçš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è„‘è½¬ç§»ç˜¤åˆ†å‰²ä¸Šçš„åº”ç”¨ã€‚æ–‡ç« å¼ºè°ƒäº†BrATS-METS 2023æŒ‘æˆ˜èµ›åœ¨æ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ä¸­çš„ä½œç”¨ï¼Œè¯¥æŒ‘æˆ˜æ—¨åœ¨æµ‹è¯•ç®—æ³•ä½¿ç”¨ä¸¥æ ¼æ³¨é‡Šçš„å›½é™…æ±‡ç¼–çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚æ–‡ç« æ€»ç»“äº†æŒ‘æˆ˜èµ›çš„ç»“æœï¼Œå¹¶æè¿°äº†å½±å“ç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ã€‚æœ€åæŒ‡å‡ºï¼ŒæŒ‘æˆ˜èµ›æˆåŠŸç­›é€‰å‡ºäº†é«˜è´¨é‡çš„å¤šæ ·æ•°æ®é›†ï¼Œä¿ƒè¿›äº†BMåˆ†å‰²æŠ€æœ¯åœ¨ä¸åŒä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚åŒæ—¶ä¸ºæ‚£è€…æä¾›äº†ä¸ªæ€§åŒ–çš„ä½“ç§¯æŠ¥å‘Šä»¥æŒ‡å¯¼BMæ²»ç–—ã€‚ç®€è¨€ä¹‹ï¼Œè¯¥ç ”ç©¶å¯¹åŒ»å­¦æˆåƒç¿»è¯‘å…·æœ‰ç§¯æå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ol>
<li>AIåœ¨åŒ»å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ä¸­ï¼Œå¤§è„‘è½¬ç§»ç˜¤åˆ†å‰²æŠ€æœ¯çš„åº”ç”¨å¾—åˆ°äº†é‡ç‚¹å‘å±•ã€‚è¿™æ¶‰åŠåˆ°æœºå™¨å­¦ä¹ ç®—æ³•çš„æ·±åº¦åº”ç”¨ä»¥åŠåŒ»å­¦å½±åƒæ•°æ®çš„å¤„ç†å’Œè§£æã€‚ </li>
<li>BraTS-METS 2023æŒ‘æˆ˜èµ›æ˜¯ä¸€ä¸ªé‡è¦äº‹ä»¶ï¼Œç”¨äºæµ‹è¯•å’Œè¯„ä¼°é’ˆå¯¹çœŸå®ä¸–ç•Œæ•°æ®é›†çš„ç®—æ³•æ€§èƒ½ã€‚è¿™ä¸€æŒ‘æˆ˜åˆ©ç”¨äº†å›½é™…ä¸Šæ±‡ç¼–çš„ä¸¥æ ¼æ³¨é‡Šçš„æ•°æ®é›†è¿›è¡Œå®è·µæµ‹è¯•ã€‚ </li>
<li>æ–‡ç« å¯¹æŒ‘æˆ˜èµ›çš„ç»“æœè¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œå¹¶æŒ‡å‡ºäº†å½±å“ç®—æ³•æ€§èƒ½çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬å¯¹å°ç—…ç¶çš„æ¼æ£€ä»¥åŠç©ºé—´å®šä½é”™è¯¯ç­‰é—®é¢˜ã€‚ </li>
<li>è¯¥æŒ‘æˆ˜èµ›æˆåŠŸæ„å»ºäº†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä¿ƒè¿›äº†å¤§è„‘è½¬ç§»ç˜¤åˆ†å‰²æŠ€æœ¯åœ¨ä¸åŒä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚è¿™å¯¹äºåŒ»å­¦å›¾åƒç¿»è¯‘å’Œäººå·¥æ™ºèƒ½åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨å…·æœ‰ç§¯æå½±å“ã€‚ </li>
<li>é€šè¿‡æŒ‘æˆ˜èµ›çš„ç»“æœï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å‡ºå½“å‰äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„å¸¸è§é”™è¯¯å’ŒæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ”¹è¿›æä¾›äº†æ–¹å‘ã€‚ </li>
<li>è¯¥ç ”ç©¶ä¸ºæ‚£è€…æä¾›äº†ä¸ªæ€§åŒ–çš„ä½“ç§¯æŠ¥å‘Šï¼Œæœ‰åŠ©äºåŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­å’Œå¤„ç†å¤§è„‘è½¬ç§»ç˜¤çš„é—®é¢˜ï¼Œè¿›è€Œä¸ºè‚¿ç˜¤æ²»ç–—æä¾›æ›´åŠ ç²¾ç¡®çš„æŒ‡å¯¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b168721366c887167f46d5b0033e7041.jpg" align="middle">
</details>




<h2 id="Vision-and-Language-Pretraining"><a href="#Vision-and-Language-Pretraining" class="headerlink" title="Vision-and-Language Pretraining"></a>Vision-and-Language Pretraining</h2><p><strong>Authors:Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu, See-Kiong Ng, Anh Tuan Luu</strong></p>
<p>With the burgeoning amount of data of image-text pairs and diversity of Vision-and-Language (V&amp;L) tasks, scholars have introduced an abundance of deep learning models in this research domain. Furthermore, in recent years, transfer learning has also shown tremendous success in Computer Vision for tasks such as Image Classification, Object Detection, etc., and in Natural Language Processing for Question Answering, Machine Translation, etc. Inheriting the spirit of Transfer Learning, research works in V&amp;L have devised multiple pretraining techniques on large-scale datasets in order to enhance the performance of downstream tasks. The aim of this article is to provide a comprehensive revision of contemporary V&amp;L pretraining models. In particular, we categorize and delineate pretraining approaches, along with the summary of state-of-the-art vision-and-language pretrained models. Moreover, a list of training datasets and downstream tasks is supplied to further polish the perspective into V&amp;L pretraining. Lastly, we decided to take a further step to discuss numerous directions for future research. </p>
<blockquote>
<p>éšç€å›¾åƒæ–‡æœ¬å¯¹æ•°æ®çš„ä¸æ–­å¢åŠ å’Œè§†è§‰ä¸è¯­è¨€ï¼ˆV&amp;Lï¼‰ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œå­¦è€…ä»¬åœ¨è¿™ä¸ªç ”ç©¶é¢†åŸŸä¸­å·²ç»å¼•å…¥äº†å¤§é‡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿‘å¹´æ¥ï¼Œè¿ç§»å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰çš„å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†çš„é—®ç­”ã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç§‰æ‰¿è¿ç§»å­¦ä¹ çš„ç²¾ç¥ï¼Œè§†è§‰ä¸è¯­è¨€é¢†åŸŸçš„ç ”ç©¶å·¥ä½œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®¾è®¡äº†å¤šç§é¢„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯å¯¹å½“ä»£è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå…¨é¢å›é¡¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ–¹æ³•è¿›è¡Œåˆ†ç±»å’Œæè¿°ï¼Œä»¥åŠå¯¹æœ€å…ˆè¿›çš„è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ€»ç»“ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ç³»åˆ—è®­ç»ƒæ•°æ®é›†å’Œä¸‹æ¸¸ä»»åŠ¡ï¼Œä»¥è¿›ä¸€æ­¥æ·±å…¥äº†è§£è§†è§‰ä¸è¯­è¨€çš„é¢„è®­ç»ƒã€‚æœ€åï¼Œæˆ‘ä»¬å†³å®šè¿›ä¸€æ­¥è®¨è®ºæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2207.01772v3">PDF</a> The content of the paper has been outdated. I would like to rewrite a   new version with completely new information.</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å›¾åƒæ–‡æœ¬å¯¹æ•°æ®çš„ä¸æ–­å¢åŠ å’Œè§†è§‰ä¸è¯­è¨€ï¼ˆV&amp;Lï¼‰ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œå­¦è€…ä»¬åœ¨æ­¤ç ”ç©¶é¢†åŸŸä¸­å¼•å…¥äº†å¤§é‡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿‘å¹´æ¥ï¼Œè¿ç§»å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é—®ç­”ã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç§‰æ‰¿è¿ç§»å­¦ä¹ çš„ç²¾ç¥ï¼Œè§†è§‰ä¸è¯­è¨€é¢†åŸŸçš„ç ”ç©¶å·¥ä½œåœ¨å¤§å‹æ•°æ®é›†ä¸Šè®¾è®¡äº†å¤šç§é¢„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢å›é¡¾å½“ä»£çš„è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬åˆ†ç±»å¹¶æ¦‚è¿°äº†é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œæ€»ç»“äº†æœ€æ–°çš„è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†è®­ç»ƒæ•°æ®é›†å’Œä¸‹æ¸¸ä»»åŠ¡çš„åˆ—è¡¨ï¼Œä»¥è¿›ä¸€æ­¥æ·±å…¥äº†è§£è§†è§‰ä¸è¯­è¨€çš„é¢„è®­ç»ƒã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æœªæ¥çš„ç ”ç©¶æ–¹å‘è¿›è¡Œäº†æ·±å…¥çš„æ¢è®¨ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è§†è§‰ä¸è¯­è¨€ï¼ˆV&amp;Lï¼‰ç ”ç©¶é¢†åŸŸå¼•å…¥äº†å¤§é‡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåº”å¯¹å›¾åƒæ–‡æœ¬å¯¹æ•°æ®çš„å¢åŠ å’Œä»»åŠ¡å¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>è¿ç§»å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>è§†è§‰ä¸è¯­è¨€é¢†åŸŸçš„ç ”ç©¶å·¥ä½œè®¾è®¡äº†å¤šç§é¢„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨å…¨é¢å›é¡¾å½“ä»£çš„è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒæ–¹æ³•çš„åˆ†ç±»å’Œæ¦‚è¿°ã€‚</li>
<li>æ€»ç»“äº†æœ€æ–°çš„è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æä¾›äº†è®­ç»ƒæ•°æ®é›†å’Œä¸‹æ¸¸ä»»åŠ¡çš„åˆ—è¡¨ï¼Œä»¥æ›´å¥½åœ°äº†è§£è§†è§‰ä¸è¯­è¨€çš„é¢„è®­ç»ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-901ffeab1491e4333b404e24ecdba330.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-680b620c5c1075fc76b70227619887d9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6b1c31a0d8f9f8fc8e1eacd6162a387c.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Exploring What Why and How A Multifaceted Benchmark for Causation   Understanding of Video Anomaly
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
