<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2024-12-12  BLADE Single-view Body Mesh Learning through Accurate Depth Estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6a7ba1e0b1885451cc6ff5247acccb5c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    24.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    102 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="BLADE-Single-view-Body-Mesh-Learning-through-Accurate-Depth-Estimation"><a href="#BLADE-Single-view-Body-Mesh-Learning-through-Accurate-Depth-Estimation" class="headerlink" title="BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation"></a>BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation</h2><p><strong>Authors:Shengze Wang, Jiefeng Li, Tianye Li, Ye Yuan, Henry Fuchs, Koki Nagano, Shalini De Mello, Michael Stengel</strong></p>
<p>Single-image human mesh recovery is a challenging task due to the ill-posed nature of simultaneous body shape, pose, and camera estimation. Existing estimators work well on images taken from afar, but they break down as the person moves close to the camera. Moreover, current methods fail to achieve both accurate 3D pose and 2D alignment at the same time. Error is mainly introduced by inaccurate perspective projection heuristically derived from orthographic parameters. To resolve this long-standing challenge, we present our method BLADE which accurately recovers perspective parameters from a single image without heuristic assumptions. We start from the inverse relationship between perspective distortion and the person’s Z-translation Tz, and we show that Tz can be reliably estimated from the image. We then discuss the important role of Tz for accurate human mesh recovery estimated from close-range images. Finally, we show that, once Tz and the 3D human mesh are estimated, one can accurately recover the focal length and full 3D translation. Extensive experiments on standard benchmarks and real-world close-range images show that our method is the first to accurately recover projection parameters from a single image, and consequently attain state-of-the-art accuracy on 3D pose estimation and 2D alignment for a wide range of images. <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/amri/projects/blade/">https://research.nvidia.com/labs/amri/projects/blade/</a> </p>
<blockquote>
<p>单图像人体网格恢复是一项具有挑战性的任务，因为同时进行身体形状、姿势和相机估计是不适定的性质。现有估计器在远距离拍摄的图像上表现良好，但当人物靠近相机时，它们会失效。此外，当前的方法无法同时实现准确的3D姿势和2D对齐。误差主要是由透视投影的不准确启发式从正交参数衍生出来的。为了解决这一长期存在的挑战，我们提出了BLADE方法，它能够准确地从单幅图像中恢复透视参数，无需启发式假设。我们从透视失真与人的Z平移Tz之间的逆向关系开始，并展示可以从图像可靠地估计Tz。然后，我们讨论了Tz对于从近距离图像准确估计人体网格恢复的重要作用。最后，我们展示一旦估计出Tz和3D人体网格，便可以准确恢复焦距和完整的3D平移。在标准基准测试和真实世界的近距离图像上的广泛实验表明，我们的方法首次从单幅图像准确恢复了投影参数，因此在广泛范围的图像上实现了最先进的3D姿势估计和2D对齐精度。详情请参阅：[<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/amri/projects/blade/]">https://research.nvidia.com/labs/amri/projects/blade/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08640v1">PDF</a> </p>
<p><strong>Summary</strong><br>单图像人体网格恢复是一项具有挑战性的任务，因为需要同时估计身体形状、姿势和相机参数。现有方法在处理远距离拍摄的人体图像时效果较好，但当人体靠近相机时，效果明显下降。当前方法难以同时实现准确的3D姿态和2D对齐。为解决这一难题，我们提出了一种方法BLADE，可以从单幅图像准确恢复透视参数，无需启发式假设。我们利用透视畸变与人的Z轴平移Tz之间的逆向关系，可靠地从图像估计Tz。然后，我们强调了Tz在基于近距离图像的人体网格恢复中的重要作用。最后，我们展示了在估计Tz和3D人体网格后，可以准确恢复焦距和完整的3D平移。在标准基准测试和真实世界近距离图像上的大量实验表明，我们的方法首次从单幅图像准确恢复了投影参数，并在3D姿态估计和2D对齐方面达到了最先进的水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>单图像人体网格恢复是同时估计身体形状、姿势和相机参数的挑战任务。</li>
<li>现有方法在近距离图像上效果不佳。</li>
<li>当前方法难以实现准确的3D姿态和2D对齐同时达成。</li>
<li>BLADE方法能准确从单幅图像恢复透视参数，无需启发式假设。</li>
<li>利用透视畸变与人的Z轴平移Tz的逆向关系估计Tz。</li>
<li>Tz在基于近距离图像的人体网格恢复中起重要作用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a69ec16cb9d31154bff9a5a40c602b0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6a7ba1e0b1885451cc6ff5247acccb5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-801f314020b37461fc778d6185f8c28e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-df5c14b6510e17b5fc939d55cdb880fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c19bc26f797d6dc98712db3613c65f94.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-634dce97742c5257dec5c7e244474f3d.jpg" align="middle">
</details>




<h2 id="TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person"><a href="#TryOffAnyone-Tiled-Cloth-Generation-from-a-Dressed-Person" class="headerlink" title="TryOffAnyone: Tiled Cloth Generation from a Dressed Person"></a>TryOffAnyone: Tiled Cloth Generation from a Dressed Person</h2><p><strong>Authors:Ioannis Xarchakos, Theodoros Koukopoulos</strong></p>
<p>The fashion industry is increasingly leveraging computer vision and deep learning technologies to enhance online shopping experiences and operational efficiencies. In this paper, we address the challenge of generating high-fidelity tiled garment images essential for personalized recommendations, outfit composition, and virtual try-on systems from photos of garments worn by models. Inspired by the success of Latent Diffusion Models (LDMs) in image-to-image translation, we propose a novel approach utilizing a fine-tuned StableDiffusion model. Our method features a streamlined single-stage network design, which integrates garmentspecific masks to isolate and process target clothing items effectively. By simplifying the network architecture through selective training of transformer blocks and removing unnecessary crossattention layers, we significantly reduce computational complexity while achieving state-of-the-art performance on benchmark datasets like VITON-HD. Experimental results demonstrate the effectiveness of our approach in producing high-quality tiled garment images for both full-body and half-body inputs. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone">https://github.com/ixarchakos/try-off-anyone</a> </p>
<blockquote>
<p>时尚产业正越来越多地利用计算机视觉和深度学习技术，以提升在线购物体验和运营效率。在本文中，我们面对的挑战是从模特所穿服装的照片生成高质量拼贴服装图像，这对于个性化推荐、服装搭配和虚拟试穿系统至关重要。受潜在扩散模型（Latent Diffusion Models，简称LDM）在图到图翻译中的成功的启发，我们提出了一种利用精细调整过的StableDiffusion模型的新方法。我们的方法采用简洁的单阶段网络设计，结合服装特定掩膜，有效地隔离和处理目标服装项目。通过有选择地训练变压器块并删除不必要的交叉注意力层，我们简化了网络架构，在计算复杂度方面实现了显著减少，同时在VITON-HD等基准数据集上实现了最先进的性能。实验结果表明，我们的方法在全身体和半身输入情况下生成高质量拼贴服装图像方面非常有效。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/ixarchakos/try-off-anyone%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ixarchakos/try-off-anyone找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08573v1">PDF</a> </p>
<p><strong>Summary</strong><br>时尚产业正积极运用计算机视觉和深度学习技术，以提升在线购物体验和运营效率。本文关注生成高质量的分块服装图像的挑战，这对于个性化推荐、服装搭配和虚拟试衣系统至关重要。受潜在扩散模型（Latent Diffusion Models，简称LDMs）在图到图转换中的成功的启发，本文提出了一种利用fine-tuned StableDiffusion模型的新方法。该方法具有简化的单阶段网络设计，通过集成服装特定掩膜，有效隔离和处理目标服装项目。通过选择性训练变压器块并去除不必要的交叉注意力层，简化了网络架构，在VITON-HD等基准数据集上实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时尚产业运用计算机视觉和深度学习技术提升在线购物体验及运营效率。</li>
<li>生成高质量分块服装图像对于个性化推荐、服装搭配和虚拟试衣系统至关重要。</li>
<li>受到潜在扩散模型（Latent Diffusion Models）成功的启发，提出使用fine-tuned StableDiffusion模型的新方法。</li>
<li>新方法具有简化的单阶段网络设计，集成服装特定掩膜以隔离和处理目标服装项目。</li>
<li>通过选择性训练和优化网络架构，实现了在基准数据集上的卓越性能。</li>
<li>方法能够处理全身和半身输入，生成高质量的分块服装图像。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4858b57b9c75d47132c65f2050bc5fe1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2856b5c51d6af52f31848166ea39e62f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5bd374d5d4a708e65208387522338bb4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3127daa555f76175132b4ef21712c878.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7748cecb9cb068dc6d44eb58141f9a77.jpg" align="middle">
</details>




<h2 id="Generate-Any-Scene-Evaluating-and-Improving-Text-to-Vision-Generation-with-Scene-Graph-Programming"><a href="#Generate-Any-Scene-Evaluating-and-Improving-Text-to-Vision-Generation-with-Scene-Graph-Programming" class="headerlink" title="Generate Any Scene: Evaluating and Improving Text-to-Vision Generation   with Scene Graph Programming"></a>Generate Any Scene: Evaluating and Improving Text-to-Vision Generation   with Scene Graph Programming</h2><p><strong>Authors:Ziqi Gao, Weikai Huang, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna</strong></p>
<p>DALL-E and Sora have gained attention by producing implausible images, such as “astronauts riding a horse in space.” Despite the proliferation of text-to-vision models that have inundated the internet with synthetic visuals, from images to 3D assets, current benchmarks predominantly evaluate these models on real-world scenes paired with captions. We introduce Generate Any Scene, a framework that systematically enumerates scene graphs representing a vast array of visual scenes, spanning realistic to imaginative compositions. Generate Any Scene leverages ‘scene graph programming’, a method for dynamically constructing scene graphs of varying complexity from a structured taxonomy of visual elements. This taxonomy includes numerous objects, attributes, and relations, enabling the synthesis of an almost infinite variety of scene graphs. Using these structured representations, Generate Any Scene translates each scene graph into a caption, enabling scalable evaluation of text-to-vision models through standard metrics. We conduct extensive evaluations across multiple text-to-image, text-to-video, and text-to-3D models, presenting key findings on model performance. We find that DiT-backbone text-to-image models align more closely with input captions than UNet-backbone models. Text-to-video models struggle with balancing dynamics and consistency, while both text-to-video and text-to-3D models show notable gaps in human preference alignment. We demonstrate the effectiveness of Generate Any Scene by conducting three practical applications leveraging captions generated by Generate Any Scene: 1) a self-improving framework where models iteratively enhance their performance using generated data, 2) a distillation process to transfer specific strengths from proprietary models to open-source counterparts, and 3) improvements in content moderation by identifying and generating challenging synthetic data. </p>
<blockquote>
<p>DALL-E和Sora通过生成不可信的图像，如“宇航员在太空中骑马”，引起了人们的关注。尽管文本到视觉的模型已经产生大量合成视觉，从图像到3D资产，目前的基准测试主要还是针对现实世界场景与说明配文的评估。我们介绍了Generate Any Scene框架，该框架系统地枚举表示各种视觉场景的场景图，涵盖真实到想象的各种组合。Generate Any Scene利用“场景图编程”，这是一种从视觉元素的结构化分类中动态构建场景图的方法，这种分类包括许多对象、属性和关系，能够实现几乎无限多种场景图的合成。通过这些结构化表示，Generate Any Scene将每个场景图翻译为说明配文，从而能够通过标准指标对文本到视觉模型进行可扩展的评估。我们对多个文本到图像、文本到视频和文本到3D模型进行了广泛评估，并提出了关于模型性能的关键发现。我们发现，与UNet主干模型相比，DiT主干文本到图像模型更紧密地与输入配文对齐。文本到视频模型在平衡动态和一致性方面遇到困难，而文本到视频和文本到3D模型在人类偏好对齐方面都存在明显的差距。我们通过开展三项实际应用，展示了Generate Any Scene的有效性，这些应用利用Generate Any Scene生成的说明配文：1）一种自我完善框架，模型利用生成数据迭代地提高其性能；2）一种蒸馏过程，将专有模型的特定优势转移到开源模型；3）通过识别和生成具有挑战性的合成数据，改进内容审核。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了两个引人注目的文本转图像模型DALL-E和Sora，它们能够生成超乎想象的图像，如“宇航员在太空骑乘马匹”。然而，当前大多数基准测试主要评估这些模型在现实场景与配对标题下的表现。为此，本文引入了一个名为Generate Any Scene的框架，该框架系统地枚举代表广泛视觉场景的场景图，涵盖了现实到想象的各种组合。该框架通过场景图编程的方法动态构建不同复杂度的场景图，并采用结构化表示形式将每个场景图转化为标题，从而实现通过标准指标对文本转视觉模型的可扩展评估。经过对多个文本转图像、文本转视频和文本转3D模型的广泛评估，发现DiT骨干的文本转图像模型比UNet骨干模型更贴近输入标题。文本转视频模型在平衡动态和一致性方面存在困难，而文本转视频和文本转3D模型在人类偏好对齐方面存在显著差距。此外，文章展示了Generate Any Scene框架在三个实际应用中的有效性，包括利用Generate Any Scene生成的标题进行自我改进框架、蒸馏过程以及内容审核改进等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DALL-E和Sora等文本转图像模型能够生成超乎想象的图像，如宇航员在太空骑乘马匹。</li>
<li>当前文本转视觉模型的评估主要基于现实场景与配对标题，缺乏系统性。</li>
<li>Generate Any Scene框架通过场景图编程方法，实现了对文本转视觉模型的扩展评估。</li>
<li>框架包含广泛视觉场景的场景图，并具备生成标题的功能。</li>
<li>评估发现DiT骨干的文本转图像模型更贴近输入标题，而文本转视频模型在平衡动态和一致性方面存在挑战。</li>
<li>Generate Any Scene框架在自我改进、蒸馏过程和内容审核改进等实际应用中表现出有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c315f584131719cbf0e51ae564191f07.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8cac25681b5b0ef4a64bd955c50a4710.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-28686693057ddfe9c59d03d7f4389268.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d6ef3519244948b5ffdb9b3ad44de264.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ff8f77c54e7bd604eed9920f8606d516.jpg" align="middle">
</details>




<h2 id="AsyncDSB-Schedule-Asynchronous-Diffusion-Schrodinger-Bridge-for-Image-Inpainting"><a href="#AsyncDSB-Schedule-Asynchronous-Diffusion-Schrodinger-Bridge-for-Image-Inpainting" class="headerlink" title="AsyncDSB: Schedule-Asynchronous Diffusion Schrödinger Bridge for Image   Inpainting"></a>AsyncDSB: Schedule-Asynchronous Diffusion Schrödinger Bridge for Image   Inpainting</h2><p><strong>Authors:Zihao Han, Baoquan Zhang, Lisai Zhang, Shanshan Feng, Kenghong Lin, Guotao Liang, Yunming Ye, Xiaochen Qi, Guangming Ye</strong></p>
<p>Image inpainting is an important image generation task, which aims to restore corrupted image from partial visible area. Recently, diffusion Schr&quot;odinger bridge methods effectively tackle this task by modeling the translation between corrupted and target images as a diffusion Schr&quot;odinger bridge process along a noising schedule path. Although these methods have shown superior performance, in this paper, we find that 1) existing methods suffer from a schedule-restoration mismatching issue, i.e., the theoretical schedule and practical restoration processes usually exist a large discrepancy, which theoretically results in the schedule not fully leveraged for restoring images; and 2) the key reason causing such issue is that the restoration process of all pixels are actually asynchronous but existing methods set a synchronous noise schedule to them, i.e., all pixels shares the same noise schedule. To this end, we propose a schedule-Asynchronous Diffusion Schr&quot;odinger Bridge (AsyncDSB) for image inpainting. Our insight is preferentially scheduling pixels with high frequency (i.e., large gradients) and then low frequency (i.e., small gradients). Based on this insight, given a corrupted image, we first train a network to predict its gradient map in corrupted area. Then, we regard the predicted image gradient as prior and design a simple yet effective pixel-asynchronous noise schedule strategy to enhance the diffusion Schr&quot;odinger bridge. Thanks to the asynchronous schedule at pixels, the temporal interdependence of restoration process between pixels can be fully characterized for high-quality image inpainting. Experiments on real-world datasets show that our AsyncDSB achieves superior performance, especially on FID with around 3% - 14% improvement over state-of-the-art baseline methods. </p>
<blockquote>
<p>图像补全是一项重要的图像生成任务，旨在从部分可见区域恢复受损图像。最近，扩散Schrödinger桥方法通过将要修复的受损图像与目标图像之间的翻译过程建模为一个沿噪声调度路径的扩散Schrödinger桥过程，从而有效地解决了这一任务。尽管这些方法已经表现出卓越的性能，但在本文中，我们发现1）现有方法存在调度恢复不匹配的问题，即理论上的调度和实际恢复过程通常存在很大的差异，这理论上导致调度没有完全用于恢复图像；2）造成这一问题的主要原因是所有像素的恢复过程是异步的，但现有方法却为它们设置了同步噪声调度，即所有像素共享相同的噪声调度。为此，我们提出了一种用于图像补全的异步扩散Schrödinger桥（AsyncDSB）。我们的见解是优先调度高频（即大梯度）像素，然后是低频（即小梯度）像素。基于这一见解，对于给定的受损图像，我们首先训练一个网络来预测其损坏区域的梯度图。然后，我们将预测的图像梯度视为先验，并设计一种简单有效的像素异步噪声调度策略，以增强扩散Schrödinger桥。由于像素的异步调度，可以充分描述像素之间恢复过程的时序相关性，从而实现高质量图像补全。在真实数据集上的实验表明，我们的AsyncDSB达到了卓越的性能，尤其是在FID指标上，较最新基线方法有约3%-14%的提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08149v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>该文探讨了图像修复的问题，提出了一种基于异步扩散Schrödinger桥的方法（AsyncDSB）。文章指出现有方法存在理论恢复计划与实际操作不匹配的问题，导致图像恢复效果受限。文章还提出，所有像素的恢复过程实际上是异步的，但现有方法设置了同步噪声时间表，造成时间上的差异问题。针对这一问题，作者优先恢复梯度大的像素区域并采用新的噪声调度策略以提高扩散效果，从而达到高质量图像修复的目的。实验结果显示，AsyncDSB在真实数据集上的性能优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像修复是重要图像生成任务，旨在从部分可见区域恢复受损图像。</li>
<li>当前扩散Schrödinger桥方法在理论上应用于图像修复已经展现出色的性能。</li>
<li>存在理论恢复计划与实际操作不匹配的问题，导致恢复效果受限。</li>
<li>恢复过程中像素的异步性被忽视，现有方法对所有像素采用同步噪声时间表。</li>
<li>提出优先恢复梯度大的像素区域并采用新的噪声调度策略以提高扩散效果。</li>
<li>AsyncDSB方法在真实数据集上的性能优于现有方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a969c8e8209876bcfa90b006f0af7706.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ca85087f8c0e96ca21f9abba07097a2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0433f55a7750d2d746fc27d77126cd19.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5f27e03caa918a42936e95a3eb577885.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0b465462134654b48578ff50f3c7778c.jpg" align="middle">
</details>




<h2 id="From-Slow-Bidirectional-to-Fast-Causal-Video-Generators"><a href="#From-Slow-Bidirectional-to-Fast-Causal-Video-Generators" class="headerlink" title="From Slow Bidirectional to Fast Causal Video Generators"></a>From Slow Bidirectional to Fast Causal Video Generators</h2><p><strong>Authors:Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</strong></p>
<p>Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to a causal transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher’s ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model supports fast streaming generation of high quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future. </p>
<blockquote>
<p>当前的视频扩散模型虽然生成质量令人印象深刻，但由于双向注意力依赖性，在交互式应用中表现困难。生成单个帧需要模型处理整个序列，包括未来信息。我们通过将预训练的双向扩散变压器调整为因果变压器来解决这个问题，该因果变压器可以即时生成帧。为了进一步降低延迟，我们将分布匹配蒸馏（DMD）扩展到视频领域，将50步扩散模型精炼为4步生成器。为了实现稳定和高质量的蒸馏，我们引入了基于教师ODE轨迹的学生初始化方案，以及一种不对称的蒸馏策略，即用双向教师监督因果学生模型。这种方法有效地减轻了自回归生成中的误差累积，即使在短片段训练的基础上也能实现长时长视频合成。得益于KV缓存，我们的模型能够在单个GPU上以9.4 FPS的速度快速生成高质量视频。我们的方法还支持流式视频到视频的翻译、图像到视频以及零样本方式的动态提示。未来，我们将基于开源模型发布代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07772v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://causvid.github.io/">https://causvid.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文解决了现有视频扩散模型在交互式应用中由于双向注意力依赖而存在的限制。通过将一个预训练的双向扩散变压器改编为因果变压器，实现在线生成帧，减少了生成单帧所需的处理时间。此外，通过扩展分布匹配蒸馏（DMD）至视频领域，将50步扩散模型简化为4步生成器，进一步降低了延迟。通过引入基于教师常微分方程轨迹的学生初始化方案以及不对称蒸馏策略，有效缓解了自回归生成中的误差累积问题。该方法支持在单个GPU上以9.4 FPS的速度快速生成高质量视频，并实现了视频到视频的流式翻译、图像到视频以及零样本动态提示功能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有视频扩散模型在交互式应用中存在双向注意力依赖的限制。</li>
<li>通过改编预训练的双向扩散变压器为因果变压器，实现了在线生成帧，减少了处理时间。</li>
<li>通过对分布匹配蒸馏（DMD）进行扩展，简化了视频生成步骤。</li>
<li>引入基于教师常微分方程轨迹的学生初始化方案，提高了稳定性和高质量蒸馏的效果。</li>
<li>采用不对称蒸馏策略，有效监督了因果学生模型的性能。</li>
<li>该方法支持快速生成高质量视频，并实现了多种流式翻译和提示功能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ee965fc3063cea1099d1a788584150d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d753d21c54ebc6456937f17d3e9e71a9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9cb62cb28a54dd058bb39b3b2bfafbe2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f6f5f4cc15dc30fe00c4852e9829e5fe.jpg" align="middle">
</details>




<h2 id="Paired-Wasserstein-Autoencoders-for-Conditional-Sampling"><a href="#Paired-Wasserstein-Autoencoders-for-Conditional-Sampling" class="headerlink" title="Paired Wasserstein Autoencoders for Conditional Sampling"></a>Paired Wasserstein Autoencoders for Conditional Sampling</h2><p><strong>Authors:Moritz Piening, Matthias Chung</strong></p>
<p>Wasserstein distances greatly influenced and coined various types of generative neural network models. Wasserstein autoencoders are particularly notable for their mathematical simplicity and straight-forward implementation. However, their adaptation to the conditional case displays theoretical difficulties. As a remedy, we propose the use of two paired autoencoders. Under the assumption of an optimal autoencoder pair, we leverage the pairwise independence condition of our prescribed Gaussian latent distribution to overcome this theoretical hurdle. We conduct several experiments to showcase the practical applicability of the resulting paired Wasserstein autoencoders. Here, we consider imaging tasks and enable conditional sampling for denoising, inpainting, and unsupervised image translation. Moreover, we connect our image translation model to the Monge map behind Wasserstein-2 distances. </p>
<blockquote>
<p>Wasserstein距离对各种生成神经网络模型产生了深远影响，并催生了多种模型。Wasserstein自编码器因其数学简单性和直接实现性而备受瞩目。然而，将其适应于条件情况却存在理论上的困难。为了解决这个问题，我们提出了使用两个配对自编码器的方案。在假设最优自编码器对的前提下，我们利用指定的高斯潜在分布的配对独立性条件来克服这一理论障碍。我们进行了多次实验，以展示所得配对Wasserstein自编码器的实际应用性。在此，我们考虑成像任务，并通过降噪、修复和无监督图像翻译实现条件采样。此外，我们将图像翻译模型与Wasserstein-2距离背后的Monge图连接起来。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07586v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本描述了Wasserstein距离对生成神经网络模型的各种影响以及Wasserstein自编码器的特点。尽管自编码器在理论上面临适应条件情况的困难，但通过使用两个配对自编码器并利用高斯潜在分布的配对独立性条件，可以克服这些困难。实验证明，配对Wasserstein自编码器在成像任务中的实用性，包括降噪、图像修复和无监督图像翻译等。此外，还将图像翻译模型与Wasserstein-2距离背后的Monge图联系起来。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wasserstein距离对生成神经网络模型有重要影响，包括Wasserstein自编码器。</li>
<li>Wasserstein自编码器具有数学简单性和直接实现性。</li>
<li>自编码器在适应条件情况时面临理论困难。</li>
<li>通过使用两个配对自编码器和高斯潜在分布的配对独立性条件，可以克服这些理论困难。</li>
<li>配对Wasserstein自编码器在成像任务中具有实用性，包括降噪、图像修复和无监督图像翻译。</li>
<li>配对Wasserstein自编码器的实验证明了其有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ae0d7dc17456f7fec88d84c5fc46e182.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2a65d33a3c660bd4862296994227cdad.jpg" align="middle">
</details>




<h2 id="A-Parametric-Approach-to-Adversarial-Augmentation-for-Cross-Domain-Iris-Presentation-Attack-Detection"><a href="#A-Parametric-Approach-to-Adversarial-Augmentation-for-Cross-Domain-Iris-Presentation-Attack-Detection" class="headerlink" title="A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris   Presentation Attack Detection"></a>A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris   Presentation Attack Detection</h2><p><strong>Authors:Debasmita Pal, Redwan Sony, Arun Ross</strong></p>
<p>Iris-based biometric systems are vulnerable to presentation attacks (PAs), where adversaries present physical artifacts (e.g., printed iris images, textured contact lenses) to defeat the system. This has led to the development of various presentation attack detection (PAD) algorithms, which typically perform well in intra-domain settings. However, they often struggle to generalize effectively in cross-domain scenarios, where training and testing employ different sensors, PA instruments, and datasets. In this work, we use adversarial training samples of both bonafide irides and PAs to improve the cross-domain performance of a PAD classifier. The novelty of our approach lies in leveraging transformation parameters from classical data augmentation schemes (e.g., translation, rotation) to generate adversarial samples. We achieve this through a convolutional autoencoder, ADV-GEN, that inputs original training samples along with a set of geometric and photometric transformations. The transformation parameters act as regularization variables, guiding ADV-GEN to generate adversarial samples in a constrained search space. Experiments conducted on the LivDet-Iris 2017 database, comprising four datasets, and the LivDet-Iris 2020 dataset, demonstrate the efficacy of our proposed method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD">https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD</a>. </p>
<blockquote>
<p>基于虹膜的生物识别系统容易受到攻击演示的影响（PAS）。对手利用物理工具（如打印的虹膜图像、纹理隐形眼镜）对系统进行攻击。这促使了多种演示攻击检测（PAD）算法的发展，这些算法在内部领域环境中通常表现良好。然而，当训练和测试使用不同的传感器、攻击仪器和数据集时，它们在跨领域场景中的泛化能力往往较差。在这项工作中，我们使用真实虹膜和攻击演示对抗训练样本，以提高PAD分类器在跨领域的性能。我们方法的新颖之处在于利用经典数据增强方案（如平移、旋转）的转换参数来生成对抗样本。我们通过卷积自编码器ADV-GEN实现这一点，该编码器输入原始训练样本以及一组几何和光度转换。转换参数作为正则化变量，指导ADV-GEN在约束搜索空间中生成对抗样本。在LivDet-Iris 2017数据库（包含四个数据集）和LivDet-Iris 2020数据集上进行的实验证明了我们的方法的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07199v1">PDF</a> IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV),   2025</p>
<p><strong>Summary</strong><br>     该研究解决了虹膜生物识别系统在跨领域场景下的呈现攻击检测问题。通过利用对抗训练样本和经典数据增强方案的转换参数，提出了一种基于卷积自编码器的对抗样本生成方法，提高了PAD分类器在跨领域场景下的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>虹膜生物识别系统面临呈现攻击（PAs）的问题，需要开发有效的呈现攻击检测（PAD）算法。</li>
<li>PAD算法在跨领域场景下往往表现不佳，特别是在训练和测试使用不同传感器、PA仪器和数据集的情况下。</li>
<li>对抗训练样本被用于提高PAD分类器的跨领域性能。</li>
<li>该研究利用经典数据增强方案的转换参数来生成对抗样本，这是一种新颖的方法。</li>
<li>使用卷积自编码器ADV-GEN来生成对抗样本，该自编码器接受原始训练样本以及一组几何和光度转换。</li>
<li>转换参数作为正则化变量，指导ADV-GEN在受限的搜索空间内生成对抗样本。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b11ea9e34245ae992abb61019ca85424.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c6c2cc0aeffe8b9ab96925608d0991bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-53cb9da6a411a164b0012f856f3d4b3f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a2ffba784837ee8c3d0e774b00049510.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0383b325ddc6153979ad230975efe4b6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3d73ec5db30159c31133f901134bfdc1.jpg" align="middle">
</details>




<h2 id="MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation"><a href="#MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation" class="headerlink" title="MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation"></a>MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation</h2><p><strong>Authors:Bo Li, Shaolin Zhu, Lijie Wen</strong></p>
<p>Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority. </p>
<blockquote>
<p>图像翻译（IT）在各个领域具有巨大的潜力，能够实现图像内文本内容的跨语言翻译。然而，现有数据集在规模、多样性和质量方面存在诸多局限，阻碍了IT模型的开发与评估。为了解决这一问题，我们推出了MIT-10M，这是一个大规模的多语言图像翻译平行语料库，包含超过1000万个图像文本对，来源于真实世界数据，并经过了广泛的数据清洗和多语言翻译验证。它包含三种尺寸、28个类别的84万张图像，任务难度分为三个级别，以及14种语言的图像文本对，对现有数据集来说是一个显著的改进。我们在MIT-10M上进行了大量的实验来评估和训练模型。实验结果清楚地表明，我们的数据集在评估模型应对现实世界中具有挑战性和复杂性的图像翻译任务的性能时，具有更高的适应性。此外，与基线模型相比，使用MIT-10M进行微调后的模型性能提高了三倍，进一步证明了其优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07147v1">PDF</a> Accepted in COLING 2025</p>
<p><strong>Summary</strong><br>     引入MIT-10M多语言图像翻译大型平行语料库，包含超过千万图像文本对，用于解决图像翻译领域的痛点。该数据集经过数据清洗和多语言翻译验证，包含不同尺寸、类别和难度的任务，以及多种语言的图像文本对。实验证明，该数据集在评估模型性能上表现优越，经过MIT-10M微调的模型性能大幅提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIT-10M是一个大型的多语言图像翻译平行语料库，包含超过千万的图像文本对。</li>
<li>数据集涵盖多种尺寸、类别和难度的任务，满足多样化的图像翻译需求。</li>
<li>数据集经过严格的数据清洗和多语言翻译验证，保证数据质量。</li>
<li>实验表明MIT-10M在评估模型性能上具有更高的适应性。</li>
<li>与基线模型相比，使用MIT-10M微调后的模型性能显著提升。</li>
<li>MIT-10M的引入有助于推动图像翻译领域的进一步发展。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-af0d0f7fcb2a8ff0029db307da64e83a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-60f79ddea5e0335376407887ffb9b7fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-06c903befd571e21804bdbe9a878a09b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b02f93a7a822cc0ea3809244981ef4f0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffad01de36274cabab6343e55be2746b.jpg" align="middle">
</details>




<h2 id="Unsupervised-Variational-Translator-for-Bridging-Image-Restoration-and-High-Level-Vision-Tasks"><a href="#Unsupervised-Variational-Translator-for-Bridging-Image-Restoration-and-High-Level-Vision-Tasks" class="headerlink" title="Unsupervised Variational Translator for Bridging Image Restoration and   High-Level Vision Tasks"></a>Unsupervised Variational Translator for Bridging Image Restoration and   High-Level Vision Tasks</h2><p><strong>Authors:Jiawei Wu, Zhi Jin</strong></p>
<p>Recent research tries to extend image restoration capabilities from human perception to machine perception, thereby enhancing the performance of high-level vision tasks in degraded environments. These methods, primarily based on supervised learning, typically involve the retraining of restoration networks or high-level vision networks. However, collecting paired data in real-world scenarios and retraining large-scale models are challenge. To this end, we propose an unsupervised learning method called \textbf{Va}riational \textbf{T}ranslator (VaT), which does not require retraining existing restoration and high-level vision networks. Instead, it establishes a lightweight network that serves as an intermediate bridge between them. By variational inference, VaT approximates the joint distribution of restoration output and high-level vision input, dividing the optimization objective into preserving content and maximizing marginal likelihood associated with high-level vision tasks. By cleverly leveraging self-training paradigms, VaT achieves the above optimization objective without requiring labels. As a result, the translated images maintain a close resemblance to their original content while also demonstrating exceptional performance on high-level vision tasks. Extensive experiments in dehazing and low-light enhancement for detection and classification show the superiority of our method over other state-of-the-art unsupervised counterparts, even significantly surpassing supervised methods in some complex real-world scenarios.Code is available at <a target="_blank" rel="noopener" href="https://github.com/Fire-friend/VaT">https://github.com/Fire-friend/VaT</a>. </p>
<blockquote>
<p>最近的研究尝试将从人类感知到机器感知的图像恢复能力进行扩展，从而提高在恶劣环境下高级视觉任务的性能。这些方法主要基于有监督学习，通常涉及恢复网络或高级视觉网络的再训练。然而，在真实场景收集配对数据并重新训练大规模模型是挑战。为此，我们提出了一种无需重新训练现有恢复和高级视觉网络的监督学习方法，名为“变译”（VaT）。相反，它建立了一个轻量级的网络，作为它们之间的中间桥梁。通过变分推理，VaT近似恢复输出和高级视觉输入的共同分布，将优化目标分为保持内容和最大化与高级视觉任务相关的边缘可能性。通过巧妙地利用自训练模式，VaT在不需标签的情况下实现了上述优化目标。因此，翻译后的图像保持了与原始内容的紧密相似性，同时在高级视觉任务上表现出卓越的性能。去雾和低光增强检测与分类的广泛实验表明，我们的方法在其它先进的无监督方法之上具有优越性，甚至在某些复杂的真实场景中显著超过了有监督方法。代码可在 <a target="_blank" rel="noopener" href="https://github.com/Fire-friend/VaT">https://github.com/Fire-friend/VaT</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08149v3">PDF</a> </p>
<p><strong>Summary</strong><br>     研究试图通过延长机器对人图像的复原能力来提高高层次的视觉任务性能。提出的变分翻译器（VaT）通过实现不需要重训现有复原网络和高级视觉网络的中间桥梁来实现这一目标。利用变分推理近似计算复原输出和高级视觉输入的结合分布，并将其分解为保留内容和最大化与高级视觉任务相关的边缘概率的优化目标。通过自我训练模式，VaT在不依赖标签的情况下实现了上述优化目标。实验证明，在除雾和低光增强检测与分类任务中，该方法优于其他先进的无监督方法，甚至在某些复杂现实场景中显著优于监督方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究致力于将人类感知的图像恢复能力扩展到机器感知，以提高退化环境中高级视觉任务的表现。</li>
<li>提出了一种名为变分翻译器（VaT）的无监督学习方法，无需重训现有的恢复和高级视觉网络。</li>
<li>VaT通过建立轻量级网络作为中间桥梁，实现了在不需要重训的情况下提升图像恢复和高级视觉任务性能。</li>
<li>通过变分推理，VaT近似计算恢复输出和高级视觉输入的结合分布，并分解为保留内容和最大化与高级视觉任务相关的边缘概率的优化目标。</li>
<li>VaT利用自我训练模式实现优化目标，无需依赖标签。</li>
<li>实验证明，在除雾和低光增强检测与分类任务中，VaT表现优越，甚至在某些复杂场景中超过监督方法。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a437890eacdc6cc0a6c2e50be1487fb9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2d92153b75e6744523118686295aa463.jpg" align="middle">
</details>




<h2 id="Leveraging-Pre-trained-Models-for-FF-to-FFPE-Histopathological-Image-Translation"><a href="#Leveraging-Pre-trained-Models-for-FF-to-FFPE-Histopathological-Image-Translation" class="headerlink" title="Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image   Translation"></a>Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image   Translation</h2><p><strong>Authors:Qilai Zhang, Jiawen Li, Peiran Liao, Jiali Hu, Tian Guan, Anjia Han, Yonghong He</strong></p>
<p>The two primary types of Hematoxylin and Eosin (H&amp;E) slides in histopathology are Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides offer high quality histopathological images but require a labor-intensive acquisition process. In contrast, FF slides can be prepared quickly, but the image quality is relatively poor. Our task is to translate FF images into FFPE style, thereby improving the image quality for diagnostic purposes. In this paper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological image translation using a pre-trained diffusion model. Specifically, we utilize a one-step diffusion model as the generator, which we fine-tune using LoRA adapters within an adversarial learning framework. To enable the model to effectively capture both global structural patterns and local details, we introduce a multi-scale feature fusion module that leverages two VAE encoders to extract features at different image resolutions, performing feature fusion before inputting them into the UNet. Additionally, a pre-trained vision-language model for histopathology serves as the backbone for the discriminator, enhancing model performance. Our FF-to-FFPE translation experiments on the TCGA-NSCLC dataset demonstrate that the proposed approach outperforms existing methods. The code and models are released at <a target="_blank" rel="noopener" href="https://github.com/QilaiZhang/Diffusion-FFPE">https://github.com/QilaiZhang/Diffusion-FFPE</a>. </p>
<blockquote>
<p>在病理学中，苏木精和伊红（H&amp;E）染色玻片的两种主要类型是福尔马林固定石蜡包埋（FFPE）和新鲜冷冻（FF）。FFPE玻片提供高质量的病理图像，但需要劳动密集型的采集过程。相比之下，FF玻片可以快速制备，但图像质量相对较差。我们的任务是将FF图像转换为FFPE风格，从而提高图像质量，以用于诊断目的。在本文中，我们提出了Diffusion-FFPE方法，这是一种使用预训练的扩散模型实现FF到FFPE病理图像翻译的方法。具体来说，我们利用一步扩散模型作为生成器，在一个对抗性学习框架内使用LoRA适配器对其进行微调。为了使得模型能够有效地捕捉全局结构模式和局部细节，我们引入了一个多尺度特征融合模块，该模块利用两个VAE编码器在不同图像分辨率下提取特征，在输入UNet之前进行特征融合。此外，病理学中预训练的视觉语言模型作为判别器的骨干网，提高了模型性能。我们在TCGA-NSCLC数据集上进行的FF到FFPE翻译实验表明，所提出的方法优于现有方法。相关代码和模型已发布在<a target="_blank" rel="noopener" href="https://github.com/QilaiZhang/Diffusion-FFPE%E3%80%82">https://github.com/QilaiZhang/Diffusion-FFPE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18054v3">PDF</a> Accepted at IEEE BIBM 2024</p>
<p><strong>Summary</strong><br>     论文介绍了两种主要的病理切片类型——FFPE和FF，并提出一种利用预训练的扩散模型将FF图像转换为FFPE风格的方法，以提高诊断图像质量。采用具有多尺度特征融合模块的一站式扩散模型生成器，通过LoRA适配器在对抗学习框架中进行微调。实验证明该方法在TCGA-NSCLC数据集上的FF-to-FFPE翻译效果优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了两种主要的病理切片类型FFPE和FF，并强调了将FF图像转换为FFPE风格的必要性以提高诊断图像质量。</li>
<li>提出一种基于预训练的扩散模型的图像翻译方法，用于实现FF到FFPE的转换。</li>
<li>采用具有多尺度特征融合模块的一站式扩散模型生成器，以提高模型的性能。</li>
<li>使用LoRA适配器在对抗学习框架中对生成器进行微调，以优化模型的翻译效果。</li>
<li>引入预训练的视觉语言模型作为判别器，以增强模型的性能。</li>
<li>在TCGA-NSCLC数据集上进行的实验表明，该方法在FF-to-FFPE翻译任务上优于现有方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ee3ca2d0a2bc9314487c725339a6d52.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32908570bb7a726faf05b588144c6129.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-de7ef6ead762cb3b1676edb1c6664be9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e32302b30daf4186e0e8938e260e3904.jpg" align="middle">
</details>




<h2 id="SurgeMOD-Translating-image-space-tissue-motions-into-vision-based-surgical-forces"><a href="#SurgeMOD-Translating-image-space-tissue-motions-into-vision-based-surgical-forces" class="headerlink" title="SurgeMOD: Translating image-space tissue motions into vision-based   surgical forces"></a>SurgeMOD: Translating image-space tissue motions into vision-based   surgical forces</h2><p><strong>Authors:Mikel De Iturrate Reyzabal, Dionysios Malas, Shuai Wang, Sebastien Ourselin, Hongbin Liu</strong></p>
<p>We present a new approach for vision-based force estimation in Minimally Invasive Robotic Surgery based on frequency domain basis of motion of organs derived directly from video. Using internal movements generated by natural processes like breathing or the cardiac cycle, we infer the image-space basis of the motion on the frequency domain. As we are working with this representation, we discretize the problem to a limited amount of low-frequencies to build an image-space mechanical model of the environment. We use this pre-built model to define our force estimation problem as a dynamic constraint problem. We demonstrate that this method can estimate point contact forces reliably for silicone phantom and ex-vivo experiments, matching real readings from a force sensor. In addition, we perform qualitative experiments in which we synthesize coherent force textures from surgical videos over a certain region of interest selected by the user. Our method demonstrates good results for both quantitative and qualitative analysis, providing a good starting point for a purely vision-based method for surgical force estimation. </p>
<blockquote>
<p>我们提出了一种基于视频直接导出器官运动频率域特征的最小侵入式机器人手术中的基于视觉的力估计新方法。利用呼吸或心脏周期等自然过程产生的内部运动，我们在频率域推断运动的图像空间基础。由于我们处理的是这种表示形式，我们将问题离散化为有限数量的低频，以建立图像空间环境机械模型。我们使用预先构建的模型将力估计问题定义为动态约束问题。我们证明，该方法可以可靠地估计硅胶幻影和离体实验中的点接触力，与力传感器的实际读数相匹配。此外，我们在用户选择的特定感兴趣区域从手术视频中合成连贯的力纹理，进行了定性实验。我们的方法在定量和定性分析中均取得了良好结果，为纯粹的基于视觉的手术力估计方法提供了一个良好的起点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17707v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于视频频域运动基础，提出一种用于微创机器人手术中的基于视觉的力估计新方法。利用自然过程（如呼吸或心脏周期）产生的内部运动，推断图像空间的频域运动基础。在此表示形式的基础上，我们将问题离散化为有限数量的低频，以建立图像空间环境机械模型。我们使用此预构建模型将力估计问题定义为动态约束问题。该方法可以可靠地估计硅胶幻影和离体实验的点接触力，与来自力传感器的实际读数相匹配。此外，我们还进行了定性实验，合成用户选择的感兴趣区域的连贯力纹理。我们的方法对于定量和定性分析均表现出良好结果，为纯粹的基于视觉的手术力估计方法提供了良好的起点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了基于视频频域运动基础的微创机器人手术中新的视觉力估计方法。</li>
<li>方法利用自然过程产生的内部运动来推断图像空间的频域运动基础。</li>
<li>将问题离散化为低频建立图像空间环境机械模型。</li>
<li>将力估计问题定义为动态约束问题，并使用预构建模型进行解决。</li>
<li>方法能够可靠估计硅胶幻影和离体实验的点接触力，与力传感器读数相匹配。</li>
<li>进行合成力纹理的定性实验，以合成用户选择的感兴趣区域的连贯力信息。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-02c374a6f8b80512d1eeaf903a256cdd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3af6a04bad4876c2bea06d39250e8505.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3a6143a28d17003256f23dabd1c17e1a.jpg" align="middle">
</details>




<h2 id="Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions"><a href="#Rethinking-Score-Distillation-as-a-Bridge-Between-Image-Distributions" class="headerlink" title="Rethinking Score Distillation as a Bridge Between Image Distributions"></a>Rethinking Score Distillation as a Bridge Between Image Distributions</h2><p><strong>Authors:David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa</strong></p>
<p>Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains. Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications. In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution. Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target). We argue that current methods’ characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution. We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead. Our method can be easily applied across many domains, matching or beating the performance of specialized methods. We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real. We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors. </p>
<blockquote>
<p>得分蒸馏采样（SDS）已被证明是一个重要工具，能够利用大规模扩散先验知识，对数据匮乏领域中的任务进行操作。然而，SDS存在一些特性化的伪迹，限制了其在通用应用中的实用性。在本文中，我们通过将SDS及其变体视为从源分布到目标分布的最优成本传输路径的解决方式，从而取得了对SDS行为的理解的进步。在这种新的解释下，这些方法试图将损坏的图像（源）传输到自然图像分布（目标）。我们认为当前方法的特性伪迹是由（1）最优路径的线性近似和（2）源分布估计不佳造成的。我们表明，校准源分布的文本条件可以产生高质量生成和翻译结果，并且只需很少额外的开销。我们的方法可轻松应用于多个领域，达到或超过专用方法的性能。我们在文本到二维、基于文本的NeRF优化、绘画到真实图像的翻译、光学错觉生成和三维草图到现实等任务中展示了其实用性。我们将方法与现有的分数蒸馏采样方法进行了比较，并证明它能够生成具有逼真颜色的高频细节。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09417v2">PDF</a> NeurIPS 2024. Project webpage: <a target="_blank" rel="noopener" href="https://sds-bridge.github.io/">https://sds-bridge.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了分数蒸馏采样（SDS）在处理数据贫瘠领域任务时的应用价值。尽管SDS有一些局限性，但其仍然被用作重要工具用于大型扩散先验的运用。为了提升SDS的表现和解决其在一般应用场景中的固有局限性，该文引入了一个新的角度来解决SDS及变体的最优成本传输路径问题，通过将处理过程视作是从源分布到目标分布的一个传输过程。新方法解决了现有方法的两个主要问题：最优路径的线性近似和源分布估算的不准确。通过校准源分布的文本条件，新方法能够生成高质量图像并达到低额外开销的实用翻译效果。其在多领域应用中表现出了优异的表现能力，可以匹配或超越一些专业领域方法的应用效果。该论文展示的方法被用于文本到二维图像转换、基于文本的NeRF优化、画作到现实图像的翻译、光学幻觉生成以及三维草图到现实的转换等场景。实验表明，新方法能够生成具有真实色彩的高频细节图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SDS作为一种处理数据贫瘠领域任务的重要工具已被证实其价值，但其具有一定的局限性。</li>
<li>新方法通过解决SDS的最优成本传输路径问题来提升其表现并解决其局限性。</li>
<li>新方法将处理过程视作从源分布到目标分布的传输过程。</li>
<li>当前方法的特征缺陷源于最优路径的线性近似和源分布估算的不准确。</li>
<li>通过校准源分布的文本条件，新方法能够生成高质量图像并达到实用翻译效果。</li>
<li>新方法在多领域应用中表现出优异的表现能力，包括文本到二维图像转换、基于文本的NeRF优化等场景。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ab688bcbe79403b9dc0a82fa87e55b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-583f9aab3efdf9acf3d30ae12a8d5845.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd2ee8b180e521f6c21caaa035b7cc5f.jpg" align="middle">
</details>




<h2 id="SPIN-Spacecraft-Imagery-for-Navigation"><a href="#SPIN-Spacecraft-Imagery-for-Navigation" class="headerlink" title="SPIN: Spacecraft Imagery for Navigation"></a>SPIN: Spacecraft Imagery for Navigation</h2><p><strong>Authors:Javier Montalvo, Juan Ignacio Bravo Pérez-Villar, Álvaro García-Martín, Pablo Carballeira, Jesús Bescós</strong></p>
<p>The scarcity of data acquired under actual space operational conditions poses a significant challenge for developing learning-based visual navigation algorithms crucial for autonomous spacecraft navigation. This data shortage is primarily due to the prohibitive costs and inherent complexities of space operations. While existing datasets, predominantly relying on computer-simulated data, have partially addressed this gap, they present notable limitations. Firstly, these datasets often utilize proprietary image generation tools, restricting the evaluation of navigation methods in novel, unseen scenarios. Secondly, they provide limited ground-truth data, typically focusing solely on the spacecraft’s translation and rotation relative to the camera. To address these limitations, we present SPIN (SPacecraft Imagery for Navigation), an open-source spacecraft image generation tool designed to support a wide range of visual navigation scenarios in space, with a particular focus on relative navigation tasks. SPIN provides multiple modalities of ground-truth data and allows researchers to employ custom 3D models of satellites, define specific camera-relative poses, and adjust settings such as camera parameters or environmental illumination conditions. We also propose a method for exploiting our tool as a data augmentation module. We validate our tool on the spacecraft pose estimation task by training with a SPIN-generated replica of SPEED+, reaching a 47% average error reduction on SPEED+ testbed data (that simulates realistic space conditions), further reducing it to a 60% error reduction when using SPIN as a data augmentation method. Both the SPIN tool (and source code) and our SPIN-generated version of SPEED+ will be publicly released upon paper acceptance on GitHub. <a target="_blank" rel="noopener" href="https://github.com/vpulab/SPIN">https://github.com/vpulab/SPIN</a> </p>
<blockquote>
<p>在真实的太空操作条件下获取的数据稀缺，对于开发基于学习的视觉导航算法构成了重大挑战，这些算法对于自主航天器导航至关重要。这一数据短缺主要归因于太空操作的高昂成本和固有复杂性。虽然现有的数据集主要依赖于计算机模拟数据，已部分解决了这一差距，但它们存在明显的局限性。</p>
</blockquote>
<p>首先，这些数据集通常使用专有图像生成工具，限制了在新颖、未见过的场景中对导航方法的评估。其次，它们提供的真实数据有限，通常只专注于航天器相对于相机的平移和旋转。</p>
<p>为了解决这个问题，我们推出了SPIN（航天器图像导航），这是一款开源的航天器图像生成工具，旨在支持各种太空视觉导航场景，特别侧重于相对导航任务。SPIN提供了多种模式的真实数据，并允许研究人员使用定制的卫星3D模型、定义特定的相机相对姿态、并调整相机参数或环境照明条件等设置。我们还提出了一种利用该工具作为数据增强模块的方法。我们通过使用SPIN生成的SPEED+复制品对航天器姿态估计任务进行训练，在模拟真实太空条件的SPEED+测试台上将平均误差减少了47%，在使用SPIN作为数据增强方法时，误差进一步减少了60%。</p>
<p>SPIN工具（及源代码）和我们使用SPIN生成的SPEED+版本将在论文被接受后公开发布在GitHub上。<a target="_blank" rel="noopener" href="https://github.com/vpulab/SPIN">https://github.com/vpulab/SPIN</a></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07500v3">PDF</a> </p>
<p><strong>Summary</strong><br>    数据获取难度大，导致实际空间操作环境下的视觉导航算法发展受限。提出SPIN工具，旨在支持多种空间视觉导航场景，特别是相对导航任务。该工具提供多种模态的地面真实数据，允许研究者使用自定义卫星模型，调整相机参数和环境照明条件等。利用该工具进行数据增强方法验证，训练结果降低了速度误差。SPIN工具及源码将公开在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>数据获取难题影响了基于学习的视觉导航算法在自主航天器导航领域的发展。</li>
<li>SPIN工具旨在解决现有数据集依赖计算机模拟数据的问题，支持多种空间视觉导航场景。</li>
<li>SPIN工具提供多种模态的地面真实数据，允许研究者自定义卫星模型和环境设置。</li>
<li>利用SPIN工具进行数据增强能提高算法性能，减少速度误差。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e269abba545f64322d8ae2c3a78b5aa3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3e2e58772db9cdf48a54e001582c1577.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6ed0e5144e5b418f80261f10d3dffa4d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1d4271e1912f5ef34de0d1f21797e4c5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2ec7feed52ebbd38914a42c159a4f578.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-042235db958f1be0730872b1cfbe83fe.jpg" align="middle">
</details>




<h2 id="Equivariant-Machine-Learning-on-Graphs-with-Nonlinear-Spectral-Filters"><a href="#Equivariant-Machine-Learning-on-Graphs-with-Nonlinear-Spectral-Filters" class="headerlink" title="Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters"></a>Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</h2><p><strong>Authors:Ya-Wei Eileen Lin, Ronen Talmon, Ron Levie</strong></p>
<p>Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space. We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks. </p>
<blockquote>
<p>等价机器学习是一种设计深度学习模型的方法，它尊重问题的对称性，旨在降低模型复杂性并改善泛化能力。在本文中，我们重点关注平移等价性的扩展，这是图像卷积网络的基础，并将其扩展到一般图上。与图像不同，图没有域平移的自然概念。因此，我们将图功能平移视为对称群：与图平移算子交换的酉算子。值得注意的是，这种对称性在信号空间而不是直接在空间空间中起作用。我们注意到，标准谱图神经网络（GNN）的每一层线性滤波器都与图功能平移交换，但激活函数会破坏这种对称性。相反，我们提出了完全等价于图功能平移的非线性谱滤波器（NLSFs），并证明了它们具有通用逼近属性。所提出的NLSFs基于一种可在图之间转移的新谱域形式。我们在节点和图分类基准测试中展示了NLSF相对于现有谱GNN的优越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01249v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文研究了如何在深度学习中应用等价学习理论来解决问题，尤其是通过扩展图像卷积网络的平移等价性到一般图形上。论文指出，不同于图像有自然的平移概念，图形没有自然的平移概念，因此考虑图形功能平移作为对称群是必要的。文章提出了非线性谱滤波器（NLSFs），它是完全等价于图形功能平移的，并且具有通用逼近性质。此外，NLSFs基于一种可在不同图形之间传递的新形式的谱域。实验结果显示，在节点和图形分类任务中，NLSFs的表现优于现有的谱图神经网络。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>等价学习理论被应用于深度学习中，旨在通过尊重问题的对称性来设计和优化深度学习模型。</li>
<li>文章扩展了图像卷积网络的平移等价性理论到一般图形上。</li>
<li>论文指出图形没有自然的平移概念，因此提出了利用图形功能平移作为对称群的新方法。</li>
<li>NLSFs是一种完全等价于图形功能平移的非线性滤波器，具有通用逼近性质。</li>
<li>NLSFs基于一种新型谱域结构，该结构在不同图形之间具有可迁移性。</li>
<li>实验结果表明，NLSFs在节点和图形分类任务上的性能优于现有的谱图神经网络。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-003476765b4d9b263cc5914f409c4edc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d41e4cb336668d9f32ebf187289f5cfb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1ad900ee3a425d8fac42614ce4f27e05.jpg" align="middle">
</details>




<h2 id="Imitating-the-Functionality-of-Image-to-Image-Models-Using-a-Single-Example"><a href="#Imitating-the-Functionality-of-Image-to-Image-Models-Using-a-Single-Example" class="headerlink" title="Imitating the Functionality of Image-to-Image Models Using a Single   Example"></a>Imitating the Functionality of Image-to-Image Models Using a Single   Example</h2><p><strong>Authors:Nurit Spingarn-Eliezer, Tomer Michaeli</strong></p>
<p>We study the possibility of imitating the functionality of an image-to-image translation model by observing input-output pairs. We focus on cases where training the model from scratch is impossible, either because training data are unavailable or because the model architecture is unknown. This is the case, for example, with commercial models for biological applications. Since the development of these models requires large investments, their owners commonly keep them confidential, and reveal only a few input-output examples on the company’s website or in an academic paper. Surprisingly, we find that even a single example typically suffices for learning to imitate the model’s functionality, and that this can be achieved using a simple distillation approach. We present an extensive ablation study encompassing a wide variety of model architectures, datasets and tasks, to characterize the factors affecting vulnerability to functionality imitation, and provide a preliminary theoretical discussion on the reasons for this unwanted behavior. </p>
<blockquote>
<p>我们研究了通过观察输入-输出对来模仿图像到图像翻译模型功能的可能性。我们专注于那些从头开始训练模型不可能的情况，因为训练数据不可用或模型架构未知。这种情况例如适用于生物应用中的商业模型。由于开发这些模型需要大量的投资，因此其所有者通常将其保密，仅在公司的网站或学术论文中公开少数输入-输出示例。令人惊讶的是，我们发现即使是一个例子通常也足以学习模仿模型的功能，而且可以使用简单的蒸馏方法来实现这一点。我们进行了一项广泛的消融研究，涵盖了各种模型架构、数据集和任务，以刻画影响功能模仿脆弱性的因素，并就这种不良行为的原因进行了初步的理论讨论。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00828v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了通过观测输入输出对来模仿图像到图像翻译模型功能的可能性。研究重点是在无法从头开始训练模型的情况，例如训练数据不可用或模型架构未知。特别是在商业应用中，某些模型涉及大量投资，其所有权人通常会保密处理并仅在官网或学术刊物上发表少量输入输出实例。令人惊讶的是，我们发现即使是单一示例通常也足以模仿模型的功能，且可以通过简单的蒸馏方法实现。本研究进行了广泛的消融研究，涵盖了多种模型架构、数据集和任务，以刻画影响功能模仿的因素，并对这种非期望行为的原因进行了初步的理论讨论。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究旨在探索模仿图像到图像翻译模型功能的可能性，特别是在无法从头开始训练模型的情况下。</li>
<li>主要关注无法使用训练数据或未知模型架构的场景。</li>
<li>研究对象主要是商业应用中的模型，这些模型需要大量投资且通常被保密处理。</li>
<li>通过简单的蒸馏方法，即使是单一的示例也可能足以模仿模型的功能。</li>
<li>进行了广泛的消融研究，涵盖了不同的模型架构、数据集和任务。</li>
<li>研究初步探讨了影响功能模仿的因素。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9f250ebdf3361a1f875c2913c1fdc7a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-996cc6c7fce145ea5b01b5ba6cb2727c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-010ecd8d09e7a166e3f0a0e78fc412ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-057358cce4f2fa1bbfe039a46c31db2f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f5b35d2cc8bd4961798620ac2ce9da52.jpg" align="middle">
</details>




<h2 id="Multi-Sensor-Diffusion-Driven-Optical-Image-Translation-for-Large-Scale-Applications"><a href="#Multi-Sensor-Diffusion-Driven-Optical-Image-Translation-for-Large-Scale-Applications" class="headerlink" title="Multi-Sensor Diffusion-Driven Optical Image Translation for Large-Scale   Applications"></a>Multi-Sensor Diffusion-Driven Optical Image Translation for Large-Scale   Applications</h2><p><strong>Authors:João Gabriel Vinholi, Marco Chini, Anis Amziane, Renato Machado, Danilo Silva, Patrick Matgen</strong></p>
<p>Comparing images captured by disparate sensors is a common challenge in remote sensing. This requires image translation – converting imagery from one sensor domain to another while preserving the original content. Denoising Diffusion Implicit Models (DDIM) are potential state-of-the-art solutions for such domain translation due to their proven superiority in multiple image-to-image translation tasks in computer vision. However, these models struggle with reproducing radiometric features of large-scale multi-patch imagery, resulting in inconsistencies across the full image. This renders downstream tasks like Heterogeneous Change Detection impractical. To overcome these limitations, we propose a method that leverages denoising diffusion for effective multi-sensor optical image translation over large areas. Our approach super-resolves large-scale low spatial resolution images into high-resolution equivalents from disparate optical sensors, ensuring uniformity across hundreds of patches. Our contributions lie in new forward and reverse diffusion processes that address the challenges of large-scale image translation. Extensive experiments using paired Sentinel-II (10m) and Planet Dove (3m) images demonstrate that our approach provides precise domain adaptation, preserving image content while improving radiometric accuracy and feature representation. A thorough image quality assessment and comparisons with the standard DDIM framework and five other leading methods are presented. We reach a mean Learned Perceptual Image Patch Similarity (mLPIPS) of 0.1884 and a Fr&#39;echet Inception Distance (FID) of 45.64, expressively outperforming all compared methods, including DDIM, ShuffleMixer, and SwinIR. The usefulness of our approach is further demonstrated in two Heterogeneous Change Detection tasks. </p>
<blockquote>
<p>在遥感领域，比较由不同传感器捕获的图像是一个常见的挑战。这需要进行图像翻译，即将图像从一个传感器领域转换到另一个传感器领域，同时保留原始内容。去噪扩散隐式模型（DDIM）是此类领域翻译中潜在的先进解决方案，因为它们在计算机视觉中的多个图像到图像翻译任务中已证明其卓越性。然而，这些模型在重现大规模多补丁图像的辐射特征方面遇到困难，导致整个图像的不一致性。这使得像异构变化检测这样的下游任务变得不切实际。为了克服这些限制，我们提出了一种利用去噪扩散进行多传感器光学图像有效翻译的方法，该方法适用于大规模区域。我们的方法将大规模低空间分辨率图像超分辨率转化为来自不同光学传感器的高分辨率等效图像，确保数百个补丁的一致性。我们的贡献在于新的前向和反向扩散过程，解决了大规模图像翻译的挑战。使用配对的Sentinel-II（10米）和Planet Dove（3米）图像进行的广泛实验表明，我们的方法提供了精确的领域适应性，在保留图像内容的同时提高了辐射精度和特征表示。进行了彻底的图像质量评估，并与标准的DDIM框架和其他五种领先方法进行了比较。我们达到了平均学习感知图像补丁相似性（mLPIPS）0.1884和Fr´echet Inception Distance（FID）45.64，显著优于所有比较方法，包括DDIM、ShuffleMixer和SwinIR。我们的方法的有用性在两项异构变化检测任务中得到了进一步证明。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11243v4">PDF</a> This is the accepted version of the manuscript published in IEEE   Journal of Selected Topics in Applied Earth Observations and Remote Sensing   (JSTARS). Please access the final version at IEEEXplore (Open Access). DOI   10.1109&#x2F;JSTARS.2024.3506032. This technology is protected by a patent filed   on 23 december 2023 at Office Luxembourgeois de la propri&#39;et&#39;e   intellectuelle (LU505861)</p>
<p><strong>Summary</strong><br>     针对遥感领域不同传感器图像转换的挑战，提出一种基于去噪扩散模型的大型多传感器光学图像翻译方法。该方法能够有效解决大型多斑块图像的放射特征再现问题，提高图像翻译的准确性和一致性。通过新的前向和反向扩散过程，实现对百余个斑块内图像的超级解析，将其从低分辨率转化为高分辨率的等效图像。实验结果显示，该方法在图像内容保存、放射精度提高及特征表现方面表现出优异性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>不同传感器图像比较在遥感中是常见挑战，需要进行图像翻译，即在一个传感器域中将图像转换为另一个传感器域的图像，同时保留原始内容。</li>
<li>去噪扩散隐式模型（DDIM）在图像到图像翻译任务中表现出卓越性能，但面临大型多斑块图像的放射特征再现问题。</li>
<li>提出一种基于去噪扩散的大型多传感器光学图像翻译方法，解决大型图像翻译中的不一致性问题，实现跨百余个斑块的一致性。</li>
<li>通过新的前向和反向扩散过程，将低分辨率的大型图像超级解析为等效的高分辨率图像。</li>
<li>实验结果显示，该方法在图像内容保存、放射精度及特征表现方面优于其他方法，包括DDIM、ShuffleMixer和SwinIR等。</li>
<li>所提出的方法在两个异质变化检测任务中表现出实用性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-cfdaac647fea9a9da3e300d824dca1d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5e066beb09802f355d490e3b5b325f04.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ef4c11e8c2fb8d018d9bc744f478600a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ee3a521043f07a16627886f2c9276e08.jpg" align="middle">
</details>




<h2 id="In-Context-Translation-Towards-Unifying-Image-Recognition-Processing-and-Generation"><a href="#In-Context-Translation-Towards-Unifying-Image-Recognition-Processing-and-Generation" class="headerlink" title="In-Context Translation: Towards Unifying Image Recognition, Processing,   and Generation"></a>In-Context Translation: Towards Unifying Image Recognition, Processing,   and Generation</h2><p><strong>Authors:Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang</strong></p>
<p>We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis). Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks. However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines. To this end, ICT introduces two designs. Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format. This turns different tasks into a general translation task between two RGB images. Secondly, it standardizes the training of different tasks into a general in-context learning, where “in-context” means the input comprises an example input-output pair of the target task and a query image. The learning objective is to generate the “missing” data paired with the query. The implicit translation process is thus between the query and the generated image. In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks. Notably, ICT performs well across three major categories of computer vision tasks, while its two competitors (Painter and PromptDiffusion) are only effective in at most two of these task categories. In addition, compared to its competitors, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training. </p>
<blockquote>
<p>我们提出了上下文翻译（ICT）这一通用学习框架，旨在统一视觉识别（例如语义分割）、低级图像处理（例如去噪）和条件图像生成（例如边缘到图像合成）。由于统一化设计，ICT显著减少了设计针对特定任务的模型所固有的归纳偏见，并最大限度地提高了类似任务之间的相互促进。然而，由于各种数据格式和训练管道，大量任务的统一并不容易。为此，ICT引入了两种设计。首先，它将不同任务的输入输出数据标准化为RGB图像对。例如，语义分割数据将RGB图像与其分割掩膜配对在同一RGB格式中。这将不同的任务转变为两个RGB图像之间的通用翻译任务。其次，它将不同任务的训练标准化为一般的上下文内学习，其中“上下文内”意味着输入包含目标任务的示例输入输出对和查询图像。学习目标是生成与查询配对“缺失”的数据。因此，隐式翻译过程发生在查询和生成的图像之间。在实验中，ICT统一了十种视觉任务，并在各自的基准测试中取得了令人印象深刻的性能。值得注意的是，ICT在三个主要的计算机视觉任务类别中都表现良好，而它的两个竞争对手（Painter和PromptDiffusion）最多只能在两个任务类别中有效。此外，与竞争对手相比，ICT仅在4个RTX 3090 GPU上进行训练，显示出更高的训练效率和更低的成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09633v2">PDF</a> </p>
<p><strong>Summary</strong><br>     提出一种名为In-Context Translation（ICT）的通用学习框架，实现视觉识别、低阶图像处理和条件图像生成的统一。ICT通过标准化任务输入&#x2F;输出数据和训练流程，减少特定任务的固有归纳偏见，并优化相似任务间的相互增强。它将不同任务转化为RGB图像对之间的通用翻译任务，并通过上下文学习进行训练。在多个计算机视觉任务上表现优异，相比其他模型更具效率和成本效益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICT是一个通用学习框架，能统一视觉识别、低阶图像处理和条件图像生成任务。</li>
<li>ICT通过标准化任务输入&#x2F;输出数据和训练流程来减少特定任务的归纳偏见。</li>
<li>ICT将不同任务转化为RGB图像对之间的翻译任务。</li>
<li>ICT采用上下文学习的方式进行训练，即输入包含目标任务的输入输出示例对和查询图像。</li>
<li>ICT在多个计算机视觉任务上表现优异，尤其是三大类别任务。</li>
<li>ICT相比其他模型（如Painter和PromptDiffusion）具有更广泛的应用性和效率。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-99464ec2b923496956b27481b8d471d2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aabc87a9b02f989dcf59036a740af033.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8fcfacd5c38b9f3ed43fd1cbf6c0ff74.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3b83bf71754d2658a9a2bbb4c73ee5c1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7641f5688750eabf7b67a6966681f18b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e313d821833c57ee4b726969c7b55d60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e3958cbdc15cdaf48765c9d33cdfe244.jpg" align="middle">
</details>




<h2 id="ContourDiff-Unpaired-Image-to-Image-Translation-with-Structural-Consistency-for-Medical-Imaging"><a href="#ContourDiff-Unpaired-Image-to-Image-Translation-with-Structural-Consistency-for-Medical-Imaging" class="headerlink" title="ContourDiff: Unpaired Image-to-Image Translation with Structural   Consistency for Medical Imaging"></a>ContourDiff: Unpaired Image-to-Image Translation with Structural   Consistency for Medical Imaging</h2><p><strong>Authors:Yuwen Chen, Nicholas Konz, Hanxue Gu, Haoyu Dong, Yaqian Chen, Lin Li, Jisoo Lee, Maciej A. Mazurowski</strong></p>
<p>Preserving object structure through image-to-image translation is crucial, particularly in applications such as medical imaging (e.g., CT-to-MRI translation), where downstream clinical and machine learning applications will often rely on such preservation. However, typical image-to-image translation algorithms prioritize perceptual quality with respect to output domain features over the preservation of anatomical structures. To address these challenges, we first introduce a novel metric to quantify the structural bias between domains which must be considered for proper translation. We then propose ContourDiff, a novel image-to-image translation algorithm that leverages domain-invariant anatomical contour representations of images to preserve the anatomical structures during translation. These contour representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. ContourDiff applies an input image contour representation as a constraint at every sampling step of a diffusion model trained in the output domain, ensuring anatomical content preservation for the output image. We evaluate our method on challenging lumbar spine and hip-and-thigh CT-to-MRI translation tasks, via (1) the performance of segmentation models trained on translated images applied to real MRIs, and (2) the foreground FID and KID of translated images with respect to real MRIs. Our method outperforms other unpaired image translation methods by a significant margin across almost all metrics and scenarios. Moreover, it achieves this without the need to access any input domain information during training. </p>
<blockquote>
<p>通过图像到图像的翻译来保留对象结构至关重要，特别是在医疗成像（例如CT到MRI的翻译）等应用中。下游的临床和机器学习应用通常都会依赖这种保留。然而，典型的图像到图像翻译算法会优先考虑输出域特征的感知质量，而不是保留解剖结构。为了解决这些挑战，我们首先引入了一种新的度量标准，来量化翻译时必须要考虑的域之间的结构偏差。然后，我们提出了ContourDiff，这是一种新的图像到图像翻译算法，它利用图像的域不变解剖轮廓表示在翻译过程中保留解剖结构。这些轮廓表示法可以从图像中轻松提取，但为它们的解剖内容形成了精确的空间约束。ContourDiff将输入图像的轮廓表示法作为扩散模型在输出域训练的每个采样步骤的约束，确保输出图像的解剖内容保留。我们通过具有挑战性的腰椎和髋关节CT到MRI的翻译任务来评估我们的方法，（1）训练于翻译图像上的分割模型在真实MRI上的应用性能，（2）翻译图像相对于真实MRI的前景FID和KID。我们的方法在几乎所有指标和场景中，都大大优于其他非配对图像翻译方法，而且，它在训练过程中无需访问任何输入域信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10786v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对医学成像等领域中的图像翻译问题，本文提出了一种新的图像翻译算法ContourDiff。该算法通过利用图像的域不变解剖轮廓表示来保留解剖结构，解决了传统图像翻译算法在翻译过程中忽略结构保留的问题。通过训练输出域的扩散模型，ContourDiff在采样步骤中应用输入图像的轮廓表示作为约束，确保输出图像的解剖内容得到保留。在腰椎和髋关节CT到MRI的翻译任务上，该方法显著优于其他非配对图像翻译方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像翻译中保留对象结构至关重要，特别是在医学影像领域。</li>
<li>传统图像翻译算法在保留解剖结构上存在挑战。</li>
<li>引入了一种新的度量标准来量化域之间的结构偏差，为正确翻译提供了考量。</li>
<li>ContourDiff算法利用图像的域不变解剖轮廓表示来解决结构保留问题。</li>
<li>ContourDiff通过训练输出域的扩散模型，并在采样步骤中应用输入图像轮廓作为约束来工作。</li>
<li>该方法在腰椎和髋关节CT到MRI的翻译任务上表现优异，显著优于其他非配对图像翻译方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da00fb386f746f96f3a237dc7602f46f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4b9f4eb7f30c1ec7c2892ca99f70f944.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-70e818591ae058f7b22f7740ea5b1081.jpg" align="middle">
</details>




<h2 id="Auxiliary-CycleGAN-guidance-for-Task-Aware-Domain-Translation-from-Duplex-to-Monoplex-IHC-Images"><a href="#Auxiliary-CycleGAN-guidance-for-Task-Aware-Domain-Translation-from-Duplex-to-Monoplex-IHC-Images" class="headerlink" title="Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from   Duplex to Monoplex IHC Images"></a>Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from   Duplex to Monoplex IHC Images</h2><p><strong>Authors:Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter, Shashank Saran, Marlon Rebelatto, Günter Schmidt</strong></p>
<p>Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches. </p>
<blockquote>
<p>生成模型能够从容易获得训练模型的源图像域翻译到训练期间未见的目标域。虽然循环生成对抗网络（GANs）已经建立，但相关的循环一致性约束依赖于两个域之间存在可逆映射。然而，对于染色单用和双重免疫组织化学（IHC）测定法之间的图像翻译而言，情况并非如此。我们专注于从后者到前者的翻译，通过引入新型训练设计，利用一组免疫荧光（IF）图像作为非配对图像域，提出一种替代约束。下游分割任务的定量和定性结果展示了该方法相较于基准方法的好处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07389v2">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了生成模型在图像域转换中的应用，特别是在Cycle GANs的改进上。针对免疫组织化学染色图像与免疫荧光图像之间的转换问题，提出了一种新的训练设计方法，利用免疫荧光图像作为辅助非配对图像域，提高转换效果。在下游分割任务上的定量和定性结果表明该方法相较于基准方法更具优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型可实现从易训练模型所在源域到训练期间未见目标域的翻译。</li>
<li>Cycle GANs依赖两个域之间存在可逆映射，但某些图像转换（如免疫组织化学染色与免疫荧光图像间转换）中此条件并不满足。</li>
<li>提出了一种新型训练设计方法，用于解决特定图像转换问题，即通过引入免疫荧光图像作为辅助的非配对图像域进行约束。</li>
<li>新方法在下游分割任务上进行了定量和定性测试，结果证明了其相较于基准方法的效果提升。</li>
<li>该方法对于处理复杂图像转换问题具有潜在应用价值。</li>
<li>研究展示了跨学科融合（如生物医学图像处理与机器学习）在解决特定问题中的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f3e7f2c8f6b547e5cfda44f7d51f2dd1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abfadb78af9b21221c2a37c6f5448951.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffdfb6ab93b365ba5c1ce937f218ada2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-88588e91023ccf64705b7c67715db491.jpg" align="middle">
</details>




<h2 id="FDDM-Unsupervised-Medical-Image-Translation-with-a-Frequency-Decoupled-Diffusion-Model"><a href="#FDDM-Unsupervised-Medical-Image-Translation-with-a-Frequency-Decoupled-Diffusion-Model" class="headerlink" title="FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled   Diffusion Model"></a>FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled   Diffusion Model</h2><p><strong>Authors:Yunxiang Li, Hua-Chieh Shao, Xiaoxue Qian, You Zhang</strong></p>
<p>Diffusion models have demonstrated significant potential in producing high-quality images in medical image translation to aid disease diagnosis, localization, and treatment. Nevertheless, current diffusion models have limited success in achieving faithful image translations that can accurately preserve the anatomical structures of medical images, especially for unpaired datasets. The preservation of structural and anatomical details is essential to reliable medical diagnosis and treatment planning, as structural mismatches can lead to disease misidentification and treatment errors. In this study, we introduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT conversion. FDDM first obtains the anatomical information of the CT image from the MR image through an initial conversion module. This anatomical information then guides a subsequent diffusion model to generate high-quality CT images. Our diffusion model uses a dual-path reverse diffusion process for low-frequency and high-frequency information, achieving a better balance between image quality and anatomical accuracy. We extensively evaluated FDDM using public datasets for brain MR-to-CT and pelvis MR-to-CT translations, demonstrating its superior performance to other GAN-based, VAE-based, and diffusion-based models. The evaluation metrics included Frechet Inception Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). FDDM achieved the best scores on all metrics for both datasets, particularly excelling in FID, with scores of 25.9 for brain data and 29.2 for pelvis data, significantly outperforming other methods. These results demonstrate that FDDM can generate high-quality target domain images while maintaining the accuracy of translated anatomical structures. </p>
<blockquote>
<p>扩散模型在医学图像翻译中生成高质量图像以辅助疾病诊断、定位和治疗的潜力巨大。然而，当前扩散模型在忠实图像翻译方面取得的成效有限，难以准确保留医学图像的结构信息，尤其在非配对数据集方面更是如此。保留结构和解剖细节对于可靠的医学诊断和治疗计划至关重要，结构不匹配可能导致疾病误判和治疗失误。本研究中，我们针对MR到CT的转换引入了频率解耦扩散模型（FDDM）。FDDM首先通过初始转换模块从MR图像中获取CT图像的解剖信息。然后，此解剖信息引导后续的扩散模型生成高质量的CT图像。我们的扩散模型采用双路径反向扩散过程处理低频和高频信息，在图像质量和解剖准确性之间取得更好的平衡。我们利用公共数据集对FDDM进行了广泛的脑部MR到CT和骨盆MR到CT的翻译评估，证明了其优于其他基于GAN、VAE和扩散的模型。评估指标包括弗雷歇特入距离（FID）、峰值信噪比（PSNR）和结构相似性指数度量（SSIM）。FDDM在两个数据集的所有指标上都取得了最佳成绩，特别是在FID上表现尤为出色，脑部数据得分为25.9，骨盆数据得分为29.2，显著优于其他方法。这些结果表明，FDDM可以生成高质量的目标域图像，同时保持翻译解剖结构的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12070v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了频率解耦扩散模型（FDDM）在医学图像翻译中的应用，特别是在磁共振（MR）图像到计算机断层扫描（CT）图像的转换中的表现。该模型通过结合初始转换模块和双重路径反向扩散过程，提高了图像质量和结构准确性之间的平衡。在公开数据集上的广泛评估显示，FDDM在MR到CT转换中优于其他基于GAN、VAE和扩散的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在医学图像翻译中产生高质量图像方面具有显著潜力，有助于疾病诊断、定位和治疗的辅助。</li>
<li>当前扩散模型在忠实图像翻译方面存在局限性，难以准确保留医学图像的结构细节。</li>
<li>FDDM模型通过结合初始转换模块和双重路径反向扩散过程实现医学图像的高质量转换。</li>
<li>FDDM模型能够在MR图像到CT图像的转换中更好地平衡图像质量和结构准确性。</li>
<li>FDDM模型在公共数据集上进行了广泛评估，表现出优异的性能，优于其他基于GAN、VAE和扩散的模型。</li>
<li>FDDM模型的评估指标包括Frechet Inception Distance (FID)、Peak Signal-to-Noise Ratio (PSNR)和Structural Similarity Index Measure (SSIM)，在所有指标上都取得了最佳成绩。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1b2e498047ac9af8e439b084063560b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ca1b8bd30c8b11ff29e405463dc84309.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e451820915d082591eefb3de75ce1914.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c994d26ec8e9340fc481d7283cf92a0e.jpg" align="middle">
</details>




<h2 id="One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN"><a href="#One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN" class="headerlink" title="One-for-All: Towards Universal Domain Translation with a Single StyleGAN"></a>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h2><p><strong>Authors:Yong Du, Jiahui Zhan, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, Shengfeng He</strong></p>
<p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the StyleGAN’s latent space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public. </p>
<blockquote>
<p>本文提出了一种新型翻译模型UniTranslator，用于在训练数据有限和视觉差异显著的情况下，实现不同视觉领域之间的表示转换。我们的方法的主要思想是利用CLIP的中立域能力作为桥梁机制，同时使用一个单独的模块来从源领域和目标领域的嵌入中提取抽象、领域无关语义。将这些抽象语义与目标特定语义融合，得到CLIP空间内的转换嵌入。为了弥CLIP和StyleGAN之间不同世界的差距，我们引入了一种新的非线性映射器——CLIP2P映射器。该模块利用CLIP嵌入进行定制，以近似StyleGAN潜在空间中的潜在分布，有效地作为这两个空间之间的连接器。所提出的UniTranslator通用性强，能够执行各种任务，包括风格混合、风格化和翻译，即使在跨不同视觉领域的视觉挑战场景中也是如此。值得注意的是，UniTranslator生成的翻译作品展示了领域相关性、多样性和改进的图像质量。UniTranslator超越了现有通用模型的性能，在代表性任务中表现良好，甚至超越了专业模型。源代码和训练后的模型将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14222v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型翻译模型UniTranslator，用于在训练数据有限和视觉差异显著的情况下，实现不同视觉领域之间的表示转换。该模型利用CLIP的域中性能力作为桥梁机制，同时使用单独模块从源域和目标域嵌入中提取抽象、领域无关的语义。这些抽象语义与目标特定语义融合，在CLIP空间内生成转换后的嵌入。为弥合CLIP和StyleGAN之间鸿沟，引入了新的非线性映射器CLIP2P映射器。该模块利用CLIP嵌入，旨在近似StyleGAN潜在空间中的潜在分布，从而在这两个空间之间发挥连接作用。UniTranslator功能强大，可执行包括风格混合、风格化和翻译在内的各种任务，即使在跨不同视觉领域的视觉挑战场景中也能生成高质量、具有领域相关性、多样性和改进的图像质量的翻译。它超越了现有通用模型的性能，并在代表性任务中表现良好，甚至超过了专业模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTranslator是一个用于视觉领域间转换的新型翻译模型。</li>
<li>该模型利用CLIP的域中性能力作为桥梁机制。</li>
<li>UniTranslator使用单独模块提取源域和目标域的抽象、领域无关的语义。</li>
<li>CLIP2P映射器的引入，旨在弥合CLIP和StyleGAN之间的鸿沟。</li>
<li>UniTranslator能执行多种任务，包括风格混合、风格化和翻译。</li>
<li>UniTranslator能生成高质量、具有领域相关性、多样性和改进的图像质量的翻译。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-95e6272f85d7fa4d37d1ae29b4643ac3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7a24b3f85d591a3f1544bb1f8be5a6ea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0f2ec04d706bea45231594a7de44808b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-32e26e6d642dd74ec5491b260c7f5d69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ed3350cdfd555c1320fc865348c6003.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b5fcd9c09d6cdabfa03da9bbb7f05557.jpg" align="middle">
</details>




<h2 id="RL-I2IT-Image-to-Image-Translation-with-Deep-Reinforcement-Learning"><a href="#RL-I2IT-Image-to-Image-Translation-with-Deep-Reinforcement-Learning" class="headerlink" title="RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning"></a>RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning</h2><p><strong>Authors:Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu, Hongtu Zhu, Xin Li, Siwei Lyu</strong></p>
<p>Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dimensional action. In the RL-I2IT framework, we also employ a task-specific auxiliary learning strategy to stabilize the training process and improve the performance of the corresponding task. Experiments on several I2IT tasks demonstrate the effectiveness and robustness of the proposed method when facing high-dimensional continuous action space problems. Our implementation of the RL-I2IT framework is available at <a target="_blank" rel="noopener" href="https://github.com/Algolzw/SPAC-Deformable-Registration">https://github.com/Algolzw/SPAC-Deformable-Registration</a>. </p>
<blockquote>
<p>现有的大多数图像到图像翻译（I2IT）方法都是使用深度学习（DL）模型在一次运行中生成图像。然而，设计这样的单步模型总是具有挑战性，需要大量的参数，并且容易陷入糟糕的全局最小值和过度拟合。在这项工作中，我们通过深度强化学习（DRL）重新定义了I2IT作为一个逐步决策问题，并提出了一种基于强化学习的I2IT（RL-I2IT）新型框架。RL-I2IT框架的关键功能是将单一的学习过程分解为具有小型模型的多个步骤，以逐步将源图像连续变换为目标图像。考虑到在传统的强化学习框架中处理高维连续状态和动作空间是一个挑战，我们引入了元策略，并引入了新概念计划到标准的Actor-Critic模型中。这是一个比原始图像更低维度的概念，可以促进Actor生成可控制的高维动作。在RL-I2IT框架中，我们还采用特定任务的辅助学习策略来稳定训练过程并提高相应任务的性能。在几个I2IT任务上的实验表明，在面对高维连续动作空间问题时，该方法的有效性和鲁棒性。我们的RL-I2IT框架实现可在<a target="_blank" rel="noopener" href="https://github.com/Algolzw/SPAC-Deformable-Registration%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Algolzw/SPAC-Deformable-Registration上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13672v7">PDF</a> </p>
<p><strong>Summary</strong><br>基于深度强化学习（DRL）的图像到图像翻译（I2IT）方法。传统的I2IT方法面临挑战，例如需要大量参数和容易陷入不良全局最小值和过度拟合。本研究将I2IT重新构建为基于步骤的决策问题，并提出了基于强化学习的I2IT框架（RL-I2IT）。通过分解单一学习过程为多个小步骤，使用轻量级模型逐步将源图像转换为目标图像。为解决传统强化学习框架处理高维连续状态和动作空间的问题，引入元政策和计划概念到标准Actor-Critic模型中，降低维度并促进生成可追踪的高维动作。同时采用任务特定的辅助学习策略来稳定训练过程并改进任务性能。在多个I2IT任务上的实验验证了所提方法在处理高维连续动作空间问题时的有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究将传统的图像到图像翻译（I2IT）方法重新构建为基于步骤的决策问题。</li>
<li>提出了一种新型的基于强化学习的I2IT框架（RL-I2IT），通过分解单一学习过程为多个小步骤来解决传统I2IT面临的挑战。</li>
<li>引入元政策和计划概念来解决高维连续状态和动作空间的问题，使得强化学习模型能够更好地处理图像转换任务。</li>
<li>采用任务特定的辅助学习策略来稳定训练过程并提高任务性能。</li>
<li>实验结果证明了该框架在处理高维连续动作空间问题时的有效性和稳健性。</li>
<li>该研究的实现代码已公开在GitHub上可供参考和使用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f361861676b29cf916d3420e8181d712.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c58af038f40fea38ddc32cbf07c73153.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e6d58993ad91ecc7661f77c504c782d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e45581393a587038f09dc17aa45fc9cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-937bbf91c309ca7aedf958e5e20fa69a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-25e7d2205f30b34b480b2e864b2270c4.jpg" align="middle">
</details>




<h2 id="The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI"><a href="#The-Brain-Tumor-Segmentation-BraTS-METS-Challenge-2023-Brain-Metastasis-Segmentation-on-Pre-treatment-MRI" class="headerlink" title="The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI"></a>The Brain Tumor Segmentation (BraTS-METS) Challenge 2023: Brain   Metastasis Segmentation on Pre-treatment MRI</h2><p><strong>Authors:Ahmed W. Moawad, Anastasia Janas, Ujjwal Baid, Divya Ramakrishnan, Rachit Saluja, Nader Ashraf, Nazanin Maleki, Leon Jekel, Nikolay Yordanov, Pascal Fehringer, Athanasios Gkampenis, Raisa Amiruddin, Amirreza Manteghinejad, Maruf Adewole, Jake Albrecht, Udunna Anazodo, Sanjay Aneja, Syed Muhammad Anwar, Timothy Bergquist, Veronica Chiang, Verena Chung, Gian Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Nastaran Khalili, Keyvan Farahani, Juan Eugenio Iglesias, Zhifan Jiang, Elaine Johanson, Anahita Fathi Kazerooni, Florian Kofler, Kiril Krantchev, Dominic LaBella, Koen Van Leemput, Hongwei Bran Li, Marius George Linguraru, Xinyang Liu, Zeke Meier, Bjoern H Menze, Harrison Moy, Klara Osenberg, Marie Piraud, Zachary Reitman, Russell Takeshi Shinohara, Chunhao Wang, Benedikt Wiestler, Walter Wiggins, Umber Shafique, Klara Willms, Arman Avesta, Khaled Bousabarah, Satrajit Chakrabarty, Nicolo Gennaro, Wolfgang Holler, Manpreet Kaur, Pamela LaMontagne, MingDe Lin, Jan Lost, Daniel S. Marcus, Ryan Maresca, Sarah Merkaj, Gabriel Cassinelli Pedersen, Marc von Reppert, Aristeidis Sotiras, Oleg Teytelboym, Niklas Tillmans, Malte Westerhoff, Ayda Youssef, Devon Godfrey, Scott Floyd, Andreas Rauschecker, Javier Villanueva-Meyer, Irada Pfluger, Jaeyoung Cho, Martin Bendszus, Gianluca Brugnara, Justin Cramer, Gloria J. Guzman Perez-Carillo, Derek R. Johnson, Anthony Kam, Benjamin Yin Ming Kwan, Lillian Lai, Neil U. Lall, Fatima Memon, Mark Krycia, Satya Narayana Patro, Bojan Petrovic, Tiffany Y. So, Gerard Thompson, Lei Wu, E. Brooke Schrickel, Anu Bansal, Frederik Barkhof, Cristina Besada, Sammy Chu, Jason Druzgal, Alexandru Dusoi, Luciano Farage, Fabricio Feltrin, Amy Fong, Steve H. Fung, R. Ian Gray, Ichiro Ikuta, Michael Iv, Alida A. Postma, Amit Mahajan, David Joyner, Chase Krumpelman, Laurent Letourneau-Guillon, Christie M. Lincoln, Mate E. Maros, Elka Miller, Fanny Moron, Esther A. Nimchinsky, Ozkan Ozsarlak, Uresh Patel, Saurabh Rohatgi, Atin Saha, Anousheh Sayah, Eric D. Schwartz, Robert Shih, Mark S. Shiroishi, Juan E. Small, Manoj Tanwar, Jewels Valerie, Brent D. Weinberg, Matthew L. White, Robert Young, Vahe M. Zohrabian, Aynur Azizova, Melanie Maria Theresa Bruseler, Mohanad Ghonim, Mohamed Ghonim, Abdullah Okar, Luca Pasquini, Yasaman Sharifi, Gagandeep Singh, Nico Sollmann, Theodora Soumala, Mahsa Taherzadeh, Philipp Vollmuth, Martha Foltyn-Dumitru, Ajay Malhotra, Aly H. Abayazeed, Francesco Dellepiane, Philipp Lohmann, Victor M. Perez-Garcia, Hesham Elhalawani, Maria Correia de Verdier, Sanaria Al-Rubaiey, Rui Duarte Armindo, Kholod Ashraf, Moamen M. Asla, Mohamed Badawy, Jeroen Bisschop, Nima Broomand Lomer, Jan Bukatz, Jim Chen, Petra Cimflova, Felix Corr, Alexis Crawley, Lisa Deptula, Tasneem Elakhdar, Islam H. Shawali, Shahriar Faghani, Alexandra Frick, Vaibhav Gulati, Muhammad Ammar Haider, Fatima Hierro, Rasmus Holmboe Dahl, Sarah Maria Jacobs, Kuang-chun Jim Hsieh, Sedat G. Kandemirli, Katharina Kersting, Laura Kida, Sofia Kollia, Ioannis Koukoulithras, Xiao Li, Ahmed Abouelatta, Aya Mansour, Ruxandra-Catrinel Maria-Zamfirescu, Marcela Marsiglia, Yohana Sarahi Mateo-Camacho, Mark McArthur, Olivia McDonnell, Maire McHugh, Mana Moassefi, Samah Mostafa Morsi, Alexander Munteanu, Khanak K. Nandolia, Syed Raza Naqvi, Yalda Nikanpour, Mostafa Alnoury, Abdullah Mohamed Aly Nouh, Francesca Pappafava, Markand D. Patel, Samantha Petrucci, Eric Rawie, Scott Raymond, Borna Roohani, Sadeq Sabouhi, Laura M. Sanchez-Garcia, Zoe Shaked, Pokhraj P. Suthar, Talissa Altes, Edvin Isufi, Yaseen Dhemesh, Jaime Gass, Jonathan Thacker, Abdul Rahman Tarabishy, Benjamin Turner, Sebastiano Vacca, George K. Vilanilam, Daniel Warren, David Weiss, Fikadu Worede, Sara Yousry, Wondwossen Lerebo, Alejandro Aristizabal, Alexandros Karargyris, Hasan Kassem, Sarthak Pati, Micah Sheller, Katherine E. Link, Evan Calabrese, Nourel hoda Tahon, Ayman Nada, Yuri S. Velichko, Spyridon Bakas, Jeffrey D. Rudie, Mariam Aboian</strong></p>
<p>The translation of AI-generated brain metastases (BM) segmentation into clinical practice relies heavily on diverse, high-quality annotated medical imaging datasets. The BraTS-METS 2023 challenge has gained momentum for testing and benchmarking algorithms using rigorously annotated internationally compiled real-world datasets. This study presents the results of the segmentation challenge and characterizes the challenging cases that impacted the performance of the winning algorithms. Untreated brain metastases on standard anatomic MRI sequences (T1, T2, FLAIR, T1PG) from eight contributed international datasets were annotated in stepwise method: published UNET algorithms, student, neuroradiologist, final approver neuroradiologist. Segmentations were ranked based on lesion-wise Dice and Hausdorff distance (HD95) scores. False positives (FP) and false negatives (FN) were rigorously penalized, receiving a score of 0 for Dice and a fixed penalty of 374 for HD95. Eight datasets comprising 1303 studies were annotated, with 402 studies (3076 lesions) released on Synapse as publicly available datasets to challenge competitors. Additionally, 31 studies (139 lesions) were held out for validation, and 59 studies (218 lesions) were used for testing. Segmentation accuracy was measured as rank across subjects, with the winning team achieving a LesionWise mean score of 7.9. Common errors among the leading teams included false negatives for small lesions and misregistration of masks in space.The BraTS-METS 2023 challenge successfully curated well-annotated, diverse datasets and identified common errors, facilitating the translation of BM segmentation across varied clinical environments and providing personalized volumetric reports to patients undergoing BM treatment. </p>
<blockquote>
<p>将人工智能生成的脑转移瘤（BM）分割转化为临床实践，很大程度上依赖于多样且高质量标注的医学影像数据集。BraTS-METS 2023挑战赛利用严格标注的国际真实数据集对算法进行测试和基准测试，从而获得了动力。本研究介绍了分割挑战的结果，并对影响获胜算法性能的挑战性病例进行了特征描述。对来自八个国际数据集的未经治疗的脑转移瘤在标准解剖MRI序列（T1、T2、FLAIR、T1PG）上采用逐步方法进行了标注：公开UNET算法、学生、神经放射学家、最终审批神经放射学家。根据病灶级别的Dice和Hausdorff距离（HD95）得分对分割进行排名。假阳性（FP）和假阴性（FN）受到严格惩罚，Dice得分为0，HD95的固定惩罚为374。对包含1303项研究的八个数据集进行了标注，其中在Synapse上发布了包含公众可用数据集的402项研究（涉及3076个病灶）以挑战竞争对手。此外，还有31项研究（涉及139个病灶）用于验证，还有59项研究（涉及218个病灶）用于测试。分割精度是通过不同受试者之间的排名来衡量的，冠军团队的病灶级平均得分为7.9分。顶尖团队常见的错误包括小病灶的假阴性以及口罩在空间上的错位。BraTS-METS 2023挑战赛成功地精心挑选了标注良好且多样的数据集，并确定了常见错误，促进了BM分割在临床环境中的转化，并为接受BM治疗的病人提供个性化的体积报告。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.00838v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了人工智能在医学领域的进展，特别是在大脑转移瘤分割上的应用。文章强调了BrATS-METS 2023挑战赛在推动该领域发展中的作用，该挑战旨在测试算法使用严格注释的国际汇编的真实世界数据集。文章总结了挑战赛的结果，并描述了影响算法性能的挑战性案例。最后指出，挑战赛成功筛选出了高质量的多样数据集，促进了BM分割技术在不同临床环境中的实际应用。同时为患者提供了个性化的体积报告以指导BM治疗。简言之，该研究对医学成像翻译具有积极影响。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是该文本的关键见解要点：</p>
<ol>
<li>AI在医学领域的广泛应用中，大脑转移瘤分割技术的应用得到了重点发展。这涉及到机器学习算法的深度应用以及医学影像数据的处理和解析。 </li>
<li>BraTS-METS 2023挑战赛是一个重要事件，用于测试和评估针对真实世界数据集的算法性能。这一挑战利用了国际上汇编的严格注释的数据集进行实践测试。 </li>
<li>文章对挑战赛的结果进行了详细分析，并指出了影响算法性能的挑战性案例，包括对小病灶的漏检以及空间定位错误等问题。 </li>
<li>该挑战赛成功构建了高质量、多样化的数据集，促进了大脑转移瘤分割技术在不同临床环境中的实际应用。这对于医学图像翻译和人工智能在临床实践中的应用具有积极影响。 </li>
<li>通过挑战赛的结果，我们能够识别出当前人工智能在医学图像处理中的常见错误和挑战，为未来的研究和改进提供了方向。 </li>
<li>该研究为患者提供了个性化的体积报告，有助于医生更准确地诊断和处理大脑转移瘤的问题，进而为肿瘤治疗提供更加精确的指导。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b168721366c887167f46d5b0033e7041.jpg" align="middle">
</details>




<h2 id="Vision-and-Language-Pretraining"><a href="#Vision-and-Language-Pretraining" class="headerlink" title="Vision-and-Language Pretraining"></a>Vision-and-Language Pretraining</h2><p><strong>Authors:Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu, See-Kiong Ng, Anh Tuan Luu</strong></p>
<p>With the burgeoning amount of data of image-text pairs and diversity of Vision-and-Language (V&amp;L) tasks, scholars have introduced an abundance of deep learning models in this research domain. Furthermore, in recent years, transfer learning has also shown tremendous success in Computer Vision for tasks such as Image Classification, Object Detection, etc., and in Natural Language Processing for Question Answering, Machine Translation, etc. Inheriting the spirit of Transfer Learning, research works in V&amp;L have devised multiple pretraining techniques on large-scale datasets in order to enhance the performance of downstream tasks. The aim of this article is to provide a comprehensive revision of contemporary V&amp;L pretraining models. In particular, we categorize and delineate pretraining approaches, along with the summary of state-of-the-art vision-and-language pretrained models. Moreover, a list of training datasets and downstream tasks is supplied to further polish the perspective into V&amp;L pretraining. Lastly, we decided to take a further step to discuss numerous directions for future research. </p>
<blockquote>
<p>随着图像文本对数据的不断增加和视觉与语言（V&amp;L）任务的多样性，学者们在这个研究领域中已经引入了大量的深度学习模型。此外，近年来，迁移学习在计算机视觉的图像分类、目标检测等任务以及自然语言处理的问答、机器翻译等任务中也取得了巨大的成功。秉承迁移学习的精神，视觉与语言领域的研究工作在大规模数据集上设计了多种预训练技术，以提高下游任务的性能。本文的目的是对当代视觉与语言预训练模型进行全面回顾。特别是，我们对预训练方法进行分类和描述，以及对最先进的视觉和语言预训练模型进行总结。此外，还提供了一系列训练数据集和下游任务，以进一步深入了解视觉与语言的预训练。最后，我们决定进一步讨论未来研究的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2207.01772v3">PDF</a> The content of the paper has been outdated. I would like to rewrite a   new version with completely new information.</p>
<p><strong>摘要</strong></p>
<p>随着图像文本对数据的不断增加和视觉与语言（V&amp;L）任务的多样性，学者们在此研究领域中引入了大量的深度学习模型。近年来，迁移学习在计算机视觉领域的图像分类、目标检测等任务以及自然语言处理领域的问答、机器翻译等任务中取得了巨大成功。秉承迁移学习的精神，视觉与语言领域的研究工作在大型数据集上设计了多种预训练技术，以提高下游任务性能。本文旨在全面回顾当代的视觉与语言预训练模型。我们分类并概述了预训练的方法，总结了最新的视觉与语言预训练模型。此外，还提供了训练数据集和下游任务的列表，以进一步深入了解视觉与语言的预训练。最后，我们对未来的研究方向进行了深入的探讨。</p>
<p><strong>要点</strong></p>
<ol>
<li>视觉与语言（V&amp;L）研究领域引入了大量的深度学习模型，应对图像文本对数据的增加和任务多样性的挑战。</li>
<li>迁移学习在计算机视觉和自然语言处理任务中取得了显著成功。</li>
<li>视觉与语言领域的研究工作设计了多种预训练技术，以提高下游任务性能。</li>
<li>本文旨在全面回顾当代的视觉与语言预训练模型，包括预训练方法的分类和概述。</li>
<li>总结了最新的视觉与语言预训练模型。</li>
<li>提供了训练数据集和下游任务的列表，以更好地了解视觉与语言的预训练。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-901ffeab1491e4333b404e24ecdba330.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-680b620c5c1075fc76b70227619887d9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6b1c31a0d8f9f8fc8e1eacd6162a387c.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2024-12-12  Exploring What Why and How A Multifaceted Benchmark for Causation   Understanding of Video Anomaly
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-49140b188a6f956a499b95873261af74.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
