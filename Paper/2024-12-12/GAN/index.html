<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p>
<p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the modelâ€™s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models. </p>
<blockquote>
<p>å›¾åƒåå°„å»é™¤å¯¹äºæ¢å¤å›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚æ‰­æ›²çš„å›¾åƒä¼šå¯¹ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å•å¹…å›¾åƒè¿›è¡Œå›¾åƒåå°„å»é™¤çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æ²¡æœ‰å…³æ³¨æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§å¯æ¨å¹¿è‡³å›¾åƒåˆ°å›¾åƒé—®é¢˜çš„æ–°è®­ç»ƒæŠ€æœ¯ï¼Œè¾“å…¥å’Œè¾“å‡ºçš„æ€§è´¨ç›¸ä¼¼ã€‚è¿™ç§æŠ€æœ¯ä½“ç°åœ¨æˆ‘ä»¬çš„å¤šæ­¥æŸå¤±æœºåˆ¶ä¸­ï¼Œåœ¨åå°„å»é™¤ä»»åŠ¡ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆPix2Pix GANï¼‰åˆæˆäº†ä¸€ç§é«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGanï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹å­¦ä¹ åå°„å»é™¤çš„æ›´å¥½æ¨¡å¼çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„èŒƒå›´æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å…¶ç¼ºä¹åå°„æ·±åº¦ä¼°è®¡çš„ç‰¹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SIR^2åŸºå‡†å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ä¼˜äºå…¶ä»–æœ€æ–°æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å•å›¾åƒè¿›è¡Œå›¾åƒåå°„å»é™¤çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯å¹¿æ³›åº”ç”¨äºåŒç±»å›¾åƒè½¬æ¢é—®é¢˜ã€‚é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•åœ¨åå°„å»é™¤ä»»åŠ¡ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºè§£å†³åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜åˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å…¶ç¼ºä¹åå°„çš„ç‰¹æ€§ã€‚è¯¥æ–¹æ³•åœ¨SIR^2åŸºå‡†æµ‹è¯•å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯å¹¿æ³›åº”ç”¨äºåŒç±»å›¾åƒè½¬æ¢é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶ï¼Œæé«˜äº†åå°„å»é™¤çš„æ•ˆæœã€‚</li>
<li>åˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œæé«˜äº†åå°„å»é™¤çš„å‡†ç¡®åº¦ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨SIR^2åŸºå‡†æµ‹è¯•å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä¸ºå›¾åƒåå°„å»é™¤é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90610239ae13c51e1614f127958f2a81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bde3fb0cbe989335b0d5bb7a4d46e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b852c3cdd4ace9f1729fdab0540434.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab1d495bb0788c654b34111e35dbd40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f69c1d9eb0ee3bcc36085da2e8bd9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9175c1aae7fa57889e8641ff09cdbe6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f9871afd6e42b8e1cadb0f3e2faa7fc.jpg" align="middle">
</details>




<h2 id="Fine-grained-Text-to-Image-Synthesis"><a href="#Fine-grained-Text-to-Image-Synthesis" class="headerlink" title="Fine-grained Text to Image Synthesis"></a>Fine-grained Text to Image Synthesis</h2><p><strong>Authors:Xu Ouyang, Ying Chen, Kaiyue Zhu, Gady Agam</strong></p>
<p>Fine-grained text to image synthesis involves generating images from texts that belong to different categories. In contrast to general text to image synthesis, in fine-grained synthesis there is high similarity between images of different subclasses, and there may be linguistic discrepancy among texts describing the same image. Recent Generative Adversarial Networks (GAN), such as the Recurrent Affine Transformation (RAT) GAN model, are able to synthesize clear and realistic images from texts. However, GAN models ignore fine-grained level information. In this paper we propose an approach that incorporates an auxiliary classifier in the discriminator and a contrastive learning method to improve the accuracy of fine-grained details in images synthesized by RAT GAN. The auxiliary classifier helps the discriminator classify the class of images, and helps the generator synthesize more accurate fine-grained images. The contrastive learning method minimizes the similarity between images from different subclasses and maximizes the similarity between images from the same subclass. We evaluate on several state-of-the-art methods on the commonly used CUB-200-2011 bird dataset and Oxford-102 flower dataset, and demonstrated superior performance. </p>
<blockquote>
<p>ç»†ç²’åº¦æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ¶‰åŠä»ä¸åŒç±»åˆ«çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚ä¸ä¸€èˆ¬çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆç›¸æ¯”ï¼Œåœ¨ç»†ç²’åº¦åˆæˆä¸­ï¼Œä¸åŒå­ç±»çš„å›¾åƒä¹‹é—´å­˜åœ¨é«˜ç›¸ä¼¼æ€§ï¼Œæè¿°åŒä¸€å›¾åƒçš„æ–‡æœ¬ä¹‹é—´ä¹Ÿå¯èƒ½å­˜åœ¨è¯­è¨€å·®å¼‚ã€‚æœ€è¿‘çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå¦‚å¾ªç¯ä»¿å°„å˜æ¢ï¼ˆRATï¼‰GANæ¨¡å‹ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬ä¸­åˆæˆæ¸…æ™°å’Œç°å®çš„å›¾åƒã€‚ç„¶è€Œï¼ŒGANæ¨¡å‹å¿½ç•¥äº†ç»†ç²’åº¦çº§åˆ«çš„ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨é‰´åˆ«å™¨ä¸­èå…¥è¾…åŠ©åˆ†ç±»å™¨ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜ç”±RAT GANåˆæˆçš„å›¾åƒçš„ç»†ç²’åº¦ç»†èŠ‚çš„å‡†ç¡®æ€§ã€‚è¾…åŠ©åˆ†ç±»å™¨å¸®åŠ©é‰´åˆ«å™¨å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¸®åŠ©ç”Ÿæˆå™¨åˆæˆæ›´å‡†ç¡®çš„ç»†ç²’åº¦å›¾åƒã€‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•æœ€å°åŒ–ä¸åŒå­ç±»å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æœ€å¤§åŒ–åŒä¸€å­ç±»å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬åœ¨å¸¸ç”¨çš„CUB-200-2011é¸Ÿç±»æ•°æ®é›†å’ŒOxford-102èŠ±å‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07196v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒåˆæˆæŠ€æœ¯èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆå¯¹åº”çš„å›¾åƒã€‚ä½†åœ¨ç²¾ç»†ç²’åº¦çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­ï¼Œä¸åŒå­ç±»åˆ«çš„å›¾åƒé—´é«˜åº¦ç›¸ä¼¼ï¼Œæè¿°ç›¸åŒå›¾åƒçš„æ–‡æœ¬å¯èƒ½å­˜åœ¨è¯­è¨€å·®å¼‚ã€‚ä¸ºæé«˜åˆæˆå›¾åƒçš„ç²¾ç»†ç²’åº¦ç»†èŠ‚å‡†ç¡®æ€§ï¼Œæœ¬æ–‡æå‡ºåœ¨åˆ¤åˆ«å™¨ä¸­åŠ å…¥è¾…åŠ©åˆ†ç±»å™¨å¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚è¾…åŠ©åˆ†ç±»å™¨å¸®åŠ©åˆ¤åˆ«å™¨å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼ŒåŒæ—¶å¸®åŠ©ç”Ÿæˆå™¨ç”Ÿæˆæ›´å‡†ç¡®çš„ç²¾ç»†ç²’åº¦å›¾åƒã€‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•åˆ™å‡å°‘ä¸åŒå­ç±»å›¾åƒé—´çš„ç›¸ä¼¼æ€§å¹¶å¢åŠ ç›¸åŒå­ç±»å›¾åƒé—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨å¸¸ç”¨çš„CUB-200-2011é¸Ÿç±»æ•°æ®é›†å’ŒOxford-102èŠ±å‰æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒåˆæˆæŠ€æœ¯å¯ç”Ÿæˆä¸æ–‡æœ¬ç›¸å¯¹åº”çš„å›¾åƒã€‚</li>
<li>ç²¾ç»†ç²’åº¦æ–‡æœ¬åˆ°å›¾åƒåˆæˆé¢ä¸´ä¸åŒå­ç±»å›¾åƒé«˜åº¦ç›¸ä¼¼çš„é—®é¢˜ã€‚</li>
<li>è¾…åŠ©åˆ†ç±»å™¨çš„åŠ å…¥æœ‰åŠ©äºæé«˜åˆ¤åˆ«å™¨çš„åˆ†ç±»èƒ½åŠ›ï¼Œè¿›è€Œæå‡ç”Ÿæˆå›¾åƒçš„ç²¾ç»†ç²’åº¦ç»†èŠ‚å‡†ç¡®æ€§ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ–¹æ³•ç”¨äºå‡å°‘ä¸åŒå­ç±»å›¾åƒé—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å¢åŠ ç›¸åŒå­ç±»å›¾åƒé—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>åœ¨å¸¸ç”¨çš„æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¾…åŠ©åˆ†ç±»å™¨åŒæ—¶å¸®åŠ©ç”Ÿæˆå™¨ç”Ÿæˆæ›´å‡†ç¡®çš„ç²¾ç»†ç²’åº¦å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ddabde15480b7c735a4b219c5e32d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7185a0f87e292154d753c68ebd946585.jpg" align="middle">
</details>




<h2 id="Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks"><a href="#Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks" class="headerlink" title="Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks"></a>Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks</h2><p><strong>Authors:Sebastian Hereu, Qianfei Hu</strong></p>
<p>Convolutional neural networks (CNNs) have been combined with generative adversarial networks (GANs) to create deep convolutional generative adversarial networks (DCGANs) with great success. DCGANs have been used for generating images and videos from creative domains such as fashion design and painting. A common critique of the use of DCGANs in creative applications is that they are limited in their ability to generate creative products because the generator simply learns to copy the training distribution. We explore an extension of DCGANs, creative adversarial networks (CANs). Using CANs, we generate novel, creative portraits, using the WikiArt dataset to train the network. Moreover, we introduce our extension of CANs, conditional creative adversarial networks (CCANs), and demonstrate their potential to generate creative portraits conditioned on a style label. We argue that generating products that are conditioned, or inspired, on a style label closely emulates real creative processes in which humans produce imaginative work that is still rooted in previous styles. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æˆåŠŸç»“åˆäº§ç”Ÿäº†æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANsï¼‰ã€‚DCGANså·²è¢«ç”¨äºä»æ—¶å°šè®¾è®¡å’Œç»˜ç”»ç­‰åˆ›æ„é¢†åŸŸç”Ÿæˆå›¾åƒå’Œè§†é¢‘ã€‚ä½¿ç”¨DCGANsåœ¨åˆ›æ„åº”ç”¨ä¸­çš„å¸¸è§æ‰¹è¯„æ˜¯ï¼Œå®ƒä»¬åœ¨ç”Ÿæˆåˆ›æ„äº§å“æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œå› ä¸ºç”Ÿæˆå™¨åªæ˜¯å­¦ä¹ å¤åˆ¶è®­ç»ƒåˆ†å¸ƒã€‚æˆ‘ä»¬æ¢ç´¢äº†DCGANsçš„æ‰©å±•ç‰ˆæœ¬â€”â€”åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCANsï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨CANsç”Ÿæˆæ–°å‹åˆ›æ„è‚–åƒï¼Œå¹¶ä½¿ç”¨WikiArtæ•°æ®é›†æ¥è®­ç»ƒç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†CANsçš„æ‰©å±•ç‰ˆæœ¬â€”â€”æ¡ä»¶åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCCANsï¼‰ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬æ ¹æ®é£æ ¼æ ‡ç­¾ç”Ÿæˆåˆ›æ„è‚–åƒçš„æ½œåŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”ŸæˆåŸºäºé£æ ¼æ ‡ç­¾æˆ–å—å…¶å¯å‘è€Œäº§ç”Ÿçš„äº§å“ç´§å¯†æ¨¡æ‹Ÿäº†çœŸæ­£çš„åˆ›æ„è¿‡ç¨‹ï¼Œäººç±»åœ¨è¿™ç§è¿‡ç¨‹ä¸­ä¼šåˆ›ä½œå‡ºæ ¹æ¤äºå…ˆå‰é£æ ¼ä½†åˆå¯Œæœ‰æƒ³è±¡åŠ›çš„ä½œå“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07091v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç»“åˆåˆ›å»ºäº†æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰ï¼Œåœ¨æ—¶å°šè®¾è®¡ç­‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚ä½†DCGANåœ¨åˆ›é€ æ€§åº”ç”¨ä¸Šè¢«æ‰¹è¯„ä¸ºå±€é™äºç”Ÿæˆè®­ç»ƒåˆ†å¸ƒçš„äº§å“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¢ç´¢äº†DCGANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCANï¼‰ï¼Œå¹¶ä½¿ç”¨WikiArtæ•°æ®é›†ç”Ÿæˆæ–°é¢–åˆ›æ„è‚–åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†CANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”æ¡ä»¶åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCCANï¼‰ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬æ ¹æ®é£æ ¼æ ‡ç­¾ç”Ÿæˆåˆ›æ„è‚–åƒçš„æ½œåŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”Ÿæˆå—é£æ ¼æ ‡ç­¾å½±å“æˆ–å¯å‘çš„äº§å“æ›´æ¥è¿‘äººç±»çš„çœŸå®åˆ›æ„è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DCGANå·²æˆåŠŸåº”ç”¨äºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œå°¤å…¶åœ¨æ—¶å°šè®¾è®¡å’Œç»˜ç”»ç­‰åˆ›æ„é¢†åŸŸã€‚</li>
<li>DCGANçš„ä¸€ä¸ªå¸¸è§æ‰¹è¯„æ˜¯å…¶ç”Ÿæˆåˆ›æ„äº§å“çš„èƒ½åŠ›å—é™ï¼Œä»…å­¦ä¹ å¤åˆ¶è®­ç»ƒåˆ†å¸ƒã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCANï¼‰ï¼Œä½¿ç”¨WikiArtæ•°æ®é›†ç”Ÿæˆæ–°é¢–åˆ›æ„è‚–åƒã€‚</li>
<li>CANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”æ¡ä»¶åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCCANï¼‰èƒ½å¤Ÿæ ¹æ®é£æ ¼æ ‡ç­¾ç”Ÿæˆåˆ›æ„è‚–åƒã€‚</li>
<li>CCANçš„æ½œåŠ›å±•ç¤ºäº†å®ƒåœ¨æ¨¡æ‹Ÿäººç±»çœŸå®åˆ›æ„è¿‡ç¨‹æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå³ç”Ÿæˆå—é£æ ¼æ ‡ç­¾å½±å“æˆ–å¯å‘çš„äº§å“ã€‚</li>
<li>CCANçš„åº”ç”¨å¯èƒ½æ¨åŠ¨åˆ›æ„é¢†åŸŸçš„å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ ¹æ®ç‰¹å®šé£æ ¼æˆ–è¦æ±‚ç”Ÿæˆå†…å®¹çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f449af10b6b724d393aa486988a72232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57fcb2b2735bf7bdf971d6bd4f6e43c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8c0d0fa86b8dc9c60493948af361fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef0866153897d19d7ed04ee03ef0461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-919534e13329a643d8d5a59a53ea98b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0460b23720fe9a0735ecd7c632e2a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89a8d2483234fcf74b6af7fc50907d24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-260ae46b1080cb4b32ea46d808541992.jpg" align="middle">
</details>




<h2 id="HiFiVFS-High-Fidelity-Video-Face-Swapping"><a href="#HiFiVFS-High-Fidelity-Video-Face-Swapping" class="headerlink" title="HiFiVFS: High Fidelity Video Face Swapping"></a>HiFiVFS: High Fidelity Video Face Swapping</h2><p><strong>Authors:Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, Chengjie Wang</strong></p>
<p>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively. </p>
<blockquote>
<p>é¢éƒ¨æ›¿æ¢æ—¨åœ¨ç”Ÿæˆç»“åˆæºèº«ä»½å’Œç›®æ ‡å±æ€§çš„ç»“æœã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„é¢éƒ¨æ›¿æ¢ä¸Šã€‚åœ¨å¤„ç†è§†é¢‘æ—¶ï¼Œæ¯ä¸€å¸§éƒ½æ˜¯ç‹¬ç«‹å¤„ç†çš„ï¼Œå¾ˆéš¾ä¿è¯æ—¶é—´ç¨³å®šæ€§ã€‚ä»æ¨¡å‹çš„è§’åº¦æ¥çœ‹ï¼Œé¢éƒ¨æ›¿æ¢æ­£é€æ¸ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå› ä¸ºDMsæ˜¾ç¤ºå‡ºæ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚å½“å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¿®å¤æŠ€æœ¯ï¼Œè¿™åœ¨ä¿ç•™å…‰ç…§å’Œå¦†å®¹ç­‰ç²¾ç»†å±æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜ä¿çœŸè§†é¢‘é¢éƒ¨æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—æ€§å­¦ä¹ æå–èº«ä»½åˆ†ç¦»å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è¯¦ç»†çš„èº«ä»½æ³¨å…¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘é¢éƒ¨æ›¿æ¢ä¸­å®ç°äº†å®šæ€§å’Œå®šé‡ä¸Šçš„ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18293v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºç¨³å®šè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆSVDï¼‰çš„é«˜ä¿çœŸè§†é¢‘äººè„¸æ›¿æ¢æ¡†æ¶è¢«æå‡ºï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒçŸ¥è¯†æ¥è§£å†³è§†é¢‘äººè„¸æ›¿æ¢ä¸­çš„ç²¾ç»†å±æ€§ä¿ç•™å’Œæ—¶åºç¨³å®šæ€§é—®é¢˜ã€‚é€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—å­¦ä¹ ï¼Œæ„å»ºäº†ç²¾ç»†å±æ€§æ¨¡å—æ¥æå–èº«ä»½åˆ†ç¦»å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è¯¦ç»†çš„èº«ä»½æ³¨å…¥æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘äººè„¸æ›¿æ¢æ–¹é¢è¾¾åˆ°äº†å®šæ€§å’Œå®šé‡ä¸Šçš„æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘äººè„¸æ›¿æ¢é¢ä¸´æ—¶åºç¨³å®šæ€§å’Œç²¾ç»†å±æ€§ä¿ç•™çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒä¸­çš„äººè„¸æ›¿æ¢ï¼Œå¤„ç†è§†é¢‘æ—¶ç‹¬ç«‹å¤„ç†æ¯ä¸€å¸§ï¼Œéš¾ä»¥ä¿è¯æ—¶åºç¨³å®šæ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ­£é€æ¸åº”ç”¨äºäººè„¸æ›¿æ¢é¢†åŸŸã€‚</li>
<li>æå‡ºçš„HiFiVFSæ¡†æ¶åˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—å­¦ä¹ æå–èº«ä»½åˆ†ç¦»å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚</li>
<li>å¼•å…¥è¯¦ç»†èº«ä»½æ³¨å…¥æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-122dfa5cb8ad5ef604f88cde5b275d81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31457317ca85d09e8d2b9d29ee530829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52192381a3ec2d89bb7e63e4b4659e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac370c44fad37f8bd34147d044b76d0a.jpg" align="middle">
</details>




<h2 id="Leveraging-generative-models-to-characterize-the-failure-conditions-of-image-classifiers"><a href="#Leveraging-generative-models-to-characterize-the-failure-conditions-of-image-classifiers" class="headerlink" title="Leveraging generative models to characterize the failure conditions of   image classifiers"></a>Leveraging generative models to characterize the failure conditions of   image classifiers</h2><p><strong>Authors:Adrien LeCoz, StÃ©phane Herbin, Faouzi Adjed</strong></p>
<p>We address in this work the question of identifying the failure conditions of a given image classifier. To do so, we exploit the capacity of producing controllable distributions of high quality image data made available by recent Generative Adversarial Networks (StyleGAN2): the failure conditions are expressed as directions of strong performance degradation in the generative model latent space. This strategy of analysis is used to discover corner cases that combine multiple sources of corruption, and to compare in more details the behavior of different classifiers. The directions of degradation can also be rendered visually by generating data for better interpretability. Some degradations such as image quality can affect all classes, whereas other ones such as shape are more class-specific. The approach is demonstrated on the MNIST dataset that has been completed by two sources of corruption: noise and blur, and shows a promising way to better understand and control the risks of exploiting Artificial Intelligence components for safety-critical applications. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†è¯†åˆ«ç»™å®šå›¾åƒåˆ†ç±»å™¨å¤±æ•ˆæ¡ä»¶çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€è¿‘å‡ºç°çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆStyleGAN2ï¼‰äº§ç”Ÿçš„é«˜è´¨é‡å›¾åƒæ•°æ®çš„å¯æ§åˆ†å¸ƒèƒ½åŠ›ï¼šå¤±æ•ˆæ¡ä»¶è¡¨ç°ä¸ºç”Ÿæˆæ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­çš„æ€§èƒ½ä¸¥é‡ä¸‹é™æ–¹å‘ã€‚è¿™ç§åˆ†æç­–ç•¥ç”¨äºå‘ç°ç»“åˆå¤šç§è…è´¥æ¥æºçš„æç«¯æƒ…å†µï¼Œå¹¶æ›´è¯¦ç»†åœ°æ¯”è¾ƒä¸åŒåˆ†ç±»å™¨çš„è¡Œä¸ºã€‚é€šè¿‡ç”Ÿæˆæ•°æ®åœ¨è§†è§‰ä¸Šå‘ˆç°é€€åŒ–æ–¹å‘ï¼Œä»¥æé«˜å…¶å¯è§£é‡Šæ€§ã€‚ä¸€äº›é€€åŒ–ï¼ˆå¦‚å›¾åƒè´¨é‡ï¼‰å¯èƒ½å½±å“æ‰€æœ‰ç±»åˆ«ï¼Œè€Œå…¶ä»–ä¸€äº›é€€åŒ–ï¼ˆå¦‚å½¢çŠ¶ï¼‰åˆ™æ›´ç‰¹å®šäºç±»åˆ«ã€‚è¯¥æ–¹æ³•åœ¨MNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¼”ç¤ºï¼Œè¯¥æ•°æ®é›†é€šè¿‡å™ªå£°å’Œæ¨¡ç³Šä¸¤ç§æ¥æºè¿›è¡Œäº†è¡¥å……ï¼Œå¹¶æ˜¾ç¤ºäº†ä¸€ç§æ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶å°†äººå·¥æ™ºèƒ½ç»„ä»¶ç”¨äºå®‰å…¨å…³é”®åº”ç”¨çš„é£é™©çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12814v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æ¢è®¨äº†é€šè¿‡åˆ©ç”¨æœ€è¿‘å‡ºç°çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆStyleGAN2ï¼‰äº§ç”Ÿçš„å¯æ§é«˜è´¨é‡å›¾åƒæ•°æ®é›†æ¥è¯†åˆ«ç»™å®šå›¾åƒåˆ†ç±»å™¨çš„å¤±è´¥æ¡ä»¶ã€‚å¤±è´¥æ¡ä»¶è¢«è¡¨è¾¾ä¸ºç”Ÿæˆæ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­æ€§èƒ½ä¸¥é‡é€€åŒ–çš„æ–¹å‘ã€‚è¿™ç§åˆ†æç­–ç•¥ç”¨äºå‘ç°ç»“åˆå¤šç§è…è´¥æ¥æºçš„æç«¯æƒ…å†µï¼Œå¹¶æ›´è¯¦ç»†åœ°æ¯”è¾ƒä¸åŒåˆ†ç±»å™¨çš„è¡Œä¸ºã€‚é€šè¿‡ç”Ÿæˆæ•°æ®å‘ˆç°é€€åŒ–æ–¹å‘ä»¥æé«˜å¯è§£é‡Šæ€§ã€‚ä¸€äº›å…¨å±€é€€åŒ–ï¼ˆå¦‚å›¾åƒè´¨é‡ï¼‰ä¼šå½±å“æ‰€æœ‰ç±»åˆ«ï¼Œè€Œå…¶ä»–ç±»åˆ«ç‰¹å®šé€€åŒ–ï¼ˆå¦‚å½¢çŠ¶ï¼‰åˆ™æ›´å…·ç±»åˆ«ç‰¹å¼‚æ€§ã€‚åœ¨MNISTæ•°æ®é›†ä¸Šæ¼”ç¤ºäº†è¯¥æ–¹æ³•ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å™ªå£°å’Œæ¨¡ç³Šä¸¤ç§æ¥æºå®Œæˆï¼Œå±•ç¤ºäº†ä¸€ç§æ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶åˆ©ç”¨äººå·¥æ™ºèƒ½ç»„ä»¶è¿›è¡Œå®‰å…¨å…³é”®åº”ç”¨é£é™©çš„æœ‰å‰é€”çš„æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨StyleGAN2ç”Ÿæˆå¯æ§çš„é«˜è´¨é‡å›¾åƒæ•°æ®é›†æ¥è¯†åˆ«å›¾åƒåˆ†ç±»å™¨çš„å¤±è´¥æ¡ä»¶ã€‚</li>
<li>å¤±è´¥æ¡ä»¶è¢«è¡¨è¾¾ä¸ºç”Ÿæˆæ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­æ€§èƒ½é€€åŒ–çš„æ–¹å‘ã€‚</li>
<li>é€šè¿‡åˆ†æç­–ç•¥å‘ç°ç»“åˆå¤šç§è…è´¥æ¥æºçš„æç«¯æƒ…å†µã€‚</li>
<li>é€€åŒ–æ–¹å‘å¯ä»¥é€šè¿‡ç”Ÿæˆæ•°æ®å¯è§†åŒ–ï¼Œä»¥æé«˜è§£é‡Šæ€§ã€‚</li>
<li>ä¸€äº›é€€åŒ–å½±å“æ‰€æœ‰ç±»åˆ«ï¼ˆå¦‚å›¾åƒè´¨é‡ï¼‰ï¼Œè€Œå…¶ä»–é€€åŒ–æ›´å…·ç±»åˆ«ç‰¹å¼‚æ€§ï¼ˆå¦‚å½¢çŠ¶ï¼‰ã€‚</li>
<li>åœ¨MNISTæ•°æ®é›†ä¸Šæ¼”ç¤ºäº†è¯¥æ–¹æ³•ï¼Œç»“åˆäº†å™ªå£°å’Œæ¨¡ç³Šä¸¤ç§æ•°æ®æ¥æºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d85af4b9068c955ba01b2437d7f09bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc9d3164f985b6a36503f08a51f6d3a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3cd3af68c93bbc93e88462d1c9c6463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0dae7878cbbd1c2946bfb1041104e37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65b53c01638382906023fde0150eed61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8777a04096c03f4dad850dca2cb890.jpg" align="middle">
</details>




<h2 id="Detecting-Unforeseen-Data-Properties-with-Diffusion-Autoencoder-Embeddings-using-Spine-MRI-data"><a href="#Detecting-Unforeseen-Data-Properties-with-Diffusion-Autoencoder-Embeddings-using-Spine-MRI-data" class="headerlink" title="Detecting Unforeseen Data Properties with Diffusion Autoencoder   Embeddings using Spine MRI data"></a>Detecting Unforeseen Data Properties with Diffusion Autoencoder   Embeddings using Spine MRI data</h2><p><strong>Authors:Robert Graf, Florian Hunecke, Soeren Pohl, Matan Atad, Hendrik Moeller, Sophie Starck, Thomas Kroencke, Stefanie Bette, Fabian Bamberg, Tobias Pischon, Thoralf Niendorf, Carsten Schmidt, Johannes C. Paetzold, Daniel Rueckert, Jan S Kirschke</strong></p>
<p>Deep learning has made significant strides in medical imaging, leveraging the use of large datasets to improve diagnostics and prognostics. However, large datasets often come with inherent errors through subject selection and acquisition. In this paper, we investigate the use of Diffusion Autoencoder (DAE) embeddings for uncovering and understanding data characteristics and biases, including biases for protected variables like sex and data abnormalities indicative of unwanted protocol variations. We use sagittal T2-weighted magnetic resonance (MR) images of the neck, chest, and lumbar region from 11186 German National Cohort (NAKO) participants. We compare DAE embeddings with existing generative models like StyleGAN and Variational Autoencoder. Evaluations on a large-scale dataset consisting of sagittal T2-weighted MR images of three spine regions show that DAE embeddings effectively separate protected variables such as sex and age. Furthermore, we used t-SNE visualization to identify unwanted variations in imaging protocols, revealing differences in head positioning. Our embedding can identify samples where a sex predictor will have issues learning the correct sex. Our findings highlight the potential of using advanced embedding techniques like DAEs to detect data quality issues and biases in medical imaging datasets. Identifying such hidden relations can enhance the reliability and fairness of deep learning models in healthcare applications, ultimately improving patient care and outcomes. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†æ¥æé«˜è¯Šæ–­å’Œé¢„åã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡æ•°æ®é›†å¾€å¾€ç”±äºå—è¯•è€…é€‰æ‹©å’Œé‡‡é›†è€Œå¸¦æœ‰å†…åœ¨é”™è¯¯ã€‚æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEï¼‰åµŒå…¥æ¥æ­ç¤ºå’Œç†è§£æ•°æ®ç‰¹å¾å’Œåè§ï¼ŒåŒ…æ‹¬æ€§åˆ«ç­‰ä¿æŠ¤å˜é‡çš„åè§ä»¥åŠæŒ‡ç¤ºä¸éœ€è¦çš„åè®®å˜åŒ–çš„å¼‚å¸¸æ•°æ®ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªå¾·å›½å›½å®¶é˜Ÿåˆ—ç ”ç©¶ï¼ˆNAKOï¼‰çš„å‚ä¸è€…é¢ˆéƒ¨ã€èƒ¸éƒ¨å’Œè…°æ¤åŒºåŸŸçš„çŸ¢çŠ¶é¢T2åŠ æƒç£å…±æŒ¯ï¼ˆMRï¼‰å›¾åƒã€‚æˆ‘ä»¬å°†DAEåµŒå…¥ä¸ç°æœ‰çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚StyleGANå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚å¯¹åŒ…å«ä¸‰ä¸ªè„Šæ¤åŒºåŸŸçŸ¢çŠ¶é¢T2åŠ æƒMRå›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDAEåµŒå…¥å¯ä»¥æœ‰æ•ˆåœ°åˆ†ç¦»ä¿æŠ¤å˜é‡ï¼Œå¦‚æ€§åˆ«å’Œå¹´é¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨t-SNEå¯è§†åŒ–æ¥è¯†åˆ«æˆåƒåè®®ä¸­çš„ä¸éœ€è¦çš„å˜åŒ–ï¼Œæ­ç¤ºå¤´éƒ¨å®šä½çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„åµŒå…¥å¯ä»¥è¯†åˆ«æ€§åˆ«é¢„æµ‹å™¨åœ¨å­¦ä¹ æ­£ç¡®æ€§åˆ«æ—¶ä¼šé‡åˆ°é—®é¢˜çš„ä¸€äº›æ ·æœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä½¿ç”¨å…ˆè¿›çš„åµŒå…¥æŠ€æœ¯ï¼ˆå¦‚DAEsï¼‰æ£€æµ‹åŒ»å­¦æˆåƒæ•°æ®é›†ä¸­çš„æ•°æ®è´¨é‡é—®é¢˜å’Œåè§çš„æ½œåŠ›ã€‚è¯†åˆ«è¿™äº›éšè—å…³ç³»å¯ä»¥å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­çš„å¯é æ€§å’Œå…¬å¹³æ€§ï¼Œæœ€ç»ˆæ”¹å–„æ‚£è€…æŠ¤ç†å’Œç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10220v1">PDF</a> This paper was accepted in the â€œWorkshop on Interpretability of   Machine Intelligence in Medical Image Computingâ€ (iMIMIC) at MICCAI 2024</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEï¼‰åµŒå…¥æŠ€æœ¯ï¼Œæ¢ç©¶åœ¨åŒ»å­¦æˆåƒä¸­æ­ç¤ºå’Œç†è§£æ•°æ®ç‰¹æ€§å’Œåè§çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹ä¿æŠ¤å˜é‡ï¼ˆå¦‚æ€§åˆ«ï¼‰å’Œæ•°æ®å¼‚å¸¸å€¼çš„è¯†åˆ«ã€‚é€šè¿‡å¯¹æ¯”StyleGANå’Œå˜åˆ†è‡ªç¼–ç å™¨ç­‰ç°æœ‰ç”Ÿæˆæ¨¡å‹ï¼ŒDAEåµŒå…¥åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œèƒ½æœ‰æ•ˆåŒºåˆ†ä¿æŠ¤å˜é‡ï¼Œå¦‚æ€§åˆ«å’Œå¹´é¾„ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ©ç”¨t-SNEå¯è§†åŒ–æŠ€æœ¯è¯†åˆ«æˆåƒåè®®ä¸­çš„æ„å¤–å˜åŒ–ï¼Œæ­ç¤ºäº†å¤´éƒ¨å®šä½çš„å·®å¼‚ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†ä½¿ç”¨é«˜çº§åµŒå…¥æŠ€æœ¯å¦‚DAEsæ£€æµ‹åŒ»å­¦æˆåƒæ•°æ®é›†æ•°æ®è´¨é‡é—®é¢˜å’Œåè§çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„å¯é æ€§å’Œå…¬å¹³æ€§ï¼Œæœ€ç»ˆæ”¹å–„æ‚£è€…æŠ¤ç†å’Œç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEï¼‰åµŒå…¥æŠ€æœ¯ç”¨äºåŒ»å­¦æˆåƒæ•°æ®çš„åˆ†æå’Œç†è§£ã€‚</li>
<li>DAEåµŒå…¥èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¿æŠ¤å˜é‡ï¼Œå¦‚æ€§åˆ«å’Œå¹´é¾„ã€‚</li>
<li>ä¸ç°æœ‰ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚StyleGANå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼‰ç›¸æ¯”ï¼ŒDAEåµŒå…¥åœ¨å¤§å‹åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨t-SNEå¯è§†åŒ–æŠ€æœ¯æ­ç¤ºäº†æˆåƒåè®®ä¸­çš„æ„å¤–å˜åŒ–ã€‚</li>
<li>DAEåµŒå…¥èƒ½å¤Ÿè¯†åˆ«æ•°æ®è´¨é‡é—®é¢˜å’Œåè§ï¼Œæœ‰åŠ©äºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„å¯é æ€§å’Œå…¬å¹³æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹äºæé«˜æ‚£è€…æŠ¤ç†å’Œç»“æœå…·æœ‰æ½œåœ¨æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25cf2ec6902b5245f5875b8203f7e3cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c68accc95789b495887d32e311de1da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6ad2e5dfc74a642062244006a2851a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-169cc315f5012b4372cdbf3507a2c439.jpg" align="middle">
</details>




<h2 id="Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks"><a href="#Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks" class="headerlink" title="Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks"></a>Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks</h2><p><strong>Authors:Srinitish Srinivasan, Varenya Pathak, Abirami S</strong></p>
<p>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem. </p>
<blockquote>
<p>æ‘˜è¦è‰ºæœ¯æ˜¯ä¸€ç§éå¸¸æµè¡Œä¸”å¤‡å—è®¨è®ºçš„è‰ºæœ¯å½¢å¼ï¼Œé€šå¸¸èƒ½å¤Ÿæç»˜å‡ºè‰ºæœ¯å®¶çš„æƒ…æ„Ÿã€‚è®¸å¤šç ”ç©¶äººå‘˜å·²ç»å°è¯•ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ è¿›è¡Œè¾¹ç¼˜æ£€æµ‹ã€ç¬”è§¦å’Œæƒ…æ„Ÿè¯†åˆ«ç®—æ³•æ¥ç ”ç©¶æŠ½è±¡è‰ºæœ¯ã€‚æœ¬æ–‡æè¿°äº†ä¸€ç§ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œï¼ˆGANï¼‰ç ”ç©¶å¹¿æ³›åˆ†å¸ƒçš„æŠ½è±¡ç»˜ç”»çš„ç ”ç©¶ã€‚GANå…·æœ‰å­¦ä¹ å’Œå¤åˆ¶åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œä½¿ç ”ç©¶è€…å’Œç§‘å­¦å®¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢å’Œç ”ç©¶ç”Ÿæˆçš„å›¾åƒç©ºé—´ã€‚ç„¶è€Œï¼ŒæŒ‘æˆ˜åœ¨äºå¼€å‘ä¸€ç§æœ‰æ•ˆçš„GANæ¶æ„ï¼Œèƒ½å¤Ÿå…‹æœå¸¸è§çš„è®­ç»ƒé™·é˜±ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ç§é’ˆå¯¹é«˜è´¨é‡è‰ºæœ¯ä½œå“ç”Ÿæˆçš„æ”¹è¿›å‹DCGANï¼ˆmDCGANï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•æ¶‰åŠå¯¹æ‰€åšçš„ä¿®æ”¹çš„å½»åº•æ¢ç´¢ï¼Œæ·±å…¥ç ”ç©¶DCGANçš„ç²¾ç»†å·¥ä½œåŸç†ã€ä¼˜åŒ–æŠ€æœ¯å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‰ºæœ¯ç”Ÿæˆçš„ç¨³å®šæ€§å’ŒçœŸå®æ€§ï¼Œä»è€Œæœ‰æ•ˆç ”ç©¶ç”Ÿæˆçš„å›¾æ¡ˆã€‚æ‰€æå‡ºçš„mDCGANå¯¹å±‚é…ç½®å’Œæ¶æ„é€‰æ‹©è¿›è¡Œäº†ç²¾ç»†è°ƒæ•´ï¼Œä¸ºè‰ºæœ¯ç”Ÿæˆæä¾›äº†é‡èº«å®šåˆ¶çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æœ‰æ•ˆåœ°è§£å†³äº†æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡æ‰§è¡Œéšæœºæ¸¸èµ°æ¢ç´¢ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œäº†è§£æŠ½è±¡è‰ºæœ¯ç©ºé—´ä¸­ç¬”è§¦å’Œé¢œè‰²ä¹‹é—´çš„å‘é‡å…³ç³»ï¼Œå¹¶åˆ†æäº†GANè®­ç»ƒæŸæ®µæ—¶é—´åä¸ç¨³å®šè¾“å‡ºçš„ç»Ÿè®¡æ•°æ®åŠå…¶æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å‘ç°éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒå…¶åœ¨æ•°å­—è‰ºæœ¯ç”Ÿæˆå’Œæ•°å­—è‰ºæœ¯ç”Ÿæ€ç³»ç»Ÿé¢†åŸŸä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18397v2">PDF</a> Accepted for publication by Intelligent Decision Technologies</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œï¼ˆGANï¼‰å¯¹æŠ½è±¡ç»˜ç”»è¿›è¡Œå¹¿æ³›åˆ†å¸ƒçš„ç ”ç©¶ã€‚GANså…·æœ‰å­¦ä¹ å’Œå¤åˆ¶åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œä½¿ç ”ç©¶è€…å’Œç§‘å­¦å®¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢å’Œç ”ç©¶ç”Ÿæˆçš„å›¾åƒç©ºé—´ã€‚ç ”ç©¶é’ˆå¯¹å¼€å‘é«˜æ•ˆGANæ¶æ„ä»¥å…‹æœå¸¸è§è®­ç»ƒéš¾é¢˜çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„DCGANï¼ˆmDCGANï¼‰ï¼Œä¸“é—¨ç”¨äºé«˜è´¨é‡è‰ºæœ¯ä½œå“ç”Ÿæˆã€‚è¯¥æ–¹æ³•æ·±å…¥æ¢è®¨äº†æ‰€åšçš„ä¿®æ”¹ï¼Œæ¢è®¨äº†DCGANsçš„ç²¾ç»†å·¥ä½œåŸç†ã€ä¼˜åŒ–æŠ€æœ¯å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‰ºæœ¯ç”Ÿæˆçš„ç¨³å®šæ€§å’Œé€¼çœŸæ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°ç ”ç©¶ç”Ÿæˆçš„å›¾æ¡ˆã€‚mDCGANåœ¨å±‚é…ç½®å’Œæ¶æ„é€‰æ‹©æ–¹é¢è¿›è¡Œäº†ç²¾ç»†è°ƒæ•´ï¼Œä¸ºè‰ºæœ¯ç”Ÿæˆçš„ç‹¬ç‰¹éœ€æ±‚æä¾›é‡èº«å®šåˆ¶çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æœ‰æ•ˆè§£å†³æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡éšæœºæ¸¸èµ°çš„æ–¹å¼æ¢ç´¢äº†ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œç†è§£æŠ½è±¡è‰ºæœ¯ç©ºé—´ä¸­ç¬”è§¦å’Œé¢œè‰²ä¹‹é—´çš„å‘é‡å…³ç³»ï¼Œå¹¶å¯¹GANè®­ç»ƒæŸä¸€æ®µæ—¶é—´åä¸ç¨³å®šè¾“å‡ºçš„ç»Ÿè®¡ç»“æœè¿›è¡Œåˆ†æå’Œæ¯”è¾ƒï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨æ•°å­—è‰ºæœ¯ç”Ÿæˆå’Œæ•°å­—è‰ºæœ¯ç”Ÿæ€ç³»ç»Ÿé¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œï¼ˆGANï¼‰ç ”ç©¶æŠ½è±¡ç»˜ç”»çš„åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†æ”¹è¿›çš„DCGANï¼ˆmDCGANï¼‰æ¶æ„ï¼Œç”¨äºé«˜è´¨é‡çš„è‰ºæœ¯ä½œå“ç”Ÿæˆã€‚</li>
<li>mDCGANåœ¨å±‚é…ç½®å’Œæ¶æ„é€‰æ‹©ä¸Šçš„ç²¾ç»†è°ƒæ•´ï¼Œæ»¡è¶³äº†è‰ºæœ¯ç”Ÿæˆçš„ç‹¬ç‰¹éœ€æ±‚ã€‚</li>
<li>mDCGANè§£å†³äº†æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ï¼Œæé«˜äº†è‰ºæœ¯ç”Ÿæˆçš„ç¨³å®šæ€§å’Œé€¼çœŸæ€§ã€‚</li>
<li>é€šè¿‡éšæœºæ¸¸èµ°æ¢ç´¢ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œç†è§£æŠ½è±¡è‰ºæœ¯ç©ºé—´ä¸­ç¬”è§¦å’Œé¢œè‰²ä¹‹é—´çš„å‘é‡å…³ç³»ã€‚</li>
<li>å¯¹GANè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šè¾“å‡ºè¿›è¡Œç»Ÿè®¡åˆ†æå’Œæ¯”è¾ƒã€‚</li>
<li>éªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒäº†å…¶åœ¨æ•°å­—è‰ºæœ¯ç”Ÿæˆå’Œæ•°å­—è‰ºæœ¯ç”Ÿæ€ç³»ç»Ÿé¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c86fb3b63917812a573d399ded69ef3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-785ec5270ebb6ad154707406e7a5ac8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64858c227d4fe7e862156750ec3605f9.jpg" align="middle">
</details>




<h2 id="Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training"><a href="#Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training" class="headerlink" title="Representation Learning of Multivariate Time Series using Attention and   Adversarial Training"></a>Representation Learning of Multivariate Time Series using Attention and   Adversarial Training</h2><p><strong>Authors:Leon ScharwÃ¤chter, Sebastian Otte</strong></p>
<p>A critical factor in trustworthy machine learning is to develop robust representations of the training data. Only under this guarantee methods are legitimate to artificially generate data, for example, to counteract imbalanced datasets or provide counterfactual explanations for blackbox decision-making systems. In recent years, Generative Adversarial Networks (GANs) have shown considerable results in forming stable representations and generating realistic data. While many applications focus on generating image data, less effort has been made in generating time series data, especially multivariate signals. In this work, a Transformer-based autoencoder is proposed that is regularized using an adversarial training scheme to generate artificial multivariate time series signals. The representation is evaluated using t-SNE visualizations, Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the generated signals exhibit higher similarity to an exemplary dataset than using a convolutional network approach. </p>
<blockquote>
<p>åœ¨å¯ä¿¡æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®å› ç´ æ˜¯å¼€å‘ç¨³å¥çš„è®­ç»ƒæ•°æ®è¡¨ç¤ºã€‚åªæœ‰åœ¨è¿™ä¸ªä¿è¯ä¸‹ï¼Œäººå·¥ç”Ÿæˆæ•°æ®çš„æ–¹æ³•æ‰æ˜¯åˆæ³•çš„ï¼Œä¾‹å¦‚ï¼Œå¯¹æŠ—ä¸å¹³è¡¡æ•°æ®é›†æˆ–ä¸ºé»‘ç®±å†³ç­–åˆ¶å®šç³»ç»Ÿæä¾›åäº‹å®è§£é‡Šã€‚è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨å½¢æˆç¨³å®šè¡¨ç¤ºå’Œç”ŸæˆçœŸå®æ•°æ®æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚è™½ç„¶è®¸å¤šåº”ç”¨éƒ½é›†ä¸­åœ¨ç”Ÿæˆå›¾åƒæ•°æ®ä¸Šï¼Œä½†åœ¨ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®æ–¹é¢æ‰€ä»˜å‡ºçš„åŠªåŠ›è¾ƒå°‘ï¼Œå°¤å…¶æ˜¯å¤šå…ƒä¿¡å·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªç¼–ç å™¨ï¼Œé€šè¿‡ä½¿ç”¨å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆè¿›è¡Œæ­£åˆ™åŒ–æ¥ç”Ÿæˆäººå·¥å¤šå…ƒæ—¶é—´åºåˆ—ä¿¡å·ã€‚é€šè¿‡t-SNEå¯è§†åŒ–ã€åŠ¨æ€æ—¶é—´å¼¯æ›²ï¼ˆDTWï¼‰å’Œç†µåˆ†æ•°æ¥è¯„ä¼°è¡¨ç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„ä¿¡å·ä¸æ ·æœ¬æ•°æ®é›†ç›¸æ¯”ï¼Œä½¿ç”¨å·ç§¯ç½‘ç»œæ–¹æ³•å…·æœ‰æ›´é«˜çš„ç›¸ä¼¼æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01987v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒæœºå™¨å­¦ä¹ ä¸­çš„ç¨³å¥è¡¨ç¤ºæ³•çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”Ÿæˆçš„æ•°æ®è€Œè¨€ã€‚é€šè¿‡åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æŠ€æœ¯ï¼Œå¯ä»¥å½¢æˆç¨³å®šçš„æ•°æ®è¡¨ç¤ºå¹¶ç”ŸæˆçœŸå®çš„æ•°æ®ã€‚å°½ç®¡å›¾åƒæ•°æ®ç”Ÿæˆåº”ç”¨å¹¿æ³›ï¼Œä½†åœ¨ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®æ–¹é¢ï¼Œå°¤å…¶æ˜¯å¤šå…ƒä¿¡å·ç”Ÿæˆæ–¹é¢çš„åŠªåŠ›è¾ƒå°‘ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªç¼–ç å™¨ï¼Œé‡‡ç”¨å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥ç”Ÿæˆäººå·¥å¤šå…ƒæ—¶é—´åºåˆ—ä¿¡å·ã€‚é€šè¿‡t-SNEå¯è§†åŒ–ã€åŠ¨æ€æ—¶é—´å¼¯æ›²ï¼ˆDTWï¼‰å’Œç†µè¯„åˆ†å¯¹è¡¨ç¤ºè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ç”Ÿæˆçš„ä¿¡å·ä¸æ ·æœ¬æ•°æ®é›†æœ‰è¾ƒé«˜çš„ç›¸ä¼¼æ€§ï¼Œä¼˜äºå·ç§¯ç½‘ç»œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ ä¸­çš„ç¨³å¥è¡¨ç¤ºæ³•å¯¹äºç”Ÿæˆæ•°æ®è‡³å…³é‡è¦ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å¯ç”¨äºå½¢æˆç¨³å®šçš„æ•°æ®è¡¨ç¤ºå¹¶ç”ŸæˆçœŸå®æ•°æ®ã€‚</li>
<li>å°½ç®¡å›¾åƒæ•°æ®ç”Ÿæˆåº”ç”¨å¹¿æ³›ï¼Œä½†æ—¶é—´åºåˆ—æ•°æ®çš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯å¤šå…ƒä¿¡å·çš„ç”Ÿæˆå—åˆ°çš„å…³æ³¨è¾ƒå°‘ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªç¼–ç å™¨ï¼Œé‡‡ç”¨å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥ç”Ÿæˆå¤šå…ƒæ—¶é—´åºåˆ—ä¿¡å·ã€‚</li>
<li>é€šè¿‡å¤šç§è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬t-SNEå¯è§†åŒ–ã€åŠ¨æ€æ—¶é—´å¼¯æ›²ï¼ˆDTWï¼‰å’Œç†µè¯„åˆ†ï¼ŒéªŒè¯äº†ç”Ÿæˆçš„ä¿¡å·çš„çœŸå®æ€§åŠä¸æ ·æœ¬æ•°æ®çš„é«˜ç›¸ä¼¼æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºä½¿ç”¨å·ç§¯ç½‘ç»œçš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b941c35f07b9443c2d8562fcd05075d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0feafb45ee0492776796259b551baad5.jpg" align="middle">
</details>




<h2 id="GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks"><a href="#GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks" class="headerlink" title="GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks"></a>GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks</h2><p><strong>Authors:Mehran Hosseini, Peyman Hosseini</strong></p>
<p>The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative n-dimensional Cartesian coordinate system. We show this drastically improves quality of images generated by Diffusion Models, GANs, and Variational AutoEncoders (VAE). </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆæ¨¡å‹é•¿æœŸæ— æ³•é‡ç°å¤æ‚å‡ ä½•ç‰¹å¾ï¼Œå¦‚äººç±»çš„æ‰‹å’Œæ‰‹æŒ‡ç­‰ç‰¹å¾ï¼Œè¿™ä¸€é—®é¢˜åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå·²æŒç»­è¿‘åå¹´ã€‚å°½ç®¡é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡å’Œå¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†ï¼Œå·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†è¿™ä¸€é—®é¢˜åœ¨åŒ…æ‹¬é™å™ªæ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨å†…çš„æ‰€æœ‰æ¨¡å‹ä¸­ä¾ç„¶æ™®éå­˜åœ¨ï¼Œè¿™è¡¨æ˜ç°æœ‰æ¶æ„å­˜åœ¨æ ¹æœ¬æ€§ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¢å¼ºå·ç§¯å±‚çš„å‡ ä½•èƒ½åŠ›æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå…·ä½“åšæ³•æ˜¯ä¸ºå…¶æä¾›ä¸€ä¸ªåŒ…å«ç›¸å¯¹nç»´ç¬›å¡å°”åæ ‡ç³»ç»Ÿçš„å•ä¸€è¾“å…¥é€šé“ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™å¯ä»¥æå¤§æ”¹å–„æ‰©æ•£æ¨¡å‹ã€GANå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01951v2">PDF</a> Accepted at WACV 2025. Contains 19 pages, 15 figures, and 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨é‡å»ºå¤æ‚å‡ ä½•ç‰¹å¾ï¼ˆå¦‚äººæ‰‹å’Œæ‰‹æŒ‡ï¼‰æ–¹é¢çš„æŒç»­éš¾é¢˜ã€‚å°½ç®¡é€šè¿‡å¢å¤§æ¨¡å‹è§„æ¨¡å’Œå¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†è¿™ä¸€é—®é¢˜åœ¨åŒ…æ‹¬å»å™ªæ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨å†…çš„æ‰€æœ‰æ¨¡å‹ä¸­ä»ç„¶æ™®éå­˜åœ¨ï¼Œè¿™è¡¨æ˜ç°æœ‰æ¶æ„å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ã€‚æœ¬æ–‡æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡ä¸ºå·ç§¯å±‚æä¾›åŒ…å«ç›¸å¯¹nç»´ç¬›å¡å°”åæ ‡ç³»ç»Ÿçš„å•ä¸€è¾“å…¥é€šé“æ¥å¢å¼ºå…¶å‡ ä½•èƒ½åŠ›ï¼Œä»è€Œç¼“è§£è¿™ä¸€é—®é¢˜ã€‚æ­¤æ–¹æ³•æ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹ã€GANå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨é‡å»ºå¤æ‚å‡ ä½•ç‰¹å¾æ–¹é¢å­˜åœ¨æŒç»­éš¾é¢˜ï¼Œå°¤å…¶æ˜¯äººæ‰‹å’Œæ‰‹æŒ‡çš„ç»†èŠ‚ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹ã€GANå’ŒVAEï¼Œéƒ½é¢ä¸´è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é—®é¢˜æŒ‡å‘ç°æœ‰æ¨¡å‹æ¶æ„çš„æ ¹æœ¬æ€§çŸ­æ¿ã€‚</li>
<li>é€šè¿‡ä¸ºå·ç§¯å±‚æä¾›åŒ…å«nç»´ç¬›å¡å°”åæ ‡ç³»ç»Ÿçš„å•ä¸€è¾“å…¥é€šé“ï¼Œå¯ä»¥å¢å¼ºå…¶å‡ ä½•èƒ½åŠ›ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºæ”¹å–„æ‰©æ•£æ¨¡å‹ã€GANå’ŒVAEç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥ç­–ç•¥ä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a0e59b4dbd04d0d8521d495c872ebae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38274ed949f889a70e7b31e7e6019427.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8daa63128009486f6689a0da6a5320cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-115a2a4082a9d7e1fef8a75282c4c1be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd81a303d0434e5c85c0015a53779dc1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27aad86691678ae5aeaeb9b61eeb1637.jpg" align="middle">
</details>




<h2 id="HuRef-HUman-REadable-Fingerprint-for-Large-Language-Models"><a href="#HuRef-HUman-REadable-Fingerprint-for-Large-Language-Models" class="headerlink" title="HuRef: HUman-REadable Fingerprint for Large Language Models"></a>HuRef: HUman-REadable Fingerprint for Large Language Models</h2><p><strong>Authors:Boyi Zeng, Lizheng Wang, Yuncong Hu, Yi Xu, Chenghu Zhou, Xinbing Wang, Yu Yu, Zhouhan Lin</strong></p>
<p>Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without interfering with training or exposing model parameters to the public. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, with negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parametersâ€™ direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLMâ€™s base model. Due to the potential risk of information leakage, we cannot publish invariant terms directly. Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image. In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners. To ensure the published fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP). Experimental results across various LLMs demonstrate the effectiveness of our method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LUMIA-Group/HuRef">https://github.com/LUMIA-Group/HuRef</a>. </p>
<blockquote>
<p>ä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰ˆæƒå¯¹äºå…¶èµ„æºå¯†é›†å‹è®­ç»ƒå’Œç²¾å¿ƒè®¾è®¡çš„è®¸å¯è¯å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ½œåœ¨å‚æ•°æ”¹åŠ¨ï¼Œç¡®å®šLLMçš„åŸå§‹åŸºç¡€æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HuRefï¼Œè¿™æ˜¯ä¸€ç§äººç±»å¯è¯»çš„LLMæŒ‡çº¹ï¼Œå¯ä»¥å”¯ä¸€åœ°è¯†åˆ«åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸ä¼šå¹²æ‰°è®­ç»ƒæˆ–å‘å…¬ä¼—æš´éœ²æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œåœ¨é¢„è®­ç»ƒæ”¶æ•›åï¼ŒLLMå‚æ•°çš„å‘é‡æ–¹å‘ä¿æŒç¨³å®šï¼Œåç»­è®­ç»ƒæ­¥éª¤ï¼ˆåŒ…æ‹¬ç»§ç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠRLHFï¼‰å¼•èµ·çš„æ‰°åŠ¨å¾®ä¹å…¶å¾®ï¼Œè¿™è¶³ä»¥æˆä¸ºè¯†åˆ«åŸºç¡€æ¨¡å‹çš„å……åˆ†æ¡ä»¶ã€‚è¿™ä¸€å¿…è¦æ€§çš„éªŒè¯æ˜¯é€šè¿‡ç»§ç»­è®­ç»ƒLLMå¹¶æ·»åŠ é¢å¤–é¡¹æ¥é©±ä½¿æ¨¡å‹å‚æ•°çš„æ–¹å‘å˜åŒ–ï¼Œå¯¼è‡´æ¨¡å‹å—æŸã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ–¹å‘å®¹æ˜“å—åˆ°ç®€å•çš„æ”»å‡»ï¼Œå¦‚ç»´åº¦ç½®æ¢æˆ–çŸ©é˜µæ—‹è½¬ï¼Œè¿™äº›æ”»å‡»ä¼šæ˜¾è‘—æ”¹å˜æ–¹å‘è€Œä¸ä¼šå½±å“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨Transformerç»“æ„ï¼Œç³»ç»Ÿåœ°åˆ†æäº†æ½œåœ¨æ”»å‡»å¹¶å®šä¹‰äº†ä¸‰ä¸ªä¸å˜æœ¯è¯­æ¥è¯†åˆ«LLMçš„åŸºç¡€æ¨¡å‹ã€‚ç”±äºæ½œåœ¨çš„ä¿¡æ¯æ³„éœ²é£é™©ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥å‘å¸ƒä¸å˜æœ¯è¯­ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†å…¶æ˜ å°„åˆ°é«˜æ–¯å‘é‡å¹¶ä½¿ç”¨ç¼–ç å™¨ï¼Œç„¶åä½¿ç”¨StyleGAN2å°†å…¶è½¬æ¢ä¸ºè‡ªç„¶å›¾åƒå¹¶æœ€ç»ˆå‘å¸ƒã€‚åœ¨æˆ‘ä»¬çš„é»‘ç®±è®¾ç½®ä¸­ï¼Œæ‰€æœ‰æŒ‡çº¹æ‰“å°æ­¥éª¤å‡ç”±LLMæ‰€æœ‰è€…å†…éƒ¨è¿›è¡Œã€‚ä¸ºäº†ç¡®ä¿å‘å¸ƒçš„æŒ‡çº¹æ˜¯è¯šå®ç”Ÿæˆçš„ï¼Œæˆ‘ä»¬å¼•å…¥äº†é›¶çŸ¥è¯†è¯æ˜ï¼ˆZKPï¼‰ã€‚åœ¨ä¸åŒLLMä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LUMIA-Group/HuRef%E3%80%82">https://github.com/LUMIA-Group/HuRefã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.04828v4">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹ç‰ˆæƒä¿æŠ¤æ–¹æ³•â€”â€”HuRefã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆäººç±»å¯è¯»çš„æŒ‡çº¹æ¥å”¯ä¸€è¯†åˆ«LLMçš„åŸºç¡€æ¨¡å‹ï¼Œä¸”ä¸ä¼šå¹²æ‰°è®­ç»ƒæˆ–å…¬å¼€æ¨¡å‹å‚æ•°ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°LLMå‚æ•°å‘é‡æ–¹å‘åœ¨é¢„è®­ç»ƒæ”¶æ•›åä¿æŒç¨³å®šï¼Œå› æ­¤å¯ä½œä¸ºè¯†åˆ«åŸºç¡€æ¨¡å‹çš„å……åˆ†æ¡ä»¶ã€‚ä¸ºåº”å¯¹æ½œåœ¨æ”»å‡»ï¼Œç ”ç©¶åˆ©ç”¨Transformerç»“æ„å®šä¹‰äº†ä¸‰ä¸ªä¸å˜æœ¯è¯­ï¼Œå¹¶é€šè¿‡ç¼–ç å™¨å°†å…¶æ˜ å°„åˆ°é«˜æ–¯å‘é‡ï¼Œå†è½¬æ¢æˆè‡ªç„¶å›¾åƒè¿›è¡Œå‘å¸ƒã€‚åŒæ—¶å¼•å…¥é›¶çŸ¥è¯†è¯æ˜ï¼ˆZKPï¼‰ç¡®ä¿å‘å¸ƒçš„æŒ‡çº¹çœŸå®æœ‰æ•ˆã€‚è¯¥æ–¹æ³•åœ¨å¤šç§LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰ˆæƒä¿æŠ¤è‡³å…³é‡è¦ï¼Œå› å…¶åœ¨èµ„æºå¯†é›†å‹çš„è®­ç»ƒå’Œç²¾å¿ƒè®¾è®¡çš„è®¸å¯ä¸‹ã€‚</li>
<li>HuRefä½œä¸ºä¸€ç§æ–°å‹ç‰ˆæƒä¿æŠ¤æ–¹æ³•ï¼Œèƒ½å¤Ÿå”¯ä¸€è¯†åˆ«LLMçš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>LLMå‚æ•°å‘é‡æ–¹å‘åœ¨é¢„è®­ç»ƒæ”¶æ•›åçš„ç¨³å®šæ€§ä¸ºè¯†åˆ«åŸºç¡€æ¨¡å‹æä¾›äº†ä¾æ®ã€‚</li>
<li>HuRefé€šè¿‡ç”Ÿæˆäººç±»å¯è¯»çš„æŒ‡çº¹ï¼Œæ—¢ä¸ä¼šå¹²æ‰°è®­ç»ƒï¼Œä¹Ÿä¸ä¼šå…¬å¼€æ¨¡å‹å‚æ•°ã€‚</li>
<li>ä¸ºåº”å¯¹æ½œåœ¨æ”»å‡»ï¼Œç ”ç©¶åˆ©ç”¨Transformerç»“æ„å®šä¹‰äº†ä¸‰ä¸ªä¸å˜æœ¯è¯­è¿›è¡Œè¯†åˆ«ã€‚</li>
<li>å‘å¸ƒæŒ‡çº¹æ—¶ï¼Œåˆ©ç”¨ç¼–ç å™¨å°†å…¶æ˜ å°„åˆ°é«˜æ–¯å‘é‡ï¼Œå†è½¬æ¢æˆè‡ªç„¶å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-793f284f974f5ec9502278f916a94f57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db507bbc24e35859401626f8caa35d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7a01743996810baf11b9032598bdb62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-625ba6b6d8ddb291215275637e732372.jpg" align="middle">
</details>




<h2 id="Feature-Extraction-for-Generative-Medical-Imaging-Evaluation-New-Evidence-Against-an-Evolving-Trend"><a href="#Feature-Extraction-for-Generative-Medical-Imaging-Evaluation-New-Evidence-Against-an-Evolving-Trend" class="headerlink" title="Feature Extraction for Generative Medical Imaging Evaluation: New   Evidence Against an Evolving Trend"></a>Feature Extraction for Generative Medical Imaging Evaluation: New   Evidence Against an Evolving Trend</h2><p><strong>Authors:McKell Woodland, Austin Castelo, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</strong></p>
<p>Fr&#39;echet Inception Distance (FID) is a widely used metric for assessing synthetic image quality. It relies on an ImageNet-based feature extractor, making its applicability to medical imaging unclear. A recent trend is to adapt FID to medical imaging through feature extractors trained on medical images. Our study challenges this practice by demonstrating that ImageNet-based extractors are more consistent and aligned with human judgment than their RadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across four medical imaging modalities and four data augmentation techniques with Fr&#39;echet distances (FDs) computed using eleven ImageNet or RadImageNet-trained feature extractors. Comparison with human judgment via visual Turing tests revealed that ImageNet-based extractors produced rankings consistent with human judgment, with the FD derived from the ImageNet-trained SwAV extractor significantly correlating with expert evaluations. In contrast, RadImageNet-based rankings were volatile and inconsistent with human judgment. Our findings challenge prevailing assumptions, providing novel evidence that medical image-trained feature extractors do not inherently improve FDs and can even compromise their reliability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/fid-med-eval">https://github.com/mckellwoodland/fid-med-eval</a>. </p>
<blockquote>
<p>FrÃ©chet Inception Distanceï¼ˆFIDï¼‰æ˜¯è¯„ä¼°åˆæˆå›¾åƒè´¨é‡çš„ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡ã€‚å®ƒä¾èµ–äºåŸºäºImageNetçš„ç‰¹å¾æå–å™¨ï¼Œè¿™ä½¿å¾—å…¶åœ¨åŒ»å­¦å½±åƒä¸­çš„åº”ç”¨ä¸æ˜ç¡®ã€‚æœ€è¿‘çš„è¶‹åŠ¿æ˜¯é€šè¿‡åœ¨åŒ»å­¦å½±åƒä¸Šè®­ç»ƒçš„ç‰¹å¾æå–å™¨æ¥é€‚åº”FIDã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡è¯æ˜åŸºäºImageNetçš„æå–å™¨æ¯”RadImageNetå¯¹åº”çš„æå–å™¨åœ¨åŒ»å­¦æˆåƒä¸­çš„è¡¨ç°æ›´ä¸ºä¸€è‡´ä¸”ä¸äººç±»åˆ¤æ–­æ›´å»åˆï¼ŒæŒ‘æˆ˜äº†è¿™ä¸€åšæ³•ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªåŒ»å­¦æˆåƒæ¨¡æ€å’Œå››ç§æ•°æ®å¢å¼ºæŠ€æœ¯çš„åå…­ä¸ªStyleGAN2ç½‘ç»œï¼Œä½¿ç”¨åŸºäºåä¸€ä¸ªImageNetæˆ–RadImageNetè®­ç»ƒçš„ç‰¹å¾æå–å™¨è®¡ç®—FrÃ©chetè·ç¦»ï¼ˆFDsï¼‰ã€‚é€šè¿‡è§†è§‰å›¾çµæµ‹è¯•ä¸äººçš„åˆ¤æ–­è¿›è¡Œæ¯”è¾ƒæ˜¾ç¤ºï¼ŒåŸºäºImageNetçš„æå–å™¨äº§ç”Ÿçš„æ’åä¸äººçš„åˆ¤æ–­ä¸€è‡´ï¼Œä»ImageNetè®­ç»ƒçš„SwAVæå–å™¨å¾—å‡ºçš„FDä¸ä¸“å®¶è¯„ä»·æœ‰ç€æ˜¾è‘—çš„ç›¸å…³æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºRadImageNetçš„æ’åæ³¢åŠ¨æ€§è¾ƒå¤§ä¸”ä¸äººç±»åˆ¤æ–­ä¸ä¸€è‡´ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†ç°æœ‰çš„å‡è®¾ï¼Œæä¾›äº†æ–°çš„è¯æ®è¡¨æ˜ç»è¿‡åŒ»å­¦å›¾åƒè®­ç»ƒçš„ç‰¹å¾æå–å™¨å¹¶ä¸ä¸€å®šä¼šæé«˜FIDï¼Œç”šè‡³å¯èƒ½æŸå®³å…¶å¯é æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/fid-med-eval%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mckellwoodland/fid-med-evalä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.13717v5">PDF</a> This preprint has not undergone peer review or any post-submission   improvements or corrections. The Version of Record of this contribution is   published in LNCS vol. 15012, and is available online at   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-031-72390-2_9">https://doi.org/10.1007/978-3-031-72390-2_9</a></p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æŒ‘æˆ˜äº†å°†FrÃ©chet Inception Distanceï¼ˆFIDï¼‰è¯„ä¼°æŒ‡æ ‡åº”ç”¨äºåŒ»å­¦å›¾åƒè´¨é‡çš„å¸¸è§„åšæ³•ã€‚é€šè¿‡å¯¹åå…­ä¸ªStyleGAN2ç½‘ç»œåœ¨ä¸åŒåŒ»å­¦æˆåƒæ¨¡å¼å’Œä¸åŒæ•°æ®å¢å¼ºæŠ€æœ¯ä¸Šçš„è¯„ä¼°å‘ç°ï¼ŒåŸºäºImageNetçš„ç‰¹å¾æå–å™¨ä¸åŸºäºRadImageNetçš„ç‰¹å¾æå–å™¨ç›¸æ¯”ï¼Œæ›´ä¸ºä¸€è‡´ä¸”ä¸äººä¸ºåˆ¤æ–­å¯¹é½ã€‚é‡‡ç”¨ImageNetè®­ç»ƒçš„SwAVç‰¹å¾æå–å™¨è®¡ç®—çš„FrÃ©chetè·ç¦»ä¸äººç±»ä¸“å®¶è¯„ä»·çš„ç›¸å…³æ€§æ›´é«˜ï¼Œè€ŒåŸºäºRadImageNetçš„æ’ååˆ™è¡¨ç°å‡ºä¸ç¨³å®šä¸”ä¸äººä¸ºåˆ¤æ–­ä¸ä¸€è‡´ã€‚æœ¬ç ”ç©¶å¯¹ç°æœ‰çš„å‡è®¾æå‡ºæŒ‘æˆ˜ï¼Œæä¾›æ–°çš„è¯æ®è¡¨æ˜ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒè®­ç»ƒçš„ç‰¹å¾æå–å™¨å¹¶ä¸ä¸€å®šèƒ½æé«˜FIDçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FIDä½œä¸ºä¸€ç§è¯„ä¼°åˆæˆå›¾åƒè´¨é‡çš„æŒ‡æ ‡ï¼Œå…¶é€‚ç”¨æ€§äºåŒ»å­¦æˆåƒé¢†åŸŸå°šä¸æ¸…æ¥šã€‚</li>
<li>ç›®å‰æœ‰ä¸€ç§è¶‹åŠ¿æ˜¯å°†FIDé€‚åº”äºåŒ»å­¦æˆåƒï¼Œé€šè¿‡åŸºäºåŒ»å­¦å›¾åƒçš„ç‰¹å¾æå–å™¨è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æœ¬ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºImageNetçš„ç‰¹å¾æå–å™¨åœ¨è¯„ä¼°åŒ»å­¦å›¾åƒè´¨é‡æ—¶ï¼Œç›¸è¾ƒäºåŸºäºRadImageNetçš„ç‰¹å¾æå–å™¨ï¼Œå…·æœ‰æ›´é«˜çš„ä¸€è‡´æ€§å’Œä¸äººä¸ºåˆ¤æ–­çš„å¯¹é½åº¦ã€‚</li>
<li>ä½¿ç”¨ImageNetè®­ç»ƒçš„SwAVç‰¹å¾æå–å™¨è®¡ç®—çš„FrÃ©chetè·ç¦»ä¸äººç±»ä¸“å®¶è¯„ä»·æœ‰è¾ƒé«˜çš„ç›¸å…³æ€§ã€‚</li>
<li>åŸºäºRadImageNetçš„ç‰¹å¾æå–å™¨åœ¨è¯„ä¼°åŒ»å­¦å›¾åƒè´¨é‡æ—¶è¡¨ç°å‡ºä¸ç¨³å®šæ€§å’Œä¸äººä¸ºåˆ¤æ–­çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒè®­ç»ƒçš„ç‰¹å¾æå–å™¨å¹¶ä¸ä¸€å®šèƒ½æé«˜FIDè¯„ä¼°çš„å¯é æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f74b3f8a5bb559564a3758396da5560a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c286e02c4f326d81759bd7159fbabd80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be000bc78815de10fc4233167cc51c58.jpg" align="middle">
</details>




<h2 id="One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN"><a href="#One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN" class="headerlink" title="One-for-All: Towards Universal Domain Translation with a Single StyleGAN"></a>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h2><p><strong>Authors:Yong Du, Jiahui Zhan, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, Shengfeng He</strong></p>
<p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the StyleGANâ€™s latent space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç¿»è¯‘æ¨¡å‹UniTranslatorï¼Œç”¨äºåœ¨è®­ç»ƒæ•°æ®æœ‰é™å’Œè§†è§‰å·®å¼‚æ˜¾è‘—çš„æƒ…å†µä¸‹ï¼Œå®ç°ä¸åŒè§†è§‰é¢†åŸŸä¹‹é—´çš„è¡¨ç¤ºè½¬æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨CLIPçš„ä¸­ç«‹åŸŸèƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„æ¨¡å—ä»æºé¢†åŸŸå’Œç›®æ ‡é¢†åŸŸçš„åµŒå…¥ä¸­æå–æŠ½è±¡ã€é¢†åŸŸæ— å…³è¯­ä¹‰ã€‚å°†è¿™äº›æŠ½è±¡è¯­ä¹‰ä¸ç›®æ ‡ç‰¹å®šè¯­ä¹‰èåˆï¼Œå¾—åˆ°CLIPç©ºé—´å†…çš„è½¬æ¢åµŒå…¥ã€‚ä¸ºäº†å¼¥CLIPå’ŒStyleGANä¹‹é—´ä¸åŒä¸–ç•Œçš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„éçº¿æ€§æ˜ å°„å™¨â€”â€”CLIP2Pæ˜ å°„å™¨ã€‚è¯¥æ¨¡å—åˆ©ç”¨CLIPåµŒå…¥è¿›è¡Œå®šåˆ¶ï¼Œä»¥è¿‘ä¼¼StyleGANæ½œåœ¨ç©ºé—´ä¸­çš„æ½œåœ¨åˆ†å¸ƒï¼Œæœ‰æ•ˆåœ°ä½œä¸ºè¿™ä¸¤ä¸ªç©ºé—´ä¹‹é—´çš„è¿æ¥å™¨ã€‚æ‰€æå‡ºçš„UniTranslatoré€šç”¨æ€§å¼ºï¼Œèƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ï¼Œå³ä½¿åœ¨è·¨ä¸åŒè§†è§‰é¢†åŸŸçš„è§†è§‰æŒ‘æˆ˜åœºæ™¯ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUniTranslatorç”Ÿæˆçš„é«˜è´¨é‡ç¿»è¯‘å±•ç¤ºäº†é¢†åŸŸç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œæ”¹è¿›çš„å›¾åƒè´¨é‡ã€‚UniTranslatorè¶…è¶Šäº†ç°æœ‰é€šç”¨æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³è¶…è¿‡äº†ä¸“ä¸šæ¨¡å‹ã€‚æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14222v2">PDF</a> </p>
<p><strong>Summary</strong><br>UniTranslatoræ¨¡å‹åˆ©ç”¨CLIPçš„è·¨åŸŸä¸­æ€§èƒ½åŠ›ï¼Œé€šè¿‡èåˆæºåŸŸå’Œç›®æ ‡åŸŸçš„æŠ½è±¡è¯­ä¹‰ï¼Œå®ç°åœ¨æœ‰é™è®­ç»ƒæ•°æ®å’Œæ˜¾è‘—è§†è§‰å·®å¼‚æ¡ä»¶ä¸‹ä¸åŒè§†è§‰é¢†åŸŸé—´çš„è¡¨ç¤ºè½¬æ¢ã€‚å¼•å…¥CLIP2Péçº¿æ€§æ˜ å°„å™¨ï¼Œåœ¨CLIPå’ŒStyleGANä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œå®ç°ä¸¤è€…ç©ºé—´çš„æœ‰æ•ˆè¿æ¥ã€‚UniTranslatorå…·æœ‰å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ç­‰ï¼Œå¯åœ¨ä¸åŒè§†è§‰é¢†åŸŸå®ç°é«˜è´¨é‡ç¿»è¯‘ï¼Œå±•ç¤ºé¢†åŸŸç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œæ”¹å–„çš„å›¾åƒè´¨é‡ã€‚è¯¥æ¨¡å‹è¶…è¶Šç°æœ‰é€šç”¨æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTranslatoræ˜¯ä¸€ä¸ªæ–°å‹ç¿»è¯‘æ¨¡å‹ï¼Œèƒ½åœ¨æœ‰é™è®­ç»ƒæ•°æ®å’Œæ˜¾è‘—è§†è§‰å·®å¼‚æ¡ä»¶ä¸‹å®ç°ä¸åŒè§†è§‰é¢†åŸŸé—´çš„è¡¨ç¤ºè½¬æ¢ã€‚</li>
<li>åˆ©ç”¨CLIPçš„è·¨åŸŸä¸­æ€§èƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ï¼ŒèåˆæºåŸŸå’Œç›®æ ‡åŸŸçš„æŠ½è±¡è¯­ä¹‰ã€‚</li>
<li>å¼•å…¥CLIP2Péçº¿æ€§æ˜ å°„å™¨ï¼Œè¿æ¥CLIPå’ŒStyleGANç©ºé—´ã€‚</li>
<li>UniTranslatorå…·å¤‡å¤šç§åŠŸèƒ½ï¼Œå¦‚é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ç­‰ã€‚</li>
<li>UniTranslatorèƒ½åœ¨è§†è§‰ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ç”Ÿæˆé«˜è´¨é‡ç¿»è¯‘ã€‚</li>
<li>UniTranslatoræ¨¡å‹æ€§èƒ½è¶…è¶Šç°æœ‰é€šç”¨æ¨¡å‹ï¼Œå¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95e6272f85d7fa4d37d1ae29b4643ac3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a24b3f85d591a3f1544bb1f8be5a6ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f2ec04d706bea45231594a7de44808b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32e26e6d642dd74ec5491b260c7f5d69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed3350cdfd555c1320fc865348c6003.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5fcd9c09d6cdabfa03da9bbb7f05557.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>å¯¹äºå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ï¼ˆOCTAï¼‰å›¾åƒï¼Œæœ‰é™çš„æ‰«æé€Ÿç‡å¯¼è‡´è§†é‡ï¼ˆFOVï¼‰ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚è™½ç„¶è¾ƒå¤§çš„FOVå›¾åƒå¯èƒ½ä¼šæ­ç¤ºæ›´å¤šçš„æ—é»„æ–‘è¡€ç®¡ç—…å˜ï¼Œä½†ç”±äºåˆ†è¾¨ç‡è¾ƒä½ï¼Œå…¶åº”ç”¨å—åˆ°å¾ˆå¤§é˜»ç¢ã€‚ä¸ºäº†æå‡åˆ†è¾¨ç‡ï¼Œæ—©æœŸçš„ç ”ç©¶å·¥ä½œåªæœ‰åœ¨åˆ©ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶æ‰èƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œä½†ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨å—é™äºæ”¶é›†å¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå¯¹æ— é…å¯¹æ–¹æ³•çš„éœ€æ±‚æé«˜ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ— éœ€é…å¯¹çš„ç¯å¢ƒä¸­å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä½†å®ƒå¯èƒ½åœ¨ç²¾ç¡®ä¿ç•™ç²¾ç»†æ¯›ç»†è¡€ç®¡ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¯¹äºOCTAæ¥è¯´æ˜¯éå¸¸å…³é”®çš„ç”Ÿç‰©æ ‡è®°ç‰©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢‘ç‡ä¿¡æ¯æ¥ä¿ç•™è¿™äº›è¯¦ç»†ä¿¡æ¯ï¼Œå…¶ä¸­é«˜é¢‘ä»£è¡¨ç»†èŠ‚ï¼Œä½é¢‘ä»£è¡¨ç²—ç²’èƒŒæ™¯ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ç”¨äºOCTAå›¾åƒï¼Œå¹¶é€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨ç‰¹åˆ«å¼ºè°ƒäº†é«˜é¢‘çš„ç²¾ç»†æ¯›ç»†è¡€ç®¡ã€‚ä¸ºäº†ä¿ƒè¿›é‡å»ºå›¾åƒçš„ç²¾ç¡®é¢‘è°±ï¼Œæˆ‘ä»¬è¿˜ä¸ºé‰´åˆ«å™¨æå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æ€§æŸå¤±ï¼Œå¹¶ä¸ºç«¯åˆ°ç«¯çš„ä¼˜åŒ–å¼•å…¥äº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰æ–¹é¢éƒ½ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ— é…å¯¹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºå¤„ç†OCTAå›¾åƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢‘ç‡ä¿¡æ¯ï¼Œé€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨å¼ºè°ƒé«˜é¢‘ç»†èŠ‚ï¼Œå¹¶é‡‡ç”¨é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„ç²¾åº¦å’Œè§†è§‰æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCTAå›¾åƒä¸­ï¼Œæ‰«æç‡é™åˆ¶å¯¼è‡´è§†é‡ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>è¾ƒå¤§çš„è§†é‡å›¾åƒå¯èƒ½æ­ç¤ºæ›´å¤šçš„çœ¼æ—è¡€ç®¡ç—…å˜ï¼Œä½†å…¶åˆ†è¾¨ç‡è¾ƒä½ã€‚</li>
<li>ä»¥å¾€çš„ç ”ç©¶é€šè¿‡ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒå–å¾—äº†ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œä½†ç°å®åº”ç”¨å—é™äºæ”¶é›†å¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºå¤„ç†OCTAå›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é¢‘ç‡ä¿¡æ¯ï¼Œå¼ºè°ƒé«˜é¢‘ç»†èŠ‚ï¼Œå¹¶é€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨è¿›è¡Œå®ç°ã€‚</li>
<li>ä¸ºäº†æé«˜å›¾åƒé‡å»ºçš„ç²¾åº¦ï¼Œè®ºæ–‡å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>




<h2 id="Diverse-Similarity-Encoder-for-Deep-GAN-Inversion"><a href="#Diverse-Similarity-Encoder-for-Deep-GAN-Inversion" class="headerlink" title="Diverse Similarity Encoder for Deep GAN Inversion"></a>Diverse Similarity Encoder for Deep GAN Inversion</h2><p><strong>Authors:Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</strong></p>
<p>Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN. </p>
<blockquote>
<p>å½“å‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å¯ä»¥åˆæˆé«˜è´¨é‡ï¼ˆHQï¼‰å›¾åƒï¼Œå› æ­¤ä½¿ç”¨GANså­¦ä¹ è¡¨ç¤ºæ˜¯æœ‰åˆ©çš„ã€‚GANåæ¼”æ˜¯æ–°å…´æ–¹æ³•ä¹‹ä¸€ï¼Œç ”ç©¶å¦‚ä½•å°†å›¾åƒåæ¼”åˆ°æ½œåœ¨ç©ºé—´ã€‚ç°æœ‰çš„GANç¼–ç å™¨å¯ä»¥åœ¨StyleGANä¸Šè¿›è¡Œå›¾åƒåæ¼”ï¼Œä½†ä¸èƒ½é€‚åº”å…¶ä»–æ·±åº¦GANsã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¸­çš„ä¸åŒç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ç¼–ç å™¨ï¼Œå‘½åä¸ºå¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰ï¼Œå¯ä»¥æ‰©å±•åˆ°å„ç§æœ€å…ˆè¿›çš„GANsã€‚DSEä½¿GANsèƒ½å¤Ÿä»HQå›¾åƒé‡å»ºæ›´é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œæ— è®ºæ˜¯åˆæˆçš„è¿˜æ˜¯çœŸå®çš„å›¾åƒã€‚DSEå…·æœ‰ç»Ÿä¸€çš„å·ç§¯å—ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°é€‚åº”ä¸»æµæ·±åº¦GANsï¼Œä¾‹å¦‚PGGANã€StyleGANå’ŒBigGANã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.10201v3">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå½“å‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰èƒ½åˆæˆé«˜è´¨é‡å›¾åƒçš„ä¼˜åŠ¿ï¼Œå­¦ä¹ å…¶è¡¨ç¤ºæ–¹æ³•å˜å¾—éå¸¸æœ‰åˆ©ã€‚é’ˆå¯¹ç°æœ‰GANç¼–ç å™¨åœ¨StyleGANä¸Šèƒ½å¤Ÿå€’ç½®å›¾åƒï¼Œä½†æ— æ³•é€‚åº”å…¶ä»–æ·±åº¦GANçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åä¸ºå¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰çš„é€‚åº”æ€§ç¼–ç å™¨ï¼Œå®ƒå¯ä»¥æ‰©å±•åˆ°å„ç§æœ€å…ˆè¿›çš„GANsã€‚DSEä½¿GANsèƒ½å¤Ÿä»é«˜è´¨é‡å›¾åƒä¸­é‡å»ºæ›´é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œæ— è®ºæ˜¯åˆæˆçš„è¿˜æ˜¯çœŸå®çš„å›¾åƒã€‚DSEå…·æœ‰ç»Ÿä¸€çš„å·ç§¯å—ï¼Œå¹¶å¾ˆå¥½åœ°é€‚åº”ä¸»æµæ·±åº¦GANï¼Œä¾‹å¦‚PGGANã€StyleGANå’ŒBigGANã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ·±åº¦GANèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä¿ƒä½¿ç ”ç©¶å¦‚ä½•å€ŸåŠ©GANå­¦ä¹ æœ‰æ•ˆè¡¨ç¤ºã€‚</li>
<li>GANå€’ç½®æˆä¸ºæ–°å…´æ–¹å‘ï¼Œæ—¨åœ¨å°†å›¾åƒå€’ç½®å›æ½œåœ¨ç©ºé—´ã€‚</li>
<li>ç°æœ‰GANç¼–ç å™¨ä¸»è¦é€‚ç”¨äºStyleGANï¼Œä½†ç¼ºä¹å¯¹å…¶ä»–æ·±åº¦GANçš„é€‚åº”æ€§ã€‚</li>
<li>æå‡ºäº†åä¸ºDSEçš„å¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼Œæ—¨åœ¨è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>DSEé€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§è¿›è¡Œè®¾è®¡ã€‚</li>
<li>DSEå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯æ‰©å±•åˆ°å¤šç§æœ€å…ˆè¿›çš„GANsã€‚</li>
<li>DSEèƒ½æé«˜GANsä»é«˜è´¨é‡å›¾åƒé‡å»ºå›¾åƒçš„ä¿çœŸåº¦ï¼Œé€‚ç”¨äºåˆæˆå’ŒçœŸå®å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c0940fc7f9e47f7ef709ff59469c9b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26229e3100c8c64811dfe13fae729ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-991864f280fa90ecd2fc3e0b5b8d9de9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-849a0bfe560207f45ea80d5eb85b9501.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04e472c619da5bae43ce50aa91f60b42.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  GASP Gaussian Avatars with Synthetic Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae7c1d5252a93267770b0f0d8e9bc329.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  SegFace Face Segmentation of Long-Tail Classes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13316.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
