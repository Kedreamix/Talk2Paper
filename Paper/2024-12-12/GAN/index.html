<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN 方向最新论文已更新，请持续关注 Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_GAN/2309.17269v2/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p>
<p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the model’s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models. </p>
<blockquote>
<p>图像反射去除对于恢复图像质量至关重要。扭曲的图像会对目标检测和图像分割等任务产生负面影响。在本文中，我们提出了一种使用单幅图像进行图像反射去除的新方法。我们没有关注模型架构，而是引入了一种可推广至图像到图像问题的新训练技术，输入和输出的性质相似。这一技术体现在我们的多步损失机制中，该机制在去除反射的任务中已被证明是有效的。此外，我们通过使用Pix2Pix GAN合成了一种高质量的非线性合成数据集RefGan，解决了反射去除训练数据不足的问题。该数据集显著提高了模型学习反射去除的更好模式的能力。我们还利用从环境图像的深度估计中提取的范围深度图作为辅助特征，利用其缺乏反射深度估计的属性。我们的方法在SIR^2基准和其他真实世界数据集上展示了卓越的性能，证明了其优于其他最先进模型的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种利用单图像进行图像反射去除的新方法。该方法引入了一种新的训练技术，可推广应用于同类图像到图像的问题。通过多步骤损失机制，该方法在反射去除任务中证明了其有效性。此外，还利用Pix2Pix GAN合成了一个高质量的非线性合成数据集RefGAN，以解决反射去除训练数据不足的问题。同时，利用从环境图像的深度估计中提取的深度图作为辅助特征，提高模型性能。该方法在SIR^2基准和其他真实世界数据集上表现出卓越的性能，超越了其他最先进的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的训练技术，适用于图像到图像的反射去除问题。</li>
<li>通过多步骤损失机制提高了反射去除任务的性能。</li>
<li>利用Pix2Pix GAN合成了一个高质量的非线性合成数据集RefGAN，解决了反射去除训练数据不足的问题。</li>
<li>利用从环境图像的深度估计中提取的深度图作为辅助特征。</li>
<li>提高了模型在反射去除任务中的泛化能力。</li>
<li>在SIR^2基准和其他真实世界数据集上实现了卓越的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-90610239ae13c51e1614f127958f2a81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8bde3fb0cbe989335b0d5bb7a4d46e4a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-00b852c3cdd4ace9f1729fdab0540434.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ab1d495bb0788c654b34111e35dbd40d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96f69c1d9eb0ee3bcc36085da2e8bd9c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9175c1aae7fa57889e8641ff09cdbe6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4f9871afd6e42b8e1cadb0f3e2faa7fc" align="middle">
</details>




<h2 id="Fine-grained-Text-to-Image-Synthesis"><a href="#Fine-grained-Text-to-Image-Synthesis" class="headerlink" title="Fine-grained Text to Image Synthesis"></a>Fine-grained Text to Image Synthesis</h2><p><strong>Authors:Xu Ouyang, Ying Chen, Kaiyue Zhu, Gady Agam</strong></p>
<p>Fine-grained text to image synthesis involves generating images from texts that belong to different categories. In contrast to general text to image synthesis, in fine-grained synthesis there is high similarity between images of different subclasses, and there may be linguistic discrepancy among texts describing the same image. Recent Generative Adversarial Networks (GAN), such as the Recurrent Affine Transformation (RAT) GAN model, are able to synthesize clear and realistic images from texts. However, GAN models ignore fine-grained level information. In this paper we propose an approach that incorporates an auxiliary classifier in the discriminator and a contrastive learning method to improve the accuracy of fine-grained details in images synthesized by RAT GAN. The auxiliary classifier helps the discriminator classify the class of images, and helps the generator synthesize more accurate fine-grained images. The contrastive learning method minimizes the similarity between images from different subclasses and maximizes the similarity between images from the same subclass. We evaluate on several state-of-the-art methods on the commonly used CUB-200-2011 bird dataset and Oxford-102 flower dataset, and demonstrated superior performance. </p>
<blockquote>
<p>细粒度文本到图像合成涉及从不同类别的文本生成图像。与一般的文本到图像合成相比，在细粒度合成中，不同子类的图像之间存在高度相似性，描述同一图像的文本之间也可能存在语言差异。最近的生成对抗网络（GAN），如循环仿射变换（RAT）GAN模型，能够从文本中合成清晰和现实的图像。然而，GAN模型忽略了细粒度级别的信息。在本文中，我们提出了一种方法，该方法在判别器中加入辅助分类器，并采用对比学习方法，以提高由RAT GAN合成图像的细节精度。辅助分类器有助于判别器对图像进行分类，并有助于生成器合成更精确的细粒度图像。对比学习方法最小化不同子类图像之间的相似性，并最大化同一子类图像之间的相似性。我们在常用的CUB-200-2011鸟类数据集和Oxford-102花卉数据集上评估了最先进的方法，并展示了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07196v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于文本的精细粒度图像合成是利用文本生成属于不同类别的图像。最新的生成对抗网络（GAN），如循环仿射变换（RAT）GAN模型，能够合成清晰且逼真的图像。然而，GAN模型忽略了精细级别的信息。本文提出一种方法，通过在判别器中引入辅助分类器并采用对比学习方法，提高由RAT GAN合成的图像的精细级别细节的准确性。辅助分类器帮助判别器对图像进行分类，并帮助生成器生成更精确的精细粒度图像。对比学习方法能减少不同子类图像之间的相似性并增加同一子类图像之间的相似性。在常用的CUB-200-2011鸟类数据集和Oxford-102花卉数据集上进行了评估，表现卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像合成可以分为精细粒度合成和一般合成，其中精细粒度合成存在图像子类间高相似性和描述同一图像的文本间语言差异的问题。</li>
<li>现有的GAN模型如RAT GAN能够合成清晰且逼真的图像，但忽略了图像的精细级别信息。</li>
<li>提出的方法通过在判别器中引入辅助分类器和采用对比学习方法来提高图像的精细级别细节的准确性。</li>
<li>辅助分类器有助于判别器对图像进行分类，并促进生成器生成更精确的精细粒度图像。</li>
<li>对比学习方法可以减少不同子类图像间的相似性并增加同一子类图像间的相似性。</li>
<li>该方法在CUB-200-2011鸟类数据集和Oxford-102花卉数据集上进行了评估，并表现出卓越的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9ddabde15480b7c735a4b219c5e32d1f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7185a0f87e292154d753c68ebd946585.jpg" align="middle">
</details>




<h2 id="Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks"><a href="#Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks" class="headerlink" title="Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks"></a>Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks</h2><p><strong>Authors:Sebastian Hereu, Qianfei Hu</strong></p>
<p>Convolutional neural networks (CNNs) have been combined with generative adversarial networks (GANs) to create deep convolutional generative adversarial networks (DCGANs) with great success. DCGANs have been used for generating images and videos from creative domains such as fashion design and painting. A common critique of the use of DCGANs in creative applications is that they are limited in their ability to generate creative products because the generator simply learns to copy the training distribution. We explore an extension of DCGANs, creative adversarial networks (CANs). Using CANs, we generate novel, creative portraits, using the WikiArt dataset to train the network. Moreover, we introduce our extension of CANs, conditional creative adversarial networks (CCANs), and demonstrate their potential to generate creative portraits conditioned on a style label. We argue that generating products that are conditioned, or inspired, on a style label closely emulates real creative processes in which humans produce imaginative work that is still rooted in previous styles. </p>
<blockquote>
<p>卷积神经网络（CNN）与生成对抗网络（GAN）的成功结合，产生了深度卷积生成对抗网络（DCGAN）。DCGAN已被广泛应用于时尚设计、绘画等创意领域的图像和视频生成。关于DCGAN在创意应用中的使用，一个常见的批评是它们生成创意产品的能力有限，因为生成器只是学习复制训练分布。我们探讨了DCGAN的扩展版本——创意对抗网络（CAN）。我们使用CAN生成新的创意肖像，并使用WikiArt数据集来训练网络。此外，我们还介绍了CAN的扩展版本——条件创意对抗网络（CCAN），并展示了它们在基于风格标签生成创意肖像方面的潜力。我们认为，生成基于或受风格标签启发的产品，紧密模拟了真实的创意过程，人类在此过程中会产生根植于先前风格的有创意的作品。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>卷积神经网络（CNN）与生成对抗网络（GAN）相结合，创建了深度卷积生成对抗网络（DCGAN），在图像和视频生成领域取得了巨大成功，尤其在时尚设计和绘画等创意领域。然而，DCGAN在创意应用方面存在局限性，主要因为生成器仅学习复制训练分布。为解决这一问题，本文探索了DCGAN的扩展版本——创意对抗网络（CAN），并利用WikiArt数据集生成新型创意肖像。此外，还介绍了CAN的扩展版本——条件创意对抗网络（CCAN），并展示了其根据风格标签生成创意肖像的潜力。本文认为，生成受风格标签影响或启发产品更贴近现实创意过程，人类能在遵循原有风格的基础上创作出富有想象力的作品。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>卷积神经网络与生成对抗网络结合创造了深度卷积生成对抗网络（DCGAN），在图像和视频生成领域应用广泛。</li>
<li>DCGAN在创意应用方面存在局限性，仅学习复制训练分布。</li>
<li>创意对抗网络（CAN）是DCGAN的扩展，能够生成新型创意肖像。</li>
<li>使用WikiArt数据集训练CAN生成创意肖像。</li>
<li>引入条件创意对抗网络（CCAN），能根据风格标签生成创意肖像。</li>
<li>生成受风格标签影响或启发产品更贴近现实创意过程。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f449af10b6b724d393aa486988a72232.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f57fcb2b2735bf7bdf971d6bd4f6e43c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bd8c0d0fa86b8dc9c60493948af361fc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9ef0866153897d19d7ed04ee03ef0461.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-919534e13329a643d8d5a59a53ea98b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a0460b23720fe9a0735ecd7c632e2a6b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89a8d2483234fcf74b6af7fc50907d24.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-260ae46b1080cb4b32ea46d808541992.jpg" align="middle">
</details>




<h2 id="HiFiVFS-High-Fidelity-Video-Face-Swapping"><a href="#HiFiVFS-High-Fidelity-Video-Face-Swapping" class="headerlink" title="HiFiVFS: High Fidelity Video Face Swapping"></a>HiFiVFS: High Fidelity Video Face Swapping</h2><p><strong>Authors:Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, Chengjie Wang</strong></p>
<p>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively. </p>
<blockquote>
<p>面部替换旨在生成结合源身份的与目标属性结果。现有方法主要集中在基于图像的面部替换上。在处理视频时，每一帧都是独立处理的，很难保证时间稳定性。从模型角度看，面部替换正逐渐从生成对抗网络（GANs）转向扩散模型（DMs），因为DMs已显示出具有更强的生成能力。当前的基于扩散的方法通常采用图像修复技术，这在保留光照和妆容等精细属性方面存在困难。为了应对这些挑战，我们提出了高保真视频面部替换（HiFiVFS）框架，该框架利用稳定视频扩散（SVD）的强大生成能力和时间先验。我们构建了一个精细属性模块，通过身份脱敏和对抗性学习提取身份无关和精细的属性特征。此外，我们还引入了详细的身份注入，以进一步增强身份相似性。大量实验表明，我们的方法在视频面部替换方面无论是在定性还是定量方面都达到了最先进的水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18293v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于生成对抗网络（GANs）和扩散模型（DMs）的面部分割技术，正逐渐向视频形式转移。由于扩散模型拥有更强的生成能力，现有的方法主要采用图像化的面部数据替换方法，面临动态时间下的连贯性不足等问题。为了提升细节品质和处理属性信息的稳定性，本文提出了一个基于稳定视频扩散模型的高保真视频面部替换框架（HiFiVFS），它融合了身份特征识别与精细属性特征分离等能力。本文的解决方案还包括对面部信息的详尽分析以及如何实现个性化与性能同步的创新点等关键技术手段，本文所提出的方法在视频面部替换上达到了行业领先的成效。简单概括为本文开发了一种创新的面部分割技术（HiFiVFS），能在保留特征的基础上成功切换脸部视频源的设定主体和具体情境的同时进行持续的匹配和视频维持处理以保持一致性与质量等综合能力的一种综合手段，从个人角度来看主要服务于完成效果至上的匹配最优与理想的最强化整合生成场景能力等等综合性目标的实现。。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频面部分割技术面临时间连贯性的挑战，当前解决方案多聚焦于图像处理模式缺乏长久时效连续性考虑；此方面难点将被论文的创新解决方案优化提升以保持稳定为考量出发点逐步落实，此为重中之重也是第一点的显著影响效应方向点，构建更具兼容性与泛化能力平衡的优秀面部分割体系平台来高效协同目标核心表现具有代表性不可替代性以及能大力激发自主合作服务部署参与目标的概率进度设计促成完备格局的理论水平认识建构的基石。在创新技术中融合使用生成对抗网络（GANs）和扩散模型（DMs）是当下研究趋势。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-122dfa5cb8ad5ef604f88cde5b275d81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-31457317ca85d09e8d2b9d29ee530829.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-52192381a3ec2d89bb7e63e4b4659e73.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ac370c44fad37f8bd34147d044b76d0a.jpg" align="middle">
</details>




<h2 id="Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks"><a href="#Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks" class="headerlink" title="Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks"></a>Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks</h2><p><strong>Authors:Srinitish Srinivasan, Varenya Pathak, Abirami S</strong></p>
<p>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem. </p>
<blockquote>
<p>抽象艺术是一种广受欢迎和讨论的艺术形式，通常能够描绘艺术家的情感。许多研究人员已经尝试以边缘检测、笔触和情绪识别算法的形式，利用机器学习和深度学习来研究抽象艺术。本文描述了使用生成对抗神经网络（GAN）对大量抽象绘画作品的研究。GAN具有学习和再现分布的能力，使研究者和科学家能够有效地探索和研究生成的图像空间。然而，挑战在于开发一种有效的GAN架构，能够克服常见的训练陷阱。本文通过引入一种专门用于高质量艺术作品生成的改进型DCGAN（mDCGAN）来解决这一挑战。该方法涉及对所做的修改进行彻底探索，深入研究DCGAN的精细工作原理、优化技术和正则化方法，旨在提高艺术创作的稳定性和逼真度，使生成的模式研究有效。所提出的mDCGAN对层配置和架构选择进行了细致调整，为艺术创作的独特需求提供量身定制的解决方案，同时有效解决了模式崩溃和梯度消失等问题。此外，本文还通过随机漫步探索生成的潜在空间，了解抽象艺术空间中笔触和颜色之间的向量关系，以及对GAN训练一段时间后不稳定输出的统计分析，并比较其显著差异。这些发现验证了所提出方法的有效性，强调了其在数字艺术生成和数字艺术生态系统领域革命的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18397v2">PDF</a> Accepted for publication by Intelligent Decision Technologies</p>
<p><strong>摘要</strong><br>    本研究利用生成对抗神经网络（GAN）对抽象画作进行了广泛分布的研究。GANs具有学习和复制分布的能力，使得研究人员和科学家能够有效地探索和研究生成的图像空间。研究中的挑战在于开发一种有效的GAN架构，以克服常见的训练难题。本文解决这一挑战的方法是通过引入一种针对高质量艺术品生成的改进型DCGAN（mDCGAN）。该方法深入探讨了所做的修改，详细研究了DCGAN的工作原理、优化技术和正则化方法，这些方法旨在提高艺术品生成的稳定性和逼真度，从而实现对生成模式的有效研究。提出的mDCGAN在层配置和建筑选择方面进行了细致调整，为艺术生成提供了量身定制的解决方案，同时有效地解决了模式崩溃和梯度消失等问题。此外，本文通过随机游走的方式探索了生成的潜在空间，了解了抽象艺术空间中笔触和颜色之间的向量关系，并对GAN训练一定时间后不稳定输出的统计差异进行了分析比较。这些发现验证了该方法的有效性，强调了其在数字艺术生成和数字艺术生态系统领域的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用生成对抗神经网络（GAN）研究抽象画作分布。</li>
<li>mDCGAN的设计专门用于高质量艺术品生成，通过对DCGAN的改进克服常见训练难题。</li>
<li>mDCGAN在层配置和建筑选择上的细致调整，为艺术生成提供了量身定制的解决方案。</li>
<li>mDCGAN能够解决模式崩溃和梯度消失等问题，提高艺术品生成的稳定性和逼真度。</li>
<li>通过随机游走探索生成的潜在空间，理解抽象艺术空间中笔触和颜色的向量关系。</li>
<li>对GAN训练过程中的不稳定输出进行统计差异分析。</li>
<li>研究结果验证了mDCGAN方法的有效性，并展示了其在数字艺术生成领域的潜力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0c86fb3b63917812a573d399ded69ef3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-785ec5270ebb6ad154707406e7a5ac8a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-64858c227d4fe7e862156750ec3605f9.jpg" align="middle">
</details>




<h2 id="Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training"><a href="#Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training" class="headerlink" title="Representation Learning of Multivariate Time Series using Attention and   Adversarial Training"></a>Representation Learning of Multivariate Time Series using Attention and   Adversarial Training</h2><p><strong>Authors:Leon Scharwächter, Sebastian Otte</strong></p>
<p>A critical factor in trustworthy machine learning is to develop robust representations of the training data. Only under this guarantee methods are legitimate to artificially generate data, for example, to counteract imbalanced datasets or provide counterfactual explanations for blackbox decision-making systems. In recent years, Generative Adversarial Networks (GANs) have shown considerable results in forming stable representations and generating realistic data. While many applications focus on generating image data, less effort has been made in generating time series data, especially multivariate signals. In this work, a Transformer-based autoencoder is proposed that is regularized using an adversarial training scheme to generate artificial multivariate time series signals. The representation is evaluated using t-SNE visualizations, Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the generated signals exhibit higher similarity to an exemplary dataset than using a convolutional network approach. </p>
<blockquote>
<p>在可信机器学习中的关键因素是开发稳健的训练数据表示。只有在这个保证下，人工生成数据的方法才是合法的，例如，对抗不平衡数据集或为黑箱决策制定系统提供反事实解释。近年来，生成对抗网络（GANs）在形成稳定表示和生成真实数据方面取得了显著成果。虽然许多应用都集中在生成图像数据上，但在生成时间序列数据方面投入的努力较少，尤其是多元信号。在这项工作中，提出了一种基于Transformer的自编码器，通过使用对抗性训练方案进行正则化来生成人工多元时间序列信号。通过t-SNE可视化、动态时间扭曲（DTW）和熵评分来评估表示。我们的结果表明，与卷积网络方法相比，生成的信号与示例数据集具有更高的相似性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01987v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了机器学习中可靠表示训练数据的重要性，并指出只有在确保数据可靠的前提下，才能合法地生成人工数据。文章利用生成对抗网络（GANs）生成稳定的数据表示和真实数据，并展示了其在生成时间序列数据方面的潜力。本研究提出了一种基于Transformer的自编码器，通过对抗训练策略生成人工多元时间序列信号。实验结果显示，该方法的生成信号与样本数据集相似度更高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成对抗网络（GANs）在形成稳定的数据表示和生成真实数据方面表现出显著效果。</li>
<li>在机器学习中，可靠的数据表示是一个关键要素，是生成人工数据的前提。</li>
<li>研究提出了一种基于Transformer的自编码器，用于生成人工多元时间序列信号。</li>
<li>该自编码器通过对抗训练策略进行正则化。</li>
<li>实验评估显示，该方法生成的信号与样本数据集相似度更高。</li>
<li>该研究使用了t-SNE可视化、动态时间弯曲（DTW）和熵值等方法来评估表示效果。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9b941c35f07b9443c2d8562fcd05075d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0feafb45ee0492776796259b551baad5.jpg" align="middle">
</details>




<h2 id="GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks"><a href="#GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks" class="headerlink" title="GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks"></a>GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks</h2><p><strong>Authors:Mehran Hosseini, Peyman Hosseini</strong></p>
<p>The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative n-dimensional Cartesian coordinate system. We show this drastically improves quality of images generated by Diffusion Models, GANs, and Variational AutoEncoders (VAE). </p>
<blockquote>
<p>近十年来，图像生成模型在重现复杂几何特征（如人类的手和手指特征）方面的持续无能一直是图像生成领域的一个持续存在的问题。尽管通过增加模型规模和多样化训练数据集已经取得了一些进展，但这一问题在包括降噪扩散模型、生成对抗网络（GAN）在内的所有模型中仍然普遍存在，这表明基础架构存在根本性不足。在本文中，我们展示了如何通过增强卷积层的几何能力来解决这个问题，方法是提供一个包含相对n维笛卡尔坐标系统的单一输入通道。我们证明了这可以极大地提高扩散模型、GAN和变分自动编码器（VAE）生成的图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01951v2">PDF</a> Accepted at WACV 2025. Contains 19 pages, 15 figures, and 9 tables</p>
<p><strong>Summary</strong></p>
<p>本文指出图像生成模型在重建复杂几何特征（如人手和手指）方面存在长期难题，尽管通过增加模型规模和多样化训练数据集取得了一些进展，但这一问题在各类模型中依然普遍存在。本文提出通过增加一个包含相对n维笛卡尔坐标系统的单一输入通道来增强卷积层的几何能力，从而缓解这一问题，并显著提高扩散模型、生成对抗网络和变分自编码器的图像生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成模型在重建复杂几何特征方面存在难题，这一问题已持续近十年。</li>
<li>各类模型，包括扩散模型、生成对抗网络和变分自编码器都面临这一问题。</li>
<li>问题根源可能在于现有模型架构的固有缺陷。</li>
<li>通过增加一个包含相对n维笛卡尔坐标系统的单一输入通道，可以增强模型的几何处理能力。</li>
<li>此方法能显著提高图像生成质量，特别是在复杂几何特征的重建方面。</li>
<li>该方法为解决图像生成模型中长期存在的问题提供了一种有效解决方案。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2a0e59b4dbd04d0d8521d495c872ebae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-38274ed949f889a70e7b31e7e6019427.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8daa63128009486f6689a0da6a5320cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-115a2a4082a9d7e1fef8a75282c4c1be.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd81a303d0434e5c85c0015a53779dc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-27aad86691678ae5aeaeb9b61eeb1637.jpg" align="middle">
</details>




<h2 id="One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN"><a href="#One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN" class="headerlink" title="One-for-All: Towards Universal Domain Translation with a Single StyleGAN"></a>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h2><p><strong>Authors:Yong Du, Jiahui Zhan, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, Shengfeng He</strong></p>
<p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the StyleGAN’s latent space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public. </p>
<blockquote>
<p>本文提出了一种新型翻译模型UniTranslator，用于在训练数据有限和视觉差异显著的情况下，实现不同视觉领域之间的表示转换。我们的方法的主要思想是利用CLIP的中立域能力作为桥梁机制，同时使用一个单独的模块从源领域和目标领域的嵌入中提取抽象、领域无关语义。将这些抽象语义与目标特定语义融合，得到CLIP空间内的转换嵌入。为了弥合CLIP和StyleGAN之间截然不同的世界，我们引入了一种新的非线性映射器——CLIP2P映射器。该模块利用CLIP嵌入进行定制，以近似StyleGAN潜在空间中的潜在分布，有效地作为这两个空间之间的连接器。所提出的UniTranslator通用性强，能够执行各种任务，包括风格混合、风格化和翻译，即使在跨越不同视觉领域的视觉挑战场景中也是如此。值得注意的是，UniTranslator生成的高质量翻译展示了领域相关性、多样性和改进的图像质量。UniTranslator超越了现有通用模型的性能，并在代表性任务中表现良好，甚至超过了专业模型。源代码和训练好的模型将向公众开放。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14222v2">PDF</a> </p>
<p><strong>Summary</strong><br>UniTranslator模型利用CLIP的跨域中性能力，通过融合源域和目标域嵌入的抽象语义，实现在有限训练数据和显著视觉差异条件下不同视觉域之间的表示转换。引入CLIP2P非线性映射器，缩小CLIP和StyleGAN之间的差距。UniTranslator功能强大，能在不同视觉领域执行风格混合、风格化和翻译等任务，生成高质量、相关性强、多样化的图像。其性能超越现有通用模型并在代表性任务中表现良好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTranslator模型被提出用于在有限训练数据和显著视觉差异条件下实现不同视觉域之间的表示转换。</li>
<li>利用CLIP的跨域中性能力作为桥梁机制。</li>
<li>引入CLIP2P非线性映射器以连接CLIP和StyleGAN两个空间。</li>
<li>UniTranslator模型功能强大，可执行风格混合、风格化和翻译等任务。</li>
<li>UniTranslator生成的翻译具有高质量、相关性强和多样化的特点。</li>
<li>UniTranslator性能超越现有通用模型，并在代表性任务中表现良好。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-95e6272f85d7fa4d37d1ae29b4643ac3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7a24b3f85d591a3f1544bb1f8be5a6ea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0f2ec04d706bea45231594a7de44808b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-32e26e6d642dd74ec5491b260c7f5d69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6ed3350cdfd555c1320fc865348c6003.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b5fcd9c09d6cdabfa03da9bbb7f05557.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>对于光学相干断层扫描血管造影（OCTA）图像，有限的扫描速率导致视野（FOV）与成像分辨率之间的权衡。虽然较大的FOV图像可能会揭示更多的旁黄斑血管病变，但由于分辨率较低，其应用受到很大阻碍。为了提高分辨率，以前的工作只有在使用配对数据进行训练时才能达到令人满意的效果，但现实世界的应用受到收集大规模配对图像的挑战的限制。因此，非常需要一种无需配对的方法。生成对抗网络（GAN）已在无需配对的情况下广泛使用，但它可能难以准确保留毛细血管的细粒度细节，这对于OCTA来说是非常重要的生物标志物。本文的方法旨在通过利用频率信息来保留这些细节，将细节视为高频（hf），将粗粒度背景视为低频（lf）。总的来说，我们提出了一种基于GAN的无需配对的OCTA图像超分辨率方法，并通过双路径生成器特别强调了hf细毛细血管。为了促进重建图像的精确光谱，我们还为鉴别器提出了频率感知对抗性损失，并引入了频率感知焦点一致性损失来进行端到端优化。实验表明，我们的方法在定量和视觉上均优于其他最先进的无需配对的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>针对光学相干层析血管造影术（OCTA）图像的特点，扫描率限制导致了视野与成像分辨率之间的权衡。论文提出了一种基于GAN的无配对超分辨率方法，强调高频信息对保留毛细血管细节的重要性，并采用双路径生成器进行精细毛细血管的重建。同时，引入频率感知对抗损失和频率感知焦点一致性损失，以优化图像频谱和端到端的优化。实验表明，该方法在定量和视觉表现上均优于其他顶尖的无配对方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCTA图像存在扫描率与视野分辨率的权衡问题。</li>
<li>现有方法使用配对数据进行训练以提高分辨率，但实际应用受限于大规模配对图像的收集难度。</li>
<li>提出一种基于GAN的无配对超分辨率方法用于OCTA图像。</li>
<li>重视高频信息以保留毛细血管细节，使用双路径生成器进行重建。</li>
<li>引入频率感知对抗损失，优化图像频谱的重建。</li>
<li>提出频率感知焦点一致性损失，实现端到端的优化。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>




<h2 id="Diverse-Similarity-Encoder-for-Deep-GAN-Inversion"><a href="#Diverse-Similarity-Encoder-for-Deep-GAN-Inversion" class="headerlink" title="Diverse Similarity Encoder for Deep GAN Inversion"></a>Diverse Similarity Encoder for Deep GAN Inversion</h2><p><strong>Authors:Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</strong></p>
<p>Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN. </p>
<blockquote>
<p>当前深度生成对抗网络（GANs）可以合成高质量（HQ）图像，因此使用GANs学习表示是有利的。GAN反演是新兴方法之一，研究如何将图像反演到潜在空间。现有的GAN编码器可以在StyleGAN上进行图像反演，但不能适应其他深度GANs。我们提出了一种新的方法来解决这个问题。通过评估潜在向量和图像中的不同相似性，我们设计了一种自适应编码器，命名为多样相似性编码器（DSE），可以扩展到各种最先进的GANs。DSE使GANs能够从HQ图像重建出更高保真度的图像，无论是合成的还是真实的图像。DSE具有统一的卷积块，并能很好地适应主流深度GANs，例如PGGAN、StyleGAN和BigGAN。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.10201v3">PDF</a> </p>
<p><strong>Summary</strong><br>在现有深度生成对抗网络（GANs）的基础上，提出了一种新的GAN反卷积方法，即多样相似性编码器（DSE）。该方法通过评估潜在向量和图像之间的不同相似性，可以扩展到各种先进的GANs。DSE能够重构出高质量的真实和合成图像。它适用于主流的深度GANs，如PGGAN、StyleGAN和BigGAN。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前深度生成对抗网络（GANs）能够合成高质量图像，学习表示成为热门研究主题。</li>
<li>GAN反卷积是新兴的研究方向之一，旨在研究如何将图像反转回潜在空间。</li>
<li>现有的GAN编码器可以在StyleGAN上进行图像反卷积，但无法适应其他深度GANs。</li>
<li>提出了一种新的方法——多样相似性编码器（DSE），可以解决该问题。</li>
<li>DSE通过评估潜在向量和图像之间的不同相似性进行设计。</li>
<li>DSE可应用于各种先进的GANs，具有良好的扩展性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2c0940fc7f9e47f7ef709ff59469c9b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26229e3100c8c64811dfe13fae729ff8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-991864f280fa90ecd2fc3e0b5b8d9de9.jpg" align="middle">

</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_元宇宙_虚拟人/2412.04955v2/page_2_0.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-12  GASP Gaussian Avatars with Synthetic Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Face Swapping/2412.07260v1/page_0_0.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2024-12-12  SegFace Face Segmentation of Long-Tail Classes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
