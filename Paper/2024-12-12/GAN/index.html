<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_GAN/2309.17269v2/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p>
<p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the modelâ€™s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models. </p>
<blockquote>
<p>å›¾åƒåå°„å»é™¤å¯¹äºæ¢å¤å›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚æ‰­æ›²çš„å›¾åƒä¼šå¯¹ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å•å¹…å›¾åƒè¿›è¡Œå›¾åƒåå°„å»é™¤çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æ²¡æœ‰å…³æ³¨æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§å¯æ¨å¹¿è‡³å›¾åƒåˆ°å›¾åƒé—®é¢˜çš„æ–°è®­ç»ƒæŠ€æœ¯ï¼Œè¾“å…¥å’Œè¾“å‡ºçš„æ€§è´¨ç›¸ä¼¼ã€‚è¿™ä¸€æŠ€æœ¯ä½“ç°åœ¨æˆ‘ä»¬çš„å¤šæ­¥æŸå¤±æœºåˆ¶ä¸­ï¼Œè¯¥æœºåˆ¶åœ¨å»é™¤åå°„çš„ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨Pix2Pix GANåˆæˆäº†ä¸€ç§é«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGanï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹å­¦ä¹ åå°„å»é™¤çš„æ›´å¥½æ¨¡å¼çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„èŒƒå›´æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å…¶ç¼ºä¹åå°„æ·±åº¦ä¼°è®¡çš„å±æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SIR^2åŸºå‡†å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å•å›¾åƒè¿›è¡Œå›¾åƒåå°„å»é™¤çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯æ¨å¹¿åº”ç”¨äºåŒç±»å›¾åƒåˆ°å›¾åƒçš„é—®é¢˜ã€‚é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•åœ¨åå°„å»é™¤ä»»åŠ¡ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œä»¥è§£å†³åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨SIR^2åŸºå‡†å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œé€‚ç”¨äºå›¾åƒåˆ°å›¾åƒçš„åå°„å»é™¤é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¤šæ­¥éª¤æŸå¤±æœºåˆ¶æé«˜äº†åå°„å»é™¤ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨Pix2Pix GANåˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„éçº¿æ€§åˆæˆæ•°æ®é›†RefGANï¼Œè§£å†³äº†åå°„å»é™¤è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨ä»ç¯å¢ƒå›¾åƒçš„æ·±åº¦ä¼°è®¡ä¸­æå–çš„æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‚</li>
<li>æé«˜äº†æ¨¡å‹åœ¨åå°„å»é™¤ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨SIR^2åŸºå‡†å’Œå…¶ä»–çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-90610239ae13c51e1614f127958f2a81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8bde3fb0cbe989335b0d5bb7a4d46e4a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-00b852c3cdd4ace9f1729fdab0540434.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ab1d495bb0788c654b34111e35dbd40d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96f69c1d9eb0ee3bcc36085da2e8bd9c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9175c1aae7fa57889e8641ff09cdbe6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4f9871afd6e42b8e1cadb0f3e2faa7fc" align="middle">
</details>




<h2 id="Fine-grained-Text-to-Image-Synthesis"><a href="#Fine-grained-Text-to-Image-Synthesis" class="headerlink" title="Fine-grained Text to Image Synthesis"></a>Fine-grained Text to Image Synthesis</h2><p><strong>Authors:Xu Ouyang, Ying Chen, Kaiyue Zhu, Gady Agam</strong></p>
<p>Fine-grained text to image synthesis involves generating images from texts that belong to different categories. In contrast to general text to image synthesis, in fine-grained synthesis there is high similarity between images of different subclasses, and there may be linguistic discrepancy among texts describing the same image. Recent Generative Adversarial Networks (GAN), such as the Recurrent Affine Transformation (RAT) GAN model, are able to synthesize clear and realistic images from texts. However, GAN models ignore fine-grained level information. In this paper we propose an approach that incorporates an auxiliary classifier in the discriminator and a contrastive learning method to improve the accuracy of fine-grained details in images synthesized by RAT GAN. The auxiliary classifier helps the discriminator classify the class of images, and helps the generator synthesize more accurate fine-grained images. The contrastive learning method minimizes the similarity between images from different subclasses and maximizes the similarity between images from the same subclass. We evaluate on several state-of-the-art methods on the commonly used CUB-200-2011 bird dataset and Oxford-102 flower dataset, and demonstrated superior performance. </p>
<blockquote>
<p>ç»†ç²’åº¦æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ¶‰åŠä»ä¸åŒç±»åˆ«çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚ä¸ä¸€èˆ¬çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆç›¸æ¯”ï¼Œåœ¨ç»†ç²’åº¦åˆæˆä¸­ï¼Œä¸åŒå­ç±»çš„å›¾åƒä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸ä¼¼æ€§ï¼Œæè¿°åŒä¸€å›¾åƒçš„æ–‡æœ¬ä¹‹é—´ä¹Ÿå¯èƒ½å­˜åœ¨è¯­è¨€å·®å¼‚ã€‚æœ€è¿‘çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå¦‚å¾ªç¯ä»¿å°„å˜æ¢ï¼ˆRATï¼‰GANæ¨¡å‹ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬ä¸­åˆæˆæ¸…æ™°å’Œç°å®çš„å›¾åƒã€‚ç„¶è€Œï¼ŒGANæ¨¡å‹å¿½ç•¥äº†ç»†ç²’åº¦çº§åˆ«çš„ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆ¤åˆ«å™¨ä¸­åŠ å…¥è¾…åŠ©åˆ†ç±»å™¨ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜ç”±RAT GANåˆæˆå›¾åƒçš„ç»†èŠ‚ç²¾åº¦ã€‚è¾…åŠ©åˆ†ç±»å™¨æœ‰åŠ©äºåˆ¤åˆ«å™¨å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¹¶æœ‰åŠ©äºç”Ÿæˆå™¨åˆæˆæ›´ç²¾ç¡®çš„ç»†ç²’åº¦å›¾åƒã€‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•æœ€å°åŒ–ä¸åŒå­ç±»å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æœ€å¤§åŒ–åŒä¸€å­ç±»å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬åœ¨å¸¸ç”¨çš„CUB-200-2011é¸Ÿç±»æ•°æ®é›†å’ŒOxford-102èŠ±å‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07196v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬çš„ç²¾ç»†ç²’åº¦å›¾åƒåˆæˆæ˜¯åˆ©ç”¨æ–‡æœ¬ç”Ÿæˆå±äºä¸åŒç±»åˆ«çš„å›¾åƒã€‚æœ€æ–°çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå¦‚å¾ªç¯ä»¿å°„å˜æ¢ï¼ˆRATï¼‰GANæ¨¡å‹ï¼Œèƒ½å¤Ÿåˆæˆæ¸…æ™°ä¸”é€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼ŒGANæ¨¡å‹å¿½ç•¥äº†ç²¾ç»†çº§åˆ«çš„ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åœ¨åˆ¤åˆ«å™¨ä¸­å¼•å…¥è¾…åŠ©åˆ†ç±»å™¨å¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæé«˜ç”±RAT GANåˆæˆçš„å›¾åƒçš„ç²¾ç»†çº§åˆ«ç»†èŠ‚çš„å‡†ç¡®æ€§ã€‚è¾…åŠ©åˆ†ç±»å™¨å¸®åŠ©åˆ¤åˆ«å™¨å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¸®åŠ©ç”Ÿæˆå™¨ç”Ÿæˆæ›´ç²¾ç¡®çš„ç²¾ç»†ç²’åº¦å›¾åƒã€‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•èƒ½å‡å°‘ä¸åŒå­ç±»å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶å¢åŠ åŒä¸€å­ç±»å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨å¸¸ç”¨çš„CUB-200-2011é¸Ÿç±»æ•°æ®é›†å’ŒOxford-102èŠ±å‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒåˆæˆå¯ä»¥åˆ†ä¸ºç²¾ç»†ç²’åº¦åˆæˆå’Œä¸€èˆ¬åˆæˆï¼Œå…¶ä¸­ç²¾ç»†ç²’åº¦åˆæˆå­˜åœ¨å›¾åƒå­ç±»é—´é«˜ç›¸ä¼¼æ€§å’Œæè¿°åŒä¸€å›¾åƒçš„æ–‡æœ¬é—´è¯­è¨€å·®å¼‚çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„GANæ¨¡å‹å¦‚RAT GANèƒ½å¤Ÿåˆæˆæ¸…æ™°ä¸”é€¼çœŸçš„å›¾åƒï¼Œä½†å¿½ç•¥äº†å›¾åƒçš„ç²¾ç»†çº§åˆ«ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•é€šè¿‡åœ¨åˆ¤åˆ«å™¨ä¸­å¼•å…¥è¾…åŠ©åˆ†ç±»å™¨å’Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ¥æé«˜å›¾åƒçš„ç²¾ç»†çº§åˆ«ç»†èŠ‚çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¾…åŠ©åˆ†ç±»å™¨æœ‰åŠ©äºåˆ¤åˆ«å™¨å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¿ƒè¿›ç”Ÿæˆå™¨ç”Ÿæˆæ›´ç²¾ç¡®çš„ç²¾ç»†ç²’åº¦å›¾åƒã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯ä»¥å‡å°‘ä¸åŒå­ç±»å›¾åƒé—´çš„ç›¸ä¼¼æ€§å¹¶å¢åŠ åŒä¸€å­ç±»å›¾åƒé—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨CUB-200-2011é¸Ÿç±»æ•°æ®é›†å’ŒOxford-102èŠ±å‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9ddabde15480b7c735a4b219c5e32d1f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7185a0f87e292154d753c68ebd946585.jpg" align="middle">
</details>




<h2 id="Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks"><a href="#Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks" class="headerlink" title="Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks"></a>Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks</h2><p><strong>Authors:Sebastian Hereu, Qianfei Hu</strong></p>
<p>Convolutional neural networks (CNNs) have been combined with generative adversarial networks (GANs) to create deep convolutional generative adversarial networks (DCGANs) with great success. DCGANs have been used for generating images and videos from creative domains such as fashion design and painting. A common critique of the use of DCGANs in creative applications is that they are limited in their ability to generate creative products because the generator simply learns to copy the training distribution. We explore an extension of DCGANs, creative adversarial networks (CANs). Using CANs, we generate novel, creative portraits, using the WikiArt dataset to train the network. Moreover, we introduce our extension of CANs, conditional creative adversarial networks (CCANs), and demonstrate their potential to generate creative portraits conditioned on a style label. We argue that generating products that are conditioned, or inspired, on a style label closely emulates real creative processes in which humans produce imaginative work that is still rooted in previous styles. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æˆåŠŸç»“åˆï¼Œäº§ç”Ÿäº†æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰ã€‚DCGANå·²è¢«å¹¿æ³›åº”ç”¨äºæ—¶å°šè®¾è®¡ã€ç»˜ç”»ç­‰åˆ›æ„é¢†åŸŸçš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚å…³äºDCGANåœ¨åˆ›æ„åº”ç”¨ä¸­çš„ä½¿ç”¨ï¼Œä¸€ä¸ªå¸¸è§çš„æ‰¹è¯„æ˜¯å®ƒä»¬ç”Ÿæˆåˆ›æ„äº§å“çš„èƒ½åŠ›æœ‰é™ï¼Œå› ä¸ºç”Ÿæˆå™¨åªæ˜¯å­¦ä¹ å¤åˆ¶è®­ç»ƒåˆ†å¸ƒã€‚æˆ‘ä»¬æ¢è®¨äº†DCGANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCANï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨CANç”Ÿæˆæ–°çš„åˆ›æ„è‚–åƒï¼Œå¹¶ä½¿ç”¨WikiArtæ•°æ®é›†æ¥è®­ç»ƒç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†CANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”æ¡ä»¶åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCCANï¼‰ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬åœ¨åŸºäºé£æ ¼æ ‡ç­¾ç”Ÿæˆåˆ›æ„è‚–åƒæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”ŸæˆåŸºäºæˆ–å—é£æ ¼æ ‡ç­¾å¯å‘çš„äº§å“ï¼Œç´§å¯†æ¨¡æ‹Ÿäº†çœŸå®çš„åˆ›æ„è¿‡ç¨‹ï¼Œäººç±»åœ¨æ­¤è¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿæ ¹æ¤äºå…ˆå‰é£æ ¼çš„æœ‰åˆ›æ„çš„ä½œå“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç›¸ç»“åˆï¼Œåˆ›å»ºäº†æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰ï¼Œåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå°¤å…¶åœ¨æ—¶å°šè®¾è®¡å’Œç»˜ç”»ç­‰åˆ›æ„é¢†åŸŸã€‚ç„¶è€Œï¼ŒDCGANåœ¨åˆ›æ„åº”ç”¨æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦å› ä¸ºç”Ÿæˆå™¨ä»…å­¦ä¹ å¤åˆ¶è®­ç»ƒåˆ†å¸ƒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ¢ç´¢äº†DCGANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCANï¼‰ï¼Œå¹¶åˆ©ç”¨WikiArtæ•°æ®é›†ç”Ÿæˆæ–°å‹åˆ›æ„è‚–åƒã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†CANçš„æ‰©å±•ç‰ˆæœ¬â€”â€”æ¡ä»¶åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCCANï¼‰ï¼Œå¹¶å±•ç¤ºäº†å…¶æ ¹æ®é£æ ¼æ ‡ç­¾ç”Ÿæˆåˆ›æ„è‚–åƒçš„æ½œåŠ›ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œç”Ÿæˆå—é£æ ¼æ ‡ç­¾å½±å“æˆ–å¯å‘äº§å“æ›´è´´è¿‘ç°å®åˆ›æ„è¿‡ç¨‹ï¼Œäººç±»èƒ½åœ¨éµå¾ªåŸæœ‰é£æ ¼çš„åŸºç¡€ä¸Šåˆ›ä½œå‡ºå¯Œæœ‰æƒ³è±¡åŠ›çš„ä½œå“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç»“åˆåˆ›é€ äº†æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰ï¼Œåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚</li>
<li>DCGANåœ¨åˆ›æ„åº”ç”¨æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä»…å­¦ä¹ å¤åˆ¶è®­ç»ƒåˆ†å¸ƒã€‚</li>
<li>åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCANï¼‰æ˜¯DCGANçš„æ‰©å±•ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–°å‹åˆ›æ„è‚–åƒã€‚</li>
<li>ä½¿ç”¨WikiArtæ•°æ®é›†è®­ç»ƒCANç”Ÿæˆåˆ›æ„è‚–åƒã€‚</li>
<li>å¼•å…¥æ¡ä»¶åˆ›æ„å¯¹æŠ—ç½‘ç»œï¼ˆCCANï¼‰ï¼Œèƒ½æ ¹æ®é£æ ¼æ ‡ç­¾ç”Ÿæˆåˆ›æ„è‚–åƒã€‚</li>
<li>ç”Ÿæˆå—é£æ ¼æ ‡ç­¾å½±å“æˆ–å¯å‘äº§å“æ›´è´´è¿‘ç°å®åˆ›æ„è¿‡ç¨‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f449af10b6b724d393aa486988a72232.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f57fcb2b2735bf7bdf971d6bd4f6e43c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bd8c0d0fa86b8dc9c60493948af361fc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9ef0866153897d19d7ed04ee03ef0461.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-919534e13329a643d8d5a59a53ea98b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a0460b23720fe9a0735ecd7c632e2a6b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89a8d2483234fcf74b6af7fc50907d24.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-260ae46b1080cb4b32ea46d808541992.jpg" align="middle">
</details>




<h2 id="HiFiVFS-High-Fidelity-Video-Face-Swapping"><a href="#HiFiVFS-High-Fidelity-Video-Face-Swapping" class="headerlink" title="HiFiVFS: High Fidelity Video Face Swapping"></a>HiFiVFS: High Fidelity Video Face Swapping</h2><p><strong>Authors:Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, Chengjie Wang</strong></p>
<p>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively. </p>
<blockquote>
<p>é¢éƒ¨æ›¿æ¢æ—¨åœ¨ç”Ÿæˆç»“åˆæºèº«ä»½çš„ä¸ç›®æ ‡å±æ€§ç»“æœã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„é¢éƒ¨æ›¿æ¢ä¸Šã€‚åœ¨å¤„ç†è§†é¢‘æ—¶ï¼Œæ¯ä¸€å¸§éƒ½æ˜¯ç‹¬ç«‹å¤„ç†çš„ï¼Œå¾ˆéš¾ä¿è¯æ—¶é—´ç¨³å®šæ€§ã€‚ä»æ¨¡å‹è§’åº¦çœ‹ï¼Œé¢éƒ¨æ›¿æ¢æ­£é€æ¸ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå› ä¸ºDMså·²æ˜¾ç¤ºå‡ºå…·æœ‰æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚å½“å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å›¾åƒä¿®å¤æŠ€æœ¯ï¼Œè¿™åœ¨ä¿ç•™å…‰ç…§å’Œå¦†å®¹ç­‰ç²¾ç»†å±æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜ä¿çœŸè§†é¢‘é¢éƒ¨æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—æ€§å­¦ä¹ æå–èº«ä»½æ— å…³å’Œç²¾ç»†çš„å±æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è¯¦ç»†çš„èº«ä»½æ³¨å…¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘é¢éƒ¨æ›¿æ¢æ–¹é¢æ— è®ºæ˜¯åœ¨å®šæ€§è¿˜æ˜¯å®šé‡æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18293v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„é¢éƒ¨åˆ†å‰²æŠ€æœ¯ï¼Œæ­£é€æ¸å‘è§†é¢‘å½¢å¼è½¬ç§»ã€‚ç”±äºæ‰©æ•£æ¨¡å‹æ‹¥æœ‰æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é‡‡ç”¨å›¾åƒåŒ–çš„é¢éƒ¨æ•°æ®æ›¿æ¢æ–¹æ³•ï¼Œé¢ä¸´åŠ¨æ€æ—¶é—´ä¸‹çš„è¿è´¯æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†æå‡ç»†èŠ‚å“è´¨å’Œå¤„ç†å±æ€§ä¿¡æ¯çš„ç¨³å®šæ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç¨³å®šè§†é¢‘æ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸè§†é¢‘é¢éƒ¨æ›¿æ¢æ¡†æ¶ï¼ˆHiFiVFSï¼‰ï¼Œå®ƒèåˆäº†èº«ä»½ç‰¹å¾è¯†åˆ«ä¸ç²¾ç»†å±æ€§ç‰¹å¾åˆ†ç¦»ç­‰èƒ½åŠ›ã€‚æœ¬æ–‡çš„è§£å†³æ–¹æ¡ˆè¿˜åŒ…æ‹¬å¯¹é¢éƒ¨ä¿¡æ¯çš„è¯¦å°½åˆ†æä»¥åŠå¦‚ä½•å®ç°ä¸ªæ€§åŒ–ä¸æ€§èƒ½åŒæ­¥çš„åˆ›æ–°ç‚¹ç­‰å…³é”®æŠ€æœ¯æ‰‹æ®µï¼Œæœ¬æ–‡æ‰€æå‡ºçš„æ–¹æ³•åœ¨è§†é¢‘é¢éƒ¨æ›¿æ¢ä¸Šè¾¾åˆ°äº†è¡Œä¸šé¢†å…ˆçš„æˆæ•ˆã€‚ç®€å•æ¦‚æ‹¬ä¸ºæœ¬æ–‡å¼€å‘äº†ä¸€ç§åˆ›æ–°çš„é¢éƒ¨åˆ†å‰²æŠ€æœ¯ï¼ˆHiFiVFSï¼‰ï¼Œèƒ½åœ¨ä¿ç•™ç‰¹å¾çš„åŸºç¡€ä¸ŠæˆåŠŸåˆ‡æ¢è„¸éƒ¨è§†é¢‘æºçš„è®¾å®šä¸»ä½“å’Œå…·ä½“æƒ…å¢ƒçš„åŒæ—¶è¿›è¡ŒæŒç»­çš„åŒ¹é…å’Œè§†é¢‘ç»´æŒå¤„ç†ä»¥ä¿æŒä¸€è‡´æ€§ä¸è´¨é‡ç­‰ç»¼åˆèƒ½åŠ›çš„ä¸€ç§ç»¼åˆæ‰‹æ®µï¼Œä»ä¸ªäººè§’åº¦æ¥çœ‹ä¸»è¦æœåŠ¡äºå®Œæˆæ•ˆæœè‡³ä¸Šçš„åŒ¹é…æœ€ä¼˜ä¸ç†æƒ³çš„æœ€å¼ºåŒ–æ•´åˆç”Ÿæˆåœºæ™¯èƒ½åŠ›ç­‰ç­‰ç»¼åˆæ€§ç›®æ ‡çš„å®ç°ã€‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘é¢éƒ¨åˆ†å‰²æŠ€æœ¯é¢ä¸´æ—¶é—´è¿è´¯æ€§çš„æŒ‘æˆ˜ï¼Œå½“å‰è§£å†³æ–¹æ¡ˆå¤šèšç„¦äºå›¾åƒå¤„ç†æ¨¡å¼ç¼ºä¹é•¿ä¹…æ—¶æ•ˆè¿ç»­æ€§è€ƒè™‘ï¼›æ­¤æ–¹é¢éš¾ç‚¹å°†è¢«è®ºæ–‡çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆä¼˜åŒ–æå‡ä»¥ä¿æŒç¨³å®šä¸ºè€ƒé‡å‡ºå‘ç‚¹é€æ­¥è½å®ï¼Œæ­¤ä¸ºé‡ä¸­ä¹‹é‡ä¹Ÿæ˜¯ç¬¬ä¸€ç‚¹çš„æ˜¾è‘—å½±å“æ•ˆåº”æ–¹å‘ç‚¹ï¼Œæ„å»ºæ›´å…·å…¼å®¹æ€§ä¸æ³›åŒ–èƒ½åŠ›å¹³è¡¡çš„ä¼˜ç§€é¢éƒ¨åˆ†å‰²ä½“ç³»å¹³å°æ¥é«˜æ•ˆååŒç›®æ ‡æ ¸å¿ƒè¡¨ç°å…·æœ‰ä»£è¡¨æ€§ä¸å¯æ›¿ä»£æ€§ä»¥åŠèƒ½å¤§åŠ›æ¿€å‘è‡ªä¸»åˆä½œæœåŠ¡éƒ¨ç½²å‚ä¸ç›®æ ‡çš„æ¦‚ç‡è¿›åº¦è®¾è®¡ä¿ƒæˆå®Œå¤‡æ ¼å±€çš„ç†è®ºæ°´å¹³è®¤è¯†å»ºæ„çš„åŸºçŸ³ã€‚åœ¨åˆ›æ–°æŠ€æœ¯ä¸­èåˆä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ˜¯å½“ä¸‹ç ”ç©¶è¶‹åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-122dfa5cb8ad5ef604f88cde5b275d81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-31457317ca85d09e8d2b9d29ee530829.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-52192381a3ec2d89bb7e63e4b4659e73.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ac370c44fad37f8bd34147d044b76d0a.jpg" align="middle">
</details>




<h2 id="Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks"><a href="#Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks" class="headerlink" title="Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks"></a>Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks</h2><p><strong>Authors:Srinitish Srinivasan, Varenya Pathak, Abirami S</strong></p>
<p>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem. </p>
<blockquote>
<p>æŠ½è±¡è‰ºæœ¯æ˜¯ä¸€ç§å¹¿å—æ¬¢è¿å’Œè®¨è®ºçš„è‰ºæœ¯å½¢å¼ï¼Œé€šå¸¸èƒ½å¤Ÿæç»˜è‰ºæœ¯å®¶çš„æƒ…æ„Ÿã€‚è®¸å¤šç ”ç©¶äººå‘˜å·²ç»å°è¯•ä»¥è¾¹ç¼˜æ£€æµ‹ã€ç¬”è§¦å’Œæƒ…ç»ªè¯†åˆ«ç®—æ³•çš„å½¢å¼ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¥ç ”ç©¶æŠ½è±¡è‰ºæœ¯ã€‚æœ¬æ–‡æè¿°äº†ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œï¼ˆGANï¼‰å¯¹å¤§é‡æŠ½è±¡ç»˜ç”»ä½œå“çš„ç ”ç©¶ã€‚GANå…·æœ‰å­¦ä¹ å’Œå†ç°åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œä½¿ç ”ç©¶è€…å’Œç§‘å­¦å®¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢å’Œç ”ç©¶ç”Ÿæˆçš„å›¾åƒç©ºé—´ã€‚ç„¶è€Œï¼ŒæŒ‘æˆ˜åœ¨äºå¼€å‘ä¸€ç§æœ‰æ•ˆçš„GANæ¶æ„ï¼Œèƒ½å¤Ÿå…‹æœå¸¸è§çš„è®­ç»ƒé™·é˜±ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ç§ä¸“é—¨ç”¨äºé«˜è´¨é‡è‰ºæœ¯ä½œå“ç”Ÿæˆçš„æ”¹è¿›å‹DCGANï¼ˆmDCGANï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•æ¶‰åŠå¯¹æ‰€åšçš„ä¿®æ”¹è¿›è¡Œå½»åº•æ¢ç´¢ï¼Œæ·±å…¥ç ”ç©¶DCGANçš„ç²¾ç»†å·¥ä½œåŸç†ã€ä¼˜åŒ–æŠ€æœ¯å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‰ºæœ¯åˆ›ä½œçš„ç¨³å®šæ€§å’Œé€¼çœŸåº¦ï¼Œä½¿ç”Ÿæˆçš„æ¨¡å¼ç ”ç©¶æœ‰æ•ˆã€‚æ‰€æå‡ºçš„mDCGANå¯¹å±‚é…ç½®å’Œæ¶æ„é€‰æ‹©è¿›è¡Œäº†ç»†è‡´è°ƒæ•´ï¼Œä¸ºè‰ºæœ¯åˆ›ä½œçš„ç‹¬ç‰¹éœ€æ±‚æä¾›é‡èº«å®šåˆ¶çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æœ‰æ•ˆè§£å†³äº†æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡éšæœºæ¼«æ­¥æ¢ç´¢ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œäº†è§£æŠ½è±¡è‰ºæœ¯ç©ºé—´ä¸­ç¬”è§¦å’Œé¢œè‰²ä¹‹é—´çš„å‘é‡å…³ç³»ï¼Œä»¥åŠå¯¹GANè®­ç»ƒä¸€æ®µæ—¶é—´åä¸ç¨³å®šè¾“å‡ºçš„ç»Ÿè®¡åˆ†æï¼Œå¹¶æ¯”è¾ƒå…¶æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å‘ç°éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨æ•°å­—è‰ºæœ¯ç”Ÿæˆå’Œæ•°å­—è‰ºæœ¯ç”Ÿæ€ç³»ç»Ÿé¢†åŸŸé©å‘½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18397v2">PDF</a> Accepted for publication by Intelligent Decision Technologies</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œï¼ˆGANï¼‰å¯¹æŠ½è±¡ç”»ä½œè¿›è¡Œäº†å¹¿æ³›åˆ†å¸ƒçš„ç ”ç©¶ã€‚GANså…·æœ‰å­¦ä¹ å’Œå¤åˆ¶åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œç§‘å­¦å®¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢å’Œç ”ç©¶ç”Ÿæˆçš„å›¾åƒç©ºé—´ã€‚ç ”ç©¶ä¸­çš„æŒ‘æˆ˜åœ¨äºå¼€å‘ä¸€ç§æœ‰æ•ˆçš„GANæ¶æ„ï¼Œä»¥å…‹æœå¸¸è§çš„è®­ç»ƒéš¾é¢˜ã€‚æœ¬æ–‡è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ–¹æ³•æ˜¯é€šè¿‡å¼•å…¥ä¸€ç§é’ˆå¯¹é«˜è´¨é‡è‰ºæœ¯å“ç”Ÿæˆçš„æ”¹è¿›å‹DCGANï¼ˆmDCGANï¼‰ã€‚è¯¥æ–¹æ³•æ·±å…¥æ¢è®¨äº†æ‰€åšçš„ä¿®æ”¹ï¼Œè¯¦ç»†ç ”ç©¶äº†DCGANçš„å·¥ä½œåŸç†ã€ä¼˜åŒ–æŠ€æœ¯å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ—¨åœ¨æé«˜è‰ºæœ¯å“ç”Ÿæˆçš„ç¨³å®šæ€§å’Œé€¼çœŸåº¦ï¼Œä»è€Œå®ç°å¯¹ç”Ÿæˆæ¨¡å¼çš„æœ‰æ•ˆç ”ç©¶ã€‚æå‡ºçš„mDCGANåœ¨å±‚é…ç½®å’Œå»ºç­‘é€‰æ‹©æ–¹é¢è¿›è¡Œäº†ç»†è‡´è°ƒæ•´ï¼Œä¸ºè‰ºæœ¯ç”Ÿæˆæä¾›äº†é‡èº«å®šåˆ¶çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æœ‰æ•ˆåœ°è§£å†³äº†æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡éšæœºæ¸¸èµ°çš„æ–¹å¼æ¢ç´¢äº†ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œäº†è§£äº†æŠ½è±¡è‰ºæœ¯ç©ºé—´ä¸­ç¬”è§¦å’Œé¢œè‰²ä¹‹é—´çš„å‘é‡å…³ç³»ï¼Œå¹¶å¯¹GANè®­ç»ƒä¸€å®šæ—¶é—´åä¸ç¨³å®šè¾“å‡ºçš„ç»Ÿè®¡å·®å¼‚è¿›è¡Œäº†åˆ†ææ¯”è¾ƒã€‚è¿™äº›å‘ç°éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨æ•°å­—è‰ºæœ¯ç”Ÿæˆå’Œæ•°å­—è‰ºæœ¯ç”Ÿæ€ç³»ç»Ÿé¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç¥ç»ç½‘ç»œï¼ˆGANï¼‰ç ”ç©¶æŠ½è±¡ç”»ä½œåˆ†å¸ƒã€‚</li>
<li>mDCGANçš„è®¾è®¡ä¸“é—¨ç”¨äºé«˜è´¨é‡è‰ºæœ¯å“ç”Ÿæˆï¼Œé€šè¿‡å¯¹DCGANçš„æ”¹è¿›å…‹æœå¸¸è§è®­ç»ƒéš¾é¢˜ã€‚</li>
<li>mDCGANåœ¨å±‚é…ç½®å’Œå»ºç­‘é€‰æ‹©ä¸Šçš„ç»†è‡´è°ƒæ•´ï¼Œä¸ºè‰ºæœ¯ç”Ÿæˆæä¾›äº†é‡èº«å®šåˆ¶çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>mDCGANèƒ½å¤Ÿè§£å†³æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ï¼Œæé«˜è‰ºæœ¯å“ç”Ÿæˆçš„ç¨³å®šæ€§å’Œé€¼çœŸåº¦ã€‚</li>
<li>é€šè¿‡éšæœºæ¸¸èµ°æ¢ç´¢ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œç†è§£æŠ½è±¡è‰ºæœ¯ç©ºé—´ä¸­ç¬”è§¦å’Œé¢œè‰²çš„å‘é‡å…³ç³»ã€‚</li>
<li>å¯¹GANè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šè¾“å‡ºè¿›è¡Œç»Ÿè®¡å·®å¼‚åˆ†æã€‚</li>
<li>ç ”ç©¶ç»“æœéªŒè¯äº†mDCGANæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ•°å­—è‰ºæœ¯ç”Ÿæˆé¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0c86fb3b63917812a573d399ded69ef3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-785ec5270ebb6ad154707406e7a5ac8a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-64858c227d4fe7e862156750ec3605f9.jpg" align="middle">
</details>




<h2 id="Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training"><a href="#Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training" class="headerlink" title="Representation Learning of Multivariate Time Series using Attention and   Adversarial Training"></a>Representation Learning of Multivariate Time Series using Attention and   Adversarial Training</h2><p><strong>Authors:Leon ScharwÃ¤chter, Sebastian Otte</strong></p>
<p>A critical factor in trustworthy machine learning is to develop robust representations of the training data. Only under this guarantee methods are legitimate to artificially generate data, for example, to counteract imbalanced datasets or provide counterfactual explanations for blackbox decision-making systems. In recent years, Generative Adversarial Networks (GANs) have shown considerable results in forming stable representations and generating realistic data. While many applications focus on generating image data, less effort has been made in generating time series data, especially multivariate signals. In this work, a Transformer-based autoencoder is proposed that is regularized using an adversarial training scheme to generate artificial multivariate time series signals. The representation is evaluated using t-SNE visualizations, Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the generated signals exhibit higher similarity to an exemplary dataset than using a convolutional network approach. </p>
<blockquote>
<p>åœ¨å¯ä¿¡æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®å› ç´ æ˜¯å¼€å‘ç¨³å¥çš„è®­ç»ƒæ•°æ®è¡¨ç¤ºã€‚åªæœ‰åœ¨è¿™ä¸ªä¿è¯ä¸‹ï¼Œäººå·¥ç”Ÿæˆæ•°æ®çš„æ–¹æ³•æ‰æ˜¯åˆæ³•çš„ï¼Œä¾‹å¦‚ï¼Œå¯¹æŠ—ä¸å¹³è¡¡æ•°æ®é›†æˆ–ä¸ºé»‘ç®±å†³ç­–åˆ¶å®šç³»ç»Ÿæä¾›åäº‹å®è§£é‡Šã€‚è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨å½¢æˆç¨³å®šè¡¨ç¤ºå’Œç”ŸæˆçœŸå®æ•°æ®æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚è™½ç„¶è®¸å¤šåº”ç”¨éƒ½é›†ä¸­åœ¨ç”Ÿæˆå›¾åƒæ•°æ®ä¸Šï¼Œä½†åœ¨ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®æ–¹é¢æŠ•å…¥çš„åŠªåŠ›è¾ƒå°‘ï¼Œå°¤å…¶æ˜¯å¤šå…ƒä¿¡å·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªç¼–ç å™¨ï¼Œé€šè¿‡ä½¿ç”¨å¯¹æŠ—æ€§è®­ç»ƒæ–¹æ¡ˆè¿›è¡Œæ­£åˆ™åŒ–æ¥ç”Ÿæˆäººå·¥å¤šå…ƒæ—¶é—´åºåˆ—ä¿¡å·ã€‚é€šè¿‡t-SNEå¯è§†åŒ–ã€åŠ¨æ€æ—¶é—´æ‰­æ›²ï¼ˆDTWï¼‰å’Œç†µè¯„åˆ†æ¥è¯„ä¼°è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸å·ç§¯ç½‘ç»œæ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„ä¿¡å·ä¸ç¤ºä¾‹æ•°æ®é›†å…·æœ‰æ›´é«˜çš„ç›¸ä¼¼æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01987v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­å¯é è¡¨ç¤ºè®­ç»ƒæ•°æ®çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºåªæœ‰åœ¨ç¡®ä¿æ•°æ®å¯é çš„å‰æä¸‹ï¼Œæ‰èƒ½åˆæ³•åœ°ç”Ÿæˆäººå·¥æ•°æ®ã€‚æ–‡ç« åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç”Ÿæˆç¨³å®šçš„æ•°æ®è¡¨ç¤ºå’ŒçœŸå®æ•°æ®ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®æ–¹é¢çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªç¼–ç å™¨ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒç­–ç•¥ç”Ÿæˆäººå·¥å¤šå…ƒæ—¶é—´åºåˆ—ä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•çš„ç”Ÿæˆä¿¡å·ä¸æ ·æœ¬æ•°æ®é›†ç›¸ä¼¼åº¦æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨å½¢æˆç¨³å®šçš„æ•°æ®è¡¨ç¤ºå’Œç”ŸæˆçœŸå®æ•°æ®æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚</li>
<li>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯é çš„æ•°æ®è¡¨ç¤ºæ˜¯ä¸€ä¸ªå…³é”®è¦ç´ ï¼Œæ˜¯ç”Ÿæˆäººå·¥æ•°æ®çš„å‰æã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªç¼–ç å™¨ï¼Œç”¨äºç”Ÿæˆäººå·¥å¤šå…ƒæ—¶é—´åºåˆ—ä¿¡å·ã€‚</li>
<li>è¯¥è‡ªç¼–ç å™¨é€šè¿‡å¯¹æŠ—è®­ç»ƒç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ä¿¡å·ä¸æ ·æœ¬æ•°æ®é›†ç›¸ä¼¼åº¦æ›´é«˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨äº†t-SNEå¯è§†åŒ–ã€åŠ¨æ€æ—¶é—´å¼¯æ›²ï¼ˆDTWï¼‰å’Œç†µå€¼ç­‰æ–¹æ³•æ¥è¯„ä¼°è¡¨ç¤ºæ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9b941c35f07b9443c2d8562fcd05075d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0feafb45ee0492776796259b551baad5.jpg" align="middle">
</details>




<h2 id="GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks"><a href="#GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks" class="headerlink" title="GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks"></a>GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks</h2><p><strong>Authors:Mehran Hosseini, Peyman Hosseini</strong></p>
<p>The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative n-dimensional Cartesian coordinate system. We show this drastically improves quality of images generated by Diffusion Models, GANs, and Variational AutoEncoders (VAE). </p>
<blockquote>
<p>è¿‘åå¹´æ¥ï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨é‡ç°å¤æ‚å‡ ä½•ç‰¹å¾ï¼ˆå¦‚äººç±»çš„æ‰‹å’Œæ‰‹æŒ‡ç‰¹å¾ï¼‰æ–¹é¢çš„æŒç»­æ— èƒ½ä¸€ç›´æ˜¯å›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªæŒç»­å­˜åœ¨çš„é—®é¢˜ã€‚å°½ç®¡é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡å’Œå¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†è¿™ä¸€é—®é¢˜åœ¨åŒ…æ‹¬é™å™ªæ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨å†…çš„æ‰€æœ‰æ¨¡å‹ä¸­ä»ç„¶æ™®éå­˜åœ¨ï¼Œè¿™è¡¨æ˜åŸºç¡€æ¶æ„å­˜åœ¨æ ¹æœ¬æ€§ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¢å¼ºå·ç§¯å±‚çš„å‡ ä½•èƒ½åŠ›æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–¹æ³•æ˜¯æä¾›ä¸€ä¸ªåŒ…å«ç›¸å¯¹nç»´ç¬›å¡å°”åæ ‡ç³»ç»Ÿçš„å•ä¸€è¾“å…¥é€šé“ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™å¯ä»¥æå¤§åœ°æé«˜æ‰©æ•£æ¨¡å‹ã€GANå’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01951v2">PDF</a> Accepted at WACV 2025. Contains 19 pages, 15 figures, and 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨é‡å»ºå¤æ‚å‡ ä½•ç‰¹å¾ï¼ˆå¦‚äººæ‰‹å’Œæ‰‹æŒ‡ï¼‰æ–¹é¢å­˜åœ¨é•¿æœŸéš¾é¢˜ï¼Œå°½ç®¡é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡å’Œå¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†è¿™ä¸€é—®é¢˜åœ¨å„ç±»æ¨¡å‹ä¸­ä¾ç„¶æ™®éå­˜åœ¨ã€‚æœ¬æ–‡æå‡ºé€šè¿‡å¢åŠ ä¸€ä¸ªåŒ…å«ç›¸å¯¹nç»´ç¬›å¡å°”åæ ‡ç³»ç»Ÿçš„å•ä¸€è¾“å…¥é€šé“æ¥å¢å¼ºå·ç§¯å±‚çš„å‡ ä½•èƒ½åŠ›ï¼Œä»è€Œç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œå¹¶æ˜¾è‘—æé«˜æ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨é‡å»ºå¤æ‚å‡ ä½•ç‰¹å¾æ–¹é¢å­˜åœ¨éš¾é¢˜ï¼Œè¿™ä¸€é—®é¢˜å·²æŒç»­è¿‘åå¹´ã€‚</li>
<li>å„ç±»æ¨¡å‹ï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨éƒ½é¢ä¸´è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é—®é¢˜æ ¹æºå¯èƒ½åœ¨äºç°æœ‰æ¨¡å‹æ¶æ„çš„å›ºæœ‰ç¼ºé™·ã€‚</li>
<li>é€šè¿‡å¢åŠ ä¸€ä¸ªåŒ…å«ç›¸å¯¹nç»´ç¬›å¡å°”åæ ‡ç³»ç»Ÿçš„å•ä¸€è¾“å…¥é€šé“ï¼Œå¯ä»¥å¢å¼ºæ¨¡å‹çš„å‡ ä½•å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½æ˜¾è‘—æé«˜å›¾åƒç”Ÿæˆè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å‡ ä½•ç‰¹å¾çš„é‡å»ºæ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­é•¿æœŸå­˜åœ¨çš„é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2a0e59b4dbd04d0d8521d495c872ebae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-38274ed949f889a70e7b31e7e6019427.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8daa63128009486f6689a0da6a5320cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-115a2a4082a9d7e1fef8a75282c4c1be.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd81a303d0434e5c85c0015a53779dc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-27aad86691678ae5aeaeb9b61eeb1637.jpg" align="middle">
</details>




<h2 id="One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN"><a href="#One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN" class="headerlink" title="One-for-All: Towards Universal Domain Translation with a Single StyleGAN"></a>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h2><p><strong>Authors:Yong Du, Jiahui Zhan, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, Shengfeng He</strong></p>
<p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the StyleGANâ€™s latent space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç¿»è¯‘æ¨¡å‹UniTranslatorï¼Œç”¨äºåœ¨è®­ç»ƒæ•°æ®æœ‰é™å’Œè§†è§‰å·®å¼‚æ˜¾è‘—çš„æƒ…å†µä¸‹ï¼Œå®ç°ä¸åŒè§†è§‰é¢†åŸŸä¹‹é—´çš„è¡¨ç¤ºè½¬æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨CLIPçš„ä¸­ç«‹åŸŸèƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„æ¨¡å—ä»æºé¢†åŸŸå’Œç›®æ ‡é¢†åŸŸçš„åµŒå…¥ä¸­æå–æŠ½è±¡ã€é¢†åŸŸæ— å…³è¯­ä¹‰ã€‚å°†è¿™äº›æŠ½è±¡è¯­ä¹‰ä¸ç›®æ ‡ç‰¹å®šè¯­ä¹‰èåˆï¼Œå¾—åˆ°CLIPç©ºé—´å†…çš„è½¬æ¢åµŒå…¥ã€‚ä¸ºäº†å¼¥åˆCLIPå’ŒStyleGANä¹‹é—´æˆªç„¶ä¸åŒçš„ä¸–ç•Œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„éçº¿æ€§æ˜ å°„å™¨â€”â€”CLIP2Pæ˜ å°„å™¨ã€‚è¯¥æ¨¡å—åˆ©ç”¨CLIPåµŒå…¥è¿›è¡Œå®šåˆ¶ï¼Œä»¥è¿‘ä¼¼StyleGANæ½œåœ¨ç©ºé—´ä¸­çš„æ½œåœ¨åˆ†å¸ƒï¼Œæœ‰æ•ˆåœ°ä½œä¸ºè¿™ä¸¤ä¸ªç©ºé—´ä¹‹é—´çš„è¿æ¥å™¨ã€‚æ‰€æå‡ºçš„UniTranslatoré€šç”¨æ€§å¼ºï¼Œèƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ï¼Œå³ä½¿åœ¨è·¨è¶Šä¸åŒè§†è§‰é¢†åŸŸçš„è§†è§‰æŒ‘æˆ˜åœºæ™¯ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUniTranslatorç”Ÿæˆçš„é«˜è´¨é‡ç¿»è¯‘å±•ç¤ºäº†é¢†åŸŸç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œæ”¹è¿›çš„å›¾åƒè´¨é‡ã€‚UniTranslatorè¶…è¶Šäº†ç°æœ‰é€šç”¨æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³è¶…è¿‡äº†ä¸“ä¸šæ¨¡å‹ã€‚æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å°†å‘å…¬ä¼—å¼€æ”¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14222v2">PDF</a> </p>
<p><strong>Summary</strong><br>UniTranslatoræ¨¡å‹åˆ©ç”¨CLIPçš„è·¨åŸŸä¸­æ€§èƒ½åŠ›ï¼Œé€šè¿‡èåˆæºåŸŸå’Œç›®æ ‡åŸŸåµŒå…¥çš„æŠ½è±¡è¯­ä¹‰ï¼Œå®ç°åœ¨æœ‰é™è®­ç»ƒæ•°æ®å’Œæ˜¾è‘—è§†è§‰å·®å¼‚æ¡ä»¶ä¸‹ä¸åŒè§†è§‰åŸŸä¹‹é—´çš„è¡¨ç¤ºè½¬æ¢ã€‚å¼•å…¥CLIP2Péçº¿æ€§æ˜ å°„å™¨ï¼Œç¼©å°CLIPå’ŒStyleGANä¹‹é—´çš„å·®è·ã€‚UniTranslatoråŠŸèƒ½å¼ºå¤§ï¼Œèƒ½åœ¨ä¸åŒè§†è§‰é¢†åŸŸæ‰§è¡Œé£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ç­‰ä»»åŠ¡ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ç›¸å…³æ€§å¼ºã€å¤šæ ·åŒ–çš„å›¾åƒã€‚å…¶æ€§èƒ½è¶…è¶Šç°æœ‰é€šç”¨æ¨¡å‹å¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTranslatoræ¨¡å‹è¢«æå‡ºç”¨äºåœ¨æœ‰é™è®­ç»ƒæ•°æ®å’Œæ˜¾è‘—è§†è§‰å·®å¼‚æ¡ä»¶ä¸‹å®ç°ä¸åŒè§†è§‰åŸŸä¹‹é—´çš„è¡¨ç¤ºè½¬æ¢ã€‚</li>
<li>åˆ©ç”¨CLIPçš„è·¨åŸŸä¸­æ€§èƒ½åŠ›ä½œä¸ºæ¡¥æ¢æœºåˆ¶ã€‚</li>
<li>å¼•å…¥CLIP2Péçº¿æ€§æ˜ å°„å™¨ä»¥è¿æ¥CLIPå’ŒStyleGANä¸¤ä¸ªç©ºé—´ã€‚</li>
<li>UniTranslatoræ¨¡å‹åŠŸèƒ½å¼ºå¤§ï¼Œå¯æ‰§è¡Œé£æ ¼æ··åˆã€é£æ ¼åŒ–å’Œç¿»è¯‘ç­‰ä»»åŠ¡ã€‚</li>
<li>UniTranslatorç”Ÿæˆçš„ç¿»è¯‘å…·æœ‰é«˜è´¨é‡ã€ç›¸å…³æ€§å¼ºå’Œå¤šæ ·åŒ–çš„ç‰¹ç‚¹ã€‚</li>
<li>UniTranslatoræ€§èƒ½è¶…è¶Šç°æœ‰é€šç”¨æ¨¡å‹ï¼Œå¹¶åœ¨ä»£è¡¨æ€§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-95e6272f85d7fa4d37d1ae29b4643ac3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7a24b3f85d591a3f1544bb1f8be5a6ea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0f2ec04d706bea45231594a7de44808b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-32e26e6d642dd74ec5491b260c7f5d69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6ed3350cdfd555c1320fc865348c6003.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b5fcd9c09d6cdabfa03da9bbb7f05557.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>å¯¹äºå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ï¼ˆOCTAï¼‰å›¾åƒï¼Œæœ‰é™çš„æ‰«æé€Ÿç‡å¯¼è‡´è§†é‡ï¼ˆFOVï¼‰ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚è™½ç„¶è¾ƒå¤§çš„FOVå›¾åƒå¯èƒ½ä¼šæ­ç¤ºæ›´å¤šçš„æ—é»„æ–‘è¡€ç®¡ç—…å˜ï¼Œä½†ç”±äºåˆ†è¾¨ç‡è¾ƒä½ï¼Œå…¶åº”ç”¨å—åˆ°å¾ˆå¤§é˜»ç¢ã€‚ä¸ºäº†æé«˜åˆ†è¾¨ç‡ï¼Œä»¥å‰çš„å·¥ä½œåªæœ‰åœ¨ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶æ‰èƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œä½†ç°å®ä¸–ç•Œçš„åº”ç”¨å—åˆ°æ”¶é›†å¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æŒ‘æˆ˜çš„é™åˆ¶ã€‚å› æ­¤ï¼Œéå¸¸éœ€è¦ä¸€ç§æ— éœ€é…å¯¹çš„æ–¹æ³•ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å·²åœ¨æ— éœ€é…å¯¹çš„æƒ…å†µä¸‹å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒå¯èƒ½éš¾ä»¥å‡†ç¡®ä¿ç•™æ¯›ç»†è¡€ç®¡çš„ç»†ç²’åº¦ç»†èŠ‚ï¼Œè¿™å¯¹äºOCTAæ¥è¯´æ˜¯éå¸¸é‡è¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æœ¬æ–‡çš„æ–¹æ³•æ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢‘ç‡ä¿¡æ¯æ¥ä¿ç•™è¿™äº›ç»†èŠ‚ï¼Œå°†ç»†èŠ‚è§†ä¸ºé«˜é¢‘ï¼ˆhfï¼‰ï¼Œå°†ç²—ç²’åº¦èƒŒæ™¯è§†ä¸ºä½é¢‘ï¼ˆlfï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— éœ€é…å¯¹çš„OCTAå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå¹¶é€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨ç‰¹åˆ«å¼ºè°ƒäº†hfç»†æ¯›ç»†è¡€ç®¡ã€‚ä¸ºäº†ä¿ƒè¿›é‡å»ºå›¾åƒçš„ç²¾ç¡®å…‰è°±ï¼Œæˆ‘ä»¬è¿˜ä¸ºé‰´åˆ«å™¨æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æ€§æŸå¤±ï¼Œå¹¶å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±æ¥è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰ä¸Šå‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ— éœ€é…å¯¹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å…‰å­¦ç›¸å¹²å±‚æè¡€ç®¡é€ å½±æœ¯ï¼ˆOCTAï¼‰å›¾åƒçš„ç‰¹ç‚¹ï¼Œæ‰«æç‡é™åˆ¶å¯¼è‡´äº†è§†é‡ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå¼ºè°ƒé«˜é¢‘ä¿¡æ¯å¯¹ä¿ç•™æ¯›ç»†è¡€ç®¡ç»†èŠ‚çš„é‡è¦æ€§ï¼Œå¹¶é‡‡ç”¨åŒè·¯å¾„ç”Ÿæˆå™¨è¿›è¡Œç²¾ç»†æ¯›ç»†è¡€ç®¡çš„é‡å»ºã€‚åŒæ—¶ï¼Œå¼•å…¥é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä¼˜åŒ–å›¾åƒé¢‘è°±å’Œç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰è¡¨ç°ä¸Šå‡ä¼˜äºå…¶ä»–é¡¶å°–çš„æ— é…å¯¹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCTAå›¾åƒå­˜åœ¨æ‰«æç‡ä¸è§†é‡åˆ†è¾¨ç‡çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒä»¥æé«˜åˆ†è¾¨ç‡ï¼Œä½†å®é™…åº”ç”¨å—é™äºå¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æ”¶é›†éš¾åº¦ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ç”¨äºOCTAå›¾åƒã€‚</li>
<li>é‡è§†é«˜é¢‘ä¿¡æ¯ä»¥ä¿ç•™æ¯›ç»†è¡€ç®¡ç»†èŠ‚ï¼Œä½¿ç”¨åŒè·¯å¾„ç”Ÿæˆå™¨è¿›è¡Œé‡å»ºã€‚</li>
<li>å¼•å…¥é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±ï¼Œä¼˜åŒ–å›¾åƒé¢‘è°±çš„é‡å»ºã€‚</li>
<li>æå‡ºé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ï¼Œå®ç°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>




<h2 id="Diverse-Similarity-Encoder-for-Deep-GAN-Inversion"><a href="#Diverse-Similarity-Encoder-for-Deep-GAN-Inversion" class="headerlink" title="Diverse Similarity Encoder for Deep GAN Inversion"></a>Diverse Similarity Encoder for Deep GAN Inversion</h2><p><strong>Authors:Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</strong></p>
<p>Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN. </p>
<blockquote>
<p>å½“å‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å¯ä»¥åˆæˆé«˜è´¨é‡ï¼ˆHQï¼‰å›¾åƒï¼Œå› æ­¤ä½¿ç”¨GANså­¦ä¹ è¡¨ç¤ºæ˜¯æœ‰åˆ©çš„ã€‚GANåæ¼”æ˜¯æ–°å…´æ–¹æ³•ä¹‹ä¸€ï¼Œç ”ç©¶å¦‚ä½•å°†å›¾åƒåæ¼”åˆ°æ½œåœ¨ç©ºé—´ã€‚ç°æœ‰çš„GANç¼–ç å™¨å¯ä»¥åœ¨StyleGANä¸Šè¿›è¡Œå›¾åƒåæ¼”ï¼Œä½†ä¸èƒ½é€‚åº”å…¶ä»–æ·±åº¦GANsã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¸­çš„ä¸åŒç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ç¼–ç å™¨ï¼Œå‘½åä¸ºå¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰ï¼Œå¯ä»¥æ‰©å±•åˆ°å„ç§æœ€å…ˆè¿›çš„GANsã€‚DSEä½¿GANsèƒ½å¤Ÿä»HQå›¾åƒé‡å»ºå‡ºæ›´é«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œæ— è®ºæ˜¯åˆæˆçš„è¿˜æ˜¯çœŸå®çš„å›¾åƒã€‚DSEå…·æœ‰ç»Ÿä¸€çš„å·ç§¯å—ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°é€‚åº”ä¸»æµæ·±åº¦GANsï¼Œä¾‹å¦‚PGGANã€StyleGANå’ŒBigGANã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.10201v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç°æœ‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ–°çš„GANåå·ç§¯æ–¹æ³•ï¼Œå³å¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§ï¼Œå¯ä»¥æ‰©å±•åˆ°å„ç§å…ˆè¿›çš„GANsã€‚DSEèƒ½å¤Ÿé‡æ„å‡ºé«˜è´¨é‡çš„çœŸå®å’Œåˆæˆå›¾åƒã€‚å®ƒé€‚ç”¨äºä¸»æµçš„æ·±åº¦GANsï¼Œå¦‚PGGANã€StyleGANå’ŒBigGANã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰èƒ½å¤Ÿåˆæˆé«˜è´¨é‡å›¾åƒï¼Œå­¦ä¹ è¡¨ç¤ºæˆä¸ºçƒ­é—¨ç ”ç©¶ä¸»é¢˜ã€‚</li>
<li>GANåå·ç§¯æ˜¯æ–°å…´çš„ç ”ç©¶æ–¹å‘ä¹‹ä¸€ï¼Œæ—¨åœ¨ç ”ç©¶å¦‚ä½•å°†å›¾åƒåè½¬å›æ½œåœ¨ç©ºé—´ã€‚</li>
<li>ç°æœ‰çš„GANç¼–ç å™¨å¯ä»¥åœ¨StyleGANä¸Šè¿›è¡Œå›¾åƒåå·ç§¯ï¼Œä½†æ— æ³•é€‚åº”å…¶ä»–æ·±åº¦GANsã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¤šæ ·ç›¸ä¼¼æ€§ç¼–ç å™¨ï¼ˆDSEï¼‰ï¼Œå¯ä»¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>DSEé€šè¿‡è¯„ä¼°æ½œåœ¨å‘é‡å’Œå›¾åƒä¹‹é—´çš„ä¸åŒç›¸ä¼¼æ€§è¿›è¡Œè®¾è®¡ã€‚</li>
<li>DSEå¯åº”ç”¨äºå„ç§å…ˆè¿›çš„GANsï¼Œå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2c0940fc7f9e47f7ef709ff59469c9b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26229e3100c8c64811dfe13fae729ff8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-991864f280fa90ecd2fc3e0b5b8d9de9.jpg" align="middle">

</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_å…ƒå®‡å®™_è™šæ‹Ÿäºº/2412.04955v2/page_2_0.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  GASP Gaussian Avatars with Synthetic Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Face Swapping/2412.07260v1/page_0_0.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  SegFace Face Segmentation of Long-Tail Classes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
