<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN 方向最新论文已更新，请持续关注 Update in 2024-12-12  Utilizing Multi-step Loss for Single Image Reflection Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    53 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal"><a href="#Utilizing-Multi-step-Loss-for-Single-Image-Reflection-Removal" class="headerlink" title="Utilizing Multi-step Loss for Single Image Reflection Removal"></a>Utilizing Multi-step Loss for Single Image Reflection Removal</h2><p><strong>Authors:Abdelrahman Elnenaey, Marwan Torki</strong></p>
<p>Image reflection removal is crucial for restoring image quality. Distorted images can negatively impact tasks like object detection and image segmentation. In this paper, we present a novel approach for image reflection removal using a single image. Instead of focusing on model architecture, we introduce a new training technique that can be generalized to image-to-image problems, with input and output being similar in nature. This technique is embodied in our multi-step loss mechanism, which has proven effective in the reflection removal task. Additionally, we address the scarcity of reflection removal training data by synthesizing a high-quality, non-linear synthetic dataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances the model’s ability to learn better patterns for reflection removal. We also utilize a ranged depth map, extracted from the depth estimation of the ambient image, as an auxiliary feature, leveraging its property of lacking depth estimations for reflections. Our approach demonstrates superior performance on the SIR^2 benchmark and other real-world datasets, proving its effectiveness by outperforming other state-of-the-art models. </p>
<blockquote>
<p>图像反射去除对于恢复图像质量至关重要。扭曲的图像会对目标检测和图像分割等任务产生负面影响。在本文中，我们提出了一种使用单幅图像进行图像反射去除的新方法。我们没有关注模型架构，而是引入了一种可推广至图像到图像问题的新训练技术，输入和输出的性质相似。这种技术体现在我们的多步损失机制中，在反射去除任务中证明了其有效性。此外，我们通过对抗生成网络（Pix2Pix GAN）合成了一种高质量的非线性合成数据集RefGan，解决了反射去除训练数据的稀缺问题。该数据集显著提高了模型学习反射去除的更好模式的能力。我们还使用从环境图像的深度估计中提取的范围深度图作为辅助特征，利用其缺乏反射深度估计的特性。我们的方法在SIR^2基准和其他真实世界数据集上表现出卓越的性能，证明了其优于其他最新模型的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08582v1">PDF</a> 6 pages, 6 figures, IEEE ICASSP 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种利用单图像进行图像反射去除的新方法。该方法引入了一种新的训练技术，可广泛应用于同类图像转换问题。通过多步骤损失机制，该方法在反射去除任务中证明了其有效性。为解决反射去除训练数据不足的问题，研究团队还利用Pix2Pix GAN合成了一个高质量的非线性合成数据集RefGAN。此外，该研究还利用从环境图像的深度估计中提取的深度图作为辅助特征，利用其缺乏反射的特性。该方法在SIR^2基准测试和其他真实世界数据集上表现出卓越的性能，超越了其他最先进的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的训练技术，该技术可广泛应用于同类图像转换问题。</li>
<li>通过多步骤损失机制，提高了反射去除的效果。</li>
<li>利用Pix2Pix GAN合成了一个高质量的非线性合成数据集RefGAN，解决了反射去除训练数据不足的问题。</li>
<li>利用从环境图像的深度估计中提取的深度图作为辅助特征，提高了反射去除的准确度。</li>
<li>该研究在SIR^2基准测试和其他真实世界数据集上进行了实验验证，证明了其方法的优越性。</li>
<li>该方法超越了其他最先进的模型，为图像反射去除领域提供了新的解决方案。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-90610239ae13c51e1614f127958f2a81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bde3fb0cbe989335b0d5bb7a4d46e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b852c3cdd4ace9f1729fdab0540434.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc27e6721c2ab163bba45905a25f88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab1d495bb0788c654b34111e35dbd40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f69c1d9eb0ee3bcc36085da2e8bd9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9175c1aae7fa57889e8641ff09cdbe6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f9871afd6e42b8e1cadb0f3e2faa7fc.jpg" align="middle">
</details>




<h2 id="Fine-grained-Text-to-Image-Synthesis"><a href="#Fine-grained-Text-to-Image-Synthesis" class="headerlink" title="Fine-grained Text to Image Synthesis"></a>Fine-grained Text to Image Synthesis</h2><p><strong>Authors:Xu Ouyang, Ying Chen, Kaiyue Zhu, Gady Agam</strong></p>
<p>Fine-grained text to image synthesis involves generating images from texts that belong to different categories. In contrast to general text to image synthesis, in fine-grained synthesis there is high similarity between images of different subclasses, and there may be linguistic discrepancy among texts describing the same image. Recent Generative Adversarial Networks (GAN), such as the Recurrent Affine Transformation (RAT) GAN model, are able to synthesize clear and realistic images from texts. However, GAN models ignore fine-grained level information. In this paper we propose an approach that incorporates an auxiliary classifier in the discriminator and a contrastive learning method to improve the accuracy of fine-grained details in images synthesized by RAT GAN. The auxiliary classifier helps the discriminator classify the class of images, and helps the generator synthesize more accurate fine-grained images. The contrastive learning method minimizes the similarity between images from different subclasses and maximizes the similarity between images from the same subclass. We evaluate on several state-of-the-art methods on the commonly used CUB-200-2011 bird dataset and Oxford-102 flower dataset, and demonstrated superior performance. </p>
<blockquote>
<p>细粒度文本到图像合成涉及从不同类别的文本生成图像。与一般的文本到图像合成相比，在细粒度合成中，不同子类的图像之间存在高相似性，描述同一图像的文本之间也可能存在语言差异。最近的生成对抗网络（GAN），如循环仿射变换（RAT）GAN模型，能够从文本中合成清晰和现实的图像。然而，GAN模型忽略了细粒度级别的信息。在本文中，我们提出了一种方法，该方法在鉴别器中融入辅助分类器，并采用对比学习方法，以提高由RAT GAN合成的图像的细粒度细节的准确性。辅助分类器帮助鉴别器对图像进行分类，并帮助生成器合成更准确的细粒度图像。对比学习方法最小化不同子类图像之间的相似性，并最大化同一子类图像之间的相似性。我们在常用的CUB-200-2011鸟类数据集和Oxford-102花卉数据集上评估了最先进的方法，并展示了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07196v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本到图像合成技术能够根据不同的文本生成对应的图像。但在精细粒度的文本到图像合成中，不同子类别的图像间高度相似，描述相同图像的文本可能存在语言差异。为提高合成图像的精细粒度细节准确性，本文提出在判别器中加入辅助分类器并采用对比学习方法。辅助分类器帮助判别器对图像进行分类，同时帮助生成器生成更准确的精细粒度图像。对比学习方法则减少不同子类图像间的相似性并增加相同子类图像间的相似性。在常用的CUB-200-2011鸟类数据集和Oxford-102花卉数据集上，本文方法表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像合成技术可生成与文本相对应的图像。</li>
<li>精细粒度文本到图像合成面临不同子类图像高度相似的问题。</li>
<li>辅助分类器的加入有助于提高判别器的分类能力，进而提升生成图像的精细粒度细节准确性。</li>
<li>对比学习方法用于减少不同子类图像间的相似性，并增加相同子类图像间的相似性。</li>
<li>在常用的数据集上，本文提出的方法表现优越。</li>
<li>辅助分类器同时帮助生成器生成更准确的精细粒度图像。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9ddabde15480b7c735a4b219c5e32d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7185a0f87e292154d753c68ebd946585.jpg" align="middle">
</details>




<h2 id="Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks"><a href="#Creative-Portraiture-Exploring-Creative-Adversarial-Networks-and-Conditional-Creative-Adversarial-Networks" class="headerlink" title="Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks"></a>Creative Portraiture: Exploring Creative Adversarial Networks and   Conditional Creative Adversarial Networks</h2><p><strong>Authors:Sebastian Hereu, Qianfei Hu</strong></p>
<p>Convolutional neural networks (CNNs) have been combined with generative adversarial networks (GANs) to create deep convolutional generative adversarial networks (DCGANs) with great success. DCGANs have been used for generating images and videos from creative domains such as fashion design and painting. A common critique of the use of DCGANs in creative applications is that they are limited in their ability to generate creative products because the generator simply learns to copy the training distribution. We explore an extension of DCGANs, creative adversarial networks (CANs). Using CANs, we generate novel, creative portraits, using the WikiArt dataset to train the network. Moreover, we introduce our extension of CANs, conditional creative adversarial networks (CCANs), and demonstrate their potential to generate creative portraits conditioned on a style label. We argue that generating products that are conditioned, or inspired, on a style label closely emulates real creative processes in which humans produce imaginative work that is still rooted in previous styles. </p>
<blockquote>
<p>卷积神经网络（CNNs）与生成对抗网络（GANs）的成功结合产生了深度卷积生成对抗网络（DCGANs）。DCGANs已被用于从时尚设计和绘画等创意领域生成图像和视频。使用DCGANs在创意应用中的常见批评是，它们在生成创意产品方面的能力有限，因为生成器只是学习复制训练分布。我们探索了DCGANs的扩展版本——创意对抗网络（CANs）。我们使用CANs生成新型创意肖像，并使用WikiArt数据集来训练网络。此外，我们还介绍了CANs的扩展版本——条件创意对抗网络（CCANs），并展示了它们根据风格标签生成创意肖像的潜力。我们认为，生成基于风格标签或受其启发而产生的产品紧密模拟了真正的创意过程，人类在这种过程中会创作出根植于先前风格但又富有想象力的作品。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07091v1">PDF</a> </p>
<p><strong>Summary</strong>：卷积神经网络（CNN）与生成对抗网络（GAN）结合创建了深度卷积生成对抗网络（DCGAN），在时尚设计等领域取得了巨大成功。但DCGAN在创造性应用上被批评为局限于生成训练分布的产品。为此，我们探索了DCGAN的扩展版本——创意对抗网络（CAN），并使用WikiArt数据集生成新颖创意肖像。此外，我们还介绍了CAN的扩展版本——条件创意对抗网络（CCAN），并展示了它们根据风格标签生成创意肖像的潜力。我们认为，生成受风格标签影响或启发的产品更接近人类的真实创意过程。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>DCGAN已成功应用于图像和视频生成，尤其在时尚设计和绘画等创意领域。</li>
<li>DCGAN的一个常见批评是其生成创意产品的能力受限，仅学习复制训练分布。</li>
<li>为解决此问题，提出了创意对抗网络（CAN），使用WikiArt数据集生成新颖创意肖像。</li>
<li>CAN的扩展版本——条件创意对抗网络（CCAN）能够根据风格标签生成创意肖像。</li>
<li>CCAN的潜力展示了它在模拟人类真实创意过程方面的优势，即生成受风格标签影响或启发的产品。</li>
<li>CCAN的应用可能推动创意领域的发展，尤其是在需要根据特定风格或要求生成内容的情况下。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f449af10b6b724d393aa486988a72232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57fcb2b2735bf7bdf971d6bd4f6e43c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8c0d0fa86b8dc9c60493948af361fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef0866153897d19d7ed04ee03ef0461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-919534e13329a643d8d5a59a53ea98b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0460b23720fe9a0735ecd7c632e2a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89a8d2483234fcf74b6af7fc50907d24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-260ae46b1080cb4b32ea46d808541992.jpg" align="middle">
</details>




<h2 id="HiFiVFS-High-Fidelity-Video-Face-Swapping"><a href="#HiFiVFS-High-Fidelity-Video-Face-Swapping" class="headerlink" title="HiFiVFS: High Fidelity Video Face Swapping"></a>HiFiVFS: High Fidelity Video Face Swapping</h2><p><strong>Authors:Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, Chengjie Wang</strong></p>
<p>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively. </p>
<blockquote>
<p>面部替换旨在生成结合源身份和目标属性的结果。现有方法主要集中在基于图像的面部替换上。在处理视频时，每一帧都是独立处理的，很难保证时间稳定性。从模型的角度来看，面部替换正逐渐从生成对抗网络（GANs）转向扩散模型（DMs），因为DMs显示出更强的生成能力。当前的基于扩散的方法通常采用修复技术，这在保留光照和妆容等精细属性方面存在困难。为了应对这些挑战，我们提出了高保真视频面部替换（HiFiVFS）框架，它利用稳定视频扩散（SVD）的强大生成能力和时间先验。我们构建了一个精细属性模块，通过身份脱敏和对抗性学习提取身份分离和精细属性特征。此外，我们还引入了详细的身份注入，以进一步增强身份相似性。大量实验表明，我们的方法在视频面部替换中实现了定性和定量上的业界最佳水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18293v2">PDF</a> </p>
<p><strong>Summary</strong><br>     基于稳定视频扩散模型（SVD）的高保真视频人脸替换框架被提出，它利用扩散模型的强大生成能力和时间先验知识来解决视频人脸替换中的精细属性保留和时序稳定性问题。通过身份脱敏和对抗学习，构建了精细属性模块来提取身份分离和精细属性特征。此外，引入了详细的身份注入技术，进一步增强身份相似性。实验证明，该方法在视频人脸替换方面达到了定性和定量上的最佳效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频人脸替换面临时序稳定性和精细属性保留的挑战。</li>
<li>现有方法主要关注图像中的人脸替换，处理视频时独立处理每一帧，难以保证时序稳定性。</li>
<li>扩散模型（DMs）具有强大的生成能力，正逐渐应用于人脸替换领域。</li>
<li>提出的HiFiVFS框架利用稳定视频扩散（SVD）的生成能力和时间先验来解决这些问题。</li>
<li>构建了一个精细属性模块，通过身份脱敏和对抗学习提取身份分离和精细属性特征。</li>
<li>引入详细身份注入技术，进一步增强身份相似性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-122dfa5cb8ad5ef604f88cde5b275d81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31457317ca85d09e8d2b9d29ee530829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52192381a3ec2d89bb7e63e4b4659e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac370c44fad37f8bd34147d044b76d0a.jpg" align="middle">
</details>




<h2 id="Leveraging-generative-models-to-characterize-the-failure-conditions-of-image-classifiers"><a href="#Leveraging-generative-models-to-characterize-the-failure-conditions-of-image-classifiers" class="headerlink" title="Leveraging generative models to characterize the failure conditions of   image classifiers"></a>Leveraging generative models to characterize the failure conditions of   image classifiers</h2><p><strong>Authors:Adrien LeCoz, Stéphane Herbin, Faouzi Adjed</strong></p>
<p>We address in this work the question of identifying the failure conditions of a given image classifier. To do so, we exploit the capacity of producing controllable distributions of high quality image data made available by recent Generative Adversarial Networks (StyleGAN2): the failure conditions are expressed as directions of strong performance degradation in the generative model latent space. This strategy of analysis is used to discover corner cases that combine multiple sources of corruption, and to compare in more details the behavior of different classifiers. The directions of degradation can also be rendered visually by generating data for better interpretability. Some degradations such as image quality can affect all classes, whereas other ones such as shape are more class-specific. The approach is demonstrated on the MNIST dataset that has been completed by two sources of corruption: noise and blur, and shows a promising way to better understand and control the risks of exploiting Artificial Intelligence components for safety-critical applications. </p>
<blockquote>
<p>在这项工作中，我们解决了识别给定图像分类器失效条件的问题。为此，我们利用最近出现的生成对抗网络（StyleGAN2）产生的高质量图像数据的可控分布能力：失效条件表现为生成模型潜在空间中的性能严重下降方向。这种分析策略用于发现结合多种腐败来源的极端情况，并更详细地比较不同分类器的行为。通过生成数据在视觉上呈现退化方向，以提高其可解释性。一些退化（如图像质量）可能影响所有类别，而其他一些退化（如形状）则更特定于类别。该方法在MNIST数据集上进行了演示，该数据集通过噪声和模糊两种来源进行了补充，并显示了一种更好地理解和控制将人工智能组件用于安全关键应用的风险的有前途的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12814v2">PDF</a> </p>
<p><strong>Summary</strong><br>本文探讨了通过利用最近出现的生成对抗网络（StyleGAN2）产生的可控高质量图像数据集来识别给定图像分类器的失败条件。失败条件被表达为生成模型潜在空间中性能严重退化的方向。这种分析策略用于发现结合多种腐败来源的极端情况，并更详细地比较不同分类器的行为。通过生成数据呈现退化方向以提高可解释性。一些全局退化（如图像质量）会影响所有类别，而其他类别特定退化（如形状）则更具类别特异性。在MNIST数据集上演示了该方法，该数据集通过噪声和模糊两种来源完成，展示了一种更好地理解和控制利用人工智能组件进行安全关键应用风险的有前途的方式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用StyleGAN2生成可控的高质量图像数据集来识别图像分类器的失败条件。</li>
<li>失败条件被表达为生成模型潜在空间中性能退化的方向。</li>
<li>通过分析策略发现结合多种腐败来源的极端情况。</li>
<li>退化方向可以通过生成数据可视化，以提高解释性。</li>
<li>一些退化影响所有类别（如图像质量），而其他退化更具类别特异性（如形状）。</li>
<li>在MNIST数据集上演示了该方法，结合了噪声和模糊两种数据来源。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d85af4b9068c955ba01b2437d7f09bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc9d3164f985b6a36503f08a51f6d3a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3cd3af68c93bbc93e88462d1c9c6463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0dae7878cbbd1c2946bfb1041104e37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65b53c01638382906023fde0150eed61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8777a04096c03f4dad850dca2cb890.jpg" align="middle">
</details>




<h2 id="Detecting-Unforeseen-Data-Properties-with-Diffusion-Autoencoder-Embeddings-using-Spine-MRI-data"><a href="#Detecting-Unforeseen-Data-Properties-with-Diffusion-Autoencoder-Embeddings-using-Spine-MRI-data" class="headerlink" title="Detecting Unforeseen Data Properties with Diffusion Autoencoder   Embeddings using Spine MRI data"></a>Detecting Unforeseen Data Properties with Diffusion Autoencoder   Embeddings using Spine MRI data</h2><p><strong>Authors:Robert Graf, Florian Hunecke, Soeren Pohl, Matan Atad, Hendrik Moeller, Sophie Starck, Thomas Kroencke, Stefanie Bette, Fabian Bamberg, Tobias Pischon, Thoralf Niendorf, Carsten Schmidt, Johannes C. Paetzold, Daniel Rueckert, Jan S Kirschke</strong></p>
<p>Deep learning has made significant strides in medical imaging, leveraging the use of large datasets to improve diagnostics and prognostics. However, large datasets often come with inherent errors through subject selection and acquisition. In this paper, we investigate the use of Diffusion Autoencoder (DAE) embeddings for uncovering and understanding data characteristics and biases, including biases for protected variables like sex and data abnormalities indicative of unwanted protocol variations. We use sagittal T2-weighted magnetic resonance (MR) images of the neck, chest, and lumbar region from 11186 German National Cohort (NAKO) participants. We compare DAE embeddings with existing generative models like StyleGAN and Variational Autoencoder. Evaluations on a large-scale dataset consisting of sagittal T2-weighted MR images of three spine regions show that DAE embeddings effectively separate protected variables such as sex and age. Furthermore, we used t-SNE visualization to identify unwanted variations in imaging protocols, revealing differences in head positioning. Our embedding can identify samples where a sex predictor will have issues learning the correct sex. Our findings highlight the potential of using advanced embedding techniques like DAEs to detect data quality issues and biases in medical imaging datasets. Identifying such hidden relations can enhance the reliability and fairness of deep learning models in healthcare applications, ultimately improving patient care and outcomes. </p>
<blockquote>
<p>深度学习在医学成像领域取得了重大进展，利用大规模数据集来提高诊断和预后。然而，大规模数据集往往由于受试者选择和采集而带有内在错误。本文研究了使用扩散自编码器（DAE）嵌入来揭示和理解数据特征和偏见，包括性别等保护变量的偏见以及指示不需要的协议变化的异常数据。我们使用来自德国国家队列研究（NAKO）的参与者颈部、胸部和腰椎区域的矢状面T2加权磁共振（MR）图像。我们将DAE嵌入与现有的生成模型（如StyleGAN和变分自编码器）进行比较。对包含三个脊椎区域矢状面T2加权MR图像的大规模数据集的评估显示，DAE嵌入可以有效地分离保护变量，如性别和年龄。此外，我们还使用t-SNE可视化来识别成像协议中的不需要的变化，揭示头部定位的差异。我们的嵌入可以识别性别预测器在学习正确性别时会遇到问题的一些样本。我们的研究结果强调了使用先进的嵌入技术（如DAEs）检测医学成像数据集中的数据质量问题和偏见的潜力。识别这些隐藏关系可以增强深度学习模型在医疗保健应用中的可靠性和公平性，最终改善患者护理和结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10220v1">PDF</a> This paper was accepted in the “Workshop on Interpretability of   Machine Intelligence in Medical Image Computing” (iMIMIC) at MICCAI 2024</p>
<p><strong>Summary</strong><br>     本研究利用扩散自编码器（DAE）嵌入技术，探究在医学成像中揭示和理解数据特性和偏见的方法，特别是对保护变量（如性别）和数据异常值的识别。通过对比StyleGAN和变分自编码器等现有生成模型，DAE嵌入在大型数据集上表现出优异性能，能有效区分保护变量，如性别和年龄。此外，该研究还利用t-SNE可视化技术识别成像协议中的意外变化，揭示了头部定位的差异。本研究揭示了使用高级嵌入技术如DAEs检测医学成像数据集数据质量问题和偏见的潜力，有助于增强深度学习模型在医疗应用中的可靠性和公平性，最终改善患者护理和结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散自编码器（DAE）嵌入技术用于医学成像数据的分析和理解。</li>
<li>DAE嵌入能够有效区分保护变量，如性别和年龄。</li>
<li>与现有生成模型（如StyleGAN和变分自编码器）相比，DAE嵌入在大型医学图像数据集上表现出更好的性能。</li>
<li>利用t-SNE可视化技术揭示了成像协议中的意外变化。</li>
<li>DAE嵌入能够识别数据质量问题和偏见，有助于增强深度学习模型在医疗应用中的可靠性和公平性。</li>
<li>该研究对于提高患者护理和结果具有潜在意义。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-25cf2ec6902b5245f5875b8203f7e3cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c68accc95789b495887d32e311de1da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6ad2e5dfc74a642062244006a2851a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-169cc315f5012b4372cdbf3507a2c439.jpg" align="middle">
</details>




<h2 id="Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks"><a href="#Colour-and-Brush-Stroke-Pattern-Recognition-in-Abstract-Art-using-Modified-Deep-Convolutional-Generative-Adversarial-Networks" class="headerlink" title="Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks"></a>Colour and Brush Stroke Pattern Recognition in Abstract Art using   Modified Deep Convolutional Generative Adversarial Networks</h2><p><strong>Authors:Srinitish Srinivasan, Varenya Pathak, Abirami S</strong></p>
<p>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem. </p>
<blockquote>
<p>摘要艺术是一种非常流行且备受讨论的艺术形式，通常能够描绘出艺术家的情感。许多研究人员已经尝试使用机器学习和深度学习进行边缘检测、笔触和情感识别算法来研究抽象艺术。本文描述了一种使用生成对抗神经网络（GAN）研究广泛分布的抽象绘画的研究。GAN具有学习和复制分布的能力，使研究者和科学家能够有效地探索和研究生成的图像空间。然而，挑战在于开发一种有效的GAN架构，能够克服常见的训练陷阱。本文通过引入一种针对高质量艺术作品生成的改进型DCGAN（mDCGAN）来解决这一挑战。该方法涉及对所做的修改的彻底探索，深入研究DCGAN的精细工作原理、优化技术和正则化方法，旨在提高艺术生成的稳定性和真实性，从而有效研究生成的图案。所提出的mDCGAN对层配置和架构选择进行了精细调整，为艺术生成提供了量身定制的解决方案，同时有效地解决了模式崩溃和梯度消失等问题。此外，本文通过执行随机游走探索生成的潜在空间，了解抽象艺术空间中笔触和颜色之间的向量关系，并分析了GAN训练某段时间后不稳定输出的统计数据及其显著差异。这些发现验证了所提出方法的有效性，并强调其在数字艺术生成和数字艺术生态系统领域中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18397v2">PDF</a> Accepted for publication by Intelligent Decision Technologies</p>
<p><strong>摘要</strong><br>    本研究利用生成对抗神经网络（GAN）对抽象绘画进行广泛分布的研究。GANs具有学习和复制分布的能力，使研究者和科学家能够有效地探索和研究生成的图像空间。研究针对开发高效GAN架构以克服常见训练难题的挑战，提出了一种改进的DCGAN（mDCGAN），专门用于高质量艺术作品生成。该方法深入探讨了所做的修改，探讨了DCGANs的精细工作原理、优化技术和正则化方法，旨在提高艺术生成的稳定性和逼真性，从而有效地研究生成的图案。mDCGAN在层配置和架构选择方面进行了精细调整，为艺术生成的独特需求提供量身定制的解决方案，同时有效解决模式崩溃和梯度消失等问题。此外，本文还通过随机游走的方式探索了生成的潜在空间，理解抽象艺术空间中笔触和颜色之间的向量关系，并对GAN训练某一段时间后不稳定输出的统计结果进行分析和比较，验证了所提方法的有效性，强调了其在数字艺术生成和数字艺术生态系统领域的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用生成对抗神经网络（GAN）研究抽象绘画的分布。</li>
<li>提出了改进的DCGAN（mDCGAN）架构，用于高质量的艺术作品生成。</li>
<li>mDCGAN在层配置和架构选择上的精细调整，满足了艺术生成的独特需求。</li>
<li>mDCGAN解决了模式崩溃和梯度消失等问题，提高了艺术生成的稳定性和逼真性。</li>
<li>通过随机游走探索生成的潜在空间，理解抽象艺术空间中笔触和颜色之间的向量关系。</li>
<li>对GAN训练过程中的不稳定输出进行统计分析和比较。</li>
<li>验证了所提方法的有效性，并强调了其在数字艺术生成和数字艺术生态系统领域的潜力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c86fb3b63917812a573d399ded69ef3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-785ec5270ebb6ad154707406e7a5ac8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64858c227d4fe7e862156750ec3605f9.jpg" align="middle">
</details>




<h2 id="Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training"><a href="#Representation-Learning-of-Multivariate-Time-Series-using-Attention-and-Adversarial-Training" class="headerlink" title="Representation Learning of Multivariate Time Series using Attention and   Adversarial Training"></a>Representation Learning of Multivariate Time Series using Attention and   Adversarial Training</h2><p><strong>Authors:Leon Scharwächter, Sebastian Otte</strong></p>
<p>A critical factor in trustworthy machine learning is to develop robust representations of the training data. Only under this guarantee methods are legitimate to artificially generate data, for example, to counteract imbalanced datasets or provide counterfactual explanations for blackbox decision-making systems. In recent years, Generative Adversarial Networks (GANs) have shown considerable results in forming stable representations and generating realistic data. While many applications focus on generating image data, less effort has been made in generating time series data, especially multivariate signals. In this work, a Transformer-based autoencoder is proposed that is regularized using an adversarial training scheme to generate artificial multivariate time series signals. The representation is evaluated using t-SNE visualizations, Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the generated signals exhibit higher similarity to an exemplary dataset than using a convolutional network approach. </p>
<blockquote>
<p>在可信机器学习中的关键因素是开发稳健的训练数据表示。只有在这个保证下，人工生成数据的方法才是合法的，例如，对抗不平衡数据集或为黑箱决策制定系统提供反事实解释。近年来，生成对抗网络（GANs）在形成稳定表示和生成真实数据方面取得了显著成果。虽然许多应用都集中在生成图像数据上，但在生成时间序列数据方面所付出的努力较少，尤其是多元信号。在这项工作中，提出了一种基于Transformer的自编码器，通过使用对抗训练方案进行正则化来生成人工多元时间序列信号。通过t-SNE可视化、动态时间弯曲（DTW）和熵分数来评估表示方法。我们的结果表明，生成的信号与样本数据集相比，使用卷积网络方法具有更高的相似性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01987v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文强调机器学习中的稳健表示法的重要性，特别是对于生成的数据而言。通过利用生成对抗网络（GANs）技术，可以形成稳定的数据表示并生成真实的数据。尽管图像数据生成应用广泛，但在生成时间序列数据方面，尤其是多元信号生成方面的努力较少。本研究提出了一种基于Transformer的自编码器，采用对抗训练方案进行正则化，以生成人工多元时间序列信号。通过t-SNE可视化、动态时间弯曲（DTW）和熵评分对表示进行评估，结果表明生成的信号与样本数据集有较高的相似性，优于卷积网络方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习中的稳健表示法对于生成数据至关重要。</li>
<li>生成对抗网络（GANs）可用于形成稳定的数据表示并生成真实数据。</li>
<li>尽管图像数据生成应用广泛，但时间序列数据的生成，特别是多元信号的生成受到的关注较少。</li>
<li>研究提出了一种基于Transformer的自编码器，采用对抗训练方案进行正则化，以生成多元时间序列信号。</li>
<li>通过多种评估方法，包括t-SNE可视化、动态时间弯曲（DTW）和熵评分，验证了生成的信号的真实性及与样本数据的高相似性。</li>
<li>该方法优于使用卷积网络的方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b941c35f07b9443c2d8562fcd05075d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0feafb45ee0492776796259b551baad5.jpg" align="middle">
</details>




<h2 id="GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks"><a href="#GeoPos-A-Minimal-Positional-Encoding-for-Enhanced-Fine-Grained-Details-in-Image-Synthesis-Using-Convolutional-Neural-Networks" class="headerlink" title="GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks"></a>GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details   in Image Synthesis Using Convolutional Neural Networks</h2><p><strong>Authors:Mehran Hosseini, Peyman Hosseini</strong></p>
<p>The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative n-dimensional Cartesian coordinate system. We show this drastically improves quality of images generated by Diffusion Models, GANs, and Variational AutoEncoders (VAE). </p>
<blockquote>
<p>图像生成模型长期无法重现复杂几何特征，如人类的手和手指等特征，这一问题在图像生成领域已持续近十年。尽管通过增加模型规模和多样化训练数据集，已经取得了一些进展，但这一问题在包括降噪扩散模型、生成对抗网络（GAN）在内的所有模型中依然普遍存在，这表明现有架构存在根本性不足。在本文中，我们展示了如何通过增强卷积层的几何能力来解决这一问题，具体做法是为其提供一个包含相对n维笛卡尔坐标系统的单一输入通道。我们证明了这可以极大改善扩散模型、GAN和变分自编码器（VAE）生成的图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01951v2">PDF</a> Accepted at WACV 2025. Contains 19 pages, 15 figures, and 9 tables</p>
<p><strong>Summary</strong></p>
<p>本文探讨了图像生成模型在重建复杂几何特征（如人手和手指）方面的持续难题。尽管通过增大模型规模和多样化训练数据集已经取得了一些进展，但这一问题在包括去噪扩散模型、生成对抗网络（GAN）在内的所有模型中仍然普遍存在，这表明现有架构存在根本性缺陷。本文演示了如何通过为卷积层提供包含相对n维笛卡尔坐标系统的单一输入通道来增强其几何能力，从而缓解这一问题。此方法显著提高了扩散模型、GAN和变分自编码器（VAE）生成的图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成模型在重建复杂几何特征方面存在持续难题，尤其是人手和手指的细节。</li>
<li>现有模型，包括扩散模型、GAN和VAE，都面临这一问题。</li>
<li>问题指向现有模型架构的根本性短板。</li>
<li>通过为卷积层提供包含n维笛卡尔坐标系统的单一输入通道，可以增强其几何能力。</li>
<li>此方法有助于改善扩散模型、GAN和VAE生成的图像质量。</li>
<li>该策略为图像生成模型的进一步发展提供了新的方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a0e59b4dbd04d0d8521d495c872ebae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38274ed949f889a70e7b31e7e6019427.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8daa63128009486f6689a0da6a5320cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-115a2a4082a9d7e1fef8a75282c4c1be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd81a303d0434e5c85c0015a53779dc1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27aad86691678ae5aeaeb9b61eeb1637.jpg" align="middle">
</details>




<h2 id="HuRef-HUman-REadable-Fingerprint-for-Large-Language-Models"><a href="#HuRef-HUman-REadable-Fingerprint-for-Large-Language-Models" class="headerlink" title="HuRef: HUman-REadable Fingerprint for Large Language Models"></a>HuRef: HUman-REadable Fingerprint for Large Language Models</h2><p><strong>Authors:Boyi Zeng, Lizheng Wang, Yuncong Hu, Yi Xu, Chenghu Zhou, Xinbing Wang, Yu Yu, Zhouhan Lin</strong></p>
<p>Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without interfering with training or exposing model parameters to the public. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, with negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters’ direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM’s base model. Due to the potential risk of information leakage, we cannot publish invariant terms directly. Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image. In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners. To ensure the published fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP). Experimental results across various LLMs demonstrate the effectiveness of our method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LUMIA-Group/HuRef">https://github.com/LUMIA-Group/HuRef</a>. </p>
<blockquote>
<p>保护大型语言模型（LLM）的版权对于其资源密集型训练和精心设计的许可证变得至关重要。然而，由于潜在参数改动，确定LLM的原始基础模型具有挑战性。在这项研究中，我们介绍了HuRef，这是一种人类可读的LLM指纹，可以唯一地识别基础模型，而不会干扰训练或向公众暴露模型参数。我们首先观察到，在预训练收敛后，LLM参数的向量方向保持稳定，后续训练步骤（包括继续预训练、监督微调以及RLHF）引起的扰动微乎其微，这足以成为识别基础模型的充分条件。这一必要性的验证是通过继续训练LLM并添加额外项来驱使模型参数的方向变化，导致模型受损。然而，这个方向容易受到简单的攻击，如维度置换或矩阵旋转，这些攻击会显著改变方向而不会影响性能。为了解决这个问题，我们利用Transformer结构，系统地分析了潜在攻击并定义了三个不变术语来识别LLM的基础模型。由于潜在的信息泄露风险，我们无法直接发布不变术语。相反，我们将其映射到高斯向量并使用编码器，然后使用StyleGAN2将其转换为自然图像并最终发布。在我们的黑箱设置中，所有指纹打印步骤均由LLM所有者内部进行。为了确保发布的指纹是诚实生成的，我们引入了零知识证明（ZKP）。在不同LLM上的实验结果证明了我们的方法的有效性。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/LUMIA-Group/HuRef%E3%80%82">https://github.com/LUMIA-Group/HuRef。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.04828v4">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对大型语言模型（LLM）的新型版权保护方法——HuRef。该方法通过生成人类可读的指纹来唯一识别LLM的基础模型，且不会干扰训练或公开模型参数。研究观察到LLM参数向量方向在预训练收敛后保持稳定，因此可作为识别基础模型的充分条件。为应对潜在攻击，研究利用Transformer结构定义了三个不变术语，并通过编码器将其映射到高斯向量，再转换成自然图像进行发布。同时引入零知识证明（ZKP）确保发布的指纹真实有效。该方法在多种LLM上的实验结果表明其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的版权保护至关重要，因其在资源密集型的训练和精心设计的许可下。</li>
<li>HuRef作为一种新型版权保护方法，能够唯一识别LLM的基础模型。</li>
<li>LLM参数向量方向在预训练收敛后的稳定性为识别基础模型提供了依据。</li>
<li>HuRef通过生成人类可读的指纹，既不会干扰训练，也不会公开模型参数。</li>
<li>为应对潜在攻击，研究利用Transformer结构定义了三个不变术语进行识别。</li>
<li>发布指纹时，利用编码器将其映射到高斯向量，再转换成自然图像。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-793f284f974f5ec9502278f916a94f57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db507bbc24e35859401626f8caa35d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7a01743996810baf11b9032598bdb62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-625ba6b6d8ddb291215275637e732372.jpg" align="middle">
</details>




<h2 id="Feature-Extraction-for-Generative-Medical-Imaging-Evaluation-New-Evidence-Against-an-Evolving-Trend"><a href="#Feature-Extraction-for-Generative-Medical-Imaging-Evaluation-New-Evidence-Against-an-Evolving-Trend" class="headerlink" title="Feature Extraction for Generative Medical Imaging Evaluation: New   Evidence Against an Evolving Trend"></a>Feature Extraction for Generative Medical Imaging Evaluation: New   Evidence Against an Evolving Trend</h2><p><strong>Authors:McKell Woodland, Austin Castelo, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</strong></p>
<p>Fr&#39;echet Inception Distance (FID) is a widely used metric for assessing synthetic image quality. It relies on an ImageNet-based feature extractor, making its applicability to medical imaging unclear. A recent trend is to adapt FID to medical imaging through feature extractors trained on medical images. Our study challenges this practice by demonstrating that ImageNet-based extractors are more consistent and aligned with human judgment than their RadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across four medical imaging modalities and four data augmentation techniques with Fr&#39;echet distances (FDs) computed using eleven ImageNet or RadImageNet-trained feature extractors. Comparison with human judgment via visual Turing tests revealed that ImageNet-based extractors produced rankings consistent with human judgment, with the FD derived from the ImageNet-trained SwAV extractor significantly correlating with expert evaluations. In contrast, RadImageNet-based rankings were volatile and inconsistent with human judgment. Our findings challenge prevailing assumptions, providing novel evidence that medical image-trained feature extractors do not inherently improve FDs and can even compromise their reliability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/fid-med-eval">https://github.com/mckellwoodland/fid-med-eval</a>. </p>
<blockquote>
<p>Fréchet Inception Distance（FID）是评估合成图像质量的一个广泛使用的指标。它依赖于基于ImageNet的特征提取器，这使得其在医学影像中的应用不明确。最近的趋势是通过在医学影像上训练的特征提取器来适应FID。我们的研究通过证明基于ImageNet的提取器比RadImageNet对应的提取器在医学成像中的表现更为一致且与人类判断更吻合，挑战了这一做法。我们评估了四个医学成像模态和四种数据增强技术的十六个StyleGAN2网络，使用基于十一个ImageNet或RadImageNet训练的特征提取器计算Fréchet距离（FDs）。通过视觉图灵测试与人的判断进行比较显示，基于ImageNet的提取器产生的排名与人的判断一致，从ImageNet训练的SwAV提取器得出的FD与专家评价有着显著的相关性。相比之下，基于RadImageNet的排名波动性较大且与人类判断不一致。我们的研究结果挑战了现有的假设，提供了新的证据表明经过医学图像训练的特征提取器并不一定会提高FID，甚至可能损害其可靠性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/fid-med-eval%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mckellwoodland/fid-med-eval中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.13717v5">PDF</a> This preprint has not undergone peer review or any post-submission   improvements or corrections. The Version of Record of this contribution is   published in LNCS vol. 15012, and is available online at   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-031-72390-2_9">https://doi.org/10.1007/978-3-031-72390-2_9</a></p>
<p><strong>Summary</strong><br>     本研究挑战了将Fréchet Inception Distance（FID）评估指标应用于医学图像质量的常规做法。通过对十六个StyleGAN2网络在不同医学成像模式和不同数据增强技术上的评估发现，基于ImageNet的特征提取器与基于RadImageNet的特征提取器相比，更为一致且与人为判断对齐。采用ImageNet训练的SwAV特征提取器计算的Fréchet距离与人类专家评价的相关性更高，而基于RadImageNet的排名则表现出不稳定且与人为判断不一致。本研究对现有的假设提出挑战，提供新的证据表明，针对医学图像训练的特征提取器并不一定能提高FID的可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FID作为一种评估合成图像质量的指标，其适用性于医学成像领域尚不清楚。</li>
<li>目前有一种趋势是将FID适应于医学成像，通过基于医学图像的特征提取器进行训练。</li>
<li>本研究表明，基于ImageNet的特征提取器在评估医学图像质量时，相较于基于RadImageNet的特征提取器，具有更高的一致性和与人为判断的对齐度。</li>
<li>使用ImageNet训练的SwAV特征提取器计算的Fréchet距离与人类专家评价有较高的相关性。</li>
<li>基于RadImageNet的特征提取器在评估医学图像质量时表现出不稳定性和与人为判断的不一致性。</li>
<li>研究发现，针对医学图像训练的特征提取器并不一定能提高FID评估的可靠性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f74b3f8a5bb559564a3758396da5560a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c286e02c4f326d81759bd7159fbabd80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be000bc78815de10fc4233167cc51c58.jpg" align="middle">
</details>




<h2 id="One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN"><a href="#One-for-All-Towards-Universal-Domain-Translation-with-a-Single-StyleGAN" class="headerlink" title="One-for-All: Towards Universal Domain Translation with a Single StyleGAN"></a>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h2><p><strong>Authors:Yong Du, Jiahui Zhan, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, Shengfeng He</strong></p>
<p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the StyleGAN’s latent space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public. </p>
<blockquote>
<p>本文提出了一种新型翻译模型UniTranslator，用于在训练数据有限和视觉差异显著的情况下，实现不同视觉领域之间的表示转换。我们的方法的主要思想是利用CLIP的中立域能力作为桥梁机制，同时使用一个单独的模块从源领域和目标领域的嵌入中提取抽象、领域无关语义。将这些抽象语义与目标特定语义融合，得到CLIP空间内的转换嵌入。为了弥CLIP和StyleGAN之间不同世界的差距，我们引入了一种新的非线性映射器——CLIP2P映射器。该模块利用CLIP嵌入进行定制，以近似StyleGAN潜在空间中的潜在分布，有效地作为这两个空间之间的连接器。所提出的UniTranslator通用性强，能够执行各种任务，包括风格混合、风格化和翻译，即使在跨不同视觉领域的视觉挑战场景中也是如此。值得注意的是，UniTranslator生成的高质量翻译展示了领域相关性、多样性和改进的图像质量。UniTranslator超越了现有通用模型的性能，并在代表性任务中表现良好，甚至超过了专业模型。源代码和训练好的模型将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14222v2">PDF</a> </p>
<p><strong>Summary</strong><br>UniTranslator模型利用CLIP的跨域中性能力，通过融合源域和目标域的抽象语义，实现在有限训练数据和显著视觉差异条件下不同视觉领域间的表示转换。引入CLIP2P非线性映射器，在CLIP和StyleGAN之间建立桥梁，实现两者空间的有效连接。UniTranslator具有多种功能，包括风格混合、风格化和翻译等，可在不同视觉领域实现高质量翻译，展示领域相关性、多样性和改善的图像质量。该模型超越现有通用模型性能，并在代表性任务中表现良好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTranslator是一个新型翻译模型，能在有限训练数据和显著视觉差异条件下实现不同视觉领域间的表示转换。</li>
<li>利用CLIP的跨域中性能力作为桥梁机制，融合源域和目标域的抽象语义。</li>
<li>引入CLIP2P非线性映射器，连接CLIP和StyleGAN空间。</li>
<li>UniTranslator具备多种功能，如风格混合、风格化和翻译等。</li>
<li>UniTranslator能在视觉上具有挑战性的场景中生成高质量翻译。</li>
<li>UniTranslator模型性能超越现有通用模型，并在代表性任务中表现良好。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-95e6272f85d7fa4d37d1ae29b4643ac3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a24b3f85d591a3f1544bb1f8be5a6ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f2ec04d706bea45231594a7de44808b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32e26e6d642dd74ec5491b260c7f5d69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed3350cdfd555c1320fc865348c6003.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5fcd9c09d6cdabfa03da9bbb7f05557.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>对于光学相干断层扫描血管造影（OCTA）图像，有限的扫描速率导致视野（FOV）与成像分辨率之间的权衡。虽然较大的FOV图像可能会揭示更多的旁黄斑血管病变，但由于分辨率较低，其应用受到很大阻碍。为了提升分辨率，早期的研究工作只有在利用配对数据进行训练时才能达到令人满意的效果，但现实世界的实际应用受限于收集大规模配对图像的挑战。因此，对无配对方法的需求极高。生成对抗网络（GAN）在无需配对的环境中已经得到了广泛的应用，但它可能在精确保留精细毛细血管细节方面存在困难，这对于OCTA来说是非常关键的生物标记物。在本文中，我们的方法旨在通过利用频率信息来保留这些详细信息，其中高频代表细节，低频代表粗粒背景。总的来说，我们提出了一种基于GAN的无配对超分辨率方法用于OCTA图像，并通过双路径生成器特别强调了高频的精细毛细血管。为了促进重建图像的精确频谱，我们还为鉴别器提出了一种频率感知对抗性损失，并为端到端的优化引入了一种频率感知焦点一致性损失。实验表明，我们的方法在定量和视觉方面都优于其他先进的无配对方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于GAN的无配对超分辨率方法，用于处理OCTA图像。该方法利用频率信息，通过双路径生成器强调高频细节，并采用频率感知对抗损失和频率感知焦点一致性损失，提高了图像重建的精度和视觉效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCTA图像中，扫描率限制导致视野与成像分辨率之间的权衡。</li>
<li>较大的视野图像可能揭示更多的眼旁血管病变，但其分辨率较低。</li>
<li>以往的研究通过使用配对数据进行训练取得了令人满意的效果，但现实应用受限于收集大规模配对图像的挑战。</li>
<li>论文提出了一种基于GAN的无配对超分辨率方法，用于处理OCTA图像。</li>
<li>该方法利用频率信息，强调高频细节，并通过双路径生成器进行实现。</li>
<li>为了提高图像重建的精度，论文引入了频率感知对抗损失和频率感知焦点一致性损失。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>




<h2 id="Diverse-Similarity-Encoder-for-Deep-GAN-Inversion"><a href="#Diverse-Similarity-Encoder-for-Deep-GAN-Inversion" class="headerlink" title="Diverse Similarity Encoder for Deep GAN Inversion"></a>Diverse Similarity Encoder for Deep GAN Inversion</h2><p><strong>Authors:Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</strong></p>
<p>Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN. </p>
<blockquote>
<p>当前深度生成对抗网络（GANs）可以合成高质量（HQ）图像，因此使用GANs学习表示是有利的。GAN反演是新兴方法之一，研究如何将图像反演到潜在空间。现有的GAN编码器可以在StyleGAN上进行图像反演，但不能适应其他深度GANs。我们提出了一种新的方法来解决这个问题。通过评估潜在向量和图像中的不同相似性，我们设计了一种自适应编码器，命名为多样相似性编码器（DSE），可以扩展到各种最先进的GANs。DSE使GANs能够从HQ图像重建更高保真度的图像，无论是合成的还是真实的图像。DSE具有统一的卷积块，并能很好地适应主流深度GANs，例如PGGAN、StyleGAN和BigGAN。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.10201v3">PDF</a> </p>
<p><strong>Summary</strong><br>基于当前深度生成对抗网络（GANs）能合成高质量图像的优势，学习其表示方法变得非常有利。针对现有GAN编码器在StyleGAN上能够倒置图像，但无法适应其他深度GAN的问题，我们提出了一种新的方法来解决这个问题。通过评估潜在向量和图像之间的不同相似性，我们设计了一种名为多样相似性编码器（DSE）的适应性编码器，它可以扩展到各种最先进的GANs。DSE使GANs能够从高质量图像中重建更高保真度的图像，无论是合成的还是真实的图像。DSE具有统一的卷积块，并很好地适应主流深度GAN，例如PGGAN、StyleGAN和BigGAN。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前深度GAN能够生成高质量图像，促使研究如何借助GAN学习有效表示。</li>
<li>GAN倒置成为新兴方向，旨在将图像倒置回潜在空间。</li>
<li>现有GAN编码器主要适用于StyleGAN，但缺乏对其他深度GAN的适应性。</li>
<li>提出了名为DSE的多样相似性编码器，旨在解决此问题。</li>
<li>DSE通过评估潜在向量和图像之间的不同相似性进行设计。</li>
<li>DSE具有广泛的应用性，可扩展到多种最先进的GANs。</li>
<li>DSE能提高GANs从高质量图像重建图像的保真度，适用于合成和真实图像。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c0940fc7f9e47f7ef709ff59469c9b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26229e3100c8c64811dfe13fae729ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96c2092a822b7eb9c5a94f13ea3f45e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-991864f280fa90ecd2fc3e0b5b8d9de9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-849a0bfe560207f45ea80d5eb85b9501.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-04e472c619da5bae43ce50aa91f60b42.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-12  GASP Gaussian Avatars with Synthetic Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae7c1d5252a93267770b0f0d8e9bc329.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2024-12-12  SegFace Face Segmentation of Long-Tail Classes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">13316.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
