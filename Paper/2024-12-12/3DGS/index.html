<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-12  SLGaussian Fast Language Gaussian Splatting in Sparse Views">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_3DGS/2412.03844v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    56 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="SLGaussian-Fast-Language-Gaussian-Splatting-in-Sparse-Views"><a href="#SLGaussian-Fast-Language-Gaussian-Splatting-in-Sparse-Views" class="headerlink" title="SLGaussian: Fast Language Gaussian Splatting in Sparse Views"></a>SLGaussian: Fast Language Gaussian Splatting in Sparse Views</h2><p><strong>Authors:Kangjie Chen, BingQuan Dai, Minghan Qin, Dongbin Zhang, Peihao Li, Yingshuang Zou, Haoqian Wang</strong></p>
<p>3D semantic field learning is crucial for applications like autonomous navigation, AR&#x2F;VR, and robotics, where accurate comprehension of 3D scenes from limited viewpoints is essential. Existing methods struggle under sparse view conditions, relying on inefficient per-scene multi-view optimizations, which are impractical for many real-world tasks. To address this, we propose SLGaussian, a feed-forward method for constructing 3D semantic fields from sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring consistent SAM segmentations through video tracking and using low-dimensional indexing for high-dimensional CLIP features, SLGaussian efficiently embeds language information in 3D space, offering a robust solution for accurate 3D scene understanding under sparse view conditions. In experiments on two-view sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets, SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy, and mIoU. Moreover, our model achieves scene inference in under 30 seconds and open-vocabulary querying in just 0.011 seconds per query. </p>
<blockquote>
<p>三维语义场学习对于自主导航、AR&#x2F;VR和机器人等应用至关重要，这些应用需要准确理解从有限视角观察到的三维场景。现有方法在稀疏视角条件下表现不佳，依赖于不切实际的场景多视角优化，这在许多现实世界任务中并不实用。为了解决这个问题，我们提出了SLGaussian方法，这是一种前馈方法，可以从稀疏视角构建三维语义场，允许直接推断基于3DGS的场景。通过确保通过视频跟踪的一致SAM分割，并使用低维索引处理高维CLIP特征，SLGaussian有效地将语言信息嵌入三维空间，为稀疏视角条件下的准确三维场景理解提供了稳健的解决方案。在LERF和3D-OVS数据集上的两视角稀疏三维对象查询和分割实验中，SLGaussian在选择的IoU、定位精度和mIoU方面均优于现有方法。此外，我们的模型能够在不到30秒内完成场景推断，并且每个查询的开词汇查询时间仅为0.011秒。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08331v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对自主导航、AR&#x2F;VR和机器人等应用的3D语义场学习的重要性。针对现有方法在稀疏视点条件下的不足，提出了一种名为SLGaussian的前馈方法，用于从稀疏视点构建3D语义场，实现基于3DGS的场景的直接推理。该方法通过视频跟踪确保SAM分割的一致性，并使用高维CLIP特征的低维索引，有效嵌入语言信息在3D空间中。实验表明，SLGaussian在稀疏视点条件下的3D场景理解和分割任务上优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D语义场学习对于自主导航、AR&#x2F;VR和机器人应用至关重要。</li>
<li>现有方法在稀疏视点条件下存在不足，需要更有效的方法来构建3D语义场。</li>
<li>SLGaussian是一种前馈方法，用于从稀疏视点构建3D语义场，实现直接推理。</li>
<li>SLGaussian通过视频跟踪确保SAM分割的一致性。</li>
<li>SLGaussian使用低维索引嵌入高维CLIP特征，有效结合语言信息在3D空间中。</li>
<li>SLGaussian在稀疏视点条件下的3D场景理解和分割任务上表现优异。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8912ae6a66cd29d5f2031c5a794bbf6c.png" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5daa037b7a54349652e6e8d01f2656dc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ecc0bcd944a462f4ef58085159d3c13.jpg" align="middle">
</details>




<h2 id="Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing"><a href="#Diffusion-Based-Attention-Warping-for-Consistent-3D-Scene-Editing" class="headerlink" title="Diffusion-Based Attention Warping for Consistent 3D Scene Editing"></a>Diffusion-Based Attention Warping for Consistent 3D Scene Editing</h2><p><strong>Authors:Eyal Gomel, Lior Wolf</strong></p>
<p>We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \url{<a target="_blank" rel="noopener" href="https://attention-warp.github.io}/">https://attention-warp.github.io}</a> </p>
<blockquote>
<p>我们提出了一种使用扩散模型进行3D场景编辑的新方法，旨在确保不同视角下的视图一致性和逼真性。我们的方法利用从单张参考图像中提取的注意力特征来定义预期的编辑。通过将这些特征与从高斯贴片深度估计得出的场景几何结构进行对齐，将这些特征跨多个视图进行变形。将这些变形的特征注入到其他视角中，可以实现编辑的连贯传播，实现3D空间中的高保真和空间对齐。广泛评估表明，我们的方法在生成多样化的3D场景编辑方面非常有效，与现有方法相比，极大地提高了场景操作的能力。项目页面：<a target="_blank" rel="noopener" href="https://attention-warp.github.io/">https://attention-warp.github.io</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07984v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种利用扩散模型进行3D场景编辑的新方法，该方法通过提取单张参考图像的特征进行编辑，并利用高斯投影深度估计的场景几何结构，将特征在不同视角之间进行映射和融合。此方法实现了高保真和空间对齐的3D场景编辑，显著提高了场景操作的灵活性。项目页面：<a target="_blank" rel="noopener" href="https://attention-warp.github.io/">https://attention-warp.github.io</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用扩散模型进行3D场景编辑。</li>
<li>通过提取单张参考图像的特征进行编辑。</li>
<li>利用高斯投影深度估计的场景几何结构进行多视角特征映射和融合。</li>
<li>实现高保真和空间对齐的编辑效果。</li>
<li>方法能有效生成多样化的3D场景编辑。</li>
<li>与现有方法相比，显著提高了场景操作的灵活性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8f42584dfad759e1b2aecc279277861.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5766ba7cdaf9078e0a54b1a828295c72.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b0b7b0d5610c524aa4aec4e062420c76.png" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6a0003e9cacf1db217bda34beeb1a59b.jpg" align="middle">
</details>




<h2 id="GASP-Gaussian-Avatars-with-Synthetic-Priors"><a href="#GASP-Gaussian-Avatars-with-Synthetic-Priors" class="headerlink" title="GASP: Gaussian Avatars with Synthetic Priors"></a>GASP: Gaussian Avatars with Synthetic Priors</h2><p><strong>Authors:Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell</strong></p>
<p>Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (<a target="_blank" rel="noopener" href="https://microsoft.github.io/GASP/">https://microsoft.github.io/GASP/</a>) for results. </p>
<blockquote>
<p>高斯混刷技术彻底改变了实时逼真的渲染效果。高斯混刷最流行的应用之一是创建可动画的化身，称为高斯化身。虽然近期的研究在质量和渲染效率方面取得了突破，但它们仍面临两个主要局限性。它们要么需要昂贵的多相机装置来生成具有自由视角渲染的化身，要么它们可以使用单个相机进行训练，但仅能从这个固定视角以高质量进行渲染。理想模型将使用可用的硬件（如网络摄像头）的短时间单眼视频或图像进行训练，并且能够从任何视角进行渲染。为此，我们提出了GASP：带有合成先验的高斯化身。为了克服现有数据集的局限性，我们利用合成数据的像素完美性来训练高斯化身先验。通过将此先验模型拟合到一张照片或视频并进行微调，我们获得了支持360°渲染的高质量高斯化身。我们的先验仅适用于拟合，而不适用于推断，可实现实时应用。通过我们的方法，我们从有限数据中获得了高质量的动画化身，可以在商业硬件上以每秒70帧的速度进行动画和渲染。有关结果，请参见我们的项目页面（<a target="_blank" rel="noopener" href="https://microsoft.github.io/GASP/%EF%BC%89%E3%80%82">https://microsoft.github.io/GASP/）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07739v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://microsoft.github.io/GASP/">https://microsoft.github.io/GASP/</a></p>
<p><strong>Summary</strong><br>     高斯混合技术革新了实时逼真的渲染技术，尤其是用于创建动态化身方面。近期的研究虽提高了质量和渲染效率，但仍存在两大局限：要么需要昂贵的多相机装置来制作可在任意视角渲染的化身，要么能用单相机训练但仅在该固定视角高质量渲染。理想模型应能通过现有硬件如网络摄像头进行短期单目视频或图像的训练，并能从任意视角进行渲染。为此，我们提出GASP：带有合成先验的高斯化身技术。为克服现有数据集的限制，我们利用合成数据的像素完美性来训练高斯化身先验模型。通过将此先验模型拟合到单张图片或视频并进行微调，我们得到支持360°渲染的高品质高斯化身。我们的先验模型仅用于拟合，而非推理过程，可实现实时应用。通过我们的方法，能在有限数据下获得高质量、可动画的化身，能在商业硬件上以每秒70帧的速度进行动画和渲染。更多结果请见项目页面（<a target="_blank" rel="noopener" href="https://microsoft.github.io/GASP/%EF%BC%89%E3%80%82">https://microsoft.github.io/GASP/）。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Gaussian Splatting技术已改变实时逼真渲染的游戏规则。</li>
<li>Gaussian Avatars是Gaussian Splatting的一个热门应用。</li>
<li>现有研究在高质量和高效渲染方面存在局限，需要新的解决方案。</li>
<li>GASP技术通过使用合成数据先验模型解决这一问题。</li>
<li>通过单张照片或视频的先验模型拟合和微调，可获得支持360°渲染的高品质Gaussian Avatars。</li>
<li>该先验模型仅用于拟合，大大提升了实时应用的可行性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e19b29f587f280e23cc1e57106a6bcbe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-04e472c619da5bae43ce50aa91f60b42.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c11d7b7083faa97909f606baa7fe525a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-944de707da2f388cdf3c0fc04389a487.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-996e6c697734752977cdf86d9cb77536.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8cb38b72dfef6aa12c96c6a8c2d8b33c.jpg" align="middle">
</details>




<h2 id="Proc-GS-Procedural-Building-Generation-for-City-Assembly-with-3D-Gaussians"><a href="#Proc-GS-Procedural-Building-Generation-for-City-Assembly-with-3D-Gaussians" class="headerlink" title="Proc-GS: Procedural Building Generation for City Assembly with 3D   Gaussians"></a>Proc-GS: Procedural Building Generation for City Assembly with 3D   Gaussians</h2><p><strong>Authors:Yixuan Li, Xingjian Ran, Linning Xu, Tao Lu, Mulin Yu, Zhenzhi Wang, Yuanbo Xiangli, Dahua Lin, Bo Dai</strong></p>
<p>Buildings are primary components of cities, often featuring repeated elements such as windows and doors. Traditional 3D building asset creation is labor-intensive and requires specialized skills to develop design rules. Recent generative models for building creation often overlook these patterns, leading to low visual fidelity and limited scalability. Drawing inspiration from procedural modeling techniques used in the gaming and visual effects industry, our method, Proc-GS, integrates procedural code into the 3D Gaussian Splatting (3D-GS) framework, leveraging their advantages in high-fidelity rendering and efficient asset management from both worlds. By manipulating procedural code, we can streamline this process and generate an infinite variety of buildings. This integration significantly reduces model size by utilizing shared foundational assets, enabling scalable generation with precise control over building assembly. We showcase the potential for expansive cityscape generation while maintaining high rendering fidelity and precise control on both real and synthetic cases. </p>
<blockquote>
<p>建筑是城市的主要组成部分，通常包含窗户和门等重复元素。传统的3D建筑资产创建劳动强度大，需要制定设计规则，并具备专业技能。最近用于建筑创作的生成模型往往忽略了这些模式，导致视觉逼真度低和可扩展性有限。我们的方法Proc-GS从游戏和视觉效果行业中使用的程序建模技术中汲取灵感，将程序代码集成到3D高斯拼贴（3D-GS）框架中，融合了两者在高保真渲染和高效资产管理方面的优势。通过操作程序代码，我们可以简化流程并生成无限多样的建筑。通过利用共享基础资产，这种集成显著减少了模型大小，实现了具有精确控制建筑组装的可扩展生成。我们在真实和合成案例中展示了大规模城市景观生成的潜力，同时保持了高渲染保真度和精确控制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07660v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://city-super.github.io/procgs/">https://city-super.github.io/procgs/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种将过程建模技术融入3D高斯散斑（3D-GS）框架的方法，用于创建城市建筑。该方法通过操纵过程代码简化了建筑创建过程，可以生成无限多样的建筑。通过利用共享基础资产，该方法显著减少了模型大小，实现了对建筑物组装过程的精确控制，并在真实和合成案例中展示了大规模城市景观生成的潜力，同时保持了高渲染保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过程建模技术被用于城市建筑的创建过程。</li>
<li>Proc-GS方法结合了过程代码和3D高斯散斑框架。</li>
<li>该方法能够生成无限多样的建筑，同时简化了创建流程。</li>
<li>通过利用共享基础资产，模型大小得到了显著减少。</li>
<li>该方法实现了对建筑物组装过程的精确控制。</li>
<li>在真实和合成案例中，该方法展示了大规模城市景观生成的潜力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5710b4f4363b54396f9b0e96cdb6fbf8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-07144f8786ffa3040945104645988eda.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1d33e4e7177d665e4a9c5e6d2c854d5e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-74effd4da61c07925b87d7fc0a18b02d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-087b51ec44f3d8951246011f9d193dc0.jpg" align="middle">
</details>




<h2 id="Faster-and-Better-3D-Splatting-via-Group-Training"><a href="#Faster-and-Better-3D-Splatting-via-Group-Training" class="headerlink" title="Faster and Better 3D Splatting via Group Training"></a>Faster and Better 3D Splatting via Group Training</h2><p><strong>Authors:Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30% faster convergence and improved rendering quality across diverse scenarios. </p>
<blockquote>
<p>3D高斯摊铺（3DGS）作为一种新兴的强大技术，在新型视图合成中表现出卓越的性能。它通过高斯基本体表示法在高保真场景重建方面展现了非凡的能力。然而，由大量基本体引起的大量计算开销对训练效率构成了重大瓶颈。为了克服这一挑战，我们提出了群组训练策略，该策略将高斯基本体组织成可管理的群组，优化训练效率，提高渲染质量。该方法与现有的3DGS框架具有普遍兼容性，包括普通3DGS和Mip-Splatting，在加速训练的同时保持出色的合成质量。大量实验表明，我们简单的群组训练策略实现了高达30%的更快收敛速度，并在各种场景中提高了渲染质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07608v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了三维高斯贴图（3DGS）在场景重建中的出色表现，但其计算开销限制了训练效率。为此，本文提出了群组训练策略，通过组织高斯原始体进行分组优化，以提高训练效率和渲染质量。实验表明，群组训练策略在主流框架中通用兼容，显著提高训练速度并维持高质量的场景重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>三维高斯贴图（3DGS）是高效的新视图合成技术。</li>
<li>高斯原始体表示法可实现高保真场景重建。</li>
<li>大量原始体造成计算开销，影响训练效率。</li>
<li>提出了群组训练策略，以优化训练效率和渲染质量。</li>
<li>群组训练策略与现有3DGS框架兼容，可加速训练并维持高质量场景重建。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-195c326876a722cd4df5e5dceab23f07.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-30e9b40b5f52523ae0d2c330a05f8235.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-35852866968856b4582fe7d0e5ca5c92.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-01f420f65a4ab0c567307c1367461440.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d4947e4924a5b99765be47829d80c57f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9b16e9fbe782082d06a132d3819a3ecf.jpg" align="middle">
</details>




<h2 id="ResGS-Residual-Densification-of-3D-Gaussian-for-Efficient-Detail-Recovery"><a href="#ResGS-Residual-Densification-of-3D-Gaussian-for-Efficient-Detail-Recovery" class="headerlink" title="ResGS: Residual Densification of 3D Gaussian for Efficient Detail   Recovery"></a>ResGS: Residual Densification of 3D Gaussian for Efficient Detail   Recovery</h2><p><strong>Authors:Yanzhe Lyu, Kai Cheng, Xin Kang, Xuejin Chen</strong></p>
<p>Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view synthesis, achieving high fidelity and efficiency. However, it often struggles to capture rich details and complete geometry. Our analysis highlights a key limitation of 3D-GS caused by the fixed threshold in densification, which balances geometry coverage against detail recovery as the threshold varies. To address this, we introduce a novel densification method, residual split, which adds a downscaled Gaussian as a residual. Our approach is capable of adaptively retrieving details and complementing missing geometry while enabling progressive refinement. To further support this method, we propose a pipeline named ResGS. Specifically, we integrate a Gaussian image pyramid for progressive supervision and implement a selection scheme that prioritizes the densification of coarse Gaussians over time. Extensive experiments demonstrate that our method achieves SOTA rendering quality. Consistent performance improvements can be achieved by applying our residual split on various 3D-GS variants, underscoring its versatility and potential for broader application in 3D-GS-based applications. </p>
<blockquote>
<p>最近，3D高斯拼接（3D-GS）在新视角合成中很受欢迎，具有高保真度和高效率的特点。然而，它通常在捕捉丰富细节和完整几何结构方面存在困难。我们的分析强调了3D-GS的一个关键局限，这是由于密集化中的固定阈值引起的，该阈值在几何覆盖和细节恢复之间取得平衡，随着阈值的变化而变化。为了解决这一问题，我们引入了一种新的密集化方法——残差分割，该方法增加了一个降标度的高斯残差。我们的方法能够自适应地检索细节并补充缺失的几何结构，同时实现渐进的细化。为了进一步完善该方法，我们提出了一种名为ResGS的管道。具体来说，我们结合了高斯图像金字塔进行渐进监督，并实施了一种选择方案，该方案优先对粗高斯进行长期密集化。大量实验表明，我们的方法达到了最先进的渲染质量。将我们的残差分割应用于各种3D-GS变体，可以实现性能的一致性提高，这证明了其通用性和在基于3D-GS的应用中更广泛应用的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07494v1">PDF</a> </p>
<p><strong>总结</strong><br>    本文介绍了3D高斯映射（3D-GS）在新型视图合成中的优势与局限。为解决3D-GS在细节捕捉和完整几何表现上的不足，提出了一种新的细化方法——残差分割，并构建了ResGS管道。该方法通过下采样高斯图进行残差添加，能够自适应地恢复细节并补充缺失的几何信息，实现渐进式的优化。实验证明，该方法在渲染质量上达到了领先水平，且在不同3D-GS变种上应用均取得了稳定的性能提升，显示出其广泛的应用潜力和灵活性。</p>
<p><strong>要点</strong></p>
<ol>
<li>3D高斯映射（3D-GS）在新型视图合成中表现出高保真度和效率。</li>
<li>3D-GS存在固定阈值导致的细节捕捉和完整几何表现上的局限性。</li>
<li>提出了一种新的细化方法——残差分割，通过添加下采样高斯作为残差，自适应地恢复细节并补充缺失的几何信息。</li>
<li>构建了ResGS管道，整合高斯图像金字塔进行渐进式监督，并实施了优先选择对粗糙高斯进行细化的选择方案。</li>
<li>实验证明，该方法在渲染质量上达到了领先水平。</li>
<li>残差分割方法在不同3D-GS变种上的广泛应用，显示出其良好的适应性和潜力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fe9174eb40bccfc8191a90109d34b1bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c658fdf5dbe14321dabd648e260b75fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8aaec11b615ee2b9a068ea1c3d9ffb7d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a44d38f0ce96535eff3cf797573a1404.jpg" align="middle">
</details>




<h2 id="EventSplat-3D-Gaussian-Splatting-from-Moving-Event-Cameras-for-Real-time-Rendering"><a href="#EventSplat-3D-Gaussian-Splatting-from-Moving-Event-Cameras-for-Real-time-Rendering" class="headerlink" title="EventSplat: 3D Gaussian Splatting from Moving Event Cameras for   Real-time Rendering"></a>EventSplat: 3D Gaussian Splatting from Moving Event Cameras for   Real-time Rendering</h2><p><strong>Authors:Toshiya Yura, Ashkan Mirzaei, Igor Gilitschenski</strong></p>
<p>We introduce a method for using event camera data in novel view synthesis via Gaussian Splatting. Event cameras offer exceptional temporal resolution and a high dynamic range. Leveraging these capabilities allows us to effectively address the novel view synthesis challenge in the presence of fast camera motion. For initialization of the optimization process, our approach uses prior knowledge encoded in an event-to-video model. We also use spline interpolation for obtaining high quality poses along the event camera trajectory. This enhances the reconstruction quality from fast-moving cameras while overcoming the computational limitations traditionally associated with event-based Neural Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that our results achieve higher visual fidelity and better performance than existing event-based NeRF approaches while being an order of magnitude faster to render. </p>
<blockquote>
<p>我们介绍了一种利用事件相机数据通过高斯拼贴法合成新视角的方法。事件相机提供了出色的时间分辨率和高动态范围。利用这些功能，我们可以有效解决快速相机运动下的新视角合成挑战。为了初始化优化过程，我们的方法使用事件到视频的先验知识模型。我们还使用折线插值来获得事件相机轨迹的高质量姿态。这提高了快速相机拍摄的重建质量，同时克服了传统上事件神经网络辐射场（NeRF）方法所关联的计算限制。我们的实验评估表明，我们的结果达到了更高的视觉保真度和性能表现，并且渲染速度比现有的事件NeRF方法快一个数量级。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07293v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了一种利用事件相机数据进行新颖视角合成的方法，通过高斯拼贴技术实现。事件相机拥有出色的时间分辨率和高动态范围，借助这些功能，我们能够有效地解决快速相机运动下新颖视角合成的挑战。该方法利用事件到视频的模型编码的先验知识来初始化优化过程，并使用样条插值来获得高质量的姿态，沿事件相机轨迹进行插值。这提高了快速移动相机的重建质量，并克服了传统事件基础上的神经辐射场（NeRF）方法的计算限制。实验评估表明，该方法的结果具有更高的视觉保真度和性能，并且渲染速度比现有事件基础上的NeRF方法快一个数量级。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种利用事件相机数据进行新颖视角合成的方法。</li>
<li>事件相机数据的高时间分辨率和高动态范围被用于优化过程。</li>
<li>利用事件到视频的模型编码的先验知识初始化优化过程。</li>
<li>采用样条插值获取高质量的姿态，沿事件相机轨迹进行插值。</li>
<li>该方法提高了快速移动相机的重建质量。</li>
<li>克服了传统事件基础上的神经辐射场（NeRF）方法的计算限制。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d334757298e3a609e928d5ed7294448e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-19e5b8ae6652639ff3b16a241816277d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d149ec977c9fe15354931880a088b743.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eb0e3141278ff281d9541c89ddc28abc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e40e4fc8211446e76a71bf1452fa8ae9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-637fa5732e7af15df7f6e821040e650f.jpg" align="middle">
</details>




<h2 id="MV-DUSt3R-Single-Stage-Scene-Reconstruction-from-Sparse-Views-In-2-Seconds"><a href="#MV-DUSt3R-Single-Stage-Scene-Reconstruction-from-Sparse-Views-In-2-Seconds" class="headerlink" title="MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2   Seconds"></a>MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2   Seconds</h2><p><strong>Authors:Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan</strong></p>
<p>Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released. </p>
<blockquote>
<p>近期如DUSt3R和MASt3R的稀疏多视角场景重建技术不再需要相机标定和相机姿态估计。然而，它们仅一次处理一对视角来推断像素对齐的点图。在处理超过两个视角时，通常伴随着大量的易出错的对偶重建，随后是昂贵的全局优化，这往往无法纠正对偶重建的错误。为了处理更多的视角，减少错误，提高推理时间，我们提出了快速单阶段前馈网络MV-DUSt3R。其核心是多视角解码器块，在参考一个视角的同时，可以交换任何数量视角的信息。为了使我们的方法对参考视角选择具有鲁棒性，我们进一步提出了MV-DUSt3R+，它采用跨参考视角块来融合不同参考视角选择的信息。为了进一步实现新颖视角合成，我们通过添加并联合训练高斯贴片头来扩展两者。在多视角立体重建、多视角姿态估计和新颖视角合成方面的实验证实，我们的方法在现有技术的基础上有了显著的改进。代码将发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期，针对稀疏多视角场景重建的研究进展，如DUSt3R和MASt3R，已无需相机校准和相机姿态估计。但它们仅处理一对视图进行像素对齐的点图推断。处理多于两个视图时，通常需要经历一系列易出错的配对重建，接着是昂贵的全局优化，这常常无法纠正配对重建的错误。为解决多视角处理、减少错误并提升推断时间的问题，我们提出了快速单阶段前馈网络MV-DUSt3R。其核心为多视角解码器块，能在任何数量的视图中交换信息，同时参考一个视图。为提高方法对于参考视图选择的稳健性，我们进一步提出MV-DUSt3R+，采用跨参考视图块来融合不同参考视图选择的信息。为实现新颖视图合成，我们扩展了两者，增加了高斯平铺头并联合训练。实验证明，我们的方法在多元视角立体重建、多元视角姿态估计和新颖视图合成方面均有显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期多视角场景重建技术进展允许无需相机校准和姿态估计。<br>2.现有方法主要处理一对视图，存在配对重建错误和全局优化成本高的问题。</li>
<li>MV-DUSt3R被提出以解决多视角处理、减少错误并提升推断速度。</li>
<li>MV-DUSt3R采用多视角解码器块来交换任意数量视图的信息，同时参考一个视图。</li>
<li>MV-DUSt3R+进一步提高稳健性，通过跨参考视图块融合不同参考选择的信息。</li>
<li>方法通过添加高斯平铺头实现新颖视图合成，并联合训练。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e262d5467c20358baa18010e12172fde.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7822d14547a86d998e93b8c3a3ad6420.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-04f65d396dabfdcb24f8d426fcbf04a2.jpg" align="middle">
</details>




<h2 id="Extrapolated-Urban-View-Synthesis-Benchmark"><a href="#Extrapolated-Urban-View-Synthesis-Benchmark" class="headerlink" title="Extrapolated Urban View Synthesis Benchmark"></a>Extrapolated Urban View Synthesis Benchmark</h2><p><strong>Authors:Xiangyu Han, Zhen Jia, Boyi Li, Yan Wang, Boris Ivanovic, Yurong You, Lingjie Liu, Yue Wang, Marco Pavone, Chen Feng, Yiming Li</strong></p>
<p>Photorealistic simulators are essential for the training and evaluation of vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis (NVS), a crucial capability that generates diverse unseen viewpoints to accommodate the broad and continuous pose distribution of AVs. Recent advances in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic rendering at real-time speeds and have been widely used in modeling large-scale driving scenes. However, their performance is commonly evaluated using an interpolated setup with highly correlated training and test views. In contrast, extrapolation, where test views largely deviate from training views, remains underexplored, limiting progress in generalizable simulation technology. To address this gap, we leverage publicly available AV datasets with multiple traversals, multiple vehicles, and multiple cameras to build the first Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct quantitative and qualitative evaluations of state-of-the-art Gaussian Splatting methods across different difficulty levels. Our results show that Gaussian Splatting is prone to overfitting to training views. Besides, incorporating diffusion priors and improving geometry cannot fundamentally improve NVS under large view changes, highlighting the need for more robust approaches and large-scale training. We have released our data to help advance self-driving and urban robotics simulation technology. </p>
<blockquote>
<p>真实感模拟器对于以视觉为中心的自动驾驶汽车的训练和评估至关重要。其核心是新型视图合成（NVS），这是一种能够生成各种未见观点以适应自动驾驶汽车广泛且连续姿态分布的关键能力。最近，辐射场方面的进展，如3D高斯拼贴技术，实现了实时速度的逼真渲染，并已广泛应用于建模大规模驾驶场景。然而，它们的性能通常使用插值设置进行评估，其中训练和测试视图高度相关。相比之下，外推（即测试视图与训练视图有较大偏差）仍然很少被探索，这限制了通用仿真技术的进步。为了弥补这一差距，我们利用公开可用的自动驾驶数据集（包含多次遍历、多辆车和多相机）来建立第一个外推城市视图合成（EUVS）基准测试。同时，我们对不同难度级别的最新高斯拼贴方法进行了定量和定性的评估。结果表明，高斯拼贴容易过度拟合训练视图。此外，融入扩散先验知识和改进几何结构并不能在较大视角变化下从根本上改善NVS，这凸显了需要更稳健的方法和大规模训练。我们已经发布数据以帮助推动自动驾驶和城市机器人仿真技术的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05256v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ai4ce.github.io/EUVS-Benchmark/">https://ai4ce.github.io/EUVS-Benchmark/</a></p>
<p><strong>Summary</strong></p>
<p>本文强调了真实感模拟器对于以视觉为中心的自动驾驶汽车（AVs）训练和评估的重要性。文章介绍了关键技术——Novel View Synthesis（NVS），它能生成多样化的未见过视角以适应AVs的广泛和连续姿态分布。虽然使用如3D高斯拼贴等最新辐射场技术可以实现实时渲染，但现有评估方法主要关注插值设置，涉及高度相关的训练和测试视角，而对更大视角偏差的推演设置则研究较少。为弥补这一空白，研究团队利用多个横穿、多车辆和多相机的公开AV数据集，建立了首个Extrapolated Urban View Synthesis（EUVS）基准测试平台。对现有高斯拼贴方法的定量和定性评估表明，其易对训练视角产生过拟合现象，且在大视角变化下，引入扩散先验和改进几何并不能从根本上改善NVS性能。研究团队已发布相关数据，以期推动自动驾驶和城市机器人模拟技术的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实感模拟器对自动驾驶汽车的训练和评估至关重要。</li>
<li>Novel View Synthesis（NVS）技术能生成多样化的视角以适应自动驾驶汽车的姿态分布。</li>
<li>现有技术如3D高斯拼贴在实时渲染方面表现出优势，但评估方法主要关注插值设置，缺乏对更大视角变化的推演研究。</li>
<li>研究团队建立了首个Extrapolated Urban View Synthesis（EUVS）基准测试平台以填补这一空白。</li>
<li>高斯拼贴方法存在对训练视角过拟合的问题。</li>
<li>在大视角变化下，引入扩散先验和改进几何并不能根本改善NVS性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bcb7ff0da7475b40df91e74bedda2ef0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d9cb94809b59abfde99df6fbbcac73e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4c361afdbd89649544c9166e25355d1e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-49e3691ead92f7eb48a6c213847eb901.jpg" align="middle">
</details>




<h2 id="MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting"><a href="#MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting" class="headerlink" title="MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting"></a>MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu</strong></p>
<p>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>. </p>
<blockquote>
<p>重建高保真3D头像对于虚拟现实等各种应用至关重要。开创性的方法使用神经辐射场（NeRF）重建逼真的头像，但受限于训练和渲染速度。基于3D高斯拼贴（3DGS）的最近的方法显著提高了训练和渲染的效率。然而，3DGS的表面不一致导致几何精度不佳；后来的2DGS使用2D surfels提高几何精度，但牺牲了渲染保真度。为了结合2DGS和3DGS的优点，我们提出了一种名为MixedGaussianAvatar的新方法，用于真实且几何精确的头像重建。我们的主要思想是利用2D高斯重建3D头像的表面，以确保几何精度。我们将2D高斯附加到FLAME模型的三角网格上，并在2DGS渲染质量不足的地方连接额外的3D高斯，创建混合的2D-3D高斯表示。这些2D-3D高斯可以使用FLAME参数进行动画设置。我们还引入了一种渐进的训练策略，首先训练2D高斯，然后对混合的2D-3D高斯进行微调。我们通过全面的实验证明了MixedGaussianAvatar的优越性。代码将在<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04955v2">PDF</a> Project: <a target="_blank" rel="noopener" href="https://chenvoid.github.io/MGA/">https://chenvoid.github.io/MGA/</a></p>
<p><strong>Summary</strong><br>     针对虚拟现实中高保真3D头像重建的需求，新的MixedGaussianAvatar方法融合了2DGS和3DGS的优点，采用混合的2D-3D高斯表示来确保几何准确性和渲染质量。通过利用FLAME模型的三角网格连接附加的3D高斯，以改善2DGS的渲染质量不足部分。采用渐进式训练策略，先训练2D高斯，再微调混合的2D-3D高斯。此方法已在实验中表现出优越性。</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> * MixedGaussianAvatar融合了两种现有的方法：利用NeRF重建真实头像的3DGS和增强几何准确性的2DGS。
 
 * 该方法使用混合的二维和三维高斯表示来确保几何准确性和渲染质量，通过利用FLAME模型的三角网格连接附加的二维和三维高斯来改进渲染质量。
 
 * 利用附着于二维和高斯数据的特性提高三维头模的构建效率和质量。动画性能通过在原有动画工具中加入此新方法可以维持原有性能的同时提高渲染质量。
</code></pre>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fee7aa56253f4eb6856f0bdf9d9655e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0ef5787956810f1e111d21adf0bdcf5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ce4f964cf25207a6db5a28f7f85bd755.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a7e93cc4f1cccfe010d043da886dc390.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43282ee6f05919c0b760a138b7ff6c40.jpg" align="middle">
</details>




<h2 id="HybridGS-Decoupling-Transients-and-Statics-with-2D-and-3D-Gaussian-Splatting"><a href="#HybridGS-Decoupling-Transients-and-Statics-with-2D-and-3D-Gaussian-Splatting" class="headerlink" title="HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian   Splatting"></a>HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian   Splatting</h2><p><strong>Authors:Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye</strong></p>
<p>Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements. </p>
<blockquote>
<p>生成三维高斯平展（3DGS）的高质量新视角渲染，在具有瞬态对象的场景中是一项挑战。我们提出了一种新的混合表示方法，称为HybridGS，使用针对每张图像的二维高斯来表示瞬态对象，并保持对整个静态场景的传统三维高斯表示。需要注意的是，3DGS本身更适合于模拟假设多视角一致性的静态场景，但瞬态对象偶尔出现并不符合这一假设，因此我们将它们建模为单视角的平面对象，用二维高斯表示。我们的新表示方法从基本视点一致性的角度对场景进行分解，使其更合理。此外，我们还提出了一种用于3DGS的新型多视角监管监督方法，该方法利用共可见区域的信息，进一步增强了瞬态和静态之间的区别。然后，我们提出了一种简单有效的多阶段训练策略，以确保在各种设置中进行稳健的训练和高质量的观点合成。在基准数据集上的实验表明，我们在室内和室外场景的新视角合成方面达到了最新技术水平，即使在存在干扰元素的情况下也是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03844v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gujiaqivadin.github.io/hybridgs/">https://gujiaqivadin.github.io/hybridgs/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为HybridGS的混合表示方法，用于在包含瞬态对象的场景中生成高质量的新型视图渲染。该方法使用针对图像中瞬态对象的2D高斯，并维持对整体静态场景的传统3D高斯表示。针对3DGS在建模假设多视角一致性的静态场景时较为适用，但对于瞬态对象，由于其偶尔出现并不符合此假设，故将其建模为平面对象，以2D高斯表示。该方法从基本视点一致性角度分解场景，更加合理。同时，还提出了一种新型的多视角监管方法，利用可见区域的资讯，进一步区分瞬态和静态物体。此外，研究还提出了简洁有效的多阶段训练策略，确保在各种设置下实现稳健的训练和高质量的视图合成。实验结果表明，该方法在室内和室外场景的新型视图合成中表现卓越，即使在有干扰元素的情况下也是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了HybridGS混合表示方法，结合2D和3D高斯模型，针对包含瞬态对象的场景进行高质量视图合成。</li>
<li>针对瞬态对象使用2D高斯表示，将其建模为平面对象，以适应其偶尔出现且不遵守多视角一致性假设的特性。</li>
<li>从基本视点一致性角度分解场景，使场景分解更合理。</li>
<li>提出了多视角监管方法，利用可见区域的资讯来增强瞬态和静态物体的区分。</li>
<li>采用了多阶段训练策略，确保在各种设置下实现稳健的训练和高质量视图合成。</li>
<li>在室内和室外场景的新型视图合成中取得了卓越表现。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5d505d9ebbb9cdc27f1872affd9a285c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f99336e9eaadb0ddb4c3dfffa1d84b60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d8ea9f1db4f0f1b69e7501baa9d7ab5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2d581b1f15c71511a73c736ef082e6b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2d77371974063ce2cd70dc1eeac57387.jpg" align="middle">
</details>




<h2 id="LineGS-3D-Line-Segment-Representation-on-3D-Gaussian-Splatting"><a href="#LineGS-3D-Line-Segment-Representation-on-3D-Gaussian-Splatting" class="headerlink" title="LineGS : 3D Line Segment Representation on 3D Gaussian Splatting"></a>LineGS : 3D Line Segment Representation on 3D Gaussian Splatting</h2><p><strong>Authors:Chenggang Yang, Yuang Shi, Wei Tsang Ooi</strong></p>
<p>Abstract representations of 3D scenes play a crucial role in computer vision, enabling a wide range of applications such as mapping, localization, surface reconstruction, and even advanced tasks like SLAM and rendering. Among these representations, line segments are widely used because of their ability to succinctly capture the structural features of a scene. However, existing 3D reconstruction methods often face significant challenges. Methods relying on 2D projections suffer from instability caused by errors in multi-view matching and occlusions, while direct 3D approaches are hampered by noise and sparsity in 3D point cloud data. This paper introduces LineGS, a novel method that combines geometry-guided 3D line reconstruction with a 3D Gaussian splatting model to address these challenges and improve representation ability. The method leverages the high-density Gaussian point distributions along the edge of the scene to refine and optimize initial line segments generated from traditional geometric approaches. By aligning these segments with the underlying geometric features of the scene, LineGS achieves a more precise and reliable representation of 3D structures. The results show significant improvements in both geometric accuracy and model compactness compared to baseline methods. </p>
<blockquote>
<p>摘要：三维场景抽象表示在计算机视觉中发挥着至关重要的作用，可实现诸如地图绘制、定位、表面重建以及高级任务如SLAM和渲染等广泛应用。在这些表示方法中，线段因其能够简洁地捕捉场景的结构特征而得到广泛应用。然而，现有的三维重建方法常常面临重大挑战。依赖二维投影的方法受到多视角匹配和遮挡错误导致的稳定性问题的影响，而直接的三维方法则受到三维点云数据中的噪声和稀疏性的限制。本文介绍了一种新方法LineGS，它结合了几何引导的的三维线段重建和三维高斯展布模型，以解决这些挑战并提高表示能力。该方法利用场景边缘的高密度高斯点分布来细化和优化从传统几何方法生成的初始线段。通过将这些线段与场景的基本几何特征对齐，LineGS实现了对三维结构更精确和可靠的表现。结果表明，与基线方法相比，LineGS在几何精度和模型紧凑性方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00477v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为LineGS的新方法，该方法结合了几何引导的3D线重建和3D高斯展布模型，解决了现有3D重建方法面临的挑战，提高了场景的三维结构表示能力。该方法利用场景边缘的高密度高斯点分布来优化和改进由传统几何方法生成的初始线段。通过与场景的底层几何特征对齐，LineGS实现了更精确和可靠的三维结构表示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LineGS方法结合了几何引导和3D高斯展布模型，用于提高三维场景表示能力。</li>
<li>现有3D重建方法面临多视匹配误差和遮挡导致的稳定性问题以及3D点云数据的噪声和稀疏性问题。</li>
<li>LineGS利用场景边缘的高密度高斯点分布进行优化，改进了传统几何方法生成的初始线段。</li>
<li>LineGS方法通过对齐线段与场景的底层几何特征，实现了更精确和可靠的三维结构表示。</li>
<li>LineGS在几何精度和模型紧凑性方面相比基准方法有明显的改进。</li>
<li>LineGS方法对于计算机视觉中的多种应用（如映射、定位、表面重建、SLAM和渲染等）具有潜在的应用价值。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c07511f05a3611f234b87581650fb20e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bd938f99313932c1aadd9898b81963f6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d1f0e109dfb474c4e2c955c2042cc72e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3fa63d3f9a7cb1b1d752537b6cb5824b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bacd5145c1f53380f5f76f647902e9b8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-42063926814c788414789afee835efa1.jpg" align="middle">
</details>




<h2 id="SuperGS-Super-Resolution-3D-Gaussian-Splatting-Enhanced-by-Variational-Residual-Features-and-Uncertainty-Augmented-Learning"><a href="#SuperGS-Super-Resolution-3D-Gaussian-Splatting-Enhanced-by-Variational-Residual-Features-and-Uncertainty-Augmented-Learning" class="headerlink" title="SuperGS: Super-Resolution 3D Gaussian Splatting Enhanced by Variational   Residual Features and Uncertainty-Augmented Learning"></a>SuperGS: Super-Resolution 3D Gaussian Splatting Enhanced by Variational   Residual Features and Uncertainty-Augmented Learning</h2><p><strong>Authors:Shiyun Xie, Zhiru Wang, Xu Wang, Yinghao Zhu, Chengwei Pan, Xiwang Dong</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis (NVS) with its real-time rendering capabilities and superior quality. However, it faces challenges for high-resolution novel view synthesis (HRNVS) due to the coarse nature of primitives derived from low-resolution input views. To address this issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion of 3DGS designed with a two-stage coarse-to-fine training framework. In this framework, we use a latent feature field to represent the low-resolution scene, serving as both the initialization and foundational information for super-resolution optimization. Additionally, we introduce variational residual features to enhance high-resolution details, using their variance as uncertainty estimates to guide the densification process and loss computation. Furthermore, the introduction of a multi-view joint learning approach helps mitigate ambiguities caused by multi-view inconsistencies in the pseudo labels. Extensive experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods on both real-world and synthetic datasets using only low-resolution inputs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SYXieee/SuperGS">https://github.com/SYXieee/SuperGS</a>. </p>
<blockquote>
<p>近期，3D高斯扩展（3DGS）凭借其实时渲染能力和卓越质量在新型视图合成（NVS）方面表现出色。然而，它在高分辨率新型视图合成（HRNVS）方面面临挑战，这是由于从低分辨率输入视图派生的基本元素较为粗糙所导致的。为了解决这一问题，我们提出了超级分辨率3DGS（SuperGS），它是3DGS的扩展，采用两阶段由粗到细的训练框架设计。在此框架中，我们使用潜在特征场来表示低分辨率场景，作为超分辨率优化的初始化和基础信息。此外，我们引入了变异残留特征以增强高分辨率细节，并使用其方差作为不确定性估计来指导密集化过程和损失计算。而且，引入多视图联合学习方法有助于减少由伪标签的多视图不一致性引起的歧义。大量实验表明，仅使用低分辨率输入的SuperGS在真实世界和合成数据集上的HRNVS方法均超过了最先进水平。代码可通过<a target="_blank" rel="noopener" href="https://github.com/SYXieee/SuperGS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SYXieee/SuperGS获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02571v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于实时渲染能力的优势在三维视角合成技术（NVS）领域中的杰出表现的三维高斯喷射技术（3DGS）。然而，在处理高解析度三维视角合成技术（HRNVS）时面临了由低解析度输入视角带来的挑战。为此，研究人员提出了一种名为SuperGS的超解析度三维高斯喷射技术，该技术采用两阶段精细训练框架设计，使用潜在特征场表示低解析度场景作为超解析度优化的初始和基础信息。此外，引入变异残差特征增强高解析度细节，并利用其变异作为不确定性估计来指导稠密化过程和损失计算。多视角联合学习方法则有助于减轻由于视角不一致产生的模糊伪标签的问题。实验结果证实SuperGS在高分辨率数据集和真实世界数据集上的表现均优于现有HRNVS方法，仅使用低分辨率输入即可实现卓越性能。更多信息可访问相关代码库。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS在NVS领域具有出色的实时渲染能力和优势。</li>
<li>HRNVS面临由低分辨率输入视角带来的挑战。</li>
<li>SuperGS是一种扩展的3DGS技术，采用两阶段精细训练框架设计来处理高解析度场景。</li>
<li>潜在特征场用于表示低解析度场景，作为超解析度优化的基础。</li>
<li>引入变异残差特征以增强高解析度细节。利用变异度量不确定性来指导处理过程。</li>
<li>多视角联合学习方法用于减轻由于视角不一致导致的模糊伪标签问题。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d72741767bf6e1da43a7973ad9958daa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cb832b722defb2458c803f27c3a4903a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bca170c323e9a9963817520d2935ca45.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-21f10558631b0a2da0fc12a9bd6b3e04.jpg" align="middle">
</details>




<h2 id="Learning-based-Multi-View-Stereo-A-Survey"><a href="#Learning-based-Multi-View-Stereo-A-Survey" class="headerlink" title="Learning-based Multi-View Stereo: A Survey"></a>Learning-based Multi-View Stereo: A Survey</h2><p><strong>Authors:Fangjinhua Wang, Qingtian Zhu, Di Chang, Quankai Gao, Junlin Han, Tong Zhang, Richard Hartley, Marc Pollefeys</strong></p>
<p>3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented&#x2F;Virtual Reality (AR&#x2F;VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area. </p>
<blockquote>
<p>三维重建旨在恢复场景的密集三维结构。它在增强&#x2F;虚拟现实（AR&#x2F;VR）、自动驾驶和机器人技术等各种应用中发挥着重要作用。利用从不同视角捕获的场景的多个视图，多视角立体（MVS）算法合成了一个全面的三维表示，能够在复杂环境中实现精确重建。由于其高效性和有效性，MVS已成为基于图像的3D重建中的关键方法。最近，随着深度学习取得的成就，已经提出了许多基于学习的MVS方法，相较于传统方法，这些方法取得了令人印象深刻的效果。我们将这些基于学习的方法分为以下几类：基于深度图的方法、基于体素的方法、基于NeRF的方法、基于三维高斯贴图的方法和大型前馈方法。其中，我们重点关注基于深度图的方法，由于其简洁性、灵活性和可扩展性，它们成为MVS的主要家族。在这篇综述中，我们对当前文献进行了全面的回顾。我们调查了这些基于学习的方法，总结了它们在流行基准测试上的性能，并讨论了该领域未来有前景的研究方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15235v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了三维重建中的多视角立体（MVS）算法，其在增强现实&#x2F;虚拟现实、自动驾驶和机器人等领域有广泛应用。文章重点介绍了基于深度学习的学习型MVS方法，包括深度图、体素、NeRF、三维高斯散斑等方法，并重点关注了深度图方法的重要性和前景。文章总结了现有文献的综合评价，概述了这些学习型方法在流行基准测试上的表现，并探讨了未来研究的有益方向。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D重建旨在恢复场景的密集三维结构，广泛应用于AR&#x2F;VR、自动驾驶和机器人等领域。</li>
<li>多视角立体（MVS）算法通过从不同视角捕捉场景的多个视图来合成全面的三维表示，为复杂环境中的精确重建提供了可能。</li>
<li>基于深度学习的学习型MVS方法包括深度图、体素、NeRF和三维高斯散斑等方法，其中深度图方法因其简洁性、灵活性和可扩展性而受到广泛关注。</li>
<li>现有文献对基于深度学习的学习型MVS方法进行了全面综述，包括对各种方法的详细分析和它们在流行基准测试上的性能总结。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a5e5346a998309aa296c8385f856de80.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-22ada0555da5fef891a724a431157d98.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-03bb12f1f648696bd6045b65e15edfd1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a69c8889a02a0a54adc1f576279f164.jpg" align="middle">
</details>




<h2 id="SpikeGS-Reconstruct-3D-scene-via-fast-moving-bio-inspired-sensors"><a href="#SpikeGS-Reconstruct-3D-scene-via-fast-moving-bio-inspired-sensors" class="headerlink" title="SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors"></a>SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors</h2><p><strong>Authors:Yijia Guo, Liwen Hu, Yuanxi Bai, Lei Ma, Tiejun Huang</strong></p>
<p>3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images. Fulfilling this requirement can be challenging in real-world scenarios especially when the camera moves fast, which severely limits the application of 3DGS. To address these challenges, we proposed Spike Gausian Splatting (SpikeGS), the first framework that integrates the spike streams into 3DGS pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With accumulation rasterization, interval supervision, and a specially designed pipeline, SpikeGS extracts detailed geometry and texture from high temporal resolution but texture lacking spike stream, reconstructs 3D scenes captured in 1 second. Extensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of SpikeGS compared with existing spike-based and deblur 3D scene reconstruction methods. Codes and data will be released soon. </p>
<blockquote>
<p>3D高斯描画（3DGS）在3D场景重建中表现出了无与伦比的优势性能。然而，3DGS严重依赖于清晰图像。在现实世界场景中，尤其是相机快速移动时，满足这一要求可能会具有挑战性，这严重限制了3DGS的应用。为了解决这些挑战，我们提出了Spike高斯描画（SpikeGS）框架，它是第一个将脉冲流集成到3DGS管道中的框架，通过快速移动的仿生相机重建3D场景。通过累积光栅化、间隔监督以及专门设计的管道，SpikeGS能够从高时间分辨率但纹理缺失的脉冲流中提取详细的几何和纹理信息，重建在1秒内捕获的3D场景。在多个合成和真实数据集上的大量实验表明，与现有的基于脉冲和去模糊的3D场景重建方法相比，SpikeGS具有优越性。代码和数据将很快发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03771v3">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong><br>     3D高斯融合技术（3DGS）在三维场景重建中表现出卓越性能，但高度依赖清晰图像。在快速移动相机等现实场景中获取清晰图像是一大挑战。为解决此问题，提出Spike高斯融合（SpikeGS）框架，它首创性地通过累积渲染、间隔监控及特制管线设计，以存在高清时序分辨率但纹理缺失的Spike流重建三维场景。SpikeGS能在一秒内捕捉并重建三维场景，详细提取几何和纹理信息。在合成和真实数据集上的大量实验表明，SpikeGS相较于现有基于Spike和去模糊的三维场景重建方法具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS在三维场景重建中表现优越性能。</li>
<li>3DGS高度依赖清晰图像，现实场景中获取清晰图像是一大挑战。</li>
<li>SpikeGS框架解决了快速移动相机下的图像获取问题。</li>
<li>SpikeGS结合了Spike流与3DGS流程进行三维场景重建。</li>
<li>SpikeGS能够通过积累的渲染与监控技术提取高清晰度且精确的几何与纹理信息。</li>
<li>SpikeGS在短短一秒内重建三维场景，显示出了强大的实时处理能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-044434c4e8ea2f0bb67a913139a53290.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6b04627506f5c1774843adb3d45e12b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3904aa5e74a03e6a096708c3a0d2134c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3d5eaab5a84053f41ee82ffc9cfa63fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f28fa20edf76b0d4d4e800e5f9b3d45c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f20214ab8dba2d5336708bd32974fcd2.jpg" align="middle">
</details>




<h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p>
<p>3D Gaussian Splatting (3DGS) has significantly advanced 3D scene reconstruction and novel view synthesis. However, like Neural Radiance Fields (NeRF), 3DGS struggles with accurately modeling physical reflections, particularly in mirrors, leading to incorrect reconstructions and inconsistent reflective properties. To address this challenge, we introduce Mirror-3DGS, a novel framework designed to accurately handle mirror geometries and reflections, thereby generating realistic mirror reflections. By incorporating mirror attributes into 3DGS and leveraging plane mirror imaging principles, Mirror-3DGS simulates a mirrored viewpoint from behind the mirror, enhancing the realism of scene renderings. Extensive evaluations on both synthetic and real-world scenes demonstrate that our method can render novel views with improved fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF, especially in mirror regions. </p>
<blockquote>
<p>三维高斯融合（3DGS）在三维场景重建和新型视角合成方面取得了显著进展。然而，与神经辐射场（NeRF）一样，3DGS在模拟物理反射，特别是在镜子上的反射时存在困难，导致重建不准确和反射属性不一致。为了应对这一挑战，我们引入了Mirror-3DGS这一新型框架，它能够准确处理镜面几何形状和反射，从而生成逼真的镜面反射。通过将镜面属性融入3DGS并利用平面镜像成像原理，Mirror-3DGS能够模拟镜子后面的镜像视角，提高场景渲染的逼真度。对合成场景和真实场景的大量评估表明，我们的方法能够在实时渲染新视角时提高保真度，超越最先进的Mirror-NeRF，特别是在镜子区域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01168v2">PDF</a> IEEE International Conference on Visual Communications and Image   Processing (VCIP 2024, Oral)</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了为解决当前利用神经辐射场与高斯隐写技术处理三维场景重建与合成时遇到的镜像反射问题而提出的Mirror-3DGS框架。该框架通过引入镜像属性并借助平面镜像成像原理，改进了现有技术的局限性，从而能够在处理镜面反射时生成更逼真的镜像场景渲染。评价结果表明，与现有的Mirror-NeRF相比，Mirror-3DGS在合成和真实场景中渲染新视角时具有更高的保真度和实时性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Mirror-3DGS针对当前3D场景重建技术中的镜像反射问题提出了解决方案。</li>
<li>通过引入镜像属性并结合平面镜像成像原理，Mirror-3DGS可以模拟镜子背后的视角，增强了场景渲染的逼真度。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dd7869e208d9fb8d79482f7e49bf8dfd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-547b2159e15ad9259a5f5758f4258655.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc1bfa303ba582248284272ab2f58d3c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c71cd65107819c859fa830f5e804483d.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e38e0e09c2122c1a141338bb8dd78188.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-12  GN-FRGeneralizable Neural Radiance Fields for Flare Removal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_元宇宙_虚拟人/2412.04955v2/page_2_0.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-12  GASP Gaussian Avatars with Synthetic Priors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
