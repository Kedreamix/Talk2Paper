<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  CoPrUS Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="CoPrUS-Consistency-Preserving-Utterance-Synthesis-towards-more-realistic-benchmark-dialogues"><a href="#CoPrUS-Consistency-Preserving-Utterance-Synthesis-towards-more-realistic-benchmark-dialogues" class="headerlink" title="CoPrUS: Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues"></a>CoPrUS: Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues</h2><p><strong>Authors:Sebastian Steindl, Ulrich SchÃ¤fer, Bernd Ludwig</strong></p>
<p>Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep learning-based dialogue systems. While they are successful as benchmark datasets, they lack certain types of utterances, which would make them more realistic. In this work, we investigate the creation of synthetic communication errors in an automatic pipeline. Based on linguistic theory, we propose and follow a simple error taxonomy. We focus on three types of miscommunications that could happen in real-world dialogues but are underrepresented in the benchmark dataset: misunderstandings, non-understandings and vaguely related questions. Our two-step approach uses a state-of-the-art Large Language Model (LLM) to first create the error and secondly the repairing utterance. We perform Language Model-based evaluation to ensure the quality of the generated utterances. We apply the method to the MultiWOZ dataset and evaluate it both qualitatively and empirically as well as with human judges. Our results indicate that current LLMs can aid in adding post-hoc miscommunications to benchmark datasets as a form of data augmentation. We publish the resulting dataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ to facilitate future work on dialogue systems. </p>
<blockquote>
<p>å¤§è§„æ¨¡Wizard-Of-Ozå¯¹è¯æ•°æ®é›†å·²ç»æ”¯æŒåŸºäºæ·±åº¦å­¦ä¹ çš„å¯¹è¯ç³»ç»Ÿçš„è®­ç»ƒã€‚è™½ç„¶å®ƒä»¬ä½œä¸ºåŸºå‡†æ•°æ®é›†æ˜¯æˆåŠŸçš„ï¼Œä½†å®ƒä»¬ç¼ºå°‘æŸäº›ç±»å‹çš„è¡¨è¿°ï¼Œè¿™ä¼šä½¿å®ƒä»¬æ›´çœŸå®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è‡ªåŠ¨ç®¡é“ä¸­åˆæˆé€šä¿¡é”™è¯¯çš„å‡ºç°ã€‚åŸºäºè¯­è¨€å­¦ç†è®ºï¼Œæˆ‘ä»¬æå‡ºå¹¶éµå¾ªä¸€ä¸ªç®€å•çš„é”™è¯¯åˆ†ç±»æ³•ã€‚æˆ‘ä»¬å…³æ³¨çœŸå®ä¸–ç•Œå¯¹è¯ä¸­å¯èƒ½å‘ç”Ÿä½†åŸºå‡†æ•°æ®é›†ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ä¸‰ç§è¯¯è§£ï¼šè¯¯è§£ã€éç†è§£å’Œå«ç³Šç›¸å…³é—®é¢˜ã€‚æˆ‘ä»¬çš„ä¸¤æ­¥æ–¹æ³•ä½¿ç”¨æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¦–å…ˆåˆ›å»ºé”™è¯¯ï¼Œç„¶åä¿®å¤è¡¨è¿°ã€‚æˆ‘ä»¬æ‰§è¡ŒåŸºäºè¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä»¥ç¡®ä¿ç”Ÿæˆçš„è¡¨è¿°è´¨é‡ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºMultiWOZæ•°æ®é›†ï¼Œå¹¶ä»å®šæ€§ã€å®è¯å’Œäººç±»è¯„å§”ä¸¤ä¸ªæ–¹é¢å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºåŸºå‡†æ•°æ®é›†äº‹åæ·»åŠ è¯¯é€šä¿¡çš„ä¸€ç§å½¢å¼çš„æ•°æ®å¢å¼ºå·¥å…·ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¿®æ”¹åçš„æ•°æ®é›†CoPrUS-MultiWOZï¼Œå…¶ä¸­è¿‘1.9ä¸‡æ¡å¯¹è¯å·²è¢«ä¿®æ”¹ï¼Œä»¥ä¿ƒè¿›æœªæ¥å¯¹è¯ç³»ç»Ÿçš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07515v1">PDF</a> Accepted at COLING 2025 (main, long paper)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤§è§„æ¨¡Wizard-Of-Ozå¯¹è¯æ•°æ®é›†ä¸Šåˆ›å»ºåˆæˆé€šä¿¡é”™è¯¯çš„æ–¹æ³•ã€‚ç”±äºè¿™äº›æ•°æ®é›†ç¼ºä¹æŸäº›ç±»å‹çš„çœŸå®å¯¹è¯ä¸­å¸¸è§çš„è¯¯é€šä¿¡ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºè¯­è¨€å­¦ç†è®ºçš„ç®€å•é”™è¯¯åˆ†ç±»æ³•ï¼Œå¹¶ä¸“æ³¨äºä¸‰ç§åœ¨åŸºå‡†æ•°æ®é›†ä¸­ä»£è¡¨æ€§ä¸è¶³çš„è¯¯é€šä¿¡ç±»å‹ï¼šè¯¯è§£ã€æ— æ³•ç†è§£ä»¥åŠä¸ç›¸å…³çš„æé—®ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ä¸¤æ­¥æ–¹æ³•ï¼Œåˆ©ç”¨æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥åˆ›å»ºé”™è¯¯å¹¶ä¿®å¤å¯¹è¯å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºåŸºå‡†æ•°æ®é›†çš„ä¸€ç§æ•°æ®å¢å¼ºæ‰‹æ®µæ¥æ·»åŠ äº‹åè¯¯é€šä¿¡ã€‚æœ€ç»ˆå‘å¸ƒä¿®æ”¹åçš„æ•°æ®é›†CoPrUS-MultiWOZï¼Œå…¶ä¸­åŒ…å«è¿‘1900ä¸ªå¯¹è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡Wizard-Of-Ozå¯¹è¯æ•°æ®é›†è™½ç„¶å·²ç»æˆåŠŸè®­ç»ƒå‡ºæ·±åº¦å­¦ä¹ å¯¹è¯ç³»ç»Ÿï¼Œä½†å®ƒä»¬ç¼ºä¹æŸäº›çœŸå®å¯¹è¯ä¸­å‡ºç°çš„é€šä¿¡é”™è¯¯ç±»å‹ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸Šç¼ºä¹é€¼çœŸåº¦ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºåŸºäºè¯­è¨€å­¦ç†è®ºçš„ç®€å•é”™è¯¯åˆ†ç±»æ³•ï¼Œé‡ç‚¹ç ”ç©¶ä¸‰ç§åŸºå‡†æ•°æ®é›†ä¸­ä»£è¡¨æ€§ä¸è¶³çš„è¯¯é€šä¿¡ç±»å‹ï¼šè¯¯è§£ã€æ— æ³•ç†è§£ä»¥åŠä¸ç›¸å…³çš„æé—®ã€‚</li>
<li>é‡‡ç”¨ä¸¤æ­¥æ–¹æ³•åˆ›å»ºåˆæˆé€šä¿¡é”™è¯¯ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¿®å¤å¯¹è¯å†…å®¹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆå¢å¼ºåŸºå‡†æ•°æ®é›†çš„çœŸå®æ„Ÿã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºä¸€ç§æ•°æ®å¢å¼ºæ‰‹æ®µæ¥æ·»åŠ äº‹åè¯¯é€šä¿¡åˆ°åŸºå‡†æ•°æ®é›†ä¸­ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹åçš„æ•°æ®é›†CoPrUS-MultiWOZè¢«å…¬å¼€å‘å¸ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«è¿‘1900ä¸ªä¿®æ”¹åçš„å¯¹è¯ã€‚è¿™äº›æ•°æ®å¯ç”¨äºè®­ç»ƒæ›´åŠ çœŸå®ä¸–ç•Œçš„å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>æ•°æ®é›†ä¸­çš„é€šä¿¡é”™è¯¯å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œè¯„ä¼°å’ŒéªŒè¯ï¼ŒåŒ…æ‹¬åŸºäºè¯­è¨€æ¨¡å‹çš„è¯„ä¼°ã€å®šæ€§è¯„ä¼°å’Œå®è¯è¯„ä¼°ä»¥åŠäººå·¥è¯„ä¼°ã€‚è¿™è¯æ˜äº†è¯¥ç ”ç©¶æ–¹æ³•çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-485d377f239710a1eb6365258bdf4847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8554fd4871d43aaaaf1f1dc01eea58d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27c8a458443ed6964c6d6a21e4eadea0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2088b716b8604e50f5540c0ba439619f.jpg" align="middle">
</details>




<h2 id="Enhancing-Fenton-like-Photo-degradation-and-Electrocatalytic-Oxygen-Evolution-Reaction-OER-in-Fe-doped-Copper-Oxide-CuO-Catalysts"><a href="#Enhancing-Fenton-like-Photo-degradation-and-Electrocatalytic-Oxygen-Evolution-Reaction-OER-in-Fe-doped-Copper-Oxide-CuO-Catalysts" class="headerlink" title="Enhancing Fenton-like Photo-degradation and Electrocatalytic Oxygen   Evolution Reaction (OER) in Fe-doped Copper Oxide (CuO) Catalysts"></a>Enhancing Fenton-like Photo-degradation and Electrocatalytic Oxygen   Evolution Reaction (OER) in Fe-doped Copper Oxide (CuO) Catalysts</h2><p><strong>Authors:Suresh Chandra Baral, Dilip Sasmal, Sayak Datta, Mange Ram, Krishna Kanta Haldar, A. Mekki, Somaditya Sen</strong></p>
<p>Although hydrogen generation by water electrolysis is the cheapest of all other available sources, water splitting still occurs with sluggish kinetics. It is a challenging barrier for H2 production on a large scale. Moreover, research is still underway to understand the oxygen evolution reaction (OER) and design the catalysts with improved OER performance. Herein, we report the synthesis, characterization, and OER performance of iron-doped copper oxide (CuO) as low-cost catalysts for water oxidation. The OER occurs at about 1.49 V versus the RHE with a Tafel slope of 69 mV&#x2F;dec in a 1 M KOH solution. The overpotential of 338 mV at 10 mA&#x2F;cm2 is among the lowest compared with other copper-based materials. The catalyst can deliver a stable current density of &gt;10 mA&#x2F;cm2 for more than 10 hours. Additionally, wastewater treatment, particularly synthetic dye wastewater, is vital for preventing water scarcity and adverse effects on human health and ecotoxicology. The as-synthesized catalysts are also utilized for Fenton-like photo-degradation under low-power visible household LED lights toward the most commonly industrially used simulated Methylene blue dye wastewater. Almost complete degradation of the MB dye has been achieved within 50 minutes of visible light irradiation with a first-order rate constant of 0.0973&#x2F;min. This dual functionality feature can open new pathways as a non-noble, highly efficient, and robust catalyst for OER and wastewater treatments. </p>
<blockquote>
<p>å°½ç®¡é€šè¿‡æ°´ç”µè§£ç”Ÿäº§æ°¢æ°”æ˜¯æ‰€æœ‰å…¶ä»–å¯ç”¨æ¥æºä¸­æˆæœ¬æœ€ä½çš„ï¼Œä½†æ°´çš„åˆ†è§£ååº”åŠ¨åŠ›å­¦ä»ç„¶è¾ƒæ…¢ã€‚è¿™æ˜¯å¤§è§„æ¨¡ç”Ÿäº§æ°¢æ°”æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç›®å‰ä»åœ¨ç ”ç©¶æ°§æå‡ºååº”ï¼ˆOERï¼‰ï¼Œå¹¶è®¾è®¡å…·æœ‰æ”¹è¿›OERæ€§èƒ½çš„å‚¬åŒ–å‰‚ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†é“æºæ‚çš„æ°§åŒ–é“œï¼ˆCuOï¼‰çš„åˆæˆã€è¡¨å¾å’ŒOERæ€§èƒ½ï¼Œä½œä¸ºç”¨äºæ°´æ°§åŒ–çš„ä½æˆæœ¬å‚¬åŒ–å‰‚ã€‚åœ¨1MKOHæº¶æ¶²ä¸­ï¼ŒOERåœ¨çº¦1.49Vï¼ˆç›¸å¯¹äºRHEï¼‰ä¸‹è¿›è¡Œï¼ŒTafelæ–œç‡ä¸º69mV&#x2F;decã€‚åœ¨10mA&#x2F;cm2ä¸‹çš„è¿‡ç”µä½ä¸å…¶ä»–é“œåŸºææ–™ç›¸æ¯”ï¼Œæ˜¯æœ€ä½çš„ä¹‹ä¸€ã€‚è¯¥å‚¬åŒ–å‰‚èƒ½ç¨³å®šæä¾›è¶…è¿‡10mA&#x2F;cm2çš„ç”µæµå¯†åº¦é•¿è¾¾10å°æ—¶ä»¥ä¸Šã€‚å¦å¤–ï¼ŒåºŸæ°´å¤„ç†ï¼Œç‰¹åˆ«æ˜¯åˆæˆæŸ“æ–™åºŸæ°´å¤„ç†ï¼Œå¯¹äºé¢„é˜²æ°´çŸ­ç¼ºä»¥åŠå¯¹äººç±»å¥åº·å’Œç”Ÿæ€æ¯’æ€§çš„ä¸è‰¯å½±å“è‡³å…³é‡è¦ã€‚æ‰€åˆæˆçš„å‚¬åŒ–å‰‚è¿˜ç”¨äºä½åŠŸç‡å¯è§å…‰å®¶ç”¨LEDç¯ä¸‹çš„èŠ¬é¡¿å¼å…‰é™è§£ï¼Œé’ˆå¯¹å·¥ä¸šä¸Šæœ€å¸¸ç”¨çš„æ¨¡æ‹Ÿç”²åŸºè“æŸ“æ–™åºŸæ°´ã€‚åœ¨å¯è§å…‰ç…§å°„50åˆ†é’Ÿå†…ï¼Œå‡ ä¹å¯ä»¥å®Œå…¨é™è§£MBæŸ“æ–™ï¼Œä¸€é˜¶é€Ÿç‡å¸¸æ•°ä¸º0.0973&#x2F;åˆ†é’Ÿã€‚è¿™ç§åŒé‡åŠŸèƒ½ç‰¹æ€§å¯ä»¥å¼€è¾Ÿæ–°çš„é€”å¾„ï¼Œä½œä¸ºéè´µé‡‘å±ã€é«˜æ•ˆä¸”ç¨³å®šçš„å‚¬åŒ–å‰‚ï¼Œç”¨äºOERå’ŒåºŸæ°´å¤„ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05637v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŠ¥é“äº†åˆæˆã€è¡¨å¾åŠæ°§æ¼”åŒ–ååº”æ€§èƒ½çš„é“æºæ‚æ°§åŒ–é“œä½œä¸ºä½æˆæœ¬å‚¬åŒ–å‰‚åœ¨æ°´æ°§åŒ–ä¸­çš„åº”ç”¨ã€‚è¯¥å‚¬åŒ–å‰‚å…·æœ‰ä½è¶…ç”µåŠ¿ã€è‰¯å¥½ç¨³å®šæ€§å’ŒåŒåŠŸèƒ½ç‰¹æ€§ï¼Œå¯é«˜æ•ˆé™è§£æŸ“æ–™åºŸæ°´ã€‚è¯¥å‚¬åŒ–å‰‚æœ‰æœ›ä½œä¸ºé«˜æ•ˆã€ç¨³å¥çš„éè´µé‡‘å±å‚¬åŒ–å‰‚åœ¨æ°´ç”µè§£åŠæ°´å¤„ç†é¢†åŸŸå…·æœ‰å¹¿é˜”åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é“æºæ‚æ°§åŒ–é“œå‚¬åŒ–å‰‚åœ¨æ°´æ°§åŒ–ä¸­çš„åˆæˆæ–¹æ³•å’Œæ€§èƒ½ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†è¯¥å‚¬åŒ–å‰‚ä¸å…¶ä»–é“œåŸºææ–™ç›¸æ¯”å…·æœ‰è¾ƒä½çš„è¶…ç”µåŠ¿ã€‚</li>
<li>å‚¬åŒ–å‰‚èƒ½ç¨³å®šå·¥ä½œè¶…è¿‡10å°æ—¶ï¼Œå…·æœ‰è¾ƒé«˜çš„ç¨³å®šæ€§ã€‚</li>
<li>ä»‹ç»äº†è¯¥å‚¬åŒ–å‰‚åœ¨æŸ“æ–™åºŸæ°´å¤„ç†ä¸­çš„åº”ç”¨ï¼Œå¹¶å±•ç¤ºäº†å…¶å¯¹ç”²åŸºè“æŸ“æ–™çš„å¿«é€Ÿé™è§£æ•ˆæœã€‚</li>
<li>å¼ºè°ƒäº†å‚¬åŒ–å‰‚çš„åŒåŠŸèƒ½ç‰¹æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>ä»‹ç»äº†æ°´å¤„ç†çš„å¿…è¦æ€§ï¼Œä»¥åŠå…¶å¯¹äºè§£å†³æ°´èµ„æºçŸ­ç¼ºå’Œç”Ÿæ€ç¯å¢ƒæ¯’æ€§çš„é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b78a511a2949f235c0813daad5a68ff1.jpg" align="middle">
</details>




<h2 id="Multi-Party-Supervised-Fine-tuning-of-Language-Models-for-Multi-Party-Dialogue-Generation"><a href="#Multi-Party-Supervised-Fine-tuning-of-Language-Models-for-Multi-Party-Dialogue-Generation" class="headerlink" title="Multi-Party Supervised Fine-tuning of Language Models for Multi-Party   Dialogue Generation"></a>Multi-Party Supervised Fine-tuning of Language Models for Multi-Party   Dialogue Generation</h2><p><strong>Authors:Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji</strong></p>
<p>Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ç»è¿‡å¾®è°ƒä»¥å‚ä¸äºŒå…ƒæˆ–ä¸¤æ–¹å¯¹è¯ï¼Œä½†å®ƒä»¬æ— æ³•å¾ˆå¥½åœ°é€‚åº”å¤šæ–¹å¯¹è¯ï¼ˆMPDï¼‰ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨åŒ…æ‹¬å¤šäººä¼šè®®ã€è®¨è®ºå’Œæ—¥å¸¸äº¤æµç­‰åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¹‹å‰åŸºäºLLMçš„ç ”ç©¶ä¸»è¦å…³æ³¨å¤šä»£ç†æ¡†æ¶ï¼Œè€Œå®ƒä»¬çš„åŸºå‡†LLMsä»ç„¶æ˜¯å¯¹ä¸¤ä¸¤å¯¹è¯è¿›è¡Œå¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºLLMè®¾è®¡äº†å¤šæ–¹å¾®è°ƒæ¡†æ¶ï¼ˆMuPaSï¼‰ï¼Œè¯¥æ¡†æ¶åŸºäºå¤šæ–¹å¯¹è¯æ•°æ®é›†ï¼Œå¹¶è¯æ˜è¿™ç§ç®€å•çš„æ¡†æ¶å¯ä»¥è®©LLMé«˜æ•ˆä¸”æœ‰æ•ˆåœ°é€‚åº”å¤šæ–¹å¯¹è¯é£æ ¼ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸¤ç§è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥å°†MuPaSè½¬åŒ–ä¸ºMPDæ¨¡æ‹Ÿå™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMuPaSå¯ä»¥å®ç°æœ€æ–°çš„å¤šæ–¹å“åº”ã€æ›´é«˜çš„ä¸‹ä¸€ä½å‘è¨€è€…é¢„æµ‹å‡†ç¡®ç‡ã€æ›´é«˜çš„äººç±»å’Œè‡ªåŠ¨è¯„ä¼°çš„è¯è¯­è´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥åœ¨åˆ†å¸ƒå¤–çš„åœºæ™¯ã€è¯é¢˜å’Œè§’è‰²æè¿°ä¸­ç”Ÿæˆåˆç†çš„å“åº”ã€‚MuPaSæ¡†æ¶å°†LLMè®­ç»ƒä¸æ›´å¤æ‚çš„å¤šæ–¹åº”ç”¨ï¼ˆå¦‚å¯¹è¯ç”Ÿæˆã€è™šæ‹Ÿæ’ç»ƒæˆ–å…ƒå®‡å®™ï¼‰ç›¸ç»“åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”å¯¹å¤šæ–¹å¯¹è¯ï¼ˆMPDï¼‰æ—¶å­˜åœ¨ä¸é€‚åº”çš„é—®é¢˜ï¼Œåˆ¶çº¦äº†å…¶åœ¨å¤šäººä¼šè®®ã€è®¨è®ºå’Œæ—¥å¸¸æ²Ÿé€šç­‰åœºæ™¯çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹å¤šæ–¹å¯¹è¯æ•°æ®é›†çš„å¤šæ–¹å¾®è°ƒæ¡†æ¶ï¼ˆMuPaSï¼‰ï¼Œèƒ½æœ‰æ•ˆæå‡LLMå¯¹å¤šæ–¹å¯¹è¯çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œè¿˜è®¾è®¡äº†ä¸¤ç§è®­ç»ƒç­–ç•¥ï¼Œå°†MuPaSè½¬åŒ–ä¸ºMPDæ¨¡æ‹Ÿå™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMuPaSåœ¨å¤šæ–¹å“åº”ã€é¢„æµ‹ä¸‹ä¸€ä½å‘è¨€è€…ç­‰æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæé«˜äº†è‡ªåŠ¨è¯„ä»·çš„è¯è¯­è´¨é‡ã€‚è¯¥æ¡†æ¶ä¿ƒè¿›äº†LLMåœ¨å¯¹è¯ç”Ÿæˆã€è™šæ‹Ÿæ’ç»ƒæˆ–å…ƒå®‡å®™ç­‰æ›´å¤æ‚çš„å¤šæ–¹åº”ç”¨ä¸­çš„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ–¹å¯¹è¯ï¼ˆMPDï¼‰ä¸­è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šäººåœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ–¹å¾®è°ƒæ¡†æ¶ï¼ˆMuPaSï¼‰ï¼Œæ—¨åœ¨æé«˜LLMåœ¨å¤šæ–¹å¯¹è¯æ•°æ®é›†ä¸­çš„é€‚åº”æ€§ã€‚</li>
<li>è®¾è®¡äº†ä¸¤ç§è®­ç»ƒç­–ç•¥ï¼Œå°†MuPaSè½¬åŒ–ä¸ºMPDæ¨¡æ‹Ÿå™¨ã€‚</li>
<li>MuPaSåœ¨å¤šæ–¹å“åº”å’Œé¢„æµ‹ä¸‹ä¸€ä½å‘è¨€è€…æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>MuPaSæé«˜äº†è‡ªåŠ¨è¯„ä»·çš„è¯è¯­è´¨é‡ã€‚</li>
<li>MuPaSæ¡†æ¶ä¿ƒè¿›äº†LLMåœ¨å¯¹è¯ç”Ÿæˆã€è™šæ‹Ÿæ’ç»ƒæˆ–å…ƒå®‡å®™ç­‰å¤šæ–¹å¤æ‚åº”ç”¨ä¸­çš„è®­ç»ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81acca221e8caa32aee81922407ca2ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f0c53cb747f415c1123463eb547a55b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2085aade886ea917fc6534e67574fc14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c66789496eb570fcccf376bd82cf5d4.jpg" align="middle">
</details>




<h2 id="Benchmarking-Open-ended-Audio-Dialogue-Understanding-for-Large-Audio-Language-Models"><a href="#Benchmarking-Open-ended-Audio-Dialogue-Understanding-for-Large-Audio-Language-Models" class="headerlink" title="Benchmarking Open-ended Audio Dialogue Understanding for Large   Audio-Language Models"></a>Benchmarking Open-ended Audio Dialogue Understanding for Large   Audio-Language Models</h2><p><strong>Authors:Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, Jindong Gu</strong></p>
<p>Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., â€œReally!?â€ with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. </p>
<blockquote>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰å·²ç»è§£é”äº†éŸ³é¢‘å¯¹è¯èƒ½åŠ›ï¼Œå…¶ä¸­éŸ³é¢‘å¯¹è¯æ˜¯LALMsä¸äººç±»ä¹‹é—´å£å¤´è¯­è¨€çš„ç›´æ¥äº¤æµã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚GPT-4oï¼Œå·²ç»ä½¿LALMsèƒ½å¤Ÿè¿›è¡Œä¸äººç±»ä¹‹é—´çš„æ¥å›éŸ³é¢‘å¯¹è¯ã€‚è¿™ä¸€è¿›å±•ä¸ä»…çªå‡ºäº†LALMsçš„æ½œåŠ›ï¼Œä¹Ÿæ‰©å¤§äº†å…¶åœ¨å—éŸ³é¢‘å¯¹è¯æ”¯æŒçš„å„ç§å®é™…åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°è¿™äº›è¿›å±•ï¼Œç›®å‰ä»ç¼ºä¹ä¸€ä¸ªå…¨é¢åŸºå‡†æ¥è¯„ä¼°LALMsåœ¨å¼€æ”¾éŸ³é¢‘å¯¹è¯ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†éŸ³é¢‘å¯¹è¯ç†è§£åŸºå‡†ï¼ˆADU-Benchï¼‰ï¼Œå®ƒç”±4ä¸ªåŸºå‡†æ•°æ®é›†ç»„æˆã€‚è¿™äº›æ•°æ®é›†è¯„ä¼°LALMsåœ¨3ä¸ªä¸€èˆ¬åœºæ™¯ã€12é¡¹æŠ€èƒ½ã€9ç§å¤šè¯­è¨€ä»¥åŠ4ç±»æ­§ä¹‰å¤„ç†ä¸­çš„å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºéŸ³é¢‘å¯¹è¯ä¸­æ­§ä¹‰å¤„ç†çš„è¯„ä¼°ï¼Œè¿™æ¶‰åŠåˆ°å¥å­ç›¸åŒå­—é¢æ„ä¹‰ä¸‹ä¸åŒæ„å›¾çš„è¡¨è¾¾ï¼Œä¾‹å¦‚å¸¦æœ‰ä¸åŒéŸ³è°ƒçš„â€œçœŸçš„å—ï¼ï¼Ÿâ€æ€»è€Œè¨€ä¹‹ï¼ŒADU-BenchåŒ…å«è¶…è¿‡20000ä¸ªå¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ï¼Œç”¨äºè¯„ä¼°LALMsã€‚é€šè¿‡å¯¹13ä¸ªLALMè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œç°æœ‰LALMçš„éŸ³é¢‘å¯¹è¯ç†è§£èƒ½åŠ›ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚ç‰¹åˆ«åœ°ï¼Œä»–ä»¬åœ¨å¤„ç†æ•°å­¦ç¬¦å·å’Œå…¬å¼ã€ç†è§£äººç±»è¡Œä¸ºï¼ˆå¦‚è§’è‰²æ‰®æ¼”ï¼‰ã€ç†è§£å¤šç§è¯­è¨€ä»¥åŠå¤„ç†æ¥è‡ªä¸åŒè¯­éŸ³å…ƒç´ çš„éŸ³é¢‘å¯¹è¯æ­§ä¹‰ï¼ˆå¦‚éŸ³è°ƒã€åœé¡¿ä½ç½®å’ŒåŒéŸ³å­—ï¼‰æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰å·²ç»è§£é”äº†éŸ³é¢‘å¯¹è¯èƒ½åŠ›ï¼Œå¯ä»¥å®ç°ä¸äººç±»çš„ç›´æ¥è¯­éŸ³äº¤æµã€‚å°½ç®¡GPT-4oç­‰æœ€æ–°è¿›å±•ä¸ºLALMså¸¦æ¥äº†åŒå‘éŸ³é¢‘å¯¹è¯çš„èƒ½åŠ›ï¼Œä½†åœ¨å¼€æ”¾éŸ³é¢‘å¯¹è¯ç†è§£æ–¹é¢ä»ç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†éŸ³é¢‘å¯¹è¯ç†è§£åŸºå‡†ï¼ˆADU-Benchï¼‰ï¼ŒåŒ…å«å››ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ç§é€šç”¨åœºæ™¯ã€åäºŒé¡¹æŠ€èƒ½ã€ä¹ç§å¤šè¯­è¨€ä»¥åŠå››ç±»æ­§ä¹‰å¤„ç†ã€‚å°¤å…¶æ˜¯é¦–æ¬¡æå‡ºåœ¨éŸ³é¢‘å¯¹è¯ä¸­è¯„ä¼°æ­§ä¹‰å¤„ç†çš„èƒ½åŠ›ï¼Œå¦‚å¥å­å­—é¢æ„ä¹‰ä¹‹å¤–çš„æ„å›¾è¡¨è¾¾ï¼Œå¦‚å¸¦æœ‰ä¸åŒè¯­è°ƒçš„â€œçœŸçš„å—ï¼Ÿâ€ã€‚æ€»ä¹‹ï¼ŒADU-BenchåŒ…å«è¶…è¿‡ä¸¤ä¸‡æ¬¡çš„å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ï¼Œç”¨äºè¯„ä¼°LALMsã€‚å®éªŒæ˜¾ç¤ºï¼Œç°æœ‰LALMsåœ¨éŸ³é¢‘å¯¹è¯ç†è§£èƒ½åŠ›æ–¹é¢ä»æœ‰è¾ƒå¤§æå‡ç©ºé—´ï¼Œå°¤å…¶åœ¨ç†è§£æ•°å­¦ç¬¦å·å’Œå…¬å¼ã€è§’è‰²è¡Œä¸ºã€å¤šè¯­è¨€ä»¥åŠå¤„ç†ç”±è¯­éŸ³ç‰¹å¾äº§ç”Ÿçš„æ­§ä¹‰ç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰å·²ç»æ”¯æŒéŸ³é¢‘å¯¹è¯èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°ä¸äººç±»çš„ç›´æ¥è¯­éŸ³äº¤æµã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°LALMsåœ¨å¼€æ”¾éŸ³é¢‘å¯¹è¯ç†è§£æ–¹é¢çš„å…¨é¢åŸºå‡†ã€‚</li>
<li>æå‡ºäº†éŸ³é¢‘å¯¹è¯ç†è§£åŸºå‡†ï¼ˆADU-Benchï¼‰ï¼ŒåŒ…å«å››ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºå…¨é¢è¯„ä¼°LALMsçš„èƒ½åŠ›ã€‚</li>
<li>ADU-Benchæ¶µç›–äº†å¤šç§åœºæ™¯ã€æŠ€èƒ½ã€è¯­è¨€å’Œæ­§ä¹‰å¤„ç†ç±»å‹ã€‚</li>
<li>ç¬¬ä¸€æ¬¡åœ¨éŸ³é¢‘å¯¹è¯ä¸­æå‡ºäº†å¯¹æ­§ä¹‰å¤„ç†çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸åŒè¯­è°ƒã€åœé¡¿ä½ç½®å’ŒåŒéŸ³å­—ç­‰è¯­éŸ³ç‰¹å¾äº§ç”Ÿçš„æ­§ä¹‰ã€‚</li>
<li>LALMsåœ¨éŸ³é¢‘å¯¹è¯ç†è§£èƒ½åŠ›æ–¹é¢ä»æœ‰è¾ƒå¤§æå‡ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ•°å­¦ç¬¦å·å’Œå…¬å¼ã€è§’è‰²è¡Œä¸ºç†è§£ã€å¤šè¯­è¨€ä»¥åŠå¤„ç†ç”±è¯­éŸ³ç‰¹å¾äº§ç”Ÿçš„æ­§ä¹‰ç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7d53563cd6de3320a499032bdab5069.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfe950308ea5f2db405f91acf20ab485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-070dffcb66d0d5a9be25450089640d7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56770e3a85e1f0266ff1b0e9b03944a6.jpg" align="middle">
</details>




<h2 id="DEMO-Reframing-Dialogue-Interaction-with-Fine-grained-Element-Modeling"><a href="#DEMO-Reframing-Dialogue-Interaction-with-Fine-grained-Element-Modeling" class="headerlink" title="DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling"></a>DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling</h2><p><strong>Authors:Minzheng Wang, Xinghua Zhang, Kun Chen, Nan Xu, Haiyang Yu, Fei Huang, Wenji Mao, Yongbin Li</strong></p>
<p>Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task $\textbf{D}$ialogue $\textbf{E}$lement $\textbf{MO}$deling, including $\textit{Element Awareness}$ and $\textit{Dialogue Agent Interaction}$, and propose a novel benchmark, $\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å°†å¯¹è¯ä½œä¸ºäººæœºäº¤äº’çš„ä¸­å¿ƒæ¨¡å¼ä¹‹ä¸€ï¼Œè¿™å¯¼è‡´äº†å¤§é‡å¯¹è¯æ—¥å¿—çš„ç§¯ç´¯å’Œå¯¹å¯¹è¯ç”Ÿæˆéœ€æ±‚çš„å¢åŠ ã€‚å¯¹è¯ç”Ÿå‘½å‘¨æœŸä»åºæ›²åˆ°å¯¹è¯å†åˆ°å°¾ç« ï¼ŒåŒ…å«å„ç§å…ƒç´ ã€‚å°½ç®¡å­˜åœ¨è®¸å¤šä¸å¯¹è¯ç›¸å…³çš„ç ”ç©¶ï¼Œä½†ç¼ºä¹åŒ…å«å…¨é¢å¯¹è¯å…ƒç´ çš„åŸºå‡†æµ‹è¯•ï¼Œé˜»ç¢äº†ç²¾ç¡®å»ºæ¨¡å’Œç³»ç»Ÿè¯„ä¼°ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åˆ›æ–°çš„ç ”ç©¶ä»»åŠ¡ï¼Œå³â€œå¯¹è¯å…ƒç´ å»ºæ¨¡â€ï¼ŒåŒ…æ‹¬â€œå…ƒç´ æ„è¯†â€å’Œâ€œå¯¹è¯ä»£ç†äº¤äº’â€ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç”¨äºå…¨é¢å¯¹è¯å»ºæ¨¡å’Œè¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå³â€œDEMOâ€ã€‚å—åˆ°æ¨¡ä»¿å­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†åŸºäºDEMOåŸºå‡†çš„å…·æœ‰ç†Ÿç»ƒå»ºæ¨¡å¯¹è¯å…ƒç´ èƒ½åŠ›çš„ä»£ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰å¾ˆå¤§çš„æå‡æ½œåŠ›ï¼Œæˆ‘ä»¬çš„DEMOä»£ç†åœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04905v1">PDF</a> We release the code and data at <a target="_blank" rel="noopener" href="https://github.com/MozerWang/DEMO">https://github.com/MozerWang/DEMO</a></p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿å¯¹è¯æˆä¸ºäººæœºäº¤äº’çš„æ ¸å¿ƒæ¨¡å¼ä¹‹ä¸€ï¼Œäº§ç”Ÿäº†å¤§é‡çš„å¯¹è¯æ—¥å¿—å’Œå¯¹å¯¹è¯ç”Ÿæˆçš„éœ€æ±‚ã€‚å¯¹è¯ç”Ÿå‘½å‘¨æœŸåŒ…æ‹¬åºæ›²ã€å¯¹è¯å’Œå°¾å¥ç­‰å¤šä¸ªå…ƒç´ ã€‚å°½ç®¡å­˜åœ¨è®¸å¤šä¸å¯¹è¯ç›¸å…³çš„ç ”ç©¶ï¼Œä½†ä»ç¼ºä¹åŒ…å«å…¨é¢å¯¹è¯å…ƒç´ çš„åŸºå‡†æµ‹è¯•ï¼Œé˜»ç¢äº†ç²¾ç¡®å»ºæ¨¡å’Œç³»ç»Ÿè¯„ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åˆ›æ–°çš„ç ”ç©¶ä»»åŠ¡â€œå¯¹è¯å…ƒç´ å»ºæ¨¡â€ï¼ŒåŒ…æ‹¬â€œå…ƒç´ æ„è¯†â€å’Œâ€œå¯¹è¯ä»£ç†äº¤äº’â€ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•â€œDEMOâ€ï¼Œç”¨äºå…¨é¢çš„å¯¹è¯å»ºæ¨¡å’Œè¯„ä¼°ã€‚å—æ¨¡ä»¿å­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†èƒ½å¤ŸåŸºäºDEMOåŸºå‡†æµ‹è¯•ç†Ÿç»ƒå»ºæ¨¡å¯¹è¯å…ƒç´ çš„ä»£ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰å¾ˆå¤§çš„æå‡æ½œåŠ›ï¼Œæˆ‘ä»¬çš„DEMOä»£ç†åœ¨åŸŸå†…å’Œè·¨åŸŸä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆä¸ºå¯¹è¯ç”Ÿæˆçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œäº§ç”Ÿäº†å¤§é‡çš„å¯¹è¯æ—¥å¿—ã€‚</li>
<li>å¯¹è¯ç”Ÿå‘½å‘¨æœŸåŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼Œä»åºæ›²åˆ°å¯¹è¯å†åˆ°å°¾å¥ã€‚</li>
<li>ç›®å‰ç¼ºä¹åŒ…å«å…¨é¢å¯¹è¯å…ƒç´ çš„åŸºå‡†æµ‹è¯•ï¼Œé˜»ç¢äº†ç²¾ç¡®å»ºæ¨¡å’Œç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„ç ”ç©¶ä»»åŠ¡â€œå¯¹è¯å…ƒç´ å»ºæ¨¡â€ï¼ŒåŒ…æ‹¬å…ƒç´ æ„è¯†å’Œå¯¹è¯ä»£ç†äº¤äº’ã€‚</li>
<li>æå‡ºäº†DEMOåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢çš„å¯¹è¯å»ºæ¨¡å’Œè¯„ä¼°ã€‚</li>
<li>åŸºäºæ¨¡ä»¿å­¦ä¹ ï¼Œæ„å»ºäº†èƒ½ç†Ÿç»ƒå»ºæ¨¡å¯¹è¯å…ƒç´ çš„ä»£ç†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰æå‡æ½œåŠ›ï¼ŒDEMOä»£ç†åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42841a8ec39ccef99ff9bde9b4019030.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-289d8db578e64b847f59e1ee1115f8a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bcb5d62416311b4be3f57a1e669ca74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41213fe21ccd9e875114ddfc3a6e9f04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bff41c1cf8cc9abb2f801a8dfab08cde.jpg" align="middle">
</details>




<h2 id="INFP-Audio-Driven-Interactive-Head-Generation-in-Dyadic-Conversations"><a href="#INFP-Audio-Driven-Interactive-Head-Generation-in-Dyadic-Conversations" class="headerlink" title="INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations"></a>INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations</h2><p><strong>Authors:Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, Zhipeng Ge</strong></p>
<p>Imagine having a conversation with a socially intelligent agent. It can attentively listen to your words and offer visual and linguistic feedback promptly. This seamless interaction allows for multiple rounds of conversation to flow smoothly and naturally. In pursuit of actualizing it, we propose INFP, a novel audio-driven head generation framework for dyadic interaction. Unlike previous head generation works that only focus on single-sided communication, or require manual role assignment and explicit role switching, our model drives the agent portrait dynamically alternates between speaking and listening state, guided by the input dyadic audio. Specifically, INFP comprises a Motion-Based Head Imitation stage and an Audio-Guided Motion Generation stage. The first stage learns to project facial communicative behaviors from real-life conversation videos into a low-dimensional motion latent space, and use the motion latent codes to animate a static image. The second stage learns the mapping from the input dyadic audio to motion latent codes through denoising, leading to the audio-driven head generation in interactive scenarios. To facilitate this line of research, we introduce DyConv, a large scale dataset of rich dyadic conversations collected from the Internet. Extensive experiments and visualizations demonstrate superior performance and effectiveness of our method. Project Page: <a target="_blank" rel="noopener" href="https://grisoon.github.io/INFP/">https://grisoon.github.io/INFP/</a>. </p>
<blockquote>
<p>æƒ³è±¡ä¸€ä¸‹ä¸ä¸€ä¸ªå…·æœ‰ç¤¾ä¼šæ™ºèƒ½çš„ä»£ç†è¿›è¡Œå¯¹è¯ã€‚å®ƒèƒ½å¤Ÿä¸“æ³¨åœ°å€¾å¬ä½ çš„è¯è¯­ï¼Œå¹¶æä¾›åŠæ—¶çš„è§†è§‰å’Œè¯­è¨€åé¦ˆã€‚è¿™ç§æ— ç¼äº’åŠ¨å…è®¸å¤šè½®å¯¹è¯æµç•…è‡ªç„¶åœ°å±•å¼€ã€‚ä¸ºäº†å°†å…¶å®ç°ï¼Œæˆ‘ä»¬æå‡ºäº†INFPï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹éŸ³é¢‘é©±åŠ¨çš„å¤´éƒ¨ä½å§¿ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºäºŒå…ƒäº’åŠ¨ã€‚ä¸åŒäºä¹‹å‰åªå…³æ³¨å•æ–¹é¢æ²Ÿé€šçš„å¤´éƒ¨ç”Ÿæˆå·¥ä½œï¼Œæˆ–éœ€è¦æ‰‹åŠ¨åˆ†é…è§’è‰²å’Œæ˜ç¡®çš„è§’è‰²åˆ‡æ¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é©±åŠ¨ä»£ç†å¤´åƒåœ¨è¯´è¯å’Œå€¾å¬çŠ¶æ€ä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œç”±è¾“å…¥çš„åŒå£°éŸ³é¢‘å¼•å¯¼ã€‚å…·ä½“æ¥è¯´ï¼ŒINFPåŒ…æ‹¬åŸºäºè¿åŠ¨çš„å¤´éƒ¨æ¨¡ä»¿é˜¶æ®µå’ŒéŸ³é¢‘å¼•å¯¼çš„è¿åŠ¨ç”Ÿæˆé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µå­¦ä¹ å°†æ¥è‡ªçœŸå®å¯¹è¯è§†é¢‘çš„é¢éƒ¨äº¤æµè¡Œä¸ºæŠ•å½±åˆ°ä¸€ä¸ªä½ç»´è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä½¿ç”¨è¿åŠ¨æ½œåœ¨ä»£ç æ¥é©±åŠ¨é™æ€å›¾åƒã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡å»å™ªå­¦ä¹ å°†è¾“å…¥çš„åŒå£°éŸ³é¢‘æ˜ å°„åˆ°è¿åŠ¨æ½œåœ¨ä»£ç ï¼Œä»è€Œå®ç°äº¤äº’åœºæ™¯ä¸­çš„éŸ³é¢‘é©±åŠ¨å¤´éƒ¨ç”Ÿæˆã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†DyConvï¼Œè¿™æ˜¯ä¸€ä¸ªä»äº’è”ç½‘æ”¶é›†çš„å¤§è§„æ¨¡ä¸°å¯Œçš„äºŒå…ƒå¯¹è¯æ•°æ®é›†ã€‚å¤§é‡çš„å®éªŒå’Œå¯è§†åŒ–å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œæ•ˆæœã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://grisoon.github.io/INFP/%E3%80%82">https://grisoon.github.io/INFP/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04037v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹è¯åœºæ™¯æ„å»ºçš„æ™ºèƒ½äº¤äº’ä»£ç†å½¢è±¡å±•ç¤ºæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨éŸ³é¢‘é©±åŠ¨å¤´éƒ¨ç”Ÿæˆæ¡†æ¶å®ç°åŒæ–¹äº’åŠ¨ä¸­çš„å¯¹è¯æ¨¡æ‹Ÿã€‚é‡‡ç”¨åŸºäºåŠ¨ä½œå¤´éƒ¨æ¨¡ä»¿é˜¶æ®µå’ŒéŸ³é¢‘å¼•å¯¼åŠ¨ä½œç”Ÿæˆé˜¶æ®µçš„æ–¹å¼ï¼Œå®Œæˆä»£ç†è§’è‰²é—´çš„è‡ªç„¶äº¤æµæ¨¡æ‹Ÿã€‚æŠ€æœ¯å…¬å¼€æ¼”ç¤ºé¡µä¸ºï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºå¯¹è¯åœºæ™¯çš„æ™ºèƒ½äº¤äº’ä»£ç†æŠ€æœ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„éŸ³é¢‘é©±åŠ¨å¤´éƒ¨ç”Ÿæˆæ¡†æ¶INFPï¼Œç”¨äºå®ç°åŒæ–¹äº’åŠ¨ä¸­çš„å¯¹è¯æ¨¡æ‹Ÿã€‚</li>
<li>INFPæ¨¡å‹èƒ½è‡ªåŠ¨é©±åŠ¨ä»£ç†è§’è‰²åœ¨å¯¹è¯ä¸­äº¤æ›¿è¿›è¡Œå‘è¨€å’Œè†å¬çŠ¶æ€ï¼Œæ— éœ€æ‰‹åŠ¨è§’è‰²åˆ†é…å’Œåˆ‡æ¢ã€‚</li>
<li>INFPåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šåŸºäºåŠ¨ä½œå¤´éƒ¨æ¨¡ä»¿é˜¶æ®µå’ŒéŸ³é¢‘å¼•å¯¼åŠ¨ä½œç”Ÿæˆé˜¶æ®µã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µå­¦ä¹ ä»ç°å®å¯¹è¯è§†é¢‘ä¸­æå–é¢éƒ¨äº¤æµè¡Œä¸ºï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°ä½ç»´åº¦åŠ¨ä½œæ½œåœ¨ç©ºé—´ï¼Œå¹¶ä½¿ç”¨åŠ¨ä½œæ½œåœ¨ä»£ç é©±åŠ¨é™æ€å›¾åƒè¿›è¡ŒåŠ¨ç”»ç”Ÿæˆã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡é™å™ªå­¦ä¹ å°†è¾“å…¥çš„åŒäººå¯¹è¯éŸ³é¢‘æ˜ å°„åˆ°åŠ¨ä½œæ½œåœ¨ä»£ç ï¼Œå®ç°éŸ³é¢‘é©±åŠ¨çš„å¤´éƒ¨ç”Ÿæˆåœ¨äº¤äº’åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16c76e149541f70b8cde77669efb7290.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3be762eba6154196ed65c70710399ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c084dff357cd500a71ead5639334cda0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1495909db6c3934be6b148d04c1c0a90.jpg" align="middle">
</details>




<h2 id="Educational-Psychological-Dialogue-Robot-Based-on-Multi-Agent-Collaboration"><a href="#Educational-Psychological-Dialogue-Robot-Based-on-Multi-Agent-Collaboration" class="headerlink" title="Educational-Psychological Dialogue Robot Based on Multi-Agent   Collaboration"></a>Educational-Psychological Dialogue Robot Based on Multi-Agent   Collaboration</h2><p><strong>Authors:Shiwen Ni, Min Yang</strong></p>
<p>Intelligent dialogue systems are increasingly used in modern education and psychological counseling fields, but most existing systems are limited to a single domain, cannot deal with both educational and psychological issues, and often lack accuracy and professionalism when dealing with complex issues. To address these problems, this paper proposes an intelligent dialog system that combines educational and psychological counseling functions. The system consists of multiple AI agent, including security detection agent, intent identification agent, educational LLM agent, and psychological LLM agent, which work in concert to ensure the provision of accurate educational knowledge Q&amp;A and psychological support services. Specifically, the system recognizes user-input intentions through an intention classification model and invokes a retrieval-enhanced educational grand model and a psychological grand model fine-tuned with psychological data in order to provide professional educational advice and psychological support. </p>
<blockquote>
<p>æ™ºèƒ½å¯¹è¯ç³»ç»Ÿåœ¨ç°ä»£æ•™è‚²å’Œå¿ƒç†å’¨è¯¢é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰ç³»ç»Ÿä»…é™äºå•ä¸€é¢†åŸŸï¼Œæ— æ³•åŒæ—¶å¤„ç†æ•™è‚²å’Œå¿ƒç†é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§å¸¸å¸¸ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ•™è‚²å’Œå¿ƒç†å’¨è¯¢åŠŸèƒ½çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€‚è¯¥ç³»ç»ŸåŒ…å«å¤šä¸ªAIä»£ç†ï¼ŒåŒ…æ‹¬å®‰å…¨æ£€æµ‹ä»£ç†ã€æ„å›¾è¯†åˆ«ä»£ç†ã€æ•™è‚²LLMä»£ç†å’Œå¿ƒç†LLMä»£ç†ï¼Œå®ƒä»¬ååŒå·¥ä½œï¼Œä»¥ç¡®ä¿æä¾›å‡†ç¡®çš„æ•™è‚²çŸ¥è¯†é—®ç­”å’Œå¿ƒç†å’¨è¯¢æœåŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ„å›¾åˆ†ç±»æ¨¡å‹è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„æ„å›¾ï¼Œå¹¶è°ƒç”¨ç»è¿‡æ•™è‚²å¤§æ•°æ®å¢å¼ºå’Œç”¨å¿ƒç†æ•°æ®å¾®è°ƒçš„æ•™è‚²ç²¾ç»†æ¨¡å‹å’Œå¿ƒç†å’¨è¯¢ç²¾ç»†æ¨¡å‹ï¼Œä»¥æä¾›ä¸“ä¸šçš„æ•™è‚²å»ºè®®å’Œå¿ƒç†å’¨è¯¢æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03847v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ•™è‚²ä¸å¿ƒç†å’¨è¯¢åŠŸèƒ½çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç”±å¤šä¸ªAIä»£ç†ç»„æˆï¼ŒåŒ…æ‹¬å®‰å…¨æ£€æµ‹ä»£ç†ã€æ„å›¾è¯†åˆ«ä»£ç†ã€æ•™è‚²LLMä»£ç†å’Œå¿ƒç†LLMä»£ç†ï¼Œèƒ½å¤Ÿå‡†ç¡®æä¾›æ•™è‚²çŸ¥è¯†é—®ç­”å’Œå¿ƒç†å’¨è¯¢æ”¯æŒæœåŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½å¯¹è¯ç³»ç»Ÿåœ¨ç°ä»£æ•™è‚²å’Œå¿ƒç†å’¨è¯¢é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°ç³»ç»Ÿä»…é™äºå•ä¸€é¢†åŸŸï¼Œæ— æ³•åŒæ—¶å¤„ç†æ•™è‚²å’Œå¿ƒç†é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç»“åˆäº†æ•™è‚²å’Œå¿ƒç†å’¨è¯¢åŠŸèƒ½ã€‚</li>
<li>ç³»ç»ŸåŒ…å«å¤šä¸ªAIä»£ç†ï¼Œå¦‚å®‰å…¨æ£€æµ‹ã€æ„å›¾è¯†åˆ«ã€æ•™è‚²LLMå’Œå¿ƒç†LLMä»£ç†ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡æ„å›¾åˆ†ç±»æ¨¡å‹è¯†åˆ«ç”¨æˆ·è¾“å…¥æ„å›¾ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨æ£€ç´¢å¢å¼ºæ•™è‚²å¤§æ¨¡å‹å’Œç»è¿‡å¿ƒç†æ•°æ®å¾®è°ƒçš„å¿ƒç†å¤§æ¨¡å‹ï¼Œä»¥æä¾›ä¸“ä¸šæ•™è‚²å»ºè®®å’Œå¿ƒç†å’¨è¯¢æ”¯æŒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bfa147367f0f1b795aaed06fe4f05137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf0fe3ed11afe96199b408e5ff620423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1019293f102fbfa37112be9ccd9e1990.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87a87c92fdf0838bd3be026663c0e8c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae02a269ab87ac6c30f2c0a24641031c.jpg" align="middle">
</details>




<h2 id="Intent-driven-In-context-Learning-for-Few-shot-Dialogue-State-Tracking"><a href="#Intent-driven-In-context-Learning-for-Few-shot-Dialogue-State-Tracking" class="headerlink" title="Intent-driven In-context Learning for Few-shot Dialogue State Tracking"></a>Intent-driven In-context Learning for Few-shot Dialogue State Tracking</h2><p><strong>Authors:Zihao Yi, Zhe Xu, Ying Shen</strong></p>
<p>Dialogue state tracking (DST) plays an essential role in task-oriented dialogue systems. However, userâ€™s input may contain implicit information, posing significant challenges for DST tasks. Additionally, DST data includes complex information, which not only contains a large amount of noise unrelated to the current turn, but also makes constructing DST datasets expensive. To address these challenges, we introduce Intent-driven In-context Learning for Few-shot DST (IDIC-DST). By extracting userâ€™s intent, we propose an Intent-driven Dialogue Information Augmentation module to augment the dialogue information, which can track dialogue states more effectively. Moreover, we mask noisy information from DST data and rewrite userâ€™s input in the Intent-driven Examples Retrieval module, where we retrieve similar examples. We then utilize a pre-trained large language model to update the dialogue state using the augmented dialogue information and examples. Experimental results demonstrate that IDIC-DST achieves state-of-the-art performance in few-shot settings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets. </p>
<blockquote>
<p>å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œç”¨æˆ·çš„è¾“å…¥å¯èƒ½åŒ…å«éšæ™¦çš„ä¿¡æ¯ï¼Œç»™DSTä»»åŠ¡å¸¦æ¥äº†æå¤§çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒDSTæ•°æ®åŒ…å«å¤æ‚çš„ä¿¡æ¯ï¼Œä¸ä»…åŒ…å«å¤§é‡ä¸å½“å‰è½®æ¬¡æ— å…³çš„å™ªå£°ï¼Œè€Œä¸”æ„å»ºDSTæ•°æ®é›†çš„æˆæœ¬ä¹Ÿå¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ„å›¾çš„æƒ…å¢ƒå†…å­¦ä¹ ç”¨äºå°‘é‡DSTï¼ˆIDIC-DSTï¼‰ã€‚é€šè¿‡æå–ç”¨æˆ·çš„æ„å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ„å›¾çš„å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼Œä»¥å¢å¼ºå¯¹è¯ä¿¡æ¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è·Ÿè¸ªå¯¹è¯çŠ¶æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»DSTæ•°æ®ä¸­å±è”½äº†å˜ˆæ‚çš„ä¿¡æ¯ï¼Œå¹¶åœ¨åŸºäºæ„å›¾çš„ç¤ºä¾‹æ£€ç´¢æ¨¡å—ä¸­é‡å†™äº†ç”¨æˆ·çš„è¾“å…¥ï¼Œä»ä¸­æ£€ç´¢ç›¸ä¼¼çš„ç¤ºä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨å¢å¼ºåçš„å¯¹è¯ä¿¡æ¯å’Œç¤ºä¾‹æ¥æ›´æ–°å¯¹è¯çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°‘é‡è®¾ç½®ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03270v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚ç„¶è€Œï¼Œç”¨æˆ·çš„è¾“å…¥å¯èƒ½åŒ…å«éšå¼ä¿¡æ¯ï¼Œç»™DSTä»»åŠ¡å¸¦æ¥æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒDSTæ•°æ®åŒ…å«å¤æ‚ä¿¡æ¯ï¼Œä¸ä»…åŒ…å«å¤§é‡ä¸å½“å‰å¯¹è¯æ— å…³çš„å™ªå£°ï¼Œè€Œä¸”æ„å»ºDSTæ•°æ®é›†çš„æˆæœ¬å¾ˆé«˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ„å›¾çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼ˆIDIC-DSTï¼‰ã€‚é€šè¿‡æå–ç”¨æˆ·æ„å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ„å›¾çš„å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼Œä»¥å¢å¼ºå¯¹è¯ä¿¡æ¯ï¼Œæ›´æœ‰æ•ˆåœ°è·Ÿè¸ªå¯¹è¯çŠ¶æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»DSTæ•°æ®ä¸­å±è”½äº†å™ªéŸ³ä¿¡æ¯ï¼Œå¹¶åœ¨åŸºäºæ„å›¾çš„ç¤ºä¾‹æ£€ç´¢æ¨¡å—ä¸­é‡å†™äº†ç”¨æˆ·è¾“å…¥ï¼Œä»ä¸­æ£€ç´¢äº†ç±»ä¼¼çš„ç¤ºä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„ å¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆ©ç”¨å¢å¼ºçš„å¯¹è¯ä¿¡æ¯å’Œç¤ºä¾‹æ›´æ–°å¯¹è¯çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°æ ·æœ¬è®¾ç½®ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç”¨æˆ·çš„è¾“å…¥å¯èƒ½åŒ…å«éšå¼ä¿¡æ¯ï¼Œå¯¹DSTä»»åŠ¡æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>DSTæ•°æ®åŒ…å«å¤æ‚ä¿¡æ¯å’Œå™ªéŸ³ï¼Œå¢åŠ äº†æ„å»ºæ•°æ®é›†çš„æˆæœ¬ã€‚</li>
<li>IDIC-DSTæ–¹æ³•é€šè¿‡åŸºäºæ„å›¾çš„å¯¹è¯ä¿¡æ¯å¢å¼ºå’Œç¤ºä¾‹æ£€ç´¢æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>IDIC-DSTèƒ½æ›´æœ‰æ•ˆåœ°è·Ÿè¸ªå¯¹è¯çŠ¶æ€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°æ ·æœ¬è®¾ç½®è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8742c6b8028b15b6244c52f761b6bde9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-947f300819b2f915778e4b9639623290.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1cf68b86876326142a7ddfc117835ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cf7ba27ee5a1f9a8924ebb587f9c4db.jpg" align="middle">
</details>




<h2 id="Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data"><a href="#Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data" class="headerlink" title="Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data"></a>Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data</h2><p><strong>Authors:Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</strong></p>
<p>The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni%7D">https://huggingface.co/spaces/KE-Team/KE-Omni}</a>. </p>
<blockquote>
<p>GPT-4oæ˜¯å®æ—¶é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚å…¶æ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æµç•…æ€§ä¸ä»…å¼•èµ·äº†å…³æ³¨ï¼Œè¿˜åˆºæ¿€äº†è¯¥é¢†åŸŸçš„ç ”ç©¶å…´è¶£ã€‚è¿™ç§å®æ—¶è¯­éŸ³äº¤äº’åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå³æ—¶å“åº”çš„åœºæ™¯ä¸­å°¤å…¶æœ‰ä»·å€¼ï¼Œèƒ½æå¤§åœ°æå‡ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œå…³äºå®æ—¶å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹ä¸­æ–‡çš„ç ”ç©¶ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†KE-Omniï¼Œè¿™æ˜¯ä¸€æ¬¾æ— ç¼å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºKe-SpeechChatæ„å»ºã€‚Ke-SpeechChatæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡åˆæˆè¯­éŸ³äº¤äº’æ•°æ®é›†ï¼ŒåŒ…å«700ä¸‡ä¸­è‹±æ–‡å­—ç¬¦çš„å¯¹è¯å†…å®¹ï¼Œæ¶µç›–42,002ä½å‘è¨€äººï¼Œæ€»è®¡è¶…è¿‡6ä¸‡å°æ—¶ï¼Œæå¤§åœ°æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ä¸å‘å±•ã€‚æ¼”ç¤ºåœ°å€ï¼š[<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni]%E3%80%82">https://huggingface.co/spaces/KE-Team/KE-Omni]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01078v2">PDF</a> KE-Omni, Ke-SpeechChat</p>
<p><strong>Summary</strong></p>
<p>GPT-4oå®ç°äº†é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®æ—¶äº¤äº’ï¼Œå…¶ä½å»¶è¿Ÿå’Œé«˜æµç•…æ€§å¼•äººæ³¨ç›®å¹¶æ¿€å‘äº†ç ”ç©¶å…´è¶£ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå³æ—¶å“åº”çš„åœºæ™¯ä¸­ï¼Œè¿™ç§å®æ—¶è¯­éŸ³äº¤äº’æå¤§åœ°æå‡äº†ç”¨æˆ·ä½“éªŒã€‚KE-Omniæ˜¯ä¸€ä¸ªåŸºäºKe-SpeechChatæ•°æ®é›†æ„å»ºçš„æ— ç¼å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«700ä¸‡ä¸­è‹±æ–‡å¯¹è¯ï¼Œæ¶µç›–42,002ä½å‘è¨€è€…çš„è¶…è¿‡6ä¸‡å°æ—¶çš„æ•°æ®ã€‚è¿™æå¤§åœ°æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ä¸å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oå®ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å®æ—¶è¯­éŸ³äº¤äº’ã€‚</li>
<li>GPT-4oå…·æœ‰ä½å»¶è¿Ÿå’Œé«˜æµç•…æ€§ç‰¹ç‚¹ã€‚</li>
<li>å®æ—¶è¯­éŸ³äº¤äº’åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå“åº”çš„åœºæ™¯ä¸­éå¸¸æœ‰ä»·å€¼ã€‚</li>
<li>KE-Omniæ˜¯ä¸€ä¸ªæ— ç¼çš„å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚</li>
<li>KE-Omniæ˜¯åŸºäºKe-SpeechChatæ•°æ®é›†æ„å»ºçš„ï¼ŒåŒ…å«700ä¸‡ä¸­è‹±æ–‡å¯¹è¯ã€‚</li>
<li>Ke-SpeechChatæ•°æ®é›†æ¶µç›–å¤§é‡å‘è¨€è€…å’Œå°æ—¶æ•°ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸç ”ç©¶ä¸å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be63d03367d1646c10129ffa39a43920.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40f6a84b1329d36c056c958d085b932a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e846567311c2df8847f41091a8724d2.jpg" align="middle">
</details>




<h2 id="Listening-for-Expert-Identified-Linguistic-Features-Assessment-of-Audio-Deepfake-Discernment-among-Undergraduate-Students"><a href="#Listening-for-Expert-Identified-Linguistic-Features-Assessment-of-Audio-Deepfake-Discernment-among-Undergraduate-Students" class="headerlink" title="Listening for Expert Identified Linguistic Features: Assessment of Audio   Deepfake Discernment among Undergraduate Students"></a>Listening for Expert Identified Linguistic Features: Assessment of Audio   Deepfake Discernment among Undergraduate Students</h2><p><strong>Authors:Noshaba N. Bhalli, Nehal Naqvi, Chloe Evered, Christine Mallinson, Vandana P. Janeja</strong></p>
<p>This paper evaluates the impact of training undergraduate students to improve their audio deepfake discernment ability by listening for expert-defined linguistic features. Such features have been shown to improve performance of AI algorithms; here, we ascertain whether this improvement in AI algorithms also translates to improvement of the perceptual awareness and discernment ability of listeners. With humans as the weakest link in any cybersecurity solution, we propose that listener discernment is a key factor for improving trustworthiness of audio content. In this study we determine whether training that familiarizes listeners with English language variation can improve their abilities to discern audio deepfakes. We focus on undergraduate students, as this demographic group is constantly exposed to social media and the potential for deception and misinformation online. To the best of our knowledge, our work is the first study to uniquely address English audio deepfake discernment through such techniques. Our research goes beyond informational training by introducing targeted linguistic cues to listeners as a deepfake discernment mechanism, via a training module. In a pre-&#x2F;post- experimental design, we evaluated the impact of the training across 264 students as a representative cross section of all students at the University of Maryland, Baltimore County, and across experimental and control sections. Findings show that the experimental group showed a statistically significant decrease in their unsurety when evaluating audio clips and an improvement in their ability to correctly identify clips they were initially unsure about. While results are promising, future research will explore more robust and comprehensive trainings for greater impact. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è¯„ä¼°é€šè¿‡å¬å–ä¸“å®¶å®šä¹‰çš„è¯­è¨€ç‰¹å¾æ¥è®­ç»ƒæœ¬ç§‘ç”Ÿï¼Œæé«˜å…¶è¾¨è¯†éŸ³é¢‘æ·±åº¦ä¼ªé€ å†…å®¹çš„èƒ½åŠ›ã€‚è¿™äº›è¯­è¨€ç‰¹å¾å·²è¢«è¯æ˜å¯ä»¥æé«˜AIç®—æ³•çš„æ€§èƒ½ï¼›åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç¡®å®šAIç®—æ³•çš„æ”¹è¿›æ˜¯å¦ä¹Ÿè½¬åŒ–ä¸ºå¬ä¼—æ„ŸçŸ¥æ„è¯†å’Œè¾¨è¯†èƒ½åŠ›çš„æ”¹è¿›ã€‚é‰´äºäººç±»æ˜¯ä»»ä½•ç½‘ç»œå®‰å…¨è§£å†³æ–¹æ¡ˆä¸­æœ€è–„å¼±çš„ç¯èŠ‚ï¼Œæˆ‘ä»¬æå‡ºå¬ä¼—çš„è¾¨è¯†èƒ½åŠ›æ˜¯æé«˜éŸ³é¢‘å†…å®¹å¯ä¿¡åº¦çš„å…³é”®å› ç´ ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šé€šè¿‡è®©å¬ä¼—ç†Ÿæ‚‰è‹±è¯­è¯­è¨€å˜åŒ–æ¥åŸ¹è®­æ˜¯å¦å¯ä»¥æé«˜ä»–ä»¬è¾¨è¯†éŸ³é¢‘æ·±åº¦ä¼ªé€ å†…å®¹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å…³æ³¨æœ¬ç§‘ç”Ÿï¼Œå› ä¸ºè¿™ä¸ªç¾¤ä½“ä¸æ–­æ¥è§¦ç¤¾äº¤åª’ä½“å’Œåœ¨çº¿æ¬ºéª—å’Œè¯¯å¯¼ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯é€šè¿‡è¿™äº›æŠ€æœ¯ç‹¬ç‰¹è§£å†³è‹±è¯­éŸ³é¢‘æ·±åº¦ä¼ªé€ è¾¨è¯†çš„é¦–ä¸ªç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡å¼•å…¥æœ‰é’ˆå¯¹æ€§çš„è¯­è¨€çº¿ç´¢ä½œä¸ºæ·±åº¦ä¼ªé€ è¾¨è¯†æœºåˆ¶æ¥è¶…è¶Šä¿¡æ¯åŸ¹è®­ï¼Œé€šè¿‡åŸ¹è®­æ¨¡å—å‘å¬ä¼—ä»‹ç»ã€‚åœ¨é¢„å®éªŒå’Œå®éªŒè®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†åŸ¹è®­å¯¹é©¬é‡Œå…°å¤§å­¦å·´å°”çš„æ‘©å¿æ‰€æœ‰å­¦ç”Ÿçš„ä»£è¡¨æ ·æœ¬ä»¥åŠå®éªŒå’Œå¯¹ç…§ç»„å…±264åå­¦ç”Ÿçš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå®éªŒç»„åœ¨è¯„ä¼°éŸ³é¢‘å‰ªè¾‘æ—¶çš„ä¸ç¡®å®šæ€§æ˜¾è‘—é™ä½ï¼Œå¹¶ä¸”åœ¨è¯†åˆ«ä»–ä»¬æœ€åˆä¸ç¡®å®šçš„å‰ªè¾‘ç‰‡æ®µæ—¶èƒ½åŠ›æœ‰æ‰€æé«˜ã€‚è™½ç„¶ç»“æœä»¤äººé¼“èˆï¼Œä½†æœªæ¥çš„ç ”ç©¶å°†æ¢ç´¢æ›´ç¨³å¥å’Œå…¨é¢çš„åŸ¹è®­ä»¥äº§ç”Ÿæ›´å¤§çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14586v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡æ¢ç´¢äº†è®­ç»ƒæœ¬ç§‘ç”Ÿæå‡è¾¨è¯†éŸ³é¢‘æ·±åº¦ä¼ªé€ èƒ½åŠ›çš„æ–¹æ³•ï¼Œé‡ç‚¹æ˜¯é€šè¿‡å¬å–ä¸“å®¶å®šä¹‰çš„è¯­éŸ³ç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚ç ”ç©¶é€šè¿‡è®©äººå·¥æ™ºèƒ½ç®—æ³•ä¸äººç±»å¬ä¼—çš„æ„ŸçŸ¥è¾¨è¯†èƒ½åŠ›è¿›è¡Œæ¯”è¾ƒï¼Œç¡®å®šAIç®—æ³•çš„æ”¹è¿›æ˜¯å¦ä¹Ÿèƒ½æå‡å¬ä¼—çš„è¾¨è¯†èƒ½åŠ›ã€‚é‰´äºäººç±»æ˜¯ä»»ä½•ç½‘ç»œå®‰å…¨è§£å†³æ–¹æ¡ˆä¸­çš„è–„å¼±ç¯èŠ‚ï¼Œå› æ­¤å¬ä¼—çš„è¾¨è¯†èƒ½åŠ›æ˜¯æå‡éŸ³é¢‘å†…å®¹å¯ä¿¡åº¦çš„å…³é”®å› ç´ ã€‚æœ¬ç ”ç©¶èšç„¦æœ¬ç§‘ç”Ÿç¾¤ä½“ï¼Œé’ˆå¯¹ä»–ä»¬ç†Ÿæ‚‰è‹±è¯­è¯­éŸ³å˜åŒ–è¿›è¡ŒåŸ¹è®­ï¼Œä»¥æå‡ä»–ä»¬è¾¨è¯†éŸ³é¢‘æ·±åº¦ä¼ªé€ çš„èƒ½åŠ›ï¼Œå› ä¸ºè¿™ä¸€ç¾¤ä½“é¢‘ç¹æ¥è§¦ç¤¾äº¤åª’ä½“å’Œåœ¨çº¿æ¬ºéª—ä¸è¯¯å¯¼ä¿¡æ¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡é’ˆå¯¹æ­¤ç±»æŠ€æœ¯é‡‡ç”¨ç‹¬ç‰¹çš„æ–¹æ³•è§£å†³è‹±è¯­éŸ³é¢‘æ·±åº¦ä¼ªé€ çš„è¾¨è¯†é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡å¼•å…¥æœ‰é’ˆå¯¹æ€§çš„è¯­è¨€çº¿ç´¢ä½œä¸ºæ·±åº¦ä¼ªé€ è¾¨è¯†æœºåˆ¶æ¥è¶…è¶Šä¿¡æ¯åŸ¹è®­ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å—è®©å¬ä¼—æŒæ¡æ­¤ç§æœºåˆ¶ã€‚æˆ‘ä»¬ä»¥å‰æœŸå’ŒåæœŸå®éªŒè®¾è®¡è¯„ä¼°äº†åŸ¹è®­å¯¹é©¬é‡Œå…°å¤§å­¦å·´å°”çš„æ‘©å¿æ ¡åŒºå…¨ä½“å­¦ç”Ÿä¸­é€‰å–çš„ä»£è¡¨æ€§æ ·æœ¬â€”â€”å…±264åå­¦ç”Ÿçš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå®éªŒç»„äººå‘˜åœ¨è¯„ä¼°éŸ³é¢‘ç‰‡æ®µæ—¶çš„ä¸ç¡®å®šæ„Ÿæ˜æ˜¾é™ä½ï¼Œä»–ä»¬æœ€åˆä¸ç¡®å®šçš„ç‰‡æ®µçš„è¾¨è¯†å‡†ç¡®æ€§ä¹Ÿå¾—åˆ°æå‡ã€‚è™½ç„¶è¿™äº›ç»“æœé¢‡å…·å‰æ™¯ï¼Œä½†æœªæ¥ä»éœ€æ¢ç´¢æ›´ç¨³å¥å’Œå…¨é¢çš„åŸ¹è®­æ–¹æ¡ˆä»¥äº§ç”Ÿæ›´å¤§çš„å½±å“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶è¯„ä¼°äº†è®­ç»ƒæœ¬ç§‘ç”Ÿæå‡éŸ³é¢‘æ·±åº¦ä¼ªé€ è¾¨è¯†èƒ½åŠ›çš„æ•ˆæœã€‚</li>
<li>é€šè¿‡å¬å–ä¸“å®¶å®šä¹‰çš„è¯­éŸ³ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œä»¥æå‡å¬ä¼—çš„è¾¨è¯†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°AIç®—æ³•çš„æ”¹è¿›åŒæ ·æå‡äº†äººç±»å¬ä¼—çš„è¾¨è¯†èƒ½åŠ›æ„ŸçŸ¥ã€‚</li>
<li>äººç±»æ˜¯ç½‘ç»œå®‰å…¨ä¸­çš„è–„å¼±ç¯èŠ‚ï¼Œå› æ­¤å¬ä¼—çš„è¾¨è¯†èƒ½åŠ›å¯¹éŸ³é¢‘å†…å®¹çš„ä¿¡ä»»åº¦è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶èšç„¦æœ¬ç§‘ç”Ÿç¾¤ä½“ï¼Œå…³æ³¨ä»–ä»¬å¯¹è‹±è¯­è¯­éŸ³å˜åŒ–çš„ç†Ÿæ‚‰ç¨‹åº¦åŸ¹è®­ï¼Œä»¥å¼ºåŒ–ä»–ä»¬è¾¨è¯†éŸ³é¢‘æ·±åº¦ä¼ªé€ çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æ˜¯é¦–æ¬¡é’ˆå¯¹è‹±è¯­éŸ³é¢‘æ·±åº¦ä¼ªé€ çš„è¾¨è¯†é—®é¢˜è¿›è¡Œçš„ç‹¬ç‰¹ç ”ç©¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a831978796f5f8c3a940366417d31a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a47ccfe0961d6b005824e2f8a4036b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84cbf9d02d5d7cb80266c1459c4cc034.jpg" align="middle">
</details>




<h2 id="Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening"><a href="#Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening" class="headerlink" title="Uncovering the role of semantic and acoustic cues in normal and dichotic   listening"></a>Uncovering the role of semantic and acoustic cues in normal and dichotic   listening</h2><p><strong>Authors:Akshara Soman, Sai Samrat Kankanala, Sriram Ganapathy</strong></p>
<p>Despite extensive research, the precise role of acoustic and semantic cues in complex speech perception tasks remains unclear. In this study, we propose a paradigm to understand the encoding of these cues in electroencephalogram (EEG) data, using match-mismatch (MM) classification task. The MM task involves determining whether the stimulus and response correspond to each other or not. We design a multi-modal sequence model, based on long short term memory (LSTM) architecture, to perform the MM task. The model is input with acoustic stimulus (derived from the speech envelope), semantic stimulus (derived from textual representations of the speech content), and neural response (derived from the EEG data). Our experiments are performed on two separate conditions, i) natural passive listening condition and, ii) an auditory attention based dichotic listening condition. Using the MM task as the analysis framework, we observe that - a) speech perception is fragmented based on word boundaries, b) acoustic and semantic cues offer similar levels of MM task performance in natural listening conditions, and c) semantic cues offer significantly improved MM classification over acoustic cues in dichotic listening task. Further, the study provides evidence of right ear advantage in dichotic listening conditions. </p>
<blockquote>
<p>å°½ç®¡è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œä½†å£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚çš„è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾ç¡®ä½œç”¨ä»ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®ç¼–ç è¿™äº›çº¿ç´¢çš„ç†è§£èŒƒå¼ï¼Œé‡‡ç”¨åŒ¹é…-ä¸åŒ¹é…ï¼ˆMMï¼‰åˆ†ç±»ä»»åŠ¡ã€‚MMä»»åŠ¡æ¶‰åŠç¡®å®šåˆºæ¿€ä¸å“åº”æ˜¯å¦ç›¸å¯¹åº”ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºé•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰æ¶æ„çš„å¤šæ¨¡æ€åºåˆ—æ¨¡å‹ï¼Œä»¥æ‰§è¡ŒMMä»»åŠ¡ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥åŒ…æ‹¬å£°éŸ³åˆºæ¿€ï¼ˆæ¥æºäºè¯­éŸ³åŒ…ç»œï¼‰ã€è¯­ä¹‰åˆºæ¿€ï¼ˆæ¥æºäºè¯­éŸ³å†…å®¹çš„æ–‡æœ¬è¡¨ç¤ºï¼‰å’Œç¥ç»å“åº”ï¼ˆæ¥æºäºEEGæ•°æ®ï¼‰ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨ä¸¤ç§ä¸åŒæ¡ä»¶ä¸‹è¿›è¡Œçš„ï¼Œå³ä¸€ï¼‰è‡ªç„¶è¢«åŠ¨è†å¬æ¡ä»¶ï¼Œä»¥åŠäºŒï¼‰åŸºäºå¬è§‰æ³¨æ„åŠ›çš„åŒè€³è†å¬æ¡ä»¶ã€‚ä»¥MMä»»åŠ¡ä¸ºåˆ†ææ¡†æ¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼šaï¼‰è¯­éŸ³æ„ŸçŸ¥æ˜¯åŸºäºå•è¯è¾¹ç•Œç‰‡æ®µåŒ–çš„ï¼›bï¼‰åœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹ï¼Œå£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨MMä»»åŠ¡ä¸­çš„è¡¨ç°ç›¸ä¼¼ï¼›cï¼‰åœ¨åŒè€³è†å¬ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢ç›¸å¯¹äºå£°éŸ³çº¿ç´¢èƒ½æ˜¾è‘—æ”¹å–„MMåˆ†ç±»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†åŒè€³è†å¬æ¡ä»¶ä¸‹å³è€³ä¼˜åŠ¿çš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11308v1">PDF</a> 9 Pages, 4 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç¼–ç æœºåˆ¶ã€‚é€šè¿‡åŒ¹é…-ä¸åŒ¹é…åˆ†ç±»ä»»åŠ¡ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§åŸºäºLSTMæ¶æ„çš„å¤šæ¨¡å¼åºåˆ—æ¨¡å‹æ¥å¤„ç†å£°å­¦åˆºæ¿€ã€è¯­ä¹‰åˆºæ¿€å’Œç¥ç»ååº”ã€‚å®éªŒåˆ†ä¸ºè‡ªç„¶è¢«åŠ¨è†å¬å’ŒåŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒæ­§å¼è†å¬ä¸¤ç§æ¡ä»¶ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­éŸ³æ„ŸçŸ¥ä»¥å•è¯è¾¹ç•Œä¸ºåŸºç¡€è¿›è¡Œåˆ†å‰²ï¼Œåœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹ï¼Œå£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…ä»»åŠ¡è¡¨ç°ç›¸ä¼¼ï¼›è€Œåœ¨äºŒæ­§å¼è†å¬ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…åˆ†ç±»æ˜æ˜¾ä¼˜äºå£°å­¦çº¿ç´¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜äº†äºŒæ­§å¼è†å¬æ¡ä»¶ä¸‹çš„å³è€³ä¼˜åŠ¿ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨åŒ¹é…-ä¸åŒ¹é…åˆ†ç±»ä»»åŠ¡ï¼Œä»¥äº†è§£å£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç¼–ç ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºLSTMæ¶æ„çš„å¤šæ¨¡å¼åºåˆ—æ¨¡å‹ï¼Œå¤„ç†å£°å­¦åˆºæ¿€ã€è¯­ä¹‰åˆºæ¿€å’Œç¥ç»ååº”ã€‚</li>
<li>å®éªŒåœ¨è‡ªç„¶è¢«åŠ¨è†å¬å’ŒåŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒæ­§å¼è†å¬ä¸¤ç§æ¡ä»¶ä¸‹è¿›è¡Œã€‚</li>
<li>è¯­éŸ³æ„ŸçŸ¥ä»¥å•è¯è¾¹ç•Œä¸ºåŸºç¡€è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>åœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹ï¼Œå£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…ä»»åŠ¡è¡¨ç°ç›¸ä¼¼ã€‚</li>
<li>åœ¨äºŒæ­§å¼è†å¬ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…åˆ†ç±»ä¼˜äºå£°å­¦çº¿ç´¢ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-702194aa412ab66d9b55f848bb4b0802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f88c633e521445f6a5e0c1930330554c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2caaa7973dbbc8b7feaaeb52a5006feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77deb2d8824993a0831777932f5b46c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e94c411962448b979a6fa8a23923d9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4df85715af024e597b14960f49d00b90.jpg" align="middle">
</details>




<h2 id="Can-Generic-LLMs-Help-Analyze-Child-adult-Interactions-Involving-Children-with-Autism-in-Clinical-Observation"><a href="#Can-Generic-LLMs-Help-Analyze-Child-adult-Interactions-Involving-Children-with-Autism-in-Clinical-Observation" class="headerlink" title="Can Generic LLMs Help Analyze Child-adult Interactions Involving   Children with Autism in Clinical Observation?"></a>Can Generic LLMs Help Analyze Child-adult Interactions Involving   Children with Autism in Clinical Observation?</h2><p><strong>Authors:Tiantian Feng, Anfeng Xu, Rimita Lahiri, Helen Tager-Flusberg, So Hyun Kim, Somer Bishop, Catherine Lord, Shrikanth Narayanan</strong></p>
<p>Large Language Models (LLMs) have shown significant potential in understanding human communication and interaction. However, their performance in the domain of child-inclusive interactions, including in clinical settings, remains less explored. In this work, we evaluate generic LLMsâ€™ ability to analyze child-adult dyadic interactions in a clinically relevant context involving children with ASD. Specifically, we explore LLMs in performing four tasks: classifying child-adult utterances, predicting engaged activities, recognizing language skills and understanding traits that are clinically relevant. Our evaluation shows that generic LLMs are highly capable of analyzing long and complex conversations in clinical observation sessions, often surpassing the performance of non-expert human evaluators. The results show their potential to segment interactions of interest, assist in language skills evaluation, identify engaged activities, and offer clinical-relevant context for assessments. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£äººç±»æ²Ÿé€šå’Œäº¤äº’æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å„¿ç«¥äº¤äº’é¢†åŸŸï¼ŒåŒ…æ‹¬åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†é€šç”¨LLMåˆ†ææ‚£æœ‰è‡ªé—­ç—‡è°±ç³»éšœç¢çš„å„¿ç«¥åœ¨çš„ä¸´åºŠç›¸å…³ä¸Šä¸‹æ–‡ä¸­ä¸æˆäººè¿›è¡ŒäºŒå…ƒäº’åŠ¨çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†LLMæ‰§è¡Œå››ä¸ªä»»åŠ¡çš„èƒ½åŠ›ï¼šåˆ†ç±»å„¿ç«¥ä¸æˆäººçš„è¯è¯­ã€é¢„æµ‹å‚ä¸çš„æ´»åŠ¨ã€è¯†åˆ«è¯­è¨€æŠ€èƒ½ä»¥åŠç†è§£ä¸´åºŠç›¸å…³çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œé€šç”¨LLMéå¸¸æ“…é•¿åˆ†æä¸´åºŠè§‚å¯Ÿä¼šè¯ä¸­çš„å†—é•¿å’Œå¤æ‚å¯¹è¯ï¼Œå¾€å¾€è¶…è¿‡äº†éä¸“å®¶äººç±»è¯„ä¼°è€…çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå®ƒä»¬å…·æœ‰åˆ†ææ„Ÿå…´è¶£äº’åŠ¨ã€ååŠ©è¯­è¨€æŠ€èƒ½è¯„ä¼°ã€è¯†åˆ«å‚ä¸çš„æ´»åŠ¨ä»¥åŠä¸ºè¯„ä¼°æä¾›ä¸´åºŠç›¸å…³èƒŒæ™¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10761v1">PDF</a> GenAI for Health Workshop, NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£äººç±»æ²Ÿé€šå’Œäº¤äº’æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å„¿ç«¥äº¤äº’é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„æ€§èƒ½ä»éœ€è¿›ä¸€æ­¥æ¢ç´¢ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†é€šç”¨LLMsåˆ†ææ‚£æœ‰è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰çš„å„¿ç«¥ä¸æˆäººä¹‹é—´çš„ä¸´åºŠç›¸å…³äº’åŠ¨çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åˆ†ç±»å„¿ç«¥ä¸æˆäººçš„è¨€è¯­ã€é¢„æµ‹å‚ä¸çš„æ´»åŠ¨ã€è¯†åˆ«è¯­è¨€æŠ€èƒ½å’Œç†è§£ä¸´åºŠç›¸å…³çš„ç‰¹å¾ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œé€šç”¨LLMsèƒ½å¤Ÿå¾ˆå¥½åœ°åˆ†æä¸´åºŠè§‚å¯Ÿä¼šè¯ä¸­çš„é•¿è€Œå¤æ‚çš„å¯¹è¯ï¼Œå…¶æ€§èƒ½å¾€å¾€è¶…è¶Šéä¸“å®¶äººç±»è¯„ä¼°è€…çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£äººç±»æ²Ÿé€šå’Œäº¤äº’æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>åœ¨å„¿ç«¥äº¤äº’é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯ä¸´åºŠç¯å¢ƒä¸­çš„æ€§èƒ½ä»éœ€è¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
<li>é€šç”¨LLMsèƒ½å¤Ÿåˆ†ææ‚£æœ‰è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰çš„å„¿ç«¥ä¸æˆäººä¹‹é—´çš„ä¸´åºŠç›¸å…³äº’åŠ¨ã€‚</li>
<li>LLMså¯ä»¥å®Œæˆå››é¡¹ä»»åŠ¡ï¼šåˆ†ç±»å„¿ç«¥ä¸æˆäººçš„è¨€è¯­ã€é¢„æµ‹å‚ä¸çš„æ´»åŠ¨ã€è¯†åˆ«è¯­è¨€æŠ€èƒ½å’Œç†è§£ä¸´åºŠç›¸å…³ç‰¹å¾ã€‚</li>
<li>LLMsèƒ½å¤Ÿåˆ†æé•¿è€Œå¤æ‚çš„å¯¹è¯ï¼Œæ€§èƒ½è¶…è¶Šéä¸“å®¶äººç±»è¯„ä¼°è€…ã€‚</li>
<li>LLMsæœ‰æ½œåŠ›åˆ†å‰²æ„Ÿå…´è¶£çš„äº’åŠ¨ã€è¾…åŠ©è¯­è¨€æŠ€èƒ½è¯„ä¼°ã€è¯†åˆ«å‚ä¸çš„æ´»åŠ¨ï¼Œå¹¶ä¸ºè¯„ä¼°æä¾›ä¸´åºŠç›¸å…³èƒŒæ™¯ã€‚</li>
<li>ç ”ç©¶ç»“æœå±•ç¤ºäº†LLMsåœ¨åŒ»ç–—å¯¹è¯åˆ†æä¸­çš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6c717340f072df7eeadbabe15f450e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c1a05f5fce0e2a6f8ab4eea06593fb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df1f737329191b17b00ebcfe6e85bc36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a87e083dcbcfb803a45df10a833622c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1dab679eb227c3ccb68166b7d90ec1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66037f6e786ff028ace3fa6b7580e3c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c73e72b39e62071261748ef6f2c18f2.jpg" align="middle">
</details>




<h2 id="WMT24-Test-Suite-Gender-Resolution-in-Speaker-Listener-Dialogue-Roles"><a href="#WMT24-Test-Suite-Gender-Resolution-in-Speaker-Listener-Dialogue-Roles" class="headerlink" title="WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles"></a>WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles</h2><p><strong>Authors:Hillary Dawkins, Isar Nejadgholi, Chi-kiu Lo</strong></p>
<p>We assess the difficulty of gender resolution in literary-style dialogue settings and the influence of gender stereotypes. Instances of the test suite contain spoken dialogue interleaved with external meta-context about the characters and the manner of speaking. We find that character and manner stereotypes outside of the dialogue significantly impact the gender agreement of referents within the dialogue. </p>
<blockquote>
<p>æˆ‘ä»¬è¯„ä¼°äº†æ–‡å­¦é£æ ¼å¯¹è¯åœºæ™¯ä¸­æ€§åˆ«è§£æçš„éš¾åº¦ä»¥åŠæ€§åˆ«åˆ»æ¿å°è±¡çš„å½±å“ã€‚æµ‹è¯•é›†çš„å®ä¾‹åŒ…å«äº†å¯¹è¯ä¸å¤–éƒ¨äººç‰©åŠå…¶è¯´è¯æ–¹å¼çš„å…ƒè¯­å¢ƒäº¤ç»‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹è¯ä¹‹å¤–çš„äººç‰©å’Œæ–¹å¼åˆ»æ¿å°è±¡ä¼šæ˜¾è‘—å½±å“å¯¹è¯ä¸­æåŠäººç‰©çš„æ€§åˆ«ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06194v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æ¢è®¨äº†æ–‡å­¦é£æ ¼å¯¹è¯è®¾ç½®ä¸­æ€§åˆ«è¯†åˆ«çš„éš¾åº¦ä»¥åŠæ€§åˆ«åˆ»æ¿å°è±¡çš„å½±å“ã€‚æµ‹è¯•å¥—ä»¶ä¸­çš„å®ä¾‹åŒ…å«å…³äºäººç‰©å’Œè¯´è¯æ–¹å¼çš„å¤–éƒ¨è¯­å¢ƒäº¤ç»‡çš„å¯¹è¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹è¯å¤–çš„è§’è‰²å’Œæ–¹å¼åˆ»æ¿å°è±¡ä¼šæ˜¾è‘—å½±å“å¯¹è¯å†…å‚è€ƒå¯¹è±¡çš„æ€§åˆ«ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡å­¦é£æ ¼å¯¹è¯ä¸­çš„æ€§åˆ«è¯†åˆ«å­˜åœ¨éš¾åº¦ã€‚</li>
<li>å¯¹è¯ä¸­çš„å¤–éƒ¨å…ƒè¯­å¢ƒå…³äºäººç‰©å’Œè¯´è¯æ–¹å¼çš„ä¿¡æ¯å¯¹æ€§åˆ«è¯†åˆ«æœ‰å½±å“ã€‚</li>
<li>è§’è‰²åˆ»æ¿å°è±¡ä¼šå½±å“å¯¹è¯ä¸­å‚è€ƒå¯¹è±¡çš„æ€§åˆ«ä¸€è‡´æ€§ã€‚</li>
<li>æ–¹å¼åˆ»æ¿å°è±¡ä¹Ÿæ˜¯å½±å“å¯¹è¯ä¸­æ€§åˆ«è¯†åˆ«çš„é‡è¦å› ç´ ã€‚</li>
<li>æµ‹è¯•å¥—ä»¶å®ä¾‹æ­ç¤ºäº†æ€§åˆ«è¯†åˆ«ä¸­çš„å¤æ‚æ€§å’Œå½±å“å› ç´ ã€‚</li>
<li>å¯¹è¯ä¸­çš„è¯­å¢ƒä¿¡æ¯åœ¨æ€§åˆ«è¯†åˆ«ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5dcc1eccf1280a168245abdfd544eb13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-631a395a3838df03be7cb13b0a3e165d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e52d40c0217ab7d6c931a67bf5d485.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be8b7bc4b9db27ccc3fb22a62b642818.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34d2e6fc23e316651fa490a3043524ff.jpg" align="middle">
</details>




<h2 id="SANN-PSZ-Spatially-Adaptive-Neural-Network-for-Head-Tracked-Personal-Sound-Zones"><a href="#SANN-PSZ-Spatially-Adaptive-Neural-Network-for-Head-Tracked-Personal-Sound-Zones" class="headerlink" title="SANN-PSZ: Spatially Adaptive Neural Network for Head-Tracked Personal   Sound Zones"></a>SANN-PSZ: Spatially Adaptive Neural Network for Head-Tracked Personal   Sound Zones</h2><p><strong>Authors:Yue Qiao, Edgar Choueiri</strong></p>
<p>A deep learning framework for dynamically rendering personal sound zones (PSZs) with head tracking is presented, utilizing a spatially adaptive neural network (SANN) that inputs listenersâ€™ head coordinates and outputs PSZ filter coefficients. The SANN model is trained using either simulated acoustic transfer functions (ATFs) with data augmentation for robustness in uncertain environments or a mix of simulated and measured ATFs for customization under known conditions. It is found that augmenting room reflections in the training data can more effectively improve the model robustness than augmenting the system imperfections, and that adding constraints such as filter compactness to the loss function does not significantly affect the modelâ€™s performance. Comparisons of the best-performing model with traditional filter design methods show that, when no measured ATFs are available, the model yields equal or higher isolation in an actual room environment with fewer filter artifacts. Furthermore, the model achieves significant data compression (100x) and computational efficiency (10x) compared to the traditional methods, making it suitable for real-time rendering of PSZs that adapt to the listenersâ€™ head movements. </p>
<blockquote>
<p>æå‡ºäº†ä¸€ç§åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶åŠ¨æ€æ¸²æŸ“ä¸ªäººå£°éŸ³åŒºåŸŸï¼ˆPSZsï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¤´éƒ¨è·Ÿè¸ªåŠŸèƒ½ï¼Œå¹¶åˆ©ç”¨ç©ºé—´è‡ªé€‚åº”ç¥ç»ç½‘ç»œï¼ˆSANNï¼‰è¿›è¡Œå®ç°ã€‚SANNæ¨¡å‹ä»¥å¬ä¼—çš„å¤´éƒ¨åæ ‡ä¸ºè¾“å…¥ï¼Œè¾“å‡ºPSZæ»¤æ³¢å™¨ç³»æ•°ã€‚SANNæ¨¡å‹å¯ä»¥ä½¿ç”¨æ¨¡æ‹Ÿçš„å£°å­¦ä¼ é€’å‡½æ•°ï¼ˆATFï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æé«˜ä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡æ‹Ÿå’Œå®é™…æµ‹é‡çš„ATFæ··åˆè®­ç»ƒå®ç°å®šåˆ¶é€‚åº”å·²çŸ¥æ¡ä»¶ä¸‹çš„åœºæ™¯ã€‚ç ”ç©¶å‘ç°åœ¨è®­ç»ƒæ•°æ®ä¸­åŠ å…¥æˆ¿é—´åå°„æ¯”åœ¨ç³»ç»Ÿç¼ºé™·ä¸­åŠ å…¥è®­ç»ƒæ•°æ®æ›´èƒ½æœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸”åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ è¿‡æ»¤å™¨ç´§å‡‘æ€§ç­‰çº¦æŸæ¡ä»¶å¹¶ä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚å°†è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¸ä¼ ç»Ÿæ»¤æ³¢å™¨è®¾è®¡æ–¹æ³•è¿›è¡Œæ¯”è¾ƒè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰å¯ç”¨çš„å®é™…æµ‹é‡ATFæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æˆ¿é—´ç¯å¢ƒä¸­äº§ç”Ÿçš„éš”ç¦»æ•ˆæœä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ï¼Œå¹¶ä¸”äº§ç”Ÿçš„æ»¤æ³¢å™¨ä¼ªå½±æ›´å°‘ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ•°æ®å‹ç¼©ï¼ˆ100å€ï¼‰å’Œè®¡ç®—æ•ˆç‡ï¼ˆ10å€ï¼‰ï¼Œéå¸¸é€‚åˆå®æ—¶æ¸²æŸ“é€‚åº”å¬ä¼—å¤´éƒ¨ç§»åŠ¨çš„PSZsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00772v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç¯‡å…³äºåˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶åŠ¨æ€æ¸²æŸ“ä¸ªäººå£°éŸ³åŒºåŸŸï¼ˆPSZsï¼‰çš„è®ºæ–‡ï¼Œä»‹ç»äº†é‡‡ç”¨å¤´æˆ´è·Ÿè¸ªæŠ€æœ¯çš„ç©ºé—´è‡ªé€‚åº”ç¥ç»ç½‘ç»œï¼ˆSANNï¼‰ã€‚è¯¥ç½‘ç»œä»¥å¬è€…çš„å¤´éƒ¨åæ ‡ä¸ºè¾“å…¥ï¼Œè¾“å‡ºPSZæ»¤æ³¢å™¨ç³»æ•°ã€‚è®ºæ–‡é€šè¿‡æ¨¡æ‹Ÿå£°å­¦ä¼ è¾“å‡½æ•°ï¼ˆATFsï¼‰è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ï¼Œæˆ–ä½¿ç”¨æ··åˆæ¨¡æ‹Ÿå’Œå®é™…æµ‹é‡çš„ATFsè¿›è¡Œå®šåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸­å¢åŠ æˆ¿é—´åå°„æ¯”å¢åŠ ç³»ç»Ÿç¼ºé™·æ›´èƒ½æé«˜æ¨¡å‹ç¨³å¥æ€§ï¼ŒæŸå¤±å‡½æ•°ä¸­æ·»åŠ è¿‡æ»¤å™¨ç´§å‡‘æ€§çº¦æŸå¹¶ä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ»¤æ³¢å™¨è®¾è®¡æ–¹æ³•ç›¸æ¯”ï¼Œæœ€ä½³æ€§èƒ½çš„æ¨¡å‹åœ¨æ— å®é™…æµ‹é‡çš„ATFsæƒ…å†µä¸‹ï¼Œåœ¨çœŸå®ç¯å¢ƒä¸­å®ç°äº†åŒç­‰æˆ–æ›´é«˜çš„éš”ç¦»æ•ˆæœï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ•°æ®å‹ç¼©ï¼ˆ100å€ï¼‰å’Œè®¡ç®—æ•ˆç‡ï¼ˆ10å€ï¼‰ï¼Œé€‚åˆå®æ—¶æ¸²æŸ“é€‚åº”å¬è€…å¤´éƒ¨ç§»åŠ¨çš„ä¸ªäººå£°éŸ³åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶å’Œç©ºé—´è‡ªé€‚åº”ç¥ç»ç½‘ç»œï¼ˆSANNï¼‰å®ç°ä¸ªäººå£°éŸ³åŒºåŸŸï¼ˆPSZï¼‰çš„åŠ¨æ€æ¸²æŸ“ï¼Œç»“åˆå¤´æˆ´è·Ÿè¸ªæŠ€æœ¯ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå£°å­¦ä¼ è¾“å‡½æ•°ï¼ˆATFsï¼‰è¿›è¡Œæ•°æ®å¢å¼ºï¼Œæé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å¯¹æ¯”æ¨¡æ‹Ÿå’Œå®é™…æµ‹é‡çš„ATFsæ··åˆä½¿ç”¨ï¼Œå®ç°å®šåˆ¶åŒ–çš„PSZæ¸²æŸ“ã€‚</li>
<li>ç ”ç©¶å‘ç°å¢åŠ æˆ¿é—´åå°„åˆ°è®­ç»ƒæ•°æ®ä¸­æ›´æœ‰æ•ˆæé«˜æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>æŸå¤±å‡½æ•°ä¸­æ·»åŠ è¿‡æ»¤å™¨ç´§å‡‘æ€§çº¦æŸå¯¹æ¨¡å‹æ€§èƒ½å½±å“ä¸æ˜¾è‘—ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ»¤æ³¢å™¨è®¾è®¡æ–¹æ³•ç›¸æ¯”ï¼Œæœ€ä½³æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸­å®ç°åŒç­‰æˆ–æ›´é«˜çš„éš”ç¦»æ•ˆæœï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘æ•°æ®å‹ç¼©å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ac674fa49cc59b166392a8f68bef6f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e858b865c44f8ce7772b7a7697df1c67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d408059c4f0677cc5cd62cfc214748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6208c82f1f1d8ff5109de3bb9ff8db11.jpg" align="middle">
</details>




<h2 id="Exploring-Knowledge-Tracing-in-Tutor-Student-Dialogues-using-LLMs"><a href="#Exploring-Knowledge-Tracing-in-Tutor-Student-Dialogues-using-LLMs" class="headerlink" title="Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs"></a>Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs</h2><p><strong>Authors:Alexander Scarlatos, Ryan S. Baker, Andrew Lan</strong></p>
<p>Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components&#x2F;skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLMâ€™s accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„è¾…å¯¼èŠå¤©æœºå™¨äººæä¾›äº†å‘å±•æœºä¼šï¼Œæ˜¾ç¤ºå‡ºåœ¨æä¾›é«˜è´¨é‡ä¸ªæ€§åŒ–æ•™è‚²æ–¹é¢æä¾›å¹¿æ³›è®¿é—®çš„æ½œåŠ›ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢è®¨äº†å¦‚ä½•ä½¿LLMéµå¾ªè¾…å¯¼åŸåˆ™ï¼Œä½†å°šæœªç ”ç©¶LLMåœ¨æ”¯æŒè¾…å¯¼æ–¹é¢çš„æ›´å¹¿æ³›åº”ç”¨ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¯¹äºå¼€æ”¾å¼å¯¹è¯è¾…å¯¼è€Œè¨€ï¼Œè¿½è¸ªå­¦ç”Ÿçš„çŸ¥è¯†å’Œåˆ†æè¯¯è§£ä¸€ç›´éš¾ä»¥å®ç°ä¸”è€—æ—¶çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†LLMæ˜¯å¦èƒ½å¤Ÿæ”¯æŒè¿™é¡¹ä»»åŠ¡ï¼šæˆ‘ä»¬é¦–å…ˆä½¿ç”¨LLMæç¤ºæ–¹æ³•æ¥è¯†åˆ«æ¯ä¸ªå¯¹è¯å›åˆæ‰€æ¶‰åŠçš„çŸ¥è¯†æˆåˆ†&#x2F;æŠ€èƒ½ï¼Œå³æ•™å¸ˆçš„è¯è¯­æå‡ºä»»åŠ¡æˆ–å­¦ç”Ÿå¯¹ä»»åŠ¡çš„å›åº”ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°å­¦ç”Ÿå¯¹æ•™å¸ˆçš„å›åº”æ˜¯å¦æ­£ç¡®ï¼Œå¹¶ä½¿ç”¨äººç±»ä¸“å®¶æ³¨é‡Šæ¥éªŒè¯LLMçš„å‡†ç¡®æ€§ã€‚ç„¶åæˆ‘ä»¬åœ¨å¾—åˆ°çš„æ ‡è®°æ•°æ®ä¸Šåº”ç”¨ä¸€ç³»åˆ—çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰æ–¹æ³•ï¼Œä»¥è·Ÿè¸ªæ•´ä¸ªå¯¹è¯è¿‡ç¨‹ä¸­çš„å­¦ç”ŸçŸ¥è¯†æ°´å¹³ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªè¾…å¯¼å¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ä¸€ç§æ–°é¢–è€Œç®€å•çš„åŸºäºLLMçš„æ–¹æ³•LLMKTåœ¨é¢„æµ‹å¯¹è¯ä¸­å­¦ç”Ÿå›åº”çš„æ­£ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„KTæ–¹æ³•ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®šæ€§åˆ†æï¼Œä»¥çªå‡ºå¯¹è¯KTçš„æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥å·¥ä½œçš„å¤šä¸ªæ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16490v2">PDF</a> Published in LAK 2025: The 15th International Learning Analytics and   Knowledge Conference</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é©±åŠ¨çš„è¾…å¯¼èŠå¤©æœºå™¨äººæä¾›äº†å¹¿é˜”çš„å‘å±•ç©ºé—´ï¼Œæœ‰æœ›ä¸ºé«˜è´¨é‡ä¸ªæ€§åŒ–æ•™è‚²æä¾›å¹¿æ³›æ¸ é“ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†LLMåœ¨æ”¯æŒè¾…å¯¼æ–¹é¢çš„æ›´å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶æ˜¯é€šè¿‡LLMæç¤ºæ–¹æ³•æ¥è¿½è¸ªå­¦ç”Ÿçš„çŸ¥è¯†å’Œåˆ†æè¯¯è§£ï¼Œç”¨äºå¼€æ”¾å¼å¯¹è¯è¾…å¯¼ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ä¸€ç§æ–°å‹LLMçŸ¥è¯†è¿½è¸ªæ–¹æ³•LLMKTï¼Œå®ƒåœ¨é¢„æµ‹å­¦ç”Ÿå¯¹è¯ä¸­çš„å›åº”æ­£ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†AIè¾…å¯¼èŠå¤©æœºå™¨äººçš„å‘å±•ï¼Œä¸ºä¸ªæ€§åŒ–æ•™è‚²æä¾›äº†æ–°æœºä¼šã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å¦‚ä½•ä½¿LLMéµå¾ªè¾…å¯¼åŸåˆ™ï¼Œè€Œæœ¬ç ”ç©¶æ›´å¹¿æ³›åœ°æ¢è®¨äº†LLMåœ¨æ”¯æŒè¾…å¯¼æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>é€šè¿‡LLMæç¤ºæ–¹æ³•ï¼Œå¯ä»¥è¯†åˆ«å¯¹è¯ä¸­çš„çŸ¥è¯†ç»„ä»¶&#x2F;æŠ€èƒ½ï¼Œå¹¶è¯„ä¼°å­¦ç”Ÿå¯¹è¾…å¯¼çš„å›åº”æ˜¯å¦æ­£ç¡®ã€‚</li>
<li>LLMKTæ˜¯ä¸€ç§æ–°å‹ä¸”ç®€å•çš„çŸ¥è¯†è¿½è¸ªæ–¹æ³•ï¼Œèƒ½æ˜¾è‘—é¢„æµ‹å­¦ç”Ÿå¯¹è¯ä¸­çš„å›åº”æ­£ç¡®æ€§ã€‚</li>
<li>LLMKTåœ¨é¢„æµ‹å­¦ç”Ÿå›åº”æ­£ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çŸ¥è¯†è¿½è¸ªæ–¹æ³•ã€‚</li>
<li>å¯¹è¯è¾…å¯¼ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å­¦ç”ŸçŸ¥è¯†çš„è¿½è¸ªå’Œè¯¯è§£çš„åˆ†æã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0efb9126876fb5c3e895ac511a2424ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f31ce3aefadf83a786a124d522cfb90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6004a83095926968c8c882230005dc.jpg" align="middle">
</details>




<h2 id="Look-Listen-and-Answer-Overcoming-Biases-for-Audio-Visual-Question-Answering"><a href="#Look-Listen-and-Answer-Overcoming-Biases-for-Audio-Visual-Question-Answering" class="headerlink" title="Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question   Answering"></a>Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question   Answering</h2><p><strong>Authors:Jie Ma, Min Hu, Pinghui Wang, Wangchun Sun, Lingyun Song, Hongbin Pei, Jun Liu, Youtian Du</strong></p>
<p>Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, MUSIC-AVQA-R, crafted in two steps: rephrasing questions within the test split of a public dataset (MUSIC-AVQA) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/reml-group/MUSIC-AVQA-R">https://github.com/reml-group/MUSIC-AVQA-R</a>. </p>
<blockquote>
<p>éŸ³é¢‘è§†è§‰é—®ç­”ï¼ˆAVQAï¼‰æ˜¯ä¸€é¡¹å¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå®ƒè¦æ±‚æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®éŸ³è§†é¢‘è¾“å…¥å¯¹è‡ªç„¶è¯­è¨€é—®é¢˜åšå‡ºå‡†ç¡®å“åº”ã€‚ç„¶è€Œï¼Œæµè¡Œçš„AVQAæ–¹æ³•å®¹æ˜“è¿‡åº¦å­¦ä¹ æ•°æ®é›†çš„åè§ï¼Œå¯¼è‡´ç¨³å¥æ€§è¾ƒå·®ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†å¯èƒ½æ— æ³•ä¸ºè¿™äº›æ–¹æ³•æä¾›ç²¾ç¡®çš„è¯Šæ–­ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºMUSIC-AVQA-Rçš„æ–°æ•°æ®é›†ï¼Œåˆ†ä¸¤æ­¥æ„å»ºï¼šåœ¨å…¬å¼€æ•°æ®é›†ï¼ˆMUSIC-AVQAï¼‰çš„æµ‹è¯•é›†ä¸­é‡æ–°è¡¨è¿°é—®é¢˜ï¼Œç„¶åå¼•å…¥åˆ†å¸ƒå˜åŒ–æ¥æ‹†åˆ†é—®é¢˜ã€‚å‰è€…å¯¼è‡´äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æµ‹è¯•ç©ºé—´ï¼Œè€Œåè€…åˆ™å¯¹ç½•è§ã€é¢‘ç¹å’Œæ€»ä½“é—®é¢˜è¿›è¡Œäº†å…¨é¢çš„ç¨³å¥æ€§è¯„ä¼°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨å¤šæ–¹é¢çš„å¾ªç¯ååŒå»åç­–ç•¥æ¥å…‹æœåç½®å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨MUSIC-AVQA-Rä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œå°¤å…¶æ˜¯è·å¾—äº†9.32%çš„æ˜¾è‘—æ”¹è¿›ã€‚ä¸ºäº†åˆ†æå»åç­–ç•¥ä¸­ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸Šè¿°ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèå®éªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°å¼ºè°ƒäº†ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•çš„æœ‰é™ç¨³å¥æ€§ã€‚ä¸ºäº†éªŒè¯å…¶å³æ’å³ç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå°†å„ç§åŸºçº¿æ–¹æ³•ä¸æˆ‘ä»¬æ‰€æå‡ºçš„ç­–ç•¥ç›¸ç»“åˆè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/reml-group/MUSIC-AVQA-R%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/reml-group/MUSIC-AVQA-Ræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12020v3">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘è§†è§‰é—®ç­”ï¼ˆAVQAï¼‰ä»»åŠ¡çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç³»ç»Ÿè¿‡æ‹Ÿåˆæ•°æ®é›†åå·®çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†MUSIC-AVQA-Rï¼Œé€šè¿‡æ”¹å†™é—®é¢˜å’Œå¼•å…¥åˆ†å¸ƒåç§»æ¥æ„å»ºã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šé¢å¾ªç¯ååŒå»åç­–ç•¥çš„ç¨³å¥æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨MUSIC-AVQA-Rä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜äº†9.32%çš„é²æ£’æ€§ã€‚åŒæ—¶è¯„ä¼°äº†ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•çš„æœ‰é™é²æ£’æ€§ã€‚æ•°æ®é›†å’Œä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰é—®ç­”ï¼ˆAVQAï¼‰æ˜¯ä¸€ä¸ªå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚ç³»ç»ŸåŸºäºéŸ³è§†é¢‘è¾“å…¥å¯¹è‡ªç„¶è¯­è¨€é—®é¢˜ä½œå‡ºå‡†ç¡®å›åº”ã€‚</li>
<li>å½“å‰AVQAæ–¹æ³•å­˜åœ¨è¿‡å­¦ä¹ æ•°æ®é›†åå·®çš„é—®é¢˜ï¼Œå¯¼è‡´ç¨³å¥æ€§ä¸è¶³ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ•°æ®é›†MUSIC-AVQA-Rï¼Œé€šè¿‡æ”¹å†™é—®é¢˜å’Œå¼•å…¥åˆ†å¸ƒåç§»æ¥æ„å»ºï¼Œä»¥æ‰©å¤§æµ‹è¯•ç©ºé—´å’Œè¿›è¡Œç¨³å¥æ€§è¯„ä¼°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šé¢å¾ªç¯ååŒå»åç­–ç•¥çš„ç¨³å¥æ¶æ„ï¼Œå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>å®éªŒéªŒè¯äº†è¯¥æ¶æ„åœ¨MUSIC-AVQA-Rä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸å¤šç§åŸºçº¿æ–¹æ³•ç»“åˆå®éªŒï¼Œè¯æ˜äº†å…¶å³æ’å³ç”¨èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•çš„é²æ£’æ€§æœ‰é™ï¼Œéœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3644d5431a122a4d95c24400da94552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf746806dc7469dc61eaead9caee38a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1858fc8405f925cd07ff2cc0534e3220.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f07b3ad49c6f0332156d1e7d62a16e48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e1b49f647bbc0287c6b0beb79eb45d7.jpg" align="middle">
</details>




<h2 id="Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test"><a href="#Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test" class="headerlink" title="Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test"></a>Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test</h2><p><strong>Authors:Peter Leer, Jesper Jensen, Laurel H. Carney, Zheng-Hua Tan, Jan Ã˜stergaard, Lars BramslÃ¸w</strong></p>
<p>This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation. Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation. DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification. In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals. First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway. We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation. Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy. Finally, we evaluate, to our knowledge for the first time, the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners. The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies. Our proposed approach was shown to provide an increase in speech intelligibility versus an unprocessed baseline and was found to outperform a conventional approach in terms of both intelligibility and preference. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¬åŠ›æŸå¤±è¡¥å¿ä¸­çš„åº”ç”¨ã€‚å¬åŠ›æŸå¤±æ˜¯ä¸€ä¸ªå½±å“å…¨çƒæ•°ç™¾ä¸‡äººçš„æ™®éé—®é¢˜ï¼Œè€Œä¼ ç»Ÿçš„åŠ©å¬å™¨åœ¨æä¾›æ»¡æ„è¡¥å¿æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å„ç§å¬è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººè¯†åˆ«å’ŒéŸ³ä¹åˆ†ç±»ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯åœ¨å¬åŠ›å—æŸå’Œæ­£å¸¸å¬åŠ›çš„æ·±åº¦ç¥ç»ç½‘ç»œå¬è§‰æ¨¡å‹å¯¹è¯­éŸ³ä¿¡å·çš„è¾“å‡ºååº”ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿå¬è§‰æ¨¡å‹çš„æ¡†æ¶ï¼Œé‡ç‚¹æ˜¯ä¸€ä¸ªå¬è§‰ç¥ç»æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•çš„çº¿æ€§åŒ–åˆ†æï¼Œç”¨äºåˆ†æåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥é€‰æ‹©å¬è§‰æ¨¡å‹ä¸­ç”¨äºè¡¥å¿ç­–ç•¥çš„å£°ä¸­å¿ƒé¢‘ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å¬åŠ›å—æŸè€…çš„å¬åŠ›æµ‹è¯•è¯„ä¼°äº†åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å¯è¡Œçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥ã€‚ä¸æœªå¤„ç†çš„åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†è¯­éŸ³æ¸…æ™°åº¦ï¼Œå¹¶ä¸”åœ¨å¯ç†è§£åº¦å’Œåå¥½æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10420v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è¿›è¡Œå¬åŠ›æŸå¤±è¡¥å¿ç ”ç©¶ã€‚æ–‡ç« æå‡ºäº†åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œè®­ç»ƒäºå¬åŠ›å—æŸå’Œæ­£å¸¸å¬åŠ›çš„DNNå¬è§‰æ¨¡å‹å¯¹è¯­éŸ³ä¿¡å·çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ç®€å•é€‰æ‹©å¬è§‰æ¨¡å‹çš„ä¸­å¿ƒé¢‘ç‡è¡¥å¿ç­–ç•¥çš„æ–¹æ³•ã€‚é€šè¿‡å¬åŠ›å—æŸè€…çš„å¬åŠ›æµ‹è¯•è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å¯æœ‰æ•ˆæé«˜å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥çš„å¯è¡Œæ€§ï¼Œç›¸è¾ƒäºæœªå¤„ç†çš„åŸºç¡€çº¿å’Œä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨å¯ç†è§£æ€§å’Œåå¥½ä¸Šè¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¬åŠ›æŸå¤±è¡¥å¿æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>DNNåœ¨å¬è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€è¯´è¯è€…è¯†åˆ«å’ŒéŸ³ä¹åˆ†ç±»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œè®­ç»ƒåœ¨å¬éšœå’Œæ­£å¸¸å¬åŠ›è€…çš„å¬è§‰æ¨¡å‹ä¸Šã€‚</li>
<li>é‡‡ç”¨çº¿æ€§åŒ–åˆ†æï¼Œä¸ºåŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥æä¾›äº†ç†è®ºåŸºç¡€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•é€‰æ‹©å¬è§‰æ¨¡å‹ä¸­å¿ƒé¢‘ç‡çš„æ–¹æ³•ï¼Œç”¨äºè¡¥å¿ç­–ç•¥ã€‚</li>
<li>é€šè¿‡å¬åŠ›å—æŸè€…çš„æµ‹è¯•è¯„ä¼°äº†åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb96d0056db88ee2c733128931b7fdde.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b17b20446f95113e30d62616a7728759.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e67e7cf7bc58f078a820af9ac539ae7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c45eba05418f6e1ebeb65af3ab0e310e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b2b0e9ca74e4133de6132871478d14d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f583d4a12289ffcf1e06220ca86407.jpg" align="middle">
</details>




<h2 id="Sibyl-Empowering-Empathetic-Dialogue-Generation-in-Large-Language-Models-via-Sensible-and-Visionary-Commonsense-Inference"><a href="#Sibyl-Empowering-Empathetic-Dialogue-Generation-in-Large-Language-Models-via-Sensible-and-Visionary-Commonsense-Inference" class="headerlink" title="Sibyl: Empowering Empathetic Dialogue Generation in Large Language   Models via Sensible and Visionary Commonsense Inference"></a>Sibyl: Empowering Empathetic Dialogue Generation in Large Language   Models via Sensible and Visionary Commonsense Inference</h2><p><strong>Authors:Lanrui Wang, Jiangnan Li, Chenxu Yang, Zheng Lin, Hongyin Tang, Huan Liu, Yanan Cao, Jingang Wang, Weiping Wang</strong></p>
<p>Recently, there has been a heightened interest in building chatbots based on Large Language Models (LLMs) to emulate human-like qualities in multi-turn conversations. Despite having access to commonsense knowledge to better understand the psychological aspects and causality of dialogue context, even these powerful LLMs struggle to achieve the goals of empathy and emotional support. Current commonsense knowledge derived from dialogue contexts is inherently limited and often fails to adequately anticipate the future course of a dialogue. This lack of foresight can mislead LLMs and hinder their ability to provide effective support. In response to this challenge, we present an innovative framework named Sensible and Visionary Commonsense Knowledge (Sibyl). Designed to concentrate on the immediately succeeding dialogue, this paradigm equips LLMs with the capability to uncover the implicit requirements of the conversation, aiming to elicit more empathetic responses. Experimental results demonstrate that incorporating our paradigm for acquiring commonsense knowledge into LLMs comprehensively enhances the quality of their responses. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºèŠå¤©æœºå™¨äººä»¥æ¨¡æ‹Ÿäººç±»åœ¨å¤šè½®å¯¹è¯ä¸­çš„äººæ€§å“è´¨çš„å…´è¶£æ—¥ç›Šæµ“åšã€‚å°½ç®¡å¯ä»¥è®¿é—®å¸¸è¯†çŸ¥è¯†ä»¥æ›´å¥½åœ°ç†è§£å¯¹è¯å¿ƒç†çš„æ–¹é¢å’Œå› æœå…³ç³»ï¼Œä½†å³ä½¿æ˜¯è¿™äº›åŠŸèƒ½å¼ºå¤§çš„LLMä¹Ÿå¾ˆéš¾å®ç°åŒæƒ…å’Œæƒ…æ„Ÿæ”¯æŒçš„ç›®æ ‡ã€‚å½“å‰ä»å¯¹è¯è¯­å¢ƒä¸­å¾—å‡ºçš„å¸¸è¯†çŸ¥è¯†æœ¬è´¨ä¸Šæ˜¯æœ‰é™çš„ï¼Œé€šå¸¸æ— æ³•å……åˆ†é¢„æµ‹å¯¹è¯çš„æœªæ¥èµ°å‘ã€‚è¿™ç§ç¼ºä¹è¿œè§å¯èƒ½ä¼šè¯¯å¯¼LLMå¹¶é˜»ç¢å®ƒä»¬æä¾›æœ‰æ•ˆæ”¯æŒçš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºSibylçš„åˆ›æ–°çš„å¸¸è¯†çŸ¥è¯†æ¡†æ¶ã€‚æ­¤èŒƒå¼æ—¨åœ¨å…³æ³¨ç´§éšå…¶åçš„å¯¹è¯ï¼Œæ—¨åœ¨ä¸ºLLMæä¾›å‘ç°å¯¹è¯ä¸­éšå«éœ€æ±‚çš„èƒ½åŠ›ï¼Œæ—¨åœ¨æ¿€å‘æ›´å¤šå¯Œæœ‰åŒæƒ…å¿ƒçš„å›åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†æˆ‘ä»¬çš„å¸¸è¯†çŸ¥è¯†è·å–èŒƒå¼èå…¥LLMä¸­ï¼Œå¯ä»¥å…¨é¢æé«˜å…¶å“åº”çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.15316v4">PDF</a> Accepted by COLING 2025</p>
<p><strong>Summary</strong><br>     åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŠå¤©æœºå™¨äººè™½å…·å¤‡å¸¸è¯†çŸ¥è¯†ä»¥æ›´å¥½åœ°ç†è§£å¯¹è¯çš„å¿ƒç†å’Œå› æœèƒŒæ™¯ï¼Œä½†ä»éš¾ä»¥åœ¨æ¨¡æ‹Ÿäººç±»å¤šè½®å¯¹è¯ä¸­å±•ç°åŒç†å¿ƒå’Œæƒ…æ„Ÿæ”¯æŒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSibylçš„æ„ŸçŸ¥å’Œå‰ç»æ€§å¸¸è¯†çŸ¥è¯†æ¡†æ¶ï¼Œæ—¨åœ¨é›†ä¸­äºæ¥ä¸‹æ¥çš„å¯¹è¯å†…å®¹ï¼Œå¸®åŠ©LLMæ­ç¤ºå¯¹è¯çš„éšå«éœ€æ±‚ï¼Œä»è€Œå¼•å‘æ›´å…·åŒç†å¿ƒçš„å›åº”ã€‚å®éªŒè¯æ˜ï¼Œå°†è¿™ä¸€çŸ¥è¯†è·å–èŒƒå¼èå…¥LLMä¸­ï¼Œèƒ½æ˜¾è‘—æé«˜å›åº”è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨¡æ‹Ÿäººç±»å¤šè½®å¯¹è¯æ—¶ï¼Œå°½ç®¡å…·å¤‡å¸¸è¯†çŸ¥è¯†ï¼Œä½†ä»é¢ä¸´å®ç°åŒç†å¿ƒå’Œæƒ…æ„Ÿæ”¯æŒçš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ä»å¯¹è¯èƒŒæ™¯ä¸­è·å–çš„å¸¸è¯†çŸ¥è¯†å…·æœ‰å†…åœ¨å±€é™æ€§ï¼Œå¸¸å¸¸æ— æ³•å……åˆ†é¢„æµ‹å¯¹è¯çš„æœªæ¥èµ°å‘ã€‚</li>
<li>æå‡ºçš„Sibylæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå…¶ä¸“æ³¨äºæ¥ä¸‹æ¥çš„å¯¹è¯å†…å®¹ï¼Œå¸®åŠ©LLMæ­ç¤ºå¯¹è¯éšå«éœ€æ±‚ã€‚</li>
<li>Sibylæ¡†æ¶å¢å¼ºäº†LLMçš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿäº§ç”Ÿæ›´å…·åŒç†å¿ƒçš„å›åº”ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œèå…¥Sibylæ¡†æ¶åï¼ŒLLMçš„å›åº”è´¨é‡å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>è¿™ä¸€åˆ›æ–°æ¡†æ¶å¯¹äºæ”¹è¿›èŠå¤©æœºå™¨äººçš„æ€§èƒ½å…·æœ‰æ½œåœ¨çš„é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87daefafdf98a536c51175465a7c3ed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1fb60f561c8070a313a84e35ff5779f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c09589e33348ba9b95d2881f818abd49.jpg" align="middle">
</details>




<h2 id="ReactFace-Online-Multiple-Appropriate-Facial-Reaction-Generation-in-Dyadic-Interactions"><a href="#ReactFace-Online-Multiple-Appropriate-Facial-Reaction-Generation-in-Dyadic-Interactions" class="headerlink" title="ReactFace: Online Multiple Appropriate Facial Reaction Generation in   Dyadic Interactions"></a>ReactFace: Online Multiple Appropriate Facial Reaction Generation in   Dyadic Interactions</h2><p><strong>Authors:Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, Hatice Gunes</strong></p>
<p>In dyadic interaction, predicting the listenerâ€™s facial reactions is challenging as different reactions could be appropriate in response to the same speakerâ€™s behaviour. Previous approaches predominantly treated this task as an interpolation or fitting problem, emphasizing deterministic outcomes but ignoring the diversity and uncertainty of human facial reactions. Furthermore, these methods often failed to model short-range and long-range dependencies within the interaction context, leading to issues in the synchrony and appropriateness of the generated facial reactions. To address these limitations, this paper reformulates the task as an extrapolation or prediction problem, and proposes an novel framework (called ReactFace) to generate multiple different but appropriate facial reactions from a speaker behaviour rather than merely replicating the corresponding listener facial behaviours. Our ReactFace generates multiple different but appropriate photo-realistic human facial reactions by: (i) learning an appropriate facial reaction distribution representing multiple different but appropriate facial reactions; and (ii) synchronizing the generated facial reactions with the speaker verbal and non-verbal behaviours at each time stamp, resulting in realistic 2D facial reaction sequences. Experimental results demonstrate the effectiveness of our approach in generating multiple diverse, synchronized, and appropriate facial reactions from each speakerâ€™s behaviour. The quality of the generated facial reactions is intimately tied to the speakerâ€™s speech and facial expressions, achieved through our novel speaker-listener interaction modules. Our code is made publicly available at \url{<a target="_blank" rel="noopener" href="https://github.com/lingjivoo/ReactFace%7D">https://github.com/lingjivoo/ReactFace}</a>. </p>
<blockquote>
<p>åœ¨åŒäººäº’åŠ¨ä¸­ï¼Œé¢„æµ‹å¬è€…çš„é¢éƒ¨è¡¨æƒ…æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå¯¹äºåŒä¸€ä¸ªè¯´è¯è€…çš„è¡Œä¸ºï¼Œä¸åŒçš„ååº”å¯èƒ½æ˜¯æ°å½“çš„ã€‚ä»¥å‰çš„æ–¹æ³•ä¸»è¦å°†æ­¤ä»»åŠ¡è§†ä¸ºæ’å€¼æˆ–æ‹Ÿåˆé—®é¢˜ï¼Œå¼ºè°ƒç¡®å®šæ€§çš„ç»“æœï¼Œä½†å¿½ç•¥äº†äººç±»é¢éƒ¨è¡¨æƒ…çš„å¤šæ ·æ€§å’Œä¸ç¡®å®šæ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€æ— æ³•å¯¹äº’åŠ¨è¯­å¢ƒä¸­çš„çŸ­æœŸå’Œé•¿æœŸä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¯¼è‡´ç”Ÿæˆçš„é¢éƒ¨è¡¨æƒ…çš„åŒæ­¥æ€§å’Œé€‚å½“æ€§å‡ºç°é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡é‡æ–°å®šä¹‰äº†ä»»åŠ¡ä½œä¸ºå¤–æ¨æˆ–é¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼ˆç§°ä¸ºReactFaceï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»è¯´è¯äººçš„è¡Œä¸ºä¸­äº§ç”Ÿå¤šç§ä¸åŒä½†æ°å½“çš„é¢éƒ¨è¡¨æƒ…ï¼Œè€Œä¸ä»…ä»…æ˜¯å¤åˆ¶ç›¸åº”çš„å¬ä¼—é¢éƒ¨è¡¨æƒ…ã€‚æˆ‘ä»¬çš„ReactFaceé€šè¿‡ä»¥ä¸‹æ–¹å¼ç”Ÿæˆå¤šç§ä¸åŒä½†æ°å½“çš„é€¼çœŸçš„äººç±»é¢éƒ¨è¡¨æƒ…ï¼šï¼ˆiï¼‰å­¦ä¹ é€‚å½“çš„é¢éƒ¨è¡¨æƒ…åˆ†å¸ƒï¼Œä»¥è¡¨ç¤ºå¤šç§ä¸åŒä½†æ°å½“çš„é¢éƒ¨è¡¨æƒ…ï¼›ï¼ˆiiï¼‰å°†ç”Ÿæˆçš„é¢éƒ¨è¡¨æƒ…ä¸è¯´è¯è€…çš„è¨€è¯­å’Œéè¨€è¯­è¡Œä¸ºåœ¨æ¯ä¸ªæ—¶é—´æˆ³è¿›è¡ŒåŒæ­¥ï¼Œä»è€Œäº§ç”Ÿé€¼çœŸçš„2Dé¢éƒ¨è¡¨æƒ…åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ¹æ®æ¯ä¸ªè¯´è¯è€…çš„è¡Œä¸ºç”Ÿæˆå¤šç§å¤šæ ·ã€åŒæ­¥ä¸”æ°å½“çš„é¢éƒ¨è¡¨æƒ…æ–¹é¢éå¸¸æœ‰æ•ˆã€‚ç”Ÿæˆé¢éƒ¨è¡¨æƒ…çš„è´¨é‡ä¸è¯´è¯è€…çš„è¨€è¯­å’Œé¢éƒ¨è¡¨æƒ…å¯†åˆ‡ç›¸å…³ï¼Œè¿™æ˜¯é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„è¯´å¬äº’åŠ¨æ¨¡å—å®ç°çš„ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/lingjivoo/ReactFace">https://github.com/lingjivoo/ReactFace</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.15748v2">PDF</a> Accepted to IEEE Transactions on Visualization and Computer Graphics   (TVCG), 18 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°çš„æŠ€æœ¯ï¼Œç”¨äºæ¨¡æ‹Ÿåœ¨åŒäººäº’åŠ¨ä¸­é¢„æµ‹å¬è€…é¢éƒ¨è¡¨æƒ…çš„æŒ‘æˆ˜ã€‚è¿‡å»çš„æ–¹æ³•ä¸»è¦å°†æ­¤ä»»åŠ¡è§†ä¸ºæ’å€¼æˆ–æ‹Ÿåˆé—®é¢˜ï¼Œä½†å¿½ç•¥äº†äººç±»é¢éƒ¨è¡¨æƒ…çš„å¤šæ ·æ€§å’Œä¸ç¡®å®šæ€§ã€‚æ–°æ–¹æ³•å°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå¤–æ¨æˆ–é¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºReactFaceçš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¯´è¯è€…çš„è¡Œä¸ºç”Ÿæˆå¤šç§ä¸åŒçš„é€‚å½“é¢éƒ¨è¡¨æƒ…ï¼Œè€Œä¸æ˜¯ä»…ä»…å¤åˆ¶å¯¹åº”å¬è€…çš„é¢éƒ¨è¡¨æƒ…è¡Œä¸ºã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ å’ŒåŒæ­¥ç”Ÿæˆçš„é¢éƒ¨è¡¨æƒ…ä¸è¯´è¯è€…çš„è¨€è¯­å’Œéè¨€è¯­è¡Œä¸ºï¼Œç”Ÿæˆé€¼çœŸçš„2Dé¢éƒ¨è¡¨æƒ…åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå¤šç§å¤šæ ·ã€åŒæ­¥ä¸”é€‚å½“çš„é¢éƒ¨è¡¨æƒ…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨åŒäººäº’åŠ¨ä¸­é¢„æµ‹å¬è€…çš„é¢éƒ¨è¡¨æƒ…ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è¿‡å»çš„æ–¹æ³•ä¸»è¦å…³æ³¨ç¡®å®šæ€§ç»“æœï¼Œè€Œæ–°æ–¹æ³•å¼ºè°ƒäººç±»é¢éƒ¨è¡¨æƒ…çš„å¤šæ ·æ€§å’Œä¸ç¡®å®šæ€§ã€‚</li>
<li>ReactFaceæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šç§ä¸åŒçš„é€‚å½“é¢éƒ¨è¡¨æƒ…ï¼Œä¸ä»…ä»…æ˜¯å¤åˆ¶å¬è€…çš„è¡Œä¸ºã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ å’ŒåŒæ­¥ç”Ÿæˆçš„é¢éƒ¨è¡¨æƒ…ä¸è¯´è¯è€…çš„è¨€è¯­å’Œéè¨€è¯­è¡Œä¸ºï¼Œå®ç°é€¼çœŸçš„2Dé¢éƒ¨è¡¨æƒ…åºåˆ—ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ã€åŒæ­¥ä¸”é€‚å½“çš„é¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>ç”Ÿæˆé¢éƒ¨è¡¨æƒ…çš„è´¨é‡ä¸è¯´è¯è€…çš„è¯­éŸ³å’Œé¢éƒ¨è¡¨æƒ…ç´§å¯†ç›¸å…³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d5923b3c5ee4fcfe8ee3007c125b8dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e8e18f35955b5d2e731a7e6e892351c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1dc16171ac6fded16a0eb866e561ed8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01642370bab2be01b0559842cde4e76f.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-b3653a69e0fff4eeb4d002dd16804e36.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  PortraitTalk Towards Customizable One-Shot Audio-to-Talking Face   Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-777bb3fc0dac447b1537675992ccfd8f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Multimodal Latent Language Modeling with Next-Token Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">12990.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
