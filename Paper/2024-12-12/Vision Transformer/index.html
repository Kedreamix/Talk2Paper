<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2024-12-12  EOV-Seg Efficient Open-Vocabulary Panoptic Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Vision Transformer/2403.08271v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    31.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    129 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation"><a href="#EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation" class="headerlink" title="EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation"></a>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</h2><p><strong>Authors:Hongwei Niu, Jie Hu, Jianghang Lin, Shengchuan Zhang</strong></p>
<p>Open-vocabulary panoptic segmentation aims to segment and classify everything in diverse scenes across an unbounded vocabulary. Existing methods typically employ two-stage or single-stage framework. The two-stage framework involves cropping the image multiple times using masks generated by a mask generator, followed by feature extraction, while the single-stage framework relies on a heavyweight mask decoder to make up for the lack of spatial position information through self-attention and cross-attention in multiple stacked Transformer blocks. Both methods incur substantial computational overhead, thereby hindering the efficiency of model inference. To fill the gap in efficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and spatial-aware framework designed for open-vocabulary panoptic segmentation. Specifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware Selection (VAS) module is proposed to improve the semantic comprehension of visual aggregated features and alleviate the feature interaction burden on the mask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE), which efficiently utilizes the spatial awareness capabilities of ViT-based CLIP backbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary panoptic segmentation framework towards efficiency, which runs faster and achieves competitive performance compared with state-of-the-art methods. Specifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and 12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and the inference time of EOV-Seg is 4-21 times faster than state-of-the-art methods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with only 71M parameters on a single RTX 3090 GPU. Code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%7D">https://github.com/nhw649/EOV-Seg}</a>. </p>
<blockquote>
<p>开放词汇全景分割旨在分割并分类跨无限词汇表的多样场景中的所有内容。现有方法通常采用两阶段或单阶段框架。两阶段框架涉及使用掩膜生成器生成的掩膜多次裁剪图像，然后进行特征提取，而单阶段框架依赖于重量级的掩膜解码器，通过堆叠的Transformer块中的自注意力和交叉注意力来弥补空间位置信息的缺失。这两种方法都会产生大量的计算开销，从而阻碍模型推理的效率。为了弥补效率上的差距，我们提出了EOV-Seg，这是一种为开放词汇全景分割设计的新型单阶段、共享、高效和具有空间感知的框架。具体来说，EOV-Seg在两个方面进行了创新。首先，提出了词汇感知选择（VAS）模块，以提高视觉聚合特征语义理解能力并减轻掩膜解码器的特征交互负担。其次，我们引入了双向动态嵌入专家（TDEE），它有效地利用了基于ViT的CLIP主干的空间感知能力。据我们所知，EOV-Seg是首个面向效率的开放词汇全景分割框架，与最先进的方法相比，它运行更快并实现了具有竞争力的性能。具体来说，仅在COCO训练集上，EOV-Seg在ADE20K数据集上的全景和语义分割任务上实现了24.2的PQ（全景质量）、31.6的mIoU（平均交并比），并且每秒处理帧数为12.7。特别是，配备ResNet-50主干的EOV-Seg在单个RTX 3090 GPU上仅使用71M参数即可达到每秒25帧的处理速度。代码可通过\url{<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%7D%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/nhw649/EOV-Seg}获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08628v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>该文本介绍了一种针对开放词汇全景分割的高效、单一阶段、共享、空间感知框架EOV-Seg。EOV-Seg通过引入词汇感知选择模块和双向动态嵌入专家技术，提高了语义理解和特征交互效率，并利用基于ViT的CLIP主干的空间感知能力。相较于现有方法，EOV-Seg运行更快，性能更具竞争力。在仅有COCO训练数据的情况下，EOV-Seg在ADE20K数据集上的全景和语义分割任务中实现了24.2 PQ、31.6 mIoU，并且推理时间是现有方法的4-21倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EOV-Seg是一个针对开放词汇全景分割的高效框架。</li>
<li>EOV-Seg采用单一阶段设计，提高了模型推理的效率。</li>
<li>EOV-Seg通过引入词汇感知选择模块，改善了语义理解。</li>
<li>双向动态嵌入专家技术被用于提高特征交互效率。</li>
<li>EOV-Seg利用基于ViT的CLIP主干的空间感知能力。</li>
<li>在仅有COCO训练数据的情况下，EOV-Seg在ADE20K数据集上实现了具有竞争力的性能。</li>
<li>EOV-Seg的推理时间是现有方法的4-21倍，并且在配备ResNet-50主干的条件下，其在单个RTX 3090 GPU上的运行速度为25 FPS，参数仅为71M。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1a4c8da09353ee9d88ed21e70da5e270.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c2d13e725ff84932cdcc5baa8246aae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fe03575aa93513e66c4d741401bd1e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0a26926b98fb664f5c2de0901a0a265c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e55df3ae6bba2f5adc54dab7fecb8925.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4abdef90e4c964b25d477441019e3fb2.jpg" align="middle">
</details>




<h2 id="SAM-Mamba-Mamba-Guided-SAM-Architecture-for-Generalized-Zero-Shot-Polyp-Segmentation"><a href="#SAM-Mamba-Mamba-Guided-SAM-Architecture-for-Generalized-Zero-Shot-Polyp-Segmentation" class="headerlink" title="SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp   Segmentation"></a>SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp   Segmentation</h2><p><strong>Authors:Tapas Kumar Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha</strong></p>
<p>Polyp segmentation in colonoscopy is crucial for detecting colorectal cancer. However, it is challenging due to variations in the structure, color, and size of polyps, as well as the lack of clear boundaries with surrounding tissues. Traditional segmentation models based on Convolutional Neural Networks (CNNs) struggle to capture detailed patterns and global context, limiting their performance. Vision Transformer (ViT)-based models address some of these issues but have difficulties in capturing local context and lack strong zero-shot generalization. To this end, we propose the Mamba-guided Segment Anything Model (SAM-Mamba) for efficient polyp segmentation. Our approach introduces a Mamba-Prior module in the encoder to bridge the gap between the general pre-trained representation of SAM and polyp-relevant trivial clues. It injects salient cues of polyp images into the SAM image encoder as a domain prior while capturing global dependencies at various scales, leading to more accurate segmentation results. Extensive experiments on five benchmark datasets show that SAM-Mamba outperforms traditional CNN, ViT, and Adapter-based models in both quantitative and qualitative measures. Additionally, SAM-Mamba demonstrates excellent adaptability to unseen datasets, making it highly suitable for real-time clinical use. </p>
<blockquote>
<p>结肠镜检查中的息肉分割对于检测结直肠癌至关重要。然而，由于息肉的结构、颜色和大小差异以及与周围组织的边界不清，这使得其成为一项挑战。基于卷积神经网络（CNN）的传统分割模型难以捕捉详细的模式和全局上下文，从而限制了其性能。基于Vision Transformer（ViT）的模型解决了其中的一些问题，但在捕捉局部上下文和缺乏强大的零样本泛化方面存在困难。为此，我们提出了Mamba引导的分段任何模型（SAM-Mamba），以实现有效的息肉分割。我们的方法是在编码器中加入Mamba-Prior模块，以弥合SAM的一般预训练表示和息肉相关的琐碎线索之间的鸿沟。它将息肉图像的关键线索注入SAM图像编码器作为领域先验知识，同时在不同尺度上捕获全局依赖性，从而得到更精确的分割结果。在五个基准数据集上的广泛实验表明，SAM-Mamba在定量和定性指标上均优于传统的CNN、ViT和基于适配器的模型。此外，SAM-Mamba在未标记的数据集上表现出出色的适应性，使其成为实时临床应用的理想选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08482v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了结肠镜检查中息肉分割对检测结直肠癌的重要性。由于息肉结构、颜色和尺寸的差异以及与周围组织的边界不清，导致分割存在挑战。基于卷积神经网络（CNN）的传统分割模型难以捕捉详细模式和全局上下文，性能受限。而基于Vision Transformer（ViT）的模型解决了这些问题，但在捕捉局部上下文和零样本泛化方面存在困难。为此，本文提出了Mamba引导的分割任何模型（SAM-Mamba）进行高效的息肉分割。该方法引入了Mamba先验模块，以弥合SAM的一般预训练表示和息肉相关线索之间的差距。它将息肉图像的关键线索注入SAM图像编码器作为领域先验知识，同时捕捉不同尺度的全局依赖关系，以获得更精确的分割结果。在五个基准数据集上的实验表明，SAM-Mamba在定量和定性度量方面都优于传统的CNN、ViT和适配器模型。此外，SAM-Mamba在未标记的数据集上表现出出色的适应性，非常适合实时临床应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>结肠镜检查中的息肉分割对检测结直肠癌至关重要。</li>
<li>传统基于CNN的分割模型在捕捉详细模式和全局上下文方面存在局限性。</li>
<li>Vision Transformer（ViT）模型解决了CNN模型的一些问题，但在局部上下文和零样本泛化方面仍有挑战。</li>
<li>提出的SAM-Mamba模型通过引入Mamba先验模块，提高了息肉分割的准确性和效率。</li>
<li>SAM-Mamba模型结合了一般预训练表示和息肉相关线索，通过捕捉全局依赖关系获得更精确的分割结果。</li>
<li>在多个基准数据集上，SAM-Mamba的性能优于其他模型，表现出良好的适应性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-541edeac0d0fb433f0faca63f08a05e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b9bcc80c550949c0421566ca502aa5c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c1c472678ee9be4b29eb58d0ad79eb99.jpg" align="middle">
</details>




<h2 id="TextRefiner-Internal-Visual-Feature-as-Efficient-Refiner-for-Vision-Language-Models-Prompt-Tuning"><a href="#TextRefiner-Internal-Visual-Feature-as-Efficient-Refiner-for-Vision-Language-Models-Prompt-Tuning" class="headerlink" title="TextRefiner: Internal Visual Feature as Efficient Refiner for   Vision-Language Models Prompt Tuning"></a>TextRefiner: Internal Visual Feature as Efficient Refiner for   Vision-Language Models Prompt Tuning</h2><p><strong>Authors:Jingjing Xie, Yuxin Zhang, Jun Peng, Zhaohong Huang, Liujuan Cao</strong></p>
<p>Despite the efficiency of prompt learning in transferring vision-language models (VLMs) to downstream tasks, existing methods mainly learn the prompts in a coarse-grained manner where the learned prompt vectors are shared across all categories. Consequently, the tailored prompts often fail to discern class-specific visual concepts, thereby hindering the transferred performance for classes that share similar or complex visual attributes. Recent advances mitigate this challenge by leveraging external knowledge from Large Language Models (LLMs) to furnish class descriptions, yet incurring notable inference costs. In this paper, we introduce TextRefiner, a plug-and-play method to refine the text prompts of existing methods by leveraging the internal knowledge of VLMs. Particularly, TextRefiner builds a novel local cache module to encapsulate fine-grained visual concepts derivedfrom local tokens within the image branch. By aggregating and aligning the cached visual descriptions with the original output of the text branch, TextRefiner can efficiently refine and enrich the learned prompts from existing methods without relying on any external expertise. For example, it improves the performance of CoOp from 71.66 % to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise features for text prompts. Equipped with TextRefiner, PromptKD achieves state-of-the-art performance and is efficient in inference. Our code is relesed at <a target="_blank" rel="noopener" href="https://github.com/xjjxmu/TextRefiner">https://github.com/xjjxmu/TextRefiner</a> </p>
<blockquote>
<p>尽管提示学习在将视觉语言模型（VLMs）转移到下游任务时具有很高的效率，但现有方法主要采用了粗粒度的方式学习提示，其中学习到的提示向量是跨所有类别共享的。因此，定制的提示往往无法识别特定类别的视觉概念，从而阻碍了具有相似或复杂视觉属性的类别的迁移性能。最近的进展通过利用大型语言模型（LLM）的外部知识来提供类别描述来缓解这一挑战，但由此产生的推理成本相当可观。在本文中，我们介绍了TextRefiner，这是一种即插即用方法来优化现有方法的文本提示，通过利用VLMs的内部知识。特别地，TextRefiner建立了一个新颖的本地缓存模块，以封装来自图像分支中局部标记的细粒度视觉概念。通过聚合和对齐缓存的视觉描述与文本分支的原始输出，TextRefiner可以有效地优化和丰富现有方法中的学习提示，而无需依赖任何外部专家知识。例如，它在11个基准测试上将CoOp的性能从71.66%提高到76.94%，超越了引入实例级特征的CoCoOp用于文本提示的方法。配备TextRefiner后，PromptKD达到了最先进的性能并且在推理方面非常高效。我们的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/xjjxmu/TextRefiner%E4%B8%8B%E3%80%82">https://github.com/xjjxmu/TextRefiner下。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08176v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为TextRefiner的即插即用方法，用于通过利用视觉语言模型（VLMs）的内部知识来优化现有方法的文本提示。TextRefiner建立了一个新型本地缓存模块，以封装图像分支中局部标记的细粒度视觉概念。通过聚合和对齐缓存的视觉描述与文本分支的原始输出，TextRefiner能够在不依赖任何外部专业知识的情况下，有效地优化和丰富现有方法的学到的提示。使用TextRefiner，PromptKD在推理上实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有方法在粗粒度方式下学习提示，导致针对特定类别的提示无法区分相似的视觉概念，限制了下游任务的性能。</li>
<li>TextRefiner通过利用视觉语言模型（VLMs）的内部知识来优化文本提示，提出了一种即插即用的方法。</li>
<li>TextRefiner建立了一个新型本地缓存模块，该模块能够封装图像分支中的细粒度视觉概念。</li>
<li>TextRefiner通过聚合和对齐缓存的视觉描述与文本分支的原始输出，能够丰富并优化现有方法的学到的提示。</li>
<li>TextRefiner在不依赖任何外部专业知识的情况下，提高了现有方法的性能。</li>
<li>在多个基准测试中，TextRefiner提高了CoOp的性能，超越了引入实例级特征的CoCoOp方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7d402bcb2d7f33b54c8bfc63dc2df5b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ba1a62f4d387ed5429c0963047c71f57.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-362f18538a8926ab6d9349ce6cfcd492.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7823e4ced9b560b6aea55bb844dd2591.jpg" align="middle">
</details>




<h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. </p>
<blockquote>
<p>在利用对比语言图像预训练（CLIP）在互联网规模的图像文本对上构建视觉模型方面取得了巨大成功，但是利用CLIP管道构建可迁移的图神经网络（GNN）却面临三大挑战：标注数据缺乏和文本监督、下游任务的不同层次以及领域间的概念差距。在这项工作中，为了应对这些问题，我们利用多模态提示学习来有效地适应预训练GNN到下游任务和只有少数语义标记样本的数据上，每个样本的文本监督都非常弱。我们的新范式通过将图直接嵌入与大型语言模型（LLM）相同的空间，通过同时学习图提示和文本提示来实现。为了完成这项工作，我们改进了最先进的图提示方法，然后提出了第一个利用预训练模型知识的图-语言多模态提示学习方法。值得注意的是，由于微调时的监督不足，在我们的范式中，预训练的GNN和LLM保持不变，因此可学习的参数远远少于微调任何预训练模型。在现实数据集上的大量实验表明，我们的范式在少样本、多任务级别和跨域设置中均表现出卓越的性能。此外，我们构建了第一个CLIP风格的零样本分类原型，可以将GNN推广到具有非常弱文本监督的未见类别中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v1">PDF</a> Preprint, 26 pages</p>
<p><strong>Summary</strong><br>     针对构建具有对比语言图像预训练（CLIP）的互联网规模图像文本对的视觉模型已取得巨大成功，但利用CLIP管道构建可迁移的图神经网络（GNN）面临三大基本问题：缺乏标注数据和文本监督、下游任务层次不同以及领域之间的概念鸿沟。为解决这些问题，我们利用多模式提示学习，仅使用少量语义标注样本和极弱的文本监督，有效地适应预训练GNN到下游任务和数据。我们的新范式通过将图直接嵌入与大型语言模型（LLM）相同的空间，同时学习图提示和文本提示。我们改进了最先进的图提示方法，并首次提出了用于挖掘预训练模型中知识的图语言多模式提示学习方法。值得注意的是，由于微调时的监督不足，在我们的范式中，预训练的GNN和LLM保持不变，因此可学习参数远少于对任何预训练模型的微调。通过现实数据集的大量实验，我们证明了我们的范式在少样本、多任务级别和跨域设置中的优越性能。此外，我们构建了第一个CLIP风格的零样本分类原型，能够利用极弱的文本监督将GNN推广到未见类别。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用对比语言图像预训练（CLIP）构建视觉模型虽已取得成功，但在应用CLIP管道构建可迁移的图神经网络（GNN）时面临三大挑战。</li>
<li>缺乏标注数据和文本监督、下游任务层次不同以及领域间概念鸿沟是构建可迁移GNN的主要难题。</li>
<li>通过多模式提示学习，能有效适应预训练GNN到下游任务和数据，仅使用少量语义标注样本和极弱的文本监督。</li>
<li>新的范式将图直接嵌入与大型语言模型（LLM）相同的空间，同时学习图提示和文本提示。</li>
<li>改进了图提示方法，并首次提出图语言多模式提示学习方法，以利用预训练模型中的知识。</li>
<li>在微调过程中，由于监督不足，预训练的GNN和LLM保持不变，降低了可学习参数的数量。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-418010f18183fa1efe9737e4f27a25e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fea15ced17268cfd3a73eccc52e763fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a61bf3a22face5a83a5c80d12004d0a7.jpg" align="middle">
</details>




<h2 id="Leveraging-Content-and-Context-Cues-for-Low-Light-Image-Enhancement"><a href="#Leveraging-Content-and-Context-Cues-for-Low-Light-Image-Enhancement" class="headerlink" title="Leveraging Content and Context Cues for Low-Light Image Enhancement"></a>Leveraging Content and Context Cues for Low-Light Image Enhancement</h2><p><strong>Authors:Igor Morawski, Kai He, Shusil Dangi, Winston H. Hsu</strong></p>
<p>Low-light conditions have an adverse impact on machine cognition, limiting the performance of computer vision systems in real life. Since low-light data is limited and difficult to annotate, we focus on image processing to enhance low-light images and improve the performance of any downstream task model, instead of fine-tuning each of the models which can be prohibitively expensive. We propose to improve the existing zero-reference low-light enhancement by leveraging the CLIP model to capture image prior and for semantic guidance. Specifically, we propose a data augmentation strategy to learn an image prior via prompt learning, based on image sampling, to learn the image prior without any need for paired or unpaired normal-light data. Next, we propose a semantic guidance strategy that maximally takes advantage of existing low-light annotation by introducing both content and context cues about the image training patches. We experimentally show, in a qualitative study, that the proposed prior and semantic guidance help to improve the overall image contrast and hue, as well as improve background-foreground discrimination, resulting in reduced over-saturation and noise over-amplification, common in related zero-reference methods. As we target machine cognition, rather than rely on assuming the correlation between human perception and downstream task performance, we conduct and present an ablation study and comparison with related zero-reference methods in terms of task-based performance across many low-light datasets, including image classification, object and face detection, showing the effectiveness of our proposed method. </p>
<blockquote>
<p>低光条件对机器认知产生不利影响，限制了计算机视觉系统在现实生活中的性能。由于低光数据有限且难以标注，我们专注于图像处理，以增强低光图像并改善任何下游任务模型的性能，而不是微调可能非常昂贵的每个模型。我们提出利用CLIP模型捕获图像先验并进行语义引导，以改进现有的无参考低光增强。具体来说，我们提出了一种基于提示学习通过图像采样学习图像先验的数据增强策略，无需任何配对或非配对正常光数据。接下来，我们提出了一种语义引导策略，通过引入关于图像训练补丁的内容和上下文线索，最大限度地利用现有的低光标注。我们在一项定性研究中发现，所提出的前驱知识和语义引导有助于提高图像的整体对比度和色调，改善背景前景的辨识能力，减少相关无参考方法中常见的过饱和和噪声过度放大问题。鉴于我们的目标领域是机器认知，因此并未假定人类感知与下游任务性能之间的相关性。我们进行了一项消融研究，并在多个低光数据集上与相关无参考方法进行了任务性能比较，包括图像分类、物体和面部检测等，展示了我们方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07693v1">PDF</a> Accepted to the IEEE Transactions on Multimedia</p>
<p><strong>Summary</strong><br>低光环境对机器认知有负面影响。为提高计算机视觉系统在低光环境下的性能，研究团队提出利用CLIP模型捕获图像先验信息和语义指导的策略。通过数据增强策略学习图像先验信息，无需配对或未配对正常光照数据。同时，提出语义指导策略充分利用现有的低光标注信息，引入图像训练片段的内容和上下文线索。实验证明，所提出的方法和语义指导有助于改善图像整体对比度和色调，提高背景与前景的辨识能力，减少相关零参考方法常见的过饱和和噪声过度放大问题。研究团队以机器认知为目标，通过对比实验和对比相关零参考方法，展示其在多个低光照数据集上的任务性能表现的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低光环境对机器认知有负面影响，限制了计算机视觉系统的性能。</li>
<li>研究团队聚焦于图像处理以增强低光图像，而不是微调模型以应对高昂的成本。</li>
<li>利用CLIP模型捕获图像先验信息和语义指导，用于改善低光图像性能。</li>
<li>通过数据增强策略学习图像先验信息，无需正常光照数据。</li>
<li>提出语义指导策略充分利用现有的低光标注信息，包含内容和上下文线索。</li>
<li>实验结果显示，所提出的方法提高了图像对比度和色调，改善了背景与前景的辨识能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-767a0f93c595a60a9ca97a321bfc3787.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b7b92e95081ad085b9918ddd57fdf836.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6198317ed762b401441d9e4a7949bfa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d0e888355bc133d29c9f5300aafb061f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-acae8c89829e76df73ac2e0771e6fee5.jpg" align="middle">
</details>




<h2 id="Retaining-and-Enhancing-Pre-trained-Knowledge-in-Vision-Language-Models-with-Prompt-Ensembling"><a href="#Retaining-and-Enhancing-Pre-trained-Knowledge-in-Vision-Language-Models-with-Prompt-Ensembling" class="headerlink" title="Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models   with Prompt Ensembling"></a>Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models   with Prompt Ensembling</h2><p><strong>Authors:Donggeun Kim, Yujin Jo, Myungjoo Lee, Taesup Kim</strong></p>
<p>The advancement of vision-language models, particularly the Contrastive Language-Image Pre-training (CLIP) model, has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to understand and respond to previously unseen data without task-specific training. However, adapting CLIP to integrate specialized knowledge from various domains while retaining its zero-shot capabilities remains a significant challenge. To address this, we introduce a novel prompt ensemble learning approach called Group-wise Prompt Ensemble (GPE). This method aims to enhance CLIP’s zero-shot capabilities by incorporating new domain knowledge while improving its adaptability and robustness against data distribution shifts. Our approach hinges on three main strategies: prompt grouping with masked attention to optimize CLIP’s adaptability while safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts for the seamless integration of new domain insights without disrupting the original model’s representation; and an ensemble learning strategy that effectively merges original and new knowledge. Through rigorous experimentation, including more challenging cross-dataset transfer evaluations, our GPE method redefines the benchmarks for the adaptability and efficiency of vision-language models, surpassing existing models across various scenarios. </p>
<blockquote>
<p>视觉语言模型的进步，特别是对比语言图像预训练（CLIP）模型的进步，已经通过实现强大的零样本学习能力，彻底改变了机器学习领域。这些能力使得模型能够理解和响应之前未见过的数据，而无需进行特定任务的训练。然而，将CLIP适应以整合来自不同领域的专业知识，同时保持其零样本学习能力，仍然是一个巨大的挑战。为了解决这一问题，我们提出了一种新的提示集成学习方法，称为群组提示集成（GPE）。该方法旨在通过结合新的领域知识，增强CLIP的零样本学习能力，同时提高其适应性和对数据分布变化的稳健性。我们的方法依赖于三个主要策略：通过提示分组和掩膜注意力来优化CLIP的适应性，同时保护其零样本学习能力；通过引入辅助提示，实现新领域洞察的无缝集成，而不会破坏原始模型的表示；以及一种有效的集成学习策略，可以很好地融合新旧知识。通过严格的实验，包括更具挑战性的跨数据集迁移评估，我们的GPE方法重新定义了视觉语言模型适应性和效率的标准，在各种场景下均超越了现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07077v1">PDF</a> IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV)   2025</p>
<p><strong>Summary</strong></p>
<p>CLIP模型通过实现强大的零样本学习能力，为机器学习领域带来了革命性的变革。然而，在集成特定领域知识的同时保持其零样本学习能力仍是CLIP模型面临的挑战。为了应对这一挑战，我们提出了一种新颖的提示集成学习方法——分组提示集成（GPE）。通过精细的提示分组与掩膜注意力机制、辅助提示的融入以及有效的集成学习策略，GPE在保障CLIP模型零样本学习能力的同时提升了其适应性和稳健性。实验证明，GPE方法在不同场景下均超越了现有模型，重新定义了视觉语言模型的适应性和效率基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP模型实现了零样本学习能力，为机器学习领域带来重大变革。</li>
<li>在集成特定领域知识的同时保持CLIP模型的零样本学习能力是一项挑战。</li>
<li>分组提示集成（GPE）方法通过精细的提示分组与掩膜注意力机制优化CLIP模型的适应性。</li>
<li>辅助提示的融入实现了新领域知识与原始模型表示的无缝集成。</li>
<li>GPE采用有效的集成学习策略，将原始知识与新知识有效结合。</li>
<li>实验证明，GPE方法在适应性和效率方面超越了现有模型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-279149c621356f1222167de36f0a8df6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-75cf14cba4f83c1c094e3bcbfea43f90.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bf34f58f9ac27d52dc2c5d1821d4d1e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14c76a110d190eccc16befbe3591091d.jpg" align="middle">
</details>




<h2 id="Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning"><a href="#Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning" class="headerlink" title="Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning"></a>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</h2><p><strong>Authors:Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou</strong></p>
<p>Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner’s capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence. </p>
<blockquote>
<p>视觉语言模型（VLMs）在多模态推理任务中取得了显著的进步。然而，由于诸如虚构的图像理解或粗糙的推理路径等问题，它们仍然经常产生不准确或无关的响应。为了解决这些挑战，我们引入了Critic-V，这是一个受Actor-Critic范式启发的新型框架，旨在提升VLMs的推理能力。该框架通过集成两个独立组件来解耦推理过程和批评过程：Reasoner根据视觉和文本输入生成推理路径，而Critic提供建设性批评以优化这些路径。在这个方法中，Reasoner根据文本提示生成推理响应，这些响应可以基于Critic的反馈作为策略进行迭代演化。这一交互过程是由强化学习框架驱动的，其中Critic提供自然语言批评而不是标量奖励，从而可以提供更细微的反馈来提升Reasoner在复杂推理任务上的能力。Critic模型使用直接偏好优化（DPO）进行训练，利用基于规则的奖励（RBR）排名的批评偏好数据集来增强其批评能力.评估结果表明，Critic-V框架在8个基准测试中优于现有方法（包括GPT-4V），特别是在推理准确性和效率方面。结合Reasoner的动态文本策略以及来自偏好优化后的Critic的建设性反馈，实现了更可靠和语境敏感的多模态推理过程。我们的方法为提升VLMs的可靠性提供了有前景的解决方案，改进其在现实世界推理密集型多模态应用（如自动驾驶和智能实体）中的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18203v2">PDF</a> 16 pages, 11 figures</p>
<p><strong>摘要</strong></p>
<p>VLM（视觉语言模型）在多模态推理任务中展现出显著进展，但仍存在生成不准确或无关响应的问题。为此，本文提出了Critic-V框架，该框架受Actor-Critic范式的启发，旨在提高VLM的推理能力。该框架通过整合两个独立组件——Reasoner和Critic，实现了推理过程和批评过程的解耦。Reasoner根据视觉和文本输入生成推理路径，而Critic提供建设性批评以优化这些路径。在该方法中，Reasoner根据文本提示生成推理响应，这些响应可以根据来自Critic的反馈进行迭代演化。这一交互过程由强化学习框架驱动，Critic提供自然语言批评而非标量奖励，为Reasoner在复杂推理任务上的能力提升提供更微妙的反馈。Critic模型使用直接偏好优化（DPO）进行训练，利用基于规则的奖励（RBR）对批评进行排名，以提高其批评能力。评估结果表明，Critic-V框架在5项指标中的表现显著优于现有方法（包括GPT-4V），特别是在推理准确性和效率方面。结合基于文本的动态政策和经过优化的偏好Critic的反馈，为更可靠和上下文敏感的多模态推理过程提供了有前途的解决方案。本研究为提高VLM在现实世界推理密集型多模态应用中的可靠性（如自动驾驶和智能嵌入）提供了可靠方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VLM在多模态推理任务中展现出优势，但仍存在生成不准确或无关响应的问题。</li>
<li>Critic-V框架基于Actor-Critic范式构建，旨在提高VLM的推理能力。</li>
<li>框架包含Reasoner和Critic两个独立组件，分别负责生成和优化推理路径。</li>
<li>Critic使用自然语言批评进行强化学习训练，以提供更微妙的反馈。</li>
<li>直接偏好优化（DPO）用于训练Critic模型，以提高其批评能力。</li>
<li>Critic-V框架在多个基准测试中表现优于现有方法，特别是在推理准确性和效率方面。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-618f29ee5430b286ee8b46c0e035f63f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6837c2c367cd1a4e7ee3ad4ec2bd8ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-269d187768ea4ab5aa83a7d89fb133bc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f2278620f94584191480f74c9cf8598c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg" align="middle">
</details>




<h2 id="COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding"><a href="#COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding" class="headerlink" title="COBRA: A Continual Learning Approach to Vision-Brain Understanding"></a>COBRA: A Continual Learning Approach to Vision-Brain Understanding</h2><p><strong>Authors:Xuan-Bac Nguyen, Arabinda Kumar Choudhary, Pawan Sinha, Xin Li, Khoa Luu</strong></p>
<p>Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module contains a transformer encoder and decoder that learns the fMRI features for VBU from common and specific patterns. In a continual learning setup, COBRA is trained in new PSS and MRIFormer modules for new subjects, leaving the modules of previous subjects unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods. </p>
<blockquote>
<p>视觉大脑理解（VBU）旨在从通过功能性磁共振成像（fMRI）记录的大脑活动中提取人类感知到的视觉信息。尽管近年来取得了显著的进展，但VBU的现有研究仍然面临着灾难性遗忘的挑战，即模型在适应新主题时丢失了先前主题的知识。因此，解决该领域的持续学习至关重要。本文介绍了一个名为COBRA的持续学习框架，以解决VBU中的持续学习问题。我们的方法包括三个新颖模块：主题共性（SC）模块、基于提示的主题特定（PSS）模块和基于变压器的fMRI模块，称为MRIFormer模块。SC模块捕获跨主题的共同视觉大脑模式，并在模型遇到新主题时保留这些知识，从而减少灾难性遗忘的影响。另一方面，PSS模块学习每个主题特有的视觉大脑模式。最后，MRIFormer模块包含一个变压器编码器和解码器，从公共和特定模式中学习VBU的fMRI特征。在持续学习设置中，COBRA针对新主题训练新的PSS和MRIFormer模块，而不影响以前的模块。因此，COBRA有效地解决了灾难性遗忘问题，并在持续学习和视觉大脑重建任务中实现了最先进的性能，超越了以前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17475v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视觉脑理解（VBU）领域的持续学习问题，提出了一种新的框架——COBRA。该框架包含三个模块：主体共性（SC）模块、基于提示的主体特定（PSS）模块和基于变压器的fMRI模块（MRIFormer）。其中SC模块捕捉不同主体间的共享视觉脑模式，减少灾难性遗忘的影响；PSS模块学习每个主体的独特视觉脑模式；MRIFormer模块则通过变压器编码器解码器学习VBU的fMRI特征。COBRA框架在持续学习设置中训练新的PSS和MRIFormer模块以适应新主体，同时保持先前主体的模块不受影响，从而有效地解决灾难性遗忘问题，并在持续学习和视觉脑重建任务中达到优于先前方法的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Brain Understanding (VBU) 通过功能磁共振成像 (fMRI) 提取人类感知的视觉信息。</li>
<li>VBU领域面临灾难性遗忘的挑战，即模型在适应新主体时丢失先前知识。</li>
<li>COBRA框架包含三个模块：SC模块、PSS模块和MRIFormer模块，分别用于捕捉视觉脑模式的共享特征、学习主体特定的视觉脑模式以及学习VBU的fMRI特征。</li>
<li>COBRA通过训练新的PSS和MRIFormer模块来解决灾难性遗忘问题，同时保持先前主体的模块不变。</li>
<li>COBRA框架在持续学习和视觉脑重建任务中达到先进性能，优于先前方法。</li>
<li>SC模块在模型遇到新主体时能够保留知识，降低灾难性遗忘的影响。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6c702b3809502647f842e98a6554e0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7070aad685269c094671faaee23e433e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c8b7e55ab8521742037def62f4d2e55d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3409f541bd3078454792cebe005df6e1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f197015eba6fee457426dc6a87da541b.jpg" align="middle">
</details>




<h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits models’ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>开放词汇语义分割（OVSS）随着最近的视觉语言模型（VLMs）的发展而进步，通过各种学习方案实现了超出预定类别的分割。值得注意的是，无训练方法为处理未见数据提供了可扩展、易于部署的解决方案，这是OVSS的关键目标。然而，仍存在一个问题：在OVSS的挑战环境中，基于任意查询提示对复杂对象进行分割时，缺乏对象级别的上下文考虑。这一疏忽限制了模型将对象内语义一致元素分组并将其精确映射到用户定义的任意类别的能力。在这项工作中，我们引入了一种新方法，通过结合图像中的对象级上下文知识来克服这一局限性。具体来说，我们的模型通过将从视觉基础模型蒸馏出的光谱驱动特征融入视觉编码器的注意机制中，增强了对象内部的一致性，使语义一致的组件能够形成单个对象掩码。此外，我们还通过零样本对象存在可能性对文本嵌入进行了精炼，以确保与图像中表示的具体对象准确对齐。通过利用对象级的上下文知识，我们提出的方法在多个数据集上实现了最先进的性能，并具有较强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于开放词汇语义分割（OVSS）的新方法，该方法通过将图像中的对象级上下文知识纳入考虑，解决了在没有预先定义类别的情境下，通过任意查询提示对复杂对象进行分割的问题。通过在视觉基础模型的频谱驱动特征中提炼注意力机制，实现跨多种数据集进行语义一致的分割和识别，提高了模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>开放词汇语义分割（OVSS）允许模型处理超越预设类别的分割任务。</li>
<li>训练免费的方法为处理未见过的数据提供了可扩展和易于部署的解决方案。</li>
<li>当前方法缺乏对象级别的上下文考虑，在复杂环境中对基于任意查询提示的对象进行分割时存在局限。</li>
<li>新方法通过融入图像中的对象级上下文知识来克服这一局限。</li>
<li>通过提炼视觉基础模型的频谱驱动特征到视觉编码器的注意力机制，增强了模型对同一对象的内部一致性。</li>
<li>使用零样本对象存在概率来优化文本嵌入，确保与图像中特定对象的准确对齐。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f7b6fbeb2826a7dad8df4b062d3486a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffccfcbd45ef2a94b19be0bce7c72be4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bbe7a6fdd528b4861885240dd1defd77.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-90fa9c3ab36b67f06e6c20c1253b7353.jpg" align="middle">
</details>




<h2 id="Free-2-Guide-Gradient-Free-Path-Integral-Control-for-Enhancing-Text-to-Video-Generation-with-Large-Vision-Language-Models"><a href="#Free-2-Guide-Gradient-Free-Path-Integral-Control-for-Enhancing-Text-to-Video-Generation-with-Large-Vision-Language-Models" class="headerlink" title="Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing   Text-to-Video Generation with Large Vision-Language Models"></a>Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing   Text-to-Video Generation with Large Vision-Language Models</h2><p><strong>Authors:Jaemin Kim, Bryan S Kim, Jong Chul Ye</strong></p>
<p>Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free$^2$Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free$^2$Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free$^2$Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos. </p>
<blockquote>
<p>扩散模型在文本到图像（T2I）和文本到视频（T2V）生成等生成任务中取得了令人印象深刻的结果。然而，由于帧之间的复杂时间依赖性，在T2V生成中实现准确的文本对齐仍然是一个挑战。现有的基于强化学习（RL）的方法来提高文本对齐性通常需要一个可微分的奖励函数，或者仅限于有限的提示，阻碍了其可扩展性和适用性。在本文中，我们提出了Free$^2$Guide，这是一个新颖的无需梯度的框架，用于将生成的视频与文本提示对齐，而无需额外的模型训练。借助路径积分控制原理，Free$^2$Guide使用不可微分的奖励函数来近似扩散模型的指导，从而能够整合强大的黑盒大型视觉语言模型（LVLMs）作为奖励模型。此外，我们的框架支持灵活地集成多个奖励模型，包括大规模图像模型，以协同提高对齐性，而不会产生大量的计算开销。我们证明，Free$^2$Guide在各个方面都显著提高了文本对齐性，并提高了生成视频的整体质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17041v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出了Free$^2$Guide框架，该框架无需梯度即可实现文本与视频生成的准确对齐，无需额外的模型训练。利用路径积分控制原理，该框架可以使用非可微奖励函数为扩散模型提供指导，并能灵活集成多种奖励模型（包括大型图像模型），协同提高对齐性能，同时不产生显著的计算开销。实验表明，Free$^2$Guide可显著提高文本对齐的多个维度质量并增强生成视频的总体质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Free$^2$Guide是一个无需梯度的框架，用于实现文本与视频生成的准确对齐，无需额外的模型训练。</li>
<li>利用路径积分控制原理，Free$^2$Guide可以使用非可微奖励函数为扩散模型提供指导。</li>
<li>该框架支持集成大型视觉语言模型（LVLMs）作为奖励模型。</li>
<li>Free$^2$Guide能够灵活集成多种奖励模型，包括大型图像模型，以提高对齐性能。</li>
<li>该框架在不产生显著计算开销的情况下协同提高对齐质量。</li>
<li>实验表明，Free$^2$Guide显著提高文本对齐的多个维度质量。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ba9efcd661a35290481f3768b433b309.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e4438e21327f8b25ae41719540dc1b7d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a2b8f0ff8a42045f29b53e669a15022.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-46ed930ef96424876051209fae4d6526.jpg" align="middle">
</details>




<h2 id="Active-Prompt-Learning-with-Vision-Language-Model-Priors"><a href="#Active-Prompt-Learning-with-Vision-Language-Model-Priors" class="headerlink" title="Active Prompt Learning with Vision-Language Model Priors"></a>Active Prompt Learning with Vision-Language Model Priors</h2><p><strong>Authors:Hoyoung Kim, Seokhee Jin, Changhwan Sung, Jaechang Kim, Jungseul Ok</strong></p>
<p>Vision-language models (VLMs) have demonstrated remarkable zero-shot performance across various classification tasks. Nonetheless, their reliance on hand-crafted text prompts for each task hinders efficient adaptation to new tasks. While prompt learning offers a promising solution, most studies focus on maximizing the utilization of given few-shot labeled datasets, often overlooking the potential of careful data selection strategies, which enable higher accuracy with fewer labeled data. This motivates us to study a budget-efficient active prompt learning framework. Specifically, we introduce a class-guided clustering that leverages the pre-trained image and text encoders of VLMs, thereby enabling our cluster-balanced acquisition function from the initial round of active learning. Furthermore, considering the substantial class-wise variance in confidence exhibited by VLMs, we propose a budget-saving selective querying based on adaptive class-wise thresholds. Extensive experiments in active learning scenarios across nine datasets demonstrate that our method outperforms existing baselines. </p>
<blockquote>
<p>视觉语言模型（VLMs）在各种分类任务中表现出了显著的零样本性能。然而，它们对每项任务的手工文本提示的依赖阻碍了其对新任务的效率适应。虽然提示学习提供了有前景的解决方案，但大多数研究侧重于最大限度地利用给定的少量标注数据集，往往忽视了精心选择数据策略的潜力，这些策略可以较少的标注数据实现更高的精度。这促使我们研究一种经济高效的活动提示学习框架。具体来说，我们引入了一种类指导聚类的方法，利用VLMs的预训练图像和文本编码器，从而在我们的活动学习初始阶段实现集群平衡采集功能。此外，考虑到VLMs表现出的巨大类别间信心差异，我们提出了一种基于自适应类别阈值的预算节省选择性查询方法。在九个数据集的活动学习场景下的广泛实验表明，我们的方法超过了现有基线水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16722v1">PDF</a> </p>
<p><strong>Summary</strong><br>     视觉语言模型（VLMs）在各种分类任务中表现出出色的零样本性能，但其依赖于为每个任务手工制作的文本提示，阻碍了在新任务上的有效适应。本研究提出了一种预算高效的活动提示学习框架，通过引入类指导聚类，利用VLMs的预训练图像和文本编码器，实现集群平衡采集功能。此外，考虑到VLMs展现的类间置信度差异较大，我们提出了基于自适应类阈值的预算节约选择性查询。在九个数据集上的主动学习场景实验表明，我们的方法优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在各种分类任务中表现出出色的零样本性能。</li>
<li>现有模型依赖于手工制作的文本提示，难以适应新任务。</li>
<li>研究提出了一种预算高效的活动提示学习框架来解决这一问题。</li>
<li>通过引入类指导聚类，利用预训练图像和文本编码器实现集群平衡采集功能。</li>
<li>考虑到了VLMs展现的类间置信度差异大，提出了基于自适应类阈值的预算节约选择性查询策略。</li>
<li>在九个数据集上的实验表明，该方法在主动学习场景中表现优于现有基线。</li>
<li>此方法在提高模型效率和适应性方面具有潜在应用价值。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3b1870a9668332c94cbafa2aecd08c4e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-938e9cbd8a98eb2489756bc58a9fe4d9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b9ca16ce76baa8dd4fc4becd56178131.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-874cecae9f5d4ba2733d4e3799a61963.jpg" align="middle">
</details>




<h2 id="BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models"><a href="#BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"></a>BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a>. </p>
<blockquote>
<p>最近，视觉语言模型（如CLIP）在视觉任务的自监督表示学习方面取得了巨大成功。然而，将视觉语言模型有效地适应到下游应用仍然是一个挑战，因为它们的准确性往往依赖于耗时且需要专业知识的提示工程，而全面调整模型又很昂贵。特别是对于生物医学图像，与天然图像不同，它们通常受限于标注数据集、图像对比度不够直观以及微妙的视觉特征。最近的提示学习技术（如CoOp）旨在解决这些问题，但仍缺乏通用性。同时，针对生物医学图像分析的提示学习探索仍然非常有限。在这项工作中，我们提出了BiomedCoOp，这是一种新型的提示学习框架，能够高效地适应BiomedCLIP，以实现准确且高度通用的生物医学图像分类。我们的方法通过利用与大型语言模型（LLM）的平均提示集合的语义一致性以及基于统计的提示选择策略的知识蒸馏，实现了有效的提示上下文学习。我们在跨越9种模态和10个器官的11个医疗数据集上对所提出的框架进行了全面的验证，与现有的最先进的方法相比，准确性和通用性都有显著提高。代码将在 <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a> 上公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15232v1">PDF</a> 18 pages, 5 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对生物医学图像分类任务的新的提示学习框架BiomedCoOp。该框架结合了语义一致性、大型语言模型的平均提示集合以及基于统计的提示选择策略，实现了对BiomedCLIP模型的有效提示上下文学习，在少数标注样本上达到高精度和高度泛化的分类效果。该框架在跨越9种模态和10种器官的11个医学数据集上进行了全面的验证，与现有最先进的方法相比，准确性和泛化能力均显著提高。代码将在GitHub上公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种新的提示学习框架BiomedCoOp，用于解决生物医学图像分类中的挑战性问题。</li>
<li>BiomedCoOp结合了语义一致性、大型语言模型的平均提示集合以及基于统计的提示选择策略，提高了模型的准确性和泛化能力。</li>
<li>该框架能够实现对BiomedCLIP模型的有效提示上下文学习，实现在少数标注样本上的高精度分类。</li>
<li>在跨越多种模态和器官的多个医学数据集上的验证结果表明，BiomedCoOp的准确性和泛化能力显著提高。</li>
<li>BiomedCoOp解决了现有的生物医学图像分类任务面临的挑战，包括有限标注数据集、图像对比度不够直观以及微妙的视觉特征等问题。</li>
<li>本文的创新点在于结合语义一致性进行提示上下文学习，以及采用基于统计的提示选择策略。这种结合方式有助于提高模型的泛化能力和准确性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f47ee0177c47ded9dbffb2322847edc0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b35924fcdaa57f51ad7edfe0b37e082e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1e42816bd1d7c7d6931bb05d1a5ec10b.jpg" align="middle">
</details>




<h2 id="Looking-Beyond-Text-Reducing-Language-bias-in-Large-Vision-Language-Models-via-Multimodal-Dual-Attention-and-Soft-Image-Guidance"><a href="#Looking-Beyond-Text-Reducing-Language-bias-in-Large-Vision-Language-Models-via-Multimodal-Dual-Attention-and-Soft-Image-Guidance" class="headerlink" title="Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance"></a>Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance</h2><p><strong>Authors:Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang</strong></p>
<p>Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on images and ineffective visual comprehension. We identify two primary reasons for this bias: 1. Different scales of training data between the pretraining stage of LLM and multimodal alignment stage. 2. The learned inference bias due to short-term dependency of text data. Therefore, we propose LACING, a systemic framework designed to address the language bias of LVLMs with muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG). Specifically, MDA introduces a parallel dual-attention mechanism that enhances the integration of visual inputs across the model. IFG introduces a learnable soft visual prompt during training and inference to replace visual inputs, designed to compel LVLMs to prioritize text inputs. Then, IFG further proposes a novel decoding strategy using the soft visual prompt to mitigate the model’s over-reliance on adjacent text inputs. Comprehensive experiments demonstrate that our method effectively debiases LVLMs from their language bias, enhancing visual comprehension and reducing hallucinations without requiring additional training resources or data. The code and model are available at <a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>. </p>
<blockquote>
<p>大型视觉语言模型（LVLMs）在各种视觉语言任务中取得了令人印象深刻的结果。然而，尽管表现出有希望的性能，LVLMs却存在由语言偏见导致的幻觉问题，这导致对图像的关注度降低和视觉理解无效。我们确定了这种偏见的两个主要原因：1. 介于大型语言模型预训练阶段和多模态对齐阶段之间的训练数据规模不同。2. 由于文本数据的短期依赖性而产生的推断偏见。因此，我们提出了LACING系统框架，旨在解决LVLMs的语言偏见问题，采用多模态双注意力机制（MDA）和软图像引导（IFG）。具体而言，MDA引入了一种并行双注意力机制，增强了模型中视觉输入的集成。IFG在训练和推理过程中引入了一种可学习的软视觉提示来替代视觉输入，旨在迫使LVLMs优先考虑文本输入。然后，IFG进一步提出了一种使用软视觉提示的新解码策略，以减轻模型对相邻文本输入的过度依赖。综合实验表明，我们的方法有效地消除了LVLMs的语言偏见，提高了视觉理解能力，减少了幻觉，且无需额外的训练资源或数据。代码和模型可在<a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14279v1">PDF</a> 19 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型视觉语言模型（LVLMs）在视觉语言任务中取得了令人印象深刻的成果，但它们存在语言偏见导致的幻觉问题，影响了对图像的关注力和视觉理解能力。为解决这一问题，本文提出了LACING框架，该框架通过引入多模态双注意力机制和软图像引导来纠正LVLMs的语言偏见。实验表明，该方法能提升视觉理解力并减少幻觉现象。详情请访问<a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LVLMs在视觉语言任务中表现出色，但存在语言偏见导致的幻觉问题。</li>
<li>偏见的主要原因是训练数据规模不同以及文本数据的短期依赖性。</li>
<li>LACING框架通过引入多模态双注意力机制和软图像引导来纠正LVLMs的语言偏见。</li>
<li>软图像引导可在训练和推理过程中替代视觉输入，促使LVLMs更重视文本输入。</li>
<li>LACING框架采用新的解码策略，使用软视觉提示减少模型对相邻文本输入的依赖。</li>
<li>综合实验表明，LACING方法能有效纠正LVLMs的语言偏见，提高视觉理解力和减少幻觉现象。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c8e25e0c4c4756d97250e95df89ceb63.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-67ce42f158bf6263df9b3ed306ddd4ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cfe243aff073e1742ceed0eca4ad04bc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4e1bd75a9d32e6557561b4146d803ba2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-77ab27e8b7b3ac5a903851fbe4378ab0.jpg" align="middle">
</details>




<h2 id="Segment-Any-Class-SAC-Multi-Class-Few-Shot-Semantic-Segmentation-via-Class-Region-Proposals"><a href="#Segment-Any-Class-SAC-Multi-Class-Few-Shot-Semantic-Segmentation-via-Class-Region-Proposals" class="headerlink" title="Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via   Class Region Proposals"></a>Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via   Class Region Proposals</h2><p><strong>Authors:Hussni Mohd Zakir, Eric Tatt Wei Ho</strong></p>
<p>The Segment-Anything Model (SAM) is a vision foundation model for segmentation with a prompt-driven framework. SAM generates class-agnostic masks based on user-specified instance-referring prompts. However, adapting SAM for automated segmentation – where manual input is absent – of specific object classes often requires additional model training. We present Segment Any Class (SAC), a novel, training-free approach that task-adapts SAM for Multi-class segmentation. SAC generates Class-Region Proposals (CRP) on query images which allows us to automatically generate class-aware prompts on probable locations of class instances. CRPs are derived from elementary intra-class and inter-class feature distinctions without any additional training. Our method is versatile, accommodating any N-way K-shot configurations for the multi-class few-shot semantic segmentation (FSS) task. Unlike gradient-learning adaptation of generalist models which risk the loss of generalization and potentially suffer from catastrophic forgetting, SAC solely utilizes automated prompting and achieves superior results over state-of-the-art methods on the COCO-20i benchmark, particularly excelling in high N-way class scenarios. SAC is an interesting demonstration of a prompt-only approach to adapting foundation models for novel tasks with small, limited datasets without any modifications to the foundation model itself. This method offers interesting benefits such as intrinsic immunity to concept or feature loss and rapid, online task adaptation of foundation models. </p>
<blockquote>
<p>Segment-Anything Model（SAM）是一个基于提示驱动的框架进行分割的视觉基础模型。SAM根据用户指定的实例引用提示生成类无关掩码。然而，将SAM适应于没有手动输入的特定对象类的自动分割通常需要额外的模型训练。我们提出了Segment Any Class（SAC）这一新型无训练方法，用于对SAM进行多任务适应的多类分割。SAC在查询图像上生成类区域提案（CRP），使我们能够在类实例的可能位置自动产生类感知提示。CRP是从基本的类内和类间特征差异中得出的，无需任何额外训练。我们的方法通用性强，适应任何针对多类少样本语义分割（FSS）任务的N路K射击配置。与可能导致泛化损失和潜在灾难性遗忘的通用模型的梯度学习适应不同，SAC仅利用自动化提示，并在COCO-20i基准测试上实现了优于最新方法的结果，特别是在高N路类别场景中表现出色。SAC是一个有趣的演示，展示了仅使用提示的方法如何适应新型任务的基础模型，使用小且有限的数据集，而无需对基础模型本身进行任何修改。这种方法提供了有趣的优点，如内在地避免了概念或特征损失和快速、在线的任务适应基础模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13774v1">PDF</a> 8 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Segment-Anything Model（SAM）的改进版本Segment Any Class（SAC）。SAC是一种无需训练的方法，可自适应地将SAM用于多类分割任务。它通过生成类区域提案（CRPs）来自动产生类感知提示，从而无需手动输入即可实现特定对象类的自动化分割。该方法具有灵活性，适用于多类别少样本语义分割任务的任何N路K射击配置。SAC在COCO-20i基准测试中实现了优于现有技术的方法的结果，特别是在高N路类别场景中表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Segment Any Class (SAC)是一个无需训练的方法，用于自适应地将Segment-Anything Model（SAM）应用于多类分割任务。</li>
<li>SAC通过生成类区域提案（CRPs）自动产生类感知提示，实现特定对象类的自动化分割。</li>
<li>CRPs是基于图像内和图像间的基本特征差异得出的，无需任何额外的训练。</li>
<li>SAC方法具有灵活性，可适应任何N路K射击配置的多类别少样本语义分割任务。</li>
<li>SAC在COCO-20i基准测试中实现了卓越的结果，特别是在高N路类别场景中。</li>
<li>SAC方法具有内在的优势，如对抗概念或特征丢失的免疫性和基础模型的快速在线任务适应性。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d971b8f1fd31197fc322d5dbcf0fbddf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-19d42f313dc3c9fb2555211abd55ff49.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-719226815bc206ee26208752a81162b5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-641afa3769daf97fd5fc45721f55b89c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c914aa2015da19ab2f59b760fe680218.jpg" align="middle">
</details>




<h2 id="SAG-ViT-A-Scale-Aware-High-Fidelity-Patching-Approach-with-Graph-Attention-for-Vision-Transformers"><a href="#SAG-ViT-A-Scale-Aware-High-Fidelity-Patching-Approach-with-Graph-Attention-for-Vision-Transformers" class="headerlink" title="SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph   Attention for Vision Transformers"></a>SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph   Attention for Vision Transformers</h2><p><strong>Authors:Shravan Venkatraman, Jaskaran Singh Walia, Joe Dhanith P R</strong></p>
<p>Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multiscale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance. Our code and weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/shravan-18/SAG-ViT">https://github.com/shravan-18/SAG-ViT</a> </p>
<blockquote>
<p>图像分类是计算机视觉任务之一，模型通过分析图像将其分类为特定标签。视觉转换器（ViT）通过利用自我注意力机制来捕捉图像补丁之间的复杂模式和长程关系，从而改进了这一任务。然而，ViT的关键挑战在于有效地融入多尺度特征表示，这是CNN的固有属性，得益于其分层结构。在本文中，我们引入了Scale-Aware Graph Attention Vision Transformer（SAG-ViT）这一新型框架，通过融合多尺度特征来解决这一挑战。使用EfficientNet作为骨干网，该模型提取多尺度特征图，将其划分为补丁以保留语义信息。这些补丁基于空间特征和特征相似性组织成一个图，通过图注意力网络（GAT）对节点嵌入进行精炼。最后，变压器编码器捕捉长程依赖关系和复杂交互。SAG-ViT在基准数据集上进行了评估，证明了其在提高图像分类性能方面的有效性。我们的代码和权重可在<a target="_blank" rel="noopener" href="https://github.com/shravan-18/SAG-ViT%E5%85%AC%E5%BC%BA%E8%8E%B7%E3%80%82">https://github.com/shravan-18/SAG-ViT公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09420v2">PDF</a> 10 pages, 4 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>模型分析图像并将其分类为特定标签是计算机视觉任务中的一种。本文引入了一种名为Scale-Aware Graph Attention Vision Transformer（SAG-ViT）的新框架，它通过整合多尺度特征来改进这一任务。通过使用EfficientNet作为骨干网，模型提取多尺度特征映射并划分为区块以保留语义信息。基于空间特征相似性，这些区块被组织成图结构，再通过图注意力网络（GAT）优化节点嵌入。最后，Transformer编码器捕捉长期依赖关系和复杂交互作用。SAG-ViT在基准数据集上的评估证明了其在提高图像分类性能方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViT)利用自注意力机制改善图像分类任务。</li>
<li>多尺度特征表示是ViT面临的关键挑战，而CNN通过其层次结构固有地包含此特性。</li>
<li>SAG-ViT框架解决了这一挑战，通过整合多尺度特征提高了图像分类性能。</li>
<li>EfficientNet作为骨干网用于提取多尺度特征映射，划分区块以保留语义信息。</li>
<li>图注意力网络（GAT）用于优化基于空间特征相似性的区块节点嵌入。</li>
<li>Transformer编码器捕捉长期依赖关系和复杂交互作用。</li>
<li>SAG-ViT在基准数据集上的评估证明了其有效性。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8fb6033fd9f0d6d2b6fb77c021c73886.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-adf3ae7d6c3114687cbe0c207e504746.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1f64ae851224f57cef4251b9b722ab40.jpg" align="middle">
</details>




<h2 id="GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection"></a>GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection</h2><p><strong>Authors:Jiyul Ham, Yonggon Jung, Jun-Geol Baek</strong></p>
<p>Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting anomalous patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP">https://github.com/YUL-git/GlocalCLIP</a>. </p>
<blockquote>
<p>零样本异常检测（ZSAD）对于在无需训练样本的情况下检测目标数据集中的异常模式至关重要，特别是在目标域与训练数据之间存在分布差异或由于访问受限而导致数据稀缺的场景中。尽管最近预训练的视觉语言模型在各种视觉任务中表现出强大的零样本性能，但它们主要关注类别语义的学习，这使得它们直接应用于ZSAD具有挑战性。为了应对这一场景，我们提出了GlocalCLIP方法，该方法独特地分离全局和局部提示并进行联合优化。这一方法使得与对象无关的局部语义提示能够有效地捕捉通用正常和异常模式，而无需依赖图像中的特定对象。我们通过利用文本编码器的深度文本提示调整来完善文本提示，以实现更精确的调整。在视觉编码器方面，我们应用V-V注意力层来捕捉详细的局部图像特征。最后，我们引入了局部对比学习，以提高全局和局部提示的互补学习，有效地检测各种领域的异常模式。GlocalCLIP在ZSAD中的泛化性能在来自工业和医疗领域的15个真实世界数据集上得到了验证，与现有方法相比表现出卓越的性能。代码将在<a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YUL-git/GlocalCLIP上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06071v3">PDF</a> 29 pages, 36 figures</p>
<p><strong>Summary</strong></p>
<p>GlocalCLIP方法利用全局和局部提示的分离和联合优化，实现零样本异常检测。通过深度文本提示调整和V-V注意力层，提高文本编码器和视觉编码器性能。引入全局对比学习，有效检测不同领域的异常模式，并在真实世界的工业和医疗数据集上实现卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GlocalCLIP解决了零样本异常检测的挑战，特别是当目标域与训练数据存在分布差异或数据稀缺时。</li>
<li>方法通过分离全局和局部提示，并对其进行联合优化，以捕捉正常的和异常的图案。</li>
<li>深度文本提示调整和V-V注意力层的使用，提高了文本编码器和视觉编码器的性能。</li>
<li>引入全局对比学习，促进全局和局部提示的互补学习。</li>
<li>GlocalCLIP在多种真实世界数据集上实现了卓越性能，包括工业和医疗领域。</li>
<li>该方法不仅关注对象语义的学习，还能有效地捕捉一般的正常和异常模式。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26e62baf3d484faf017e96a0933d2b89.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1125ab6e3ab5117d2b7066fa7f664de1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2f8c17425f03137645e1037af7d759b4.jpg" align="middle">
</details>




<h2 id="Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy"><a href="#Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy" class="headerlink" title="Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy"></a>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy</h2><p><strong>Authors:Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</strong></p>
<p>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large GIE dataset, called EndoExtend24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of GIE medical image diagnosis. Specifically, the EVA-02 model, which is based on the ViT architecture and trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762 and a balanced accuracy of 37.1% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched EndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics. </p>
<blockquote>
<p>视频胶囊内镜技术通过提供一种无创方法，能够捕捉胃肠道的详细图像，从而实现了胃肠道内窥镜（GIE）诊断的变革，使早期疾病检测成为可能。然而，其潜力受限于成像过程中产生的图像数量庞大，成像过程可能需要6-8小时，并且经常产生高达100万张图像，因此需要自动化分析。此外，这些图像的变异性，加上需要专家标注以及大规模、高质量标注数据集的稀缺性，限制了当前医学图像分析模型的有效性。为了解决这一问题，我们引入了一个新型大型GIE数据集EndoExtend24，它是通过合并十个现有的公共和私有数据集创建的，确保了跨分割的患者完整性。EndoExtend24包含超过22万张标注图像，以及动态类映射，可以在不同标签粒度的数据集上进行统一训练，支持多达123种不同的病理发现。此外，我们提议利用基于自监督的通用图像数据域自适应预训练的模型进行预训练（基础模型）。具体来说，EVA-02模型基于ViT架构，在ImageNet-22k上采用掩码图像建模（使用EVA-CLIP作为MIM教师）进行训练，然后在EndoExtend24数据集上进行预训练以实现域适应，并最终在Capsule Endoscopy 2024 Challenge数据集上进行训练。我们的模型表现出稳健的性能，在Capsule Endoscopy 2024挑战中获得了第三名。在测试集上，我们的模型实现了宏观AUC为0.762和平衡精度为37.1%。这些结果强调了我们域自适应预训练方法和丰富的EndoExtend24数据集在推进胃肠道内窥镜诊断方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21302v4">PDF</a> </p>
<p><strong>Summary</strong><br>     视频胶囊内镜为胃肠道内窥镜（GIE）诊断提供了一种非侵入性的图像捕捉方法，有助于早期疾病检测。然而，由于成像过程中产生的图像数量庞大，以及图像之间的差异和缺乏大型高质量标签数据集，限制了其潜力。为解决这些问题，我们创建了一个名为EndoExtend24的大型GIE数据集，包含超过22.6万张标记图像，并支持统一的跨数据集训练。此外，我们提出了基于域自适应预训练的方法，利用在通用图像数据上自我监督训练的模型进行任务适应。我们的EVA-02模型基于ViT架构，在ImageNet-22k上进行预训练并适应EndoExtend24数据集，最后在Capsule Endoscopy 2024挑战数据集上表现出强大性能。本研究强调了域自适应预训练方法和丰富多样的EndoExtend24数据集在推进胃肠道内窥镜诊断方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频胶囊内镜为胃肠道内窥镜诊断提供了非侵入性的图像捕捉方法，有助于早期疾病检测。</li>
<li>成像过程中产生的图像数量庞大，对自动分析提出了需求。</li>
<li>创建了名为EndoExtend24的大型GIE数据集，包含超过22.6万张标记图像，并支持统一的跨数据集训练。</li>
<li>引入域自适应预训练方法，利用通用图像数据上的预训练模型进行任务适应。</li>
<li>EVA-02模型基于ViT架构进行开发，表现强大性能，在Capsule Endoscopy 2024挑战中获得了第三名的好成绩。</li>
<li>预训练模型在EndoExtend24数据集上的表现优异，验证了域自适应预训练方法和数据集的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf9713e7ffc155a83738fb6388d5f539.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7c49edf263644ff07b1db7da8210619a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8f4db2dad3e28af984765d3201e9f3f.jpg" align="middle">
</details>




<h2 id="Croc-Pretraining-Large-Multimodal-Models-with-Cross-Modal-Comprehension"><a href="#Croc-Pretraining-Large-Multimodal-Models-with-Cross-Modal-Comprehension" class="headerlink" title="Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"></a>Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension</h2><p><strong>Authors:Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng</strong></p>
<p>Recent advances in Large Language Models (LLMs) have catalyzed the development of Large Multimodal Models (LMMs). However, existing research primarily focuses on tuning language and image instructions, ignoring the critical pretraining phase where models learn to process textual and visual modalities jointly. In this paper, we propose a new pretraining paradigm for LMMs to enhance the visual comprehension capabilities of LLMs by introducing a novel cross-modal comprehension stage. Specifically, we design a dynamically learnable prompt token pool and employ the Hungarian algorithm to replace part of the original visual tokens with the most relevant prompt tokens. Then, we conceptualize visual tokens as analogous to a “foreign language” for the LLMs and propose a mixed attention mechanism with bidirectional visual attention and unidirectional textual attention to comprehensively enhance the understanding of visual tokens. Meanwhile, we integrate a detailed caption generation task, leveraging rich descriptions to further facilitate LLMs in understanding visual semantic information. After pretraining on 1.5 million publicly accessible data, we present a new foundation model called Croc. Experimental results demonstrate that Croc achieves new state-of-the-art performance on massive vision-language benchmarks. To support reproducibility and facilitate further research, we release the training code and pre-trained model weights at <a target="_blank" rel="noopener" href="https://github.com/deepglint/Croc">https://github.com/deepglint/Croc</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展推动了大型多模态模型（LMM）的发展。然而，现有研究主要集中在调整语言和图像指令上，忽略了模型学习处理文本和视觉模态的联合关键预训练阶段。在本文中，我们提出了针对LMM的新型预训练范式，通过引入新型跨模态理解阶段，增强LMM的视觉理解能力。具体来说，我们设计了一个可动态学习的提示令牌池，并使用匈牙利算法将最相关的提示令牌替换部分原始视觉令牌。然后，我们将视觉令牌概念化为对LLM而言类似于“外语”，并提出了一种混合注意力机制，包括双向视觉注意力和单向文本注意力，以全面增强对视觉令牌的理解。同时，我们整合了详细的标题生成任务，利用丰富的描述来进一步帮助LLM理解视觉语义信息。在150万个可公开访问的数据上进行预训练后，我们推出了一种新型基础模型——名为Croc。实验结果表明，Croc在大规模视觉语言基准测试上取得了最新先进性能。为了支持复现性和促进进一步研究，我们在<a target="_blank" rel="noopener" href="https://github.com/deepglint/Croc%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/deepglint/Croc上发布了训练代码和预训练模型权重。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14332v2">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的大型多模态模型（LMMs）预训练范式，通过引入跨模态理解阶段，增强LLMs对视觉内容的理解能力。设计动态可学习的提示令牌池，并采用匈牙利算法替换原始视觉令牌中最相关的提示令牌。提出混合注意力机制，增强对视觉令牌的理解。集成详细的字幕生成任务，利用丰富的描述来进一步促进LLMs对视觉语义信息的理解。经过在公开数据上150万数据的预训练后，推出了名为Croc的新基础模型，在大型视觉语言基准测试中取得了最新性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的预训练范式，旨在增强大型多模态模型（LMMs）对视觉内容的理解能力。</li>
<li>设计了动态可学习的提示令牌池并使用匈牙利算法替换原始视觉令牌，以提升模型的跨模态理解性能。</li>
<li>引入了一种混合注意力机制，其中包括双向视觉注意力和单向文本注意力，以全面增强对视觉令牌的理解。</li>
<li>通过集成详细的字幕生成任务，利用丰富的描述促进LLMs对视觉语义信息的理解。</li>
<li>在大量公开数据上进行了预训练，并推出名为Croc的新基础模型。</li>
<li>Croc模型在多个大规模视觉语言基准测试中取得了最新性能表现。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4fac2c5023429cd38d7cfd08b23d572d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-83d0b44eaeba4e5120d67b0acb18ed3a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-510283aee83c3ca612cfb20cdd7a5698.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6c6e1965293b5e4d1fdb140adac90060.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3b0d48f4ce6c1df054446abdf6f08b6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f2b356769bcd341221808c5d6c482e2.jpg" align="middle">
</details>




<h2 id="Local-to-Global-Self-Supervised-Representation-Learning-for-Diabetic-Retinopathy-Grading"><a href="#Local-to-Global-Self-Supervised-Representation-Learning-for-Diabetic-Retinopathy-Grading" class="headerlink" title="Local-to-Global Self-Supervised Representation Learning for Diabetic   Retinopathy Grading"></a>Local-to-Global Self-Supervised Representation Learning for Diabetic   Retinopathy Grading</h2><p><strong>Authors:Mostafa Hajighasemlou, Samad Sheikhaei, Hamid Soltanian-Zadeh</strong></p>
<p>Artificial intelligence algorithms have demonstrated their image classification and segmentation ability in the past decade. However, artificial intelligence algorithms perform less for actual clinical data than those used for simulations. This research aims to present a novel hybrid learning model using self-supervised learning and knowledge distillation, which can achieve sufficient generalization and robustness. The self-attention mechanism and tokens employed in ViT, besides the local-to-global learning approach used in the hybrid model, enable the proposed algorithm to extract a high-dimensional and high-quality feature space from images. To demonstrate the proposed neural network’s capability in classifying and extracting feature spaces from medical images, we use it on a dataset of Diabetic Retinopathy images, specifically the EyePACS dataset. This dataset is more complex structurally and challenging regarding damaged areas than other medical images. For the first time in this study, self-supervised learning and knowledge distillation are used to classify this dataset. In our algorithm, for the first time among all self-supervised learning and knowledge distillation models, the test dataset is 50% larger than the training dataset. Unlike many studies, we have not removed any images from the dataset. Finally, our algorithm achieved an accuracy of 79.1% in the linear classifier and 74.36% in the k-NN algorithm for multiclass classification. Compared to a similar state-of-the-art model, our results achieved higher accuracy and more effective representation spaces. </p>
<blockquote>
<p>在过去的十年中，人工智能算法已经证明了它们在图像分类和分割方面的能力。然而，与模拟数据相比，人工智能算法在实际的临床数据上的表现较差。本研究旨在提出一种新型的混合学习模型，该模型结合了自监督学习和知识蒸馏，可实现足够的通用性和稳健性。ViT中使用的自注意力机制和令牌，以及混合模型中采用的从局部到全局的学习方法，使得所提出算法能够从图像中提取高维高质量的特征空间。为了展示所提出神经网络在医学图像分类和特征空间提取方面的能力，我们在糖尿病视网膜病变图像的数据集上使用了它，特别是EyePACS数据集。与其他医学图像相比，此数据集在结构上更为复杂，并且在损伤区域方面更具挑战性。在这项研究中，首次使用自监督学习和知识蒸馏对此数据集进行分类。在我们的算法中，测试数据集首次在所有自监督学习和知识蒸馏模型中比训练数据集大50%。与其他许多研究不同，我们没有从数据集中删除任何图像。最后，我们的算法在线性分类器中达到了79.1%的准确率，在k-NN算法的多类分类中达到了74.36%的准确率。与类似的最先进模型相比，我们的结果实现了更高的准确性和更有效的表示空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合自监督学习与知识蒸馏的新型混合学习模型，旨在提高在真实临床数据上的图像分类与分割能力。该研究使用ViT的自注意力机制和令牌，以及混合模型的局部到全局学习方法，从图像中提取高维高质量的特征空间。在眼病数据集EyePACS上进行实验，未移除任何图像，首次使用自监督学习与知识蒸馏对此数据集进行分类，且测试数据集大小是训练数据集的50%。实验结果显示，该算法在线性分类器上达到了79.1%的准确率，在k-NN算法上的多分类准确率为74.36%，相较于类似的前沿模型，具有更高的准确率和更有效的表示空间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新型混合学习模型结合了自监督学习与知识蒸馏，旨在提高在真实临床数据上的图像分类与分割性能。</li>
<li>研究使用了ViT的自注意力机制和令牌进行特征提取。</li>
<li>首次使用自监督学习与知识蒸馏对EyePACS数据集进行分类。</li>
<li>实验在较大的测试数据集上进行，保留了所有图像，未进行任何移除。</li>
<li>算法在线性分类器上达到了79.1%的准确率，在k-NN算法上的多分类准确率为74.36%。</li>
<li>与类似的前沿模型相比，该算法具有更高的准确率和更有效的表示空间。</li>
<li>该研究展示了局部到全局的学习方法在特征提取中的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-71166837b37799c2254baa0b71b8f5ca.jpg" align="middle">
</details>




<h2 id="Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches"><a href="#Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches" class="headerlink" title="Patch Ranking: Efficient CLIP by Learning to Rank Local Patches"></a>Patch Ranking: Efficient CLIP by Learning to Rank Local Patches</h2><p><strong>Authors:Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado</strong></p>
<p>Contrastive image-text pre-trained models such as CLIP have shown remarkable adaptability to downstream tasks. However, they face challenges due to the high computational requirements of the Vision Transformer (ViT) backbone. Current strategies to boost ViT efficiency focus on pruning patch tokens but fall short in addressing the multimodal nature of CLIP and identifying the optimal subset of tokens for maximum performance. To address this, we propose greedy search methods to establish a “Golden Ranking” and introduce a lightweight predictor specifically trained to approximate this Ranking. To compensate for any performance degradation resulting from token pruning, we incorporate learnable visual tokens that aid in restoring and potentially enhancing the model’s performance. Our work presents a comprehensive and systematic investigation of pruning tokens within the ViT backbone of CLIP models. Through our framework, we successfully reduced 40% of patch tokens in CLIP’s ViT while only suffering a minimal average accuracy loss of 0.3 across seven datasets. Our study lays the groundwork for building more computationally efficient multimodal models without sacrificing their performance, addressing a key challenge in the application of advanced vision-language models. </p>
<blockquote>
<p>对比图像文本预训练模型，如CLIP，对下游任务表现出显著的适应性。然而，由于视觉转换器（ViT）主干的高计算要求，它们面临挑战。当前提高ViT效率的策略主要集中在修剪补丁令牌上，但未能充分解决CLIP的多模式性质，并确定获得最佳性能令牌的最优子集。为了解决这一问题，我们提出贪心搜索方法来建立“金牌排名”，并引入一个专门训练的轻量级预测器来近似这个排名。为了弥补因令牌修剪导致的性能下降，我们引入了可学习的视觉令牌，有助于恢复并可能提高模型的性能。我们的工作对CLIP模型的ViT主干中的令牌修剪进行了全面系统的研究。通过我们的框架，我们成功地在CLIP的ViT中减少了40%的补丁令牌，同时在七个数据集上只遭受了最小的平均精度损失0.3。我们的研究为构建更高效的多模式模型奠定了基础，不会牺牲其性能，解决了先进视觉语言模型应用中的关键挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14607v2">PDF</a> Accepted by WACV 2025</p>
<p><strong>Summary</strong></p>
<p>CLIP等对比图像文本预训练模型在下游任务中展现出强大的适应性，但由于Vision Transformer（ViT）的高计算需求而面临挑战。当前提升ViT效率的策略主要聚焦于修剪patch tokens，但未能充分解决CLIP的多模态特性，难以识别出最佳token子集以实现最佳性能。本研究提出贪心搜索方法建立“Golden Ranking”，并引入轻量级预测器进行近似训练。为弥补因token修剪导致的性能下降，研究融入可学习视觉token以助力恢复并可能提升模型性能。本研究系统探讨了CLIP模型中ViT的token修剪问题。通过框架成功减少CLIP中ViT的40% patch tokens，在七个数据集上的平均精度损失仅为0.3%。研究为构建更高效的多模态模型奠定基础，解决了先进视觉语言模型应用中的关键挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比图像文本预训练模型（如CLIP）在下游任务中展现出强大适应性，但面临高计算需求的挑战。</li>
<li>当前ViT效率提升策略主要聚焦于修剪patch tokens，但未能解决多模态特性问题。</li>
<li>引入贪心搜索方法建立“Golden Ranking”，以优化token选择。</li>
<li>引入轻量级预测器进行近似训练，提高模型效率。</li>
<li>通过融入可学习视觉token，弥补因token修剪导致的性能下降。</li>
<li>研究成功减少CLIP中ViT的40% patch tokens，平均精度损失较小。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-664c7eb1e38c884b8462b58f14c9bb58.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4b50734fdd8a61a05103e07a48cddc85.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-acc868f79e316a0a2a222f988321e1e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f6fc089ef5f80ecfc84c97cba9322acd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-25436ad5d9b413ff1e62690a36714432.jpg" align="middle">
</details>




<h2 id="Generalizing-Deepfake-Video-Detection-with-Plug-and-Play-Video-Level-Blending-and-Spatiotemporal-Adapter-Tuning"><a href="#Generalizing-Deepfake-Video-Detection-with-Plug-and-Play-Video-Level-Blending-and-Spatiotemporal-Adapter-Tuning" class="headerlink" title="Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level   Blending and Spatiotemporal Adapter Tuning"></a>Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level   Blending and Spatiotemporal Adapter Tuning</h2><p><strong>Authors:Zhiyuan Yan, Yandan Zhao, Shen Chen, Mingyi Guo, Xinghe Fu, Taiping Yao, Shouhong Ding, Li Yuan</strong></p>
<p>Three key challenges hinder the development of current deepfake video detection: (1) Temporal features can be complex and diverse: how can we identify general temporal artifacts to enhance model generalization? (2) Spatiotemporal models often lean heavily on one type of artifact and ignore the other: how can we ensure balanced learning from both? (3) Videos are naturally resource-intensive: how can we tackle efficiency without compromising accuracy? This paper attempts to tackle the three challenges jointly. First, inspired by the notable generality of using image-level blending data for image forgery detection, we investigate whether and how video-level blending can be effective in video. We then perform a thorough analysis and identify a previously underexplored temporal forgery artifact: Facial Feature Drift (FFD), which commonly exists across different forgeries. To reproduce FFD, we then propose a novel Video-level Blending data (VB), where VB is implemented by blending the original image and its warped version frame-by-frame, serving as a hard negative sample to mine more general artifacts. Second, we carefully design a lightweight Spatiotemporal Adapter (StA) to equip a pretrained image model (both ViTs and CNNs) with the ability to capture both spatial and temporal features jointly and efficiently. StA is designed with two-stream 3D-Conv with varying kernel sizes, allowing it to process spatial and temporal features separately. Extensive experiments validate the effectiveness of the proposed methods; and show our approach can generalize well to previously unseen forgery videos, even the latest generation methods. </p>
<blockquote>
<p>当前深度伪造视频检测的发展面临三大关键挑战：</p>
</blockquote>
<p>（1）时序特征可能复杂且多样：我们如何识别通用时序伪迹以提高模型的泛化能力？</p>
<p>（2）时空模型通常偏向于一种伪迹而忽视另一种：我们如何确保从两者中都能实现均衡学习？</p>
<p>（3）视频本身是资源密集型的：我们如何在不损害准确性的情况下提高效率？</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17065v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文旨在联合解决深度伪造视频检测中的三大挑战：一是如何识别通用时序特征以提高模型泛化能力；二是如何确保时空模型的平衡学习；三是如何在不降低准确性的情况下解决资源密集问题。研究通过视频级别的混合策略，发现了一种被忽视的时序伪造特征——面部特征漂移（FFD）。此外，设计了一种轻量级的时空适配器（StA），使预训练图像模型能够联合且高效地捕获空间和时间特征。实验证明，该方法能很好地泛化到未见过的伪造视频，包括最新一代方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文指出了深度伪造视频检测的三个主要挑战，并尝试同时解决它们。</li>
<li>研究通过视频级别的混合策略，发现了面部特征漂移（FFD）这一重要的时序伪造特征。</li>
<li>提出了一种新的视频级别混合数据（VB）方法，通过混合原始图像和其变形版本帧来挖掘更通用的特征。</li>
<li>设计了一种轻量级的时空适配器（StA），使预训练图像模型能够联合捕获空间和时间特征。</li>
<li>StA采用双流3D卷积设计，具有不同内核大小，可分别处理空间和时序特征。</li>
<li>实验证明了所提出方法的有效性，并能很好地泛化到未见过的伪造视频。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26403bcf86a4e06aa0a0588d5f8dc98d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3ab2357afab60d89a4f9d8798d9a900b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e36d6c564478b66dfef23d88cf5ac8b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ca20647e44cbfe4651a7c3403acdbcf0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5de1a1d6927644a276545568298cb68d.jpg" align="middle">
</details>




<h2 id="GalLoP-Learning-Global-and-Local-Prompts-for-Vision-Language-Models"><a href="#GalLoP-Learning-Global-and-Local-Prompts-for-Vision-Language-Models" class="headerlink" title="GalLoP: Learning Global and Local Prompts for Vision-Language Models"></a>GalLoP: Learning Global and Local Prompts for Vision-Language Models</h2><p><strong>Authors:Marc Lafon, Elias Ramzi, Clément Rambour, Nicolas Audebert, Nicolas Thome</strong></p>
<p>Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs), e.g. CLIP, for few-shot image classification. Despite their success, most prompt learning methods trade-off between classification accuracy and robustness, e.g. in domain generalization or out-of-distribution (OOD) detection. In this work, we introduce Global-Local Prompts (GalLoP), a new prompt learning method that learns multiple diverse prompts leveraging both global and local visual features. The training of the local prompts relies on local features with an enhanced vision-text alignment. To focus only on pertinent features, this local alignment is coupled with a sparsity strategy in the selection of the local features. We enforce diversity on the set of prompts using a new &#96;&#96;prompt dropout’’ technique and a multiscale strategy on the local prompts. GalLoP outperforms previous prompt learning methods on accuracy on eleven datasets in different few shots settings and with various backbones. Furthermore, GalLoP shows strong robustness performances in both domain generalization and OOD detection, even outperforming dedicated OOD detection methods. Code and instructions to reproduce our results: <a target="_blank" rel="noopener" href="https://github.com/MarcLafon/gallop">https://github.com/MarcLafon/gallop</a>. </p>
<blockquote>
<p>提示学习已被广泛应用于高效地适应视觉语言模型（如CLIP），以进行少量图像分类。尽管取得了成功，但大多数提示学习方法在分类精度和稳健性（例如在域泛化或异常值检测）之间进行了权衡。在这项工作中，我们引入了全局局部提示（GalLoP），这是一种新的提示学习方法，它利用全局和局部视觉特征学习多个不同的提示。局部提示的训练依赖于具有增强视觉文本对齐的局部特征。为了只关注重要特征，这种局部对齐与选择局部特征的稀疏策略相结合。我们通过一种新的“提示丢弃”技术和局部提示的多尺度策略，在提示集上强制执行多样性。在具有不同少量样本和不同骨干网络的十一个数据集上，GalLoP在准确性方面的表现超过了先前的提示学习方法。此外，即使在领域泛化和异常值检测中，GalLoP也表现出强大的稳健性能，甚至超越了专门的异常值检测方法。我们的结果的复现代码和说明：<a target="_blank" rel="noopener" href="https://github.com/MarcLafon/gallop%E3%80%82">https://github.com/MarcLafon/gallop。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01400v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的提示学习方法——全局局部提示（GalLoP），该方法结合全局和局部视觉特征学习多个不同的提示。通过增强视觉文本对齐和局部特征选择中的稀疏策略，专注于关键特征。使用新的提示丢弃技术和局部提示的多尺度策略来增强提示的多样性。在多种不同数据集和背景下的少量样本设置中，GalLoP在准确度上优于先前的提示学习方法，并且在域泛化和异常值检测中表现出强大的稳健性，甚至超越了专门的异常值检测方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GalLoP是一种新的提示学习方法，结合全局和局部视觉特征学习多个提示。</li>
<li>通过增强视觉文本对齐和局部特征的稀疏选择策略，专注于关键特征。</li>
<li>使用提示丢弃技术和多尺度策略来增强提示的多样性。</li>
<li>GalLoP在多种数据集和背景下的少量样本设置中具有更高的分类准确度。</li>
<li>GalLoP在域泛化和异常值检测方面表现出强大的稳健性。</li>
<li>GalLoP甚至超越了专门的异常值检测方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-237f00d55c73090da6b9b04ad47f2a83.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e01c9df94e2c5f46fb9d421e1f32aba2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-76aeb8413b2e6a351f64557226fbf06e.jpg" align="middle">
</details>




<h2 id="3DStyleGLIP-Part-Tailored-Text-Guided-3D-Neural-Stylization"><a href="#3DStyleGLIP-Part-Tailored-Text-Guided-3D-Neural-Stylization" class="headerlink" title="3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization"></a>3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization</h2><p><strong>Authors:SeungJeh Chung, JooHyun Park, HyeongYeop Kang</strong></p>
<p>3D stylization, the application of specific styles to three-dimensional objects, offers substantial commercial potential by enabling the creation of uniquely styled 3D objects tailored to diverse scenes. Recent advancements in artificial intelligence and text-driven manipulation methods have made the stylization process increasingly intuitive and automated. While these methods reduce human costs by minimizing reliance on manual labor and expertise, they predominantly focus on holistic stylization, neglecting the application of desired styles to individual components of a 3D object. This limitation restricts the fine-grained controllability. To address this gap, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization. Given a 3D mesh and a text prompt, 3DStyleGLIP utilizes the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize individual parts of the 3D mesh and modify their appearance to match the styles specified in the text prompt. 3DStyleGLIP effectively integrates part localization and stylization guidance within GLIP’s shared embedding space through an end-to-end process, enabled by part-level style loss and two complementary learning techniques. This neural methodology meets the user’s need for fine-grained style editing and delivers high-quality part-specific stylization results, opening new possibilities for customization and flexibility in 3D content creation. Our code and results are available at <a target="_blank" rel="noopener" href="https://github.com/sj978/3DStyleGLIP">https://github.com/sj978/3DStyleGLIP</a>. </p>
<blockquote>
<p>3D风格化是将特定风格应用于三维物体的应用，它通过创建适应不同场景的独特风格化的3D物体，拥有巨大的商业潜力。人工智能和文本驱动操作方法的最新进展使得风格化过程越来越直观和自动化。这些方法通过最小化对人工劳动和专业知识的依赖，降低了人力成本，但它们主要关注整体风格化，忽视了将所需风格应用于三维物体的单个组件。这种限制限制了精细粒度的可控性。为了解决这一差距，我们引入了3DStyleGLIP，这是一个专门为文本驱动、部分定制的3D风格化而设计的新型框架。给定一个3D网格和一个文本提示，3DStyleGLIP利用基于Grounded Language-Image Pre-training（GLIP）模型的视觉语言嵌入空间来定位3D网格的单个部分，并修改它们的外观以匹配文本提示中指定的风格。3DStyleGLIP通过端到端的过程有效地在GLIP的共享嵌入空间中集成了部分定位和风格化指导，这得益于部分级别的风格损失和两种互补的学习技术。这种神经方法满足了用户对精细粒度风格编辑的需求，并提供了高质量的部件特定风格化结果，为3D内容创建提供了定制和灵活性。我们的代码和结果可在<a target="_blank" rel="noopener" href="https://github.com/sj978/3DStyleGLIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sj978/3DStyleGLIP上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.02634v2">PDF</a> 12 pages, 8 figures, 2024 Pacific Graphics Conferences (PG 2024)</p>
<p><strong>Summary</strong></p>
<p>基于文本驱动的方法，我们提出了一个新颖的框架——3DStyleGLIP，专门用于处理部分定制的3D风格化问题。它利用GLIP模型的视觉语言嵌入空间，根据文本提示定位并修改3D网格的各部分风格。此方法实现了部分定位和风格化指导的集成，为细粒度风格编辑提供了高效解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本驱动的个性化部分定制在三维物体风格化中的潜力巨大。</li>
<li>当前技术主要关注整体风格化，忽视了特定组件的风格应用，存在精细控制上的限制。</li>
<li>提出的新型框架——3DStyleGLIP能够结合使用三维网格和文本提示，利用视觉语言嵌入空间定位并调整特定部分风格。</li>
<li>3DStyleGLIP通过集成部分定位和风格化指导，实现了在GLIP共享嵌入空间中的端到端流程。</li>
<li>部分级风格损失和两种互补学习技术增强了该神经方法的性能，满足了用户对细粒度风格编辑的需求。</li>
<li>3DStyleGLIP提供了高质量的部分特定风格化结果，为三维内容创建提供了定制和灵活性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3133d6bf6b203e85050414ddfd5ab99f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-33e4def37236c4fccbc9fb6c61070294.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ca72a2f55e6f7a8f97316d793c79a84c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1523e24eacbd43e57e7cb372597e00e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-729e26f6ded4fc5ba4cfc9bd31d04e15.jpg" align="middle">
</details>




<h2 id="Just-Shift-It-Test-Time-Prototype-Shifting-for-Zero-Shot-Generalization-with-Vision-Language-Models"><a href="#Just-Shift-It-Test-Time-Prototype-Shifting-for-Zero-Shot-Generalization-with-Vision-Language-Models" class="headerlink" title="Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization   with Vision-Language Models"></a>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization   with Vision-Language Models</h2><p><strong>Authors:Elaine Sui, Xiaohan Wang, Serena Yeung-Levy</strong></p>
<p>Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 image classification datasets involving natural distribution shifts and cross-dataset generalization, as well as in context-dependent visual reasoning, demonstrate TPS’s superior performance, achieving state-of-the-art results while reducing resource requirements. </p>
<blockquote>
<p>视觉语言模型（VLMs）的进展推动了计算机视觉领域的发展，特别是在零样本学习环境中。尽管这些模型具有潜力，但由于测试环境中的域偏移，它们的效力往往会降低。为了解决这个问题，我们引入了测试时间原型偏移（TPS）框架，这是一种创新性的方法，旨在利用未标记的测试输入使VLM适应测试数据集。我们的方法基于在共享嵌入空间中调制每类原型的概念。通过预计算和缓存使用预训练文本编码器生成的原型，TPS不仅便于后续预测的优化原型重用，还可以无缝集成到当前的提示工程进展中。在测试时，TPS仅根据给定的测试样本动态学习每个原型的偏移向量，有效地弥合了域差距并提高了分类精度。我们框架的一个显著特点是与常规文本提示调整方法相比，它大大减少了内存和计算需求。在涉及自然分布偏移、跨数据集泛化和上下文相关视觉推理的15个图像分类数据集上的广泛评估证明了TPS的卓越性能，它在降低资源要求的同时取得了最新结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12952v2">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视觉语言模型（VLMs）在测试环境中出现的领域偏移问题，提出的Test-Time Prototype Shifting（TPS）框架。该框架利用未标记的测试输入自适应调整VLMs，通过调制共享嵌入空间中的每类原型来实现。TPS不仅实现了优化前的原型重用，便于后续预测，还易于与当前的提示工程技术相结合。在测试时，TPS根据给定的测试样本动态学习每个原型的偏移向量，从而有效地缩小领域差距并提高分类精度。与常规的文本提示调整方法相比，TPS的内存和计算需求大大降低。在涉及自然分布偏移、跨数据集泛化和上下文相关视觉推理的15个图像分类数据集上的广泛评估显示，TPS表现优异，实现了业界领先的结果，并降低了资源要求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TPS框架旨在解决视觉语言模型（VLMs）在测试环境中出现的领域偏移问题。</li>
<li>TPS通过调制共享嵌入空间中的每类原型来实现自适应调整。</li>
<li>TPS实现了优化前的原型重用，便于后续预测，并与当前的提示工程技术相结合。</li>
<li>TPS在测试时根据给定的测试样本动态学习每个原型的偏移向量。</li>
<li>TPS有效缩小了领域差距，提高了分类精度。</li>
<li>与其他方法相比，TPS具有较低的内存和计算需求。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a47e8f55a73fd9a415be32e8b16e2e4d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d9f1396abd34c5c00c8fc795acb48d19.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2258dedc9776a7be355d1d33bffc4e7b.jpg" align="middle">
</details>




<h2 id="Efficient-Prompt-Tuning-of-Large-Vision-Language-Model-for-Fine-Grained-Ship-Classification"><a href="#Efficient-Prompt-Tuning-of-Large-Vision-Language-Model-for-Fine-Grained-Ship-Classification" class="headerlink" title="Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained   Ship Classification"></a>Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained   Ship Classification</h2><p><strong>Authors:Long Lan, Fengxiang Wang, Xiangtao Zheng, Zengmao Wang, Xinwang Liu</strong></p>
<p>Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model’s generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available. </p>
<blockquote>
<p>在遥感技术中对船只进行精细分类（RS-FGSC）是一项巨大的挑战，因为各类之间的相似性很高，且标注数据有限，这限制了传统监督分类方法的有效性。最近的大型预训练视觉语言模型（VLMs）在少量或零样本学习方面表现出了令人印象深刻的能力，特别是在理解图像内容方面。本研究旨在探索利用VLMs的潜力来提高对未见船只类别的分类精度，这在由于成本或隐私约束而导致数据受限的场景中具有重大意义。直接对RS-FGSC进行微调常常会面临过度拟合已见类别的问题，导致对未见类别的泛化能力不佳，这突显了在区分复杂背景和捕捉船只独特特征方面的困难。为了解决这个问题，我们引入了一种新颖的提示微调技术，该技术采用分层多粒度提示设计。我们的方法通过偏置项融入遥感船只先验知识，这些偏置项来自一个小型可训练网络。此策略提高了模型的泛化能力，同时提高了其区分复杂背景和学习能力辨别船只特征的能力。此外，我们为领域做出了贡献，引入了综合数据集FGSCM-52，大幅扩展了现有数据集，包含了更多广泛的数据和罕见的船只类别的详细注释。大量的实验评估证明，我们提出的方法优于当前最先进的技术。源代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08271v2">PDF</a> It has been accepted by TGRS</p>
<p><strong>Summary</strong></p>
<p>基于遥感影像的精细船舶分类（RS-FGSC）面临巨大挑战，包括类别高度相似性和标注数据有限的问题，这限制了传统监督分类方法的有效性。最近的大型预训练视觉语言模型（VLMs）在少样本或零样本学习上表现出强大的能力，特别是在理解图像内容上。本研究探讨了如何利用VLMs的潜力来提高未见船舶类别的分类精度，这在成本或隐私约束导致数据受限的场景中具有重大意义。直接使用微调VLMs进行RS-FGSC常常会遇到过度拟合已见类别的问题，导致对未见类别的泛化能力不佳，这凸显了区分复杂背景和捕捉船舶特征困难的挑战。为解决这些问题，我们引入了一种新的提示调整技术，该技术采用分层多粒度提示设计。我们的方法通过从可训练的小网络中学习遥感船舶先验，将先验知识融入偏差项中。此策略提高了模型的泛化能力，同时提高了其区分复杂背景和学习船舶特征的能力。此外，我们还为领域做出了贡献，推出了综合数据集FGSCM-52，该数据集在现有数据集的基础上进行了大幅扩展，包含了更广泛的数据和详细的罕见船舶类别注释。实验评估表明，我们提出的方法优于当前最先进的技术。源代码将公开发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RS-FGSC面临高类别相似性和标注数据有限性的挑战。</li>
<li>大型预训练视觉语言模型（VLMs）在少样本或零样本学习上表现出强大的能力。</li>
<li>直接微调VLMs会导致过度拟合已见类别，影响对未见类别的泛化能力。</li>
<li>提出了一种新的提示调整技术，采用分层多粒度提示设计来解决上述问题。</li>
<li>通过学习遥感船舶先验，提高了模型的泛化能力和区分复杂背景及船舶特征的能力。</li>
<li>推出了综合数据集FGSCM-52，包含更广泛的数据和详细的罕见船舶类别注释。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26c768e0586a22813e2adc7d42a80eae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d369a45a88991a559d65f564ff6ac511.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8d75df78e4f9149a31145f46ab6c5bb7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-feef1fc452d9919594ee10d0a70945c1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-55031060c37f2f6edfcf6bf517e5a7b3.jpg" align="middle">
</details>




<h2 id="Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning"><a href="#Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning"></a>Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Wangmeng Zuo, Chunmei Feng</strong></p>
<p>Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>. </p>
<blockquote>
<p>少量样本类增量学习（FSCIL）旨在基于非常有限的训练数据持续学习新类别，同时不忘掉已遇到的旧类别。现有研究仅依赖于纯视觉网络，而本文我们通过利用视觉语言模型（例如CLIP）来解决FSCIL问题，并提出一个简单有效的框架，名为基于分布特征回放的学习提示（LP-DiF）。我们发现仅使用CLIP进行零样本评估即可大大超越最具影响力的方法。接着，我们引入了提示调整技术，以进一步提高其适应能力，使模型能够持续从每个会话中捕获特定知识。为了防止学习提示忘记新会话中的旧知识，我们提出了一种伪特征回放方法。具体来说，我们通过维持一个由训练图像特征和变自动编码器生成的合成特征估计得到的对角协方差矩阵形式的特征级高斯分布来保留每个旧类的旧知识。当进入一个新会话时，伪特征是从旧类分布中采样并结合当前会话的训练图像来优化提示，从而能够使模型在学习新知识的同时保留旧知识。在CIFAR100、mini-ImageNet、CUB-200三个流行基准以及本文提出的更具挑战性的SUN-397和CUB-200$*$上的实验展示了LP-DiF的优越性，在FSCIL领域达到了新的最新水平。代码已公开在<a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01598v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文解决了少样本类增量学习（FSCIL）问题，利用跨视觉语言模型（如CLIP）构建了一个名为LP-DiF的简单有效框架。通过引入提示调优技术，该模型能更好地适应不同类知识的学习。为防止在新课程中遗忘旧知识，提出基于分布的特征回放方法，利用特征级别的高斯分布估计训练图像的特征以及变自编码器生成的合成特征来保留旧知识的记忆。实验结果表明，LP-DiF框架在多个基准测试上表现优异，实现了少样本类增量学习的最新成果。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LP-DiF框架利用跨视觉语言模型（如CLIP）解决少样本类增量学习问题。</li>
<li>提示调优技术被引入以提高模型的适应能力。</li>
<li>LP-DiF框架使用基于分布的特征回放方法以防止模型在学习新课程时遗忘旧知识。</li>
<li>特征级别的高斯分布用于估计旧知识的记忆，包括训练图像特征和合成特征。</li>
<li>LP-DiF框架在多个基准测试上的表现优于其他方法，达到少样本类增量学习的最新成果。</li>
<li>代码已公开供公众使用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b68ef099e7713b014eeb2b3e7b49e8c2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a76b9eff0fad3d63dcb46aee459a0673.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0a24a512d0bc42141ba455a1b4b860e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f07311872d24975899a6d4fd3d1d5e60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-74c2a8fa494ec56d4ee19c96dabbeb8f.jpg" align="middle">
</details>




<h2 id="M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models"><a href="#M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models" class="headerlink" title="M$^{2}$UGen: Multi-modal Music Understanding and Generation with the   Power of Large Language Models"></a>M$^{2}$UGen: Multi-modal Music Understanding and Generation with the   Power of Large Language Models</h2><p><strong>Authors:Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan</strong></p>
<p>The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as text, speech, images, videos, etc. They also utilize LLMs to understand human intention and generate desired outputs like images, videos, and music. However, research that combines both understanding and generation using LLMs is still limited and in its nascent stage. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M$^{2}$UGen) framework that integrates LLM’s abilities to comprehend and generate music for different modalities. The M$^{2}$UGen framework is purpose-built to unlock creative potential from diverse sources of inspiration, encompassing music, image, and video through the use of pretrained MERT, ViT, and ViViT models, respectively. To enable music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging multi-modal understanding and music generation is accomplished through the integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA model to generate extensive datasets that support text&#x2F;image&#x2F;video-to-music generation, facilitating the training of our M$^{2}$UGen framework. We conduct a thorough evaluation of our proposed framework. The experimental results demonstrate that our model achieves or surpasses the performance of the current state-of-the-art models. </p>
<blockquote>
<p>当前利用大型语言模型（LLM）的研究领域正经历一次热潮。许多研究利用这些模型的强大推理能力来理解各种模态，如文本、语音、图像、视频等。他们还利用LLM来理解人类意图并生成图像、视频和音乐等所需输出。然而，将理解和生成结合使用LLM的研究仍然有限，处于初级阶段。为了弥补这一差距，我们引入了一个多模态音乐理解和生成（M$^{2}$UGen）框架，该框架集成了LLM理解和生成不同模态音乐的能力。M$^{2}$UGen框架专为解锁来自不同灵感来源的创造力而构建，通过使用预训练的MERT、ViT和ViViT模型，分别涵盖音乐、图像和视频。为了实现音乐生成，我们探索了AudioLDM 2和MusicGen的使用。通过LLaMA 2模型的集成，实现了多模态理解和音乐生成的桥梁。此外，我们还利用MU-LLaMA模型生成了大量支持文本&#x2F;图像&#x2F;视频到音乐生成的数据集，为我们的M$^{2}$UGen框架提供训练支持。我们对提出的框架进行了全面评估。实验结果表明，我们的模型达到了或超越了当前最先进模型的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11255v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个多模态音乐理解和生成（M$^{2}$UGen）框架，该框架结合了大型语言模型（LLMs）的理解和生成能力，用于处理音乐、图像和视频等多模态数据。通过使用预训练的MERT、ViT和ViViT模型，结合AudioLDM 2和MusicGen进行音乐生成，并通过LLaMA 2模型实现多模态理解和音乐生成的桥梁。同时，利用MU-LLaMA模型生成大量数据集，支持文本&#x2F;图像&#x2F;视频到音乐的生成，促进了M$^{2}$UGen框架的训练。实验结果表明，该框架的性能达到了或超越了当前先进模型的水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）正在被广泛应用于多模态理解和生成任务中，包括文本、语音、图像、视频等。</li>
<li>M$^{2}$UGen框架结合了LLMs的理解和生成能力，针对多模态数据（如音乐、图像和视频）进行专门设计。</li>
<li>M$^{2}$UGen框架使用预训练的MERT、ViT和ViViT模型来处理图像和视频数据，使用AudioLDM 2和MusicGen进行音乐生成。</li>
<li>LLaMA 2模型被用于连接多模态理解和音乐生成任务。</li>
<li>MU-LLaMA模型被用来生成大量数据集，以支持文本&#x2F;图像&#x2F;视频到音乐的生成任务，从而促进了M$^{2}$UGen框架的训练。</li>
<li>提出的M$^{2}$UGen框架经过全面评估，实验结果表明其性能达到了或超越了当前先进模型的水平。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e2ec9dcf3634ad1383813fb8f9777507.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-146269967bb50b37095b2802028e8b5d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd273c2f6fa5c144401c057a5c938155.png" align="middle">
</details>




<h2 id="Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models"><a href="#Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models" class="headerlink" title="Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models"></a>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models</h2><p><strong>Authors:Zhaozheng Chen, Qianru Sun</strong></p>
<p>The rapid development of deep learning has driven significant progress in image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this journal, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area. </p>
<blockquote>
<p>深度学习的快速发展推动了图像语义分割领域的显著进步，这是计算机视觉中的一项基础任务。语义分割算法通常依赖于像素级标签（即对象掩膜）的可用性，而这些标签的获取成本高昂、耗时且劳力密集。弱监督语义分割（WSSS）是避免这种标注的有效解决方案。它仅利用部分或不完整的注释，为全监督语义分割提供了经济实惠的替代方案。在本期刊中，我们的重点是在图像级标签的WSSS，这是WSSS中最具挑战性的形式。我们的工作分为两部分。首先，我们对传统方法进行了全面调查，主要关注在主要研究会议上提出的方法。我们根据它们的方法操作位置将它们分为四类：像素级、图像级、跨图像和外部数据。其次，我们研究了视觉基础模型，如分段任何模型（SAM）在WSSS中的应用。我们在两个有趣的场景中仔细审查了SAM：文本提示和零样本学习。我们深入探讨了将视觉基础模型用于WSSS的潜力和挑战，有助于推动这一激动人心的研究领域的未来发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13026v2">PDF</a> Accepted to ACM Computing Surveys</p>
<p><strong>Summary</strong><br>深度学习的快速发展推动了图像语义分割领域的显著进步，这是计算机视觉中的一项基础任务。语义分割算法通常依赖于像素级标签（即对象掩膜）的可用性，这些标签的获取成本高昂、耗时且劳力密集。弱监督语义分割（WSSS）是一种避免此类标签的有效解决方案，它仅利用部分或不完整的注释，为全监督语义分割提供了具有成本效益的替代方案。本文主要关注利用图像级标签的WSSS，这是WSSS中最具挑战性的形式。文章分为两部分：首先，我们对传统方法进行了全面调查，主要关注在主要研究会议上发表的方法。我们根据它们的方法操作地将它们分为四组：像素级、图像级、跨图像和外部数据。其次，我们探讨了视觉基础模型（如Segment Anything Model，SAM）在WSSS中的应用。我们对SAM在两个有趣场景中的适用性进行了审视：文本提示和零样本学习。我们深入了解了将视觉基础模型用于WSSS的潜力和挑战，为这一激动人心的研究领域未来的发展提供了便利。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习的进步显著推动了图像语义分割领域的发展。</li>
<li>语义分割算法依赖于像素级标签，但这些标签的获取成本高昂且耗时。</li>
<li>弱监督语义分割（WSSS）是一种利用部分或不完整注释的有效方法，作为全监督语义分割的替代方案。</li>
<li>利用图像级标签的WSSS是最具挑战性的形式。</li>
<li>文章调查了传统方法，并根据其操作方式进行了分类。</li>
<li>文章探讨了视觉基础模型（如Segment Anything Model）在WSSS中的应用，特别是在文本提示和零样本学习场景下的适用性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5de8bbf35b6e19bb15d97b70d1365142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6f366106e2aa1a029e0793160273fb8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c826195ccfa445054e06431b7d6c4bdc.jpg" align="middle">
</details>




<h2 id="NoisyNN-Exploring-the-Impact-of-Information-Entropy-Change-in-Learning-Systems"><a href="#NoisyNN-Exploring-the-Impact-of-Information-Entropy-Change-in-Learning-Systems" class="headerlink" title="NoisyNN: Exploring the Impact of Information Entropy Change in Learning   Systems"></a>NoisyNN: Exploring the Impact of Information Entropy Change in Learning   Systems</h2><p><strong>Authors:Xiaowei Yu, Zhe Huang, Minheng Chen, Yao Xue, Tianming Liu, Dajiang Zhu</strong></p>
<p>We investigate the impact of entropy change in deep learning systems by noise injection at different levels, including the embedding space and the image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep models under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the task complexity. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of 95$%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change. </p>
<blockquote>
<p>我们通过不同层次的噪声注入，包括嵌入空间和图像，研究了熵变化对深度学习系统的影响。采用我们方法的系列模型统称为“噪声神经网络（Noisy Neural Networks，NoisyNN）”，例如NoisyViT和NoisyCNN等。噪声在传统上被视为各种深度学习架构（如卷积神经网络（CNNs）和视觉转换器（ViTs））以及不同学习任务（如图像分类和迁移学习）中的有害扰动。然而，这项工作表明，噪声可以是改变学习系统熵的有效途径。我们证明，在特定条件下，特定噪声可以提高各种深度模型的性能。我们从理论上证明了正面噪声通过减少信息熵定义的任务复杂性所带来的提升，并通过实验展示了在大型图像数据集（如ImageNet）中的显著性能提升。在这里，我们使用信息熵来定义任务的复杂性。我们将噪声分为正面噪声（PN）和有害噪声（HN）两种类型，这取决于噪声是否有助于降低任务复杂性。对CNN和ViT的广泛实验表明，通过主动注入正面噪声可以实现性能改进，我们在ImageNet上实现了前所未有的95%的Top-1准确率。理论分析和实证证据都证实，正面噪声的存在有益于学习过程，而传统上认为的有害噪声确实会损害深度学习模型。噪声的不同作用为特定任务的深度模型提供了新的解释，为提高模型性能提供了新的范式。此外，它提醒我们，我们可以通过影响信息熵来改变学习系统的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10625v4">PDF</a> Task Entropy, NoisyViT, NoisyCNN</p>
<p><strong>Summary</strong></p>
<p>该文本研究了深度学习系统中熵变化的影响，通过在不同层级注入噪声，如嵌入空间和图像。文章探讨了噪声对深度学习模型的影响，并提出将此类模型称为“噪声神经网络”（NoisyNN），例如NoisyViT和NoisyCNN。尽管噪声在传统上被视为对深度学习架构（如CNN和ViT）以及图像分类和迁移学习等学习任务的有害扰动，但本文展示了噪声可以作为改变学习系统熵的有效方法。文章证明特定噪声在某些条件下可以促进各种深度模型的性能，并使用信息熵来定义任务复杂性。文章将噪声分为正向噪声和有害噪声两类，基于噪声是否能有助于降低任务复杂性。在ImageNet等大型图像数据集上进行的实验表明，通过主动注入正向噪声可以实现性能改进，并在ImageNet上达到了前所未有的95%的Top-1准确率。理论分析和实证证据表明，正向噪声的存在有益于学习过程，而传统认为的有害噪声确实会损害深度学习模型。不同的噪声角色为特定任务上的深度学习模型提供了新的解释和改进模型性能的范例。同时提醒我们，可以通过改变信息熵来影响学习系统的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了通过噪声注入改变深度学习系统中的熵变化。</li>
<li>介绍了正向噪声和有害噪声的概念，前者能降低任务复杂性并提升模型性能。</li>
<li>通过实验证明在大型图像数据集上，如ImageNet，正向噪声能显著提高模型性能，达到前所未有的准确率。</li>
<li>理论分析和实证证据表明正向噪声有益于学习过程，而有害噪声会损害模型性能。</li>
<li>不同的噪声角色为特定任务上的深度学习模型提供了新的解释和改进范例。</li>
<li>强调了信息熵在影响学习系统性能中的重要作用。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c9d81b126fb99dbe4d9bbb0b950177d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-45374987c49986a53eeed8988fe0bda8.jpg" align="middle">
</details>




<h2 id="Retrieval-Enhanced-Visual-Prompt-Learning-for-Few-shot-Classification"><a href="#Retrieval-Enhanced-Visual-Prompt-Learning-for-Few-shot-Classification" class="headerlink" title="Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"></a>Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</h2><p><strong>Authors:Jintao Rong, Hao Chen, Linlin Ou, Tianxiao Chen, Xinyi Yu, Yifan Liu</strong></p>
<p>The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. Reprompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）模型在各种下游视觉任务中得到了广泛应用。少样本学习范式已被广泛采用以增强其对这些任务的能力。然而，由于域差距的扩大，当前范式可能在细粒度分类（如卫星图像识别）方面遇到困难。为了解决这一局限性，我们提出了检索增强视觉提示学习（RePrompt），它引入了检索机制来缓存和重复使用下游任务的知识。RePrompt从训练样本或可用的外部数据中构建检索数据库，并使用检索机制增强简单提示学习基线（baseline）的多个阶段，从而缩小域差距。在推理过程中，我们增强的模型可以引用检索带来的相似样本以做出更准确的预测。详细分析表明，检索有助于改善后期特征的分布，从而提高下游任务的泛化能力。RePrompt在广泛的视觉数据集上达到了最先进的性能，包括11个图像数据集、3个视频数据集、1个多视图数据集和4个域泛化基准测试集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.02243v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIP模型广泛应用于多种下游视觉任务，通过少样本学习模式提升了模型容量。但在精细分类任务中如卫星图像识别存在局限，为应对挑战提出了基于检索增强的视觉提示学习（RePrompt）。RePrompt通过构建检索数据库缓存并复用下游任务知识，增强简单提示学习基线模型的多个阶段，缩小领域差距。推理阶段模型能引用检索引入的相似样本作出更精确预测。详细分析表明检索改善特征分布的尾部阶段进而提升下游任务的泛化性能。RePrompt在多个视觉数据集上取得领先水平，包括图像数据集、视频数据集和多视图数据集等。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIP模型广泛应用于下游视觉任务，但面临精细分类任务的挑战。</li>
<li>RePrompt模型引入检索机制缓存并复用下游任务知识以缩小领域差距。</li>
<li>RePrompt构建检索数据库增强提示学习基线模型的多个阶段。</li>
<li>检索机制有助于改善特征分布的尾部阶段，提升模型泛化性能。</li>
<li>RePrompt在多种视觉数据集上取得领先水平，包括图像、视频和多视图数据集等。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-025d9590e06a0b2170c821bf8e254402.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-50e6c61180a9242d3442e2b49b197a9e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4251402a4494d924dc0ed0d4211dffd7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-300e5ceb1c743c415d6bd844f9567a29.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_检测_分割_跟踪/2411.17425v1/page_5_0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2024-12-12  Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6b1c31a0d8f9f8fc8e1eacd6162a387c.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2024-12-12  Exploring What Why and How A Multifaceted Benchmark for Causation   Understanding of Video Anomaly
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
