<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  EOV-Seg Efficient Open-Vocabulary Panoptic Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Vision Transformer/2403.08271v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    31.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    129 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation"><a href="#EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation" class="headerlink" title="EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation"></a>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</h2><p><strong>Authors:Hongwei Niu, Jie Hu, Jianghang Lin, Shengchuan Zhang</strong></p>
<p>Open-vocabulary panoptic segmentation aims to segment and classify everything in diverse scenes across an unbounded vocabulary. Existing methods typically employ two-stage or single-stage framework. The two-stage framework involves cropping the image multiple times using masks generated by a mask generator, followed by feature extraction, while the single-stage framework relies on a heavyweight mask decoder to make up for the lack of spatial position information through self-attention and cross-attention in multiple stacked Transformer blocks. Both methods incur substantial computational overhead, thereby hindering the efficiency of model inference. To fill the gap in efficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and spatial-aware framework designed for open-vocabulary panoptic segmentation. Specifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware Selection (VAS) module is proposed to improve the semantic comprehension of visual aggregated features and alleviate the feature interaction burden on the mask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE), which efficiently utilizes the spatial awareness capabilities of ViT-based CLIP backbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary panoptic segmentation framework towards efficiency, which runs faster and achieves competitive performance compared with state-of-the-art methods. Specifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and 12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and the inference time of EOV-Seg is 4-21 times faster than state-of-the-art methods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with only 71M parameters on a single RTX 3090 GPU. Code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%7D">https://github.com/nhw649/EOV-Seg}</a>. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ—¨åœ¨åˆ†å‰²å¹¶åˆ†ç±»è·¨æ— é™è¯æ±‡è¡¨çš„å¤šæ ·åœºæ™¯ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæˆ–å•é˜¶æ®µæ¡†æ¶ã€‚ä¸¤é˜¶æ®µæ¡†æ¶æ¶‰åŠä½¿ç”¨æ©è†œç”Ÿæˆå™¨ç”Ÿæˆçš„æ©è†œå¤šæ¬¡è£å‰ªå›¾åƒï¼Œç„¶åè¿›è¡Œç‰¹å¾æå–ï¼Œè€Œå•é˜¶æ®µæ¡†æ¶ä¾èµ–äºé‡é‡çº§çš„æ©è†œè§£ç å™¨ï¼Œé€šè¿‡å †å çš„Transformerå—ä¸­çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æ¥å¼¥è¡¥ç©ºé—´ä½ç½®ä¿¡æ¯çš„ç¼ºå¤±ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œé˜»ç¢æ¨¡å‹æ¨ç†çš„æ•ˆç‡ã€‚ä¸ºäº†å¼¥è¡¥æ•ˆç‡ä¸Šçš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†EOV-Segï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²è®¾è®¡çš„æ–°å‹å•é˜¶æ®µã€å…±äº«ã€é«˜æ•ˆå’Œå…·æœ‰ç©ºé—´æ„ŸçŸ¥çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒEOV-Segåœ¨ä¸¤ä¸ªæ–¹é¢è¿›è¡Œäº†åˆ›æ–°ã€‚é¦–å…ˆï¼Œæå‡ºäº†è¯æ±‡æ„ŸçŸ¥é€‰æ‹©ï¼ˆVASï¼‰æ¨¡å—ï¼Œä»¥æé«˜è§†è§‰èšåˆç‰¹å¾è¯­ä¹‰ç†è§£èƒ½åŠ›å¹¶å‡è½»æ©è†œè§£ç å™¨çš„ç‰¹å¾äº¤äº’è´Ÿæ‹…ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‘åŠ¨æ€åµŒå…¥ä¸“å®¶ï¼ˆTDEEï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†åŸºäºViTçš„CLIPä¸»å¹²çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒEOV-Segæ˜¯é¦–ä¸ªé¢å‘æ•ˆç‡çš„å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ¡†æ¶ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒè¿è¡Œæ›´å¿«å¹¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œä»…åœ¨COCOè®­ç»ƒé›†ä¸Šï¼ŒEOV-Segåœ¨ADE20Kæ•°æ®é›†ä¸Šçš„å…¨æ™¯å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†24.2çš„PQï¼ˆå…¨æ™¯è´¨é‡ï¼‰ã€31.6çš„mIoUï¼ˆå¹³å‡äº¤å¹¶æ¯”ï¼‰ï¼Œå¹¶ä¸”æ¯ç§’å¤„ç†å¸§æ•°ä¸º12.7ã€‚ç‰¹åˆ«æ˜¯ï¼Œé…å¤‡ResNet-50ä¸»å¹²çš„EOV-Segåœ¨å•ä¸ªRTX 3090 GPUä¸Šä»…ä½¿ç”¨71Må‚æ•°å³å¯è¾¾åˆ°æ¯ç§’25å¸§çš„å¤„ç†é€Ÿåº¦ã€‚ä»£ç å¯é€šè¿‡\url{<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%7D%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/nhw649/EOV-Seg}è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08628v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²çš„é«˜æ•ˆã€å•ä¸€é˜¶æ®µã€å…±äº«ã€ç©ºé—´æ„ŸçŸ¥æ¡†æ¶EOV-Segã€‚EOV-Segé€šè¿‡å¼•å…¥è¯æ±‡æ„ŸçŸ¥é€‰æ‹©æ¨¡å—å’ŒåŒå‘åŠ¨æ€åµŒå…¥ä¸“å®¶æŠ€æœ¯ï¼Œæé«˜äº†è¯­ä¹‰ç†è§£å’Œç‰¹å¾äº¤äº’æ•ˆç‡ï¼Œå¹¶åˆ©ç”¨åŸºäºViTçš„CLIPä¸»å¹²çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒEOV-Segè¿è¡Œæ›´å¿«ï¼Œæ€§èƒ½æ›´å…·ç«äº‰åŠ›ã€‚åœ¨ä»…æœ‰COCOè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒEOV-Segåœ¨ADE20Kæ•°æ®é›†ä¸Šçš„å…¨æ™¯å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†24.2 PQã€31.6 mIoUï¼Œå¹¶ä¸”æ¨ç†æ—¶é—´æ˜¯ç°æœ‰æ–¹æ³•çš„4-21å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EOV-Segæ˜¯ä¸€ä¸ªé’ˆå¯¹å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²çš„é«˜æ•ˆæ¡†æ¶ã€‚</li>
<li>EOV-Segé‡‡ç”¨å•ä¸€é˜¶æ®µè®¾è®¡ï¼Œæé«˜äº†æ¨¡å‹æ¨ç†çš„æ•ˆç‡ã€‚</li>
<li>EOV-Segé€šè¿‡å¼•å…¥è¯æ±‡æ„ŸçŸ¥é€‰æ‹©æ¨¡å—ï¼Œæ”¹å–„äº†è¯­ä¹‰ç†è§£ã€‚</li>
<li>åŒå‘åŠ¨æ€åµŒå…¥ä¸“å®¶æŠ€æœ¯è¢«ç”¨äºæé«˜ç‰¹å¾äº¤äº’æ•ˆç‡ã€‚</li>
<li>EOV-Segåˆ©ç”¨åŸºäºViTçš„CLIPä¸»å¹²çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>åœ¨ä»…æœ‰COCOè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒEOV-Segåœ¨ADE20Kæ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
<li>EOV-Segçš„æ¨ç†æ—¶é—´æ˜¯ç°æœ‰æ–¹æ³•çš„4-21å€ï¼Œå¹¶ä¸”åœ¨é…å¤‡ResNet-50ä¸»å¹²çš„æ¡ä»¶ä¸‹ï¼Œå…¶åœ¨å•ä¸ªRTX 3090 GPUä¸Šçš„è¿è¡Œé€Ÿåº¦ä¸º25 FPSï¼Œå‚æ•°ä»…ä¸º71Mã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1a4c8da09353ee9d88ed21e70da5e270.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c2d13e725ff84932cdcc5baa8246aae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fe03575aa93513e66c4d741401bd1e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0a26926b98fb664f5c2de0901a0a265c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e55df3ae6bba2f5adc54dab7fecb8925.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4abdef90e4c964b25d477441019e3fb2.jpg" align="middle">
</details>




<h2 id="SAM-Mamba-Mamba-Guided-SAM-Architecture-for-Generalized-Zero-Shot-Polyp-Segmentation"><a href="#SAM-Mamba-Mamba-Guided-SAM-Architecture-for-Generalized-Zero-Shot-Polyp-Segmentation" class="headerlink" title="SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp   Segmentation"></a>SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp   Segmentation</h2><p><strong>Authors:Tapas Kumar Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha</strong></p>
<p>Polyp segmentation in colonoscopy is crucial for detecting colorectal cancer. However, it is challenging due to variations in the structure, color, and size of polyps, as well as the lack of clear boundaries with surrounding tissues. Traditional segmentation models based on Convolutional Neural Networks (CNNs) struggle to capture detailed patterns and global context, limiting their performance. Vision Transformer (ViT)-based models address some of these issues but have difficulties in capturing local context and lack strong zero-shot generalization. To this end, we propose the Mamba-guided Segment Anything Model (SAM-Mamba) for efficient polyp segmentation. Our approach introduces a Mamba-Prior module in the encoder to bridge the gap between the general pre-trained representation of SAM and polyp-relevant trivial clues. It injects salient cues of polyp images into the SAM image encoder as a domain prior while capturing global dependencies at various scales, leading to more accurate segmentation results. Extensive experiments on five benchmark datasets show that SAM-Mamba outperforms traditional CNN, ViT, and Adapter-based models in both quantitative and qualitative measures. Additionally, SAM-Mamba demonstrates excellent adaptability to unseen datasets, making it highly suitable for real-time clinical use. </p>
<blockquote>
<p>ç»“è‚ é•œæ£€æŸ¥ä¸­çš„æ¯è‚‰åˆ†å‰²å¯¹äºæ£€æµ‹ç»“ç›´è‚ ç™Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ¯è‚‰çš„ç»“æ„ã€é¢œè‰²å’Œå¤§å°å·®å¼‚ä»¥åŠä¸å‘¨å›´ç»„ç»‡çš„è¾¹ç•Œä¸æ¸…ï¼Œè¿™ä½¿å¾—å…¶æˆä¸ºä¸€é¡¹æŒ‘æˆ˜ã€‚åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹éš¾ä»¥æ•æ‰è¯¦ç»†çš„æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œä»è€Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¨¡å‹è§£å†³äº†å…¶ä¸­çš„ä¸€äº›é—®é¢˜ï¼Œä½†åœ¨æ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡å’Œç¼ºä¹å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Mambaå¼•å¯¼çš„åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAM-Mambaï¼‰ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¯è‚‰åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨ç¼–ç å™¨ä¸­åŠ å…¥Mamba-Prioræ¨¡å—ï¼Œä»¥å¼¥åˆSAMçš„ä¸€èˆ¬é¢„è®­ç»ƒè¡¨ç¤ºå’Œæ¯è‚‰ç›¸å…³çš„çç¢çº¿ç´¢ä¹‹é—´çš„é¸¿æ²Ÿã€‚å®ƒå°†æ¯è‚‰å›¾åƒçš„å…³é”®çº¿ç´¢æ³¨å…¥SAMå›¾åƒç¼–ç å™¨ä½œä¸ºé¢†åŸŸå…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶åœ¨ä¸åŒå°ºåº¦ä¸Šæ•è·å…¨å±€ä¾èµ–æ€§ï¼Œä»è€Œå¾—åˆ°æ›´ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSAM-Mambaåœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„CNNã€ViTå’ŒåŸºäºé€‚é…å™¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSAM-Mambaåœ¨æœªæ ‡è®°çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„é€‚åº”æ€§ï¼Œä½¿å…¶æˆä¸ºå®æ—¶ä¸´åºŠåº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08482v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“è‚ é•œæ£€æŸ¥ä¸­æ¯è‚‰åˆ†å‰²å¯¹æ£€æµ‹ç»“ç›´è‚ ç™Œçš„é‡è¦æ€§ã€‚ç”±äºæ¯è‚‰ç»“æ„ã€é¢œè‰²å’Œå°ºå¯¸çš„å·®å¼‚ä»¥åŠä¸å‘¨å›´ç»„ç»‡çš„è¾¹ç•Œä¸æ¸…ï¼Œå¯¼è‡´åˆ†å‰²å­˜åœ¨æŒ‘æˆ˜ã€‚åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹éš¾ä»¥æ•æ‰è¯¦ç»†æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ€§èƒ½å—é™ã€‚è€ŒåŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¨¡å‹è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½†åœ¨æ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡å’Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Mambaå¼•å¯¼çš„åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAM-Mambaï¼‰è¿›è¡Œé«˜æ•ˆçš„æ¯è‚‰åˆ†å‰²ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†Mambaå…ˆéªŒæ¨¡å—ï¼Œä»¥å¼¥åˆSAMçš„ä¸€èˆ¬é¢„è®­ç»ƒè¡¨ç¤ºå’Œæ¯è‚‰ç›¸å…³çº¿ç´¢ä¹‹é—´çš„å·®è·ã€‚å®ƒå°†æ¯è‚‰å›¾åƒçš„å…³é”®çº¿ç´¢æ³¨å…¥SAMå›¾åƒç¼–ç å™¨ä½œä¸ºé¢†åŸŸå…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶æ•æ‰ä¸åŒå°ºåº¦çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œä»¥è·å¾—æ›´ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-Mambaåœ¨å®šé‡å’Œå®šæ€§åº¦é‡æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿçš„CNNã€ViTå’Œé€‚é…å™¨æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSAM-Mambaåœ¨æœªæ ‡è®°çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„é€‚åº”æ€§ï¼Œéå¸¸é€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“è‚ é•œæ£€æŸ¥ä¸­çš„æ¯è‚‰åˆ†å‰²å¯¹æ£€æµ‹ç»“ç›´è‚ ç™Œè‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»ŸåŸºäºCNNçš„åˆ†å‰²æ¨¡å‹åœ¨æ•æ‰è¯¦ç»†æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹è§£å†³äº†CNNæ¨¡å‹çš„ä¸€äº›é—®é¢˜ï¼Œä½†åœ¨å±€éƒ¨ä¸Šä¸‹æ–‡å’Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„SAM-Mambaæ¨¡å‹é€šè¿‡å¼•å…¥Mambaå…ˆéªŒæ¨¡å—ï¼Œæé«˜äº†æ¯è‚‰åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>SAM-Mambaæ¨¡å‹ç»“åˆäº†ä¸€èˆ¬é¢„è®­ç»ƒè¡¨ç¤ºå’Œæ¯è‚‰ç›¸å…³çº¿ç´¢ï¼Œé€šè¿‡æ•æ‰å…¨å±€ä¾èµ–å…³ç³»è·å¾—æ›´ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒSAM-Mambaçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-541edeac0d0fb433f0faca63f08a05e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b9bcc80c550949c0421566ca502aa5c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c1c472678ee9be4b29eb58d0ad79eb99.jpg" align="middle">
</details>




<h2 id="TextRefiner-Internal-Visual-Feature-as-Efficient-Refiner-for-Vision-Language-Models-Prompt-Tuning"><a href="#TextRefiner-Internal-Visual-Feature-as-Efficient-Refiner-for-Vision-Language-Models-Prompt-Tuning" class="headerlink" title="TextRefiner: Internal Visual Feature as Efficient Refiner for   Vision-Language Models Prompt Tuning"></a>TextRefiner: Internal Visual Feature as Efficient Refiner for   Vision-Language Models Prompt Tuning</h2><p><strong>Authors:Jingjing Xie, Yuxin Zhang, Jun Peng, Zhaohong Huang, Liujuan Cao</strong></p>
<p>Despite the efficiency of prompt learning in transferring vision-language models (VLMs) to downstream tasks, existing methods mainly learn the prompts in a coarse-grained manner where the learned prompt vectors are shared across all categories. Consequently, the tailored prompts often fail to discern class-specific visual concepts, thereby hindering the transferred performance for classes that share similar or complex visual attributes. Recent advances mitigate this challenge by leveraging external knowledge from Large Language Models (LLMs) to furnish class descriptions, yet incurring notable inference costs. In this paper, we introduce TextRefiner, a plug-and-play method to refine the text prompts of existing methods by leveraging the internal knowledge of VLMs. Particularly, TextRefiner builds a novel local cache module to encapsulate fine-grained visual concepts derivedfrom local tokens within the image branch. By aggregating and aligning the cached visual descriptions with the original output of the text branch, TextRefiner can efficiently refine and enrich the learned prompts from existing methods without relying on any external expertise. For example, it improves the performance of CoOp from 71.66 % to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise features for text prompts. Equipped with TextRefiner, PromptKD achieves state-of-the-art performance and is efficient in inference. Our code is relesed at <a target="_blank" rel="noopener" href="https://github.com/xjjxmu/TextRefiner">https://github.com/xjjxmu/TextRefiner</a> </p>
<blockquote>
<p>å°½ç®¡æç¤ºå­¦ä¹ åœ¨å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶å…·æœ‰å¾ˆé«˜çš„æ•ˆç‡ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨äº†ç²—ç²’åº¦çš„æ–¹å¼å­¦ä¹ æç¤ºï¼Œå…¶ä¸­å­¦ä¹ åˆ°çš„æç¤ºå‘é‡æ˜¯è·¨æ‰€æœ‰ç±»åˆ«å…±äº«çš„ã€‚å› æ­¤ï¼Œå®šåˆ¶çš„æç¤ºå¾€å¾€æ— æ³•è¯†åˆ«ç‰¹å®šç±»åˆ«çš„è§†è§‰æ¦‚å¿µï¼Œä»è€Œé˜»ç¢äº†å…·æœ‰ç›¸ä¼¼æˆ–å¤æ‚è§†è§‰å±æ€§çš„ç±»åˆ«çš„è¿ç§»æ€§èƒ½ã€‚æœ€è¿‘çš„è¿›å±•é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤–éƒ¨çŸ¥è¯†æ¥æä¾›ç±»åˆ«æè¿°æ¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ¨ç†æˆæœ¬ç›¸å½“å¯è§‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TextRefinerï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨æ–¹æ³•æ¥ä¼˜åŒ–ç°æœ‰æ–¹æ³•çš„æ–‡æœ¬æç¤ºï¼Œé€šè¿‡åˆ©ç”¨VLMsçš„å†…éƒ¨çŸ¥è¯†ã€‚ç‰¹åˆ«åœ°ï¼ŒTextRefinerå»ºç«‹äº†ä¸€ä¸ªæ–°é¢–çš„æœ¬åœ°ç¼“å­˜æ¨¡å—ï¼Œä»¥å°è£…æ¥è‡ªå›¾åƒåˆ†æ”¯ä¸­å±€éƒ¨æ ‡è®°çš„ç»†ç²’åº¦è§†è§‰æ¦‚å¿µã€‚é€šè¿‡èšåˆå’Œå¯¹é½ç¼“å­˜çš„è§†è§‰æè¿°ä¸æ–‡æœ¬åˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼ŒTextRefinerå¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–å’Œä¸°å¯Œç°æœ‰æ–¹æ³•ä¸­çš„å­¦ä¹ æç¤ºï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨ä¸“å®¶çŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå°†CoOpçš„æ€§èƒ½ä»71.66%æé«˜åˆ°76.94%ï¼Œè¶…è¶Šäº†å¼•å…¥å®ä¾‹çº§ç‰¹å¾çš„CoCoOpç”¨äºæ–‡æœ¬æç¤ºçš„æ–¹æ³•ã€‚é…å¤‡TextRefineråï¼ŒPromptKDè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å¹¶ä¸”åœ¨æ¨ç†æ–¹é¢éå¸¸é«˜æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/xjjxmu/TextRefiner%E4%B8%8B%E3%80%82">https://github.com/xjjxmu/TextRefinerä¸‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08176v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTextRefinerçš„å³æ’å³ç”¨æ–¹æ³•ï¼Œç”¨äºé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å†…éƒ¨çŸ¥è¯†æ¥ä¼˜åŒ–ç°æœ‰æ–¹æ³•çš„æ–‡æœ¬æç¤ºã€‚TextRefinerå»ºç«‹äº†ä¸€ä¸ªæ–°å‹æœ¬åœ°ç¼“å­˜æ¨¡å—ï¼Œä»¥å°è£…å›¾åƒåˆ†æ”¯ä¸­å±€éƒ¨æ ‡è®°çš„ç»†ç²’åº¦è§†è§‰æ¦‚å¿µã€‚é€šè¿‡èšåˆå’Œå¯¹é½ç¼“å­˜çš„è§†è§‰æè¿°ä¸æ–‡æœ¬åˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼ŒTextRefinerèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ä»»ä½•å¤–éƒ¨ä¸“ä¸šçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°ä¼˜åŒ–å’Œä¸°å¯Œç°æœ‰æ–¹æ³•çš„å­¦åˆ°çš„æç¤ºã€‚ä½¿ç”¨TextRefinerï¼ŒPromptKDåœ¨æ¨ç†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•åœ¨ç²—ç²’åº¦æ–¹å¼ä¸‹å­¦ä¹ æç¤ºï¼Œå¯¼è‡´é’ˆå¯¹ç‰¹å®šç±»åˆ«çš„æç¤ºæ— æ³•åŒºåˆ†ç›¸ä¼¼çš„è§†è§‰æ¦‚å¿µï¼Œé™åˆ¶äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>TextRefineré€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å†…éƒ¨çŸ¥è¯†æ¥ä¼˜åŒ–æ–‡æœ¬æç¤ºï¼Œæå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ã€‚</li>
<li>TextRefinerå»ºç«‹äº†ä¸€ä¸ªæ–°å‹æœ¬åœ°ç¼“å­˜æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°è£…å›¾åƒåˆ†æ”¯ä¸­çš„ç»†ç²’åº¦è§†è§‰æ¦‚å¿µã€‚</li>
<li>TextRefineré€šè¿‡èšåˆå’Œå¯¹é½ç¼“å­˜çš„è§†è§‰æè¿°ä¸æ–‡æœ¬åˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼Œèƒ½å¤Ÿä¸°å¯Œå¹¶ä¼˜åŒ–ç°æœ‰æ–¹æ³•çš„å­¦åˆ°çš„æç¤ºã€‚</li>
<li>TextRefineråœ¨ä¸ä¾èµ–ä»»ä½•å¤–éƒ¨ä¸“ä¸šçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTextRefineræé«˜äº†CoOpçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å¼•å…¥å®ä¾‹çº§ç‰¹å¾çš„CoCoOpæ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7d402bcb2d7f33b54c8bfc63dc2df5b7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ba1a62f4d387ed5429c0963047c71f57.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-362f18538a8926ab6d9349ce6cfcd492.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7823e4ced9b560b6aea55bb844dd2591.jpg" align="middle">
</details>




<h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†æ˜¯åˆ©ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å´é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ ‡æ³¨æ•°æ®ç¼ºä¹å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çš„ä¸åŒå±‚æ¬¡ä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥æœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œåªæœ‰å°‘æ•°è¯­ä¹‰æ ‡è®°æ ·æœ¬çš„æ•°æ®ä¸Šï¼Œæ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬ç›‘ç£éƒ½éå¸¸å¼±ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ï¼Œé€šè¿‡åŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºæ¥å®ç°ã€‚ä¸ºäº†å®Œæˆè¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾æç¤ºæ–¹æ³•ï¼Œç„¶åæå‡ºäº†ç¬¬ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„å›¾-è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºå¾®è°ƒæ—¶çš„ç›‘ç£ä¸è¶³ï¼Œåœ¨æˆ‘ä»¬çš„èŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒä¸å˜ï¼Œå› æ­¤å¯å­¦ä¹ çš„å‚æ•°è¿œè¿œå°‘äºå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„èŒƒå¼åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œå¯ä»¥å°†GNNæ¨å¹¿åˆ°å…·æœ‰éå¸¸å¼±æ–‡æœ¬ç›‘ç£çš„æœªè§ç±»åˆ«ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v1">PDF</a> Preprint, 26 pages</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ„å»ºå…·æœ‰å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„äº’è”ç½‘è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹çš„è§†è§‰æ¨¡å‹å·²å–å¾—å·¨å¤§æˆåŠŸï¼Œä½†åˆ©ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰é¢ä¸´ä¸‰å¤§åŸºæœ¬é—®é¢˜ï¼šç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡å±‚æ¬¡ä¸åŒä»¥åŠé¢†åŸŸä¹‹é—´çš„æ¦‚å¿µé¸¿æ²Ÿã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡å¼æç¤ºå­¦ä¹ ï¼Œä»…ä½¿ç”¨å°‘é‡è¯­ä¹‰æ ‡æ³¨æ ·æœ¬å’Œæå¼±çš„æ–‡æœ¬ç›‘ç£ï¼Œæœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾æç¤ºæ–¹æ³•ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†ç”¨äºæŒ–æ˜é¢„è®­ç»ƒæ¨¡å‹ä¸­çŸ¥è¯†çš„å›¾è¯­è¨€å¤šæ¨¡å¼æç¤ºå­¦ä¹ æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºå¾®è°ƒæ—¶çš„ç›‘ç£ä¸è¶³ï¼Œåœ¨æˆ‘ä»¬çš„èŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒä¸å˜ï¼Œå› æ­¤å¯å­¦ä¹ å‚æ•°è¿œå°‘äºå¯¹ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒã€‚é€šè¿‡ç°å®æ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„èŒƒå¼åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨æå¼±çš„æ–‡æœ¬ç›‘ç£å°†GNNæ¨å¹¿åˆ°æœªè§ç±»åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ„å»ºè§†è§‰æ¨¡å‹è™½å·²å–å¾—æˆåŠŸï¼Œä½†åœ¨åº”ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ—¶é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡å±‚æ¬¡ä¸åŒä»¥åŠé¢†åŸŸé—´æ¦‚å¿µé¸¿æ²Ÿæ˜¯æ„å»ºå¯è¿ç§»GNNçš„ä¸»è¦éš¾é¢˜ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡å¼æç¤ºå­¦ä¹ ï¼Œèƒ½æœ‰æ•ˆé€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ï¼Œä»…ä½¿ç”¨å°‘é‡è¯­ä¹‰æ ‡æ³¨æ ·æœ¬å’Œæå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚</li>
<li>æ–°çš„èŒƒå¼å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚</li>
<li>æ”¹è¿›äº†å›¾æç¤ºæ–¹æ³•ï¼Œå¹¶é¦–æ¬¡æå‡ºå›¾è¯­è¨€å¤šæ¨¡å¼æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚</li>
<li>åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºç›‘ç£ä¸è¶³ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒä¸å˜ï¼Œé™ä½äº†å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-418010f18183fa1efe9737e4f27a25e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fea15ced17268cfd3a73eccc52e763fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a61bf3a22face5a83a5c80d12004d0a7.jpg" align="middle">
</details>




<h2 id="Leveraging-Content-and-Context-Cues-for-Low-Light-Image-Enhancement"><a href="#Leveraging-Content-and-Context-Cues-for-Low-Light-Image-Enhancement" class="headerlink" title="Leveraging Content and Context Cues for Low-Light Image Enhancement"></a>Leveraging Content and Context Cues for Low-Light Image Enhancement</h2><p><strong>Authors:Igor Morawski, Kai He, Shusil Dangi, Winston H. Hsu</strong></p>
<p>Low-light conditions have an adverse impact on machine cognition, limiting the performance of computer vision systems in real life. Since low-light data is limited and difficult to annotate, we focus on image processing to enhance low-light images and improve the performance of any downstream task model, instead of fine-tuning each of the models which can be prohibitively expensive. We propose to improve the existing zero-reference low-light enhancement by leveraging the CLIP model to capture image prior and for semantic guidance. Specifically, we propose a data augmentation strategy to learn an image prior via prompt learning, based on image sampling, to learn the image prior without any need for paired or unpaired normal-light data. Next, we propose a semantic guidance strategy that maximally takes advantage of existing low-light annotation by introducing both content and context cues about the image training patches. We experimentally show, in a qualitative study, that the proposed prior and semantic guidance help to improve the overall image contrast and hue, as well as improve background-foreground discrimination, resulting in reduced over-saturation and noise over-amplification, common in related zero-reference methods. As we target machine cognition, rather than rely on assuming the correlation between human perception and downstream task performance, we conduct and present an ablation study and comparison with related zero-reference methods in terms of task-based performance across many low-light datasets, including image classification, object and face detection, showing the effectiveness of our proposed method. </p>
<blockquote>
<p>ä½å…‰æ¡ä»¶å¯¹æœºå™¨è®¤çŸ¥äº§ç”Ÿä¸åˆ©å½±å“ï¼Œé™åˆ¶äº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨ç°å®ç”Ÿæ´»ä¸­çš„æ€§èƒ½ã€‚ç”±äºä½å…‰æ•°æ®æœ‰é™ä¸”éš¾ä»¥æ ‡æ³¨ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå›¾åƒå¤„ç†ï¼Œä»¥å¢å¼ºä½å…‰å›¾åƒå¹¶æ”¹å–„ä»»ä½•ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸æ˜¯å¾®è°ƒå¯èƒ½éå¸¸æ˜‚è´µçš„æ¯ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨CLIPæ¨¡å‹æ•è·å›¾åƒå…ˆéªŒå¹¶è¿›è¡Œè¯­ä¹‰å¼•å¯¼ï¼Œä»¥æ”¹è¿›ç°æœ‰çš„æ— å‚è€ƒä½å…‰å¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæç¤ºå­¦ä¹ é€šè¿‡å›¾åƒé‡‡æ ·å­¦ä¹ å›¾åƒå…ˆéªŒçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ— éœ€ä»»ä½•é…å¯¹æˆ–éé…å¯¹æ­£å¸¸å…‰æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰å¼•å¯¼ç­–ç•¥ï¼Œé€šè¿‡å¼•å…¥å…³äºå›¾åƒè®­ç»ƒè¡¥ä¸çš„å†…å®¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œæœ€å¤§é™åº¦åœ°åˆ©ç”¨ç°æœ‰çš„ä½å…‰æ ‡æ³¨ã€‚æˆ‘ä»¬åœ¨ä¸€é¡¹å®šæ€§ç ”ç©¶ä¸­å‘ç°ï¼Œæ‰€æå‡ºçš„å‰é©±çŸ¥è¯†å’Œè¯­ä¹‰å¼•å¯¼æœ‰åŠ©äºæé«˜å›¾åƒçš„æ•´ä½“å¯¹æ¯”åº¦å’Œè‰²è°ƒï¼Œæ”¹å–„èƒŒæ™¯å‰æ™¯çš„è¾¨è¯†èƒ½åŠ›ï¼Œå‡å°‘ç›¸å…³æ— å‚è€ƒæ–¹æ³•ä¸­å¸¸è§çš„è¿‡é¥±å’Œå’Œå™ªå£°è¿‡åº¦æ”¾å¤§é—®é¢˜ã€‚é‰´äºæˆ‘ä»¬çš„ç›®æ ‡é¢†åŸŸæ˜¯æœºå™¨è®¤çŸ¥ï¼Œå› æ­¤å¹¶æœªå‡å®šäººç±»æ„ŸçŸ¥ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèç ”ç©¶ï¼Œå¹¶åœ¨å¤šä¸ªä½å…‰æ•°æ®é›†ä¸Šä¸ç›¸å…³æ— å‚è€ƒæ–¹æ³•è¿›è¡Œäº†ä»»åŠ¡æ€§èƒ½æ¯”è¾ƒï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç‰©ä½“å’Œé¢éƒ¨æ£€æµ‹ç­‰ï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07693v1">PDF</a> Accepted to the IEEE Transactions on Multimedia</p>
<p><strong>Summary</strong><br>ä½å…‰ç¯å¢ƒå¯¹æœºå™¨è®¤çŸ¥æœ‰è´Ÿé¢å½±å“ã€‚ä¸ºæé«˜è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨ä½å…‰ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºåˆ©ç”¨CLIPæ¨¡å‹æ•è·å›¾åƒå…ˆéªŒä¿¡æ¯å’Œè¯­ä¹‰æŒ‡å¯¼çš„ç­–ç•¥ã€‚é€šè¿‡æ•°æ®å¢å¼ºç­–ç•¥å­¦ä¹ å›¾åƒå…ˆéªŒä¿¡æ¯ï¼Œæ— éœ€é…å¯¹æˆ–æœªé…å¯¹æ­£å¸¸å…‰ç…§æ•°æ®ã€‚åŒæ—¶ï¼Œæå‡ºè¯­ä¹‰æŒ‡å¯¼ç­–ç•¥å……åˆ†åˆ©ç”¨ç°æœ‰çš„ä½å…‰æ ‡æ³¨ä¿¡æ¯ï¼Œå¼•å…¥å›¾åƒè®­ç»ƒç‰‡æ®µçš„å†…å®¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚å®éªŒè¯æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å’Œè¯­ä¹‰æŒ‡å¯¼æœ‰åŠ©äºæ”¹å–„å›¾åƒæ•´ä½“å¯¹æ¯”åº¦å’Œè‰²è°ƒï¼Œæé«˜èƒŒæ™¯ä¸å‰æ™¯çš„è¾¨è¯†èƒ½åŠ›ï¼Œå‡å°‘ç›¸å…³é›¶å‚è€ƒæ–¹æ³•å¸¸è§çš„è¿‡é¥±å’Œå’Œå™ªå£°è¿‡åº¦æ”¾å¤§é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿä»¥æœºå™¨è®¤çŸ¥ä¸ºç›®æ ‡ï¼Œé€šè¿‡å¯¹æ¯”å®éªŒå’Œå¯¹æ¯”ç›¸å…³é›¶å‚è€ƒæ–¹æ³•ï¼Œå±•ç¤ºå…¶åœ¨å¤šä¸ªä½å…‰ç…§æ•°æ®é›†ä¸Šçš„ä»»åŠ¡æ€§èƒ½è¡¨ç°çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å…‰ç¯å¢ƒå¯¹æœºå™¨è®¤çŸ¥æœ‰è´Ÿé¢å½±å“ï¼Œé™åˆ¶äº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿèšç„¦äºå›¾åƒå¤„ç†ä»¥å¢å¼ºä½å…‰å›¾åƒï¼Œè€Œä¸æ˜¯å¾®è°ƒæ¨¡å‹ä»¥åº”å¯¹é«˜æ˜‚çš„æˆæœ¬ã€‚</li>
<li>åˆ©ç”¨CLIPæ¨¡å‹æ•è·å›¾åƒå…ˆéªŒä¿¡æ¯å’Œè¯­ä¹‰æŒ‡å¯¼ï¼Œç”¨äºæ”¹å–„ä½å…‰å›¾åƒæ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºç­–ç•¥å­¦ä¹ å›¾åƒå…ˆéªŒä¿¡æ¯ï¼Œæ— éœ€æ­£å¸¸å…‰ç…§æ•°æ®ã€‚</li>
<li>æå‡ºè¯­ä¹‰æŒ‡å¯¼ç­–ç•¥å……åˆ†åˆ©ç”¨ç°æœ‰çš„ä½å…‰æ ‡æ³¨ä¿¡æ¯ï¼ŒåŒ…å«å†…å®¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†å›¾åƒå¯¹æ¯”åº¦å’Œè‰²è°ƒï¼Œæ”¹å–„äº†èƒŒæ™¯ä¸å‰æ™¯çš„è¾¨è¯†èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-767a0f93c595a60a9ca97a321bfc3787.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b7b92e95081ad085b9918ddd57fdf836.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6198317ed762b401441d9e4a7949bfa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d0e888355bc133d29c9f5300aafb061f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-acae8c89829e76df73ac2e0771e6fee5.jpg" align="middle">
</details>




<h2 id="Retaining-and-Enhancing-Pre-trained-Knowledge-in-Vision-Language-Models-with-Prompt-Ensembling"><a href="#Retaining-and-Enhancing-Pre-trained-Knowledge-in-Vision-Language-Models-with-Prompt-Ensembling" class="headerlink" title="Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models   with Prompt Ensembling"></a>Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models   with Prompt Ensembling</h2><p><strong>Authors:Donggeun Kim, Yujin Jo, Myungjoo Lee, Taesup Kim</strong></p>
<p>The advancement of vision-language models, particularly the Contrastive Language-Image Pre-training (CLIP) model, has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to understand and respond to previously unseen data without task-specific training. However, adapting CLIP to integrate specialized knowledge from various domains while retaining its zero-shot capabilities remains a significant challenge. To address this, we introduce a novel prompt ensemble learning approach called Group-wise Prompt Ensemble (GPE). This method aims to enhance CLIPâ€™s zero-shot capabilities by incorporating new domain knowledge while improving its adaptability and robustness against data distribution shifts. Our approach hinges on three main strategies: prompt grouping with masked attention to optimize CLIPâ€™s adaptability while safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts for the seamless integration of new domain insights without disrupting the original modelâ€™s representation; and an ensemble learning strategy that effectively merges original and new knowledge. Through rigorous experimentation, including more challenging cross-dataset transfer evaluations, our GPE method redefines the benchmarks for the adaptability and efficiency of vision-language models, surpassing existing models across various scenarios. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹çš„è¿›æ­¥ï¼Œå·²ç»é€šè¿‡å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå½»åº•æ”¹å˜äº†æœºå™¨å­¦ä¹ é¢†åŸŸã€‚è¿™äº›èƒ½åŠ›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œå“åº”ä¹‹å‰æœªè§è¿‡çš„æ•°æ®ï¼Œè€Œæ— éœ€è¿›è¡Œç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚ç„¶è€Œï¼Œå°†CLIPé€‚åº”ä»¥æ•´åˆæ¥è‡ªä¸åŒé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒå…¶é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºé›†æˆå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç¾¤ç»„æç¤ºé›†æˆï¼ˆGPEï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡ç»“åˆæ–°çš„é¢†åŸŸçŸ¥è¯†ï¼Œå¢å¼ºCLIPçš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜å…¶é€‚åº”æ€§å’Œå¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºä¸‰ä¸ªä¸»è¦ç­–ç•¥ï¼šé€šè¿‡æç¤ºåˆ†ç»„å’Œæ©è†œæ³¨æ„åŠ›æ¥ä¼˜åŒ–CLIPçš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä¿æŠ¤å…¶é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼›é€šè¿‡å¼•å…¥è¾…åŠ©æç¤ºï¼Œå®ç°æ–°é¢†åŸŸæ´å¯Ÿçš„æ— ç¼é›†æˆï¼Œè€Œä¸ä¼šç ´ååŸå§‹æ¨¡å‹çš„è¡¨ç¤ºï¼›ä»¥åŠä¸€ç§æœ‰æ•ˆçš„é›†æˆå­¦ä¹ ç­–ç•¥ï¼Œå¯ä»¥å¾ˆå¥½åœ°èåˆæ–°æ—§çŸ¥è¯†ã€‚é€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼ŒåŒ…æ‹¬æ›´å…·æŒ‘æˆ˜æ€§çš„è·¨æ•°æ®é›†è¿ç§»è¯„ä¼°ï¼Œæˆ‘ä»¬çš„GPEæ–¹æ³•é‡æ–°å®šä¹‰äº†è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”æ€§å’Œæ•ˆç‡çš„æ ‡å‡†ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹å‡è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07077v1">PDF</a> IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV)   2025</p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹é€šè¿‡å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œåœ¨é›†æˆç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ä¿æŒå…¶é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ä»æ˜¯CLIPæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºé›†æˆå­¦ä¹ æ–¹æ³•â€”â€”åˆ†ç»„æç¤ºé›†æˆï¼ˆGPEï¼‰ã€‚é€šè¿‡ç²¾ç»†çš„æç¤ºåˆ†ç»„ä¸æ©è†œæ³¨æ„åŠ›æœºåˆ¶ã€è¾…åŠ©æç¤ºçš„èå…¥ä»¥åŠæœ‰æ•ˆçš„é›†æˆå­¦ä¹ ç­–ç•¥ï¼ŒGPEåœ¨ä¿éšœCLIPæ¨¡å‹é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„åŒæ—¶æå‡äº†å…¶é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒGPEæ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹å‡è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œé‡æ–°å®šä¹‰äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§å’Œæ•ˆç‡åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹å®ç°äº†é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸå¸¦æ¥é‡å¤§å˜é©ã€‚</li>
<li>åœ¨é›†æˆç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ä¿æŒCLIPæ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚</li>
<li>åˆ†ç»„æç¤ºé›†æˆï¼ˆGPEï¼‰æ–¹æ³•é€šè¿‡ç²¾ç»†çš„æç¤ºåˆ†ç»„ä¸æ©è†œæ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–CLIPæ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>è¾…åŠ©æç¤ºçš„èå…¥å®ç°äº†æ–°é¢†åŸŸçŸ¥è¯†ä¸åŸå§‹æ¨¡å‹è¡¨ç¤ºçš„æ— ç¼é›†æˆã€‚</li>
<li>GPEé‡‡ç”¨æœ‰æ•ˆçš„é›†æˆå­¦ä¹ ç­–ç•¥ï¼Œå°†åŸå§‹çŸ¥è¯†ä¸æ–°çŸ¥è¯†æœ‰æ•ˆç»“åˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒGPEæ–¹æ³•åœ¨é€‚åº”æ€§å’Œæ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-279149c621356f1222167de36f0a8df6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-75cf14cba4f83c1c094e3bcbfea43f90.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bf34f58f9ac27d52dc2c5d1821d4d1e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14c76a110d190eccc16befbe3591091d.jpg" align="middle">
</details>




<h2 id="Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning"><a href="#Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning" class="headerlink" title="Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning"></a>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</h2><p><strong>Authors:Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou</strong></p>
<p>Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasonerâ€™s capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºè¯¸å¦‚è™šæ„çš„å›¾åƒç†è§£æˆ–ç²—ç³™çš„æ¨ç†è·¯å¾„ç­‰é—®é¢˜ï¼Œå®ƒä»¬ä»ç„¶ç»å¸¸äº§ç”Ÿä¸å‡†ç¡®æˆ–æ— å…³çš„å“åº”ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Critic-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå—Actor-CriticèŒƒå¼å¯å‘çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶æ¥è§£è€¦æ¨ç†è¿‡ç¨‹å’Œæ‰¹è¯„è¿‡ç¨‹ï¼šReasoneræ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œè€ŒCriticæä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼ŒReasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†å“åº”ï¼Œè¿™äº›å“åº”å¯ä»¥åŸºäºCriticçš„åé¦ˆä½œä¸ºç­–ç•¥è¿›è¡Œè¿­ä»£æ¼”åŒ–ã€‚è¿™ä¸€äº¤äº’è¿‡ç¨‹æ˜¯ç”±å¼ºåŒ–å­¦ä¹ æ¡†æ¶é©±åŠ¨çš„ï¼Œå…¶ä¸­Criticæä¾›è‡ªç„¶è¯­è¨€æ‰¹è¯„è€Œä¸æ˜¯æ ‡é‡å¥–åŠ±ï¼Œä»è€Œå¯ä»¥æä¾›æ›´ç»†å¾®çš„åé¦ˆæ¥æå‡Reasoneråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰æ’åçš„æ‰¹è¯„åå¥½æ•°æ®é›†æ¥å¢å¼ºå…¶æ‰¹è¯„èƒ½åŠ›.è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCritic-Væ¡†æ¶åœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬GPT-4Vï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚ç»“åˆReasonerçš„åŠ¨æ€æ–‡æœ¬ç­–ç•¥ä»¥åŠæ¥è‡ªåå¥½ä¼˜åŒ–åçš„Criticçš„å»ºè®¾æ€§åé¦ˆï¼Œå®ç°äº†æ›´å¯é å’Œè¯­å¢ƒæ•æ„Ÿçš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæå‡VLMsçš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ”¹è¿›å…¶åœ¨ç°å®ä¸–ç•Œæ¨ç†å¯†é›†å‹å¤šæ¨¡æ€åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½å®ä½“ï¼‰ä¸­çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18203v2">PDF</a> 16 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆä¸å‡†ç¡®æˆ–æ— å…³å“åº”çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Critic-Væ¡†æ¶ï¼Œè¯¥æ¡†æ¶å—Actor-CriticèŒƒå¼çš„å¯å‘ï¼Œæ—¨åœ¨æé«˜VLMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶â€”â€”Reasonerå’ŒCriticï¼Œå®ç°äº†æ¨ç†è¿‡ç¨‹å’Œæ‰¹è¯„è¿‡ç¨‹çš„è§£è€¦ã€‚Reasoneræ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œè€ŒCriticæä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼ŒReasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†å“åº”ï¼Œè¿™äº›å“åº”å¯ä»¥æ ¹æ®æ¥è‡ªCriticçš„åé¦ˆè¿›è¡Œè¿­ä»£æ¼”åŒ–ã€‚è¿™ä¸€äº¤äº’è¿‡ç¨‹ç”±å¼ºåŒ–å­¦ä¹ æ¡†æ¶é©±åŠ¨ï¼ŒCriticæä¾›è‡ªç„¶è¯­è¨€æ‰¹è¯„è€Œéæ ‡é‡å¥–åŠ±ï¼Œä¸ºReasoneråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æå‡æä¾›æ›´å¾®å¦™çš„åé¦ˆã€‚Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰å¯¹æ‰¹è¯„è¿›è¡Œæ’åï¼Œä»¥æé«˜å…¶æ‰¹è¯„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCritic-Væ¡†æ¶åœ¨5é¡¹æŒ‡æ ‡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬GPT-4Vï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚ç»“åˆåŸºäºæ–‡æœ¬çš„åŠ¨æ€æ”¿ç­–å’Œç»è¿‡ä¼˜åŒ–çš„åå¥½Criticçš„åé¦ˆï¼Œä¸ºæ›´å¯é å’Œä¸Šä¸‹æ–‡æ•æ„Ÿçš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶ä¸ºæé«˜VLMåœ¨ç°å®ä¸–ç•Œæ¨ç†å¯†é›†å‹å¤šæ¨¡æ€åº”ç”¨ä¸­çš„å¯é æ€§ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½åµŒå…¥ï¼‰æä¾›äº†å¯é æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VLMåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆä¸å‡†ç¡®æˆ–æ— å…³å“åº”çš„é—®é¢˜ã€‚</li>
<li>Critic-Væ¡†æ¶åŸºäºActor-CriticèŒƒå¼æ„å»ºï¼Œæ—¨åœ¨æé«˜VLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶åŒ…å«Reasonerå’ŒCriticä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆå’Œä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚</li>
<li>Criticä½¿ç”¨è‡ªç„¶è¯­è¨€æ‰¹è¯„è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä»¥æä¾›æ›´å¾®å¦™çš„åé¦ˆã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç”¨äºè®­ç»ƒCriticæ¨¡å‹ï¼Œä»¥æé«˜å…¶æ‰¹è¯„èƒ½åŠ›ã€‚</li>
<li>Critic-Væ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-618f29ee5430b286ee8b46c0e035f63f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6837c2c367cd1a4e7ee3ad4ec2bd8ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-269d187768ea4ab5aa83a7d89fb133bc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f2278620f94584191480f74c9cf8598c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg" align="middle">
</details>




<h2 id="COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding"><a href="#COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding" class="headerlink" title="COBRA: A Continual Learning Approach to Vision-Brain Understanding"></a>COBRA: A Continual Learning Approach to Vision-Brain Understanding</h2><p><strong>Authors:Xuan-Bac Nguyen, Arabinda Kumar Choudhary, Pawan Sinha, Xin Li, Khoa Luu</strong></p>
<p>Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module contains a transformer encoder and decoder that learns the fMRI features for VBU from common and specific patterns. In a continual learning setup, COBRA is trained in new PSS and MRIFormer modules for new subjects, leaving the modules of previous subjects unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods. </p>
<blockquote>
<p>è§†è§‰å¤§è„‘ç†è§£ï¼ˆVBUï¼‰æ—¨åœ¨ä»é€šè¿‡åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰è®°å½•çš„å¤§è„‘æ´»åŠ¨ä¸­æå–äººç±»æ„ŸçŸ¥åˆ°çš„è§†è§‰ä¿¡æ¯ã€‚å°½ç®¡è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†VBUçš„ç°æœ‰ç ”ç©¶ä»ç„¶é¢ä¸´ç€ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹åœ¨é€‚åº”æ–°ä¸»é¢˜æ—¶ä¸¢å¤±äº†å…ˆå‰ä¸»é¢˜çš„çŸ¥è¯†ã€‚å› æ­¤ï¼Œè§£å†³è¯¥é¢†åŸŸçš„æŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCOBRAçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œä»¥è§£å†³VBUä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–æ¨¡å—ï¼šä¸»é¢˜å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»é¢˜ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—ï¼Œç§°ä¸ºMRIFormeræ¨¡å—ã€‚SCæ¨¡å—æ•è·è·¨ä¸»é¢˜çš„å…±åŒè§†è§‰å¤§è„‘æ¨¡å¼ï¼Œå¹¶åœ¨æ¨¡å‹é‡åˆ°æ–°ä¸»é¢˜æ—¶ä¿ç•™è¿™äº›çŸ¥è¯†ï¼Œä»è€Œå‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼ŒPSSæ¨¡å—å­¦ä¹ æ¯ä¸ªä¸»é¢˜ç‰¹æœ‰çš„è§†è§‰å¤§è„‘æ¨¡å¼ã€‚æœ€åï¼ŒMRIFormeræ¨¡å—åŒ…å«ä¸€ä¸ªå˜å‹å™¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä»å…¬å…±å’Œç‰¹å®šæ¨¡å¼ä¸­å­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚åœ¨æŒç»­å­¦ä¹ è®¾ç½®ä¸­ï¼ŒCOBRAé’ˆå¯¹æ–°ä¸»é¢˜è®­ç»ƒæ–°çš„PSSå’ŒMRIFormeræ¨¡å—ï¼Œè€Œä¸å½±å“ä»¥å‰çš„æ¨¡å—ã€‚å› æ­¤ï¼ŒCOBRAæœ‰æ•ˆåœ°è§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰å¤§è„‘é‡å»ºä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17475v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è„‘ç†è§£ï¼ˆVBUï¼‰é¢†åŸŸçš„æŒç»­å­¦ä¹ é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”COBRAã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šä¸»ä½“å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»ä½“ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—ï¼ˆMRIFormerï¼‰ã€‚å…¶ä¸­SCæ¨¡å—æ•æ‰ä¸åŒä¸»ä½“é—´çš„å…±äº«è§†è§‰è„‘æ¨¡å¼ï¼Œå‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ï¼›PSSæ¨¡å—å­¦ä¹ æ¯ä¸ªä¸»ä½“çš„ç‹¬ç‰¹è§†è§‰è„‘æ¨¡å¼ï¼›MRIFormeræ¨¡å—åˆ™é€šè¿‡å˜å‹å™¨ç¼–ç å™¨è§£ç å™¨å­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚COBRAæ¡†æ¶åœ¨æŒç»­å­¦ä¹ è®¾ç½®ä¸­è®­ç»ƒæ–°çš„PSSå’ŒMRIFormeræ¨¡å—ä»¥é€‚åº”æ–°ä¸»ä½“ï¼ŒåŒæ—¶ä¿æŒå…ˆå‰ä¸»ä½“çš„æ¨¡å—ä¸å—å½±å“ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰è„‘é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°ä¼˜äºå…ˆå‰æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Brain Understanding (VBU) é€šè¿‡åŠŸèƒ½ç£å…±æŒ¯æˆåƒ (fMRI) æå–äººç±»æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>VBUé¢†åŸŸé¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹åœ¨é€‚åº”æ–°ä¸»ä½“æ—¶ä¸¢å¤±å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>COBRAæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šSCæ¨¡å—ã€PSSæ¨¡å—å’ŒMRIFormeræ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºæ•æ‰è§†è§‰è„‘æ¨¡å¼çš„å…±äº«ç‰¹å¾ã€å­¦ä¹ ä¸»ä½“ç‰¹å®šçš„è§†è§‰è„‘æ¨¡å¼ä»¥åŠå­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚</li>
<li>COBRAé€šè¿‡è®­ç»ƒæ–°çš„PSSå’ŒMRIFormeræ¨¡å—æ¥è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå…ˆå‰ä¸»ä½“çš„æ¨¡å—ä¸å˜ã€‚</li>
<li>COBRAæ¡†æ¶åœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰è„‘é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</li>
<li>SCæ¨¡å—åœ¨æ¨¡å‹é‡åˆ°æ–°ä¸»ä½“æ—¶èƒ½å¤Ÿä¿ç•™çŸ¥è¯†ï¼Œé™ä½ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6c702b3809502647f842e98a6554e0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7070aad685269c094671faaee23e433e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c8b7e55ab8521742037def62f4d2e55d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3409f541bd3078454792cebe005df6e1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f197015eba6fee457426dc6a87da541b.jpg" align="middle">
</details>




<h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits modelsâ€™ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éšç€æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œé€šè¿‡å„ç§å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…å‡ºé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®­ç»ƒæ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šåœ¨OVSSçš„æŒ‘æˆ˜ç¯å¢ƒä¸­ï¼ŒåŸºäºä»»æ„æŸ¥è¯¢æç¤ºå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶ï¼Œç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚è¿™ä¸€ç–å¿½é™åˆ¶äº†æ¨¡å‹å°†å¯¹è±¡å†…è¯­ä¹‰ä¸€è‡´å…ƒç´ åˆ†ç»„å¹¶å°†å…¶ç²¾ç¡®æ˜ å°„åˆ°ç”¨æˆ·å®šä¹‰çš„ä»»æ„ç±»åˆ«çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå›¾åƒä¸­çš„å¯¹è±¡çº§ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥å…‹æœè¿™ä¸€å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†ä»è§†è§‰åŸºç¡€æ¨¡å‹è’¸é¦å‡ºçš„å…‰è°±é©±åŠ¨ç‰¹å¾èå…¥è§†è§‰ç¼–ç å™¨çš„æ³¨æ„æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œä½¿è¯­ä¹‰ä¸€è‡´çš„ç»„ä»¶èƒ½å¤Ÿå½¢æˆå•ä¸ªå¯¹è±¡æ©ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨å¯èƒ½æ€§å¯¹æ–‡æœ¬åµŒå…¥è¿›è¡Œäº†ç²¾ç‚¼ï¼Œä»¥ç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„å…·ä½“å¯¹è±¡å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å›¾åƒä¸­çš„å¯¹è±¡çº§ä¸Šä¸‹æ–‡çŸ¥è¯†çº³å…¥è€ƒè™‘ï¼Œè§£å†³äº†åœ¨æ²¡æœ‰é¢„å…ˆå®šä¹‰ç±»åˆ«çš„æƒ…å¢ƒä¸‹ï¼Œé€šè¿‡ä»»æ„æŸ¥è¯¢æç¤ºå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²çš„é—®é¢˜ã€‚é€šè¿‡åœ¨è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾ä¸­æç‚¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°è·¨å¤šç§æ•°æ®é›†è¿›è¡Œè¯­ä¹‰ä¸€è‡´çš„åˆ†å‰²å’Œè¯†åˆ«ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰å…è®¸æ¨¡å‹å¤„ç†è¶…è¶Šé¢„è®¾ç±»åˆ«çš„åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>è®­ç»ƒå…è´¹çš„æ–¹æ³•ä¸ºå¤„ç†æœªè§è¿‡çš„æ•°æ®æä¾›äº†å¯æ‰©å±•å’Œæ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­å¯¹åŸºäºä»»æ„æŸ¥è¯¢æç¤ºçš„å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡èå…¥å›¾åƒä¸­çš„å¯¹è±¡çº§ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥å…‹æœè¿™ä¸€å±€é™ã€‚</li>
<li>é€šè¿‡æç‚¼è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åŒä¸€å¯¹è±¡çš„å†…éƒ¨ä¸€è‡´æ€§ã€‚</li>
<li>ä½¿ç”¨é›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨æ¦‚ç‡æ¥ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f7b6fbeb2826a7dad8df4b062d3486a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ffccfcbd45ef2a94b19be0bce7c72be4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bbe7a6fdd528b4861885240dd1defd77.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-90fa9c3ab36b67f06e6c20c1253b7353.jpg" align="middle">
</details>




<h2 id="Free-2-Guide-Gradient-Free-Path-Integral-Control-for-Enhancing-Text-to-Video-Generation-with-Large-Vision-Language-Models"><a href="#Free-2-Guide-Gradient-Free-Path-Integral-Control-for-Enhancing-Text-to-Video-Generation-with-Large-Vision-Language-Models" class="headerlink" title="Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing   Text-to-Video Generation with Large Vision-Language Models"></a>Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing   Text-to-Video Generation with Large Vision-Language Models</h2><p><strong>Authors:Jaemin Kim, Bryan S Kim, Jong Chul Ye</strong></p>
<p>Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free$^2$Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free$^2$Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free$^2$Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆç­‰ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œç”±äºå¸§ä¹‹é—´çš„å¤æ‚æ—¶é—´ä¾èµ–æ€§ï¼Œåœ¨T2Vç”Ÿæˆä¸­å®ç°å‡†ç¡®çš„æ–‡æœ¬å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•æ¥æé«˜æ–‡æœ¬å¯¹é½æ€§é€šå¸¸éœ€è¦ä¸€ä¸ªå¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°ï¼Œæˆ–è€…ä»…é™äºæœ‰é™çš„æç¤ºï¼Œé˜»ç¢äº†å…¶å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Free$^2$Guideï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ— éœ€æ¢¯åº¦çš„æ¡†æ¶ï¼Œç”¨äºå°†ç”Ÿæˆçš„è§†é¢‘ä¸æ–‡æœ¬æç¤ºå¯¹é½ï¼Œè€Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚å€ŸåŠ©è·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†ï¼ŒFree$^2$Guideä½¿ç”¨ä¸å¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°æ¥è¿‘ä¼¼æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼ï¼Œä»è€Œèƒ½å¤Ÿæ•´åˆå¼ºå¤§çš„é»‘ç›’å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒçµæ´»åœ°é›†æˆå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å›¾åƒæ¨¡å‹ï¼Œä»¥ååŒæé«˜å¯¹é½æ€§ï¼Œè€Œä¸ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒFree$^2$Guideåœ¨å„ä¸ªæ–¹é¢éƒ½æ˜¾è‘—æé«˜äº†æ–‡æœ¬å¯¹é½æ€§ï¼Œå¹¶æé«˜äº†ç”Ÿæˆè§†é¢‘çš„æ•´ä½“è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17041v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Free$^2$Guideæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€æ¢¯åº¦å³å¯å®ç°æ–‡æœ¬ä¸è§†é¢‘ç”Ÿæˆçš„å‡†ç¡®å¯¹é½ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚åˆ©ç”¨è·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä½¿ç”¨éå¯å¾®å¥–åŠ±å‡½æ•°ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æŒ‡å¯¼ï¼Œå¹¶èƒ½çµæ´»é›†æˆå¤šç§å¥–åŠ±æ¨¡å‹ï¼ˆåŒ…æ‹¬å¤§å‹å›¾åƒæ¨¡å‹ï¼‰ï¼ŒååŒæé«˜å¯¹é½æ€§èƒ½ï¼ŒåŒæ—¶ä¸äº§ç”Ÿæ˜¾è‘—çš„è®¡ç®—å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼ŒFree$^2$Guideå¯æ˜¾è‘—æé«˜æ–‡æœ¬å¯¹é½çš„å¤šä¸ªç»´åº¦è´¨é‡å¹¶å¢å¼ºç”Ÿæˆè§†é¢‘çš„æ€»ä½“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Free$^2$Guideæ˜¯ä¸€ä¸ªæ— éœ€æ¢¯åº¦çš„æ¡†æ¶ï¼Œç”¨äºå®ç°æ–‡æœ¬ä¸è§†é¢‘ç”Ÿæˆçš„å‡†ç¡®å¯¹é½ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨è·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†ï¼ŒFree$^2$Guideå¯ä»¥ä½¿ç”¨éå¯å¾®å¥–åŠ±å‡½æ•°ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æŒ‡å¯¼ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒé›†æˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>Free$^2$Guideèƒ½å¤Ÿçµæ´»é›†æˆå¤šç§å¥–åŠ±æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§å‹å›¾åƒæ¨¡å‹ï¼Œä»¥æé«˜å¯¹é½æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¸äº§ç”Ÿæ˜¾è‘—è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ååŒæé«˜å¯¹é½è´¨é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFree$^2$Guideæ˜¾è‘—æé«˜æ–‡æœ¬å¯¹é½çš„å¤šä¸ªç»´åº¦è´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ba9efcd661a35290481f3768b433b309.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e4438e21327f8b25ae41719540dc1b7d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a2b8f0ff8a42045f29b53e669a15022.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-46ed930ef96424876051209fae4d6526.jpg" align="middle">
</details>




<h2 id="Active-Prompt-Learning-with-Vision-Language-Model-Priors"><a href="#Active-Prompt-Learning-with-Vision-Language-Model-Priors" class="headerlink" title="Active Prompt Learning with Vision-Language Model Priors"></a>Active Prompt Learning with Vision-Language Model Priors</h2><p><strong>Authors:Hoyoung Kim, Seokhee Jin, Changhwan Sung, Jaechang Kim, Jungseul Ok</strong></p>
<p>Vision-language models (VLMs) have demonstrated remarkable zero-shot performance across various classification tasks. Nonetheless, their reliance on hand-crafted text prompts for each task hinders efficient adaptation to new tasks. While prompt learning offers a promising solution, most studies focus on maximizing the utilization of given few-shot labeled datasets, often overlooking the potential of careful data selection strategies, which enable higher accuracy with fewer labeled data. This motivates us to study a budget-efficient active prompt learning framework. Specifically, we introduce a class-guided clustering that leverages the pre-trained image and text encoders of VLMs, thereby enabling our cluster-balanced acquisition function from the initial round of active learning. Furthermore, considering the substantial class-wise variance in confidence exhibited by VLMs, we propose a budget-saving selective querying based on adaptive class-wise thresholds. Extensive experiments in active learning scenarios across nine datasets demonstrate that our method outperforms existing baselines. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹æ¯é¡¹ä»»åŠ¡çš„æ‰‹å·¥æ–‡æœ¬æç¤ºçš„ä¾èµ–é˜»ç¢äº†å…¶å¯¹æ–°ä»»åŠ¡çš„æ•ˆç‡é€‚åº”ã€‚è™½ç„¶æç¤ºå­¦ä¹ æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä¾§é‡äºæœ€å¤§é™åº¦åœ°åˆ©ç”¨ç»™å®šçš„å°‘é‡æ ‡æ³¨æ•°æ®é›†ï¼Œå¾€å¾€å¿½è§†äº†ç²¾å¿ƒé€‰æ‹©æ•°æ®ç­–ç•¥çš„æ½œåŠ›ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥è¾ƒå°‘çš„æ ‡æ³¨æ•°æ®å®ç°æ›´é«˜çš„ç²¾åº¦ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬ç ”ç©¶ä¸€ç§ç»æµé«˜æ•ˆçš„æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç±»æŒ‡å¯¼èšç±»çš„æ–¹æ³•ï¼Œåˆ©ç”¨VLMsçš„é¢„è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œä»è€Œåœ¨æˆ‘ä»¬çš„æ´»åŠ¨å­¦ä¹ åˆå§‹é˜¶æ®µå®ç°é›†ç¾¤å¹³è¡¡é‡‡é›†åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°VLMsè¡¨ç°å‡ºçš„å·¨å¤§ç±»åˆ«é—´ä¿¡å¿ƒå·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”ç±»åˆ«é˜ˆå€¼çš„é¢„ç®—èŠ‚çœé€‰æ‹©æ€§æŸ¥è¯¢æ–¹æ³•ã€‚åœ¨ä¹ä¸ªæ•°æ®é›†çš„æ´»åŠ¨å­¦ä¹ åœºæ™¯ä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¿‡äº†ç°æœ‰åŸºçº¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16722v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å…¶ä¾èµ–äºä¸ºæ¯ä¸ªä»»åŠ¡æ‰‹å·¥åˆ¶ä½œçš„æ–‡æœ¬æç¤ºï¼Œé˜»ç¢äº†åœ¨æ–°ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆé€‚åº”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é¢„ç®—é«˜æ•ˆçš„æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç±»æŒ‡å¯¼èšç±»ï¼Œåˆ©ç”¨VLMsçš„é¢„è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå®ç°é›†ç¾¤å¹³è¡¡é‡‡é›†åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°VLMså±•ç°çš„ç±»é—´ç½®ä¿¡åº¦å·®å¼‚è¾ƒå¤§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè‡ªé€‚åº”ç±»é˜ˆå€¼çš„é¢„ç®—èŠ‚çº¦é€‰æ‹©æ€§æŸ¥è¯¢ã€‚åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„ä¸»åŠ¨å­¦ä¹ åœºæ™¯å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æ–‡æœ¬æç¤ºï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§é¢„ç®—é«˜æ•ˆçš„æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç±»æŒ‡å¯¼èšç±»ï¼Œåˆ©ç”¨é¢„è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨å®ç°é›†ç¾¤å¹³è¡¡é‡‡é›†åŠŸèƒ½ã€‚</li>
<li>è€ƒè™‘åˆ°äº†VLMså±•ç°çš„ç±»é—´ç½®ä¿¡åº¦å·®å¼‚å¤§ï¼Œæå‡ºäº†åŸºäºè‡ªé€‚åº”ç±»é˜ˆå€¼çš„é¢„ç®—èŠ‚çº¦é€‰æ‹©æ€§æŸ¥è¯¢ç­–ç•¥ã€‚</li>
<li>åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»åŠ¨å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨æé«˜æ¨¡å‹æ•ˆç‡å’Œé€‚åº”æ€§æ–¹é¢å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3b1870a9668332c94cbafa2aecd08c4e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-938e9cbd8a98eb2489756bc58a9fe4d9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b9ca16ce76baa8dd4fc4becd56178131.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-874cecae9f5d4ba2733d4e3799a61963.jpg" align="middle">
</details>




<h2 id="BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models"><a href="#BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"></a>BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨è§†è§‰ä»»åŠ¡çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå°†è§†è§‰è¯­è¨€æ¨¡å‹æœ‰æ•ˆåœ°é€‚åº”åˆ°ä¸‹æ¸¸åº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬çš„å‡†ç¡®æ€§å¾€å¾€ä¾èµ–äºè€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„æç¤ºå·¥ç¨‹ï¼Œè€Œå…¨é¢è°ƒæ•´æ¨¡å‹åˆå¾ˆæ˜‚è´µã€‚ç‰¹åˆ«æ˜¯å¯¹äºç”Ÿç‰©åŒ»å­¦å›¾åƒï¼Œä¸å¤©ç„¶å›¾åƒä¸åŒï¼Œå®ƒä»¬é€šå¸¸å—é™äºæ ‡æ³¨æ•°æ®é›†ã€å›¾åƒå¯¹æ¯”åº¦ä¸å¤Ÿç›´è§‚ä»¥åŠå¾®å¦™çš„è§†è§‰ç‰¹å¾ã€‚æœ€è¿‘çš„æç¤ºå­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚CoOpï¼‰æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ä»ç¼ºä¹é€šç”¨æ€§ã€‚åŒæ—¶ï¼Œé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„æç¤ºå­¦ä¹ æ¢ç´¢ä»ç„¶éå¸¸æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BiomedCoOpï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”BiomedCLIPï¼Œä»¥å®ç°å‡†ç¡®ä¸”é«˜åº¦é€šç”¨çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§ä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥çš„çŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨è·¨è¶Š9ç§æ¨¡æ€å’Œ10ä¸ªå™¨å®˜çš„11ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå¯¹æ‰€æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†å…¨é¢çš„éªŒè¯ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®æ€§å’Œé€šç”¨æ€§éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚ä»£ç å°†åœ¨ <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a> ä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15232v1">PDF</a> 18 pages, 5 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ–°çš„æç¤ºå­¦ä¹ æ¡†æ¶BiomedCoOpã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­ä¹‰ä¸€è‡´æ€§ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡æç¤ºé›†åˆä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥ï¼Œå®ç°äº†å¯¹BiomedCLIPæ¨¡å‹çš„æœ‰æ•ˆæç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œåœ¨å°‘æ•°æ ‡æ³¨æ ·æœ¬ä¸Šè¾¾åˆ°é«˜ç²¾åº¦å’Œé«˜åº¦æ³›åŒ–çš„åˆ†ç±»æ•ˆæœã€‚è¯¥æ¡†æ¶åœ¨è·¨è¶Š9ç§æ¨¡æ€å’Œ10ç§å™¨å®˜çš„11ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„éªŒè¯ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›å‡æ˜¾è‘—æé«˜ã€‚ä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ¡†æ¶BiomedCoOpï¼Œç”¨äºè§£å†³ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>BiomedCoOpç»“åˆäº†è¯­ä¹‰ä¸€è‡´æ€§ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡æç¤ºé›†åˆä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°å¯¹BiomedCLIPæ¨¡å‹çš„æœ‰æ•ˆæç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå®ç°åœ¨å°‘æ•°æ ‡æ³¨æ ·æœ¬ä¸Šçš„é«˜ç²¾åº¦åˆ†ç±»ã€‚</li>
<li>åœ¨è·¨è¶Šå¤šç§æ¨¡æ€å’Œå™¨å®˜çš„å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼ŒBiomedCoOpçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—æé«˜ã€‚</li>
<li>BiomedCoOpè§£å†³äº†ç°æœ‰çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™æ ‡æ³¨æ•°æ®é›†ã€å›¾åƒå¯¹æ¯”åº¦ä¸å¤Ÿç›´è§‚ä»¥åŠå¾®å¦™çš„è§†è§‰ç‰¹å¾ç­‰é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆè¯­ä¹‰ä¸€è‡´æ€§è¿›è¡Œæç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»¥åŠé‡‡ç”¨åŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥ã€‚è¿™ç§ç»“åˆæ–¹å¼æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f47ee0177c47ded9dbffb2322847edc0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b35924fcdaa57f51ad7edfe0b37e082e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1e42816bd1d7c7d6931bb05d1a5ec10b.jpg" align="middle">
</details>




<h2 id="Looking-Beyond-Text-Reducing-Language-bias-in-Large-Vision-Language-Models-via-Multimodal-Dual-Attention-and-Soft-Image-Guidance"><a href="#Looking-Beyond-Text-Reducing-Language-bias-in-Large-Vision-Language-Models-via-Multimodal-Dual-Attention-and-Soft-Image-Guidance" class="headerlink" title="Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance"></a>Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance</h2><p><strong>Authors:Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang</strong></p>
<p>Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on images and ineffective visual comprehension. We identify two primary reasons for this bias: 1. Different scales of training data between the pretraining stage of LLM and multimodal alignment stage. 2. The learned inference bias due to short-term dependency of text data. Therefore, we propose LACING, a systemic framework designed to address the language bias of LVLMs with muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG). Specifically, MDA introduces a parallel dual-attention mechanism that enhances the integration of visual inputs across the model. IFG introduces a learnable soft visual prompt during training and inference to replace visual inputs, designed to compel LVLMs to prioritize text inputs. Then, IFG further proposes a novel decoding strategy using the soft visual prompt to mitigate the modelâ€™s over-reliance on adjacent text inputs. Comprehensive experiments demonstrate that our method effectively debiases LVLMs from their language bias, enhancing visual comprehension and reducing hallucinations without requiring additional training resources or data. The code and model are available at <a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œå°½ç®¡è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ï¼ŒLVLMså´å­˜åœ¨ç”±è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œè¿™å¯¼è‡´å¯¹å›¾åƒçš„å…³æ³¨åº¦é™ä½å’Œè§†è§‰ç†è§£æ— æ•ˆã€‚æˆ‘ä»¬ç¡®å®šäº†è¿™ç§åè§çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼š1. ä»‹äºå¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒé˜¶æ®µå’Œå¤šæ¨¡æ€å¯¹é½é˜¶æ®µä¹‹é—´çš„è®­ç»ƒæ•°æ®è§„æ¨¡ä¸åŒã€‚2. ç”±äºæ–‡æœ¬æ•°æ®çš„çŸ­æœŸä¾èµ–æ€§è€Œäº§ç”Ÿçš„æ¨æ–­åè§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LACINGç³»ç»Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LVLMsçš„è¯­è¨€åè§é—®é¢˜ï¼Œé‡‡ç”¨å¤šæ¨¡æ€åŒæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMDAï¼‰å’Œè½¯å›¾åƒå¼•å¯¼ï¼ˆIFGï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒMDAå¼•å…¥äº†ä¸€ç§å¹¶è¡ŒåŒæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹ä¸­è§†è§‰è¾“å…¥çš„é›†æˆã€‚IFGåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„è½¯è§†è§‰æç¤ºæ¥æ›¿ä»£è§†è§‰è¾“å…¥ï¼Œæ—¨åœ¨è¿«ä½¿LVLMsä¼˜å…ˆè€ƒè™‘æ–‡æœ¬è¾“å…¥ã€‚ç„¶åï¼ŒIFGè¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä½¿ç”¨è½¯è§†è§‰æç¤ºçš„æ–°è§£ç ç­–ç•¥ï¼Œä»¥å‡è½»æ¨¡å‹å¯¹ç›¸é‚»æ–‡æœ¬è¾“å…¥çš„è¿‡åº¦ä¾èµ–ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æ¶ˆé™¤äº†LVLMsçš„è¯­è¨€åè§ï¼Œæé«˜äº†è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå‡å°‘äº†å¹»è§‰ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒèµ„æºæˆ–æ•°æ®ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14279v1">PDF</a> 19 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œä½†å®ƒä»¬å­˜åœ¨è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œå½±å“äº†å¯¹å›¾åƒçš„å…³æ³¨åŠ›å’Œè§†è§‰ç†è§£èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LACINGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¤šæ¨¡æ€åŒæ³¨æ„åŠ›æœºåˆ¶å’Œè½¯å›¾åƒå¼•å¯¼æ¥çº æ­£LVLMsçš„è¯­è¨€åè§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æå‡è§†è§‰ç†è§£åŠ›å¹¶å‡å°‘å¹»è§‰ç°è±¡ã€‚è¯¦æƒ…è¯·è®¿é—®<a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LVLMsåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>åè§çš„ä¸»è¦åŸå› æ˜¯è®­ç»ƒæ•°æ®è§„æ¨¡ä¸åŒä»¥åŠæ–‡æœ¬æ•°æ®çš„çŸ­æœŸä¾èµ–æ€§ã€‚</li>
<li>LACINGæ¡†æ¶é€šè¿‡å¼•å…¥å¤šæ¨¡æ€åŒæ³¨æ„åŠ›æœºåˆ¶å’Œè½¯å›¾åƒå¼•å¯¼æ¥çº æ­£LVLMsçš„è¯­è¨€åè§ã€‚</li>
<li>è½¯å›¾åƒå¼•å¯¼å¯åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ›¿ä»£è§†è§‰è¾“å…¥ï¼Œä¿ƒä½¿LVLMsæ›´é‡è§†æ–‡æœ¬è¾“å…¥ã€‚</li>
<li>LACINGæ¡†æ¶é‡‡ç”¨æ–°çš„è§£ç ç­–ç•¥ï¼Œä½¿ç”¨è½¯è§†è§‰æç¤ºå‡å°‘æ¨¡å‹å¯¹ç›¸é‚»æ–‡æœ¬è¾“å…¥çš„ä¾èµ–ã€‚</li>
<li>ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLACINGæ–¹æ³•èƒ½æœ‰æ•ˆçº æ­£LVLMsçš„è¯­è¨€åè§ï¼Œæé«˜è§†è§‰ç†è§£åŠ›å’Œå‡å°‘å¹»è§‰ç°è±¡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c8e25e0c4c4756d97250e95df89ceb63.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-67ce42f158bf6263df9b3ed306ddd4ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cfe243aff073e1742ceed0eca4ad04bc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4e1bd75a9d32e6557561b4146d803ba2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-77ab27e8b7b3ac5a903851fbe4378ab0.jpg" align="middle">
</details>




<h2 id="Segment-Any-Class-SAC-Multi-Class-Few-Shot-Semantic-Segmentation-via-Class-Region-Proposals"><a href="#Segment-Any-Class-SAC-Multi-Class-Few-Shot-Semantic-Segmentation-via-Class-Region-Proposals" class="headerlink" title="Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via   Class Region Proposals"></a>Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via   Class Region Proposals</h2><p><strong>Authors:Hussni Mohd Zakir, Eric Tatt Wei Ho</strong></p>
<p>The Segment-Anything Model (SAM) is a vision foundation model for segmentation with a prompt-driven framework. SAM generates class-agnostic masks based on user-specified instance-referring prompts. However, adapting SAM for automated segmentation â€“ where manual input is absent â€“ of specific object classes often requires additional model training. We present Segment Any Class (SAC), a novel, training-free approach that task-adapts SAM for Multi-class segmentation. SAC generates Class-Region Proposals (CRP) on query images which allows us to automatically generate class-aware prompts on probable locations of class instances. CRPs are derived from elementary intra-class and inter-class feature distinctions without any additional training. Our method is versatile, accommodating any N-way K-shot configurations for the multi-class few-shot semantic segmentation (FSS) task. Unlike gradient-learning adaptation of generalist models which risk the loss of generalization and potentially suffer from catastrophic forgetting, SAC solely utilizes automated prompting and achieves superior results over state-of-the-art methods on the COCO-20i benchmark, particularly excelling in high N-way class scenarios. SAC is an interesting demonstration of a prompt-only approach to adapting foundation models for novel tasks with small, limited datasets without any modifications to the foundation model itself. This method offers interesting benefits such as intrinsic immunity to concept or feature loss and rapid, online task adaptation of foundation models. </p>
<blockquote>
<p>Segment-Anything Modelï¼ˆSAMï¼‰æ˜¯ä¸€ä¸ªåŸºäºæç¤ºé©±åŠ¨çš„æ¡†æ¶è¿›è¡Œåˆ†å‰²çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚SAMæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„å®ä¾‹å¼•ç”¨æç¤ºç”Ÿæˆç±»æ— å…³æ©ç ã€‚ç„¶è€Œï¼Œå°†SAMé€‚åº”äºæ²¡æœ‰æ‰‹åŠ¨è¾“å…¥çš„ç‰¹å®šå¯¹è±¡ç±»çš„è‡ªåŠ¨åˆ†å‰²é€šå¸¸éœ€è¦é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†Segment Any Classï¼ˆSACï¼‰è¿™ä¸€æ–°å‹æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¯¹SAMè¿›è¡Œå¤šä»»åŠ¡é€‚åº”çš„å¤šç±»åˆ†å‰²ã€‚SACåœ¨æŸ¥è¯¢å›¾åƒä¸Šç”Ÿæˆç±»åŒºåŸŸææ¡ˆï¼ˆCRPï¼‰ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç±»å®ä¾‹çš„å¯èƒ½ä½ç½®è‡ªåŠ¨äº§ç”Ÿç±»æ„ŸçŸ¥æç¤ºã€‚CRPæ˜¯ä»åŸºæœ¬çš„ç±»å†…å’Œç±»é—´ç‰¹å¾å·®å¼‚ä¸­å¾—å‡ºçš„ï¼Œæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šç”¨æ€§å¼ºï¼Œé€‚åº”ä»»ä½•é’ˆå¯¹å¤šç±»å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰ä»»åŠ¡çš„Nè·¯Kå°„å‡»é…ç½®ã€‚ä¸å¯èƒ½å¯¼è‡´æ³›åŒ–æŸå¤±å’Œæ½œåœ¨ç¾éš¾æ€§é—å¿˜çš„é€šç”¨æ¨¡å‹çš„æ¢¯åº¦å­¦ä¹ é€‚åº”ä¸åŒï¼ŒSACä»…åˆ©ç”¨è‡ªåŠ¨åŒ–æç¤ºï¼Œå¹¶åœ¨COCO-20iåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¼˜äºæœ€æ–°æ–¹æ³•çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜Nè·¯ç±»åˆ«åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚SACæ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ¼”ç¤ºï¼Œå±•ç¤ºäº†ä»…ä½¿ç”¨æç¤ºçš„æ–¹æ³•å¦‚ä½•é€‚åº”æ–°å‹ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨å°ä¸”æœ‰é™çš„æ•°æ®é›†ï¼Œè€Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹æœ¬èº«è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚è¿™ç§æ–¹æ³•æä¾›äº†æœ‰è¶£çš„ä¼˜ç‚¹ï¼Œå¦‚å†…åœ¨åœ°é¿å…äº†æ¦‚å¿µæˆ–ç‰¹å¾æŸå¤±å’Œå¿«é€Ÿã€åœ¨çº¿çš„ä»»åŠ¡é€‚åº”åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13774v1">PDF</a> 8 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Segment-Anything Modelï¼ˆSAMï¼‰çš„æ”¹è¿›ç‰ˆæœ¬Segment Any Classï¼ˆSACï¼‰ã€‚SACæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯è‡ªé€‚åº”åœ°å°†SAMç”¨äºå¤šç±»åˆ†å‰²ä»»åŠ¡ã€‚å®ƒé€šè¿‡ç”Ÿæˆç±»åŒºåŸŸææ¡ˆï¼ˆCRPsï¼‰æ¥è‡ªåŠ¨äº§ç”Ÿç±»æ„ŸçŸ¥æç¤ºï¼Œä»è€Œæ— éœ€æ‰‹åŠ¨è¾“å…¥å³å¯å®ç°ç‰¹å®šå¯¹è±¡ç±»çš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚è¯¥æ–¹æ³•å…·æœ‰çµæ´»æ€§ï¼Œé€‚ç”¨äºå¤šç±»åˆ«å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ä»»ä½•Nè·¯Kå°„å‡»é…ç½®ã€‚SACåœ¨COCO-20iåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ–¹æ³•çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜Nè·¯ç±»åˆ«åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Segment Any Class (SAC)æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç”¨äºè‡ªé€‚åº”åœ°å°†Segment-Anything Modelï¼ˆSAMï¼‰åº”ç”¨äºå¤šç±»åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>SACé€šè¿‡ç”Ÿæˆç±»åŒºåŸŸææ¡ˆï¼ˆCRPsï¼‰è‡ªåŠ¨äº§ç”Ÿç±»æ„ŸçŸ¥æç¤ºï¼Œå®ç°ç‰¹å®šå¯¹è±¡ç±»çš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚</li>
<li>CRPsæ˜¯åŸºäºå›¾åƒå†…å’Œå›¾åƒé—´çš„åŸºæœ¬ç‰¹å¾å·®å¼‚å¾—å‡ºçš„ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚</li>
<li>SACæ–¹æ³•å…·æœ‰çµæ´»æ€§ï¼Œå¯é€‚åº”ä»»ä½•Nè·¯Kå°„å‡»é…ç½®çš„å¤šç±»åˆ«å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>SACåœ¨COCO-20iåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜Nè·¯ç±»åˆ«åœºæ™¯ä¸­ã€‚</li>
<li>SACæ–¹æ³•å…·æœ‰å†…åœ¨çš„ä¼˜åŠ¿ï¼Œå¦‚å¯¹æŠ—æ¦‚å¿µæˆ–ç‰¹å¾ä¸¢å¤±çš„å…ç–«æ€§å’ŒåŸºç¡€æ¨¡å‹çš„å¿«é€Ÿåœ¨çº¿ä»»åŠ¡é€‚åº”æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d971b8f1fd31197fc322d5dbcf0fbddf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-19d42f313dc3c9fb2555211abd55ff49.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-719226815bc206ee26208752a81162b5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-641afa3769daf97fd5fc45721f55b89c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c914aa2015da19ab2f59b760fe680218.jpg" align="middle">
</details>




<h2 id="SAG-ViT-A-Scale-Aware-High-Fidelity-Patching-Approach-with-Graph-Attention-for-Vision-Transformers"><a href="#SAG-ViT-A-Scale-Aware-High-Fidelity-Patching-Approach-with-Graph-Attention-for-Vision-Transformers" class="headerlink" title="SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph   Attention for Vision Transformers"></a>SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph   Attention for Vision Transformers</h2><p><strong>Authors:Shravan Venkatraman, Jaskaran Singh Walia, Joe Dhanith P R</strong></p>
<p>Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multiscale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance. Our code and weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/shravan-18/SAG-ViT">https://github.com/shravan-18/SAG-ViT</a> </p>
<blockquote>
<p>å›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¹‹ä¸€ï¼Œæ¨¡å‹é€šè¿‡åˆ†æå›¾åƒå°†å…¶åˆ†ç±»ä¸ºç‰¹å®šæ ‡ç­¾ã€‚è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰é€šè¿‡åˆ©ç”¨è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰å›¾åƒè¡¥ä¸ä¹‹é—´çš„å¤æ‚æ¨¡å¼å’Œé•¿ç¨‹å…³ç³»ï¼Œä»è€Œæ”¹è¿›äº†è¿™ä¸€ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒViTçš„å…³é”®æŒ‘æˆ˜åœ¨äºæœ‰æ•ˆåœ°èå…¥å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼Œè¿™æ˜¯CNNçš„å›ºæœ‰å±æ€§ï¼Œå¾—ç›Šäºå…¶åˆ†å±‚ç»“æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Scale-Aware Graph Attention Vision Transformerï¼ˆSAG-ViTï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡èåˆå¤šå°ºåº¦ç‰¹å¾æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ä½¿ç”¨EfficientNetä½œä¸ºéª¨å¹²ç½‘ï¼Œè¯¥æ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œå°†å…¶åˆ’åˆ†ä¸ºè¡¥ä¸ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚è¿™äº›è¡¥ä¸åŸºäºç©ºé—´ç‰¹å¾å’Œç‰¹å¾ç›¸ä¼¼æ€§ç»„ç»‡æˆä¸€ä¸ªå›¾ï¼Œé€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰å¯¹èŠ‚ç‚¹åµŒå…¥è¿›è¡Œç²¾ç‚¼ã€‚æœ€åï¼Œå˜å‹å™¨ç¼–ç å™¨æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»å’Œå¤æ‚äº¤äº’ã€‚SAG-ViTåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜å›¾åƒåˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shravan-18/SAG-ViT%E5%85%AC%E5%BC%BA%E8%8E%B7%E3%80%82">https://github.com/shravan-18/SAG-ViTå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09420v2">PDF</a> 10 pages, 4 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹åˆ†æå›¾åƒå¹¶å°†å…¶åˆ†ç±»ä¸ºç‰¹å®šæ ‡ç­¾æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„ä¸€ç§ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åä¸ºScale-Aware Graph Attention Vision Transformerï¼ˆSAG-ViTï¼‰çš„æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•´åˆå¤šå°ºåº¦ç‰¹å¾æ¥æ”¹è¿›è¿™ä¸€ä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨EfficientNetä½œä¸ºéª¨å¹²ç½‘ï¼Œæ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾æ˜ å°„å¹¶åˆ’åˆ†ä¸ºåŒºå—ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚åŸºäºç©ºé—´ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œè¿™äº›åŒºå—è¢«ç»„ç»‡æˆå›¾ç»“æ„ï¼Œå†é€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰ä¼˜åŒ–èŠ‚ç‚¹åµŒå…¥ã€‚æœ€åï¼ŒTransformerç¼–ç å™¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œå¤æ‚äº¤äº’ä½œç”¨ã€‚SAG-ViTåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶åœ¨æé«˜å›¾åƒåˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViT)åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ”¹å–„å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºæ˜¯ViTé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œè€ŒCNNé€šè¿‡å…¶å±‚æ¬¡ç»“æ„å›ºæœ‰åœ°åŒ…å«æ­¤ç‰¹æ€§ã€‚</li>
<li>SAG-ViTæ¡†æ¶è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œé€šè¿‡æ•´åˆå¤šå°ºåº¦ç‰¹å¾æé«˜äº†å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>EfficientNetä½œä¸ºéª¨å¹²ç½‘ç”¨äºæå–å¤šå°ºåº¦ç‰¹å¾æ˜ å°„ï¼Œåˆ’åˆ†åŒºå—ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰ç”¨äºä¼˜åŒ–åŸºäºç©ºé—´ç‰¹å¾ç›¸ä¼¼æ€§çš„åŒºå—èŠ‚ç‚¹åµŒå…¥ã€‚</li>
<li>Transformerç¼–ç å™¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œå¤æ‚äº¤äº’ä½œç”¨ã€‚</li>
<li>SAG-ViTåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8fb6033fd9f0d6d2b6fb77c021c73886.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-adf3ae7d6c3114687cbe0c207e504746.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1f64ae851224f57cef4251b9b722ab40.jpg" align="middle">
</details>




<h2 id="GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection"></a>GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection</h2><p><strong>Authors:Jiyul Ham, Yonggon Jung, Jun-Geol Baek</strong></p>
<p>Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting anomalous patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP">https://github.com/YUL-git/GlocalCLIP</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰å¯¹äºåœ¨æ— éœ€è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹æ£€æµ‹ç›®æ ‡æ•°æ®é›†ä¸­çš„å¼‚å¸¸æ¨¡å¼è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡åŸŸä¸è®­ç»ƒæ•°æ®ä¹‹é—´å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–ç”±äºè®¿é—®å—é™è€Œå¯¼è‡´æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚å°½ç®¡æœ€è¿‘é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨ç±»åˆ«è¯­ä¹‰çš„å­¦ä¹ ï¼Œè¿™ä½¿å¾—å®ƒä»¬ç›´æ¥åº”ç”¨äºZSADå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºäº†GlocalCLIPæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç‹¬ç‰¹åœ°åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºå¹¶è¿›è¡Œè”åˆä¼˜åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä½¿å¾—ä¸å¯¹è±¡æ— å…³çš„å±€éƒ¨è¯­ä¹‰æç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰é€šç”¨æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ï¼Œè€Œæ— éœ€ä¾èµ–å›¾åƒä¸­çš„ç‰¹å®šå¯¹è±¡ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´æ¥å®Œå–„æ–‡æœ¬æç¤ºï¼Œä»¥å®ç°æ›´ç²¾ç¡®çš„è°ƒæ•´ã€‚åœ¨è§†è§‰ç¼–ç å™¨æ–¹é¢ï¼Œæˆ‘ä»¬åº”ç”¨V-Væ³¨æ„åŠ›å±‚æ¥æ•æ‰è¯¦ç»†çš„å±€éƒ¨å›¾åƒç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†å±€éƒ¨å¯¹æ¯”å­¦ä¹ ï¼Œä»¥æé«˜å…¨å±€å’Œå±€éƒ¨æç¤ºçš„äº’è¡¥å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°æ£€æµ‹å„ç§é¢†åŸŸçš„å¼‚å¸¸æ¨¡å¼ã€‚GlocalCLIPåœ¨ZSADä¸­çš„æ³›åŒ–æ€§èƒ½åœ¨æ¥è‡ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„15ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YUL-git/GlocalCLIPä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06071v3">PDF</a> 29 pages, 36 figures</p>
<p><strong>Summary</strong></p>
<p>GlocalCLIPæ–¹æ³•åˆ©ç”¨å…¨å±€å’Œå±€éƒ¨æç¤ºçš„åˆ†ç¦»å’Œè”åˆä¼˜åŒ–ï¼Œå®ç°é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ã€‚é€šè¿‡æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´å’ŒV-Væ³¨æ„åŠ›å±‚ï¼Œæé«˜æ–‡æœ¬ç¼–ç å™¨å’Œè§†è§‰ç¼–ç å™¨æ€§èƒ½ã€‚å¼•å…¥å…¨å±€å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆæ£€æµ‹ä¸åŒé¢†åŸŸçš„å¼‚å¸¸æ¨¡å¼ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„å·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GlocalCLIPè§£å†³äº†é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“ç›®æ ‡åŸŸä¸è®­ç»ƒæ•°æ®å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–æ•°æ®ç¨€ç¼ºæ—¶ã€‚</li>
<li>æ–¹æ³•é€šè¿‡åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºï¼Œå¹¶å¯¹å…¶è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥æ•æ‰æ­£å¸¸çš„å’Œå¼‚å¸¸çš„å›¾æ¡ˆã€‚</li>
<li>æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´å’ŒV-Væ³¨æ„åŠ›å±‚çš„ä½¿ç”¨ï¼Œæé«˜äº†æ–‡æœ¬ç¼–ç å™¨å’Œè§†è§‰ç¼–ç å™¨çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥å…¨å±€å¯¹æ¯”å­¦ä¹ ï¼Œä¿ƒè¿›å…¨å±€å’Œå±€éƒ¨æç¤ºçš„äº’è¡¥å­¦ä¹ ã€‚</li>
<li>GlocalCLIPåœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨å¯¹è±¡è¯­ä¹‰çš„å­¦ä¹ ï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°æ•æ‰ä¸€èˆ¬çš„æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26e62baf3d484faf017e96a0933d2b89.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1125ab6e3ab5117d2b7066fa7f664de1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2f8c17425f03137645e1037af7d759b4.jpg" align="middle">
</details>




<h2 id="Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy"><a href="#Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy" class="headerlink" title="Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy"></a>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy</h2><p><strong>Authors:Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</strong></p>
<p>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large GIE dataset, called EndoExtend24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of GIE medical image diagnosis. Specifically, the EVA-02 model, which is based on the ViT architecture and trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762 and a balanced accuracy of 37.1% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched EndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics. </p>
<blockquote>
<p>è§†é¢‘èƒ¶å›Šå†…é•œæŠ€æœ¯é€šè¿‡æä¾›ä¸€ç§æ— åˆ›æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰èƒƒè‚ é“çš„è¯¦ç»†å›¾åƒï¼Œä»è€Œå®ç°äº†èƒƒè‚ é“å†…çª¥é•œï¼ˆGIEï¼‰è¯Šæ–­çš„å˜é©ï¼Œä½¿æ—©æœŸç–¾ç—…æ£€æµ‹æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå…¶æ½œåŠ›å—é™äºæˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œæˆåƒè¿‡ç¨‹å¯èƒ½éœ€è¦6-8å°æ—¶ï¼Œå¹¶ä¸”ç»å¸¸äº§ç”Ÿé«˜è¾¾100ä¸‡å¼ å›¾åƒï¼Œå› æ­¤éœ€è¦è‡ªåŠ¨åŒ–åˆ†æã€‚æ­¤å¤–ï¼Œè¿™äº›å›¾åƒçš„å˜å¼‚æ€§ï¼ŒåŠ ä¸Šéœ€è¦ä¸“å®¶æ ‡æ³¨ä»¥åŠå¤§è§„æ¨¡ã€é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œé™åˆ¶äº†å½“å‰åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹å¤§å‹GIEæ•°æ®é›†EndoExtend24ï¼Œå®ƒæ˜¯é€šè¿‡åˆå¹¶åä¸ªç°æœ‰çš„å…¬å…±å’Œç§æœ‰æ•°æ®é›†åˆ›å»ºçš„ï¼Œç¡®ä¿äº†è·¨åˆ†å‰²çš„æ‚£è€…å®Œæ•´æ€§ã€‚EndoExtend24åŒ…å«è¶…è¿‡22ä¸‡å¼ æ ‡æ³¨å›¾åƒï¼Œä»¥åŠåŠ¨æ€ç±»æ˜ å°„ï¼Œå¯ä»¥åœ¨ä¸åŒæ ‡ç­¾ç²’åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œç»Ÿä¸€è®­ç»ƒï¼Œæ”¯æŒå¤šè¾¾123ç§ä¸åŒçš„ç—…ç†å‘ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨åŸºäºè‡ªç›‘ç£çš„é€šç”¨å›¾åƒæ•°æ®åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼ˆåŸºç¡€æ¨¡å‹ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒEVA-02æ¨¡å‹åŸºäºViTæ¶æ„ï¼Œåœ¨ImageNet-22kä¸Šé‡‡ç”¨æ©ç å›¾åƒå»ºæ¨¡ï¼ˆä½¿ç”¨EVA-CLIPä½œä¸ºMIMæ•™å¸ˆï¼‰è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨EndoExtend24æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥å®ç°åŸŸé€‚åº”ï¼Œå¹¶æœ€ç»ˆåœ¨Capsule Endoscopy 2024 Challengeæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œåœ¨Capsule Endoscopy 2024æŒ‘æˆ˜ä¸­è·å¾—äº†ç¬¬ä¸‰åã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å®è§‚AUCä¸º0.762å’Œå¹³è¡¡ç²¾åº¦ä¸º37.1%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æˆ‘ä»¬åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’Œä¸°å¯Œçš„EndoExtend24æ•°æ®é›†åœ¨æ¨è¿›èƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21302v4">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…çª¥é•œï¼ˆGIEï¼‰è¯Šæ–­æä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„å›¾åƒæ•æ‰æ–¹æ³•ï¼Œæœ‰åŠ©äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚ç„¶è€Œï¼Œç”±äºæˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œä»¥åŠå›¾åƒä¹‹é—´çš„å·®å¼‚å’Œç¼ºä¹å¤§å‹é«˜è´¨é‡æ ‡ç­¾æ•°æ®é›†ï¼Œé™åˆ¶äº†å…¶æ½œåŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºEndoExtend24çš„å¤§å‹GIEæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡22.6ä¸‡å¼ æ ‡è®°å›¾åƒï¼Œå¹¶æ”¯æŒç»Ÿä¸€çš„è·¨æ•°æ®é›†è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨åœ¨é€šç”¨å›¾åƒæ•°æ®ä¸Šè‡ªæˆ‘ç›‘ç£è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œä»»åŠ¡é€‚åº”ã€‚æˆ‘ä»¬çš„EVA-02æ¨¡å‹åŸºäºViTæ¶æ„ï¼Œåœ¨ImageNet-22kä¸Šè¿›è¡Œé¢„è®­ç»ƒå¹¶é€‚åº”EndoExtend24æ•°æ®é›†ï¼Œæœ€ååœ¨Capsule Endoscopy 2024æŒ‘æˆ˜æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’Œä¸°å¯Œå¤šæ ·çš„EndoExtend24æ•°æ®é›†åœ¨æ¨è¿›èƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æä¾›äº†éä¾µå…¥æ€§çš„å›¾åƒæ•æ‰æ–¹æ³•ï¼Œæœ‰åŠ©äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚</li>
<li>æˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œå¯¹è‡ªåŠ¨åˆ†ææå‡ºäº†éœ€æ±‚ã€‚</li>
<li>åˆ›å»ºäº†åä¸ºEndoExtend24çš„å¤§å‹GIEæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡22.6ä¸‡å¼ æ ‡è®°å›¾åƒï¼Œå¹¶æ”¯æŒç»Ÿä¸€çš„è·¨æ•°æ®é›†è®­ç»ƒã€‚</li>
<li>å¼•å…¥åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨é€šç”¨å›¾åƒæ•°æ®ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä»»åŠ¡é€‚åº”ã€‚</li>
<li>EVA-02æ¨¡å‹åŸºäºViTæ¶æ„è¿›è¡Œå¼€å‘ï¼Œè¡¨ç°å¼ºå¤§æ€§èƒ½ï¼Œåœ¨Capsule Endoscopy 2024æŒ‘æˆ˜ä¸­è·å¾—äº†ç¬¬ä¸‰åçš„å¥½æˆç»©ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹åœ¨EndoExtend24æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf9713e7ffc155a83738fb6388d5f539.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7c49edf263644ff07b1db7da8210619a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8f4db2dad3e28af984765d3201e9f3f.jpg" align="middle">
</details>




<h2 id="Croc-Pretraining-Large-Multimodal-Models-with-Cross-Modal-Comprehension"><a href="#Croc-Pretraining-Large-Multimodal-Models-with-Cross-Modal-Comprehension" class="headerlink" title="Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"></a>Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension</h2><p><strong>Authors:Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng</strong></p>
<p>Recent advances in Large Language Models (LLMs) have catalyzed the development of Large Multimodal Models (LMMs). However, existing research primarily focuses on tuning language and image instructions, ignoring the critical pretraining phase where models learn to process textual and visual modalities jointly. In this paper, we propose a new pretraining paradigm for LMMs to enhance the visual comprehension capabilities of LLMs by introducing a novel cross-modal comprehension stage. Specifically, we design a dynamically learnable prompt token pool and employ the Hungarian algorithm to replace part of the original visual tokens with the most relevant prompt tokens. Then, we conceptualize visual tokens as analogous to a â€œforeign languageâ€ for the LLMs and propose a mixed attention mechanism with bidirectional visual attention and unidirectional textual attention to comprehensively enhance the understanding of visual tokens. Meanwhile, we integrate a detailed caption generation task, leveraging rich descriptions to further facilitate LLMs in understanding visual semantic information. After pretraining on 1.5 million publicly accessible data, we present a new foundation model called Croc. Experimental results demonstrate that Croc achieves new state-of-the-art performance on massive vision-language benchmarks. To support reproducibility and facilitate further research, we release the training code and pre-trained model weights at <a target="_blank" rel="noopener" href="https://github.com/deepglint/Croc">https://github.com/deepglint/Croc</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è°ƒæ•´è¯­è¨€å’Œå›¾åƒæŒ‡ä»¤ä¸Šï¼Œå¿½ç•¥äº†æ¨¡å‹å­¦ä¹ å¤„ç†æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„è”åˆå…³é”®é¢„è®­ç»ƒé˜¶æ®µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹LMMçš„æ–°å‹é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼•å…¥æ–°å‹è·¨æ¨¡æ€ç†è§£é˜¶æ®µï¼Œå¢å¼ºLMMçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯åŠ¨æ€å­¦ä¹ çš„æç¤ºä»¤ç‰Œæ± ï¼Œå¹¶ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•å°†æœ€ç›¸å…³çš„æç¤ºä»¤ç‰Œæ›¿æ¢éƒ¨åˆ†åŸå§‹è§†è§‰ä»¤ç‰Œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è§†è§‰ä»¤ç‰Œæ¦‚å¿µåŒ–ä¸ºå¯¹LLMè€Œè¨€ç±»ä¼¼äºâ€œå¤–è¯­â€ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬åŒå‘è§†è§‰æ³¨æ„åŠ›å’Œå•å‘æ–‡æœ¬æ³¨æ„åŠ›ï¼Œä»¥å…¨é¢å¢å¼ºå¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ•´åˆäº†è¯¦ç»†çš„æ ‡é¢˜ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨ä¸°å¯Œçš„æè¿°æ¥è¿›ä¸€æ­¥å¸®åŠ©LLMç†è§£è§†è§‰è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨150ä¸‡ä¸ªå¯å…¬å¼€è®¿é—®çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ç§æ–°å‹åŸºç¡€æ¨¡å‹â€”â€”åä¸ºCrocã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrocåœ¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°å…ˆè¿›æ€§èƒ½ã€‚ä¸ºäº†æ”¯æŒå¤ç°æ€§å’Œä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepglint/Croc%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/deepglint/Crocä¸Šå‘å¸ƒäº†è®­ç»ƒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14332v2">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼•å…¥è·¨æ¨¡æ€ç†è§£é˜¶æ®µï¼Œå¢å¼ºLLMså¯¹è§†è§‰å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚è®¾è®¡åŠ¨æ€å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œæ± ï¼Œå¹¶é‡‡ç”¨åŒˆç‰™åˆ©ç®—æ³•æ›¿æ¢åŸå§‹è§†è§‰ä»¤ç‰Œä¸­æœ€ç›¸å…³çš„æç¤ºä»¤ç‰Œã€‚æå‡ºæ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºå¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£ã€‚é›†æˆè¯¦ç»†çš„å­—å¹•ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨ä¸°å¯Œçš„æè¿°æ¥è¿›ä¸€æ­¥ä¿ƒè¿›LLMså¯¹è§†è§‰è¯­ä¹‰ä¿¡æ¯çš„ç†è§£ã€‚ç»è¿‡åœ¨å…¬å¼€æ•°æ®ä¸Š150ä¸‡æ•°æ®çš„é¢„è®­ç»ƒåï¼Œæ¨å‡ºäº†åä¸ºCrocçš„æ–°åŸºç¡€æ¨¡å‹ï¼Œåœ¨å¤§å‹è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¯¹è§†è§‰å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†åŠ¨æ€å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œæ± å¹¶ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ›¿æ¢åŸå§‹è§†è§‰ä»¤ç‰Œï¼Œä»¥æå‡æ¨¡å‹çš„è·¨æ¨¡æ€ç†è§£æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­åŒ…æ‹¬åŒå‘è§†è§‰æ³¨æ„åŠ›å’Œå•å‘æ–‡æœ¬æ³¨æ„åŠ›ï¼Œä»¥å…¨é¢å¢å¼ºå¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£ã€‚</li>
<li>é€šè¿‡é›†æˆè¯¦ç»†çš„å­—å¹•ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨ä¸°å¯Œçš„æè¿°ä¿ƒè¿›LLMså¯¹è§†è§‰è¯­ä¹‰ä¿¡æ¯çš„ç†è§£ã€‚</li>
<li>åœ¨å¤§é‡å…¬å¼€æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶æ¨å‡ºåä¸ºCrocçš„æ–°åŸºç¡€æ¨¡å‹ã€‚</li>
<li>Crocæ¨¡å‹åœ¨å¤šä¸ªå¤§è§„æ¨¡è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4fac2c5023429cd38d7cfd08b23d572d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-83d0b44eaeba4e5120d67b0acb18ed3a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-510283aee83c3ca612cfb20cdd7a5698.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6c6e1965293b5e4d1fdb140adac90060.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3b0d48f4ce6c1df054446abdf6f08b6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f2b356769bcd341221808c5d6c482e2.jpg" align="middle">
</details>




<h2 id="Local-to-Global-Self-Supervised-Representation-Learning-for-Diabetic-Retinopathy-Grading"><a href="#Local-to-Global-Self-Supervised-Representation-Learning-for-Diabetic-Retinopathy-Grading" class="headerlink" title="Local-to-Global Self-Supervised Representation Learning for Diabetic   Retinopathy Grading"></a>Local-to-Global Self-Supervised Representation Learning for Diabetic   Retinopathy Grading</h2><p><strong>Authors:Mostafa Hajighasemlou, Samad Sheikhaei, Hamid Soltanian-Zadeh</strong></p>
<p>Artificial intelligence algorithms have demonstrated their image classification and segmentation ability in the past decade. However, artificial intelligence algorithms perform less for actual clinical data than those used for simulations. This research aims to present a novel hybrid learning model using self-supervised learning and knowledge distillation, which can achieve sufficient generalization and robustness. The self-attention mechanism and tokens employed in ViT, besides the local-to-global learning approach used in the hybrid model, enable the proposed algorithm to extract a high-dimensional and high-quality feature space from images. To demonstrate the proposed neural networkâ€™s capability in classifying and extracting feature spaces from medical images, we use it on a dataset of Diabetic Retinopathy images, specifically the EyePACS dataset. This dataset is more complex structurally and challenging regarding damaged areas than other medical images. For the first time in this study, self-supervised learning and knowledge distillation are used to classify this dataset. In our algorithm, for the first time among all self-supervised learning and knowledge distillation models, the test dataset is 50% larger than the training dataset. Unlike many studies, we have not removed any images from the dataset. Finally, our algorithm achieved an accuracy of 79.1% in the linear classifier and 74.36% in the k-NN algorithm for multiclass classification. Compared to a similar state-of-the-art model, our results achieved higher accuracy and more effective representation spaces. </p>
<blockquote>
<p>åœ¨è¿‡å»çš„åå¹´ä¸­ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•å·²ç»è¯æ˜äº†å®ƒä»¬åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸æ¨¡æ‹Ÿæ•°æ®ç›¸æ¯”ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•åœ¨å®é™…çš„ä¸´åºŠæ•°æ®ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°å‹çš„æ··åˆå­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦ï¼Œå¯å®ç°è¶³å¤Ÿçš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚ViTä¸­ä½¿ç”¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä»¤ç‰Œï¼Œä»¥åŠæ··åˆæ¨¡å‹ä¸­é‡‡ç”¨çš„ä»å±€éƒ¨åˆ°å…¨å±€çš„å­¦ä¹ æ–¹æ³•ï¼Œä½¿å¾—æ‰€æå‡ºç®—æ³•èƒ½å¤Ÿä»å›¾åƒä¸­æå–é«˜ç»´é«˜è´¨é‡çš„ç‰¹å¾ç©ºé—´ã€‚ä¸ºäº†å±•ç¤ºæ‰€æå‡ºç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»å’Œç‰¹å¾ç©ºé—´æå–æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜å›¾åƒçš„æ•°æ®é›†ä¸Šä½¿ç”¨äº†å®ƒï¼Œç‰¹åˆ«æ˜¯EyePACSæ•°æ®é›†ã€‚ä¸å…¶ä»–åŒ»å­¦å›¾åƒç›¸æ¯”ï¼Œæ­¤æ•°æ®é›†åœ¨ç»“æ„ä¸Šæ›´ä¸ºå¤æ‚ï¼Œå¹¶ä¸”åœ¨æŸä¼¤åŒºåŸŸæ–¹é¢æ›´å…·æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œé¦–æ¬¡ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦å¯¹æ­¤æ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚åœ¨æˆ‘ä»¬çš„ç®—æ³•ä¸­ï¼Œæµ‹è¯•æ•°æ®é›†é¦–æ¬¡åœ¨æ‰€æœ‰è‡ªç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦æ¨¡å‹ä¸­æ¯”è®­ç»ƒæ•°æ®é›†å¤§50%ã€‚ä¸å…¶ä»–è®¸å¤šç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬æ²¡æœ‰ä»æ•°æ®é›†ä¸­åˆ é™¤ä»»ä½•å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨çº¿æ€§åˆ†ç±»å™¨ä¸­è¾¾åˆ°äº†79.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨k-NNç®—æ³•çš„å¤šç±»åˆ†ç±»ä¸­è¾¾åˆ°äº†74.36%çš„å‡†ç¡®ç‡ã€‚ä¸ç±»ä¼¼çš„æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç»“æœå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´æœ‰æ•ˆçš„è¡¨ç¤ºç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆè‡ªç›‘ç£å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦çš„æ–°å‹æ··åˆå­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜åœ¨çœŸå®ä¸´åºŠæ•°æ®ä¸Šçš„å›¾åƒåˆ†ç±»ä¸åˆ†å‰²èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä½¿ç”¨ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä»¤ç‰Œï¼Œä»¥åŠæ··åˆæ¨¡å‹çš„å±€éƒ¨åˆ°å…¨å±€å­¦ä¹ æ–¹æ³•ï¼Œä»å›¾åƒä¸­æå–é«˜ç»´é«˜è´¨é‡çš„ç‰¹å¾ç©ºé—´ã€‚åœ¨çœ¼ç—…æ•°æ®é›†EyePACSä¸Šè¿›è¡Œå®éªŒï¼Œæœªç§»é™¤ä»»ä½•å›¾åƒï¼Œé¦–æ¬¡ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦å¯¹æ­¤æ•°æ®é›†è¿›è¡Œåˆ†ç±»ï¼Œä¸”æµ‹è¯•æ•°æ®é›†å¤§å°æ˜¯è®­ç»ƒæ•°æ®é›†çš„50%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•åœ¨çº¿æ€§åˆ†ç±»å™¨ä¸Šè¾¾åˆ°äº†79.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨k-NNç®—æ³•ä¸Šçš„å¤šåˆ†ç±»å‡†ç¡®ç‡ä¸º74.36%ï¼Œç›¸è¾ƒäºç±»ä¼¼çš„å‰æ²¿æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´æœ‰æ•ˆçš„è¡¨ç¤ºç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹æ··åˆå­¦ä¹ æ¨¡å‹ç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦ï¼Œæ—¨åœ¨æé«˜åœ¨çœŸå®ä¸´åºŠæ•°æ®ä¸Šçš„å›¾åƒåˆ†ç±»ä¸åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä»¤ç‰Œè¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>é¦–æ¬¡ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦å¯¹EyePACSæ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>å®éªŒåœ¨è¾ƒå¤§çš„æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œä¿ç•™äº†æ‰€æœ‰å›¾åƒï¼Œæœªè¿›è¡Œä»»ä½•ç§»é™¤ã€‚</li>
<li>ç®—æ³•åœ¨çº¿æ€§åˆ†ç±»å™¨ä¸Šè¾¾åˆ°äº†79.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨k-NNç®—æ³•ä¸Šçš„å¤šåˆ†ç±»å‡†ç¡®ç‡ä¸º74.36%ã€‚</li>
<li>ä¸ç±»ä¼¼çš„å‰æ²¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç®—æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´æœ‰æ•ˆçš„è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†å±€éƒ¨åˆ°å…¨å±€çš„å­¦ä¹ æ–¹æ³•åœ¨ç‰¹å¾æå–ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-71166837b37799c2254baa0b71b8f5ca.jpg" align="middle">
</details>




<h2 id="Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches"><a href="#Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches" class="headerlink" title="Patch Ranking: Efficient CLIP by Learning to Rank Local Patches"></a>Patch Ranking: Efficient CLIP by Learning to Rank Local Patches</h2><p><strong>Authors:Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado</strong></p>
<p>Contrastive image-text pre-trained models such as CLIP have shown remarkable adaptability to downstream tasks. However, they face challenges due to the high computational requirements of the Vision Transformer (ViT) backbone. Current strategies to boost ViT efficiency focus on pruning patch tokens but fall short in addressing the multimodal nature of CLIP and identifying the optimal subset of tokens for maximum performance. To address this, we propose greedy search methods to establish a â€œGolden Rankingâ€ and introduce a lightweight predictor specifically trained to approximate this Ranking. To compensate for any performance degradation resulting from token pruning, we incorporate learnable visual tokens that aid in restoring and potentially enhancing the modelâ€™s performance. Our work presents a comprehensive and systematic investigation of pruning tokens within the ViT backbone of CLIP models. Through our framework, we successfully reduced 40% of patch tokens in CLIPâ€™s ViT while only suffering a minimal average accuracy loss of 0.3 across seven datasets. Our study lays the groundwork for building more computationally efficient multimodal models without sacrificing their performance, addressing a key challenge in the application of advanced vision-language models. </p>
<blockquote>
<p>å¯¹æ¯”å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚CLIPï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°å‡ºæ˜¾è‘—çš„é€‚åº”æ€§ã€‚ç„¶è€Œï¼Œç”±äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸»å¹²çš„é«˜è®¡ç®—è¦æ±‚ï¼Œå®ƒä»¬é¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰æé«˜ViTæ•ˆç‡çš„ç­–ç•¥ä¸»è¦é›†ä¸­åœ¨ä¿®å‰ªè¡¥ä¸ä»¤ç‰Œä¸Šï¼Œä½†æœªèƒ½å……åˆ†è§£å†³CLIPçš„å¤šæ¨¡å¼æ€§è´¨ï¼Œå¹¶ç¡®å®šè·å¾—æœ€ä½³æ€§èƒ½ä»¤ç‰Œçš„æœ€ä¼˜å­é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè´ªå¿ƒæœç´¢æ–¹æ³•æ¥å»ºç«‹â€œé‡‘ç‰Œæ’åâ€ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªä¸“é—¨è®­ç»ƒçš„è½»é‡çº§é¢„æµ‹å™¨æ¥è¿‘ä¼¼è¿™ä¸ªæ’åã€‚ä¸ºäº†å¼¥è¡¥å› ä»¤ç‰Œä¿®å‰ªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å­¦ä¹ çš„è§†è§‰ä»¤ç‰Œï¼Œæœ‰åŠ©äºæ¢å¤å¹¶å¯èƒ½æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯¹CLIPæ¨¡å‹çš„ViTä¸»å¹²ä¸­çš„ä»¤ç‰Œä¿®å‰ªè¿›è¡Œäº†å…¨é¢ç³»ç»Ÿçš„ç ”ç©¶ã€‚é€šè¿‡æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬æˆåŠŸåœ°åœ¨CLIPçš„ViTä¸­å‡å°‘äº†40%çš„è¡¥ä¸ä»¤ç‰Œï¼ŒåŒæ—¶åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šåªé­å—äº†æœ€å°çš„å¹³å‡ç²¾åº¦æŸå¤±0.3ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„å¤šæ¨¡å¼æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œä¸ä¼šç‰ºç‰²å…¶æ€§èƒ½ï¼Œè§£å†³äº†å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14607v2">PDF</a> Accepted by WACV 2025</p>
<p><strong>Summary</strong></p>
<p>CLIPç­‰å¯¹æ¯”å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ï¼Œä½†ç”±äºVision Transformerï¼ˆViTï¼‰çš„é«˜è®¡ç®—éœ€æ±‚è€Œé¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰æå‡ViTæ•ˆç‡çš„ç­–ç•¥ä¸»è¦èšç„¦äºä¿®å‰ªpatch tokensï¼Œä½†æœªèƒ½å……åˆ†è§£å†³CLIPçš„å¤šæ¨¡æ€ç‰¹æ€§ï¼Œéš¾ä»¥è¯†åˆ«å‡ºæœ€ä½³tokenå­é›†ä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºè´ªå¿ƒæœç´¢æ–¹æ³•å»ºç«‹â€œGolden Rankingâ€ï¼Œå¹¶å¼•å…¥è½»é‡çº§é¢„æµ‹å™¨è¿›è¡Œè¿‘ä¼¼è®­ç»ƒã€‚ä¸ºå¼¥è¡¥å› tokenä¿®å‰ªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œç ”ç©¶èå…¥å¯å­¦ä¹ è§†è§‰tokenä»¥åŠ©åŠ›æ¢å¤å¹¶å¯èƒ½æå‡æ¨¡å‹æ€§èƒ½ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†CLIPæ¨¡å‹ä¸­ViTçš„tokenä¿®å‰ªé—®é¢˜ã€‚é€šè¿‡æ¡†æ¶æˆåŠŸå‡å°‘CLIPä¸­ViTçš„40% patch tokensï¼Œåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡ç²¾åº¦æŸå¤±ä»…ä¸º0.3%ã€‚ç ”ç©¶ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹å¥ å®šåŸºç¡€ï¼Œè§£å†³äº†å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§é€‚åº”æ€§ï¼Œä½†é¢ä¸´é«˜è®¡ç®—éœ€æ±‚çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ViTæ•ˆç‡æå‡ç­–ç•¥ä¸»è¦èšç„¦äºä¿®å‰ªpatch tokensï¼Œä½†æœªèƒ½è§£å†³å¤šæ¨¡æ€ç‰¹æ€§é—®é¢˜ã€‚</li>
<li>å¼•å…¥è´ªå¿ƒæœç´¢æ–¹æ³•å»ºç«‹â€œGolden Rankingâ€ï¼Œä»¥ä¼˜åŒ–tokené€‰æ‹©ã€‚</li>
<li>å¼•å…¥è½»é‡çº§é¢„æµ‹å™¨è¿›è¡Œè¿‘ä¼¼è®­ç»ƒï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>é€šè¿‡èå…¥å¯å­¦ä¹ è§†è§‰tokenï¼Œå¼¥è¡¥å› tokenä¿®å‰ªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ç ”ç©¶æˆåŠŸå‡å°‘CLIPä¸­ViTçš„40% patch tokensï¼Œå¹³å‡ç²¾åº¦æŸå¤±è¾ƒå°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-664c7eb1e38c884b8462b58f14c9bb58.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4b50734fdd8a61a05103e07a48cddc85.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-acc868f79e316a0a2a222f988321e1e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f6fc089ef5f80ecfc84c97cba9322acd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-25436ad5d9b413ff1e62690a36714432.jpg" align="middle">
</details>




<h2 id="Generalizing-Deepfake-Video-Detection-with-Plug-and-Play-Video-Level-Blending-and-Spatiotemporal-Adapter-Tuning"><a href="#Generalizing-Deepfake-Video-Detection-with-Plug-and-Play-Video-Level-Blending-and-Spatiotemporal-Adapter-Tuning" class="headerlink" title="Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level   Blending and Spatiotemporal Adapter Tuning"></a>Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level   Blending and Spatiotemporal Adapter Tuning</h2><p><strong>Authors:Zhiyuan Yan, Yandan Zhao, Shen Chen, Mingyi Guo, Xinghe Fu, Taiping Yao, Shouhong Ding, Li Yuan</strong></p>
<p>Three key challenges hinder the development of current deepfake video detection: (1) Temporal features can be complex and diverse: how can we identify general temporal artifacts to enhance model generalization? (2) Spatiotemporal models often lean heavily on one type of artifact and ignore the other: how can we ensure balanced learning from both? (3) Videos are naturally resource-intensive: how can we tackle efficiency without compromising accuracy? This paper attempts to tackle the three challenges jointly. First, inspired by the notable generality of using image-level blending data for image forgery detection, we investigate whether and how video-level blending can be effective in video. We then perform a thorough analysis and identify a previously underexplored temporal forgery artifact: Facial Feature Drift (FFD), which commonly exists across different forgeries. To reproduce FFD, we then propose a novel Video-level Blending data (VB), where VB is implemented by blending the original image and its warped version frame-by-frame, serving as a hard negative sample to mine more general artifacts. Second, we carefully design a lightweight Spatiotemporal Adapter (StA) to equip a pretrained image model (both ViTs and CNNs) with the ability to capture both spatial and temporal features jointly and efficiently. StA is designed with two-stream 3D-Conv with varying kernel sizes, allowing it to process spatial and temporal features separately. Extensive experiments validate the effectiveness of the proposed methods; and show our approach can generalize well to previously unseen forgery videos, even the latest generation methods. </p>
<blockquote>
<p>å½“å‰æ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹çš„å‘å±•é¢ä¸´ä¸‰å¤§å…³é”®æŒ‘æˆ˜ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰æ—¶åºç‰¹å¾å¯èƒ½å¤æ‚ä¸”å¤šæ ·ï¼šæˆ‘ä»¬å¦‚ä½•è¯†åˆ«é€šç”¨æ—¶åºä¼ªè¿¹ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿ</p>
<p>ï¼ˆ2ï¼‰æ—¶ç©ºæ¨¡å‹é€šå¸¸åå‘äºä¸€ç§ä¼ªè¿¹è€Œå¿½è§†å¦ä¸€ç§ï¼šæˆ‘ä»¬å¦‚ä½•ç¡®ä¿ä»ä¸¤è€…ä¸­éƒ½èƒ½å®ç°å‡è¡¡å­¦ä¹ ï¼Ÿ</p>
<p>ï¼ˆ3ï¼‰è§†é¢‘æœ¬èº«æ˜¯èµ„æºå¯†é›†å‹çš„ï¼šæˆ‘ä»¬å¦‚ä½•åœ¨ä¸æŸå®³å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æé«˜æ•ˆç‡ï¼Ÿ</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17065v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è”åˆè§£å†³æ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å¦‚ä½•è¯†åˆ«é€šç”¨æ—¶åºç‰¹å¾ä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼›äºŒæ˜¯å¦‚ä½•ç¡®ä¿æ—¶ç©ºæ¨¡å‹çš„å¹³è¡¡å­¦ä¹ ï¼›ä¸‰æ˜¯å¦‚ä½•åœ¨ä¸é™ä½å‡†ç¡®æ€§çš„æƒ…å†µä¸‹è§£å†³èµ„æºå¯†é›†é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡è§†é¢‘çº§åˆ«çš„æ··åˆç­–ç•¥ï¼Œå‘ç°äº†ä¸€ç§è¢«å¿½è§†çš„æ—¶åºä¼ªé€ ç‰¹å¾â€”â€”é¢éƒ¨ç‰¹å¾æ¼‚ç§»ï¼ˆFFDï¼‰ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„æ—¶ç©ºé€‚é…å™¨ï¼ˆStAï¼‰ï¼Œä½¿é¢„è®­ç»ƒå›¾åƒæ¨¡å‹èƒ½å¤Ÿè”åˆä¸”é«˜æ•ˆåœ°æ•è·ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„ä¼ªé€ è§†é¢‘ï¼ŒåŒ…æ‹¬æœ€æ–°ä¸€ä»£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æŒ‡å‡ºäº†æ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶å°è¯•åŒæ—¶è§£å†³å®ƒä»¬ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è§†é¢‘çº§åˆ«çš„æ··åˆç­–ç•¥ï¼Œå‘ç°äº†é¢éƒ¨ç‰¹å¾æ¼‚ç§»ï¼ˆFFDï¼‰è¿™ä¸€é‡è¦çš„æ—¶åºä¼ªé€ ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘çº§åˆ«æ··åˆæ•°æ®ï¼ˆVBï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ··åˆåŸå§‹å›¾åƒå’Œå…¶å˜å½¢ç‰ˆæœ¬å¸§æ¥æŒ–æ˜æ›´é€šç”¨çš„ç‰¹å¾ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„æ—¶ç©ºé€‚é…å™¨ï¼ˆStAï¼‰ï¼Œä½¿é¢„è®­ç»ƒå›¾åƒæ¨¡å‹èƒ½å¤Ÿè”åˆæ•è·ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚</li>
<li>StAé‡‡ç”¨åŒæµ3Då·ç§¯è®¾è®¡ï¼Œå…·æœ‰ä¸åŒå†…æ ¸å¤§å°ï¼Œå¯åˆ†åˆ«å¤„ç†ç©ºé—´å’Œæ—¶åºç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„ä¼ªé€ è§†é¢‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26403bcf86a4e06aa0a0588d5f8dc98d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3ab2357afab60d89a4f9d8798d9a900b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e36d6c564478b66dfef23d88cf5ac8b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ca20647e44cbfe4651a7c3403acdbcf0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5de1a1d6927644a276545568298cb68d.jpg" align="middle">
</details>




<h2 id="GalLoP-Learning-Global-and-Local-Prompts-for-Vision-Language-Models"><a href="#GalLoP-Learning-Global-and-Local-Prompts-for-Vision-Language-Models" class="headerlink" title="GalLoP: Learning Global and Local Prompts for Vision-Language Models"></a>GalLoP: Learning Global and Local Prompts for Vision-Language Models</h2><p><strong>Authors:Marc Lafon, Elias Ramzi, ClÃ©ment Rambour, Nicolas Audebert, Nicolas Thome</strong></p>
<p>Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs), e.g. CLIP, for few-shot image classification. Despite their success, most prompt learning methods trade-off between classification accuracy and robustness, e.g. in domain generalization or out-of-distribution (OOD) detection. In this work, we introduce Global-Local Prompts (GalLoP), a new prompt learning method that learns multiple diverse prompts leveraging both global and local visual features. The training of the local prompts relies on local features with an enhanced vision-text alignment. To focus only on pertinent features, this local alignment is coupled with a sparsity strategy in the selection of the local features. We enforce diversity on the set of prompts using a new &#96;&#96;prompt dropoutâ€™â€™ technique and a multiscale strategy on the local prompts. GalLoP outperforms previous prompt learning methods on accuracy on eleven datasets in different few shots settings and with various backbones. Furthermore, GalLoP shows strong robustness performances in both domain generalization and OOD detection, even outperforming dedicated OOD detection methods. Code and instructions to reproduce our results: <a target="_blank" rel="noopener" href="https://github.com/MarcLafon/gallop">https://github.com/MarcLafon/gallop</a>. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²è¢«å¹¿æ³›åº”ç”¨äºé«˜æ•ˆåœ°é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ï¼Œä»¥è¿›è¡Œå°‘é‡å›¾åƒåˆ†ç±»ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†å¤§å¤šæ•°æç¤ºå­¦ä¹ æ–¹æ³•åœ¨åˆ†ç±»ç²¾åº¦å’Œç¨³å¥æ€§ï¼ˆä¾‹å¦‚åœ¨åŸŸæ³›åŒ–æˆ–å¼‚å¸¸å€¼æ£€æµ‹ï¼‰ä¹‹é—´è¿›è¡Œäº†æƒè¡¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å±€éƒ¨æç¤ºï¼ˆGalLoPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨è§†è§‰ç‰¹å¾å­¦ä¹ å¤šä¸ªä¸åŒçš„æç¤ºã€‚å±€éƒ¨æç¤ºçš„è®­ç»ƒä¾èµ–äºå…·æœ‰å¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½çš„å±€éƒ¨ç‰¹å¾ã€‚ä¸ºäº†åªå…³æ³¨é‡è¦ç‰¹å¾ï¼Œè¿™ç§å±€éƒ¨å¯¹é½ä¸é€‰æ‹©å±€éƒ¨ç‰¹å¾çš„ç¨€ç–ç­–ç•¥ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°çš„â€œæç¤ºä¸¢å¼ƒâ€æŠ€æœ¯å’Œå±€éƒ¨æç¤ºçš„å¤šå°ºåº¦ç­–ç•¥ï¼Œåœ¨æç¤ºé›†ä¸Šå¼ºåˆ¶æ‰§è¡Œå¤šæ ·æ€§ã€‚åœ¨å…·æœ‰ä¸åŒå°‘é‡æ ·æœ¬å’Œä¸åŒéª¨å¹²ç½‘ç»œçš„åä¸€ä¸ªæ•°æ®é›†ä¸Šï¼ŒGalLoPåœ¨å‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°è¶…è¿‡äº†å…ˆå‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨é¢†åŸŸæ³›åŒ–å’Œå¼‚å¸¸å€¼æ£€æµ‹ä¸­ï¼ŒGalLoPä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“é—¨çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœçš„å¤ç°ä»£ç å’Œè¯´æ˜ï¼š<a target="_blank" rel="noopener" href="https://github.com/MarcLafon/gallop%E3%80%82">https://github.com/MarcLafon/gallopã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01400v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•â€”â€”å…¨å±€å±€éƒ¨æç¤ºï¼ˆGalLoPï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆå…¨å±€å’Œå±€éƒ¨è§†è§‰ç‰¹å¾å­¦ä¹ å¤šä¸ªä¸åŒçš„æç¤ºã€‚é€šè¿‡å¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½å’Œå±€éƒ¨ç‰¹å¾é€‰æ‹©ä¸­çš„ç¨€ç–ç­–ç•¥ï¼Œä¸“æ³¨äºå…³é”®ç‰¹å¾ã€‚ä½¿ç”¨æ–°çš„æç¤ºä¸¢å¼ƒæŠ€æœ¯å’Œå±€éƒ¨æç¤ºçš„å¤šå°ºåº¦ç­–ç•¥æ¥å¢å¼ºæç¤ºçš„å¤šæ ·æ€§ã€‚åœ¨å¤šç§ä¸åŒæ•°æ®é›†å’ŒèƒŒæ™¯ä¸‹çš„å°‘é‡æ ·æœ¬è®¾ç½®ä¸­ï¼ŒGalLoPåœ¨å‡†ç¡®åº¦ä¸Šä¼˜äºå…ˆå‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨åŸŸæ³›åŒ–å’Œå¼‚å¸¸å€¼æ£€æµ‹ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“é—¨çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GalLoPæ˜¯ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆå…¨å±€å’Œå±€éƒ¨è§†è§‰ç‰¹å¾å­¦ä¹ å¤šä¸ªæç¤ºã€‚</li>
<li>é€šè¿‡å¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½å’Œå±€éƒ¨ç‰¹å¾çš„ç¨€ç–é€‰æ‹©ç­–ç•¥ï¼Œä¸“æ³¨äºå…³é”®ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨æç¤ºä¸¢å¼ƒæŠ€æœ¯å’Œå¤šå°ºåº¦ç­–ç•¥æ¥å¢å¼ºæç¤ºçš„å¤šæ ·æ€§ã€‚</li>
<li>GalLoPåœ¨å¤šç§æ•°æ®é›†å’ŒèƒŒæ™¯ä¸‹çš„å°‘é‡æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰æ›´é«˜çš„åˆ†ç±»å‡†ç¡®åº¦ã€‚</li>
<li>GalLoPåœ¨åŸŸæ³›åŒ–å’Œå¼‚å¸¸å€¼æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
<li>GalLoPç”šè‡³è¶…è¶Šäº†ä¸“é—¨çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-237f00d55c73090da6b9b04ad47f2a83.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e01c9df94e2c5f46fb9d421e1f32aba2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-76aeb8413b2e6a351f64557226fbf06e.jpg" align="middle">
</details>




<h2 id="3DStyleGLIP-Part-Tailored-Text-Guided-3D-Neural-Stylization"><a href="#3DStyleGLIP-Part-Tailored-Text-Guided-3D-Neural-Stylization" class="headerlink" title="3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization"></a>3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization</h2><p><strong>Authors:SeungJeh Chung, JooHyun Park, HyeongYeop Kang</strong></p>
<p>3D stylization, the application of specific styles to three-dimensional objects, offers substantial commercial potential by enabling the creation of uniquely styled 3D objects tailored to diverse scenes. Recent advancements in artificial intelligence and text-driven manipulation methods have made the stylization process increasingly intuitive and automated. While these methods reduce human costs by minimizing reliance on manual labor and expertise, they predominantly focus on holistic stylization, neglecting the application of desired styles to individual components of a 3D object. This limitation restricts the fine-grained controllability. To address this gap, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization. Given a 3D mesh and a text prompt, 3DStyleGLIP utilizes the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize individual parts of the 3D mesh and modify their appearance to match the styles specified in the text prompt. 3DStyleGLIP effectively integrates part localization and stylization guidance within GLIPâ€™s shared embedding space through an end-to-end process, enabled by part-level style loss and two complementary learning techniques. This neural methodology meets the userâ€™s need for fine-grained style editing and delivers high-quality part-specific stylization results, opening new possibilities for customization and flexibility in 3D content creation. Our code and results are available at <a target="_blank" rel="noopener" href="https://github.com/sj978/3DStyleGLIP">https://github.com/sj978/3DStyleGLIP</a>. </p>
<blockquote>
<p>3Dé£æ ¼åŒ–æ˜¯å°†ç‰¹å®šé£æ ¼åº”ç”¨äºä¸‰ç»´ç‰©ä½“çš„åº”ç”¨ï¼Œå®ƒé€šè¿‡åˆ›å»ºé€‚åº”ä¸åŒåœºæ™¯çš„ç‹¬ç‰¹é£æ ¼åŒ–çš„3Dç‰©ä½“ï¼Œæ‹¥æœ‰å·¨å¤§çš„å•†ä¸šæ½œåŠ›ã€‚äººå·¥æ™ºèƒ½å’Œæ–‡æœ¬é©±åŠ¨æ“ä½œæ–¹æ³•çš„æœ€æ–°è¿›å±•ä½¿å¾—é£æ ¼åŒ–è¿‡ç¨‹è¶Šæ¥è¶Šç›´è§‚å’Œè‡ªåŠ¨åŒ–ã€‚è¿™äº›æ–¹æ³•é€šè¿‡æœ€å°åŒ–å¯¹äººå·¥åŠ³åŠ¨å’Œä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ï¼Œé™ä½äº†äººåŠ›æˆæœ¬ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨æ•´ä½“é£æ ¼åŒ–ï¼Œå¿½è§†äº†å°†æ‰€éœ€é£æ ¼åº”ç”¨äºä¸‰ç»´ç‰©ä½“çš„å•ä¸ªç»„ä»¶ã€‚è¿™ç§é™åˆ¶é™åˆ¶äº†ç²¾ç»†ç²’åº¦çš„å¯æ§æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†3DStyleGLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ–‡æœ¬é©±åŠ¨ã€éƒ¨åˆ†å®šåˆ¶çš„3Dé£æ ¼åŒ–è€Œè®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚ç»™å®šä¸€ä¸ª3Dç½‘æ ¼å’Œä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œ3DStyleGLIPåˆ©ç”¨åŸºäºGrounded Language-Image Pre-trainingï¼ˆGLIPï¼‰æ¨¡å‹çš„è§†è§‰è¯­è¨€åµŒå…¥ç©ºé—´æ¥å®šä½3Dç½‘æ ¼çš„å•ä¸ªéƒ¨åˆ†ï¼Œå¹¶ä¿®æ”¹å®ƒä»¬çš„å¤–è§‚ä»¥åŒ¹é…æ–‡æœ¬æç¤ºä¸­æŒ‡å®šçš„é£æ ¼ã€‚3DStyleGLIPé€šè¿‡ç«¯åˆ°ç«¯çš„è¿‡ç¨‹æœ‰æ•ˆåœ°åœ¨GLIPçš„å…±äº«åµŒå…¥ç©ºé—´ä¸­é›†æˆäº†éƒ¨åˆ†å®šä½å’Œé£æ ¼åŒ–æŒ‡å¯¼ï¼Œè¿™å¾—ç›Šäºéƒ¨åˆ†çº§åˆ«çš„é£æ ¼æŸå¤±å’Œä¸¤ç§äº’è¡¥çš„å­¦ä¹ æŠ€æœ¯ã€‚è¿™ç§ç¥ç»æ–¹æ³•æ»¡è¶³äº†ç”¨æˆ·å¯¹ç²¾ç»†ç²’åº¦é£æ ¼ç¼–è¾‘çš„éœ€æ±‚ï¼Œå¹¶æä¾›äº†é«˜è´¨é‡çš„éƒ¨ä»¶ç‰¹å®šé£æ ¼åŒ–ç»“æœï¼Œä¸º3Då†…å®¹åˆ›å»ºæä¾›äº†å®šåˆ¶å’Œçµæ´»æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sj978/3DStyleGLIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sj978/3DStyleGLIPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.02634v2">PDF</a> 12 pages, 8 figures, 2024 Pacific Graphics Conferences (PG 2024)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬é©±åŠ¨çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶â€”â€”3DStyleGLIPï¼Œä¸“é—¨ç”¨äºå¤„ç†éƒ¨åˆ†å®šåˆ¶çš„3Dé£æ ¼åŒ–é—®é¢˜ã€‚å®ƒåˆ©ç”¨GLIPæ¨¡å‹çš„è§†è§‰è¯­è¨€åµŒå…¥ç©ºé—´ï¼Œæ ¹æ®æ–‡æœ¬æç¤ºå®šä½å¹¶ä¿®æ”¹3Dç½‘æ ¼çš„å„éƒ¨åˆ†é£æ ¼ã€‚æ­¤æ–¹æ³•å®ç°äº†éƒ¨åˆ†å®šä½å’Œé£æ ¼åŒ–æŒ‡å¯¼çš„é›†æˆï¼Œä¸ºç»†ç²’åº¦é£æ ¼ç¼–è¾‘æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬é©±åŠ¨çš„ä¸ªæ€§åŒ–éƒ¨åˆ†å®šåˆ¶åœ¨ä¸‰ç»´ç‰©ä½“é£æ ¼åŒ–ä¸­çš„æ½œåŠ›å·¨å¤§ã€‚</li>
<li>å½“å‰æŠ€æœ¯ä¸»è¦å…³æ³¨æ•´ä½“é£æ ¼åŒ–ï¼Œå¿½è§†äº†ç‰¹å®šç»„ä»¶çš„é£æ ¼åº”ç”¨ï¼Œå­˜åœ¨ç²¾ç»†æ§åˆ¶ä¸Šçš„é™åˆ¶ã€‚</li>
<li>æå‡ºçš„æ–°å‹æ¡†æ¶â€”â€”3DStyleGLIPèƒ½å¤Ÿç»“åˆä½¿ç”¨ä¸‰ç»´ç½‘æ ¼å’Œæ–‡æœ¬æç¤ºï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€åµŒå…¥ç©ºé—´å®šä½å¹¶è°ƒæ•´ç‰¹å®šéƒ¨åˆ†é£æ ¼ã€‚</li>
<li>3DStyleGLIPé€šè¿‡é›†æˆéƒ¨åˆ†å®šä½å’Œé£æ ¼åŒ–æŒ‡å¯¼ï¼Œå®ç°äº†åœ¨GLIPå…±äº«åµŒå…¥ç©ºé—´ä¸­çš„ç«¯åˆ°ç«¯æµç¨‹ã€‚</li>
<li>éƒ¨åˆ†çº§é£æ ¼æŸå¤±å’Œä¸¤ç§äº’è¡¥å­¦ä¹ æŠ€æœ¯å¢å¼ºäº†è¯¥ç¥ç»æ–¹æ³•çš„æ€§èƒ½ï¼Œæ»¡è¶³äº†ç”¨æˆ·å¯¹ç»†ç²’åº¦é£æ ¼ç¼–è¾‘çš„éœ€æ±‚ã€‚</li>
<li>3DStyleGLIPæä¾›äº†é«˜è´¨é‡çš„éƒ¨åˆ†ç‰¹å®šé£æ ¼åŒ–ç»“æœï¼Œä¸ºä¸‰ç»´å†…å®¹åˆ›å»ºæä¾›äº†å®šåˆ¶å’Œçµæ´»æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3133d6bf6b203e85050414ddfd5ab99f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-33e4def37236c4fccbc9fb6c61070294.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ca72a2f55e6f7a8f97316d793c79a84c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1523e24eacbd43e57e7cb372597e00e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-729e26f6ded4fc5ba4cfc9bd31d04e15.jpg" align="middle">
</details>




<h2 id="Just-Shift-It-Test-Time-Prototype-Shifting-for-Zero-Shot-Generalization-with-Vision-Language-Models"><a href="#Just-Shift-It-Test-Time-Prototype-Shifting-for-Zero-Shot-Generalization-with-Vision-Language-Models" class="headerlink" title="Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization   with Vision-Language Models"></a>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization   with Vision-Language Models</h2><p><strong>Authors:Elaine Sui, Xiaohan Wang, Serena Yeung-Levy</strong></p>
<p>Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 image classification datasets involving natural distribution shifts and cross-dataset generalization, as well as in context-dependent visual reasoning, demonstrate TPSâ€™s superior performance, achieving state-of-the-art results while reducing resource requirements. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰æ½œåŠ›ï¼Œä½†ç”±äºæµ‹è¯•ç¯å¢ƒä¸­çš„åŸŸåç§»ï¼Œå®ƒä»¬çš„æ•ˆåŠ›å¾€å¾€ä¼šé™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶é—´åŸå‹åç§»ï¼ˆTPSï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°æ€§çš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨æœªæ ‡è®°çš„æµ‹è¯•è¾“å…¥ä½¿VLMé€‚åº”æµ‹è¯•æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­è°ƒåˆ¶æ¯ç±»åŸå‹çš„æ¦‚å¿µã€‚é€šè¿‡é¢„è®¡ç®—å’Œç¼“å­˜ä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆçš„åŸå‹ï¼ŒTPSä¸ä»…ä¾¿äºåç»­é¢„æµ‹çš„ä¼˜åŒ–åŸå‹é‡ç”¨ï¼Œè¿˜å¯ä»¥æ— ç¼é›†æˆåˆ°å½“å‰çš„æç¤ºå·¥ç¨‹è¿›å±•ä¸­ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒTPSä»…æ ¹æ®ç»™å®šçš„æµ‹è¯•æ ·æœ¬åŠ¨æ€å­¦ä¹ æ¯ä¸ªåŸå‹çš„åç§»å‘é‡ï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†åŸŸå·®è·å¹¶æé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚æˆ‘ä»¬æ¡†æ¶çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯ä¸å¸¸è§„æ–‡æœ¬æç¤ºè°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå¤§å¤§å‡å°‘äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚åœ¨æ¶‰åŠè‡ªç„¶åˆ†å¸ƒåç§»ã€è·¨æ•°æ®é›†æ³›åŒ–å’Œä¸Šä¸‹æ–‡ç›¸å…³è§†è§‰æ¨ç†çš„15ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†TPSçš„å“è¶Šæ€§èƒ½ï¼Œå®ƒåœ¨é™ä½èµ„æºè¦æ±‚çš„åŒæ—¶å–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12952v2">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æµ‹è¯•ç¯å¢ƒä¸­å‡ºç°çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œæå‡ºçš„Test-Time Prototype Shiftingï¼ˆTPSï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æœªæ ‡è®°çš„æµ‹è¯•è¾“å…¥è‡ªé€‚åº”è°ƒæ•´VLMsï¼Œé€šè¿‡è°ƒåˆ¶å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„æ¯ç±»åŸå‹æ¥å®ç°ã€‚TPSä¸ä»…å®ç°äº†ä¼˜åŒ–å‰çš„åŸå‹é‡ç”¨ï¼Œä¾¿äºåç»­é¢„æµ‹ï¼Œè¿˜æ˜“äºä¸å½“å‰çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ç›¸ç»“åˆã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒTPSæ ¹æ®ç»™å®šçš„æµ‹è¯•æ ·æœ¬åŠ¨æ€å­¦ä¹ æ¯ä¸ªåŸå‹çš„åç§»å‘é‡ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼©å°é¢†åŸŸå·®è·å¹¶æé«˜åˆ†ç±»ç²¾åº¦ã€‚ä¸å¸¸è§„çš„æ–‡æœ¬æç¤ºè°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼ŒTPSçš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚å¤§å¤§é™ä½ã€‚åœ¨æ¶‰åŠè‡ªç„¶åˆ†å¸ƒåç§»ã€è·¨æ•°æ®é›†æ³›åŒ–å’Œä¸Šä¸‹æ–‡ç›¸å…³è§†è§‰æ¨ç†çš„15ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒTPSè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœï¼Œå¹¶é™ä½äº†èµ„æºè¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TPSæ¡†æ¶æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æµ‹è¯•ç¯å¢ƒä¸­å‡ºç°çš„é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>TPSé€šè¿‡è°ƒåˆ¶å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„æ¯ç±»åŸå‹æ¥å®ç°è‡ªé€‚åº”è°ƒæ•´ã€‚</li>
<li>TPSå®ç°äº†ä¼˜åŒ–å‰çš„åŸå‹é‡ç”¨ï¼Œä¾¿äºåç»­é¢„æµ‹ï¼Œå¹¶ä¸å½“å‰çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ç›¸ç»“åˆã€‚</li>
<li>TPSåœ¨æµ‹è¯•æ—¶æ ¹æ®ç»™å®šçš„æµ‹è¯•æ ·æœ¬åŠ¨æ€å­¦ä¹ æ¯ä¸ªåŸå‹çš„åç§»å‘é‡ã€‚</li>
<li>TPSæœ‰æ•ˆç¼©å°äº†é¢†åŸŸå·®è·ï¼Œæé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒTPSå…·æœ‰è¾ƒä½çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a47e8f55a73fd9a415be32e8b16e2e4d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d9f1396abd34c5c00c8fc795acb48d19.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2258dedc9776a7be355d1d33bffc4e7b.jpg" align="middle">
</details>




<h2 id="Efficient-Prompt-Tuning-of-Large-Vision-Language-Model-for-Fine-Grained-Ship-Classification"><a href="#Efficient-Prompt-Tuning-of-Large-Vision-Language-Model-for-Fine-Grained-Ship-Classification" class="headerlink" title="Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained   Ship Classification"></a>Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained   Ship Classification</h2><p><strong>Authors:Long Lan, Fengxiang Wang, Xiangtao Zheng, Zengmao Wang, Xinwang Liu</strong></p>
<p>Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the modelâ€™s generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available. </p>
<blockquote>
<p>åœ¨é¥æ„ŸæŠ€æœ¯ä¸­å¯¹èˆ¹åªè¿›è¡Œç²¾ç»†åˆ†ç±»ï¼ˆRS-FGSCï¼‰æ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå„ç±»ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾ˆé«˜ï¼Œä¸”æ ‡æ³¨æ•°æ®æœ‰é™ï¼Œè¿™é™åˆ¶äº†ä¼ ç»Ÿç›‘ç£åˆ†ç±»æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€è¿‘çš„å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°‘é‡æˆ–é›¶æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å›¾åƒå†…å®¹æ–¹é¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢åˆ©ç”¨VLMsçš„æ½œåŠ›æ¥æé«˜å¯¹æœªè§èˆ¹åªç±»åˆ«çš„åˆ†ç±»ç²¾åº¦ï¼Œè¿™åœ¨ç”±äºæˆæœ¬æˆ–éšç§çº¦æŸè€Œå¯¼è‡´æ•°æ®å—é™çš„åœºæ™¯ä¸­å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç›´æ¥å¯¹RS-FGSCè¿›è¡Œå¾®è°ƒå¸¸å¸¸ä¼šé¢ä¸´è¿‡åº¦æ‹Ÿåˆå·²è§ç±»åˆ«çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ä¸ä½³ï¼Œè¿™çªæ˜¾äº†åœ¨åŒºåˆ†å¤æ‚èƒŒæ™¯å’Œæ•æ‰èˆ¹åªç‹¬ç‰¹ç‰¹å¾æ–¹é¢çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æç¤ºå¾®è°ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨åˆ†å±‚å¤šç²’åº¦æç¤ºè®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åç½®é¡¹èå…¥é¥æ„Ÿèˆ¹åªå…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›åç½®é¡¹æ¥è‡ªä¸€ä¸ªå°å‹å¯è®­ç»ƒç½‘ç»œã€‚æ­¤ç­–ç•¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜äº†å…¶åŒºåˆ†å¤æ‚èƒŒæ™¯å’Œå­¦ä¹ èƒ½åŠ›è¾¨åˆ«èˆ¹åªç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºé¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œå¼•å…¥äº†ç»¼åˆæ•°æ®é›†FGSCM-52ï¼Œå¤§å¹…æ‰©å±•äº†ç°æœ‰æ•°æ®é›†ï¼ŒåŒ…å«äº†æ›´å¤šå¹¿æ³›çš„æ•°æ®å’Œç½•è§çš„èˆ¹åªç±»åˆ«çš„è¯¦ç»†æ³¨é‡Šã€‚å¤§é‡çš„å®éªŒè¯„ä¼°è¯æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æºä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08271v2">PDF</a> It has been accepted by TGRS</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¥æ„Ÿå½±åƒçš„ç²¾ç»†èˆ¹èˆ¶åˆ†ç±»ï¼ˆRS-FGSCï¼‰é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç±»åˆ«é«˜åº¦ç›¸ä¼¼æ€§å’Œæ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†ä¼ ç»Ÿç›‘ç£åˆ†ç±»æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€è¿‘çš„å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å›¾åƒå†…å®¹ä¸Šã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨VLMsçš„æ½œåŠ›æ¥æé«˜æœªè§èˆ¹èˆ¶ç±»åˆ«çš„åˆ†ç±»ç²¾åº¦ï¼Œè¿™åœ¨æˆæœ¬æˆ–éšç§çº¦æŸå¯¼è‡´æ•°æ®å—é™çš„åœºæ™¯ä¸­å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç›´æ¥ä½¿ç”¨å¾®è°ƒVLMsè¿›è¡ŒRS-FGSCå¸¸å¸¸ä¼šé‡åˆ°è¿‡åº¦æ‹Ÿåˆå·²è§ç±»åˆ«çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ä¸ä½³ï¼Œè¿™å‡¸æ˜¾äº†åŒºåˆ†å¤æ‚èƒŒæ™¯å’Œæ•æ‰èˆ¹èˆ¶ç‰¹å¾å›°éš¾çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨åˆ†å±‚å¤šç²’åº¦æç¤ºè®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»å¯è®­ç»ƒçš„å°ç½‘ç»œä¸­å­¦ä¹ é¥æ„Ÿèˆ¹èˆ¶å…ˆéªŒï¼Œå°†å…ˆéªŒçŸ¥è¯†èå…¥åå·®é¡¹ä¸­ã€‚æ­¤ç­–ç•¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜äº†å…¶åŒºåˆ†å¤æ‚èƒŒæ™¯å’Œå­¦ä¹ èˆ¹èˆ¶ç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºé¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œæ¨å‡ºäº†ç»¼åˆæ•°æ®é›†FGSCM-52ï¼Œè¯¥æ•°æ®é›†åœ¨ç°æœ‰æ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†å¤§å¹…æ‰©å±•ï¼ŒåŒ…å«äº†æ›´å¹¿æ³›çš„æ•°æ®å’Œè¯¦ç»†çš„ç½•è§èˆ¹èˆ¶ç±»åˆ«æ³¨é‡Šã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æºä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RS-FGSCé¢ä¸´é«˜ç±»åˆ«ç›¸ä¼¼æ€§å’Œæ ‡æ³¨æ•°æ®æœ‰é™æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>ç›´æ¥å¾®è°ƒVLMsä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆå·²è§ç±»åˆ«ï¼Œå½±å“å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œé‡‡ç”¨åˆ†å±‚å¤šç²’åº¦æç¤ºè®¾è®¡æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡å­¦ä¹ é¥æ„Ÿèˆ¹èˆ¶å…ˆéªŒï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’ŒåŒºåˆ†å¤æ‚èƒŒæ™¯åŠèˆ¹èˆ¶ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>æ¨å‡ºäº†ç»¼åˆæ•°æ®é›†FGSCM-52ï¼ŒåŒ…å«æ›´å¹¿æ³›çš„æ•°æ®å’Œè¯¦ç»†çš„ç½•è§èˆ¹èˆ¶ç±»åˆ«æ³¨é‡Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-26c768e0586a22813e2adc7d42a80eae.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d369a45a88991a559d65f564ff6ac511.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8d75df78e4f9149a31145f46ab6c5bb7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-feef1fc452d9919594ee10d0a70945c1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-55031060c37f2f6edfcf6bf517e5a7b3.jpg" align="middle">
</details>




<h2 id="Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning"><a href="#Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning"></a>Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Wangmeng Zuo, Chunmei Feng</strong></p>
<p>Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ—¨åœ¨åŸºäºéå¸¸æœ‰é™çš„è®­ç»ƒæ•°æ®æŒç»­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸å¿˜æ‰å·²é‡åˆ°çš„æ—§ç±»åˆ«ã€‚ç°æœ‰ç ”ç©¶ä»…ä¾èµ–äºçº¯è§†è§‰ç½‘ç»œï¼Œè€Œæœ¬æ–‡æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰æ¥è§£å†³FSCILé—®é¢˜ï¼Œå¹¶æå‡ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåä¸ºåŸºäºåˆ†å¸ƒç‰¹å¾å›æ”¾çš„å­¦ä¹ æç¤ºï¼ˆLP-DiFï¼‰ã€‚æˆ‘ä»¬å‘ç°ä»…ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å³å¯å¤§å¤§è¶…è¶Šæœ€å…·å½±å“åŠ›çš„æ–¹æ³•ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶é€‚åº”èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸæŒç»­ä»æ¯ä¸ªä¼šè¯ä¸­æ•è·ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºäº†é˜²æ­¢å­¦ä¹ æç¤ºå¿˜è®°æ–°ä¼šè¯ä¸­çš„æ—§çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç»´æŒä¸€ä¸ªç”±è®­ç»ƒå›¾åƒç‰¹å¾å’Œå˜è‡ªåŠ¨ç¼–ç å™¨ç”Ÿæˆçš„åˆæˆç‰¹å¾ä¼°è®¡å¾—åˆ°çš„å¯¹è§’åæ–¹å·®çŸ©é˜µå½¢å¼çš„ç‰¹å¾çº§é«˜æ–¯åˆ†å¸ƒæ¥ä¿ç•™æ¯ä¸ªæ—§ç±»çš„æ—§çŸ¥è¯†ã€‚å½“è¿›å…¥ä¸€ä¸ªæ–°ä¼šè¯æ—¶ï¼Œä¼ªç‰¹å¾æ˜¯ä»æ—§ç±»åˆ†å¸ƒä¸­é‡‡æ ·å¹¶ç»“åˆå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œä»è€Œèƒ½å¤Ÿä½¿æ¨¡å‹åœ¨å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚åœ¨CIFAR100ã€mini-ImageNetã€CUB-200ä¸‰ä¸ªæµè¡ŒåŸºå‡†ä»¥åŠæœ¬æ–‡æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„SUN-397å’ŒCUB-200$*$ä¸Šçš„å®éªŒå±•ç¤ºäº†LP-DiFçš„ä¼˜è¶Šæ€§ï¼Œåœ¨FSCILé¢†åŸŸè¾¾åˆ°äº†æ–°çš„æœ€æ–°æ°´å¹³ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01598v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é—®é¢˜ï¼Œåˆ©ç”¨è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ„å»ºäº†ä¸€ä¸ªåä¸ºLP-DiFçš„ç®€å•æœ‰æ•ˆæ¡†æ¶ã€‚é€šè¿‡å¼•å…¥æç¤ºè°ƒä¼˜æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹èƒ½æ›´å¥½åœ°é€‚åº”ä¸åŒç±»çŸ¥è¯†çš„å­¦ä¹ ã€‚ä¸ºé˜²æ­¢åœ¨æ–°è¯¾ç¨‹ä¸­é—å¿˜æ—§çŸ¥è¯†ï¼Œæå‡ºåŸºäºåˆ†å¸ƒçš„ç‰¹å¾å›æ”¾æ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å¾çº§åˆ«çš„é«˜æ–¯åˆ†å¸ƒä¼°è®¡è®­ç»ƒå›¾åƒçš„ç‰¹å¾ä»¥åŠå˜è‡ªç¼–ç å™¨ç”Ÿæˆçš„åˆæˆç‰¹å¾æ¥ä¿ç•™æ—§çŸ¥è¯†çš„è®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLP-DiFæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æœ€æ–°æˆæœã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LP-DiFæ¡†æ¶åˆ©ç”¨è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è§£å†³å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ é—®é¢˜ã€‚</li>
<li>æç¤ºè°ƒä¼˜æŠ€æœ¯è¢«å¼•å…¥ä»¥æé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>LP-DiFæ¡†æ¶ä½¿ç”¨åŸºäºåˆ†å¸ƒçš„ç‰¹å¾å›æ”¾æ–¹æ³•ä»¥é˜²æ­¢æ¨¡å‹åœ¨å­¦ä¹ æ–°è¯¾ç¨‹æ—¶é—å¿˜æ—§çŸ¥è¯†ã€‚</li>
<li>ç‰¹å¾çº§åˆ«çš„é«˜æ–¯åˆ†å¸ƒç”¨äºä¼°è®¡æ—§çŸ¥è¯†çš„è®°å¿†ï¼ŒåŒ…æ‹¬è®­ç»ƒå›¾åƒç‰¹å¾å’Œåˆæˆç‰¹å¾ã€‚</li>
<li>LP-DiFæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æœ€æ–°æˆæœã€‚</li>
<li>ä»£ç å·²å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b68ef099e7713b014eeb2b3e7b49e8c2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a76b9eff0fad3d63dcb46aee459a0673.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0a24a512d0bc42141ba455a1b4b860e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f07311872d24975899a6d4fd3d1d5e60.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-74c2a8fa494ec56d4ee19c96dabbeb8f.jpg" align="middle">
</details>




<h2 id="M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models"><a href="#M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models" class="headerlink" title="M$^{2}$UGen: Multi-modal Music Understanding and Generation with the   Power of Large Language Models"></a>M$^{2}$UGen: Multi-modal Music Understanding and Generation with the   Power of Large Language Models</h2><p><strong>Authors:Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan</strong></p>
<p>The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as text, speech, images, videos, etc. They also utilize LLMs to understand human intention and generate desired outputs like images, videos, and music. However, research that combines both understanding and generation using LLMs is still limited and in its nascent stage. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M$^{2}$UGen) framework that integrates LLMâ€™s abilities to comprehend and generate music for different modalities. The M$^{2}$UGen framework is purpose-built to unlock creative potential from diverse sources of inspiration, encompassing music, image, and video through the use of pretrained MERT, ViT, and ViViT models, respectively. To enable music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging multi-modal understanding and music generation is accomplished through the integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA model to generate extensive datasets that support text&#x2F;image&#x2F;video-to-music generation, facilitating the training of our M$^{2}$UGen framework. We conduct a thorough evaluation of our proposed framework. The experimental results demonstrate that our model achieves or surpasses the performance of the current state-of-the-art models. </p>
<blockquote>
<p>å½“å‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶é¢†åŸŸæ­£ç»å†ä¸€æ¬¡çƒ­æ½®ã€‚è®¸å¤šç ”ç©¶åˆ©ç”¨è¿™äº›æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›æ¥ç†è§£å„ç§æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒã€è§†é¢‘ç­‰ã€‚ä»–ä»¬è¿˜åˆ©ç”¨LLMæ¥ç†è§£äººç±»æ„å›¾å¹¶ç”Ÿæˆå›¾åƒã€è§†é¢‘å’ŒéŸ³ä¹ç­‰æ‰€éœ€è¾“å‡ºã€‚ç„¶è€Œï¼Œå°†ç†è§£å’Œç”Ÿæˆç»“åˆä½¿ç”¨LLMçš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå¤„äºåˆçº§é˜¶æ®µã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆï¼ˆM$^{2}$UGenï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†LLMç†è§£å’Œç”Ÿæˆä¸åŒæ¨¡æ€éŸ³ä¹çš„èƒ½åŠ›ã€‚M$^{2}$UGenæ¡†æ¶ä¸“ä¸ºè§£é”æ¥è‡ªä¸åŒçµæ„Ÿæ¥æºçš„åˆ›é€ åŠ›è€Œæ„å»ºï¼Œé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„MERTã€ViTå’ŒViViTæ¨¡å‹ï¼Œåˆ†åˆ«æ¶µç›–éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘ã€‚ä¸ºäº†å®ç°éŸ³ä¹ç”Ÿæˆï¼Œæˆ‘ä»¬æ¢ç´¢äº†AudioLDM 2å’ŒMusicGençš„ä½¿ç”¨ã€‚é€šè¿‡LLaMA 2æ¨¡å‹çš„é›†æˆï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£å’ŒéŸ³ä¹ç”Ÿæˆçš„æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨MU-LLaMAæ¨¡å‹ç”Ÿæˆäº†å¤§é‡æ”¯æŒæ–‡æœ¬&#x2F;å›¾åƒ&#x2F;è§†é¢‘åˆ°éŸ³ä¹ç”Ÿæˆçš„æ•°æ®é›†ï¼Œä¸ºæˆ‘ä»¬çš„M$^{2}$UGenæ¡†æ¶æä¾›è®­ç»ƒæ”¯æŒã€‚æˆ‘ä»¬å¯¹æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11255v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆï¼ˆM$^{2}$UGenï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œç”¨äºå¤„ç†éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„MERTã€ViTå’ŒViViTæ¨¡å‹ï¼Œç»“åˆAudioLDM 2å’ŒMusicGenè¿›è¡ŒéŸ³ä¹ç”Ÿæˆï¼Œå¹¶é€šè¿‡LLaMA 2æ¨¡å‹å®ç°å¤šæ¨¡æ€ç†è§£å’ŒéŸ³ä¹ç”Ÿæˆçš„æ¡¥æ¢ã€‚åŒæ—¶ï¼Œåˆ©ç”¨MU-LLaMAæ¨¡å‹ç”Ÿæˆå¤§é‡æ•°æ®é›†ï¼Œæ”¯æŒæ–‡æœ¬&#x2F;å›¾åƒ&#x2F;è§†é¢‘åˆ°éŸ³ä¹çš„ç”Ÿæˆï¼Œä¿ƒè¿›äº†M$^{2}$UGenæ¡†æ¶çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å½“å‰å…ˆè¿›æ¨¡å‹çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨è¢«å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒã€è§†é¢‘ç­‰ã€‚</li>
<li>M$^{2}$UGenæ¡†æ¶ç»“åˆäº†LLMsçš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé’ˆå¯¹å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘ï¼‰è¿›è¡Œä¸“é—¨è®¾è®¡ã€‚</li>
<li>M$^{2}$UGenæ¡†æ¶ä½¿ç”¨é¢„è®­ç»ƒçš„MERTã€ViTå’ŒViViTæ¨¡å‹æ¥å¤„ç†å›¾åƒå’Œè§†é¢‘æ•°æ®ï¼Œä½¿ç”¨AudioLDM 2å’ŒMusicGenè¿›è¡ŒéŸ³ä¹ç”Ÿæˆã€‚</li>
<li>LLaMA 2æ¨¡å‹è¢«ç”¨äºè¿æ¥å¤šæ¨¡æ€ç†è§£å’ŒéŸ³ä¹ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>MU-LLaMAæ¨¡å‹è¢«ç”¨æ¥ç”Ÿæˆå¤§é‡æ•°æ®é›†ï¼Œä»¥æ”¯æŒæ–‡æœ¬&#x2F;å›¾åƒ&#x2F;è§†é¢‘åˆ°éŸ³ä¹çš„ç”Ÿæˆä»»åŠ¡ï¼Œä»è€Œä¿ƒè¿›äº†M$^{2}$UGenæ¡†æ¶çš„è®­ç»ƒã€‚</li>
<li>æå‡ºçš„M$^{2}$UGenæ¡†æ¶ç»è¿‡å…¨é¢è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å½“å‰å…ˆè¿›æ¨¡å‹çš„æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e2ec9dcf3634ad1383813fb8f9777507.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-146269967bb50b37095b2802028e8b5d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd273c2f6fa5c144401c057a5c938155.png" align="middle">
</details>




<h2 id="Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models"><a href="#Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models" class="headerlink" title="Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models"></a>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models</h2><p><strong>Authors:Zhaozheng Chen, Qianru Sun</strong></p>
<p>The rapid development of deep learning has driven significant progress in image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this journal, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚è¯­ä¹‰åˆ†å‰²ç®—æ³•é€šå¸¸ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼ˆå³å¯¹è±¡æ©è†œï¼‰çš„å¯ç”¨æ€§ï¼Œè€Œè¿™äº›æ ‡ç­¾çš„è·å–æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ã€‚å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯é¿å…è¿™ç§æ ‡æ³¨çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å®ƒä»…åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´çš„æ³¨é‡Šï¼Œä¸ºå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²æä¾›äº†ç»æµå®æƒ çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨æœ¬æœŸåˆŠä¸­ï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨å›¾åƒçº§æ ‡ç­¾çš„WSSSï¼Œè¿™æ˜¯WSSSä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚æˆ‘ä»¬çš„å·¥ä½œåˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œä¸»è¦å…³æ³¨åœ¨ä¸»è¦ç ”ç©¶ä¼šè®®ä¸Šæå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„æ–¹æ³•æ“ä½œä½ç½®å°†å®ƒä»¬åˆ†ä¸ºå››ç±»ï¼šåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¦‚åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰åœ¨WSSSä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæœ‰è¶£çš„åœºæ™¯ä¸­ä»”ç»†å®¡æŸ¥äº†SAMï¼šæ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ ã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å°†è§†è§‰åŸºç¡€æ¨¡å‹ç”¨äºWSSSçš„æ½œåŠ›å’ŒæŒ‘æˆ˜ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„ç ”ç©¶é¢†åŸŸçš„æœªæ¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13026v2">PDF</a> Accepted to ACM Computing Surveys</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚è¯­ä¹‰åˆ†å‰²ç®—æ³•é€šå¸¸ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼ˆå³å¯¹è±¡æ©è†œï¼‰çš„å¯ç”¨æ€§ï¼Œè¿™äº›æ ‡ç­¾çš„è·å–æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ã€‚å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯ä¸€ç§é¿å…æ­¤ç±»æ ‡ç­¾çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå®ƒä»…åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´çš„æ³¨é‡Šï¼Œä¸ºå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²æä¾›äº†å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾çš„WSSSï¼Œè¿™æ˜¯WSSSä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚æ–‡ç« åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¯¹ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œä¸»è¦å…³æ³¨åœ¨ä¸»è¦ç ”ç©¶ä¼šè®®ä¸Šå‘è¡¨çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„æ–¹æ³•æ“ä½œåœ°å°†å®ƒä»¬åˆ†ä¸ºå››ç»„ï¼šåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢è®¨äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Segment Anything Modelï¼ŒSAMï¼‰åœ¨WSSSä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¯¹SAMåœ¨ä¸¤ä¸ªæœ‰è¶£åœºæ™¯ä¸­çš„é€‚ç”¨æ€§è¿›è¡Œäº†å®¡è§†ï¼šæ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ ã€‚æˆ‘ä»¬æ·±å…¥äº†è§£äº†å°†è§†è§‰åŸºç¡€æ¨¡å‹ç”¨äºWSSSçš„æ½œåŠ›å’ŒæŒ‘æˆ˜ï¼Œä¸ºè¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„ç ”ç©¶é¢†åŸŸæœªæ¥çš„å‘å±•æä¾›äº†ä¾¿åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ çš„è¿›æ­¥æ˜¾è‘—æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>è¯­ä¹‰åˆ†å‰²ç®—æ³•ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼Œä½†è¿™äº›æ ‡ç­¾çš„è·å–æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯ä¸€ç§åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´æ³¨é‡Šçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½œä¸ºå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾çš„WSSSæ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚</li>
<li>æ–‡ç« è°ƒæŸ¥äº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶æ ¹æ®å…¶æ“ä½œæ–¹å¼è¿›è¡Œäº†åˆ†ç±»ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Segment Anything Modelï¼‰åœ¨WSSSä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5de8bbf35b6e19bb15d97b70d1365142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6f366106e2aa1a029e0793160273fb8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c826195ccfa445054e06431b7d6c4bdc.jpg" align="middle">
</details>




<h2 id="NoisyNN-Exploring-the-Impact-of-Information-Entropy-Change-in-Learning-Systems"><a href="#NoisyNN-Exploring-the-Impact-of-Information-Entropy-Change-in-Learning-Systems" class="headerlink" title="NoisyNN: Exploring the Impact of Information Entropy Change in Learning   Systems"></a>NoisyNN: Exploring the Impact of Information Entropy Change in Learning   Systems</h2><p><strong>Authors:Xiaowei Yu, Zhe Huang, Minheng Chen, Yao Xue, Tianming Liu, Dajiang Zhu</strong></p>
<p>We investigate the impact of entropy change in deep learning systems by noise injection at different levels, including the embedding space and the image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep models under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the task complexity. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of 95$%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change. </p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡ä¸åŒå±‚æ¬¡çš„å™ªå£°æ³¨å…¥ï¼ŒåŒ…æ‹¬åµŒå…¥ç©ºé—´å’Œå›¾åƒï¼Œç ”ç©¶äº†ç†µå˜åŒ–å¯¹æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å½±å“ã€‚é‡‡ç”¨æˆ‘ä»¬æ–¹æ³•çš„ç³»åˆ—æ¨¡å‹ç»Ÿç§°ä¸ºâ€œå™ªå£°ç¥ç»ç½‘ç»œï¼ˆNoisy Neural Networksï¼ŒNoisyNNï¼‰â€ï¼Œä¾‹å¦‚NoisyViTå’ŒNoisyCNNç­‰ã€‚å™ªå£°åœ¨ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå„ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ï¼‰ä»¥åŠä¸åŒå­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»å’Œè¿ç§»å­¦ä¹ ï¼‰ä¸­çš„æœ‰å®³æ‰°åŠ¨ã€‚ç„¶è€Œï¼Œè¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå™ªå£°å¯ä»¥æ˜¯æ”¹å˜å­¦ä¹ ç³»ç»Ÿç†µçš„æœ‰æ•ˆé€”å¾„ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œç‰¹å®šå™ªå£°å¯ä»¥æé«˜å„ç§æ·±åº¦æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†æ­£é¢å™ªå£°é€šè¿‡å‡å°‘ä¿¡æ¯ç†µå®šä¹‰çš„ä»»åŠ¡å¤æ‚æ€§æ‰€å¸¦æ¥çš„æå‡ï¼Œå¹¶é€šè¿‡å®éªŒå±•ç¤ºäº†åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¿¡æ¯ç†µæ¥å®šä¹‰ä»»åŠ¡çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å°†å™ªå£°åˆ†ä¸ºæ­£é¢å™ªå£°ï¼ˆPNï¼‰å’Œæœ‰å®³å™ªå£°ï¼ˆHNï¼‰ä¸¤ç§ç±»å‹ï¼Œè¿™å–å†³äºå™ªå£°æ˜¯å¦æœ‰åŠ©äºé™ä½ä»»åŠ¡å¤æ‚æ€§ã€‚å¯¹CNNå’ŒViTçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ä¸»åŠ¨æ³¨å…¥æ­£é¢å™ªå£°å¯ä»¥å®ç°æ€§èƒ½æ”¹è¿›ï¼Œæˆ‘ä»¬åœ¨ImageNetä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„95%çš„Top-1å‡†ç¡®ç‡ã€‚ç†è®ºåˆ†æå’Œå®è¯è¯æ®éƒ½è¯å®ï¼Œæ­£é¢å™ªå£°çš„å­˜åœ¨æœ‰ç›Šäºå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œä¼ ç»Ÿä¸Šè®¤ä¸ºçš„æœ‰å®³å™ªå£°ç¡®å®ä¼šæŸå®³æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å™ªå£°çš„ä¸åŒä½œç”¨ä¸ºç‰¹å®šä»»åŠ¡çš„æ·±åº¦æ¨¡å‹æä¾›äº†æ–°çš„è§£é‡Šï¼Œä¸ºæé«˜æ¨¡å‹æ€§èƒ½æä¾›äº†æ–°çš„èŒƒå¼ã€‚æ­¤å¤–ï¼Œå®ƒæé†’æˆ‘ä»¬ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å½±å“ä¿¡æ¯ç†µæ¥æ”¹å˜å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10625v4">PDF</a> Task Entropy, NoisyViT, NoisyCNN</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­ç†µå˜åŒ–çš„å½±å“ï¼Œé€šè¿‡åœ¨ä¸åŒå±‚çº§æ³¨å…¥å™ªå£°ï¼Œå¦‚åµŒå…¥ç©ºé—´å’Œå›¾åƒã€‚æ–‡ç« æ¢è®¨äº†å™ªå£°å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å½±å“ï¼Œå¹¶æå‡ºå°†æ­¤ç±»æ¨¡å‹ç§°ä¸ºâ€œå™ªå£°ç¥ç»ç½‘ç»œâ€ï¼ˆNoisyNNï¼‰ï¼Œä¾‹å¦‚NoisyViTå’ŒNoisyCNNã€‚å°½ç®¡å™ªå£°åœ¨ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå¯¹æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚CNNå’ŒViTï¼‰ä»¥åŠå›¾åƒåˆ†ç±»å’Œè¿ç§»å­¦ä¹ ç­‰å­¦ä¹ ä»»åŠ¡çš„æœ‰å®³æ‰°åŠ¨ï¼Œä½†æœ¬æ–‡å±•ç¤ºäº†å™ªå£°å¯ä»¥ä½œä¸ºæ”¹å˜å­¦ä¹ ç³»ç»Ÿç†µçš„æœ‰æ•ˆæ–¹æ³•ã€‚æ–‡ç« è¯æ˜ç‰¹å®šå™ªå£°åœ¨æŸäº›æ¡ä»¶ä¸‹å¯ä»¥ä¿ƒè¿›å„ç§æ·±åº¦æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨ä¿¡æ¯ç†µæ¥å®šä¹‰ä»»åŠ¡å¤æ‚æ€§ã€‚æ–‡ç« å°†å™ªå£°åˆ†ä¸ºæ­£å‘å™ªå£°å’Œæœ‰å®³å™ªå£°ä¸¤ç±»ï¼ŒåŸºäºå™ªå£°æ˜¯å¦èƒ½æœ‰åŠ©äºé™ä½ä»»åŠ¡å¤æ‚æ€§ã€‚åœ¨ImageNetç­‰å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ä¸»åŠ¨æ³¨å…¥æ­£å‘å™ªå£°å¯ä»¥å®ç°æ€§èƒ½æ”¹è¿›ï¼Œå¹¶åœ¨ImageNetä¸Šè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„95%çš„Top-1å‡†ç¡®ç‡ã€‚ç†è®ºåˆ†æå’Œå®è¯è¯æ®è¡¨æ˜ï¼Œæ­£å‘å™ªå£°çš„å­˜åœ¨æœ‰ç›Šäºå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œä¼ ç»Ÿè®¤ä¸ºçš„æœ‰å®³å™ªå£°ç¡®å®ä¼šæŸå®³æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¸åŒçš„å™ªå£°è§’è‰²ä¸ºç‰¹å®šä»»åŠ¡ä¸Šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ–°çš„è§£é‡Šå’Œæ”¹è¿›æ¨¡å‹æ€§èƒ½çš„èŒƒä¾‹ã€‚åŒæ—¶æé†’æˆ‘ä»¬ï¼Œå¯ä»¥é€šè¿‡æ”¹å˜ä¿¡æ¯ç†µæ¥å½±å“å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†é€šè¿‡å™ªå£°æ³¨å…¥æ”¹å˜æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­çš„ç†µå˜åŒ–ã€‚</li>
<li>ä»‹ç»äº†æ­£å‘å™ªå£°å’Œæœ‰å®³å™ªå£°çš„æ¦‚å¿µï¼Œå‰è€…èƒ½é™ä½ä»»åŠ¡å¤æ‚æ€§å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šï¼Œå¦‚ImageNetï¼Œæ­£å‘å™ªå£°èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œè¾¾åˆ°å‰æ‰€æœªæœ‰çš„å‡†ç¡®ç‡ã€‚</li>
<li>ç†è®ºåˆ†æå’Œå®è¯è¯æ®è¡¨æ˜æ­£å‘å™ªå£°æœ‰ç›Šäºå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œæœ‰å®³å™ªå£°ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸åŒçš„å™ªå£°è§’è‰²ä¸ºç‰¹å®šä»»åŠ¡ä¸Šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ–°çš„è§£é‡Šå’Œæ”¹è¿›èŒƒä¾‹ã€‚</li>
<li>å¼ºè°ƒäº†ä¿¡æ¯ç†µåœ¨å½±å“å­¦ä¹ ç³»ç»Ÿæ€§èƒ½ä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c9d81b126fb99dbe4d9bbb0b950177d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-45374987c49986a53eeed8988fe0bda8.jpg" align="middle">
</details>




<h2 id="Retrieval-Enhanced-Visual-Prompt-Learning-for-Few-shot-Classification"><a href="#Retrieval-Enhanced-Visual-Prompt-Learning-for-Few-shot-Classification" class="headerlink" title="Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"></a>Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</h2><p><strong>Authors:Jintao Rong, Hao Chen, Linlin Ou, Tianxiao Chen, Xinyi Yu, Yifan Liu</strong></p>
<p>The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. Reprompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼å·²è¢«å¹¿æ³›é‡‡ç”¨ä»¥å¢å¼ºå…¶å¯¹è¿™äº›ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºåŸŸå·®è·çš„æ‰©å¤§ï¼Œå½“å‰èŒƒå¼å¯èƒ½åœ¨ç»†ç²’åº¦åˆ†ç±»ï¼ˆå¦‚å«æ˜Ÿå›¾åƒè¯†åˆ«ï¼‰æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ£€ç´¢å¢å¼ºè§†è§‰æç¤ºå­¦ä¹ ï¼ˆRePromptï¼‰ï¼Œå®ƒå¼•å…¥äº†æ£€ç´¢æœºåˆ¶æ¥ç¼“å­˜å’Œé‡å¤ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„çŸ¥è¯†ã€‚RePromptä»è®­ç»ƒæ ·æœ¬æˆ–å¯ç”¨çš„å¤–éƒ¨æ•°æ®ä¸­æ„å»ºæ£€ç´¢æ•°æ®åº“ï¼Œå¹¶ä½¿ç”¨æ£€ç´¢æœºåˆ¶å¢å¼ºç®€å•æç¤ºå­¦ä¹ åŸºçº¿ï¼ˆbaselineï¼‰çš„å¤šä¸ªé˜¶æ®µï¼Œä»è€Œç¼©å°åŸŸå·®è·ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¢å¼ºçš„æ¨¡å‹å¯ä»¥å¼•ç”¨æ£€ç´¢å¸¦æ¥çš„ç›¸ä¼¼æ ·æœ¬ä»¥åšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼Œæ£€ç´¢æœ‰åŠ©äºæ”¹å–„åæœŸç‰¹å¾çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚RePromptåœ¨å¹¿æ³›çš„è§†è§‰æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬11ä¸ªå›¾åƒæ•°æ®é›†ã€3ä¸ªè§†é¢‘æ•°æ®é›†ã€1ä¸ªå¤šè§†å›¾æ•°æ®é›†å’Œ4ä¸ªåŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.02243v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹å¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ï¼Œé€šè¿‡å°‘æ ·æœ¬å­¦ä¹ æ¨¡å¼æå‡äº†æ¨¡å‹å®¹é‡ã€‚ä½†åœ¨ç²¾ç»†åˆ†ç±»ä»»åŠ¡ä¸­å¦‚å«æ˜Ÿå›¾åƒè¯†åˆ«å­˜åœ¨å±€é™ï¼Œä¸ºåº”å¯¹æŒ‘æˆ˜æå‡ºäº†åŸºäºæ£€ç´¢å¢å¼ºçš„è§†è§‰æç¤ºå­¦ä¹ ï¼ˆRePromptï¼‰ã€‚RePrompté€šè¿‡æ„å»ºæ£€ç´¢æ•°æ®åº“ç¼“å­˜å¹¶å¤ç”¨ä¸‹æ¸¸ä»»åŠ¡çŸ¥è¯†ï¼Œå¢å¼ºç®€å•æç¤ºå­¦ä¹ åŸºçº¿æ¨¡å‹çš„å¤šä¸ªé˜¶æ®µï¼Œç¼©å°é¢†åŸŸå·®è·ã€‚æ¨ç†é˜¶æ®µæ¨¡å‹èƒ½å¼•ç”¨æ£€ç´¢å¼•å…¥çš„ç›¸ä¼¼æ ·æœ¬ä½œå‡ºæ›´ç²¾ç¡®é¢„æµ‹ã€‚è¯¦ç»†åˆ†æè¡¨æ˜æ£€ç´¢æ”¹å–„ç‰¹å¾åˆ†å¸ƒçš„å°¾éƒ¨é˜¶æ®µè¿›è€Œæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ã€‚RePromptåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šå–å¾—é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬å›¾åƒæ•°æ®é›†ã€è§†é¢‘æ•°æ®é›†å’Œå¤šè§†å›¾æ•°æ®é›†ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPæ¨¡å‹å¹¿æ³›åº”ç”¨äºä¸‹æ¸¸è§†è§‰ä»»åŠ¡ï¼Œä½†é¢ä¸´ç²¾ç»†åˆ†ç±»ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>RePromptæ¨¡å‹å¼•å…¥æ£€ç´¢æœºåˆ¶ç¼“å­˜å¹¶å¤ç”¨ä¸‹æ¸¸ä»»åŠ¡çŸ¥è¯†ä»¥ç¼©å°é¢†åŸŸå·®è·ã€‚</li>
<li>RePromptæ„å»ºæ£€ç´¢æ•°æ®åº“å¢å¼ºæç¤ºå­¦ä¹ åŸºçº¿æ¨¡å‹çš„å¤šä¸ªé˜¶æ®µã€‚</li>
<li>æ£€ç´¢æœºåˆ¶æœ‰åŠ©äºæ”¹å–„ç‰¹å¾åˆ†å¸ƒçš„å°¾éƒ¨é˜¶æ®µï¼Œæå‡æ¨¡å‹æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>RePromptåœ¨å¤šç§è§†è§‰æ•°æ®é›†ä¸Šå–å¾—é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œå¤šè§†å›¾æ•°æ®é›†ç­‰ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-025d9590e06a0b2170c821bf8e254402.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-50e6c61180a9242d3442e2b49b197a9e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4251402a4494d924dc0ed0d4211dffd7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-300e5ceb1c743c415d6bd844f9567a29.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.17425v1/page_5_0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6b1c31a0d8f9f8fc8e1eacd6162a387c.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Exploring What Why and How A Multifaceted Benchmark for Causation   Understanding of Video Anomaly
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
