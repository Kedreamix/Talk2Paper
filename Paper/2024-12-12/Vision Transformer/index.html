<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Vision Transformer"><meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  EOV-Seg Efficient Open-Vocabulary Panoptic Segmentation"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>Vision Transformer | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>é¦–é¡µ</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>æ ‡ç­¾</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>åˆ†ç±»</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>å½’æ¡£</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> é¦–é¡µ</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> æ ‡ç­¾</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> åˆ†ç±»</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> å½’æ¡£</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://picx.zhimg.com/v2-26403bcf86a4e06aa0a0588d5f8dc98d.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Vision Transformer</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/Vision-Transformer/"><span class="chip bg-color">Vision Transformer</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">Vision Transformer</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp; 2024-12-12</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp; 2024-12-13</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> æ–‡ç« å­—æ•°:&nbsp;&nbsp; 31.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> é˜…è¯»æ—¶é•¿:&nbsp;&nbsp; 128 åˆ†</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation"><a href="#EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation" class="headerlink" title="EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation"></a>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</h2><p><strong>Authors:Hongwei Niu, Jie Hu, Jianghang Lin, Shengchuan Zhang</strong></p><p>Open-vocabulary panoptic segmentation aims to segment and classify everything in diverse scenes across an unbounded vocabulary. Existing methods typically employ two-stage or single-stage framework. The two-stage framework involves cropping the image multiple times using masks generated by a mask generator, followed by feature extraction, while the single-stage framework relies on a heavyweight mask decoder to make up for the lack of spatial position information through self-attention and cross-attention in multiple stacked Transformer blocks. Both methods incur substantial computational overhead, thereby hindering the efficiency of model inference. To fill the gap in efficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and spatial-aware framework designed for open-vocabulary panoptic segmentation. Specifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware Selection (VAS) module is proposed to improve the semantic comprehension of visual aggregated features and alleviate the feature interaction burden on the mask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE), which efficiently utilizes the spatial awareness capabilities of ViT-based CLIP backbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary panoptic segmentation framework towards efficiency, which runs faster and achieves competitive performance compared with state-of-the-art methods. Specifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and 12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and the inference time of EOV-Seg is 4-21 times faster than state-of-the-art methods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with only 71M parameters on a single RTX 3090 GPU. Code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%7D">https://github.com/nhw649/EOV-Seg}</a>.</p><blockquote><p>å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ—¨åœ¨åˆ†å‰²å¹¶åˆ†ç±»æ— é™è¯æ±‡è¡¨ä¸­ä¸åŒåœºæ™¯ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæˆ–å•é˜¶æ®µæ¡†æ¶ã€‚ä¸¤é˜¶æ®µæ¡†æ¶æ¶‰åŠä½¿ç”¨ç”±æ©è†œç”Ÿæˆå™¨ç”Ÿæˆçš„æ©è†œå¤šæ¬¡è£å‰ªå›¾åƒï¼Œç„¶åè¿›è¡Œç‰¹å¾æå–ï¼Œè€Œå•é˜¶æ®µæ¡†æ¶ä¾èµ–äºé‡é‡çº§æ©è†œè§£ç å™¨ï¼Œé€šè¿‡å †å çš„å¤šä¸ªTransformerå—ä¸­çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æ¥å¼¥è¡¥ç©ºé—´ä½ç½®ä¿¡æ¯çš„ç¼ºå¤±ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œå½±å“äº†æ¨¡å‹æ¨ç†çš„æ•ˆç‡ã€‚ä¸ºäº†å¼¥è¡¥æ•ˆç‡ä¸Šçš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†EOV-Segï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²çš„æ–°å‹å•é˜¶æ®µå…±äº«é«˜æ•ˆç©ºé—´æ„ŸçŸ¥æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒEOV-Segåœ¨ä¸¤ä¸ªæ–¹é¢è¿›è¡Œäº†åˆ›æ–°ã€‚é¦–å…ˆï¼Œæå‡ºäº†è¯æ±‡æ„ŸçŸ¥é€‰æ‹©ï¼ˆVASï¼‰æ¨¡å—ï¼Œä»¥æé«˜è§†è§‰èšåˆç‰¹å¾è¯­ä¹‰ç†è§£èƒ½åŠ›å¹¶å‡è½»æ©è†œè§£ç å™¨çš„ç‰¹å¾äº¤äº’è´Ÿæ‹…ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‘åŠ¨æ€åµŒå…¥ä¸“å®¶ï¼ˆTDEEï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†åŸºäºViTçš„CLIPä¸»å¹²çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒEOV-Segæ˜¯é¢å‘æ•ˆç‡çš„é¦–ä¸ªå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ¡†æ¶ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒè¿è¡Œæ›´å¿«å¹¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œä»…åœ¨COCOæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒEOV-Segåœ¨ADE20Kæ•°æ®é›†ä¸Šçš„å…¨æ™¯å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†24.2çš„PQï¼ˆå…¨æ™¯è´¨é‡ï¼‰ã€31.6çš„mIoUï¼ˆå¹³å‡äº¤å¹¶æ¯”ï¼‰ï¼Œå¹¶ä¸”æ¯ç§’å¤„ç†12.7å¸§ã€‚æ­¤å¤–ï¼Œé…å¤‡ResNet-50ä¸»å¹²çš„EOV-Segåœ¨å•ä¸ªRTX 3090 GPUä¸Šä»…ä»¥71Må‚æ•°å°±èƒ½è¾¾åˆ°æ¯ç§’25å¸§çš„å¤„ç†é€Ÿåº¦ã€‚ä»£ç å¯é€šè¿‡\url{<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%7D%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/nhw649/EOV-Seg}è·å–ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08628v1">PDF</a> Accepted by AAAI 2025</p><p><strong>Summary</strong><br>æœ¬æ‘˜è¦ä»‹ç»äº†é’ˆå¯¹å¼€æ”¾å¼è¯æ±‡è¡¨å…¨æ™¯åˆ†å‰²çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆã€å•é˜¶æ®µçš„ç©ºé—´æ„ŸçŸ¥æ¡†æ¶EOV-Segã€‚å®ƒé€šè¿‡åˆ›æ–°çš„Vocabulary-Aware Selectionæ¨¡å—å’ŒTwo-way Dynamic Embedding Expertsæ–¹æ³•æé«˜äº†è¯­ä¹‰ç†è§£å’Œç‰¹å¾äº¤äº’æ•ˆç‡ï¼Œä½¿å¾—æ¨¡å‹æ›´æœ‰æ•ˆç‡å¹¶ä¸”è¡¨ç°è‰¯å¥½ã€‚å®ƒåœ¨æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œç‰¹åˆ«æ˜¯åœ¨COCOè®­ç»ƒä¸Šå–å¾—äº†å¯è§‚çš„æˆç»©ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†è¿è¡Œé€Ÿåº¦ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šå…±äº«ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>EOV-Segæ˜¯ä¸€ä¸ªé’ˆå¯¹å¼€æ”¾å¼è¯æ±‡è¡¨å…¨æ™¯åˆ†å‰²çš„é«˜æ•ˆã€å•é˜¶æ®µæ¡†æ¶ã€‚</li><li>EOV-Segé€šè¿‡åˆ›æ–°çš„VASæ¨¡å—æ”¹è¿›äº†è¯­ä¹‰ç†è§£ï¼Œå¹¶å‡è½»äº†ç‰¹å¾äº¤äº’çš„è´Ÿæ‹…ã€‚</li><li>TDEEæ¨¡å—çš„å¼•å…¥æœ‰æ•ˆåˆ©ç”¨äº†åŸºäºViTçš„CLIPéª¨å¹²ç½‘çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚</li><li>EOV-Segåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ADE20Kæ•°æ®é›†ä¸Šçš„å…¨æ™¯å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æˆç»©ã€‚</li><li>ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒEOV-Segæ˜¾è‘—æé«˜äº†è¿è¡Œé€Ÿåº¦ï¼Œå¹¶ä¸”å‚æ•°æ›´å°‘ã€‚</li><li>EOV-Segçš„ä»£ç å·²ç»å…¬å¼€å¯ä¾›ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1a4c8da09353ee9d88ed21e70da5e270.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c2d13e725ff84932cdcc5baa8246aae.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fe03575aa93513e66c4d741401bd1e8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0a26926b98fb664f5c2de0901a0a265c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e55df3ae6bba2f5adc54dab7fecb8925.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4abdef90e4c964b25d477441019e3fb2.jpg" align="middle"></details><h2 id="SAM-Mamba-Mamba-Guided-SAM-Architecture-for-Generalized-Zero-Shot-Polyp-Segmentation"><a href="#SAM-Mamba-Mamba-Guided-SAM-Architecture-for-Generalized-Zero-Shot-Polyp-Segmentation" class="headerlink" title="SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp   Segmentation"></a>SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp Segmentation</h2><p><strong>Authors:Tapas Kumar Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha</strong></p><p>Polyp segmentation in colonoscopy is crucial for detecting colorectal cancer. However, it is challenging due to variations in the structure, color, and size of polyps, as well as the lack of clear boundaries with surrounding tissues. Traditional segmentation models based on Convolutional Neural Networks (CNNs) struggle to capture detailed patterns and global context, limiting their performance. Vision Transformer (ViT)-based models address some of these issues but have difficulties in capturing local context and lack strong zero-shot generalization. To this end, we propose the Mamba-guided Segment Anything Model (SAM-Mamba) for efficient polyp segmentation. Our approach introduces a Mamba-Prior module in the encoder to bridge the gap between the general pre-trained representation of SAM and polyp-relevant trivial clues. It injects salient cues of polyp images into the SAM image encoder as a domain prior while capturing global dependencies at various scales, leading to more accurate segmentation results. Extensive experiments on five benchmark datasets show that SAM-Mamba outperforms traditional CNN, ViT, and Adapter-based models in both quantitative and qualitative measures. Additionally, SAM-Mamba demonstrates excellent adaptability to unseen datasets, making it highly suitable for real-time clinical use.</p><blockquote><p>ç»“è‚ é•œæ£€æŸ¥ä¸­çš„æ¯è‚‰åˆ†å‰²å¯¹äºæ£€æµ‹ç»“ç›´è‚ ç™Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ¯è‚‰çš„ç»“æ„ã€é¢œè‰²å’Œå¤§å°å·®å¼‚ä»¥åŠä¸å‘¨å›´ç»„ç»‡çš„è¾¹ç•Œä¸æ¸…ï¼Œè¿™å¸¦æ¥äº†æå¤§çš„æŒ‘æˆ˜ã€‚åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹éš¾ä»¥æ•æ‰è¯¦ç»†çš„æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œä»è€Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¨¡å‹è§£å†³äº†å…¶ä¸­çš„ä¸€äº›é—®é¢˜ï¼Œä½†åœ¨æ•è·å±€éƒ¨ä¸Šä¸‹æ–‡å’Œç¼ºä¹å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Mambaå¼•å¯¼çš„åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAM-Mambaï¼‰ï¼Œç”¨äºæœ‰æ•ˆçš„æ¯è‚‰åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¼–ç å™¨ä¸­åŠ å…¥äº†ä¸€ä¸ªMamba-Prioræ¨¡å—ï¼Œä»¥å¼¥åˆSAMçš„ä¸€èˆ¬é¢„è®­ç»ƒè¡¨ç¤ºå’Œæ¯è‚‰ç›¸å…³çš„çç¢çº¿ç´¢ä¹‹é—´çš„é¸¿æ²Ÿã€‚å®ƒå°†æ¯è‚‰å›¾åƒçš„å…³é”®çº¿ç´¢æ³¨å…¥SAMå›¾åƒç¼–ç å™¨ä½œä¸ºé¢†åŸŸå…ˆéªŒï¼ŒåŒæ—¶æ•æ‰ä¸åŒå°ºåº¦çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œä»è€Œå¾—åˆ°æ›´ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSAM-Mambaåœ¨å®šé‡å’Œå®šæ€§æªæ–½ä¸Šéƒ½ä¼˜äºä¼ ç»Ÿçš„CNNã€ViTå’ŒåŸºäºé€‚é…å™¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSAM-Mambaåœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„é€‚åº”æ€§ï¼Œéå¸¸é€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08482v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ç»“è‚ é•œæ£€æŸ¥ä¸­æ¯è‚‰åˆ†å‰²å¯¹æ£€æµ‹ç»“ç›´è‚ ç™Œçš„é‡è¦æ€§ã€‚ç”±äºæ¯è‚‰ç»“æ„ã€é¢œè‰²å’Œå°ºå¯¸çš„å·®å¼‚ä»¥åŠä¸å‘¨å›´ç»„ç»‡çš„è¾¹ç•Œä¸æ¸…ï¼Œå¯¼è‡´åˆ†å‰²ä»»åŠ¡å……æ»¡æŒ‘æˆ˜ã€‚ä¼ ç»ŸåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„åˆ†å‰²æ¨¡å‹éš¾ä»¥æ•æ‰è¯¦ç»†æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ€§èƒ½å—é™ã€‚Vision Transformerï¼ˆViTï¼‰æ¨¡å‹å¯ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†åœ¨æ•è·å±€éƒ¨ä¸Šä¸‹æ–‡å’Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Mambaå¼•å¯¼çš„åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAM-Mambaï¼‰ï¼Œç”¨äºé«˜æ•ˆæ¯è‚‰åˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨ç¼–ç å™¨ä¸­åŠ å…¥Mamba-Prioræ¨¡å—ï¼Œä»¥å¼¥è¡¥SAMçš„ä¸€èˆ¬é¢„è®­ç»ƒè¡¨ç¤ºå’Œæ¯è‚‰ç›¸å…³çº¿ç´¢ä¹‹é—´çš„å·®è·ã€‚å®ƒå°†æ¯è‚‰å›¾åƒçš„å…³é”®çº¿ç´¢æ³¨å…¥SAMå›¾åƒç¼–ç å™¨ä½œä¸ºé¢†åŸŸå…ˆéªŒï¼ŒåŒæ—¶æ•æ‰ä¸åŒå°ºåº¦çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSAM-Mambaåœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»ŸCNNã€ViTå’Œé€‚é…å™¨æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSAM-Mambaåœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„é€‚åº”æ€§ï¼Œéå¸¸é€‚åˆå®æ—¶ä¸´åºŠä½¿ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç»“è‚ é•œæ£€æŸ¥ä¸­çš„æ¯è‚‰åˆ†å‰²å¯¹æ£€æµ‹ç»“ç›´è‚ ç™Œè‡³å…³é‡è¦ã€‚</li><li>ä¼ ç»ŸCNNæ¨¡å‹åœ¨æ¯è‚‰åˆ†å‰²ä¸Šæ€§èƒ½å—é™ï¼Œéš¾ä»¥æ•æ‰è¯¦ç»†æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li><li>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åœ¨æ¯è‚‰åˆ†å‰²ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†å­˜åœ¨æ•è·å±€éƒ¨ä¸Šä¸‹æ–‡å’Œé›¶æ ·æœ¬æ³›åŒ–çš„å›°éš¾ã€‚</li><li>æå‡ºçš„SAM-Mambaæ¨¡å‹é€šè¿‡å¼•å…¥Mamba-Prioræ¨¡å—ï¼Œç»“åˆäº†ViTå’ŒCNNçš„ä¼˜ç‚¹ï¼Œå®ç°äº†æ›´å‡†ç¡®çš„æ¯è‚‰åˆ†å‰²ã€‚</li><li>SAM-Mambaåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºä¼ ç»ŸCNNã€ViTå’Œé€‚é…å™¨æ¨¡å‹ã€‚</li><li>SAM-Mambaå…·æœ‰è‰¯å¥½çš„æœªçŸ¥æ•°æ®é€‚åº”æ€§ï¼Œé€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-541edeac0d0fb433f0faca63f08a05e3.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b9bcc80c550949c0421566ca502aa5c9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c1c472678ee9be4b29eb58d0ad79eb99.jpg" align="middle"></details><h2 id="TextRefiner-Internal-Visual-Feature-as-Efficient-Refiner-for-Vision-Language-Models-Prompt-Tuning"><a href="#TextRefiner-Internal-Visual-Feature-as-Efficient-Refiner-for-Vision-Language-Models-Prompt-Tuning" class="headerlink" title="TextRefiner: Internal Visual Feature as Efficient Refiner for   Vision-Language Models Prompt Tuning"></a>TextRefiner: Internal Visual Feature as Efficient Refiner for Vision-Language Models Prompt Tuning</h2><p><strong>Authors:Jingjing Xie, Yuxin Zhang, Jun Peng, Zhaohong Huang, Liujuan Cao</strong></p><p>Despite the efficiency of prompt learning in transferring vision-language models (VLMs) to downstream tasks, existing methods mainly learn the prompts in a coarse-grained manner where the learned prompt vectors are shared across all categories. Consequently, the tailored prompts often fail to discern class-specific visual concepts, thereby hindering the transferred performance for classes that share similar or complex visual attributes. Recent advances mitigate this challenge by leveraging external knowledge from Large Language Models (LLMs) to furnish class descriptions, yet incurring notable inference costs. In this paper, we introduce TextRefiner, a plug-and-play method to refine the text prompts of existing methods by leveraging the internal knowledge of VLMs. Particularly, TextRefiner builds a novel local cache module to encapsulate fine-grained visual concepts derivedfrom local tokens within the image branch. By aggregating and aligning the cached visual descriptions with the original output of the text branch, TextRefiner can efficiently refine and enrich the learned prompts from existing methods without relying on any external expertise. For example, it improves the performance of CoOp from 71.66 % to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise features for text prompts. Equipped with TextRefiner, PromptKD achieves state-of-the-art performance and is efficient in inference. Our code is relesed at <a target="_blank" rel="noopener" href="https://github.com/xjjxmu/TextRefiner">https://github.com/xjjxmu/TextRefiner</a></p><blockquote><p>å°½ç®¡æç¤ºå­¦ä¹ åœ¨å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶éå¸¸æœ‰æ•ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨äº†ç²—ç²’åº¦çš„æ–¹å¼å­¦ä¹ æç¤ºï¼Œå…¶ä¸­å­¦ä¹ åˆ°çš„æç¤ºå‘é‡ä¼šå…±äº«ç»™æ‰€æœ‰ç±»åˆ«ã€‚å› æ­¤ï¼Œé’ˆå¯¹ç‰¹å®šç±»åˆ«çš„æç¤ºå¾€å¾€æ— æ³•åŒºåˆ†ç‰¹å®šç±»åˆ«çš„è§†è§‰æ¦‚å¿µï¼Œä»è€Œé˜»ç¢äº†ç›¸ä¼¼æˆ–å¤æ‚è§†è§‰å±æ€§ç±»åˆ«çš„è¿ç§»æ€§èƒ½ã€‚æœ€è¿‘çš„è¿›å±•é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤–éƒ¨çŸ¥è¯†æ¥æä¾›ç±»åˆ«æè¿°æ¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†è¿™ä¼šå¯¼è‡´æ¨ç†æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TextRefinerï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†æ¥ä¼˜åŒ–ç°æœ‰æ–¹æ³•çš„æ–‡æœ¬æç¤ºçš„å³æ’å³ç”¨æ–¹æ³•ã€‚ç‰¹åˆ«åœ°ï¼ŒTextRefineræ„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„å±€éƒ¨ç¼“å­˜æ¨¡å—ï¼Œä»¥å°è£…æ¥è‡ªå›¾åƒåˆ†æ”¯çš„å±€éƒ¨æ ‡è®°ä¸­æ´¾ç”Ÿçš„ç»†ç²’åº¦è§†è§‰æ¦‚å¿µã€‚é€šè¿‡èšåˆå’Œå¯¹é½ç¼“å­˜çš„è§†è§‰æè¿°ä¸æ–‡æœ¬åˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼ŒTextRefinerå¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–å’Œä¸°å¯Œç°æœ‰æ–¹æ³•çš„æ‰€å­¦æç¤ºï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨ä¸“å®¶çŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå°†CoOpçš„æ€§èƒ½ä»71.66%æé«˜åˆ°76.94%ï¼Œè¶…è¶Šäº†å¼•å…¥å®ä¾‹ç‰¹å¾ç”¨äºæ–‡æœ¬æç¤ºçš„CoCoOpã€‚é…å¤‡TextRefinerçš„PromptKDè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ¨ç†æ–¹é¢ä¹Ÿéå¸¸é«˜æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/xjjxmu/TextRefiner">https://github.com/xjjxmu/TextRefiner</a>ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08176v1">PDF</a> Accepted by AAAI2025</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTextRefinerçš„å³æ’å³ç”¨æ–¹æ³•ï¼Œç”¨äºé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å†…éƒ¨çŸ¥è¯†æ¥ä¼˜åŒ–ç°æœ‰æ–¹æ³•çš„æ–‡æœ¬æç¤ºã€‚TextRefinerå»ºç«‹äº†ä¸€ä¸ªæ–°å‹æœ¬åœ°ç¼“å­˜æ¨¡å—ï¼Œä»¥å°è£…å›¾åƒåˆ†æ”¯ä¸­å±€éƒ¨æ ‡è®°çš„ç»†ç²’åº¦è§†è§‰æ¦‚å¿µã€‚é€šè¿‡èšåˆå’Œå¯¹é½ç¼“å­˜çš„è§†è§‰æè¿°ä¸æ–‡æœ¬åˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼ŒTextRefinerèƒ½å¤Ÿä¼˜åŒ–å¹¶ä¸°å¯Œç°æœ‰æ–¹æ³•çš„å·²å­¦æç¤ºï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€‚æ­¤æ–¹æ³•æé«˜äº†CoOpåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å¼•å…¥å®ä¾‹çº§ç‰¹å¾çš„CoCoOpæ–¹æ³•ã€‚é…å¤‡TextRefineråï¼ŒPromptKDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å¹¶åœ¨æ¨ç†ä¸­è¡¨ç°å‡ºé«˜æ•ˆç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>TextRefineræ˜¯ä¸€ç§ç”¨äºä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­æ–‡æœ¬æç¤ºçš„æ–¹æ³•ã€‚</li><li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ç²—ç²’åº¦æ–¹å¼å­¦ä¹ æç¤ºï¼Œå¯¼è‡´é’ˆå¯¹ç‰¹å®šç±»åˆ«çš„æç¤ºæ— æ³•åŒºåˆ†ç›¸ä¼¼çš„è§†è§‰æ¦‚å¿µã€‚</li><li>TextRefineré€šè¿‡åˆ©ç”¨VLMsçš„å†…éƒ¨çŸ¥è¯†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li><li>TextRefinerå»ºç«‹äº†ä¸€ä¸ªæœ¬åœ°ç¼“å­˜æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°è£…å›¾åƒåˆ†æ”¯ä¸­çš„ç»†ç²’åº¦è§†è§‰æ¦‚å¿µã€‚</li><li>TextRefineré€šè¿‡èšåˆå’Œå¯¹é½ç¼“å­˜çš„è§†è§‰æè¿°ä¸æ–‡æœ¬åˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼Œä¼˜åŒ–å¹¶ä¸°å¯Œäº†ç°æœ‰æ–¹æ³•çš„æç¤ºã€‚</li><li>TextRefineråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†å¼•å…¥å®ä¾‹çº§ç‰¹å¾çš„CoCoOpæ–¹æ³•ã€‚</li><li>é…å¤‡TextRefineråï¼ŒPromptKDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¡¨ç°å‡ºé«˜æ•ˆç‡ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7d402bcb2d7f33b54c8bfc63dc2df5b7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ba1a62f4d387ed5429c0963047c71f57.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-362f18538a8926ab6d9349ce6cfcd492.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7823e4ced9b560b6aea55bb844dd2591.jpg" align="middle"></details><h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p><p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision.</p><blockquote><p>åœ¨åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸåï¼Œä½¿ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å´é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ ‡è®°æ•°æ®ç¨€ç¼ºä»¥åŠæ–‡æœ¬ç›‘ç£ä¸è¶³ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥æœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ï¼Œä»…ä½¿ç”¨å°‘é‡è¯­ä¹‰æ ‡è®°æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½å…·æœ‰æå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ï¼Œé€šè¿‡åŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºæ¥å®ç°ã€‚ä¸ºäº†å®Œæˆè¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾æç¤ºæ–¹æ³•ï¼Œç„¶åæå‡ºäº†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„é¦–ä¸ªå›¾è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºå¾®è°ƒæ—¶çš„ç›‘ç£ä¸è¶³ï¼Œåœ¨æˆ‘ä»¬çš„èŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒä¸å˜ï¼Œå› æ­¤å¯å­¦ä¹ çš„å‚æ•°è¿œå°‘äºå¯¹ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒã€‚é€šè¿‡åœ¨å®é™…æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„èŒƒå¼åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨æå¼±çš„æ–‡æœ¬ç›‘ç£å°†GNNæ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v1">PDF</a> Preprint, 26 pages</p><p><strong>Summary</strong><br>é’ˆå¯¹åŸºäºCLIPçš„é¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œåœ¨é¢ä¸´ç¼ºä¹æ ‡ç­¾æ•°æ®ã€æ–‡æœ¬ç›‘ç£ä¸è¶³ã€ä¸‹æ¸¸ä»»åŠ¡å±‚æ¬¡ä¸åŒåŠé¢†åŸŸé—´æ¦‚å¿µé¸¿æ²Ÿç­‰æŒ‘æˆ˜æ—¶ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ ï¼Œä»…åˆ©ç”¨å°‘é‡å…·æœ‰æå¼±æ–‡æœ¬ç›‘ç£çš„è¯­ä¹‰æ ‡è®°æ ·æœ¬ï¼Œæœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œé€‚åº”äºä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®çš„åœºæ™¯ã€‚è¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹ç›¸åŒçš„ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºæ¥å®ç°è¿™ä¸€ç›®çš„ã€‚è¿™ä¸€æ–°æ–¹æ³•åœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶åœ¨å°æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†é¦–ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿå€ŸåŠ©æå¼±çš„æ–‡æœ¬ç›‘ç£å°†å›¾ç¥ç»ç½‘ç»œæ¨å¹¿åˆ°æœªè§ç±»åˆ«ä¸­ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>åŸºäºCLIPçš„é¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œé¢ä¸´å¤šæ–¹é¢çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹æ ‡ç­¾æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ä¸è¶³ç­‰ã€‚</li><li>ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½¿å¾—é¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€‚åº”ä¸åŒä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ã€‚</li><li>é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›¸åŒç©ºé—´ï¼Œå®ç°å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºçš„åŒæ­¥å­¦ä¹ ã€‚</li><li>æå‡ºä¸€ç§åŸºäºCLIPçš„é¦–ä¸ªé›¶æ ·æœ¬åˆ†ç±»åŸå‹ç³»ç»Ÿï¼Œå¯ä»¥åœ¨æç«¯æ–‡æœ¬ç›‘ç£æƒ…å†µä¸‹è¿›è¡Œç±»åˆ«æ‰©å±•ã€‚</li><li>ç ”ç©¶æ”¹è¿›äº†ç°æœ‰çš„å›¾æç¤ºæ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</li><li>åœ¨å°æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-418010f18183fa1efe9737e4f27a25e5.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fea15ced17268cfd3a73eccc52e763fa.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a61bf3a22face5a83a5c80d12004d0a7.jpg" align="middle"></details><h2 id="Leveraging-Content-and-Context-Cues-for-Low-Light-Image-Enhancement"><a href="#Leveraging-Content-and-Context-Cues-for-Low-Light-Image-Enhancement" class="headerlink" title="Leveraging Content and Context Cues for Low-Light Image Enhancement"></a>Leveraging Content and Context Cues for Low-Light Image Enhancement</h2><p><strong>Authors:Igor Morawski, Kai He, Shusil Dangi, Winston H. Hsu</strong></p><p>Low-light conditions have an adverse impact on machine cognition, limiting the performance of computer vision systems in real life. Since low-light data is limited and difficult to annotate, we focus on image processing to enhance low-light images and improve the performance of any downstream task model, instead of fine-tuning each of the models which can be prohibitively expensive. We propose to improve the existing zero-reference low-light enhancement by leveraging the CLIP model to capture image prior and for semantic guidance. Specifically, we propose a data augmentation strategy to learn an image prior via prompt learning, based on image sampling, to learn the image prior without any need for paired or unpaired normal-light data. Next, we propose a semantic guidance strategy that maximally takes advantage of existing low-light annotation by introducing both content and context cues about the image training patches. We experimentally show, in a qualitative study, that the proposed prior and semantic guidance help to improve the overall image contrast and hue, as well as improve background-foreground discrimination, resulting in reduced over-saturation and noise over-amplification, common in related zero-reference methods. As we target machine cognition, rather than rely on assuming the correlation between human perception and downstream task performance, we conduct and present an ablation study and comparison with related zero-reference methods in terms of task-based performance across many low-light datasets, including image classification, object and face detection, showing the effectiveness of our proposed method.</p><blockquote><p>ä½å…‰æ¡ä»¶å¯¹æœºå™¨è®¤çŸ¥äº§ç”Ÿä¸åˆ©å½±å“ï¼Œé™åˆ¶äº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨ç°å®ç”Ÿæ´»ä¸­çš„æ€§èƒ½ã€‚ç”±äºä½å…‰æ•°æ®æœ‰é™ä¸”éš¾ä»¥æ ‡æ³¨ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå›¾åƒå¤„ç†ï¼Œä»¥å¢å¼ºä½å…‰å›¾åƒå¹¶æ”¹å–„ä»»ä½•ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸æ˜¯å¾®è°ƒå¯èƒ½éå¸¸æ˜‚è´µçš„æ¯ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨CLIPæ¨¡å‹æ•æ‰å›¾åƒå…ˆéªŒå¹¶è¿›è¡Œè¯­ä¹‰æŒ‡å¯¼ï¼Œä»¥æ”¹è¿›ç°æœ‰çš„æ— å‚è€ƒä½å…‰å¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæç¤ºå­¦ä¹ ï¼ˆprompt learningï¼‰çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡å›¾åƒé‡‡æ ·æ¥å­¦ä¹ å›¾åƒå…ˆéªŒï¼Œæ— éœ€é…å¯¹æˆ–æœªé…å¯¹æ­£å¸¸å…‰æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰æŒ‡å¯¼ç­–ç•¥ï¼Œé€šè¿‡å¼•å…¥å…³äºå›¾åƒè®­ç»ƒè¡¥ä¸çš„å†…å®¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢æ¥å……åˆ†åˆ©ç”¨ç°æœ‰çš„ä½å…‰æ ‡æ³¨ã€‚æˆ‘ä»¬åœ¨å®šæ€§ç ”ç©¶ä¸­é€šè¿‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å…ˆéªŒçŸ¥è¯†å’Œè¯­ä¹‰æŒ‡å¯¼æœ‰åŠ©äºæé«˜å›¾åƒçš„æ•´ä½“å¯¹æ¯”åº¦å’Œè‰²è°ƒï¼Œæé«˜èƒŒæ™¯ä¸å‰æ™¯çš„è¾¨åˆ«èƒ½åŠ›ï¼Œå‡å°‘è¿‡åº¦é¥±å’Œå’Œå™ªå£°è¿‡åº¦æ”¾å¤§ï¼Œè¿™åœ¨ç›¸å…³çš„æ— å‚è€ƒæ–¹æ³•ä¸­å¾ˆå¸¸è§ã€‚ç”±äºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœºå™¨è®¤çŸ¥ï¼Œè€Œä¸æ˜¯ä¾èµ–äººç±»æ„ŸçŸ¥ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„ç›¸å…³æ€§å‡è®¾ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä½å…‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¶ˆèç ”ç©¶å’Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç‰©ä½“å’Œé¢éƒ¨æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07693v1">PDF</a> Accepted to the IEEE Transactions on Multimedia</p><p><strong>æ‘˜è¦</strong><br>æå‡ä½å…‰æ¡ä»¶ä¸‹æœºå™¨è®¤çŸ¥æ€§èƒ½çš„å…³é”®åœ¨äºå¢å¼ºå›¾åƒè´¨é‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨CLIPæ¨¡å‹æ•æ‰å›¾åƒå…ˆéªŒå’Œè¯­ä¹‰æŒ‡å¯¼çš„æ–¹æ³•ï¼Œæ”¹è¿›ç°æœ‰çš„æ— å‚è€ƒä½å…‰å¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å›¾åƒé‡‡æ ·è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå­¦ä¹ å›¾åƒå…ˆéªŒï¼Œæ— éœ€é…å¯¹æˆ–æœªé…å¯¹æ­£å¸¸å…‰ç…§æ•°æ®ã€‚åŒæ—¶ï¼Œæå‡ºè¯­ä¹‰å¼•å¯¼ç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨ç°æœ‰ä½å…‰æ ‡æ³¨ï¼Œå¼•å…¥å›¾åƒè®­ç»ƒè¡¥ä¸çš„å†…å®¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„å…ˆéªŒå’Œè¯­ä¹‰å¼•å¯¼æœ‰åŠ©äºæé«˜å›¾åƒæ•´ä½“å¯¹æ¯”åº¦å’Œè‰²è°ƒï¼Œæ”¹å–„èƒŒæ™¯å‰æ™¯è¾¨è¯†èƒ½åŠ›ï¼Œå‡å°‘ç›¸å…³é›¶å‚è€ƒæ–¹æ³•å¸¸è§çš„è¿‡é¥±å’Œå’Œå™ªå£°è¿‡åº¦æ”¾å¤§é—®é¢˜ã€‚æœ¬ç ”ç©¶ä»¥æœºå™¨è®¤çŸ¥ä¸ºç›®æ ‡ï¼Œç›´æ¥é’ˆå¯¹ä»»åŠ¡æ€§èƒ½è¿›è¡Œæ¶ˆèç ”ç©¶å’Œå¯¹æ¯”å®éªŒï¼Œåœ¨å¤šä¸ªä½å…‰ç…§æ•°æ®é›†ä¸Šè¿›è¡Œå›¾åƒåˆ†ç±»ã€ç‰©ä½“å’Œäººè„¸æ£€æµ‹ä»»åŠ¡ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p><p><strong>å…³é”®è§è§£</strong></p><ol><li>ä½å…‰ç…§æ¡ä»¶å¯¹æœºå™¨è®¤çŸ¥æœ‰è´Ÿé¢å½±å“ï¼Œé™åˆ¶äº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨ç°å®ç”Ÿæ´»ä¸­çš„æ€§èƒ½ã€‚</li><li>æå‡ºåˆ©ç”¨CLIPæ¨¡å‹æ”¹è¿›æ— å‚è€ƒä½å…‰å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡å›¾åƒé‡‡æ ·å­¦ä¹ å›¾åƒå…ˆéªŒã€‚</li><li>å¼•å…¥è¯­ä¹‰å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨ä½å…‰æ ‡æ³¨ä¸­çš„å†…å®¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æé«˜å›¾åƒå¯¹æ¯”åº¦å’Œè‰²è°ƒï¼Œæ”¹å–„èƒŒæ™¯å‰æ™¯è¾¨è¯†èƒ½åŠ›ã€‚</li><li>æ–¹æ³•æœ‰åŠ©äºè§£å†³ç›¸å…³é›¶å‚è€ƒæ–¹æ³•å¸¸è§çš„è¿‡é¥±å’Œå’Œå™ªå£°è¿‡åº¦æ”¾å¤§é—®é¢˜ã€‚</li><li>ç ”ç©¶ä»¥æœºå™¨è®¤çŸ¥ä¸ºç›®æ ‡ï¼Œç›´æ¥é’ˆå¯¹ä»»åŠ¡æ€§èƒ½è¿›è¡Œå®éªŒéªŒè¯ã€‚</li><li>åœ¨å¤šä¸ªä½å…‰ç…§æ•°æ®é›†ä¸Šè¿›è¡Œå›¾åƒåˆ†ç±»ã€ç‰©ä½“å’Œäººè„¸æ£€æµ‹ä»»åŠ¡ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-767a0f93c595a60a9ca97a321bfc3787.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b7b92e95081ad085b9918ddd57fdf836.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6198317ed762b401441d9e4a7949bfa.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d0e888355bc133d29c9f5300aafb061f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-acae8c89829e76df73ac2e0771e6fee5.jpg" align="middle"></details><h2 id="Retaining-and-Enhancing-Pre-trained-Knowledge-in-Vision-Language-Models-with-Prompt-Ensembling"><a href="#Retaining-and-Enhancing-Pre-trained-Knowledge-in-Vision-Language-Models-with-Prompt-Ensembling" class="headerlink" title="Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models   with Prompt Ensembling"></a>Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models with Prompt Ensembling</h2><p><strong>Authors:Donggeun Kim, Yujin Jo, Myungjoo Lee, Taesup Kim</strong></p><p>The advancement of vision-language models, particularly the Contrastive Language-Image Pre-training (CLIP) model, has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to understand and respond to previously unseen data without task-specific training. However, adapting CLIP to integrate specialized knowledge from various domains while retaining its zero-shot capabilities remains a significant challenge. To address this, we introduce a novel prompt ensemble learning approach called Group-wise Prompt Ensemble (GPE). This method aims to enhance CLIPâ€™s zero-shot capabilities by incorporating new domain knowledge while improving its adaptability and robustness against data distribution shifts. Our approach hinges on three main strategies: prompt grouping with masked attention to optimize CLIPâ€™s adaptability while safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts for the seamless integration of new domain insights without disrupting the original modelâ€™s representation; and an ensemble learning strategy that effectively merges original and new knowledge. Through rigorous experimentation, including more challenging cross-dataset transfer evaluations, our GPE method redefines the benchmarks for the adaptability and efficiency of vision-language models, surpassing existing models across various scenarios.</p><blockquote><p>è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹çš„è¿›æ­¥ï¼Œå·²ç»é€šè¿‡å®ç°å¼ºå¤§çš„é›¶æ¬¡å­¦ä¹ ï¼ˆzero-shot learningï¼‰èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚è¿™äº›èƒ½åŠ›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç†è§£å¹¶å“åº”ä»¥å‰æœªè§è¿‡çš„æ•°æ®ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå¦‚ä½•ä½¿CLIPåœ¨èåˆå„ç§é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„åŒæ—¶ä¿ç•™å…¶é›¶æ¬¡å­¦ä¹ èƒ½åŠ›ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºé›†æˆå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç¾¤ç»„æç¤ºé›†æˆï¼ˆGPEï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡èå…¥æ–°çš„é¢†åŸŸçŸ¥è¯†æ¥æå‡CLIPçš„é›¶æ¬¡å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜å…¶é€‚åº”æ€§å’Œé’ˆå¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºä¸‰ä¸ªä¸»è¦ç­–ç•¥ï¼šé€šè¿‡æç¤ºåˆ†ç»„å’Œæ©è†œæ³¨æ„åŠ›æ¥ä¼˜åŒ–CLIPçš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä¿æŠ¤å…¶é›¶æ¬¡å­¦ä¹ èƒ½åŠ›ï¼›é€šè¿‡å¼•å…¥è¾…åŠ©æç¤ºï¼Œå®ç°æ–°é¢†åŸŸæ´å¯Ÿçš„æ— ç¼é›†æˆï¼Œè€Œä¸ä¼šç ´ååŸå§‹æ¨¡å‹çš„è¡¨ç¤ºï¼›ä»¥åŠä¸€ç§æœ‰æ•ˆçš„é›†æˆå­¦ä¹ ç­–ç•¥ï¼Œå¯ä»¥èåˆåŸå§‹å’Œæ–°çŸ¥è¯†ã€‚é€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼ŒåŒ…æ‹¬æ›´å…·æŒ‘æˆ˜æ€§çš„è·¨æ•°æ®é›†è¿ç§»è¯„ä¼°ï¼Œæˆ‘ä»¬çš„GPEæ–¹æ³•åœ¨é€‚åº”æ€§å’Œæ•ˆç‡æ–¹é¢é‡æ–°å®šä¹‰äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºå‡†ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹å‡è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07077v1">PDF</a> IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision (WACV) 2025</p><p><strong>Summary</strong></p><p>CLIPæ¨¡å‹é€šè¿‡å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œåœ¨é›†æˆç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ä¿æŒå…¶é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºGPEçš„ç¾¤ä½“æç¤ºé›†æˆå­¦ä¹ æ–°æ–¹æ³•ã€‚å®ƒé€šè¿‡æç¤ºåˆ†ç»„ã€è¾…åŠ©æç¤ºå’Œé›†æˆå­¦ä¹ ç­–ç•¥ä¸‰ç§ä¸»è¦ç­–ç•¥ï¼Œå¢å¼ºäº†CLIPçš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒGPEæ–¹æ³•åœ¨è·¨æ•°æ®é›†è½¬ç§»è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé‡æ–°å®šä¹‰äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§å’Œæ•ˆç‡åŸºå‡†ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>CLIPæ¨¡å‹å®ç°äº†é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸå¸¦æ¥é‡å¤§å˜é©ã€‚</li><li>åœ¨é›†æˆç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ä¿æŒCLIPæ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li><li>GPEæ–¹æ³•é€šè¿‡æç¤ºåˆ†ç»„ã€è¾…åŠ©æç¤ºå’Œé›†æˆå­¦ä¹ ç­–ç•¥æ¥å¢å¼ºCLIPæ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li><li>æç¤ºåˆ†ç»„åˆ©ç”¨æ©ç æ³¨æ„åŠ›ä¼˜åŒ–CLIPçš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä¿æŠ¤å…¶é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li><li>è¾…åŠ©æç¤ºèƒ½å¤Ÿæ— ç¼é›†æˆæ–°é¢†åŸŸçŸ¥è¯†ï¼Œä¸å½±å“åŸå§‹æ¨¡å‹çš„è¡¨ç¤ºã€‚</li><li>GPEæ–¹æ³•é€šè¿‡æœ‰æ•ˆçš„çŸ¥è¯†èåˆç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-279149c621356f1222167de36f0a8df6.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-75cf14cba4f83c1c094e3bcbfea43f90.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bf34f58f9ac27d52dc2c5d1821d4d1e7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-14c76a110d190eccc16befbe3591091d.jpg" align="middle"></details><h2 id="Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning"><a href="#Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning" class="headerlink" title="Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning"></a>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</h2><p><strong>Authors:Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou</strong></p><p>Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasonerâ€™s capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.</p><blockquote><p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºè¯¸å¦‚è™šæ„çš„å›¾åƒç†è§£æˆ–ç²—ç³™çš„æ¨ç†è·¯å¾„ç­‰é—®é¢˜ï¼Œå®ƒä»¬ä»ç„¶ç»å¸¸äº§ç”Ÿä¸å‡†ç¡®æˆ–æ— å…³çš„å“åº”ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Critic-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå—Actor-CriticèŒƒå¼å¯å‘çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶æ¥å®ç°æ¨ç†è¿‡ç¨‹å’Œæ‰¹è¯„è¿‡ç¨‹çš„è§£è€¦ï¼šReasonerï¼Œå®ƒæ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼›Criticï¼Œå®ƒæä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼ŒReasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†å“åº”ï¼Œè¿™äº›å“åº”å¯ä»¥æ ¹æ®Criticçš„åé¦ˆè¿­ä»£åœ°ä½œä¸ºç­–ç•¥å‘å±•ã€‚è¿™ä¸ªäº¤äº’è¿‡ç¨‹æ˜¯ç”±å¼ºåŒ–å­¦ä¹ æ¡†æ¶é©±åŠ¨çš„ï¼Œå…¶ä¸­Criticæä¾›è‡ªç„¶è¯­è¨€æ‰¹è¯„è€Œä¸æ˜¯æ ‡é‡å¥–åŠ±ï¼Œè¿™ä½¿å¾—å¯ä»¥ä¸ºReasoneråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æä¾›æ›´å¾®å¦™çš„åé¦ˆã€‚Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨æŒ‰åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰æ’åçš„è¯„è®ºåå¥½æ•°æ®é›†æ¥å¢å¼ºå…¶æ‰¹è¯„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCritic-Væ¡†æ¶åœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬GPT-4Våœ¨å†…ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚ç»“åˆReasonerçš„åŠ¨æ€æ–‡æœ¬ç­–ç•¥ä»¥åŠæ¥è‡ªåå¥½ä¼˜åŒ–åçš„Criticçš„å»ºè®¾æ€§åé¦ˆï¼Œå®ç°äº†æ›´å¯é å’Œä¸Šä¸‹æ–‡æ•æ„Ÿçš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæå‡VLMsçš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ”¹è¿›å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ¨ç†å¯†é›†å‹å¤šæ¨¡æ€åº”ç”¨è¡¨ç°ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½é›†æˆç­‰é¢†åŸŸã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18203v2">PDF</a> 16 pages, 11 figures</p><p><strong>Summary</strong><br>Vision-languageæ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨ä¸å‡†ç¡®æˆ–æ— å…³çš„ååº”é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Critic-Væ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Actor-Criticç†å¿µæ¥æå‡VLMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ•´åˆäº†Reasonerå’ŒCriticä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶ï¼Œå‰è€…åŸºäºè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œåè€…æä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚Reasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†ååº”ï¼Œå¹¶å¯æ ¹æ®æ¥è‡ªCriticçš„åé¦ˆè¿›è¡Œè¿­ä»£æ›´æ–°ç­–ç•¥ã€‚Criticçš„è®­ç»ƒé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰å¯¹è¯„è®ºè¿›è¡Œæ’åï¼Œä»¥å¢å¼ºå…¶æ‰¹è¯„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCritic-Væ¡†æ¶åœ¨å¤§éƒ¨åˆ†ç°æœ‰æ–¹æ³•ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚è¿™ä¸ºæé«˜VLMåœ¨ç°å®ä¸–ç•Œå¤šæ¨¡æ€åº”ç”¨ä¸­çš„å¯é æ€§æä¾›äº†å‰æ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>Vision-language models (VLMs) åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸å‡†ç¡®æˆ–æ— å…³çš„ååº”é—®é¢˜ã€‚</li><li>Critic-Væ¡†æ¶ç»“åˆäº†Actor-Criticç†å¿µæ¥æå‡VLMçš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡æ•´åˆReasonerå’ŒCriticä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶å®ç°ã€‚</li><li>Reasoneræ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œè€ŒCriticåˆ™æä¾›å»ºè®¾æ€§åé¦ˆä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚</li><li>Criticæ¨¡å‹é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•è®­ç»ƒï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰å¢å¼ºæ‰¹è¯„èƒ½åŠ›ã€‚</li><li>Critic-Væ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚</li><li>åŠ¨æ€æ–‡æœ¬ç­–ç•¥é©±åŠ¨çš„Reasonerå’Œåå¥½ä¼˜åŒ–çš„Criticç›¸ç»“åˆï¼Œä½¿å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹æ›´åŠ å¯é å’Œä¸Šä¸‹æ–‡æ•æ„Ÿã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-618f29ee5430b286ee8b46c0e035f63f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b6837c2c367cd1a4e7ee3ad4ec2bd8ec.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-269d187768ea4ab5aa83a7d89fb133bc.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f2278620f94584191480f74c9cf8598c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg" align="middle"></details><h2 id="COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding"><a href="#COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding" class="headerlink" title="COBRA: A Continual Learning Approach to Vision-Brain Understanding"></a>COBRA: A Continual Learning Approach to Vision-Brain Understanding</h2><p><strong>Authors:Xuan-Bac Nguyen, Arabinda Kumar Choudhary, Pawan Sinha, Xin Li, Khoa Luu</strong></p><p>Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module contains a transformer encoder and decoder that learns the fMRI features for VBU from common and specific patterns. In a continual learning setup, COBRA is trained in new PSS and MRIFormer modules for new subjects, leaving the modules of previous subjects unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods.</p><blockquote><p>è§†è§‰å¤§è„‘ç†è§£ï¼ˆVBUï¼‰æ—¨åœ¨ä»é€šè¿‡åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰è®°å½•çš„å¤§è„‘æ´»åŠ¨ä¸­æå–äººç±»æ„ŸçŸ¥åˆ°çš„è§†è§‰ä¿¡æ¯ã€‚å°½ç®¡è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†VBUçš„ç°æœ‰ç ”ç©¶ä»é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹åœ¨é€‚åº”æ–°ä¸»é¢˜æ—¶ä¸§å¤±äº†å…ˆå‰ä¸»é¢˜çš„çŸ¥è¯†ã€‚å› æ­¤ï¼Œè§£å†³è¯¥é¢†åŸŸçš„æŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCOBRAçš„æŒç»­å­¦ä¹ è§†è§‰å¤§è„‘æ¡†æ¶ï¼Œä»¥è§£å†³VBUä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–æ¨¡å—ï¼šä¸»é¢˜å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»é¢˜ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—ï¼Œç§°ä¸ºMRIFormeræ¨¡å—ã€‚SCæ¨¡å—æ•è·ä¸»é¢˜ä¹‹é—´çš„å…±äº«è§†è§‰å¤§è„‘æ¨¡å¼ï¼Œä¿ç•™è¿™äº›çŸ¥è¯†ï¼Œä½¿æ¨¡å‹åœ¨é‡åˆ°æ–°ä¸»é¢˜æ—¶èƒ½å¤Ÿå‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼ŒPSSæ¨¡å—å­¦ä¹ æ¯ä¸ªä¸»é¢˜çš„ç‹¬ç‰¹è§†è§‰å¤§è„‘æ¨¡å¼ã€‚æœ€åï¼ŒMRIFormeræ¨¡å—åŒ…å«ä¸€ä¸ªå˜å‹å™¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä»å…¬å…±å’Œç‰¹å®šæ¨¡å¼ä¸­å­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚åœ¨æŒç»­å­¦ä¹ è®¾ç½®ä¸­ï¼ŒCOBRAé’ˆå¯¹æ–°ä¸»é¢˜è®­ç»ƒæ–°çš„PSSå’ŒMRIFormeræ¨¡å—ï¼Œè€Œä¸å½±å“å…ˆå‰ä¸»é¢˜çš„æ¨¡å—ã€‚å› æ­¤ï¼ŒCOBRAæœ‰æ•ˆåœ°è§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰å¤§è„‘é‡å»ºä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17475v1">PDF</a></p><p><strong>Summary</strong></p><p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºCOBRAçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è„‘ç†è§£ï¼ˆVBUï¼‰é¢†åŸŸä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜ã€‚COBRAåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šä¸»ä½“å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»ä½“ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—ï¼ˆMRIFormerï¼‰ã€‚è¿™äº›æ¨¡å—æ—¨åœ¨æ•æ‰è§†è§‰è„‘æ¨¡å¼çš„å…±æ€§ä»¥åŠé’ˆå¯¹æ¯ä¸ªä¸»ä½“çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œä»è€Œå®ç°æŒç»­å­¦ä¹ å¹¶å‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚åœ¨æŒç»­å­¦ä¹ è®¾ç½®ä¸­ï¼ŒCOBRAé’ˆå¯¹æ–°ä¸»ä½“è¿›è¡Œæ–°çš„PSSå’ŒMRIFormeræ¨¡å—çš„è®­ç»ƒï¼Œè€Œä¹‹å‰çš„æ¨¡å—ä¸å—å½±å“ã€‚å› æ­¤ï¼ŒCOBRAæœ‰æ•ˆåœ°è§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰è„‘é‡å»ºä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p><p><strong>Key Takeaways</strong></p><p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„å…³é”®è¦ç‚¹ï¼Œä»¥ç®€åŒ–æ±‰å­—å‘ˆç°ï¼š</p><ol><li>Vision-Brain Understanding (VBU) æ—¨åœ¨ä»è„‘åŠŸèƒ½ç£å…±æŒ¯æˆåƒä¸­æå–äººç±»æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯ã€‚</li><li>å°½ç®¡è¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†VBUçš„ç ”ç©¶ä»é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ã€‚</li><li>COBRAæ¡†æ¶å¼•å…¥ä¸‰ä¸ªæ–°æ¨¡å—æ¥è§£å†³VBUä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜ï¼šä¸»ä½“å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»ä½“ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒMRIFormeræ¨¡å—ã€‚</li><li>SCæ¨¡å—æ•æ‰ä¸»ä½“é—´çš„å…±äº«è§†è§‰è„‘æ¨¡å¼ï¼Œä»¥å‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚</li><li>PSSæ¨¡å—å­¦ä¹ æ¯ä¸ªä¸»ä½“çš„ç‹¬ç‰¹è§†è§‰è„‘æ¨¡å¼ã€‚</li><li>MRIFormeræ¨¡å—åŒ…å«å˜å‹å™¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œç”¨äºä»å…¬å…±å’Œç‰¹å®šæ¨¡å¼ä¸­å­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6c702b3809502647f842e98a6554e0e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7070aad685269c094671faaee23e433e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c8b7e55ab8521742037def62f4d2e55d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3409f541bd3078454792cebe005df6e1.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f197015eba6fee457426dc6a87da541b.jpg" align="middle"></details><h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p><p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits modelsâ€™ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets.</p><blockquote><p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éšç€æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œé€šè¿‡å„ç§å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…è¶Šé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®­ç»ƒæ–¹æ³•æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå¤„ç†æœªè§è¿‡çš„æ•°æ®ï¼Œè¿™æ˜¯OVSSçš„ä¸»è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼šåœ¨OVSSçš„æŒ‘æˆ˜ç¯å¢ƒä¸­ï¼ŒåŸºäºä»»æ„æŸ¥è¯¢æç¤ºå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶ï¼Œç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚è¿™ä¸€ç–å¿½é™åˆ¶äº†æ¨¡å‹å°†å¯¹è±¡å†…éƒ¨è¯­ä¹‰ä¸€è‡´å…ƒç´ åˆ†ç»„å¹¶å°†å…¶ç²¾ç¡®æ˜ å°„åˆ°ç”¨æˆ·å®šä¹‰çš„ä»»æ„ç±»åˆ«çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å…‹æœè¿™ä¸€é™åˆ¶çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨å›¾åƒå†…èå…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä»è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æç‚¼å…‰è°±é©±åŠ¨ç‰¹å¾å¹¶å°†å…¶è’¸é¦åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œä½¿å¾—è¯­ä¹‰ä¸€è‡´çš„ç»„ä»¶èƒ½å¤Ÿå½¢æˆå•ä¸ªå¯¹è±¡æ©è†œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨å¯èƒ½æ€§å¯¹æ–‡æœ¬åµŒå…¥è¿›è¡Œäº†ç²¾ç‚¼ï¼Œä»¥ç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v1">PDF</a></p><p><strong>Summary</strong></p><p>åŸºäºæœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰æŠ€æœ¯å–å¾—è¿›å±•ï¼Œèƒ½å¤Ÿå®ç°è¶…è¶Šé¢„è®¾ç±»åˆ«çš„åˆ†å‰²ï¼Œé€šè¿‡ä¸åŒçš„å­¦ä¹ æ–¹æ¡ˆæ¥å¤„ç†æœªè§è¿‡çš„æ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¯¹è±¡çš„è¯­ä¹‰åˆ†å‰²æ—¶ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ï¼Œéš¾ä»¥æ ¹æ®ä»»æ„æŸ¥è¯¢æç¤ºè¿›è¡Œç²¾ç¡®åˆ†å‰²ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æç‚¼è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾å¹¶èå…¥è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºå¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ç”¨é›¶æ ·æœ¬å¯¹è±¡å­˜åœ¨æ¦‚ç‡ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰æŠ€æœ¯èƒ½å¤Ÿå¤„ç†è¶…è¶Šé¢„è®¾ç±»åˆ«çš„åˆ†å‰²ã€‚</li><li>è®­ç»ƒå…è´¹çš„æ–¹æ³•ä¸ºå¤„ç†æœªè§è¿‡çš„æ•°æ®æä¾›äº†å¯ä¼¸ç¼©ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¯¹è±¡çš„è¯­ä¹‰åˆ†å‰²æ—¶ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚</li><li>æœ¬ç ”ç©¶é€šè¿‡èå…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li><li>æ–¹æ³•é€šè¿‡æç‚¼è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾å¹¶èå…¥è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºå¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ã€‚</li><li>é€šè¿‡ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0f7b6fbeb2826a7dad8df4b062d3486a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ffccfcbd45ef2a94b19be0bce7c72be4.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bbe7a6fdd528b4861885240dd1defd77.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-90fa9c3ab36b67f06e6c20c1253b7353.jpg" align="middle"></details><h2 id="Free-2-Guide-Gradient-Free-Path-Integral-Control-for-Enhancing-Text-to-Video-Generation-with-Large-Vision-Language-Models"><a href="#Free-2-Guide-Gradient-Free-Path-Integral-Control-for-Enhancing-Text-to-Video-Generation-with-Large-Vision-Language-Models" class="headerlink" title="Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing   Text-to-Video Generation with Large Vision-Language Models"></a>Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models</h2><p><strong>Authors:Jaemin Kim, Bryan S Kim, Jong Chul Ye</strong></p><p>Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free$^2$Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free$^2$Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free$^2$Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos.</p><blockquote><p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰åˆæˆç­‰ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚ç„¶è€Œï¼Œç”±äºå¸§ä¹‹é—´å¤æ‚çš„æ—¶åºä¾èµ–æ€§ï¼Œåœ¨T2Vç”Ÿæˆä¸­å®ç°å‡†ç¡®çš„æ–‡æœ¬å¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•æ¥æå‡æ–‡æœ¬å¯¹é½é€šå¸¸éœ€è¦å¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°ï¼Œæˆ–è€…ä»…é™äºæœ‰é™çš„æç¤ºï¼Œé˜»ç¢äº†å…¶å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Free$^2$Guideï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— æ¢¯åº¦å¯¹é½æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†ç”Ÿæˆçš„è§†é¢‘ä¸æ–‡æœ¬æç¤ºå¯¹é½ï¼Œè€Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚å€ŸåŠ©è·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†ï¼ŒFree$^2$Guideä½¿ç”¨ä¸å¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°æ¥è¿‘ä¼¼æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼ï¼Œä»è€Œèƒ½å¤Ÿæ•´åˆå¼ºå¤§çš„é»‘ç›’å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒçµæ´»åœ°é›†æˆå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å›¾åƒæ¨¡å‹ï¼Œä»¥ååŒæé«˜å¯¹é½æ•ˆæœï¼Œè€Œä¸ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒFree$^2$Guideåœ¨å„ä¸ªæ–¹é¢éƒ½æ˜¾è‘—æé«˜äº†æ–‡æœ¬å¯¹é½çš„æ•ˆæœï¼Œå¹¶æé«˜äº†ç”Ÿæˆè§†é¢‘çš„æ•´ä½“è´¨é‡ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17041v1">PDF</a> 15 pages</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†Free$^2$Guideæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ¢¯åº¦çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¯¹é½æ–‡æœ¬æç¤ºä¸ç”Ÿæˆçš„è§†é¢‘ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è·¯å¾„ç§¯åˆ†æ§åˆ¶çš„åŸç†ï¼Œä½¿ç”¨éå¯å¾®å¥–åŠ±å‡½æ•°ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æŒ‡å¯¼ï¼Œæ•´åˆå¼ºå¤§çš„é»‘ç›’å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒçµæ´»åœ°é›†æˆå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å›¾åƒæ¨¡å‹ï¼Œä»¥ååŒå¢å¼ºå¯¹é½æ•ˆæœï¼Œä¸”ä¸ä¼šé€ æˆå¤ªå¤§çš„è®¡ç®—å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼ŒFree$^2$Guideèƒ½æ˜¾è‘—æé«˜æ–‡æœ¬çš„è·¨ç»´åº¦å¯¹é½æ•ˆæœï¼Œå¹¶å¢å¼ºç”Ÿæˆè§†é¢‘çš„æ•´ä½“è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>Free$^2$Guideæ˜¯ä¸€ç§æ–°å‹çš„æ— éœ€æ¢¯åº¦çš„æ¡†æ¶ï¼Œç”¨äºæé«˜æ–‡æœ¬ä¸ç”Ÿæˆè§†é¢‘çš„å¯¹é½å‡†ç¡®æ€§ã€‚</li><li>è¯¥æ¡†æ¶åˆ©ç”¨è·¯å¾„ç§¯åˆ†æ§åˆ¶çš„åŸç†ï¼Œé‡‡ç”¨éå¯å¾®å¥–åŠ±å‡½æ•°ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æŒ‡å¯¼ã€‚</li><li>Free$^2$Guideèƒ½æ•´åˆå¼ºå¤§çš„é»‘ç›’å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ã€‚</li><li>è¯¥æ¡†æ¶æ”¯æŒçµæ´»åœ°é›†æˆå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å›¾åƒæ¨¡å‹ï¼Œä»¥æé«˜å¯¹é½æ•ˆæœã€‚</li><li>Free$^2$Guideæé«˜äº†æ–‡æœ¬çš„è·¨ç»´åº¦å¯¹é½æ•ˆæœã€‚</li><li>è¯¥æ–¹æ³•å¢å¼ºäº†ç”Ÿæˆè§†é¢‘çš„æ•´ä½“è´¨é‡ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ba9efcd661a35290481f3768b433b309.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e4438e21327f8b25ae41719540dc1b7d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a2b8f0ff8a42045f29b53e669a15022.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46ed930ef96424876051209fae4d6526.jpg" align="middle"></details><h2 id="Active-Prompt-Learning-with-Vision-Language-Model-Priors"><a href="#Active-Prompt-Learning-with-Vision-Language-Model-Priors" class="headerlink" title="Active Prompt Learning with Vision-Language Model Priors"></a>Active Prompt Learning with Vision-Language Model Priors</h2><p><strong>Authors:Hoyoung Kim, Seokhee Jin, Changhwan Sung, Jaechang Kim, Jungseul Ok</strong></p><p>Vision-language models (VLMs) have demonstrated remarkable zero-shot performance across various classification tasks. Nonetheless, their reliance on hand-crafted text prompts for each task hinders efficient adaptation to new tasks. While prompt learning offers a promising solution, most studies focus on maximizing the utilization of given few-shot labeled datasets, often overlooking the potential of careful data selection strategies, which enable higher accuracy with fewer labeled data. This motivates us to study a budget-efficient active prompt learning framework. Specifically, we introduce a class-guided clustering that leverages the pre-trained image and text encoders of VLMs, thereby enabling our cluster-balanced acquisition function from the initial round of active learning. Furthermore, considering the substantial class-wise variance in confidence exhibited by VLMs, we propose a budget-saving selective querying based on adaptive class-wise thresholds. Extensive experiments in active learning scenarios across nine datasets demonstrate that our method outperforms existing baselines.</p><blockquote><p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹æ¯é¡¹ä»»åŠ¡çš„æ‰‹å·¥æ–‡æœ¬æç¤ºçš„ä¾èµ–é˜»ç¢äº†å…¶åœ¨æ–°ä»»åŠ¡ä¸Šçš„é«˜æ•ˆé€‚åº”ã€‚è™½ç„¶æç¤ºå­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç»™å®šçš„å°‘é‡æœ‰æ ‡ç­¾æ•°æ®é›†ä¸Šï¼Œå¾€å¾€å¿½è§†äº†ç²¾å¿ƒé€‰æ‹©æ•°æ®ç­–ç•¥çš„æ½œåŠ›ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥ç”¨æ›´å°‘çš„æ ‡è®°æ•°æ®å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬ç ”ç©¶ä¸€ç§é¢„ç®—é«˜æ•ˆçš„æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç±»æŒ‡å¯¼èšç±»çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨VLMsçš„é¢„è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œä»è€Œåœ¨æˆ‘ä»¬çš„èšç±»å¹³è¡¡é‡‡é›†å‡½æ•°ä¸­å®ç°ä»ä¸»åŠ¨å­¦ä¹ çš„åˆå§‹è½®æ¬¡å¼€å§‹çš„åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°VLMsåœ¨ä¿¡å¿ƒä¸Šçš„å·¨å¤§ç±»é—´å·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”ç±»çº§é˜ˆå€¼çš„é¢„ç®—èŠ‚çº¦é€‰æ‹©æ€§æŸ¥è¯¢ã€‚åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„ä¸»åŠ¨å­¦ä¹ åœºæ™¯çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16722v1">PDF</a></p><p><strong>æ‘˜è¦</strong></p><p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§åˆ†ç±»ä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ‰‹å·¥ç¼–å†™æ–‡æœ¬æç¤ºï¼Œè¿™é˜»ç¢äº†åœ¨æ–°ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆé€‚åº”ã€‚è™½ç„¶æç¤ºå­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä¾§é‡äºæœ€å¤§åŒ–ç»™å®šå°‘é‡æ ‡æ³¨æ•°æ®é›†çš„åˆ©ç”¨ï¼Œå¾€å¾€å¿½è§†äº†ç²¾å¿ƒé€‰æ‹©æ•°æ®ç­–ç•¥çš„æ½œåŠ›ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥ç”¨æ›´å°‘çš„æ ‡æ³¨æ•°æ®å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬ç ”ç©¶ä¸€ç§é¢„ç®—é«˜æ•ˆçš„æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç±»æŒ‡å¯¼èšç±»ï¼Œåˆ©ç”¨VLMsçš„é¢„è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œä»è€Œå®ç°æˆ‘ä»¬çš„ä»ä¸»åŠ¨å­¦ä¹ çš„åˆå§‹è½®æ¬¡å¼€å§‹çš„é›†ç¾¤å¹³è¡¡é‡‡é›†åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°VLMså±•ç°çš„ç±»é—´ç½®ä¿¡åº¦å·¨å¤§å·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”ç±»çº§é˜ˆå€¼çš„é¢„ç®—èŠ‚çº¦é€‰æ‹©æ€§æŸ¥è¯¢ã€‚åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„ä¸»åŠ¨å­¦ä¹ åœºæ™¯çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p><p><strong>å…³é”®è§è§£</strong></p><ol><li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†æ‰‹å·¥ç¼–å†™æ–‡æœ¬æç¤ºé™åˆ¶äº†æ–°ä»»åŠ¡çš„é€‚åº”æ•ˆç‡ã€‚</li><li>å¤§å¤šæ•°ç ”ç©¶åœ¨æç¤ºå­¦ä¹ ä¸Šä¾§é‡äºæœ€å¤§åŒ–ç»™å®šå°‘é‡æ ‡æ³¨æ•°æ®é›†çš„åˆ©ç”¨ï¼Œå¿½è§†äº†ç²¾å¿ƒé€‰æ‹©æ•°æ®ç­–ç•¥çš„æ½œåŠ›ã€‚</li><li>å¼•å…¥äº†ä¸€ç§é¢„ç®—é«˜æ•ˆçš„æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç»“åˆç±»æŒ‡å¯¼èšç±»å’Œé›†ç¾¤å¹³è¡¡é‡‡é›†åŠŸèƒ½ï¼Œä»¥æé«˜VLMsåœ¨æ–°ä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</li><li>è€ƒè™‘åˆ°VLMsçš„ç±»é—´ç½®ä¿¡åº¦å·®å¼‚ï¼Œæå‡ºäº†ä¸€ç§é¢„ç®—èŠ‚çº¦é€‰æ‹©æ€§æŸ¥è¯¢æ–¹æ³•ï¼ŒåŸºäºè‡ªé€‚åº”ç±»çº§é˜ˆå€¼ã€‚</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»åŠ¨å­¦ä¹ åœºæ™¯ä¸‹ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li><li>è¯¥æ–¹æ³•é€šè¿‡æ›´æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©å’Œåˆ©ç”¨ï¼Œæœ‰å¯èƒ½å‡å°‘æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œé™ä½å­¦ä¹ æˆæœ¬ã€‚</li><li>è¿™ç§æ´»åŠ¨æç¤ºå­¦ä¹ æ¡†æ¶çš„å¼•å…¥ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œé€‚åº”æä¾›äº†æ–°æ–¹å‘ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3b1870a9668332c94cbafa2aecd08c4e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-938e9cbd8a98eb2489756bc58a9fe4d9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b9ca16ce76baa8dd4fc4becd56178131.jpg" align="middle"></details><h2 id="BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models"><a href="#BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"></a>BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p><p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a>.</p><blockquote><p>è¿‘æœŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨è§†è§‰ä»»åŠ¡çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†VLMsæœ‰æ•ˆåœ°é€‚åº”åˆ°ä¸‹æ¸¸åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬çš„å‡†ç¡®æ€§å¾€å¾€ä¾èµ–äºè€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„æç¤ºå·¥ç¨‹ï¼Œè€Œå…¨æ¨¡å‹çš„å¾®è°ƒæˆæœ¬åˆå¾ˆé«˜ã€‚ç‰¹åˆ«æ˜¯å¯¹äºç”Ÿç‰©åŒ»å­¦å›¾åƒï¼Œä¸è‡ªç„¶å›¾åƒä¸åŒï¼Œå®ƒä»¬é€šå¸¸å—é™äºæ ‡æ³¨æ•°æ®é›†ã€å›¾åƒå¯¹æ¯”åº¦ä¸å¤Ÿç›´è§‚ä»¥åŠå¾®å¦™çš„è§†è§‰ç‰¹å¾ã€‚æœ€è¿‘çš„æç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œå¦‚ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†åœ¨é€šç”¨æ€§æ–¹é¢ä»æœ‰ä¸è¶³ã€‚åŒæ—¶ï¼Œé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„æç¤ºå­¦ä¹ æ¢ç´¢ä»ç„¶éå¸¸æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BiomedCoOpï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹BiomedCLIPè¿›è¡Œé€‚é…ï¼Œä»¥å®ç°å‡†ç¡®ä¸”é«˜åº¦é€šç”¨çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒå°‘æ ·æœ¬åˆ†ç±»ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨æ¶µç›–9ç§æ¨¡æ€å’Œæ¶‰åŠè·¨ç»„ç»‡å™¨ä¸åŒã€å…±è®¡æœ‰åŒ…å«å¤šä¸ªæ•°æ®é›†çš„æƒ…å†µä¸‹è¿›è¡Œäº†å…¨é¢éªŒè¯ï¼Œå¹¶ä¸ç°æœ‰çš„æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒè¯æ˜æˆ‘ä»¬åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç›¸å…³ä»£ç å°†å…¬å¼€å‘å¸ƒåœ¨ <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a> ä¸Šã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15232v1">PDF</a> 18 pages, 5 figures, 10 tables</p><p><strong>Summary</strong></p><p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸä¸­çš„æ–°æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBiomedCoOpçš„æ–°å‹æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå®ç°äº†å¯¹BiomedCLIPçš„é«˜æ•ˆé€‚åº”ï¼Œç”¨äºå‡†ç¡®ä¸”é«˜åº¦é€šç”¨çš„å°‘æ ·æœ¬ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ç»è¿‡å¹¿æ³›çš„éªŒè¯ï¼Œåœ¨é’ˆå¯¹ä¹ä¸ªå™¨å®˜ã€æ¶‰åŠåä¸ªæ¨¡æ€çš„åä¸€å¥—åŒ»ç–—æ•°æ®é›†ä¸Šï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/HealthX-Lab/BiomedCoOpå…¬å¼€å¯ç”¨ã€‚</a></p><p><strong>Key Takeaways</strong></p><p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p><ol><li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æœ‰é™æ ‡æ³¨æ•°æ®é›†ã€éç›´è§‚å›¾åƒå¯¹æ¯”å’Œå¾®å¦™è§†è§‰ç‰¹å¾ã€‚</li><li>å½“å‰æç¤ºå­¦ä¹ æŠ€æœ¯å¦‚CoOpå­˜åœ¨æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li><li>å¼•å…¥æ–°å‹æç¤ºå­¦ä¹ æ¡†æ¶BiomedCoOpï¼Œèƒ½å¤Ÿå®ç°å¯¹BiomedCLIPçš„é«˜æ•ˆé€‚åº”ï¼Œç”¨äºå°‘æ ·æœ¬ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚</li><li>BiomedCoOpé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§å’ŒåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå®ç°æœ‰æ•ˆæç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li><li>åœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒBiomedCoOpåœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢è¾ƒç°æœ‰æ–¹æ³•æ˜¾è‘—æå‡ã€‚</li><li>è¯¥ç ”ç©¶å°†å…¬å¼€å…¶ä»£ç ä»¥æ¨åŠ¨ç›¸å…³ç ”ç©¶å‘å±•ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f47ee0177c47ded9dbffb2322847edc0.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b35924fcdaa57f51ad7edfe0b37e082e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1e42816bd1d7c7d6931bb05d1a5ec10b.jpg" align="middle"></details><h2 id="Looking-Beyond-Text-Reducing-Language-bias-in-Large-Vision-Language-Models-via-Multimodal-Dual-Attention-and-Soft-Image-Guidance"><a href="#Looking-Beyond-Text-Reducing-Language-bias-in-Large-Vision-Language-Models-via-Multimodal-Dual-Attention-and-Soft-Image-Guidance" class="headerlink" title="Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance"></a>Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance</h2><p><strong>Authors:Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang</strong></p><p>Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on images and ineffective visual comprehension. We identify two primary reasons for this bias: 1. Different scales of training data between the pretraining stage of LLM and multimodal alignment stage. 2. The learned inference bias due to short-term dependency of text data. Therefore, we propose LACING, a systemic framework designed to address the language bias of LVLMs with muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG). Specifically, MDA introduces a parallel dual-attention mechanism that enhances the integration of visual inputs across the model. IFG introduces a learnable soft visual prompt during training and inference to replace visual inputs, designed to compel LVLMs to prioritize text inputs. Then, IFG further proposes a novel decoding strategy using the soft visual prompt to mitigate the modelâ€™s over-reliance on adjacent text inputs. Comprehensive experiments demonstrate that our method effectively debiases LVLMs from their language bias, enhancing visual comprehension and reducing hallucinations without requiring additional training resources or data. The code and model are available at <a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>.</p><blockquote><p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œå°½ç®¡è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ï¼ŒLVLMså´å­˜åœ¨ç”±è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œè¿™å¯¼è‡´å¯¹å›¾åƒçš„å…³æ³¨åº¦é™ä½å’Œè§†è§‰ç†è§£æ— æ•ˆã€‚æˆ‘ä»¬ç¡®å®šäº†é€ æˆè¿™ç§åè§çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼š1. é¢„è®­ç»ƒé˜¶æ®µçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¯¹é½é˜¶æ®µä¹‹é—´è®­ç»ƒæ•°æ®è§„æ¨¡çš„ä¸åŒï¼›2. ç”±äºæ–‡æœ¬æ•°æ®çš„çŸ­æœŸä¾èµ–æ€§å¯¼è‡´çš„æ¨ç†åè§å­¦ä¹ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LACINGç³»ç»Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LVLMsçš„è¯­è¨€åè§é—®é¢˜ï¼Œå®ƒé‡‡ç”¨å¤šæ¨¡æ€åŒæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMDAï¼‰å’Œè½¯å›¾åƒå¼•å¯¼ï¼ˆIFGï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒMDAå¼•å…¥äº†ä¸€ç§å¹¶è¡ŒåŒæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹ä¸­è§†è§‰è¾“å…¥çš„é›†æˆã€‚IFGåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„è½¯è§†è§‰æç¤ºæ¥æ›¿ä»£è§†è§‰è¾“å…¥ï¼Œæ—¨åœ¨è¿«ä½¿LVLMsä¼˜å…ˆè€ƒè™‘æ–‡æœ¬è¾“å…¥ã€‚ç„¶åï¼ŒIFGè¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä½¿ç”¨è½¯è§†è§‰æç¤ºçš„æ–°å‹è§£ç ç­–ç•¥ï¼Œä»¥å‡è½»æ¨¡å‹å¯¹ç›¸é‚»æ–‡æœ¬è¾“å…¥çš„è¿‡åº¦ä¾èµ–ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æ¶ˆé™¤äº†LVLMsçš„è¯­è¨€åè§ï¼Œæé«˜äº†è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå‡å°‘äº†å¹»è§‰çš„äº§ç”Ÿï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒèµ„æºæˆ–æ•°æ®æ”¯æŒã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://lacing-lvlm.github.io/">lacing-lvlm.github.io</a>è·å–ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14279v1">PDF</a> 19 pages, 12 figures</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶å–å¾—çš„æ˜¾è‘—æˆæœåŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚LVLMså­˜åœ¨è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œå½±å“äº†å¯¹å›¾åƒçš„å…³æ³¨åº¦å’Œè§†è§‰ç†è§£èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LACINGæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¤šæ¨¡æ€åŒæ³¨æ„åŠ›æœºåˆ¶å’Œè½¯å›¾åƒå¼•å¯¼æ¥è§£å†³LVLMsçš„è¯­è¨€åè§é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†æ¨¡å‹çš„å¹»è§‰å’Œä¾èµ–è¯­è¨€åè§çš„é—®é¢˜ï¼Œæé«˜äº†è§†è§‰ç†è§£èƒ½åŠ›å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶çš„ä»£ç å’Œæ¨¡å‹å·²åœ¨ç½‘ç«™ä¸Šå‘å¸ƒã€‚</p><p><strong>Key Takeaways</strong></p><p>ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹ï¼š</p><ul><li>LVLMsåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å­˜åœ¨è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜ã€‚</li><li>è¯­è¨€åè§å¯¼è‡´LVLMså¯¹å›¾åƒå…³æ³¨åº¦é™ä½å’Œè§†è§‰ç†è§£èƒ½åŠ›å—é™ã€‚</li><li>LACINGæ¡†æ¶é€šè¿‡å¼•å…¥å¤šæ¨¡æ€åŒæ³¨æ„åŠ›æœºåˆ¶å’Œè½¯å›¾åƒå¼•å¯¼è§£å†³LVLMsçš„è¯­è¨€åè§é—®é¢˜ã€‚å…¶ä¸­MDAæœºåˆ¶å¢å¼ºæ¨¡å‹ä¸­å¯¹è§†è§‰è¾“å…¥çš„æ•´åˆèƒ½åŠ›ï¼ŒIFGå¼•å…¥è½¯è§†è§‰æç¤ºæ›¿ä»£è§†è§‰è¾“å…¥ï¼Œä¿ƒä½¿LVLMsä¼˜å…ˆå¤„ç†æ–‡æœ¬è¾“å…¥ã€‚æ­¤å¤–ï¼ŒIFGè¿˜æå‡ºäº†ä¸€ç§æ–°çš„è§£ç ç­–ç•¥ï¼Œå‡å°‘æ¨¡å‹å¯¹ç›¸é‚»æ–‡æœ¬è¾“å…¥çš„ä¾èµ–ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c8e25e0c4c4756d97250e95df89ceb63.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-67ce42f158bf6263df9b3ed306ddd4ec.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cfe243aff073e1742ceed0eca4ad04bc.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4e1bd75a9d32e6557561b4146d803ba2.jpg" align="middle"></details><h2 id="Segment-Any-Class-SAC-Multi-Class-Few-Shot-Semantic-Segmentation-via-Class-Region-Proposals"><a href="#Segment-Any-Class-SAC-Multi-Class-Few-Shot-Semantic-Segmentation-via-Class-Region-Proposals" class="headerlink" title="Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via   Class Region Proposals"></a>Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via Class Region Proposals</h2><p><strong>Authors:Hussni Mohd Zakir, Eric Tatt Wei Ho</strong></p><p>The Segment-Anything Model (SAM) is a vision foundation model for segmentation with a prompt-driven framework. SAM generates class-agnostic masks based on user-specified instance-referring prompts. However, adapting SAM for automated segmentation â€“ where manual input is absent â€“ of specific object classes often requires additional model training. We present Segment Any Class (SAC), a novel, training-free approach that task-adapts SAM for Multi-class segmentation. SAC generates Class-Region Proposals (CRP) on query images which allows us to automatically generate class-aware prompts on probable locations of class instances. CRPs are derived from elementary intra-class and inter-class feature distinctions without any additional training. Our method is versatile, accommodating any N-way K-shot configurations for the multi-class few-shot semantic segmentation (FSS) task. Unlike gradient-learning adaptation of generalist models which risk the loss of generalization and potentially suffer from catastrophic forgetting, SAC solely utilizes automated prompting and achieves superior results over state-of-the-art methods on the COCO-20i benchmark, particularly excelling in high N-way class scenarios. SAC is an interesting demonstration of a prompt-only approach to adapting foundation models for novel tasks with small, limited datasets without any modifications to the foundation model itself. This method offers interesting benefits such as intrinsic immunity to concept or feature loss and rapid, online task adaptation of foundation models.</p><blockquote><p>Segment-Anything Modelï¼ˆSAMï¼‰æ˜¯ä¸€ä¸ªåŸºäºæç¤ºé©±åŠ¨çš„æ¡†æ¶è¿›è¡Œåˆ†å‰²çš„æ„¿æ™¯åŸºç¡€æ¨¡å‹ã€‚SAMæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„å®ä¾‹å¼•ç”¨æç¤ºç”Ÿæˆç±»æ— å…³æ©ç ã€‚ç„¶è€Œï¼Œå°†SAMé€‚åº”äºæ— éœ€æ‰‹åŠ¨è¾“å…¥çš„ç‰¹å®šå¯¹è±¡ç±»çš„è‡ªåŠ¨åˆ†å‰²é€šå¸¸éœ€è¦é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†Segment Any Classï¼ˆSACï¼‰è¿™ä¸€æ–°é¢–çš„æ— è®­ç»ƒé€‚åº”æ–¹æ³•ï¼Œç”¨äºå¯¹SAMè¿›è¡Œå¤šç±»åˆ†å‰²çš„ä»»åŠ¡é€‚åº”ã€‚SACåœ¨æŸ¥è¯¢å›¾åƒä¸Šç”Ÿæˆç±»åŒºåŸŸææ¡ˆï¼ˆCRPï¼‰ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç±»å®ä¾‹çš„å¯èƒ½ä½ç½®è‡ªåŠ¨äº§ç”Ÿç±»æ„ŸçŸ¥æç¤ºã€‚CRPæ˜¯ä»åŸºæœ¬çš„ç±»å†…å’Œç±»é—´ç‰¹å¾å·®å¼‚ä¸­å¾—å‡ºçš„ï¼Œæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šç”¨æ€§å¼ºï¼Œå¯é€‚åº”å¤šç±»å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰ä»»åŠ¡çš„ä»»ä½•Nè·¯Kå°„å‡»é…ç½®ã€‚ä¸å¯èƒ½å¯¼è‡´é€šç”¨æ€§æŸå¤±å’Œæ½œåœ¨ç¾éš¾æ€§é—å¿˜çš„é€šç”¨æ¨¡å‹çš„æ¢¯åº¦å­¦ä¹ é€‚åº”æ–¹æ³•ä¸åŒï¼ŒSACä»…åˆ©ç”¨è‡ªåŠ¨åŒ–æç¤ºï¼Œå¹¶åœ¨COCO-20iåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¯¹æœ€æ–°æŠ€æœ¯çš„ä¼˜è¶Šç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜Nè·¯ç±»åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚SACæ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ¼”ç¤ºï¼Œå±•ç¤ºäº†ä»…ä½¿ç”¨æç¤ºçš„æ–¹æ³•å¦‚ä½•é€‚åº”æ–°ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨å°ä¸”æœ‰é™çš„æ•°æ®é›†ï¼Œè€Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹æœ¬èº«è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚è¯¥æ–¹æ³•æä¾›äº†æœ‰è¶£çš„ä¼˜åŠ¿ï¼Œå¦‚å›ºæœ‰çš„æ¦‚å¿µæˆ–ç‰¹å¾æŸå¤±å…ç–«ä»¥åŠåŸºç¡€æ¨¡å‹çš„å¿«é€Ÿåœ¨çº¿ä»»åŠ¡é€‚åº”ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13774v1">PDF</a> 8 pages, 2 figures, 3 tables</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†Segment-Anything Modelï¼ˆSAMï¼‰çš„æ”¹è¿›ç‰ˆæœ¬Segment Any Classï¼ˆSACï¼‰ã€‚SACæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç”¨äºå°†SAMè‡ªé€‚åº”äºå¤šç±»åˆ†å‰²ä»»åŠ¡ã€‚å®ƒé€šè¿‡ç”Ÿæˆç±»åŒºåŸŸææ¡ˆï¼ˆCRPsï¼‰æ¥è‡ªåŠ¨äº§ç”Ÿç±»æ„ŸçŸ¥æç¤ºï¼Œæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒå³å¯åœ¨æŸ¥è¯¢å›¾åƒä¸Šè¿›è¡Œå¤šç±»åˆ†å‰²ã€‚è¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå¯åœ¨å°æ•°æ®é›†ä¸Šå®ç°ä¼˜äºæœ€æ–°æŠ€æœ¯çš„ç»“æœï¼Œå¹¶å…·æœ‰å†…åœ¨å…ç–«æ¦‚å¿µæˆ–ç‰¹å¾æŸå¤±ä»¥åŠå¿«é€Ÿåœ¨çº¿ä»»åŠ¡è‡ªé€‚åº”çš„ä¼˜ç‚¹ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>Segment Any Class (SAC)æ˜¯Segment-Anything Modelï¼ˆSAMï¼‰çš„ä¸€ä¸ªæ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨äºå®ç°å¤šç±»åˆ†å‰²ä»»åŠ¡ã€‚</li><li>SACé‡‡ç”¨äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆç±»åŒºåŸŸææ¡ˆï¼ˆCRPsï¼‰è‡ªåŠ¨äº§ç”Ÿç±»æ„ŸçŸ¥æç¤ºã€‚</li><li>CRPsåŸºäºå›¾åƒå†…ç±»å’Œç±»é—´çš„ç‰¹å¾å·®å¼‚è¿›è¡Œæ¨å¯¼ã€‚</li><li>SACèƒ½å¤Ÿé€‚åº”ä»»ä½•Nç±»K-shoté…ç½®çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li><li>ä¸é€šç”¨æ¨¡å‹çš„æ¢¯åº¦å­¦ä¹ é€‚åº”ç›¸æ¯”ï¼ŒSACä»…ä½¿ç”¨è‡ªåŠ¨åŒ–æç¤ºï¼Œé¿å…äº†æŸå¤±é€šç”¨åŒ–å’Œæ½œåœ¨çš„ç¾éš¾æ€§é—å¿˜é£é™©ã€‚</li><li>SACåœ¨COCO-20iåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜Nç±»åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d971b8f1fd31197fc322d5dbcf0fbddf.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-19d42f313dc3c9fb2555211abd55ff49.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-719226815bc206ee26208752a81162b5.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-641afa3769daf97fd5fc45721f55b89c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c914aa2015da19ab2f59b760fe680218.jpg" align="middle"></details><h2 id="SAG-ViT-A-Scale-Aware-High-Fidelity-Patching-Approach-with-Graph-Attention-for-Vision-Transformers"><a href="#SAG-ViT-A-Scale-Aware-High-Fidelity-Patching-Approach-with-Graph-Attention-for-Vision-Transformers" class="headerlink" title="SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph   Attention for Vision Transformers"></a>SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers</h2><p><strong>Authors:Shravan Venkatraman, Jaskaran Singh Walia, Joe Dhanith P R</strong></p><p>Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multiscale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance. Our code and weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/shravan-18/SAG-ViT">https://github.com/shravan-18/SAG-ViT</a></p><blockquote><p>å›¾åƒåˆ†ç±»æ˜¯ä¸€ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ¨¡å‹é€šè¿‡åˆ†æå›¾åƒå°†å…¶å½’ç±»ä¸ºç‰¹å®šæ ‡ç­¾ã€‚è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰é€šè¿‡åˆ©ç”¨è‡ªæˆ‘æ³¨æ„åŠ›æ¥æ•æ‰å›¾åƒè¡¥ä¸ä¹‹é—´çš„å¤æ‚æ¨¡å¼å’Œé•¿è¿œå…³ç³»ï¼Œä»è€Œæ”¹è¿›æ­¤ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒViTçš„å…³é”®æŒ‘æˆ˜æ˜¯æœ‰æ•ˆåœ°èå…¥å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼Œè¿™æ˜¯CNNçš„å›ºæœ‰å±æ€§ï¼Œå¾—ç›Šäºå…¶åˆ†å±‚ç»“æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Scale-Aware Graph Attention Vision Transformerï¼ˆSAG-ViTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ•´åˆå¤šå°ºåº¦ç‰¹å¾æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ–°å‹æ¡†æ¶ã€‚ä½¿ç”¨EfficientNetä½œä¸ºéª¨å¹²ç½‘ï¼Œè¯¥æ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œå°†è¿™äº›ç‰¹å¾å›¾åˆ’åˆ†ä¸ºè¡¥ä¸ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚è¿™äº›è¡¥ä¸åŸºäºç©ºé—´å’Œç‰¹å¾ç›¸ä¼¼æ€§ç»„ç»‡æˆä¸€ä¸ªå›¾ï¼Œå…¶ä¸­Graph Attention Networkï¼ˆGATï¼‰å¯¹èŠ‚ç‚¹åµŒå…¥è¿›è¡Œç²¾ç‚¼ã€‚æœ€åï¼ŒTransformerç¼–ç å™¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œå¤æ‚äº¤äº’ã€‚SAG-ViTåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜å›¾åƒåˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shravan-18/SAG-ViT%E5%85%AC%E5%BC%BA%E8%AE%BE%E8%AE%A1%E3%80%82">https://github.com/shravan-18/SAG-ViTå…¬å¼€è®¿é—®ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09420v2">PDF</a> 10 pages, 4 figures, 3 tables</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­Vision Transformerï¼ˆViTï¼‰çš„åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚é’ˆå¯¹ViTåœ¨å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºæ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”Scale-Aware Graph Attention Vision Transformerï¼ˆSAG-ViTï¼‰ã€‚è¯¥æ¡†æ¶ä»¥EfficientNetä¸ºéª¨å¹²ç½‘ï¼Œæå–å¤šå°ºåº¦ç‰¹å¾å›¾å¹¶åˆ’åˆ†ä¸ºè¡¥ä¸ï¼Œä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚åŸºäºç©ºé—´ç‰¹å¾å’Œç‰¹å¾ç›¸ä¼¼æ€§ç»„ç»‡è¿™äº›è¡¥ä¸æˆå›¾ï¼Œé€šè¿‡Graph Attention Networkï¼ˆGATï¼‰ä¼˜åŒ–èŠ‚ç‚¹åµŒå…¥ã€‚æœ€åï¼Œä½¿ç”¨Transformerç¼–ç å™¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œå¤æ‚äº¤äº’ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSAG-ViTæé«˜äº†å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>Vision Transformers (ViT) åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œæ•æ‰å¤æ‚æ¨¡å¼å’Œå›¾åƒè¡¥ä¸é—´çš„é•¿è·ç¦»å…³ç³»ã€‚</li><li>ViTé¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯æœ‰æ•ˆåœ°ç»“åˆå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼Œè¿™æ˜¯CNNçš„å›ºæœ‰ä¼˜åŠ¿ã€‚</li><li>å¼•å…¥SAG-ViTæ¡†æ¶ï¼Œç»“åˆå¤šå°ºåº¦ç‰¹å¾ä»¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li><li>SAG-ViTä½¿ç”¨EfficientNetä½œä¸ºéª¨å¹²ç½‘ï¼Œæå–å¤šå°ºåº¦ç‰¹å¾å›¾å¹¶åˆ’åˆ†ä¸ºè¡¥ä¸ï¼Œä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>åŸºäºç©ºé—´ç‰¹å¾å’Œç‰¹å¾ç›¸ä¼¼æ€§ç»„ç»‡è¡¥ä¸æˆå›¾ï¼Œé€šè¿‡Graph Attention Networkï¼ˆGATï¼‰ä¼˜åŒ–èŠ‚ç‚¹åµŒå…¥ã€‚</li><li>SAG-ViTä½¿ç”¨Transformerç¼–ç å™¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œå¤æ‚äº¤äº’ã€‚</li><li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSAG-ViTæé«˜äº†å›¾åƒåˆ†ç±»æ€§èƒ½ï¼Œä»£ç å’Œæƒé‡å·²å…¬å¼€æä¾›ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8fb6033fd9f0d6d2b6fb77c021c73886.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-adf3ae7d6c3114687cbe0c207e504746.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-1f64ae851224f57cef4251b9b722ab40.jpg" align="middle"></details><h2 id="GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#GlocalCLIP-Object-agnostic-Global-Local-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot   Anomaly Detection"></a>GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot Anomaly Detection</h2><p><strong>Authors:Jiyul Ham, Yonggon Jung, Jun-Geol Baek</strong></p><p>Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting anomalous patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP">https://github.com/YUL-git/GlocalCLIP</a>.</p><blockquote><p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰å¯¹äºåœ¨æ— éœ€è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹æ£€æµ‹ç›®æ ‡æ•°æ®é›†ä¸­çš„å¼‚å¸¸æ¨¡å¼è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡åŸŸä¸è®­ç»ƒæ•°æ®ä¹‹é—´å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–ç”±äºè®¿é—®å—é™è€Œå¯¼è‡´æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚å°½ç®¡æœ€è¿‘é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨ç±»åˆ«è¯­ä¹‰çš„å­¦ä¹ ï¼Œè¿™ä½¿å¾—å®ƒä»¬ç›´æ¥åº”ç”¨äºZSADå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºäº†GlocalCLIPæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç‹¬ç‰¹åœ°åˆ†ç¦»äº†å…¨å±€å’Œå±€éƒ¨æç¤ºï¼Œå¹¶è”åˆä¼˜åŒ–å®ƒä»¬ã€‚è¿™ä¸€ç‹¬ç‰¹è®¾è®¡ä½¿å¾—å¯¹è±¡æ— å…³çš„å…¨å±€å±€éƒ¨è¯­ä¹‰æç¤ºèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é€šç”¨æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ï¼Œè€Œæ— éœ€ä¾èµ–å›¾åƒä¸­çš„ç‰¹å®šå¯¹è±¡ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´æ¥å®Œå–„æ–‡æœ¬æç¤ºï¼Œä»¥å®ç°æ›´ç²¾ç¡®çš„è°ƒæ•´ã€‚åœ¨è§†è§‰ç¼–ç å™¨æ–¹é¢ï¼Œæˆ‘ä»¬åº”ç”¨V-Væ³¨æ„åŠ›å±‚æ¥æ•æ‰è¯¦ç»†çš„å±€éƒ¨å›¾åƒç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å±€éƒ¨å¯¹æ¯”å­¦ä¹ ï¼Œä»¥æé«˜å…¨å±€å’Œå±€éƒ¨æç¤ºçš„äº’è¡¥å­¦ä¹ ï¼Œæœ‰æ•ˆæ£€æµ‹ä¸åŒé¢†åŸŸçš„å¼‚å¸¸æ¨¡å¼ã€‚GlocalCLIPåœ¨ZSADä¸­çš„æ³›åŒ–æ€§èƒ½åœ¨æ¥è‡ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„1 5ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/YUL-git/GlocalCLIP%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YUL-git/GlocalCLIPä¸Šæä¾›ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06071v3">PDF</a> 29 pages, 36 figures</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰çš„æ–°æ–¹æ³•GlocalCLIPã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºå¹¶è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œè§£å†³äº†ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æŒ‘æˆ˜ã€‚åˆ©ç”¨æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´å’ŒV-Væ³¨æ„åŠ›å±‚ï¼ŒGlocalCLIPèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹æ•è·ä¸€èˆ¬æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†å…¨å±€å’Œå±€éƒ¨æç¤ºçš„å¯¹æ¯”å­¦ä¹ ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>GlocalCLIPè§£å†³äº†åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li><li>é€šè¿‡åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨æç¤ºå¹¶è¿›è¡Œè”åˆä¼˜åŒ–ï¼ŒGlocalCLIPèƒ½æ•è·ä¸€èˆ¬æ­£å¸¸å’Œå¼‚å¸¸æ¨¡å¼ã€‚</li><li>GlocalCLIPä½¿ç”¨æ·±åº¦æ–‡æœ¬æç¤ºè°ƒæ•´ä»¥æé«˜ç²¾ç¡®åº¦ï¼Œå¹¶é€šè¿‡V-Væ³¨æ„åŠ›å±‚æ•è·è¯¦ç»†çš„å±€éƒ¨å›¾åƒç‰¹å¾ã€‚</li><li>å¼•å…¥å…¨å±€å’Œå±€éƒ¨æç¤ºçš„å¯¹æ¯”å­¦ä¹ ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>GlocalCLIPåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li><li>GlocalCLIPå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡æ•°æ®é›†ä¸è®­ç»ƒæ•°æ®å­˜åœ¨åˆ†å¸ƒå·®å¼‚æˆ–æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1125ab6e3ab5117d2b7066fa7f664de1.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2f8c17425f03137645e1037af7d759b4.jpg" align="middle"></details><h2 id="Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy"><a href="#Domain-Adaptive-Pre-training-of-Self-Supervised-Foundation-Models-for-Medical-Image-Classification-in-Gastrointestinal-Endoscopy" class="headerlink" title="Domain-Adaptive Pre-training of Self-Supervised Foundation Models for   Medical Image Classification in Gastrointestinal Endoscopy"></a>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy</h2><p><strong>Authors:Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</strong></p><p>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large GIE dataset, called EndoExtend24, created by merging ten existing public and private datasets, ensuring patient integrity across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models trained with self-supervision on generic image data, to adapt them to the task of GIE medical image diagnosis. Specifically, the EVA-02 model, which is based on the ViT architecture and trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Our model demonstrates robust performance, securing third place in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762 and a balanced accuracy of 37.1% on the test set. These results emphasize the effectiveness of our domain-adaptive pre-training approach and the enriched EndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.</p><blockquote><p>è§†é¢‘èƒ¶å›Šå†…é•œæŠ€æœ¯é€šè¿‡æä¾›ä¸€ç§æ— åˆ›æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰èƒƒè‚ é“çš„è¯¦ç»†å›¾åƒï¼Œä»è€Œå®ç°äº†èƒƒè‚ é“å†…çª¥é•œï¼ˆGIEï¼‰è¯Šæ–­çš„å˜é©ï¼Œä½¿æ—©æœŸç–¾ç—…æ£€æµ‹æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå…¶åœ¨æˆåƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œæˆåƒè¿‡ç¨‹å¯èƒ½éœ€è¦6-8å°æ—¶ï¼Œå¹¶å¯èƒ½äº§ç”Ÿé«˜è¾¾100ä¸‡å¼ å›¾åƒï¼Œè¿™é™åˆ¶äº†å…¶æ½œåŠ›ï¼Œéœ€è¦è‡ªåŠ¨åŒ–åˆ†æã€‚æ­¤å¤–ï¼Œè¿™äº›å›¾åƒçš„å˜é‡ç»“åˆä¸“å®¶æ ‡æ³¨çš„éœ€æ±‚ä»¥åŠå¤§å‹ã€é«˜è´¨é‡æ ‡è®°æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œåˆ¶çº¦äº†å½“å‰åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p></blockquote><p>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤§å‹GIEæ•°æ®é›†EndoExtend24ï¼Œå®ƒæ˜¯é€šè¿‡åˆå¹¶åä¸ªç°æœ‰çš„å…¬å…±å’Œç§æœ‰æ•°æ®é›†åˆ›å»ºçš„ï¼Œç¡®ä¿äº†è·¨åˆ†å‰²çš„æ‚£è€…å®Œæ•´æ€§ã€‚EndoExtend24åŒ…å«è¶…è¿‡22ä¸‡å¼ æ ‡è®°å›¾åƒï¼Œä»¥åŠåŠ¨æ€ç±»æ˜ å°„ï¼Œå…è®¸åœ¨å…·æœ‰ä¸åŒæ ‡è®°ç²’åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œç»Ÿä¸€è®­ç»ƒï¼Œæ”¯æŒå¤šè¾¾123ç§ä¸åŒçš„ç—…ç†å‘ç°ã€‚</p><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21302v4">PDF</a></p><p><strong>Summary</strong><br>è§†é¢‘èƒ¶å›Šå†…é•œæŠ€æœ¯ä¸ºèƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æä¾›äº†éä¾µå…¥æ€§çš„è¯¦ç»†æˆåƒæ–¹æ³•ï¼Œæœ‰åŠ©äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚ç„¶è€Œï¼Œç”±äºæˆåƒè¿‡ç¨‹äº§ç”Ÿçš„å›¾åƒæ•°é‡åºå¤§ï¼Œåˆ†æéœ€æ±‚è‡ªåŠ¨åŒ–ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºæ–°çš„å¤§å‹èƒƒè‚ é“å†…çª¥é•œæ•°æ®é›†EndoExtend24ï¼ŒåŒ…å«è¶…è¿‡22.6ä¸‡å¼ æ ‡è®°å›¾åƒï¼Œå¹¶æ”¯æŒå¤šç§ç—…ç†å‘ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºåˆ©ç”¨åŸºäºåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„é€šç”¨å›¾åƒæ•°æ®è‡ªç›‘ç£è®­ç»ƒæ–¹æ³•ï¼Œå°†åŸºç¡€æ¨¡å‹é€‚åº”äºèƒƒè‚ é“å†…çª¥é•œå›¾åƒè¯Šæ–­ä»»åŠ¡ã€‚å…¶ä¸­EVA-02æ¨¡å‹åŸºäºViTæ¶æ„ï¼Œåœ¨ImageNet-22kæ•°æ®é›†ä¸Šé‡‡ç”¨æ©è†œå›¾åƒå»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨EndoExtend24æ•°æ®é›†ä¸Šè¿›è¡ŒåŸŸé€‚åº”ï¼Œæœ€ç»ˆåœ¨Capsule Endoscopy 2024 Challengeæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè·å¾—ç¨³å¥æ€§èƒ½ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè·å¾—å®è§‚AUC 0.762å’Œå¹³è¡¡å‡†ç¡®ç‡37.1%ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>è§†é¢‘èƒ¶å›Šå†…é•œä¸ºèƒƒè‚ é“å†…çª¥é•œè¯Šæ–­æä¾›äº†éä¾µå…¥æ€§çš„è¯¦ç»†æˆåƒæ–¹æ³•ã€‚</li><li>æˆåƒè¿‡ç¨‹äº§ç”Ÿå¤§é‡å›¾åƒï¼Œéœ€è¦è‡ªåŠ¨åŒ–åˆ†æã€‚</li><li>æ¨å‡ºæ–°çš„å¤§å‹èƒƒè‚ é“å†…çª¥é•œæ•°æ®é›†EndoExtend24ï¼ŒåŒ…å«è¶…è¿‡22.6ä¸‡å¼ æ ‡è®°å›¾åƒã€‚</li><li>EndoExtend24æ”¯æŒå¤šç§ç—…ç†å‘ç°ï¼Œå¹¶å…è®¸è·¨ä¸åŒæ ‡ç­¾ç²’åº¦çš„æ•°æ®é›†è¿›è¡Œç»Ÿä¸€è®­ç»ƒã€‚</li><li>æå‡ºåˆ©ç”¨åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•ï¼Œå°†é€šç”¨å›¾åƒæ•°æ®è‡ªç›‘ç£è®­ç»ƒçš„åŸºç¡€æ¨¡å‹é€‚åº”äºèƒƒè‚ é“å†…çª¥é•œå›¾åƒè¯Šæ–­ä»»åŠ¡ã€‚</li><li>EVA-02æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œè¡¨ç°å‡ºç¨³å¥æ€§èƒ½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cf9713e7ffc155a83738fb6388d5f539.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7c49edf263644ff07b1db7da8210619a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8f4db2dad3e28af984765d3201e9f3f.jpg" align="middle"></details><h2 id="Croc-Pretraining-Large-Multimodal-Models-with-Cross-Modal-Comprehension"><a href="#Croc-Pretraining-Large-Multimodal-Models-with-Cross-Modal-Comprehension" class="headerlink" title="Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"></a>Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension</h2><p><strong>Authors:Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng</strong></p><p>Recent advances in Large Language Models (LLMs) have catalyzed the development of Large Multimodal Models (LMMs). However, existing research primarily focuses on tuning language and image instructions, ignoring the critical pretraining phase where models learn to process textual and visual modalities jointly. In this paper, we propose a new pretraining paradigm for LMMs to enhance the visual comprehension capabilities of LLMs by introducing a novel cross-modal comprehension stage. Specifically, we design a dynamically learnable prompt token pool and employ the Hungarian algorithm to replace part of the original visual tokens with the most relevant prompt tokens. Then, we conceptualize visual tokens as analogous to a â€œforeign languageâ€ for the LLMs and propose a mixed attention mechanism with bidirectional visual attention and unidirectional textual attention to comprehensively enhance the understanding of visual tokens. Meanwhile, we integrate a detailed caption generation task, leveraging rich descriptions to further facilitate LLMs in understanding visual semantic information. After pretraining on 1.5 million publicly accessible data, we present a new foundation model called Croc. Experimental results demonstrate that Croc achieves new state-of-the-art performance on massive vision-language benchmarks. To support reproducibility and facilitate further research, we release the training code and pre-trained model weights at <a target="_blank" rel="noopener" href="https://github.com/deepglint/Croc">https://github.com/deepglint/Croc</a>.</p><blockquote><p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è°ƒæ•´è¯­è¨€å’Œå›¾åƒæŒ‡ä»¤ä¸Šï¼Œå¿½ç•¥äº†æ¨¡å‹å­¦ä¹ å¤„ç†æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„è”åˆé¢„è®­ç»ƒé˜¶æ®µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„LMMé¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„è·¨æ¨¡æ€ç†è§£é˜¶æ®µï¼Œä»¥å¢å¼ºLMMçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯åŠ¨æ€å­¦ä¹ çš„æç¤ºä»¤ç‰Œæ± ï¼Œå¹¶ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•å°†æœ€ç›¸å…³çš„æç¤ºä»¤ç‰Œæ›¿æ¢æ‰éƒ¨åˆ†åŸå§‹è§†è§‰ä»¤ç‰Œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è§†è§‰ä»¤ç‰Œæ¦‚å¿µåŒ–ä¸ºå¯¹LLMè€Œè¨€ç±»ä¼¼äºâ€œå¤–è¯­â€ï¼Œå¹¶æå‡ºä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬åŒå‘è§†è§‰æ³¨æ„åŠ›å’Œå•å‘æ–‡æœ¬æ³¨æ„åŠ›ï¼Œä»¥å…¨é¢æé«˜å¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ•´åˆäº†è¯¦ç»†çš„æ ‡é¢˜ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨ä¸°å¯Œçš„æè¿°æ¥è¿›ä¸€æ­¥å¸®åŠ©LLMç†è§£è§†è§‰è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨å…¬å¼€å¯ç”¨çš„150ä¸‡æ¡æ•°æ®è¿›è¡Œé¢„è®­ç»ƒåï¼Œæˆ‘ä»¬æ¨å‡ºäº†åä¸ºCrocçš„æ–°åŸºç¡€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrocåœ¨å¤§é‡çš„è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æ”¯æŒå¯å¤åˆ¶æ€§å’Œä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepglint/Croc%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%82">https://github.com/deepglint/Crocä¸Šå‘å¸ƒäº†è®­ç»ƒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14332v2">PDF</a> 18 pages, 11 figures</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼•å…¥è·¨æ¨¡æ€ç†è§£é˜¶æ®µï¼Œå¢å¼ºLLMså¯¹è§†è§‰å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è®¾è®¡äº†åŠ¨æ€å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œæ± ï¼Œå¹¶é‡‡ç”¨åŒˆç‰™åˆ©ç®—æ³•æ›¿æ¢æœ€ç›¸å…³çš„è§†è§‰ä»¤ç‰Œã€‚åŒæ—¶ï¼Œæå‡ºäº†æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•´åˆè¯¦ç»†çš„å›¾åƒæè¿°ç”Ÿæˆä»»åŠ¡ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºLLMå¯¹è§†è§‰è¯­ä¹‰ä¿¡æ¯çš„ç†è§£ã€‚ç»è¿‡åœ¨150ä¸‡å…¬å¼€æ•°æ®ä¸Šçš„é¢„è®­ç»ƒï¼Œæ–°æå‡ºçš„æ¨¡å‹Crocåœ¨è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é¢„è®­ç»ƒèŒƒå¼ï¼Œä¸“æ³¨äºå¢å¼ºæ¨¡å‹å¯¹è§†è§‰å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚</li><li>è®¾è®¡äº†åŠ¨æ€å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œæ± ï¼Œåˆ©ç”¨åŒˆç‰™åˆ©ç®—æ³•æ›¿æ¢è§†è§‰ä»¤ç‰Œã€‚</li><li>å¼•å…¥æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•´åˆåŒå‘è§†è§‰æ³¨æ„åŠ›å’Œå•å‘æ–‡æœ¬æ³¨æ„åŠ›ã€‚</li><li>åˆ©ç”¨è¯¦ç»†çš„å›¾åƒæè¿°ç”Ÿæˆä»»åŠ¡ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›LLMç†è§£è§†è§‰è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>åœ¨å¤§é‡å…¬å¼€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæå‡ºäº†åä¸ºCrocçš„æ–°åŸºç¡€æ¨¡å‹ã€‚</li><li>Crocæ¨¡å‹åœ¨è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4fac2c5023429cd38d7cfd08b23d572d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-83d0b44eaeba4e5120d67b0acb18ed3a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-510283aee83c3ca612cfb20cdd7a5698.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3b0d48f4ce6c1df054446abdf6f08b6a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8f2b356769bcd341221808c5d6c482e2.jpg" align="middle"></details><h2 id="Local-to-Global-Self-Supervised-Representation-Learning-for-Diabetic-Retinopathy-Grading"><a href="#Local-to-Global-Self-Supervised-Representation-Learning-for-Diabetic-Retinopathy-Grading" class="headerlink" title="Local-to-Global Self-Supervised Representation Learning for Diabetic   Retinopathy Grading"></a>Local-to-Global Self-Supervised Representation Learning for Diabetic Retinopathy Grading</h2><p><strong>Authors:Mostafa Hajighasemlou, Samad Sheikhaei, Hamid Soltanian-Zadeh</strong></p><p>Artificial intelligence algorithms have demonstrated their image classification and segmentation ability in the past decade. However, artificial intelligence algorithms perform less for actual clinical data than those used for simulations. This research aims to present a novel hybrid learning model using self-supervised learning and knowledge distillation, which can achieve sufficient generalization and robustness. The self-attention mechanism and tokens employed in ViT, besides the local-to-global learning approach used in the hybrid model, enable the proposed algorithm to extract a high-dimensional and high-quality feature space from images. To demonstrate the proposed neural networkâ€™s capability in classifying and extracting feature spaces from medical images, we use it on a dataset of Diabetic Retinopathy images, specifically the EyePACS dataset. This dataset is more complex structurally and challenging regarding damaged areas than other medical images. For the first time in this study, self-supervised learning and knowledge distillation are used to classify this dataset. In our algorithm, for the first time among all self-supervised learning and knowledge distillation models, the test dataset is 50% larger than the training dataset. Unlike many studies, we have not removed any images from the dataset. Finally, our algorithm achieved an accuracy of 79.1% in the linear classifier and 74.36% in the k-NN algorithm for multiclass classification. Compared to a similar state-of-the-art model, our results achieved higher accuracy and more effective representation spaces.</p><blockquote><p>åœ¨è¿‡å»çš„åå¹´ä¸­ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•å·²ç»è¯æ˜äº†å®ƒä»¬åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸æ¨¡æ‹Ÿä¸­æ‰€ç”¨çš„ç›¸æ¯”ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•åœ¨å®é™…ä¸´åºŠæ•°æ®ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°å‹æ··åˆå­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå¯ä»¥å®ç°è¶³å¤Ÿçš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚ViTä¸­ä½¿ç”¨çš„è‡ªæ³¨æ„æœºåˆ¶å’Œä»¤ç‰Œä»¥åŠæ··åˆæ¨¡å‹ä¸­é‡‡ç”¨çš„ä»å±€éƒ¨åˆ°å…¨å±€çš„å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ‰€æè®®çš„ç®—æ³•èƒ½å¤Ÿä»å›¾åƒä¸­æå–é«˜ç»´å’Œé«˜è´¨é‡ç‰¹å¾ç©ºé—´ã€‚ä¸ºäº†å±•ç¤ºæ‰€æå‡ºç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»å’Œç‰¹å¾ç©ºé—´æå–æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜å›¾åƒçš„æ•°æ®é›†ä¸Šä½¿ç”¨äº†å®ƒï¼Œç‰¹åˆ«æ˜¯EyePACSæ•°æ®é›†ã€‚ä¸å…¶ä»–åŒ»å­¦å›¾åƒç›¸æ¯”ï¼Œæ­¤æ•°æ®é›†åœ¨ç»“æ„ä¸Šæ›´ä¸ºå¤æ‚ï¼Œåœ¨å—æŸåŒºåŸŸæ–¹é¢æ›´å…·æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶ä¸­é¦–æ¬¡ä½¿ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦å¯¹æ­¤æ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚åœ¨æˆ‘ä»¬çš„ç®—æ³•ä¸­ï¼Œä¸å…¶ä»–æ‰€æœ‰è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦æ¨¡å‹ç›¸æ¯”ï¼Œæµ‹è¯•æ•°æ®é›†é¦–æ¬¡æ¯”è®­ç»ƒæ•°æ®é›†å¤§50%ã€‚ä¸å…¶ä»–è®¸å¤šç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬æ²¡æœ‰ä»æ•°æ®é›†ä¸­åˆ é™¤ä»»ä½•å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨çº¿æ€§åˆ†ç±»å™¨ä¸­è¾¾åˆ°äº†79.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨k-NNç®—æ³•ä¸­çš„å¤šç±»åˆ†ç±»è¾¾åˆ°äº†74.36%ã€‚ä¸ç±»ä¼¼çš„æœ€æ–°æŠ€æœ¯æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç»“æœè·å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´æœ‰æ•ˆçš„è¡¨ç¤ºç©ºé—´ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00779v3">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆè‡ªç›‘ç£å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦çš„æ–°å‹æ··åˆå­¦ä¹ æ¨¡å‹ï¼Œç”¨äºå¤„ç†åŒ»å­¦å›¾åƒåˆ†ç±»ä¸ç‰¹å¾æå–ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ViTä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ä»¤ç‰Œï¼Œç»“åˆå±€éƒ¨åˆ°å…¨å±€çš„å­¦ä¹ æ–¹å¼ï¼Œèƒ½å¤Ÿä»å›¾åƒä¸­æå–å‡ºé«˜è´¨é‡çš„é«˜ç»´ç‰¹å¾ç©ºé—´ã€‚åœ¨Diabetic Retinopathyçš„EyePACSæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»ä¸ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œè¾¾åˆ°äº†79.1%çš„çº¿æ€§åˆ†ç±»å™¨å‡†ç¡®ç‡å’Œ74.36%çš„k-NNç®—æ³•å¤šç±»åˆ†ç±»å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºç±»ä¼¼çš„å‰æ²¿æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´æœ‰æ•ˆçš„è¡¨å¾ç©ºé—´ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆå­¦ä¹ æ¨¡å‹ï¼Œç»“åˆè‡ªç›‘ç£å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦ã€‚</li><li>æ¨¡å‹é‡‡ç”¨ViTä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ä»¤ç‰Œï¼Œå®ç°å›¾åƒåˆ†ç±»ä¸åˆ†å‰²ã€‚</li><li>æ¨¡å‹èƒ½å¤Ÿæå–å‡ºé«˜è´¨é‡çš„é«˜ç»´ç‰¹å¾ç©ºé—´ã€‚</li><li>åœ¨Diabetic Retinopathyçš„EyePACSæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li><li>æ¨¡å‹å¤„ç†å¤æ‚ç»“æ„çš„æ•°æ®é›†è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li><li>æµ‹è¯•æ•°æ®é›†é¦–æ¬¡å¤§äºè®­ç»ƒæ•°æ®é›†ï¼Œä¸”æœªåˆ é™¤ä»»ä½•å›¾åƒã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-71166837b37799c2254baa0b71b8f5ca.jpg" align="middle"></details><h2 id="Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches"><a href="#Patch-Ranking-Efficient-CLIP-by-Learning-to-Rank-Local-Patches" class="headerlink" title="Patch Ranking: Efficient CLIP by Learning to Rank Local Patches"></a>Patch Ranking: Efficient CLIP by Learning to Rank Local Patches</h2><p><strong>Authors:Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado</strong></p><p>Contrastive image-text pre-trained models such as CLIP have shown remarkable adaptability to downstream tasks. However, they face challenges due to the high computational requirements of the Vision Transformer (ViT) backbone. Current strategies to boost ViT efficiency focus on pruning patch tokens but fall short in addressing the multimodal nature of CLIP and identifying the optimal subset of tokens for maximum performance. To address this, we propose greedy search methods to establish a â€œGolden Rankingâ€ and introduce a lightweight predictor specifically trained to approximate this Ranking. To compensate for any performance degradation resulting from token pruning, we incorporate learnable visual tokens that aid in restoring and potentially enhancing the modelâ€™s performance. Our work presents a comprehensive and systematic investigation of pruning tokens within the ViT backbone of CLIP models. Through our framework, we successfully reduced 40% of patch tokens in CLIPâ€™s ViT while only suffering a minimal average accuracy loss of 0.3 across seven datasets. Our study lays the groundwork for building more computationally efficient multimodal models without sacrificing their performance, addressing a key challenge in the application of advanced vision-language models.</p><blockquote><p>å¯¹æ¯”å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚CLIPï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°å‡ºæ˜¾è‘—çš„é€‚åº”æ€§ã€‚ç„¶è€Œï¼Œç”±äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸»å¹²çš„é«˜è®¡ç®—è¦æ±‚ï¼Œå®ƒä»¬é¢ä¸´ç€æŒ‘æˆ˜ã€‚å½“å‰æé«˜ViTæ•ˆç‡çš„ç­–ç•¥ä¸»è¦é›†ä¸­äºä¿®å‰ªè¡¥ä¸ä»¤ç‰Œï¼Œä½†åœ¨å¤„ç†CLIPçš„å¤šæ¨¡æ€ç‰¹æ€§å’Œç¡®å®šè·å¾—æœ€ä½³æ€§èƒ½çš„ä»¤ç‰Œæœ€ä¼˜å­é›†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè´ªå¿ƒæœç´¢æ–¹æ³•å»ºç«‹â€œé‡‘ç‰Œæ’åâ€ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªä¸“é—¨è®­ç»ƒçš„è½»é‡çº§é¢„æµ‹å™¨æ¥è¿‘ä¼¼è¿™ä¸ªæ’åã€‚ä¸ºäº†å¼¥è¡¥å› ä»¤ç‰Œä¿®å‰ªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å­¦ä¹ çš„è§†è§‰ä»¤ç‰Œï¼Œæœ‰åŠ©äºæ¢å¤å¹¶å¯èƒ½æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œå…¨é¢ç³»ç»Ÿåœ°ç ”ç©¶äº†CLIPæ¨¡å‹ä¸­ViTéª¨å¹²çš„ä»¤ç‰Œä¿®å‰ªã€‚é€šè¿‡æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨CLIPçš„ViTä¸­æˆåŠŸå‡å°‘äº†40%çš„è¡¥ä¸ä»¤ç‰Œï¼ŒåŒæ—¶åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®åº¦ä»…æŸå¤±äº†0.3%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºæ›´åŠ è®¡ç®—é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œè€Œä¸ä¼šç‰ºç‰²å…¶æ€§èƒ½ï¼Œè¿™è§£å†³äº†å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14607v2">PDF</a> Accepted by WACV 2025</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ç ”ç©¶äº†CLIPæ¨¡å‹ä¸­çš„Vision Transformerï¼ˆViTï¼‰åœ¨å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„é«˜æ•ˆæ€§æŒ‘æˆ˜ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºè´ªå¿ƒæœç´¢çš„â€œé»„é‡‘æ’åâ€æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§é¢„æµ‹å™¨è¿›è¡Œè¿‘ä¼¼æ’åã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è§†è§‰ä»¤ç‰Œæ¥å¼¥è¡¥ç”±äºä»¤ç‰Œå‰ªæé€ æˆçš„æ€§èƒ½ä¸‹é™ï¼Œç”šè‡³å¯ä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°å¯¹CLIPæ¨¡å‹ä¸­ViTçš„ä»¤ç‰Œå‰ªæè¿›è¡Œäº†æ¢ç©¶ï¼ŒæˆåŠŸåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šå‡å°‘40%çš„ä»¤ç‰Œï¼Œä¸”å¹³å‡å‡†ç¡®åº¦ä»…ä¸‹é™0.3%ã€‚è¿™ä¸ºæ„å»ºé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œè§£å†³äº†å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>CLIPç­‰å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ã€‚</li><li>Vision Transformerï¼ˆViTï¼‰æ˜¯CLIPæ¨¡å‹çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä½†è®¡ç®—è¦æ±‚è¾ƒé«˜ã€‚</li><li>å½“å‰ç­–ç•¥ä¾§é‡äºé€šè¿‡å‰ªæä»¤ç‰Œæ¥æé«˜ViTçš„æ•ˆç‡ï¼Œä½†æœªèƒ½å……åˆ†è§£å†³CLIPçš„å¤šæ¨¡æ€ç‰¹æ€§å’Œè¯†åˆ«æœ€ä½³ä»¤ç‰Œå­é›†çš„é—®é¢˜ã€‚</li><li>æå‡ºåŸºäºè´ªå¿ƒæœç´¢çš„â€œé»„é‡‘æ’åâ€æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li><li>å¼•å…¥è½»é‡çº§é¢„æµ‹å™¨æ¥è¿‘ä¼¼æ’åï¼Œä»¥å¼¥è¡¥å› ä»¤ç‰Œå‰ªæå¯¼è‡´çš„æ€§èƒ½æŸå¤±ã€‚</li><li>é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è§†è§‰ä»¤ç‰Œæ¥æ¢å¤å¹¶å¯èƒ½å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4b50734fdd8a61a05103e07a48cddc85.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-acc868f79e316a0a2a222f988321e1e2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f6fc089ef5f80ecfc84c97cba9322acd.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-25436ad5d9b413ff1e62690a36714432.jpg" align="middle"></details><h2 id="Generalizing-Deepfake-Video-Detection-with-Plug-and-Play-Video-Level-Blending-and-Spatiotemporal-Adapter-Tuning"><a href="#Generalizing-Deepfake-Video-Detection-with-Plug-and-Play-Video-Level-Blending-and-Spatiotemporal-Adapter-Tuning" class="headerlink" title="Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level   Blending and Spatiotemporal Adapter Tuning"></a>Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning</h2><p><strong>Authors:Zhiyuan Yan, Yandan Zhao, Shen Chen, Mingyi Guo, Xinghe Fu, Taiping Yao, Shouhong Ding, Li Yuan</strong></p><p>Three key challenges hinder the development of current deepfake video detection: (1) Temporal features can be complex and diverse: how can we identify general temporal artifacts to enhance model generalization? (2) Spatiotemporal models often lean heavily on one type of artifact and ignore the other: how can we ensure balanced learning from both? (3) Videos are naturally resource-intensive: how can we tackle efficiency without compromising accuracy? This paper attempts to tackle the three challenges jointly. First, inspired by the notable generality of using image-level blending data for image forgery detection, we investigate whether and how video-level blending can be effective in video. We then perform a thorough analysis and identify a previously underexplored temporal forgery artifact: Facial Feature Drift (FFD), which commonly exists across different forgeries. To reproduce FFD, we then propose a novel Video-level Blending data (VB), where VB is implemented by blending the original image and its warped version frame-by-frame, serving as a hard negative sample to mine more general artifacts. Second, we carefully design a lightweight Spatiotemporal Adapter (StA) to equip a pretrained image model (both ViTs and CNNs) with the ability to capture both spatial and temporal features jointly and efficiently. StA is designed with two-stream 3D-Conv with varying kernel sizes, allowing it to process spatial and temporal features separately. Extensive experiments validate the effectiveness of the proposed methods; and show our approach can generalize well to previously unseen forgery videos, even the latest generation methods.</p><blockquote><p>å½“å‰æ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹çš„å‘å±•é¢ä¸´ä¸‰å¤§å…³é”®æŒ‘æˆ˜ï¼š</p></blockquote><p>ï¼ˆ1ï¼‰æ—¶åºç‰¹å¾å¯èƒ½å¤æ‚ä¸”å¤šæ ·ï¼šæˆ‘ä»¬å¦‚ä½•è¯†åˆ«é€šç”¨æ—¶åºä¼ªè¿¹ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿ</p><p>ï¼ˆ2ï¼‰æ—¶ç©ºæ¨¡å‹é€šå¸¸åå‘äºä¸€ç§ä¼ªè¿¹è€Œå¿½è§†å¦ä¸€ç§ï¼šæˆ‘ä»¬å¦‚ä½•ç¡®ä¿ä»ä¸¤è€…ä¸­å¹³è¡¡å­¦ä¹ ï¼Ÿ</p><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17065v2">PDF</a></p><p><strong>æ‘˜è¦</strong></p><p>æœ¬æ–‡å°è¯•è”åˆè§£å†³å½“å‰æ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼š1ï¼‰å¤æ‚ä¸”å¤šæ ·çš„æ—¶é—´ç‰¹å¾ï¼Œå¦‚ä½•è¯†åˆ«é€šç”¨æ—¶é—´ä¼ªå½±ä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼›2ï¼‰æ—¶ç©ºæ¨¡å‹å€¾å‘äºä¾èµ–ä¸€ç§ä¼ªå½±è€Œå¿½è§†å¦ä¸€ç§ï¼Œå¦‚ä½•ç¡®ä¿ä¸¤è€…çš„å¹³è¡¡å­¦ä¹ ï¼›3ï¼‰è§†é¢‘èµ„æºå¯†é›†ï¼Œå¦‚ä½•åœ¨ä¸é™ä½å‡†ç¡®æ€§çš„æƒ…å†µä¸‹è§£å†³æ•ˆç‡é—®é¢˜ã€‚ç ”ç©¶å—å›¾åƒçº§æ··åˆæ•°æ®ç”¨äºå›¾åƒä¼ªé€ æ£€æµ‹çš„æ™®éæ€§å¯å‘ï¼Œæ¢ç´¢è§†é¢‘çº§æ··åˆåœ¨è§†é¢‘æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°äº†ä¸€ç§ä¹‹å‰æœªè¢«å……åˆ†ç ”ç©¶çš„ä¸´æ—¶ä¼ªé€ ä¼ªå½±ï¼šé¢éƒ¨ç‰¹å¾æ¼‚ç§»ï¼ˆFFDï¼‰ã€‚ä¸ºäº†å†ç°FFDï¼Œæå‡ºäº†ä¸€ç§æ–°å‹è§†é¢‘çº§æ··åˆæ•°æ®ï¼ˆVBï¼‰ï¼Œé€šè¿‡å°†åŸå§‹å›¾åƒåŠå…¶å˜å½¢ç‰ˆæœ¬é€å¸§æ··åˆæ¥å®æ–½VBï¼Œä½œä¸ºæŒ–æ˜æ›´é€šç”¨ä¼ªé€ çš„ç¡¬è´Ÿæ ·æœ¬ã€‚ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„æ—¶ç©ºé€‚é…å™¨ï¼ˆStAï¼‰ï¼Œä½¿é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹ï¼ˆåŒ…æ‹¬ViTså’ŒCNNsï¼‰èƒ½å¤Ÿè”åˆä¸”é«˜æ•ˆåœ°æ•è·ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚StAé‡‡ç”¨å…·æœ‰ä¸åŒå†…æ ¸å¤§å°çš„ä¸¤æµ3Då·ç§¯è®¾è®¡ï¼Œå¯åˆ†åˆ«å¤„ç†ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚å¤§é‡å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°ä»¥å‰æœªè§è¿‡çš„ä¼ªé€ è§†é¢‘ï¼Œç”šè‡³æ˜¯æœ€æ–°çš„æ–¹æ³•ã€‚</p><p><strong>å…³é”®è§è§£</strong></p><ol><li>å½“å‰æ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ—¶é—´ç‰¹å¾å¤æ‚æ€§ã€æ—¶ç©ºæ¨¡å‹çš„å¹³è¡¡å­¦ä¹ ã€è§†é¢‘èµ„æºçš„é«˜å¯†åº¦ã€‚</li><li>æå‡ºä½¿ç”¨è§†é¢‘çº§æ··åˆæ¥å‘æ˜æ›´é€šç”¨çš„ä¼ªé€ ä¼ªå½±ï¼Œå¹¶å¼•å…¥é¢éƒ¨ç‰¹å¾æ¼‚ç§»ï¼ˆFFDï¼‰è¿™ä¸€æ–°çš„ä¸´æ—¶ä¼ªé€ ä¼ªå½±æ¦‚å¿µã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°å‹çš„è½»é‡çº§æ—¶ç©ºé€‚é…å™¨ï¼ˆStAï¼‰ï¼Œä½¿é¢„è®­ç»ƒå›¾åƒæ¨¡å‹èƒ½å¤Ÿè”åˆæ•è·ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚</li><li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶å¯¹æœªçŸ¥ä¼ªé€ è§†é¢‘çš„ä¼˜å¼‚æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§ç±»å‹çš„ä¼ªé€ è§†é¢‘ï¼Œå³ä½¿æ˜¯æœ€æ–°çš„ä¼ªé€ æŠ€æœ¯ä¹Ÿèƒ½æœ‰æ•ˆæ£€æµ‹ã€‚</li><li>è¯¥æ–¹æ³•ä¸ä»…æ³¨é‡å‡†ç¡®æ€§ï¼Œè¿˜å…¼é¡¾æ•ˆç‡å’Œæ¨¡å‹è½»é‡åŒ–ï¼Œä¸ºè§£å†³è§†é¢‘èµ„æºå¯†é›†å‹é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li><li>è§†é¢‘çº§æ··åˆå’Œæ—¶ç©ºé€‚é…å™¨çš„ç»“åˆä½¿ç”¨ä¸ºæ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26403bcf86a4e06aa0a0588d5f8dc98d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3ab2357afab60d89a4f9d8798d9a900b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e36d6c564478b66dfef23d88cf5ac8b2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ca20647e44cbfe4651a7c3403acdbcf0.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5de1a1d6927644a276545568298cb68d.jpg" align="middle"></details><h2 id="GalLoP-Learning-Global-and-Local-Prompts-for-Vision-Language-Models"><a href="#GalLoP-Learning-Global-and-Local-Prompts-for-Vision-Language-Models" class="headerlink" title="GalLoP: Learning Global and Local Prompts for Vision-Language Models"></a>GalLoP: Learning Global and Local Prompts for Vision-Language Models</h2><p><strong>Authors:Marc Lafon, Elias Ramzi, ClÃ©ment Rambour, Nicolas Audebert, Nicolas Thome</strong></p><p>Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs), e.g. CLIP, for few-shot image classification. Despite their success, most prompt learning methods trade-off between classification accuracy and robustness, e.g. in domain generalization or out-of-distribution (OOD) detection. In this work, we introduce Global-Local Prompts (GalLoP), a new prompt learning method that learns multiple diverse prompts leveraging both global and local visual features. The training of the local prompts relies on local features with an enhanced vision-text alignment. To focus only on pertinent features, this local alignment is coupled with a sparsity strategy in the selection of the local features. We enforce diversity on the set of prompts using a new &#96;&#96;prompt dropoutâ€™â€™ technique and a multiscale strategy on the local prompts. GalLoP outperforms previous prompt learning methods on accuracy on eleven datasets in different few shots settings and with various backbones. Furthermore, GalLoP shows strong robustness performances in both domain generalization and OOD detection, even outperforming dedicated OOD detection methods. Code and instructions to reproduce our results: <a target="_blank" rel="noopener" href="https://github.com/MarcLafon/gallop">https://github.com/MarcLafon/gallop</a>.</p><blockquote><p>æç¤ºå­¦ä¹ å·²è¢«å¹¿æ³›åº”ç”¨äºé«˜æ•ˆåœ°é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ï¼Œä»¥è¿›è¡Œå°‘é‡å›¾åƒåˆ†ç±»ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†å¤§å¤šæ•°æç¤ºå­¦ä¹ æ–¹æ³•åœ¨åˆ†ç±»ç²¾åº¦å’Œç¨³å¥æ€§ï¼ˆä¾‹å¦‚åœ¨åŸŸæ³›åŒ–æˆ–å¼‚å¸¸å€¼æ£€æµ‹ï¼‰ä¹‹é—´è¿›è¡Œäº†æƒè¡¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å±€éƒ¨æç¤ºï¼ˆGalLoPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨è§†è§‰ç‰¹å¾å­¦ä¹ å¤šä¸ªä¸åŒçš„æç¤ºã€‚å±€éƒ¨æç¤ºçš„è®­ç»ƒä¾èµ–äºå…·æœ‰å¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½çš„å±€éƒ¨ç‰¹å¾ã€‚ä¸ºäº†åªå…³æ³¨å…³é”®ç‰¹å¾ï¼Œè¿™ç§å±€éƒ¨å¯¹é½ä¸é€‰æ‹©å±€éƒ¨ç‰¹å¾çš„ç¨€ç–ç­–ç•¥ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°çš„â€œæç¤ºä¸¢å¼ƒâ€æŠ€æœ¯å’Œå±€éƒ¨æç¤ºçš„å¤šå°ºåº¦ç­–ç•¥ï¼Œåœ¨æç¤ºé›†ä¸Šå¼ºåˆ¶æ‰§è¡Œå¤šæ ·æ€§ã€‚GalLoPåœ¨å¤šä¸ªæ•°æ®é›†çš„ä¸åŒå°‘é‡è®¾ç½®å’Œå¤šç§ä¸»å¹²ç½‘ç»œä¸Šï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºä¹‹å‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨åŸŸæ³›åŒ–å’Œå¼‚å¸¸å€¼æ£€æµ‹æ–¹é¢ï¼ŒGalLoPä¹Ÿæ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç¨³å¥æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“é—¨çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•ã€‚å¤åˆ¶æˆ‘ä»¬çš„ç»“æœçš„ä»£ç å’Œè¯´æ˜ï¼š<a target="_blank" rel="noopener" href="https://github.com/MarcLafon/gallop">https://github.com/MarcLafon/gallop</a>ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01400v2">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•â€”â€”å…¨å±€å±€éƒ¨æç¤ºï¼ˆGalLoPï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…¨å±€å’Œå±€éƒ¨è§†è§‰ç‰¹å¾å­¦ä¹ å¤šä¸ªä¸åŒçš„æç¤ºã€‚é€šè¿‡å¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½å’Œé€‰æ‹©å±€éƒ¨ç‰¹å¾çš„ç¨€ç–ç­–ç•¥æ¥è®­ç»ƒå±€éƒ¨æç¤ºã€‚ä½¿ç”¨æ–°çš„æç¤ºä¸¢å¼ƒæŠ€æœ¯å’Œå±€éƒ¨æç¤ºçš„å¤šå°ºåº¦ç­–ç•¥æ¥å¢å¼ºæç¤ºé›†çš„å¤šæ ·æ€§ã€‚åœ¨ä¸åŒçš„å°‘æ ·æœ¬è®¾ç½®å’Œå¤šç§ä¸»å¹²ç½‘ç»œä¸Šï¼ŒGalLoPåœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å‡†ç¡®åº¦è¶…è¿‡äº†ä¹‹å‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåœ¨åŸŸæ¨å¹¿å’ŒOODæ£€æµ‹ä¸­ï¼ŒGalLoPè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“é—¨çš„OODæ£€æµ‹æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å…¨å±€å±€éƒ¨æç¤ºï¼ˆGalLoPï¼‰æ˜¯ä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº†å…¨å±€å’Œå±€éƒ¨è§†è§‰ç‰¹å¾è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li><li>å±€éƒ¨æç¤ºçš„è®­ç»ƒä¾èµ–äºå¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½å’Œå±€éƒ¨ç‰¹å¾çš„ç¨€ç–é€‰æ‹©ç­–ç•¥ã€‚</li><li>GalLoPé€šè¿‡ä½¿ç”¨æç¤ºä¸¢å¼ƒæŠ€æœ¯å’Œå¤šå°ºåº¦ç­–ç•¥æ¥å¢å¼ºæç¤ºé›†çš„å¤šæ ·æ€§ã€‚</li><li>GalLoPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å°‘æ ·æœ¬è®¾ç½®è¡¨ç°å‡ºé«˜å‡†ç¡®åº¦ã€‚</li><li>GalLoPåœ¨åŸŸæ¨å¹¿ä»»åŠ¡ä¸­å±•ç°å‡ºç¨³å¥æ€§ã€‚</li><li>GalLoPåœ¨OODæ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ç”šè‡³è¶…è¶Šäº†æŸäº›ä¸“é—¨çš„æ–¹æ³•ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-237f00d55c73090da6b9b04ad47f2a83.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e01c9df94e2c5f46fb9d421e1f32aba2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-76aeb8413b2e6a351f64557226fbf06e.jpg" align="middle"></details><h2 id="3DStyleGLIP-Part-Tailored-Text-Guided-3D-Neural-Stylization"><a href="#3DStyleGLIP-Part-Tailored-Text-Guided-3D-Neural-Stylization" class="headerlink" title="3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization"></a>3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization</h2><p><strong>Authors:SeungJeh Chung, JooHyun Park, HyeongYeop Kang</strong></p><p>3D stylization, the application of specific styles to three-dimensional objects, offers substantial commercial potential by enabling the creation of uniquely styled 3D objects tailored to diverse scenes. Recent advancements in artificial intelligence and text-driven manipulation methods have made the stylization process increasingly intuitive and automated. While these methods reduce human costs by minimizing reliance on manual labor and expertise, they predominantly focus on holistic stylization, neglecting the application of desired styles to individual components of a 3D object. This limitation restricts the fine-grained controllability. To address this gap, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization. Given a 3D mesh and a text prompt, 3DStyleGLIP utilizes the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize individual parts of the 3D mesh and modify their appearance to match the styles specified in the text prompt. 3DStyleGLIP effectively integrates part localization and stylization guidance within GLIPâ€™s shared embedding space through an end-to-end process, enabled by part-level style loss and two complementary learning techniques. This neural methodology meets the userâ€™s need for fine-grained style editing and delivers high-quality part-specific stylization results, opening new possibilities for customization and flexibility in 3D content creation. Our code and results are available at <a target="_blank" rel="noopener" href="https://github.com/sj978/3DStyleGLIP">https://github.com/sj978/3DStyleGLIP</a>.</p><blockquote><p>3Dé£æ ¼åŒ–æ˜¯å°†ç‰¹å®šé£æ ¼åº”ç”¨äºä¸‰ç»´ç‰©ä½“çš„åº”ç”¨ï¼Œå®ƒé€šè¿‡åˆ›å»ºé€‚åº”ä¸åŒåœºæ™¯çš„ç‹¬ç‰¹é£æ ¼åŒ–çš„3Dç‰©ä½“ï¼Œå…·æœ‰å·¨å¤§çš„å•†ä¸šæ½œåŠ›ã€‚äººå·¥æ™ºèƒ½å’Œæ–‡æœ¬é©±åŠ¨æ“ä½œæ–¹æ³•çš„æœ€æ–°è¿›å±•ä½¿å¾—é£æ ¼åŒ–è¿‡ç¨‹è¶Šæ¥è¶Šç›´è§‚å’Œè‡ªåŠ¨åŒ–ã€‚è¿™äº›æ–¹æ³•é€šè¿‡æœ€å°åŒ–å¯¹äººå·¥åŠ³åŠ¨å’Œä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ï¼Œé™ä½äº†äººåŠ›æˆæœ¬ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨æ•´ä½“é£æ ¼åŒ–ï¼Œå¿½ç•¥äº†å°†æ‰€éœ€é£æ ¼åº”ç”¨äºä¸‰ç»´ç‰©ä½“çš„å•ä¸ªç»„ä»¶ã€‚è¿™ä¸€é™åˆ¶åˆ¶çº¦äº†ç»†ç²’åº¦æ§åˆ¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†3DStyleGLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ–‡æœ¬é©±åŠ¨ã€éƒ¨åˆ†å®šåˆ¶çš„3Dé£æ ¼åŒ–è®¾è®¡çš„å…¨æ–°æ¡†æ¶ã€‚ç»™å®šä¸€ä¸ª3Dç½‘æ ¼å’Œä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œ3DStyleGLIPåˆ©ç”¨åŸºäºGrounded Language-Image Pre-trainingï¼ˆGLIPï¼‰æ¨¡å‹çš„è§†è§‰è¯­è¨€åµŒå…¥ç©ºé—´ï¼Œå®šä½3Dç½‘æ ¼çš„å•ä¸ªéƒ¨åˆ†ï¼Œå¹¶ä¿®æ”¹å®ƒä»¬çš„å¤–è§‚ä»¥åŒ¹é…æ–‡æœ¬æç¤ºä¸­æŒ‡å®šçš„é£æ ¼ã€‚3DStyleGLIPé€šè¿‡ç«¯åˆ°ç«¯è¿‡ç¨‹æœ‰æ•ˆåœ°åœ¨GLIPçš„å…±äº«åµŒå…¥ç©ºé—´å†…æ•´åˆéƒ¨åˆ†å®šä½å’Œé£æ ¼åŒ–æŒ‡å¯¼ï¼Œè¿™å¾—ç›Šäºéƒ¨åˆ†çº§åˆ«çš„é£æ ¼æŸå¤±å’Œä¸¤ç§äº’è¡¥çš„å­¦ä¹ æŠ€æœ¯ã€‚è¿™ç§ç¥ç»æ–¹æ³•æ»¡è¶³äº†ç”¨æˆ·å¯¹ç»†ç²’åº¦é£æ ¼ç¼–è¾‘çš„éœ€æ±‚ï¼Œå¹¶æä¾›äº†é«˜è´¨é‡çš„éƒ¨ä»¶ç‰¹å®šé£æ ¼åŒ–ç»“æœï¼Œä¸º3Då†…å®¹åˆ›å»ºæä¾›äº†å®šåˆ¶å’Œçµæ´»æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sj978/3DStyleGLIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sj978/3DStyleGLIPä¸Šæ‰¾åˆ°ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.02634v2">PDF</a> 12 pages, 8 figures, 2024 Pacific Graphics Conferences (PG 2024)</p><p><strong>Summary</strong></p><p>åŸºäºæ–‡æœ¬é©±åŠ¨çš„æ–¹æ³•å¯¹ä¸‰ç»´ç‰©ä½“è¿›è¡Œç²¾ç»†åŒ–é£æ ¼åŒ–ç ”ç©¶å…·æœ‰å·¨å¤§çš„å•†ä¸šæ½œåŠ›ã€‚è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½å’Œæ–‡æœ¬é©±åŠ¨æ“ä½œæ–¹æ³•çš„è¿›æ­¥ä½¿å¾—é£æ ¼åŒ–è¿‡ç¨‹æ›´åŠ ç›´è§‚å’Œè‡ªåŠ¨åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•´ä½“é£æ ¼åŒ–ï¼Œå¿½è§†äº†åœ¨ä¸‰ç»´ç‰©ä½“çš„ä¸ªåˆ«ç»„ä»¶ä¸Šåº”ç”¨æ‰€éœ€é£æ ¼ï¼Œé™åˆ¶äº†ç²¾ç»†æ§åˆ¶çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†3DStyleGLIPæ¡†æ¶ï¼Œä¸“ä¸ºæ–‡æœ¬é©±åŠ¨ã€éƒ¨åˆ†å®šåˆ¶çš„ä¸‰ç»´é£æ ¼åŒ–è®¾è®¡ã€‚ç»™å®šä¸€ä¸ªä¸‰ç»´ç½‘æ ¼å’Œæ–‡æœ¬æç¤ºï¼Œ3DStyleGLIPåˆ©ç”¨åŸºäºè§†è§‰è¯­è¨€çš„GLIPæ¨¡å‹çš„åµŒå…¥ç©ºé—´æ¥å®šä½ä¸‰ç»´ç½‘æ ¼çš„å•ä¸ªéƒ¨åˆ†ï¼Œå¹¶ä¿®æ”¹å®ƒä»¬çš„å¤–è§‚ä»¥åŒ¹é…æ–‡æœ¬æç¤ºä¸­çš„æŒ‡å®šé£æ ¼ã€‚æ­¤æ–¹æ³•é€šè¿‡éƒ¨åˆ†é£æ ¼æŸå¤±å’Œä¸¤ç§äº’è¡¥å­¦ä¹ æŠ€æœ¯ï¼Œåœ¨GLIPçš„å…±äº«åµŒå…¥ç©ºé—´ä¸­æœ‰æ•ˆåœ°ç»“åˆäº†éƒ¨åˆ†å®šä½å’Œé£æ ¼åŒ–æŒ‡å¯¼ï¼Œæ»¡è¶³ç”¨æˆ·å¯¹ç²¾ç»†é£æ ¼ç¼–è¾‘çš„éœ€æ±‚ï¼Œå¹¶ä¸ºä¸‰ç»´å†…å®¹åˆ›å»ºæä¾›äº†é«˜åº¦å®šåˆ¶å’Œçµæ´»çš„å¯èƒ½æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æ–‡æœ¬é©±åŠ¨çš„è‡ªåŠ¨åŒ–é£æ ¼åŒ–æŠ€æœ¯åœ¨å•†ä¸šé¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li><li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•´ä½“é£æ ¼åŒ–ï¼Œç¼ºä¹é’ˆå¯¹ä¸‰ç»´ç‰©ä½“ä¸ªåˆ«ç»„ä»¶çš„é£æ ¼åº”ç”¨ã€‚</li><li>3DStyleGLIPæ¡†æ¶è§£å†³äº†è¿™ä¸€å±€é™æ€§ï¼Œå®ç°äº†æ–‡æœ¬é©±åŠ¨çš„éƒ¨åˆ†å®šåˆ¶ä¸‰ç»´é£æ ¼åŒ–ã€‚</li><li>3DStyleGLIPåˆ©ç”¨GLIPæ¨¡å‹çš„åµŒå…¥ç©ºé—´è¿›è¡Œéƒ¨åˆ†å®šä½å’Œé£æ ¼åŒ¹é…ã€‚</li><li>è¯¥æ–¹æ³•é€šè¿‡éƒ¨åˆ†é£æ ¼æŸå¤±å’Œäº’è¡¥å­¦ä¹ æŠ€æœ¯å®ç°æœ‰æ•ˆçš„é›†æˆã€‚</li><li>3DStyleGLIPæ»¡è¶³äº†ç”¨æˆ·å¯¹ç²¾ç»†é£æ ¼ç¼–è¾‘çš„éœ€æ±‚ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3133d6bf6b203e85050414ddfd5ab99f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-33e4def37236c4fccbc9fb6c61070294.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ca72a2f55e6f7a8f97316d793c79a84c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1523e24eacbd43e57e7cb372597e00e8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-729e26f6ded4fc5ba4cfc9bd31d04e15.jpg" align="middle"></details><h2 id="Just-Shift-It-Test-Time-Prototype-Shifting-for-Zero-Shot-Generalization-with-Vision-Language-Models"><a href="#Just-Shift-It-Test-Time-Prototype-Shifting-for-Zero-Shot-Generalization-with-Vision-Language-Models" class="headerlink" title="Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization   with Vision-Language Models"></a>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models</h2><p><strong>Authors:Elaine Sui, Xiaohan Wang, Serena Yeung-Levy</strong></p><p>Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 image classification datasets involving natural distribution shifts and cross-dataset generalization, as well as in context-dependent visual reasoning, demonstrate TPSâ€™s superior performance, achieving state-of-the-art results while reducing resource requirements.</p><blockquote><p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰æ½œåŠ›ï¼Œä½†ç”±äºæµ‹è¯•ç¯å¢ƒä¸­çš„åŸŸåç§»ï¼Œå®ƒä»¬çš„æ•ˆåŠ›å¾€å¾€ä¼šé™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶åŸå‹åç§»ï¼ˆTPSï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨æ— æ ‡ç­¾çš„æµ‹è¯•è¾“å…¥ä½¿VLMsé€‚åº”æµ‹è¯•æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­è°ƒåˆ¶æ¯ç±»åŸå‹çš„æ¦‚å¿µã€‚é€šè¿‡é¢„è®¡ç®—å’Œç¼“å­˜ä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆçš„åŸå‹ï¼ŒTPSä¸ä»…ä¾¿äºåç»­é¢„æµ‹çš„ä¼˜åŒ–åŸå‹é‡ç”¨ï¼Œè¿˜å¯æ— ç¼èå…¥å½“å‰çš„æç¤ºå·¥ç¨‹è¿›å±•ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒTPSä»…æ ¹æ®ç»™å®šçš„æµ‹è¯•æ ·æœ¬åŠ¨æ€å­¦ä¹ æ¯ä¸ªåŸå‹çš„åç§»å‘é‡ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†åŸŸå·®è·å¹¶æé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚ä¸ä¼ ç»Ÿæ–‡æœ¬æç¤ºè°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æ¡†æ¶çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯å…¶å¤§å¤§é™ä½äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚åœ¨æ¶‰åŠè‡ªç„¶åˆ†å¸ƒåç§»ã€è·¨æ•°æ®é›†æ¨å¹¿ä»¥åŠä¸Šä¸‹æ–‡ç›¸å…³è§†è§‰æ¨ç†çš„15ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œå±•ç¤ºäº†TPSçš„å“è¶Šæ€§èƒ½ï¼Œåœ¨é™ä½èµ„æºè¦æ±‚çš„åŒæ—¶å–å¾—äº†æœ€æ–°ç»“æœã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12952v2">PDF</a> Accepted at WACV 2025</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æµ‹è¯•ç¯å¢ƒä¸­å‡ºç°é¢†åŸŸæ¼‚ç§»é—®é¢˜çš„ä¸€ç§è§£å†³æ–¹æ¡ˆâ€”â€”Test-Time Prototype Shiftingï¼ˆTPSï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æœªæ ‡è®°çš„æµ‹è¯•è¾“å…¥è‡ªé€‚åº”è°ƒæ•´æ¨¡å‹ï¼Œé€šè¿‡è°ƒåˆ¶å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„æ¯ç±»åŸå‹æ¥å®ç°ã€‚è¯¥æ–¹æ³•åŸºäºé¢„è®¡ç®—çš„åŸå‹å’Œæ–‡æœ¬ç¼–ç å™¨çš„ç¼“å­˜ï¼Œä¸ä»…å®ç°äº†ä¼˜åŒ–åçš„åŸå‹é‡ç”¨ï¼Œè€Œä¸”å¯ä»¥ä¸å½“å‰çš„æç¤ºå·¥ç¨‹æŠ€æœ¯æ— ç¼é›†æˆã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒTPSæ ¹æ®ç»™å®šçš„æµ‹è¯•æ ·æœ¬åŠ¨æ€å­¦ä¹ æ¯ä¸ªåŸå‹çš„è½¬ç§»å‘é‡ï¼Œæœ‰æ•ˆå¼¥åˆé¢†åŸŸå·®è·å¹¶æé«˜äº†åˆ†ç±»å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿæ–‡æœ¬æç¤ºè°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼ŒTPSå…·æœ‰æ˜¾è‘—å‡å°‘çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼ŒTPSåœ¨æ¶‰åŠè‡ªç„¶åˆ†å¸ƒå˜åŒ–å’Œè·¨æ•°æ®é›†æ³›åŒ–çš„å›¾åƒåˆ†ç±»ä»¥åŠä¸Šä¸‹æ–‡ç›¸å…³çš„è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Test-Time Prototype Shifting (TPS) æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è‡³æµ‹è¯•æ•°æ®é›†çš„æ¡†æ¶ã€‚</li><li>TPS é€šè¿‡è°ƒåˆ¶å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„æ¯ç±»åŸå‹æ¥è§£å†³é¢†åŸŸæ¼‚ç§»é—®é¢˜ã€‚</li><li>TPS åˆ©ç”¨é¢„è®¡ç®—çš„åŸå‹å’Œæ–‡æœ¬ç¼–ç å™¨çš„ç¼“å­˜å®ç°ä¼˜åŒ–åçš„åŸå‹é‡ç”¨ã€‚</li><li>TPS åœ¨æµ‹è¯•æ—¶æ ¹æ®æµ‹è¯•æ ·æœ¬åŠ¨æ€è°ƒæ•´åŸå‹ï¼Œå¼¥åˆé¢†åŸŸå·®è·å¹¶æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</li><li>TPS ä¸ä¼ ç»Ÿæ–‡æœ¬æç¤ºè°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚</li><li>TPS åœ¨å›¾åƒåˆ†ç±»ã€è‡ªç„¶åˆ†å¸ƒå˜åŒ–å’Œè·¨æ•°æ®é›†æ³›åŒ–ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a47e8f55a73fd9a415be32e8b16e2e4d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d9f1396abd34c5c00c8fc795acb48d19.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2258dedc9776a7be355d1d33bffc4e7b.jpg" align="middle"></details><h2 id="Efficient-Prompt-Tuning-of-Large-Vision-Language-Model-for-Fine-Grained-Ship-Classification"><a href="#Efficient-Prompt-Tuning-of-Large-Vision-Language-Model-for-Fine-Grained-Ship-Classification" class="headerlink" title="Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained   Ship Classification"></a>Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification</h2><p><strong>Authors:Long Lan, Fengxiang Wang, Xiangtao Zheng, Zengmao Wang, Xinwang Liu</strong></p><p>Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the modelâ€™s generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available.</p><blockquote><p>é¥æ„Ÿä¸­çš„ç²¾ç»†ç²’åº¦èˆ¹èˆ¶åˆ†ç±»ï¼ˆRS-FGSCï¼‰ç”±äºç±»åˆ«ä¹‹é—´çš„ç›¸ä¼¼æ€§é«˜å’Œæ ‡è®°æ•°æ®æœ‰é™ï¼Œå¯¹ä¼ ç»Ÿç›‘ç£åˆ†ç±»æ–¹æ³•çš„æœ‰æ•ˆæ€§æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘çš„å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°‘é‡æˆ–é›¶æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å›¾åƒå†…å®¹æ–¹é¢ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºæŒ–æ˜VLMsçš„æ½œåŠ›ï¼Œä»¥æé«˜å¯¹æœªè§èˆ¹èˆ¶ç±»åˆ«çš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œè¿™åœ¨ç”±äºæˆæœ¬æˆ–éšç§çº¦æŸè€Œå¯¼è‡´æ•°æ®å—é™çš„åœºæ™¯ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç›´æ¥å¯¹RS-FGSCå¾®è°ƒVLMsç»å¸¸ä¼šé‡åˆ°è¿‡åº¦æ‹Ÿåˆå·²è§ç±»åˆ«çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ä¸ä½³ï¼Œè¿™çªå‡ºäº†åŒºåˆ†å¤æ‚èƒŒæ™¯å’Œæ•æ‰èˆ¹èˆ¶ç‰¹å¾å›°éš¾çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨åˆ†å±‚å¤šç²’åº¦æç¤ºè®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»å°å‹å¯è®­ç»ƒç½‘ç»œä¸­å­¦ä¹ é¥æ„Ÿèˆ¹èˆ¶å…ˆéªŒï¼Œå°†é¥æ„Ÿèˆ¹èˆ¶å…ˆéªŒé›†æˆåˆ°æ¨¡å‹ä¸­ã€‚æ­¤ç­–ç•¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜äº†å…¶åŒºåˆ†å¤æ‚èƒŒæ™¯å’Œå­¦ä¹ èƒ½åŠ›èˆ¹èˆ¶ç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¼•å…¥ç»¼åˆæ•°æ®é›†FGSCM-52ä¸ºè¯¥é¢†åŸŸåšå‡ºè´¡çŒ®ï¼Œè¯¥æ•°æ®é›†å¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œäº†é‡å¤§æ‰©å±•ï¼ŒåŒ…å«æ›´å¤šå¹¿æ³›çš„æ•°æ®å’Œç½•è§çš„èˆ¹èˆ¶ç±»åˆ«çš„è¯¦ç»†æ³¨é‡Šã€‚å¤§é‡çš„å®éªŒè¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æºä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08271v2">PDF</a> It has been accepted by TGRS</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œè¿œç¨‹æ„ŸçŸ¥ç²¾ç»†èˆ¹èˆ¶åˆ†ç±»çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚é’ˆå¯¹æ•°æ®å—é™çš„åœºæ™¯ï¼Œç ”ç©¶æå‡ºäº†åŸºäºå±‚æ¬¡åŒ–å¤šç²’åº¦æç¤ºè®¾è®¡çš„å¾®è°ƒæŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨æœªæ ‡æ³¨èˆ¹èˆ¶ç±»åˆ«ä¸Šçš„åˆ†ç±»ç²¾åº¦ã€‚é€šè¿‡å¼•å…¥è¿œç¨‹æ„Ÿåº”èˆ¹èˆ¶å…ˆéªŒï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¾¨è¯†å¤æ‚èƒŒæ™¯åŠå­¦ä¹ èˆ¹èˆ¶ç‰¹å¾çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜è´¡çŒ®äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†FGSCM-52ï¼Œå¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œäº†æ‰©å……ï¼ŒåŒ…å«æ›´å¹¿æ³›çš„æ•°æ®å’Œè¯¦ç»†çš„æ³¨é‡Šã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰å…ˆè¿›æŠ€æœ¯ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>è¿œç¨‹æ„ŸçŸ¥ç²¾ç»†èˆ¹èˆ¶åˆ†ç±»é¢ä¸´é«˜ç±»åˆ«ç›¸ä¼¼æ€§å’Œæ ‡æ³¨æ•°æ®æœ‰é™çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ä¼ ç»Ÿç›‘ç£åˆ†ç±»æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç†è§£å›¾åƒå†…å®¹æ–¹é¢ã€‚</li><li>ç›´æ¥å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¿œç¨‹æ„ŸçŸ¥ç²¾ç»†èˆ¹èˆ¶åˆ†ç±»å¯èƒ½ä¼šé¢ä¸´è¿‡æ‹Ÿåˆå·²è§ç±»åˆ«çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ä¸ä½³ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºå±‚æ¬¡åŒ–å¤šç²’åº¦æç¤ºè®¾è®¡çš„æç¤ºå¾®è°ƒæŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥è¿œç¨‹æ„Ÿåº”èˆ¹èˆ¶å…ˆéªŒï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¾¨è¯†å¤æ‚èƒŒæ™¯åŠå­¦ä¹ èˆ¹èˆ¶ç‰¹å¾çš„èƒ½åŠ›ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†FGSCM-52ï¼Œå¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œäº†æ‰©å……ï¼ŒåŒ…å«æ›´å¹¿æ³›çš„æ•°æ®å’Œè¯¦ç»†çš„æ³¨é‡Šã€‚</li><li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœªè§èˆ¹èˆ¶ç±»åˆ«ä¸Šçš„åˆ†ç±»ç²¾åº¦ä¼˜äºå½“å‰å…ˆè¿›æŠ€æœ¯ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-26c768e0586a22813e2adc7d42a80eae.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d369a45a88991a559d65f564ff6ac511.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8d75df78e4f9149a31145f46ab6c5bb7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-feef1fc452d9919594ee10d0a70945c1.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-55031060c37f2f6edfcf6bf517e5a7b3.jpg" align="middle"></details><h2 id="Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning"><a href="#Learning-Prompt-with-Distribution-Based-Feature-Replay-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learning Prompt with Distribution-Based Feature Replay for Few-Shot   Class-Incremental Learning"></a>Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Wangmeng Zuo, Chunmei Feng</strong></p><p>Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a simple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can substantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually capture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new session, pseudo-features are sampled from old-class distributions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experiments on three prevalent benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF">https://github.com/1170300714/LP-DiF</a>.</p><blockquote><p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ—¨åœ¨åŸºäºéå¸¸æœ‰é™çš„è®­ç»ƒæ•°æ®æŒç»­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸å¿˜æ‰ä¹‹å‰é‡åˆ°çš„æ—§ç±»åˆ«ã€‚ç°æœ‰ç ”ç©¶ä»…ä¾èµ–çº¯ç²¹çš„è§†è§‰ç½‘ç»œï¼Œè€Œæœ¬æ–‡ä¸­æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰æ¥è§£å†³FSCILé—®é¢˜ï¼Œå¹¶æå‡ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç§°ä¸ºåŸºäºåˆ†å¸ƒç‰¹å¾å›æ”¾çš„å­¦ä¹ æç¤ºï¼ˆLP-DiFï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å°±å¯ä»¥å¤§å¤§è¶…è¶Šæœ€å…·å½±å“åŠ›çš„æ–¹æ³•ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶é€‚åº”èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸæŒç»­ä»æ¯ä¸ªä¼šè¯ä¸­æ•è·ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºäº†é˜²æ­¢å­¦ä¹ æç¤ºå¿˜è®°æ–°ä¼šè¯ä¸­çš„æ—§çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç»´æŒä¸€ä¸ªç”±è®­ç»ƒå›¾åƒç‰¹å¾å’Œå˜è‡ªåŠ¨ç¼–ç å™¨ç”Ÿæˆçš„åˆæˆç‰¹å¾ä¼°è®¡çš„åæ–¹å·®çŸ©é˜µä¸ºå¯¹è§’çš„é«˜æ–¯åˆ†å¸ƒæ¥ä¿ç•™æ¯ä¸ªç±»çš„æ—§çŸ¥è¯†ã€‚å½“è¿›å…¥æ–°ä¼šè¯æ—¶ï¼Œä¼ªç‰¹å¾æ˜¯ä»æ—§ç±»åˆ†å¸ƒä¸­é‡‡æ ·å¹¶ç»“åˆå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œä»è€Œèƒ½å¤Ÿä½¿æ¨¡å‹åœ¨å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚åœ¨CIFAR100ã€mini-ImageNetå’ŒCUB-200ä¸‰ä¸ªæµè¡Œçš„åŸºå‡†æµ‹è¯•ä»¥åŠæœ¬æ–‡æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„SUN-397å’ŒCUB-200$*$åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒå±•ç¤ºäº†LP-DiFçš„ä¼˜è¶Šæ€§ï¼Œåœ¨FSCILé¢†åŸŸå®ç°äº†æ–°çš„æœ€æ–°çŠ¶æ€ï¼ˆSOTAï¼‰ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF%E3%80%82">https://github.com/1170300714/LP-DiFã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.01598v3">PDF</a></p><p><strong>æ‘˜è¦</strong></p><p>æœ¬æ–‡æ—¨åœ¨è§£å†³å°æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é—®é¢˜ï¼Œé€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåä¸ºLearning Prompt with Distribution-based Feature Replayï¼ˆLP-DiFï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œä»…ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å·²ç»å¯ä»¥æ˜¾è‘—è¶…è¶Šæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥æç¤ºå¾®è°ƒæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè¿ç»­æ•æ‰æ¯ä¸ªä¼šè¯çš„ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºé˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­é—å¿˜æ—§çŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç»´æŒç‰¹å¾çº§çš„Gaussianåˆ†å¸ƒï¼ˆé€šè¿‡å¯¹è§’åæ–¹å·®çŸ©é˜µä¼°è®¡è®­ç»ƒå›¾åƒçš„ç‰¹å¾å’Œä»VAEç”Ÿæˆçš„åˆæˆç‰¹å¾ï¼‰æ¥ä¿ç•™æ¯ä¸ªç±»çš„æ—§çŸ¥è¯†ã€‚å½“è¿›å…¥æ–°ä¼šè¯æ—¶ï¼Œå°†ä¼ªç‰¹å¾ä»æ—§ç±»åˆ†å¸ƒä¸­é‡‡æ ·å¹¶ä¸å½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒç»“åˆï¼Œä»¥ä¼˜åŒ–æç¤ºï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚åœ¨å¤šä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå¦‚CIFAR100ã€mini-ImageNetå’ŒCUB-200ï¼‰ä»¥åŠæœ¬æ–‡æå‡ºçš„ä¸¤ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆSUN-397å’ŒCUB-200*ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLP-DiFå…·æœ‰ä¼˜è¶Šæ€§ï¼Œåœ¨FSCILé¢†åŸŸå–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æˆæœã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/1170300714/LP-DiF%E3%80%82">https://github.com/1170300714/LP-DiFã€‚</a></p><p><strong>è¦ç‚¹è§£æ</strong></p><ol><li>LP-DiFæ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è§£å†³äº†å°æ ·ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é—®é¢˜ã€‚</li><li>ä»…ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°å·²è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li><li>å¼•å…¥æç¤ºå¾®è°ƒæŠ€æœ¯è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ï¼Œä½¿å…¶èƒ½è¿ç»­æ•æ‰æ¯ä¸ªä¼šè¯çš„ç‰¹å®šçŸ¥è¯†ã€‚</li><li>æå‡ºä¼ªç‰¹å¾å›æ”¾æ–¹æ³•ï¼Œé˜²æ­¢å­¦ä¹ æç¤ºåœ¨æ–°ä¼šè¯ä¸­é—å¿˜æ—§çŸ¥è¯†ã€‚</li><li>é€šè¿‡ç»´æŠ¤ç‰¹å¾çº§çš„Gaussianåˆ†å¸ƒæ¥ä¿ç•™æ—§çŸ¥è¯†ã€‚</li><li>ç»“åˆä¼ªç‰¹å¾å’Œå½“å‰ä¼šè¯çš„è®­ç»ƒå›¾åƒæ¥ä¼˜åŒ–æç¤ºï¼Œå®ç°æ–°æ—§çŸ¥è¯†çš„å¹³è¡¡å­¦ä¹ ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b68ef099e7713b014eeb2b3e7b49e8c2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a76b9eff0fad3d63dcb46aee459a0673.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0a24a512d0bc42141ba455a1b4b860e8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f07311872d24975899a6d4fd3d1d5e60.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-74c2a8fa494ec56d4ee19c96dabbeb8f.jpg" align="middle"></details><h2 id="M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models"><a href="#M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models" class="headerlink" title="M$^{2}$UGen: Multi-modal Music Understanding and Generation with the   Power of Large Language Models"></a>M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models</h2><p><strong>Authors:Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan</strong></p><p>The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as text, speech, images, videos, etc. They also utilize LLMs to understand human intention and generate desired outputs like images, videos, and music. However, research that combines both understanding and generation using LLMs is still limited and in its nascent stage. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M$^{2}$UGen) framework that integrates LLMâ€™s abilities to comprehend and generate music for different modalities. The M$^{2}$UGen framework is purpose-built to unlock creative potential from diverse sources of inspiration, encompassing music, image, and video through the use of pretrained MERT, ViT, and ViViT models, respectively. To enable music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging multi-modal understanding and music generation is accomplished through the integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA model to generate extensive datasets that support text&#x2F;image&#x2F;video-to-music generation, facilitating the training of our M$^{2}$UGen framework. We conduct a thorough evaluation of our proposed framework. The experimental results demonstrate that our model achieves or surpasses the performance of the current state-of-the-art models.</p><blockquote><p>å½“å‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶é¢†åŸŸæ­£ç»å†ä¸€æ¬¡çƒ­æ½®ã€‚è®¸å¤šç ”ç©¶åˆ©ç”¨è¿™äº›æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›æ¥ç†è§£å„ç§æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒã€è§†é¢‘ç­‰ã€‚ä»–ä»¬è¿˜åˆ©ç”¨LLMæ¥ç†è§£äººç±»æ„å›¾å¹¶ç”Ÿæˆæƒ³è¦çš„è¾“å‡ºï¼Œå¦‚å›¾åƒã€è§†é¢‘å’ŒéŸ³ä¹ã€‚ç„¶è€Œï¼Œç»“åˆLLMçš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå¤„äºåˆçº§é˜¶æ®µã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆï¼ˆM^2^UGenï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†LLMç†è§£å’Œç”Ÿæˆä¸åŒæ¨¡æ€éŸ³ä¹çš„èƒ½åŠ›ã€‚M^2^UGenæ¡†æ¶ä¸“ä¸ºè§£é”æ¥è‡ªå„ç§çµæ„Ÿæ¥æºçš„åˆ›é€ åŠ›è€Œæ„å»ºï¼Œé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„MERTã€ViTå’ŒViViTæ¨¡å‹ï¼Œåˆ†åˆ«æ¶µç›–éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘ã€‚ä¸ºäº†å®ç°éŸ³ä¹ç”Ÿæˆï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨AudioLDM 2å’ŒMusicGenã€‚é€šè¿‡LLaMA 2æ¨¡å‹çš„é›†æˆï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£å’ŒéŸ³ä¹ç”Ÿæˆçš„æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨MU-LLaMAæ¨¡å‹ç”Ÿæˆäº†å¤§é‡æ”¯æŒæ–‡æœ¬&#x2F;å›¾åƒ&#x2F;è§†é¢‘åˆ°éŸ³ä¹ç”Ÿæˆçš„æ•°æ®é›†ï¼Œä¿ƒè¿›äº†æˆ‘ä»¬çš„M^2^UGenæ¡†æ¶çš„è®­ç»ƒã€‚æˆ‘ä»¬å¯¹æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11255v5">PDF</a></p><p><strong>Summary</strong></p><p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆé¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³ä¹é¢†åŸŸã€‚é’ˆå¯¹è¿™ä¸€è¶‹åŠ¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆï¼ˆM^2UGenï¼‰æ¡†æ¶ï¼Œèåˆäº†LLMsçš„éŸ³ä¹ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶å€ŸåŠ©é¢„è®­ç»ƒçš„MERTã€ViTå’ŒViViTæ¨¡å‹ï¼Œå®ç°äº†éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€èåˆã€‚æ­¤å¤–ï¼Œé€šè¿‡æ•´åˆLLaMA 2æ¨¡å‹å’ŒMU-LLaMAæ¨¡å‹ï¼Œæ¡†æ¶æ”¯æŒæ–‡æœ¬&#x2F;å›¾åƒ&#x2F;è§†é¢‘åˆ°éŸ³ä¹çš„ç”Ÿæˆï¼Œå¹¶åœ¨å®éªŒè¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³ä¹é¢†åŸŸçš„åº”ç”¨æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚</li><li>å½“å‰ç ”ç©¶åœ¨ç»“åˆç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä½¿ç”¨LLMsæ–¹é¢ä»å¤„äºåˆçº§é˜¶æ®µã€‚</li><li>M^2UGenæ¡†æ¶èåˆäº†LLMsçš„éŸ³ä¹ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæ—¨åœ¨ä»ä¸åŒæ¥æºçš„çµæ„Ÿä¸­è§£é”åˆ›æ„æ½œåŠ›ã€‚</li><li>è¯¥æ¡†æ¶å€ŸåŠ©é¢„è®­ç»ƒçš„MERTã€ViTå’ŒViViTæ¨¡å‹ï¼Œå®ç°äº†éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€èåˆã€‚</li><li>é€šè¿‡æ•´åˆLLaMA 2æ¨¡å‹å’ŒMU-LLaMAæ¨¡å‹ï¼ŒM^2UGenæ”¯æŒæ–‡æœ¬&#x2F;å›¾åƒ&#x2F;è§†é¢‘åˆ°éŸ³ä¹çš„ç”Ÿæˆã€‚</li><li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒM^2UGenæ¡†æ¶æ€§èƒ½å“è¶Šï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æœ€æ–°æ¨¡å‹ã€‚</li><li>è¯¥æ¡†æ¶çš„æå‡ºä¸ºæœªæ¥çš„å¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e2ec9dcf3634ad1383813fb8f9777507.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-146269967bb50b37095b2802028e8b5d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2428dd87014b95f1b4a9bf4d6426d243.jpg" align="middle"></details><h2 id="Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models"><a href="#Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Labels-from-Traditional-Models-to-Foundation-Models" class="headerlink" title="Weakly-Supervised Semantic Segmentation with Image-Level Labels: from   Traditional Models to Foundation Models"></a>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models</h2><p><strong>Authors:Zhaozheng Chen, Qianru Sun</strong></p><p>The rapid development of deep learning has driven significant progress in image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this journal, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area.</p><blockquote><p>æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚è¯­ä¹‰åˆ†å‰²ç®—æ³•é€šå¸¸ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼ˆå³å¯¹è±¡æ©è†œï¼‰çš„å¯ç”¨æ€§ï¼Œè€Œè¿™äº›æ ‡ç­¾çš„è·å–æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ã€‚å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯é¿å…è¿™ç§æ ‡æ³¨çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å®ƒä»…åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´çš„æ³¨é‡Šï¼Œä¸ºå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²æä¾›äº†ç»æµå®æƒ çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨æœ¬æœŸåˆŠä¸­ï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨å›¾åƒçº§æ ‡ç­¾çš„WSSSï¼Œè¿™æ˜¯WSSSä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚æˆ‘ä»¬çš„å·¥ä½œåˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œä¸»è¦å…³æ³¨åœ¨ä¸»è¦ç ”ç©¶ä¼šè®®ä¸Šæå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„æ–¹æ³•æ“ä½œä½ç½®å°†å®ƒä»¬åˆ†ä¸ºå››ç±»ï¼šåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¦‚åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰åœ¨WSSSä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæœ‰è¶£çš„åœºæ™¯ä¸­ä»”ç»†å®¡æŸ¥äº†SAMï¼šæ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ ã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å°†è§†è§‰åŸºç¡€æ¨¡å‹ç”¨äºWSSSçš„æ½œåŠ›å’ŒæŒ‘æˆ˜ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„ç ”ç©¶é¢†åŸŸçš„æœªæ¥å‘å±•ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13026v2">PDF</a> Accepted to ACM Computing Surveys</p><p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚è¯­ä¹‰åˆ†å‰²ç®—æ³•é€šå¸¸ä¾èµ–äºåƒç´ çº§æ ‡ç­¾ï¼ˆå³å¯¹è±¡æ©è†œï¼‰çš„å¯ç”¨æ€§ï¼Œè¿™äº›æ ‡ç­¾æ˜‚è´µã€è€—æ—¶ä¸”åŠ³åŠ¨å¯†é›†ã€‚å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯é¿å…è¿™ç§æ ‡æ³¨çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå®ƒä»…åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´çš„æ³¨é‡Šï¼Œä¸ºå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²æä¾›äº†æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾çš„WSSSï¼Œè¿™æ˜¯WSSSä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚æœ¬æ–‡é¦–å…ˆå¯¹ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œä¸»è¦å…³æ³¨åœ¨ä¸»è¦ç ”ç©¶ä¼šè®®ä¸Šå‘è¡¨çš„æ–¹æ³•ï¼Œå°†å…¶åˆ†ä¸ºå››ç±»ï¼šåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®ã€‚æ¥ç€æ¢è®¨äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Segment Anything Modelï¼ŒSAMï¼‰åœ¨WSSSä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡åœ¨ä¸¤ç§æœ‰è¶£çš„æƒ…å†µä¸‹å¯¹SAMè¿›è¡Œäº†å®¡æŸ¥ï¼šæ–‡æœ¬æç¤ºå’Œé›¶æ ·æœ¬å­¦ä¹ ã€‚æœ¬æ–‡æä¾›äº†å°†è§†è§‰åŸºç¡€æ¨¡å‹ç”¨äºWSSSçš„æ½œåŠ›å’ŒæŒ‘æˆ˜çš„è§è§£ï¼Œæœ‰åŠ©äºè¿™ä¸€ä»¤äººå…´å¥‹çš„ç ”ç©¶é¢†åŸŸæœªæ¥çš„å‘å±•ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æ·±åº¦å­¦ä¹ çš„è¿›æ­¥æ¨åŠ¨äº†å›¾åƒè¯­ä¹‰åˆ†å‰²çš„å‘å±•ã€‚</li><li>è¯­ä¹‰åˆ†å‰²ç®—æ³•ä¾èµ–åƒç´ çº§æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚</li><li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ˜¯ä¸€ç§åˆ©ç”¨éƒ¨åˆ†æˆ–ä¸å®Œæ•´æ³¨é‡Šçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li><li>ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾çš„WSSSæ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„å½¢å¼ã€‚</li><li>ä¼ ç»Ÿæ–¹æ³•è¢«åˆ†ç±»ä¸ºåƒç´ çº§ã€å›¾åƒçº§ã€è·¨å›¾åƒå’Œå¤–éƒ¨æ•°æ®å››ç±»ã€‚</li><li>Segment Anything Modelï¼ˆSAMï¼‰ç­‰è§†è§‰åŸºç¡€æ¨¡å‹åœ¨WSSSä¸­å…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5de8bbf35b6e19bb15d97b70d1365142.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6f366106e2aa1a029e0793160273fb8f.jpg" align="middle"></details><h2 id="NoisyNN-Exploring-the-Impact-of-Information-Entropy-Change-in-Learning-Systems"><a href="#NoisyNN-Exploring-the-Impact-of-Information-Entropy-Change-in-Learning-Systems" class="headerlink" title="NoisyNN: Exploring the Impact of Information Entropy Change in Learning   Systems"></a>NoisyNN: Exploring the Impact of Information Entropy Change in Learning Systems</h2><p><strong>Authors:Xiaowei Yu, Zhe Huang, Minheng Chen, Yao Xue, Tianming Liu, Dajiang Zhu</strong></p><p>We investigate the impact of entropy change in deep learning systems by noise injection at different levels, including the embedding space and the image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep models under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the task complexity. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of 95$%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change.</p><blockquote><p>æˆ‘ä»¬é€šè¿‡åœ¨æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„ä¸åŒå±‚æ¬¡ï¼ˆåŒ…æ‹¬åµŒå…¥ç©ºé—´å’Œå›¾åƒï¼‰æ³¨å…¥å™ªå£°æ¥ç ”ç©¶å…¶å¯¹æ·±åº¦å­¦ä¹ ç³»ç»Ÿç†µå˜åŒ–çš„å½±å“ã€‚é‡‡ç”¨æˆ‘ä»¬æ–¹æ³•çš„ç³»åˆ—æ¨¡å‹ç»Ÿç§°ä¸ºå™ªå£°ç¥ç»ç½‘ç»œï¼ˆNoisy Neural Networksï¼ŒNoisyNNï¼‰ï¼Œä¾‹å¦‚NoisyViTå’ŒNoisyCNNç­‰ã€‚å™ªå£°åœ¨ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå¯¹æ·±åº¦å­¦ä¹ æ¶æ„æœ‰å®³çš„æ‰°åŠ¨ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œä»¥åŠä¸åŒçš„å­¦ä¹ ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œè¿ç§»å­¦ä¹ ç­‰ã€‚ç„¶è€Œï¼Œè¿™é¡¹å·¥ä½œè¡¨æ˜å™ªå£°å¯ä»¥æ˜¯æ”¹å˜å­¦ä¹ ç³»ç»Ÿç†µçš„æœ‰æ•ˆé€”å¾„ã€‚æˆ‘ä»¬è¯æ˜åœ¨æŸäº›æ¡ä»¶ä¸‹ï¼Œç‰¹å®šå™ªå£°å¯ä»¥æé«˜å„ç§æ·±åº¦æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†æ­£é¢å™ªå£°é€šè¿‡å‡å°‘ç”±ä¿¡æ¯ç†µå®šä¹‰çš„ä»»åŠ¡å¤æ‚æ€§æ‰€å¸¦æ¥çš„å¢å¼ºæ•ˆæœï¼Œå¹¶é€šè¿‡å®éªŒæ˜¾ç¤ºäº†åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¿¡æ¯ç†µæ¥å®šä¹‰ä»»åŠ¡çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å°†å™ªå£°åˆ†ä¸ºæ­£é¢å™ªå£°ï¼ˆPNï¼‰å’Œæœ‰å®³å™ªå£°ï¼ˆHNï¼‰ä¸¤ç§ç±»å‹ï¼Œå–å†³äºå™ªå£°æ˜¯å¦æœ‰åŠ©äºé™ä½ä»»åŠ¡å¤æ‚æ€§ã€‚å¯¹CNNå’ŒViTçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ä¸»åŠ¨æ³¨å…¥æ­£é¢å™ªå£°å¯ä»¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ImageNetä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„95%çš„Top-1å‡†ç¡®ç‡ã€‚ç†è®ºåˆ†æå’Œå®è¯è¯æ®éƒ½è¯å®ï¼Œæ­£é¢å™ªå£°çš„å­˜åœ¨æœ‰ç›Šäºå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œä¼ ç»Ÿä¸Šè®¤ä¸ºçš„æœ‰å®³å™ªå£°ç¡®å®ä¼šæŸå®³æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å™ªå£°çš„ä¸åŒä½œç”¨ä¸ºç‰¹å®šä»»åŠ¡çš„æ·±åº¦æ¨¡å‹æä¾›äº†æ–°çš„è§£é‡Šï¼Œå¹¶ä¸ºæé«˜æ¨¡å‹æ€§èƒ½æä¾›äº†æ–°çš„èŒƒå¼ã€‚æ­¤å¤–ï¼Œå®ƒæé†’æˆ‘ä»¬ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜ä¿¡æ¯ç†µæ¥å½±å“å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10625v4">PDF</a> Task Entropy, NoisyViT, NoisyCNN</p><p><strong>Summary</strong></p><p>è¯¥æ–‡æœ¬ç ”ç©¶äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­ç†µå˜åŒ–çš„å½±å“ï¼Œé€šè¿‡åœ¨åµŒå…¥ç©ºé—´å’Œå›¾åƒç­‰ä¸åŒå±‚é¢æ³¨å…¥å™ªå£°æ¥å®ç°ã€‚æ–‡ä¸­ä»‹ç»äº†è¢«ç§°ä¸ºâ€œå™ªå£°ç¥ç»ç½‘ç»œâ€ï¼ˆNoisy Neural Networksï¼ŒNoisyNNï¼‰çš„ä¸€ç³»åˆ—æ¨¡å‹ï¼Œå¦‚NoisyViTå’ŒNoisyCNNã€‚å°½ç®¡å™ªå£°åœ¨ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå¯¹æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚CNNå’ŒViTï¼‰ä»¥åŠå›¾åƒåˆ†ç±»å’Œè¿ç§»å­¦ä¹ ç­‰å­¦ä¹ ä»»åŠ¡æœ‰å®³çš„æ‰°åŠ¨ï¼Œä½†æœ¬æ–‡æ˜¾ç¤ºå™ªå£°å¯ä»¥æœ‰æ•ˆåœ°æ”¹å˜å­¦ä¹ ç³»ç»Ÿçš„ç†µã€‚ç‰¹å®šå™ªå£°åœ¨æŸäº›æ¡ä»¶ä¸‹å¯ä»¥ä¿ƒè¿›å„ç§æ·±åº¦æ¨¡å‹çš„æ€§èƒ½ã€‚æ–‡ç« ä½¿ç”¨ä¿¡æ¯ç†µæ¥å®šä¹‰ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå°†å™ªå£°åˆ†ä¸ºæœ‰åŠ©äºå‡å°‘ä»»åŠ¡å¤æ‚æ€§çš„æ­£é¢å™ªå£°ï¼ˆPNï¼‰å’Œæœ‰å®³å™ªå£°ï¼ˆHNï¼‰ã€‚é€šè¿‡ä¸»åŠ¨æ³¨å…¥æ­£é¢å™ªå£°ï¼ŒCNNå’ŒViTçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºäº†æ€§èƒ½æ”¹è¿›ï¼Œåœ¨ImageNetä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„95ï¼…çš„é¡¶çº§å‡†ç¡®ç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æ–‡ä¸­æ¢è®¨äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­ç†µå˜åŒ–çš„å½±å“ï¼Œé€šè¿‡åœ¨ä¸åŒå±‚é¢ï¼ˆå¦‚åµŒå…¥ç©ºé—´å’Œå›¾åƒï¼‰æ³¨å…¥å™ªå£°è¿›è¡Œç ”ç©¶ã€‚</li><li>ä»‹ç»äº†â€œå™ªå£°ç¥ç»ç½‘ç»œâ€ï¼ˆNoisyNNï¼‰ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬NoisyViTå’ŒNoisyCNNç­‰ã€‚</li><li>ä¸åŒäºä¼ ç»Ÿè§‚ç‚¹ï¼Œè¯¥ç ”ç©¶è¡¨æ˜å™ªå£°å¯ä»¥æœ‰æ•ˆåœ°æ”¹å˜å­¦ä¹ ç³»ç»Ÿçš„ç†µã€‚</li><li>ç‰¹å®šç±»å‹çš„å™ªå£°åœ¨ç‰¹å®šæ¡ä»¶ä¸‹èƒ½å¤Ÿä¿ƒè¿›æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>æ–‡ç« ä½¿ç”¨ä¿¡æ¯ç†µæ¥å®šä¹‰ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå°†å™ªå£°åˆ†ä¸ºæ­£é¢å™ªå£°å’Œæœ‰å®³å™ªå£°ä¸¤ç±»ã€‚</li><li>é€šè¿‡ä¸»åŠ¨æ³¨å…¥æ­£é¢å™ªå£°ï¼Œåœ¨ImageNetç­‰å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c9d81b126fb99dbe4d9bbb0b950177d7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-45374987c49986a53eeed8988fe0bda8.jpg" align="middle"></details><h2 id="Retrieval-Enhanced-Visual-Prompt-Learning-for-Few-shot-Classification"><a href="#Retrieval-Enhanced-Visual-Prompt-Learning-for-Few-shot-Classification" class="headerlink" title="Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"></a>Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</h2><p><strong>Authors:Jintao Rong, Hao Chen, Linlin Ou, Tianxiao Chen, Xinyi Yu, Yifan Liu</strong></p><p>The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. Reprompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.</p><blockquote><p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼å·²è¢«å¹¿æ³›é‡‡ç”¨ä»¥å¢å¼ºå…¶å¯¹è¿™äº›ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºåŸŸå·®è·çš„æ‰©å¤§ï¼Œå½“å‰èŒƒå¼å¯èƒ½åœ¨ç»†ç²’åº¦åˆ†ç±»ï¼ˆå¦‚å«æ˜Ÿå›¾åƒè¯†åˆ«ï¼‰æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ£€ç´¢å¢å¼ºè§†è§‰æç¤ºå­¦ä¹ ï¼ˆRePromptï¼‰ï¼Œå®ƒå¼•å…¥äº†æ£€ç´¢æœºåˆ¶æ¥ç¼“å­˜å’Œé‡å¤ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„çŸ¥è¯†ã€‚RePromptä»è®­ç»ƒæ ·æœ¬æˆ–å¯ç”¨çš„å¤–éƒ¨æ•°æ®ä¸­æ„å»ºæ£€ç´¢æ•°æ®åº“ï¼Œå¹¶ä½¿ç”¨æ£€ç´¢æœºåˆ¶å¢å¼ºç®€å•æç¤ºå­¦ä¹ åŸºå‡†çš„å¤šä¸ªé˜¶æ®µï¼Œä»è€Œç¼©å°åŸŸå·®è·ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¢å¼ºçš„æ¨¡å‹å¯ä»¥å¼•ç”¨æ£€ç´¢å¸¦æ¥çš„ç›¸ä¼¼æ ·æœ¬ä»¥åšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼Œæ£€ç´¢æœ‰åŠ©äºæ”¹è¿›åæœŸç‰¹å¾çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚RePromptåœ¨å¹¿æ³›çš„è§†è§‰æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬11ä¸ªå›¾åƒæ•°æ®é›†ã€3ä¸ªè§†é¢‘æ•°æ®é›†ã€1ä¸ªå¤šè§†å›¾æ•°æ®é›†å’Œ4ä¸ªåŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.02243v3">PDF</a></p><p><strong>Summary</strong></p><p>åŸºäºCLIPæ¨¡å‹çš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒå·²å¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ã€‚ä¸ºæé«˜æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œç ”ç©¶è€…å¤šé‡‡ç”¨å°æ ·æœ¬å­¦ä¹ èŒƒå¼ã€‚ç„¶è€Œï¼Œå¯¹äºå«æ˜Ÿå›¾åƒè¯†åˆ«ç­‰ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ï¼Œç°æœ‰èŒƒå¼å¯èƒ½ä¼šå› é¢†åŸŸå·®è·æ‰©å¤§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå¢å¼ºæ£€ç´¢çš„è§†è§‰æç¤ºå­¦ä¹ ï¼ˆRePromptï¼‰æ–¹æ³•ï¼Œå¼•å…¥æ£€ç´¢æœºåˆ¶ä»¥ç¼“å­˜å’Œé‡ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„çŸ¥è¯†ã€‚RePromptæ„å»ºäº†ä¸€ä¸ªæ£€ç´¢æ•°æ®åº“ï¼Œè¯¥æ•°æ®åº“æ¥æºäºè®­ç»ƒæ ·æœ¬æˆ–å¯ç”¨çš„å¤–éƒ¨æ•°æ®ï¼Œå¹¶ä½¿ç”¨æ£€ç´¢æœºåˆ¶å¢å¼ºç®€å•æç¤ºå­¦ä¹ åŸºçº¿æ¨¡å‹çš„å¤šä¸ªé˜¶æ®µï¼Œä»è€Œç¼©å°é¢†åŸŸå·®è·ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„å¢å¼ºæ¨¡å‹å¯ä»¥å¼•ç”¨æ£€ç´¢å¸¦æ¥çš„ç›¸ä¼¼æ ·æœ¬ï¼Œåšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼Œæ£€ç´¢æœ‰åŠ©äºæ”¹è¿›æ™šæœŸç‰¹å¾çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚RePromptåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬11ä¸ªå›¾åƒæ•°æ®é›†ã€3ä¸ªè§†é¢‘æ•°æ®é›†ã€1ä¸ªå¤šè§†å›¾æ•°æ®é›†å’Œ4ä¸ªé¢†åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>CLIPæ¨¡å‹å¹¿æ³›åº”ç”¨äºä¸‹æ¸¸è§†è§‰ä»»åŠ¡ï¼Œä½†é¢ä¸´ç»†ç²’åº¦åˆ†ç±»æŒ‘æˆ˜ã€‚</li><li>ç°æœ‰èŒƒå¼åœ¨é¢†åŸŸå·®è·è¾ƒå¤§çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚</li><li>RePrompté€šè¿‡å¼•å…¥æ£€ç´¢æœºåˆ¶æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œæ„å»ºæ£€ç´¢æ•°æ®åº“ä»¥ç¼“å­˜å’Œé‡ç”¨çŸ¥è¯†ã€‚</li><li>æ£€ç´¢æœºåˆ¶æœ‰åŠ©äºæ”¹è¿›æ™šæœŸç‰¹å¾çš„åˆ†å¸ƒï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>RePromptåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘ã€å¤šè§†å›¾å’Œé¢†åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ã€‚</li><li>RePromptæ¨¡å‹é€šè¿‡å¼•ç”¨ç›¸ä¼¼æ ·æœ¬åšå‡ºæ›´å‡†ç¡®é¢„æµ‹ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-025d9590e06a0b2170c821bf8e254402.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-50e6c61180a9242d3442e2b49b197a9e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4251402a4494d924dc0ed0d4211dffd7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-300e5ceb1c743c415d6bd844f9567a29.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">æ–‡ç« ä½œè€…:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">æ–‡ç« é“¾æ¥:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Vision%20Transformer/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">ç‰ˆæƒå£°æ˜:</i></span> <span class="reprint-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/Vision-Transformer/"><span class="chip bg-color">Vision Transformer</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;ä¸Šä¸€ç¯‡</div><div class="card"><a href="/Talk2Paper/Paper/2024-12-12/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-8abcd76d4814ccdac70d5f15e6e7b2ea.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³"> <span class="card-title">äººè„¸ç›¸å…³</span></div></a><div class="card-content article-content"><div class="summary block-with-text">äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12 Local Features Meet Stochastic Anonymization Revolutionizing Privacy-Preserving Face Recognition for Black-Box Models</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-12</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">äººè„¸ç›¸å…³</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/"><span class="chip bg-color">äººè„¸ç›¸å…³</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-12-12/LLM/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-2e1596481b78f5192b48d8e35b392431.jpg" class="responsive-img" alt="LLM"> <span class="card-title">LLM</span></div></a><div class="card-content article-content"><div class="summary block-with-text">LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12 Generative Semantic Communication Architectures, Technologies, and Applications</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-12</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/LLM/" class="post-category">LLM</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span class="white-color">5676k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="æœ¬ç«™å·²è¿è¡Œ "+c+" å¤©",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="æœ¬ç«™å·²è¿è¡Œ "+l+" å¹´ "+c+" å¤©",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">çŸ¥</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;æœç´¢</span> <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ",clearTimeout(st)):(document.title="Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>