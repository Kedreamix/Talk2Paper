<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2024-12-12  AdvWave Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Speech/2410.16059v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    23.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    94 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-12-更新"><a href="#2024-12-12-更新" class="headerlink" title="2024-12-12 更新"></a>2024-12-12 更新</h1><h2 id="AdvWave-Stealthy-Adversarial-Jailbreak-Attack-against-Large-Audio-Language-Models"><a href="#AdvWave-Stealthy-Adversarial-Jailbreak-Attack-against-Large-Audio-Language-Models" class="headerlink" title="AdvWave: Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models"></a>AdvWave: Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models</h2><p><strong>Authors:Mintong Kang, Chejian Xu, Bo Li</strong></p>
<p>Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate AI regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate. </p>
<blockquote>
<p>最近，大型音频语言模型（LALM）的进展为用户提供了基于语音的交互方式，这极大地提升了用户体验，并加速了LALM在真实世界应用中的部署。然而，确保LALM的安全至关重要，以防止产生可能引发社会担忧或违反AI法规的风险输出。尽管这个问题非常重要，但由于LALM的兴起时间相对较短，以及与基于DNN的音频模型相比存在的额外技术挑战，关于破解LALM的研究仍然有限。具体来说，LALM中的音频编码器涉及离散操作，往往会导致梯度碎裂，阻碍依赖于梯度优化的攻击的有效性。LALM的行为可变性进一步增加了有效（对抗性）优化目标的识别难度。此外，对对抗性音频波形强制执行隐蔽性约束引入了较小的非凸可行解空间，进一步加剧了优化过程的挑战。为了克服这些挑战，我们开发了第一款针对LALM的破解框架AdvWave。我们提出了一种双阶段优化方法来解决梯度碎裂问题，从而实现有效的端到端梯度优化。此外，我们还开发了一种自适应对抗目标搜索算法，该算法根据LALM对特定查询的响应模式动态调整对抗优化目标。为了确保对抗性音频对人类听众来说在感知上保持自然，我们设计了一种分类器引导的优化方法，该方法生成的对抗性噪声类似于常见的城市声音。在多个先进LALM上的广泛评估表明，AdvWave优于基准方法，实现了高达40%的平均破解成功率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08608v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着大型音频语言模型（LALM）的近期发展，语音交互用户体验大幅提升，加速LALM在真实世界应用中的部署。然而，确保LALM的安全至关重要，以防止可能引发社会担忧或违反AI法规的风险输出。尽管安全问题至关重要，但由于LALM的近期兴起以及与其相关的技术挑战，关于破解LALM的研究仍然有限。本文提出AdvWave，首个针对LALM的破解框架。通过双阶段优化方法解决梯度破碎问题，实现有效的端到端梯度优化。此外，我们开发了一种自适应对抗目标搜索算法，根据特定查询的响应模式动态调整对抗优化目标。为确保对抗性音频对人类听众而言感知自然，我们设计了一种分类器引导的优化方法，生成类似于常见城市声音的对抗性噪声。在多个先进LALM上的广泛评估表明，AdvWave优于基线方法，实现40%更高的平均破解成功率。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型音频语言模型（LALM）的语音交互提升了用户体验并推动了其在真实世界应用的部署。</li>
<li>LALM的安全性问题至关重要，需要防止可能引发社会担忧或违反AI法规的风险输出。</li>
<li>目前针对LALM的研究破解仍然有限，主要由于它们的新兴和技术挑战的复杂性。</li>
<li>AdvWave是首个针对LALM的破解框架，通过双阶段优化方法解决梯度破碎问题。</li>
<li>自适应对抗目标搜索算法能根据LALM对特定查询的响应模式动态调整对抗优化目标。</li>
<li>分类器引导的优化方法确保生成的对抗性音频对人类听众而言感知自然。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ec334f447e36183801430adf2a52ff1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5c71d1c51dc501da740ab9e363d042ab.jpg" align="middle">
</details>




<h2 id="Bilevel-Joint-Unsupervised-and-Supervised-Training-for-Automatic-Speech-Recognition"><a href="#Bilevel-Joint-Unsupervised-and-Supervised-Training-for-Automatic-Speech-Recognition" class="headerlink" title="Bilevel Joint Unsupervised and Supervised Training for Automatic Speech   Recognition"></a>Bilevel Joint Unsupervised and Supervised Training for Automatic Speech   Recognition</h2><p><strong>Authors:Xiaodong Cui, A F M Saif, Songtao Lu, Lisha Chen, Tianyi Chen, Brian Kingsbury, George Saon</strong></p>
<p>In this paper, we propose a bilevel joint unsupervised and supervised training (BL-JUST) framework for automatic speech recognition. Compared to the conventional pre-training and fine-tuning strategy which is a disconnected two-stage process, BL-JUST tries to optimize an acoustic model such that it simultaneously minimizes both the unsupervised and supervised loss functions. Because BL-JUST seeks matched local optima of both loss functions, acoustic representations learned by the acoustic model strike a good balance between being generic and task-specific. We solve the BL-JUST problem using penalty-based bilevel gradient descent and evaluate the trained deep neural network acoustic models on various datasets with a variety of architectures and loss functions. We show that BL-JUST can outperform the widely-used pre-training and fine-tuning strategy and some other popular semi-supervised techniques. </p>
<blockquote>
<p>在这篇论文中，我们提出了一种名为BL-JUST（双层次联合无监督与监督训练）的自动语音识别框架。与传统的预训练和微调策略（这是一个分离的两阶段过程）相比，BL-JUST试图优化语音模型，使其能够同时最小化无监督和监督损失函数。由于BL-JUST寻求两种损失函数的匹配局部最优解，因此语音模型学到的声学表示在通用性和任务特异性之间达到了很好的平衡。我们使用基于罚分的双层次梯度下降来解决BL-JUST问题，并在各种数据集上评估了经过训练的深度神经网络声学模型，这些模型具有多种架构和损失函数。我们展示BL-JUST可以超越广泛使用的预训练和微调策略以及其他一些流行的半监督技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08548v1">PDF</a> Accepted by IEEE&#x2F;ACM Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为BL-JUST（双级联合无监督与监督训练）的自动语音识别框架。与传统的预训练与微调策略不同，BL-JUST旨在优化声学模型，使其同时最小化无监督与监督损失函数。通过寻找两种损失函数的匹配局部最优解，声学模型学到的声音表现既通用又特定于任务。使用基于惩罚的双级梯度下降法解决BL-JUST问题，并在各种数据集上评估训练后的深度神经网络声学模型。实验表明，BL-JUST在性能上超越了广泛使用的预训练与微调策略以及其他一些流行的半监督技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了BL-JUST框架，结合无监督与监督训练进行自动语音识别。</li>
<li>BL-JUST优化声学模型，同时最小化无监督与监督损失函数。</li>
<li>通过匹配两种损失函数的局部最优解，实现声学模型的通用性与任务特定性的平衡。</li>
<li>采用基于惩罚的双级梯度下降法解决BL-JUST问题。</li>
<li>在多种数据集和架构上评估了BL-JUST框架的性能。</li>
<li>BL-JUST在性能上超越了传统的预训练与微调策略。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13d0661540efff6aa6df7452a885cc8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56488b7c9ec2950e3911c6ef8b92bbe9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4e9c4413afdf631a1da701fc2457bf8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-be0444451118e4360c0695914064f710.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bb11e2f8b6e9595c3aba7f8de129ff45.jpg" align="middle">
</details>




<h2 id="GR-NLP-TOOLKIT-An-Open-Source-NLP-Toolkit-for-Modern-Greek"><a href="#GR-NLP-TOOLKIT-An-Open-Source-NLP-Toolkit-for-Modern-Greek" class="headerlink" title="GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek"></a>GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek</h2><p><strong>Authors:Lefteris Loukas, Nikolaos Smyrnioudis, Chrysa Dikonomaki, Spyros Barbakos, Anastasios Toumazatos, John Koutsikakis, Manolis Kyriakakis, Mary Georgiou, Stavros Vassos, John Pavlopoulos, Ion Androutsopoulos</strong></p>
<p>We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP) toolkit developed specifically for modern Greek. The toolkit provides state-of-the-art performance in five core NLP tasks, namely part-of-speech tagging, morphological tagging, dependency parsing, named entity recognition, and Greeklishto-Greek transliteration. The toolkit is based on pre-trained Transformers, it is freely available, and can be easily installed in Python (pip install gr-nlp-toolkit). It is also accessible through a demonstration platform on HuggingFace, along with a publicly available API for non-commercial use. We discuss the functionality provided for each task, the underlying methods, experiments against comparable open-source toolkits, and future possible enhancements. The toolkit is available at: <a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit">https://github.com/nlpaueb/gr-nlp-toolkit</a> </p>
<blockquote>
<p>我们推出GR-NLP-TOOLKIT，这是一个专门为现代希腊语开发的开源自然语言处理（NLP）工具包。该工具包在五个核心NLP任务中提供最先进的性能，即词性标注、形态标注、依存句法分析、命名实体识别和希腊语转写。该工具包基于预训练的Transformer，可免费提供，并且可以在Python中轻松安装（通过pip install gr-nlp-toolkit）。它还可以通过HuggingFace上的演示平台和面向非商业用途的公开API进行访问。我们讨论了针对每项任务提供的功能、基本方法、与类似开源工具包的实验对比以及可能的未来改进。该工具包可在以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit">https://github.com/nlpaueb/gr-nlp-toolkit</a> </p>
</blockquote>
<p>中文翻译如下：</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08520v1">PDF</a> Accepted Demo Paper @ COLING 2025 (Github:   <a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit/">https://github.com/nlpaueb/gr-nlp-toolkit/</a>, Demo:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo">https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo</a>, API:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API">https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API</a>)</p>
<p><strong>总结</strong><br>    我们介绍了GR-NLP-TOOLKIT，这是一个专为现代希腊语开发的开源自然语言处理工具包。该工具包在五个核心NLP任务中提供最新技术性能，包括词性标注、形态标注、依存解析、命名实体识别和希腊语翻译。它基于预训练的Transformer，免费且易于在Python中安装（通过pip install gr-nlp-toolkit）。它还通过HuggingFace上的演示平台和公开可用的API供非商业使用。我们讨论了每个任务提供的功能、底层方法以及与类似开源工具包的实验对比，还有未来可能的改进。该工具包可在<a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/nlpaueb/gr-nlp-toolkit上获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>GR-NLP-TOOLKIT是一个专为现代希腊语开发的开源自然语言处理工具包。</li>
<li>它提供了五个核心NLP任务的最先进技术性能，包括词性标注、形态标注等。</li>
<li>该工具包基于预训练的Transformer，易于在Python中安装和使用。</li>
<li>提供了通过HuggingFace的演示平台和公开API的非商业使用访问。</li>
<li>工具包具有强大的功能和广泛的适用性，可应用于多种NLP任务。</li>
<li>与其他类似工具相比，该工具包在性能和功能上表现出优越性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1671657de697805d937ea34c10bbf126.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9233c8f9e9c07907e734375b7ba10c51.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3470d927cb9de26810790614e5921b9f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b929042d8e02d5a1848f9865d52d1ff2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef0c6ec8c7430573bcf5384172ef2b74.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-914377345c34db0469799d222b297c72.jpg" align="middle">
</details>




<h2 id="Evaluating-the-Impact-of-Discriminative-and-Generative-E2E-Speech-Enhancement-Models-on-Syllable-Stress-Preservation"><a href="#Evaluating-the-Impact-of-Discriminative-and-Generative-E2E-Speech-Enhancement-Models-on-Syllable-Stress-Preservation" class="headerlink" title="Evaluating the Impact of Discriminative and Generative E2E Speech   Enhancement Models on Syllable Stress Preservation"></a>Evaluating the Impact of Discriminative and Generative E2E Speech   Enhancement Models on Syllable Stress Preservation</h2><p><strong>Authors:Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Sai Harshitha Aluru, Chiranjeevi Yarra</strong></p>
<p>Automatic syllable stress detection is a crucial component in Computer-Assisted Language Learning (CALL) systems for language learners. Current stress detection models are typically trained on clean speech, which may not be robust in real-world scenarios where background noise is prevalent. To address this, speech enhancement (SE) models, designed to enhance speech by removing noise, might be employed, but their impact on preserving syllable stress patterns is not well studied. This study examines how different SE models, representing discriminative and generative modeling approaches, affect syllable stress detection under noisy conditions. We assess these models by applying them to speech data with varying signal-to-noise ratios (SNRs) from 0 to 20 dB, and evaluating their effectiveness in maintaining stress patterns. Additionally, we explore different feature sets to determine which ones are most effective for capturing stress patterns amidst noise. To further understand the impact of SE models, a human-based perceptual study is conducted to compare the perceived stress patterns in SE-enhanced speech with those in clean speech, providing insights into how well these models preserve syllable stress as perceived by listeners. Experiments are performed on English speech data from non-native speakers of German and Italian. And the results reveal that the stress detection performance is robust with the generative SE models when heuristic features are used. Also, the observations from the perceptual study are consistent with the stress detection outcomes under all SE models. </p>
<blockquote>
<p>自动音节重音检测是计算机辅助语言学习（CALL）系统对学习者的一个重要组成部分。当前的应力检测模型通常是在干净语音上进行训练的，这在背景噪声普遍存在的现实场景中可能不够稳健。为解决这一问题，可采用语音增强（SE）模型，旨在通过消除噪声来提高语音质量，但它们对保持音节重音模式的影响尚未得到充分研究。本研究探讨了代表区分性和生成性建模方法的不同SE模型在噪声条件下对音节重音检测的影响。我们通过对信噪比（SNR）从0到20分贝的语音数据进行应用评估这些模型，并评估它们在保持重音模式方面的有效性。此外，我们还探索了不同的特征集，以确定哪些特征在噪声中捕捉重音模式最有效。为了进一步研究SE模型的影响，我们进行了一项基于人类感知的研究，比较了SE增强语音和干净语音中的感知重音模式，从而了解这些模型如何很好地保留听众感知到的音节重音。实验是在非英语母语者（德语和意大利语）的英语语音数据上进行的。结果表明，当使用启发式特征时，生成性SE模型的应力检测性能是稳健的。此外，感知研究中的观察结果与所有SE模型下的应力检测结果相一致。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08306v1">PDF</a> </p>
<p><strong>Summary</strong><br>在背景噪声普遍存在的情况下，针对计算机辅助语言学习系统，本论文探讨了不同语音增强模型对音系音强检测的影响。通过应用不同的语音增强模型至不同信噪比（SNR）的语音数据上，研究评估了这些模型在维持音强模式方面的效能。同时结合人类感知研究，探讨这些语音增强模型在保持音节音强方面的表现，研究结果表明生成式语音增强模型结合启发式特征使用对音强检测的稳健性较好。这一发现有助于语言学习系统的噪音环境下表现提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动音节重音检测在计算机辅助语言学习系统中至关重要。</li>
<li>当前重音检测模型主要在干净语音上训练，但在现实环境中存在背景噪声的情况下可能不够稳健。</li>
<li>研究评估了不同语音增强模型对音系音强检测的影响。</li>
<li>实验表明生成式语音增强模型结合启发式特征使用时，音强检测的稳健性较好。</li>
<li>研究结合了人类感知研究，探讨了语音增强模型在保持音节音强方面的表现。</li>
<li>研究发现，不同信噪比下的语音数据实验结果揭示了语音增强模型在不同噪声环境下的效能差异。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9afdca59561274ab12b5678a3cd8d8d6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eda9920ae92bf8647bb6d14579682541.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f68108814ceffc787171ff5052537240.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>基于LLM的系统众所周知是数据密集型的。最近的基于LLM的TTS工作通常采用复杂的数据处理管道来获得高质量的训练数据。这些复杂管道需要在每个阶段都有优秀的模型（例如，语音降噪、语音增强、说话人分帧和标点模型），而这些模型本身需要高质量的训练数据，并且很少开源。即使使用最先进的模型，仍然存在一些问题，例如背景噪声去除不完全以及标点符号与实际语音停顿之间的不匹配。此外，严格的过滤策略通常只保留原始数据的10-30%，极大地阻碍了数据扩展工作。在这项工作中，我们利用噪声鲁棒的音频标记器（S3Tokenizer）设计了一个简化而有效的TTS数据处理管道，该管道能够在保持数据质量的同时，显著降低数据获取成本，实现超过50%的数据保留率。除了数据扩展挑战外，基于LLM的TTS系统与传统方法相比还带来了更高的部署成本。当前系统通常仅使用LLM进行文本到标记的生成，而需要使用单独的模型（如流匹配模型）进行标记到波形的生成，这些模型不能由LLM推理引擎直接执行，进一步加剧了部署的复杂性。为了应对这些挑战，我们消除了LLM和流组件中的冗余模块，并用LLM架构替代了流模型的骨干。基于这种简化的流骨干，我们提出了一个适用于流式和非流式推理的统一架构，显著降低了部署成本。最后，由于简化的管道和S3Tokenizer降低了对TTS训练数据的质量要求，我们探索了使用同一数据进行TTS和ASR任务训练的可行性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>基于LLM的TTS系统采用简化的数据加工流程，借助噪声鲁棒的音频分词器（S3Tokenizer）设计简化而有效的数据加工流程，同时保持数据质量并减少数据获取成本，数据保留率超过一半以上。简化流程消除冗余模块，用LLM架构替代流量模型主干，降低部署成本，并提出统一架构用于流式和非流式推理。此外，简化管道和S3分词器使得统一TTS和ASR任务训练数据成为可能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTS系统对数据需求极高，需要复杂的数据加工流程和高质量的训练数据。</li>
<li>复杂的加工流程导致数据损失严重，仅保留10-30%的原始数据。</li>
<li>采用噪声鲁棒的音频分词器（S3Tokenizer）可以简化数据加工流程并保留更多数据。</li>
<li>通过简化LLM和流量组件，消除冗余模块，降低部署成本。</li>
<li>提出统一架构用于流式和非流式推理，进一步降低部署成本。</li>
<li>简化后的数据和流程使得统一TTS和ASR任务训练数据成为可能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>基于扩散的生成人工智能因其对生成对抗网络和变分自动编码器等其他生成技术的卓越性能而受到广泛关注。虽然它在计算机视觉和自然语言处理等领域取得了显著的进步，但在语音生成方面的应用仍然被探索得不够深入。主流的文本到语音系统主要将输出映射到梅尔频谱图的频谱空间中，由于梅尔频谱的稀疏性导致了较高的计算负载。为了解决这些局限性，我们提出了LatentSpeech，这是一种利用潜在扩散模型的新型文本到语音生成方法。LatentSpeech使用潜在嵌入作为中间表示，将目标维度降低到梅尔频谱所需维度的5%，从而简化了文本到语音编码器及vocoder的处理过程，并实现了高效高质量的语音生成。这项研究标志着潜在扩散模型在文本到语音技术中的首次集成，提高了生成语音的准确性和自然度。在基准数据集上的实验结果表明，与现有模型相比，LatentSpeech在单词错误率方面提高了25%，梅尔倒谱失真提高了24%，随着训练数据的增加，这两项指标分别进一步提高到49.5%和26%。这些发现凸显了LatentSpeech在推动文本到语音技术的最新发展方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>摘要</strong><br>    基于扩散的生成式AI技术在计算机视觉和自然语言处理等领域取得了显著进展，但在语音生成方面的应用仍被忽视。主流文本转语音系统主要映射输出到梅尔频谱图，导致高计算负载。本研究提出LatentSpeech，一种利用潜在扩散模型的新型文本转语音生成方法，采用潜在嵌入作为中间表示形式，减少了目标维度，简化了TTS编码器和音频编解码器的处理过程，实现了高效高质量的语音生成。实验结果表明，LatentSpeech在单词错误率和梅尔倒谱失真方面较现有模型分别提高了25%和24%，随着训练数据的增加，这些改进分别上升到49.5%和26%。这突显了LatentSpeech在TTS技术方面的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>基于扩散的生成式AI技术受到关注，并在计算机视觉和自然语言处理领域表现出卓越性能。</li>
<li>当前主流文本转语音系统面临高计算负载问题。</li>
<li>LatentSpeech是一种新型的文本转语音生成方法，采用潜在扩散模型，旨在解决上述问题。</li>
<li>LatentSpeech通过降低目标维度来简化TTS编码器和音频编解码器的处理过程。</li>
<li>LatentSpeech实现了高效高质量的语音生成。</li>
<li>实验结果表明，LatentSpeech在单词错误率和梅尔倒谱失真方面较现有模型有显著改进。</li>
<li>随着训练数据的增加，LatentSpeech的改进潜力进一步凸显。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Style-agnostic-evaluation-of-ASR-using-multiple-reference-transcripts"><a href="#Style-agnostic-evaluation-of-ASR-using-multiple-reference-transcripts" class="headerlink" title="Style-agnostic evaluation of ASR using multiple reference transcripts"></a>Style-agnostic evaluation of ASR using multiple reference transcripts</h2><p><strong>Authors:Quinten McNamara, Miguel Ángel del Río Fernández, Nishchal Bhandari, Martin Ratajczak, Danny Chen, Corey Miller, Migüel Jetté</strong></p>
<p>Word error rate (WER) as a metric has a variety of limitations that have plagued the field of speech recognition. Evaluation datasets suffer from varying style, formality, and inherent ambiguity of the transcription task. In this work, we attempt to mitigate some of these differences by performing style-agnostic evaluation of ASR systems using multiple references transcribed under opposing style parameters. As a result, we find that existing WER reports are likely significantly over-estimating the number of contentful errors made by state-of-the-art ASR systems. In addition, we have found our multireference method to be a useful mechanism for comparing the quality of ASR models that differ in the stylistic makeup of their training data and target task. </p>
<blockquote>
<p>语音识别领域的词错误率（WER）作为一个评价指标存在着多种局限，困扰着该领域的发展。评估数据集因风格各异、形式不同以及转录任务固有的模糊性而受到困扰。在这项工作中，我们尝试通过采用风格无关的评估方法（使用多种在不同风格参数下转录的参考标准）来减少这些差异。我们发现，现有的WER报告可能大大高估了前沿语音识别系统产生的实质性错误数量。此外，我们发现我们的多参考方法对于比较不同风格的训练数据和目标任务的语音识别模型的质量非常有用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文指出词错误率（WER）作为语音识别领域的一个评价指标存在多种局限性。评估数据集在风格、正式程度以及转录任务本身的模糊性上存在差异。为缓解这些问题，本文提出了一种采用多种参照标准的风格无关评估方法对自动语音识别（ASR）系统进行评估。研究发现，现有的WER报告可能高估了前沿ASR系统的实质性错误数量。同时，多参考方法对于比较不同风格训练数据和目标任务的ASR模型质量具有实用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>词错误率（WER）作为语音识别评价指标存在局限性。</li>
<li>评估数据集在风格、正式程度和任务模糊性上存在差异。</li>
<li>提出了一种采用多种参照标准的风格无关评估方法来缓解上述问题。</li>
<li>现有WER报告可能高估了前沿ASR系统的实质性错误数量。</li>
<li>多参考方法有助于比较不同风格训练数据和目标任务的ASR模型质量。</li>
<li>该方法能够提供更准确的评估，使研究人员更好地了解ASR系统的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8100aa3369042097b569553fdb91a950.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f2f8050d75582967cd02358fd1e52a26.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4d263a62158e7c69b13333453efe1157.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ebf1585cb83308d7dade17759a71ae0a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3f63a64a21ad0d44c6c915ed7876b904.jpg" align="middle">
</details>




<h2 id="Real-time-Sign-Language-Recognition-Using-MobileNetV2-and-Transfer-Learning"><a href="#Real-time-Sign-Language-Recognition-Using-MobileNetV2-and-Transfer-Learning" class="headerlink" title="Real-time Sign Language Recognition Using MobileNetV2 and Transfer   Learning"></a>Real-time Sign Language Recognition Using MobileNetV2 and Transfer   Learning</h2><p><strong>Authors:Smruti Jagtap, Kanika Jadhav, Rushikesh Temkar, Minal Deshmukh</strong></p>
<p>The hearing-impaired community in India deserves the access to tools that help them communicate, however, there is limited known technology solutions that make use of Indian Sign Language (ISL) at present. Even though there are many ISL users, ISL cannot access social and education arenas because there is not yet an efficient technology to convert the ISL signal into speech or text. We initiated this initiative owing to the rising demand for products and technologies that are inclusive and help ISL, filling the gap of communication for the ones with hearing disability. Our goal is to build an reliable sign language recognition system with the help of Convolutional Neural Networks (CNN) to . By expanding communication access, we aspire toward better educational opportunities and a more inclusive society for hearing impaired people in India. </p>
<blockquote>
<p>印度的听障群体理应获得帮助他们沟通的工具，然而目前利用印度手语（ISL）的技术解决方案却十分有限。尽管有很多ISL使用者，但ISL无法进入社会和教育的领域，因为没有有效的技术将ISL信号转化为语音或文字。我们发起这一倡议是因为市场对包容性和帮助ISL的产品和技术有着日益增长的需求，以填补听障人士的沟通空白。我们的目标是借助卷积神经网络（CNN）建立可靠的手语识别系统。通过扩大沟通渠道，我们期望为印度的听障人士提供更好的教育机会和更具包容性的社会。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07486v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>印度的听障社区需要能够辅助他们沟通的工具，但目前利用印度手语（ISL）的技术解决方案十分有限。尽管有很多ISl使用者，但由于缺乏将ISl信号转换为语音或文字的有效技术，他们无法进入社会和教育的领域。我们发起这一倡议，是因为对能够帮助听障人士与听障群体沟通的产品和技术有着日益增长的需求，以填补沟通方面的空白。我们的目标是借助卷积神经网络（CNN）建立一个可靠的识别系统。通过扩大沟通渠道，我们期望为印度的听障人士提供更好的教育机会和更具包容性的社会。</p>
<p><strong>要点</strong></p>
<ol>
<li>听障群体需要有效沟通工具，尤其针对印度手语（ISL）的解决方案。</li>
<li>当前缺乏将ISL转化为语音或文字的技术，限制了听障人士的社会和教育参与。</li>
<li>倡议建立基于卷积神经网络（CNN）的可靠手语识别系统。</li>
<li>通过扩大沟通渠道，帮助听障人士更好地融入社会。</li>
<li>重视为听障人士提供教育机会，促进社会包容性增长。</li>
<li>该技术解决方案对于推动社会公平和对听障群体的支持至关重要。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da4d0df39252ba738b590da304437be9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4a872f8cb8b8c0514ca9ea9e047e158a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-309235b1c989ff255c1d135e9d752368.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fed24bac77a824631d4a7b4553991f98.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4b5ac3245503cdd065d5d2f064a658d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5184640a16d47a3d979f8ac429ffb86f.jpg" align="middle">
</details>




<h2 id="Effective-Text-Adaptation-for-LLM-based-ASR-through-Soft-Prompt-Fine-Tuning"><a href="#Effective-Text-Adaptation-for-LLM-based-ASR-through-Soft-Prompt-Fine-Tuning" class="headerlink" title="Effective Text Adaptation for LLM-based ASR through Soft Prompt   Fine-Tuning"></a>Effective Text Adaptation for LLM-based ASR through Soft Prompt   Fine-Tuning</h2><p><strong>Authors:Yingyi Ma, Zhe Liu, Ozlem Kalinli</strong></p>
<p>The advent of Large Language Models (LLM) has reformed the Automatic Speech Recognition (ASR). Prompting LLM with audio embeddings to generate transcriptions becomes the new state-of-the-art ASR. Despite LLMs being trained with an extensive amount of text corpora, high-quality domain-specific text data can still significantly enhance ASR performance on domain adaptation tasks. Although LLM-based ASR can naturally incorporate more text corpora by fine-tuning the LLM decoder, fine-tuning such ASR on text-only data without paired prompts may diminish the effectiveness of domain-specific knowledge. To mitigate this issue, we propose a two-step soft prompt fine-tuning strategy that enhances domain-specific text adaptation. Experimental results show that text adaptation with our proposed method achieved a relative up to 9% Word Error Rate (WER) reduction and up to 18% Entity Error Rate (EER) reduction on the target domain compared to the baseline ASR. Combining this with domain-specific Language Model (LM) fusion can further improve the EER by a relative 2-5% </p>
<blockquote>
<p>大型语言模型（LLM）的出现已经改变了自动语音识别（ASR）的技术面貌。使用音频嵌入来提示LLM生成转录本已经成为最新的先进ASR技术。尽管LLM通过大量的文本语料库进行训练，但高质量的专业领域特定文本数据仍然可以显著增强领域自适应任务上的ASR性能。虽然基于LLM的ASR可以通过微调LLM解码器自然地融入更多的文本语料库，但在仅使用文本数据进行微调而没有配对提示的情况下，可能会降低特定领域知识的有效性。为了缓解这一问题，我们提出了一种两步软提示微调策略，以提高特定领域的文本适应性。实验结果表明，与基线ASR相比，使用我们提出的方法进行的文本适应相对实现了高达9%的单词错误率（WER）减少和高达18%的实体错误率（EER）减少。将其与特定领域的语言模型（LM）融合相结合，可以进一步改善EER，相对提高2-5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06967v1">PDF</a> accepted as SLT 2024 proceeding</p>
<p><strong>Summary</strong>：随着大型语言模型（LLM）的出现，自动语音识别（ASR）领域发生了变革。使用音频嵌入来提示LLM生成转录成为最新的ASR技术。尽管LLM接受了大量的文本语料库训练，但高质量的专业领域文本数据仍然可以显著提高域适应任务的ASR性能。为了解决在仅文本数据上微调ASR可能会降低领域特定知识有效性这一问题，我们提出了一种两步软提示微调策略，以增强特定领域的文本适应性。实验结果表明，与基线ASR相比，使用所提出的方法在目标领域上实现了相对高达9%的单词错误率（WER）减少和高达18%的实体错误率（EER）减少。结合领域特定的语言模型（LM）融合可以进一步改善EER，相对提高2-5%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）的引入已使自动语音识别（ASR）发生变革，使用音频嵌入提示LLM生成转录成为最新技术。</li>
<li>即使是在LLM接受了大量文本语料库训练的情况下，高质量的专业领域文本数据也能显著提高ASR在域适应任务上的性能。</li>
<li>在仅文本数据上微调ASR可能会降低领域特定知识的有效性。</li>
<li>为了解决上述问题，提出了一种两步软提示微调策略，以增强特定领域的文本适应性。</li>
<li>实验表明，该策略可实现显著的单词错误率（WER）和实体错误率（EER）减少。</li>
<li>结合领域特定的语言模型（LM）融合可以进一步提高EER。</li>
<li>整体而言，此策略为提高ASR性能提供了新的方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db6f9e81c564c2baea34172f69335291.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8502c1556cec1d4276370e422f9cbc59.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c326572579e4be46117fa0f2b7eb4235.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-90cc62091becd597b71ff060787cceaa.jpg" align="middle">
</details>




<h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>控制语音合成的风格和特性对于适应特定的上下文和用户要求至关重要。之前的文本到语音（TTS）工作主要集中在产生自然语音的技术方面，如语调、节奏和清晰度。然而，他们忽略了这样一个事实，即对合成语音的空间感知的重视程度日益增加，这可能会在游戏和虚拟现实领域提供沉浸式体验。为了解决这一问题，本文提出了一种新型的多模式TTS方法，即图像指示沉浸式文本到语音合成（I2TTS）。具体来说，我们引入了一个场景提示编码器，它直接将视觉场景提示集成到合成管道中，以控制语音生成过程。此外，我们提出了一种混响分类和细化技术，该技术可以调整合成的梅尔频谱图以增强沉浸式体验，确保所涉及的混响条件与场景准确匹配。实验结果表明，我们的模型在不影响语音自然性的情况下实现了高质量的场景和空间匹配，标志着上下文感知语音合成领域取得了重大进展。项目演示页面：<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> 索引术语-语音合成，场景提示，空间感知。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v2">PDF</a> The paper is missing some information</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的多模态文本转语音合成方法——图像指示沉浸式文本转语音合成（I2TTS）。该方法通过引入场景提示编码器，将视觉场景提示直接融入合成管道，控制语音生成过程。同时，提出一种混响分类与细化技术，调整合成频谱图，增强沉浸式体验，确保混响条件与场景匹配。实验结果显示，该方法实现了高质量的场景和空间匹配，且不影响语音的自然性，标志着语境感知语音合成的重大进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转语音合成中控制风格和特点对于适应特定语境和用户要求至关重要。</li>
<li>以往的研究主要关注语音合成的技术方面，如语调、节奏和清晰度，但忽视了合成语音的空间感知。</li>
<li>论文提出了一种新型的多模态文本转语音合成方法——I2TTS，该方法结合了视觉场景提示。</li>
<li>I2TTS通过场景提示编码器控制语音生成过程，提高语音合成的灵活性和实用性。</li>
<li>为了增强沉浸式体验，论文提出了混响分类与细化技术，调整合成频谱图，确保混响条件与场景匹配。</li>
<li>实验结果显示，I2TTS实现了高质量的场景和空间匹配，且不影响语音的自然性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-66e6ccdd517a50e9ee5dddf9637b47e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4689ec2cdbacd48202667ffe499ad66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf7fa3b4564f4f1f3a720aedb6cba728.jpg" align="middle">
</details>




<h2 id="Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening"><a href="#Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening" class="headerlink" title="Uncovering the role of semantic and acoustic cues in normal and dichotic   listening"></a>Uncovering the role of semantic and acoustic cues in normal and dichotic   listening</h2><p><strong>Authors:Akshara Soman, Sai Samrat Kankanala, Sriram Ganapathy</strong></p>
<p>Despite extensive research, the precise role of acoustic and semantic cues in complex speech perception tasks remains unclear. In this study, we propose a paradigm to understand the encoding of these cues in electroencephalogram (EEG) data, using match-mismatch (MM) classification task. The MM task involves determining whether the stimulus and response correspond to each other or not. We design a multi-modal sequence model, based on long short term memory (LSTM) architecture, to perform the MM task. The model is input with acoustic stimulus (derived from the speech envelope), semantic stimulus (derived from textual representations of the speech content), and neural response (derived from the EEG data). Our experiments are performed on two separate conditions, i) natural passive listening condition and, ii) an auditory attention based dichotic listening condition. Using the MM task as the analysis framework, we observe that - a) speech perception is fragmented based on word boundaries, b) acoustic and semantic cues offer similar levels of MM task performance in natural listening conditions, and c) semantic cues offer significantly improved MM classification over acoustic cues in dichotic listening task. Further, the study provides evidence of right ear advantage in dichotic listening conditions. </p>
<blockquote>
<p>尽管进行了大量研究，但声音和语义线索在复杂的语音感知任务中的精确作用仍不清楚。在这项研究中，我们提出了一种基于脑电图（EEG）数据编码这些线索的理解范式，采用匹配-不匹配（MM）分类任务。MM任务涉及确定刺激与响应是否相对应。我们设计了一种基于长短时记忆（LSTM）架构的多模态序列模型，以执行MM任务。该模型的输入包括声音刺激（来源于语音包络）、语义刺激（来源于语音内容的文本表示）和神经响应（来源于EEG数据）。我们的实验在两个单独的条件下进行：i)自然被动聆听条件；ii)基于听觉注意力的二听条件。使用MM任务作为分析框架，我们观察到：a)语音感知是基于词边界的片段化；b)在自然聆听条件下，声音和语义线索在MM任务中提供相似的表现；c)在二听任务中，语义线索相对于声音线索能显著提高MM分类效果。此外，该研究为二听条件下的右耳优势提供了证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11308v1">PDF</a> 9 Pages, 4 Figures</p>
<p><strong>Summary</strong></p>
<p>该研究探讨了声学线索和语义线索在复杂语音感知任务中的精确作用。研究采用匹配-不匹配分类任务，以理解这些线索在脑电图数据中的编码方式。实验设计了一个基于LSTM架构的多模式序列模型，用于执行匹配-不匹配任务。该模型以语音包络产生的声学刺激、语音内容的文本表示产生的语义刺激和EEG数据产生的神经反应为输入。在自然被动听觉条件和基于听觉注意力的二选一听觉条件下进行实验。观察发现：语音感知以单词边界为基础进行分割；在自然听觉条件下，声学线索和语义线索提供类似的匹配任务性能；在二选一听觉任务中，语义线索显著优于声学线索，并且在二选一听觉条件下存在右耳优势现象。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究使用匹配-不匹配分类任务来探究声学线索和语义线索在复杂语音感知中的贡献。</li>
<li>基于LSTM的多模式序列模型用于执行匹配-不匹配任务，该模型结合了声学刺激、语义刺激和神经反应数据。</li>
<li>在自然被动听觉条件下，声学线索和语义线索在匹配任务中的表现相似。</li>
<li>在二选一听觉条件下，语义线索对匹配-不匹配任务的贡献显著超过声学线索。</li>
<li>实验结果显示语音感知是以单词边界为基础进行分割的。</li>
<li>该研究提供了在二选一听觉条件下右耳优势的证据。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-702194aa412ab66d9b55f848bb4b0802.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f88c633e521445f6a5e0c1930330554c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2caaa7973dbbc8b7feaaeb52a5006feb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-77deb2d8824993a0831777932f5b46c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9e94c411962448b979a6fa8a23923d9b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4df85715af024e597b14960f49d00b90.jpg" align="middle">
</details>




<h2 id="Interactive-Cycle-Model-–-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses"><a href="#Interactive-Cycle-Model-–-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses" class="headerlink" title="Interactive Cycle Model – The Linkage Combination among Automatic   Speech Recognition, Large Language Models and Smart Glasses"></a>Interactive Cycle Model – The Linkage Combination among Automatic   Speech Recognition, Large Language Models and Smart Glasses</h2><p><strong>Authors:Libo Wang</strong></p>
<p>This research proposes the interaction loop model “ASR-LLMs-Smart Glasses”, which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLMs. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>本研究提出了“ASR-LLMs-智能眼镜”互动循环模型。此模型结合了自动语音识别、大型语言模型和智能眼镜，以促进无缝的人机互动。此外，本研究的方法论涉及将互动过程分解成不同的阶段和元素。语音由ASR捕获并处理，然后由LLMs进行分析和解释。结果随后传输到智能眼镜进行显示。当用户与显示的数据互动时，反馈循环就完成了。本研究使用数学公式来量化模型的表现，主要围绕核心评估点：ASR语音转文本的准确性、连贯性和延迟。研究结果在理论上提供了模型的可行性和性能测试和评估。详细的架构细节和实验过程已上传至Github，链接为：<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10362v2">PDF</a> OpenReview submitted. 10 pages of text and 2 figures</p>
<p><strong>Summary</strong>：该研究提出了一个名为“ASR-LLMs-智能眼镜”的互动循环模型，该模型结合了自动语音识别、大型语言模型和智能眼镜，以促进无缝的人机交互。研究方法涉及将互动过程分解成不同的阶段和元素。语音被捕获并经过ASR处理，然后通过LLMs进行分析和解释。结果传输到智能眼镜进行显示。用户与显示的数据进行交互时，反馈循环就完成了。研究用数学公式来量化模型性能，围绕核心评估点：ASR语音转文本的准确性、连贯性和延迟性。该模型的理论测试结果和性能评估已详细上传至GitHub。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究提出了ASR-LLMs-智能眼镜互动循环模型，整合自动语音识别、大型语言模型和智能眼镜技术。</li>
<li>互动过程被分解为不同阶段和元素，包括语音捕获、ASR处理、LLMs分析和结果展示。</li>
<li>用户与智能眼镜显示的数据进行交互，完成反馈循环。</li>
<li>模型性能通过数学公式量化，主要评估点为ASR语音转文本的准确性、连贯性和延迟性。</li>
<li>模型的理论测试结果和性能评估已详细上传至GitHub，便于进一步研究和参考。</li>
<li>该模型有助于实现无缝的人机交互，提升用户体验。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg" align="middle">
</details>




<h2 id="DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions"><a href="#DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions" class="headerlink" title="DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions"></a>DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions</h2><p><strong>Authors:Shu-Tong Niu, Jun Du, Ruo-Yu Wang, Gao-Bin Yang, Tian Gao, Jia Pan, Yu Hu</strong></p>
<p>We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end speech recognition, combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries during decoding. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art single-channel speech recognition performance. </p>
<blockquote>
<p>我们提出了一种单通道深度级联融合分治与分离（DCF-DS）后端语音识别框架，该框架结合了神经说话人分治（NSD）和语音分离（SS）。首先，我们在联合训练框架内按顺序整合NSD和SS模块，使分离模块能够有效地利用分治模块中的说话人时间边界。接着，为了补充DCF-DS训练，我们引入了一种窗口级解码方案，以解决数据收敛不稳定的问题。我们还探索使用在真实数据集上训练的NSD系统，以在解码过程中提供更准确的说话人边界。此外，我们在DCF-DS框架中加入了可选的多输入多输出语音增强模块（MIMO-SE），进一步提升了性能。最后，我们通过重新聚类DCF-DS输出来提高分治结果，从而提高语音识别准确率。通过采用DCF-DS方法，我们在CHiME-8 NOTSOFAR-1挑战的现实单通道赛道上获得第一名。我们在开放的LibriCSS数据集上也进行了评估，实现了最新的单通道语音识别性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了单通道深度级联融合分治与分离（DCF-DS）框架，用于后端语音识别。该框架结合了神经网络说话人分治（NSD）和语音分离（SS）。首先，在联合训练框架中按顺序整合NSD和SS模块，使分离模块能有效利用分治模块提供的说话人时间边界信息。其次，为补充DCF-DS训练，引入窗口级别解码方案，以解决稀疏数据收敛不稳定（SDCI）问题。此外，还探索使用在真实数据集上训练的NSD系统，以在解码过程中提供更准确的说话人边界。最后，在DCF-DS框架中加入了可选的多输入多输出语音增强模块（MIMO-SE），进一步提升了性能。通过采用DCF-DS方法，在CHiME-8 NOTSOFAR-1挑战的现实中单通道赛道上取得了第一名，并在开放的LibriCSS数据集上实现了最新的单通道语音识别性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了单通道Deep Cascade Fusion of Diarization and Separation (DCF-DS)框架，整合了神经网络说话人分治（NSD）和语音分离（SS）。</li>
<li>通过联合训练NSD和SS模块，利用说话人时间边界信息。</li>
<li>引入窗口级别解码方案，解决稀疏数据收敛不稳定问题。</li>
<li>使用真实数据集训练的NSD系统提供更准确的说话人边界信息。</li>
<li>加入了MIMO-SE模块，进一步提升了性能。</li>
<li>在CHiME-8 NOTSOFAR-1挑战中取得第一名。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2c1f133fe48629ed42ebefd43e4198ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-50b1be278625e9ff5f56e1bc19f5ff48.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5e9f3bb5b76e7725e9c9d2c4a617f5a6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a249e24e14206090f21ae688491614b2.jpg" align="middle">
</details>




<h2 id="Dialectal-Coverage-And-Generalization-in-Arabic-Speech-Recognition"><a href="#Dialectal-Coverage-And-Generalization-in-Arabic-Speech-Recognition" class="headerlink" title="Dialectal Coverage And Generalization in Arabic Speech Recognition"></a>Dialectal Coverage And Generalization in Arabic Speech Recognition</h2><p><strong>Authors:Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki</strong></p>
<p>Developing robust automatic speech recognition (ASR) systems for Arabic, a language characterized by its rich dialectal diversity and often considered a low-resource language in speech technology, demands effective strategies to manage its complexity. This study explores three critical factors influencing ASR performance: the role of dialectal coverage in pre-training, the effectiveness of dialect-specific fine-tuning compared to a multi-dialectal approach, and the ability to generalize to unseen dialects. Through extensive experiments across different dialect combinations, our findings offer key insights towards advancing the development of ASR systems for pluricentric languages like Arabic. </p>
<blockquote>
<p>开发针对阿拉伯语的稳健自动语音识别（ASR）系统，阿拉伯语以其丰富的方言多样性而著称，且在语音技术中常被视为资源匮乏的语言，需要有效的策略来管理其复杂性。本研究探讨了影响ASR性能的三个关键因素：预训练中的方言覆盖作用、与多方言方法相比针对特定方言的微调的有效性，以及推广到未见方言的能力。通过对不同方言组合的大量实验，我们的研究结果为推进针对阿拉伯等多元中心语言发展ASR系统提供了关键见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05872v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了影响阿拉伯语自动语音识别（ASR）系统性能的三个关键因素：预训练中的方言覆盖作用、针对方言的微调与多方言方法的有效性，以及推广至未见方言的能力。通过对不同方言组合的大量实验，本研究为阿拉伯语等多中心语言ASR系统的发展提供了关键见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>阿拉伯语作为低资源语言在语音技术中的挑战。</li>
<li>预训练中方言覆盖对ASR性能的影响。</li>
<li>方言特定微调与多方言方法的有效性比较。</li>
<li>推广到未见方言的能力对ASR系统的重要性。</li>
<li>大量实验证明不同方言组合下的关键洞察。</li>
<li>对发展多中心语言如阿拉伯语的ASR系统的启示。</li>
<li>本研究为提升ASR系统性能提供了方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2419d1ac438dcd757d023f4e59c4be1b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13fc695d00050799ec88e2bedaddff8d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8363b36a0c1b877eada0c3e37a8cec6b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5230d2c6b2b5e6c1a4b718729bb8ecb3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5cf328d5548f4ac8577c0c6eb782f612.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-03ac340c8eb3b576cbe9d0982638bc3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ed00c8b2f71f9870839b176c8daffc70.jpg" align="middle">
</details>




<h2 id="VoiceBench-Benchmarking-LLM-Based-Voice-Assistants"><a href="#VoiceBench-Benchmarking-LLM-Based-Voice-Assistants" class="headerlink" title="VoiceBench: Benchmarking LLM-Based Voice Assistants"></a>VoiceBench: Benchmarking LLM-Based Voice Assistants</h2><p><strong>Authors:Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li</strong></p>
<p>Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field. </p>
<blockquote>
<p>基于大型语言模型（LLM）的成功，最近的进展，如GPT-4o，已经能够通过LLM语音助手实现实时语音交互，与基于文本的传统交互方式相比，为用户提供了显著改善的体验。然而，缺乏用于评估这些语音交互能力的基准测试阻碍了LLM语音助手的发展。当前的评估主要集中在自动语音识别（ASR）或清洁语音的一般知识评估上，忽视了涉及多种说话人特征、环境和内容因素的更复杂、真实的现实世界场景。为了解决这一问题，我们引入了VoiceBench，这是第一个旨在提供LLM语音助手多方面评估的基准测试。VoiceBench还包括真实和合成语音指令，这些指令结合了上述三个关键现实世界的变异因素。大量实验揭示了当前LLM语音助手模型的局限性，并为该领域的未来研究和开发提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17196v3">PDF</a> Work in progress. Data is available at   <a target="_blank" rel="noopener" href="https://github.com/MatthewCYM/VoiceBench">https://github.com/MatthewCYM/VoiceBench</a></p>
<p><strong>Summary</strong>：基于大型语言模型（LLM）的成功，GPT-4o等最新进展通过LLM语音助手实现了实时语音交互，显著提升了用户体验。然而，缺乏评估这些语音交互能力的基准测试阻碍了LLM语音助手的发展。为解决这一问题，我们引入了VoiceBench，这是一个为LLM语音助手提供多方面评估的首个基准测试。它包含真实和合成语音指令，涵盖真实世界中的多种说话人特征、环境和内容因素。实验揭示了当前LLM语音助手模型的局限性，为未来的研究和发展提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLMs的进步推动了实时语音交互的发展，通过LLM语音助手提升了用户体验。</li>
<li>目前缺乏评估LLM语音助手交互能力的基准测试。</li>
<li>VoiceBench是首个为LLM语音助手提供多方面评估的基准测试。</li>
<li>VoiceBench包含真实和合成的语音指令，涵盖多种真实世界的说话人特征、环境和内容因素。</li>
<li>现有LLM语音助手模型在真实世界场景下存在局限性。</li>
<li>通过对VoiceBench的广泛实验揭示了这些局限性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-69ae0a5443de6281affd9aaaa8657b10.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-86de805d5a4c3dd28764ed73475c70f7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bc8f15069422afe64369d4abe5a0a4d9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-15a9acc59b778b744382cabc12652f69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20de57dd7f623666968b886a207ce214.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-95b1aa211796e45875eb98816b841157.jpg" align="middle">
</details>




<h2 id="Multi-Level-Speaker-Representation-for-Target-Speaker-Extraction"><a href="#Multi-Level-Speaker-Representation-for-Target-Speaker-Extraction" class="headerlink" title="Multi-Level Speaker Representation for Target Speaker Extraction"></a>Multi-Level Speaker Representation for Target Speaker Extraction</h2><p><strong>Authors:Ke Zhang, Junjie Li, Shuai Wang, Yangjie Wei, Yi Wang, Yannan Wang, Haizhou Li</strong></p>
<p>Target speaker extraction (TSE) relies on a reference cue of the target to extract the target speech from a speech mixture. While a speaker embedding is commonly used as the reference cue, such embedding pre-trained with a large number of speakers may suffer from confusion of speaker identity. In this work, we propose a multi-level speaker representation approach, from raw features to neural embeddings, to serve as the speaker reference cue. We generate a spectral-level representation from the enrollment magnitude spectrogram as a raw, low-level feature, which significantly improves the model’s generalization capability. Additionally, we propose a contextual embedding feature based on cross-attention mechanisms that integrate frame-level embeddings from a pre-trained speaker encoder. By incorporating speaker features across multiple levels, we significantly enhance the performance of the TSE model. Our approach achieves a 2.74 dB improvement and a 4.94% increase in extraction accuracy on Libri2mix test set over the baseline. </p>
<blockquote>
<p>目标说话人提取（TSE）依赖于目标的参考线索，从语音混合中提取目标语音。虽然常用说话人嵌入作为参考线索，但这种使用大量说话人预训练的嵌入可能会导致说话人身份混淆。在本文中，我们提出了一种多层次的说话人表示方法，从原始特征到神经嵌入，作为说话人参考线索。我们从注册幅度谱图中生成频谱级表示作为原始的低级特征，这大大提高了模型的泛化能力。此外，我们提出了一种基于交叉注意力机制的上文嵌入特征，融合了预训练说话人编码器的帧级嵌入。通过结合多个层次的说话人特征，我们显著提高了TSE模型的性能。我们的方法在Libri2mix测试集上相对于基线实现了2.74 dB的改进和4.94%的提取准确度提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16059v2">PDF</a> 5 pages. Submitted to ICASSP 2025. Implementation will be released at   <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wesep">https://github.com/wenet-e2e/wesep</a></p>
<p><strong>摘要</strong></p>
<p>本文研究了目标说话人提取（TSE）技术中的问题，即利用参考线索从语音混合物中提取目标语音。虽然常用的参考线索是说话人嵌入，但使用大量说话者进行预训练的嵌入可能导致说话人身份混淆的问题。因此，本文提出了一种多层次的说话人表示方法，从原始特征到神经嵌入，作为说话人的参考线索。我们从注册幅度谱图中生成频谱级别的表示作为原始的低级别特征，这显著提高了模型的泛化能力。此外，我们还提出了一种基于交叉注意力机制的上文嵌入特征，该特征融合了预训练说话人编码器的帧级别嵌入。通过结合多个层次的说话人特征，我们显著提高了TSE模型的性能。在Libri2mix测试集上，与基线相比，我们的方法实现了2.74 dB的改进和4.94%的提取精度提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>目标说话人提取（TSE）技术使用参考线索从混合语音中提取目标语音。</li>
<li>说话人嵌入常用作参考线索，但可能存在大量说话人的预训练嵌入导致的身份混淆问题。</li>
<li>提出了一种多层次的说话人表示方法，包括从原始特征（如频谱级别表示）到神经嵌入的不同层次的信息。</li>
<li>频谱级别的表示是通过注册幅度谱图生成的，这提高了模型的泛化能力。</li>
<li>引入了一种基于交叉注意力机制的上文嵌入特征，融合了帧级别的嵌入信息。</li>
<li>结合多个层次的说话人特征显著提高了TSE模型的性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4591c50d6ecf92a22e2e31de06235684.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4445d1ed24eff7a383b1bfed45d962c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ad4cd00d97f928c7bc7d510abcb172ab.jpg" align="middle">
</details>




<h2 id="CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition"><a href="#CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition" class="headerlink" title="CR-CTC: Consistency regularization on CTC for improved speech   recognition"></a>CR-CTC: Consistency regularization on CTC for improved speech   recognition</h2><p><strong>Authors:Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey</strong></p>
<p>Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC. It significantly improves the CTC performance, achieving state-of-the-art results comparable to those attained by transducer or systems combining CTC and attention-based encoder-decoder (CTC&#x2F;AED). We release our code at \url{<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%7D">https://github.com/k2-fsa/icefall}</a>. </p>
<blockquote>
<p>连接时序分类（CTC）是一种广泛应用于自动语音识别（ASR）的方法，以其简单性和计算效率而闻名。然而，它在识别性能上常常表现不足。在这项工作中，我们提出了一致性正则化CTC（CR-CTC），它强制输入语音梅尔频谱图的两个不同增强视图所获得的两个CTC分布之间的一致性。我们从三个角度对其核心行为进行了深入了解：1）它在处理不同增强视图的随机子模型对之间进行自我蒸馏；2）它通过掩码预测学习时间掩码区域内位置的上下文表示，尤其是当我们增加时间掩码的数量时；3）它抑制了过于尖锐的CTC分布，从而减少过拟合并提高了泛化能力。在LibriSpeech、Aishell-1和GigaSpeech数据集上的大量实验证明了我们的CR-CTC的有效性。它显著提高了CTC的性能，实现了与转换器或结合CTC和基于注意力的编码器解码器（CTC&#x2F;AED）的系统所取得的结果相媲的国家前沿水平。我们在<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/k2-fsa/icefall上发布了我们的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05101v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>提出一种基于一致性正则化的CTC（CR-CTC）方法，通过在不同增强视图中获得CTC分布并强制一致性，以提高语音识别性能。该方法通过自我蒸馏、学习上下文表示和抑制极端峰值CTC分布等行为来改善CTC的缺陷。在多个数据集上的实验表明，CR-CTC显著提高CTC性能，达到与转换器或CTC与基于注意力的编码器解码器结合的系统相当的最先进结果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>CR-CTC方法通过强制在不同增强视图中获得的CTC分布一致性来提高语音识别性能。</li>
<li>CR-CTC采用自我蒸馏技术，处理不同增强视图时的随机子模型对。</li>
<li>CR-CTC通过学习上下文表示来改善CTC的缺陷，特别是在增加时间掩蔽量时。</li>
<li>CR-CTC通过抑制极端峰值CTC分布来减少过拟合，提高泛化能力。</li>
<li>在LibriSpeech、Aishell-1和GigaSpeech数据集上的实验表明CR-CTC的有效性。</li>
<li>CR-CTC显著提高了CTC的性能，达到最先进水平，与转换器或结合CTC和注意力编码器解码器的系统相当。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-973348c54f2da3118c75e78867354959.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5e34519caf7b30b4d5d9b5322f289e7a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e1ec9bc930d4c1997854dfb61b3d0882.jpg" align="middle">
</details>




<h2 id="A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation"><a href="#A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation" class="headerlink" title="A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation"></a>A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation</h2><p><strong>Authors:Jingyuan Wang, Jie Zhang, Shihao Chen, Miao Sun</strong></p>
<p>Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under various noise conditions, but with a much lower computational cost and a better SCP. The reproducible code and audio examples are available at <a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN">https://github.com/jywanng/LBCCN</a>. </p>
<blockquote>
<p>双耳语音增强（BSE）旨在联合改善听力设备接收到的带噪语音的质量和清晰度，并保留目标语音的空间线索以实现自然听感。现有方法常常在降噪（NR）能力和空间线索保留（SCP）准确性之间有所妥协，并且在复杂声场环境中计算需求较高。在这项工作中，我们提出了一种基于学习的轻量级双耳复杂卷积网络（LBCCN），它通过过滤低频带实现出色的降噪效果并保持其余部分。此外，我们的方法明确地结合了通道间相对声学传输函数的估计，以确保空间线索的保真度和语音清晰度。结果表明，在多种噪声条件下，所提出的LBCCN的降噪性能与最先进的方法相当，但计算成本更低，SCP性能更好。可复用的代码和音频示例可在<a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jywanng/LBCCN找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了双耳语音增强（BSE）技术的目标及其存在的问题。为了解决这个问题，作者提出了一种基于学习的轻量级双耳复杂卷积网络（LBCCN）。该网络通过过滤低频带并保留其余部分实现出色的降噪性能，并通过估计通道间相对声学传输函数来确保空间线索的保真性和语音清晰度。结果表明，与传统的降噪方法相比，LBCCN在各种噪声条件下实现了出色的降噪性能，同时具有更低的计算成本和更好的空间线索保留能力。相关代码和音频示例可在链接中找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>双耳语音增强（BSE）技术旨在提高噪声信号的语音质量和清晰度，同时保留目标的空间线索以实现自然聆听体验。</li>
<li>当前方法面临在降噪（NR）能力、空间线索保留（SCP）准确性和高计算需求之间的权衡问题。</li>
<li>提出了一种基于学习的轻量级双耳复杂卷积网络（LBCCN），旨在解决上述问题。</li>
<li>LBCCN通过过滤低频带实现出色的降噪性能，同时保留高频部分以保持语音清晰度。</li>
<li>LBCCN通过估计通道间相对声学传输函数来确保空间线索的保真性。</li>
<li>LBCCN在各种噪声条件下实现了出色的降噪性能，与现有先进技术相当。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7310f39e4f46abd5e4a8dab4831ac5b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0dc96b1ca49689a0be21d73c04ec780.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-94f01c1e0f4cf7074ca80b5d6147b6b5.jpg" align="middle">
</details>




<h2 id="Resource-Efficient-Adaptation-of-Speech-Foundation-Models-for-Multi-Speaker-ASR"><a href="#Resource-Efficient-Adaptation-of-Speech-Foundation-Models-for-Multi-Speaker-ASR" class="headerlink" title="Resource-Efficient Adaptation of Speech Foundation Models for   Multi-Speaker ASR"></a>Resource-Efficient Adaptation of Speech Foundation Models for   Multi-Speaker ASR</h2><p><strong>Authors:Weiqing Wang, Kunal Dhawan, Taejin Park, Krishna C. Puvvada, Ivan Medennikov, Somshubra Majumdar, He Huang, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Speech foundation models have achieved state-of-the-art (SoTA) performance across various tasks, such as automatic speech recognition (ASR) in hundreds of languages. However, multi-speaker ASR remains a challenging task for these models due to data scarcity and sparsity. In this paper, we present approaches to enable speech foundation models to process and understand multi-speaker speech with limited training data. Specifically, we adapt a speech foundation model for the multi-speaker ASR task using only telephonic data. Remarkably, the adapted model also performs well on meeting data without any fine-tuning, demonstrating the generalization ability of our approach. We conduct several ablation studies to analyze the impact of different parameters and strategies on model performance. Our findings highlight the effectiveness of our methods. Results show that less parameters give better overall cpWER, which, although counter-intuitive, provides insights into adapting speech foundation models for multi-speaker ASR tasks with minimal annotated data. </p>
<blockquote>
<p>语音基础模型已在各种任务上达到了最先进的性能，如在数百种语言中的自动语音识别（ASR）。然而，由于数据稀缺和稀疏，多说话者ASR对这些模型来说仍然是一个具有挑战性的任务。在本文中，我们提出了使语音基础模型能够在有限训练数据上处理和理解多说话者语音的方法。具体来说，我们只使用电话数据来适应多说话者ASR任务的语音基础模型。值得注意的是，适应的模型在会议数据上无需任何微调也能表现良好，这证明了我们方法的泛化能力。我们进行了几次消溶研究，分析不同参数和策略对模型性能的影响。我们的研究结果突显了我们方法的有效性。结果表明，较少的参数能提供更好的整体cpWER，虽然这有点反直觉，但为我们提供了在有限标注数据下适应多说话者ASR任务的语音基础模型的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01438v2">PDF</a> Accepted by SLT 2024</p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音基础模型在多语种自动语音识别（ASR）任务上的卓越性能，并针对多说话者ASR任务面临的挑战，提出了一系列适应语音基础模型的解决方案。通过使用电话数据对模型进行训练，使其能够处理和理解多说话者的语音，甚至在未经微调的情况下也能在会议数据上表现良好。研究还发现，较少的参数能够带来更好的整体cpWER性能，这为在少量标注数据下适应多说话者ASR任务的语音基础模型提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音基础模型已在多种语言的自动语音识别（ASR）任务上达到最新技术水平。</li>
<li>多说话者ASR仍然是一个挑战，主要由于数据稀缺和稀疏性。</li>
<li>通过使用电话数据训练，可以使语音基础模型适应多说话者ASR任务。</li>
<li>适应后的模型在会议数据上表现良好，无需进一步微调。</li>
<li>研究发现较少的参数能带来更好的整体cpWER性能。</li>
<li>该方法展示了良好的泛化能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4e36ba4275e7af7d573f54800905876e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ce2f870fbab0b62b3007dcf01150d569.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-80a25913d424de691d8a992e7306ad81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aec25fbde60f27d7f855ba26596de064.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c27a4c662f0a810b957be4bf27d079d6.jpg" align="middle">
</details>




<h2 id="Integrating-audiological-datasets-via-federated-merging-of-Auditory-Profiles"><a href="#Integrating-audiological-datasets-via-federated-merging-of-Auditory-Profiles" class="headerlink" title="Integrating audiological datasets via federated merging of Auditory   Profiles"></a>Integrating audiological datasets via federated merging of Auditory   Profiles</h2><p><strong>Authors:Samira Saak, Dirk Oetting, Birger Kollmeier, Mareike Buhl</strong></p>
<p>Audiological datasets contain valuable knowledge about hearing loss in patients, which can be uncovered using data-driven, federated learning techniques. Our previous approach summarized patient information from one audiological dataset into distinct Auditory Profiles (APs). To obtain a better estimate of the audiological patient population, however, patient patterns must be analyzed across multiple, separated datasets, and finally, be integrated into a combined set of APs.   This study aimed at extending the existing profile generation pipeline with an AP merging step, enabling the combination of APs from different datasets based on their similarity across audiological measures. The 13 previously generated APs (NA&#x3D;595) were merged with 31 newly generated APs from a second dataset (NB&#x3D;1272) using a similarity score derived from the overlapping densities of common features across the two datasets. To ensure clinical applicability, random forest models were created for various scenarios, encompassing different combinations of audiological measures.   A new set with 13 combined APs is proposed, providing separable profiles, which still capture detailed patient information from various test outcome combinations. The classification performance across these profiles is satisfactory. The best performance was achieved using a combination of loudness scaling, audiogram and speech test information, while single measures performed worst.   The enhanced profile generation pipeline demonstrates the feasibility of combining APs across datasets, which should generalize to all datasets and could lead to an interpretable global profile set in the future. The classification models maintain clinical applicability. </p>
<blockquote>
<p>听力数据集中包含了关于患者听力损失的重要知识，这些知识可以通过数据驱动和联邦学习技术来发现。我们之前的方法将一位患者的信息汇总为一个独特的听觉特征集（AP）。然而，为了更好地估算听力患者群体，必须分析多个独立数据集的患者模式，并最终将其整合为一组综合的听觉特征集。本研究旨在将现有的特征生成管道扩展到具有AP合并步骤的功能，以便能够根据跨听力测试的相似性将来自不同数据集的AP组合在一起。使用从两个数据集中常见特征的重叠密度得出的相似度得分，将先前生成的13个听觉特征集（NA&#x3D;595）与新生成的来自第二数据集的31个听觉特征集（NB&#x3D;1272）合并。为确保临床适用性，针对包含不同听力测试组合的各种场景创建了随机森林模型。提出了一套包含有鉴别力的十三种综合听觉特征集，能够从各种测试结果组合中捕捉详细的病人信息。这些特征分类性能令人满意。使用响度标度、听力图和语音测试信息的组合取得了最佳性能，而单一指标表现最差。增强后的特征生成管道展示了在不同数据集之间合并听觉特征集的可能性，这将推广到所有数据集，并可能在未来导致形成可解读的全球特征集。分类模型仍然保持了临床适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20765v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究旨在扩展现有的听力状况分析生成管道，增加听觉特征集合并入步骤，实现不同数据集间的听觉特征分析。通过对不同数据集产生的听觉特征集（APs）进行合并分析，发现更具代表性的听力状况组合分布模式。本研究将先前生成的13个APs（数据集NA&#x3D;595）与基于第二个数据集新生成的31个APs（数据集NB&#x3D;1272）进行合并，依据两者间重叠特征的相似性得分来建立合并模型。为确保临床应用性，本研究构建随机森林模型模拟不同听力状况组合场景。最终成功合并得到新的AP组合模型集，不仅能够展现出更具体的病人特征分析轮廓，也提高了音频资料组合的准确性能分析。基于声音尺度测量、听力图以及语音测试信息的组合表现最佳，而单一度量措施表现最差。该改进型生成管道展示了跨数据集合并APs的可行性，有望在未来推广到所有数据集并产生可解读的全球听力状况分析集。分类模型依然保持临床适用性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究通过跨多个数据集分析听觉特征集（APs），以获得更准确的听力状况评估结果。</li>
<li>通过合并先前和新的数据集产生的APs，形成新的组合模型集，提供更详细的病人特征分析轮廓。</li>
<li>研究采用基于特征重叠相似性的方法进行APs合并。</li>
<li>随机森林模型用于模拟不同听力状况组合场景，以确保模型的临床适用性。</li>
<li>组合模型性能分析显示，综合考虑声音尺度测量、听力图和语音测试信息的模型表现最佳。</li>
<li>研究展示了跨数据集合并APs的可行性，未来有望推广到所有数据集并产生全球性听力状况分析集。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-077a5aed8fe333946050487152273996.jpg" align="middle">
</details>




<h2 id="Whisper-Flamingo-Integrating-Visual-Features-into-Whisper-for-Audio-Visual-Speech-Recognition-and-Translation"><a href="#Whisper-Flamingo-Integrating-Visual-Features-into-Whisper-for-Audio-Visual-Speech-Recognition-and-Translation" class="headerlink" title="Whisper-Flamingo: Integrating Visual Features into Whisper for   Audio-Visual Speech Recognition and Translation"></a>Whisper-Flamingo: Integrating Visual Features into Whisper for   Audio-Visual Speech Recognition and Translation</h2><p><strong>Authors:Andrew Rouditchenko, Yuan Gong, Samuel Thomas, Leonid Karlinsky, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, speech models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better speech-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper speech recognition and translation model with gated cross attention. Our models achieve state-of-the-art ASR WER (0.68%) and AVSR WER (0.76%) on LRS3, and state-of-the-art ASR WER (1.3%) and AVSR WER (1.4%) on LRS2. Audio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech recognition and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is versatile and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language. </p>
<blockquote>
<p>视听语音识别（AVSR）利用基于嘴唇的视频来改进噪声环境中的性能。由于视频比音频更难获取，AVSR模型的视频训练数据通常仅限于数千小时。相比之下，诸如Whisper之类的语音模型使用数以百万计小时的数据进行训练，从而学习更好的语音到文本的解码器。巨大的训练数据差异激励我们将Whisper适应于处理视频输入。受Flamingo将视觉特征注入语言模型的启发，我们提出了整合视觉特征的Whisper-Flamingo，用于Whisper语音识别和翻译模型，采用门控交叉注意力机制。我们的模型在LRS3上达到了最先进的ASR WER（0.68％）和AVSR WER（0.76％），在LRS2上达到了最先进的ASR WER（1.3％）和AVSR WER（1.4％）。视听Whisper-Flamingo在噪声环境下的英语语音识别和英-其他六种语言的翻译上优于仅音频的Whisper。而且，Whisper-Flamingo非常通用，使用一组参数即可完成所有这些任务，而先前的方法则针对每种语言分别进行训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10082v3">PDF</a> Interspeech 2024. V3: Added results on LRS2. Code at   <a target="_blank" rel="noopener" href="https://github.com/roudimit/whisper-flamingo">https://github.com/roudimit/whisper-flamingo</a></p>
<p><strong>Summary</strong></p>
<p>基于视频的视频语音识别的性能提升和新技术研究。利用视觉信息增强语音模型的表现力，并整合多种语言的模型训练技术实现卓越的性能表现。该新技术通过使用闸门交叉注意力机制和融合语言模型的视觉特征来取得更高的准确性。音频视频双模式在该模型下的语音识别与翻译能力均取得领先水平，特别是能针对噪音环境下中英文语音的翻译工作，取得了优于原有方法的性能表现。而且相比之前的独立语言训练方式，新模型参数共用、更具通用性。此模型提高了数据使用效率和跨语言识别的能力。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSR通过结合视觉信息提升性能，特别是在噪声环境下。使用基于唇形的视频辅助语音识别的效果更佳。但视频获取难度大且训练数据量受限。</li>
<li>对比于AVSR模型使用的数千小时视频训练数据，Whisper等语音模型具备海量数据支持并构建有更高的语料-文本解码精度。差异推动了通过适配的方法利用视频信息。提出在结合语言模型的特性中，尝试集成视觉特征的方法。</li>
<li>提出集成视觉特征的Whisper-Flamingo模型，该模型结合了Whisper的语音识别和翻译能力，并采用闸门交叉注意力机制处理视频输入信息。该模型在LRS3和LRS2上取得了先进的语音识别性能表现。在噪声环境下，该模型在语音识别和跨语言翻译方面表现出色。相较于仅依赖音频的Whisper模型，音视频结合的Whisper-Flamingo展现出更出色的表现效果。通过加入视频输入能够显著地增强识别准确率，且显著提高了在多语言环境中的性能表现。</li>
<li>Whisper-Flamingo具有强大的泛化能力，能使用同一组参数进行多项任务处理，相比之下传统的训练方法需要在每种语言上进行单独训练，这种方法在灵活性和效率方面表现出了优越性。通过使用同一个参数集应对多项任务的模式能够提高模型对复杂语言处理和多语言的应对能力同时显著地提升训练效率和数据使用的有效性降低了运算成本和提高了适用性广泛性 。这种灵活性让该模型能在各种复杂多变的场景中发挥出良好的性能表现也大大提高了它的商业价值和社会应用前景。同时也大大增强了模型应对数据不均衡问题的能力也体现了技术的深度应用和潜力的巨大。这种技术将有可能推动语音识别和翻译领域的进一步发展和革新同时也将对多语言处理和人机交互领域产生深远影响 。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e4b7a9914ffaa0d5807dab12a05582f4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4d82db65fd9bac4f80abd87f16c335fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1d066c3aa4fa6bfbeafff65a7261d8b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a12a100f8e3e0757ca865e2fe061789.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3ce46907bde275417f2e23069671f046.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>随着文本转语音（TTS）技术的进步带来的各种威胁，对合成语音进行可靠追踪的需求变得迫切。然而，当前解决这个问题的方法是在语音生成后单独添加水印，这一过程既影响了语音质量，也影响了水印的不易察觉性。此外，这些方法在稳健性和灵活性方面存在局限性。为了解决这些问题，我们提出了TraceableSpeech，这是一种新型的TTS模型，能够直接生成带水印的语音，提高了水印的不易察觉性和语音质量。此外，我们设计了帧级水印的印制和提取，提高了对抗重新拼接攻击的稳健性和操作的时效性灵活性。实验结果表明，TraceableSpeech在不易察觉性、语音质量和抵抗重新拼接攻击方面超越了VALL-E或HiFicodec单独使用WavMark的强大基线。它还可以应用于各种时长的语音。代码可通过<a target="_blank" rel="noopener" href="https://github.com/zjzsers/TraceableSpeech%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/zjzsers/TraceableSpeech进行访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>随着文本转语音（TTS）技术的进步，合成语音的溯源问题日益重要。现有方法常在生成音频后再加水印，影响语音质量和水印的隐蔽性。为解决这一问题，我们提出TraceableSpeech，一个可直接生成带水印语音的新型TTS模型，提高水印的隐蔽性和语音质量。该模型实现帧级水印印盖和提取，对抗拼接攻击更具鲁棒性，操作更灵活。实验显示，TraceableSpeech在水印隐蔽性、语音质量和抗拼接攻击方面优于使用WavMark的VALL-E或HiFicodec。它适用于各种时长的语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS技术的进步带来合成语音溯源的需求。</li>
<li>现有方法添加水印影响语音质量和水印隐蔽性。</li>
<li>TraceableSpeech是一个新型的TTS模型，可直接生成带水印的语音。</li>
<li>TraceableSpeech提高水印的隐蔽性和语音质量。</li>
<li>该模型实现帧级水印印盖和提取，增强鲁棒性和灵活性。</li>
<li>实验显示，TraceableSpeech在水印、语音质量和抗攻击方面优于其他方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test"><a href="#Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test" class="headerlink" title="Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test"></a>Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test</h2><p><strong>Authors:Peter Leer, Jesper Jensen, Laurel H. Carney, Zheng-Hua Tan, Jan Østergaard, Lars Bramsløw</strong></p>
<p>This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation. Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation. DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification. In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals. First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway. We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation. Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy. Finally, we evaluate, to our knowledge for the first time, the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners. The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies. Our proposed approach was shown to provide an increase in speech intelligibility versus an unprocessed baseline and was found to outperform a conventional approach in terms of both intelligibility and preference. </p>
<blockquote>
<p>本文探讨了深度神经网络（DNN）在听力损失补偿方面的应用。听力损失是一个影响全球数百万人的普遍问题，而传统的助听器在提供满意的补偿方面存在局限性。深度神经网络在各种听觉任务中表现出卓越的性能，包括语音识别、说话人识别和音乐分类。在这项研究中，我们提出了一种基于深度神经网络的听力损失补偿方法，该方法是在听力受损和正常听力基于深度神经网络的听觉模型对语音信号的输出反应中进行训练的。首先，我们介绍了一种使用深度神经网络模拟听觉模型的框架，重点是一个听觉路径中的听觉神经模型。我们提出了基于深度神经网络的线性化方法，用于分析基于深度神经网络的听力损失补偿。此外，我们还开发了一种简单的方法来选择用于补偿策略的听觉模型的声中心频率。最后，我们通过听力受损听众的聆听测试，首次对基于深度神经网络的听力损失补偿策略进行了评估。结果表明，所提出的方法导致可行的听力损失补偿策略。与我们未处理的基本方案相比，所提出的方法提高了语音清晰度，并且在清晰度和偏好方面都优于传统方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10420v2">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习神经网络（DNN）用于听力损失补偿的研究。文章介绍了一种基于DNN的听力损失补偿方法，通过训练正常听力和听力受损的DNN听觉模型对语音信号的响应来实现。提出一种模拟听觉模型的框架，并线性化DNN方法进行分析。此外，开发了一种简单的方法来选择听觉模型的中心频率用于补偿策略。评估结果显示，该方法能够提高听力损失者的语言理解力，相较于传统方法更具优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DNN用于听力损失补偿的研究。</li>
<li>基于DNN的听力损失补偿方法通过训练正常听力和听力受损的听觉模型的响应实现。</li>
<li>提出了模拟听觉模型的框架，并重点研究听觉路径中的听觉神经模型。</li>
<li>对DNN方法进行线性化分析。</li>
<li>开发了一种简单选择听觉模型中心频率的方法，用于补偿策略。</li>
<li>评估结果显示，该方法能提高听力损失者的语言理解力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-eb96d0056db88ee2c733128931b7fdde.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b17b20446f95113e30d62616a7728759.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e67e7cf7bc58f078a820af9ac539ae7a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c45eba05418f6e1ebeb65af3ab0e310e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5b2b0e9ca74e4133de6132871478d14d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c4f583d4a12289ffcf1e06220ca86407.jpg" align="middle">
</details>




<h2 id="Speech-Robust-Bench-A-Robustness-Benchmark-For-Speech-Recognition"><a href="#Speech-Robust-Bench-A-Robustness-Benchmark-For-Speech-Recognition" class="headerlink" title="Speech Robust Bench: A Robustness Benchmark For Speech Recognition"></a>Speech Robust Bench: A Robustness Benchmark For Speech Recognition</h2><p><strong>Authors:Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Bhiksha Raj, Nicolas Kourtellis</strong></p>
<p>As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 114 input perturbations which simulate an heterogeneous range of corruptions that ASR models may encounter when deployed in the wild. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as the use of discrete representations, or self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females. Our results revealed noticeable disparities in the model’s robustness across subgroups. We believe that SRB will significantly facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations. </p>
<blockquote>
<p>随着自动语音识别（ASR）模型在物理世界和数字世界的普及，确保它们在存在的各种腐蚀情况下做出可靠预测变得至关重要。我们提出了语音鲁棒基准（SRB），这是一个全面评估ASR模型对各种腐蚀鲁棒性的基准测试。SRB由114个输入扰动组成，这些扰动模拟了ASR模型在野外部署时可能遇到的各种异质腐蚀。我们使用SRB来评估一些最先进的ASR模型的鲁棒性，并观察到模型大小以及某些建模选择，如使用离散表示或自训练，有助于增强鲁棒性。我们将此分析扩展到衡量来自不同人群子组的数据的ASR模型的鲁棒性，即英语和西班牙语使用者以及男性和女性。我们的结果揭示了模型在不同子组之间的鲁棒性存在明显的差异。我们相信，通过使进行全面和可比较的鲁棒性评估变得更容易，SRB将极大地促进未来对稳健ASR模型的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07937v3">PDF</a> submitted to NeurIPS datasets and benchmark track 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了自动语音识别（ASR）模型的鲁棒性评估问题。针对ASR模型在实际应用中可能遇到的多种干扰，提出了一个名为Speech Robust Bench（SRB）的综合评估基准。通过在该基准上进行评估，作者发现模型规模以及采用离散表示和自训练等特定建模选择都有助于提升ASR模型的鲁棒性。然而，针对不同群体的数据（如不同语言和性别的演讲者），模型的鲁棒性存在显著差异。作者认为，Speech Robust Bench将极大地推动未来研究更加稳健的ASR模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出了一种名为Speech Robust Bench（SRB）的综合基准，用于评估自动语音识别（ASR）模型在各种干扰下的鲁棒性。</li>
<li>SRB包含了模拟野外ASR模型可能遇到的多种干扰的114种输入扰动。</li>
<li>作者通过多个前沿ASR模型在SRB上的表现发现，模型规模及特定建模选择如使用离散表示和自训练有助于提高模型的鲁棒性。</li>
<li>不同群体的数据（如不同语言和性别）在模型鲁棒性上存在差异。</li>
<li>ASR模型的鲁棒性评估需要更全面和可比较的方法，而Speech Robust Bench为此提供了便利。</li>
<li>Speech Robust Bench的提出将有助于推动未来更稳健的ASR模型的研究。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c582f037a408bd577abfa57d725d7bda.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2103d6119e46973d4d2973b31715cea4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ae3e3036452a0b887e4798ca7935fd7e.jpg" align="middle">
</details>




<h2 id="PixIT-Joint-Training-of-Speaker-Diarization-and-Speech-Separation-from-Real-world-Multi-speaker-Recordings"><a href="#PixIT-Joint-Training-of-Speaker-Diarization-and-Speech-Separation-from-Real-world-Multi-speaker-Recordings" class="headerlink" title="PixIT: Joint Training of Speaker Diarization and Speech Separation from   Real-world Multi-speaker Recordings"></a>PixIT: Joint Training of Speaker Diarization and Speech Separation from   Real-world Multi-speaker Recordings</h2><p><strong>Authors:Joonas Kalda, Clément Pagés, Ricard Marxer, Tanel Alumäe, Hervé Bredin</strong></p>
<p>A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization. Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio. We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep. With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD. We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them. PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning. </p>
<blockquote>
<p>监督语音分离（SSep）系统的一个主要缺点是它们依赖于合成数据，导致在现实世界中的泛化能力较差。提出了混合不变训练（MixIT）作为一种使用真实录音的无监督替代方案，但它面临着过度分离和适应长音频的挑战。我们引入了PixIT，这是一种联合方法，结合了用于说话人分割（SD）的排列不变训练（PIT）和用于SSep的MixIT。它只需要少量的SD标签，解决了过度分离问题，并允许利用基于聚类的神经网络SD的现有工作来拼接局部分离源。我们通过将自动语音识别（ASR）系统应用于分离源来测量其质量。PixIT提高了两个会议语料库中各种ASR系统的性能，既降低了基于说话人的词错误率，也降低了基于话语的词错误率，同时无需进行任何微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02288v2">PDF</a> Speaker Odyssey 2024</p>
<p><strong>Summary</strong></p>
<p>PixIT是一种结合了排列不变训练（PIT）用于说话人聚类（SD）和混合不变训练（MixIT）用于语音分离（SSep）的联合方法。它解决了过度分离的问题，通过利用现有的基于聚类的神经网络SD，实现了局部分离源的拼接。PixIT通过在两个会议语料库上应用自动语音识别（ASR）系统来衡量分离源的质量，提高了说话人属性和基于话语的单词错误率性能，并且无需任何微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>监督语音分离（SSep）系统存在依赖于合成数据的问题，导致在真实世界中的泛化能力较差。</li>
<li>混合不变训练（MixIT）作为一种无监督方法，使用真实录音数据，但面临过度分离和适应长音频的挑战。</li>
<li>PixIT是结合排列不变训练（PIT）和MixIT的联合方法，用于解决语音分离和说话人聚类的问题。</li>
<li>PixIT通过利用现有的基于聚类的神经网络说话人聚类（SD），实现了局部分离源的拼接。</li>
<li>PixIT提高了自动语音识别（ASR）系统的性能，表现在说话人属性和基于话语的单词错误率上。</li>
<li>PixIT方法不需要任何微调，可以直接应用于真实世界的语音数据。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cb8c7c975906de0cc2794e814b8e474f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-70becf78ea369ed7bca4ec084f0bed37.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-88ffc36e5afa09a7eb2baf7aa9b9d5db.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d138978b544622ade6c58ac7b8512d65.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b3175ee73eb958a6b067ef7c692d2d41.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ddc3774b23fc9d08df84942104d1466f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-74c56e981436794750632941f563d4e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fd83f38f67461d69033f4764feafef20.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Face Swapping/2412.07260v1/page_0_0.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2024-12-12  SegFace Face Segmentation of Long-Tail Classes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_无监督_半监督_对比学习/2410.13471v3/page_0_0.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2024-12-12  ConDSeg A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
