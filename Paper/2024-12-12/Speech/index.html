<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  AdvWave Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Speech/2410.16059v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    94 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="AdvWave-Stealthy-Adversarial-Jailbreak-Attack-against-Large-Audio-Language-Models"><a href="#AdvWave-Stealthy-Adversarial-Jailbreak-Attack-against-Large-Audio-Language-Models" class="headerlink" title="AdvWave: Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models"></a>AdvWave: Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models</h2><p><strong>Authors:Mintong Kang, Chejian Xu, Bo Li</strong></p>
<p>Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate AI regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿›å±•ä¸ºç”¨æˆ·æä¾›äº†åŸºäºè¯­éŸ³çš„äº¤äº’æ–¹å¼ï¼Œè¿™æå¤§åœ°æå‡äº†ç”¨æˆ·ä½“éªŒï¼Œå¹¶åŠ é€Ÿäº†LALMåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ç„¶è€Œï¼Œç¡®ä¿LALMçš„å®‰å…¨è‡³å…³é‡è¦ï¼Œä»¥é˜²æ­¢äº§ç”Ÿå¯èƒ½å¼•å‘ç¤¾ä¼šæ‹…å¿§æˆ–è¿åAIæ³•è§„çš„é£é™©è¾“å‡ºã€‚å°½ç®¡è¿™ä¸ªé—®é¢˜éå¸¸é‡è¦ï¼Œä½†ç”±äºLALMçš„å…´èµ·æ—¶é—´ç›¸å¯¹è¾ƒçŸ­ï¼Œä»¥åŠä¸åŸºäºDNNçš„éŸ³é¢‘æ¨¡å‹ç›¸æ¯”å­˜åœ¨çš„é¢å¤–æŠ€æœ¯æŒ‘æˆ˜ï¼Œå…³äºç ´è§£LALMçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚å…·ä½“æ¥è¯´ï¼ŒLALMä¸­çš„éŸ³é¢‘ç¼–ç å™¨æ¶‰åŠç¦»æ•£æ“ä½œï¼Œå¾€å¾€ä¼šå¯¼è‡´æ¢¯åº¦ç¢è£‚ï¼Œé˜»ç¢ä¾èµ–äºæ¢¯åº¦ä¼˜åŒ–çš„æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚LALMçš„è¡Œä¸ºå¯å˜æ€§è¿›ä¸€æ­¥å¢åŠ äº†æœ‰æ•ˆï¼ˆå¯¹æŠ—æ€§ï¼‰ä¼˜åŒ–ç›®æ ‡çš„è¯†åˆ«éš¾åº¦ã€‚æ­¤å¤–ï¼Œå¯¹å¯¹æŠ—æ€§éŸ³é¢‘æ³¢å½¢å¼ºåˆ¶æ‰§è¡Œéšè”½æ€§çº¦æŸå¼•å…¥äº†è¾ƒå°çš„éå‡¸å¯è¡Œè§£ç©ºé—´ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†ä¼˜åŒ–è¿‡ç¨‹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ç¬¬ä¸€æ¬¾é’ˆå¯¹LALMçš„ç ´è§£æ¡†æ¶AdvWaveã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•æ¥è§£å†³æ¢¯åº¦ç¢è£‚é—®é¢˜ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„ç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§è‡ªé€‚åº”å¯¹æŠ—ç›®æ ‡æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•æ ¹æ®LALMå¯¹ç‰¹å®šæŸ¥è¯¢çš„å“åº”æ¨¡å¼åŠ¨æ€è°ƒæ•´å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ã€‚ä¸ºäº†ç¡®ä¿å¯¹æŠ—æ€§éŸ³é¢‘å¯¹äººç±»å¬ä¼—æ¥è¯´åœ¨æ„ŸçŸ¥ä¸Šä¿æŒè‡ªç„¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†ç±»å™¨å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§å™ªå£°ç±»ä¼¼äºå¸¸è§çš„åŸå¸‚å£°éŸ³ã€‚åœ¨å¤šä¸ªå…ˆè¿›LALMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAdvWaveä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾40%çš„å¹³å‡ç ´è§£æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08608v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿‘æœŸå‘å±•ï¼Œè¯­éŸ³äº¤äº’ç”¨æˆ·ä½“éªŒå¤§å¹…æå‡ï¼ŒåŠ é€ŸLALMåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ç„¶è€Œï¼Œç¡®ä¿LALMçš„å®‰å…¨è‡³å…³é‡è¦ï¼Œä»¥é˜²æ­¢å¯èƒ½å¼•å‘ç¤¾ä¼šæ‹…å¿§æˆ–è¿åAIæ³•è§„çš„é£é™©è¾“å‡ºã€‚å°½ç®¡å®‰å…¨é—®é¢˜è‡³å…³é‡è¦ï¼Œä½†ç”±äºLALMçš„è¿‘æœŸå…´èµ·ä»¥åŠä¸å…¶ç›¸å…³çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œå…³äºç ´è§£LALMçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºAdvWaveï¼Œé¦–ä¸ªé’ˆå¯¹LALMçš„ç ´è§£æ¡†æ¶ã€‚é€šè¿‡åŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•è§£å†³æ¢¯åº¦ç ´ç¢é—®é¢˜ï¼Œå®ç°æœ‰æ•ˆçš„ç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªé€‚åº”å¯¹æŠ—ç›®æ ‡æœç´¢ç®—æ³•ï¼Œæ ¹æ®ç‰¹å®šæŸ¥è¯¢çš„å“åº”æ¨¡å¼åŠ¨æ€è°ƒæ•´å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ã€‚ä¸ºç¡®ä¿å¯¹æŠ—æ€§éŸ³é¢‘å¯¹äººç±»å¬ä¼—è€Œè¨€æ„ŸçŸ¥è‡ªç„¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†ç±»å™¨å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç”Ÿæˆç±»ä¼¼äºå¸¸è§åŸå¸‚å£°éŸ³çš„å¯¹æŠ—æ€§å™ªå£°ã€‚åœ¨å¤šä¸ªå…ˆè¿›LALMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAdvWaveä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå®ç°40%æ›´é«˜çš„å¹³å‡ç ´è§£æˆåŠŸç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¯­éŸ³äº¤äº’æå‡äº†ç”¨æˆ·ä½“éªŒå¹¶æ¨åŠ¨äº†å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨çš„éƒ¨ç½²ã€‚</li>
<li>LALMçš„å®‰å…¨æ€§é—®é¢˜è‡³å…³é‡è¦ï¼Œéœ€è¦é˜²æ­¢å¯èƒ½å¼•å‘ç¤¾ä¼šæ‹…å¿§æˆ–è¿åAIæ³•è§„çš„é£é™©è¾“å‡ºã€‚</li>
<li>ç›®å‰é’ˆå¯¹LALMçš„ç ”ç©¶ç ´è§£ä»ç„¶æœ‰é™ï¼Œä¸»è¦ç”±äºå®ƒä»¬çš„æ–°å…´å’ŒæŠ€æœ¯æŒ‘æˆ˜çš„å¤æ‚æ€§ã€‚</li>
<li>AdvWaveæ˜¯é¦–ä¸ªé’ˆå¯¹LALMçš„ç ´è§£æ¡†æ¶ï¼Œé€šè¿‡åŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•è§£å†³æ¢¯åº¦ç ´ç¢é—®é¢˜ã€‚</li>
<li>è‡ªé€‚åº”å¯¹æŠ—ç›®æ ‡æœç´¢ç®—æ³•èƒ½æ ¹æ®LALMå¯¹ç‰¹å®šæŸ¥è¯¢çš„å“åº”æ¨¡å¼åŠ¨æ€è°ƒæ•´å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ã€‚</li>
<li>åˆ†ç±»å™¨å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ç¡®ä¿ç”Ÿæˆçš„å¯¹æŠ—æ€§éŸ³é¢‘å¯¹äººç±»å¬ä¼—è€Œè¨€æ„ŸçŸ¥è‡ªç„¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ec334f447e36183801430adf2a52ff1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5c71d1c51dc501da740ab9e363d042ab.jpg" align="middle">
</details>




<h2 id="Bilevel-Joint-Unsupervised-and-Supervised-Training-for-Automatic-Speech-Recognition"><a href="#Bilevel-Joint-Unsupervised-and-Supervised-Training-for-Automatic-Speech-Recognition" class="headerlink" title="Bilevel Joint Unsupervised and Supervised Training for Automatic Speech   Recognition"></a>Bilevel Joint Unsupervised and Supervised Training for Automatic Speech   Recognition</h2><p><strong>Authors:Xiaodong Cui, A F M Saif, Songtao Lu, Lisha Chen, Tianyi Chen, Brian Kingsbury, George Saon</strong></p>
<p>In this paper, we propose a bilevel joint unsupervised and supervised training (BL-JUST) framework for automatic speech recognition. Compared to the conventional pre-training and fine-tuning strategy which is a disconnected two-stage process, BL-JUST tries to optimize an acoustic model such that it simultaneously minimizes both the unsupervised and supervised loss functions. Because BL-JUST seeks matched local optima of both loss functions, acoustic representations learned by the acoustic model strike a good balance between being generic and task-specific. We solve the BL-JUST problem using penalty-based bilevel gradient descent and evaluate the trained deep neural network acoustic models on various datasets with a variety of architectures and loss functions. We show that BL-JUST can outperform the widely-used pre-training and fine-tuning strategy and some other popular semi-supervised techniques. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºBL-JUSTï¼ˆåŒå±‚æ¬¡è”åˆæ— ç›‘ç£ä¸ç›‘ç£è®­ç»ƒï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ï¼ˆè¿™æ˜¯ä¸€ä¸ªåˆ†ç¦»çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼‰ç›¸æ¯”ï¼ŒBL-JUSTè¯•å›¾ä¼˜åŒ–è¯­éŸ³æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶æœ€å°åŒ–æ— ç›‘ç£å’Œç›‘ç£æŸå¤±å‡½æ•°ã€‚ç”±äºBL-JUSTå¯»æ±‚ä¸¤ç§æŸå¤±å‡½æ•°çš„åŒ¹é…å±€éƒ¨æœ€ä¼˜è§£ï¼Œå› æ­¤è¯­éŸ³æ¨¡å‹å­¦åˆ°çš„å£°å­¦è¡¨ç¤ºåœ¨é€šç”¨æ€§å’Œä»»åŠ¡ç‰¹å¼‚æ€§ä¹‹é—´è¾¾åˆ°äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºç½šåˆ†çš„åŒå±‚æ¬¡æ¢¯åº¦ä¸‹é™æ¥è§£å†³BL-JUSTé—®é¢˜ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸Šè¯„ä¼°äº†ç»è¿‡è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œå£°å­¦æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å…·æœ‰å¤šç§æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å±•ç¤ºBL-JUSTå¯ä»¥è¶…è¶Šå¹¿æ³›ä½¿ç”¨çš„é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ä»¥åŠå…¶ä»–ä¸€äº›æµè¡Œçš„åŠç›‘ç£æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08548v1">PDF</a> Accepted by IEEE&#x2F;ACM Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBL-JUSTï¼ˆåŒçº§è”åˆæ— ç›‘ç£ä¸ç›‘ç£è®­ç»ƒï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ä¸åŒï¼ŒBL-JUSTæ—¨åœ¨ä¼˜åŒ–å£°å­¦æ¨¡å‹ï¼Œä½¿å…¶åŒæ—¶æœ€å°åŒ–æ— ç›‘ç£ä¸ç›‘ç£æŸå¤±å‡½æ•°ã€‚é€šè¿‡å¯»æ‰¾ä¸¤ç§æŸå¤±å‡½æ•°çš„åŒ¹é…å±€éƒ¨æœ€ä¼˜è§£ï¼Œå£°å­¦æ¨¡å‹å­¦åˆ°çš„å£°éŸ³è¡¨ç°æ—¢é€šç”¨åˆç‰¹å®šäºä»»åŠ¡ã€‚ä½¿ç”¨åŸºäºæƒ©ç½šçš„åŒçº§æ¢¯åº¦ä¸‹é™æ³•è§£å†³BL-JUSTé—®é¢˜ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸Šè¯„ä¼°è®­ç»ƒåçš„æ·±åº¦ç¥ç»ç½‘ç»œå£°å­¦æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒBL-JUSTåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å¹¿æ³›ä½¿ç”¨çš„é¢„è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ä»¥åŠå…¶ä»–ä¸€äº›æµè¡Œçš„åŠç›‘ç£æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†BL-JUSTæ¡†æ¶ï¼Œç»“åˆæ— ç›‘ç£ä¸ç›‘ç£è®­ç»ƒè¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>BL-JUSTä¼˜åŒ–å£°å­¦æ¨¡å‹ï¼ŒåŒæ—¶æœ€å°åŒ–æ— ç›‘ç£ä¸ç›‘ç£æŸå¤±å‡½æ•°ã€‚</li>
<li>é€šè¿‡åŒ¹é…ä¸¤ç§æŸå¤±å‡½æ•°çš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œå®ç°å£°å­¦æ¨¡å‹çš„é€šç”¨æ€§ä¸ä»»åŠ¡ç‰¹å®šæ€§çš„å¹³è¡¡ã€‚</li>
<li>é‡‡ç”¨åŸºäºæƒ©ç½šçš„åŒçº§æ¢¯åº¦ä¸‹é™æ³•è§£å†³BL-JUSTé—®é¢˜ã€‚</li>
<li>åœ¨å¤šç§æ•°æ®é›†å’Œæ¶æ„ä¸Šè¯„ä¼°äº†BL-JUSTæ¡†æ¶çš„æ€§èƒ½ã€‚</li>
<li>BL-JUSTåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„é¢„è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13d0661540efff6aa6df7452a885cc8f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-56488b7c9ec2950e3911c6ef8b92bbe9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4e9c4413afdf631a1da701fc2457bf8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-be0444451118e4360c0695914064f710.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bb11e2f8b6e9595c3aba7f8de129ff45.jpg" align="middle">
</details>




<h2 id="GR-NLP-TOOLKIT-An-Open-Source-NLP-Toolkit-for-Modern-Greek"><a href="#GR-NLP-TOOLKIT-An-Open-Source-NLP-Toolkit-for-Modern-Greek" class="headerlink" title="GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek"></a>GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek</h2><p><strong>Authors:Lefteris Loukas, Nikolaos Smyrnioudis, Chrysa Dikonomaki, Spyros Barbakos, Anastasios Toumazatos, John Koutsikakis, Manolis Kyriakakis, Mary Georgiou, Stavros Vassos, John Pavlopoulos, Ion Androutsopoulos</strong></p>
<p>We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP) toolkit developed specifically for modern Greek. The toolkit provides state-of-the-art performance in five core NLP tasks, namely part-of-speech tagging, morphological tagging, dependency parsing, named entity recognition, and Greeklishto-Greek transliteration. The toolkit is based on pre-trained Transformers, it is freely available, and can be easily installed in Python (pip install gr-nlp-toolkit). It is also accessible through a demonstration platform on HuggingFace, along with a publicly available API for non-commercial use. We discuss the functionality provided for each task, the underlying methods, experiments against comparable open-source toolkits, and future possible enhancements. The toolkit is available at: <a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit">https://github.com/nlpaueb/gr-nlp-toolkit</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºGR-NLP-TOOLKITï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç°ä»£å¸Œè…Šè¯­å¼€å‘çš„å¼€æºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…åœ¨äº”ä¸ªæ ¸å¿ƒNLPä»»åŠ¡ä¸­æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³è¯æ€§æ ‡æ³¨ã€å½¢æ€æ ‡æ³¨ã€ä¾å­˜å¥æ³•åˆ†æã€å‘½åå®ä½“è¯†åˆ«å’Œå¸Œè…Šè¯­è½¬å†™ã€‚è¯¥å·¥å…·åŒ…åŸºäºé¢„è®­ç»ƒçš„Transformerï¼Œå¯å…è´¹æä¾›ï¼Œå¹¶ä¸”å¯ä»¥åœ¨Pythonä¸­è½»æ¾å®‰è£…ï¼ˆé€šè¿‡pip install gr-nlp-toolkitï¼‰ã€‚å®ƒè¿˜å¯ä»¥é€šè¿‡HuggingFaceä¸Šçš„æ¼”ç¤ºå¹³å°å’Œé¢å‘éå•†ä¸šç”¨é€”çš„å…¬å¼€APIè¿›è¡Œè®¿é—®ã€‚æˆ‘ä»¬è®¨è®ºäº†é’ˆå¯¹æ¯é¡¹ä»»åŠ¡æä¾›çš„åŠŸèƒ½ã€åŸºæœ¬æ–¹æ³•ã€ä¸ç±»ä¼¼å¼€æºå·¥å…·åŒ…çš„å®éªŒå¯¹æ¯”ä»¥åŠå¯èƒ½çš„æœªæ¥æ”¹è¿›ã€‚è¯¥å·¥å…·åŒ…å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit">https://github.com/nlpaueb/gr-nlp-toolkit</a> </p>
</blockquote>
<p>ä¸­æ–‡ç¿»è¯‘å¦‚ä¸‹ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08520v1">PDF</a> Accepted Demo Paper @ COLING 2025 (Github:   <a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit/">https://github.com/nlpaueb/gr-nlp-toolkit/</a>, Demo:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo">https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo</a>, API:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API">https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API</a>)</p>
<p><strong>æ€»ç»“</strong><br>    æˆ‘ä»¬ä»‹ç»äº†GR-NLP-TOOLKITï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºç°ä»£å¸Œè…Šè¯­å¼€å‘çš„å¼€æºè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…åœ¨äº”ä¸ªæ ¸å¿ƒNLPä»»åŠ¡ä¸­æä¾›æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€å½¢æ€æ ‡æ³¨ã€ä¾å­˜è§£æã€å‘½åå®ä½“è¯†åˆ«å’Œå¸Œè…Šè¯­ç¿»è¯‘ã€‚å®ƒåŸºäºé¢„è®­ç»ƒçš„Transformerï¼Œå…è´¹ä¸”æ˜“äºåœ¨Pythonä¸­å®‰è£…ï¼ˆé€šè¿‡pip install gr-nlp-toolkitï¼‰ã€‚å®ƒè¿˜é€šè¿‡HuggingFaceä¸Šçš„æ¼”ç¤ºå¹³å°å’Œå…¬å¼€å¯ç”¨çš„APIä¾›éå•†ä¸šä½¿ç”¨ã€‚æˆ‘ä»¬è®¨è®ºäº†æ¯ä¸ªä»»åŠ¡æä¾›çš„åŠŸèƒ½ã€åº•å±‚æ–¹æ³•ä»¥åŠä¸ç±»ä¼¼å¼€æºå·¥å…·åŒ…çš„å®éªŒå¯¹æ¯”ï¼Œè¿˜æœ‰æœªæ¥å¯èƒ½çš„æ”¹è¿›ã€‚è¯¥å·¥å…·åŒ…å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/nlpaueb/gr-nlp-toolkitä¸Šè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GR-NLP-TOOLKITæ˜¯ä¸€ä¸ªä¸“ä¸ºç°ä»£å¸Œè…Šè¯­å¼€å‘çš„å¼€æºè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚</li>
<li>å®ƒæä¾›äº†äº”ä¸ªæ ¸å¿ƒNLPä»»åŠ¡çš„æœ€å…ˆè¿›æŠ€æœ¯æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€å½¢æ€æ ‡æ³¨ç­‰ã€‚</li>
<li>è¯¥å·¥å…·åŒ…åŸºäºé¢„è®­ç»ƒçš„Transformerï¼Œæ˜“äºåœ¨Pythonä¸­å®‰è£…å’Œä½¿ç”¨ã€‚</li>
<li>æä¾›äº†é€šè¿‡HuggingFaceçš„æ¼”ç¤ºå¹³å°å’Œå…¬å¼€APIçš„éå•†ä¸šä½¿ç”¨è®¿é—®ã€‚</li>
<li>å·¥å…·åŒ…å…·æœ‰å¼ºå¤§çš„åŠŸèƒ½å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¯åº”ç”¨äºå¤šç§NLPä»»åŠ¡ã€‚</li>
<li>ä¸å…¶ä»–ç±»ä¼¼å·¥å…·ç›¸æ¯”ï¼Œè¯¥å·¥å…·åŒ…åœ¨æ€§èƒ½å’ŒåŠŸèƒ½ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1671657de697805d937ea34c10bbf126.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9233c8f9e9c07907e734375b7ba10c51.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3470d927cb9de26810790614e5921b9f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b929042d8e02d5a1848f9865d52d1ff2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef0c6ec8c7430573bcf5384172ef2b74.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-914377345c34db0469799d222b297c72.jpg" align="middle">
</details>




<h2 id="Evaluating-the-Impact-of-Discriminative-and-Generative-E2E-Speech-Enhancement-Models-on-Syllable-Stress-Preservation"><a href="#Evaluating-the-Impact-of-Discriminative-and-Generative-E2E-Speech-Enhancement-Models-on-Syllable-Stress-Preservation" class="headerlink" title="Evaluating the Impact of Discriminative and Generative E2E Speech   Enhancement Models on Syllable Stress Preservation"></a>Evaluating the Impact of Discriminative and Generative E2E Speech   Enhancement Models on Syllable Stress Preservation</h2><p><strong>Authors:Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Sai Harshitha Aluru, Chiranjeevi Yarra</strong></p>
<p>Automatic syllable stress detection is a crucial component in Computer-Assisted Language Learning (CALL) systems for language learners. Current stress detection models are typically trained on clean speech, which may not be robust in real-world scenarios where background noise is prevalent. To address this, speech enhancement (SE) models, designed to enhance speech by removing noise, might be employed, but their impact on preserving syllable stress patterns is not well studied. This study examines how different SE models, representing discriminative and generative modeling approaches, affect syllable stress detection under noisy conditions. We assess these models by applying them to speech data with varying signal-to-noise ratios (SNRs) from 0 to 20 dB, and evaluating their effectiveness in maintaining stress patterns. Additionally, we explore different feature sets to determine which ones are most effective for capturing stress patterns amidst noise. To further understand the impact of SE models, a human-based perceptual study is conducted to compare the perceived stress patterns in SE-enhanced speech with those in clean speech, providing insights into how well these models preserve syllable stress as perceived by listeners. Experiments are performed on English speech data from non-native speakers of German and Italian. And the results reveal that the stress detection performance is robust with the generative SE models when heuristic features are used. Also, the observations from the perceptual study are consistent with the stress detection outcomes under all SE models. </p>
<blockquote>
<p>è‡ªåŠ¨éŸ³èŠ‚é‡éŸ³æ£€æµ‹æ˜¯è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰ç³»ç»Ÿå¯¹å­¦ä¹ è€…çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚å½“å‰çš„åº”åŠ›æ£€æµ‹æ¨¡å‹é€šå¸¸æ˜¯åœ¨å¹²å‡€è¯­éŸ³ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™åœ¨èƒŒæ™¯å™ªå£°æ™®éå­˜åœ¨çš„ç°å®åœºæ™¯ä¸­å¯èƒ½ä¸å¤Ÿç¨³å¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¯é‡‡ç”¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ¶ˆé™¤å™ªå£°æ¥æé«˜è¯­éŸ³è´¨é‡ï¼Œä½†å®ƒä»¬å¯¹ä¿æŒéŸ³èŠ‚é‡éŸ³æ¨¡å¼çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä»£è¡¨åŒºåˆ†æ€§å’Œç”Ÿæˆæ€§å»ºæ¨¡æ–¹æ³•çš„ä¸åŒSEæ¨¡å‹åœ¨å™ªå£°æ¡ä»¶ä¸‹å¯¹éŸ³èŠ‚é‡éŸ³æ£€æµ‹çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä»0åˆ°20åˆ†è´çš„è¯­éŸ³æ•°æ®è¿›è¡Œåº”ç”¨è¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨ä¿æŒé‡éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸åŒçš„ç‰¹å¾é›†ï¼Œä»¥ç¡®å®šå“ªäº›ç‰¹å¾åœ¨å™ªå£°ä¸­æ•æ‰é‡éŸ³æ¨¡å¼æœ€æœ‰æ•ˆã€‚ä¸ºäº†è¿›ä¸€æ­¥ç ”ç©¶SEæ¨¡å‹çš„å½±å“ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹åŸºäºäººç±»æ„ŸçŸ¥çš„ç ”ç©¶ï¼Œæ¯”è¾ƒäº†SEå¢å¼ºè¯­éŸ³å’Œå¹²å‡€è¯­éŸ³ä¸­çš„æ„ŸçŸ¥é‡éŸ³æ¨¡å¼ï¼Œä»è€Œäº†è§£è¿™äº›æ¨¡å‹å¦‚ä½•å¾ˆå¥½åœ°ä¿ç•™å¬ä¼—æ„ŸçŸ¥åˆ°çš„éŸ³èŠ‚é‡éŸ³ã€‚å®éªŒæ˜¯åœ¨éè‹±è¯­æ¯è¯­è€…ï¼ˆå¾·è¯­å’Œæ„å¤§åˆ©è¯­ï¼‰çš„è‹±è¯­è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œçš„ã€‚ç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨å¯å‘å¼ç‰¹å¾æ—¶ï¼Œç”Ÿæˆæ€§SEæ¨¡å‹çš„åº”åŠ›æ£€æµ‹æ€§èƒ½æ˜¯ç¨³å¥çš„ã€‚æ­¤å¤–ï¼Œæ„ŸçŸ¥ç ”ç©¶ä¸­çš„è§‚å¯Ÿç»“æœä¸æ‰€æœ‰SEæ¨¡å‹ä¸‹çš„åº”åŠ›æ£€æµ‹ç»“æœç›¸ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08306v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨èƒŒæ™¯å™ªå£°æ™®éå­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ç³»ç»Ÿï¼Œæœ¬è®ºæ–‡æ¢è®¨äº†ä¸åŒè¯­éŸ³å¢å¼ºæ¨¡å‹å¯¹éŸ³ç³»éŸ³å¼ºæ£€æµ‹çš„å½±å“ã€‚é€šè¿‡åº”ç”¨ä¸åŒçš„è¯­éŸ³å¢å¼ºæ¨¡å‹è‡³ä¸åŒä¿¡å™ªæ¯”ï¼ˆSNRï¼‰çš„è¯­éŸ³æ•°æ®ä¸Šï¼Œç ”ç©¶è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨ç»´æŒéŸ³å¼ºæ¨¡å¼æ–¹é¢çš„æ•ˆèƒ½ã€‚åŒæ—¶ç»“åˆäººç±»æ„ŸçŸ¥ç ”ç©¶ï¼Œæ¢è®¨è¿™äº›è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨ä¿æŒéŸ³èŠ‚éŸ³å¼ºæ–¹é¢çš„è¡¨ç°ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¨¡å‹ç»“åˆå¯å‘å¼ç‰¹å¾ä½¿ç”¨å¯¹éŸ³å¼ºæ£€æµ‹çš„ç¨³å¥æ€§è¾ƒå¥½ã€‚è¿™ä¸€å‘ç°æœ‰åŠ©äºè¯­è¨€å­¦ä¹ ç³»ç»Ÿçš„å™ªéŸ³ç¯å¢ƒä¸‹è¡¨ç°æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨éŸ³èŠ‚é‡éŸ³æ£€æµ‹åœ¨è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰é‡éŸ³æ£€æµ‹æ¨¡å‹ä¸»è¦åœ¨å¹²å‡€è¯­éŸ³ä¸Šè®­ç»ƒï¼Œä½†åœ¨ç°å®ç¯å¢ƒä¸­å­˜åœ¨èƒŒæ™¯å™ªå£°çš„æƒ…å†µä¸‹å¯èƒ½ä¸å¤Ÿç¨³å¥ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸åŒè¯­éŸ³å¢å¼ºæ¨¡å‹å¯¹éŸ³ç³»éŸ³å¼ºæ£€æµ‹çš„å½±å“ã€‚</li>
<li>å®éªŒè¡¨æ˜ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¨¡å‹ç»“åˆå¯å‘å¼ç‰¹å¾ä½¿ç”¨æ—¶ï¼ŒéŸ³å¼ºæ£€æµ‹çš„ç¨³å¥æ€§è¾ƒå¥½ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†äººç±»æ„ŸçŸ¥ç ”ç©¶ï¼Œæ¢è®¨äº†è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨ä¿æŒéŸ³èŠ‚éŸ³å¼ºæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œä¸åŒä¿¡å™ªæ¯”ä¸‹çš„è¯­éŸ³æ•°æ®å®éªŒç»“æœæ­ç¤ºäº†è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨ä¸åŒå™ªå£°ç¯å¢ƒä¸‹çš„æ•ˆèƒ½å·®å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9afdca59561274ab12b5678a3cd8d8d6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eda9920ae92bf8647bb6d14579682541.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f68108814ceffc787171ff5052537240.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>åŸºäºLLMçš„ç³»ç»Ÿä¼—æ‰€å‘¨çŸ¥æ˜¯æ•°æ®å¯†é›†å‹çš„ã€‚æœ€è¿‘çš„åŸºäºLLMçš„TTSå·¥ä½œé€šå¸¸é‡‡ç”¨å¤æ‚çš„æ•°æ®å¤„ç†ç®¡é“æ¥è·å¾—é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›å¤æ‚ç®¡é“éœ€è¦åœ¨æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ä¼˜ç§€çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œè¯­éŸ³é™å™ªã€è¯­éŸ³å¢å¼ºã€è¯´è¯äººåˆ†å¸§å’Œæ ‡ç‚¹æ¨¡å‹ï¼‰ï¼Œè€Œè¿™äº›æ¨¡å‹æœ¬èº«éœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¾ˆå°‘å¼€æºã€‚å³ä½¿ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨ä»¥åŠæ ‡ç‚¹ç¬¦å·ä¸å®é™…è¯­éŸ³åœé¡¿ä¹‹é—´çš„ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œä¸¥æ ¼çš„è¿‡æ»¤ç­–ç•¥é€šå¸¸åªä¿ç•™åŸå§‹æ•°æ®çš„10-30%ï¼Œæå¤§åœ°é˜»ç¢äº†æ•°æ®æ‰©å±•å·¥ä½œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘æ ‡è®°å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡äº†ä¸€ä¸ªç®€åŒ–è€Œæœ‰æ•ˆçš„TTSæ•°æ®å¤„ç†ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿåœ¨ä¿æŒæ•°æ®è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½æ•°æ®è·å–æˆæœ¬ï¼Œå®ç°è¶…è¿‡50%çš„æ•°æ®ä¿ç•™ç‡ã€‚é™¤äº†æ•°æ®æ‰©å±•æŒ‘æˆ˜å¤–ï¼ŒåŸºäºLLMçš„TTSç³»ç»Ÿä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”è¿˜å¸¦æ¥äº†æ›´é«˜çš„éƒ¨ç½²æˆæœ¬ã€‚å½“å‰ç³»ç»Ÿé€šå¸¸ä»…ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬åˆ°æ ‡è®°çš„ç”Ÿæˆï¼Œè€Œéœ€è¦ä½¿ç”¨å•ç‹¬çš„æ¨¡å‹ï¼ˆå¦‚æµåŒ¹é…æ¨¡å‹ï¼‰è¿›è¡Œæ ‡è®°åˆ°æ³¢å½¢çš„ç”Ÿæˆï¼Œè¿™äº›æ¨¡å‹ä¸èƒ½ç”±LLMæ¨ç†å¼•æ“ç›´æ¥æ‰§è¡Œï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†éƒ¨ç½²çš„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†LLMå’Œæµç»„ä»¶ä¸­çš„å†—ä½™æ¨¡å—ï¼Œå¹¶ç”¨LLMæ¶æ„æ›¿ä»£äº†æµæ¨¡å‹çš„éª¨å¹²ã€‚åŸºäºè¿™ç§ç®€åŒ–çš„æµéª¨å¹²ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€‚ç”¨äºæµå¼å’Œéæµå¼æ¨ç†çš„ç»Ÿä¸€æ¶æ„ï¼Œæ˜¾è‘—é™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚æœ€åï¼Œç”±äºç®€åŒ–çš„ç®¡é“å’ŒS3Tokenizeré™ä½äº†å¯¹TTSè®­ç»ƒæ•°æ®çš„è´¨é‡è¦æ±‚ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨åŒä¸€æ•°æ®è¿›è¡ŒTTSå’ŒASRä»»åŠ¡è®­ç»ƒçš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„TTSç³»ç»Ÿé‡‡ç”¨ç®€åŒ–çš„æ•°æ®åŠ å·¥æµç¨‹ï¼Œå€ŸåŠ©å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡ç®€åŒ–è€Œæœ‰æ•ˆçš„æ•°æ®åŠ å·¥æµç¨‹ï¼ŒåŒæ—¶ä¿æŒæ•°æ®è´¨é‡å¹¶å‡å°‘æ•°æ®è·å–æˆæœ¬ï¼Œæ•°æ®ä¿ç•™ç‡è¶…è¿‡ä¸€åŠä»¥ä¸Šã€‚ç®€åŒ–æµç¨‹æ¶ˆé™¤å†—ä½™æ¨¡å—ï¼Œç”¨LLMæ¶æ„æ›¿ä»£æµé‡æ¨¡å‹ä¸»å¹²ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æå‡ºç»Ÿä¸€æ¶æ„ç”¨äºæµå¼å’Œéæµå¼æ¨ç†ã€‚æ­¤å¤–ï¼Œç®€åŒ–ç®¡é“å’ŒS3åˆ†è¯å™¨ä½¿å¾—ç»Ÿä¸€TTSå’ŒASRä»»åŠ¡è®­ç»ƒæ•°æ®æˆä¸ºå¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTSç³»ç»Ÿå¯¹æ•°æ®éœ€æ±‚æé«˜ï¼Œéœ€è¦å¤æ‚çš„æ•°æ®åŠ å·¥æµç¨‹å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>å¤æ‚çš„åŠ å·¥æµç¨‹å¯¼è‡´æ•°æ®æŸå¤±ä¸¥é‡ï¼Œä»…ä¿ç•™10-30%çš„åŸå§‹æ•°æ®ã€‚</li>
<li>é‡‡ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰å¯ä»¥ç®€åŒ–æ•°æ®åŠ å·¥æµç¨‹å¹¶ä¿ç•™æ›´å¤šæ•°æ®ã€‚</li>
<li>é€šè¿‡ç®€åŒ–LLMå’Œæµé‡ç»„ä»¶ï¼Œæ¶ˆé™¤å†—ä½™æ¨¡å—ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚</li>
<li>æå‡ºç»Ÿä¸€æ¶æ„ç”¨äºæµå¼å’Œéæµå¼æ¨ç†ï¼Œè¿›ä¸€æ­¥é™ä½éƒ¨ç½²æˆæœ¬ã€‚</li>
<li>ç®€åŒ–åçš„æ•°æ®å’Œæµç¨‹ä½¿å¾—ç»Ÿä¸€TTSå’ŒASRä»»åŠ¡è®­ç»ƒæ•°æ®æˆä¸ºå¯èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆäººå·¥æ™ºèƒ½å› å…¶å¯¹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ç­‰å…¶ä»–ç”ŸæˆæŠ€æœ¯çš„å“è¶Šæ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶å®ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚ä¸»æµçš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿä¸»è¦å°†è¾“å‡ºæ˜ å°„åˆ°æ¢…å°”é¢‘è°±å›¾çš„é¢‘è°±ç©ºé—´ä¸­ï¼Œç”±äºæ¢…å°”é¢‘è°±çš„ç¨€ç–æ€§å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—è´Ÿè½½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LatentSpeechï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚LatentSpeechä½¿ç”¨æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå°†ç›®æ ‡ç»´åº¦é™ä½åˆ°æ¢…å°”é¢‘è°±æ‰€éœ€ç»´åº¦çš„5%ï¼Œä»è€Œç®€åŒ–äº†æ–‡æœ¬åˆ°è¯­éŸ³ç¼–ç å™¨åŠvocoderçš„å¤„ç†è¿‡ç¨‹ï¼Œå¹¶å®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™é¡¹ç ”ç©¶æ ‡å¿—ç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯ä¸­çš„é¦–æ¬¡é›†æˆï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLatentSpeechåœ¨å•è¯é”™è¯¯ç‡æ–¹é¢æé«˜äº†25%ï¼Œæ¢…å°”å€’è°±å¤±çœŸæé«˜äº†24%ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè¿™ä¸¤é¡¹æŒ‡æ ‡åˆ†åˆ«è¿›ä¸€æ­¥æé«˜åˆ°49.5%å’Œ26%ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†LatentSpeechåœ¨æ¨åŠ¨æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯çš„æœ€æ–°å‘å±•æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼AIæŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»è¢«å¿½è§†ã€‚ä¸»æµæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿä¸»è¦æ˜ å°„è¾“å‡ºåˆ°æ¢…å°”é¢‘è°±å›¾ï¼Œå¯¼è‡´é«˜è®¡ç®—è´Ÿè½½ã€‚æœ¬ç ”ç©¶æå‡ºLatentSpeechï¼Œä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œå‡å°‘äº†ç›®æ ‡ç»´åº¦ï¼Œç®€åŒ–äº†TTSç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–è§£ç å™¨çš„å¤„ç†è¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentSpeechåœ¨å•è¯é”™è¯¯ç‡å’Œæ¢…å°”å€’è°±å¤±çœŸæ–¹é¢è¾ƒç°æœ‰æ¨¡å‹åˆ†åˆ«æé«˜äº†25%å’Œ24%ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè¿™äº›æ”¹è¿›åˆ†åˆ«ä¸Šå‡åˆ°49.5%å’Œ26%ã€‚è¿™çªæ˜¾äº†LatentSpeechåœ¨TTSæŠ€æœ¯æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼AIæŠ€æœ¯å—åˆ°å…³æ³¨ï¼Œå¹¶åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å½“å‰ä¸»æµæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿé¢ä¸´é«˜è®¡ç®—è´Ÿè½½é—®é¢˜ã€‚</li>
<li>LatentSpeechæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LatentSpeeché€šè¿‡é™ä½ç›®æ ‡ç»´åº¦æ¥ç®€åŒ–TTSç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–è§£ç å™¨çš„å¤„ç†è¿‡ç¨‹ã€‚</li>
<li>LatentSpeechå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentSpeechåœ¨å•è¯é”™è¯¯ç‡å’Œæ¢…å°”å€’è°±å¤±çœŸæ–¹é¢è¾ƒç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>éšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼ŒLatentSpeechçš„æ”¹è¿›æ½œåŠ›è¿›ä¸€æ­¥å‡¸æ˜¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Style-agnostic-evaluation-of-ASR-using-multiple-reference-transcripts"><a href="#Style-agnostic-evaluation-of-ASR-using-multiple-reference-transcripts" class="headerlink" title="Style-agnostic evaluation of ASR using multiple reference transcripts"></a>Style-agnostic evaluation of ASR using multiple reference transcripts</h2><p><strong>Authors:Quinten McNamara, Miguel Ãngel del RÃ­o FernÃ¡ndez, Nishchal Bhandari, Martin Ratajczak, Danny Chen, Corey Miller, MigÃ¼el JettÃ©</strong></p>
<p>Word error rate (WER) as a metric has a variety of limitations that have plagued the field of speech recognition. Evaluation datasets suffer from varying style, formality, and inherent ambiguity of the transcription task. In this work, we attempt to mitigate some of these differences by performing style-agnostic evaluation of ASR systems using multiple references transcribed under opposing style parameters. As a result, we find that existing WER reports are likely significantly over-estimating the number of contentful errors made by state-of-the-art ASR systems. In addition, we have found our multireference method to be a useful mechanism for comparing the quality of ASR models that differ in the stylistic makeup of their training data and target task. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä½œä¸ºä¸€ä¸ªè¯„ä»·æŒ‡æ ‡å­˜åœ¨ç€å¤šç§å±€é™ï¼Œå›°æ‰°ç€è¯¥é¢†åŸŸçš„å‘å±•ã€‚è¯„ä¼°æ•°æ®é›†å› é£æ ¼å„å¼‚ã€å½¢å¼ä¸åŒä»¥åŠè½¬å½•ä»»åŠ¡å›ºæœ‰çš„æ¨¡ç³Šæ€§è€Œå—åˆ°å›°æ‰°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡é‡‡ç”¨é£æ ¼æ— å…³çš„è¯„ä¼°æ–¹æ³•ï¼ˆä½¿ç”¨å¤šç§åœ¨ä¸åŒé£æ ¼å‚æ•°ä¸‹è½¬å½•çš„å‚è€ƒæ ‡å‡†ï¼‰æ¥å‡å°‘è¿™äº›å·®å¼‚ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„WERæŠ¥å‘Šå¯èƒ½å¤§å¤§é«˜ä¼°äº†å‰æ²¿è¯­éŸ³è¯†åˆ«ç³»ç»Ÿäº§ç”Ÿçš„å®è´¨æ€§é”™è¯¯æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å¤šå‚è€ƒæ–¹æ³•å¯¹äºæ¯”è¾ƒä¸åŒé£æ ¼çš„è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä»»åŠ¡çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è´¨é‡éå¸¸æœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºè¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä½œä¸ºè¯­éŸ³è¯†åˆ«é¢†åŸŸçš„ä¸€ä¸ªè¯„ä»·æŒ‡æ ‡å­˜åœ¨å¤šç§å±€é™æ€§ã€‚è¯„ä¼°æ•°æ®é›†åœ¨é£æ ¼ã€æ­£å¼ç¨‹åº¦ä»¥åŠè½¬å½•ä»»åŠ¡æœ¬èº«çš„æ¨¡ç³Šæ€§ä¸Šå­˜åœ¨å·®å¼‚ã€‚ä¸ºç¼“è§£è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é‡‡ç”¨å¤šç§å‚ç…§æ ‡å‡†çš„é£æ ¼æ— å…³è¯„ä¼°æ–¹æ³•å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„WERæŠ¥å‘Šå¯èƒ½é«˜ä¼°äº†å‰æ²¿ASRç³»ç»Ÿçš„å®è´¨æ€§é”™è¯¯æ•°é‡ã€‚åŒæ—¶ï¼Œå¤šå‚è€ƒæ–¹æ³•å¯¹äºæ¯”è¾ƒä¸åŒé£æ ¼è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä»»åŠ¡çš„ASRæ¨¡å‹è´¨é‡å…·æœ‰å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä½œä¸ºè¯­éŸ³è¯†åˆ«è¯„ä»·æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è¯„ä¼°æ•°æ®é›†åœ¨é£æ ¼ã€æ­£å¼ç¨‹åº¦å’Œä»»åŠ¡æ¨¡ç³Šæ€§ä¸Šå­˜åœ¨å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é‡‡ç”¨å¤šç§å‚ç…§æ ‡å‡†çš„é£æ ¼æ— å…³è¯„ä¼°æ–¹æ³•æ¥ç¼“è§£ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ç°æœ‰WERæŠ¥å‘Šå¯èƒ½é«˜ä¼°äº†å‰æ²¿ASRç³»ç»Ÿçš„å®è´¨æ€§é”™è¯¯æ•°é‡ã€‚</li>
<li>å¤šå‚è€ƒæ–¹æ³•æœ‰åŠ©äºæ¯”è¾ƒä¸åŒé£æ ¼è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä»»åŠ¡çš„ASRæ¨¡å‹è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´å‡†ç¡®çš„è¯„ä¼°ï¼Œä½¿ç ”ç©¶äººå‘˜æ›´å¥½åœ°äº†è§£ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8100aa3369042097b569553fdb91a950.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f2f8050d75582967cd02358fd1e52a26.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4d263a62158e7c69b13333453efe1157.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ebf1585cb83308d7dade17759a71ae0a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3f63a64a21ad0d44c6c915ed7876b904.jpg" align="middle">
</details>




<h2 id="Real-time-Sign-Language-Recognition-Using-MobileNetV2-and-Transfer-Learning"><a href="#Real-time-Sign-Language-Recognition-Using-MobileNetV2-and-Transfer-Learning" class="headerlink" title="Real-time Sign Language Recognition Using MobileNetV2 and Transfer   Learning"></a>Real-time Sign Language Recognition Using MobileNetV2 and Transfer   Learning</h2><p><strong>Authors:Smruti Jagtap, Kanika Jadhav, Rushikesh Temkar, Minal Deshmukh</strong></p>
<p>The hearing-impaired community in India deserves the access to tools that help them communicate, however, there is limited known technology solutions that make use of Indian Sign Language (ISL) at present. Even though there are many ISL users, ISL cannot access social and education arenas because there is not yet an efficient technology to convert the ISL signal into speech or text. We initiated this initiative owing to the rising demand for products and technologies that are inclusive and help ISL, filling the gap of communication for the ones with hearing disability. Our goal is to build an reliable sign language recognition system with the help of Convolutional Neural Networks (CNN) to . By expanding communication access, we aspire toward better educational opportunities and a more inclusive society for hearing impaired people in India. </p>
<blockquote>
<p>å°åº¦çš„å¬éšœç¾¤ä½“ç†åº”è·å¾—å¸®åŠ©ä»–ä»¬æ²Ÿé€šçš„å·¥å…·ï¼Œç„¶è€Œç›®å‰åˆ©ç”¨å°åº¦æ‰‹è¯­ï¼ˆISLï¼‰çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆå´ååˆ†æœ‰é™ã€‚å°½ç®¡æœ‰å¾ˆå¤šISLä½¿ç”¨è€…ï¼Œä½†ISLæ— æ³•è¿›å…¥ç¤¾ä¼šå’Œæ•™è‚²çš„é¢†åŸŸï¼Œå› ä¸ºæ²¡æœ‰æœ‰æ•ˆçš„æŠ€æœ¯å°†ISLä¿¡å·è½¬åŒ–ä¸ºè¯­éŸ³æˆ–æ–‡å­—ã€‚æˆ‘ä»¬å‘èµ·è¿™ä¸€å€¡è®®æ˜¯å› ä¸ºå¸‚åœºå¯¹åŒ…å®¹æ€§å’Œå¸®åŠ©ISLçš„äº§å“å’ŒæŠ€æœ¯æœ‰ç€æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œä»¥å¡«è¡¥å¬éšœäººå£«çš„æ²Ÿé€šç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å€ŸåŠ©å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å»ºç«‹å¯é çš„æ‰‹è¯­è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡æ‰©å¤§æ²Ÿé€šæ¸ é“ï¼Œæˆ‘ä»¬æœŸæœ›ä¸ºå°åº¦çš„å¬éšœäººå£«æä¾›æ›´å¥½çš„æ•™è‚²æœºä¼šå’Œæ›´å…·åŒ…å®¹æ€§çš„ç¤¾ä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07486v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>å°åº¦çš„å¬éšœç¤¾åŒºéœ€è¦èƒ½å¤Ÿè¾…åŠ©ä»–ä»¬æ²Ÿé€šçš„å·¥å…·ï¼Œä½†ç›®å‰åˆ©ç”¨å°åº¦æ‰‹è¯­ï¼ˆISLï¼‰çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆååˆ†æœ‰é™ã€‚å°½ç®¡æœ‰å¾ˆå¤šISlä½¿ç”¨è€…ï¼Œä½†ç”±äºç¼ºä¹å°†ISlä¿¡å·è½¬æ¢ä¸ºè¯­éŸ³æˆ–æ–‡å­—çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œä»–ä»¬æ— æ³•è¿›å…¥ç¤¾ä¼šå’Œæ•™è‚²çš„é¢†åŸŸã€‚æˆ‘ä»¬å‘èµ·è¿™ä¸€å€¡è®®ï¼Œæ˜¯å› ä¸ºå¯¹èƒ½å¤Ÿå¸®åŠ©å¬éšœäººå£«ä¸å¬éšœç¾¤ä½“æ²Ÿé€šçš„äº§å“å’ŒæŠ€æœ¯æœ‰ç€æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œä»¥å¡«è¡¥æ²Ÿé€šæ–¹é¢çš„ç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å€ŸåŠ©å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å»ºç«‹ä¸€ä¸ªå¯é çš„è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡æ‰©å¤§æ²Ÿé€šæ¸ é“ï¼Œæˆ‘ä»¬æœŸæœ›ä¸ºå°åº¦çš„å¬éšœäººå£«æä¾›æ›´å¥½çš„æ•™è‚²æœºä¼šå’Œæ›´å…·åŒ…å®¹æ€§çš„ç¤¾ä¼šã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¬éšœç¾¤ä½“éœ€è¦æœ‰æ•ˆæ²Ÿé€šå·¥å…·ï¼Œå°¤å…¶é’ˆå¯¹å°åº¦æ‰‹è¯­ï¼ˆISLï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰ç¼ºä¹å°†ISLè½¬åŒ–ä¸ºè¯­éŸ³æˆ–æ–‡å­—çš„æŠ€æœ¯ï¼Œé™åˆ¶äº†å¬éšœäººå£«çš„ç¤¾ä¼šå’Œæ•™è‚²å‚ä¸ã€‚</li>
<li>å€¡è®®å»ºç«‹åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å¯é æ‰‹è¯­è¯†åˆ«ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡æ‰©å¤§æ²Ÿé€šæ¸ é“ï¼Œå¸®åŠ©å¬éšœäººå£«æ›´å¥½åœ°èå…¥ç¤¾ä¼šã€‚</li>
<li>é‡è§†ä¸ºå¬éšœäººå£«æä¾›æ•™è‚²æœºä¼šï¼Œä¿ƒè¿›ç¤¾ä¼šåŒ…å®¹æ€§å¢é•¿ã€‚</li>
<li>è¯¥æŠ€æœ¯è§£å†³æ–¹æ¡ˆå¯¹äºæ¨åŠ¨ç¤¾ä¼šå…¬å¹³å’Œå¯¹å¬éšœç¾¤ä½“çš„æ”¯æŒè‡³å…³é‡è¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da4d0df39252ba738b590da304437be9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4a872f8cb8b8c0514ca9ea9e047e158a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-309235b1c989ff255c1d135e9d752368.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fed24bac77a824631d4a7b4553991f98.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4b5ac3245503cdd065d5d2f064a658d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5184640a16d47a3d979f8ac429ffb86f.jpg" align="middle">
</details>




<h2 id="Effective-Text-Adaptation-for-LLM-based-ASR-through-Soft-Prompt-Fine-Tuning"><a href="#Effective-Text-Adaptation-for-LLM-based-ASR-through-Soft-Prompt-Fine-Tuning" class="headerlink" title="Effective Text Adaptation for LLM-based ASR through Soft Prompt   Fine-Tuning"></a>Effective Text Adaptation for LLM-based ASR through Soft Prompt   Fine-Tuning</h2><p><strong>Authors:Yingyi Ma, Zhe Liu, Ozlem Kalinli</strong></p>
<p>The advent of Large Language Models (LLM) has reformed the Automatic Speech Recognition (ASR). Prompting LLM with audio embeddings to generate transcriptions becomes the new state-of-the-art ASR. Despite LLMs being trained with an extensive amount of text corpora, high-quality domain-specific text data can still significantly enhance ASR performance on domain adaptation tasks. Although LLM-based ASR can naturally incorporate more text corpora by fine-tuning the LLM decoder, fine-tuning such ASR on text-only data without paired prompts may diminish the effectiveness of domain-specific knowledge. To mitigate this issue, we propose a two-step soft prompt fine-tuning strategy that enhances domain-specific text adaptation. Experimental results show that text adaptation with our proposed method achieved a relative up to 9% Word Error Rate (WER) reduction and up to 18% Entity Error Rate (EER) reduction on the target domain compared to the baseline ASR. Combining this with domain-specific Language Model (LM) fusion can further improve the EER by a relative 2-5% </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°å·²ç»æ”¹å˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æŠ€æœ¯é¢è²Œã€‚ä½¿ç”¨éŸ³é¢‘åµŒå…¥æ¥æç¤ºLLMç”Ÿæˆè½¬å½•æœ¬å·²ç»æˆä¸ºæœ€æ–°çš„å…ˆè¿›ASRæŠ€æœ¯ã€‚å°½ç®¡LLMé€šè¿‡å¤§é‡çš„æ–‡æœ¬è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œä½†é«˜è´¨é‡çš„ä¸“ä¸šé¢†åŸŸç‰¹å®šæ–‡æœ¬æ•°æ®ä»ç„¶å¯ä»¥æ˜¾è‘—å¢å¼ºé¢†åŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸Šçš„ASRæ€§èƒ½ã€‚è™½ç„¶åŸºäºLLMçš„ASRå¯ä»¥é€šè¿‡å¾®è°ƒLLMè§£ç å™¨è‡ªç„¶åœ°èå…¥æ›´å¤šçš„æ–‡æœ¬è¯­æ–™åº“ï¼Œä½†åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æ•°æ®è¿›è¡Œå¾®è°ƒè€Œæ²¡æœ‰é…å¯¹æç¤ºçš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šé™ä½ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤æ­¥è½¯æç¤ºå¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ASRç›¸æ¯”ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¿›è¡Œçš„æ–‡æœ¬é€‚åº”ç›¸å¯¹å®ç°äº†é«˜è¾¾9%çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å‡å°‘å’Œé«˜è¾¾18%çš„å®ä½“é”™è¯¯ç‡ï¼ˆEERï¼‰å‡å°‘ã€‚å°†å…¶ä¸ç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èåˆç›¸ç»“åˆï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„EERï¼Œç›¸å¯¹æé«˜2-5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06967v1">PDF</a> accepted as SLT 2024 proceeding</p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢†åŸŸå‘ç”Ÿäº†å˜é©ã€‚ä½¿ç”¨éŸ³é¢‘åµŒå…¥æ¥æç¤ºLLMç”Ÿæˆè½¬å½•æˆä¸ºæœ€æ–°çš„ASRæŠ€æœ¯ã€‚å°½ç®¡LLMæ¥å—äº†å¤§é‡çš„æ–‡æœ¬è¯­æ–™åº“è®­ç»ƒï¼Œä½†é«˜è´¨é‡çš„ä¸“ä¸šé¢†åŸŸæ–‡æœ¬æ•°æ®ä»ç„¶å¯ä»¥æ˜¾è‘—æé«˜åŸŸé€‚åº”ä»»åŠ¡çš„ASRæ€§èƒ½ã€‚ä¸ºäº†è§£å†³åœ¨ä»…æ–‡æœ¬æ•°æ®ä¸Šå¾®è°ƒASRå¯èƒ½ä¼šé™ä½é¢†åŸŸç‰¹å®šçŸ¥è¯†æœ‰æ•ˆæ€§è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤æ­¥è½¯æç¤ºå¾®è°ƒç­–ç•¥ï¼Œä»¥å¢å¼ºç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ASRç›¸æ¯”ï¼Œä½¿ç”¨æ‰€æå‡ºçš„æ–¹æ³•åœ¨ç›®æ ‡é¢†åŸŸä¸Šå®ç°äº†ç›¸å¯¹é«˜è¾¾9%çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å‡å°‘å’Œé«˜è¾¾18%çš„å®ä½“é”™è¯¯ç‡ï¼ˆEERï¼‰å‡å°‘ã€‚ç»“åˆé¢†åŸŸç‰¹å®šçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èåˆå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„EERï¼Œç›¸å¯¹æé«˜2-5%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼•å…¥å·²ä½¿è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å‘ç”Ÿå˜é©ï¼Œä½¿ç”¨éŸ³é¢‘åµŒå…¥æç¤ºLLMç”Ÿæˆè½¬å½•æˆä¸ºæœ€æ–°æŠ€æœ¯ã€‚</li>
<li>å³ä½¿æ˜¯åœ¨LLMæ¥å—äº†å¤§é‡æ–‡æœ¬è¯­æ–™åº“è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé«˜è´¨é‡çš„ä¸“ä¸šé¢†åŸŸæ–‡æœ¬æ•°æ®ä¹Ÿèƒ½æ˜¾è‘—æé«˜ASRåœ¨åŸŸé€‚åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨ä»…æ–‡æœ¬æ•°æ®ä¸Šå¾®è°ƒASRå¯èƒ½ä¼šé™ä½é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸¤æ­¥è½¯æç¤ºå¾®è°ƒç­–ç•¥ï¼Œä»¥å¢å¼ºç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬é€‚åº”æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥ç­–ç•¥å¯å®ç°æ˜¾è‘—çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå®ä½“é”™è¯¯ç‡ï¼ˆEERï¼‰å‡å°‘ã€‚</li>
<li>ç»“åˆé¢†åŸŸç‰¹å®šçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èåˆå¯ä»¥è¿›ä¸€æ­¥æé«˜EERã€‚</li>
<li>æ•´ä½“è€Œè¨€ï¼Œæ­¤ç­–ç•¥ä¸ºæé«˜ASRæ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db6f9e81c564c2baea34172f69335291.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8502c1556cec1d4276370e422f9cbc59.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c326572579e4be46117fa0f2b7eb4235.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-90cc62091becd597b71ff060787cceaa.jpg" align="middle">
</details>




<h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹æ€§å¯¹äºé€‚åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿè‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³å¯¹åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥çš„é‡è§†ç¨‹åº¦æ—¥ç›Šå¢åŠ ï¼Œè¿™å¯èƒ½ä¼šåœ¨æ¸¸æˆå’Œè™šæ‹Ÿç°å®é¢†åŸŸæä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå®ƒç›´æ¥å°†è§†è§‰åœºæ™¯æç¤ºé›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è°ƒæ•´åˆæˆçš„æ¢…å°”é¢‘è°±å›¾ä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ‰€æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸å½±å“è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œæ ‡å¿—ç€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> ç´¢å¼•æœ¯è¯­-è¯­éŸ³åˆæˆï¼Œåœºæ™¯æç¤ºï¼Œç©ºé—´æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v2">PDF</a> The paper is missing some information</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ–‡æœ¬è½¬è¯­éŸ³åˆæˆæ–¹æ³•â€”â€”å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬è½¬è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥èå…¥åˆæˆç®¡é“ï¼Œæ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§æ··å“åˆ†ç±»ä¸ç»†åŒ–æŠ€æœ¯ï¼Œè°ƒæ•´åˆæˆé¢‘è°±å›¾ï¼Œå¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ··å“æ¡ä»¶ä¸åœºæ™¯åŒ¹é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸”ä¸å½±å“è¯­éŸ³çš„è‡ªç„¶æ€§ï¼Œæ ‡å¿—ç€è¯­å¢ƒæ„ŸçŸ¥è¯­éŸ³åˆæˆçš„é‡å¤§è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³åˆæˆä¸­æ§åˆ¶é£æ ¼å’Œç‰¹ç‚¹å¯¹äºé€‚åº”ç‰¹å®šè¯­å¢ƒå’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚</li>
<li>ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨è¯­éŸ³åˆæˆçš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ï¼Œä½†å¿½è§†äº†åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ–‡æœ¬è½¬è¯­éŸ³åˆæˆæ–¹æ³•â€”â€”I2TTSï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è§†è§‰åœºæ™¯æç¤ºã€‚</li>
<li>I2TTSé€šè¿‡åœºæ™¯æç¤ºç¼–ç å™¨æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜è¯­éŸ³åˆæˆçš„çµæ´»æ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>ä¸ºäº†å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œè®ºæ–‡æå‡ºäº†æ··å“åˆ†ç±»ä¸ç»†åŒ–æŠ€æœ¯ï¼Œè°ƒæ•´åˆæˆé¢‘è°±å›¾ï¼Œç¡®ä¿æ··å“æ¡ä»¶ä¸åœºæ™¯åŒ¹é…ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒI2TTSå®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸”ä¸å½±å“è¯­éŸ³çš„è‡ªç„¶æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-66e6ccdd517a50e9ee5dddf9637b47e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4689ec2cdbacd48202667ffe499ad66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf7fa3b4564f4f1f3a720aedb6cba728.jpg" align="middle">
</details>




<h2 id="Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening"><a href="#Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening" class="headerlink" title="Uncovering the role of semantic and acoustic cues in normal and dichotic   listening"></a>Uncovering the role of semantic and acoustic cues in normal and dichotic   listening</h2><p><strong>Authors:Akshara Soman, Sai Samrat Kankanala, Sriram Ganapathy</strong></p>
<p>Despite extensive research, the precise role of acoustic and semantic cues in complex speech perception tasks remains unclear. In this study, we propose a paradigm to understand the encoding of these cues in electroencephalogram (EEG) data, using match-mismatch (MM) classification task. The MM task involves determining whether the stimulus and response correspond to each other or not. We design a multi-modal sequence model, based on long short term memory (LSTM) architecture, to perform the MM task. The model is input with acoustic stimulus (derived from the speech envelope), semantic stimulus (derived from textual representations of the speech content), and neural response (derived from the EEG data). Our experiments are performed on two separate conditions, i) natural passive listening condition and, ii) an auditory attention based dichotic listening condition. Using the MM task as the analysis framework, we observe that - a) speech perception is fragmented based on word boundaries, b) acoustic and semantic cues offer similar levels of MM task performance in natural listening conditions, and c) semantic cues offer significantly improved MM classification over acoustic cues in dichotic listening task. Further, the study provides evidence of right ear advantage in dichotic listening conditions. </p>
<blockquote>
<p>å°½ç®¡è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œä½†å£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚çš„è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾ç¡®ä½œç”¨ä»ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®ç¼–ç è¿™äº›çº¿ç´¢çš„ç†è§£èŒƒå¼ï¼Œé‡‡ç”¨åŒ¹é…-ä¸åŒ¹é…ï¼ˆMMï¼‰åˆ†ç±»ä»»åŠ¡ã€‚MMä»»åŠ¡æ¶‰åŠç¡®å®šåˆºæ¿€ä¸å“åº”æ˜¯å¦ç›¸å¯¹åº”ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºé•¿çŸ­æ—¶è®°å¿†ï¼ˆLSTMï¼‰æ¶æ„çš„å¤šæ¨¡æ€åºåˆ—æ¨¡å‹ï¼Œä»¥æ‰§è¡ŒMMä»»åŠ¡ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥åŒ…æ‹¬å£°éŸ³åˆºæ¿€ï¼ˆæ¥æºäºè¯­éŸ³åŒ…ç»œï¼‰ã€è¯­ä¹‰åˆºæ¿€ï¼ˆæ¥æºäºè¯­éŸ³å†…å®¹çš„æ–‡æœ¬è¡¨ç¤ºï¼‰å’Œç¥ç»å“åº”ï¼ˆæ¥æºäºEEGæ•°æ®ï¼‰ã€‚æˆ‘ä»¬çš„å®éªŒåœ¨ä¸¤ä¸ªå•ç‹¬çš„æ¡ä»¶ä¸‹è¿›è¡Œï¼ši)è‡ªç„¶è¢«åŠ¨è†å¬æ¡ä»¶ï¼›ii)åŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒå¬æ¡ä»¶ã€‚ä½¿ç”¨MMä»»åŠ¡ä½œä¸ºåˆ†ææ¡†æ¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ša)è¯­éŸ³æ„ŸçŸ¥æ˜¯åŸºäºè¯è¾¹ç•Œçš„ç‰‡æ®µåŒ–ï¼›b)åœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹ï¼Œå£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨MMä»»åŠ¡ä¸­æä¾›ç›¸ä¼¼çš„è¡¨ç°ï¼›c)åœ¨äºŒå¬ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢ç›¸å¯¹äºå£°éŸ³çº¿ç´¢èƒ½æ˜¾è‘—æé«˜MMåˆ†ç±»æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¸ºäºŒå¬æ¡ä»¶ä¸‹çš„å³è€³ä¼˜åŠ¿æä¾›äº†è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11308v1">PDF</a> 9 Pages, 4 Figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†å£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾ç¡®ä½œç”¨ã€‚ç ”ç©¶é‡‡ç”¨åŒ¹é…-ä¸åŒ¹é…åˆ†ç±»ä»»åŠ¡ï¼Œä»¥ç†è§£è¿™äº›çº¿ç´¢åœ¨è„‘ç”µå›¾æ•°æ®ä¸­çš„ç¼–ç æ–¹å¼ã€‚å®éªŒè®¾è®¡äº†ä¸€ä¸ªåŸºäºLSTMæ¶æ„çš„å¤šæ¨¡å¼åºåˆ—æ¨¡å‹ï¼Œç”¨äºæ‰§è¡ŒåŒ¹é…-ä¸åŒ¹é…ä»»åŠ¡ã€‚è¯¥æ¨¡å‹ä»¥è¯­éŸ³åŒ…ç»œäº§ç”Ÿçš„å£°å­¦åˆºæ¿€ã€è¯­éŸ³å†…å®¹çš„æ–‡æœ¬è¡¨ç¤ºäº§ç”Ÿçš„è¯­ä¹‰åˆºæ¿€å’ŒEEGæ•°æ®äº§ç”Ÿçš„ç¥ç»ååº”ä¸ºè¾“å…¥ã€‚åœ¨è‡ªç„¶è¢«åŠ¨å¬è§‰æ¡ä»¶å’ŒåŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒé€‰ä¸€å¬è§‰æ¡ä»¶ä¸‹è¿›è¡Œå®éªŒã€‚è§‚å¯Ÿå‘ç°ï¼šè¯­éŸ³æ„ŸçŸ¥ä»¥å•è¯è¾¹ç•Œä¸ºåŸºç¡€è¿›è¡Œåˆ†å‰²ï¼›åœ¨è‡ªç„¶å¬è§‰æ¡ä»¶ä¸‹ï¼Œå£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢æä¾›ç±»ä¼¼çš„åŒ¹é…ä»»åŠ¡æ€§èƒ½ï¼›åœ¨äºŒé€‰ä¸€å¬è§‰ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢æ˜¾è‘—ä¼˜äºå£°å­¦çº¿ç´¢ï¼Œå¹¶ä¸”åœ¨äºŒé€‰ä¸€å¬è§‰æ¡ä»¶ä¸‹å­˜åœ¨å³è€³ä¼˜åŠ¿ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶ä½¿ç”¨åŒ¹é…-ä¸åŒ¹é…åˆ†ç±»ä»»åŠ¡æ¥æ¢ç©¶å£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚è¯­éŸ³æ„ŸçŸ¥ä¸­çš„è´¡çŒ®ã€‚</li>
<li>åŸºäºLSTMçš„å¤šæ¨¡å¼åºåˆ—æ¨¡å‹ç”¨äºæ‰§è¡ŒåŒ¹é…-ä¸åŒ¹é…ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å£°å­¦åˆºæ¿€ã€è¯­ä¹‰åˆºæ¿€å’Œç¥ç»ååº”æ•°æ®ã€‚</li>
<li>åœ¨è‡ªç„¶è¢«åŠ¨å¬è§‰æ¡ä»¶ä¸‹ï¼Œå£°å­¦çº¿ç´¢å’Œè¯­ä¹‰çº¿ç´¢åœ¨åŒ¹é…ä»»åŠ¡ä¸­çš„è¡¨ç°ç›¸ä¼¼ã€‚</li>
<li>åœ¨äºŒé€‰ä¸€å¬è§‰æ¡ä»¶ä¸‹ï¼Œè¯­ä¹‰çº¿ç´¢å¯¹åŒ¹é…-ä¸åŒ¹é…ä»»åŠ¡çš„è´¡çŒ®æ˜¾è‘—è¶…è¿‡å£°å­¦çº¿ç´¢ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯­éŸ³æ„ŸçŸ¥æ˜¯ä»¥å•è¯è¾¹ç•Œä¸ºåŸºç¡€è¿›è¡Œåˆ†å‰²çš„ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†åœ¨äºŒé€‰ä¸€å¬è§‰æ¡ä»¶ä¸‹å³è€³ä¼˜åŠ¿çš„è¯æ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-702194aa412ab66d9b55f848bb4b0802.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f88c633e521445f6a5e0c1930330554c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2caaa7973dbbc8b7feaaeb52a5006feb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-77deb2d8824993a0831777932f5b46c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9e94c411962448b979a6fa8a23923d9b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4df85715af024e597b14960f49d00b90.jpg" align="middle">
</details>




<h2 id="Interactive-Cycle-Model-â€“-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses"><a href="#Interactive-Cycle-Model-â€“-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses" class="headerlink" title="Interactive Cycle Model â€“ The Linkage Combination among Automatic   Speech Recognition, Large Language Models and Smart Glasses"></a>Interactive Cycle Model â€“ The Linkage Combination among Automatic   Speech Recognition, Large Language Models and Smart Glasses</h2><p><strong>Authors:Libo Wang</strong></p>
<p>This research proposes the interaction loop model â€œASR-LLMs-Smart Glassesâ€, which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLMs. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†â€œASR-LLMs-æ™ºèƒ½çœ¼é•œâ€äº’åŠ¨å¾ªç¯æ¨¡å‹ã€‚æ­¤æ¨¡å‹ç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œï¼Œä»¥ä¿ƒè¿›æ— ç¼çš„äººæœºäº’åŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•è®ºæ¶‰åŠå°†äº’åŠ¨è¿‡ç¨‹åˆ†è§£æˆä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ã€‚è¯­éŸ³ç”±ASRæ•è·å¹¶å¤„ç†ï¼Œç„¶åç”±LLMsè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚ç»“æœéšåä¼ è¾“åˆ°æ™ºèƒ½çœ¼é•œè¿›è¡Œæ˜¾ç¤ºã€‚å½“ç”¨æˆ·ä¸æ˜¾ç¤ºçš„æ•°æ®äº’åŠ¨æ—¶ï¼Œåé¦ˆå¾ªç¯å°±å®Œæˆäº†ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ•°å­¦å…¬å¼æ¥é‡åŒ–æ¨¡å‹çš„è¡¨ç°ï¼Œä¸»è¦å›´ç»•æ ¸å¿ƒè¯„ä¼°ç‚¹ï¼šASRè¯­éŸ³è½¬æ–‡æœ¬çš„å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿã€‚ç ”ç©¶ç»“æœåœ¨ç†è®ºä¸Šæä¾›äº†æ¨¡å‹çš„å¯è¡Œæ€§å’Œæ€§èƒ½æµ‹è¯•å’Œè¯„ä¼°ã€‚è¯¦ç»†çš„æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ è‡³Githubï¼Œé“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10362v2">PDF</a> OpenReview submitted. 10 pages of text and 2 figures</p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œASR-LLMs-æ™ºèƒ½çœ¼é•œâ€çš„äº’åŠ¨å¾ªç¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œï¼Œä»¥ä¿ƒè¿›æ— ç¼çš„äººæœºäº¤äº’ã€‚ç ”ç©¶æ–¹æ³•æ¶‰åŠå°†äº’åŠ¨è¿‡ç¨‹åˆ†è§£æˆä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ã€‚è¯­éŸ³è¢«æ•è·å¹¶ç»è¿‡ASRå¤„ç†ï¼Œç„¶åé€šè¿‡LLMsè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚ç»“æœä¼ è¾“åˆ°æ™ºèƒ½çœ¼é•œè¿›è¡Œæ˜¾ç¤ºã€‚ç”¨æˆ·ä¸æ˜¾ç¤ºçš„æ•°æ®è¿›è¡Œäº¤äº’æ—¶ï¼Œåé¦ˆå¾ªç¯å°±å®Œæˆäº†ã€‚ç ”ç©¶ç”¨æ•°å­¦å…¬å¼æ¥é‡åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå›´ç»•æ ¸å¿ƒè¯„ä¼°ç‚¹ï¼šASRè¯­éŸ³è½¬æ–‡æœ¬çš„å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿæ€§ã€‚è¯¥æ¨¡å‹çš„ç†è®ºæµ‹è¯•ç»“æœå’Œæ€§èƒ½è¯„ä¼°å·²è¯¦ç»†ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ASR-LLMs-æ™ºèƒ½çœ¼é•œäº’åŠ¨å¾ªç¯æ¨¡å‹ï¼Œæ•´åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œæŠ€æœ¯ã€‚</li>
<li>äº’åŠ¨è¿‡ç¨‹è¢«åˆ†è§£ä¸ºä¸åŒé˜¶æ®µå’Œå…ƒç´ ï¼ŒåŒ…æ‹¬è¯­éŸ³æ•è·ã€ASRå¤„ç†ã€LLMsåˆ†æå’Œç»“æœå±•ç¤ºã€‚</li>
<li>ç”¨æˆ·ä¸æ™ºèƒ½çœ¼é•œæ˜¾ç¤ºçš„æ•°æ®è¿›è¡Œäº¤äº’ï¼Œå®Œæˆåé¦ˆå¾ªç¯ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½é€šè¿‡æ•°å­¦å…¬å¼é‡åŒ–ï¼Œä¸»è¦è¯„ä¼°ç‚¹ä¸ºASRè¯­éŸ³è½¬æ–‡æœ¬çš„å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿæ€§ã€‚</li>
<li>æ¨¡å‹çš„ç†è®ºæµ‹è¯•ç»“æœå’Œæ€§èƒ½è¯„ä¼°å·²è¯¦ç»†ä¸Šä¼ è‡³GitHubï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå‚è€ƒã€‚</li>
<li>è¯¥æ¨¡å‹æœ‰åŠ©äºå®ç°æ— ç¼çš„äººæœºäº¤äº’ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg" align="middle">
</details>




<h2 id="DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions"><a href="#DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions" class="headerlink" title="DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions"></a>DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions</h2><p><strong>Authors:Shu-Tong Niu, Jun Du, Ruo-Yu Wang, Gao-Bin Yang, Tian Gao, Jia Pan, Yu Hu</strong></p>
<p>We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end speech recognition, combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries during decoding. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art single-channel speech recognition performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å•é€šé“æ·±åº¦çº§è”èåˆåˆ†æ²»ä¸åˆ†ç¦»ï¼ˆDCF-DSï¼‰åç«¯è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç¥ç»è¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨è”åˆè®­ç»ƒæ¡†æ¶å†…æŒ‰é¡ºåºæ•´åˆNSDå’ŒSSæ¨¡å—ï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆ†æ²»æ¨¡å—ä¸­çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œã€‚æ¥ç€ï¼Œä¸ºäº†è¡¥å……DCF-DSè®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çª—å£çº§è§£ç æ–¹æ¡ˆï¼Œä»¥è§£å†³æ•°æ®æ”¶æ•›ä¸ç¨³å®šçš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢ä½¿ç”¨åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„NSDç³»ç»Ÿï¼Œä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­æä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨DCF-DSæ¡†æ¶ä¸­åŠ å…¥äº†å¯é€‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°èšç±»DCF-DSè¾“å‡ºæ¥æé«˜åˆ†æ²»ç»“æœï¼Œä»è€Œæé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ã€‚é€šè¿‡é‡‡ç”¨DCF-DSæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜çš„ç°å®å•é€šé“èµ›é“ä¸Šè·å¾—ç¬¬ä¸€åã€‚æˆ‘ä»¬åœ¨å¼€æ”¾çš„LibriCSSæ•°æ®é›†ä¸Šä¹Ÿè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†æœ€æ–°çš„å•é€šé“è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å•é€šé“æ·±åº¦çº§è”èåˆåˆ†æ²»ä¸åˆ†ç¦»ï¼ˆDCF-DSï¼‰æ¡†æ¶ï¼Œç”¨äºåç«¯è¯­éŸ³è¯†åˆ«ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚é¦–å…ˆï¼Œåœ¨è”åˆè®­ç»ƒæ¡†æ¶ä¸­æŒ‰é¡ºåºæ•´åˆNSDå’ŒSSæ¨¡å—ï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½æœ‰æ•ˆåˆ©ç”¨åˆ†æ²»æ¨¡å—æä¾›çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œä¸ºè¡¥å……DCF-DSè®­ç»ƒï¼Œå¼•å…¥çª—å£çº§åˆ«è§£ç æ–¹æ¡ˆï¼Œä»¥è§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šï¼ˆSDCIï¼‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢ä½¿ç”¨åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„NSDç³»ç»Ÿï¼Œä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­æä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚æœ€åï¼Œåœ¨DCF-DSæ¡†æ¶ä¸­åŠ å…¥äº†å¯é€‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨DCF-DSæ–¹æ³•ï¼Œåœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜çš„ç°å®ä¸­å•é€šé“èµ›é“ä¸Šå–å¾—äº†ç¬¬ä¸€åï¼Œå¹¶åœ¨å¼€æ”¾çš„LibriCSSæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°çš„å•é€šé“è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å•é€šé“Deep Cascade Fusion of Diarization and Separation (DCF-DS)æ¡†æ¶ï¼Œæ•´åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒNSDå’ŒSSæ¨¡å—ï¼Œåˆ©ç”¨è¯´è¯äººæ—¶é—´è¾¹ç•Œä¿¡æ¯ã€‚</li>
<li>å¼•å…¥çª—å£çº§åˆ«è§£ç æ–¹æ¡ˆï¼Œè§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>ä½¿ç”¨çœŸå®æ•°æ®é›†è®­ç»ƒçš„NSDç³»ç»Ÿæä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œä¿¡æ¯ã€‚</li>
<li>åŠ å…¥äº†MIMO-SEæ¨¡å—ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚</li>
<li>åœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜ä¸­å–å¾—ç¬¬ä¸€åã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2c1f133fe48629ed42ebefd43e4198ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-50b1be278625e9ff5f56e1bc19f5ff48.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5e9f3bb5b76e7725e9c9d2c4a617f5a6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a249e24e14206090f21ae688491614b2.jpg" align="middle">
</details>




<h2 id="Dialectal-Coverage-And-Generalization-in-Arabic-Speech-Recognition"><a href="#Dialectal-Coverage-And-Generalization-in-Arabic-Speech-Recognition" class="headerlink" title="Dialectal Coverage And Generalization in Arabic Speech Recognition"></a>Dialectal Coverage And Generalization in Arabic Speech Recognition</h2><p><strong>Authors:Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki</strong></p>
<p>Developing robust automatic speech recognition (ASR) systems for Arabic, a language characterized by its rich dialectal diversity and often considered a low-resource language in speech technology, demands effective strategies to manage its complexity. This study explores three critical factors influencing ASR performance: the role of dialectal coverage in pre-training, the effectiveness of dialect-specific fine-tuning compared to a multi-dialectal approach, and the ability to generalize to unseen dialects. Through extensive experiments across different dialect combinations, our findings offer key insights towards advancing the development of ASR systems for pluricentric languages like Arabic. </p>
<blockquote>
<p>å¼€å‘é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„ç¨³å¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œé˜¿æ‹‰ä¼¯è¯­ä»¥å…¶ä¸°å¯Œçš„æ–¹è¨€å¤šæ ·æ€§è€Œè‘—ç§°ï¼Œä¸”åœ¨è¯­éŸ³æŠ€æœ¯ä¸­å¸¸è¢«è§†ä¸ºèµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œéœ€è¦æœ‰æ•ˆçš„ç­–ç•¥æ¥ç®¡ç†å…¶å¤æ‚æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å½±å“ASRæ€§èƒ½çš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šé¢„è®­ç»ƒä¸­çš„æ–¹è¨€è¦†ç›–ä½œç”¨ã€ä¸å¤šæ–¹è¨€æ–¹æ³•ç›¸æ¯”é’ˆå¯¹ç‰¹å®šæ–¹è¨€çš„å¾®è°ƒçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ¨å¹¿åˆ°æœªè§æ–¹è¨€çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒæ–¹è¨€ç»„åˆçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºæ¨è¿›é’ˆå¯¹é˜¿æ‹‰ä¼¯ç­‰å¤šå…ƒä¸­å¿ƒè¯­è¨€å‘å±•ASRç³»ç»Ÿæä¾›äº†å…³é”®è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05872v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å½±å“é˜¿æ‹‰ä¼¯è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ€§èƒ½çš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šé¢„è®­ç»ƒä¸­çš„æ–¹è¨€è¦†ç›–ä½œç”¨ã€é’ˆå¯¹æ–¹è¨€çš„å¾®è°ƒä¸å¤šæ–¹è¨€æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ¨å¹¿è‡³æœªè§æ–¹è¨€çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒæ–¹è¨€ç»„åˆçš„å¤§é‡å®éªŒï¼Œæœ¬ç ”ç©¶ä¸ºé˜¿æ‹‰ä¼¯è¯­ç­‰å¤šä¸­å¿ƒè¯­è¨€ASRç³»ç»Ÿçš„å‘å±•æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯è¯­ä½œä¸ºä½èµ„æºè¯­è¨€åœ¨è¯­éŸ³æŠ€æœ¯ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>é¢„è®­ç»ƒä¸­æ–¹è¨€è¦†ç›–å¯¹ASRæ€§èƒ½çš„å½±å“ã€‚</li>
<li>æ–¹è¨€ç‰¹å®šå¾®è°ƒä¸å¤šæ–¹è¨€æ–¹æ³•çš„æœ‰æ•ˆæ€§æ¯”è¾ƒã€‚</li>
<li>æ¨å¹¿åˆ°æœªè§æ–¹è¨€çš„èƒ½åŠ›å¯¹ASRç³»ç»Ÿçš„é‡è¦æ€§ã€‚</li>
<li>å¤§é‡å®éªŒè¯æ˜ä¸åŒæ–¹è¨€ç»„åˆä¸‹çš„å…³é”®æ´å¯Ÿã€‚</li>
<li>å¯¹å‘å±•å¤šä¸­å¿ƒè¯­è¨€å¦‚é˜¿æ‹‰ä¼¯è¯­çš„ASRç³»ç»Ÿçš„å¯ç¤ºã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºæå‡ASRç³»ç»Ÿæ€§èƒ½æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2419d1ac438dcd757d023f4e59c4be1b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13fc695d00050799ec88e2bedaddff8d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8363b36a0c1b877eada0c3e37a8cec6b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5230d2c6b2b5e6c1a4b718729bb8ecb3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5cf328d5548f4ac8577c0c6eb782f612.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-03ac340c8eb3b576cbe9d0982638bc3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ed00c8b2f71f9870839b176c8daffc70.jpg" align="middle">
</details>




<h2 id="VoiceBench-Benchmarking-LLM-Based-Voice-Assistants"><a href="#VoiceBench-Benchmarking-LLM-Based-Voice-Assistants" class="headerlink" title="VoiceBench: Benchmarking LLM-Based Voice Assistants"></a>VoiceBench: Benchmarking LLM-Based Voice Assistants</h2><p><strong>Authors:Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li</strong></p>
<p>Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸï¼Œæœ€è¿‘çš„è¿›å±•ï¼Œå¦‚GPT-4oï¼Œå·²ç»èƒ½å¤Ÿé€šè¿‡LLMè¯­éŸ³åŠ©æ‰‹å®ç°å®æ—¶è¯­éŸ³äº¤äº’ï¼Œä¸åŸºäºæ–‡æœ¬çš„ä¼ ç»Ÿäº¤äº’æ–¹å¼ç›¸æ¯”ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ˜¾è‘—æ”¹å–„çš„ä½“éªŒã€‚ç„¶è€Œï¼Œç¼ºä¹ç”¨äºè¯„ä¼°è¿™äº›è¯­éŸ³äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†LLMè¯­éŸ³åŠ©æ‰‹çš„å‘å±•ã€‚å½“å‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ¸…æ´è¯­éŸ³çš„ä¸€èˆ¬çŸ¥è¯†è¯„ä¼°ä¸Šï¼Œå¿½è§†äº†æ¶‰åŠå¤šç§è¯´è¯äººç‰¹å¾ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ çš„æ›´å¤æ‚ã€çœŸå®çš„ç°å®ä¸–ç•Œåœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoiceBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨æä¾›LLMè¯­éŸ³åŠ©æ‰‹å¤šæ–¹é¢è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ã€‚VoiceBenchè¿˜åŒ…æ‹¬çœŸå®å’Œåˆæˆè¯­éŸ³æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤ç»“åˆäº†ä¸Šè¿°ä¸‰ä¸ªå…³é”®ç°å®ä¸–ç•Œçš„å˜å¼‚å› ç´ ã€‚å¤§é‡å®éªŒæ­ç¤ºäº†å½“å‰LLMè¯­éŸ³åŠ©æ‰‹æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17196v3">PDF</a> Work in progress. Data is available at   <a target="_blank" rel="noopener" href="https://github.com/MatthewCYM/VoiceBench">https://github.com/MatthewCYM/VoiceBench</a></p>
<p><strong>Summary</strong>ï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸï¼ŒGPT-4oç­‰æœ€æ–°è¿›å±•é€šè¿‡LLMè¯­éŸ³åŠ©æ‰‹å®ç°äº†å®æ—¶è¯­éŸ³äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œç¼ºä¹è¯„ä¼°è¿™äº›è¯­éŸ³äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†LLMè¯­éŸ³åŠ©æ‰‹çš„å‘å±•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoiceBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºLLMè¯­éŸ³åŠ©æ‰‹æä¾›å¤šæ–¹é¢è¯„ä¼°çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«çœŸå®å’Œåˆæˆè¯­éŸ³æŒ‡ä»¤ï¼Œæ¶µç›–çœŸå®ä¸–ç•Œä¸­çš„å¤šç§è¯´è¯äººç‰¹å¾ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ ã€‚å®éªŒæ­ç¤ºäº†å½“å‰LLMè¯­éŸ³åŠ©æ‰‹æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œå‘å±•æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsçš„è¿›æ­¥æ¨åŠ¨äº†å®æ—¶è¯­éŸ³äº¤äº’çš„å‘å±•ï¼Œé€šè¿‡LLMè¯­éŸ³åŠ©æ‰‹æå‡äº†ç”¨æˆ·ä½“éªŒã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°LLMè¯­éŸ³åŠ©æ‰‹äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VoiceBenchæ˜¯é¦–ä¸ªä¸ºLLMè¯­éŸ³åŠ©æ‰‹æä¾›å¤šæ–¹é¢è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VoiceBenchåŒ…å«çœŸå®å’Œåˆæˆçš„è¯­éŸ³æŒ‡ä»¤ï¼Œæ¶µç›–å¤šç§çœŸå®ä¸–ç•Œçš„è¯´è¯äººç‰¹å¾ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ ã€‚</li>
<li>ç°æœ‰LLMè¯­éŸ³åŠ©æ‰‹æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>é€šè¿‡å¯¹VoiceBenchçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†è¿™äº›å±€é™æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-69ae0a5443de6281affd9aaaa8657b10.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-86de805d5a4c3dd28764ed73475c70f7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bc8f15069422afe64369d4abe5a0a4d9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-15a9acc59b778b744382cabc12652f69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-20de57dd7f623666968b886a207ce214.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-95b1aa211796e45875eb98816b841157.jpg" align="middle">
</details>




<h2 id="Multi-Level-Speaker-Representation-for-Target-Speaker-Extraction"><a href="#Multi-Level-Speaker-Representation-for-Target-Speaker-Extraction" class="headerlink" title="Multi-Level Speaker Representation for Target Speaker Extraction"></a>Multi-Level Speaker Representation for Target Speaker Extraction</h2><p><strong>Authors:Ke Zhang, Junjie Li, Shuai Wang, Yangjie Wei, Yi Wang, Yannan Wang, Haizhou Li</strong></p>
<p>Target speaker extraction (TSE) relies on a reference cue of the target to extract the target speech from a speech mixture. While a speaker embedding is commonly used as the reference cue, such embedding pre-trained with a large number of speakers may suffer from confusion of speaker identity. In this work, we propose a multi-level speaker representation approach, from raw features to neural embeddings, to serve as the speaker reference cue. We generate a spectral-level representation from the enrollment magnitude spectrogram as a raw, low-level feature, which significantly improves the modelâ€™s generalization capability. Additionally, we propose a contextual embedding feature based on cross-attention mechanisms that integrate frame-level embeddings from a pre-trained speaker encoder. By incorporating speaker features across multiple levels, we significantly enhance the performance of the TSE model. Our approach achieves a 2.74 dB improvement and a 4.94% increase in extraction accuracy on Libri2mix test set over the baseline. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä¾èµ–äºç›®æ ‡çš„å‚è€ƒçº¿ç´¢ï¼Œä»è¯­éŸ³æ··åˆä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚è™½ç„¶å¸¸ç”¨è¯´è¯äººåµŒå…¥ä½œä¸ºå‚è€ƒçº¿ç´¢ï¼Œä½†è¿™ç§ä½¿ç”¨å¤§é‡è¯´è¯äººé¢„è®­ç»ƒçš„åµŒå…¥å¯èƒ½ä¼šå¯¼è‡´è¯´è¯äººèº«ä»½æ··æ·†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡çš„è¯´è¯äººè¡¨ç¤ºæ–¹æ³•ï¼Œä»åŸå§‹ç‰¹å¾åˆ°ç¥ç»åµŒå…¥ï¼Œä½œä¸ºè¯´è¯äººå‚è€ƒçº¿ç´¢ã€‚æˆ‘ä»¬ä»æ³¨å†Œå¹…åº¦è°±å›¾ä¸­ç”Ÿæˆé¢‘è°±çº§è¡¨ç¤ºä½œä¸ºåŸå§‹çš„ä½çº§ç‰¹å¾ï¼Œè¿™å¤§å¤§æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šæ–‡åµŒå…¥ç‰¹å¾ï¼Œèåˆäº†é¢„è®­ç»ƒè¯´è¯äººç¼–ç å™¨çš„å¸§çº§åµŒå…¥ã€‚é€šè¿‡ç»“åˆå¤šä¸ªå±‚æ¬¡çš„è¯´è¯äººç‰¹å¾ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†TSEæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Libri2mixæµ‹è¯•é›†ä¸Šç›¸å¯¹äºåŸºçº¿å®ç°äº†2.74 dBçš„æ”¹è¿›å’Œ4.94%çš„æå–å‡†ç¡®åº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16059v2">PDF</a> 5 pages. Submitted to ICASSP 2025. Implementation will be released at   <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wesep">https://github.com/wenet-e2e/wesep</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰æŠ€æœ¯ä¸­çš„é—®é¢˜ï¼Œå³åˆ©ç”¨å‚è€ƒçº¿ç´¢ä»è¯­éŸ³æ··åˆç‰©ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚è™½ç„¶å¸¸ç”¨çš„å‚è€ƒçº¿ç´¢æ˜¯è¯´è¯äººåµŒå…¥ï¼Œä½†ä½¿ç”¨å¤§é‡è¯´è¯è€…è¿›è¡Œé¢„è®­ç»ƒçš„åµŒå…¥å¯èƒ½å¯¼è‡´è¯´è¯äººèº«ä»½æ··æ·†çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡çš„è¯´è¯äººè¡¨ç¤ºæ–¹æ³•ï¼Œä»åŸå§‹ç‰¹å¾åˆ°ç¥ç»åµŒå…¥ï¼Œä½œä¸ºè¯´è¯äººçš„å‚è€ƒçº¿ç´¢ã€‚æˆ‘ä»¬ä»æ³¨å†Œå¹…åº¦è°±å›¾ä¸­ç”Ÿæˆé¢‘è°±çº§åˆ«çš„è¡¨ç¤ºä½œä¸ºåŸå§‹çš„ä½çº§åˆ«ç‰¹å¾ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šæ–‡åµŒå…¥ç‰¹å¾ï¼Œè¯¥ç‰¹å¾èåˆäº†é¢„è®­ç»ƒè¯´è¯äººç¼–ç å™¨çš„å¸§çº§åˆ«åµŒå…¥ã€‚é€šè¿‡ç»“åˆå¤šä¸ªå±‚æ¬¡çš„è¯´è¯äººç‰¹å¾ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†TSEæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨Libri2mixæµ‹è¯•é›†ä¸Šï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†2.74 dBçš„æ”¹è¿›å’Œ4.94%çš„æå–ç²¾åº¦æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰æŠ€æœ¯ä½¿ç”¨å‚è€ƒçº¿ç´¢ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚</li>
<li>è¯´è¯äººåµŒå…¥å¸¸ç”¨ä½œå‚è€ƒçº¿ç´¢ï¼Œä½†å¯èƒ½å­˜åœ¨å¤§é‡è¯´è¯äººçš„é¢„è®­ç»ƒåµŒå…¥å¯¼è‡´çš„èº«ä»½æ··æ·†é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡çš„è¯´è¯äººè¡¨ç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä»åŸå§‹ç‰¹å¾ï¼ˆå¦‚é¢‘è°±çº§åˆ«è¡¨ç¤ºï¼‰åˆ°ç¥ç»åµŒå…¥çš„ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯ã€‚</li>
<li>é¢‘è°±çº§åˆ«çš„è¡¨ç¤ºæ˜¯é€šè¿‡æ³¨å†Œå¹…åº¦è°±å›¾ç”Ÿæˆçš„ï¼Œè¿™æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šæ–‡åµŒå…¥ç‰¹å¾ï¼Œèåˆäº†å¸§çº§åˆ«çš„åµŒå…¥ä¿¡æ¯ã€‚</li>
<li>ç»“åˆå¤šä¸ªå±‚æ¬¡çš„è¯´è¯äººç‰¹å¾æ˜¾è‘—æé«˜äº†TSEæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4591c50d6ecf92a22e2e31de06235684.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4445d1ed24eff7a383b1bfed45d962c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ad4cd00d97f928c7bc7d510abcb172ab.jpg" align="middle">
</details>




<h2 id="CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition"><a href="#CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition" class="headerlink" title="CR-CTC: Consistency regularization on CTC for improved speech   recognition"></a>CR-CTC: Consistency regularization on CTC for improved speech   recognition</h2><p><strong>Authors:Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey</strong></p>
<p>Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC. It significantly improves the CTC performance, achieving state-of-the-art results comparable to those attained by transducer or systems combining CTC and attention-based encoder-decoder (CTC&#x2F;AED). We release our code at \url{<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%7D">https://github.com/k2-fsa/icefall}</a>. </p>
<blockquote>
<p>è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–¹æ³•ï¼Œä»¥å…¶ç®€å•æ€§å’Œè®¡ç®—æ•ˆç‡è€Œé—»åã€‚ç„¶è€Œï¼Œå®ƒåœ¨è¯†åˆ«æ€§èƒ½ä¸Šå¸¸å¸¸è¡¨ç°ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€è‡´æ€§æ­£åˆ™åŒ–CTCï¼ˆCR-CTCï¼‰ï¼Œå®ƒå¼ºåˆ¶è¾“å…¥è¯­éŸ³æ¢…å°”é¢‘è°±å›¾çš„ä¸¤ä¸ªä¸åŒå¢å¼ºè§†å›¾æ‰€è·å¾—çš„ä¸¤ä¸ªCTCåˆ†å¸ƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬ä»ä¸‰ä¸ªè§’åº¦å¯¹å…¶æ ¸å¿ƒè¡Œä¸ºè¿›è¡Œäº†æ·±å…¥äº†è§£ï¼š1ï¼‰å®ƒåœ¨å¤„ç†ä¸åŒå¢å¼ºè§†å›¾çš„éšæœºå­æ¨¡å‹å¯¹ä¹‹é—´è¿›è¡Œè‡ªæˆ‘è’¸é¦ï¼›2ï¼‰å®ƒé€šè¿‡æ©ç é¢„æµ‹å­¦ä¹ æ—¶é—´æ©ç åŒºåŸŸå†…ä½ç½®çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯å½“æˆ‘ä»¬å¢åŠ æ—¶é—´æ©ç çš„æ•°é‡æ—¶ï¼›3ï¼‰å®ƒæŠ‘åˆ¶äº†è¿‡äºå°–é”çš„CTCåˆ†å¸ƒï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨LibriSpeechã€Aishell-1å’ŒGigaSpeechæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„CR-CTCçš„æœ‰æ•ˆæ€§ã€‚å®ƒæ˜¾è‘—æé«˜äº†CTCçš„æ€§èƒ½ï¼Œå®ç°äº†ä¸è½¬æ¢å™¨æˆ–ç»“åˆCTCå’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ï¼ˆCTC&#x2F;AEDï¼‰çš„ç³»ç»Ÿæ‰€å–å¾—çš„ç»“æœç›¸åª²çš„å›½å®¶å‰æ²¿æ°´å¹³ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/k2-fsa/icefallä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05101v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºä¸€ç§åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–çš„CTCï¼ˆCR-CTCï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸åŒå¢å¼ºè§†å›¾ä¸­è·å¾—CTCåˆ†å¸ƒå¹¶å¼ºåˆ¶ä¸€è‡´æ€§ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªæˆ‘è’¸é¦ã€å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºå’ŒæŠ‘åˆ¶æç«¯å³°å€¼CTCåˆ†å¸ƒç­‰è¡Œä¸ºæ¥æ”¹å–„CTCçš„ç¼ºé™·ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCR-CTCæ˜¾è‘—æé«˜CTCæ€§èƒ½ï¼Œè¾¾åˆ°ä¸è½¬æ¢å™¨æˆ–CTCä¸åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ç»“åˆçš„ç³»ç»Ÿç›¸å½“çš„æœ€å…ˆè¿›ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CR-CTCæ–¹æ³•é€šè¿‡å¼ºåˆ¶åœ¨ä¸åŒå¢å¼ºè§†å›¾ä¸­è·å¾—çš„CTCåˆ†å¸ƒä¸€è‡´æ€§æ¥æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>CR-CTCé‡‡ç”¨è‡ªæˆ‘è’¸é¦æŠ€æœ¯ï¼Œå¤„ç†ä¸åŒå¢å¼ºè§†å›¾æ—¶çš„éšæœºå­æ¨¡å‹å¯¹ã€‚</li>
<li>CR-CTCé€šè¿‡å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºæ¥æ”¹å–„CTCçš„ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢åŠ æ—¶é—´æ©è”½é‡æ—¶ã€‚</li>
<li>CR-CTCé€šè¿‡æŠ‘åˆ¶æç«¯å³°å€¼CTCåˆ†å¸ƒæ¥å‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨LibriSpeechã€Aishell-1å’ŒGigaSpeechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜CR-CTCçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>CR-CTCæ˜¾è‘—æé«˜äº†CTCçš„æ€§èƒ½ï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸è½¬æ¢å™¨æˆ–ç»“åˆCTCå’Œæ³¨æ„åŠ›ç¼–ç å™¨è§£ç å™¨çš„ç³»ç»Ÿç›¸å½“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-973348c54f2da3118c75e78867354959.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5e34519caf7b30b4d5d9b5322f289e7a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e1ec9bc930d4c1997854dfb61b3d0882.jpg" align="middle">
</details>




<h2 id="A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation"><a href="#A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation" class="headerlink" title="A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation"></a>A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation</h2><p><strong>Authors:Jingyuan Wang, Jie Zhang, Shihao Chen, Miao Sun</strong></p>
<p>Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under various noise conditions, but with a much lower computational cost and a better SCP. The reproducible code and audio examples are available at <a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN">https://github.com/jywanng/LBCCN</a>. </p>
<blockquote>
<p>åŒè€³è¯­éŸ³å¢å¼ºï¼ˆBSEï¼‰æ—¨åœ¨è”åˆæ”¹å–„å¬åŠ›è®¾å¤‡æ¥æ”¶åˆ°çš„å¸¦å™ªè¯­éŸ³çš„è´¨é‡å’Œæ¸…æ™°åº¦ï¼Œå¹¶ä¿ç•™ç›®æ ‡è¯­éŸ³çš„ç©ºé—´çº¿ç´¢ä»¥å®ç°è‡ªç„¶å¬æ„Ÿã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸åœ¨é™å™ªï¼ˆNRï¼‰èƒ½åŠ›å’Œç©ºé—´çº¿ç´¢ä¿ç•™ï¼ˆSCPï¼‰å‡†ç¡®æ€§ä¹‹é—´æœ‰æ‰€å¦¥åï¼Œå¹¶ä¸”åœ¨å¤æ‚å£°åœºç¯å¢ƒä¸­è®¡ç®—éœ€æ±‚è¾ƒé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡çº§åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ï¼Œå®ƒé€šè¿‡è¿‡æ»¤ä½é¢‘å¸¦å®ç°å‡ºè‰²çš„é™å™ªæ•ˆæœå¹¶ä¿æŒå…¶ä½™éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜ç¡®åœ°ç»“åˆäº†é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°çš„ä¼°è®¡ï¼Œä»¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸåº¦å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§å™ªå£°æ¡ä»¶ä¸‹ï¼Œæ‰€æå‡ºçš„LBCCNçš„é™å™ªæ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ï¼ŒSCPæ€§èƒ½æ›´å¥½ã€‚å¯å¤ç”¨çš„ä»£ç å’ŒéŸ³é¢‘ç¤ºä¾‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jywanng/LBCCNæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åŒè€³è¯­éŸ³å¢å¼ºï¼ˆBSEï¼‰æŠ€æœ¯çš„ç›®æ ‡åŠå…¶å­˜åœ¨çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡çº§åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ã€‚è¯¥ç½‘ç»œé€šè¿‡è¿‡æ»¤ä½é¢‘å¸¦å¹¶ä¿ç•™å…¶ä½™éƒ¨åˆ†å®ç°å‡ºè‰²çš„é™å™ªæ€§èƒ½ï¼Œå¹¶é€šè¿‡ä¼°è®¡é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°æ¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸæ€§å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„é™å™ªæ–¹æ³•ç›¸æ¯”ï¼ŒLBCCNåœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹å®ç°äº†å‡ºè‰²çš„é™å™ªæ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œæ›´å¥½çš„ç©ºé—´çº¿ç´¢ä¿ç•™èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’ŒéŸ³é¢‘ç¤ºä¾‹å¯åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒè€³è¯­éŸ³å¢å¼ºï¼ˆBSEï¼‰æŠ€æœ¯æ—¨åœ¨æé«˜å™ªå£°ä¿¡å·çš„è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦ï¼ŒåŒæ—¶ä¿ç•™ç›®æ ‡çš„ç©ºé—´çº¿ç´¢ä»¥å®ç°è‡ªç„¶è†å¬ä½“éªŒã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´åœ¨é™å™ªï¼ˆNRï¼‰èƒ½åŠ›ã€ç©ºé—´çº¿ç´¢ä¿ç•™ï¼ˆSCPï¼‰å‡†ç¡®æ€§å’Œé«˜è®¡ç®—éœ€æ±‚ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡çº§åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LBCCNé€šè¿‡è¿‡æ»¤ä½é¢‘å¸¦å®ç°å‡ºè‰²çš„é™å™ªæ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™é«˜é¢‘éƒ¨åˆ†ä»¥ä¿æŒè¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>LBCCNé€šè¿‡ä¼°è®¡é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°æ¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸæ€§ã€‚</li>
<li>LBCCNåœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹å®ç°äº†å‡ºè‰²çš„é™å™ªæ€§èƒ½ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸å½“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7310f39e4f46abd5e4a8dab4831ac5b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0dc96b1ca49689a0be21d73c04ec780.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-94f01c1e0f4cf7074ca80b5d6147b6b5.jpg" align="middle">
</details>




<h2 id="Resource-Efficient-Adaptation-of-Speech-Foundation-Models-for-Multi-Speaker-ASR"><a href="#Resource-Efficient-Adaptation-of-Speech-Foundation-Models-for-Multi-Speaker-ASR" class="headerlink" title="Resource-Efficient Adaptation of Speech Foundation Models for   Multi-Speaker ASR"></a>Resource-Efficient Adaptation of Speech Foundation Models for   Multi-Speaker ASR</h2><p><strong>Authors:Weiqing Wang, Kunal Dhawan, Taejin Park, Krishna C. Puvvada, Ivan Medennikov, Somshubra Majumdar, He Huang, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Speech foundation models have achieved state-of-the-art (SoTA) performance across various tasks, such as automatic speech recognition (ASR) in hundreds of languages. However, multi-speaker ASR remains a challenging task for these models due to data scarcity and sparsity. In this paper, we present approaches to enable speech foundation models to process and understand multi-speaker speech with limited training data. Specifically, we adapt a speech foundation model for the multi-speaker ASR task using only telephonic data. Remarkably, the adapted model also performs well on meeting data without any fine-tuning, demonstrating the generalization ability of our approach. We conduct several ablation studies to analyze the impact of different parameters and strategies on model performance. Our findings highlight the effectiveness of our methods. Results show that less parameters give better overall cpWER, which, although counter-intuitive, provides insights into adapting speech foundation models for multi-speaker ASR tasks with minimal annotated data. </p>
<blockquote>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹å·²åœ¨å„ç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚åœ¨æ•°ç™¾ç§è¯­è¨€ä¸­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œç¨€ç–ï¼Œå¤šè¯´è¯è€…ASRå¯¹è¿™äº›æ¨¡å‹æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿è¯­éŸ³åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸Šå¤„ç†å’Œç†è§£å¤šè¯´è¯è€…è¯­éŸ³çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åªä½¿ç”¨ç”µè¯æ•°æ®æ¥é€‚åº”å¤šè¯´è¯è€…ASRä»»åŠ¡çš„è¯­éŸ³åŸºç¡€æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€‚åº”çš„æ¨¡å‹åœ¨ä¼šè®®æ•°æ®ä¸Šæ— éœ€ä»»ä½•å¾®è°ƒä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›è¡Œäº†å‡ æ¬¡æ¶ˆæº¶ç ”ç©¶ï¼Œåˆ†æä¸åŒå‚æ•°å’Œç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¾ƒå°‘çš„å‚æ•°èƒ½æä¾›æ›´å¥½çš„æ•´ä½“cpWERï¼Œè™½ç„¶è¿™æœ‰ç‚¹åç›´è§‰ï¼Œä½†ä¸ºæˆ‘ä»¬æä¾›äº†åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹é€‚åº”å¤šè¯´è¯è€…ASRä»»åŠ¡çš„è¯­éŸ³åŸºç¡€æ¨¡å‹çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01438v2">PDF</a> Accepted by SLT 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œå¹¶é’ˆå¯¹å¤šè¯´è¯è€…ASRä»»åŠ¡é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç³»åˆ—é€‚åº”è¯­éŸ³åŸºç¡€æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä½¿ç”¨ç”µè¯æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å’Œç†è§£å¤šè¯´è¯è€…çš„è¯­éŸ³ï¼Œç”šè‡³åœ¨æœªç»å¾®è°ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½åœ¨ä¼šè®®æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè¾ƒå°‘çš„å‚æ•°èƒ½å¤Ÿå¸¦æ¥æ›´å¥½çš„æ•´ä½“cpWERæ€§èƒ½ï¼Œè¿™ä¸ºåœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸‹é€‚åº”å¤šè¯´è¯è€…ASRä»»åŠ¡çš„è¯­éŸ³åŸºç¡€æ¨¡å‹æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åŸºç¡€æ¨¡å‹å·²åœ¨å¤šç§è¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>å¤šè¯´è¯è€…ASRä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ•°æ®ç¨€ç¼ºå’Œç¨€ç–æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ç”µè¯æ•°æ®è®­ç»ƒï¼Œå¯ä»¥ä½¿è¯­éŸ³åŸºç¡€æ¨¡å‹é€‚åº”å¤šè¯´è¯è€…ASRä»»åŠ¡ã€‚</li>
<li>é€‚åº”åçš„æ¨¡å‹åœ¨ä¼šè®®æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒã€‚</li>
<li>ç ”ç©¶å‘ç°è¾ƒå°‘çš„å‚æ•°èƒ½å¸¦æ¥æ›´å¥½çš„æ•´ä½“cpWERæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4e36ba4275e7af7d573f54800905876e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ce2f870fbab0b62b3007dcf01150d569.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-80a25913d424de691d8a992e7306ad81.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aec25fbde60f27d7f855ba26596de064.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c27a4c662f0a810b957be4bf27d079d6.jpg" align="middle">
</details>




<h2 id="Integrating-audiological-datasets-via-federated-merging-of-Auditory-Profiles"><a href="#Integrating-audiological-datasets-via-federated-merging-of-Auditory-Profiles" class="headerlink" title="Integrating audiological datasets via federated merging of Auditory   Profiles"></a>Integrating audiological datasets via federated merging of Auditory   Profiles</h2><p><strong>Authors:Samira Saak, Dirk Oetting, Birger Kollmeier, Mareike Buhl</strong></p>
<p>Audiological datasets contain valuable knowledge about hearing loss in patients, which can be uncovered using data-driven, federated learning techniques. Our previous approach summarized patient information from one audiological dataset into distinct Auditory Profiles (APs). To obtain a better estimate of the audiological patient population, however, patient patterns must be analyzed across multiple, separated datasets, and finally, be integrated into a combined set of APs.   This study aimed at extending the existing profile generation pipeline with an AP merging step, enabling the combination of APs from different datasets based on their similarity across audiological measures. The 13 previously generated APs (NA&#x3D;595) were merged with 31 newly generated APs from a second dataset (NB&#x3D;1272) using a similarity score derived from the overlapping densities of common features across the two datasets. To ensure clinical applicability, random forest models were created for various scenarios, encompassing different combinations of audiological measures.   A new set with 13 combined APs is proposed, providing separable profiles, which still capture detailed patient information from various test outcome combinations. The classification performance across these profiles is satisfactory. The best performance was achieved using a combination of loudness scaling, audiogram and speech test information, while single measures performed worst.   The enhanced profile generation pipeline demonstrates the feasibility of combining APs across datasets, which should generalize to all datasets and could lead to an interpretable global profile set in the future. The classification models maintain clinical applicability. </p>
<blockquote>
<p>å¬åŠ›æ•°æ®é›†ä¸­åŒ…å«äº†å…³äºæ‚£è€…å¬åŠ›æŸå¤±çš„é‡è¦çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯ä»¥é€šè¿‡æ•°æ®é©±åŠ¨å’Œè”é‚¦å­¦ä¹ æŠ€æœ¯æ¥å‘ç°ã€‚æˆ‘ä»¬ä¹‹å‰çš„æ–¹æ³•å°†ä¸€ä½æ‚£è€…çš„ä¿¡æ¯æ±‡æ€»ä¸ºä¸€ä¸ªç‹¬ç‰¹çš„å¬è§‰ç‰¹å¾é›†ï¼ˆAPï¼‰ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´å¥½åœ°ä¼°ç®—å¬åŠ›æ‚£è€…ç¾¤ä½“ï¼Œå¿…é¡»åˆ†æå¤šä¸ªç‹¬ç«‹æ•°æ®é›†çš„æ‚£è€…æ¨¡å¼ï¼Œå¹¶æœ€ç»ˆå°†å…¶æ•´åˆä¸ºä¸€ç»„ç»¼åˆçš„å¬è§‰ç‰¹å¾é›†ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å°†ç°æœ‰çš„ç‰¹å¾ç”Ÿæˆç®¡é“æ‰©å±•åˆ°å…·æœ‰APåˆå¹¶æ­¥éª¤çš„åŠŸèƒ½ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ ¹æ®è·¨å¬åŠ›æµ‹è¯•çš„ç›¸ä¼¼æ€§å°†æ¥è‡ªä¸åŒæ•°æ®é›†çš„APç»„åˆåœ¨ä¸€èµ·ã€‚ä½¿ç”¨ä»ä¸¤ä¸ªæ•°æ®é›†ä¸­å¸¸è§ç‰¹å¾çš„é‡å å¯†åº¦å¾—å‡ºçš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œå°†å…ˆå‰ç”Ÿæˆçš„13ä¸ªå¬è§‰ç‰¹å¾é›†ï¼ˆNA&#x3D;595ï¼‰ä¸æ–°ç”Ÿæˆçš„æ¥è‡ªç¬¬äºŒæ•°æ®é›†çš„31ä¸ªå¬è§‰ç‰¹å¾é›†ï¼ˆNB&#x3D;1272ï¼‰åˆå¹¶ã€‚ä¸ºç¡®ä¿ä¸´åºŠé€‚ç”¨æ€§ï¼Œé’ˆå¯¹åŒ…å«ä¸åŒå¬åŠ›æµ‹è¯•ç»„åˆçš„å„ç§åœºæ™¯åˆ›å»ºäº†éšæœºæ£®æ—æ¨¡å‹ã€‚æå‡ºäº†ä¸€å¥—åŒ…å«æœ‰é‰´åˆ«åŠ›çš„åä¸‰ç§ç»¼åˆå¬è§‰ç‰¹å¾é›†ï¼Œèƒ½å¤Ÿä»å„ç§æµ‹è¯•ç»“æœç»„åˆä¸­æ•æ‰è¯¦ç»†çš„ç—…äººä¿¡æ¯ã€‚è¿™äº›ç‰¹å¾åˆ†ç±»æ€§èƒ½ä»¤äººæ»¡æ„ã€‚ä½¿ç”¨å“åº¦æ ‡åº¦ã€å¬åŠ›å›¾å’Œè¯­éŸ³æµ‹è¯•ä¿¡æ¯çš„ç»„åˆå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè€Œå•ä¸€æŒ‡æ ‡è¡¨ç°æœ€å·®ã€‚å¢å¼ºåçš„ç‰¹å¾ç”Ÿæˆç®¡é“å±•ç¤ºäº†åœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´åˆå¹¶å¬è§‰ç‰¹å¾é›†çš„å¯èƒ½æ€§ï¼Œè¿™å°†æ¨å¹¿åˆ°æ‰€æœ‰æ•°æ®é›†ï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥å¯¼è‡´å½¢æˆå¯è§£è¯»çš„å…¨çƒç‰¹å¾é›†ã€‚åˆ†ç±»æ¨¡å‹ä»ç„¶ä¿æŒäº†ä¸´åºŠé€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20765v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ‰©å±•ç°æœ‰çš„å¬åŠ›çŠ¶å†µåˆ†æç”Ÿæˆç®¡é“ï¼Œå¢åŠ å¬è§‰ç‰¹å¾é›†åˆå¹¶å…¥æ­¥éª¤ï¼Œå®ç°ä¸åŒæ•°æ®é›†é—´çš„å¬è§‰ç‰¹å¾åˆ†æã€‚é€šè¿‡å¯¹ä¸åŒæ•°æ®é›†äº§ç”Ÿçš„å¬è§‰ç‰¹å¾é›†ï¼ˆAPsï¼‰è¿›è¡Œåˆå¹¶åˆ†æï¼Œå‘ç°æ›´å…·ä»£è¡¨æ€§çš„å¬åŠ›çŠ¶å†µç»„åˆåˆ†å¸ƒæ¨¡å¼ã€‚æœ¬ç ”ç©¶å°†å…ˆå‰ç”Ÿæˆçš„13ä¸ªAPsï¼ˆæ•°æ®é›†NA&#x3D;595ï¼‰ä¸åŸºäºç¬¬äºŒä¸ªæ•°æ®é›†æ–°ç”Ÿæˆçš„31ä¸ªAPsï¼ˆæ•°æ®é›†NB&#x3D;1272ï¼‰è¿›è¡Œåˆå¹¶ï¼Œä¾æ®ä¸¤è€…é—´é‡å ç‰¹å¾çš„ç›¸ä¼¼æ€§å¾—åˆ†æ¥å»ºç«‹åˆå¹¶æ¨¡å‹ã€‚ä¸ºç¡®ä¿ä¸´åºŠåº”ç”¨æ€§ï¼Œæœ¬ç ”ç©¶æ„å»ºéšæœºæ£®æ—æ¨¡å‹æ¨¡æ‹Ÿä¸åŒå¬åŠ›çŠ¶å†µç»„åˆåœºæ™¯ã€‚æœ€ç»ˆæˆåŠŸåˆå¹¶å¾—åˆ°æ–°çš„APç»„åˆæ¨¡å‹é›†ï¼Œä¸ä»…èƒ½å¤Ÿå±•ç°å‡ºæ›´å…·ä½“çš„ç—…äººç‰¹å¾åˆ†æè½®å»“ï¼Œä¹Ÿæé«˜äº†éŸ³é¢‘èµ„æ–™ç»„åˆçš„å‡†ç¡®æ€§èƒ½åˆ†æã€‚åŸºäºå£°éŸ³å°ºåº¦æµ‹é‡ã€å¬åŠ›å›¾ä»¥åŠè¯­éŸ³æµ‹è¯•ä¿¡æ¯çš„ç»„åˆè¡¨ç°æœ€ä½³ï¼Œè€Œå•ä¸€åº¦é‡æªæ–½è¡¨ç°æœ€å·®ã€‚è¯¥æ”¹è¿›å‹ç”Ÿæˆç®¡é“å±•ç¤ºäº†è·¨æ•°æ®é›†åˆå¹¶APsçš„å¯è¡Œæ€§ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨å¹¿åˆ°æ‰€æœ‰æ•°æ®é›†å¹¶äº§ç”Ÿå¯è§£è¯»çš„å…¨çƒå¬åŠ›çŠ¶å†µåˆ†æé›†ã€‚åˆ†ç±»æ¨¡å‹ä¾ç„¶ä¿æŒä¸´åºŠé€‚ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶é€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†åˆ†æå¬è§‰ç‰¹å¾é›†ï¼ˆAPsï¼‰ï¼Œä»¥è·å¾—æ›´å‡†ç¡®çš„å¬åŠ›çŠ¶å†µè¯„ä¼°ç»“æœã€‚</li>
<li>é€šè¿‡åˆå¹¶å…ˆå‰å’Œæ–°çš„æ•°æ®é›†äº§ç”Ÿçš„APsï¼Œå½¢æˆæ–°çš„ç»„åˆæ¨¡å‹é›†ï¼Œæä¾›æ›´è¯¦ç»†çš„ç—…äººç‰¹å¾åˆ†æè½®å»“ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åŸºäºç‰¹å¾é‡å ç›¸ä¼¼æ€§çš„æ–¹æ³•è¿›è¡ŒAPsåˆå¹¶ã€‚</li>
<li>éšæœºæ£®æ—æ¨¡å‹ç”¨äºæ¨¡æ‹Ÿä¸åŒå¬åŠ›çŠ¶å†µç»„åˆåœºæ™¯ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ä¸´åºŠé€‚ç”¨æ€§ã€‚</li>
<li>ç»„åˆæ¨¡å‹æ€§èƒ½åˆ†ææ˜¾ç¤ºï¼Œç»¼åˆè€ƒè™‘å£°éŸ³å°ºåº¦æµ‹é‡ã€å¬åŠ›å›¾å’Œè¯­éŸ³æµ‹è¯•ä¿¡æ¯çš„æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†è·¨æ•°æ®é›†åˆå¹¶APsçš„å¯è¡Œæ€§ï¼Œæœªæ¥æœ‰æœ›æ¨å¹¿åˆ°æ‰€æœ‰æ•°æ®é›†å¹¶äº§ç”Ÿå…¨çƒæ€§å¬åŠ›çŠ¶å†µåˆ†æé›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-077a5aed8fe333946050487152273996.jpg" align="middle">
</details>




<h2 id="Whisper-Flamingo-Integrating-Visual-Features-into-Whisper-for-Audio-Visual-Speech-Recognition-and-Translation"><a href="#Whisper-Flamingo-Integrating-Visual-Features-into-Whisper-for-Audio-Visual-Speech-Recognition-and-Translation" class="headerlink" title="Whisper-Flamingo: Integrating Visual Features into Whisper for   Audio-Visual Speech Recognition and Translation"></a>Whisper-Flamingo: Integrating Visual Features into Whisper for   Audio-Visual Speech Recognition and Translation</h2><p><strong>Authors:Andrew Rouditchenko, Yuan Gong, Samuel Thomas, Leonid Karlinsky, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, speech models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better speech-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper speech recognition and translation model with gated cross attention. Our models achieve state-of-the-art ASR WER (0.68%) and AVSR WER (0.76%) on LRS3, and state-of-the-art ASR WER (1.3%) and AVSR WER (1.4%) on LRS2. Audio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech recognition and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is versatile and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åˆ©ç”¨åŸºäºå˜´å”‡çš„è§†é¢‘æ¥æ”¹è¿›å™ªå£°ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚ç”±äºè§†é¢‘æ¯”éŸ³é¢‘æ›´éš¾è·å–ï¼ŒAVSRæ¨¡å‹çš„è§†é¢‘è®­ç»ƒæ•°æ®é€šå¸¸ä»…é™äºæ•°åƒå°æ—¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¯¸å¦‚Whisperä¹‹ç±»çš„è¯­éŸ³æ¨¡å‹ä½¿ç”¨æ•°ä»¥ç™¾ä¸‡è®¡å°æ—¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå­¦ä¹ æ›´å¥½çš„è¯­éŸ³åˆ°æ–‡æœ¬çš„è§£ç å™¨ã€‚å·¨å¤§çš„è®­ç»ƒæ•°æ®å·®å¼‚æ¿€åŠ±æˆ‘ä»¬å°†Whisperé€‚åº”äºå¤„ç†è§†é¢‘è¾“å…¥ã€‚å—Flamingoå°†è§†è§‰ç‰¹å¾æ³¨å…¥è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ•´åˆè§†è§‰ç‰¹å¾çš„Whisper-Flamingoï¼Œç”¨äºWhisperè¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ¨¡å‹ï¼Œé‡‡ç”¨é—¨æ§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨LRS3ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ASR WERï¼ˆ0.68ï¼…ï¼‰å’ŒAVSR WERï¼ˆ0.76ï¼…ï¼‰ï¼Œåœ¨LRS2ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ASR WERï¼ˆ1.3ï¼…ï¼‰å’ŒAVSR WERï¼ˆ1.4ï¼…ï¼‰ã€‚è§†å¬Whisper-Flamingoåœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è‹±è¯­è¯­éŸ³è¯†åˆ«å’Œè‹±-å…¶ä»–å…­ç§è¯­è¨€çš„ç¿»è¯‘ä¸Šä¼˜äºä»…éŸ³é¢‘çš„Whisperã€‚è€Œä¸”ï¼ŒWhisper-Flamingoéå¸¸é€šç”¨ï¼Œä½¿ç”¨ä¸€ç»„å‚æ•°å³å¯å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡ï¼Œè€Œå…ˆå‰çš„æ–¹æ³•åˆ™é’ˆå¯¹æ¯ç§è¯­è¨€åˆ†åˆ«è¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10082v3">PDF</a> Interspeech 2024. V3: Added results on LRS2. Code at   <a target="_blank" rel="noopener" href="https://github.com/roudimit/whisper-flamingo">https://github.com/roudimit/whisper-flamingo</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘çš„è§†é¢‘è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½æå‡å’Œæ–°æŠ€æœ¯ç ”ç©¶ã€‚åˆ©ç”¨è§†è§‰ä¿¡æ¯å¢å¼ºè¯­éŸ³æ¨¡å‹çš„è¡¨ç°åŠ›ï¼Œå¹¶æ•´åˆå¤šç§è¯­è¨€çš„æ¨¡å‹è®­ç»ƒæŠ€æœ¯å®ç°å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–°æŠ€æœ¯é€šè¿‡ä½¿ç”¨é—¸é—¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œèåˆè¯­è¨€æ¨¡å‹çš„è§†è§‰ç‰¹å¾æ¥å–å¾—æ›´é«˜çš„å‡†ç¡®æ€§ã€‚éŸ³é¢‘è§†é¢‘åŒæ¨¡å¼åœ¨è¯¥æ¨¡å‹ä¸‹çš„è¯­éŸ³è¯†åˆ«ä¸ç¿»è¯‘èƒ½åŠ›å‡å–å¾—é¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯èƒ½é’ˆå¯¹å™ªéŸ³ç¯å¢ƒä¸‹ä¸­è‹±æ–‡è¯­éŸ³çš„ç¿»è¯‘å·¥ä½œï¼Œå–å¾—äº†ä¼˜äºåŸæœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚è€Œä¸”ç›¸æ¯”ä¹‹å‰çš„ç‹¬ç«‹è¯­è¨€è®­ç»ƒæ–¹å¼ï¼Œæ–°æ¨¡å‹å‚æ•°å…±ç”¨ã€æ›´å…·é€šç”¨æ€§ã€‚æ­¤æ¨¡å‹æé«˜äº†æ•°æ®ä½¿ç”¨æ•ˆç‡å’Œè·¨è¯­è¨€è¯†åˆ«çš„èƒ½åŠ›ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSRé€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æå‡æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°ç¯å¢ƒä¸‹ã€‚ä½¿ç”¨åŸºäºå”‡å½¢çš„è§†é¢‘è¾…åŠ©è¯­éŸ³è¯†åˆ«çš„æ•ˆæœæ›´ä½³ã€‚ä½†è§†é¢‘è·å–éš¾åº¦å¤§ä¸”è®­ç»ƒæ•°æ®é‡å—é™ã€‚</li>
<li>å¯¹æ¯”äºAVSRæ¨¡å‹ä½¿ç”¨çš„æ•°åƒå°æ—¶è§†é¢‘è®­ç»ƒæ•°æ®ï¼ŒWhisperç­‰è¯­éŸ³æ¨¡å‹å…·å¤‡æµ·é‡æ•°æ®æ”¯æŒå¹¶æ„å»ºæœ‰æ›´é«˜çš„è¯­æ–™-æ–‡æœ¬è§£ç ç²¾åº¦ã€‚å·®å¼‚æ¨åŠ¨äº†é€šè¿‡é€‚é…çš„æ–¹æ³•åˆ©ç”¨è§†é¢‘ä¿¡æ¯ã€‚æå‡ºåœ¨ç»“åˆè¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ä¸­ï¼Œå°è¯•é›†æˆè§†è§‰ç‰¹å¾çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºé›†æˆè§†è§‰ç‰¹å¾çš„Whisper-Flamingoæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†Whisperçš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨é—¸é—¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¤„ç†è§†é¢‘è¾“å…¥ä¿¡æ¯ã€‚è¯¥æ¨¡å‹åœ¨LRS3å’ŒLRS2ä¸Šå–å¾—äº†å…ˆè¿›çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½è¡¨ç°ã€‚åœ¨å™ªå£°ç¯å¢ƒä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«å’Œè·¨è¯­è¨€ç¿»è¯‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç›¸è¾ƒäºä»…ä¾èµ–éŸ³é¢‘çš„Whisperæ¨¡å‹ï¼ŒéŸ³è§†é¢‘ç»“åˆçš„Whisper-Flamingoå±•ç°å‡ºæ›´å‡ºè‰²çš„è¡¨ç°æ•ˆæœã€‚é€šè¿‡åŠ å…¥è§†é¢‘è¾“å…¥èƒ½å¤Ÿæ˜¾è‘—åœ°å¢å¼ºè¯†åˆ«å‡†ç¡®ç‡ï¼Œä¸”æ˜¾è‘—æé«˜äº†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>Whisper-Flamingoå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½ä½¿ç”¨åŒä¸€ç»„å‚æ•°è¿›è¡Œå¤šé¡¹ä»»åŠ¡å¤„ç†ï¼Œç›¸æ¯”ä¹‹ä¸‹ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•éœ€è¦åœ¨æ¯ç§è¯­è¨€ä¸Šè¿›è¡Œå•ç‹¬è®­ç»ƒï¼Œè¿™ç§æ–¹æ³•åœ¨çµæ´»æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºäº†ä¼˜è¶Šæ€§ã€‚é€šè¿‡ä½¿ç”¨åŒä¸€ä¸ªå‚æ•°é›†åº”å¯¹å¤šé¡¹ä»»åŠ¡çš„æ¨¡å¼èƒ½å¤Ÿæé«˜æ¨¡å‹å¯¹å¤æ‚è¯­è¨€å¤„ç†å’Œå¤šè¯­è¨€çš„åº”å¯¹èƒ½åŠ›åŒæ—¶æ˜¾è‘—åœ°æå‡è®­ç»ƒæ•ˆç‡å’Œæ•°æ®ä½¿ç”¨çš„æœ‰æ•ˆæ€§é™ä½äº†è¿ç®—æˆæœ¬å’Œæé«˜äº†é€‚ç”¨æ€§å¹¿æ³›æ€§ ã€‚è¿™ç§çµæ´»æ€§è®©è¯¥æ¨¡å‹èƒ½åœ¨å„ç§å¤æ‚å¤šå˜çš„åœºæ™¯ä¸­å‘æŒ¥å‡ºè‰¯å¥½çš„æ€§èƒ½è¡¨ç°ä¹Ÿå¤§å¤§æé«˜äº†å®ƒçš„å•†ä¸šä»·å€¼å’Œç¤¾ä¼šåº”ç”¨å‰æ™¯ã€‚åŒæ—¶ä¹Ÿå¤§å¤§å¢å¼ºäº†æ¨¡å‹åº”å¯¹æ•°æ®ä¸å‡è¡¡é—®é¢˜çš„èƒ½åŠ›ä¹Ÿä½“ç°äº†æŠ€æœ¯çš„æ·±åº¦åº”ç”¨å’Œæ½œåŠ›çš„å·¨å¤§ã€‚è¿™ç§æŠ€æœ¯å°†æœ‰å¯èƒ½æ¨åŠ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•å’Œé©æ–°åŒæ—¶ä¹Ÿå°†å¯¹å¤šè¯­è¨€å¤„ç†å’Œäººæœºäº¤äº’é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e4b7a9914ffaa0d5807dab12a05582f4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4d82db65fd9bac4f80abd87f16c335fe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1d066c3aa4fa6bfbeafff65a7261d8b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a12a100f8e3e0757ca865e2fe061789.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3ce46907bde275417f2e23069671f046.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥çš„å„ç§å¨èƒï¼Œå¯¹åˆæˆè¯­éŸ³è¿›è¡Œå¯é è¿½è¸ªçš„éœ€æ±‚å˜å¾—è¿«åˆ‡ã€‚ç„¶è€Œï¼Œå½“å‰è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯åœ¨è¯­éŸ³ç”Ÿæˆåå•ç‹¬æ·»åŠ æ°´å°ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢å½±å“äº†è¯­éŸ³è´¨é‡ï¼Œä¹Ÿå½±å“äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨ç¨³å¥æ€§å’Œçµæ´»æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TraceableSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„TTSæ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ï¼Œæé«˜äº†æ°´å°çš„ä¸æ˜“å¯Ÿè§‰æ€§å’Œè¯­éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¸§çº§æ°´å°çš„å°åˆ¶å’Œæå–ï¼Œæé«˜äº†å¯¹æŠ—é‡æ–°æ‹¼æ¥æ”»å‡»çš„ç¨³å¥æ€§å’Œæ“ä½œçš„æ—¶æ•ˆæ€§çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨ä¸æ˜“å¯Ÿè§‰æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠµæŠ—é‡æ–°æ‹¼æ¥æ”»å‡»æ–¹é¢è¶…è¶Šäº†VALL-Eæˆ–HiFicodecå•ç‹¬ä½¿ç”¨WavMarkçš„å¼ºå¤§åŸºçº¿ã€‚å®ƒè¿˜å¯ä»¥åº”ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/zjzsers/TraceableSpeech%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/zjzsers/TraceableSpeechè¿›è¡Œè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œåˆæˆè¯­éŸ³çš„æº¯æºé—®é¢˜æ—¥ç›Šé‡è¦ã€‚ç°æœ‰æ–¹æ³•å¸¸åœ¨ç”ŸæˆéŸ³é¢‘åå†åŠ æ°´å°ï¼Œå½±å“è¯­éŸ³è´¨é‡å’Œæ°´å°çš„éšè”½æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºTraceableSpeechï¼Œä¸€ä¸ªå¯ç›´æ¥ç”Ÿæˆå¸¦æ°´å°è¯­éŸ³çš„æ–°å‹TTSæ¨¡å‹ï¼Œæé«˜æ°´å°çš„éšè”½æ€§å’Œè¯­éŸ³è´¨é‡ã€‚è¯¥æ¨¡å‹å®ç°å¸§çº§æ°´å°å°ç›–å’Œæå–ï¼Œå¯¹æŠ—æ‹¼æ¥æ”»å‡»æ›´å…·é²æ£’æ€§ï¼Œæ“ä½œæ›´çµæ´»ã€‚å®éªŒæ˜¾ç¤ºï¼ŒTraceableSpeechåœ¨æ°´å°éšè”½æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠ—æ‹¼æ¥æ”»å‡»æ–¹é¢ä¼˜äºä½¿ç”¨WavMarkçš„VALL-Eæˆ–HiFicodecã€‚å®ƒé€‚ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSæŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥åˆæˆè¯­éŸ³æº¯æºçš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ·»åŠ æ°´å°å½±å“è¯­éŸ³è´¨é‡å’Œæ°´å°éšè”½æ€§ã€‚</li>
<li>TraceableSpeechæ˜¯ä¸€ä¸ªæ–°å‹çš„TTSæ¨¡å‹ï¼Œå¯ç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ã€‚</li>
<li>TraceableSpeechæé«˜æ°´å°çš„éšè”½æ€§å’Œè¯­éŸ³è´¨é‡ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°å¸§çº§æ°´å°å°ç›–å’Œæå–ï¼Œå¢å¼ºé²æ£’æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒTraceableSpeechåœ¨æ°´å°ã€è¯­éŸ³è´¨é‡å’ŒæŠ—æ”»å‡»æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test"><a href="#Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test" class="headerlink" title="Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test"></a>Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test</h2><p><strong>Authors:Peter Leer, Jesper Jensen, Laurel H. Carney, Zheng-Hua Tan, Jan Ã˜stergaard, Lars BramslÃ¸w</strong></p>
<p>This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation. Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation. DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification. In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals. First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway. We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation. Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy. Finally, we evaluate, to our knowledge for the first time, the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners. The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies. Our proposed approach was shown to provide an increase in speech intelligibility versus an unprocessed baseline and was found to outperform a conventional approach in terms of both intelligibility and preference. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¬åŠ›æŸå¤±è¡¥å¿æ–¹é¢çš„åº”ç”¨ã€‚å¬åŠ›æŸå¤±æ˜¯ä¸€ä¸ªå½±å“å…¨çƒæ•°ç™¾ä¸‡äººçš„æ™®éé—®é¢˜ï¼Œè€Œä¼ ç»Ÿçš„åŠ©å¬å™¨åœ¨æä¾›æ»¡æ„çš„è¡¥å¿æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å„ç§å¬è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººè¯†åˆ«å’ŒéŸ³ä¹åˆ†ç±»ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯åœ¨å¬åŠ›å—æŸå’Œæ­£å¸¸å¬åŠ›åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬è§‰æ¨¡å‹å¯¹è¯­éŸ³ä¿¡å·çš„è¾“å‡ºååº”ä¸­è¿›è¡Œè®­ç»ƒçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿå¬è§‰æ¨¡å‹çš„æ¡†æ¶ï¼Œé‡ç‚¹æ˜¯ä¸€ä¸ªå¬è§‰è·¯å¾„ä¸­çš„å¬è§‰ç¥ç»æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒ–æ–¹æ³•ï¼Œç”¨äºåˆ†æåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥é€‰æ‹©ç”¨äºè¡¥å¿ç­–ç•¥çš„å¬è§‰æ¨¡å‹çš„å£°ä¸­å¿ƒé¢‘ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å¬åŠ›å—æŸå¬ä¼—çš„è†å¬æµ‹è¯•ï¼Œé¦–æ¬¡å¯¹åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯¼è‡´å¯è¡Œçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥ã€‚ä¸æˆ‘ä»¬æœªå¤„ç†çš„åŸºæœ¬æ–¹æ¡ˆç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†è¯­éŸ³æ¸…æ™°åº¦ï¼Œå¹¶ä¸”åœ¨æ¸…æ™°åº¦å’Œåå¥½æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10420v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç”¨äºå¬åŠ›æŸå¤±è¡¥å¿çš„ç ”ç©¶ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒæ­£å¸¸å¬åŠ›å’Œå¬åŠ›å—æŸçš„DNNå¬è§‰æ¨¡å‹å¯¹è¯­éŸ³ä¿¡å·çš„å“åº”æ¥å®ç°ã€‚æå‡ºä¸€ç§æ¨¡æ‹Ÿå¬è§‰æ¨¡å‹çš„æ¡†æ¶ï¼Œå¹¶çº¿æ€§åŒ–DNNæ–¹æ³•è¿›è¡Œåˆ†æã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥é€‰æ‹©å¬è§‰æ¨¡å‹çš„ä¸­å¿ƒé¢‘ç‡ç”¨äºè¡¥å¿ç­–ç•¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜å¬åŠ›æŸå¤±è€…çš„è¯­è¨€ç†è§£åŠ›ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DNNç”¨äºå¬åŠ›æŸå¤±è¡¥å¿çš„ç ”ç©¶ã€‚</li>
<li>åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•é€šè¿‡è®­ç»ƒæ­£å¸¸å¬åŠ›å’Œå¬åŠ›å—æŸçš„å¬è§‰æ¨¡å‹çš„å“åº”å®ç°ã€‚</li>
<li>æå‡ºäº†æ¨¡æ‹Ÿå¬è§‰æ¨¡å‹çš„æ¡†æ¶ï¼Œå¹¶é‡ç‚¹ç ”ç©¶å¬è§‰è·¯å¾„ä¸­çš„å¬è§‰ç¥ç»æ¨¡å‹ã€‚</li>
<li>å¯¹DNNæ–¹æ³•è¿›è¡Œçº¿æ€§åŒ–åˆ†æã€‚</li>
<li>å¼€å‘äº†ä¸€ç§ç®€å•é€‰æ‹©å¬è§‰æ¨¡å‹ä¸­å¿ƒé¢‘ç‡çš„æ–¹æ³•ï¼Œç”¨äºè¡¥å¿ç­–ç•¥ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æé«˜å¬åŠ›æŸå¤±è€…çš„è¯­è¨€ç†è§£åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-eb96d0056db88ee2c733128931b7fdde.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b17b20446f95113e30d62616a7728759.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e67e7cf7bc58f078a820af9ac539ae7a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c45eba05418f6e1ebeb65af3ab0e310e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5b2b0e9ca74e4133de6132871478d14d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c4f583d4a12289ffcf1e06220ca86407.jpg" align="middle">
</details>




<h2 id="Speech-Robust-Bench-A-Robustness-Benchmark-For-Speech-Recognition"><a href="#Speech-Robust-Bench-A-Robustness-Benchmark-For-Speech-Recognition" class="headerlink" title="Speech Robust Bench: A Robustness Benchmark For Speech Recognition"></a>Speech Robust Bench: A Robustness Benchmark For Speech Recognition</h2><p><strong>Authors:Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Bhiksha Raj, Nicolas Kourtellis</strong></p>
<p>As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 114 input perturbations which simulate an heterogeneous range of corruptions that ASR models may encounter when deployed in the wild. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as the use of discrete representations, or self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females. Our results revealed noticeable disparities in the modelâ€™s robustness across subgroups. We believe that SRB will significantly facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨ç‰©ç†ä¸–ç•Œå’Œæ•°å­—ä¸–ç•Œçš„æ™®åŠï¼Œç¡®ä¿å®ƒä»¬åœ¨å­˜åœ¨çš„å„ç§è…èš€æƒ…å†µä¸‹åšå‡ºå¯é é¢„æµ‹å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†è¯­éŸ³é²æ£’åŸºå‡†ï¼ˆSRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°ASRæ¨¡å‹å¯¹å„ç§è…èš€é²æ£’æ€§çš„åŸºå‡†æµ‹è¯•ã€‚SRBç”±114ä¸ªè¾“å…¥æ‰°åŠ¨ç»„æˆï¼Œè¿™äº›æ‰°åŠ¨æ¨¡æ‹Ÿäº†ASRæ¨¡å‹åœ¨é‡å¤–éƒ¨ç½²æ—¶å¯èƒ½é‡åˆ°çš„å„ç§å¼‚è´¨è…èš€ã€‚æˆ‘ä»¬ä½¿ç”¨SRBæ¥è¯„ä¼°ä¸€äº›æœ€å…ˆè¿›çš„ASRæ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°æ¨¡å‹å¤§å°ä»¥åŠæŸäº›å»ºæ¨¡é€‰æ‹©ï¼Œå¦‚ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºæˆ–è‡ªè®­ç»ƒï¼Œæœ‰åŠ©äºå¢å¼ºé²æ£’æ€§ã€‚æˆ‘ä»¬å°†æ­¤åˆ†ææ‰©å±•åˆ°è¡¡é‡æ¥è‡ªä¸åŒäººç¾¤å­ç»„çš„æ•°æ®çš„ASRæ¨¡å‹çš„é²æ£’æ€§ï¼Œå³è‹±è¯­å’Œè¥¿ç­ç‰™è¯­ä½¿ç”¨è€…ä»¥åŠç”·æ€§å’Œå¥³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒå­ç»„ä¹‹é—´çš„é²æ£’æ€§å­˜åœ¨æ˜æ˜¾çš„å·®å¼‚ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œé€šè¿‡ä½¿è¿›è¡Œå…¨é¢å’Œå¯æ¯”è¾ƒçš„é²æ£’æ€§è¯„ä¼°å˜å¾—æ›´å®¹æ˜“ï¼ŒSRBå°†æå¤§åœ°ä¿ƒè¿›æœªæ¥å¯¹ç¨³å¥ASRæ¨¡å‹çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07937v3">PDF</a> submitted to NeurIPS datasets and benchmark track 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„é²æ£’æ€§è¯„ä¼°é—®é¢˜ã€‚é’ˆå¯¹ASRæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½é‡åˆ°çš„å¤šç§å¹²æ‰°ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºSpeech Robust Benchï¼ˆSRBï¼‰çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚é€šè¿‡åœ¨è¯¥åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä½œè€…å‘ç°æ¨¡å‹è§„æ¨¡ä»¥åŠé‡‡ç”¨ç¦»æ•£è¡¨ç¤ºå’Œè‡ªè®­ç»ƒç­‰ç‰¹å®šå»ºæ¨¡é€‰æ‹©éƒ½æœ‰åŠ©äºæå‡ASRæ¨¡å‹çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ä¸åŒç¾¤ä½“çš„æ•°æ®ï¼ˆå¦‚ä¸åŒè¯­è¨€å’Œæ€§åˆ«çš„æ¼”è®²è€…ï¼‰ï¼Œæ¨¡å‹çš„é²æ£’æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä½œè€…è®¤ä¸ºï¼ŒSpeech Robust Benchå°†æå¤§åœ°æ¨åŠ¨æœªæ¥ç ”ç©¶æ›´åŠ ç¨³å¥çš„ASRæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºSpeech Robust Benchï¼ˆSRBï¼‰çš„ç»¼åˆåŸºå‡†ï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨å„ç§å¹²æ‰°ä¸‹çš„é²æ£’æ€§ã€‚</li>
<li>SRBåŒ…å«äº†æ¨¡æ‹Ÿé‡å¤–ASRæ¨¡å‹å¯èƒ½é‡åˆ°çš„å¤šç§å¹²æ‰°çš„114ç§è¾“å…¥æ‰°åŠ¨ã€‚</li>
<li>ä½œè€…é€šè¿‡å¤šä¸ªå‰æ²¿ASRæ¨¡å‹åœ¨SRBä¸Šçš„è¡¨ç°å‘ç°ï¼Œæ¨¡å‹è§„æ¨¡åŠç‰¹å®šå»ºæ¨¡é€‰æ‹©å¦‚ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºå’Œè‡ªè®­ç»ƒæœ‰åŠ©äºæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>ä¸åŒç¾¤ä½“çš„æ•°æ®ï¼ˆå¦‚ä¸åŒè¯­è¨€å’Œæ€§åˆ«ï¼‰åœ¨æ¨¡å‹é²æ£’æ€§ä¸Šå­˜åœ¨å·®å¼‚ã€‚</li>
<li>ASRæ¨¡å‹çš„é²æ£’æ€§è¯„ä¼°éœ€è¦æ›´å…¨é¢å’Œå¯æ¯”è¾ƒçš„æ–¹æ³•ï¼Œè€ŒSpeech Robust Benchä¸ºæ­¤æä¾›äº†ä¾¿åˆ©ã€‚</li>
<li>Speech Robust Benchçš„æå‡ºå°†æœ‰åŠ©äºæ¨åŠ¨æœªæ¥æ›´ç¨³å¥çš„ASRæ¨¡å‹çš„ç ”ç©¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c582f037a408bd577abfa57d725d7bda.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2103d6119e46973d4d2973b31715cea4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ae3e3036452a0b887e4798ca7935fd7e.jpg" align="middle">
</details>




<h2 id="PixIT-Joint-Training-of-Speaker-Diarization-and-Speech-Separation-from-Real-world-Multi-speaker-Recordings"><a href="#PixIT-Joint-Training-of-Speaker-Diarization-and-Speech-Separation-from-Real-world-Multi-speaker-Recordings" class="headerlink" title="PixIT: Joint Training of Speaker Diarization and Speech Separation from   Real-world Multi-speaker Recordings"></a>PixIT: Joint Training of Speaker Diarization and Speech Separation from   Real-world Multi-speaker Recordings</h2><p><strong>Authors:Joonas Kalda, ClÃ©ment PagÃ©s, Ricard Marxer, Tanel AlumÃ¤e, HervÃ© Bredin</strong></p>
<p>A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization. Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio. We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep. With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD. We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them. PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning. </p>
<blockquote>
<p>ç›‘ç£è¯­éŸ³åˆ†ç¦»ï¼ˆSSepï¼‰ç³»ç»Ÿçš„ä¸€ä¸ªä¸»è¦ç¼ºç‚¹æ˜¯å®ƒä»¬ä¾èµ–äºåˆæˆæ•°æ®ï¼Œå¯¼è‡´åœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æå‡ºäº†æ··åˆä¸å˜è®­ç»ƒï¼ˆMixITï¼‰ä½œä¸ºä¸€ç§ä½¿ç”¨çœŸå®å½•éŸ³çš„æ— ç›‘ç£æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒé¢ä¸´ç€è¿‡åº¦åˆ†ç¦»å’Œé€‚åº”é•¿éŸ³é¢‘çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†PixITï¼Œè¿™æ˜¯ä¸€ç§è”åˆæ–¹æ³•ï¼Œç»“åˆäº†ç”¨äºè¯´è¯äººåˆ†å‰²ï¼ˆSDï¼‰çš„æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰å’Œç”¨äºSSepçš„MixITã€‚å®ƒåªéœ€è¦å°‘é‡çš„SDæ ‡ç­¾ï¼Œè§£å†³äº†è¿‡åº¦åˆ†ç¦»é—®é¢˜ï¼Œå¹¶å…è®¸åˆ©ç”¨åŸºäºèšç±»çš„ç¥ç»ç½‘ç»œSDçš„ç°æœ‰å·¥ä½œæ¥æ‹¼æ¥å±€éƒ¨åˆ†ç¦»æºã€‚æˆ‘ä»¬é€šè¿‡å°†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåº”ç”¨äºåˆ†ç¦»æºæ¥æµ‹é‡å…¶è´¨é‡ã€‚PixITæé«˜äº†ä¸¤ä¸ªä¼šè®®è¯­æ–™åº“ä¸­å„ç§ASRç³»ç»Ÿçš„æ€§èƒ½ï¼Œæ—¢é™ä½äº†åŸºäºè¯´è¯äººçš„è¯é”™è¯¯ç‡ï¼Œä¹Ÿé™ä½äº†åŸºäºè¯è¯­çš„è¯é”™è¯¯ç‡ï¼ŒåŒæ—¶æ— éœ€è¿›è¡Œä»»ä½•å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02288v2">PDF</a> Speaker Odyssey 2024</p>
<p><strong>Summary</strong></p>
<p>PixITæ˜¯ä¸€ç§ç»“åˆäº†æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰ç”¨äºè¯´è¯äººèšç±»ï¼ˆSDï¼‰å’Œæ··åˆä¸å˜è®­ç»ƒï¼ˆMixITï¼‰ç”¨äºè¯­éŸ³åˆ†ç¦»ï¼ˆSSepï¼‰çš„è”åˆæ–¹æ³•ã€‚å®ƒè§£å†³äº†è¿‡åº¦åˆ†ç¦»çš„é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨ç°æœ‰çš„åŸºäºèšç±»çš„ç¥ç»ç½‘ç»œSDï¼Œå®ç°äº†å±€éƒ¨åˆ†ç¦»æºçš„æ‹¼æ¥ã€‚PixITé€šè¿‡åœ¨ä¸¤ä¸ªä¼šè®®è¯­æ–™åº“ä¸Šåº”ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ¥è¡¡é‡åˆ†ç¦»æºçš„è´¨é‡ï¼Œæé«˜äº†è¯´è¯äººå±æ€§å’ŒåŸºäºè¯è¯­çš„å•è¯é”™è¯¯ç‡æ€§èƒ½ï¼Œå¹¶ä¸”æ— éœ€ä»»ä½•å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£è¯­éŸ³åˆ†ç¦»ï¼ˆSSepï¼‰ç³»ç»Ÿå­˜åœ¨ä¾èµ–äºåˆæˆæ•°æ®çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</li>
<li>æ··åˆä¸å˜è®­ç»ƒï¼ˆMixITï¼‰ä½œä¸ºä¸€ç§æ— ç›‘ç£æ–¹æ³•ï¼Œä½¿ç”¨çœŸå®å½•éŸ³æ•°æ®ï¼Œä½†é¢ä¸´è¿‡åº¦åˆ†ç¦»å’Œé€‚åº”é•¿éŸ³é¢‘çš„æŒ‘æˆ˜ã€‚</li>
<li>PixITæ˜¯ç»“åˆæ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰å’ŒMixITçš„è”åˆæ–¹æ³•ï¼Œç”¨äºè§£å†³è¯­éŸ³åˆ†ç¦»å’Œè¯´è¯äººèšç±»çš„é—®é¢˜ã€‚</li>
<li>PixITé€šè¿‡åˆ©ç”¨ç°æœ‰çš„åŸºäºèšç±»çš„ç¥ç»ç½‘ç»œè¯´è¯äººèšç±»ï¼ˆSDï¼‰ï¼Œå®ç°äº†å±€éƒ¨åˆ†ç¦»æºçš„æ‹¼æ¥ã€‚</li>
<li>PixITæé«˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æ€§èƒ½ï¼Œè¡¨ç°åœ¨è¯´è¯äººå±æ€§å’ŒåŸºäºè¯è¯­çš„å•è¯é”™è¯¯ç‡ä¸Šã€‚</li>
<li>PixITæ–¹æ³•ä¸éœ€è¦ä»»ä½•å¾®è°ƒï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºçœŸå®ä¸–ç•Œçš„è¯­éŸ³æ•°æ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cb8c7c975906de0cc2794e814b8e474f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-70becf78ea369ed7bca4ec084f0bed37.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-88ffc36e5afa09a7eb2baf7aa9b9d5db.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d138978b544622ade6c58ac7b8512d65.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b3175ee73eb958a6b067ef7c692d2d41.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ddc3774b23fc9d08df84942104d1466f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-74c56e981436794750632941f563d4e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fd83f38f67461d69033f4764feafef20.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_Face Swapping/2412.07260v1/page_0_0.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  SegFace Face Segmentation of Long-Tail Classes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2024-12-12\./crop_æ— ç›‘ç£_åŠç›‘ç£_å¯¹æ¯”å­¦ä¹ /2410.13471v3/page_0_0.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  ConDSeg A General Medical Image Segmentation Framework via   Contrast-Driven Feature Enhancement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
