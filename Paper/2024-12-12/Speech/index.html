<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  AdvWave Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f2e52d8ca53dde891901cf63796eac44.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    28.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    117 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-12-æ›´æ–°"><a href="#2024-12-12-æ›´æ–°" class="headerlink" title="2024-12-12 æ›´æ–°"></a>2024-12-12 æ›´æ–°</h1><h2 id="AdvWave-Stealthy-Adversarial-Jailbreak-Attack-against-Large-Audio-Language-Models"><a href="#AdvWave-Stealthy-Adversarial-Jailbreak-Attack-against-Large-Audio-Language-Models" class="headerlink" title="AdvWave: Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models"></a>AdvWave: Stealthy Adversarial Jailbreak Attack against Large   Audio-Language Models</h2><p><strong>Authors:Mintong Kang, Chejian Xu, Bo Li</strong></p>
<p>Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate AI regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿›æ­¥ä½¿å¾—åŸºäºè¯­éŸ³çš„ç”¨æˆ·äº¤äº’æˆä¸ºå¯èƒ½ï¼Œè¿™æå¤§åœ°æå‡äº†ç”¨æˆ·ä½“éªŒï¼Œå¹¶åŠ é€Ÿäº†LALMåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ç„¶è€Œï¼Œç¡®ä¿LALMçš„å®‰å…¨è‡³å…³é‡è¦ï¼Œä»¥é˜²æ­¢äº§ç”Ÿå¯èƒ½å¼•èµ·ç¤¾ä¼šå…³æ³¨æˆ–è¿åAIæ³•è§„çš„é£é™©è¾“å‡ºã€‚å°½ç®¡è¿™ä¸ªé—®é¢˜éå¸¸é‡è¦ï¼Œä½†ç”±äºLALMçš„å…´èµ·æ—¶é—´è¾ƒçŸ­ï¼Œä¸åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œéŸ³é¢‘æ¨¡å‹ç›¸æ¯”è¿˜å­˜åœ¨é¢å¤–çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œå› æ­¤å…³äºç ´è§£LALMçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚å…·ä½“æ¥è¯´ï¼ŒLALMä¸­çš„éŸ³é¢‘ç¼–ç å™¨æ¶‰åŠç¦»æ•£æ“ä½œï¼Œå¾€å¾€ä¼šå¯¼è‡´æ¢¯åº¦ç ´ç¢ï¼Œé˜»ç¢ä¾èµ–äºæ¢¯åº¦ä¼˜åŒ–çš„æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚LALMçš„è¡Œä¸ºå¯å˜æ€§è¿›ä¸€æ­¥å¢åŠ äº†æœ‰æ•ˆï¼ˆå¯¹æŠ—æ€§ï¼‰ä¼˜åŒ–ç›®æ ‡çš„è¯†åˆ«éš¾åº¦ã€‚æ­¤å¤–ï¼Œå¯¹å¯¹æŠ—æ€§éŸ³é¢‘æ³¢å½¢å¼ºåˆ¶æ‰§è¡Œéšè”½æ€§çº¦æŸå¼•å…¥äº†ä¸€ä¸ªå‡å°ä¸”éå‡¸çš„å¯è¡Œè§£ç©ºé—´ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†ä¼˜åŒ–è¿‡ç¨‹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†é’ˆå¯¹LALMçš„é¦–ä¸ªç ´è§£æ¡†æ¶AdvWaveã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•æ¥è§£å†³æ¢¯åº¦ç ´ç¢é—®é¢˜ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„ç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªé€‚åº”å¯¹æŠ—ç›®æ ‡æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•æ ¹æ®LALMå¯¹ç‰¹å®šæŸ¥è¯¢çš„å“åº”æ¨¡å¼åŠ¨æ€è°ƒæ•´å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ã€‚ä¸ºäº†ç¡®ä¿å¯¹æŠ—æ€§éŸ³é¢‘å¯¹äººç±»å¬ä¼—æ¥è¯´åœ¨æ„ŸçŸ¥ä¸Šä»ç„¶è‡ªç„¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†ç±»å™¨å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§å™ªå£°ç±»ä¼¼äºå¸¸è§çš„åŸå¸‚å£°éŸ³ã€‚åœ¨å¤šä¸ªå…ˆè¿›LALMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAdvWaveä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾40%çš„å¹³å‡ç ´è§£æ”»å‡»æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08608v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LALMçš„å®‰å…¨æ€§æˆä¸ºå…¶å¹¿æ³›åº”ç”¨çš„ç“¶é¢ˆã€‚æœ€è¿‘çš„æŠ€æœ¯å‘å±•åŠ é€Ÿäº†å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­çš„éƒ¨ç½²ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å‘äº†é£é™©è¾“å‡ºçš„æ‹…å¿§ã€‚ç”±äºLALMä¸­éŸ³é¢‘ç¼–ç å™¨çš„ç¦»æ•£æ“ä½œå¸¦æ¥çš„æ¢¯åº¦ç¢è£‚é—®é¢˜ä»¥åŠè¡Œä¸ºå¯å˜æ€§ï¼Œé’ˆå¯¹LALMçš„æ”»å‡»é¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåä¸ºAdvWaveçš„æ¡†æ¶ï¼Œé‡‡ç”¨åŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•å’Œè‡ªé€‚åº”å¯¹æŠ—ç›®æ ‡æœç´¢ç®—æ³•ï¼Œä»¥çªç ´è¿™äº›æŒ‘æˆ˜ï¼Œå®ç°å¯¹LALMçš„æœ‰æ•ˆæ”»å‡»ã€‚è¯¥æ¡†æ¶åœ¨ç¡®ä¿å¯¹æŠ—æ€§éŸ³é¢‘ä¿æŒè‡ªç„¶æ„ŸçŸ¥çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹å¤šä¸ªå…ˆè¿›LALMçš„é«˜æ•ˆæ”»å‡»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LALMåœ¨ç°å®åº”ç”¨ä¸­çš„éƒ¨ç½²éœ€è¦ç¡®ä¿å…¶å®‰å…¨æ€§ä»¥é¿å…é£é™©è¾“å‡ºå¼•å‘çš„ç¤¾ä¼šå…³æ³¨å’Œè¿è§„AIæ³•è§„ã€‚</li>
<li>LALMä¸­çš„éŸ³é¢‘ç¼–ç å™¨å¸¦æ¥æ¢¯åº¦ç¢è£‚é—®é¢˜ï¼Œå¢åŠ äº†å¯¹å…¶è¿›è¡Œæ”»å‡»çš„å¤æ‚æ€§ã€‚</li>
<li>AdvWaveæ¡†æ¶æ˜¯è§£å†³LALMå®‰å…¨æ€§çš„é¦–ä¸ªæ”»å‡»æ¡†æ¶ï¼Œé‡‡ç”¨åŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•è§£å†³æ¢¯åº¦ç¢è£‚é—®é¢˜ã€‚</li>
<li>AdvWaveé€šè¿‡è‡ªé€‚åº”å¯¹æŠ—ç›®æ ‡æœç´¢ç®—æ³•åŠ¨æ€è°ƒæ•´å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ï¼Œåº”å¯¹LALMè¡Œä¸ºå¯å˜æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>AdvWaveè®¾è®¡äº†åˆ†ç±»å™¨å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„å¯¹æŠ—æ€§å™ªå£°æ¨¡ä»¿å¸¸è§åŸå¸‚å£°éŸ³ï¼Œä¿æŒå¯¹äººç±»å¬è§‰çš„è‡ªç„¶æ„ŸçŸ¥ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec334f447e36183801430adf2a52ff1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c71d1c51dc501da740ab9e363d042ab.jpg" align="middle">
</details>




<h2 id="Bilevel-Joint-Unsupervised-and-Supervised-Training-for-Automatic-Speech-Recognition"><a href="#Bilevel-Joint-Unsupervised-and-Supervised-Training-for-Automatic-Speech-Recognition" class="headerlink" title="Bilevel Joint Unsupervised and Supervised Training for Automatic Speech   Recognition"></a>Bilevel Joint Unsupervised and Supervised Training for Automatic Speech   Recognition</h2><p><strong>Authors:Xiaodong Cui, A F M Saif, Songtao Lu, Lisha Chen, Tianyi Chen, Brian Kingsbury, George Saon</strong></p>
<p>In this paper, we propose a bilevel joint unsupervised and supervised training (BL-JUST) framework for automatic speech recognition. Compared to the conventional pre-training and fine-tuning strategy which is a disconnected two-stage process, BL-JUST tries to optimize an acoustic model such that it simultaneously minimizes both the unsupervised and supervised loss functions. Because BL-JUST seeks matched local optima of both loss functions, acoustic representations learned by the acoustic model strike a good balance between being generic and task-specific. We solve the BL-JUST problem using penalty-based bilevel gradient descent and evaluate the trained deep neural network acoustic models on various datasets with a variety of architectures and loss functions. We show that BL-JUST can outperform the widely-used pre-training and fine-tuning strategy and some other popular semi-supervised techniques. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åŒå±‚è”åˆæ— ç›‘ç£ä¸ç›‘ç£è®­ç»ƒï¼ˆBL-JUSTï¼‰æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ç›¸æ¯”ï¼Œå®ƒæ˜¯ä¸€ä¸ªè„±ç¦»çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼ŒBL-JUSTè¯•å›¾ä¼˜åŒ–å£°å­¦æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶æœ€å°åŒ–æ— ç›‘ç£å’Œæœ‰ç›‘ç£çš„æŸå¤±å‡½æ•°ã€‚ç”±äºBL-JUSTå¯»æ‰¾ä¸¤ä¸ªæŸå¤±å‡½æ•°çš„åŒ¹é…å±€éƒ¨æœ€ä¼˜è§£ï¼Œå› æ­¤å£°å­¦æ¨¡å‹æ‰€å­¦ä¹ çš„å£°å­¦è¡¨ç°å®ç°äº†é€šç”¨æ€§å’Œç‰¹å®šä»»åŠ¡çš„è‰¯å¥½å¹³è¡¡ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºç½šåˆ†çš„åŒå±‚æ¢¯åº¦ä¸‹é™æ¥è§£å†³BL-JUSTé—®é¢˜ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸Šè¯„ä¼°ç»è¿‡è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œå£°å­¦æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å…·æœ‰å„ç§æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å±•ç¤ºäº†BL-JUSTå¯ä»¥è¶…è¶Šå¹¿æ³›ä½¿ç”¨çš„é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ä»¥åŠå…¶ä»–ä¸€äº›æµè¡Œçš„åŠç›‘ç£æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08548v1">PDF</a> Accepted by IEEE&#x2F;ACM Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åä¸ºBL-JUSTï¼ˆåŒå±‚è”åˆæ— ç›‘ç£ä¸ç›‘ç£è®­ç»ƒï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿæ–­å¼€ä¸¤é˜¶æ®µçš„é¢„è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ä¸åŒï¼ŒBL-JUSTè¯•å›¾ä¼˜åŒ–å£°å­¦æ¨¡å‹ï¼Œä½¿æ¨¡å‹èƒ½åŒæ—¶æœ€å°åŒ–æ— ç›‘ç£å’Œç›‘ç£æŸå¤±å‡½æ•°ã€‚æ­¤ç­–ç•¥ä¸‹å­¦ä¹ çš„å£°å­¦è¡¨ç¤ºå…¼å…·é€šç”¨æ€§å’Œä»»åŠ¡ç‰¹å¼‚æ€§ã€‚é€šè¿‡åŸºäºæƒ©ç½šçš„åŒå±‚æ¢¯åº¦ä¸‹é™è§£å†³BL-JUSTé—®é¢˜ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†ä¸Šè¯„ä¼°äº†è®­ç»ƒåçš„æ·±åº¦ç¥ç»ç½‘ç»œå£°å­¦æ¨¡å‹ï¼Œæ˜¾ç¤ºBL-JUSTä¼˜äºæµè¡Œçš„é¢„è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ä»¥åŠå…¶ä»–åŠç›‘ç£æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¡†æ¶BL-JUSTã€‚</li>
<li>BL-JUSTç»“åˆäº†æ— ç›‘ç£ä¸ç›‘ç£è®­ç»ƒï¼Œæ—¨åœ¨ä¼˜åŒ–å£°å­¦æ¨¡å‹ã€‚</li>
<li>ä¸ä¼ ç»Ÿé¢„è®­ç»ƒä¸å¾®è°ƒç­–ç•¥ç›¸æ¯”ï¼ŒBL-JUSTèƒ½åŒæ—¶æœ€å°åŒ–æ— ç›‘ç£å’Œç›‘ç£æŸå¤±å‡½æ•°ã€‚</li>
<li>BL-JUSTç­–ç•¥ä¸‹çš„å£°å­¦è¡¨ç¤ºå…¼å…·é€šç”¨æ€§å’Œä»»åŠ¡ç‰¹å¼‚æ€§ã€‚</li>
<li>æ–‡ç« ä½¿ç”¨äº†åŸºäºæƒ©ç½šçš„åŒå±‚æ¢¯åº¦ä¸‹é™æ¥è§£å†³BL-JUSTé—®é¢˜ã€‚</li>
<li>åœ¨å¤šç§æ•°æ®é›†å’Œæ¶æ„ä¸Šè¯„ä¼°äº†BL-JUSTçš„å£°å­¦æ¨¡å‹è¡¨ç°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13d0661540efff6aa6df7452a885cc8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56488b7c9ec2950e3911c6ef8b92bbe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e9c4413afdf631a1da701fc2457bf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be0444451118e4360c0695914064f710.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb11e2f8b6e9595c3aba7f8de129ff45.jpg" align="middle">
</details>




<h2 id="GR-NLP-TOOLKIT-An-Open-Source-NLP-Toolkit-for-Modern-Greek"><a href="#GR-NLP-TOOLKIT-An-Open-Source-NLP-Toolkit-for-Modern-Greek" class="headerlink" title="GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek"></a>GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek</h2><p><strong>Authors:Lefteris Loukas, Nikolaos Smyrnioudis, Chrysa Dikonomaki, Spyros Barbakos, Anastasios Toumazatos, John Koutsikakis, Manolis Kyriakakis, Mary Georgiou, Stavros Vassos, John Pavlopoulos, Ion Androutsopoulos</strong></p>
<p>We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP) toolkit developed specifically for modern Greek. The toolkit provides state-of-the-art performance in five core NLP tasks, namely part-of-speech tagging, morphological tagging, dependency parsing, named entity recognition, and Greeklishto-Greek transliteration. The toolkit is based on pre-trained Transformers, it is freely available, and can be easily installed in Python (pip install gr-nlp-toolkit). It is also accessible through a demonstration platform on HuggingFace, along with a publicly available API for non-commercial use. We discuss the functionality provided for each task, the underlying methods, experiments against comparable open-source toolkits, and future possible enhancements. The toolkit is available at: <a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit">https://github.com/nlpaueb/gr-nlp-toolkit</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºGR-NLP-TOOLKITï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç°ä»£å¸Œè…Šè¯­å¼€å‘çš„å¼€æºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…åœ¨äº”ä¸ªæ ¸å¿ƒNLPä»»åŠ¡ä¸­æä¾›äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œå³è¯æ€§æ ‡æ³¨ã€å½¢æ€æ ‡æ³¨ã€ä¾å­˜è§£æã€å‘½åå®ä½“è¯†åˆ«å’Œå¸Œè…Šè¯­è½¬å†™ã€‚è¯¥å·¥å…·åŒ…åŸºäºé¢„è®­ç»ƒçš„Transformerï¼Œå¯ä»¥å…è´¹ä½¿ç”¨ï¼Œå¹¶ä¸”å¯ä»¥åœ¨Pythonä¸­è½»æ¾å®‰è£…ï¼ˆé€šè¿‡pip install gr-nlp-toolkitï¼‰ã€‚å®ƒè¿˜å¯ä»¥é€šè¿‡HuggingFaceä¸Šçš„æ¼”ç¤ºå¹³å°å’Œé¢å‘éå•†ä¸šç”¨é€”çš„å…¬å¼€APIè¿›è¡Œè®¿é—®ã€‚æˆ‘ä»¬è®¨è®ºäº†é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æä¾›çš„åŠŸèƒ½ã€åº•å±‚æ–¹æ³•ã€ä¸ç±»ä¼¼å¼€æºå·¥å…·åŒ…çš„å®éªŒå¯¹æ¯”ä»¥åŠæœªæ¥å¯èƒ½çš„æ”¹è¿›ã€‚è¯¥å·¥å…·åŒ…å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit">https://github.com/nlpaueb/gr-nlp-toolkit</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08520v1">PDF</a> Accepted Demo Paper @ COLING 2025 (Github:   <a target="_blank" rel="noopener" href="https://github.com/nlpaueb/gr-nlp-toolkit/">https://github.com/nlpaueb/gr-nlp-toolkit/</a>, Demo:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo">https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo</a>, API:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API">https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API</a>)</p>
<p><strong>Summary</strong></p>
<p>GR-NLP-TOOLKITæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç°ä»£å¸Œè…Šè¯­è®¾è®¡çš„å¼€æºè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚å®ƒæä¾›äº†äº”ä¸ªæ ¸å¿ƒNLPä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€å½¢æ€æ ‡æ³¨ã€ä¾å­˜è§£æã€å‘½åå®ä½“è¯†åˆ«å’Œå¸Œè‹±äº’è½¬ç­‰ã€‚è¯¥å·¥å…·åŒ…åŸºäºé¢„è®­ç»ƒçš„Transformeræ¨¡å‹ï¼Œæ˜“äºåœ¨Pythonä¸­å®‰è£…å’Œä½¿ç”¨ï¼Œå¹¶å¯é€šè¿‡HuggingFaceä¸Šçš„æ¼”ç¤ºå¹³å°å’Œå…¬å¼€APIè¿›è¡Œè®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GR-NLP-TOOLKITæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç°ä»£å¸Œè…Šè¯­è®¾è®¡çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚</li>
<li>å®ƒæä¾›äº†è¯æ€§æ ‡æ³¨ã€å½¢æ€æ ‡æ³¨ã€ä¾å­˜è§£æã€å‘½åå®ä½“è¯†åˆ«å’Œå¸Œè‹±äº’è½¬ç­‰äº”ä¸ªæ ¸å¿ƒNLPä»»åŠ¡çš„åŠŸèƒ½ã€‚</li>
<li>GR-NLP-TOOLKITåŸºäºé¢„è®­ç»ƒçš„Transformeræ¨¡å‹ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>è¯¥å·¥å…·åŒ…æ˜¯å¼€æºçš„ï¼Œæ˜“äºåœ¨Pythonä¸­å®‰è£…å’Œä½¿ç”¨ã€‚</li>
<li>GR-NLP-TOOLKITå¯é€šè¿‡HuggingFaceä¸Šçš„æ¼”ç¤ºå¹³å°å’Œå…¬å¼€APIè¿›è¡Œè®¿é—®ã€‚</li>
<li>æ–‡ç« æåˆ°äº†ä¸å…¶ä»–å¼€æºå·¥å…·åŒ…çš„å®éªŒå¯¹æ¯”ï¼Œå±•ç¤ºäº†è¯¥å·¥å…·åŒ…çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1671657de697805d937ea34c10bbf126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9233c8f9e9c07907e734375b7ba10c51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3470d927cb9de26810790614e5921b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b929042d8e02d5a1848f9865d52d1ff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0c6ec8c7430573bcf5384172ef2b74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-914377345c34db0469799d222b297c72.jpg" align="middle">
</details>




<h2 id="Evaluating-the-Impact-of-Discriminative-and-Generative-E2E-Speech-Enhancement-Models-on-Syllable-Stress-Preservation"><a href="#Evaluating-the-Impact-of-Discriminative-and-Generative-E2E-Speech-Enhancement-Models-on-Syllable-Stress-Preservation" class="headerlink" title="Evaluating the Impact of Discriminative and Generative E2E Speech   Enhancement Models on Syllable Stress Preservation"></a>Evaluating the Impact of Discriminative and Generative E2E Speech   Enhancement Models on Syllable Stress Preservation</h2><p><strong>Authors:Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Sai Harshitha Aluru, Chiranjeevi Yarra</strong></p>
<p>Automatic syllable stress detection is a crucial component in Computer-Assisted Language Learning (CALL) systems for language learners. Current stress detection models are typically trained on clean speech, which may not be robust in real-world scenarios where background noise is prevalent. To address this, speech enhancement (SE) models, designed to enhance speech by removing noise, might be employed, but their impact on preserving syllable stress patterns is not well studied. This study examines how different SE models, representing discriminative and generative modeling approaches, affect syllable stress detection under noisy conditions. We assess these models by applying them to speech data with varying signal-to-noise ratios (SNRs) from 0 to 20 dB, and evaluating their effectiveness in maintaining stress patterns. Additionally, we explore different feature sets to determine which ones are most effective for capturing stress patterns amidst noise. To further understand the impact of SE models, a human-based perceptual study is conducted to compare the perceived stress patterns in SE-enhanced speech with those in clean speech, providing insights into how well these models preserve syllable stress as perceived by listeners. Experiments are performed on English speech data from non-native speakers of German and Italian. And the results reveal that the stress detection performance is robust with the generative SE models when heuristic features are used. Also, the observations from the perceptual study are consistent with the stress detection outcomes under all SE models. </p>
<blockquote>
<p>è‡ªåŠ¨éŸ³èŠ‚é‡éŸ³æ£€æµ‹æ˜¯è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰ç³»ç»Ÿå¯¹å­¦ä¹ è€…çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚å½“å‰çš„åº”åŠ›æ£€æµ‹æ¨¡å‹é€šå¸¸æ˜¯åœ¨å¹²å‡€è¯­éŸ³ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™åœ¨èƒŒæ™¯å™ªå£°æ™®éå­˜åœ¨çš„ç°å®åœºæ™¯ä¸­å¯èƒ½ä¸å¤Ÿç¨³å¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¯ä»¥é‡‡ç”¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹ï¼Œå®ƒé€šè¿‡æ¶ˆé™¤å™ªå£°æ¥æé«˜è¯­éŸ³è´¨é‡ï¼Œä½†å¯¹ä¿æŒéŸ³èŠ‚é‡éŸ³æ¨¡å¼çš„å½±å“å°šæœªè¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä»£è¡¨åŒºåˆ†æ€§å’Œç”Ÿæˆæ€§å»ºæ¨¡æ–¹æ³•çš„ä¸åŒSEæ¨¡å‹åœ¨å™ªå£°æ¡ä»¶ä¸‹å¯¹éŸ³èŠ‚é‡éŸ³æ£€æµ‹çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä»0åˆ°20åˆ†è´çš„ä¸åŒè¯­éŸ³æ•°æ®è¿›è¡Œåº”ç”¨è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¿æŒé‡éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸åŒçš„ç‰¹å¾é›†ï¼Œä»¥ç¡®å®šå“ªäº›ç‰¹å¾åœ¨å™ªå£°ä¸­æ•æ‰é‡éŸ³æ¨¡å¼æœ€æœ‰æ•ˆã€‚ä¸ºäº†è¿›ä¸€æ­¥äº†è§£SEæ¨¡å‹çš„å½±å“ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹åŸºäºäººç±»æ„ŸçŸ¥çš„ç ”ç©¶ï¼Œæ¯”è¾ƒäº†SEå¢å¼ºè¯­éŸ³å’Œå¹²å‡€è¯­éŸ³çš„æ„ŸçŸ¥é‡éŸ³æ¨¡å¼ï¼Œä»è€Œäº†è§£è¿™äº›æ¨¡å‹å¦‚ä½•å¾ˆå¥½åœ°ä¿ç•™å¬ä¼—æ„ŸçŸ¥åˆ°çš„éŸ³èŠ‚é‡éŸ³ã€‚å®éªŒæ˜¯åœ¨éå¾·è¯­å’Œæ„å¤§åˆ©è¯­æ¯è¯­è€…çš„è‹±è¯­è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œçš„ã€‚ç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨å¯å‘å¼ç‰¹å¾æ—¶ï¼Œç”Ÿæˆæ€§SEæ¨¡å‹çš„åº”åŠ›æ£€æµ‹æ€§èƒ½æ˜¯ç¨³å¥çš„ã€‚æ­¤å¤–ï¼Œæ„ŸçŸ¥ç ”ç©¶ä¸­çš„è§‚å¯Ÿç»“æœä¸æ‰€æœ‰SEæ¨¡å‹ä¸‹çš„åº”åŠ›æ£€æµ‹ç»“æœç›¸ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08306v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªåŠ¨éŸ³èŠ‚é‡éŸ³æ£€æµ‹åœ¨è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰ç³»ç»Ÿä¸­çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å«å™ªç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä¸ºäº†æ”¹å–„ç°å®åœºæ™¯ä¸­é‡éŸ³æ£€æµ‹çš„é²æ£’æ€§ï¼Œæœ¬æ–‡æ¢è®¨äº†ä¸åŒè¯­éŸ³å¢å¼ºæ¨¡å‹å¯¹éŸ³èŠ‚é‡éŸ³æ£€æµ‹çš„å½±å“ã€‚ç ”ç©¶é€šè¿‡åœ¨ä¸åŒä¿¡å™ªæ¯”ï¼ˆSNRï¼‰çš„è¯­éŸ³æ•°æ®ä¸Šåº”ç”¨ä¸åŒæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨ç»´æŒé‡éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†æœ‰æ•ˆçš„ç‰¹å¾é›†ï¼Œå¹¶è¿›è¡Œäº†ä¸€ç³»åˆ—æ„ŸçŸ¥ç ”ç©¶æ¥å¯¹æ¯”SEå¢å¼ºè¯­éŸ³ä¸æ™®é€šè¯­éŸ³çš„é‡éŸ³æ„ŸçŸ¥æ•ˆæœã€‚å®éªŒç»“æœå±•ç¤ºäº†ä½¿ç”¨å¯å‘å¼ç‰¹å¾æ—¶çš„ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæ¨¡å‹å¯¹äºé‡éŸ³æ£€æµ‹æ€§èƒ½çš„æå‡ã€‚æ„ŸçŸ¥ç ”ç©¶çš„ç»“æœä¹Ÿä¸æ‰€æœ‰è¯­éŸ³å¢å¼ºæ¨¡å‹ä¸‹çš„é‡éŸ³æ£€æµ‹ç»“æœä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨éŸ³èŠ‚é‡éŸ³æ£€æµ‹åœ¨è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰ç³»ç»Ÿä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å½“å‰çš„é‡éŸ³æ£€æµ‹æ¨¡å‹ä¸»è¦åœ¨å¹²å‡€è¯­éŸ³ç¯å¢ƒä¸‹è®­ç»ƒï¼Œç°å®å«å™ªç¯å¢ƒå¯èƒ½æ•ˆæœä¸ä½³ã€‚</li>
<li>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹ç”¨äºæå‡å«å™ªè¯­éŸ³çš„é‡éŸ³æ£€æµ‹æ€§èƒ½ï¼Œä½†å¯¹å…¶å½±å“çš„ç ”ç©¶å°šä¸å……åˆ†ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸åŒä¿¡å™ªæ¯”çš„è¯­éŸ³æ•°æ®è¯„ä¼°äº†å¤šç§SEæ¨¡å‹åœ¨ç»´æŒé‡éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ‰æ•ˆçš„ç‰¹å¾é›†å¯¹äºæ•æ‰é‡éŸ³æ¨¡å¼è‡³å…³é‡è¦ã€‚</li>
<li>ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæ¨¡å‹ä¸å¯å‘å¼ç‰¹å¾ç»“åˆæ—¶ï¼Œé‡éŸ³æ£€æµ‹æ€§èƒ½æ›´ä¸ºç¨³å¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9afdca59561274ab12b5678a3cd8d8d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda9920ae92bf8647bb6d14579682541.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f68108814ceffc787171ff5052537240.jpg" align="middle">
</details>




<h2 id="TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch"><a href="#TouchTTS-An-Embarrassingly-Simple-TTS-Framework-that-Everyone-Can-Touch" class="headerlink" title="TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"></a>TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch</h2><p><strong>Authors:Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu</strong></p>
<p>It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data. </p>
<blockquote>
<p>åŸºäºLLMçš„ç³»ç»Ÿä¼—æ‰€å‘¨çŸ¥æ˜¯æ•°æ®å¯†é›†å‹ç³»ç»Ÿã€‚æœ€è¿‘çš„åŸºäºLLMçš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å·¥ä½œé€šå¸¸é‡‡ç”¨å¤æ‚çš„æ•°æ®å¤„ç†ç®¡é“æ¥è·å¾—é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›å…ˆè¿›çš„æ•°æ®å¤„ç†ç®¡é“åœ¨æ¯ä¸ªé˜¶æ®µéƒ½éœ€è¦ä¼˜ç§€çš„æ¨¡å‹ï¼ˆä¾‹å¦‚è¯­éŸ³é™å™ªã€è¯­éŸ³å¢å¼ºã€è¯´è¯äººåˆ†å—å’Œæ ‡ç‚¹æ¨¡å‹ï¼‰ï¼Œè€Œè¿™äº›æ¨¡å‹æœ¬èº«åˆéœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¾ˆå°‘å¼€æºã€‚å³ä½¿ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨ä»¥åŠæ ‡ç‚¹ç¬¦å·ä¸å®é™…è¯­éŸ³åœé¡¿ä¹‹é—´çš„ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œä¸¥æ ¼çš„è¿‡æ»¤ç­–ç•¥å¾€å¾€åªä¿ç•™åŸå§‹æ•°æ®çš„10-30%ï¼Œæå¤§åœ°é˜»ç¢äº†æ•°æ®æ‰©å±•å·¥ä½œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡äº†ä¸€ä¸ªç®€åŒ–è€Œæœ‰æ•ˆçš„TTSæ•°æ®å¤„ç†ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿåœ¨ä¿æŒæ•°æ®è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½æ•°æ®è·å–æˆæœ¬ï¼Œå®ç°è¶…è¿‡50%çš„æ•°æ®ä¿ç•™ç‡ã€‚é™¤äº†æ•°æ®æ‰©å±•æŒ‘æˆ˜ä¹‹å¤–ï¼ŒåŸºäºLLMçš„TTSç³»ç»Ÿçš„éƒ¨ç½²æˆæœ¬ä¹Ÿé«˜äºä¼ ç»Ÿæ–¹æ³•ã€‚å½“å‰çš„ç³»ç»Ÿé€šå¸¸ä»…ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬åˆ°æ ‡è®°çš„ç”Ÿæˆï¼Œè€Œéœ€è¦é¢å¤–çš„æ¨¡å‹ï¼ˆå¦‚æµåŒ¹é…æ¨¡å‹ï¼‰è¿›è¡Œæ ‡è®°åˆ°æ³¢å½¢ç”Ÿæˆçš„è½¬æ¢ï¼Œè¿™äº›æ¨¡å‹ä¸èƒ½ç”±LLMæ¨ç†å¼•æ“ç›´æ¥æ‰§è¡Œï¼Œè¿›ä¸€æ­¥å¢åŠ äº†éƒ¨ç½²çš„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†LLMå’Œæµç»„ä»¶ä¸­çš„å†—ä½™æ¨¡å—ï¼Œå¹¶ç”¨LLMæ¶æ„æ›¿ä»£æµæ¨¡å‹çš„ä¸»å¹²ã€‚åŸºäºè¿™ç§ç®€åŒ–çš„æµä¸»å¹²ï¼Œæˆ‘ä»¬æå‡ºäº†æµå¼å’Œéæµå¼æ¨ç†çš„ç»Ÿä¸€æ¶æ„ï¼Œå¤§å¤§é™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚æœ€åï¼Œç”±äºç®€åŒ–çš„ç®¡é“å’ŒS3Tokenizeré™ä½äº†å¯¹TTSè®­ç»ƒæ•°æ®çš„è´¨é‡è¦æ±‚ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨åŒä¸€æ•°æ®è¿›è¡ŒTTSå’ŒASRä»»åŠ¡è®­ç»ƒçš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08237v1">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLMåœ¨TTSç³»ç»Ÿä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ•°æ®è·å–å’Œéƒ¨ç½²æˆæœ¬è¾ƒé«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ç®€åŒ–çš„TTSæ•°æ®å¤„ç†ç®¡é“å’Œç»Ÿä¸€çš„æ¶æ„ã€‚åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰æé«˜æ•°æ®è´¨é‡å¹¶é™ä½è·å–æˆæœ¬ï¼Œå®ç°æ•°æ®ä¿ç•™ç‡è¶…è¿‡50%ã€‚åŒæ—¶ç®€åŒ–LLMå’Œæµç»„ä»¶çš„å†—ä½™æ¨¡å—ï¼Œä½¿ç”¨LLMæ¶æ„æ›¿ä»£æµæ¨¡å‹ä¸»å¹²ï¼Œæå‡ºç»Ÿä¸€æ¶æ„ç”¨äºæµå¼å’Œéæµå¼æ¨æ–­ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç®€åŒ–ç®¡é“å’ŒS3Tokenizerï¼Œæ¢ç´¢äº†ç»Ÿä¸€TTSå’ŒASRä»»åŠ¡çš„å¯è¡Œæ€§ï¼Œé™ä½äº†TTSè®­ç»ƒæ•°æ®çš„è´¨é‡è¦æ±‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLM-based TTSç³»ç»Ÿå¯¹æ•°æ®çš„éœ€æ±‚æé«˜ï¼Œéœ€è¦å¤æ‚çš„æ•°æ®å¤„ç†ç®¡é“å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>ç°æœ‰çš„æ•°æ®å¤„ç†ç®¡é“åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œå¦‚è¯­éŸ³å»å™ªã€å¢å¼ºã€è¯´è¯äººåˆ†åŒ–å’Œæ ‡ç‚¹æ¨¡å‹ç­‰ï¼Œè¿™äº›é˜¶æ®µéƒ½éœ€è¦é«˜è´¨é‡çš„æ¨¡å‹å’Œæ•°æ®è¿›è¡Œæ”¯æŒï¼Œä½†å¾ˆå°‘å¼€æºã€‚</li>
<li>é¢ä¸´èƒŒæ™¯å™ªå£°å»é™¤ä¸å®Œå…¨å’Œæ ‡ç‚¹ä¸å®é™…è¯­éŸ³åœé¡¿ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å™ªå£°é²æ£’çš„éŸ³é¢‘åˆ†è¯å™¨ï¼ˆS3Tokenizerï¼‰è®¾è®¡ç®€åŒ–çš„TTSæ•°æ®å¤„ç†ç®¡é“ï¼Œæé«˜æ•°æ®è´¨é‡å¹¶é™ä½è·å–æˆæœ¬ï¼Œæ•°æ®ä¿ç•™ç‡è¶…è¿‡50%ã€‚</li>
<li>LLM-based TTSç³»ç»Ÿçš„éƒ¨ç½²æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦é€šè¿‡ç®€åŒ–å†—ä½™æ¨¡å—å’Œé‡‡ç”¨ç»Ÿä¸€çš„æ¶æ„æ¥é™ä½è¿™äº›æˆæœ¬ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¶æ„ï¼Œæ”¯æŒæµå¼å’Œéæµå¼æ¨æ–­ï¼Œæ˜¾è‘—é™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d91e7313b79044b07753a26a37643ce9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bef70708fec7e4c6e8b76da4553082a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1139dee81eb14b1bc42512b6390cca27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87016fd509a91ca69e76f20923650156.jpg" align="middle">
</details>




<h2 id="LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation"><a href="#LatentSpeech-Latent-Diffusion-for-Text-To-Speech-Generation" class="headerlink" title="LatentSpeech: Latent Diffusion for Text-To-Speech Generation"></a>LatentSpeech: Latent Diffusion for Text-To-Speech Generation</h2><p><strong>Authors:Haowei Lou, Helen Paik, Pari Delir Haghighi, Wen Hu, Lina Yao</strong></p>
<p>Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆäººå·¥æ™ºèƒ½å› å…¶ç›¸è¾ƒäºå…¶ä»–ç”ŸæˆæŠ€æœ¯ï¼ˆå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼‰çš„å“è¶Šæ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶å®ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚ä¸»æµçš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿä¸»è¦åœ¨è°±ç©ºé—´ä¸­å°†è¾“å‡ºæ˜ å°„åˆ°æ¢…å°”é¢‘è°±ï¼Œä½†ç”±äºæ¢…å°”é¢‘è°±çš„ç¨€ç–æ€§ï¼Œå¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—è´Ÿè½½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LatentSpeechï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚LatentSpeeché€šè¿‡ä½¿ç”¨æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå°†ç›®æ ‡ç»´åº¦é™ä½åˆ°æ¢…å°”é¢‘è°±æ‰€éœ€ç»´åº¦çš„5%ï¼Œç®€åŒ–äº†æ–‡æœ¬åˆ°è¯­éŸ³ç¼–ç å™¨å’Œvocoderçš„å¤„ç†è¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™é¡¹ç ”ç©¶æ ‡å¿—ç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯ä¸­çš„é¦–æ¬¡é›†æˆï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLatentSpeechåœ¨è¯é”™è¯¯ç‡ä¸Šæé«˜äº†25%ï¼Œæ¢…å°”å€’è°±å¤±çœŸæé«˜äº†24%ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè¿™ä¸¤é¡¹æŒ‡æ ‡åˆ†åˆ«è¿›ä¸€æ­¥æé«˜åˆ°49.5%å’Œ26%ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†LatentSpeechåœ¨æ¨åŠ¨æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯æ–¹é¢é¢†å…ˆæŠ€æœ¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08117v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ‰©æ•£å¼ç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨ä¼—å¤šç”ŸæˆæŠ€æœ¯ä¸­è„±é¢–è€Œå‡ºï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ã€‚å°½ç®¡å®ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»è¢«å¿½è§†ã€‚ä¸»æµæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿä¸»è¦åœ¨è°±ç©ºé—´ä¸­å°†è¾“å‡ºæ˜ å°„åˆ°æ¢…å°”é¢‘è°±ï¼Œå¯¼è‡´æ¢…å°”é¢‘è°±çš„ç¨€ç–æ€§å¸¦æ¥è¾ƒé«˜çš„è®¡ç®—è´Ÿè½½ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†LatentSpeechï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨æ½œåœ¨åµŒå…¥ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼ŒLatentSpeechå°†ç›®æ ‡ç»´åº¦é™ä½åˆ°æ¢…å°”é¢‘è°±æ‰€éœ€ç»´åº¦çš„5%ï¼Œç®€åŒ–äº†æ–‡æœ¬è½¬è¯­éŸ³ç¼–ç å™¨åŠvocoderçš„å¤„ç†è¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™ä¸€ç ”ç©¶æ ‡å¿—ç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬è¯­éŸ³ä¸­çš„é¦–æ¬¡é›†æˆï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLatentSpeechåœ¨å•è¯é”™è¯¯ç‡ä¸Šæé«˜äº†25%ï¼Œæ¢…å°”å€’è°±å¤±çœŸæé«˜äº†24%ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè¿™ä¸¤é¡¹æŒ‡æ ‡åˆ†åˆ«æé«˜äº†49.5%å’Œ26%ï¼Œæ˜¾ç¤ºå‡ºLatentSpeechåœ¨æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£å¼ç”Ÿæˆäººå·¥æ™ºèƒ½è¡¨ç°å‡ºå¯¹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ç­‰ç”ŸæˆæŠ€æœ¯çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸»æµæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿä¸»è¦ä½¿ç”¨æ¢…å°”é¢‘è°±ï¼Œå¯¼è‡´é«˜è®¡ç®—è´Ÿè½½ã€‚</li>
<li>LatentSpeechæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹å®ç°é«˜æ•ˆé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>LatentSpeeché™ä½äº†ç›®æ ‡ç»´åº¦ï¼Œç®€åŒ–äº†å¤„ç†è¿‡ç¨‹ã€‚</li>
<li>LatentSpeechåœ¨å•è¯é”™è¯¯ç‡å’Œæ¢…å°”å€’è°±å¤±çœŸæ–¹é¢è¾ƒç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>éšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼ŒLatentSpeechçš„æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-823cfc8beca2a772fe155e8c2b8536bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc427fcee296f7351233b132ea2b344b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0247a663dcafe95eb4dfea609d414f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef2882fbc0dc39be0e103e2feaae8106.jpg" align="middle">
</details>




<h2 id="Style-agnostic-evaluation-of-ASR-using-multiple-reference-transcripts"><a href="#Style-agnostic-evaluation-of-ASR-using-multiple-reference-transcripts" class="headerlink" title="Style-agnostic evaluation of ASR using multiple reference transcripts"></a>Style-agnostic evaluation of ASR using multiple reference transcripts</h2><p><strong>Authors:Quinten McNamara, Miguel Ãngel del RÃ­o FernÃ¡ndez, Nishchal Bhandari, Martin Ratajczak, Danny Chen, Corey Miller, MigÃ¼el JettÃ©</strong></p>
<p>Word error rate (WER) as a metric has a variety of limitations that have plagued the field of speech recognition. Evaluation datasets suffer from varying style, formality, and inherent ambiguity of the transcription task. In this work, we attempt to mitigate some of these differences by performing style-agnostic evaluation of ASR systems using multiple references transcribed under opposing style parameters. As a result, we find that existing WER reports are likely significantly over-estimating the number of contentful errors made by state-of-the-art ASR systems. In addition, we have found our multireference method to be a useful mechanism for comparing the quality of ASR models that differ in the stylistic makeup of their training data and target task. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä½œä¸ºè¯„ä»·æŒ‡æ ‡å­˜åœ¨è¯¸å¤šå±€é™ï¼Œç»™è¯­éŸ³è¯†åˆ«é¢†åŸŸå¸¦æ¥äº†è¯¸å¤šå›°æ‰°ã€‚è¯„ä¼°æ•°æ®é›†çš„é£æ ¼ã€æ­£å¼ç¨‹åº¦ä»¥åŠè½¬å½•ä»»åŠ¡æœ¬èº«çš„å›ºæœ‰æ¨¡ç³Šæ€§å„ä¸ç›¸åŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡é‡‡ç”¨å¤šå‚è€ƒæ–¹å¼è¯„ä¼°ASRç³»ç»Ÿæ¥ç¼“è§£è¿™äº›å·®å¼‚ï¼Œè¿™äº›å‚è€ƒæ˜¯åŸºäºå¯¹ç«‹é£æ ¼å‚æ•°è½¬å½•çš„ã€‚å› æ­¤æˆ‘ä»¬å‘ç°ç°æœ‰çš„WERæŠ¥å‘Šå¾ˆå¯èƒ½é«˜ä¼°äº†å‰æ²¿ASRç³»ç»Ÿäº§ç”Ÿçš„å®è´¨æ€§é”™è¯¯æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å¤šå‚è€ƒæ–¹æ³•å¯¹äºæ¯”è¾ƒä¸åŒé£æ ¼çš„è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä»»åŠ¡çš„ASRæ¨¡å‹è´¨é‡éå¸¸æœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºè¯­éŸ³è¯†åˆ«é¢†åŸŸä¸­çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä½œä¸ºè¯„ä»·æŒ‡æ ‡å­˜åœ¨å¤šç§å±€é™æ€§ã€‚è¯„ä¼°æ•°æ®é›†å­˜åœ¨é£æ ¼å¤šæ ·ã€å½¢å¼æ­£å¼ä¸å¦ä»¥åŠè½¬å½•ä»»åŠ¡æœ¬èº«çš„å›ºæœ‰æ¨¡ç³Šæ€§ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨å¤šå‚è€ƒçš„è½¬å½•æ–¹å¼ï¼Œåœ¨ä¸åŒé£æ ¼å‚æ•°ä¸‹å¯¹è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè¿›è¡Œäº†é£æ ¼æ— å…³çš„è¯„ä»·ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„WERæŠ¥å‘Šå¾ˆå¯èƒ½é«˜ä¼°äº†å‰æ²¿è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å®è´¨æ€§é”™è¯¯æ•°é‡ã€‚åŒæ—¶ï¼Œå¤šå‚è€ƒæ–¹æ³•å¯¹äºæ¯”è¾ƒä¸åŒé£æ ¼è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä»»åŠ¡çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹è´¨é‡éå¸¸æœ‰ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä½œä¸ºè¯­éŸ³è¯†åˆ«é¢†åŸŸçš„è¯„ä»·æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è¯„ä¼°æ•°æ®é›†çš„é£æ ¼å¤šæ ·æ€§ã€å½¢å¼æ­£å¼ä¸å¦ä»¥åŠè½¬å½•ä»»åŠ¡çš„æ¨¡ç³Šæ€§æ˜¯é€ æˆè¿™äº›å±€é™æ€§çš„ä¸»è¦å› ç´ ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨å¤šå‚è€ƒçš„è½¬å½•æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¸åŒé£æ ¼å‚æ•°ä¸‹è¿›è¡Œé£æ ¼æ— å…³çš„è¯„ä»·ã€‚</li>
<li>ç°æœ‰WERæŠ¥å‘Šå¯èƒ½é«˜ä¼°äº†å‰æ²¿è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å®è´¨æ€§é”™è¯¯æ•°é‡ã€‚</li>
<li>å¤šå‚è€ƒæ–¹æ³•æœ‰åŠ©äºæ¯”è¾ƒä¸åŒé£æ ¼è®­ç»ƒæ•°æ®å’Œç›®æ ‡ä»»åŠ¡çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹è´¨é‡ã€‚</li>
<li>è¯¥ç ”ç©¶å°è¯•é€šè¿‡å¤šå‚è€ƒæ–¹æ³•ç¼“è§£è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè¯„ä»·ä¸­çš„å·®å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8100aa3369042097b569553fdb91a950.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2f8050d75582967cd02358fd1e52a26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d263a62158e7c69b13333453efe1157.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebf1585cb83308d7dade17759a71ae0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f63a64a21ad0d44c6c915ed7876b904.jpg" align="middle">
</details>




<h2 id="Real-time-Sign-Language-Recognition-Using-MobileNetV2-and-Transfer-Learning"><a href="#Real-time-Sign-Language-Recognition-Using-MobileNetV2-and-Transfer-Learning" class="headerlink" title="Real-time Sign Language Recognition Using MobileNetV2 and Transfer   Learning"></a>Real-time Sign Language Recognition Using MobileNetV2 and Transfer   Learning</h2><p><strong>Authors:Smruti Jagtap, Kanika Jadhav, Rushikesh Temkar, Minal Deshmukh</strong></p>
<p>The hearing-impaired community in India deserves the access to tools that help them communicate, however, there is limited known technology solutions that make use of Indian Sign Language (ISL) at present. Even though there are many ISL users, ISL cannot access social and education arenas because there is not yet an efficient technology to convert the ISL signal into speech or text. We initiated this initiative owing to the rising demand for products and technologies that are inclusive and help ISL, filling the gap of communication for the ones with hearing disability. Our goal is to build an reliable sign language recognition system with the help of Convolutional Neural Networks (CNN) to . By expanding communication access, we aspire toward better educational opportunities and a more inclusive society for hearing impaired people in India. </p>
<blockquote>
<p>å°åº¦çš„å¬åŠ›éšœç¢ç¾¤ä½“åº”å½“è·å¾—èƒ½å¤Ÿå¸®åŠ©ä»–ä»¬æ²Ÿé€šçš„å·¥å…·ã€‚ç„¶è€Œï¼Œç›®å‰å¯åˆ©ç”¨çš„å°åº¦æ‰‹è¯­ï¼ˆISLï¼‰æŠ€æœ¯è§£å†³æ–¹æ¡ˆéå¸¸æœ‰é™ã€‚å°½ç®¡æœ‰è®¸å¤šä½¿ç”¨ISLçš„äººï¼Œä½†ç”±äºå°šæœªæœ‰é«˜æ•ˆçš„æŠ€æœ¯å°†ISLä¿¡å·è½¬æ¢ä¸ºè¯­éŸ³æˆ–æ–‡å­—ï¼Œå¯¼è‡´ä»–ä»¬æ— æ³•æ¥è§¦ç¤¾ä¼šå’Œæ•™è‚²çš„å„ä¸ªé¢†åŸŸã€‚æˆ‘ä»¬å‘èµ·è¿™ä¸€å€¡è®®æ˜¯å› ä¸ºå¸‚åœºå¯¹åŒ…å®¹æ€§å’Œå¸®åŠ©ISLçš„äº§å“å’ŒæŠ€æœ¯æœ‰ç€æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œå¼¥è¡¥äº†å¬åŠ›éšœç¢è€…çš„æ²Ÿé€šç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å€ŸåŠ©å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å»ºç«‹å¯é çš„æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡æ‰©å¤§æ²Ÿé€šæ¸ é“ï¼Œæˆ‘ä»¬æœŸæœ›ä¸ºå°åº¦çš„å¬åŠ›éšœç¢äººå£«æä¾›æ›´å¥½çš„æ•™è‚²æœºä¼šå’Œæ›´å…·åŒ…å®¹æ€§çš„ç¤¾ä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07486v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå°åº¦çš„å¬éšœç¾¤ä½“éœ€è¦æ²Ÿé€šå·¥å…·ï¼Œä½†ç›®å‰å°åº¦æ‰‹è¯­ï¼ˆISLï¼‰çš„åº”ç”¨æŠ€æœ¯è§£å†³æ–¹æ¡ˆæœ‰é™ã€‚å°½ç®¡æœ‰å¾ˆå¤šISLä½¿ç”¨è€…ï¼Œä½†ç”±äºç¼ºä¹å°†ISLä¿¡å·è½¬æ¢ä¸ºè¯­éŸ³æˆ–æ–‡å­—çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œä»–ä»¬æ— æ³•è¿›å…¥ç¤¾ä¼šå’Œæ•™è‚²çš„å„ä¸ªé¢†åŸŸã€‚æˆ‘ä»¬å‘èµ·è¿™ä¸€å€¡è®®ï¼Œæ—¨åœ¨å¼€å‘å¯é çš„æ‰‹è¯­è¯†åˆ«ç³»ç»Ÿï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç­‰æŠ€æœ¯ï¼Œä¸ºå¬éšœäººå£«æä¾›æ›´å¥½çš„æ•™è‚²æœºä¼šå’Œæ›´åŒ…å®¹çš„ç¤¾ä¼šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¬éšœç¾¤ä½“éœ€è¦æ˜“äºæ²Ÿé€šçš„è¾…åŠ©å·¥å…·ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å°åº¦æ‰‹è¯­ï¼ˆISLï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç›®å‰ç¼ºä¹å°†ISLä¿¡å·è½¬æ¢ä¸ºè¯­éŸ³æˆ–æ–‡å­—çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œé™åˆ¶äº†å¬éšœäººå£«çš„ç¤¾äº¤å’Œæ•™è‚²æœºä¼šã€‚</li>
<li>åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å»ºç«‹å¯é çš„æ‰‹è¯­è¯†åˆ«ç³»ç»Ÿæ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„å…³é”®ã€‚</li>
<li>é€šè¿‡æ‰©å¤§æ²Ÿé€šæ¸ é“ï¼Œä¸ºå¬éšœäººå£«æä¾›æ›´å¥½çš„æ•™è‚²æœºä¼šã€‚</li>
<li>æé«˜ç¤¾ä¼šå¯¹å¬éšœäººå£«çš„åŒ…å®¹æ€§æ˜¯å½“å‰çš„é‡è¦ç›®æ ‡ã€‚</li>
<li>æŠ€æœ¯åˆ›æ–°åœ¨å¸®åŠ©å¬éšœäººå£«èå…¥ç¤¾ä¼šæ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da4d0df39252ba738b590da304437be9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a872f8cb8b8c0514ca9ea9e047e158a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-309235b1c989ff255c1d135e9d752368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fed24bac77a824631d4a7b4553991f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4b5ac3245503cdd065d5d2f064a658d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5184640a16d47a3d979f8ac429ffb86f.jpg" align="middle">
</details>




<h2 id="Effective-Text-Adaptation-for-LLM-based-ASR-through-Soft-Prompt-Fine-Tuning"><a href="#Effective-Text-Adaptation-for-LLM-based-ASR-through-Soft-Prompt-Fine-Tuning" class="headerlink" title="Effective Text Adaptation for LLM-based ASR through Soft Prompt   Fine-Tuning"></a>Effective Text Adaptation for LLM-based ASR through Soft Prompt   Fine-Tuning</h2><p><strong>Authors:Yingyi Ma, Zhe Liu, Ozlem Kalinli</strong></p>
<p>The advent of Large Language Models (LLM) has reformed the Automatic Speech Recognition (ASR). Prompting LLM with audio embeddings to generate transcriptions becomes the new state-of-the-art ASR. Despite LLMs being trained with an extensive amount of text corpora, high-quality domain-specific text data can still significantly enhance ASR performance on domain adaptation tasks. Although LLM-based ASR can naturally incorporate more text corpora by fine-tuning the LLM decoder, fine-tuning such ASR on text-only data without paired prompts may diminish the effectiveness of domain-specific knowledge. To mitigate this issue, we propose a two-step soft prompt fine-tuning strategy that enhances domain-specific text adaptation. Experimental results show that text adaptation with our proposed method achieved a relative up to 9% Word Error Rate (WER) reduction and up to 18% Entity Error Rate (EER) reduction on the target domain compared to the baseline ASR. Combining this with domain-specific Language Model (LM) fusion can further improve the EER by a relative 2-5% </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°å·²ç»æ”¹å˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æŠ€æœ¯é¢è²Œã€‚ä½¿ç”¨éŸ³é¢‘åµŒå…¥æ¥æç¤ºLLMç”Ÿæˆè½¬å½•æœ¬å·²ç»æˆä¸ºæœ€æ–°çš„å‰æ²¿ASRæŠ€æœ¯ã€‚å°½ç®¡LLMé€šè¿‡å¤§é‡çš„æ–‡æœ¬è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œé«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®ä»ç„¶å¯ä»¥æ˜¾è‘—å¢å¼ºé¢†åŸŸé€‚åº”ä»»åŠ¡ä¸Šçš„ASRæ€§èƒ½ã€‚è™½ç„¶åŸºäºLLMçš„ASRå¯ä»¥é€šè¿‡å¾®è°ƒLLMè§£ç å™¨è‡ªç„¶åœ°èå…¥æ›´å¤šçš„æ–‡æœ¬è¯­æ–™åº“ï¼Œä½†åœ¨ä»…æœ‰æ–‡æœ¬æ•°æ®çš„æƒ…å†µä¸‹å¯¹è¿™æ ·çš„ASRè¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šé™ä½ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ä¸¤æ­¥èµ°çš„è½¯æç¤ºå¾®è°ƒç­–ç•¥ï¼Œä»¥å¼ºåŒ–ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç»“åˆè¿›è¡Œçš„æ–‡æœ¬é€‚åº”æ€§è®­ç»ƒä¸åŸºçº¿ASRç›¸æ¯”ï¼Œç›¸å¯¹é™ä½äº†é«˜è¾¾9%çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œé«˜è¾¾18%çš„å®ä½“é”™è¯¯ç‡ï¼ˆEERï¼‰ã€‚å°†å…¶ä¸ç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èåˆç›¸ç»“åˆï¼Œå¯ä»¥è¿›ä¸€æ­¥ç›¸å¯¹æé«˜2-5%çš„EERã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06967v1">PDF</a> accepted as SLT 2024 proceeding</p>
<p><strong>Summary</strong>ï¼š<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯å¾—åˆ°äº†é©æ–°ã€‚é€šè¿‡å‘LLMæ³¨å…¥éŸ³é¢‘åµŒå…¥æ¥ç”Ÿæˆè½¬å½•æœ¬å·²æˆä¸ºæœ€æ–°çš„ASRæŠ€æœ¯ä¸»æµã€‚è™½ç„¶LLMåœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œä½†é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®ä»ç„¶å¯ä»¥æ˜¾è‘—æé«˜åŸŸé€‚åº”ä»»åŠ¡çš„ASRæ€§èƒ½ã€‚ä¸ºäº†è§£å†³åœ¨ä»…æ–‡æœ¬æ•°æ®ä¸Šå¾®è°ƒASRæ—¶å¯èƒ½å‡ºç°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸¤æ­¥è½¯æç¤ºå¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ASRç›¸æ¯”ï¼Œä½¿ç”¨æ‰€æå‡ºçš„æ–¹æ³•åœ¨ç›®æ ‡é¢†åŸŸå®ç°äº†ç›¸å¯¹é«˜è¾¾9%çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½å’Œé«˜è¾¾18%çš„å®ä½“é”™è¯¯ç‡ï¼ˆEERï¼‰é™ä½ã€‚ç»“åˆç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èåˆæŠ€æœ¯ï¼Œå¯ä»¥è¿›ä¸€æ­¥å°†EERç›¸å¯¹æé«˜2-5%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼•å…¥é©æ–°äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡æ³¨å…¥éŸ³é¢‘åµŒå…¥åˆ°LLMä¸­ä»¥ç”Ÿæˆè½¬å½•æœ¬å·²æˆä¸ºæœ€æ–°çš„ASRä¸»æµæ–¹æ³•ã€‚</li>
<li>ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜ASRåœ¨åŸŸé€‚åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸¤æ­¥è½¯æç¤ºå¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜ç‰¹å®šé¢†åŸŸæ–‡æœ¬é€‚åº”æ€§ã€‚</li>
<li>ä¸åŸºçº¿ASRç›¸æ¯”ï¼Œè¯¥ç­–ç•¥å®ç°äº†æ˜¾è‘—é™ä½çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå®ä½“é”™è¯¯ç‡ï¼ˆEERï¼‰ã€‚</li>
<li>ç»“åˆç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èåˆæŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db6f9e81c564c2baea34172f69335291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8502c1556cec1d4276370e422f9cbc59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c326572579e4be46117fa0f2b7eb4235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90cc62091becd597b71ff060787cceaa.jpg" align="middle">
</details>




<h2 id="Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data"><a href="#Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data" class="headerlink" title="Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data"></a>Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data</h2><p><strong>Authors:Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</strong></p>
<p>The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni%7D">https://huggingface.co/spaces/KE-Team/KE-Omni}</a>. </p>
<blockquote>
<p>GPT-4oä»£è¡¨äº†é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå®æ—¶äº¤äº’çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚å…¶æ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æµåˆ©æ€§ä¸ä»…å¼•èµ·äº†å…³æ³¨ï¼Œè¿˜åˆºæ¿€äº†è¯¥é¢†åŸŸçš„ç ”ç©¶å…´è¶£ã€‚è¿™ç§å®æ—¶è¯­éŸ³äº¤äº’åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå³æ—¶å“åº”çš„åœºæ™¯ä¸­å°¤å…¶æœ‰ä»·å€¼ï¼Œèƒ½æå¤§åœ°æå‡ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œå…³äºå®æ—¶å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œå°¤å…¶æ˜¯å¯¹ä¸­æ–‡çš„ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†KE-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ç¼çš„å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œå»ºç«‹åœ¨Ke-SpeechChatä¹‹ä¸Šã€‚Ke-SpeechChatæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡åˆæˆè¯­éŸ³äº¤äº’æ•°æ®é›†ï¼ŒåŒ…å«700ä¸‡ä¸­æ–‡å’Œè‹±æ–‡å¯¹è¯ï¼Œæ¶µç›–42,002åå‘è¨€è€…ï¼Œæ€»è®¡è¶…è¿‡6ä¸‡å°æ—¶ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚ç›¸å…³æ¼”ç¤ºå¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni]%E3%80%82">https://huggingface.co/spaces/KE-Team/KE-Omni]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01078v2">PDF</a> KE-Omni, Ke-SpeechChat</p>
<p><strong>Summary</strong></p>
<p>GPT-4oåœ¨é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå®æ—¶äº¤äº’æ–¹é¢å–å¾—äº†é‡è¦é‡Œç¨‹ç¢‘å¼çš„è¿›å±•ã€‚å…¶æ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æµç•…æ€§ä¸ä»…å¼•èµ·äº†å…³æ³¨ï¼Œè¿˜åˆºæ¿€äº†è¯¥é¢†åŸŸçš„ç ”ç©¶å…´è¶£ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå³æ—¶å“åº”çš„åœºæ™¯ä¸­ï¼Œè¿™ç§å®æ—¶è¯­éŸ³äº¤äº’å¤§å¤§å¢å¼ºäº†ç”¨æˆ·ä½“éªŒã€‚è¯¥ç ”ç©¶å›¢é˜Ÿæ¨å‡ºçš„KE-Omniå¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºKe-SpeechChatæ•°æ®é›†ï¼ŒåŒ…å«700ä¸‡ä¸­è‹±æ–‡å­—ç¬¦çš„ä¼šè¯æ•°æ®ï¼Œæ¶‰åŠ4ä¸‡å¤šåå‘è¨€è€…å’Œè¶…è¿‡6ä¸‡å°æ—¶çš„è¯­éŸ³äº¤äº’æ•°æ®ï¼Œå¯¹æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç›¸å…³æ¼”ç¤ºå¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oå®ç°äº†é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„å®æ—¶äº¤äº’ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æµç•…æ€§ã€‚</li>
<li>å®æ—¶è¯­éŸ³äº¤äº’åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå“åº”çš„åœºæ™¯ä¸­å¤§å¹…å¢å¼ºäº†ç”¨æˆ·ä½“éªŒã€‚</li>
<li>KE-Omniæ˜¯ä¸€ä¸ªæ— ç¼çš„å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºKe-SpeechChatæ•°æ®é›†æ„å»ºã€‚</li>
<li>Ke-SpeechChatæ•°æ®é›†åŒ…å«700ä¸‡ä¸­è‹±æ–‡å­—ç¬¦çš„ä¼šè¯æ•°æ®ï¼Œæ¶µç›–è¶…è¿‡6ä¸‡å°æ—¶çš„è¯­éŸ³äº¤äº’ã€‚</li>
<li>è¯¥æ¨¡å‹æ¶‰åŠ4ä¸‡å¤šåå‘è¨€è€…çš„å¤šæ ·åŒ–æ•°æ®ï¼Œå¯¹æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>KE-Omniæ¨¡å‹çš„æ¼”ç¤ºå¯é€šè¿‡ç‰¹å®šé“¾æ¥è®¿é—®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be63d03367d1646c10129ffa39a43920.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f6a84b1329d36c056c958d085b932a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e846567311c2df8847f41091a8724d2.jpg" align="middle">
</details>




<h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹æ€§å¯¹äºé€‚åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿè‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œé‚£å°±æ˜¯åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥è¶Šæ¥è¶Šå—åˆ°é‡è§†ï¼Œè¿™å¯èƒ½ä¼šåœ¨æ¸¸æˆå’Œè™šæ‹Ÿç°å®é¢†åŸŸæä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå®ƒå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œé€šè¿‡è°ƒæ•´åˆæˆçš„æ¢…å°”é¢‘è°±å›¾æ¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ‰€æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸å½±å“è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œæ ‡å¿—ç€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> ç´¢å¼•æœ¯è¯­-è¯­éŸ³åˆæˆã€åœºæ™¯æç¤ºã€ç©ºé—´æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v2">PDF</a> The paper is missing some information</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºI2TTSï¼ˆImage-indicated Immersive Text-to-speech Synthesisï¼‰çš„å¤šæ¨¡æ€æ–‡æœ¬è½¬è¯­éŸ³åˆæˆæ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿè°ƒæ•´åˆæˆçš„æ¢…å°”é¢‘è°±å›¾ï¼Œå¢å¼ºäº†æ²‰æµ¸æ„Ÿï¼Œå¹¶ç¡®ä¿æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸”ä¸å½±å“è¯­éŸ³çš„è‡ªç„¶æ€§ï¼Œæ ‡å¿—ç€è¯­å¢ƒæ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸçš„é‡è¦è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ä¸­ï¼Œé™¤äº†æŠ€æœ¯æ–¹é¢çš„è¯­éŸ³è¦ç´ å¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ç­‰ï¼Œç©ºé—´æ„ŸçŸ¥çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€TTSæ–¹æ³•â€”â€”I2TTSï¼Œå®ƒå¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨å¹¶ç»“åˆè§†è§‰åœºæ™¯æç¤ºæ¥å½±å“è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>I2TTSé€šè¿‡ä½¿ç”¨æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯æ¥è°ƒæ•´åˆæˆæ¢…å°”é¢‘è°±å›¾ï¼Œå¢å¼ºäº†è¯­éŸ³çš„æ²‰æµ¸æ„Ÿã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒI2TTSæ¨¡å‹èƒ½åœ¨ä¸æŸå¤±è¯­éŸ³è‡ªç„¶æ€§çš„å‰æä¸‹å®ç°é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨è™šæ‹Ÿç¯å¢ƒå’Œæ¸¸æˆç­‰éœ€è¦é«˜åº¦æ²‰æµ¸å¼ä½“éªŒçš„é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>é¡¹ç›®æ¼”ç¤ºé¡µé¢æä¾›äº†ç›´è§‚å±•ç¤ºï¼Œä¾¿äºè¿›ä¸€æ­¥äº†è§£å’ŒéªŒè¯è¯¥æ¨¡å‹çš„å®é™…æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66e6ccdd517a50e9ee5dddf9637b47e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4689ec2cdbacd48202667ffe499ad66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf7fa3b4564f4f1f3a720aedb6cba728.jpg" align="middle">
</details>




<h2 id="Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening"><a href="#Uncovering-the-role-of-semantic-and-acoustic-cues-in-normal-and-dichotic-listening" class="headerlink" title="Uncovering the role of semantic and acoustic cues in normal and dichotic   listening"></a>Uncovering the role of semantic and acoustic cues in normal and dichotic   listening</h2><p><strong>Authors:Akshara Soman, Sai Samrat Kankanala, Sriram Ganapathy</strong></p>
<p>Despite extensive research, the precise role of acoustic and semantic cues in complex speech perception tasks remains unclear. In this study, we propose a paradigm to understand the encoding of these cues in electroencephalogram (EEG) data, using match-mismatch (MM) classification task. The MM task involves determining whether the stimulus and response correspond to each other or not. We design a multi-modal sequence model, based on long short term memory (LSTM) architecture, to perform the MM task. The model is input with acoustic stimulus (derived from the speech envelope), semantic stimulus (derived from textual representations of the speech content), and neural response (derived from the EEG data). Our experiments are performed on two separate conditions, i) natural passive listening condition and, ii) an auditory attention based dichotic listening condition. Using the MM task as the analysis framework, we observe that - a) speech perception is fragmented based on word boundaries, b) acoustic and semantic cues offer similar levels of MM task performance in natural listening conditions, and c) semantic cues offer significantly improved MM classification over acoustic cues in dichotic listening task. Further, the study provides evidence of right ear advantage in dichotic listening conditions. </p>
<blockquote>
<p>å°½ç®¡è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œå£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚çš„è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾ç¡®ä½œç”¨ä»ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®ç¼–ç è¿™äº›çº¿ç´¢çš„ç†è§£èŒƒå¼ï¼Œé‡‡ç”¨åŒ¹é…ä¸åŒ¹é…ï¼ˆMMï¼‰åˆ†ç±»ä»»åŠ¡ã€‚MMä»»åŠ¡æ¶‰åŠç¡®å®šåˆºæ¿€å’Œå“åº”æ˜¯å¦ç›¸äº’å¯¹åº”ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºé•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰æ¶æ„çš„å¤šæ¨¡æ€åºåˆ—æ¨¡å‹ï¼Œä»¥æ‰§è¡ŒMMä»»åŠ¡ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥åŒ…æ‹¬å£°å­¦åˆºæ¿€ï¼ˆæ¥æºäºè¯­éŸ³åŒ…ç»œï¼‰ã€è¯­ä¹‰åˆºæ¿€ï¼ˆæ¥æºäºè¯­éŸ³å†…å®¹çš„æ–‡æœ¬è¡¨ç¤ºï¼‰å’Œç¥ç»å“åº”ï¼ˆæ¥æºäºEEGæ•°æ®ï¼‰ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨ä¸¤ç§ä¸åŒæ¡ä»¶ä¸‹è¿›è¡Œçš„ï¼Œå³ä¸€)è‡ªç„¶è¢«åŠ¨è†å¬æ¡ä»¶ï¼ŒäºŒ)åŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒæ­§å¬è¾¨æ¡ä»¶ã€‚ä»¥MMä»»åŠ¡ä¸ºåˆ†ææ¡†æ¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ša)è¯­éŸ³æ„ŸçŸ¥æ˜¯åŸºäºè¯ç•Œç‰‡æ®µåŒ–çš„ï¼›b)åœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹ï¼Œå£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨MMä»»åŠ¡ä¸­è¡¨ç°å‡ºç›¸ä¼¼çš„æ°´å¹³ï¼›c)åœ¨äºŒæ­§å¬è¾¨ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢åœ¨MMåˆ†ç±»æ–¹é¢æ˜¾è‘—ä¼˜äºå£°éŸ³çº¿ç´¢ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æä¾›äº†äºŒæ­§å¬è¾¨æ¡ä»¶ä¸‹å³è€³ä¼˜åŠ¿çš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11308v1">PDF</a> 9 Pages, 4 Figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†å£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢åœ¨å¤æ‚è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾ç¡®ä½œç”¨ã€‚é€šè¿‡åŒ¹é…-ä¸åŒ¹é…åˆ†ç±»ä»»åŠ¡ï¼Œæå‡ºä¸€ç§ç†è§£è¿™äº›çº¿ç´¢åœ¨è„‘ç”µå›¾æ•°æ®ä¸­çš„ç¼–ç èŒƒå¼ã€‚å®éªŒé‡‡ç”¨åŸºäºLSTMæ¶æ„çš„å¤šæ¨¡æ€åºåˆ—æ¨¡å‹æ¥å®ŒæˆåŒ¹é…-ä¸åŒ¹é…ä»»åŠ¡ã€‚å®éªŒåˆ†ä¸ºè‡ªç„¶è¢«åŠ¨è†å¬å’ŒåŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒæ­§å¬è§‰æµ‹è¯•ä¸¤ç§æ¡ä»¶ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°è¯­éŸ³æ„ŸçŸ¥åŸºäºè¯ç•Œç¢ç‰‡åŒ–ï¼Œåœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹å£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…åº¦ç›¸ä¼¼ï¼Œè€Œåœ¨äºŒæ­§å¬è§‰ä»»åŠ¡ä¸­è¯­ä¹‰çº¿ç´¢çš„åŒ¹é…åº¦æ˜¾è‘—é«˜äºå£°éŸ³çº¿ç´¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†äºŒæ­§å¬è§‰æ¡ä»¶ä¸‹å³è€³ä¼˜åŠ¿çš„è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æ¢è®¨äº†å¤æ‚è¯­éŸ³æ„ŸçŸ¥ä»»åŠ¡ä¸­å£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢çš„ç²¾ç¡®ä½œç”¨ã€‚</li>
<li>é‡‡ç”¨åŒ¹é…-ä¸åŒ¹é…åˆ†ç±»ä»»åŠ¡æ¥åˆ†æè¯­éŸ³æ„ŸçŸ¥ä¸­çš„ç¼–ç è¿‡ç¨‹ã€‚</li>
<li>å¤šæ¨¡æ€åºåˆ—æ¨¡å‹åŸºäºLSTMæ¶æ„å®ŒæˆåŒ¹é…-ä¸åŒ¹é…ä»»åŠ¡ã€‚</li>
<li>å®éªŒåœ¨è‡ªç„¶è¢«åŠ¨è†å¬å’ŒåŸºäºå¬è§‰æ³¨æ„åŠ›çš„äºŒæ­§å¬è§‰æµ‹è¯•ä¸¤ç§æ¡ä»¶ä¸‹è¿›è¡Œã€‚</li>
<li>è§‚å¯Ÿåˆ°è¯­éŸ³æ„ŸçŸ¥åŸºäºè¯ç•Œç¢ç‰‡åŒ–ã€‚</li>
<li>åœ¨è‡ªç„¶è†å¬æ¡ä»¶ä¸‹ï¼Œå£°éŸ³å’Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…åº¦ç›¸ä¼¼ï¼›è€Œåœ¨äºŒæ­§å¬è§‰ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰çº¿ç´¢çš„åŒ¹é…åº¦æ˜¾è‘—é«˜äºå£°éŸ³çº¿ç´¢ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-702194aa412ab66d9b55f848bb4b0802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f88c633e521445f6a5e0c1930330554c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2caaa7973dbbc8b7feaaeb52a5006feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77deb2d8824993a0831777932f5b46c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e94c411962448b979a6fa8a23923d9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4df85715af024e597b14960f49d00b90.jpg" align="middle">
</details>




<h2 id="Interactive-Cycle-Model-â€“-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses"><a href="#Interactive-Cycle-Model-â€“-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses" class="headerlink" title="Interactive Cycle Model â€“ The Linkage Combination among Automatic   Speech Recognition, Large Language Models and Smart Glasses"></a>Interactive Cycle Model â€“ The Linkage Combination among Automatic   Speech Recognition, Large Language Models and Smart Glasses</h2><p><strong>Authors:Libo Wang</strong></p>
<p>This research proposes the interaction loop model â€œASR-LLMs-Smart Glassesâ€, which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLMs. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>è¯¥ç ”ç©¶æå‡ºäº†â€œASR-LLMs-æ™ºèƒ½çœ¼é•œâ€äº’åŠ¨å¾ªç¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œï¼Œä»¥ä¿ƒè¿›æ— ç¼çš„äººæœºäº’åŠ¨ã€‚è€Œè¯¥ç ”ç©¶æ–¹æ³•æ¶‰åŠå°†äº’åŠ¨è¿‡ç¨‹åˆ†è§£æˆä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ã€‚è¯­éŸ³è¢«ASRæ•è·å¹¶å¤„ç†ï¼Œç„¶åé€šè¿‡LLMsè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚ç»“æœç„¶åä¼ è¾“åˆ°æ™ºèƒ½çœ¼é•œè¿›è¡Œæ˜¾ç¤ºã€‚å½“ç”¨æˆ·ä½¿ç”¨æ˜¾ç¤ºçš„æ•°æ®è¿›è¡Œäº’åŠ¨æ—¶ï¼Œåé¦ˆå¾ªç¯å°±å®Œæˆäº†ã€‚ä½¿ç”¨æ•°å­¦å…¬å¼æ¥é‡åŒ–æ¨¡å‹çš„è¡¨ç°ï¼Œå›´ç»•æ ¸å¿ƒè¯„ä¼°ç‚¹ï¼šASRè¯­éŸ³è½¬æ–‡æœ¬çš„å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿã€‚ç ”ç©¶ç»“æœä»ç†è®ºä¸Šæä¾›äº†æµ‹è¯•å’Œè¯„ä»·è¯¥æ¨¡å‹çš„å¯è¡Œæ€§å’Œæ€§èƒ½ã€‚è¯¦ç»†çš„æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ç»ä¸Šä¼ åˆ°Githubï¼Œé“¾æ¥æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10362v2">PDF</a> OpenReview submitted. 10 pages of text and 2 figures</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œASR-LLMs-æ™ºèƒ½çœ¼é•œâ€çš„äº’åŠ¨å¾ªç¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œï¼Œä¿ƒè¿›äº†æ— ç¼çš„äººæœºäº’åŠ¨ã€‚ç ”ç©¶æ–¹æ³•å°†äº’åŠ¨è¿‡ç¨‹åˆ†è§£æˆä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ã€‚è¯­éŸ³ç”±ASRæ•è·å¹¶å¤„ç†ï¼Œç„¶åé€šè¿‡LLMsè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚ç»“æœå†ä¼ è¾“åˆ°æ™ºèƒ½çœ¼é•œè¿›è¡Œæ˜¾ç¤ºã€‚å½“ç”¨æˆ·ä½¿ç”¨æ˜¾ç¤ºçš„æ•°æ®è¿›è¡Œäº’åŠ¨æ—¶ï¼Œåé¦ˆå¾ªç¯å°±å®Œæˆäº†ã€‚ç ”ç©¶ç”¨æ•°å­¦å…¬å¼å¯¹æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†é‡åŒ–è¯„ä¼°ï¼Œæ ¸å¿ƒè¯„ä¼°ç‚¹åŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿæ€§ã€‚ç ”ç©¶ç»“æœåœ¨ç†è®ºä¸Šè¿›è¡Œäº†æµ‹è¯•å’Œè¯„ä»·æ¨¡å‹çš„å¯è¡Œæ€§åŠæ€§èƒ½ã€‚è¯¦ç»†çš„æ¶æ„å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ è‡³GitHubï¼Œé“¾æ¥ä¸ºï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ASR-LLMs-æ™ºèƒ½çœ¼é•œçš„äº’åŠ¨å¾ªç¯æ¨¡å‹ï¼Œæ•´åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œæŠ€æœ¯ã€‚</li>
<li>äº’åŠ¨è¿‡ç¨‹è¢«åˆ†è§£ä¸ºä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ï¼ŒåŒ…æ‹¬è¯­éŸ³æ•è·ã€å¤„ç†ã€åˆ†æå’Œè§£é‡Šï¼Œä»¥åŠç»“æœçš„æ˜¾ç¤ºå’Œç”¨æˆ·åé¦ˆã€‚</li>
<li>æ•°å­¦å…¬å¼è¢«ç”¨äºé‡åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸»è¦è¯„ä¼°ç‚¹åŒ…æ‹¬å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿæ€§åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰æ— ç¼äººæœºäº’åŠ¨æ½œåŠ›ï¼Œå¯é€šè¿‡æ™ºèƒ½çœ¼é•œå®ç°æ•°æ®æ˜¾ç¤ºå’Œç”¨æˆ·äº’åŠ¨ã€‚</li>
<li>ç ”ç©¶ç»“æœé€šè¿‡ç†è®ºæµ‹è¯•ï¼Œè¯„ä¼°äº†æ¨¡å‹çš„å¯è¡Œæ€§åŠæ€§èƒ½ã€‚</li>
<li>è¯¦ç»†çš„æ¨¡å‹æ¶æ„å’Œå®éªŒè¿‡ç¨‹å·²å…¬å¼€åœ¨GitHubä¸Šï¼Œä¾›å…¬ä¼—æŸ¥é˜…å’Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</li>
<li>è¯¥æ¨¡å‹çš„åº”ç”¨å¯èƒ½ä¸ºæœªæ¥çš„äººæœºäº¤äº’æ–¹å¼å¸¦æ¥é©æ–°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg" align="middle">
</details>




<h2 id="DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions"><a href="#DCF-DS-Deep-Cascade-Fusion-of-Diarization-and-Separation-for-Speech-Recognition-under-Realistic-Single-Channel-Conditions" class="headerlink" title="DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions"></a>DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech   Recognition under Realistic Single-Channel Conditions</h2><p><strong>Authors:Shu-Tong Niu, Jun Du, Ruo-Yu Wang, Gao-Bin Yang, Tian Gao, Jia Pan, Yu Hu</strong></p>
<p>We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end speech recognition, combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries during decoding. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art single-channel speech recognition performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å•é€šé“æ·±åº¦çº§è”èåˆåˆ†èˆ±å’Œåˆ†ç¦»ï¼ˆDCF-DSï¼‰åç«¯è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç»“åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†èˆ±ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨è”åˆè®­ç»ƒæ¡†æ¶ä¸­é¡ºåºæ•´åˆNSDå’ŒSSæ¨¡å—ï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨åˆ†èˆ±æ¨¡å—æä¾›çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œã€‚ç„¶åï¼Œä¸ºäº†è¡¥å……DCF-DSè®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çª—å£çº§è§£ç æ–¹æ¡ˆï¼Œä»¥è§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šï¼ˆSDCIï¼‰é—®é¢˜ï¼Œä½¿DCF-DSæ¡†æ¶èƒ½å¤Ÿå¤„ç†è¯¥é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢ä½¿ç”¨åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„NSDç³»ç»Ÿï¼Œä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­æä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚å¦å¤–ï¼Œæˆ‘ä»¬åœ¨DCF-DSæ¡†æ¶ä¸­èå…¥äº†ä¸€ä¸ªå¯é€‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿™å¸¦æ¥äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°èšç±»DCF-DSè¾“å‡ºï¼Œæé«˜äº†åˆ†èˆ±ç»“æœï¼Œæé«˜äº†ASRå‡†ç¡®æ€§ã€‚é€šè¿‡é‡‡ç”¨DCF-DSæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜çš„ç°å®å•é€šé“èµ›é“ä¸­å–å¾—ç¬¬ä¸€åã€‚æˆ‘ä»¬åœ¨å¼€æ”¾çš„LibriCSSæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†å•é€šé“è¯­éŸ³è¯†åˆ«çš„æ–°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å•é€šé“æ·±åº¦çº§è”èåˆåˆ†æ²»ä¸åˆ†ç¦»ï¼ˆDCF-DSï¼‰æ¡†æ¶ï¼Œç”¨äºåç«¯è¯­éŸ³è¯†åˆ«ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚é¦–å…ˆï¼Œåœ¨è”åˆè®­ç»ƒæ¡†æ¶ä¸­é¡ºåºæ•´åˆNSDå’ŒSSæ¨¡å—ï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆ†æ²»æ¨¡å—çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œã€‚å…¶æ¬¡ï¼Œä¸ºè¡¥å……DCF-DSè®­ç»ƒï¼Œå¼•å…¥çª—å£çº§åˆ«è§£ç æ–¹æ¡ˆï¼Œä»¥è§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šï¼ˆSDCIï¼‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ¢ç´¢ä½¿ç”¨åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„NSDç³»ç»Ÿï¼Œä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­æä¾›æ›´å‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œã€‚è¿˜å¯åœ¨DCF-DSæ¡†æ¶ä¸­çº³å…¥å¯é€‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æœ€åï¼Œé€šè¿‡é‡æ–°èšç±»DCF-DSè¾“å‡ºï¼Œæå‡åˆ†æ²»ç»“æœï¼Œæé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ã€‚è¯¥æ¡†æ¶åœ¨CHiME-8 NOTSOFAR-1æŒ‘æˆ˜çš„ç°å®å•é€šé“èµ›é“ä¸­å–å¾—ç¬¬ä¸€åï¼Œå¹¶åœ¨å…¬å¼€çš„LibriCSSæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°çš„å•é€šé“è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å•é€šé“Deep Cascade Fusion of Diarization and Separationï¼ˆDCF-DSï¼‰æ¡†æ¶ï¼Œç»“åˆäº†ç¥ç»ç½‘ç»œè¯´è¯äººåˆ†æ²»ï¼ˆNSDï¼‰å’Œè¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰ã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒNSDå’ŒSSæ¨¡å—ï¼Œå®ç°äº†æœ‰æ•ˆçš„ä¿¡æ¯èåˆï¼Œä½¿åˆ†ç¦»æ¨¡å—èƒ½å¤Ÿåˆ©ç”¨åˆ†æ²»æ¨¡å—æä¾›çš„è¯´è¯äººæ—¶é—´è¾¹ç•Œã€‚</li>
<li>å¼•å…¥çª—å£çº§åˆ«è§£ç æ–¹æ¡ˆï¼Œè§£å†³ç¨€ç–æ•°æ®æ”¶æ•›ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>ä½¿ç”¨çœŸå®æ•°æ®é›†è®­ç»ƒçš„NSDç³»ç»Ÿï¼Œæä¾›æ›´ä¸ºå‡†ç¡®çš„è¯´è¯äººè¾¹ç•Œä¿¡æ¯ã€‚</li>
<li>å¯é€‰çº³å…¥å¤šè¾“å…¥å¤šè¾“å‡ºè¯­éŸ³å¢å¼ºæ¨¡å—ï¼ˆMIMO-SEï¼‰ï¼Œè¿›ä¸€æ­¥æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>é€šè¿‡é‡æ–°èšç±»DCF-DSè¾“å‡ºï¼Œæå‡åˆ†æ²»ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c1f133fe48629ed42ebefd43e4198ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b1be278625e9ff5f56e1bc19f5ff48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e9f3bb5b76e7725e9c9d2c4a617f5a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a249e24e14206090f21ae688491614b2.jpg" align="middle">
</details>




<h2 id="NeKo-Toward-Post-Recognition-Generative-Correction-Large-Language-Models-with-Task-Oriented-Experts"><a href="#NeKo-Toward-Post-Recognition-Generative-Correction-Large-Language-Models-with-Task-Oriented-Experts" class="headerlink" title="NeKo: Toward Post Recognition Generative Correction Large Language   Models with Task-Oriented Experts"></a>NeKo: Toward Post Recognition Generative Correction Large Language   Models with Task-Oriented Experts</h2><p><strong>Authors:Yen-Ting Lin, Chao-Han Huck Yang, Zhehuai Chen, Piotr Zelasko, Xuesong Yang, Zih-Ching Chen, Krishna C Puvvada, Szu-Wei Fu, Ke Hu, Jun Wei Chiu, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang</strong></p>
<p>Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an &#96;&#96;expertâ€™â€™ of speech-to-text, language-to-text and vision-to-text datasets by learning to route each datasetâ€™s tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative $5.0$% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to $27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model. </p>
<blockquote>
<p>æ„å»ºé€šç”¨å‹åè¯†åˆ«é”™è¯¯æ ¡æ­£å™¨æå‡ºäº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•åœ¨å¤§é‡æ··åˆé¢†åŸŸæ•°æ®é›†ä¸Šæœ€æœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹ï¼Ÿç­”æ¡ˆåœ¨äºå­¦ä¹ æ•°æ®é›†ç‰¹å®šç‰¹å¾ï¼Œå¹¶åœ¨å•ä¸ªæ¨¡å‹ä¸­æ¶ˆåŒ–å®ƒä»¬çš„çŸ¥è¯†ã€‚ä¹‹å‰çš„æ–¹æ³•æ˜¯é€šè¿‡æ‹¥æœ‰ç‹¬ç«‹çš„æ ¡æ­£è¯­è¨€æ¨¡å‹æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™å¯¼è‡´äº†å‚æ•°çš„å¤§é‡å¢åŠ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºå°†æ··åˆä¸“å®¶ï¼ˆMixture-of-Expertsï¼ŒMoEsï¼‰ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼ºè°ƒMoEsä¸ä»…ä»…æ˜¯å¯æ‰©å±•æ€§å·¥å…·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ ¡æ­£MoEï¼Œæˆ‘ä»¬è®­ç»ƒä¸“å®¶æˆä¸ºè¯­éŸ³åˆ°æ–‡æœ¬ã€è¯­è¨€åˆ°æ–‡æœ¬å’Œè§†è§‰åˆ°æ–‡æœ¬æ•°æ®é›†çš„â€œä¸“å®¶â€ï¼Œé€šè¿‡å­¦ä¹ å°†æ¯ä¸ªæ•°æ®é›†çš„ä»¤ç‰Œè·¯ç”±åˆ°å…¶æ˜ å°„çš„ä¸“å®¶ã€‚åœ¨å¼€æ”¾ASRæ’è¡Œæ¦œä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé€šè¿‡å®ç°å¹³å‡ç›¸å¯¹5.0%çš„WERå‡å°‘å’ŒBLEUåˆ†æ•°çš„å®è´¨æ€§æé«˜ï¼Œç”¨äºè¯­éŸ³å’Œç¿»è¯‘ä»»åŠ¡ã€‚åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒNeKoåœ¨HyporadiseåŸºå‡†æµ‹è¯•ä¸­ç›¸å¯¹äºGPT-3.5å’ŒClaude-Opuså®ç°äº†15.5%è‡³27.6%çš„ç›¸å¯¹WERå‡å°‘ã€‚ä½œä¸ºä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼ŒNeKoåœ¨è¯­æ³•å’ŒOCRåæ ¡æ­£æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05945v1">PDF</a> NeKo work has been done in June 2024. NeKo LMs will be open source on   <a target="_blank" rel="noopener" href="https://huggingface.co/nvidia">https://huggingface.co/nvidia</a> under the MIT license</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è®­ç»ƒä¸€ä¸ªé€šç”¨å¤šé¢†åŸŸé”™è¯¯æ ¡æ­£æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨Mixture-of-Expertsæ¶æ„ï¼Œå®ç°äº†ä¸€ä¸ªå¤šä»»åŠ¡æ ¡æ­£æ¨¡å‹ï¼Œå¯ä»¥é’ˆå¯¹è¯­éŸ³è½¬æ–‡æœ¬ã€è¯­è¨€è½¬æ–‡æœ¬å’Œè§†è§‰è½¬æ–‡æœ¬ç­‰ä¸åŒé¢†åŸŸæ•°æ®é›†è®­ç»ƒä¸åŒçš„ä¸“å®¶æ¨¡å—ï¼Œä»è€Œå®ç°äº†åœ¨æ–°é¢†åŸŸçš„ä¼˜å¼‚æ€§èƒ½æå‡ã€‚åœ¨å¼€æ”¾ASRæ’è¡Œæ¦œä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¾¾åˆ°äº†æ–°çš„é¢†å…ˆæ°´å¹³ã€‚åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒNeKoæ¨¡å‹ç›¸è¾ƒäºGPT-3.5å’ŒClaude-Opuså±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒNeKoä½œä¸ºä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼Œåœ¨è¯­æ³•å’ŒOCRæ ¡æ­£æ–¹é¢è¡¨ç°å¼ºåŠ²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æ¢è®¨äº†åœ¨æ„å»ºé€šç”¨å¤šé¢†åŸŸé”™è¯¯æ ¡æ­£æ¨¡å‹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ¨¡å‹éœ€è¦èƒ½å¤Ÿåœ¨å¤šç§ä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶å­¦ä¹ ç›¸åº”çš„ç‰¹å®šç‰¹å¾ã€‚Mixture-of-Expertsæ¶æ„ä½œä¸ºè§£å†³è¯¥é—®é¢˜çš„å…³é”®å·¥å…·ã€‚å®éªŒè¡¨æ˜è¿™ç§æ¶æ„ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šä¸ªé¢†åŸŸçš„é”™è¯¯æ ¡æ­£ä»»åŠ¡ã€‚å¯¹äºè¯­éŸ³è½¬æ–‡æœ¬ã€è¯­è¨€è½¬æ–‡æœ¬å’Œè§†è§‰è½¬æ–‡æœ¬ç­‰ä¸åŒé¢†åŸŸæ•°æ®é›†ï¼Œè®­ç»ƒä¸åŒçš„ä¸“å®¶æ¨¡å—ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ ¡æ­£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚è¿™ç§å¤šä»»åŠ¡æ¨¡å‹å…è®¸æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸä¹‹é—´å…±äº«çŸ¥è¯†ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨è¯­æ³•å’ŒOCRæ ¡æ­£æ–¹é¢ä¹Ÿè¡¨ç°å‡ºäº†å¾ˆå¼ºçš„æ€§èƒ½ã€‚å¯¹äºå¤„ç†å„ç§ä»»åŠ¡ä¸­çš„ä¸åŒé—®é¢˜æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆæ¡†æ¶ã€‚ä¾‹å¦‚è¯¥æ¨¡å‹èƒ½å¤Ÿå°†é”™è¯¯æ ¡æ­£å’ŒOCRå¤„ç†è¿‡ç¨‹èåˆåœ¨ä¸€èµ·å½¢æˆä¸€å¥—å®Œæ•´çš„ç³»ç»Ÿæ¥å¤„ç†å¤æ‚çš„ä»»åŠ¡åœºæ™¯ã€‚ä¾‹å¦‚æ–‡æ¡£æ•°å­—åŒ–æˆ–æ™ºèƒ½è¯­éŸ³è¯†åˆ«ç­‰åº”ç”¨åœºæ™¯ä¸­çš„æ–‡å­—è¯†åˆ«å’Œä¿®æ­£è¿‡ç¨‹èƒ½å¤Ÿä½¿ç”¨è¯¥æ¡†æ¶æ¥ååŒå¤„ç†å¤æ‚åœºæ™¯ä¸­çš„å¤šç§é—®é¢˜æŒ‘æˆ˜æé«˜æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚è¿™å¯¹äºè§£å†³å¤æ‚çš„å®é™…åº”ç”¨åœºæ™¯ä¸­çš„é”™è¯¯ä¿®æ­£é—®é¢˜å…·æœ‰æ½œåœ¨çš„ä»·å€¼å’Œæ„ä¹‰ã€‚ä¾‹å¦‚åœ¨æ–‡æ¡£æ•°å­—åŒ–æˆ–æ™ºèƒ½è¯­éŸ³è¯†åˆ«ç­‰åº”ç”¨ä¸­å¯ä»¥é€šè¿‡é›†æˆè¯¥æ¡†æ¶æ¥æ„å»ºæ›´ä¸ºå¼ºå¤§çš„ç³»ç»Ÿæé«˜æ€§èƒ½å’Œå‡†ç¡®æ€§åŒæ—¶æé«˜ç”¨æˆ·ä½“éªŒå’Œå®¢æˆ·æ»¡æ„åº¦é€šè¿‡æ•´åˆä¸åŒçš„ä¸“å®¶æ¨¡å—æé«˜æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½å¹¶ä¸”å¯ä»¥åœ¨å¤šä¸ªé¢†åŸŸä¹‹é—´å®ç°ååŒä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10b1661ff491b3aba1052841b1366a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37f5b110fa8d187637aa3406afa60dd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87b71bd55b283b2814630830d0c2876.jpg" align="middle">
</details>




<h2 id="Dialectal-Coverage-And-Generalization-in-Arabic-Speech-Recognition"><a href="#Dialectal-Coverage-And-Generalization-in-Arabic-Speech-Recognition" class="headerlink" title="Dialectal Coverage And Generalization in Arabic Speech Recognition"></a>Dialectal Coverage And Generalization in Arabic Speech Recognition</h2><p><strong>Authors:Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki</strong></p>
<p>Developing robust automatic speech recognition (ASR) systems for Arabic, a language characterized by its rich dialectal diversity and often considered a low-resource language in speech technology, demands effective strategies to manage its complexity. This study explores three critical factors influencing ASR performance: the role of dialectal coverage in pre-training, the effectiveness of dialect-specific fine-tuning compared to a multi-dialectal approach, and the ability to generalize to unseen dialects. Through extensive experiments across different dialect combinations, our findings offer key insights towards advancing the development of ASR systems for pluricentric languages like Arabic. </p>
<blockquote>
<p>å¼€å‘é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„ç¨³å¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œéœ€è¦åº”å¯¹å…¶ä¸°å¯Œçš„æ–¹è¨€å¤šæ ·æ€§å’Œåœ¨è¯­éŸ³æŠ€æœ¯ä¸­å¸¸è¢«è§†ä¸ºèµ„æºåŒ®ä¹çš„è¯­è¨€æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¿™è¦æ±‚é‡‡ç”¨æœ‰æ•ˆçš„ç­–ç•¥æ¥ç®¡ç†å…¶å¤æ‚æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å½±å“ASRæ€§èƒ½çš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šé¢„è®­ç»ƒä¸­çš„æ–¹è¨€è¦†ç›–ä½œç”¨ã€ä¸å¤šæ–¹è¨€æ–¹æ³•ç›¸æ¯”ç‰¹å®šæ–¹è¨€å¾®è°ƒçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ¨å¹¿æœªè§æ–¹è¨€çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸åŒæ–¹è¨€ç»„åˆä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºæ¨è¿›é’ˆå¯¹é˜¿æ‹‰ä¼¯ç­‰å¤šä¸­å¿ƒè¯­è¨€å¼€å‘ASRç³»ç»Ÿæä¾›äº†å…³é”®è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05872v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å½±å“é˜¿æ‹‰ä¼¯è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ€§èƒ½çš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šé¢„è®­ç»ƒä¸­çš„æ–¹è¨€è¦†ç›–ã€æ–¹è¨€ç‰¹å®šå¾®è°ƒä¸å¤šæ–¹è¨€æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ³›åŒ–åˆ°æœªè§æ–¹è¨€çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒæ–¹è¨€ç»„åˆè¿›è¡Œçš„å¤§é‡å®éªŒï¼Œä¸ºé˜¿æ‹‰ä¼¯ç­‰å¤šä¸­å¿ƒè¯­è¨€ASRç³»ç»Ÿçš„å‘å±•æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–¹è¨€è¦†ç›–åœ¨é¢„è®­ç»ƒä¸­çš„é‡è¦æ€§ï¼šå¯¹äºé˜¿æ‹‰ä¼¯è¯­ç­‰æ–¹è¨€å¤šæ ·çš„è¯­è¨€ï¼Œé¢„è®­ç»ƒæ¨¡å‹éœ€è¦æ¶µç›–å¹¿æ³›çš„æ–¹è¨€ï¼Œä»¥æé«˜ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>æ–¹è¨€ç‰¹å®šå¾®è°ƒä¸å¤šæ–¹è¨€æ–¹æ³•çš„æ•ˆæœæ¯”è¾ƒï¼šç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹ç‰¹å®šæ–¹è¨€çš„å¾®è°ƒå¯èƒ½æ¯”åœ¨å¤šç§æ–¹è¨€ä¸­ä½¿ç”¨å•ä¸€æ¨¡å‹çš„æ–¹æ³•æ›´æœ‰æ•ˆã€‚</li>
<li>æ³›åŒ–åˆ°æœªè§æ–¹è¨€çš„èƒ½åŠ›ï¼šASRç³»ç»Ÿåº”å…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åº”å¯¹æœªæ¥è§¦è¿‡çš„æ–¹è¨€ã€‚</li>
<li>é˜¿æ‹‰ä¼¯è¯­ä½œä¸ºä½èµ„æºè¯­è¨€åœ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸­çš„æŒ‘æˆ˜ï¼šç”±äºé˜¿æ‹‰ä¼¯è¯­çš„æ–¹è¨€å¤šæ ·æ€§å’Œèµ„æºæœ‰é™ï¼Œå¼€å‘é€‚ç”¨äºè¯¥è¯­è¨€çš„ASRç³»ç»Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å®è¯ç ”ç©¶åœ¨æ–¹è¨€ç»„åˆä¸Šçš„å‘ç°ï¼šé€šè¿‡å¤§é‡å®éªŒï¼Œç ”ç©¶å›¢é˜Ÿè·å¾—äº†å…³äºä¸åŒæ–¹è¨€ç»„åˆä¸‹ASRç³»ç»Ÿæ€§èƒ½çš„æ·±å…¥ç†è§£ã€‚</li>
<li>å…¬å¤šä¸­å¿ƒè¯­è¨€ï¼ˆå¦‚é˜¿æ‹‰ä¼¯è¯­ï¼‰åœ¨è¯­éŸ³è¯†åˆ«å‘å±•ä¸­çš„ç‰¹æ®Šè€ƒè™‘ï¼šå¯¹äºè¿™ç±»è¯­è¨€ï¼Œéœ€è¦ç‰¹åˆ«è€ƒè™‘å…¶æ–¹è¨€å¤šæ ·æ€§å’Œè¯­è¨€ç‰¹æ€§ï¼Œä»¥å¼€å‘æ›´æœ‰æ•ˆçš„ASRç³»ç»Ÿã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2419d1ac438dcd757d023f4e59c4be1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13fc695d00050799ec88e2bedaddff8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8363b36a0c1b877eada0c3e37a8cec6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5230d2c6b2b5e6c1a4b718729bb8ecb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cf328d5548f4ac8577c0c6eb782f612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ac340c8eb3b576cbe9d0982638bc3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed00c8b2f71f9870839b176c8daffc70.jpg" align="middle">
</details>




<h2 id="pTSE-T-Presentation-Target-Speaker-Extraction-using-Unaligned-Text-Cues"><a href="#pTSE-T-Presentation-Target-Speaker-Extraction-using-Unaligned-Text-Cues" class="headerlink" title="pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues"></a>pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues</h2><p><strong>Authors:Ziyang Jiang, Xinyuan Qian, Jiahe Lei, Zexu Pan, Wei Xue, Xu-cheng Yin</strong></p>
<p>TSE(Target Speaker Extraction) aims to extract the clean speech of the target speaker in an audio mixture, thus eliminating irrelevant background noise and speech. While prior work has explored various auxiliary cues including pre-recorded speech, visual information (e.g., lip motions and gestures), and spatial information, the acquisition and selection of such strong cues are infeasible in many practical scenarios. Unlike all existing work, in this paper, we condition the TSE algorithm on semantic cues extracted from limited and unaligned text content, such as condensed points from a presentation slide. This method is particularly useful in scenarios like meetings, poster sessions, or lecture presentations, where acquiring other cues in real-time is challenging. To this end, we design two different networks. Specifically, our proposed TPE fuses audio features with content-based semantic cues to facilitate time-frequency mask generation to filter out extraneous noise, while another proposal, namely TSR, employs the contrastive learning technique to associate blindly separated speech signals with semantic cues. The experimental results show the efficacy in accurately identifying the target speaker by utilizing semantic cues derived from limited and unaligned text, resulting in SI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi of 0.830 and STOIi of 0.150, respectively. Dataset and source code will be publicly available. Project demo page: <a target="_blank" rel="noopener" href="https://slidetse.github.io/">https://slideTSE.github.io/</a>. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯è€…æå–ï¼ˆTSEï¼‰æ—¨åœ¨ä»éŸ³é¢‘æ··åˆç‰©ä¸­æå–ç›®æ ‡è¯´è¯è€…çš„æ¸…æ™°è¯­éŸ³ï¼Œä»è€Œæ¶ˆé™¤æ— å…³çš„èƒŒæ™¯å™ªå£°å’Œè¯­éŸ³ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§è¾…åŠ©çº¿ç´¢ï¼ŒåŒ…æ‹¬é¢„å…ˆè®°å½•çš„è¯­éŸ³ã€è§†è§‰ä¿¡æ¯ï¼ˆä¾‹å¦‚å˜´å”‡è¿åŠ¨å’Œæ‰‹åŠ¿ï¼‰ä»¥åŠç©ºé—´ä¿¡æ¯ï¼Œä½†åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œè¿™äº›å¼ºçƒˆçº¿ç´¢çš„è·å–å’Œé€‰æ‹©éƒ½æ˜¯ä¸å¯è¡Œçš„ã€‚ä¸æ‰€æœ‰ç°æœ‰å·¥ä½œä¸åŒï¼Œæœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†TSEç®—æ³•å»ºç«‹åœ¨ä»æœ‰é™ä¸”æœªå¯¹é½çš„æ–‡æœ¬å†…å®¹ä¸­æå–çš„è¯­ä¹‰çº¿ç´¢ä¹‹ä¸Šï¼Œä¾‹å¦‚å¹»ç¯ç‰‡ä¸­çš„æµ“ç¼©è¦ç‚¹ã€‚è¿™ç§æ–¹æ³•åœ¨ä¼šè®®ã€æµ·æŠ¥ä¼šè®®æˆ–è®²åº§ç­‰åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå®æ—¶è·å–å…¶ä»–çº¿ç´¢å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§ä¸åŒçš„ç½‘ç»œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºçš„TPEå°†éŸ³é¢‘ç‰¹å¾ä¸åŸºäºå†…å®¹çš„è¯­ä¹‰çº¿ç´¢èåˆï¼Œæœ‰åŠ©äºç”Ÿæˆæ—¶é—´é¢‘ç‡æ©ç ä»¥è¿‡æ»¤æ‰é¢å¤–çš„å™ªéŸ³ï¼Œè€Œå¦ä¸€é¡¹æè®®TSRåˆ™é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æŠ€æœ¯å°†ç›²åˆ†ç¦»è¯­éŸ³ä¿¡å·ä¸è¯­ä¹‰çº¿ç´¢è”ç³»èµ·æ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æœ‰é™ä¸”æœªå¯¹é½çš„æ–‡æœ¬ä¸­æå–çš„è¯­ä¹‰çº¿ç´¢å‡†ç¡®è¯†åˆ«ç›®æ ‡è¯´è¯è€…ï¼Œå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œå…·ä½“è¡¨ç°ä¸ºSI-SDRiä¸º12.16 dBï¼ŒSDRiä¸º12.66 dBï¼ŒPESQiä¸º0.830ï¼ŒSTOIiä¸º0.150ã€‚æ•°æ®é›†å’Œæºä»£ç å°†å…¬å¼€å¯ç”¨ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://slidetse.github.io/%E3%80%82">https://slideTSE.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03109v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ–‡æœ¬è¯­ä¹‰çº¿ç´¢çš„ç›®æ ‡è¯­éŸ³æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»éŸ³é¢‘æ··åˆä¸­æå–ç›®æ ‡è¯´è¯äººçš„å¹²å‡€è¯­éŸ³ï¼ŒåŒæ—¶æ¶ˆé™¤æ— å…³çš„èƒŒæ™¯å™ªå£°å’Œè¯­éŸ³ã€‚è¯¥æ–¹æ³•ä¸åŒäºä»¥å¾€ä¾èµ–è¾…åŠ©çº¿ç´¢çš„å·¥ä½œï¼Œè€Œæ˜¯åˆ©ç”¨ä»æœ‰é™çš„æœªå¯¹é½æ–‡æœ¬å†…å®¹ä¸­æå–çš„è¯­ä¹‰çº¿ç´¢æ¥æ¡ä»¶åŒ–TSEç®—æ³•ã€‚åœ¨ä¼šè®®ã€æµ·æŠ¥ä¼šè®®æˆ–è®²åº§ç­‰åœºæ™¯ä¸­ï¼Œè¿™ç§æ–¹æ³•å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºè¿™äº›åœºæ™¯ä¸­å®æ—¶è·å–å…¶ä»–çº¿ç´¢å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ©ç”¨æ¥è‡ªæœ‰é™æœªå¯¹é½æ–‡æœ¬çš„è¯­ä¹‰çº¿ç´¢å‡†ç¡®è¯†åˆ«ç›®æ ‡è¯´è¯äººæ–¹é¢éå¸¸æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSEï¼ˆç›®æ ‡è¯­éŸ³æå–ï¼‰æ—¨åœ¨ä»éŸ³é¢‘æ··åˆä¸­æå–ç›®æ ‡è¯´è¯äººçš„å¹²å‡€è¯­éŸ³ï¼Œæ¶ˆé™¤æ— å…³èƒŒæ™¯å™ªå£°å’Œè¯­éŸ³ã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦ä¾èµ–é¢„å½•è¯­éŸ³ã€è§†è§‰ä¿¡æ¯ï¼ˆå¦‚å˜´å”‡è¿åŠ¨å’Œæ‰‹åŠ¿ï¼‰å’Œç©ºé—´ä¿¡æ¯ç­‰è¾…åŠ©çº¿ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ–‡æœ¬è¯­ä¹‰çº¿ç´¢çš„æ¡ä»¶åŒ–TSEç®—æ³•ï¼Œé€‚ç”¨äºä¼šè®®ã€æµ·æŠ¥ä¼šè®®å’Œè®²åº§ç­‰åœºæ™¯ã€‚</li>
<li>è®¾è®¡äº†ä¸¤ç§ç½‘ç»œï¼šTPEé€šè¿‡èåˆéŸ³é¢‘ç‰¹å¾å’ŒåŸºäºå†…å®¹çš„è¯­ä¹‰çº¿ç´¢æ¥ç”Ÿæˆæ—¶é—´-é¢‘ç‡æ©ç ï¼Œè¿‡æ»¤æ‰é¢å¤–å™ªå£°ï¼›TSRé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æŠ€æœ¯å°†ç›²åˆ†ç¦»è¯­éŸ³ä¿¡å·ä¸è¯­ä¹‰çº¿ç´¢ç›¸å…³è”ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æ¥è‡ªæœ‰é™æœªå¯¹é½æ–‡æœ¬çš„è¯­ä¹‰çº¿ç´¢ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå‡†ç¡®è¯†åˆ«ç›®æ ‡è¯´è¯äººã€‚</li>
<li>è¯¥æ–¹æ³•çš„æ€§èƒ½é€šè¿‡SI-SDRiã€SDRiã€PESQiå’ŒSTOIiç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œåˆ†åˆ«è¾¾åˆ°äº†12.16 dBã€12.66 dBã€0.830å’Œ0.150ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-908dbba1a2f21bf44f5e607b9c81dce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f7d11a440ef24b314693e9fe18ed41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1fec1cfba4bc769ba282dd844d687a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58d3e4e4625b4a9e2efd6038f650f9f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf388b126244a717d67c9a1a8cc5e8ed.jpg" align="middle">
</details>




<h2 id="Speech-Separation-with-Pretrained-Frontend-to-Minimize-Domain-Mismatch"><a href="#Speech-Separation-with-Pretrained-Frontend-to-Minimize-Domain-Mismatch" class="headerlink" title="Speech Separation with Pretrained Frontend to Minimize Domain Mismatch"></a>Speech Separation with Pretrained Frontend to Minimize Domain Mismatch</h2><p><strong>Authors:Wupeng Wang, Zexu Pan, Xinke Li, Shuai Wang, Haizhou Li</strong></p>
<p>Speech separation seeks to separate individual speech signals from a speech mixture. Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios. As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications. In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech. The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures. Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data. By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data. To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models. We evaluate the speech separation quality on standard benchmarks and real-world datasets. The results confirm the superiority of our DIP frontend over existing speech separation models. This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications. </p>
<blockquote>
<p>è¯­éŸ³åˆ†ç¦»æ—¨åœ¨ä»è¯­éŸ³æ··åˆç‰©ä¸­åˆ†ç¦»å‡ºå•ä¸ªè¯­éŸ³ä¿¡å·ã€‚é€šå¸¸ï¼Œç”±äºç°å®é¸¡å°¾é…’ä¼šåœºæ™¯ä¸­çš„ç›®æ ‡å‚è€ƒæ— æ³•è·å¾—ï¼Œå¤§å¤šæ•°åˆ†ç¦»æ¨¡å‹éƒ½æ˜¯åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚å› æ­¤ï¼Œåœ¨å°†è¯­éŸ³åˆ†ç¦»æ¨¡å‹éƒ¨ç½²åˆ°ç°å®åº”ç”¨æ—¶ï¼Œå­˜åœ¨çœŸå®æ•°æ®å’Œåˆæˆæ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€ç›®æ ‡å‚è€ƒè¯­éŸ³çš„æ··åˆæ•°æ®è‡ªæˆ‘ç›‘ç£é¢†åŸŸä¸å˜é¢„è®­ç»ƒï¼ˆDIPï¼‰å‰ç«¯ã€‚DIPå‰ç«¯é‡‡ç”¨Siameseç½‘ç»œï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªåˆ›æ–°çš„å‰ç½®ä»»åŠ¡ï¼Œå³æ··åˆé¢„æµ‹ç¼–ç ï¼ˆMPCï¼‰å’Œæ··åˆä¸å˜ç¼–ç ï¼ˆMICï¼‰ï¼Œä»¥æ•è·çœŸå®å’Œåˆæˆæ— æ ‡ç­¾æ··åˆç‰©ä¹‹é—´çš„å…±äº«ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚éšåï¼Œæˆ‘ä»¬åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒä¸‹æ¸¸è¯­éŸ³åˆ†ç¦»æ¨¡å‹æ—¶ï¼Œå°†DIPå‰ç«¯å†»ç»“ä¸ºç‰¹å¾æå–å™¨ã€‚é€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡çº¿ç´¢å¯¹DIPå‰ç«¯è¿›è¡Œé¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¸Œæœ›ä»åˆæˆæ•°æ®ä¸Šå­¦åˆ°çš„è¯­éŸ³åˆ†ç¦»æŠ€èƒ½å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°çœŸå®æ•°æ®ä¸Šã€‚ä¸ºäº†å—ç›ŠäºDIPå‰ç«¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åˆ†ç¦»ç®¡é“ï¼Œä»¥å¯¹é½åˆ†ç¦»æ¨¡å‹çš„ç‰¹å¾åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œå®é™…æ•°æ®é›†ä¸­è¯„ä¼°äº†è¯­éŸ³åˆ†ç¦»è´¨é‡ã€‚ç»“æœè¯å®ï¼Œæˆ‘ä»¬çš„DIPå‰ç«¯ä¼˜äºç°æœ‰çš„è¯­éŸ³åˆ†ç¦»æ¨¡å‹ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å¤§è§„æ¨¡é¢„è®­ç»ƒåœ¨æé«˜ç°å®åº”ç”¨ä¸­è¯­éŸ³åˆ†ç¦»çš„éŸ³è´¨å’Œæ¸…æ™°åº¦æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03085v1">PDF</a> IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€ç›®æ ‡å‚è€ƒè¯­éŸ³çš„è‡ªç›‘ç£åŸŸä¸å˜é¢„è®­ç»ƒï¼ˆDIPï¼‰å‰ç«¯ï¼Œç”¨äºç°å®ä¸–ç•Œçš„è¯­éŸ³åˆ†ç¦»åº”ç”¨ã€‚DIPå‰ç«¯é€šè¿‡æ··åˆé¢„æµ‹ç¼–ç ï¼ˆMPCï¼‰å’Œæ··åˆä¸å˜ç¼–ç ï¼ˆMICï¼‰ä¸¤ä¸ªåˆ›æ–°çš„å‰ç½®ä»»åŠ¡ï¼Œæ•æ‰çœŸå®å’Œåˆæˆæ— æ ‡ç­¾æ··åˆç‰©ä¹‹é—´çš„å…±äº«ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒä¸‹æ¸¸è¯­éŸ³åˆ†ç¦»æ¨¡å‹æ—¶ï¼Œå°†DIPå‰ç«¯å†»ç»“ä¸ºç‰¹å¾æå–å™¨ã€‚é€šè¿‡é¢„è®­ç»ƒDIPå‰ç«¯ï¼ŒæœŸæœ›ä»åˆæˆæ•°æ®å­¦ä¹ çš„è¯­éŸ³åˆ†ç¦»æŠ€èƒ½å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°çœŸå®æ•°æ®ã€‚ä¸ºäº†åˆ©ç”¨DIPå‰ç«¯ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„åˆ†ç¦»ç®¡é“ï¼Œä»¥å¯¹é½åˆ†ç¦»æ¨¡å‹çš„ç‰¹å¾åˆ†è¾¨ç‡ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯­éŸ³åˆ†ç¦»è´¨é‡ï¼Œè¯å®äº†DIPå‰ç«¯ä¼˜äºç°æœ‰è¯­éŸ³åˆ†ç¦»æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆ†ç¦»æ—¨åœ¨ä»è¯­éŸ³æ··åˆç‰©ä¸­åˆ†ç¦»å‡ºå•ä¸ªè¯­éŸ³ä¿¡å·ã€‚</li>
<li>ç”±äºç°å®ä¸–ç•Œä¸­é¸¡å°¾é…’ä¼šçš„ç›®æ ‡å‚è€ƒä¸å¯ç”¨ï¼Œå¤§å¤šæ•°åˆ†ç¦»æ¨¡å‹é€šå¸¸åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£çš„åŸŸä¸å˜é¢„è®­ç»ƒï¼ˆDIPï¼‰å‰ç«¯ï¼Œå¯ä»¥ç›´æ¥æ¥è§¦æ··åˆæ•°æ®ï¼Œæ— éœ€ç›®æ ‡å‚è€ƒè¯­éŸ³ã€‚</li>
<li>DIPå‰ç«¯ä½¿ç”¨Siameseç½‘ç»œï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªåˆ›æ–°çš„å‰ç½®ä»»åŠ¡â€”â€”æ··åˆé¢„æµ‹ç¼–ç ï¼ˆMPCï¼‰å’Œæ··åˆä¸å˜ç¼–ç ï¼ˆMICï¼‰â€”â€”æ¥æ•æ‰çœŸå®å’Œåˆæˆæ— æ ‡ç­¾æ··åˆç‰©ä¹‹é—´çš„å…±äº«ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒä¸‹æ¸¸è¯­éŸ³åˆ†ç¦»æ¨¡å‹æ—¶ï¼Œå°†DIPå‰ç«¯å†»ç»“ä¸ºç‰¹å¾æå–å™¨ï¼Œä»¥å®ç°ä»åˆæˆæ•°æ®åˆ°çœŸå®æ•°æ®çš„æŠ€èƒ½è½¬ç§»ã€‚</li>
<li>ä¸ºäº†åˆ©ç”¨DIPå‰ç«¯ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„è¯­éŸ³åˆ†ç¦»ç®¡é“ï¼Œä»¥å¯¹é½åˆ†ç¦»æ¨¡å‹çš„ç‰¹å¾åˆ†è¾¨ç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-684b3e8302c184b91b4fb579f8f2a7b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2e52d8ca53dde891901cf63796eac44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-278cdfbfafaab139955e067b2123c9c6.jpg" align="middle">
</details>




<h2 id="Unified-Speech-Recognition-A-Single-Model-for-Auditory-Visual-and-Audiovisual-Inputs"><a href="#Unified-Speech-Recognition-A-Single-Model-for-Auditory-Visual-and-Audiovisual-Inputs" class="headerlink" title="Unified Speech Recognition: A Single Model for Auditory, Visual, and   Audiovisual Inputs"></a>Unified Speech Recognition: A Single Model for Auditory, Visual, and   Audiovisual Inputs</h2><p><strong>Authors:Alexandros Haliassos, Rodrigo Mira, Honglie Chen, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p>
<p>Research in auditory, visual, and audiovisual speech recognition (ASR, VSR, and AVSR, respectively) has traditionally been conducted independently. Even recent self-supervised studies addressing two or all three tasks simultaneously tend to yield separate models, leading to disjoint inference pipelines with increased memory requirements and redundancies. This paper proposes unified training strategies for these systems. We demonstrate that training a single model for all three tasks enhances VSR and AVSR performance, overcoming typical optimisation challenges when training from scratch. Moreover, we introduce a greedy pseudo-labelling approach to more effectively leverage unlabelled samples, addressing shortcomings in related self-supervised methods. Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside our semi-supervised approach. Despite using a single model for all tasks, our unified approach achieves state-of-the-art performance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR, as well as on the newly released WildVSR dataset. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/ahaliassos/usr">https://github.com/ahaliassos/usr</a>. </p>
<blockquote>
<p>å…³äºå¬è§‰ã€è§†è§‰å’Œè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆASRã€VSRå’ŒAVSRï¼‰çš„ç ”ç©¶ä¼ ç»Ÿä¸Šæ˜¯ç‹¬ç«‹è¿›è¡Œçš„ã€‚å³ä½¿æœ€è¿‘åŒæ—¶è§£å†³ä¸¤ä¸ªæˆ–æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡çš„è‡ªç›‘ç£ç ”ç©¶ä¹Ÿå€¾å‘äºäº§ç”Ÿå•ç‹¬çš„æ¨¡å‹ï¼Œå¯¼è‡´åˆ†ç¦»çš„æ¨ç†ç®¡é“ï¼Œå¢åŠ äº†å†…å­˜éœ€æ±‚å’Œå†—ä½™ã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹è¿™äº›ç³»ç»Ÿçš„ç»Ÿä¸€è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬è¯æ˜äº†è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥å®Œæˆæ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡å¯ä»¥æé«˜VSRå’ŒAVSRçš„æ€§èƒ½ï¼Œå…‹æœä»å¤´å¼€å§‹è®­ç»ƒæ—¶çš„å…¸å‹ä¼˜åŒ–æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è´ªå©ªçš„ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœªæ ‡è®°æ ·æœ¬ï¼Œè§£å†³ç›¸å…³è‡ªç›‘ç£æ–¹æ³•çš„ä¸è¶³ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æˆ‘ä»¬çš„æ¡†æ¶å†…å¼€å‘äº†ä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œè¯æ˜å…¶ä¸æˆ‘ä»¬çš„åŠç›‘ç£æ–¹æ³•åŒæ ·æœ‰æ•ˆã€‚å°½ç®¡ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥å®Œæˆæ‰€æœ‰ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬çš„ç»Ÿä¸€æ–¹æ³•ç›¸è¾ƒäºåœ¨ASRã€VSRå’ŒAVSRçš„LRS3å’ŒLRS2ä»¥åŠæ–°å‘å¸ƒçš„WildVSRæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ahaliassos/usr%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ahaliassos/usræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02256v1">PDF</a> NeurIPS 2024. Code: <a target="_blank" rel="noopener" href="https://github.com/ahaliassos/usr">https://github.com/ahaliassos/usr</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºç»Ÿä¸€è®­ç»ƒç­–ç•¥ï¼Œå°†å¬è§‰ã€è§†è§‰å’Œè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆASRã€VSRå’ŒAVSRï¼‰ä¸‰ä¸ªä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå•ä¸€æ¨¡å‹åŒæ—¶å¤„ç†ä¸‰ä¸ªä»»åŠ¡èƒ½æå‡VSRå’ŒAVSRæ€§èƒ½ï¼Œå…‹æœä»å¤´å¼€å§‹è®­ç»ƒçš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå¼•å…¥è´ªå©ªä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœªæ ‡æ³¨æ ·æœ¬ï¼Œè§£å†³ç›¸å…³è‡ªç›‘ç£æ–¹æ³•çš„ä¸è¶³ã€‚åœ¨LRS3ã€LRS2å’Œå…¨æ–°å‘å¸ƒçš„WildVSRæ•°æ®é›†ä¸Šï¼Œè¯¥ç»Ÿä¸€æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å–å¾—äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥ï¼Œå°†å¬è§‰ã€è§†è§‰å’Œè§†å¬è¯­éŸ³è¯†åˆ«ä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒã€‚</li>
<li>ä½¿ç”¨å•ä¸€æ¨¡å‹åŒæ—¶å¤„ç†ä¸‰ä¸ªä»»åŠ¡ï¼Œå¯ä»¥æé«˜VSRå’ŒAVSRçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è´ªå©ªä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœªæ ‡æ³¨æ ·æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•å…‹æœäº†ä»å¤´å¼€å§‹è®­ç»ƒçš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆåŒ…æ‹¬LRS3ã€LRS2å’ŒWildVSRï¼‰ä¸Šï¼Œè¯¥ç»Ÿä¸€æ–¹æ³•å–å¾—äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶å†…è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55a1db018537c004e6224899db63b598.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-206735f075998240ab783c34b682b4f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69442c74349a9e6dd17944c66b1dd3ed.jpg" align="middle">
</details>




<h2 id="Multi-environment-Topic-Models"><a href="#Multi-environment-Topic-Models" class="headerlink" title="Multi-environment Topic Models"></a>Multi-environment Topic Models</h2><p><strong>Authors:Dominic Sobhani, Amir Feder, David Blei</strong></p>
<p>Probabilistic topic models are a powerful tool for extracting latent themes from large text datasets. In many text datasets, we also observe per-document covariates (e.g., source, style, political affiliation) that act as environments that modulate a â€œglobalâ€ (environment-agnostic) topic representation. Accurately learning these representations is important for prediction on new documents in unseen environments and for estimating the causal effect of topics on real-world outcomes. To this end, we introduce the Multi-environment Topic Model (MTM), an unsupervised probabilistic model that separates global and environment-specific terms. Through experimentation on various political content, from ads to tweets and speeches, we show that the MTM produces interpretable global topics with distinct environment-specific words. On multi-environment data, the MTM outperforms strong baselines in and out-of-distribution. It also enables the discovery of accurate causal effects. </p>
<blockquote>
<p>æ¦‚ç‡ä¸»é¢˜æ¨¡å‹æ˜¯ä»å¤§é‡æ–‡æœ¬æ•°æ®ä¸­æå–æ½œåœ¨ä¸»é¢˜çš„å¼ºå¤§å·¥å…·ã€‚åœ¨è®¸å¤šæ–‡æœ¬æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æ¯ç¯‡æ–‡æ¡£çš„åå˜é‡ï¼ˆä¾‹å¦‚æ¥æºã€é£æ ¼ã€æ”¿æ²»å€¾å‘ï¼‰ï¼Œè¿™äº›åå˜é‡ä½œä¸ºç¯å¢ƒè°ƒåˆ¶â€œå…¨å±€â€ï¼ˆç¯å¢ƒæ— å…³ï¼‰çš„ä¸»é¢˜è¡¨ç¤ºã€‚å‡†ç¡®å­¦ä¹ è¿™äº›è¡¨ç¤ºå¯¹äºé¢„æµ‹æœªè§ç¯å¢ƒä¸­çš„æ–°æ–‡æ¡£å¹¶ä¼°è®¡ä¸»é¢˜å¯¹ç°å®ä¸–ç•Œç»“æœçš„å½±å“è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šç¯å¢ƒä¸»é¢˜æ¨¡å‹ï¼ˆMTMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¯ä»¥åˆ†ç¦»å…¨å±€å’Œç¯å¢ƒç‰¹å®šæœ¯è¯­ã€‚é€šè¿‡å„ç§æ”¿æ²»å†…å®¹ï¼ˆä»å¹¿å‘Šåˆ°æ¨æ–‡å’Œæ¼”è®²ï¼‰çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†MTMèƒ½å¤Ÿäº§ç”Ÿå…·æœ‰ä¸åŒç¯å¢ƒç‰¹å®šè¯æ±‡çš„å¯è§£é‡Šå…¨å±€ä¸»é¢˜ã€‚åœ¨å¤šç¯å¢ƒæ•°æ®ä¸­ï¼ŒMTMçš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥è¿›è¡Œå‡†ç¡®çš„å› æœæ•ˆåº”å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.24126v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¦‚ç‡ä¸»é¢˜æ¨¡å‹æ˜¯ä»å¤§é‡æ–‡æœ¬æ•°æ®ä¸­æå–æ½œåœ¨ä¸»é¢˜çš„æœ‰åŠ›å·¥å…·ã€‚åœ¨å¤„ç†è®¸å¤šæ–‡æœ¬æ•°æ®æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ¯ç¯‡æ–‡æ¡£çš„åå˜é‡ï¼ˆå¦‚æ¥æºã€é£æ ¼ã€æ”¿æ²»å€¾å‘ï¼‰å……å½“è°ƒåˆ¶â€œå…¨å±€â€ï¼ˆç¯å¢ƒæ— å…³ï¼‰ä¸»é¢˜è¡¨ç¤ºçš„ç¯å¢ƒã€‚ä¸ºäº†åœ¨æ–°çš„å’Œæœªè§è¿‡çš„ç¯å¢ƒä¸­è¿›è¡Œé¢„æµ‹å¹¶ä¼°ç®—ä¸»é¢˜å¯¹ç°å®ä¸–ç•Œç»“æœçš„å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šç¯å¢ƒä¸»é¢˜æ¨¡å‹ï¼ˆMTMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¯åˆ†ç¦»å…¨å±€å’Œç¯å¢ƒç‰¹å®šæœ¯è¯­ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œæ— è®ºæ˜¯åœ¨å¹¿å‘Šã€æ¨ç‰¹è¿˜æ˜¯æ¼”è®²ä¸­ï¼ŒMTMéƒ½èƒ½äº§ç”Ÿå…·æœ‰ç¯å¢ƒç‰¹å®šè¯æ±‡çš„å¯è§£é‡Šå…¨å±€ä¸»é¢˜ã€‚åœ¨å¤šç¯å¢ƒæ•°æ®ä¸­ï¼ŒMTMçš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½å‡†ç¡®å‘ç°å› æœå…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚ç‡ä¸»é¢˜æ¨¡å‹èƒ½ä»å¤§é‡æ–‡æœ¬æ•°æ®ä¸­æå–æ½œåœ¨ä¸»é¢˜ã€‚</li>
<li>æ–‡æœ¬æ•°æ®çš„åå˜é‡ï¼ˆå¦‚æ¥æºã€é£æ ¼ã€æ”¿æ²»å€¾å‘ï¼‰å½±å“ä¸»é¢˜è¡¨ç¤ºã€‚</li>
<li>å¤šç¯å¢ƒä¸»é¢˜æ¨¡å‹ï¼ˆMTMï¼‰æ˜¯ä¸€ç§æ— ç›‘ç£çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¯åˆ†ç¦»å…¨å±€å’Œç¯å¢ƒç‰¹å®šæœ¯è¯­ã€‚</li>
<li>MTMåœ¨å¤šç§æ”¿æ²»å†…å®¹ï¼ˆå¦‚å¹¿å‘Šã€æ¨ç‰¹å’Œæ¼”è®²ï¼‰çš„å®éªŒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>MTMåœ¨æ–°å’Œæœªè§è¿‡çš„ç¯å¢ƒä¸­çš„é¢„æµ‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MTMèƒ½ç”Ÿæˆå…·æœ‰ç¯å¢ƒç‰¹å®šè¯æ±‡çš„å¯è§£é‡Šå…¨å±€ä¸»é¢˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94429d2c44cc4a62fcd64d09029e6ae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fefd2ff2f8bf232a7ea1ec999f4986b8.jpg" align="middle">
</details>




<h2 id="VoiceBench-Benchmarking-LLM-Based-Voice-Assistants"><a href="#VoiceBench-Benchmarking-LLM-Based-Voice-Assistants" class="headerlink" title="VoiceBench: Benchmarking LLM-Based Voice Assistants"></a>VoiceBench: Benchmarking LLM-Based Voice Assistants</h2><p><strong>Authors:Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li</strong></p>
<p>Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸï¼Œæœ€è¿‘çš„è¿›å±•ï¼Œå¦‚GPT-4oï¼Œå·²ç»èƒ½å¤Ÿé€šè¿‡LLMè¯­éŸ³åŠ©æ‰‹å®ç°å®æ—¶è¯­éŸ³äº¤äº’ï¼Œä¸åŸºäºæ–‡æœ¬çš„ä¼ ç»Ÿäº¤äº’æ–¹å¼ç›¸æ¯”ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ˜¾è‘—æ”¹å–„çš„ä½“éªŒã€‚ç„¶è€Œï¼Œç¼ºä¹ç”¨äºè¯„ä¼°è¿™äº›è¯­éŸ³äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†LLMè¯­éŸ³åŠ©æ‰‹çš„å‘å±•ã€‚å½“å‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–æ¸…æ´è¯­éŸ³çš„ä¸€èˆ¬çŸ¥è¯†è¯„ä¼°ä¸Šï¼Œå¿½è§†äº†æ¶‰åŠå¤šç§è¯´è¯äººç‰¹å¾ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ çš„æ›´å¤æ‚ã€çœŸå®çš„ç°å®ä¸–ç•Œåœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoiceBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸ºåŸºäºLLMçš„è¯­éŸ³åŠ©æ‰‹æä¾›å¤šæ–¹é¢çš„è¯„ä¼°ã€‚VoiceBenchè¿˜åŒ…æ‹¬çœŸå®å’Œåˆæˆè¯­éŸ³æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤ç»“åˆäº†ä¸Šè¿°ä¸‰ä¸ªå…³é”®ç°å®ä¸–ç•Œçš„å·®å¼‚ã€‚å¹¿æ³›çš„å®éªŒæ­ç¤ºäº†å½“å‰LLMè¯­éŸ³åŠ©æ‰‹æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17196v3">PDF</a> Work in progress. Data is available at   <a target="_blank" rel="noopener" href="https://github.com/MatthewCYM/VoiceBench">https://github.com/MatthewCYM/VoiceBench</a></p>
<p><strong>Summary</strong>:</p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯çªç ´å¦‚GPT-4oä½¿å¾—åŸºäºLLMçš„è¯­éŸ³åŠ©æ‰‹èƒ½å¤Ÿè¿›è¡Œå®æ—¶è¯­éŸ³äº¤äº’ï¼Œæå¤§æå‡äº†ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œç¼ºä¹é’ˆå¯¹è¯­éŸ³äº¤äº’èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†é˜»ç¢äº†å…¶å‘å±•ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†VoiceBenchåŸºå‡†ï¼Œè¯¥åŸºå‡†æä¾›å¤šæ–¹é¢çš„è¯„ä¼°ï¼Œæ¶µç›–çœŸå®å’Œåˆæˆè¯­éŸ³æŒ‡ä»¤ï¼ŒåŒ…æ‹¬å¤šæ ·çš„è¯´è¯äººç‰¹æ€§ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ ã€‚å®éªŒæ­ç¤ºäº†å½“å‰LLMè¯­éŸ³åŠ©æ‰‹çš„å±€é™ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶å’Œå‘å±•æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†å®æ—¶è¯­éŸ³äº¤äº’çš„å‘å±•ã€‚</li>
<li>GPT-4oç­‰æŠ€æœ¯æå‡äº†ç”¨æˆ·ä½“éªŒï¼Œé€šè¿‡è¯­éŸ³åŠ©æ‰‹å®ç°æ›´è‡ªç„¶çš„äº¤äº’ã€‚</li>
<li>ç›®å‰ç¼ºä¹é’ˆå¯¹LLMè¯­éŸ³åŠ©æ‰‹è¯­éŸ³äº¤äº’èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†ã€‚</li>
<li>VoiceBenchåŸºå‡†æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°LLMè¯­éŸ³åŠ©æ‰‹çš„ç»¼åˆåŸºå‡†ã€‚</li>
<li>VoiceBenchæ¶µç›–äº†çœŸå®å’Œåˆæˆçš„è¯­éŸ³æŒ‡ä»¤ï¼ŒåŒ…å«å¤šæ ·çš„è¯´è¯äººç‰¹æ€§ã€ç¯å¢ƒå’Œå†…å®¹å› ç´ ã€‚</li>
<li>é€šè¿‡å¯¹LLMè¯­éŸ³åŠ©æ‰‹çš„å…¨é¢è¯„ä¼°ï¼Œæ­ç¤ºäº†å…¶å±€é™æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-69ae0a5443de6281affd9aaaa8657b10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86de805d5a4c3dd28764ed73475c70f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc8f15069422afe64369d4abe5a0a4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15a9acc59b778b744382cabc12652f69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20de57dd7f623666968b886a207ce214.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95b1aa211796e45875eb98816b841157.jpg" align="middle">
</details>




<h2 id="Multi-Level-Speaker-Representation-for-Target-Speaker-Extraction"><a href="#Multi-Level-Speaker-Representation-for-Target-Speaker-Extraction" class="headerlink" title="Multi-Level Speaker Representation for Target Speaker Extraction"></a>Multi-Level Speaker Representation for Target Speaker Extraction</h2><p><strong>Authors:Ke Zhang, Junjie Li, Shuai Wang, Yangjie Wei, Yi Wang, Yannan Wang, Haizhou Li</strong></p>
<p>Target speaker extraction (TSE) relies on a reference cue of the target to extract the target speech from a speech mixture. While a speaker embedding is commonly used as the reference cue, such embedding pre-trained with a large number of speakers may suffer from confusion of speaker identity. In this work, we propose a multi-level speaker representation approach, from raw features to neural embeddings, to serve as the speaker reference cue. We generate a spectral-level representation from the enrollment magnitude spectrogram as a raw, low-level feature, which significantly improves the modelâ€™s generalization capability. Additionally, we propose a contextual embedding feature based on cross-attention mechanisms that integrate frame-level embeddings from a pre-trained speaker encoder. By incorporating speaker features across multiple levels, we significantly enhance the performance of the TSE model. Our approach achieves a 2.74 dB improvement and a 4.94% increase in extraction accuracy on Libri2mix test set over the baseline. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä¾èµ–äºç›®æ ‡çš„å‚è€ƒçº¿ç´¢ï¼Œä»è¯­éŸ³æ··åˆä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚è™½ç„¶é€šå¸¸ä½¿ç”¨è¯´è¯äººåµŒå…¥ä½œä¸ºå‚è€ƒçº¿ç´¢ï¼Œä½†ä½¿ç”¨å¤§é‡è¯´è¯äººè¿›è¡Œé¢„è®­ç»ƒçš„åµŒå…¥å¯èƒ½ä¼šå—åˆ°è¯´è¯äººèº«ä»½æ··æ·†çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡çš„è¯´è¯äººè¡¨ç¤ºæ–¹æ³•ï¼Œä»åŸå§‹ç‰¹å¾åˆ°ç¥ç»åµŒå…¥ï¼Œä»¥ä½œä¸ºè¯´è¯äººçš„å‚è€ƒçº¿ç´¢ã€‚æˆ‘ä»¬ä»æ³¨å†Œå¹…åº¦è°±å›¾ä¸­ç”Ÿæˆå…‰è°±çº§åˆ«çš„è¡¨ç¤ºä½œä¸ºåŸå§‹çš„ä½çº§ç‰¹å¾ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡åµŒå…¥ç‰¹å¾ï¼Œè¯¥ç‰¹å¾èåˆäº†é¢„è®­ç»ƒè¯´è¯äººç¼–ç å™¨çš„å¸§çº§åµŒå…¥ã€‚é€šè¿‡ç»“åˆå¤šä¸ªå±‚æ¬¡çš„è¯´è¯äººç‰¹å¾ï¼Œæˆ‘ä»¬æ˜¾ç€æé«˜äº†TSEæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Libri2mixæµ‹è¯•é›†ä¸Šç›¸å¯¹äºåŸºçº¿å®ç°äº†2.74 dBçš„æ”¹è¿›å’Œ4.94%çš„æå–å‡†ç¡®æ€§æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16059v2">PDF</a> 5 pages. Submitted to ICASSP 2025. Implementation will be released at   <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wesep">https://github.com/wenet-e2e/wesep</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä¾èµ–äºç›®æ ‡çš„å‚è€ƒçº¿ç´¢ï¼Œä»è¯­éŸ³æ··åˆä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚è™½ç„¶å¸¸ç”¨çš„å‚è€ƒçº¿ç´¢æ˜¯è¯´è¯äººåµŒå…¥ï¼Œä½†ä½¿ç”¨å¤§é‡è¯´è¯è€…è¿›è¡Œé¢„è®­ç»ƒçš„åµŒå…¥å¯èƒ½ä¼šå—åˆ°è¯´è¯è€…èº«ä»½æ··æ·†çš„å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡è¯´è¯äººè¡¨ç¤ºæ–¹æ³•ï¼Œä»åŸå§‹ç‰¹å¾åˆ°ç¥ç»åµŒå…¥ï¼Œä½œä¸ºè¯´è¯äººå‚è€ƒçº¿ç´¢ã€‚æˆ‘ä»¬ä»æ³¨å†Œå¹…åº¦è°±å›¾ä¸­ç”Ÿæˆé¢‘è°±çº§è¡¨ç¤ºä½œä¸ºåŸå§‹çš„ä½çº§ç‰¹å¾ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡åµŒå…¥ç‰¹å¾ï¼Œè¯¥ç‰¹å¾èåˆäº†é¢„è®­ç»ƒè¯´è¯äººç¼–ç å™¨çš„å¸§çº§åµŒå…¥ã€‚é€šè¿‡ç»“åˆå¤šä¸ªå±‚æ¬¡çš„è¯´è¯äººç‰¹å¾ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†TSEæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨Libri2mixæµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºåŸºçº¿å®ç°äº†2.74 dBçš„æ”¹è¿›å’Œ4.94%çš„æå–å‡†ç¡®æ€§æé«˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä¾èµ–äºç›®æ ‡çš„å‚è€ƒçº¿ç´¢ã€‚</li>
<li>è¯´è¯äººåµŒå…¥ä½œä¸ºå‚è€ƒçº¿ç´¢åœ¨é¢„è®­ç»ƒæ—¶å¯èƒ½é¢ä¸´è¯´è¯è€…èº«ä»½æ··æ·†çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡è¯´è¯äººè¡¨ç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä»åŸå§‹ç‰¹å¾åˆ°ç¥ç»åµŒå…¥ã€‚</li>
<li>åˆ©ç”¨æ³¨å†Œå¹…åº¦è°±å›¾ç”Ÿæˆé¢‘è°±çº§è¡¨ç¤ºï¼Œä½œä¸ºåŸå§‹çš„ä½çº§ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡åµŒå…¥ç‰¹å¾ï¼Œèåˆäº†å¸§çº§åµŒå…¥ã€‚</li>
<li>ç»“åˆå¤šä¸ªå±‚æ¬¡çš„è¯´è¯äººç‰¹å¾èƒ½æ˜¾è‘—æé«˜TSEæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨Libri2mixæµ‹è¯•é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºåŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå®ç°äº†2.74 dBçš„å¢ç›Šå’Œ4.94%çš„æå–å‡†ç¡®æ€§æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4591c50d6ecf92a22e2e31de06235684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4445d1ed24eff7a383b1bfed45d962c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad4cd00d97f928c7bc7d510abcb172ab.jpg" align="middle">
</details>




<h2 id="CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition"><a href="#CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition" class="headerlink" title="CR-CTC: Consistency regularization on CTC for improved speech   recognition"></a>CR-CTC: Consistency regularization on CTC for improved speech   recognition</h2><p><strong>Authors:Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey</strong></p>
<p>Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC. It significantly improves the CTC performance, achieving state-of-the-art results comparable to those attained by transducer or systems combining CTC and attention-based encoder-decoder (CTC&#x2F;AED). We release our code at \url{<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%7D">https://github.com/k2-fsa/icefall}</a>. </p>
<blockquote>
<p>è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ˜¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­å¹¿æ³›ä½¿ç”¨çš„ä¸€ç§æ–¹æ³•ï¼Œä»¥å…¶ç®€å•æ€§å’Œè®¡ç®—æ•ˆç‡è€Œé—»åã€‚ç„¶è€Œï¼Œå®ƒåœ¨è¯†åˆ«æ€§èƒ½ä¸Šå¸¸å¸¸æœ‰æ‰€ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€è‡´æ€§æ­£åˆ™åŒ–CTCï¼ˆCR-CTCï¼‰ï¼Œå®ƒå¼ºåˆ¶è¾“å…¥è¯­éŸ³æ¢…å°”é¢‘è°±å›¾çš„ä¸¤ä¸ªä¸åŒå¢å¼ºè§†å›¾æ‰€è·å¾—çš„ä¸¤ä¸ªCTCåˆ†å¸ƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬ä»ä»¥ä¸‹ä¸‰ä¸ªè§’åº¦å¯¹å…¶åŸºæœ¬è¡Œä¸ºè¿›è¡Œäº†æ·±å…¥æ´å¯Ÿï¼š1ï¼‰å®ƒåœ¨å¤„ç†ä¸åŒå¢å¼ºè§†å›¾çš„éšæœºå­æ¨¡å‹å¯¹ä¹‹é—´è¿›è¡Œè‡ªæˆ‘è’¸é¦ï¼›2ï¼‰å®ƒé€šè¿‡æ©ç é¢„æµ‹å­¦ä¹ æ—¶é—´æ©ç åŒºåŸŸå†…çš„ä½ç½®çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬å¢åŠ æ—¶é—´æ©ç çš„æ•°é‡æ—¶ï¼›3ï¼‰å®ƒæŠ‘åˆ¶äº†è¿‡äºå°–å³°çš„CTCåˆ†å¸ƒï¼Œä»è€Œå‡å°‘äº†è¿‡æ‹Ÿåˆï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨LibriSpeechã€Aishell-1å’ŒGigaSpeechæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬CR-CTCçš„æœ‰æ•ˆæ€§ã€‚å®ƒæ˜¾è‘—æé«˜äº†CTCçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†ä¸è½¬æ¢å™¨æˆ–ç»“åˆCTCå’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆCTC&#x2F;AEDï¼‰çš„ç³»ç»Ÿæ‰€å–å¾—çš„æœ€å…ˆè¿›ç»“æœç›¸å½“çš„æ°´å¹³ã€‚æˆ‘ä»¬åœ¨\url{<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%7D%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/k2-fsa/icefall}å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05101v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºçš„ Consistency-Regularized CTCï¼ˆCR-CTCï¼‰æ–¹æ³•é€šè¿‡åœ¨ä¸åŒå¢å¼ºè§†å›¾ä¸­è·å¾—ä¸¤ä¸ª CTC åˆ†å¸ƒå¹¶å¼ºåˆ¶å®ƒä»¬ä¿æŒä¸€è‡´ï¼Œå¢å¼ºäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªæˆ‘è’¸é¦ã€å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºä»¥åŠæŠ‘åˆ¶è¿‡äºå°–é”çš„ CTC åˆ†å¸ƒç­‰è¡Œä¸ºæ”¹å–„æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCR-CTC æ˜¾è‘—æé«˜äº† CTC æ€§èƒ½ï¼Œè¾¾åˆ°äº†ä¸è½¬æ¢å™¨æˆ– CTC ä¸åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ç»„åˆç³»ç»Ÿï¼ˆCTC&#x2F;AEDï¼‰ç›¸å½“çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Consistency-Regularized CTCï¼ˆCR-CTCï¼‰æ˜¯ä¸€ç§æ”¹è¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ Connectionist Temporal Classificationï¼ˆCTCï¼‰è¡¨ç°çš„æ–¹æ³•ã€‚</li>
<li>CR-CTC é€šè¿‡åœ¨ä¸åŒå¢å¼ºè§†å›¾ä¸­è·å¾—ä¸¤ä¸ª CTC åˆ†å¸ƒå¹¶å¼ºåˆ¶å®ƒä»¬ä¿æŒä¸€è‡´æ¥å¢å¼ºæ€§èƒ½ã€‚</li>
<li>CR-CTC é€šè¿‡è‡ªæˆ‘è’¸é¦å¤„ç†ä¸åŒå¢å¼ºè§†å›¾çš„éšæœºå­æ¨¡å‹å¯¹æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>CR-CTC é€šè¿‡å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨å¢åŠ æ—¶é—´æ©ç é‡æ—¶ï¼Œå¯¹æ—¶é—´æ©ç åŒºåŸŸå†…çš„ä½ç½®è¿›è¡Œæ©ç é¢„æµ‹æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>CR-CTC èƒ½å¤ŸæŠ‘åˆ¶è¿‡äºå°–é”çš„ CTC åˆ†å¸ƒï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCR-CTC æ˜¾è‘—æé«˜äº† CTC æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-973348c54f2da3118c75e78867354959.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e34519caf7b30b4d5d9b5322f289e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1ec9bc930d4c1997854dfb61b3d0882.jpg" align="middle">
</details>




<h2 id="A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation"><a href="#A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation" class="headerlink" title="A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation"></a>A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation</h2><p><strong>Authors:Jingyuan Wang, Jie Zhang, Shihao Chen, Miao Sun</strong></p>
<p>Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under various noise conditions, but with a much lower computational cost and a better SCP. The reproducible code and audio examples are available at <a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN">https://github.com/jywanng/LBCCN</a>. </p>
<blockquote>
<p>åŒè€³è¯­éŸ³å¢å¼ºï¼ˆBSEï¼‰æ—¨åœ¨å…±åŒæé«˜å¬åŠ›è®¾å¤‡æ¥æ”¶åˆ°çš„å™ªå£°ä¿¡å·çš„è´¨é‡å’Œæ¸…æ™°åº¦ï¼Œå¹¶ä¿ç•™ç›®æ ‡çš„ç©ºé—´çº¿ç´¢ä»¥å®ç°è‡ªç„¶è†å¬ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨å™ªå£°æŠ‘åˆ¶ï¼ˆNRï¼‰èƒ½åŠ›å’Œç©ºé—´çº¿ç´¢ä¿ç•™ï¼ˆSCPï¼‰å‡†ç¡®æ€§ä¹‹é—´å¦¥åï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„å£°å­¦åœºæ™¯ä¸­è®¡ç®—éœ€æ±‚è¾ƒé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡çº§åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ï¼Œå®ƒé€šè¿‡è¿‡æ»¤ä½é¢‘æ®µæ¥å‡ºè‰²åœ°è¿›è¡Œå™ªå£°æŠ‘åˆ¶å¹¶ä¿æŒå…¶ä½™éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾å¼åœ°ç»“åˆäº†é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ é€’å‡½æ•°çš„ä¼°è®¡ï¼Œä»¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸåº¦å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LBCCNåœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹å¯ä»¥å®ç°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„å»å™ªæ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„SCPã€‚å¯é‡å¤çš„ä»£ç å’ŒéŸ³é¢‘ç¤ºä¾‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jywanng/LBCCNæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡åŒ–åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ï¼Œç”¨äºæ”¹å–„å¬åŠ›è®¾å¤‡çš„è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦ã€‚è¯¥ç½‘ç»œé€šè¿‡æ»¤æ³¢ä½é¢‘å¸¦å¹¶ä¿æŒå…¶ä½™éƒ¨åˆ†å®ç°å‡ºè‰²çš„é™å™ªæ•ˆæœï¼Œå¹¶ä¸“é—¨ä¼°è®¡é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°ä»¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸåº¦å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒLBCCNåœ¨é™å™ªæ€§èƒ½ä¸Šè¡¨ç°ç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ï¼Œç©ºé—´çº¿ç´¢ä¿çœŸåº¦æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Binaural speech enhancementæ—¨åœ¨æé«˜å¬åŠ›è®¾å¤‡çš„è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦ï¼ŒåŒæ—¶ä¿ç•™ç›®æ ‡çš„ç©ºé—´çº¿ç´¢ä»¥å®ç°è‡ªç„¶è†å¬ä½“éªŒã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨é™å™ªã€ç©ºé—´çº¿ç´¢ä¿ç•™å’Œè®¡ç®—éœ€æ±‚ä¹‹é—´æœ‰æ‰€å¦¥åï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å£°å­¦åœºæ™¯ä¸­ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡åŒ–åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ï¼Œä¸“æ³¨äºé™å™ªå¹¶ä¿ç•™ç©ºé—´çº¿ç´¢ã€‚</li>
<li>LBCCNé€šè¿‡æ»¤æ³¢ä½é¢‘å¸¦æ¥å®ç°å‡ºè‰²çš„é™å™ªæ•ˆæœï¼ŒåŒæ—¶ä¿æŒå…¶ä»–é¢‘ç‡çš„å®Œæ•´æ€§ã€‚</li>
<li>LBCCNé€šè¿‡ä¼°è®¡é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°æ¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸåº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLBCCNåœ¨é™å™ªæ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œæ›´é«˜çš„ç©ºé—´çº¿ç´¢ä¿çœŸåº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7310f39e4f46abd5e4a8dab4831ac5b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0dc96b1ca49689a0be21d73c04ec780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f01c1e0f4cf7074ca80b5d6147b6b5.jpg" align="middle">
</details>




<h2 id="Resource-Efficient-Adaptation-of-Speech-Foundation-Models-for-Multi-Speaker-ASR"><a href="#Resource-Efficient-Adaptation-of-Speech-Foundation-Models-for-Multi-Speaker-ASR" class="headerlink" title="Resource-Efficient Adaptation of Speech Foundation Models for   Multi-Speaker ASR"></a>Resource-Efficient Adaptation of Speech Foundation Models for   Multi-Speaker ASR</h2><p><strong>Authors:Weiqing Wang, Kunal Dhawan, Taejin Park, Krishna C. Puvvada, Ivan Medennikov, Somshubra Majumdar, He Huang, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Speech foundation models have achieved state-of-the-art (SoTA) performance across various tasks, such as automatic speech recognition (ASR) in hundreds of languages. However, multi-speaker ASR remains a challenging task for these models due to data scarcity and sparsity. In this paper, we present approaches to enable speech foundation models to process and understand multi-speaker speech with limited training data. Specifically, we adapt a speech foundation model for the multi-speaker ASR task using only telephonic data. Remarkably, the adapted model also performs well on meeting data without any fine-tuning, demonstrating the generalization ability of our approach. We conduct several ablation studies to analyze the impact of different parameters and strategies on model performance. Our findings highlight the effectiveness of our methods. Results show that less parameters give better overall cpWER, which, although counter-intuitive, provides insights into adapting speech foundation models for multi-speaker ASR tasks with minimal annotated data. </p>
<blockquote>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹å·²åœ¨å„ç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚åœ¨æ•°ç™¾ç§è¯­è¨€ä¸­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œç¨€ç–ï¼Œå¤šè¯´è¯è€…ASRå¯¹è¿™äº›æ¨¡å‹æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿è¯­éŸ³åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¤„ç†å’Œç†è§£å¤šè¯´è¯è€…è¯­éŸ³çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åªä½¿ç”¨ç”µè¯æ•°æ®æ¥é€‚åº”å¤šè¯´è¯è€…ASRä»»åŠ¡çš„è¯­éŸ³åŸºç¡€æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€‚åº”çš„æ¨¡å‹åœ¨ä¼šè®®æ•°æ®ä¸Šæ— éœ€ä»»ä½•å¾®è°ƒä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›è¡Œäº†å‡ æ¬¡å‰¥ç¦»ç ”ç©¶ï¼Œåˆ†æä¸åŒå‚æ•°å’Œç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¾ƒå°‘çš„å‚æ•°ä¼šæä¾›æ›´å¥½çš„æ•´ä½“cpWERï¼Œè™½ç„¶è¿™ç§ç»“æœåˆçœ‹èµ·æ¥æœ‰ç‚¹è¿åç›´è§‰ï¼Œä½†ä¸ºæˆ‘ä»¬æä¾›äº†åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹é€‚åº”å¤šè¯´è¯è€…ASRä»»åŠ¡çš„è¯­éŸ³åŸºç¡€æ¨¡å‹çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01438v2">PDF</a> Accepted by SLT 2024</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹å·²åœ¨å„ç§ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨æ•°ç™¾ç§è¯­è¨€ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šè¯´è¯äººçš„ASRä»»åŠ¡ï¼Œè¿™äº›æ¨¡å‹ä»é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œç¨€ç–çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºæ–¹æ³•ï¼Œä½¿è¯­éŸ³åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹å¤„ç†å’Œç†è§£å¤šè¯´è¯äººçš„è¯­éŸ³ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åªä½¿ç”¨ç”µè¯æ•°æ®å¯¹è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡Œå¤šè¯´è¯äººASRä»»åŠ¡é€‚é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€‚é…æ¨¡å‹åœ¨ä¼šè®®æ•°æ®ä¸Šæ— éœ€ä»»ä½•å¾®è°ƒå³å¯è¡¨ç°è‰¯å¥½ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—æ¶ˆå»ç ”ç©¶æ¥åˆ†æä¸åŒå‚æ•°å’Œç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼Œå¹¶ä¸”å‘ç°è¾ƒå°‘çš„å‚æ•°èƒ½å¸¦æ¥æ›´å¥½çš„æ•´ä½“cpWERï¼Œè™½ç„¶è¿™ä¼¼ä¹æœ‰è¿ç›´è§‰ï¼Œä½†è¿™ä¸ºåœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸‹é€‚é…è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡Œå¤šè¯´è¯äººASRä»»åŠ¡æä¾›äº†è§è§£ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>è¯­éŸ³åŸºç¡€æ¨¡å‹å·²åœ¨å¤šç§è¯­è¨€çš„ASRä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å¤šè¯´è¯äººASRä»»åŠ¡å¯¹è¯­éŸ³åŸºç¡€æ¨¡å‹ä»æ˜¯æŒ‘æˆ˜ï¼Œå°¤å…¶é¢ä¸´æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€‚é…è¯­éŸ³åŸºç¡€æ¨¡å‹ä»¥å¤„ç†å¤šè¯´è¯äººè¯­éŸ³çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨ç”µè¯æ•°æ®ã€‚</li>
<li>é€‚é…æ¨¡å‹åœ¨ä¼šè®®æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ— éœ€å¾®è°ƒï¼Œæ˜¾ç¤ºäº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¶ˆå»ç ”ç©¶åˆ†æäº†ä¸åŒå‚æ•°å’Œç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç ”ç©¶å‘ç°è¾ƒå°‘çš„å‚æ•°èƒ½å¸¦æ¥æ›´å¥½çš„æ•´ä½“cpWERï¼Œè¿™æä¾›äº†å¯¹äºå¦‚ä½•åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹é€‚é…è¯­éŸ³åŸºç¡€æ¨¡å‹çš„è§è§£ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºè¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨å¤šè¯´è¯äººASRä»»åŠ¡ä¸­çš„é€‚é…æä¾›äº†æ–°çš„æ€è·¯å’Œç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e36ba4275e7af7d573f54800905876e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce2f870fbab0b62b3007dcf01150d569.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80a25913d424de691d8a992e7306ad81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aec25fbde60f27d7f855ba26596de064.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c27a4c662f0a810b957be4bf27d079d6.jpg" align="middle">
</details>




<h2 id="Integrating-audiological-datasets-via-federated-merging-of-Auditory-Profiles"><a href="#Integrating-audiological-datasets-via-federated-merging-of-Auditory-Profiles" class="headerlink" title="Integrating audiological datasets via federated merging of Auditory   Profiles"></a>Integrating audiological datasets via federated merging of Auditory   Profiles</h2><p><strong>Authors:Samira Saak, Dirk Oetting, Birger Kollmeier, Mareike Buhl</strong></p>
<p>Audiological datasets contain valuable knowledge about hearing loss in patients, which can be uncovered using data-driven, federated learning techniques. Our previous approach summarized patient information from one audiological dataset into distinct Auditory Profiles (APs). To obtain a better estimate of the audiological patient population, however, patient patterns must be analyzed across multiple, separated datasets, and finally, be integrated into a combined set of APs.   This study aimed at extending the existing profile generation pipeline with an AP merging step, enabling the combination of APs from different datasets based on their similarity across audiological measures. The 13 previously generated APs (NA&#x3D;595) were merged with 31 newly generated APs from a second dataset (NB&#x3D;1272) using a similarity score derived from the overlapping densities of common features across the two datasets. To ensure clinical applicability, random forest models were created for various scenarios, encompassing different combinations of audiological measures.   A new set with 13 combined APs is proposed, providing separable profiles, which still capture detailed patient information from various test outcome combinations. The classification performance across these profiles is satisfactory. The best performance was achieved using a combination of loudness scaling, audiogram and speech test information, while single measures performed worst.   The enhanced profile generation pipeline demonstrates the feasibility of combining APs across datasets, which should generalize to all datasets and could lead to an interpretable global profile set in the future. The classification models maintain clinical applicability. </p>
<blockquote>
<p>å¬åŠ›æ•°æ®é›†ä¸­åŒ…å«äº†å…³äºæ‚£è€…å¬åŠ›æŸå¤±çš„å®è´µçŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯ä»¥é€šè¿‡æ•°æ®é©±åŠ¨ã€è”é‚¦å­¦ä¹ æŠ€æœ¯æ¥å‘æ˜ã€‚æˆ‘ä»¬ä¹‹å‰çš„æ–¹æ³•æ˜¯å°†ä¸€ä¸ªå¬åŠ›æ•°æ®é›†ä¸­çš„æ‚£è€…ä¿¡æ¯æ€»ç»“ä¸ºä¸åŒçš„å¬åŠ›æ¦‚å†µï¼ˆAPsï¼‰ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´å¥½åœ°ä¼°è®¡å¬åŠ›å—æŸæ‚£è€…äººç¾¤çš„æƒ…å†µï¼Œå¿…é¡»åˆ†æå¤šä¸ªç‹¬ç«‹æ•°æ®é›†ä¸­çš„æ‚£è€…æ¨¡å¼ï¼Œå¹¶æœ€ç»ˆå°†å…¶æ•´åˆåˆ°ä¸€ç»„ç»¼åˆçš„å¬åŠ›æ¦‚å†µä¸­ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¢åŠ ä¸€ä¸ªå¬åŠ›æ¦‚å†µåˆå¹¶æ­¥éª¤æ¥æ‰©å±•ç°æœ‰çš„æ¦‚å†µç”Ÿæˆæµç¨‹ï¼Œä½¿èƒ½å¤ŸåŸºäºä¸åŒæ•°æ®é›†åœ¨å¬åŠ›æµ‹é‡æ–¹é¢çš„ç›¸ä¼¼æ€§æ¥ç»„åˆä¸åŒçš„å¬åŠ›æ¦‚å†µã€‚ä½¿ç”¨ä»ä¸¤ä¸ªæ•°æ®é›†ä¸­å¸¸è§ç‰¹å¾çš„é‡å å¯†åº¦å¾—å‡ºçš„ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œå°†ä¹‹å‰ç”Ÿæˆçš„13ä¸ªå¬åŠ›æ¦‚å†µï¼ˆNA&#x3D;595ï¼‰ä¸ä»ç¬¬äºŒä¸ªæ•°æ®é›†ä¸­æ–°ç”Ÿæˆçš„31ä¸ªå¬åŠ›æ¦‚å†µï¼ˆNB&#x3D;1272ï¼‰åˆå¹¶ã€‚ä¸ºç¡®ä¿ä¸´åºŠé€‚ç”¨æ€§ï¼Œé’ˆå¯¹åŒ…å«ä¸åŒå¬åŠ›æµ‹é‡ç»„åˆçš„å„ç§åœºæ™¯åˆ›å»ºäº†éšæœºæ£®æ—æ¨¡å‹ã€‚æå‡ºäº†ä¸€å¥—åŒ…å«13ä¸ªç»„åˆå¬åŠ›æ¦‚å†µçš„æ–°é›†åˆï¼Œè¿™äº›æ¦‚å†µèƒ½å¤ŸåŒºåˆ†ä¸åŒçš„æ‚£è€…ä¿¡æ¯ï¼ŒåŒæ—¶ä»ç„¶èƒ½å¤Ÿæ•è·å„ç§æµ‹è¯•ç»“æœçš„è¯¦ç»†æ‚£è€…ä¿¡æ¯ã€‚è¿™äº›å¬åŠ›æ¦‚å†µçš„åˆ†ç±»æ€§èƒ½ä»¤äººæ»¡æ„ã€‚ä½¿ç”¨å“åº¦ç¼©æ”¾ã€å¬åŠ›å›¾å’Œè¯­éŸ³æµ‹è¯•ä¿¡æ¯çš„ç»„åˆå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè€Œå•ä¸€æŒ‡æ ‡çš„è¡¨ç°æœ€å·®ã€‚æ”¹è¿›åçš„å¬åŠ›æ¦‚å†µç”Ÿæˆæµç¨‹è¯æ˜äº†è·¨æ•°æ®é›†ç»„åˆAPçš„å¯è¡Œæ€§ï¼Œè¿™åº”è¯¥èƒ½å¤Ÿæ¨å¹¿åˆ°æ‰€æœ‰æ•°æ®é›†ï¼Œå¹¶åœ¨æœªæ¥å¯èƒ½å¯¼è‡´ä¸€ä¸ªå¯è§£é‡Šçš„å…¨å±€å¬åŠ›æ¦‚å†µé›†åˆã€‚åˆ†ç±»æ¨¡å‹ä»ç„¶é€‚ç”¨äºä¸´åºŠå®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20765v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ•°æ®é©±åŠ¨å’Œè”é‚¦å­¦ä¹ æŠ€æœ¯ä»å¤šä¸ªéŸ³é¢‘æ•°æ®é›†ä¸­æŒ–æ˜å…³äºå¬åŠ›æŸå¤±æ‚£è€…æœ‰ä»·å€¼çŸ¥è¯†çš„æ–¹æ³•ã€‚å…ˆå‰çš„æ–¹æ³•å°†æ‚£è€…ä¿¡æ¯æ±‡æ€»ä¸ºä¸åŒçš„å¬è§‰ç‰¹å¾é›†ï¼ˆAPsï¼‰ã€‚ä¸ºäº†æ›´å‡†ç¡®åœ°ä¼°è®¡å¬åŠ›å—æŸæ‚£è€…ç¾¤ä½“ï¼Œæœ¬æ–‡æ‰©å±•äº†ç°æœ‰åˆ†ææµç¨‹ï¼Œå¢åŠ äº†ä¸€ä¸ªå¬è§‰ç‰¹å¾é›†åˆå¹¶æ­¥éª¤ï¼Œä½¿ä¸åŒæ•°æ®é›†ä¸­çš„å¬è§‰ç‰¹å¾é›†å¯ä»¥æ ¹æ®å®ƒä»¬åœ¨éŸ³é¢‘æµ‹é‡ä¸Šçš„ç›¸ä¼¼æ€§è¿›è¡Œåˆå¹¶ã€‚ç ”ç©¶é€šè¿‡åˆå¹¶å…ˆå‰ç”Ÿæˆçš„13ä¸ªå¬è§‰ç‰¹å¾é›†ï¼ˆåŸºäºä¸€ä¸ªæ•°æ®é›†ï¼‰å’Œä»ç¬¬äºŒä¸ªæ•°æ®é›†æ–°ç”Ÿæˆçš„31ä¸ªå¬è§‰ç‰¹å¾é›†ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«13ä¸ªç»„åˆå¬è§‰ç‰¹å¾é›†çš„æ–°é›†åˆã€‚è¯¥é›†åˆæä¾›äº†å¯åˆ†ç¦»çš„æ‚£è€…ç‰¹å¾é›†ï¼Œèƒ½å¤Ÿè¯¦ç»†æ•æ‰ä¸åŒæµ‹è¯•ç»“æœçš„ç»„åˆä¿¡æ¯ã€‚åˆ†ç±»æ€§èƒ½è‰¯å¥½ï¼Œä½¿ç”¨å“åº¦ç¼©æ”¾ã€å¬åŠ›å›¾å’Œè¯­éŸ³æµ‹è¯•ä¿¡æ¯ç»„åˆæ—¶è¡¨ç°æœ€ä½³ï¼Œå•ä¸€æŒ‡æ ‡çš„åˆ†ç±»æ•ˆæœæœ€å·®ã€‚æ”¹è¿›çš„åˆ†ææµç¨‹è¯æ˜äº†è·¨æ•°æ®é›†åˆå¹¶å¬è§‰ç‰¹å¾é›†çš„å¯è¡Œæ€§ï¼Œæœªæ¥æœ‰æœ›å½¢æˆä¸€å¥—å¯è§£é‡Šçš„å…¨å±€å¬è§‰ç‰¹å¾é›†åˆ†ç±»æ¨¡å‹ä¿æŒä¸´åºŠé€‚ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½¿ç”¨æ•°æ®é©±åŠ¨å’Œè”é‚¦å­¦ä¹ æŠ€æœ¯å¯ä»éŸ³é¢‘æ•°æ®é›†ä¸­æŒ–æ˜å…³äºå¬åŠ›æŸå¤±çš„çŸ¥è¯†ã€‚</li>
<li>å¬è§‰ç‰¹å¾é›†ï¼ˆAPsï¼‰ç”¨äºæ±‡æ€»æ‚£è€…ä¿¡æ¯ã€‚</li>
<li>æ‰©å±•ç°æœ‰åˆ†ææµç¨‹ä»¥è·¨å¤šä¸ªæ•°æ®é›†æ•´åˆAPsã€‚</li>
<li>åŸºäºä¸¤ä¸ªæ•°æ®é›†çš„ç›¸ä¼¼æ€§å¯¹APsè¿›è¡Œåˆå¹¶ã€‚</li>
<li>æ–°æå‡ºçš„é›†åˆåŒ…å«åˆå¹¶åçš„13ä¸ªAPsï¼Œæä¾›è¯¦ç»†çš„æ‚£è€…ä¿¡æ¯ã€‚</li>
<li>åˆ†ç±»æ€§èƒ½è‰¯å¥½ï¼Œç»„åˆå“åº¦ç¼©æ”¾ã€å¬åŠ›å›¾å’Œè¯­éŸ³æµ‹è¯•ä¿¡æ¯è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-077a5aed8fe333946050487152273996.jpg" align="middle">
</details>




<h2 id="Whisper-Flamingo-Integrating-Visual-Features-into-Whisper-for-Audio-Visual-Speech-Recognition-and-Translation"><a href="#Whisper-Flamingo-Integrating-Visual-Features-into-Whisper-for-Audio-Visual-Speech-Recognition-and-Translation" class="headerlink" title="Whisper-Flamingo: Integrating Visual Features into Whisper for   Audio-Visual Speech Recognition and Translation"></a>Whisper-Flamingo: Integrating Visual Features into Whisper for   Audio-Visual Speech Recognition and Translation</h2><p><strong>Authors:Andrew Rouditchenko, Yuan Gong, Samuel Thomas, Leonid Karlinsky, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, speech models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better speech-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper speech recognition and translation model with gated cross attention. Our models achieve state-of-the-art ASR WER (0.68%) and AVSR WER (0.76%) on LRS3, and state-of-the-art ASR WER (1.3%) and AVSR WER (1.4%) on LRS2. Audio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech recognition and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is versatile and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åˆ©ç”¨åŸºäºå˜´å”‡çš„è§†é¢‘åœ¨å™ªå£°ä¸­æé«˜æ€§èƒ½ã€‚ç”±äºè§†é¢‘æ¯”éŸ³é¢‘æ›´éš¾è·å–ï¼ŒAVSRæ¨¡å‹çš„è§†é¢‘è®­ç»ƒæ•°æ®é€šå¸¸ä»…é™äºæ•°åƒå°æ—¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¯¸å¦‚Whisperä¹‹ç±»çš„è¯­éŸ³æ¨¡å‹ä½¿ç”¨æ•°åä¸‡å°æ—¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå­¦ä¹ æ›´å¥½çš„è¯­éŸ³åˆ°æ–‡æœ¬çš„è§£ç å™¨ã€‚å·¨å¤§çš„è®­ç»ƒæ•°æ®å·®å¼‚ä¿ƒä½¿æˆ‘ä»¬é€‚åº”Whisperå¤„ç†è§†é¢‘è¾“å…¥ã€‚å—Flamingoå°†è§†è§‰ç‰¹å¾æ³¨å…¥è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Whisper-Flamingoï¼Œå®ƒé€šè¿‡é—¨æ§äº¤å‰æ³¨æ„åŠ›å°†è§†è§‰ç‰¹å¾é›†æˆåˆ°Whisperè¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨LRS3ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ASR WERï¼ˆ0.68ï¼…ï¼‰å’ŒAVSR WERï¼ˆ0.76ï¼…ï¼‰ï¼Œåœ¨LRS2ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ASR WERï¼ˆ1.3ï¼…ï¼‰å’ŒAVSR WERï¼ˆ1.4ï¼…ï¼‰ã€‚è§†å¬Whisper-Flamingoåœ¨å˜ˆæ‚ç¯å¢ƒä¸‹ï¼Œå¯¹äºè‹±è¯­è¯­éŸ³è¯†åˆ«å’ŒEn-Xç¿»è¯‘å…±å…­ç§è¯­è¨€çš„è¡¨ç°ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„Whisperã€‚æ­¤å¤–ï¼ŒWhisper-Flamingoéå¸¸é€šç”¨ï¼Œä½¿ç”¨ä¸€ç»„å‚æ•°å³å¯æ‰§è¡Œæ‰€æœ‰è¿™äº›ä»»åŠ¡ï¼Œè€Œå…ˆå‰çš„æ–¹æ³•åˆ™é’ˆå¯¹æ¯ç§è¯­è¨€åˆ†åˆ«è¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10082v3">PDF</a> Interspeech 2024. V3: Added results on LRS2. Code at   <a target="_blank" rel="noopener" href="https://github.com/roudimit/whisper-flamingo">https://github.com/roudimit/whisper-flamingo</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘å”‡è¯­è¯†åˆ«çš„éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ã€‚ç„¶è€Œï¼ŒAVSRæ¨¡å‹çš„è§†é¢‘è®­ç»ƒæ•°æ®é€šå¸¸ä»…é™äºå‡ åƒå°æ—¶ï¼Œè€Œåƒwhisperè¿™æ ·çš„è¯­éŸ³æ¨¡å‹åˆ™å¯é€šè¿‡æ•°åä¸‡å°æ—¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ‹¥æœ‰æ›´å¥½çš„è¯­éŸ³è½¬æ–‡æœ¬è§£ç å™¨ã€‚å·¨å¤§çš„è®­ç»ƒæ•°æ®å·®å¼‚ä¿ƒä½¿æˆ‘ä»¬é€‚åº”whisperå¤„ç†è§†é¢‘è¾“å…¥ã€‚å—åˆ°æ³¨å…¥è¯­è¨€æ¨¡å‹çš„Flamingoçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ•´åˆè§†è§‰ç‰¹å¾çš„whisper-flamingoæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†whisperè¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ¨¡å‹çš„é—¨æ§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨LRS3å’ŒLRS2ä¸Šå®ç°äº†å…ˆè¿›çš„ASRå’ŒAVSRå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚åœ¨å™ªå£°æ¡ä»¶ä¸‹ï¼Œè§†å¬whisper-flamingoåœ¨è‹±è¯­è¯­éŸ³è¯†åˆ«å’Œ6ç§è¯­è¨€çš„è‹±Xç¿»è¯‘ä¸Šä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„whisperã€‚æ­¤å¤–ï¼Œwhisper-flamingoé€šç”¨æ€§å¼ºï¼Œä½¿ç”¨ä¸€ç»„å‚æ•°å³å¯æ‰§è¡Œæ‰€æœ‰è¿™äº›ä»»åŠ¡ï¼Œè€Œå…ˆå‰çš„æ–¹æ³•åˆ™éœ€è¦åœ¨æ¯ç§è¯­è¨€ä¸Šè¿›è¡Œå•ç‹¬è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSRä½¿ç”¨è§†é¢‘å”‡è¯­è¯†åˆ«æå‡å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>AVSRæ¨¡å‹é€šå¸¸å—é™äºæ•°åƒå°æ—¶çš„è§†é¢‘è®­ç»ƒæ•°æ®ã€‚</li>
<li>å¯¹æ¯”AVSRæ¨¡å‹ï¼Œwhisperç­‰è¯­éŸ³æ¨¡å‹å¯é€šè¿‡æ•°åä¸‡å°æ—¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ‹¥æœ‰æ›´ä½³çš„è¯­éŸ³è½¬æ–‡æœ¬è§£ç å™¨ã€‚</li>
<li>åŸºäºå·¨å¤§çš„è®­ç»ƒæ•°æ®å·®å¼‚ï¼Œwhisperéœ€è¦é€‚åº”å¤„ç†è§†é¢‘è¾“å…¥ã€‚</li>
<li>å€Ÿé‰´Flamingoçš„è®¾è®¡ï¼Œæå‡ºäº†æ•´åˆè§†è§‰ç‰¹å¾çš„whisper-flamingoæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†å…ˆè¿›çš„ASRå’ŒAVSRå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4b7a9914ffaa0d5807dab12a05582f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d82db65fd9bac4f80abd87f16c335fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d066c3aa4fa6bfbeafff65a7261d8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a12a100f8e3e0757ca865e2fe061789.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ce46907bde275417f2e23069671f046.jpg" align="middle">
</details>




<h2 id="TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking"><a href="#TraceableSpeech-Towards-Proactively-Traceable-Text-to-Speech-with-Watermarking" class="headerlink" title="TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking"></a>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with   Watermarking</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen</strong></p>
<p>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech">https://github.com/zjzser/TraceableSpeech</a> </p>
<blockquote>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥çš„å„ç§å¨èƒï¼Œå¯¹åˆæˆè¯­éŸ³çš„å¯é è¿½è¸ªå˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•æ˜¯åœ¨ç”ŸæˆéŸ³é¢‘åå•ç‹¬æ·»åŠ æ°´å°ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢å½±å“è¯­éŸ³è´¨é‡ï¼Œä¹Ÿå½±å“æ°´å°çš„ä¸å¯æ„ŸçŸ¥æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨ç¨³å¥æ€§å’Œçµæ´»æ€§æ–¹é¢ä¹Ÿå­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TraceableSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹TTSæ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„è¯­éŸ³ï¼Œæé«˜äº†æ°´å°çš„ä¸å¯æ„ŸçŸ¥æ€§å’Œè¯­éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¸§çº§æ°´å°å°åˆ¶å’Œæå–æŠ€æœ¯ï¼Œå®ç°äº†å¯¹æ‹¼æ¥æ”»å‡»çš„æ›´é«˜é²æ£’æ€§å’Œæ“ä½œä¸Šçš„æ—¶é—´çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceableSpeechåœ¨ä¸å¯æ„ŸçŸ¥æ°´å°ã€è¯­éŸ³è´¨é‡å’ŒæŠµæŠ—æ‹¼æ¥æ”»å‡»æ–¹é¢è¶…è¶Šäº†VALL-Eæˆ–HiFicodecå•ç‹¬ä½¿ç”¨WavMarkçš„å¼ºå¤§åŸºçº¿ã€‚å®ƒè¿˜å¯ä»¥åº”ç”¨äºå„ç§æ—¶é•¿çš„è¯­éŸ³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjzser/TraceableSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjzser/TraceableSpeechæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04840v3">PDF</a> acceped by interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œåˆæˆè¯­éŸ³çš„æº¯æºé—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•åœ¨éŸ³é¢‘ç”Ÿæˆåå•ç‹¬æ·»åŠ æ°´å°ï¼Œå½±å“è¯­éŸ³è´¨é‡å’Œæ°´å°çš„éšè”½æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†TraceableSpeechï¼Œä¸€ç§ç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„TTSæ¨¡å‹ï¼Œæé«˜äº†æ°´å°çš„éšè”½æ€§å’Œè¯­éŸ³è´¨é‡ï¼Œå¹¶å®ç°å¸§çº§æ°´å°å°ç›–å’Œæå–ï¼Œå¢å¼ºäº†å¯¹æŠ—æ‹¼æ¥æ”»å‡»çš„é²æ£’æ€§å’Œæ“ä½œä¸Šçš„æ—¶é—´çµæ´»æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTraceableSpeechåœ¨æ°´å°éšè”½æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠ—æ‹¼æ¥æ”»å‡»æ–¹é¢ä¼˜äºä½¿ç”¨WavMarkçš„VALL-Eæˆ–HiFicodecç­‰å¼ºåŸºçº¿æ¨¡å‹ï¼Œä¸”é€‚ç”¨äºä¸åŒæ—¶é•¿è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSæŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥äº†åˆæˆè¯­éŸ³æº¯æºçš„æ–°éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ·»åŠ æ°´å°æ–¹æ³•å½±å“è¯­éŸ³è´¨é‡å’Œæ°´å°éšè”½æ€§ã€‚</li>
<li>TraceableSpeechæ˜¯ä¸€ç§ç›´æ¥ç”Ÿæˆå¸¦æ°´å°çš„TTSæ¨¡å‹ï¼Œæé«˜æ°´å°éšè”½æ€§å’Œè¯­éŸ³è´¨é‡ã€‚</li>
<li>TraceableSpeechå®ç°å¸§çº§æ°´å°å°ç›–å’Œæå–ï¼Œå¢å¼ºé²æ£’æ€§å’Œæ—¶é—´çµæ´»æ€§ã€‚</li>
<li>TraceableSpeechåœ¨æ°´å°éšè”½æ€§ã€è¯­éŸ³è´¨é‡å’ŒæŠ—æ‹¼æ¥æ”»å‡»æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TraceableSpeeché€‚ç”¨äºä¸åŒæ—¶é•¿çš„è¯­éŸ³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97c6828d0da3286cc6920cdb3879b4fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6285c04fa08cef9b61b0f5c2ff36bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cdd16f22876d21554a7540268e64167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ded1e56fff528d7b750babfe276f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25cf3e4d4ee6ecc04ed0119d33defbda.jpg" align="middle">
</details>




<h2 id="Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test"><a href="#Hearing-Loss-Compensation-Using-Deep-Neural-Networks-A-Framework-and-Results-From-a-Listening-Test" class="headerlink" title="Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test"></a>Hearing-Loss Compensation Using Deep Neural Networks: A Framework and   Results From a Listening Test</h2><p><strong>Authors:Peter Leer, Jesper Jensen, Laurel H. Carney, Zheng-Hua Tan, Jan Ã˜stergaard, Lars BramslÃ¸w</strong></p>
<p>This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation. Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation. DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification. In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals. First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway. We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation. Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy. Finally, we evaluate, to our knowledge for the first time, the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners. The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies. Our proposed approach was shown to provide an increase in speech intelligibility versus an unprocessed baseline and was found to outperform a conventional approach in terms of both intelligibility and preference. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¬åŠ›æŸå¤±è¡¥å¿æ–¹é¢çš„åº”ç”¨ã€‚å¬åŠ›æŸå¤±æ˜¯ä¸€ä¸ªå½±å“å…¨çƒæ•°ç™¾ä¸‡äººçš„æ™®éé—®é¢˜ï¼Œä¼ ç»Ÿçš„åŠ©å¬å™¨åœ¨æä¾›æ»¡æ„çš„è¡¥å¿æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å„ç§å¬è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººè¯†åˆ«å’ŒéŸ³ä¹åˆ†ç±»ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯åœ¨å¬åŠ›å—æŸå’Œæ­£å¸¸å¬åŠ›æ·±åº¦ç¥ç»ç½‘ç»œå¬è§‰æ¨¡å‹å¯¹è¯­éŸ³ä¿¡å·çš„è¾“å‡ºååº”ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿå¬è§‰æ¨¡å‹çš„æ¡†æ¶ï¼Œé‡ç‚¹æ˜¯ä¸€ä¸ªå¬è§‰é€šè·¯ä¸­çš„å¬è§‰ç¥ç»æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•çš„çº¿æ€§åŒ–ï¼Œç”¨äºåˆ†æåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥é€‰æ‹©å¬è§‰æ¨¡å‹ä¸­çš„å£°éŸ³ä¸­å¿ƒé¢‘ç‡ï¼Œç”¨äºè¡¥å¿ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å¬åŠ›å—æŸå¬ä¼—çš„è†å¬æµ‹è¯•ï¼Œé¦–æ¬¡è¯„ä¼°äº†åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯¼è‡´äº†å¯è¡Œçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥ã€‚ä¸æœªå¤„ç†çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬æ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†è¯­éŸ³æ¸…æ™°åº¦ï¼Œå¹¶ä¸”åœ¨æ¸…æ™°åº¦å’Œåå¥½æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10420v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è¿›è¡Œå¬åŠ›æŸå¤±è¡¥å¿çš„ç ”ç©¶ã€‚æ–‡ç« ä»‹ç»äº†åŸºäºDNNçš„å¬è§‰æ¨¡å‹ä»¿çœŸæ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§çº¿æ€§åŒ–çš„DNNå¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥é€‰æ‹©å¬è§‰æ¨¡å‹ä¸­ç”¨äºè¡¥å¿ç­–ç•¥çš„å£°ä¸­å¿ƒé¢‘ç‡ã€‚é€šè¿‡å¬åŠ›å—æŸè€…çš„å¬åŠ›æµ‹è¯•ï¼ŒéªŒè¯äº†åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜è¯­éŸ³æ¸…æ™°åº¦ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å¯¹å¬åŠ›æŸå¤±è¿›è¡Œè¡¥å¿ï¼Œé’ˆå¯¹å…¨çƒæ•°ç™¾ä¸‡å¬åŠ›å—æŸäººç¾¤çš„éœ€æ±‚ã€‚</li>
<li>ä»‹ç»äº†åŸºäºDNNçš„å¬è§‰æ¨¡å‹ä»¿çœŸæ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯å¬è§‰ç¥ç»æ¨¡å‹çš„æ¨¡æ‹Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ç§çº¿æ€§åŒ–çš„DNNå¬åŠ›æŸå¤±è¡¥å¿æ–¹æ³•ï¼Œç”¨äºåˆ†æè¯¥ç­–ç•¥ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥é€‰æ‹©å¬è§‰æ¨¡å‹ä¸­ç”¨äºè¡¥å¿ç­–ç•¥çš„å£°ä¸­å¿ƒé¢‘ç‡ã€‚</li>
<li>é€šè¿‡å¬åŠ›å—æŸè€…çš„æµ‹è¯•ï¼ŒéªŒè¯äº†åŸºäºDNNçš„å¬åŠ›æŸå¤±è¡¥å¿ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åŸºäºDNNçš„è¡¥å¿ç­–ç•¥èƒ½æé«˜è¯­éŸ³æ¸…æ™°åº¦ï¼Œç›¸è¾ƒäºæœªå¤„ç†çš„åŸºç¡€çº¿åŠä¼ ç»Ÿæ–¹æ³•è¡¨ç°æ›´ä½³ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºå¬åŠ›æŸå¤±è¡¥å¿æä¾›äº†æ–°çš„å¯è¡Œç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb96d0056db88ee2c733128931b7fdde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b17b20446f95113e30d62616a7728759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e67e7cf7bc58f078a820af9ac539ae7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c45eba05418f6e1ebeb65af3ab0e310e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b2b0e9ca74e4133de6132871478d14d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4f583d4a12289ffcf1e06220ca86407.jpg" align="middle">
</details>




<h2 id="Speech-Robust-Bench-A-Robustness-Benchmark-For-Speech-Recognition"><a href="#Speech-Robust-Bench-A-Robustness-Benchmark-For-Speech-Recognition" class="headerlink" title="Speech Robust Bench: A Robustness Benchmark For Speech Recognition"></a>Speech Robust Bench: A Robustness Benchmark For Speech Recognition</h2><p><strong>Authors:Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Bhiksha Raj, Nicolas Kourtellis</strong></p>
<p>As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 114 input perturbations which simulate an heterogeneous range of corruptions that ASR models may encounter when deployed in the wild. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as the use of discrete representations, or self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females. Our results revealed noticeable disparities in the modelâ€™s robustness across subgroups. We believe that SRB will significantly facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨ç‰©ç†ä¸–ç•Œå’Œæ•°å­—ä¸–ç•Œä¸­è¶Šæ¥è¶Šæ™®åŠï¼Œç¡®ä¿å®ƒä»¬åœ¨å­˜åœ¨çš„å„ç§å¹²æ‰°ä¸‹åšå‡ºå¯é é¢„æµ‹å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†è¯­éŸ³é²æ£’åŸºå‡†ï¼ˆSRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°ASRæ¨¡å‹å¯¹å„ç§å¹²æ‰°é²æ£’æ€§çš„åŸºå‡†æµ‹è¯•ã€‚SRBç”±114ç§è¾“å…¥æ‰°åŠ¨ç»„æˆï¼Œæ¨¡æ‹Ÿäº†ASRæ¨¡å‹åœ¨é‡å¤–éƒ¨ç½²æ—¶å¯èƒ½é‡åˆ°çš„å„ç§å¼‚è´¨å¹²æ‰°ã€‚æˆ‘ä»¬ä½¿ç”¨SRBæ¥è¯„ä¼°å‡ ç§æœ€å…ˆè¿›çš„ASRæ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°æ¨¡å‹å¤§å°ä»¥åŠæŸäº›å»ºæ¨¡é€‰æ‹©ï¼Œå¦‚ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºæˆ–è‡ªè®­ç»ƒï¼Œæœ‰åŠ©äºæ¨¡å‹çš„é²æ£’æ€§ã€‚æˆ‘ä»¬å°†æ­¤åˆ†ææ‰©å±•åˆ°è¡¡é‡æ¥è‡ªä¸åŒäººå£äºšç»„çš„æ•°æ®çš„ASRæ¨¡å‹çš„é²æ£’æ€§ï¼Œå³è‹±è¯­å’Œè¥¿ç­ç‰™è¯­ä½¿ç”¨è€…ä»¥åŠç”·æ€§å’Œå¥³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†å„äºšç»„æ¨¡å‹é²æ£’æ€§çš„æ˜æ˜¾å·®å¼‚ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œé€šè¿‡ä½¿è¿›è¡Œæ›´å…¨é¢å’Œå¯æ¯”è¾ƒçš„ç¨³å®šæ€§è¯„ä¼°å˜å¾—æ›´å®¹æ˜“ï¼ŒSRBå°†æå¤§åœ°æ¨åŠ¨æœªæ¥å¯¹ç¨³å¥çš„ASRæ¨¡å‹çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07937v3">PDF</a> submitted to NeurIPS datasets and benchmark track 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å¥å£®æ€§è¯„ä¼°çš„æ–°åŸºå‡†â€”â€”Speech Robust Benchï¼ˆSRBï¼‰ã€‚è¯¥åŸºå‡†åŒ…å«äº†æ¨¡æ‹Ÿé‡å¤–ASRæ¨¡å‹å¯èƒ½é‡åˆ°çš„å¤šç§è…èš€æƒ…å†µçš„114ç§è¾“å…¥æ‰°åŠ¨ã€‚é€šè¿‡å¯¹å¤šç§å‰æ²¿ASRæ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹å¤§å°åŠæŸäº›å»ºæ¨¡é€‰æ‹©å¦‚ç¦»æ•£è¡¨ç¤ºæˆ–è‡ªè®­ç»ƒæœ‰åŠ©äºæ¨¡å‹çš„å¥å£®æ€§ã€‚åŒæ—¶ï¼Œä½œè€…å¯¹æ¥è‡ªä¸åŒäººå£äºšç»„çš„æ¨¡å‹å¥å£®æ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ä¸åŒäºšç»„ä¹‹é—´å­˜åœ¨ä¸€å®šçš„å·®å¼‚ã€‚ä½œè€…è®¤ä¸ºï¼ŒSRBå°†æå¤§åœ°æ¨åŠ¨æœªæ¥å¯¹å¥å£®æ€§å¼ºçš„ASRæ¨¡å‹çš„ç ”ç©¶ï¼Œä½¿å…¨é¢å’Œå¯æ¯”è¾ƒçš„è¯„ä»·æ›´ä¸ºä¾¿æ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Speech Robust Benchï¼ˆSRBï¼‰åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å¥å£®æ€§ã€‚</li>
<li>SRBåŒ…å«äº†æ¨¡æ‹Ÿé‡å¤–ASRæ¨¡å‹å¯èƒ½é‡åˆ°çš„å¤šç§è…èš€æƒ…å†µçš„114ç§è¾“å…¥æ‰°åŠ¨ã€‚</li>
<li>å¯¹å¤šç§å‰æ²¿ASRæ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œæ¨¡å‹å¤§å°å’ŒæŸäº›å»ºæ¨¡é€‰æ‹©å½±å“æ¨¡å‹çš„å¥å£®æ€§ã€‚</li>
<li>ç¦»æ•£è¡¨ç¤ºå’Œè‡ªè®­ç»ƒç­‰å»ºæ¨¡é€‰æ‹©å¯¹ASRæ¨¡å‹çš„å¥å£®æ€§æœ‰ç§¯æå½±å“ã€‚</li>
<li>ä¸åŒäººå£äºšç»„çš„ASRæ¨¡å‹å¥å£®æ€§å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚</li>
<li>ASRæ¨¡å‹çš„å¥å£®æ€§è¯„ä»·éœ€è¦æ›´å…¨é¢åœ°è€ƒè™‘ä¸åŒå› ç´ ï¼ŒåŒ…æ‹¬æ¨¡å‹ç»“æ„ã€è®­ç»ƒç­–ç•¥ä»¥åŠæ•°æ®æ¥æºç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c582f037a408bd577abfa57d725d7bda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2103d6119e46973d4d2973b31715cea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae3e3036452a0b887e4798ca7935fd7e.jpg" align="middle">
</details>




<h2 id="PixIT-Joint-Training-of-Speaker-Diarization-and-Speech-Separation-from-Real-world-Multi-speaker-Recordings"><a href="#PixIT-Joint-Training-of-Speaker-Diarization-and-Speech-Separation-from-Real-world-Multi-speaker-Recordings" class="headerlink" title="PixIT: Joint Training of Speaker Diarization and Speech Separation from   Real-world Multi-speaker Recordings"></a>PixIT: Joint Training of Speaker Diarization and Speech Separation from   Real-world Multi-speaker Recordings</h2><p><strong>Authors:Joonas Kalda, ClÃ©ment PagÃ©s, Ricard Marxer, Tanel AlumÃ¤e, HervÃ© Bredin</strong></p>
<p>A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization. Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio. We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep. With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD. We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them. PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning. </p>
<blockquote>
<p>ç›‘ç£è¯­éŸ³åˆ†ç¦»ï¼ˆSSepï¼‰ç³»ç»Ÿçš„ä¸€ä¸ªä¸»è¦ç¼ºç‚¹æ˜¯å®ƒä»¬ä¾èµ–äºåˆæˆæ•°æ®ï¼Œå¯¼è‡´åœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æå‡ºäº†æ··åˆä¸å˜è®­ç»ƒï¼ˆMixITï¼‰ä½œä¸ºä¸€ç§ä½¿ç”¨çœŸå®å½•éŸ³çš„æ— ç›‘ç£æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨è¶…åˆ†ç¦»å’Œé€‚åº”é•¿éŸ³é¢‘æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬ä»‹ç»äº†PixITï¼Œè¿™æ˜¯ä¸€ç§è”åˆæ–¹æ³•ï¼Œç»“åˆäº†ç”¨äºè¯´è¯äººæ—¥è®°åŒ–ï¼ˆSDï¼‰çš„æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰å’Œç”¨äºSSepçš„MixITã€‚å®ƒåªéœ€è¦å°‘é‡çš„SDæ ‡ç­¾ï¼Œè§£å†³äº†è¶…åˆ†ç¦»é—®é¢˜ï¼Œå¹¶å…è®¸åˆ©ç”¨åŸºäºèšç±»çš„ç¥ç»SDçš„ç°æœ‰å·¥ä½œæ¥æ‹¼æ¥å±€éƒ¨åˆ†ç¦»æºã€‚æˆ‘ä»¬é€šè¿‡å°†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåº”ç”¨äºåˆ†ç¦»æºæ¥æµ‹é‡å…¶è´¨é‡ã€‚PixITæé«˜äº†ä¸¤ä¸ªä¼šè®®è¯­æ–™åº“ä¸­å„ç§ASRç³»ç»Ÿçš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯åœ¨è¯´è¯äººå±æ€§å’ŒåŸºäºè¯è¯­çš„å•è¯é”™è¯¯ç‡æ–¹é¢éƒ½ä¸éœ€è¦è¿›è¡Œä»»ä½•å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02288v2">PDF</a> Speaker Odyssey 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºPixITæ–¹æ³•ï¼Œç»“åˆç”¨äºè¯­éŸ³åˆ†ç¦»çš„ç›‘ç£è®­ç»ƒä¸ç”¨äºè¯´è¯äººè¯†åˆ«çš„æ— ç›‘ç£è®­ç»ƒï¼Œè§£å†³äº†ç°æœ‰è¯­éŸ³åˆ†ç¦»ç³»ç»Ÿä¾èµ–åˆæˆæ•°æ®å¯¼è‡´çš„çœŸå®åœºæ™¯æ³›åŒ–æ€§èƒ½å·®çš„é—®é¢˜ã€‚PixITæ–¹æ³•åˆ©ç”¨ç°æœ‰çš„è¯´è¯äººèšç±»ç¥ç»ç½‘ç»œæ¥è§£å†³è¿‡åº¦åˆ†ç¦»é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ç°æœ‰çš„è¯´è¯äººæ—¥è®°è¿›è¡Œæœ¬åœ°éŸ³æºåˆ†ç¦»å’Œæ‹¼æ¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPixITå¯ä»¥æå‡å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨ä¸¤ç§ä¼šè®®è¯­æ–™åº“ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯´è¯äººå±æ€§å’ŒåŸºäºè¯è¯­çš„å•è¯é”™è¯¯ç‡ï¼Œä¸”æ— éœ€è¿›è¡Œå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PixITç»“åˆäº†ç”¨äºè¯´è¯äººæ—¥è®°çš„æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰å’Œç”¨äºè¯­éŸ³åˆ†ç¦»çš„æ··åˆä¸å˜è®­ç»ƒï¼ˆMixITï¼‰ã€‚</li>
<li>PixITè§£å†³äº†è¿‡åº¦åˆ†ç¦»é—®é¢˜ï¼Œå…è®¸é€šè¿‡ç°æœ‰çš„åŸºäºèšç±»çš„ç¥ç»ç½‘ç»œè¿›è¡Œæœ¬åœ°éŸ³æºåˆ†ç¦»å’Œæ‹¼æ¥ã€‚</li>
<li>PixITéœ€è¦å°‘é‡çš„è¯´è¯äººæ—¥è®°æ ‡ç­¾ã€‚</li>
<li>PixITæå‡äº†å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨ä¸¤ç§ä¼šè®®è¯­æ–™åº“ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ€§èƒ½æå‡ä½“ç°åœ¨è¯´è¯äººå±æ€§å’ŒåŸºäºè¯è¯­çš„å•è¯é”™è¯¯ç‡ä¸Šã€‚</li>
<li>PixITæ–¹æ³•ä¸éœ€è¦å¯¹ç°æœ‰ç³»ç»Ÿè¿›è¡Œå¾®è°ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb8c7c975906de0cc2794e814b8e474f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70becf78ea369ed7bca4ec084f0bed37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88ffc36e5afa09a7eb2baf7aa9b9d5db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3175ee73eb958a6b067ef7c692d2d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddc3774b23fc9d08df84942104d1466f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74c56e981436794750632941f563d4e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd83f38f67461d69033f4764feafef20.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ffdfb6ab93b365ba5c1ce937f218ada2.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  BLADE Single-view Body Mesh Learning through Accurate Depth Estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e4062550094aa078f557d3d317e4952a.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">12289.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
