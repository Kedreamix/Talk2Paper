<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-23  Dita Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-95b2c5cae75f3c8c7bee2c937d247270.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-23-æ›´æ–°"><a href="#2025-03-23-æ›´æ–°" class="headerlink" title="2025-03-23 æ›´æ–°"></a>2025-03-23 æ›´æ–°</h1><h2 id="Dita-Scaling-Diffusion-Transformer-for-Generalist-Vision-Language-Action-Policy"><a href="#Dita-Scaling-Diffusion-Transformer-for-Generalist-Vision-Language-Action-Policy" class="headerlink" title="Dita: Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy"></a>Dita: Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy</h2><p><strong>Authors:Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen</strong></p>
<p>While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning â€“ enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformerâ€™s scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: <a target="_blank" rel="noopener" href="https://robodita.github.io/">https://robodita.github.io/</a> </p>
<blockquote>
<p>è™½ç„¶æœ€è¿‘åŸºäºå¤šæ ·æœºå™¨äººæ•°æ®é›†è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœ‰é™é¢†åŸŸå†…æ•°æ®ä¸Šå±•ç°å‡ºæœ‰å‰æ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¾èµ–äºç´§å‡‘çš„åŠ¨ä½œå¤´æ¥é¢„æµ‹ç¦»æ•£æˆ–è¿ç»­åŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¼‚æ„åŠ¨ä½œç©ºé—´ä¸­çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Ditaï¼Œä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨Transformeræ¶æ„é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹ç›´æ¥å¯¹è¿ç»­åŠ¨ä½œåºåˆ—è¿›è¡Œå»å™ªã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•é€šè¿‡æµ…å±‚ç½‘ç»œå¯¹èåˆåµŒå…¥è¿›è¡Œå»å™ªæ¡ä»¶å¤„ç†ï¼Œè€ŒDitaé‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶å¤„ç†â€”â€”å®ç°åœ¨å»å™ªåŠ¨ä½œå’Œæ¥è‡ªå†å²è§‚å¯Ÿçš„åŸè§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜ç¡®åœ°æ¨¡æ‹Ÿäº†åŠ¨ä½œå˜åŒ–å’Œç¯å¢ƒç»†å¾®å·®å¼‚ã€‚é€šè¿‡æ‰©å¤§æ‰©æ•£åŠ¨ä½œå»å™ªå™¨ä¸Transformerçš„å¯æ‰©å±•æ€§ï¼ŒDitaæœ‰æ•ˆåœ°æ•´åˆäº†è·¨ä¸åŒç›¸æœºè§’åº¦ã€è§‚å¯Ÿåœºæ™¯ã€ä»»åŠ¡å’ŒåŠ¨ä½œç©ºé—´çš„è·¨ä½“æ€æ•°æ®é›†ã€‚è¿™ç§ååŒä½œç”¨å¢å¼ºäº†å¯¹å„ç§å˜é‡çš„ç¨³å¥æ€§ï¼Œå¹¶ä¿ƒè¿›äº†é•¿æœŸä»»åŠ¡çš„æˆåŠŸæ‰§è¡Œã€‚åœ¨å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶åœ¨ä»¿çœŸä¸­çš„å“è¶Šæˆ–ç›¸å½“çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDitaä»…é€šè¿‡ç¬¬ä¸‰äººç§°ç›¸æœºè¾“å…¥å®ç°äº†å¯¹ç¯å¢ƒå’Œå¤æ‚é•¿æœŸä»»åŠ¡çš„ç¨³å¥é€‚åº”ï¼Œé€šè¿‡10æ¬¡å°„å‡»å¾®è°ƒè¾¾åˆ°ç¨³å¥æ•ˆæœã€‚è¯¥æ¶æ„ä¸ºé€šç”¨æœºå™¨äººç­–ç•¥å­¦ä¹ å»ºç«‹äº†é€šç”¨ã€è½»ä¾¿å’Œå¼€æºçš„åŸºçº¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://robodita.github.io/">https://robodita.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15959v5">PDF</a> I want to withdraw the recent replacement (v4), given that the author   is different, the title is also different and the content is totally   different</p>
<p><strong>Summary</strong><br>åŸºäºTransformeræ¶æ„çš„Ditaæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹ç›´æ¥å¯¹è¿ç»­åŠ¨ä½œåºåˆ—è¿›è¡Œå»å™ªã€‚ä¸åŒäºä¾èµ–èåˆåµŒå…¥å¹¶é€šè¿‡æµ…å±‚ç½‘ç»œè¿›è¡Œæ¡ä»¶å»å™ªçš„æ–¹æ³•ï¼ŒDitaé‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ï¼Œå¹¶æ˜¾å¼åœ°æ¨¡æ‹ŸåŠ¨ä½œå˜åŒ–å’Œç¯å¢ƒç»†å¾®å·®åˆ«ã€‚é€šè¿‡ä¸Transformerçš„å¯æ‰©å±•æ€§ä¸€èµ·æ‰©å±•æ‰©æ•£åŠ¨ä½œå»å™ªå™¨ï¼ŒDitaèƒ½å¤ŸæˆåŠŸé›†æˆè·¨ä¸åŒè§†è§’ã€è§‚å¯Ÿåœºæ™¯ã€ä»»åŠ¡å’ŒåŠ¨ä½œç©ºé—´çš„è·¨ä½“æ€æ•°æ®é›†ã€‚è¿™æé«˜äº†å¯¹å„ç§å·®å¼‚çš„ç¨³å¥æ€§ï¼Œå¹¶ä¿ƒè¿›äº†é•¿æœŸä»»åŠ¡çš„æˆåŠŸæ‰§è¡Œã€‚åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»…æœ‰ç¬¬ä¸‰äººç§°ç›¸æœºè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å‡ æ¬¡å¾®è°ƒå°±èƒ½å®ç°å¯¹ç¯å¢ƒå˜åŒ–å’Œå¤æ‚é•¿æœŸä»»åŠ¡çš„ç¨³å¥é€‚åº”ã€‚è¿™ä¸ºæœºå™¨äººæ”¿ç­–å­¦ä¹ å»ºç«‹äº†ä¸€ä¸ªé€šç”¨ã€è½»ä¾¿ä¸”å¼€æºçš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ditaæ¡†æ¶åˆ©ç”¨Transformeræ¶æ„ç›´æ¥å¯¹è¿ç»­åŠ¨ä½œåºåˆ—è¿›è¡Œå»å™ªï¼Œå±•ç°å¼ºå¤§çš„é€‚åº”æ€§ã€‚</li>
<li>Ditaé‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œå®ç°å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸåŠ¨ä½œå˜åŒ–å’Œç¯å¢ƒç»†å¾®å·®åˆ«ï¼ŒDitaæé«˜äº†æ¨¡å‹çš„å¥å£®æ€§ã€‚</li>
<li>Ditaèƒ½æœ‰æ•ˆé›†æˆè·¨ä¸åŒè§†è§’ã€è§‚å¯Ÿåœºæ™¯ã€ä»»åŠ¡å’ŒåŠ¨ä½œç©ºé—´çš„è·¨ä½“æ€æ•°æ®é›†ã€‚</li>
<li>Ditaæ¡†æ¶å¯¹äºé•¿æœŸä»»åŠ¡çš„æ‰§è¡Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Ditaåœ¨ä»…æœ‰ç¬¬ä¸‰äººç§°ç›¸æœºè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å°‘é‡æ•°æ®å¾®è°ƒå°±èƒ½é€‚åº”ç¯å¢ƒå˜åŒ–å’Œå¤æ‚ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc071d967b80a57e0d6cdb63883f54ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-111670b48282de8885730ac30b2d90ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72d788e40e6d99eb63d6d13ebc22fe07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e2172585dffa9a8af74e0be83936f5b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPINE-Online-Semantic-Planning-for-Missions-with-Incomplete-Natural-Language-Specifications-in-Unstructured-Environments"><a href="#SPINE-Online-Semantic-Planning-for-Missions-with-Incomplete-Natural-Language-Specifications-in-Unstructured-Environments" class="headerlink" title="SPINE: Online Semantic Planning for Missions with Incomplete Natural   Language Specifications in Unstructured Environments"></a>SPINE: Online Semantic Planning for Missions with Incomplete Natural   Language Specifications in Unstructured Environments</h2><p><strong>Authors:Zachary Ravichandran, Varun Murali, Mariliza Tzes, George J. Pappas, Vijay Kumar</strong></p>
<p>As robots become increasingly capable, users will want to describe high-level missions and have robots infer the relevant details. because pre-built maps are difficult to obtain in many realistic settings, accomplishing such missions will require the robot to map and plan online. while many semantic planning methods operate online, they are typically designed for well specified missions such as object search or exploration. recently, large language models (LLMs) have demonstrated powerful contextual reasoning abilities over a range of robotic tasks described in natural language. however, existing LLM-enabled planners typically do not consider online planning or complex missions; rather, relevant subtasks and semantics are provided by a pre-built map or a user. we address these limitations via spine, an online planner for missions with incomplete mission specifications provided in natural language. the planner uses an LLM to reason about subtasks implied by the mission specification and then realizes these subtasks in a receding horizon framework. tasks are automatically validated for safety and refined online with new map observations. we evaluate spine in simulation and real-world settings with missions that require multiple steps of semantic reasoning and exploration in cluttered outdoor environments of over 20,000m$^2$. compared to baselines that use existing LLM-enabled planning approaches, our method is over twice as efficient in terms of time and distance, requires less user interactions, and does not require a full map. Additional resources are provided at: <a target="_blank" rel="noopener" href="https://zacravichandran.github.io/SPINE">https://zacravichandran.github.io/SPINE</a>. </p>
<blockquote>
<p>éšç€æœºå™¨äººçš„èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œç”¨æˆ·å°†å¸Œæœ›æè¿°é«˜çº§ä»»åŠ¡ï¼Œå¹¶è®©æœºå™¨äººæ¨æ–­ç›¸å…³ç»†èŠ‚ã€‚ç”±äºåœ¨è®¸å¤šç°å®åœºæ™¯ä¸­å¾ˆéš¾è·å¾—é¢„å…ˆæ„å»ºçš„åœ°å›¾ï¼Œå› æ­¤å®Œæˆæ­¤ç±»ä»»åŠ¡å°†è¦æ±‚æœºå™¨äººåœ¨çº¿è¿›è¡Œåœ°å›¾ç»˜åˆ¶å’Œè§„åˆ’ã€‚è™½ç„¶è®¸å¤šè¯­ä¹‰è§„åˆ’æ–¹æ³•éƒ½æ˜¯åœ¨çº¿è¿è¡Œçš„ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„ï¼Œå¦‚ç›®æ ‡æœç´¢æˆ–æ¢ç´¢ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›ä»»åŠ¡æ˜¯ç”¨è‡ªç„¶è¯­è¨€æè¿°çš„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ”¯æŒè§„åˆ’å¸ˆé€šå¸¸ä¸è€ƒè™‘åœ¨çº¿è§„åˆ’æˆ–å¤æ‚ä»»åŠ¡ï¼›è€Œæ˜¯ç”±é¢„å…ˆæ„å»ºçš„åœ°å›¾æˆ–ç”¨æˆ·æä¾›ç›¸å…³çš„å­ä»»åŠ¡å’Œè¯­ä¹‰ã€‚æˆ‘ä»¬é€šè¿‡SPINEæ¥è§£å†³è¿™äº›å±€é™æ€§ï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿è§„åˆ’å¸ˆï¼Œç”¨äºå¤„ç†ä»¥è‡ªç„¶è¯­è¨€æä¾›çš„ä¸å®Œæ•´ä»»åŠ¡è§„æ ¼çš„ä»»åŠ¡ã€‚è§„åˆ’å¸ˆä½¿ç”¨LLMæ¥æ¨ç†ä»»åŠ¡è§„æ ¼æ‰€æš—ç¤ºçš„å­ä»»åŠ¡ï¼Œç„¶ååœ¨åé€€åœ°å¹³çº¿æ¡†æ¶ä¸­å®ç°è¿™äº›å­ä»»åŠ¡ã€‚ä»»åŠ¡ä¼šè‡ªåŠ¨éªŒè¯å®‰å…¨æ€§ï¼Œå¹¶æ ¹æ®æ–°çš„åœ°å›¾è§‚å¯Ÿç»“æœè¿›è¡Œåœ¨çº¿è°ƒæ•´ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å¯¹SPINEè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦è¿›è¡Œå¤šæ­¥éª¤çš„è¯­ä¹‰æ¨ç†å’Œæ‚ä¹±æˆ·å¤–ç¯å¢ƒçš„æ¢ç´¢ï¼Œé¢ç§¯è¾¾è¶…è¿‡2ä¸‡ç±³^ 2ã€‚ä¸é‡‡ç”¨ç°æœ‰LLMæ”¯æŒçš„è§„åˆ’æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ—¶é—´å’Œè·ç¦»æ–¹é¢æ•ˆç‡æé«˜äº†ä¸¤å€ä»¥ä¸Šï¼Œå‡å°‘äº†ç”¨æˆ·äº¤äº’æ¬¡æ•°ï¼Œå¹¶ä¸”ä¸éœ€è¦å®Œæ•´çš„åœ°å›¾ã€‚æ›´å¤šèµ„æºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://zacravichandran.github.io/SPINE%E3%80%82">https://zacravichandran.github.io/SPINEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03035v2">PDF</a> Accepted to the International Conference on Robotics and Automation   (ICRA) 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æœºå™¨äººåŠŸèƒ½çš„ä¸æ–­å¢å¼ºï¼Œç”¨æˆ·å°†æ›´å€¾å‘äºæè¿°é«˜çº§ä»»åŠ¡ï¼Œå¹¶å¸Œæœ›æœºå™¨äººèƒ½å¤Ÿæ¨æ–­ç›¸å…³ç»†èŠ‚ã€‚åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œè·å–é¢„å…ˆæ„å»ºçš„åœ°å›¾ååˆ†å›°éš¾ï¼Œå› æ­¤éœ€è¦å®ç°åœ¨çº¿æ˜ å°„å’Œè§„åˆ’æ¥å®Œæˆæ­¤ç±»ä»»åŠ¡ã€‚è®¸å¤šè¯­ä¹‰è§„åˆ’æ–¹æ³•è™½ç„¶æ”¯æŒåœ¨çº¿æ“ä½œï¼Œä½†å®ƒä»¬ä¸»è¦é’ˆå¯¹å¯¹è±¡æœç´¢æˆ–æ¢ç´¢ç­‰ä»»åŠ¡æ˜ç¡®çš„ç¯å¢ƒã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨äººä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†è‡ªç„¶è¯­è¨€æè¿°çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMè§„åˆ’å™¨å¹¶ä¸æ”¯æŒåœ¨çº¿è§„åˆ’æˆ–å¤æ‚ä»»åŠ¡ï¼›å®ƒä»¬ä¾èµ–äºé¢„å…ˆæ„å»ºçš„åœ°å›¾æˆ–ç”¨æˆ·æä¾›çš„å­ä»»åŠ¡å’Œè¯­ä¹‰ä¿¡æ¯ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSPINEæ–¹æ³•ï¼Œä¸€ç§åœ¨çº¿è§„åˆ’å™¨ï¼Œç”¨äºå¤„ç†ä»¥è‡ªç„¶è¯­è¨€å½¢å¼æä¾›çš„éƒ¨åˆ†ä¸æ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ã€‚SPINEé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£ä»»åŠ¡çš„å­æŒ‡ä»¤å¹¶è¿›è¡Œåœ¨çº¿è§„åˆ’å®ç°ã€‚å®ƒèƒ½å¤Ÿåœ¨ä¸å®Œæ•´çš„åœ°å›¾ä¸­è¿›è¡Œè‡ªåŠ¨ä»»åŠ¡éªŒè¯å¹¶ç¡®ä¿å®‰å…¨æ‰§è¡Œã€‚æˆ‘ä»¬åˆ†åˆ«åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­è¯„ä¼°äº†SPINEçš„æ•ˆæœï¼Œç»“æœè¯æ˜åœ¨å¤„ç†å¤æ‚çš„å®¤å¤–ç¯å¢ƒä¸­çš„è¯­ä¹‰æ¨ç†å’Œæ¢ç´¢ä»»åŠ¡æ—¶ï¼Œç›¸æ¯”ç°æœ‰çš„åŸºäºLLMçš„è§„åˆ’æ–¹æ³•ï¼ŒSPINEå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ›´å¤šèµ„æºè¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://zacravichandran.github.io/SPINE">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€æœºå™¨äººåŠŸèƒ½å¢å¼ºï¼Œç”¨æˆ·æ›´å€¾å‘æè¿°é«˜çº§ä»»åŠ¡ï¼Œå¹¶è¦æ±‚æœºå™¨äººå…·å¤‡åœ¨çº¿æ˜ å°„å’Œè§„åˆ’èƒ½åŠ›æ¥å¤„ç†ä¸ç¡®å®šæ€§å’Œç¯å¢ƒå¤æ‚æ€§ã€‚</li>
<li>LLMå±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤„ç†è‡ªç„¶è¯­è¨€æè¿°çš„æœºå™¨äººä»»åŠ¡ã€‚</li>
<li>ç°æœ‰LLMè§„åˆ’å™¨ä¸æ”¯æŒåœ¨çº¿è§„åˆ’å’Œå¤æ‚ä»»åŠ¡å¤„ç†ï¼Œä¾èµ–äºé¢„å…ˆæ„å»ºçš„åœ°å›¾æˆ–ç”¨æˆ·æä¾›çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>SPINEæ˜¯ä¸€ç§åœ¨çº¿è§„åˆ’å™¨ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£ä»»åŠ¡çš„å­æŒ‡ä»¤å¹¶è¿›è¡Œåœ¨çº¿è§„åˆ’å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6760a0ad301c58e0d4b43be55071dde0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c48e9c34dd71253bda2ae685b2443a31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b651f17157a54bc6570e8c46afe381d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-860f01a5d1039ed9de3ae8e5d05ba251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2697dc43dcf274c6da79dfe52c136025.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a1745ed9cc59fa81b539112af438d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e322edb05c178b312f3e85716027addb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Self-Introspective-Decoding-Alleviating-Hallucinations-for-Large-Vision-Language-Models"><a href="#Self-Introspective-Decoding-Alleviating-Hallucinations-for-Large-Vision-Language-Models" class="headerlink" title="Self-Introspective Decoding: Alleviating Hallucinations for Large   Vision-Language Models"></a>Self-Introspective Decoding: Alleviating Hallucinations for Large   Vision-Language Models</h2><p><strong>Authors:Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, Peilin Zhao</strong></p>
<p>While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the &#96;hallucinationâ€™ problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing auxiliary analysis networks, which inevitable incur additional costs. Another approach, known as contrastive decoding, induces hallucinations by manually disturbing the vision or instruction raw inputs and mitigates them by contrasting the outputs of the disturbed and original LVLMs. However, these approaches rely on empirical holistic input disturbances and double the inference cost. To avoid these issues, we propose a simple yet effective method named Self-Introspective Decoding (SID). Our empirical investigation reveals that pretrained LVLMs can introspectively assess the importance of vision tokens based on preceding vision and text (both instruction and generated) tokens. We develop the Context and Text-aware Token Selection (CT2S) strategy, which preserves only unimportant vision tokens after early layers of LVLMs to adaptively amplify text-informed hallucination during the auto-regressive decoding. This approach ensures that multimodal knowledge absorbed in the early layers induces multimodal contextual rather than aimless hallucinations. Subsequently, the original token logits subtract the amplified vision-and-text association hallucinations, guiding LVLMs decoding faithfully. Extensive experiments illustrate SID generates less-hallucination and higher-quality texts across various metrics, without extra knowledge and much additional computation burdens. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å‘å±•è¿…é€Ÿï¼Œä½†è¢«ç§°ä¸ºâ€œå¹»è§‰â€é—®é¢˜çš„æ™®éé—®é¢˜å·²æˆä¸ºä¸€ä¸ªé‡å¤§ç“¶é¢ˆï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä»ä¸¤ä¸ªè§’åº¦ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼šä¸€ç§æ–¹æ³•åˆ©ç”¨é¢å¤–çš„çŸ¥è¯†ï¼Œå¦‚ç”¨ç²¾é€‰æ•°æ®é›†å¯¹LVLMsè¿›è¡Œç¨³å¥æŒ‡ä»¤è°ƒæ•´æˆ–é‡‡ç”¨è¾…åŠ©åˆ†æç½‘ç»œï¼Œè¿™ä¸å¯é¿å…åœ°ä¼šå¸¦æ¥é¢å¤–æˆæœ¬ã€‚å¦ä¸€ç§æ–¹æ³•ç§°ä¸ºå¯¹æ¯”è§£ç ï¼Œå®ƒé€šè¿‡æ‰‹åŠ¨å¹²æ‰°è§†è§‰æˆ–æŒ‡ä»¤åŸå§‹è¾“å…¥æ¥è¯±å¯¼å¹»è§‰ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å¹²æ‰°å’ŒåŸå§‹LVLMsçš„è¾“å‡ºæ¥ç¼“è§£å®ƒä»¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºç»éªŒæ€§çš„æ•´ä½“è¾“å…¥å¹²æ‰°ï¼Œå¹¶ä¸”ä½¿æ¨ç†æˆæœ¬ç¿»å€ã€‚ä¸ºäº†é¿å…è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºè‡ªæˆ‘å†…çœè§£ç ï¼ˆSIDï¼‰ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„LVLMså¯ä»¥æ ¹æ®å…ˆå‰çš„è§†è§‰å’Œæ–‡æœ¬ï¼ˆåŒ…æ‹¬æŒ‡ä»¤å’Œç”Ÿæˆæ–‡æœ¬ï¼‰ä»¤ç‰Œæ¥å†…çœåœ°è¯„ä¼°è§†è§‰ä»¤ç‰Œçš„é‡è¦æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†åŸºäºä¸Šä¸‹æ–‡å’Œæ–‡æœ¬æ„ŸçŸ¥çš„ä»¤ç‰Œé€‰æ‹©ï¼ˆCT2Sï¼‰ç­–ç•¥ï¼Œåœ¨LVLMsçš„æ—©æœŸå±‚æ¬¡ä¸­ä»…ä¿ç•™ä¸é‡è¦çš„è§†è§‰ä»¤ç‰Œï¼Œä»¥åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°æ”¾å¤§æ–‡æœ¬ä¿¡æ¯è¯±å¯¼çš„å¹»è§‰ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†æ—©æœŸå±‚æ¬¡ä¸­å¸æ”¶çš„è·¨æ¨¡æ€çŸ¥è¯†ä¼šäº§ç”Ÿè·¨æ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯æ— ç›®æ ‡çš„å¹»è§‰ã€‚éšåï¼ŒåŸå§‹ä»¤ç‰Œå¯¹æ•°å‡å»æ”¾å¤§çš„è§†è§‰å’Œæ–‡æœ¬å…³è”å¹»è§‰ï¼Œå¼•å¯¼LVLMså¿ å®è§£ç ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSIDç”Ÿæˆçš„å¹»è§‰è¾ƒå°‘ï¼Œæ–‡æœ¬è´¨é‡è¾ƒé«˜ï¼Œä¸”æ— éœ€é¢å¤–çŸ¥è¯†å’Œå¤§é‡é¢å¤–è®¡ç®—è´Ÿæ‹…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02032v3">PDF</a> ICLR2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿…é€Ÿå‘å±•çš„åŒæ—¶ï¼Œ<code>hallucination</code>é—®é¢˜å·²æˆä¸ºé˜»ç¢å…¶å®é™…éƒ¨ç½²çš„é‡å¤§ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä»ä¸¤ä¸ªè§’åº¦ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä¸€ç§æ˜¯é€šè¿‡é¢å¤–çŸ¥è¯†å¦‚ä½¿ç”¨ç²¾é€‰æ•°æ®é›†å¯¹LVLMsè¿›è¡Œç¨³å¥æŒ‡ä»¤è°ƒæ•´æˆ–é‡‡ç”¨è¾…åŠ©åˆ†æç½‘ç»œæ¥ç¼“è§£ï¼Œä½†ä¸å¯é¿å…åœ°ä¼šå¸¦æ¥é¢å¤–æˆæœ¬ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡æ‰‹åŠ¨å¹²æ‰°è§†è§‰æˆ–æŒ‡ä»¤åŸå§‹è¾“å…¥æ¥è¯±å¯¼å’Œç¼“è§£hallucinationï¼Œä½†è¿™ç§æ–¹æ³•ä¾èµ–äºç»éªŒæ€§çš„æ•´ä½“è¾“å…¥å¹²æ‰°å¹¶ä¸”ä½¿æ¨ç†æˆæœ¬åŠ å€ã€‚ä¸ºé¿å…è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåä¸ºè‡ªæˆ‘å†…çœè§£ç ï¼ˆSIDï¼‰ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒçš„LVLMså¯ä»¥æ ¹æ®å…ˆå‰çš„è§†è§‰å’Œæ–‡æœ¬ï¼ˆæŒ‡ä»¤å’Œç”Ÿæˆæ–‡æœ¬ï¼‰ä»¤ç‰Œè¿›è¡Œè‡ªæˆ‘å†…çœè¯„ä¼°è§†è§‰ä»¤ç‰Œçš„é‡è¦æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸Šä¸‹æ–‡å’Œæ–‡æœ¬æ„ŸçŸ¥ä»¤ç‰Œé€‰æ‹©ï¼ˆCT2Sï¼‰ç­–ç•¥ï¼Œåœ¨LVLMsçš„æ—©æœŸå±‚æ¬¡ä¸­ä¿ç•™ä»…ä¸é‡è¦çš„è§†è§‰ä»¤ç‰Œï¼Œä»¥åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°æ”¾å¤§æ–‡æœ¬ä¿¡æ¯å¼•èµ·çš„hallucinationã€‚è¿™ç¡®ä¿äº†æ—©æœŸå±‚æ¬¡ä¸­å¸æ”¶çš„å¤šæ¨¡æ€çŸ¥è¯†ä¼šäº§ç”Ÿå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯æ— ç›®æ ‡çš„hallucinationã€‚éšåï¼ŒåŸå§‹ä»¤ç‰Œå¯¹æ•°å‡å»æ”¾å¤§çš„è§†è§‰å’Œæ–‡æœ¬å…³è”hallucinationï¼Œå¼•å¯¼LVLMså¿ å®è§£ç ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSIDç”Ÿæˆçš„æ–‡æœ¬å¹»è§‰æ›´å°‘ã€è´¨é‡æ›´é«˜ï¼Œæ— éœ€é¢å¤–çŸ¥è¯†å’Œå¤§é‡é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Vision-Language Models (LVLMs) é¢ä¸´ <code>hallucination</code> é—®é¢˜ï¼Œé˜»ç¢å…¶å®é™…éƒ¨ç½²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡é¢å¤–çŸ¥è¯†æˆ–å¯¹æ¯”è§£ç æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œä½†ä¼šå¸¦æ¥é¢å¤–æˆæœ¬æˆ–å¢åŠ æ¨ç†æˆæœ¬ã€‚</li>
<li>æœ¬æ–‡æå‡ºè‡ªæˆ‘å†…çœè§£ç ï¼ˆSIDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨LVLMsè‡ªèº«çš„èƒ½åŠ›æ¥è¯„ä¼°è§†è§‰ä»¤ç‰Œçš„é‡è¦æ€§ã€‚</li>
<li>CT2Sç­–ç•¥åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­è‡ªé€‚åº”æ”¾å¤§æ–‡æœ¬ä¿¡æ¯å¼•èµ·çš„hallucinationï¼Œæé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚</li>
<li>SIDæ–¹æ³•é€šè¿‡å‡å°‘hallucinationå’Œæé«˜æ–‡æœ¬è´¨é‡ï¼Œæ— éœ€é¢å¤–çŸ¥è¯†å’Œå¤§é‡é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒSIDæ–¹æ³•åœ¨å„ç§æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f5a2072d5c6d6864daf3f768ed8b4b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc31f817bc76b9d05621e862ac1e3240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef2ed29392612ab5110a7a55b8aef584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0d21977fac9a3a57d93009ea0c9fd44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d28bbbcd137b50faf984dbb8992bd4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7813575ef45ae11ea53a890f6fb2e8d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="3D-MolT5-Leveraging-Discrete-Structural-Information-for-Molecule-Text-Modeling"><a href="#3D-MolT5-Leveraging-Discrete-Structural-Information-for-Molecule-Text-Modeling" class="headerlink" title="3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text   Modeling"></a>3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text   Modeling</h2><p><strong>Authors:Qizhi Pei, Rui Yan, Kaiyuan Gao, Jinhua Zhu, Lijun Wu</strong></p>
<p>The integration of molecular and natural language representations has emerged as a focal point in molecular science, with recent advancements in Language Models (LMs) demonstrating significant potential for comprehensive modeling of both domains. However, existing approaches face notable limitations, particularly in their neglect of three-dimensional (3D) information, which is crucial for understanding molecular structures and functions. While some efforts have been made to incorporate 3D molecular information into LMs using external structure encoding modules, significant difficulties remain, such as insufficient interaction across modalities in pre-training and challenges in modality alignment. To address the limitations, we propose \textbf{3D-MolT5}, a unified framework designed to model molecule in both sequence and 3D structure spaces. The key innovation of our approach lies in mapping fine-grained 3D substructure representations into a specialized 3D token vocabulary. This methodology facilitates the seamless integration of sequence and structure representations in a tokenized format, enabling 3D-MolT5 to encode molecular sequences, molecular structures, and text sequences within a unified architecture. Leveraging this tokenized input strategy, we build a foundation model that unifies the sequence and structure data formats. We then conduct joint pre-training with multi-task objectives to enhance the modelâ€™s comprehension of these diverse modalities within a shared representation space. Thus, our approach significantly improves cross-modal interaction and alignment, addressing key challenges in previous work. Further instruction tuning demonstrated that our 3D-MolT5 has strong generalization ability and surpasses existing methods with superior performance in multiple downstream tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/QizhiPei/3D-MolT5">https://github.com/QizhiPei/3D-MolT5</a>. </p>
<blockquote>
<p>åˆ†å­å’Œè‡ªç„¶è¯­è¨€è¡¨ç¤ºçš„èåˆå·²æˆä¸ºåˆ†å­ç§‘å­¦çš„ç„¦ç‚¹ã€‚æœ€è¿‘çš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªé¢†åŸŸéƒ½æœ‰å·¨å¤§çš„ç»¼åˆå»ºæ¨¡æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿½è§†ä¸‰ç»´ï¼ˆ3Dï¼‰ä¿¡æ¯æ–¹é¢ï¼Œè¿™å¯¹äºç†è§£åˆ†å­ç»“æ„å’ŒåŠŸèƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶æœ‰ä¸€äº›åŠªåŠ›å°è¯•ä½¿ç”¨å¤–éƒ¨ç»“æ„ç¼–ç æ¨¡å—å°†3Dåˆ†å­ä¿¡æ¯èå…¥è¯­è¨€æ¨¡å‹ï¼Œä½†ä»å­˜åœ¨è¯¸å¤šå›°éš¾ï¼Œå¦‚é¢„è®­ç»ƒä¸­çš„è·¨æ¨¡æ€äº¤äº’ä¸è¶³å’Œæ¨¡æ€å¯¹é½çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05797v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ†å­å’Œè‡ªç„¶è¯­è¨€è¡¨ç¤ºèåˆçš„ç ”ç©¶ç°çŠ¶åŠå…¶é‡è¦æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½è§†ä¸‰ç»´ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ç»Ÿä¸€çš„æ¡†æ¶3D-MolT5ï¼Œæ—¨åœ¨å»ºæ¨¡åˆ†å­çš„åºåˆ—å’Œä¸‰ç»´ç»“æ„ç©ºé—´ã€‚é€šè¿‡ç»†ç²’åº¦çš„ä¸‰ç»´å­ç»“æ„è¡¨ç¤ºæ˜ å°„åˆ°ä¸“é—¨çš„3Dä»¤ç‰Œè¯æ±‡è¡¨ï¼Œå®ç°äº†åºåˆ—å’Œç»“æ„è¡¨ç¤ºçš„æ— ç¼é›†æˆã€‚è¯¥æ¡†æ¶è¿›è¡Œè”åˆé¢„è®­ç»ƒï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†å­å’Œè‡ªç„¶è¯­è¨€è¡¨ç¤ºèåˆæ˜¯å½“å‰åˆ†å­ç§‘å­¦çš„ç„¦ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä¸‰ç»´ä¿¡æ¯ï¼Œè¿™æ˜¯ç†è§£åˆ†å­ç»“æ„å’ŒåŠŸèƒ½çš„å…³é”®ã€‚</li>
<li>æå‡ºçš„3D-MolT5æ¡†æ¶æ—¨åœ¨å»ºæ¨¡åˆ†å­çš„åºåˆ—å’Œä¸‰ç»´ç»“æ„ç©ºé—´ã€‚</li>
<li>é€šè¿‡æ˜ å°„ç»†ç²’åº¦çš„ä¸‰ç»´å­ç»“æ„åˆ°ä¸“é—¨çš„3Dä»¤ç‰Œè¯æ±‡è¡¨ï¼Œå®ç°äº†åºåˆ—å’Œç»“æ„çš„æ— ç¼é›†æˆã€‚</li>
<li>æ¡†æ¶è¿›è¡Œè”åˆé¢„è®­ç»ƒï¼Œæé«˜äº†è·¨æ¨¡æ€äº¤äº’å’Œå¯¹é½èƒ½åŠ›ã€‚</li>
<li>3D-MolT5åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-95b2c5cae75f3c8c7bee2c937d247270.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a06f81041b6bdf8060c9028d80d2c28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27af0c113a21a582470ff8fd7dbafb21.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TuBA-Cross-Lingual-Transferability-of-Backdoor-Attacks-in-LLMs-with-Instruction-Tuning"><a href="#TuBA-Cross-Lingual-Transferability-of-Backdoor-Attacks-in-LLMs-with-Instruction-Tuning" class="headerlink" title="TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with   Instruction Tuning"></a>TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with   Instruction Tuning</h2><p><strong>Authors:Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn</strong></p>
<p>The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. Despite the increasing support for multilingual capabilities in open-source and proprietary LLMs, the impact of backdoor attacks on these systems remains largely under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data for one or two languages can affect the outputs for languages whose instruction-tuning data were not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with high attack success rates, surpassing 90% in more than 7 out of 12 languages across various scenarios. Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High Transferability: the backdoor mechanism operates successfully in cross-lingual response scenarios across 26 languages, achieving an average attack success rate of 99%, and 2) Robustness: the proposed attack remains effective even after defenses are applied. These findings expose critical security vulnerabilities in multilingual LLMs and highlight the urgent need for more robust, targeted defense strategies to address the unique challenges posed by cross-lingual backdoor transfer. </p>
<blockquote>
<p>åé—¨æ”»å‡»å¯¹ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å½±å“å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶â€”â€”è¿™ç§æ”»å‡»å¯ä»¥é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥æ¶æ„è¡Œä¸ºå¹¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è§¦å‘æ¶æ„è¾“å‡ºã€‚å°½ç®¡å¼€æºå’Œä¸“æœ‰LLMå¯¹å¤šè¯­è¨€èƒ½åŠ›çš„æ”¯æŒä¸æ–­å¢åŠ ï¼Œä½†åé—¨æ”»å‡»å¯¹è¿™äº›ç³»ç»Ÿçš„å½±å“ä»åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†ã€‚æˆ‘ä»¬çš„ç ”ç©¶èšç„¦äºé’ˆå¯¹å¤šè¯­è¨€LLMçš„è·¨è¯­è¨€åé—¨æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯ç ”ç©¶å¯¹ä¸€ç§æˆ–ä¸¤ç§è¯­è¨€çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®è¿›è¡Œæ¯’å®³å¦‚ä½•å½±å“é‚£äº›å…¶æŒ‡ä»¤è°ƒæ•´æ•°æ®æœªè¢«æ¯’å®³çš„è¯­è¨€çš„è¾“å‡ºã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼Œä½†æˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨mT5å’ŒGPT-4oç­‰æ¨¡å‹ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œæ”»å‡»æˆåŠŸç‡é«˜ï¼Œåœ¨12ç§è¯­è¨€çš„7ç§ä»¥ä¸Šåœºæ™¯ä¸­è¶…è¿‡90%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿˜è¡¨æ˜ï¼Œæ›´å¼ºå¤§çš„æ¨¡å‹æ›´å®¹æ˜“å—åˆ°å¯è½¬ç§»è·¨è¯­è¨€åé—¨æ”»å‡»çš„å½±å“ï¼Œè¿™ä¹Ÿé€‚ç”¨äºä¸»è¦é¢„è®­ç»ƒäºè‹±è¯­æ•°æ®çš„LLMï¼Œå¦‚Llama2ã€Llama3å’ŒGemmaã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜1ï¼‰é«˜å¯è½¬ç§»æ€§ï¼šåé—¨æœºåˆ¶åœ¨è·¨è¯­è¨€çš„å“åº”åœºæ™¯ï¼ˆæ¶µç›–26ç§è¯­è¨€ï¼‰ä¸­æˆåŠŸè¿è¡Œï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾99%ï¼Œä»¥åŠ2ï¼‰ç¨³å¥æ€§ï¼šå³ä½¿åº”ç”¨äº†é˜²å¾¡æªæ–½ï¼Œæ‰€æå‡ºçš„æ”»å‡»ä»ç„¶æœ‰æ•ˆã€‚è¿™äº›å‘ç°æš´éœ²äº†å¤šè¯­è¨€LLMä¸­çš„å…³é”®å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†è¿«åˆ‡éœ€è¦æ›´å¼ºå¤§ã€æœ‰é’ˆå¯¹æ€§çš„é˜²å¾¡ç­–ç•¥æ¥è§£å†³è·¨è¯­è¨€åé—¨è½¬ç§»æ‰€å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19597v3">PDF</a> work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é­å—è·¨è¯­è¨€åé—¨æ”»å‡»çš„å½±å“ç ”ç©¶æŒ‡å‡ºï¼Œæ”»å‡»è€…é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥æ¶æ„è¡Œä¸ºï¼Œå¹¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è§¦å‘æ¶æ„è¾“å‡ºï¼Œå®ç°å¯¹LLMçš„åé—¨æ”»å‡»ã€‚ç ”ç©¶èšç„¦äºè·¨è¯­è¨€åé—¨æ”»å‡»å¯¹å¤šè¯­è¨€LLMçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯æ¢ç©¶å¯¹ä¸€ç§æˆ–ä¸¤ç§è¯­è¨€çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸­çš„æ¯’åŒ–å¦‚ä½•å½±å“å…¶ä»–æœªå—æ¯’åŒ–çš„è¯­è¨€çš„è¾“å‡ºã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨mT5å’ŒGPT-4oç­‰æ¨¡å‹ä¸­å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œåœ¨è¶…è¿‡7ç§è¯­è¨€çš„åœºæ™¯ä¸­ï¼Œæ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ã€‚æ›´å¼ºå¤§çš„æ¨¡å‹æ›´æ˜“å—åˆ°è·¨è¯­è¨€åé—¨æ”»å‡»çš„å½±å“ï¼Œè¿™ä¹Ÿé€‚ç”¨äºä»¥è‹±è¯­æ•°æ®ä¸ºä¸»è¿›è¡Œé¢„è®­ç»ƒçš„LLMï¼Œå¦‚Llama2ã€Llama3å’ŒGemmaã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥åé—¨æœºåˆ¶åœ¨è·¨è¯­è¨€å“åº”åœºæ™¯ä¸­æˆåŠŸè¿è¡Œäº†26ç§è¯­è¨€ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾99%ï¼Œä¸”åœ¨åº”ç”¨é˜²å¾¡æªæ–½åä¾ç„¶æœ‰æ•ˆã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å¤šè¯­è¨€LLMçš„å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§ã€æœ‰é’ˆå¯¹æ€§çš„é˜²å¾¡ç­–ç•¥æ¥åº”å¯¹è·¨è¯­è¨€åé—¨æ”»å‡»çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åé—¨æ”»å‡»å¯¹å¤šè¯­è¨€LLMå…·æœ‰æ˜¾è‘—å½±å“ï¼Œå¯é€šè¿‡åµŒå…¥æ¶æ„è¡Œä¸ºå¹¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è§¦å‘æ¶æ„è¾“å‡ºã€‚</li>
<li>è·¨è¯­è¨€åé—¨æ”»å‡»é’ˆå¯¹å¤šè¯­è¨€LLMçš„æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
<li>åœ¨è¶…è¿‡7ç§è¯­è¨€çš„åœºæ™¯ä¸­ï¼Œæ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ï¼Œæ˜¾ç¤ºå‡ºé«˜è½¬ç§»æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>æ›´å¼ºå¤§çš„æ¨¡å‹æ›´æ˜“å—åˆ°è·¨è¯­è¨€åé—¨æ”»å‡»çš„å½±å“ã€‚</li>
<li>è·¨è¯­è¨€åé—¨æ”»å‡»ä¹Ÿé€‚ç”¨äºä»¥è‹±è¯­æ•°æ®ä¸ºä¸»é¢„è®­ç»ƒçš„LLMã€‚</li>
<li>åé—¨æœºåˆ¶åœ¨è·¨è¯­è¨€å“åº”åœºæ™¯ä¸­æˆåŠŸè¿è¡Œäº†å¤šç§è¯­è¨€ï¼Œæ˜¾ç¤ºå…¶å¹¿æ³›æ€§å’Œå¨èƒæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.19597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f510f7038efef6affbe696b20b3979e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-766292604c3209fc62246c2fbaa35dd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c676eceef8fdb3f03c1fcafad2970992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c56a1cbfb7ceff7a275266fa57ce827e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-SR-Scientific-Equation-Discovery-via-Programming-with-Large-Language-Models"><a href="#LLM-SR-Scientific-Equation-Discovery-via-Programming-with-Large-Language-Models" class="headerlink" title="LLM-SR: Scientific Equation Discovery via Programming with Large   Language Models"></a>LLM-SR: Scientific Equation Discovery via Programming with Large   Language Models</h2><p><strong>Authors:Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy</strong></p>
<p>Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMsâ€™ scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SRâ€™s incorporation of scientific priors enables more efficient equation space exploration than the baselines. Code and data are available: <a target="_blank" rel="noopener" href="https://github.com/deep-symbolic-mathematics/LLM-SR">https://github.com/deep-symbolic-mathematics/LLM-SR</a> </p>
<blockquote>
<p>æ•°å­¦æ–¹ç¨‹åœ¨æè¿°è·¨å¤šä¸ªå­¦ç§‘çš„å¤æ‚è‡ªç„¶ç°è±¡æ–¹é¢è¡¨ç°å‡ºäº†æå…¶æœ‰æ•ˆçš„æ•ˆæœã€‚ç„¶è€Œï¼Œä»æ•°æ®ä¸­å‘æ˜è¿™äº›æ·±åˆ»çš„æ–¹ç¨‹å´é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦é¢å¯¹æå…¶åºå¤§çš„ç»„åˆå‡è®¾ç©ºé—´ã€‚ç›®å‰å¸¸ç”¨çš„æ–¹ç¨‹å‘ç°æ–¹æ³•ï¼Œè¢«ç§°ä¸ºç¬¦å·å›å½’æŠ€æœ¯ï¼Œä¸»è¦ä¾§é‡äºä»æ•°æ®ä¸­æå–æ–¹ç¨‹ï¼Œå¾€å¾€å¿½è§†äº†ç§‘å­¦å®¶é€šå¸¸ä¾èµ–çš„é¢†åŸŸç‰¹å®šå…ˆéªŒçŸ¥è¯†ã€‚å®ƒä»¬è¿˜é‡‡ç”¨æœ‰é™çš„è¡¨ç¤ºå½¢å¼ï¼Œå¦‚è¡¨è¾¾å¼æ ‘ï¼Œé™åˆ¶äº†æ–¹ç¨‹çš„æœç´¢ç©ºé—´å’Œè¡¨ç°åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€é¸¿æ²Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†LLM-SRï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸°å¯Œç§‘å­¦çŸ¥è¯†å’Œå¼ºå¤§çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ä»æ•°æ®ä¸­å‘æ˜ç§‘å­¦æ–¹ç¨‹çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒLLM-SRå°†æ–¹ç¨‹è§†ä¸ºå¸¦æœ‰æ•°å­¦è¿ç®—ç¬¦çš„ç¨‹åºï¼Œå¹¶å°†LLMçš„ç§‘å­¦å…ˆéªŒçŸ¥è¯†ä¸æ–¹ç¨‹ç¨‹åºçš„è¿›åŒ–æœç´¢ç›¸ç»“åˆã€‚LLMä¼šä»å…¶é¢†åŸŸçŸ¥è¯†ä¸­ä¸æ–­æå‡ºæ–°çš„æ–¹ç¨‹éª¨æ¶å‡è®¾ï¼Œç„¶åé’ˆå¯¹æ•°æ®è¿›è¡Œä¼˜åŒ–ä»¥ä¼°è®¡å‚æ•°ã€‚æˆ‘ä»¬åœ¨ç²¾å¿ƒè®¾è®¡çš„å››ä¸ªåŸºå‡†é—®é¢˜ä¸Šè¯„ä¼°äº†LLM-SRï¼Œè¿™äº›é—®é¢˜æ¶µç›–äº†å¤šä¸ªç§‘å­¦é¢†åŸŸï¼ˆå¦‚ç‰©ç†å­¦ã€ç”Ÿç‰©å­¦ç­‰ï¼‰ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå‘ç°è¿‡ç¨‹å¹¶é˜²æ­¢LLMé‡å¤ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLM-SRå‘ç°çš„ç‰©ç†å‡†ç¡®çš„æ–¹ç¨‹æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ç¬¦å·å›å½’åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨åŸŸæµ‹è¯•ç¯å¢ƒä¸­ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒLLM-SRèå…¥çš„ç§‘å­¦å…ˆéªŒçŸ¥è¯†ä½¿å¾—æ–¹ç¨‹ç©ºé—´çš„æ¢ç´¢æ¯”åŸºçº¿æ›´åŠ é«˜æ•ˆã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/deep-symbolic-mathematics/LLM-SR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/deep-symbolic-mathematics/LLM-SRæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18400v3">PDF</a> ICLR 2025 Oral</p>
<p><strong>Summary</strong>ï¼š</p>
<p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„ç§‘å­¦çŸ¥è¯†å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¬¦å·å›å½’æ–¹æ³•LLM-SRï¼Œç”¨äºä»æ•°æ®ä¸­å‘æ˜ç§‘å­¦æ–¹ç¨‹ã€‚è¯¥æ–¹æ³•ç»“åˆLLMçš„ç§‘å­¦å…ˆéªŒçŸ¥è¯†å’Œè¿›åŒ–æœç´¢ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£æå‡ºæ–°çš„æ–¹ç¨‹éª¨æ¶å‡è®¾ï¼Œå¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–ä»¥åŒ¹é…æ•°æ®å‚æ•°ã€‚åœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLLM-SRæ˜¾ç¤ºå‡ºæ¯”ä¼ ç»Ÿç¬¦å·å›å½’æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯èƒ½åœ¨è·¨é¢†åŸŸæµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°å‡ºè¾ƒå¼ºçš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼ŒLLM-SRçš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€å…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLM-SRåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘æ˜ç§‘å­¦æ–¹ç¨‹ï¼Œç»“åˆäº†ç§‘å­¦å…ˆéªŒçŸ¥è¯†å’Œè¿›åŒ–æœç´¢ç®—æ³•ã€‚</li>
<li>LLM-SRèƒ½å¤Ÿä»æ•°æ®ä¸­æå‡ºæ–°çš„æ–¹ç¨‹éª¨æ¶å‡è®¾å¹¶è¿›è¡Œä¼˜åŒ–ä»¥åŒ¹é…æ•°æ®å‚æ•°ã€‚</li>
<li>LLM-SRåœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨é¢†åŸŸæµ‹è¯•ç¯å¢ƒä¸­ã€‚</li>
<li>LLM-SRçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ç¬¦å·å›å½’æ–¹æ³•ã€‚</li>
<li>LLM-SRç»“åˆäº†é¢†åŸŸç›¸å…³çš„ç§‘å­¦çŸ¥è¯†ï¼Œä½¿å¾—æ–¹ç¨‹ç©ºé—´çš„æ¢ç´¢æ›´åŠ é«˜æ•ˆã€‚</li>
<li>LLM-SRçš„ä»£ç å’Œæ•°æ®å·²ç»å…¬å¼€å…±äº«ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ec439d2a4b337ce99c033f8da2848d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-720358f7ce331968c52d86c0a3f6e2c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f86ece1a33b5ca2fd11c77bccb5b4ba.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Chain-of-Evidences-and-Evidence-to-Generate-Prompting-for-Context-Grounded-and-Retrieval-Augmented-Reasoning"><a href="#Chain-of-Evidences-and-Evidence-to-Generate-Prompting-for-Context-Grounded-and-Retrieval-Augmented-Reasoning" class="headerlink" title="Chain of Evidences and Evidence to Generate: Prompting for Context   Grounded and Retrieval Augmented Reasoning"></a>Chain of Evidences and Evidence to Generate: Prompting for Context   Grounded and Retrieval Augmented Reasoning</h2><p><strong>Authors:Md Rizwan Parvez</strong></p>
<p>While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,) suffer from limitations like limited context grounding, hallucination&#x2F;inconsistent output generation, and iterative sluggishness. To overcome these challenges, we introduce a novel mono&#x2F;dual-step zero-shot prompting framework built upon two unique strategies Chain of Evidences (CoE)} and Evidence to Generate (E2G). Instead of unverified reasoning claims, our innovative approaches leverage the power of â€œevidence for decision makingâ€ by first focusing exclusively on the thought sequences explicitly mentioned in the context which then serve as extracted evidence, guiding the LLMâ€™s output generation process with greater precision and efficiency. This simple yet potent approach unlocks the full potential of chain-of-thoughts prompting, facilitating faster, more reliable, and contextually aware reasoning in LLMs. Our framework consistently achieves remarkable results across various knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%, surpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, achieving an F1 score of 83.3 on DROP. We release our prompts and outputs on these benchmarks as a new instruction tuning dataset for future research at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/">https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/</a>. </p>
<blockquote>
<p>è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå·²ç»å½»åº•æ”¹å˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ–¹å¼ï¼Œä½†å…¶å½“å‰çš„æ–¹æ³•å’Œå˜ä½“ï¼ˆä¾‹å¦‚è‡ªæ´½æ€§ã€ReACTã€åå°„ã€æ€ç»´æ ‘ï¼ˆToTï¼‰ã€ç´¯ç§¯æ¨ç†ï¼ˆCRï¼‰ç­‰ï¼‰ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚ä¸Šä¸‹æ–‡å®šä½æœ‰é™ã€å¹»è§‰&#x2F;è¾“å‡ºç”Ÿæˆä¸ä¸€è‡´å’Œè¿­ä»£ç¼“æ…¢ç­‰ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å•&#x2F;åŒæ­¥é›¶å°„å‡»æç¤ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å»ºç«‹åœ¨ä¸¤ç§ç‹¬ç‰¹ç­–ç•¥â€œChain of Evidences (CoE)â€å’Œâ€œEvidence to Generate (E2G)â€ä¹‹ä¸Šã€‚ä¸ä¼ ç»Ÿçš„æœªç»è¯å®çš„æ¨ç†å£°æ˜ä¸åŒï¼Œæˆ‘ä»¬çš„åˆ›æ–°æ–¹æ³•åˆ©ç”¨â€œè¯æ®å†³ç­–â€çš„åŠ›é‡ï¼Œé¦–å…ˆä¸“æ³¨äºä¸Šä¸‹æ–‡ä¸­æ˜ç¡®æåˆ°çš„æ€ç»´åºåˆ—ï¼Œå°†å…¶ä½œä¸ºæå–çš„è¯æ®ï¼Œä»¥æ›´é«˜çš„ç²¾åº¦å’Œæ•ˆç‡å¼•å¯¼LLMçš„è¾“å‡ºç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§ç®€å•è€Œå¼ºå¤§çš„æ–¹æ³•é‡Šæ”¾äº†é“¾å¼æ€ç»´æç¤ºçš„æ½œåŠ›ï¼Œå®ç°æ›´å¿«ã€æ›´å¯é ã€æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†çš„LLMæ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§çŸ¥è¯†å¯†é›†å‹æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸ŠæŒç»­å–å¾—æ˜¾è‘—æˆæœï¼Œè¶…è¶Šäº†ä½¿ç”¨æœ€æ–°LLMçš„åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œï¼ˆiï¼‰åœ¨LogiQAåŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨GPT-4ï¼ŒCoEè¾¾åˆ°äº†53.8%çš„æ–°å‡†ç¡®ç‡ï¼Œæ¯”CoTé«˜å‡º18%ï¼Œæ¯”ToTé«˜å‡º11%ï¼Œæ¯”CRé«˜å‡º9%ï¼›ï¼ˆiiï¼‰CoEä¸PaLM-2ç»“åˆä½¿ç”¨ï¼Œåœ¨DROPä¸Šè¶…è¶Šäº†Gemini Ultraçš„å¯å˜å°„å‡»æ€§èƒ½0.9 F1ç‚¹ï¼Œè¾¾åˆ°äº†83.3çš„F1åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/">https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/</a>ä¸Šå‘å¸ƒäº†è¿™äº›åŸºå‡†æµ‹è¯•ä¸­çš„æç¤ºå’Œè¾“å‡ºï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.05787v2">PDF</a> Accepted at NAACL KnowledgeNLP 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå˜é©äº†LLMæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ–¹å¼çš„åŒæ—¶ï¼Œå…¶ç°æœ‰æ–¹æ³•å’Œå˜ç§å­˜åœ¨å¦‚ä¸Šä¸‹æ–‡å…³è”æœ‰é™ã€è¾“å‡ºç”Ÿæˆä¸ä¸€è‡´å’Œè¿­ä»£ç¼“æ…¢ç­‰å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºChain of Evidencesï¼ˆCoEï¼‰å’ŒEvidence to Generateï¼ˆE2Gï¼‰ä¸¤ç§ç‹¬ç‰¹ç­–ç•¥çš„å•æ­¥æˆ–åŒæ­¥é›¶æ ·æœ¬æç¤ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“æ³¨äºä¸Šä¸‹æ–‡ä¸­çš„æ˜ç¡®æ€ç»´åºåˆ—ä½œä¸ºè¯æ®ï¼Œåˆ©ç”¨è¯æ®è¿›è¡Œå†³ç­–ï¼Œä»è€Œæé«˜äº†LLMè¾“å‡ºç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚åœ¨å¤šç§çŸ¥è¯†å¯†é›†å‹æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¶…è¶Šäº†ä½¿ç”¨æœ€æ–°LLMçš„åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨LogiQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-4çš„CoEè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„å‡†ç¡®ç‡53.8%ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚æˆ‘ä»¬åœ¨å…¬å¼€æ•°æ®é›†ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„æç¤ºå’Œè¾“å‡ºï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶å‚è€ƒã€‚æ›´å¤šè¯¦æƒ…å‚è§ï¼š[é“¾æ¥åœ°å€]ã€‚ç®€åŒ–æ€»ç»“ä¸ºï¼šæ–°å‹æ¡†æ¶è§£é”LLMæ¨ç†æ½œåŠ›ï¼Œé«˜æ•ˆå‡†ç¡®ç”Ÿæˆæ¨ç†è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå·²å¯¹LLMæ¨ç†ä»»åŠ¡äº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„CoTæ–¹æ³•å’Œå˜ç§å­˜åœ¨ä¸Šä¸‹æ–‡å…³è”æœ‰é™ç­‰å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºChain of Evidencesï¼ˆCoEï¼‰å’ŒEvidence to Generateï¼ˆE2Gï¼‰ç­–ç•¥çš„æ¡†æ¶æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è¯æ®è¿›è¡Œå†³ç­–ï¼Œæé«˜äº†LLMè¾“å‡ºç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.05787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa7fbfcd63d0f689d1419bb958b3c351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-645f8b56ad9619936f1d92eb6ff7bed4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc953d050b632792de9e202e41e08311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c283d9f464cfa3567d6a5c6522ff7ab5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e728f40d07944806c318e5167df031fe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GPT4RoI-Instruction-Tuning-Large-Language-Model-on-Region-of-Interest"><a href="#GPT4RoI-Instruction-Tuning-Large-Language-Model-on-Region-of-Interest" class="headerlink" title="GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"></a>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</h2><p><strong>Authors:Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo</strong></p>
<p>Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code and model can be found at <a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>. </p>
<blockquote>
<p>å¯¹å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè§†è§‰æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å…·å¤‡äº†é€šç”¨è§†è§‰è¯­è¨€èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¼ºå°‘åŒºåŸŸæ–‡æœ¬å¯¹é™åˆ¶äº†å…¶åœ¨ç²¾ç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç©ºé—´æŒ‡ä»¤è°ƒæ•´ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤ä¸­å¼•å…¥äº†æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰çš„å¼•ç”¨ã€‚åœ¨å‘é€åˆ°LLMä¹‹å‰ï¼Œè¯¥å¼•ç”¨è¢«æ›¿æ¢ä¸ºRoIç‰¹å¾ï¼Œå¹¶ä¸è¯­è¨€åµŒå…¥äº¤æ›¿ä½œä¸ºåºåˆ—ã€‚æˆ‘ä»¬çš„GPT4RoIæ¨¡å‹åœ¨7ä¸ªåŒºåŸŸæ–‡æœ¬å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ä¹‹å‰çš„å›¾åƒçº§åˆ«æ¨¡å‹ç›¸æ¯”ï¼Œå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å’Œå¯¹è¯ä½“éªŒã€‚ï¼ˆ1ï¼‰è¶…è¶Šè¯­è¨€çš„äº¤äº’ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†æ¥çµæ´»åœ°è°ƒæ•´å¼•ç”¨ç²’åº¦ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚ï¼ˆ2ï¼‰å¤šåŠŸèƒ½å¤šæ¨¡æ€èƒ½åŠ›ï¼šGPT4RoIå¯ä»¥æŒ–æ˜æ¯ä¸ªRoIå†…çš„å„ç§å±æ€§ä¿¡æ¯ï¼Œä¾‹å¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥åŸºäºå¸¸è¯†å¯¹å¤šä¸ªRoIè¿›è¡Œæ¨ç†ã€‚åœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šï¼ŒGPT4RoIçš„å‡†ç¡®ç‡è¾¾åˆ°äº†81.6%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ˆç¬¬äºŒåæ˜¯75.6%ï¼‰ï¼Œå‡ ä¹è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½85.0%ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jshilong/GPT4RoIæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03601v4">PDF</a> Code has been released at <a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„èƒ½åŠ›å·²è¾¾æˆé€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åŒºåŸŸæ–‡æœ¬å¯¹ï¼Œé™åˆ¶äº†å…¶å¯¹ç²¾ç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£çš„è¿›æ­¥ã€‚æœ¬æ–‡æå‡ºç©ºé—´æŒ‡ä»¤è°ƒæ•´ï¼Œå¼•å…¥æŒ‡ä»¤ä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰å‚è€ƒã€‚åœ¨å‘é€åˆ°LLMä¹‹å‰ï¼Œå‚è€ƒè¢«æ›¿æ¢ä¸ºRoIç‰¹å¾ï¼Œå¹¶ä¸è¯­è¨€åµŒå…¥äº¤é”™ä½œä¸ºåºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹GPT4RoIï¼Œåœ¨7ä¸ªåŒºåŸŸæ–‡æœ¬å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ä¹‹å‰çš„å›¾åƒçº§åˆ«æ¨¡å‹ç›¸æ¯”ï¼Œå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å’Œå¯¹è¯ä½“éªŒã€‚ï¼ˆ1ï¼‰è¶…è¶Šè¯­è¨€çš„äº¤äº’ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œçµæ´»åœ°è°ƒæ•´å‚è€ƒç²’åº¦ã€‚ï¼ˆ2ï¼‰å¤šåŠŸèƒ½å¤šæ¨¡æ€èƒ½åŠ›ï¼šGPT4RoIå¯ä»¥æŒ–æ˜æ¯ä¸ªRoIå†…çš„å„ç§å±æ€§ä¿¡æ¯ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥æ ¹æ®å¸¸è¯†æ¨ç†å¤šä¸ªRoIã€‚åœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šï¼ŒGPT4RoIå–å¾—äº†81.6%çš„æ˜¾è‘—å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ˆç¬¬äºŒåæ˜¯75.6%ï¼‰ï¼Œå‡ ä¹è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½85.0%ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç©ºé—´æŒ‡ä»¤è°ƒæ•´å¼•å…¥äº†æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰çš„æ¦‚å¿µï¼Œå¢å¼ºäº†LLMå¯¹ç²¾ç»†ç²’åº¦å¤šæ¨¡æ€ä¿¡æ¯çš„ç†è§£ã€‚</li>
<li>GPT4RoIæ¨¡å‹é€šè¿‡ç»“åˆå›¾åƒåŒºåŸŸå’Œæ–‡æœ¬æŒ‡ä»¤ï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å’Œå¯¹è¯ä½“éªŒã€‚</li>
<li>GPT4RoIæ”¯æŒå¤šç§è¯­è¨€äº¤äº’æ–¹å¼ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿçµæ´»åœ°è°ƒæ•´å‚è€ƒç²’åº¦ã€‚</li>
<li>GPT4RoIèƒ½å¤ŸæŒ–æ˜å›¾åƒåŒºåŸŸçš„å¤šç§å±æ€§ä¿¡æ¯ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚</li>
<li>GPT4RoIå…·å¤‡åŸºäºå¸¸è¯†çš„å¤šä¸ªå›¾åƒåŒºåŸŸæ¨ç†èƒ½åŠ›ã€‚</li>
<li>GPT4RoIåœ¨VCRæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†81.6%ï¼Œæ˜¾è‘—è¶…è¿‡äº†ç°æœ‰æ¨¡å‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.03601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-369d747915497625651546449cc65596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b78293871534fde115f713bebc0e5fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acb11774b0754ddac426919f32bdc223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0553eb2abc86b8040b971aa1bc0a8376.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-23/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-3b1cfb85ef253ce7d5d7780d27d9d33c.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-23  I2AM Interpreting Image-to-Image Latent Diffusion Models via   Bi-Attribution Maps
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e57fb8e24e7ccce9a397d7d46dd75864.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-23  KnowLogic A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
