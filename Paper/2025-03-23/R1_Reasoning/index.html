<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-23  KnowLogic A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e57fb8e24e7ccce9a397d7d46dd75864.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-23-æ›´æ–°"><a href="#2025-03-23-æ›´æ–°" class="headerlink" title="2025-03-23 æ›´æ–°"></a>2025-03-23 æ›´æ–°</h1><h2 id="KnowLogic-A-Benchmark-for-Commonsense-Reasoning-via-Knowledge-Driven-Data-Synthesis"><a href="#KnowLogic-A-Benchmark-for-Commonsense-Reasoning-via-Knowledge-Driven-Data-Synthesis" class="headerlink" title="KnowLogic: A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis"></a>KnowLogic: A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis</h2><p><strong>Authors:Weidong Zhan, Yue Wang, Nan Hu, Liming Xiao, Jingyuan Ma, Yuhang Qin, Zheng Li, Yixin Yang, Sirui Deng, Jinkun Ding, Wenhan Ma, Rui Li, Weilin Luo, Qun Liu, Zhifang Sui</strong></p>
<p>Current evaluations of commonsense reasoning in LLMs are hindered by the scarcity of natural language corpora with structured annotations for reasoning tasks. To address this, we introduce KnowLogic, a benchmark generated through a knowledge-driven synthetic data strategy. KnowLogic integrates diverse commonsense knowledge, plausible scenarios, and various types of logical reasoning. One of the key advantages of KnowLogic is its adjustable difficulty levels, allowing for flexible control over question complexity. It also includes fine-grained labels for in-depth evaluation of LLMsâ€™ reasoning abilities across multiple dimensions. Our benchmark consists of 3,000 bilingual (Chinese and English) questions across various domains, and presents significant challenges for current LLMs, with the highest-performing model achieving only 69.57%. Our analysis highlights common errors, such as misunderstandings of low-frequency commonsense, logical inconsistencies, and overthinking. This approach, along with our benchmark, provides a valuable tool for assessing and enhancing LLMsâ€™ commonsense reasoning capabilities and can be applied to a wide range of knowledge domains. </p>
<blockquote>
<p>ç›®å‰å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸è¯†æ¨ç†èƒ½åŠ›çš„è¯„ä¼°å—åˆ°ç¼ºä¹ç»“æ„åŒ–æ³¨é‡Šçš„æ¨ç†ä»»åŠ¡è‡ªç„¶è¯­è¨€è¯­æ–™åº“çš„é˜»ç¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KnowLogicï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡çŸ¥è¯†é©±åŠ¨åˆæˆæ•°æ®ç­–ç•¥ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ã€‚KnowLogicèåˆäº†å¤šæ ·åŒ–çš„å¸¸è¯†çŸ¥è¯†ã€åˆç†åœºæ™¯å’Œå¤šç§ç±»å‹çš„é€»è¾‘æ¨ç†ã€‚KnowLogicçš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯å…¶å¯è°ƒæ•´çš„éš¾åº¦çº§åˆ«ï¼Œå¯ä»¥æ§åˆ¶é—®é¢˜çš„å¤æ‚æ€§ã€‚å®ƒè¿˜åŒ…å«ç”¨äºæ·±å…¥è¯„ä¼°LLMæ¨ç†èƒ½åŠ›å¤šä¸ªç»´åº¦çš„ç²¾ç»†æ ‡ç­¾ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«3000ä¸ªåŒè¯­ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰é—®é¢˜ï¼Œæ¶µç›–å„ä¸ªé¢†åŸŸï¼Œå¯¹å½“å‰LLMæå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹åªè¾¾åˆ°äº†69.57%ã€‚æˆ‘ä»¬çš„åˆ†æå¼ºè°ƒäº†å¸¸è§é”™è¯¯ï¼Œå¦‚è¯¯è§£ä½é¢‘å¸¸è¯†ã€é€»è¾‘ä¸ä¸€è‡´å’Œè¿‡åº¦æ€è€ƒã€‚è¿™ç§æ–¹æ³•ä»¥åŠæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œå¢å¼ºLLMçš„å¸¸è¯†æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ï¼Œå¹¶å¯åº”ç”¨äºå¹¿æ³›çš„çŸ¥è¯†é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06218v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¸¸è¯†æ¨ç†è¯„ä¼°å—é™äºç¼ºä¹ç»“æ„åŒ–æ³¨é‡Šçš„æ¨ç†ä»»åŠ¡è‡ªç„¶è¯­è¨€è¯­æ–™åº“çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KnowLogicè¿™ä¸€é€šè¿‡çŸ¥è¯†é©±åŠ¨åˆæˆæ•°æ®ç­–ç•¥ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ã€‚KnowLogicèåˆäº†å¤šæ ·åŒ–çš„å¸¸è¯†çŸ¥è¯†ã€åˆç†åœºæ™¯å’Œå„ç§ç±»å‹çš„é€»è¾‘æ¨ç†ã€‚å…¶å…³é”®ä¼˜åŠ¿ä¹‹ä¸€æ˜¯å¯è°ƒèŠ‚çš„éš¾åº¦çº§åˆ«ï¼Œå¯çµæ´»æ§åˆ¶é—®é¢˜çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«ç”¨äºæ·±å…¥è¯„ä¼°LLMså¤šæ–¹é¢æ¨ç†èƒ½åŠ›çš„ç²¾ç»†æ ‡ç­¾ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«3000ä¸ªåŒè¯­ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰é—®é¢˜ï¼Œæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼Œå¯¹å½“å‰LLMsæå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œæœ€é«˜æ€§èƒ½æ¨¡å‹ä»…è¾¾åˆ°69.57%ã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†å¸¸è§çš„é”™è¯¯ï¼Œå¦‚è¯¯è§£ä½é¢‘å¸¸è¯†ã€é€»è¾‘ä¸ä¸€è‡´å’Œè¿‡åº¦æ€è€ƒã€‚è¿™ä¸€æ–¹æ³•å’Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œå¢å¼ºLLMsçš„å¸¸è¯†æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ï¼Œå¹¶å¯åº”ç”¨äºå¹¿æ³›çš„çŸ¥è¯†é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¸¸è¯†æ¨ç†è¯„ä¼°åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å—é™ï¼Œç¼ºä¹ç»“æ„åŒ–æ³¨é‡Šçš„æ¨ç†ä»»åŠ¡è‡ªç„¶è¯­è¨€è¯­æ–™åº“æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>KnowLogicæ˜¯ä¸€ä¸ªçŸ¥è¯†é©±åŠ¨åˆæˆæ•°æ®ç­–ç•¥çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>KnowLogicèåˆäº†å¤šæ ·åŒ–çš„å¸¸è¯†çŸ¥è¯†ã€åˆç†åœºæ™¯å’Œå¤šç§é€»è¾‘æ¨ç†ç±»å‹ã€‚</li>
<li>KnowLogicå…·æœ‰å¯è°ƒèŠ‚çš„éš¾åº¦çº§åˆ«ï¼Œå…è®¸çµæ´»æ§åˆ¶é—®é¢˜çš„å¤æ‚æ€§ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…å«åŒè¯­ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰é—®é¢˜ï¼Œæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼Œå¯¹å½“å‰LLMsæå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>æœ€é«˜æ€§èƒ½æ¨¡å‹åœ¨KnowLogicä¸Šçš„è¡¨ç°ä»…ä¸º69.57%ï¼Œè¯´æ˜å­˜åœ¨æå‡ç©ºé—´ã€‚</li>
<li>åˆ†ææ­ç¤ºäº†å¸¸è§çš„é”™è¯¯ç±»å‹ï¼Œå¦‚è¯¯è§£ä½é¢‘å¸¸è¯†ã€é€»è¾‘ä¸ä¸€è‡´å’Œè¿‡åº¦æ€è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9f0f8938e6d08181d07f0841826205a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a99c604b73c49e9be6cedc9b7e29b2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-688a3e1b9cf7c7ae97fe046ac244e4eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9fcd9c755a317f07679701fce962e38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aaf0892cbad09950f85516b8d111190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ed915e4b15bca3c9f50ff53d4a863e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef275b1c224e72a4c8b8b22407c67a64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af9f4fc6522283b47a0950c6cc99a8af.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning"></a>R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚è™½ç„¶å®ƒä»¬åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–å†…éƒ¨çŸ¥è¯†æ¥è§£å†³é—®é¢˜ï¼Œè¿™åœ¨æ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜ä¸Šå¯èƒ½ä¸è¶³ï¼Œå¯¼è‡´ä¸å‡†ç¡®å’Œå¹»è§‰ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>R1-Searcher</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæˆæœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿæ¥è®¿é—®é¢å¤–çš„çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®Œå…¨ä¾èµ–äºRLï¼Œæ— éœ€å¯åŠ¨è¿‡ç¨‹å¥–åŠ±æˆ–è’¸é¦ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºå…ˆå‰çš„å¼ºå¤§RAGæ–¹æ³•ï¼Œå³ä½¿åœ¨å¯¹æ¯”é—­æºçš„GPT-4o-miniæ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05592v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚è™½ç„¶ç°æœ‰æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–å†…éƒ¨çŸ¥è¯†è§£å†³é—®é¢˜ï¼Œå¯¹äºæ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜å¯èƒ½ä¸è¶³å¤Ÿå‡†ç¡®å¹¶å‡ºç°è¯¯åˆ¤ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºR1-Searcherçš„æ–°å‹ä¸¤é˜¶æ®µç»“æœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ï¼Œå…è®¸å…¶è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿè·å–é¢å¤–çŸ¥è¯†æ¥å®Œæˆæ¨ç†è¿‡ç¨‹ã€‚æ­¤æ–¹æ³•ä»…ä¾èµ–RLï¼Œæ— éœ€è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„çŸ¥è¯†è’¸é¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…ˆå‰çš„å¼ºå¤§RAGæ–¹æ³•ï¼Œå³ä½¿ä¸é—­æºçš„GPT-4o-miniç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ‰åŠ©äºæå‡å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨æ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜ä¸Šå¯èƒ½ä¸å‡†ç¡®ã€‚</li>
<li>R1-Searcheræ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç»“æœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚</li>
<li>R1-Searcherå…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿè·å–é¢å¤–çŸ¥è¯†ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…ä¾èµ–RLï¼Œä¸éœ€è¦è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„çŸ¥è¯†è’¸é¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒR1-Searcheræ˜¾è‘—ä¼˜äºå…¶ä»–å¼ºå¤§çš„RAGæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49204a03cd030bc522af99d46101df2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3092b293dc9d717edaa73b5635744227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603d2edf70630ae009a368b957e8eadc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing"><a href="#WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing" class="headerlink" title="WritingBench: A Comprehensive Benchmark for Generative Writing"></a>WritingBench: A Comprehensive Benchmark for Generative Writing</h2><p><strong>Authors:Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, Fei Huang</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The frameworkâ€™s validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆå†™ä½œä¸­è¯„ä¼°å…¶æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™çš„å†™ä½œä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰è·¨ä¸åŒé¢†åŸŸé«˜è´¨é‡å†…å®¹çš„å¤šæ ·åŒ–è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸä¸­çš„è¡¨ç°ï¼Œæ¶µç›–åˆ›é€ æ€§ã€è¯´æœåŠ›ã€ä¿¡æ¯æ€§å’ŒæŠ€æœ¯å†™ä½œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªæŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒåçš„æ‰¹è¯„æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œä»¥é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦ä¸ºè¯„ä»·æ ‡å‡†ã€‚è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ•´åˆèƒ½åŠ›å¾—åˆ°äº†è¿›ä¸€æ­¥è¯æ˜ï¼Œè¿™å¯ä»¥ä½¿æ‹¥æœ‰æ•°åäº¿å‚æ•°çš„æ¨¡å‹è¾¾åˆ°æ¥è¿‘å½“å‰æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬å¼€æºäº†åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠè¯„ä¼°å·¥å…·å’Œæ¨¡å—åŒ–æ¡†æ¶ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å†™ä½œä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05244v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸè¿›å±•æå¤§åœ°æå‡äº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¯„ä¼°å…¶åœ¨å†™ä½œæ–¹é¢çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™å†™ä½œä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰è·¨ä¸åŒé¢†åŸŸé«˜è´¨é‡å†…å®¹çš„å¤šæ ·åŒ–è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åˆ›é€ æ€§ã€è¯´æœåŠ›ã€ä¿¡æ¯æ€§å’ŒæŠ€æœ¯å†™ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒè¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œä»¥é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦è¿›è¡Œè¯„ä¼°ã€‚æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ”¶é›†èƒ½åŠ›è¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿7Bå‚æ•°æ¨¡å‹æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬å¼€æºåŸºå‡†æµ‹è¯•ã€è¯„ä¼°å·¥å…·å’Œæ¨¡å—åŒ–æ¡†æ¶ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å†™ä½œé¢†åŸŸLLMçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°å…¶åœ¨å†™ä½œæ–¹é¢çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°LLMåœ¨ä¸åŒå†™ä½œé¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>WritingBenchæ˜¯ä¸€ä¸ªå…¨æ–°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†æŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å¾®è°ƒè¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œæ¶µç›–é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦ã€‚</li>
<li>æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ”¶é›†èƒ½åŠ›å¾—åˆ°è¯æ˜ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6412f51cc0a8703a34ab9ebdb5fb1e05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9d2679ce0909222c8237a98f10f37f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b383dbe5b18325eac07de6ef521b4099.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23cd85fb8bb759282c32fb7b4c8ffc19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e186f4c47e00c084054c23e80bde1c4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73f18a576e39f854e1b087442cab49d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc3d1baf1d6bda364f0136fad377c7d6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects"><a href="#Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects" class="headerlink" title="Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects"></a>Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects</h2><p><strong>Authors:Anichur Rahman, Shahariar Hossain Mahir, Md Tanjum An Tashrif, Airin Afroj Aishi, Md Ahsan Karim, Dipanjali Kundu, Tanoy Debnath, Md. Abul Ala Moududi, MD. Zunead Abedin Eidmum</strong></p>
<p>Nowadays, DeepSeek, ChatGPT, and Google Gemini are the most trending and exciting Large Language Model (LLM) technologies for reasoning, multimodal capabilities, and general linguistic performance worldwide. DeepSeek employs a Mixture-of-Experts (MoE) approach, activating only the parameters most relevant to the task at hand, which makes it especially effective for domain-specific work. On the other hand, ChatGPT relies on a dense transformer model enhanced through reinforcement learning from human feedback (RLHF), and then Google Gemini actually uses a multimodal transformer architecture that integrates text, code, and images into a single framework. However, by using those technologies, people can be able to mine their desired text, code, images, etc, in a cost-effective and domain-specific inference. People may choose those techniques based on the best performance. In this regard, we offer a comparative study based on the DeepSeek, ChatGPT, and Gemini techniques in this research. Initially, we focus on their methods and materials, appropriately including the data selection criteria. Then, we present state-of-the-art features of DeepSeek, ChatGPT, and Gemini based on their applications. Most importantly, we show the technological comparison among them and also cover the dataset analysis for various applications. Finally, we address extensive research areas and future potential guidance regarding LLM-based AI research for the community. </p>
<blockquote>
<p>å¦‚ä»Šï¼ŒDeepSeekã€ChatGPTå’ŒGoogle Geminiæ˜¯å…¨çƒèŒƒå›´å†…æœ€æµè¡Œå’Œä»¤äººå…´å¥‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ï¼Œå®ƒä»¬åœ¨æ¨ç†ã€å¤šæ¨¡æ€èƒ½åŠ›å’Œä¸€èˆ¬è¯­è¨€æ€§èƒ½æ–¹é¢å…·æœ‰å‡ºè‰²çš„è¡¨ç°ã€‚DeepSeeké‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ï¼Œåªæ¿€æ´»ä¸æ‰‹å¤´ä»»åŠ¡æœ€ç›¸å…³çš„å‚æ•°ï¼Œä½¿å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å·¥ä½œä¸­å°¤å…¶æœ‰æ•ˆã€‚å¦ä¸€æ–¹é¢ï¼ŒChatGPTä¾èµ–äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¢å¼ºçš„å¯†é›†å˜å‹å™¨æ¨¡å‹ï¼Œç„¶åGoogle Geminiå®é™…ä¸Šä½¿ç”¨äº†ä¸€ç§å¤šæ¨¡æ€å˜å‹å™¨æ¶æ„ï¼Œè¯¥æ¶æ„å°†æ–‡æœ¬ã€ä»£ç å’Œå›¾åƒé›†æˆåˆ°ä¸€ä¸ªå•ä¸€æ¡†æ¶ä¸­ã€‚ç„¶è€Œï¼Œé€šè¿‡åˆ©ç”¨è¿™äº›æŠ€æœ¯ï¼Œäººä»¬èƒ½å¤Ÿä»¥æˆæœ¬æ•ˆç›Šé«˜å’Œé¢†åŸŸç‰¹å®šçš„æ¨ç†æ¥æŒ–æ˜ä»–ä»¬æ‰€éœ€çš„æ–‡æœ¬ã€ä»£ç ã€å›¾åƒç­‰ã€‚äººä»¬å¯èƒ½ä¼šæ ¹æ®æœ€ä½³æ€§èƒ½é€‰æ‹©è¿™äº›æŠ€æœ¯ã€‚åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨æœ¬ç ”ç©¶ä¸­åŸºäºDeepSeekã€ChatGPTå’ŒGeminiæŠ€æœ¯æä¾›äº†ä¸€é¡¹æ¯”è¾ƒç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å…³æ³¨å®ƒä»¬çš„æ–¹æ³•å’Œææ–™ï¼ŒåŒ…æ‹¬æ•°æ®é€‰æ‹©æ ‡å‡†ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„åº”ç”¨å±•ç¤ºæœ€å‰æ²¿çš„DeepSeekã€ChatGPTå’ŒGeminiåŠŸèƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œäº†æŠ€æœ¯æ¯”è¾ƒï¼Œå¹¶å¯¹å„ç§åº”ç”¨çš„æ•°æ®é›†è¿›è¡Œäº†åˆ†æã€‚æœ€åï¼Œæˆ‘ä»¬é’ˆå¯¹ç¤¾åŒºåŸºäºLLMçš„äººå·¥æ™ºèƒ½ç ”ç©¶æå‡ºäº†å¹¿æ³›çš„ç ”ç©¶é¢†åŸŸå’Œæœªæ¥çš„æ½œåœ¨æŒ‡å¯¼æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04783v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¶‹åŠ¿æ­£å…´èµ·ã€‚ç›®å‰DeepSeekã€ChatGPTå’ŒGoogle Geminiä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯å› å…¶æ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€åŠŸèƒ½å’Œæ•´ä½“è¯­è¨€æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä¸‰è€…å„æœ‰ç‰¹è‰²ï¼ŒDeepSeeké‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡æ¿€æ´»ç›¸å…³å‚æ•°ï¼›ChatGPTåˆ™ä¾èµ–å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­æå‡èƒ½åŠ›çš„å¯†é›†è½¬æ¢å™¨æ¨¡å‹ï¼›Google Geminiä½¿ç”¨å¤šæ¨¡æ€è½¬æ¢å™¨æ¶æ„ï¼Œæ•´åˆæ–‡æœ¬ã€ä»£ç å’Œå›¾åƒäºä¸€ä½“ã€‚è¯¥æŠ€æœ¯ä½¿æ–‡æœ¬æŒ–æ˜æˆæœ¬æ•ˆç›Šæ›´é«˜ï¼Œæ›´å…·é¢†åŸŸç‰¹è‰²ã€‚æœ¬ç ”ç©¶å¯¹ä¸‰è€…è¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œå…³æ³¨æ–¹æ³•ã€ææ–™ã€åº”ç”¨åŠæ•°æ®é›†åˆ†æï¼Œæ—¨åœ¨ä¸ºç¤¾åŒºæä¾›AIç ”ç©¶çš„å‰æ²¿æŒ‡å¼•å’Œæœªæ¥å±•æœ›ã€‚<br><strong>Key Takeaways</strong> </p>
<ol>
<li>DeepSeekæŠ€æœ¯ä½¿ç”¨æ··åˆä¸“å®¶æ–¹æ³•ä»¥å®ç°å¯¹ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–å¤„ç†ã€‚ </li>
<li>ChatGPTæŠ€æœ¯ä¾èµ–äºå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡äººç±»åé¦ˆæ”¹è¿›å…¶å¯†é›†è½¬æ¢å™¨æ¨¡å‹ã€‚ </li>
<li>Google Geminiåˆ©ç”¨å¤šæ¨¡æ€è½¬æ¢å™¨æ¶æ„æ•´åˆæ–‡æœ¬ã€ä»£ç å’Œå›¾åƒä¿¡æ¯ã€‚ </li>
<li>è¿™äº›æŠ€æœ¯èƒ½æé«˜æ–‡æœ¬æŒ–æ˜çš„æˆæœ¬æ•ˆç›Šå’Œé¢†åŸŸç‰¹å¼‚æ€§ã€‚ </li>
<li>ç ”ç©¶æ¯”è¾ƒåˆ†æè¿™ä¸‰ç§æŠ€æœ¯åœ¨æ–¹æ³•ã€ææ–™å’Œåº”ç”¨ä¸Šçš„ä¼˜åŠ¿ã€‚ </li>
<li>ç ”ç©¶å…³æ³¨æ•°æ®é›†åˆ†æåœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33e2fb8acf33a58345e831f7542ecab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-350ab728234c22b4fd154c528140888c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8083d3db6885b5c85ab0a40015166c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce877eaae75452f53bacda2708200a1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-System-1-vs-2-Reasoning-Approaches-for-Zero-Shot-Time-Series-Forecasting-A-Benchmark-and-Insights"><a href="#Evaluating-System-1-vs-2-Reasoning-Approaches-for-Zero-Shot-Time-Series-Forecasting-A-Benchmark-and-Insights" class="headerlink" title="Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time Series   Forecasting: A Benchmark and Insights"></a>Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time Series   Forecasting: A Benchmark and Insights</h2><p><strong>Authors:Haoxin Liu, Zhiyuan Zhao, Shiduo Li, B. Aditya Prakash</strong></p>
<p>Reasoning ability is crucial for solving challenging tasks. With the advancement of foundation models, such as the emergence of large language models (LLMs), a wide range of reasoning strategies has been proposed, including test-time enhancements, such as Chain-ofThought, and post-training optimizations, as used in DeepSeek-R1. While these reasoning strategies have demonstrated effectiveness across various challenging language or vision tasks, their applicability and impact on time-series forecasting (TSF), particularly the challenging zero-shot TSF, remain largely unexplored. In particular, it is unclear whether zero-shot TSF benefits from reasoning and, if so, what types of reasoning strategies are most effective. To bridge this gap, we propose ReC4TS, the first benchmark that systematically evaluates the effectiveness of popular reasoning strategies when applied to zero-shot TSF tasks. ReC4TS conducts comprehensive evaluations across datasets spanning eight domains, covering both unimodal and multimodal with short-term and longterm forecasting tasks. More importantly, ReC4TS provides key insights: (1) Self-consistency emerges as the most effective test-time reasoning strategy; (2) Group-relative policy optimization emerges as a more suitable approach for incentivizing reasoning ability during post-training; (3) Multimodal TSF benefits more from reasoning strategies compared to unimodal TSF. Beyond these insights, ReC4TS establishes two pioneering starting blocks to support future zero-shot TSF reasoning research: (1) A novel dataset, TimeThinking, containing forecasting samples annotated with reasoning trajectories from multiple advanced LLMs, and (2) A new and simple test-time scaling-law validated on foundational TSF models enabled by self-consistency reasoning strategy. All data and code are publicly accessible at: <a target="_blank" rel="noopener" href="https://github.com/AdityaLab/OpenTimeR">https://github.com/AdityaLab/OpenTimeR</a> </p>
<blockquote>
<p>æ¨ç†èƒ½åŠ›å¯¹äºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡è‡³å…³é‡è¦ã€‚éšç€åŸºç¡€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œå·²ç»æå‡ºäº†å¤šç§æ¨ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬æµ‹è¯•æ—¶çš„å¢å¼ºï¼ˆå¦‚Chain-ofThoughtï¼‰å’Œè®­ç»ƒåçš„ä¼˜åŒ–ï¼ˆå¦‚DeepSeek-R1ä¸­çš„ç”¨æ³•ï¼‰ã€‚å°½ç®¡è¿™äº›æ¨ç†ç­–ç•¥åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»¬åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­çš„åº”ç”¨å’Œå¯¹é›¶æ ·æœ¬TSFçš„å½±å“ä»åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚å°¤å…¶æ˜¯å°šä¸æ¸…æ¥šé›¶æ ·æœ¬TSFæ˜¯å¦å—ç›Šäºæ¨ç†ï¼Œå¦‚æœæ˜¯è¿™æ ·ï¼Œé‚£ä¹ˆå“ªç§ç±»å‹çš„æ¨ç†ç­–ç•¥æœ€ä¸ºæœ‰æ•ˆã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ReC4TSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿåœ°è¯„ä¼°æµè¡Œæ¨ç†ç­–ç•¥åœ¨é›¶æ ·æœ¬TSFä»»åŠ¡ä¸Šæœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚ReC4TSåœ¨æ¶µç›–å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»¥åŠçŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»»åŠ¡çš„å…«ä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒReC4TSæä¾›äº†å…³é”®çš„è§è§£ï¼šï¼ˆ1ï¼‰è‡ªæ´½æ€§è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ï¼›ï¼ˆ2ï¼‰ç›¸å¯¹äºç¾¤ä½“æ”¿ç­–çš„ä¼˜åŒ–æ–¹æ³•æ›´é€‚åˆäºè®­ç»ƒåçš„æ¨ç†èƒ½åŠ›æ¿€åŠ±ï¼›ï¼ˆ3ï¼‰å¤šæ¨¡æ€TSFæ¯”å•æ¨¡æ€TSFæ›´å—ç›Šäºæ¨ç†ç­–ç•¥ã€‚é™¤äº†è¿™äº›è§è§£å¤–ï¼ŒReC4TSè¿˜å»ºç«‹äº†ä¸¤ä¸ªå¼€åˆ›æ€§çš„èµ·ç‚¹ï¼Œä»¥æ”¯æŒæœªæ¥çš„é›¶æ ·æœ¬TSFæ¨ç†ç ”ç©¶ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ–°çš„æ•°æ®é›†TimeThinkingï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰å¤šä¸ªå…ˆè¿›LLMæ¨ç†è½¨è¿¹çš„é¢„æµ‹æ ·æœ¬æ³¨é‡Šï¼›ï¼ˆ2ï¼‰ä¸€ç§æ–°å‹ä¸”ç®€å•çš„æµ‹è¯•æ—¶è§„æ¨¡å®šå¾‹ï¼Œè¯¥å®šå¾‹åœ¨åŸºäºè‡ªæ´½æ¨ç†ç­–ç•¥çš„åŸºç¡€TSFæ¨¡å‹ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å‡å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/AdityaLab/OpenTimeR%E3%80%82">https://github.com/AdityaLab/OpenTimeRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01895v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æ¢è®¨äº†æ¨ç†èƒ½åŠ›åœ¨è§£å†³æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯éšç€å¤§å‹è¯­è¨€æ¨¡å‹ç­‰åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œå„ç§æ¨ç†ç­–ç•¥å·²ç»è¢«æå‡ºã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†ç­–ç•¥åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰å°¤å…¶æ˜¯é›¶æ ·æœ¬TSFä¸­çš„åº”ç”¨å’Œå½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ–‡ç« æå‡ºäº†ReC4TSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿåœ°è¯„ä¼°æµè¡Œæ¨ç†ç­–ç•¥åœ¨é›¶æ ·æœ¬TSFä»»åŠ¡ä¸Šæœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚ReC4TSåœ¨æ¶µç›–å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»¥åŠçŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»»åŠ¡çš„å…«ä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶æä¾›äº†å…³äºæœ€æœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´æ¨ç†ç­–ç•¥ã€åˆ†ç»„ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–æ–¹æ³•ä»¥åŠå¯¹å•æ¨¡æ€å’Œå¤šæ¨¡æ€TSFçš„å½±å“ç­‰çš„è§è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒReC4TSè¿˜ä¸ºæœªæ¥çš„é›¶æ ·æœ¬TSFæ¨ç†ç ”ç©¶æä¾›äº†ä¸¤ä¸ªå¼€åˆ›æ€§çš„èµ·ç‚¹ï¼šä¸€ä¸ªæ˜¯å«æœ‰æ¥è‡ªå¤šä¸ªå…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„æµ‹æ ·æœ¬åŠå…¶æ¨ç†è½¨è¿¹çš„æ–°å‹æ•°æ®é›†TimeThinkingï¼Œå¦ä¸€ä¸ªæ˜¯ç”±è‡ªæˆ‘ä¸€è‡´æ€§æ¨ç†ç­–ç•¥éªŒè¯çš„ç®€å•æµ‹è¯•æ—¶é—´è§„æ¨¡å®šå¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†èƒ½åŠ›å¯¹äºè§£å†³æŒ‘æˆ˜æ€§ä»»åŠ¡éå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­ã€‚</li>
<li>è™½ç„¶å­˜åœ¨å¤šç§æ¨ç†ç­–ç•¥ï¼Œä½†å®ƒä»¬åœ¨TSFä¸­çš„å…·ä½“åº”ç”¨å’Œå½±å“å°šæœªå®Œå…¨æ¢ç´¢ã€‚</li>
<li>ReC4TSæ˜¯é¦–ä¸ªè¯„ä¼°æ¨ç†ç­–ç•¥åœ¨é›¶æ ·æœ¬TSFä»»åŠ¡ä¸Šæœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ReC4TSçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªæˆ‘ä¸€è‡´æ€§æ˜¯æœ€æœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´æ¨ç†ç­–ç•¥ã€‚</li>
<li>åˆ†ç»„ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–æ˜¯ä¸€ç§é€‚åˆæ¿€åŠ±æ¨ç†èƒ½åŠ›çš„åè®­ç»ƒæ–¹æ³•ã€‚</li>
<li>å¤šæ¨¡æ€TSFæ¯”å•æ¨¡æ€TSFæ›´èƒ½å—ç›Šäºæ¨ç†ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abe9556a7c0ffe8b670574df8f79a6a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d17d09ef5c30b5629d46e03b907e693e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45d5da6e1e7747192efaf276a8a83236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d60f296c17c5bb9bcf7a4935a1f065.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427a77800b8bc4f5c0fcc0ecc53ad86e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87171f6f1bb7c11d906106b35a115f5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699e6f37833e75f0dd0ebdf51b151052.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d4fa48425be67cc3c676e741434f25c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SoS1-O1-and-R1-Like-Reasoning-LLMs-are-Sum-of-Square-Solvers"><a href="#SoS1-O1-and-R1-Like-Reasoning-LLMs-are-Sum-of-Square-Solvers" class="headerlink" title="SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers"></a>SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers</h2><p><strong>Authors:Kechen Li, Wenqi Zhu, Coralia Cartis, Tianbo Ji, Shiwei Liu</strong></p>
<p>Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbertâ€™s Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å„ç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†äººç±»æ°´å¹³çš„ç†Ÿç»ƒç¨‹åº¦ï¼Œä½†å®ƒä»¬åœ¨æ‰§è¡Œä¸¥æ ¼çš„æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªåŸºæœ¬ä½†è®¡ç®—ä¸Šæ£˜æ‰‹çš„çš„é—®é¢˜ï¼šç¡®å®šç»™å®šçš„å¤šå…ƒå¤šé¡¹å¼æ˜¯å¦ä¸ºéè´Ÿã€‚è¿™ä¸ªé—®é¢˜ä¸å¸Œå°”ä¼¯ç‰¹ç¬¬åä¸ƒé—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œåœ¨å…¨çƒå¤šé¡¹å¼ä¼˜åŒ–ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¹¶ä¸”åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†SoS-1Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç»„å¤§çº¦1000ä¸ªç²¾å¿ƒæŒ‘é€‰çš„å¤šé¡¹å¼ï¼Œä»¥åŠåŸºäºäº”ä¸ªæ¸è¿›æŒ‘æˆ˜æ ‡å‡†çš„ä¸“å®¶è®¾è®¡æ¨ç†æŒ‡ä»¤ã€‚è¯„ä¼°äº†å¤šä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰ç»“æ„åŒ–æŒ‡å¯¼æ—¶ï¼Œæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½éƒ½ç•¥é«˜äºéšæœºçŒœæµ‹åŸºçº¿ï¼ˆå³50%ï¼‰ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡çš„æ¨ç†æŒ‡ä»¤å¯ä»¥æ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œå°†æ€§èƒ½æé«˜åˆ°81%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è§„æ¨¡ä¸º7Bçš„æ¨¡å‹SoS-7Bï¼Œä»…åœ¨SoS-1Kä¸Šå¾®è°ƒäº†4ä¸ªå°æ—¶ï¼Œå°±è¶…è¶Šäº†è§„æ¨¡ä¸º671Bçš„DeepSeek-V3å’ŒGPT-4o-miniåœ¨å‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶ä»…éœ€è¦å®ƒä»¬æ‰€éœ€è®¡ç®—æ—¶é—´çš„1.8%å’Œ5%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰æ¨åŠ¨æ•°å­¦æ¨ç†è¾¹ç•Œå’Œè§£å†³NPéš¾é¢˜çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20545v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†äººç±»æ°´å¹³çš„èƒ½åŠ›ï¼Œä½†åœ¨è¿›è¡Œä¸¥è°¨çš„æ•°å­¦é—®é¢˜æ±‚è§£æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ä¸€ä¸ªåŸºç¡€ä½†è®¡ç®—ä¸Šæ£˜æ‰‹çš„é—®é¢˜ï¼šç¡®å®šç»™å®šçš„å¤šå…ƒå¤šé¡¹å¼æ˜¯å¦ä¸ºéè´Ÿã€‚è¿™ä¸ªé—®é¢˜ä¸å¸Œå°”ä¼¯ç‰¹çš„ç¬¬åä¸ƒé—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œåœ¨å…¨çƒå¤šé¡¹å¼ä¼˜åŒ–ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚é€šè¿‡å¼•å…¥SoS-1Kæ•°æ®é›†å’ŒåŸºäºäº”ä¸ªæ¸è¿›æŒ‘æˆ˜æ ‡å‡†çš„ä¸“å®¶è®¾è®¡æ¨ç†æŒ‡ä»¤ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰ç»“æ„æŒ‡å¯¼çš„LLMsè¡¨ç°ä»…ä¸ºéšæœºçŒœæµ‹åŸºçº¿ä¹‹ä¸Šçš„ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡çš„æ¨ç†æŒ‡ä»¤æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œå°†æ€§èƒ½æå‡è‡³é«˜è¾¾ç™¾åˆ†ä¹‹å…«åä¸€ã€‚æ­¤å¤–ï¼Œä»…åœ¨SoS-1Kä¸Šå¾®è°ƒäº†4å°æ—¶çš„SoS-7Bæ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºä»…ä½¿ç”¨å°éƒ¨åˆ†è®¡ç®—æ—¶é—´çš„DeepSeek-V3å’ŒGPT-4o-miniã€‚è¿™äº›å‘ç°çªå‡ºäº†LLMsåœ¨æ¨åŠ¨æ•°å­¦æ¨ç†è¾¹ç•Œå’Œè§£å†³NPéš¾é—®é¢˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šå…ƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢ä»æœ‰å±€é™ã€‚</li>
<li>ç¡®å®šå¤šå…ƒå¤šé¡¹å¼æ˜¯å¦éè´Ÿæ˜¯ä¸€ä¸ªåŸºç¡€ä¸”è®¡ç®—å›°éš¾çš„é—®é¢˜ï¼Œå¯¹å…¨çƒå¤šé¡¹å¼ä¼˜åŒ–å’Œå¤šä¸ªé¢†åŸŸæœ‰é‡è¦å½±å“ã€‚</li>
<li>SoS-1Kæ•°æ®é›†çš„å¼•å…¥å’Œä¸“å®¶è®¾è®¡çš„æ¨ç†æŒ‡ä»¤æœ‰åŠ©äºè¯„ä¼°LLMsåœ¨è¯¥é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ²¡æœ‰ç»“æ„æŒ‡å¯¼çš„LLMsè¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ï¼Œè¯´æ˜ç»“æ„åŒ–æŒ‡å¯¼å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>é«˜è´¨é‡æ¨ç†æŒ‡ä»¤èƒ½æ˜¾è‘—æé«˜LLMsçš„å‡†ç¡®æ€§ï¼Œæœ€å¤§æå‡å¹…åº¦è¾¾ç™¾åˆ†ä¹‹å…«åä¸€ã€‚</li>
<li>SoS-7Bæ¨¡å‹åœ¨ä»…å¾®è°ƒ4å°æ—¶åï¼Œåœ¨æŸäº›æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–å¤§å‹æ¨¡å‹ï¼ŒåŒæ—¶æ‰€éœ€è®¡ç®—æ—¶é—´å¤§å¤§å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f32b341e6aa87f4dfc8fd7bb55ac872b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e57fb8e24e7ccce9a397d7d46dd75864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f992f95240b008875dd1ef15a932006f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning"><a href="#MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning" class="headerlink" title="MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning"></a>MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning</h2><p><strong>Authors:Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert</strong></p>
<p>Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</a>. </p>
<blockquote>
<p>æ¨ç†æ˜¯æ¨è¿›åŒ»å­¦å›¾åƒåˆ†æçš„å…³é”®å‰æ²¿é¢†åŸŸï¼Œé€æ˜åº¦å’Œå¯ä¿¡åº¦åœ¨åŒ»ç”Ÿä¿¡ä»»å’Œç›‘ç®¡æ‰¹å‡†ä¸­éƒ½èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚å°½ç®¡åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ”¾å°„å­¦ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„VLMä»…äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆï¼Œè€Œæ²¡æœ‰æ­ç¤ºå…¶èƒŒåçš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedVLM-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ»å­¦VLMï¼Œèƒ½å¤Ÿæ˜ç¡®ç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œä»¥æé«˜é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚MedVLM-R1æ²¡æœ‰ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå› ä¸ºSFTç»å¸¸è¿‡åº¦æ‹Ÿåˆè®­ç»ƒåˆ†å¸ƒï¼Œå¹¶ä¸”æ— æ³•ä¿ƒè¿›çœŸæ­£çš„æ¨ç†ã€‚ç›¸åï¼ŒMedVLM-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ¿€åŠ±æ¨¡å‹å‘ç°å¯è§£é‡Šçš„æ¨ç†è·¯å¾„ï¼Œè€Œæ— éœ€ä½¿ç”¨ä»»ä½•æ¨ç†å‚è€ƒã€‚å°½ç®¡è®­ç»ƒæ•°æ®æœ‰é™ï¼ˆä»…600ä¸ªè§†è§‰é—®ç­”æ ·æœ¬ï¼‰ä¸”æ¨¡å‹å‚æ•°è¾ƒå°‘ï¼ˆ2Bï¼‰ï¼Œä½†MedVLM-R1åœ¨MRIã€CTå’ŒXå…‰åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡ä»55.11%æé«˜åˆ°78.22%ï¼Œä¼˜äºåœ¨è¶…è¿‡ä¸€ç™¾ä¸‡æ ·æœ¬ä¸Šè®­ç»ƒçš„æ›´å¤§æ¨¡å‹ã€‚å®ƒè¿˜å±•ç¤ºäº†åœ¨ä¸åŒä»»åŠ¡ä¸‹çš„ç¨³å¥é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç»Ÿä¸€åŒ»å­¦å›¾åƒåˆ†æä¸æ˜ç¡®æ¨ç†ï¼ŒMedVLM-R1æ˜¯æœç€ä¸´åºŠå®è·µä¸­å¯ä¿¡å’Œå¯è§£é‡Šçš„AIè¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚æ¨ç†æ¨¡å‹å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1%E3%80%82">https://huggingface.co/JZPeterPan/MedVLM-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19634v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»ç–—å›¾åƒåˆ†æé¢†åŸŸï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†çš„é‡è¦æ€§ï¼Œä»¥æå‡æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹MedVLM-R1ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°ä¸‹å–å¾—è¾ƒå¥½çš„æ€§èƒ½ï¼Œä¸”åœ¨MRIã€CTå’ŒXå…‰ç­‰å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚MedVLM-R1çš„æ¨ç†è·¯å¾„å¯è§£é‡Šæ€§å¼ºï¼Œæ— éœ€ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå…·æœ‰ç¨³å¥çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä¸ºä¸´åºŠå®è·µä¸­å¯ä¿¡å’Œå¯è§£é‡Šçš„AIåº”ç”¨è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†æé¢†åŸŸéœ€è¦æé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ä»¥å¢å¼ºä¸´åºŠåŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»åŠç›‘ç®¡æœºæ„çš„è®¤å¯ã€‚</li>
<li>ç°æœ‰åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸»è¦æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œç¼ºä¹æ­ç¤ºåº•å±‚æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>MedVLM-R1æ˜¯ä¸€ä¸ªæ–°çš„åŒ»ç–—VLMæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚</li>
<li>MedVLM-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œèƒ½å¤Ÿå‘ç°æ¨¡å‹çš„å¯è§£é‡Šæ€§æ¨ç†è·¯å¾„ã€‚</li>
<li>åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°ä¸‹ï¼ŒMedVLM-R1å®ç°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MedVLM-R1å…·æœ‰ç¨³å¥çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„åŒ»ç–—å›¾åƒåˆ†æä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4866253832a007295535196f5d583a46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5227bee00a8424e43c4608d4f863f0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VEM-Environment-Free-Exploration-for-Training-GUI-Agent-with-Value-Environment-Model"><a href="#VEM-Environment-Free-Exploration-for-Training-GUI-Agent-with-Value-Environment-Model" class="headerlink" title="VEM: Environment-Free Exploration for Training GUI Agent with Value   Environment Model"></a>VEM: Environment-Free Exploration for Training GUI Agent with Value   Environment Model</h2><p><strong>Authors:Jiani Zheng, Lu Wang, Fangkai Yang, Chaoyun Zhang, Lingrui Mei, Wenjie Yin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</strong></p>
<p>Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the userâ€™s goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods. </p>
<blockquote>
<p>è®­ç»ƒç”¨äºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼šåŸºäºç¯å¢ƒçš„RLéœ€è¦æ˜‚è´µçš„äº¤äº’ï¼Œè€Œæ— ç¯å¢ƒæ–¹æ³•åˆ™éš¾ä»¥åº”å¯¹åˆ†å¸ƒå˜åŒ–å’Œå¥–åŠ±æ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç¯å¢ƒçš„RLæ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ä»·å€¼ç¯å¢ƒæ¨¡å‹ï¼ˆVEMï¼‰æ¥å°†ä»·å€¼ä¼°è®¡ä¸ç­–ç•¥ä¼˜åŒ–è§£è€¦ã€‚VEMç›´æ¥ä»ç¦»çº¿æ•°æ®ä¸­é¢„æµ‹çŠ¶æ€åŠ¨ä½œå€¼ï¼Œæç‚¼å‡ºå…³äºGUIäº¤äº’ç»“æœçš„äººæœºäº¤äº’å…ˆéªŒçŸ¥è¯†ï¼Œæ— éœ€è¿›è¡Œä¸‹ä¸€æ­¥çŠ¶æ€é¢„æµ‹æˆ–ç¯å¢ƒåé¦ˆã€‚è¿™é¿å…äº†ç´¯ç§¯é”™è¯¯ï¼Œå¹¶é€šè¿‡ä¸“æ³¨äºè¯­ä¹‰æ¨ç†ï¼ˆä¾‹å¦‚ï¼Œæ­¤æ“ä½œæ˜¯å¦æœ‰åŠ©äºç”¨æˆ·çš„ç›®æ ‡ï¼Ÿï¼‰å¢å¼ºäº†åº”å¯¹UIå˜åŒ–çš„é€‚åº”æ€§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é¢„è®­ç»ƒVEMä»¥ä¼°è®¡é•¿æœŸåŠ¨ä½œæ•ˆç”¨ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯ä½¿ç”¨å†»ç»“çš„VEMä¿¡å·æ¥æŒ‡å¯¼ç­–ç•¥æ¢ç´¢ï¼Œä»è€Œå®ç°ä¸å¸ƒå±€æ— å…³çš„GUIè‡ªåŠ¨åŒ–ã€‚åœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVEMåœ¨ç¦»çº¿è®¾ç½®å’Œåœ¨çº¿è®¾ç½®ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨æ— ç¯å¢ƒåŸºçº¿æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰äº¤äº’æˆæœ¬çš„æƒ…å†µä¸‹ä¸ç¯å¢ƒåŸºå‡†æ–¹æ³•ç›¸åŒ¹é…ã€‚é‡è¦çš„æ˜¯ï¼ŒVEMè¯æ˜äº†è¯­ä¹‰æ„ŸçŸ¥ä»·å€¼ä¼°è®¡å¯ä»¥è¾¾åˆ°ä¸åœ¨çº¿è®­ç»ƒæ–¹æ³•ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18906v1">PDF</a> 20pages,5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸‹ï¼Œè®­ç»ƒç”¨äºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ— ç¯å¢ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ä»·å€¼ç¯å¢ƒæ¨¡å‹ï¼ˆVEMï¼‰æ¥åˆ†ç¦»ä»·å€¼ä¼°è®¡ä¸ç­–ç•¥ä¼˜åŒ–ã€‚VEMèƒ½ä»ç¦»çº¿æ•°æ®ä¸­é¢„æµ‹çŠ¶æ€è¡ŒåŠ¨ä»·å€¼ï¼Œé€šè¿‡å¯¹GUIäº¤äº’ç»“æœçš„äººç±»å¼ä¼˜å…ˆç†è§£æ¥æå–ä»·å€¼ä¼°è®¡ï¼Œè€Œæ— éœ€é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€æˆ–ç¯å¢ƒåé¦ˆã€‚è¯¥æ–¹æ³•é¿å…äº†å¤åˆé”™è¯¯ï¼Œå¢å¼ºäº†å¯¹äºç”¨æˆ·ç•Œé¢å˜åŒ–çš„é€‚åº”åŠ›ï¼Œä¸“æ³¨äºè¯­ä¹‰æ¨ç†ã€‚åœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºæ— ç¯å¢ƒåŸºå‡†æµ‹è¯•å’Œéœ€ç¯å¢ƒæµ‹è¯•æ–¹æ³•ï¼ŒVEMåœ¨ç¦»çº¿ä¸åœ¨çº¿è®¾ç½®ä¸­å‡å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚å°¤å…¶é‡è¦çš„æ˜¯ï¼ŒVEMå±•ç¤ºäº†è¯­ä¹‰æ„ŸçŸ¥ä»·å€¼ä¼°è®¡æ–¹æ³•èƒ½å¤Ÿè¾¾åˆ°åœ¨çº¿è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”¨äºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— ç¯å¢ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ä»·å€¼ç¯å¢ƒæ¨¡å‹ï¼ˆVEMï¼‰è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>VEMèƒ½ä»ç¦»çº¿æ•°æ®ä¸­é¢„æµ‹çŠ¶æ€è¡ŒåŠ¨ä»·å€¼ï¼Œæ— éœ€é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€æˆ–ç¯å¢ƒåé¦ˆã€‚</li>
<li>VEMé€šè¿‡åˆ†ç¦»ä»·å€¼ä¼°è®¡ä¸ç­–ç•¥ä¼˜åŒ–ï¼Œé¿å…äº†å¤åˆé”™è¯¯ï¼Œå¢å¼ºäº†é€‚åº”UIå˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>VEMåœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒåŒ¹é…äº†ç¯å¢ƒä¾èµ–æ–¹æ³•ä¸”æ— éœ€äº¤äº’æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf2f64a450801511e603cc22c72c8ae8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4511acf92e2aae6582a1d9571a18a0eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8735cda51684692a8117eb3f9c8b0a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5639d304bf24147175980d203f3d0e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reward-Shaping-to-Mitigate-Reward-Hacking-in-RLHF"><a href="#Reward-Shaping-to-Mitigate-Reward-Hacking-in-RLHF" class="headerlink" title="Reward Shaping to Mitigate Reward Hacking in RLHF"></a>Reward Shaping to Mitigate Reward Hacking in RLHF</h2><p><strong>Authors:Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PARâ€™s superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/PAR">https://github.com/PorUna-byte/PAR</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒRLHFå®¹æ˜“å—åˆ°å¥–åŠ±æ“çºµçš„å½±å“ï¼Œå³ä»£ç†åˆ©ç”¨å¥–åŠ±å‡½æ•°çš„æ¼æ´ï¼Œè€Œä¸æ˜¯å­¦ä¹ é¢„æœŸçš„è¡Œä¸ºï¼Œä»è€Œé™ä½äº†å¯¹é½ç¨‹åº¦ã€‚è™½ç„¶å¥–åŠ±å¡‘é€ æœ‰åŠ©äºç¨³å®šRLHFå¹¶éƒ¨åˆ†ç¼“è§£å¥–åŠ±æ“çºµé—®é¢˜ï¼Œä½†å¯¹äºå¡‘é€ æŠ€æœ¯åŠå…¶åŸºç¡€åŸç†çš„ç³»ç»Ÿæ€§ç ”ç©¶ä»ç„¶ç¼ºä¹ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹æµè¡Œçš„å¥–åŠ±å¡‘é€ æ–¹æ³•è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„åˆ†ææå‡ºäº†ä¸‰ä¸ªå…³é”®è®¾è®¡åŸåˆ™ï¼šï¼ˆ1ï¼‰RLå¥–åŠ±æœ€å¥½æ˜¯æœ‰é™çš„ï¼Œï¼ˆ2ï¼‰RLä»å¿«é€Ÿåˆæ­¥å¢é•¿åé€æ­¥æ”¶æ•›ä¸­å—ç›Šï¼Œï¼ˆ3ï¼‰RLå¥–åŠ±æœ€å¥½åˆ¶å®šä¸ºä¸­å¿ƒå¥–åŠ±çš„å‡½æ•°ã€‚é€šè¿‡è¿™äº›è§è§£çš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåå¥½ä½œä¸ºå¥–åŠ±â€ï¼ˆPARï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¥–åŠ±æ¨¡å‹æœ¬èº«ä¸­çš„æ½œåœ¨åå¥½ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„ä¿¡å·ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºç¡€æ¨¡å‹Gemma2-2Bå’ŒLlama3-8Bä¸Šï¼Œä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†Ultrafeedback-Binarizedå’ŒHH-RLHFè¯„ä¼°äº†PARã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–å¥–åŠ±å¡‘é€ æ–¹æ³•ç›¸æ¯”ï¼ŒPARå…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚åœ¨AlpacaEval 2.0åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPARçš„èƒœç‡è‡³å°‘æ¯”ç«äº‰æ–¹æ³•é«˜å‡º5ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼ŒPARæ˜¾ç¤ºå‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œåªéœ€è¦ä¸€ä¸ªå‚è€ƒå¥–åŠ±å°±èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç»è¿‡ä¸¤è½®å®Œæ•´è®­ç»ƒåï¼Œä»èƒ½æŠµæŠ—å¥–åŠ±æ“çºµã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/PAR%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PorUna-byte/PARä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18770v2">PDF</a> 19 pages</p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒRLHFæ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»çš„å½±å“ï¼Œå…¶ä¸­ä»£ç†åˆ©ç”¨å¥–åŠ±å‡½æ•°çš„æ¼æ´è€Œä¸æ˜¯å­¦ä¹ é¢„æœŸè¡Œä¸ºï¼Œå¯¼è‡´å¯¹é½é™çº§ã€‚è™½ç„¶å¥–åŠ±å¡‘é€ æœ‰åŠ©äºç¨³å®šRLHFå¹¶éƒ¨åˆ†ç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ï¼Œä½†å…³äºå¡‘é€ æŠ€æœ¯å’Œå…¶åŸºæœ¬åŸç†çš„ç³»ç»Ÿæ€§ç ”ç©¶ä»ç„¶ç¼ºä¹ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹æµè¡Œçš„å¥–åŠ±å¡‘é€ æ–¹æ³•è¿›è¡Œäº†æ·±å…¥ç ”ç©¶å¹¶æå‡ºäº†ä¸‰æ¡å…³é”®è®¾è®¡åŸåˆ™ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ©ç”¨å¥–åŠ±æ¨¡å‹å†…éƒ¨éšå«åå¥½ä½œä¸ºå¼ºåŒ–å­¦ä¹ ä¿¡å·çš„æ–°æ–¹æ³•â€”â€”åå¥½ä½œä¸ºå¥–åŠ±ï¼ˆPARï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPARåœ¨Gemma2-2Bå’ŒLlama3-8Bä¸¤ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¥–åŠ±å¡‘é€ æ–¹æ³•ã€‚åœ¨AlpacaEval 2.0åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPARçš„èƒœç‡è‡³å°‘æ¯”ç«äº‰æ–¹æ³•é«˜å‡º5ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼ŒPARè¡¨ç°å‡ºæ˜¾è‘—çš„æ•°æ®æ•ˆç‡ï¼Œåªéœ€ä¸€ä¸ªå‚è€ƒå¥–åŠ±å³å¯å®ç°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸¤ä¸ªå®Œæ•´çš„è®­ç»ƒå‘¨æœŸåä»ç„¶èƒ½å¤ŸæŠµå¾¡å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/PAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PorUna-byte/PARæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RLHFå¯¹äºå°†LLMä¸äººç±»ä»·å€¼è§‚å¯¹é½è‡³å…³é‡è¦ï¼Œä½†æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ã€‚</li>
<li>å¥–åŠ±å¡‘é€ æœ‰åŠ©äºç¨³å®šRLHFå¹¶éƒ¨åˆ†ç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚</li>
<li>æ™®éå­˜åœ¨çš„å¥–åŠ±å¡‘é€ æ–¹æ³•ç¼ºä¹ç³»ç»Ÿç ”ç©¶å’Œåˆ†æã€‚</li>
<li>å­˜åœ¨ä¸‰æ¡å…³é”®è®¾è®¡åŸåˆ™æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ å¥–åŠ±çš„è®¾è®¡ï¼šç†æƒ³æƒ…å†µä¸‹å¥–åŠ±æ˜¯æœ‰ç•Œçš„ã€åˆæœŸè¿…é€Ÿå¢é•¿åé€æ¸æ”¶æ•›ã€æœ€ä½³å½¢å¼æ˜¯ä¸­å¿ƒå¥–åŠ±çš„å‡½æ•°ã€‚</li>
<li>æå‡ºçš„PARæ–¹æ³•åˆ©ç”¨å¥–åŠ±æ¨¡å‹å†…éƒ¨éšå«åå¥½ä½œä¸ºå¼ºåŒ–å­¦ä¹ ä¿¡å·ã€‚</li>
<li>PARåœ¨å¤šä¸ªå®éªŒè®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–å¥–åŠ±å¡‘é€ æ–¹æ³•å…·æœ‰æ›´é«˜çš„èƒœç‡å’Œæ•°æ®æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cfc26fe413233e6b667bac7f8adcb3d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bd8aa778aa37491ffb453eb483e28c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf059551909d138ccc5c7941d179477e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8defeea010baa9d02343d016bc6ef1cb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reversal-Blessing-Thinking-Backward-May-Outpace-Thinking-Forward-in-Multi-choice-Questions"><a href="#Reversal-Blessing-Thinking-Backward-May-Outpace-Thinking-Forward-in-Multi-choice-Questions" class="headerlink" title="Reversal Blessing: Thinking Backward May Outpace Thinking Forward in   Multi-choice Questions"></a>Reversal Blessing: Thinking Backward May Outpace Thinking Forward in   Multi-choice Questions</h2><p><strong>Authors:Yizhe Zhang, Richard Bai, Zijin Gu, Ruixiang Zhang, Jiatao Gu, Emmanuel Abbe, Samy Bengio, Navdeep Jaitly</strong></p>
<p>Language models usually use left-to-right (L2R) autoregressive factorization. However, L2R factorization may not always be the best inductive bias. Therefore, we investigate whether alternative factorizations of the text distribution could be beneficial in some tasks. We investigate right-to-left (R2L) training as a compelling alternative, focusing on multiple-choice questions (MCQs) as a test bed for knowledge extraction and reasoning. Through extensive experiments across various model sizes (2B-8B parameters) and training datasets, we find that R2L models can significantly outperform L2R models on several MCQ benchmarks, including logical reasoning, commonsense understanding, and truthfulness assessment tasks. Our analysis reveals that this performance difference may be fundamentally linked to multiple factors including calibration, computability and directional conditional entropy. We ablate the impact of these factors through controlled simulation studies using arithmetic tasks, where the impacting factors can be better disentangled. Our work demonstrates that exploring alternative factorizations of the text distribution can lead to improvements in LLM capabilities and provides theoretical insights into optimal factorization towards approximating human language distribution, and when each reasoning order might be more advantageous. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹é€šå¸¸ä½¿ç”¨ä»å·¦åˆ°å³ï¼ˆL2Rï¼‰çš„è‡ªå›å½’åˆ†è§£ã€‚ç„¶è€Œï¼ŒL2Råˆ†è§£å¹¶ä¸æ€»æ˜¯æœ€å¥½çš„å½’çº³åç½®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è°ƒæŸ¥æ–‡æœ¬åˆ†å¸ƒçš„æ›¿ä»£åˆ†è§£åœ¨ä¸€äº›ä»»åŠ¡ä¸­æ˜¯å¦å¯èƒ½æœ‰ç›Šã€‚æˆ‘ä»¬ç ”ç©¶ä»å³åˆ°å·¦ï¼ˆR2Lï¼‰çš„è®­ç»ƒä½œä¸ºå¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸“æ³¨äºå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ä½œä¸ºçŸ¥è¯†æå–å’Œæ¨ç†çš„æµ‹è¯•å¹³å°ã€‚é€šè¿‡åœ¨ä¸åŒæ¨¡å‹å¤§å°ï¼ˆ2B-8Bå‚æ•°ï¼‰å’Œè®­ç»ƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°R2Læ¨¡å‹åœ¨å¤šä¸ªå¤šé¡¹é€‰æ‹©é¢˜åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºL2Ræ¨¡å‹ï¼ŒåŒ…æ‹¬é€»è¾‘æ¨ç†ã€å¸¸è¯†ç†è§£å’ŒçœŸå®æ€§è¯„ä¼°ä»»åŠ¡ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½å·®å¼‚å¯èƒ½ä¸å¤šä¸ªå› ç´ æ ¹æœ¬ç›¸å…³ï¼ŒåŒ…æ‹¬æ ¡å‡†ã€è®¡ç®—æ€§å’Œæ–¹å‘æ¡ä»¶ç†µã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç®—æœ¯ä»»åŠ¡çš„å—æ§æ¨¡æ‹Ÿç ”ç©¶æ¥å‰–æè¿™äº›å› ç´ çš„å½±å“ï¼Œåœ¨è¿™äº›ç ”ç©¶ä¸­å¯ä»¥æ›´å¥½åœ°åˆ†ç¦»å½±å“å› ç´ ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œæ¢ç´¢æ–‡æœ¬åˆ†å¸ƒçš„æ›¿ä»£åˆ†è§£å¯ä»¥å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ”¹è¿›ï¼Œå¹¶ä¸ºé€¼è¿‘äººç±»è¯­è¨€åˆ†å¸ƒçš„æœ€ä½³åˆ†è§£æä¾›ç†è®ºæ´å¯Ÿï¼Œä»¥åŠæ¯ç§æ¨ç†é¡ºåºå¯èƒ½æ›´å ä¼˜åŠ¿çš„æ—¶æœºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18435v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ†å¸ƒå› å­åŒ–é¡ºåºé—®é¢˜ï¼Œç ”ç©¶äº†é™¤äº†å¸¸ç”¨çš„ä»å·¦åˆ°å³ï¼ˆL2Rï¼‰è‡ªå›å½’å› å­åŒ–æ–¹æ³•å¤–ï¼Œå³åˆ°å·¦ï¼ˆR2Lï¼‰è®­ç»ƒåœ¨çŸ¥è¯†æå–å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œå‘ç°R2Læ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰çš„é€»è¾‘æ¨ç†ã€å¸¸è¯†ç†è§£å’ŒçœŸå®æ€§è¯„ä¼°ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºL2Ræ¨¡å‹ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½å·®å¼‚å¯èƒ½ä¸æ ¡å‡†ã€è®¡ç®—æ€§å’Œæ–¹å‘æ¡ä»¶ç†µç­‰å¤šä¸ªå› ç´ æœ‰å…³ã€‚é€šè¿‡æ§åˆ¶æ¨¡æ‹Ÿç ”ç©¶å¯¹è¿™äº›å› ç´ è¿›è¡Œäº†å½±å“å‰¥ç¦»ï¼Œæ­ç¤ºäº†æ¢ç´¢æ–‡æœ¬åˆ†å¸ƒçš„ä¸åŒå› å­åŒ–é¡ºåºèƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ç†è®ºä¾æ®å’Œå®è·µä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é™¤äº†ä¼ ç»Ÿçš„ä»å·¦åˆ°å³è‡ªå›å½’å› å­åŒ–æ–¹æ³•å¤–ï¼Œæœ¬æ–‡æ¢ç´¢äº†å³åˆ°å·¦è®­ç»ƒä½œä¸ºæ–‡æœ¬åˆ†å¸ƒçš„ä¸€ç§æ›¿ä»£å› å­åŒ–æ–¹å¼ã€‚</li>
<li>åœ¨å¤šé¡¹é€‰æ‹©é¢˜çš„çŸ¥è¯†æå–å’Œæ¨ç†ä»»åŠ¡ä¸Šï¼Œå³åˆ°å·¦æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>è¿™ç§æ€§èƒ½å·®å¼‚ä¸æ¨¡å‹çš„æ ¡å‡†ã€è®¡ç®—æ€§å’Œæ–¹å‘æ¡ä»¶ç†µç­‰å› ç´ æœ‰å…³ã€‚</li>
<li>é€šè¿‡æ§åˆ¶æ¨¡æ‹Ÿç ”ç©¶ï¼Œåˆ†æäº†å½±å“æ¨¡å‹æ€§èƒ½çš„å„ç§å› ç´ ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œæ¢ç´¢æ–‡æœ¬åˆ†å¸ƒçš„ä¸åŒå› å­åŒ–é¡ºåºæœ‰åŠ©äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºé€¼è¿‘äººç±»è¯­è¨€åˆ†å¸ƒçš„æ–‡æœ¬åˆ†å¸ƒå› å­åŒ–æä¾›äº†ç†è®ºè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39f76fb75bdb0101a18a53d559fccbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51512286e043d9d172f333df4e3d1051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d12ee4e805ddf3c6370318ea7a9f5a19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-484760914a2907c811563cd3ae44603c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13d2f679b176d4a645e47ca3ae3b93df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-361695638fc47f6de04a941f7abc4914.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DeepSeek-R1-Outperforms-Gemini-2-0-Pro-OpenAI-o1-and-o3-mini-in-Bilingual-Complex-Ophthalmology-Reasoning"><a href="#DeepSeek-R1-Outperforms-Gemini-2-0-Pro-OpenAI-o1-and-o3-mini-in-Bilingual-Complex-Ophthalmology-Reasoning" class="headerlink" title="DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in   Bilingual Complex Ophthalmology Reasoning"></a>DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in   Bilingual Complex Ophthalmology Reasoning</h2><p><strong>Authors:Pusheng Xu, Yue Wu, Kai Jin, Xiaolan Chen, Mingguang He, Danli Shi</strong></p>
<p>Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n &#x3D; 39) and management (n &#x3D; 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P&lt;0.001 compared with DeepSeek-R1), and 0.746 (P&#x3D;0.115), 0.723 (P&#x3D;0.027), and 0.577 (P&lt;0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P&lt;0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making. </p>
<blockquote>
<p>ç›®çš„ï¼šæ—¨åœ¨è¯„ä¼°DeepSeek-R1ä»¥åŠå…¶ä»–ä¸‰ç§æœ€è¿‘å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒè¯­å¤æ‚çœ¼ç§‘ç—…ä¾‹ä¸­çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚æ–¹æ³•ï¼šä»ä¸­æ–‡çœ¼ç§‘é«˜çº§ä¸“ä¸šæŠ€æœ¯èŒç§°è€ƒè¯•ä¸­æ”¶é›†ä¸è¯Šæ–­ï¼ˆn&#x3D;39ï¼‰å’Œç®¡ç†ï¼ˆn&#x3D;91ï¼‰ç›¸å…³çš„æ€»è®¡130é“é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œå¹¶å°†å…¶åˆ†ä¸ºå…­ä¸ªä¸»é¢˜ã€‚è¿™äº›é€‰æ‹©é¢˜ä½¿ç”¨DeepSeek-R1ç¿»è¯‘æˆè‹±æ–‡ã€‚åœ¨é»˜è®¤é…ç½®ä¸‹ï¼Œäº2025å¹´2æœˆ15æ—¥è‡³2æœˆ20æ—¥æœŸé—´ï¼Œç”ŸæˆDeepSeek-R1ã€Gemini 2.0 Proã€OpenAI o1å’Œo3-miniçš„å›ç­”ã€‚å‡†ç¡®æ€§è¢«è®¡ç®—ä¸ºæ­£ç¡®å›ç­”é—®é¢˜çš„æ¯”ä¾‹ï¼Œé—æ¼å’Œé¢å¤–ç­”æ¡ˆè¢«è§†ä¸ºä¸æ­£ç¡®ã€‚é€šè¿‡åˆ†æå’Œæ¢è®¨æ¨ç†é€»è¾‘å’Œæ¨ç†é”™è¯¯çš„åŸå› æ¥è¯„ä¼°æ¨ç†èƒ½åŠ›ã€‚ç»“æœï¼šDeepSeek-R1åœ¨åŒè¯­çœ¼ç§‘é€‰æ‹©é¢˜ä¸­è¡¨ç°å‡ºæœ€é«˜çš„æ€»ä½“å‡†ç¡®æ€§ï¼Œåœ¨ä¸­æ–‡é€‰æ‹©é¢˜ä¸­çš„å‡†ç¡®ç‡ä¸º0.862ï¼Œè‹±æ–‡é€‰æ‹©é¢˜ä¸­çš„å‡†ç¡®ç‡ä¸º0.808ã€‚åœ¨ä¸­æ–‡é€‰æ‹©é¢˜ä¸­ï¼ŒGemini 2.0 Proã€OpenAI o1å’ŒOpenAI o3-miniçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º0.715ã€0.685å’Œ0.692ï¼ˆä¸DeepSeek-R1ç›¸æ¯”ï¼Œæ‰€æœ‰P&lt;0.001ï¼‰ï¼Œåœ¨è‹±æ–‡é€‰æ‹©é¢˜ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º0.746ï¼ˆP&#x3D;0.115ï¼‰ã€0.723ï¼ˆP&#x3D;0.027ï¼‰å’Œ0.577ï¼ˆP&lt;0.001ï¼‰ã€‚åœ¨ä¸­æ–‡å’Œè‹±æ–‡é€‰æ‹©é¢˜ä¸­ï¼ŒDeepSeek-R1åœ¨äº”ä¸ªä¸»é¢˜ä¸­çš„å‡†ç¡®ç‡æœ€é«˜ã€‚åœ¨ç®¡ç†ç±»ä¸­æ–‡é¢˜ç›®çš„æµ‹è¯•ä¸­ï¼ŒDeepSeek-R1è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼ˆæ‰€æœ‰P&lt;0.05ï¼‰ã€‚æ¨ç†èƒ½åŠ›åˆ†ææ˜¾ç¤ºï¼Œå››ç§å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ç›¸ä¼¼çš„æ¨ç†é€»è¾‘ã€‚å¿½ç•¥é‡è¦çš„æ­£é¢ç—…å²ã€å¿½ç•¥é‡è¦çš„é˜³æ€§ä½“å¾ã€è¯¯è§£åŒ»ç–—æ•°æ®ä»¥åŠè¿‡äºæ¿€è¿›æ˜¯æ¨ç†é”™è¯¯æœ€å¸¸è§çš„æˆå› ã€‚ç»“è®ºï¼šç›¸è¾ƒäºå…¶ä»–ä¸‰ç§å…ˆè¿›çš„LLMï¼ŒDeepSeek-R1åœ¨å¤„ç†åŒè¯­å¤æ‚çœ¼ç§‘æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è™½ç„¶å…¶ä¸´åºŠé€‚ç”¨æ€§ä»æœ‰å¾…æŒ‘æˆ˜ï¼Œä½†å®ƒå¯¹äºæ”¯æŒè¯Šæ–­å’Œä¸´åºŠå†³ç­–åˆ¶å®šæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17947v1">PDF</a> 29 pages, 4 figures, 1 table</p>
<p><strong>Summary</strong><br>     è¿‘æœŸç ”ç©¶å¯¹æ¯”äº†DeepSeek-R1ä¸å…¶ä»–ä¸‰æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒè¯­å¤æ‚çœ¼ç§‘ç—…ä¾‹ä¸­çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨ä¸­æ–‡å’Œè‹±æ–‡çš„çœ¼ç§‘ä¸“ä¸šé—®é¢˜ä¸­å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°86%ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚è¡¨ç°ã€‚è™½ç„¶ä¸´åºŠåº”ç”¨ä»æœ‰æŒ‘æˆ˜ï¼Œä½†åœ¨è¾…åŠ©è¯Šæ–­å’Œä¸´åºŠå†³ç­–æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶ç›®çš„æ˜¯è¯„ä¼°DeepSeek-R1å’Œå…¶ä»–ä¸‰æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒè¯­å¤æ‚çœ¼ç§‘ç—…ä¾‹ä¸­çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>DeepSeek-R1åœ¨ä¸­æ–‡å’Œè‹±æ–‡çœ¼ç§‘ä¸“ä¸šé—®é¢˜ä¸­çš„å‡†ç¡®ç‡å‡è¶…è¿‡å…¶ä»–æ¨¡å‹ï¼Œè¾¾åˆ°æœ€é«˜æ°´å¹³ã€‚</li>
<li>DeepSeek-R1åœ¨çœ¼ç§‘ç—…ä¾‹çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åœ¨è¯Šæ–­å’Œç®¡ç†æ–¹é¢çš„ä»»åŠ¡ã€‚</li>
<li>å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è™½é€ŠäºDeepSeek-R1ï¼Œä½†å…¶åœ¨ä¸€äº›é¢†åŸŸä¹Ÿæœ‰ç›¸å¯¹è¾ƒå¥½çš„è¡¨ç°ã€‚</li>
<li>LLMsçš„å¸¸è§æ¨ç†é”™è¯¯åŸå› åŒ…æ‹¬å¿½ç•¥å…³é”®é˜³æ€§ç—…å²ã€å¿½ç•¥å…³é”®é˜³æ€§ä½“å¾ã€è¯¯è§£åŒ»ç–—æ•°æ®ä»¥åŠè¿‡äºæ¿€è¿›ç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-504eaeaf98561201ba30969a555879c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a921bbfdd8f5fa237c9cb06878299c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-224cd2a966cb94406aa7eeab5850c5de.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Image-Generation-with-Vision-Full-view-Prompt"><a href="#Autoregressive-Image-Generation-with-Vision-Full-view-Prompt" class="headerlink" title="Autoregressive Image Generation with Vision Full-view Prompt"></a>Autoregressive Image Generation with Vision Full-view Prompt</h2><p><strong>Authors:Miaomiao Cai, Guanjie Wang, Wei Li, Zhijun Tu, Hanting Chen, Shaohui Lin, Jie Hu</strong></p>
<p>In autoregressive (AR) image generation, models based on the â€˜next-token predictionâ€™ paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the imageâ€™s structure and details, impacting the generationâ€™s accuracy and stability. Additionally, the â€˜next-token predictionâ€™ paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Prompt engineering, as a key technique for guiding LLMs, leverages specifically designed prompts to improve model performance on complex natural language processing (NLP) tasks, enhancing accuracy and stability of generation while maintaining contextual coherence and logical consistency, similar to human reasoning. Inspired by prompt engineering from the field of NLP, we propose Vision Full-view prompt (VF prompt) to enhance autoregressive image generation. Specifically, we design specialized image-related VF prompts for AR image generation to simulate the process of human image creation. This enhances contextual logic ability by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without VF prompts, our method shows outstanding performance and achieves an approximate improvement of 20%. </p>
<blockquote>
<p>åœ¨è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆä¸­ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼çš„æ¨¡å‹é€šè¿‡å‡å°‘å½’çº³åè§ï¼Œå±•ç°äº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºå¤æ‚å›¾åƒç”Ÿæˆä¼šé¢ä¸´é‡å»ºå›¾åƒç»“æ„å’Œç»†èŠ‚çš„æŒ‘æˆ˜ï¼Œå½±å“ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒARæ¨¡å‹ä¸­çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼å¹¶ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥æ‰€æ¶‰åŠçš„ä¸Šæ–‡æ‰«æå’Œé€»è¾‘æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å›¾åƒç”Ÿæˆã€‚</p>
</blockquote>
<p>æç¤ºå·¥ç¨‹æ˜¯å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯ï¼Œå®ƒé€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºæ¥æ”¹å–„æ¨¡å‹åœ¨å¤æ‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡è¿è´¯å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œç±»ä¼¼äºäººç±»æ¨ç†ã€‚</p>
<p>å—è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæç¤ºå·¥ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºVision Full-view promptï¼ˆVFæç¤ºï¼‰æ¥å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºARå›¾åƒç”Ÿæˆè®¾è®¡äº†ä¸“é—¨åŒ–çš„å›¾åƒç›¸å…³VFæç¤ºï¼Œä»¥æ¨¡æ‹Ÿäººç±»å›¾åƒåˆ›å»ºçš„è¿‡ç¨‹ã€‚è¿™é€šè¿‡å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒä¹‹å‰å…ˆæ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡é€»è¾‘èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¢åŠ æ¨ç†æ­¥éª¤æé«˜äº†ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚ä¸æ²¡æœ‰VFæç¤ºçš„ARæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†çº¦20%çš„æ”¹è¿›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16965v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œæå‡ºä¸€ç§åä¸ºVision Full-view promptï¼ˆVFæç¤ºï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚é€šè¿‡è®¾è®¡ä¸“é—¨çš„å›¾åƒç›¸å…³VFæç¤ºï¼Œæ¨¡æ‹Ÿäººç±»å›¾åƒåˆ›å»ºè¿‡ç¨‹ï¼Œå¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„ä¸Šä¸‹æ–‡é€»è¾‘èƒ½åŠ›ï¼Œå¹¶æé«˜ç”Ÿæˆç¨³å®šæ€§ã€‚ç›¸è¾ƒäºæ— VFæç¤ºçš„ARæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¿‘ä¼¼æå‡äº†20%çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ARå›¾åƒç”Ÿæˆä¸­ï¼ŒåŸºäºâ€™next-token predictionâ€™çš„æ¨¡å‹åœ¨å‡å°‘å½’çº³åè§æ–¹é¢è¡¨ç°å‡ºä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚å›¾åƒç”Ÿæˆæ–¹é¢å­˜åœ¨ç»“æ„é‡å»ºå’Œç»†èŠ‚ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>â€˜Next-token predictionâ€™èŒƒå¼ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥ä¸­çš„ä¸Šä¸‹æ–‡æ‰«æå’Œé€»è¾‘æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>æç¤ºå·¥ç¨‹ä½œä¸ºå¼•å¯¼LLMsçš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºæ¥æ”¹å–„å¤æ‚NLPä»»åŠ¡ä¸Šçš„æ¨¡å‹æ€§èƒ½ï¼Œç»´æŒç”Ÿæˆçš„ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚</li>
<li>å€Ÿé‰´NLPé¢†åŸŸçš„æç¤ºå·¥ç¨‹ï¼Œæå‡ºVision Full-view promptï¼ˆVFæç¤ºï¼‰æ¥å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚</li>
<li>VFæç¤ºè®¾è®¡ä¸“é—¨çš„å›¾åƒç›¸å…³æç¤ºç”¨äºARå›¾åƒç”Ÿæˆï¼Œæ¨¡æ‹Ÿäººç±»å›¾åƒåˆ›å»ºè¿‡ç¨‹ã€‚</li>
<li>VFæç¤ºå¢å¼ºäº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é€»è¾‘èƒ½åŠ›ï¼Œå…è®¸æ¨¡å‹å…ˆæ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯å†ç”Ÿæˆå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b252fa429ab4ec13a2157e10d6a13de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63555639ec6eb587d611a611259601ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b725730f2f122c374b695d070f9afc30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-643d2f6847a7fe69826056f6ed053657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5dbcd4e79bd4bca49e9b290179c544c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5437b87bcab3fde6e5ece04db7fb2282.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Lean-and-Mean-Decoupled-Value-Policy-Optimization-with-Global-Value-Guidance"><a href="#Lean-and-Mean-Decoupled-Value-Policy-Optimization-with-Global-Value-Guidance" class="headerlink" title="Lean and Mean: Decoupled Value Policy Optimization with Global Value   Guidance"></a>Lean and Mean: Decoupled Value Policy Optimization with Global Value   Guidance</h2><p><strong>Authors:Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</strong></p>
<p>Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40% and training time by 35% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance. </p>
<blockquote>
<p>åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚å®ƒè¦æ±‚actorå’Œcriticè¿›è¡Œè”åˆè®­ç»ƒï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„å›ºå®šå¥–åŠ±æ¨¡å‹è¿›è¡ŒæŒ‡å¯¼ã€‚è¿™ç§æ–¹æ³•ç”±äºactor-criticçš„ç›¸äº’ä¾èµ–æ€§è€Œå¢åŠ äº†è®¡ç®—å¤æ‚æ€§å’Œä¸ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒPPOåœ¨LLMä»»åŠ¡ä¸­æ— æ³•è·å¾—çœŸå®çš„ç¯å¢ƒå¥–åŠ±ï¼Œè¿™é™åˆ¶äº†å…¶é€‚åº”æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒä»·å€¼æ¨¡å‹æˆ–å¥–åŠ±æ¨¡å‹å˜å¾—ç­‰ä»·ï¼Œå› ä¸ºä¸¤è€…éƒ½æä¾›äº†å›ºå®šçš„ç›‘ç£ä¿¡å·è€Œæ²¡æœ‰æ–°çš„çœŸå®åé¦ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£è€¦ä»·å€¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDVPOï¼‰â€ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾ç®€çš„æ¡†æ¶ï¼Œç”¨é¢„è®­ç»ƒçš„â€œå…¨å±€ä»·å€¼æ¨¡å‹ï¼ˆGVMï¼‰â€æ›¿ä»£ä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡ã€‚GVMæ ¹æ®æ”¿ç­–è½¨è¿¹è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå¹¶é¢„æµ‹tokençº§åˆ«çš„å›æŠ¥ä¼°è®¡ã€‚é€šè¿‡è§£è€¦ä»·å€¼æ¨¡å‹ä¸ç­–ç•¥è®­ç»ƒï¼ˆé€šè¿‡å†»ç»“çš„GVMé©±åŠ¨çš„RLç›®æ ‡ï¼‰ï¼ŒDVPOæ¶ˆé™¤äº†actor-criticçš„ç›¸äº’ä¾èµ–æ€§ï¼Œä¸å¸¸è§„çš„RLHFç›¸æ¯”ï¼Œå‡å°‘äº†40%çš„GPUå†…å­˜ä½¿ç”¨é‡å’Œ35%çš„è®­ç»ƒæ—¶é—´ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDVPOåœ¨æ€§èƒ½ä¸Šä¼˜äºé«˜æ•ˆçš„RLHFæ–¹æ³•ï¼ˆä¾‹å¦‚DPOï¼‰ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„PPOç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16944v1">PDF</a> 16 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚å®ƒè¦æ±‚åŒæ—¶å¯¹æ¼”å‘˜å’Œè¯„è®ºå®¶è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„å›ºå®šå¥–åŠ±æ¨¡å‹è¿›è¡ŒæŒ‡å¯¼ï¼Œè¿™å¢åŠ äº†è®¡ç®—å¤æ‚æ€§å’Œä¸ç¨³å®šæ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å»è€¦ä»·å€¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDVPOï¼‰ï¼Œç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å…¨çƒä»·å€¼æ¨¡å‹ï¼ˆGVMï¼‰ä»£æ›¿ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDVPOåœ¨æ•ˆç‡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„RLHFæ–¹æ³•ï¼ŒåŒæ—¶è¾¾åˆ°äº†PPOçš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PPO-based RLHFåœ¨LLMä¸äººç±»åå¥½å¯¹é½ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†éœ€è¦å¤æ‚çš„è”åˆè®­ç»ƒå’Œå›ºå®šçš„å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>æ¼”å‘˜å’Œè¯„è®ºå®¶çš„è”åˆè®­ç»ƒå¢åŠ äº†è®¡ç®—å¤æ‚æ€§å’Œä¸ç¨³å®šæ€§ã€‚</li>
<li>PPOåœ¨LLMä»»åŠ¡ä¸­æ— æ³•è·å¾—çœŸå®çš„ç¯å¢ƒå¥–åŠ±ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§ã€‚</li>
<li>DVPOé€šè¿‡å¼•å…¥å…¨çƒä»·å€¼æ¨¡å‹ï¼ˆGVMï¼‰è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œè¯¥æ¨¡å‹åŸºäºæ”¿ç­–è½¨è¿¹é¢„æµ‹tokençº§åˆ«çš„å›æŠ¥ä¼°è®¡ã€‚</li>
<li>DVPOæ¶ˆé™¤äº†æ¼”å‘˜å’Œè¯„è®ºå®¶ä¹‹é—´çš„ä¾èµ–æ€§ï¼Œé™ä½äº†GPUå†…å­˜ä½¿ç”¨é‡å’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDVPOåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„RLHFæ–¹æ³•ï¼ŒåŒæ—¶ä¸å½“å‰æœ€ä½³çš„PPOç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-160aab96acc9e3ac416483ba9cd486dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9d046c2c16294747e4cb41a2579b0c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-General-Preference-Alignment-via-Optimistic-Online-Mirror-Descent"><a href="#Improving-LLM-General-Preference-Alignment-via-Optimistic-Online-Mirror-Descent" class="headerlink" title="Improving LLM General Preference Alignment via Optimistic Online Mirror   Descent"></a>Improving LLM General Preference Alignment via Optimistic Online Mirror   Descent</h2><p><strong>Authors:Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1&#x2F;2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¾ç¤ºå‡ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚è®¸å¤šç°æœ‰çš„å¯¹é½æ–¹æ³•ä¾èµ–äºBradley-Terryï¼ˆBTï¼‰æ¨¡å‹å‡è®¾ï¼Œè¯¥å‡è®¾å‡è®¾æ¯ä¸ªæç¤º-å“åº”å¯¹éƒ½å­˜åœ¨ä¸€ä¸ªçœŸå®å¥–åŠ±ã€‚ç„¶è€Œï¼Œåœ¨æ¨¡æ‹Ÿå¤æ‚çš„äººç±»åå¥½æ—¶ï¼Œè¿™ä¸ªå‡è®¾å¯èƒ½ä¼šè¿‡äºé™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¾å¼ƒäº†BTæ¨¡å‹å‡è®¾ï¼Œå¹¶åœ¨ä¸€èˆ¬åå¥½ä¸‹ç ”ç©¶LLMå¯¹é½é—®é¢˜ï¼Œå°†å…¶åˆ¶å®šä¸ºä¸¤äººæ¸¸æˆã€‚å€Ÿé‰´æ¸¸æˆå­¦ä¹ çš„ç†è®ºè§è§£ï¼Œæˆ‘ä»¬å°†ä¹è§‚çš„åœ¨çº¿é•œåƒä¸‹é™æ³•çº³å…¥æˆ‘ä»¬çš„å¯¹é½æ¡†æ¶æ¥è¿‘ä¼¼çº³ä»€ç­–ç•¥ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¯¹å¶é—´éš™çš„O(T^-1)ç•Œï¼Œæ”¹è¿›äº†ä¹‹å‰çš„O(T^-1&#x2F;2)ç»“æœã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å®æ–½äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å®ƒåœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€æ–°çš„RLHFç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16852v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚è®¸å¤šç°æœ‰å¯¹é½æ–¹æ³•ä¾èµ–äºå¸ƒé›·å¾·åˆ©-ç‰¹é›·ï¼ˆBTï¼‰æ¨¡å‹å‡è®¾ï¼Œå³æ¯ä¸ªæç¤º-å“åº”å¯¹éƒ½å­˜åœ¨ä¸€ä¸ªçœŸå®çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œå½“æ¨¡æ‹Ÿå¤æ‚çš„äººç±»åå¥½æ—¶ï¼Œè¿™ä¸€å‡è®¾å¯èƒ½è¿‡äºé™åˆ¶ã€‚æœ¬æ–‡æ”¾å¼ƒBTæ¨¡å‹å‡è®¾ï¼Œåœ¨é€šç”¨åå¥½ä¸‹ç ”ç©¶LLMå¯¹é½é—®é¢˜ï¼Œå°†å…¶è¡¨è¿°ä¸ºä¸¤äººæ¸¸æˆã€‚ç»“åˆæ¸¸æˆä¸­å­¦ä¹ çš„ç†è®ºè§è§£ï¼Œæˆ‘ä»¬å°†ä¹è§‚çš„åœ¨çº¿é•œåƒä¸‹é™æ³•é›†æˆåˆ°æˆ‘ä»¬çš„å¯¹é½æ¡†æ¶ä¸­ï¼Œä»¥è¿‘ä¼¼çº³ä»€ç­–ç•¥ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¯¹å¶é—´éš™çš„O(T^-1)è¾¹ç•Œï¼Œæ”¹è¿›äº†ä¹‹å‰çš„O(T^-1&#x2F;2)ç»“æœã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€å…ˆè¿›çš„RLHFç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ä¸Šæ•ˆæœæ˜¾è‘—ã€‚</li>
<li>ç°æœ‰å¯¹é½æ–¹æ³•å¸¸ä¾èµ–å¸ƒé›·å¾·åˆ©-ç‰¹é›·ï¼ˆBTï¼‰æ¨¡å‹å‡è®¾ï¼Œä½†åœ¨å¤æ‚äººç±»åå¥½æ¨¡æ‹Ÿä¸Šå¯èƒ½è¿‡äºé™åˆ¶ã€‚</li>
<li>æœ¬ç ”ç©¶æ”¾å¼ƒBTæ¨¡å‹å‡è®¾ï¼Œåœ¨é€šç”¨åå¥½ä¸‹ç ”ç©¶LLMå¯¹é½ï¼Œå°†å…¶è¡¨è¿°ä¸ºä¸¤äººæ¸¸æˆã€‚</li>
<li>æ•´åˆä¹è§‚çš„åœ¨çº¿é•œåƒä¸‹é™æ³•æ¥è¿‘ä¼¼çº³ä»€ç­–ç•¥ã€‚</li>
<li>ç†è®ºä¸Šå®ç°å¯¹å¶é—´éš™çš„O(T^-1)è¾¹ç•Œçš„çªç ´ï¼Œä¼˜äºä¹‹å‰çš„O(T^-1&#x2F;2)ç»“æœã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€å…ˆè¿›çš„RLHFç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01fb35e3d2b4b98edf2acad9a098c1ca.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="IPO-Your-Language-Model-is-Secretly-a-Preference-Classifier"><a href="#IPO-Your-Language-Model-is-Secretly-a-Preference-Classifier" class="headerlink" title="IPO: Your Language Model is Secretly a Preference Classifier"></a>IPO: Your Language Model is Secretly a Preference Classifier</h2><p><strong>Authors:Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose Implicit Preference Optimization (IPO), an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„ä¸»è¦æ–¹æ³•ã€‚è™½ç„¶å®ƒèƒ½å¤Ÿè®©LLMå®ç°äººç±»çº§åˆ«çš„å¯¹é½ï¼Œä½†ç”±äºå…¶ä¾èµ–è®­ç»ƒå¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–äººç±»æ ‡æ³¨çš„åå¥½ï¼Œé€šå¸¸ä¼šå¸¦æ¥å¾ˆå¤§çš„è®¡ç®—å’Œè´¢åŠ¡æˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†éšå¼åå¥½ä¼˜åŒ–ï¼ˆIPOï¼‰è¿™ä¸€æ›¿ä»£æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆå¼LLMä½œä¸ºåå¥½åˆ†ç±»å™¨ï¼Œä»è€Œå‡å°‘äº†å¯¹å¤–éƒ¨äººç±»åé¦ˆæˆ–å¥–åŠ±æ¨¡å‹è·å¾—åå¥½çš„ä¾èµ–ã€‚æˆ‘ä»¬ä½¿ç”¨RewardBenchå¯¹LLMçš„åå¥½åˆ†ç±»èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯„ä¼°äº†ä¸åŒå¤§å°ã€æ¶æ„å’Œè®­ç»ƒæ°´å¹³çš„æ¨¡å‹ï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„å‡è®¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ä¸ºç»™å®šæŒ‡ä»¤ç”Ÿæˆå¤šä¸ªå“åº”å¹¶åˆ©ç”¨æ¨¡å‹æœ¬èº«ä½œä¸ºåŸºäºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒçš„åå¥½åˆ†ç±»å™¨ï¼Œç ”ç©¶äº†LLMçš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡IPOè®­ç»ƒçš„æ¨¡å‹åœ¨è·å¾—åå¥½æ–¹é¢çš„æ€§èƒ½ä¸åˆ©ç”¨æœ€æ–°å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16182v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¯¹é½äººç±»åå¥½çš„ä¸»è¦æ–¹æ³•ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥å®ç°äººç±»çº§åˆ«çš„å¯¹é½ï¼Œä½†ç”±äºä¾èµ–è®­ç»ƒå¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–äººç±»æ ‡æ³¨çš„åå¥½ï¼Œå®ƒå¸¸å¸¸å¸¦æ¥å·¨å¤§çš„è®¡ç®—å’Œè´¢åŠ¡æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºImplicit Preference Optimization (IPO)çš„æ›¿ä»£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¼LLMä½œä¸ºåå¥½åˆ†ç±»å™¨ï¼Œä»è€Œå‡å°‘å¯¹å¤–éƒ¨äººç±»åé¦ˆæˆ–å¥–åŠ±æ¨¡å‹çš„ä¾èµ–æ¥è·å¾—åå¥½ã€‚é€šè¿‡å¯¹ä¸åŒå¤§å°ã€æ¶æ„å’Œè®­ç»ƒçº§åˆ«çš„æ¨¡å‹è¿›è¡ŒRewardBenchçš„å…¨é¢è¯„ä¼°ï¼ŒéªŒè¯äº†å‡è®¾ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†LLMçš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ï¼Œé€šè¿‡ä¸ºç»™å®šæŒ‡ä»¤ç”Ÿæˆå¤šä¸ªå“åº”å¹¶åˆ©ç”¨æ¨¡å‹æœ¬èº«ä½œä¸ºåå¥½åˆ†ç±»å™¨è¿›è¡ŒåŸºäºDirect Preference Optimization (DPO)çš„è®­ç»ƒã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡IPOè®­ç»ƒçš„æ¨¡å‹åœ¨è·å¾—åå¥½æ–¹é¢çš„æ€§èƒ½ä¸åˆ©ç”¨æœ€æ–°å¥–åŠ±æ¨¡å‹çš„æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½äººç±»åå¥½çš„ä¸»è¦æ–¹æ³•ï¼Œä½†è®¡ç®—å’Œè´¢åŠ¡æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Implicit Preference Optimization (IPO)æ–¹æ³•ï¼Œåˆ©ç”¨ç”Ÿæˆå¼LLMä½œä¸ºåå¥½åˆ†ç±»å™¨ï¼Œå‡å°‘å¯¹å¤–éƒ¨åé¦ˆæˆ–å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ã€‚</li>
<li>é€šè¿‡RewardBenchè¯„ä¼°äº†ä¸åŒå¤§å°ã€æ¶æ„å’Œè®­ç»ƒçº§åˆ«çš„æ¨¡å‹åœ¨åå¥½åˆ†ç±»æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>IPOè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸åˆ©ç”¨æœ€æ–°å¥–åŠ±æ¨¡å‹çš„æ¨¡å‹åœ¨è·å¾—åå¥½æ–¹é¢ç›¸å½“ã€‚</li>
<li>æ¢è®¨äº†LLMçš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªå“åº”å¹¶åˆ©ç”¨æ¨¡å‹æœ¬èº«ä½œä¸ºåå¥½åˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒã€‚</li>
<li>IPOæ–¹æ³•å¯èƒ½é™ä½RLHFçš„æˆæœ¬ï¼Œå¹¶æé«˜LLMçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b62ecf2b743d65b930f623f374a1ac05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b2e2790dc6a1a2f72e8c8934c4f086f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d114f06efcf8e0eddad4cd7739377493.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2eb8aa35babe181879a79698dd72949e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e363189df2970c20d1ad7098f745a63b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepSeek-V3-GPT-4-Phi-4-and-LLaMA-3-3-generate-correct-code-for-LoRaWAN-related-engineering-tasks"><a href="#DeepSeek-V3-GPT-4-Phi-4-and-LLaMA-3-3-generate-correct-code-for-LoRaWAN-related-engineering-tasks" class="headerlink" title="DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for   LoRaWAN-related engineering tasks"></a>DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for   LoRaWAN-related engineering tasks</h2><p><strong>Authors:Daniel Fernandes, JoÃ£o P. Matos-Carvalho, Carlos M. Fernandes, Nuno Fachada</strong></p>
<p>This paper investigates the performance of 16 Large Language Models (LLMs) in automating LoRaWAN-related engineering tasks involving optimal placement of drones and received power calculation under progressively complex zero-shot, natural language prompts. The primary research question is whether lightweight, locally executed LLMs can generate correct Python code for these tasks. To assess this, we compared locally run models against state-of-the-art alternatives, such as GPT-4 and DeepSeek-V3, which served as reference points. By extracting and executing the Python functions generated by each model, we evaluated their outputs on a zero-to-five scale. Results show that while DeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller models-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance, underscoring the viability of lightweight alternatives. Other models exhibited errors stemming from incomplete understanding or syntactic issues. These findings illustrate the potential of LLM-based approaches for specialized engineering applications while highlighting the need for careful model selection, rigorous prompt design, and targeted domain fine-tuning to achieve reliable outcomes. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†16ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–LoRaWANç›¸å…³å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠæ— äººæœºçš„æœ€ä½³æ”¾ç½®å’Œé€æ­¥å¤æ‚çš„é›¶é•œå¤´è‡ªç„¶è¯­è¨€æç¤ºä¸‹çš„æ¥æ”¶åŠŸç‡è®¡ç®—ã€‚ä¸»è¦çš„ç ”ç©¶é—®é¢˜æ˜¯ï¼Œè½»é‡çº§çš„æœ¬åœ°æ‰§è¡ŒLLMæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆå®Œæˆè¿™äº›ä»»åŠ¡çš„æ­£ç¡®Pythonä»£ç ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†æœ¬åœ°è¿è¡Œæ¨¡å‹ä¸æœ€å‰æ²¿çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚GPT-4å’ŒDeepSeek-V3ï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¿™äº›æ–¹æ¡ˆä½œä¸ºå‚è€ƒç‚¹ã€‚é€šè¿‡æå–å¹¶æ‰§è¡Œæ¯ä¸ªæ¨¡å‹ç”Ÿæˆçš„Pythonå‡½æ•°ï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬çš„è¾“å‡ºè¿›è¡Œäº†ä»é›¶åˆ°äº”çš„è¯„åˆ†ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶DeepSeek-V3å’ŒGPT-4å§‹ç»ˆæä¾›äº†å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æŸäº›è¾ƒå°çš„æ¨¡å‹â€”â€”å°¤å…¶æ˜¯Phi-4å’ŒLLaMA-3.3â€”â€”ä¹Ÿè¡¨ç°å‡ºäº†å¼ºåŠ²çš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆçš„å¯èƒ½æ€§ã€‚å…¶ä»–æ¨¡å‹å‡ºç°çš„é”™è¯¯æºäºç†è§£ä¸å…¨é¢æˆ–å¥æ³•é—®é¢˜ã€‚è¿™äº›å‘ç°è¡¨æ˜äº†åŸºäºLLMçš„æ–¹æ³•åœ¨ä¸“ä¸šåŒ–å·¥ç¨‹åº”ç”¨ä¸­çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†å®ç°å¯é ç»“æœéœ€è¦è°¨æ…é€‰æ‹©æ¨¡å‹ã€ä¸¥æ ¼è®¾è®¡æç¤ºä»¥åŠæœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14926v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†16ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–LoRaWANç›¸å…³å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ— äººæœºæœ€ä¼˜æ”¾ç½®å’Œæ¥æ”¶åŠŸç‡è®¡ç®—ç­‰ä»»åŠ¡ã€‚æ–‡ç« æ¢è®¨äº†è½»é‡çº§æœ¬åœ°æ‰§è¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç”Ÿæˆç”¨äºè¿™äº›ä»»åŠ¡çš„æ­£ç¡®Pythonä»£ç ã€‚é€šè¿‡ä¸GPT-4å’ŒDeepSeek-V3ç­‰å°–ç«¯æ¨¡å‹è¿›è¡Œå¯¹æ¯”è¯„ä¼°ï¼Œå‘ç°æŸäº›å°å‹æ¨¡å‹å¦‚Phi-4å’ŒLLaMA-3.3ä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚ä½†éƒ¨åˆ†æ¨¡å‹å› ç†è§£ä¸å…¨é¢æˆ–è¯­æ³•é—®é¢˜è€Œäº§ç”Ÿé”™è¯¯ã€‚è¿™è¡¨æ˜åœ¨ç‰¹å®šå·¥ç¨‹åº”ç”¨ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰æ½œåŠ›ï¼Œä½†ä»éœ€è°¨æ…é€‰æ‹©æ¨¡å‹ã€è®¾è®¡æç¤ºä»¥åŠé’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°å¯é çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–LoRaWANç›¸å…³å·¥ç¨‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ä¸»è¦æ¢è®¨äº†è½»é‡çº§æœ¬åœ°æ‰§è¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆæ­£ç¡®Pythonä»£ç å®Œæˆè¿™äº›ä»»åŠ¡ã€‚</li>
<li>å¯¹æ¯”è¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬å°–ç«¯æ¨¡å‹å¦‚GPT-4å’ŒDeepSeek-V3ä»¥åŠå°å‹æ¨¡å‹å¦‚Phi-4å’ŒLLaMA-3.3ã€‚</li>
<li>DeepSeek-V3å’ŒGPT-4æä¾›å‡†ç¡®è§£å†³æ–¹æ¡ˆï¼Œè€Œéƒ¨åˆ†å°å‹æ¨¡å‹ä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>éƒ¨åˆ†æ¨¡å‹å­˜åœ¨å› ç†è§£ä¸å…¨é¢æˆ–è¯­æ³•é—®é¢˜è€Œå¯¼è‡´çš„é”™è¯¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6087c41f411f173c2284e1508267583e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Drift-Decoding-time-Personalized-Alignments-with-Implicit-User-Preferences"><a href="#Drift-Decoding-time-Personalized-Alignments-with-Implicit-User-Preferences" class="headerlink" title="Drift: Decoding-time Personalized Alignments with Implicit User   Preferences"></a>Drift: Decoding-time Personalized Alignments with Implicit User   Preferences</h2><p><strong>Authors:Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Kyomin Jung</strong></p>
<p>Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable. </p>
<blockquote>
<p>é’ˆå¯¹ä¸ªäººç”¨æˆ·çš„ä¸ªæ€§åŒ–å¯¹é½åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ˜¯ä¸€ä¸ªé•¿æœŸç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†Driftï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è§£ç æ—¶é—´å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®çš„æ–°æ¡†æ¶ï¼Œå¹¶è€ƒè™‘äº†éšå«çš„ç”¨æˆ·åå¥½ã€‚ä¼ ç»Ÿçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰éœ€è¦æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬å’Œæ˜‚è´µçš„æ¢¯åº¦æ›´æ–°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDriftä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼ä¸ªæ€§åŒ–LLMï¼Œä»…ä½¿ç”¨å‡ åä¸ªæ ·æœ¬å°±èƒ½é€šè¿‡é«˜æ•ˆåå¥½å»ºæ¨¡æ¥å¼•å¯¼å†»ç»“æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ºé¢„å®šä¹‰çš„å¯è§£é‡Šå±æ€§çš„ç»„åˆï¼Œå¹¶åœ¨è§£ç æ—¶é—´ä¸ä¸ªæ€§åŒ–ç”Ÿæˆå¯¹é½ã€‚åœ¨åˆæˆäººæ ¼æ•°æ®é›†ï¼ˆPerspectiveï¼‰å’ŒçœŸå®äººç±»æ ‡æ³¨æ•°æ®é›†ï¼ˆPRISMï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä½¿ç”¨ä»…50-100ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒDriftæ˜¾è‘—ä¼˜äºRLHFåŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœå’Œåˆ†æè¡¨æ˜ï¼ŒDriftåœ¨è®¡ç®—ä¸Šæ˜¯é«˜æ•ˆçš„ï¼Œå¹¶ä¸”æ˜¯å¯è§£é‡Šçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14289v2">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDriftçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è§£ç æ—¶é—´å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ï¼Œé€šè¿‡éšå¼ç”¨æˆ·åå¥½å®ç°ä¸ªæ€§åŒ–å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦é€šè¿‡æˆåƒä¸Šä¸‡æ³¨è§£ç¤ºä¾‹å’Œæ˜‚è´µæ¢¯åº¦æ›´æ–°æ¥å®ç°çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸åŒï¼ŒDriftä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼ä¸ªæ€§åŒ–LLMsï¼Œä»…ä½¿ç”¨å‡ åä¸ªç¤ºä¾‹é€šè¿‡é«˜æ•ˆåå¥½å»ºæ¨¡æ¥å¼•å¯¼å†»ç»“æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ºé¢„å®šä¹‰ã€å¯è§£é‡Šå±æ€§çš„ç»„åˆï¼Œå¹¶åœ¨è§£ç æ—¶é—´å¯¹å…¶è¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°ä¸ªæ€§åŒ–ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨åˆæˆäººæ ¼æ•°æ®é›†ï¼ˆPerspectiveï¼‰è¿˜æ˜¯çœŸå®äººç±»æ³¨é‡Šæ•°æ®é›†ï¼ˆPRISMï¼‰ä¸Šï¼ŒDriftåœ¨ä»…ä½¿ç”¨50-100ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºRLHFåŸºçº¿ã€‚åˆ†æå’Œç»“æœè¯æ˜ï¼ŒDriftæ—¢è®¡ç®—é«˜æ•ˆåˆå…·æœ‰å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Driftæ˜¯ä¸€ä¸ªæ–°å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è§£ç æ—¶é—´å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ã€‚</li>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰éœ€è¦å¤§é‡æ³¨è§£ç¤ºä¾‹å’Œæ¢¯åº¦æ›´æ–°ï¼Œè€ŒDriftåªéœ€å°‘é‡ç¤ºä¾‹å°±èƒ½å¼•å¯¼æ¨¡å‹ã€‚</li>
<li>Drifté€šè¿‡éšå¼ç”¨æˆ·åå¥½å®ç°ä¸ªæ€§åŒ–å¯¹é½ã€‚</li>
<li>Driftå°†ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ºé¢„å®šä¹‰ã€å¯è§£é‡Šå±æ€§çš„ç»„åˆã€‚</li>
<li>åœ¨åˆæˆäººæ ¼æ•°æ®é›†å’ŒçœŸå®äººç±»æ³¨é‡Šæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDriftæ˜¾è‘—ä¼˜äºRLHFåŸºçº¿ã€‚</li>
<li>Driftæ—¢è®¡ç®—é«˜æ•ˆï¼Œä¹Ÿå…·å¤‡å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df780cbf4f6c3b0b5ca6db8726a8b9dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87f84db1eeb73bb551d442218fe2bb8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cb768f405e4dde00b8447152e5e439.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2fab063af24aaa30cb0afbdd07cf7e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d38e8e864b7dec2a197b6446df29ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-042eecc385a19f5fef3f6c6e2712a1ff.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="None-of-the-Others-a-General-Technique-to-Distinguish-Reasoning-from-Memorization-in-Multiple-Choice-LLM-Evaluation-Benchmarks"><a href="#None-of-the-Others-a-General-Technique-to-Distinguish-Reasoning-from-Memorization-in-Multiple-Choice-LLM-Evaluation-Benchmarks" class="headerlink" title="None of the Others: a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks"></a>None of the Others: a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks</h2><p><strong>Authors:Eva SÃ¡nchez Salido, Julio Gonzalo, Guillermo Marco</strong></p>
<p>In LLM evaluations, reasoning is often distinguished from recall&#x2F;memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall&#x2F;memorization in current LLMsâ€™ answers. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°ä¸­ï¼Œæ¨ç†é€šå¸¸é€šè¿‡å¯¹æ•°å­¦å¯¼å‘é—®é¢˜æ‰§è¡Œæ•°å€¼å˜åŒ–æ¥ä¸å›å¿†&#x2F;è®°å¿†è¿›è¡ŒåŒºåˆ†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å¤šé€‰é¢˜çš„ä¸€èˆ¬å˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®Œå…¨æ¶ˆé™¤æ­£ç¡®ç­”æ¡ˆä¸å…ˆå‰è§è¿‡çš„æ ‡è®°æˆ–æ¦‚å¿µçš„è”ç³»ï¼Œè¦æ±‚å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç†è§£å’Œæ¨ç†ï¼ˆè€Œä¸æ˜¯è®°å¿†ï¼‰ä»¥æ­£ç¡®å›ç­”é—®é¢˜ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå¯ç”¨è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æœ€æ–°ç§æœ‰å’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼šå…¬å¼€çš„MMLUåŸºå‡†æµ‹è¯•å’Œç§æœ‰çš„UNED-Access 2024æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨æˆ‘ä»¬æå‡ºçš„å˜ä½“ä¸‹éƒ½å‡ºç°äº†æ˜¾è‘—å‡†ç¡®ç‡ä¸‹é™çš„æƒ…å†µï¼Œå…¶ä¸­MMLUä¸Šçš„å¹³å‡æŸå¤±ä¸º57%ï¼ŒUNED-Access 2024ä¸Šçš„å¹³å‡æŸå¤±ä¸º50%ï¼Œä¸åŒæ¨¡å‹ä¹‹é—´çš„æŸå¤±èŒƒå›´ä»10%åˆ°93%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æˆ‘ä»¬å®éªŒä¸­è¡¨ç°æœ€å‡†ç¡®çš„æ¨¡å‹ï¼ˆOpenAI-o3-miniï¼‰å¹¶éæœ€ç¨³å¥ï¼ˆDeepSeek-R1-70Bï¼‰ï¼Œè¿™è¡¨æ˜åœ¨æ ‡å‡†è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¯èƒ½å¹¶ä¸å…·å¤‡æœ€ä½³çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å…¬å¼€æ•°æ®é›†ï¼ˆä¸ç§æœ‰æ•°æ®é›†ç›¸æ¯”ï¼‰ä»¥åŠåŸå§‹è¯­è¨€ï¼ˆä¸æ‰‹åŠ¨ç¿»è¯‘ç›¸æ¯”ï¼‰æå‡ºçš„é—®é¢˜å‡†ç¡®ç‡ä¸‹é™å¹…åº¦æ›´å¤§ï¼Œè¿™æ˜¯æ±¡æŸ“çš„è¿¹è±¡ï¼Œä¹Ÿè¡¨æ˜å›å¿†å’Œè®°å¿†åœ¨å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ç­”æ¡ˆä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12896v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤šé€‰é¢˜çš„ä¸€èˆ¬å˜å¼‚æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿æ­£ç¡®ç­”æ¡ˆä¸å…ˆå‰è§è¿‡çš„æ ‡è®°æˆ–æ¦‚å¿µå®Œå…¨åˆ†ç¦»ï¼Œè¦æ±‚LLMç†è§£å’Œæ¨ç†ï¼ˆè€Œéè®°å¿†ï¼‰ä»¥æ­£ç¡®å›ç­”é—®é¢˜ã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›çš„ä¸“æœ‰å’Œå¼€æºLLMåœ¨è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹åœ¨æ–°æå‡ºçš„æ–¹æ³•ä¸‹å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œå¹³å‡æŸå¤±åˆ†åˆ«ä¸ºMMLUä¸Šçš„57%å’ŒUNED-Access 2024ä¸Šçš„50%ï¼Œå„æ¨¡å‹é—´æŸå¤±èŒƒå›´ä»10%åˆ°93%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å®éªŒä¸­æœ€å‡†ç¡®çš„æ¨¡å‹ï¼ˆOpenAI-o3-miniï¼‰å¹¶éæœ€ç¨³å¥ï¼ˆDeepSeek-R1-70Bï¼‰ï¼Œè¿™æç¤ºæˆ‘ä»¬æ ‡å‡†è¯„ä¼°ä¸­æœ€ä¼˜ç§€çš„æ¨¡å‹å¯èƒ½å¹¶ä¸å…·å¤‡æœ€ä½³çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå…¬å¼€æ•°æ®é›†çš„å‡†ç¡®ç‡ä¸‹é™å¹…åº¦å¤§äºç§æœ‰æ•°æ®é›†ï¼ŒåŸå§‹è¯­è¨€çš„é—®é¢˜è¾ƒæ‰‹åŠ¨ç¿»è¯‘çš„é—®é¢˜å‡†ç¡®ç‡ä¸‹é™å¹…åº¦æ›´å¤§ï¼Œè¿™è¡¨æ˜æ±¡æŸ“ç°è±¡çš„å­˜åœ¨ä»¥åŠå›å¿†å’Œè®°å¿†åœ¨å½“å‰LLMç­”æ¡ˆä¸­çš„é‡è¦ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§å¤šé€‰é¢˜çš„ä¸€èˆ¬å˜å¼‚æ–¹æ³•ï¼Œå¼ºè°ƒç†è§£å’Œæ¨ç†è€Œéè®°å¿†ã€‚</li>
<li>æ‰€æœ‰è¯„ä¼°çš„LLMæ¨¡å‹åœ¨æ–°æ–¹æ³•ä¸‹å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æœ€å‡†ç¡®çš„æ¨¡å‹åœ¨å®éªŒä¸­æœªå¿…æœ€ç¨³å¥ï¼Œæç¤ºä¼˜ç§€è¯„ä¼°æ¨¡å‹å¯èƒ½å¹¶ä¸å…·å¤‡æœ€ä½³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†çš„å‡†ç¡®ç‡ä¸‹é™å¹…åº¦å¤§äºç§æœ‰æ•°æ®é›†ï¼Œæ˜¾ç¤ºæ±¡æŸ“ç°è±¡çš„å­˜åœ¨ã€‚</li>
<li>åŸå§‹è¯­è¨€é—®é¢˜çš„å‡†ç¡®ç‡è¾ƒæ‰‹åŠ¨ç¿»è¯‘çš„é—®é¢˜ä¸‹é™å¹…åº¦æ›´å¤§ã€‚</li>
<li>ç»“æœè¡¨æ˜å›å¿†å’Œè®°å¿†åœ¨å½“å‰LLMç­”æ¡ˆä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbe7915e89d231d83d2a557cd6fc26c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdcdf060744bcbc96773425b747199fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a565b3f90516e90a97a2b97a3befb5c7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Hidden-Risks-of-Large-Reasoning-Models-A-Safety-Assessment-of-R1"><a href="#The-Hidden-Risks-of-Large-Reasoning-Models-A-Safety-Assessment-of-R1" class="headerlink" title="The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1"></a>The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1</h2><p><strong>Authors:Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, Xin Eric Wang</strong></p>
<p>The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the modelâ€™s reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 modelsâ€™ safety to close the gap. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚OpenAI-o3å’ŒDeepSeek-R1ï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä½¿å¾—å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒéæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„å¢å¼ºåŠŸèƒ½ï¼Œä»¥åŠDeepSeek-R1ç­‰æ¨¡å‹çš„å¼€æºè®¿é—®ï¼Œå¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ï¼Œå°¤å…¶æ˜¯å…³äºå…¶å¯èƒ½è¢«æ»¥ç”¨çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ¨ç†æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®‰å…¨è¯„ä¼°ï¼Œåˆ©ç”¨æ—¢å®šçš„å®‰å…¨åŸºå‡†æ¥è¯„ä¼°å…¶ç¬¦åˆå®‰å…¨æ³•è§„çš„æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è°ƒæŸ¥äº†å®ƒä»¬å¯¹å¯¹æŠ—æ€§æ”»å‡»ï¼ˆå¦‚è¶Šç‹±å’Œæç¤ºæ³¨å…¥ï¼‰çš„æ˜“æ„Ÿæ€§ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç¨³å¥æ€§ã€‚é€šè¿‡æˆ‘ä»¬å¤šæ–¹é¢çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†å››ä¸ªå…³é”®å‘ç°ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰åœ¨å®‰å…¨æ€§åŸºå‡†å’Œæ”»å‡»æ–¹é¢ï¼Œå¼€æºR1æ¨¡å‹ä¸o3-miniæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„å®‰å…¨å·®è·ï¼Œè¿™è¡¨æ˜éœ€è¦åœ¨R1ä¸ŠæŠ•å…¥æ›´å¤šçš„å®‰å…¨åŠªåŠ›ã€‚</p>
<p>ï¼ˆ2ï¼‰è’¸é¦æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§èƒ½è¾ƒå…¶å®‰å…¨å¯¹é½çš„åŸºç¡€æ¨¡å‹è¦å·®ã€‚</p>
<p>ï¼ˆ3ï¼‰æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œå½“å®ƒå›ç­”ä¸å®‰å…¨çš„é—®é¢˜æ—¶å¯èƒ½é€ æˆçš„æ½œåœ¨å±å®³å°±è¶Šå¤§ã€‚</p>
<p>ï¼ˆ4ï¼‰R1æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æ¯”å…¶æœ€ç»ˆç­”æ¡ˆå¸¦æ¥æ›´å¤§çš„å®‰å…¨éšæ‚£ã€‚æˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†å¯¹æ¨ç†æ¨¡å‹çš„å®‰å…¨å½±å“çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†R1æ¨¡å‹å®‰å…¨æ€§çš„è¿›ä¸€æ­¥å‘å±•çš„å¿…è¦æ€§ï¼Œä»¥ç¼©å°å·®è·ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12659v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹å¦‚OpenAI-o3å’ŒDeepSeek-R1çš„å¿«é€Ÿå‘å±•æ˜¾è‘—æå‡äº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œç›¸è¾ƒäºéæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç„¶è€Œï¼Œå…¶å¢å¼ºèƒ½åŠ›ä¸DeepSeek-R1ç­‰æ¨¡å‹çš„å¼€æºè®¿é—®ç›¸ç»“åˆï¼Œå¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å…³äºæ½œåœ¨è¯¯ç”¨çš„é£é™©ã€‚æœ¬ç ”ç©¶å¯¹è¿™äº›æ¨ç†æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®‰å…¨è¯„ä¼°ï¼Œåˆ©ç”¨æ—¢å®šçš„å®‰å…¨åŸºå‡†è¯„ä¼°å…¶ç¬¦åˆå®‰å…¨æ³•è§„çš„æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å®ƒä»¬å¯¹è¶Šç‹±å’Œæç¤ºæ³¨å…¥ç­‰å¯¹æŠ—æ€§æ”»å‡»çš„æ˜“æ„Ÿæ€§ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç¨³å¥æ€§ã€‚ç ”ç©¶å‘ç°å››å¤§å…³é”®ç‚¹ï¼šä¸€ã€å¼€æºR1æ¨¡å‹ä¸o3-miniæ¨¡å‹åœ¨å®‰å…¨åŸºå‡†å’Œæ”»å‡»æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å®‰å…¨å·®è·ï¼Œè¡¨æ˜éœ€è¦åœ¨R1ä¸ŠåŠ å¤§å®‰å…¨æŠ•å…¥ï¼›äºŒã€è’¸é¦æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§èƒ½è¾ƒå·®ï¼Œç›¸è¾ƒäºå…¶å®‰å…¨å¯¹é½çš„åŸºç¡€æ¨¡å‹ï¼›ä¸‰ã€æ¨¡å‹æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œå›ç­”ä¸å®‰å…¨é—®é¢˜æ—¶å¯èƒ½é€ æˆçš„æ½œåœ¨å±å®³è¶Šå¤§ï¼›å››ã€R1æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æ¯”æœ€ç»ˆç­”æ¡ˆæ›´å¼•å‘å®‰å…¨æ‹…å¿§ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†æ¨ç†æ¨¡å‹çš„å®‰å…¨å½±å“ï¼Œå¹¶å¼ºè°ƒäº†R1æ¨¡å‹å®‰å…¨æ€§çš„è¿›ä¸€æ­¥å‘å±•çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹åœ¨å®‰å…¨æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼€æºæ¨¡å‹çš„å®‰å…¨æ€§èƒ½æœ‰å¾…æå‡ã€‚</li>
<li>è’¸é¦æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§èƒ½ç›¸å¯¹è¾ƒå¼±ã€‚</li>
<li>æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸å…¶æ½œåœ¨å®‰å…¨é£é™©æˆæ­£æ¯”ã€‚</li>
<li>æ¨ç†æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æ¯”æœ€ç»ˆç­”æ¡ˆæ›´å¯èƒ½å¼•å‘å®‰å…¨é—®é¢˜ã€‚</li>
<li>éœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥å®Œå–„æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§èƒ½ã€‚</li>
<li>å¯¹æŠ—æ€§æ”»å‡»å¯¹æ¨ç†æ¨¡å‹çš„ç¨³å¥æ€§æ„æˆæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3612ce78f9d9c598c812cda0ad4abc3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-506d44a1867353dd2f156b40417904c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f62d73c25eb41a00f33baa29bad529.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31319bcc18ac226602c9a0b9a3b05808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acaea51895f3263dcf7b0dae46433a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d766e0449473c1fabb1df63f479dacfa.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning"><a href="#EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning"></a>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</strong></p>
<p>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPOâ€™s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®šä¹‰æ˜ç¡®ã€è§£å†³æ–¹æ¡ˆæ¸…æ™°çš„é—®é¢˜ä¸­å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨æ•°å­¦å’Œç¼–ç¨‹é¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œåœºæ™¯ï¼Œå¦‚å•†åŠ¡è°ˆåˆ¤ç­‰ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›åœºæ™¯è¦æ±‚æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œå³åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªå¹¶åº”å¯¹ä¸ç¡®å®šæ€§çš„é•¿æœŸç›®æ ‡ã€‚ç°æœ‰çš„æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´ç€é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æˆ˜ç•¥æ¨ç†çš„æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨LLMåœ¨å¼€æ”¾åŠ¨ä½œç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶èƒ½æ’å…¥åˆ°ä»»æ„çš„LLMä»£ç†ä¸­ä»¥æ¿€åŠ±ç›®æ ‡å¯¼å‘çš„è¡Œä¸ºã€‚ä¸ºäº†æ”¹å–„é€‚åº”æ€§å’Œç­–ç•¥è½¬ç§»æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘åšå¼ˆï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºåˆæ­¥æ­¥éª¤ã€‚åœ¨ç¤¾ä¼šå’Œç‰©ç†é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒEPOé€šè¿‡å¢å¼ºæˆ˜ç•¥æ¨ç†èƒ½åŠ›å®ç°é•¿æœŸç›®æ ‡å¯¹é½ï¼Œåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†EPOä¸­æ¶Œç°çš„å„ç§åä½œæ¨ç†æœºåˆ¶ï¼Œä»¥åŠå…¶åœ¨ç”Ÿæˆæ–°ç­–ç•¥æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æˆ˜ç•¥æ¨ç†æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12486v2">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰å…·æœ‰æ˜ç¡®ç­”æ¡ˆçš„é—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å•†ä¸šè°ˆåˆ¤ç­‰å¤æ‚ç°å®åœºæ™¯ä¸­çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚æˆ˜ç•¥æ¨ç†éœ€è¦é€‚åº”åŠ¨æ€ç¯å¢ƒï¼Œå¹¶åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹å®ç°é•¿æœŸç›®æ ‡ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è¿ç§»çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºæ˜¾æ€§ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰ç”¨äºæˆ˜ç•¥æ¨ç†ï¼Œå€ŸåŠ©LLMåœ¨å¼€æ”¾è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶å¯ä¸ä»»æ„LLMä»£ç†ç›¸ç»“åˆä»¥å¼•å¯¼ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œåˆ©ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘å¯¹æŠ—æ¥æé«˜é€‚åº”æ€§å’Œç­–ç•¥è¿ç§»èƒ½åŠ›ï¼Œæ— éœ€é¢„å…ˆè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒEPOåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šå®ç°é•¿æœŸç›®æ ‡å¯¹é½çš„å¢å¼ºæˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°ä¸šç•Œæœ€ä½³è¡¨ç°ã€‚å‘ç°EPOä¸­çš„å¤šç§åä½œæ¨ç†æœºåˆ¶å’Œç”Ÿæˆæ–°ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨ç°å®åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ˜ç¡®ç­”æ¡ˆçš„é—®é¢˜ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ç°å®åœºæ™¯å¦‚å•†ä¸šè°ˆåˆ¤ä¸­çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›å—é™ã€‚</li>
<li>æˆ˜ç•¥æ¨ç†éœ€è¦é€‚åº”åŠ¨æ€ç¯å¢ƒï¼Œå¹¶åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹å®ç°é•¿æœŸç›®æ ‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è¿ç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>æ˜¾æ€§ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œç»“åˆLLMå’Œå¼ºåŒ–å­¦ä¹ æ¥è¿›è¡Œæˆ˜ç•¥æ¨ç†ã€‚</li>
<li>EPOå¯ä»¥åœ¨å¼€æ”¾è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶å¯ä¸ä»»æ„LLMä»£ç†ç»“åˆä»¥å¼•å¯¼ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚</li>
<li>é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œç­–ç•¥è¿ç§»èƒ½åŠ›ï¼Œæ— éœ€é¢„å…ˆè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e8a74b16f086370e7760f828be8a4aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b362a1bd20af45a1de73d92c66aa409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f087bb632fb79e939e2ff04b4f3fcd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97694c8c8837f774c6734c675b5d24d4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-23/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-23/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-95b2c5cae75f3c8c7bee2c937d247270.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-23  Dita Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ae6447232f8dbbef2e71dab4cffebc4a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  UniSync A Unified Framework for Audio-Visual Synchronization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
