<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-23  KnowLogic A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e57fb8e24e7ccce9a397d7d46dd75864.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-23-更新"><a href="#2025-03-23-更新" class="headerlink" title="2025-03-23 更新"></a>2025-03-23 更新</h1><h2 id="KnowLogic-A-Benchmark-for-Commonsense-Reasoning-via-Knowledge-Driven-Data-Synthesis"><a href="#KnowLogic-A-Benchmark-for-Commonsense-Reasoning-via-Knowledge-Driven-Data-Synthesis" class="headerlink" title="KnowLogic: A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis"></a>KnowLogic: A Benchmark for Commonsense Reasoning via Knowledge-Driven   Data Synthesis</h2><p><strong>Authors:Weidong Zhan, Yue Wang, Nan Hu, Liming Xiao, Jingyuan Ma, Yuhang Qin, Zheng Li, Yixin Yang, Sirui Deng, Jinkun Ding, Wenhan Ma, Rui Li, Weilin Luo, Qun Liu, Zhifang Sui</strong></p>
<p>Current evaluations of commonsense reasoning in LLMs are hindered by the scarcity of natural language corpora with structured annotations for reasoning tasks. To address this, we introduce KnowLogic, a benchmark generated through a knowledge-driven synthetic data strategy. KnowLogic integrates diverse commonsense knowledge, plausible scenarios, and various types of logical reasoning. One of the key advantages of KnowLogic is its adjustable difficulty levels, allowing for flexible control over question complexity. It also includes fine-grained labels for in-depth evaluation of LLMs’ reasoning abilities across multiple dimensions. Our benchmark consists of 3,000 bilingual (Chinese and English) questions across various domains, and presents significant challenges for current LLMs, with the highest-performing model achieving only 69.57%. Our analysis highlights common errors, such as misunderstandings of low-frequency commonsense, logical inconsistencies, and overthinking. This approach, along with our benchmark, provides a valuable tool for assessing and enhancing LLMs’ commonsense reasoning capabilities and can be applied to a wide range of knowledge domains. </p>
<blockquote>
<p>目前对大语言模型（LLM）常识推理能力的评估受到缺乏结构化注释的推理任务自然语言语料库的阻碍。为解决这一问题，我们引入了KnowLogic，这是一个通过知识驱动合成数据策略生成的基准测试。KnowLogic融合了多样化的常识知识、合理场景和多种类型的逻辑推理。KnowLogic的一个关键优势是其可调整的难度级别，可以控制问题的复杂性。它还包含用于深入评估LLM推理能力多个维度的精细标签。我们的基准测试包含3000个双语（中文和英文）问题，涵盖各个领域，对当前LLM提出了重大挑战，表现最好的模型只达到了69.57%。我们的分析强调了常见错误，如误解低频常识、逻辑不一致和过度思考。这种方法以及我们的基准测试为评估和增强LLM的常识推理能力提供了有价值的工具，并可应用于广泛的知识领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06218v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>针对大型语言模型（LLMs）的常识推理评估受限于缺乏结构化注释的推理任务自然语言语料库的问题，我们引入了KnowLogic这一通过知识驱动合成数据策略生成的基准测试。KnowLogic融合了多样化的常识知识、合理场景和各种类型的逻辑推理。其关键优势之一是可调节的难度级别，可灵活控制问题的复杂性。此外，它还包含用于深入评估LLMs多方面推理能力的精细标签。该基准测试包含3000个双语（中文和英文）问题，涉及多个领域，对当前LLMs提出了重大挑战，最高性能模型仅达到69.57%。我们的分析突出了常见的错误，如误解低频常识、逻辑不一致和过度思考。这一方法和我们的基准测试为评估和增强LLMs的常识推理能力提供了有价值的工具，并可应用于广泛的知识领域。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>常识推理评估在大型语言模型（LLMs）中受限，缺乏结构化注释的推理任务自然语言语料库是主要挑战。</li>
<li>KnowLogic是一个知识驱动合成数据策略的基准测试，旨在解决此挑战。</li>
<li>KnowLogic融合了多样化的常识知识、合理场景和多种逻辑推理类型。</li>
<li>KnowLogic具有可调节的难度级别，允许灵活控制问题的复杂性。</li>
<li>基准测试包含双语（中文和英文）问题，涉及多个领域，对当前LLMs提出了挑战。</li>
<li>最高性能模型在KnowLogic上的表现仅为69.57%，说明存在提升空间。</li>
<li>分析揭示了常见的错误类型，如误解低频常识、逻辑不一致和过度思考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e9f0f8938e6d08181d07f0841826205a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a99c604b73c49e9be6cedc9b7e29b2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-688a3e1b9cf7c7ae97fe046ac244e4eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9fcd9c755a317f07679701fce962e38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aaf0892cbad09950f85516b8d111190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ed915e4b15bca3c9f50ff53d4a863e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef275b1c224e72a4c8b8b22407c67a64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af9f4fc6522283b47a0950c6cc99a8af.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning"></a>R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. </p>
<blockquote>
<p>现有的大型推理模型（LRMs）已经显示出强化学习（RL）在增强大型语言模型（LLMs）的复杂推理能力方面的潜力。虽然它们在数学和编程等具有挑战性的任务上取得了显著的成绩，但它们往往依赖内部知识来解决问题，这在时间敏感或知识密集型问题上可能不足，导致不准确和幻觉。针对这一问题，我们提出了<strong>R1-Searcher</strong>，这是一种新型的两阶段成果导向的RL方法，旨在增强LLM的搜索能力。这种方法允许LLM在推理过程中自主调用外部搜索系统来访问额外的知识。我们的框架完全依赖于RL，无需启动过程奖励或蒸馏。实验证明，我们的方法显著优于先前的强大RAG方法，即使在对比闭源的GPT-4o-mini时也是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05592v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习（RL）在提升大型推理模型（LRMs）的复杂推理能力方面具有潜力。虽然现有模型在数学和编程等挑战性任务上表现出卓越性能，但它们往往依赖内部知识解决问题，对于时间敏感或知识密集型问题可能不足够准确并出现误判。为解决这一问题，提出一种名为R1-Searcher的新型两阶段结果导向的RL方法，旨在增强LLM的搜索能力，允许其自主调用外部搜索系统获取额外知识来完成推理过程。此方法仅依赖RL，无需过程奖励或冷启动时的知识蒸馏。实验证明，该方法显著优于先前的强大RAG方法，即使与闭源的GPT-4o-mini相比也是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习（RL）有助于提升大型推理模型（LRMs）的复杂推理能力。</li>
<li>现有模型在数学和编程任务上表现卓越，但在时间敏感或知识密集型问题上可能不准确。</li>
<li>R1-Searcher是一种新型的两阶段结果导向的RL方法，旨在增强LLM的搜索能力。</li>
<li>R1-Searcher允许LLM在推理过程中自主调用外部搜索系统获取额外知识。</li>
<li>该方法仅依赖RL，不需要过程奖励或冷启动时的知识蒸馏。</li>
<li>实验表明，R1-Searcher显著优于其他强大的RAG方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05592">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49204a03cd030bc522af99d46101df2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3092b293dc9d717edaa73b5635744227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603d2edf70630ae009a368b957e8eadc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing"><a href="#WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing" class="headerlink" title="WritingBench: A Comprehensive Benchmark for Generative Writing"></a>WritingBench: A Comprehensive Benchmark for Generative Writing</h2><p><strong>Authors:Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, Fei Huang</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework’s validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing. </p>
<blockquote>
<p>最近大型语言模型（LLM）的进展显著增强了文本生成能力，但在生成写作中评估其性能仍然是一个挑战。现有的基准测试主要集中在通用文本生成或有限的写作任务上，无法捕捉跨不同领域高质量内容的多样化要求。为了弥补这一差距，我们推出了WritingBench，这是一个全面的基准测试，旨在评估LLM在6个核心写作领域和100个子领域中的表现，涵盖创造性、说服力、信息性和技术写作。我们进一步提出了一个查询依赖评估框架，使LLM能够动态生成特定实例的评估标准。该框架通过微调后的批评模型进行标准感知评分，以风格、格式和长度为评价标准。该框架的有效性通过其数据整合能力得到了进一步证明，这可以使拥有数十亿参数的模型达到接近当前最新技术的性能水平。我们开源了基准测试，以及评估工具和模块化框架组件，以促进写作中的大型语言模型的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05244v3">PDF</a> </p>
<p><strong>Summary</strong><br>大语言模型（LLM）的近期进展极大地提升了文本生成能力，但评估其在写作方面的性能仍然是一个挑战。现有基准测试主要集中在通用文本生成或有限写作任务上，无法捕捉跨不同领域高质量内容的多样化要求。为了弥补这一差距，我们推出了WritingBench，这是一个旨在评估LLM在6个核心写作领域和100个子领域的综合基准测试，涵盖创造性、说服力、信息性和技术写作。我们还提出了一个查询依赖评估框架，使LLM能够动态生成特定实例的评估标准。该框架通过微调评论家模型进行标准感知评分，以风格、格式和长度进行评估。框架的有效性通过其数据收集能力进一步证明，使7B参数模型接近最新技术水平。我们开源基准测试、评估工具和模块化框架组件，以促进写作领域LLM的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在文本生成方面取得了显著进展，但评估其在写作方面的性能仍然是一个挑战。</li>
<li>现有基准测试无法全面评估LLM在不同写作领域的表现。</li>
<li>WritingBench是一个全新的综合基准测试，旨在评估LLM在6个核心写作领域和100个子领域的表现。</li>
<li>提出了查询依赖评估框架，使LLM能够动态生成特定实例的评估标准。</li>
<li>框架结合了微调评论家模型进行标准感知评分，涵盖风格、格式和长度。</li>
<li>框架的有效性通过其数据收集能力得到证明，使大型语言模型性能接近最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6412f51cc0a8703a34ab9ebdb5fb1e05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9d2679ce0909222c8237a98f10f37f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b383dbe5b18325eac07de6ef521b4099.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23cd85fb8bb759282c32fb7b4c8ffc19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e186f4c47e00c084054c23e80bde1c4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73f18a576e39f854e1b087442cab49d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc3d1baf1d6bda364f0136fad377c7d6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects"><a href="#Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects" class="headerlink" title="Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects"></a>Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects</h2><p><strong>Authors:Anichur Rahman, Shahariar Hossain Mahir, Md Tanjum An Tashrif, Airin Afroj Aishi, Md Ahsan Karim, Dipanjali Kundu, Tanoy Debnath, Md. Abul Ala Moududi, MD. Zunead Abedin Eidmum</strong></p>
<p>Nowadays, DeepSeek, ChatGPT, and Google Gemini are the most trending and exciting Large Language Model (LLM) technologies for reasoning, multimodal capabilities, and general linguistic performance worldwide. DeepSeek employs a Mixture-of-Experts (MoE) approach, activating only the parameters most relevant to the task at hand, which makes it especially effective for domain-specific work. On the other hand, ChatGPT relies on a dense transformer model enhanced through reinforcement learning from human feedback (RLHF), and then Google Gemini actually uses a multimodal transformer architecture that integrates text, code, and images into a single framework. However, by using those technologies, people can be able to mine their desired text, code, images, etc, in a cost-effective and domain-specific inference. People may choose those techniques based on the best performance. In this regard, we offer a comparative study based on the DeepSeek, ChatGPT, and Gemini techniques in this research. Initially, we focus on their methods and materials, appropriately including the data selection criteria. Then, we present state-of-the-art features of DeepSeek, ChatGPT, and Gemini based on their applications. Most importantly, we show the technological comparison among them and also cover the dataset analysis for various applications. Finally, we address extensive research areas and future potential guidance regarding LLM-based AI research for the community. </p>
<blockquote>
<p>如今，DeepSeek、ChatGPT和Google Gemini是全球范围内最流行和令人兴奋的大型语言模型（LLM）技术，它们在推理、多模态能力和一般语言性能方面具有出色的表现。DeepSeek采用混合专家（MoE）方法，只激活与手头任务最相关的参数，使其在特定领域的工作中尤其有效。另一方面，ChatGPT依赖于通过强化学习从人类反馈（RLHF）增强的密集变压器模型，然后Google Gemini实际上使用了一种多模态变压器架构，该架构将文本、代码和图像集成到一个单一框架中。然而，通过利用这些技术，人们能够以成本效益高和领域特定的推理来挖掘他们所需的文本、代码、图像等。人们可能会根据最佳性能选择这些技术。在这方面，我们在本研究中基于DeepSeek、ChatGPT和Gemini技术提供了一项比较研究。首先，我们关注它们的方法和材料，包括数据选择标准。然后，我们根据它们的应用展示最前沿的DeepSeek、ChatGPT和Gemini功能。最重要的是，我们对它们进行了技术比较，并对各种应用的数据集进行了分析。最后，我们针对社区基于LLM的人工智能研究提出了广泛的研究领域和未来的潜在指导方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04783v1">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习领域的趋势正兴起。目前DeepSeek、ChatGPT和Google Gemini三种大型语言模型（LLM）技术因其推理能力、多模态功能和整体语言性能而受到广泛关注。三者各有特色，DeepSeek采用混合专家（MoE）方法，针对特定任务激活相关参数；ChatGPT则依赖强化学习从人类反馈中提升能力的密集转换器模型；Google Gemini使用多模态转换器架构，整合文本、代码和图像于一体。该技术使文本挖掘成本效益更高，更具领域特色。本研究对三者进行比较分析，关注方法、材料、应用及数据集分析，旨在为社区提供AI研究的前沿指引和未来展望。<br><strong>Key Takeaways</strong> </p>
<ol>
<li>DeepSeek技术使用混合专家方法以实现对特定任务的优化处理。 </li>
<li>ChatGPT技术依赖于强化学习，通过人类反馈改进其密集转换器模型。 </li>
<li>Google Gemini利用多模态转换器架构整合文本、代码和图像信息。 </li>
<li>这些技术能提高文本挖掘的成本效益和领域特异性。 </li>
<li>研究比较分析这三种技术在方法、材料和应用上的优势。 </li>
<li>研究关注数据集分析在不同领域的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04783">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33e2fb8acf33a58345e831f7542ecab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-350ab728234c22b4fd154c528140888c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8083d3db6885b5c85ab0a40015166c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce877eaae75452f53bacda2708200a1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-System-1-vs-2-Reasoning-Approaches-for-Zero-Shot-Time-Series-Forecasting-A-Benchmark-and-Insights"><a href="#Evaluating-System-1-vs-2-Reasoning-Approaches-for-Zero-Shot-Time-Series-Forecasting-A-Benchmark-and-Insights" class="headerlink" title="Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time Series   Forecasting: A Benchmark and Insights"></a>Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time Series   Forecasting: A Benchmark and Insights</h2><p><strong>Authors:Haoxin Liu, Zhiyuan Zhao, Shiduo Li, B. Aditya Prakash</strong></p>
<p>Reasoning ability is crucial for solving challenging tasks. With the advancement of foundation models, such as the emergence of large language models (LLMs), a wide range of reasoning strategies has been proposed, including test-time enhancements, such as Chain-ofThought, and post-training optimizations, as used in DeepSeek-R1. While these reasoning strategies have demonstrated effectiveness across various challenging language or vision tasks, their applicability and impact on time-series forecasting (TSF), particularly the challenging zero-shot TSF, remain largely unexplored. In particular, it is unclear whether zero-shot TSF benefits from reasoning and, if so, what types of reasoning strategies are most effective. To bridge this gap, we propose ReC4TS, the first benchmark that systematically evaluates the effectiveness of popular reasoning strategies when applied to zero-shot TSF tasks. ReC4TS conducts comprehensive evaluations across datasets spanning eight domains, covering both unimodal and multimodal with short-term and longterm forecasting tasks. More importantly, ReC4TS provides key insights: (1) Self-consistency emerges as the most effective test-time reasoning strategy; (2) Group-relative policy optimization emerges as a more suitable approach for incentivizing reasoning ability during post-training; (3) Multimodal TSF benefits more from reasoning strategies compared to unimodal TSF. Beyond these insights, ReC4TS establishes two pioneering starting blocks to support future zero-shot TSF reasoning research: (1) A novel dataset, TimeThinking, containing forecasting samples annotated with reasoning trajectories from multiple advanced LLMs, and (2) A new and simple test-time scaling-law validated on foundational TSF models enabled by self-consistency reasoning strategy. All data and code are publicly accessible at: <a target="_blank" rel="noopener" href="https://github.com/AdityaLab/OpenTimeR">https://github.com/AdityaLab/OpenTimeR</a> </p>
<blockquote>
<p>推理能力对于解决具有挑战性的任务至关重要。随着基础模型的进步，如大型语言模型（LLM）的出现，已经提出了多种推理策略，包括测试时的增强（如Chain-ofThought）和训练后的优化（如DeepSeek-R1中的用法）。尽管这些推理策略在各种具有挑战性的语言或视觉任务中已被证明是有效的，但它们在时间序列预测（TSF）中的应用和对零样本TSF的影响仍在很大程度上未被探索。尤其是尚不清楚零样本TSF是否受益于推理，如果是这样，那么哪种类型的推理策略最为有效。为了填补这一空白，我们提出了ReC4TS，这是第一个系统地评估流行推理策略在零样本TSF任务上有效性的基准测试。ReC4TS在涵盖单模态和多模态以及短期和长期预测任务的八个领域的数据集上进行了全面的评估。更重要的是，ReC4TS提供了关键的见解：（1）自洽性被证明是最有效的测试时推理策略；（2）相对于群体政策的优化方法更适合于训练后的推理能力激励；（3）多模态TSF比单模态TSF更受益于推理策略。除了这些见解外，ReC4TS还建立了两个开创性的起点，以支持未来的零样本TSF推理研究：（1）一个新的数据集TimeThinking，其中包含带有多个先进LLM推理轨迹的预测样本注释；（2）一种新型且简单的测试时规模定律，该定律在基于自洽推理策略的基础TSF模型上得到了验证。所有数据和代码均可公开访问：<a target="_blank" rel="noopener" href="https://github.com/AdityaLab/OpenTimeR%E3%80%82">https://github.com/AdityaLab/OpenTimeR。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01895v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文章探讨了推理能力在解决挑战性任务中的重要性，特别是随着大型语言模型等基础模型的发展，各种推理策略已经被提出。然而，这些推理策略在时间序列预测（TSF）尤其是零样本TSF中的应用和影响尚未得到充分探索。为了弥补这一空白，文章提出了ReC4TS，这是第一个系统地评估流行推理策略在零样本TSF任务上有效性的基准测试。ReC4TS在涵盖单模态和多模态以及短期和长期预测任务的八个领域的数据集上进行了全面评估，并提供了关于最有效的测试时间推理策略、分组相对政策优化方法以及对单模态和多模态TSF的影响等的见解。除此之外，ReC4TS还为未来的零样本TSF推理研究提供了两个开创性的起点：一个是含有来自多个先进大型语言模型的预测样本及其推理轨迹的新型数据集TimeThinking，另一个是由自我一致性推理策略验证的简单测试时间规模定律。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推理能力对于解决挑战性任务非常重要，特别是在时间序列预测（TSF）中。</li>
<li>虽然存在多种推理策略，但它们在TSF中的具体应用和影响尚未完全探索。</li>
<li>ReC4TS是首个评估推理策略在零样本TSF任务上有效性的基准测试。</li>
<li>ReC4TS的研究结果表明，自我一致性是最有效的测试时间推理策略。</li>
<li>分组相对政策优化是一种适合激励推理能力的后训练方法。</li>
<li>多模态TSF比单模态TSF更能受益于推理策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01895">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abe9556a7c0ffe8b670574df8f79a6a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d17d09ef5c30b5629d46e03b907e693e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45d5da6e1e7747192efaf276a8a83236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d60f296c17c5bb9bcf7a4935a1f065.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427a77800b8bc4f5c0fcc0ecc53ad86e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87171f6f1bb7c11d906106b35a115f5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699e6f37833e75f0dd0ebdf51b151052.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d4fa48425be67cc3c676e741434f25c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SoS1-O1-and-R1-Like-Reasoning-LLMs-are-Sum-of-Square-Solvers"><a href="#SoS1-O1-and-R1-Like-Reasoning-LLMs-are-Sum-of-Square-Solvers" class="headerlink" title="SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers"></a>SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers</h2><p><strong>Authors:Kechen Li, Wenqi Zhu, Coralia Cartis, Tianbo Ji, Shiwei Liu</strong></p>
<p>Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert’s Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems. </p>
<blockquote>
<p>大型语言模型（LLM）已在各种任务上达到了人类水平的熟练程度，但它们在执行严格的数学问题解决方面的能力仍然是一个挑战。在这项工作中，我们研究了一个基本但计算上棘手的的问题：确定给定的多元多项式是否为非负。这个问题与希尔伯特第十七问题密切相关，在全球多项式优化中起着至关重要的作用，并且在各个领域都有应用。首先，我们介绍了SoS-1K数据集，这是一组大约1000个精心挑选的多项式，以及基于五个渐进挑战标准的专家设计推理指令。评估了多个最先进的大型语言模型，我们发现没有结构化指导时，所有模型的性能都略高于随机猜测基线（即50%）。然而，高质量的推理指令可以显著提高准确性，将性能提高到81%。此外，我们的规模为7B的模型SoS-7B，仅在SoS-1K上微调了4个小时，就超越了规模为671B的DeepSeek-V3和GPT-4o-mini在准确性方面的表现，同时仅需要它们所需计算时间的1.8%和5%。我们的研究结果表明，大型语言模型具有推动数学推理边界和解决NP难题的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20545v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多种任务上达到了人类水平的能力，但在进行严谨的数学问题求解方面仍存在挑战。本研究调查了一个基础但计算上棘手的问题：确定给定的多元多项式是否为非负。这个问题与希尔伯特的第十七问题密切相关，在全球多项式优化中起着关键作用，并广泛应用于各个领域。通过引入SoS-1K数据集和基于五个渐进挑战标准的专家设计推理指令，我们发现没有结构指导的LLMs表现仅为随机猜测基线之上的一点。然而，高质量的推理指令显著提高了准确性，将性能提升至高达百分之八十一。此外，仅在SoS-1K上微调了4小时的SoS-7B模型在准确性方面优于仅使用小部分计算时间的DeepSeek-V3和GPT-4o-mini。这些发现突出了LLMs在推动数学推理边界和解决NP难问题方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在多元任务上表现出卓越的能力，但在数学问题解决方面仍有局限。</li>
<li>确定多元多项式是否非负是一个基础且计算困难的问题，对全球多项式优化和多个领域有重要影响。</li>
<li>SoS-1K数据集的引入和专家设计的推理指令有助于评估LLMs在该问题上的表现。</li>
<li>没有结构指导的LLMs表现接近随机猜测，说明结构化指导对于提高模型性能至关重要。</li>
<li>高质量推理指令能显著提高LLMs的准确性，最大提升幅度达百分之八十一。</li>
<li>SoS-7B模型在仅微调4小时后，在某些方面的表现优于其他大型模型，同时所需计算时间大大减少。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20545">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f32b341e6aa87f4dfc8fd7bb55ac872b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e57fb8e24e7ccce9a397d7d46dd75864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f992f95240b008875dd1ef15a932006f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning"><a href="#MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning" class="headerlink" title="MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning"></a>MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning</h2><p><strong>Authors:Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert</strong></p>
<p>Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</a>. </p>
<blockquote>
<p>推理是推进医学图像分析的关键前沿领域，透明度和可信度在医生信任和监管批准中都起着核心作用。尽管医学视觉语言模型（VLM）在放射学任务中显示出潜力，但大多数现有的VLM仅产生最终答案，而没有揭示其背后的推理过程。为了弥补这一空白，我们引入了MedVLM-R1，这是一个医学VLM，能够明确生成自然语言推理，以提高透明度和可信度。MedVLM-R1没有依赖监督微调（SFT），因为SFT经常过度拟合训练分布，并且无法促进真正的推理。相反，MedVLM-R1采用强化学习框架，激励模型发现可解释的推理路径，而无需使用任何推理参考。尽管训练数据有限（仅600个视觉问答样本）且模型参数较少（2B），但MedVLM-R1在MRI、CT和X光基准测试中的准确率从55.11%提高到78.22%，优于在超过一百万样本上训练的更大模型。它还展示了在不同任务下的稳健领域泛化能力。通过统一医学图像分析与明确推理，MedVLM-R1是朝着临床实践中可信和可解释的AI迈出的关键一步。推理模型可访问：<a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1%E3%80%82">https://huggingface.co/JZPeterPan/MedVLM-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19634v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在医疗图像分析领域，通过自然语言模型生成自然语言推理的重要性，以提升模型的透明度和可信度。针对现有模型存在的问题，提出了一种新的医疗视觉语言模型MedVLM-R1。该模型采用强化学习框架进行训练，能够在有限的训练数据和模型参数下取得较好的性能，且在MRI、CT和X光等多种基准测试中表现优异。MedVLM-R1的推理路径可解释性强，无需依赖监督微调（SFT），具有稳健的跨域泛化能力。该模型为临床实践中可信和可解释的AI应用迈出了重要的一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像分析领域需要提高模型的透明度和可信度以增强临床医生对模型的信任及监管机构的认可。</li>
<li>现有医疗视觉语言模型（VLMs）主要提供最终答案，缺乏揭示底层推理的能力。</li>
<li>MedVLM-R1是一个新的医疗VLM模型，能够生成自然语言推理，提高模型的透明度和可信度。</li>
<li>MedVLM-R1采用强化学习框架进行训练，无需依赖监督微调（SFT），能够发现模型的可解释性推理路径。</li>
<li>在有限的训练数据和模型参数下，MedVLM-R1实现了较高的性能，并在多种基准测试中表现优异。</li>
<li>MedVLM-R1具有稳健的跨域泛化能力，能够在不同的医疗图像分析任务中表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4866253832a007295535196f5d583a46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5227bee00a8424e43c4608d4f863f0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VEM-Environment-Free-Exploration-for-Training-GUI-Agent-with-Value-Environment-Model"><a href="#VEM-Environment-Free-Exploration-for-Training-GUI-Agent-with-Value-Environment-Model" class="headerlink" title="VEM: Environment-Free Exploration for Training GUI Agent with Value   Environment Model"></a>VEM: Environment-Free Exploration for Training GUI Agent with Value   Environment Model</h2><p><strong>Authors:Jiani Zheng, Lu Wang, Fangkai Yang, Chaoyun Zhang, Lingrui Mei, Wenjie Yin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</strong></p>
<p>Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the user’s goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods. </p>
<blockquote>
<p>训练用于图形用户界面（GUI）代理的视觉语言模型（VLMs）通过强化学习（RL）面临着重大挑战：基于环境的RL需要昂贵的交互，而无环境方法则难以应对分布变化和奖励泛化。我们提出了一种无环境的RL框架，它通过利用预训练的价值环境模型（VEM）来将价值估计与策略优化解耦。VEM直接从离线数据中预测状态动作值，提炼出关于GUI交互结果的人机交互先验知识，无需进行下一步状态预测或环境反馈。这避免了累积错误，并通过专注于语义推理（例如，此操作是否有助于用户的目标？）增强了应对UI变化的适应性。该框架分为两个阶段：第一阶段是预训练VEM以估计长期动作效用，第二阶段是使用冻结的VEM信号来指导策略探索，从而实现与布局无关的GUI自动化。在Android-in-the-Wild基准测试中，VEM在离线设置和在线设置中均达到了最先进的性能水平，在无环境基线测试中表现显著优于其他方法，并且在没有交互成本的情况下与环境基准方法相匹配。重要的是，VEM证明了语义感知价值估计可以达到与在线训练方法相当的性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18906v1">PDF</a> 20pages,5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在强化学习环境下，训练用于图形用户界面（GUI）代理的视觉语言模型（VLMs）的挑战。为解决这些挑战，提出了一种无环境强化学习框架，该框架通过利用预训练的价值环境模型（VEM）来分离价值估计与策略优化。VEM能从离线数据中预测状态行动价值，通过对GUI交互结果的人类式优先理解来提取价值估计，而无需预测下一个状态或环境反馈。该方法避免了复合错误，增强了对于用户界面变化的适应力，专注于语义推理。在Android-in-the-Wild基准测试中，相较于无环境基准测试和需环境测试方法，VEM在离线与在线设置中均取得最先进的性能表现。尤其重要的是，VEM展示了语义感知价值估计方法能够达到在线训练方法的性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>强化学习训练视觉语言模型（VLMs）用于图形用户界面（GUI）代理面临挑战。</li>
<li>提出了一种无环境强化学习框架，通过预训练的价值环境模型（VEM）解决这些挑战。</li>
<li>VEM能从离线数据中预测状态行动价值，无需预测下一个状态或环境反馈。</li>
<li>VEM通过分离价值估计与策略优化，避免了复合错误，增强了适应UI变化的能力。</li>
<li>VEM在Android-in-the-Wild基准测试中表现优越，匹配了环境依赖方法且无需交互成本。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf2f64a450801511e603cc22c72c8ae8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4511acf92e2aae6582a1d9571a18a0eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8735cda51684692a8117eb3f9c8b0a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5639d304bf24147175980d203f3d0e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reward-Shaping-to-Mitigate-Reward-Hacking-in-RLHF"><a href="#Reward-Shaping-to-Mitigate-Reward-Hacking-in-RLHF" class="headerlink" title="Reward Shaping to Mitigate Reward Hacking in RLHF"></a>Reward Shaping to Mitigate Reward Hacking in RLHF</h2><p><strong>Authors:Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR’s superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/PAR">https://github.com/PorUna-byte/PAR</a>. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）对于将大型语言模型（LLM）与人类价值观对齐至关重要。然而，RLHF容易受到奖励操纵的影响，即代理利用奖励函数的漏洞，而不是学习预期的行为，从而降低了对齐程度。虽然奖励塑造有助于稳定RLHF并部分缓解奖励操纵问题，但对于塑造技术及其基础原理的系统性研究仍然缺乏。为了填补这一空白，我们对流行的奖励塑造方法进行了深入研究。我们的分析提出了三个关键设计原则：（1）RL奖励最好是有限的，（2）RL从快速初步增长后逐步收敛中受益，（3）RL奖励最好制定为中心奖励的函数。通过这些见解的指导，我们提出了“偏好作为奖励”（PAR）这一新方法，它利用奖励模型本身中的潜在偏好作为强化学习的信号。我们在两个基础模型Gemma2-2B和Llama3-8B上，使用两个数据集Ultrafeedback-Binarized和HH-RLHF评估了PAR。实验结果表明，与其他奖励塑造方法相比，PAR具有卓越的性能。在AlpacaEval 2.0基准测试中，PAR的胜率至少比竞争方法高出5个百分点。此外，PAR显示出卓越的数据效率，只需要一个参考奖励就能达到最佳性能，并且在经过两轮完整训练后，仍能抵抗奖励操纵。代码可在<a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/PAR%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PorUna-byte/PAR上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18770v2">PDF</a> 19 pages</p>
<p><strong>Summary</strong>：<br>强化学习从人类反馈（RLHF）对于将大型语言模型（LLM）与人类价值观对齐至关重要。然而，RLHF易受到奖励黑客攻击的影响，其中代理利用奖励函数的漏洞而不是学习预期行为，导致对齐降级。虽然奖励塑造有助于稳定RLHF并部分缓解奖励黑客攻击的问题，但关于塑造技术和其基本原理的系统性研究仍然缺乏。为了填补这一空白，我们对流行的奖励塑造方法进行了深入研究并提出了三条关键设计原则。基于这些见解，我们提出了利用奖励模型内部隐含偏好作为强化学习信号的新方法——偏好作为奖励（PAR）。实验结果表明，PAR在Gemma2-2B和Llama3-8B两个基础模型上的表现优于其他奖励塑造方法。在AlpacaEval 2.0基准测试中，PAR的胜率至少比竞争方法高出5个百分点。此外，PAR表现出显著的数据效率，只需一个参考奖励即可实现最佳性能，并且在两个完整的训练周期后仍然能够抵御奖励黑客攻击。代码可在<a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/PAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PorUna-byte/PAR找到。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>RLHF对于将LLM与人类价值观对齐至关重要，但易受到奖励黑客攻击的问题。</li>
<li>奖励塑造有助于稳定RLHF并部分缓解奖励黑客攻击。</li>
<li>普遍存在的奖励塑造方法缺乏系统研究和分析。</li>
<li>存在三条关键设计原则指导强化学习奖励的设计：理想情况下奖励是有界的、初期迅速增长后逐渐收敛、最佳形式是中心奖励的函数。</li>
<li>提出的PAR方法利用奖励模型内部隐含偏好作为强化学习信号。</li>
<li>PAR在多个实验设置中表现优异，相较于其他奖励塑造方法具有更高的胜率和数据效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18770">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cfc26fe413233e6b667bac7f8adcb3d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bd8aa778aa37491ffb453eb483e28c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf059551909d138ccc5c7941d179477e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8defeea010baa9d02343d016bc6ef1cb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reversal-Blessing-Thinking-Backward-May-Outpace-Thinking-Forward-in-Multi-choice-Questions"><a href="#Reversal-Blessing-Thinking-Backward-May-Outpace-Thinking-Forward-in-Multi-choice-Questions" class="headerlink" title="Reversal Blessing: Thinking Backward May Outpace Thinking Forward in   Multi-choice Questions"></a>Reversal Blessing: Thinking Backward May Outpace Thinking Forward in   Multi-choice Questions</h2><p><strong>Authors:Yizhe Zhang, Richard Bai, Zijin Gu, Ruixiang Zhang, Jiatao Gu, Emmanuel Abbe, Samy Bengio, Navdeep Jaitly</strong></p>
<p>Language models usually use left-to-right (L2R) autoregressive factorization. However, L2R factorization may not always be the best inductive bias. Therefore, we investigate whether alternative factorizations of the text distribution could be beneficial in some tasks. We investigate right-to-left (R2L) training as a compelling alternative, focusing on multiple-choice questions (MCQs) as a test bed for knowledge extraction and reasoning. Through extensive experiments across various model sizes (2B-8B parameters) and training datasets, we find that R2L models can significantly outperform L2R models on several MCQ benchmarks, including logical reasoning, commonsense understanding, and truthfulness assessment tasks. Our analysis reveals that this performance difference may be fundamentally linked to multiple factors including calibration, computability and directional conditional entropy. We ablate the impact of these factors through controlled simulation studies using arithmetic tasks, where the impacting factors can be better disentangled. Our work demonstrates that exploring alternative factorizations of the text distribution can lead to improvements in LLM capabilities and provides theoretical insights into optimal factorization towards approximating human language distribution, and when each reasoning order might be more advantageous. </p>
<blockquote>
<p>语言模型通常使用从左到右（L2R）的自回归分解。然而，L2R分解并不总是最好的归纳偏置。因此，我们调查文本分布的替代分解在一些任务中是否可能有益。我们研究从右到左（R2L）的训练作为引人注目的替代方案，专注于多项选择题（MCQs）作为知识提取和推理的测试平台。通过在不同模型大小（2B-8B参数）和训练数据集上的大量实验，我们发现R2L模型在多个多项选择题基准测试上显著优于L2R模型，包括逻辑推理、常识理解和真实性评估任务。我们的分析表明，这种性能差异可能与多个因素根本相关，包括校准、计算性和方向条件熵。我们通过使用算术任务的受控模拟研究来剖析这些因素的影响，在这些研究中可以更好地分离影响因素。我们的工作表明，探索文本分布的替代分解可以导致大型语言模型能力的改进，并为逼近人类语言分布的最佳分解提供理论洞察，以及每种推理顺序可能更占优势的时机。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18435v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了语言模型的文本分布因子化顺序问题，研究了除了常用的从左到右（L2R）自回归因子化方法外，右到左（R2L）训练在知识提取和推理任务中的潜力。通过对不同模型大小和训练数据集的大量实验，发现R2L模型在多项选择题（MCQs）的逻辑推理、常识理解和真实性评估任务上显著优于L2R模型。分析表明，这种性能差异可能与校准、计算性和方向条件熵等多个因素有关。通过控制模拟研究对这些因素进行了影响剥离，揭示了探索文本分布的不同因子化顺序能提高大型语言模型性能的理论依据和实践优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>除了传统的从左到右自回归因子化方法外，本文探索了右到左训练作为文本分布的一种替代因子化方式。</li>
<li>在多项选择题的知识提取和推理任务上，右到左模型表现出显著优势。</li>
<li>这种性能差异与模型的校准、计算性和方向条件熵等因素有关。</li>
<li>通过控制模拟研究，分析了影响模型性能的各种因素。</li>
<li>研究表明，探索文本分布的不同因子化顺序有助于提升大型语言模型的能力。</li>
<li>此研究为逼近人类语言分布的文本分布因子化提供了理论见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18435">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-39f76fb75bdb0101a18a53d559fccbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51512286e043d9d172f333df4e3d1051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d12ee4e805ddf3c6370318ea7a9f5a19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-484760914a2907c811563cd3ae44603c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13d2f679b176d4a645e47ca3ae3b93df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-361695638fc47f6de04a941f7abc4914.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DeepSeek-R1-Outperforms-Gemini-2-0-Pro-OpenAI-o1-and-o3-mini-in-Bilingual-Complex-Ophthalmology-Reasoning"><a href="#DeepSeek-R1-Outperforms-Gemini-2-0-Pro-OpenAI-o1-and-o3-mini-in-Bilingual-Complex-Ophthalmology-Reasoning" class="headerlink" title="DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in   Bilingual Complex Ophthalmology Reasoning"></a>DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in   Bilingual Complex Ophthalmology Reasoning</h2><p><strong>Authors:Pusheng Xu, Yue Wu, Kai Jin, Xiaolan Chen, Mingguang He, Danli Shi</strong></p>
<p>Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n &#x3D; 39) and management (n &#x3D; 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P&lt;0.001 compared with DeepSeek-R1), and 0.746 (P&#x3D;0.115), 0.723 (P&#x3D;0.027), and 0.577 (P&lt;0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P&lt;0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making. </p>
<blockquote>
<p>目的：旨在评估DeepSeek-R1以及其他三种最近发布的大型语言模型（LLM）在双语复杂眼科病例中的准确性和推理能力。方法：从中文眼科高级专业技术职称考试中收集与诊断（n&#x3D;39）和管理（n&#x3D;91）相关的总计130道选择题（MCQs），并将其分为六个主题。这些选择题使用DeepSeek-R1翻译成英文。在默认配置下，于2025年2月15日至2月20日期间，生成DeepSeek-R1、Gemini 2.0 Pro、OpenAI o1和o3-mini的回答。准确性被计算为正确回答问题的比例，遗漏和额外答案被视为不正确。通过分析和探讨推理逻辑和推理错误的原因来评估推理能力。结果：DeepSeek-R1在双语眼科选择题中表现出最高的总体准确性，在中文选择题中的准确率为0.862，英文选择题中的准确率为0.808。在中文选择题中，Gemini 2.0 Pro、OpenAI o1和OpenAI o3-mini的准确率分别为0.715、0.685和0.692（与DeepSeek-R1相比，所有P&lt;0.001），在英文选择题中的准确率分别为0.746（P&#x3D;0.115）、0.723（P&#x3D;0.027）和0.577（P&lt;0.001）。在中文和英文选择题中，DeepSeek-R1在五个主题中的准确率最高。在管理类中文题目的测试中，DeepSeek-R1表现尤为出色（所有P&lt;0.05）。推理能力分析显示，四种大型语言模型具有相似的推理逻辑。忽略重要的正面病史、忽略重要的阳性体征、误解医疗数据以及过于激进是推理错误最常见的成因。结论：相较于其他三种先进的LLM，DeepSeek-R1在处理双语复杂眼科推理任务时表现出卓越的性能。虽然其临床适用性仍有待挑战，但它对于支持诊断和临床决策制定显示出良好的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17947v1">PDF</a> 29 pages, 4 figures, 1 table</p>
<p><strong>Summary</strong><br>     近期研究对比了DeepSeek-R1与其他三款大型语言模型在双语复杂眼科病例中的准确性和推理能力。结果显示，DeepSeek-R1在中文和英文的眼科专业问题中准确率最高，达到86%，并在推理任务上表现出优异表现。虽然临床应用仍有挑战，但在辅助诊断和临床决策方面具有潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究目的是评估DeepSeek-R1和其他三款大型语言模型在双语复杂眼科病例中的准确性和推理能力。</li>
<li>DeepSeek-R1在中文和英文眼科专业问题中的准确率均超过其他模型，达到最高水平。</li>
<li>DeepSeek-R1在眼科病例的推理任务中表现出卓越的能力，包括在诊断和管理方面的任务。</li>
<li>其他大型语言模型在准确性和推理能力方面虽逊于DeepSeek-R1，但其在一些领域也有相对较好的表现。</li>
<li>LLMs的常见推理错误原因包括忽略关键阳性病史、忽略关键阳性体征、误解医疗数据以及过于激进等。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17947">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-504eaeaf98561201ba30969a555879c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a921bbfdd8f5fa237c9cb06878299c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-224cd2a966cb94406aa7eeab5850c5de.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Image-Generation-with-Vision-Full-view-Prompt"><a href="#Autoregressive-Image-Generation-with-Vision-Full-view-Prompt" class="headerlink" title="Autoregressive Image Generation with Vision Full-view Prompt"></a>Autoregressive Image Generation with Vision Full-view Prompt</h2><p><strong>Authors:Miaomiao Cai, Guanjie Wang, Wei Li, Zhijun Tu, Hanting Chen, Shaohui Lin, Jie Hu</strong></p>
<p>In autoregressive (AR) image generation, models based on the ‘next-token prediction’ paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the image’s structure and details, impacting the generation’s accuracy and stability. Additionally, the ‘next-token prediction’ paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Prompt engineering, as a key technique for guiding LLMs, leverages specifically designed prompts to improve model performance on complex natural language processing (NLP) tasks, enhancing accuracy and stability of generation while maintaining contextual coherence and logical consistency, similar to human reasoning. Inspired by prompt engineering from the field of NLP, we propose Vision Full-view prompt (VF prompt) to enhance autoregressive image generation. Specifically, we design specialized image-related VF prompts for AR image generation to simulate the process of human image creation. This enhances contextual logic ability by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without VF prompts, our method shows outstanding performance and achieves an approximate improvement of 20%. </p>
<blockquote>
<p>在自回归（AR）图像生成中，基于大型语言模型（LLM）“下一个令牌预测”范式的模型通过减少归纳偏见，展现了与扩散模型相当的性能。然而，直接将大型语言模型应用于复杂图像生成会面临重建图像结构和细节的挑战，影响生成的准确性和稳定性。此外，AR模型中的“下一个令牌预测”范式并不符合人类视觉感知所涉及的上文扫描和逻辑推理过程，限制了有效的图像生成。</p>
</blockquote>
<p>提示工程是引导大型语言模型的关键技术，它通过专门设计的提示来改善模型在复杂自然语言处理（NLP）任务上的性能，提高生成的准确性和稳定性，同时保持上下文连贯和逻辑一致性，类似于人类推理。</p>
<p>受自然语言处理领域提示工程的启发，我们提出Vision Full-view prompt（VF提示）来增强自回归图像生成。具体来说，我们为AR图像生成设计了专门化的图像相关VF提示，以模拟人类图像创建的过程。这通过允许模型在生成图像之前先感知整体分布信息，增强了上下文逻辑能力，并通过增加推理步骤提高了生成的稳定性。与没有VF提示的AR方法相比，我们的方法表现出卓越的性能，实现了约20%的改进。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16965v3">PDF</a> </p>
<p><strong>Summary</strong>：基于自然语言处理领域的提示工程技术，提出一种名为Vision Full-view prompt（VF提示）的方法，用于增强自回归图像生成。通过设计专门的图像相关VF提示，模拟人类图像创建过程，增强自回归图像生成的上下文逻辑能力，并提高生成稳定性。相较于无VF提示的AR方法，该方法表现出卓越性能，近似提升了20%的效果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AR图像生成中，基于’next-token prediction’的模型在减少归纳偏见方面表现出与扩散模型相当的性能，但在复杂图像生成方面存在结构重建和细节缺失的问题。</li>
<li>‘Next-token prediction’范式不符合人类视觉感知中的上下文扫描和逻辑推理过程，限制了有效的图像生成。</li>
<li>提示工程作为引导LLMs的关键技术，通过专门设计的提示来改善复杂NLP任务上的模型性能，维持生成的上下文连贯性和逻辑一致性。</li>
<li>借鉴NLP领域的提示工程，提出Vision Full-view prompt（VF提示）来增强自回归图像生成。</li>
<li>VF提示设计专门的图像相关提示用于AR图像生成，模拟人类图像创建过程。</li>
<li>VF提示增强了模型的上下文逻辑能力，允许模型先感知整体分布信息再生成图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5b252fa429ab4ec13a2157e10d6a13de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63555639ec6eb587d611a611259601ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b725730f2f122c374b695d070f9afc30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-643d2f6847a7fe69826056f6ed053657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5dbcd4e79bd4bca49e9b290179c544c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5437b87bcab3fde6e5ece04db7fb2282.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Lean-and-Mean-Decoupled-Value-Policy-Optimization-with-Global-Value-Guidance"><a href="#Lean-and-Mean-Decoupled-Value-Policy-Optimization-with-Global-Value-Guidance" class="headerlink" title="Lean and Mean: Decoupled Value Policy Optimization with Global Value   Guidance"></a>Lean and Mean: Decoupled Value Policy Optimization with Global Value   Guidance</h2><p><strong>Authors:Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</strong></p>
<p>Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40% and training time by 35% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance. </p>
<blockquote>
<p>基于近端策略优化（PPO）的强化学习从人类反馈（RLHF）对于对齐大型语言模型（LLM）与人类偏好至关重要。它要求actor和critic进行联合训练，并使用预训练的固定奖励模型进行指导。这种方法由于actor-critic的相互依赖性而增加了计算复杂性和不稳定性。此外，PPO在LLM任务中无法获得真实的环境奖励，这限制了其适应性。在这种情况下，预训练价值模型或奖励模型变得等价，因为两者都提供了固定的监督信号而没有新的真实反馈。为了解决这些问题，我们提出了“解耦价值策略优化（DVPO）”，这是一个精简的框架，用预训练的“全局价值模型（GVM）”替代传统的奖励建模。GVM根据政策轨迹进行条件设置，并预测token级别的回报估计。通过解耦价值模型与策略训练（通过冻结的GVM驱动的RL目标），DVPO消除了actor-critic的相互依赖性，与常规的RLHF相比，减少了40%的GPU内存使用量和35%的训练时间。在多个基准测试上的实验表明，DVPO在性能上优于高效的RLHF方法（例如DPO），同时与最先进的PPO相匹配。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16944v1">PDF</a> 16 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>基于近端策略优化（PPO）的强化学习从人类反馈（RLHF）对于对齐大型语言模型（LLM）与人类偏好至关重要。它要求同时对演员和评论家进行训练，并使用预训练的固定奖励模型进行指导，这增加了计算复杂性和不稳定性。为解决这些问题，我们提出了去耦价值策略优化（DVPO），用一个预训练的全球价值模型（GVM）代替传统的奖励模型。实验表明，DVPO在效率上超越了传统的RLHF方法，同时达到了PPO的性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PPO-based RLHF在LLM与人类偏好对齐中起关键作用，但需要复杂的联合训练和固定的奖励模型。</li>
<li>演员和评论家的联合训练增加了计算复杂性和不稳定性。</li>
<li>PPO在LLM任务中无法获得真实的环境奖励，限制了其适应性。</li>
<li>DVPO通过引入全球价值模型（GVM）解决了这些问题，该模型基于政策轨迹预测token级别的回报估计。</li>
<li>DVPO消除了演员和评论家之间的依赖性，降低了GPU内存使用量和训练时间。</li>
<li>实验表明，DVPO在性能上超越了传统的RLHF方法，同时与当前最佳的PPO相匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16944">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-160aab96acc9e3ac416483ba9cd486dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9d046c2c16294747e4cb41a2579b0c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-General-Preference-Alignment-via-Optimistic-Online-Mirror-Descent"><a href="#Improving-LLM-General-Preference-Alignment-via-Optimistic-Online-Mirror-Descent" class="headerlink" title="Improving LLM General Preference Alignment via Optimistic Online Mirror   Descent"></a>Improving LLM General Preference Alignment via Optimistic Online Mirror   Descent</h2><p><strong>Authors:Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1&#x2F;2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）显示出将大型语言模型（LLM）与人类偏好对齐方面的显著有效性。许多现有的对齐方法依赖于Bradley-Terry（BT）模型假设，该假设假设每个提示-响应对都存在一个真实奖励。然而，在模拟复杂的人类偏好时，这个假设可能会过于限制。在本文中，我们放弃了BT模型假设，并在一般偏好下研究LLM对齐问题，将其制定为两人游戏。借鉴游戏学习的理论见解，我们将乐观的在线镜像下降法纳入我们的对齐框架来近似纳什策略。从理论上讲，我们证明了我们的方法实现了对偶间隙的O(T^-1)界，改进了之前的O(T^-1&#x2F;2)结果。更重要的是，我们实施了我们的方法，并通过实验证明它在多个代表性基准测试中优于最新的RLHF算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16852v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于人类反馈的强化学习（RLHF）在将大型语言模型（LLM）与人类偏好对齐方面表现出显著的有效性。许多现有对齐方法依赖于布雷德利-特雷（BT）模型假设，即每个提示-响应对都存在一个真实的奖励。然而，当模拟复杂的人类偏好时，这一假设可能过于限制。本文放弃BT模型假设，在通用偏好下研究LLM对齐问题，将其表述为两人游戏。结合游戏中学习的理论见解，我们将乐观的在线镜像下降法集成到我们的对齐框架中，以近似纳什策略。理论上，我们证明了我们的方法实现了对偶间隙的O(T^-1)边界，改进了之前的O(T^-1&#x2F;2)结果。更重要的是，我们通过实验证明，该方法在多个代表性基准测试上优于最先进的RLHF算法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习从人类反馈（RLHF）在大型语言模型（LLM）与人类偏好对齐上效果显著。</li>
<li>现有对齐方法常依赖布雷德利-特雷（BT）模型假设，但在复杂人类偏好模拟上可能过于限制。</li>
<li>本研究放弃BT模型假设，在通用偏好下研究LLM对齐，将其表述为两人游戏。</li>
<li>整合乐观的在线镜像下降法来近似纳什策略。</li>
<li>理论上实现对偶间隙的O(T^-1)边界的突破，优于之前的O(T^-1&#x2F;2)结果。</li>
<li>实验证明该方法在多个基准测试上优于最先进的RLHF算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01fb35e3d2b4b98edf2acad9a098c1ca.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="IPO-Your-Language-Model-is-Secretly-a-Preference-Classifier"><a href="#IPO-Your-Language-Model-is-Secretly-a-Preference-Classifier" class="headerlink" title="IPO: Your Language Model is Secretly a Preference Classifier"></a>IPO: Your Language Model is Secretly a Preference Classifier</h2><p><strong>Authors:Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose Implicit Preference Optimization (IPO), an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）已经成为将大型语言模型（LLM）与人类偏好对齐的主要方法。虽然它能够让LLM实现人类级别的对齐，但由于其依赖训练外部奖励模型或人类标注的偏好，通常会带来很大的计算和财务成本。在这项工作中，我们提出了隐式偏好优化（IPO）这一替代方法，它利用生成式LLM作为偏好分类器，从而减少了对外部人类反馈或奖励模型获得偏好的依赖。我们使用RewardBench对LLM的偏好分类能力进行了全面评估，评估了不同大小、架构和训练水平的模型，以验证我们的假设。此外，我们还通过为给定指令生成多个响应并利用模型本身作为基于直接偏好优化（DPO）训练的偏好分类器，研究了LLM的自我改进能力。我们的研究结果表明，通过IPO训练的模型在获得偏好方面的性能与利用最新奖励模型的性能相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16182v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了强化学习从人类反馈（RLHF）在大型语言模型（LLM）中对齐人类偏好的主要方法。虽然这种方法可以实现人类级别的对齐，但由于依赖训练外部奖励模型或人类标注的偏好，它常常带来巨大的计算和财务成本。本文提出了一种名为Implicit Preference Optimization (IPO)的替代方法，该方法利用生成式LLM作为偏好分类器，从而减少对外部人类反馈或奖励模型的依赖来获得偏好。通过对不同大小、架构和训练级别的模型进行RewardBench的全面评估，验证了假设。此外，本文还探讨了LLM的自我改进能力，通过为给定指令生成多个响应并利用模型本身作为偏好分类器进行基于Direct Preference Optimization (DPO)的训练。研究发现，通过IPO训练的模型在获得偏好方面的性能与利用最新奖励模型的模型相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习从人类反馈（RLHF）是大型语言模型（LLM）对齐人类偏好的主要方法，但计算和财务成本较高。</li>
<li>本文提出了Implicit Preference Optimization (IPO)方法，利用生成式LLM作为偏好分类器，减少对外部反馈或奖励模型的依赖。</li>
<li>通过RewardBench评估了不同大小、架构和训练级别的模型在偏好分类方面的能力。</li>
<li>IPO训练的模型性能与利用最新奖励模型的模型在获得偏好方面相当。</li>
<li>探讨了LLM的自我改进能力，通过生成多个响应并利用模型本身作为偏好分类器进行训练。</li>
<li>IPO方法可能降低RLHF的成本，并提高LLM的效率和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16182">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b62ecf2b743d65b930f623f374a1ac05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b2e2790dc6a1a2f72e8c8934c4f086f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d114f06efcf8e0eddad4cd7739377493.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2eb8aa35babe181879a79698dd72949e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e363189df2970c20d1ad7098f745a63b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepSeek-V3-GPT-4-Phi-4-and-LLaMA-3-3-generate-correct-code-for-LoRaWAN-related-engineering-tasks"><a href="#DeepSeek-V3-GPT-4-Phi-4-and-LLaMA-3-3-generate-correct-code-for-LoRaWAN-related-engineering-tasks" class="headerlink" title="DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for   LoRaWAN-related engineering tasks"></a>DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for   LoRaWAN-related engineering tasks</h2><p><strong>Authors:Daniel Fernandes, João P. Matos-Carvalho, Carlos M. Fernandes, Nuno Fachada</strong></p>
<p>This paper investigates the performance of 16 Large Language Models (LLMs) in automating LoRaWAN-related engineering tasks involving optimal placement of drones and received power calculation under progressively complex zero-shot, natural language prompts. The primary research question is whether lightweight, locally executed LLMs can generate correct Python code for these tasks. To assess this, we compared locally run models against state-of-the-art alternatives, such as GPT-4 and DeepSeek-V3, which served as reference points. By extracting and executing the Python functions generated by each model, we evaluated their outputs on a zero-to-five scale. Results show that while DeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller models-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance, underscoring the viability of lightweight alternatives. Other models exhibited errors stemming from incomplete understanding or syntactic issues. These findings illustrate the potential of LLM-based approaches for specialized engineering applications while highlighting the need for careful model selection, rigorous prompt design, and targeted domain fine-tuning to achieve reliable outcomes. </p>
<blockquote>
<p>本文研究了16种大型语言模型（LLM）在自动化LoRaWAN相关工程任务中的表现，这些任务涉及无人机的最佳放置和逐步复杂的零镜头自然语言提示下的接收功率计算。主要的研究问题是，轻量级的本地执行LLM是否能够生成完成这些任务的正确Python代码。为了评估这一点，我们将本地运行模型与最前沿的替代方案（如GPT-4和DeepSeek-V3）进行了比较，这些方案作为参考点。通过提取并执行每个模型生成的Python函数，我们对它们的输出进行了从零到五的评分。结果表明，虽然DeepSeek-V3和GPT-4始终提供了准确的解决方案，但某些较小的模型——尤其是Phi-4和LLaMA-3.3——也表现出了强劲的性能，这突显了轻量级替代方案的可能性。其他模型出现的错误源于理解不全面或句法问题。这些发现表明了基于LLM的方法在专业化工程应用中的潜力，同时也强调了实现可靠结果需要谨慎选择模型、严格设计提示以及有针对性的领域微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14926v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了16种大型语言模型（LLMs）在自动化LoRaWAN相关工程任务中的表现，包括无人机最优放置和接收功率计算等任务。文章探讨了轻量级本地执行的大型语言模型是否能够生成用于这些任务的正确Python代码。通过与GPT-4和DeepSeek-V3等尖端模型进行对比评估，发现某些小型模型如Phi-4和LLaMA-3.3也有出色表现。但部分模型因理解不全面或语法问题而产生错误。这表明在特定工程应用中，大型语言模型具有潜力，但仍需谨慎选择模型、设计提示以及针对特定领域进行微调，以实现可靠的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究了大型语言模型在自动化LoRaWAN相关工程任务中的性能。</li>
<li>主要探讨了轻量级本地执行的大型语言模型是否能生成正确Python代码完成这些任务。</li>
<li>对比评估了多种模型，包括尖端模型如GPT-4和DeepSeek-V3以及小型模型如Phi-4和LLaMA-3.3。</li>
<li>DeepSeek-V3和GPT-4提供准确解决方案，而部分小型模型也有出色表现。</li>
<li>部分模型存在因理解不全面或语法问题而导致的错误。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6087c41f411f173c2284e1508267583e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Drift-Decoding-time-Personalized-Alignments-with-Implicit-User-Preferences"><a href="#Drift-Decoding-time-Personalized-Alignments-with-Implicit-User-Preferences" class="headerlink" title="Drift: Decoding-time Personalized Alignments with Implicit User   Preferences"></a>Drift: Decoding-time Personalized Alignments with Implicit User   Preferences</h2><p><strong>Authors:Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Kyomin Jung</strong></p>
<p>Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable. </p>
<blockquote>
<p>针对个人用户的个性化对齐在大规模语言模型（LLM）中是一个长期目标。我们引入了Drift，这是一个在解码时间对大规模语言模型进行个性化设置的新框架，并考虑了隐含的用户偏好。传统的基于人类反馈的强化学习（RLHF）需要数千个标注样本和昂贵的梯度更新。相比之下，Drift以无需训练的方式个性化LLM，仅使用几十个样本就能通过高效偏好建模来引导冻结模型。我们的方法将用户偏好建模为预定义的可解释属性的组合，并在解码时间与个性化生成对齐。在合成人格数据集（Perspective）和真实人类标注数据集（PRISM）上的实验表明，在使用仅50-100个样本的情况下，Drift显著优于RLHF基线。我们的结果和分析表明，Drift在计算上是高效的，并且是可解释的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14289v2">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Drift的新型框架，该框架在解码时间对大型语言模型（LLMs）进行个性化处理，通过隐式用户偏好实现个性化对齐。与传统的需要通过成千上万注解示例和昂贵梯度更新来实现的人类反馈强化学习（RLHF）不同，Drift以无需训练的方式个性化LLMs，仅使用几十个示例通过高效偏好建模来引导冻结模型。该方法将用户偏好建模为预定义、可解释属性的组合，并在解码时间对其进行对齐，以实现个性化生成。实验表明，无论是在合成人格数据集（Perspective）还是真实人类注释数据集（PRISM）上，Drift在仅使用50-100个示例的情况下，显著优于RLHF基线。分析和结果证明，Drift既计算高效又具有可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Drift是一个新型的框架，能够在解码时间对大型语言模型进行个性化处理。</li>
<li>传统强化学习从人类反馈（RLHF）需要大量注解示例和梯度更新，而Drift只需少量示例就能引导模型。</li>
<li>Drift通过隐式用户偏好实现个性化对齐。</li>
<li>Drift将用户偏好建模为预定义、可解释属性的组合。</li>
<li>在合成人格数据集和真实人类注释数据集上的实验表明，Drift显著优于RLHF基线。</li>
<li>Drift既计算高效，也具备可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14289">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-df780cbf4f6c3b0b5ca6db8726a8b9dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87f84db1eeb73bb551d442218fe2bb8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cb768f405e4dde00b8447152e5e439.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2fab063af24aaa30cb0afbdd07cf7e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d38e8e864b7dec2a197b6446df29ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-042eecc385a19f5fef3f6c6e2712a1ff.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="None-of-the-Others-a-General-Technique-to-Distinguish-Reasoning-from-Memorization-in-Multiple-Choice-LLM-Evaluation-Benchmarks"><a href="#None-of-the-Others-a-General-Technique-to-Distinguish-Reasoning-from-Memorization-in-Multiple-Choice-LLM-Evaluation-Benchmarks" class="headerlink" title="None of the Others: a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks"></a>None of the Others: a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks</h2><p><strong>Authors:Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco</strong></p>
<p>In LLM evaluations, reasoning is often distinguished from recall&#x2F;memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall&#x2F;memorization in current LLMs’ answers. </p>
<blockquote>
<p>在大型语言模型（LLM）的评估中，推理通常通过对数学导向问题执行数值变化来与回忆&#x2F;记忆进行区分。在这里，我们介绍了一种多选题的一般变化方法，该方法可以完全消除正确答案与先前见过的标记或概念的联系，要求大型语言模型进行理解和推理（而不是记忆）以正确回答问题。使用这种方法，我们在两个可用英语和西班牙语的数据集上评估了最新私有和开源的大型语言模型：公开的MMLU基准测试和私有的UNED-Access 2024数据集。结果表明，所有模型在我们提出的变体下都出现了显著准确率下降的情况，其中MMLU上的平均损失为57%，UNED-Access 2024上的平均损失为50%，不同模型之间的损失范围从10%到93%。值得注意的是，在我们实验中表现最准确的模型（OpenAI-o3-mini）并非最稳健（DeepSeek-R1-70B），这表明在标准评估中表现最佳的模型可能并不具备最佳的推理能力。此外，我们还观察到公开数据集（与私有数据集相比）以及原始语言（与手动翻译相比）提出的问题准确率下降幅度更大，这是污染的迹象，也表明回忆和记忆在当前大型语言模型的答案中扮演着重要角色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12896v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对多选题的一般变异方法，该方法能够使正确答案与先前见过的标记或概念完全分离，要求LLM理解和推理（而非记忆）以正确回答问题。通过对当前最先进的专有和开源LLM在英文和西班牙语数据集上进行评估，发现所有模型在新提出的方法下准确率显著下降，平均损失分别为MMLU上的57%和UNED-Access 2024上的50%，各模型间损失范围从10%到93%。值得注意的是，在实验中最准确的模型（OpenAI-o3-mini）并非最稳健（DeepSeek-R1-70B），这提示我们标准评估中最优秀的模型可能并不具备最佳的推理能力。此外，公开数据集的准确率下降幅度大于私有数据集，原始语言的问题较手动翻译的问题准确率下降幅度更大，这表明污染现象的存在以及回忆和记忆在当前LLM答案中的重要作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种多选题的一般变异方法，强调理解和推理而非记忆。</li>
<li>所有评估的LLM模型在新方法下准确率显著下降。</li>
<li>最准确的模型在实验中未必最稳健，提示优秀评估模型可能并不具备最佳推理能力。</li>
<li>公开数据集的准确率下降幅度大于私有数据集，显示污染现象的存在。</li>
<li>原始语言问题的准确率较手动翻译的问题下降幅度更大。</li>
<li>结果表明回忆和记忆在当前LLM答案中扮演重要角色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12896">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbe7915e89d231d83d2a557cd6fc26c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdcdf060744bcbc96773425b747199fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a565b3f90516e90a97a2b97a3befb5c7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Hidden-Risks-of-Large-Reasoning-Models-A-Safety-Assessment-of-R1"><a href="#The-Hidden-Risks-of-Large-Reasoning-Models-A-Safety-Assessment-of-R1" class="headerlink" title="The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1"></a>The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1</h2><p><strong>Authors:Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, Xin Eric Wang</strong></p>
<p>The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model’s reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models’ safety to close the gap. </p>
<blockquote>
<p>大型推理模型（如OpenAI-o3和DeepSeek-R1）的快速发展，使得其在复杂推理任务上的表现较非推理大型语言模型（LLMs）有了显著提升。然而，这些模型的增强功能，以及DeepSeek-R1等模型的开源访问，引发了严重的安全担忧，尤其是关于其可能被滥用的潜力。在这项工作中，我们对这些推理模型进行了全面的安全评估，利用既定的安全基准来评估其符合安全法规的情况。此外，我们还调查了它们对对抗性攻击（如越狱和提示注入）的易感性，以评估它们在现实世界应用中的稳健性。通过我们多方面的分析，我们发现了四个关键发现：</p>
</blockquote>
<p>（1）在安全性基准和攻击方面，开源R1模型与o3-mini模型之间存在显著的安全差距，这表明需要在R1上投入更多的安全努力。</p>
<p>（2）蒸馏推理模型的安全性能较其安全对齐的基础模型要差。</p>
<p>（3）模型的推理能力越强，当它回答不安全的问题时可能造成的潜在危害就越大。</p>
<p>（4）R1模型的思考过程比其最终答案带来更大的安全隐患。我们的研究提供了对推理模型的安全影响的见解，并强调了R1模型安全性的进一步发展的必要性，以缩小差距。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12659v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型推理模型如OpenAI-o3和DeepSeek-R1的快速发展显著提升了复杂推理能力，相较于非推理大型语言模型（LLMs）。然而，其增强能力与DeepSeek-R1等模型的开源访问相结合，引发了严重的安全担忧，特别是关于潜在误用的风险。本研究对这些推理模型进行了全面的安全评估，利用既定的安全基准评估其符合安全法规的情况。此外，我们还研究了它们对越狱和提示注入等对抗性攻击的易感性，以评估它们在现实世界应用中的稳健性。研究发现四大关键点：一、开源R1模型与o3-mini模型在安全基准和攻击方面存在显著的安全差距，表明需要在R1上加大安全投入；二、蒸馏推理模型的安全性能较差，相较于其安全对齐的基础模型；三、模型推理能力越强，回答不安全问题时可能造成的潜在危害越大；四、R1模型的思考过程比最终答案更引发安全担忧。本研究揭示了推理模型的安全影响，并强调了R1模型安全性的进一步发展的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型在安全性能上存在显著差距，开源模型的安全性能有待提升。</li>
<li>蒸馏推理模型的安全性能相对较弱。</li>
<li>模型的推理能力与其潜在安全风险成正比。</li>
<li>推理模型的思考过程比最终答案更可能引发安全问题。</li>
<li>需要更多的研究来完善推理模型的安全性能。</li>
<li>对抗性攻击对推理模型的稳健性构成挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3612ce78f9d9c598c812cda0ad4abc3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-506d44a1867353dd2f156b40417904c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f62d73c25eb41a00f33baa29bad529.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31319bcc18ac226602c9a0b9a3b05808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acaea51895f3263dcf7b0dae46433a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d766e0449473c1fabb1df63f479dacfa.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning"><a href="#EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning"></a>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</strong></p>
<p>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO’s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. </p>
<blockquote>
<p>大型语言模型（LLM）在定义明确、解决方案清晰的问题中展现出了令人印象深刻的推理能力，例如在数学和编程领域。然而，它们在处理复杂的现实世界场景，如商务谈判等，仍然面临挑战。这些场景要求战略推理能力，即在动态环境中导航并应对不确定性的长期目标。现有的战略推理方法面临着适应性、可扩展性和策略转移等方面的挑战。为了解决这些问题，我们提出了针对战略推理的显式策略优化（EPO）方法。该方法使用LLM在开放动作空间中提供策略，并能插入到任意的LLM代理中以激励目标导向的行为。为了改善适应性和策略转移性，我们通过多回合强化学习（RL）训练战略推理模型，使用过程奖励和迭代自我博弈，无需监督微调（SFT）作为初步步骤。在社会和物理领域的实验表明，EPO通过增强战略推理能力实现长期目标对齐，在社会对话和网页导航任务上达到了最新技术水平。我们的研究发现了EPO中涌现的各种协作推理机制，以及其在生成新策略方面的有效性，突显了其在现实世界应用中的战略推理潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12486v2">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在数学、编程等具有明确答案的问题中展现出强大的推理能力，但在商业谈判等复杂现实场景中的战略推理能力仍然有限。战略推理需要适应动态环境，并在不确定情况下实现长期目标。现有方法面临适应性、可扩展性和策略迁移的挑战。为解决这些问题，我们提出显性策略优化（EPO）用于战略推理，借助LLM在开放行动空间中提供策略，并可与任意LLM代理相结合以引导目标导向行为。通过多回合强化学习（RL）训练战略推理模型，利用过程奖励和迭代自我对抗来提高适应性和策略迁移能力，无需预先进行有监督微调（SFT）。实验表明，EPO在社会对话和网页导航任务上实现长期目标对齐的增强战略推理能力，达到业界最佳表现。发现EPO中的多种协作推理机制和生成新策略的有效性，突显其在现实应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在明确答案的问题中表现出强大的推理能力，但在复杂现实场景如商业谈判中的战略推理能力受限。</li>
<li>战略推理需要适应动态环境，并在不确定情况下实现长期目标。</li>
<li>现有方法面临适应性、可扩展性和策略迁移的挑战。</li>
<li>显性策略优化（EPO）被提出以解决这些问题，结合LLM和强化学习来进行战略推理。</li>
<li>EPO可以在开放行动空间中提供策略，并可与任意LLM代理结合以引导目标导向行为。</li>
<li>通过多回合强化学习训练战略推理模型，以提高适应性和策略迁移能力，无需预先进行有监督微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12486">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e8a74b16f086370e7760f828be8a4aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b362a1bd20af45a1de73d92c66aa409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f087bb632fb79e939e2ff04b4f3fcd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97694c8c8837f774c6734c675b5d24d4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-23/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-23/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-95b2c5cae75f3c8c7bee2c937d247270.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-23  Dita Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ae6447232f8dbbef2e71dab4cffebc4a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-03-22  UniSync A Unified Framework for Audio-Visual Synchronization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
