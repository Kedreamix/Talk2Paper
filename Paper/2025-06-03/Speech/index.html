<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  MSDA Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR"><a href="#MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR" class="headerlink" title="MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR"></a>MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR</h2><p><strong>Authors:Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos</strong></p>
<p>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„Meta PLæ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µåŸŸè‡ªé€‚åº”ç®¡é“ï¼ˆMSDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ ·æœ¬é«˜æ•ˆçš„ã€ä¸¤é˜¶æ®µçš„è‡ªé€‚åº”æ–¹æ³•ï¼Œå®ƒå°†è‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£æŠ€æœ¯ç›¸ç»“åˆã€‚MSDAæ—¨åœ¨æé«˜ASRæ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å®ƒä»¬æ›´èƒ½é€‚åº”å„ç§æ¡ä»¶ã€‚å®ƒåœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚å¸Œè…Šè¯­ï¼‰å’Œæ ‡ç­¾æ•°æ®ç¨€ç¼ºæˆ–å˜ˆæ‚çš„å¼±ç›‘ç£åœºæ™¯ä¸­å°¤å…¶æœ‰æ•ˆã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†Meta PLå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºASRä»»åŠ¡ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸ºASRä¸­çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶å¼ºè°ƒäº†å°†è‡ªç›‘ç£ä¸è‡ªè®­ç»ƒç›¸ç»“åˆæ—¶é‡‡ç”¨çº§è”æ–¹æ³•çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24656v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†Meta PLåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”æ¡†æ¶ã€‚å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µåŸŸé€‚åº”ç®¡é“ï¼ˆMSDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ ·æœ¬é«˜æ•ˆçš„ã€ä¸¤é˜¶æ®µçš„é€‚åº”æ–¹æ³•ï¼Œå°†è‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£æŠ€æœ¯ç›¸ç»“åˆã€‚MSDAæ—¨åœ¨æé«˜ASRæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å¤šç§æ¡ä»¶ã€‚å®ƒåœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€å¦‚å¸Œè…Šè¯­ä¸­å°¤å…¶æœ‰æ•ˆï¼Œä»¥åŠåœ¨æ ‡ç­¾æ•°æ®ç¨€ç¼ºæˆ–å˜ˆæ‚çš„å¼±ç›‘ç£åœºæ™¯ä¸­ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†Meta PLåœ¨ASRä»»åŠ¡ä¸­çš„æœ‰æ•ˆåº”ç”¨ï¼Œå–å¾—äº†æœ€æ–°çš„ç»“æœï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸ºASRä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”æä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¶ˆèå®éªŒå¼ºè°ƒäº†ç»“åˆè‡ªç›‘ç£ä¸è‡ªè®­ç»ƒæ—¶é‡‡ç”¨çº§è”æ–¹æ³•çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä»‹ç»äº†Meta PLåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”æ¡†æ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¤šé˜¶æ®µåŸŸé€‚åº”ç®¡é“ï¼ˆMSDAï¼‰ï¼Œç»“åˆè‡ªç›‘ç£ä¸åŠç›‘ç£æŠ€æœ¯ã€‚</li>
<li>MSDAå¢å¼ºäº†ASRæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€‚åº”å¤šç§æ¡ä»¶ã€‚</li>
<li>MSDAåœ¨èµ„æºæœ‰é™çš„è¯­è¨€ï¼ˆå¦‚å¸Œè…Šè¯­ï¼‰å’Œå¼±ç›‘ç£åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜Meta PLåœ¨ASRä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¶ˆèå®éªŒå¼ºè°ƒäº†ç»“åˆè‡ªç›‘ç£ä¸è‡ªè®­ç»ƒæ—¶é‡‡ç”¨çº§è”æ–¹æ³•çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24df41a8d07482ce3c36c82c615ef5dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4890306a3099cba8a4d16b51184d9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a62e73aef6235c153db5fde894612e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c92477180313d34eb3075562b193aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e23ab06e38a341c09415fc5b6e7e918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49d3df1086fc48f4a5d2b8916cb52a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Composite-Predictive-Generative-Approach-to-Monaural-Universal-Speech-Enhancement"><a href="#A-Composite-Predictive-Generative-Approach-to-Monaural-Universal-Speech-Enhancement" class="headerlink" title="A Composite Predictive-Generative Approach to Monaural Universal Speech   Enhancement"></a>A Composite Predictive-Generative Approach to Monaural Universal Speech   Enhancement</h2><p><strong>Authors:Jie Zhang, Haoyin Yan, Xiaofei Li</strong></p>
<p>It is promising to design a single model that can suppress various distortions and improve speech quality, i.e., universal speech enhancement (USE). Compared to supervised learning-based predictive methods, diffusion-based generative models have shown greater potential due to the generative capacities from degraded speech with severely damaged information. However, artifacts may be introduced in highly adverse conditions, and diffusion models often suffer from a heavy computational burden due to many steps for inference. In order to jointly leverage the superiority of prediction and generation and overcome the respective defects, in this work we propose a universal speech enhancement model called PGUSE by combining predictive and generative modeling. Our model consists of two branches: the predictive branch directly predicts clean samples from degraded signals, while the generative branch optimizes the denoising objective of diffusion models. We utilize the output fusion and truncated diffusion scheme to effectively integrate predictive and generative modeling, where the former directly combines results from both branches and the latter modifies the reverse diffusion process with initial estimates from the predictive branch. Extensive experiments on several datasets verify the superiority of the proposed model over state-of-the-art baselines, demonstrating the complementarity and benefits of combining predictive and generative modeling. </p>
<blockquote>
<p>è®¾è®¡ä¸€ä¸ªèƒ½å¤ŸæŠ‘åˆ¶å„ç§å¤±çœŸå¹¶æ”¹å–„è¯­éŸ³è´¨é‡çš„å•ä¸€æ¨¡å‹å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ï¼Œå³é€šç”¨è¯­éŸ³å¢å¼ºï¼ˆUSEï¼‰ã€‚ä¸åŸºäºç›‘ç£å­¦ä¹ çš„é¢„æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ç”±äºä»ä¸¥é‡å—æŸçš„è¯­éŸ³ä¿¡æ¯ä¸­äº§ç”Ÿçš„èƒ½åŠ›è€Œæ˜¾ç¤ºå‡ºæ›´å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨æç«¯æ¶åŠ£çš„æ¡ä»¶ä¸‹å¯èƒ½ä¼šå¼•å…¥äººå·¥åˆ¶å“ï¼Œå¹¶ä¸”ç”±äºæ¨ç†è¿‡ç¨‹ä¸­æ¶‰åŠè®¸å¤šæ­¥éª¤ï¼Œæ‰©æ•£æ¨¡å‹ç»å¸¸é¢ä¸´æ²‰é‡çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†åŒæ—¶åˆ©ç”¨é¢„æµ‹å’Œç”Ÿæˆçš„ä¼˜ç‚¹å¹¶å…‹æœå„è‡ªçš„ç¼ºé™·ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­æå‡ºäº†ä¸€ç§ç»“åˆé¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡çš„é€šç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œç§°ä¸ºPGUSEã€‚æˆ‘ä»¬çš„æ¨¡å‹ç”±ä¸¤ä¸ªåˆ†æ”¯ç»„æˆï¼šé¢„æµ‹åˆ†æ”¯ç›´æ¥ä»é€€åŒ–ä¿¡å·ä¸­é¢„æµ‹æ¸…æ´æ ·æœ¬ï¼Œè€Œç”Ÿæˆåˆ†æ”¯ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„å»å™ªç›®æ ‡ã€‚æˆ‘ä»¬åˆ©ç”¨è¾“å‡ºèåˆå’Œæˆªæ–­æ‰©æ•£æ–¹æ¡ˆæœ‰æ•ˆåœ°ç»“åˆäº†é¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡ï¼Œå…¶ä¸­å‰è€…ç›´æ¥ç»“åˆäº†æ¥è‡ªä¸¤ä¸ªåˆ†æ”¯çš„ç»“æœï¼Œåè€…ä½¿ç”¨é¢„æµ‹åˆ†æ”¯çš„åˆå§‹ä¼°è®¡å€¼ä¿®æ”¹åå‘æ‰©æ•£è¿‡ç¨‹ã€‚åœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ¨¡å‹ç›¸è¾ƒäºæœ€å…ˆè¿›åŸºçº¿æŠ€æœ¯çš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†ç»“åˆé¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡çš„äº’è¡¥æ€§å’Œå¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24576v1">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong><br>åœ¨æ¶åŠ£æ¡ä»¶ä¸‹ï¼Œå•ä¸€æ¨¡å‹è®¾è®¡å¯¹äºæŠ‘åˆ¶å„ç§å¤±çœŸå’Œæé«˜è¯­éŸ³è´¨é‡å…·æœ‰å‰æ™¯ï¼Œä¾‹å¦‚é€šç”¨è¯­éŸ³å¢å¼ºï¼ˆUSEï¼‰ã€‚ç›¸æ¯”äºåŸºäºç›‘ç£å­¦ä¹ çš„é¢„æµ‹æ–¹æ³•ï¼ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹å› å…¶ä»é€€åŒ–è¯­éŸ³ä¸­ç”Ÿæˆä¿¡æ¯çš„èƒ½åŠ›è€Œæ˜¾ç¤ºå‡ºæ›´å¤§çš„æ½œåŠ›ã€‚ä½†æ‰©æ•£æ¨¡å‹åœ¨é«˜åº¦ä¸åˆ©æ¡ä»¶ä¸‹å¯èƒ½ä¼šå¼•å…¥äººå·¥åˆ¶å“ï¼Œä¸”ç”±äºæ¨ç†æ­¥éª¤ä¼—å¤šè€Œé¢ä¸´æ²‰é‡çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨é¢„æµ‹å’Œç”Ÿæˆçš„ä¼˜ç‚¹å¹¶å…‹æœå„è‡ªçš„ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆé¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡çš„é€šç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹PGUSEã€‚è¯¥æ¨¡å‹ç”±ä¸¤ä¸ªåˆ†æ”¯ç»„æˆï¼šé¢„æµ‹åˆ†æ”¯ç›´æ¥ä»é€€åŒ–ä¿¡å·ä¸­é¢„æµ‹æ¸…æ´æ ·æœ¬ï¼Œè€Œç”Ÿæˆåˆ†æ”¯ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„é™å™ªç›®æ ‡ã€‚æˆ‘ä»¬é€šè¿‡è¾“å‡ºèåˆå’Œæˆªæ–­æ‰©æ•£æ–¹æ¡ˆæœ‰æ•ˆåœ°ç»“åˆäº†é¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡ï¼Œå‰è€…ç›´æ¥ç»“åˆä¸¤ä¸ªåˆ†æ”¯çš„ç»“æœï¼Œåè€…ä½¿ç”¨é¢„æµ‹åˆ†æ”¯çš„åˆå§‹ä¼°è®¡å€¼ä¿®æ”¹åå‘æ‰©æ•£è¿‡ç¨‹ã€‚åœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ¨¡å‹ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œè¯æ˜äº†ç»“åˆé¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡çš„äº’è¡¥æ€§å’Œä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼ˆUSEï¼‰è®¾è®¡ç”¨äºæŠ‘åˆ¶å„ç§å¤±çœŸå¹¶æé«˜è¯­éŸ³è´¨é‡ã€‚</li>
<li>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä¼˜äºåŸºäºç›‘ç£å­¦ä¹ çš„é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ¶åŠ£æ¡ä»¶ä¸‹å¯èƒ½ä¼šå¼•å…¥äººå·¥åˆ¶å“ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é¢ä¸´æ²‰é‡çš„è®¡ç®—è´Ÿæ‹…ï¼Œæ¨ç†æ­¥éª¤ä¼—å¤šã€‚</li>
<li>æå‡ºçš„PGUSEæ¨¡å‹ç»“åˆäº†é¢„æµ‹å’Œç”Ÿæˆå»ºæ¨¡ï¼Œæ—¨åœ¨å…‹æœå„è‡ªçš„ç¼ºé™·ã€‚</li>
<li>PGUSEæ¨¡å‹ç”±ä¸¤ä¸ªåˆ†æ”¯ç»„æˆï¼šé¢„æµ‹åˆ†æ”¯å’Œç”Ÿæˆåˆ†æ”¯ï¼Œåˆ†åˆ«è´Ÿè´£ç›´æ¥é¢„æµ‹æ¸…æ´æ ·æœ¬å’Œä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„é™å™ªç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-05e58812c2a49b731c63a2ed741c9dab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b6d2cd8206fba4c192e032d5c2e75ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c2b8b647c76966e13524144e25da35a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a3ae4ecabd6b43fc09302822e64f49f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ARECHO-Autoregressive-Evaluation-via-Chain-Based-Hypothesis-Optimization-for-Speech-Multi-Metric-Estimation"><a href="#ARECHO-Autoregressive-Evaluation-via-Chain-Based-Hypothesis-Optimization-for-Speech-Multi-Metric-Estimation" class="headerlink" title="ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis   Optimization for Speech Multi-Metric Estimation"></a>ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis   Optimization for Speech Multi-Metric Estimation</h2><p><strong>Authors:Jiatong Shi, Yifan Cheng, Bo-Hao Su, Hye-jin Shim, Jinchuan Tian, Samuele Cornell, Yiwen Zhao, Siddhant Arora, Shinji Watanabe</strong></p>
<p>Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. </p>
<blockquote>
<p>è¯­éŸ³ä¿¡å·åˆ†æé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³è´¨é‡è¯„ä¼°å’Œè¯­éŸ³ç‰¹å¾æè¿°ç­‰ä»»åŠ¡ä¸­ï¼Œç›®æ ‡æ˜¯é¢„æµ‹å¤šä¸ªæ„ŸçŸ¥å’Œå®¢è§‚åº¦é‡æŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼ŒåƒPESQï¼ˆè¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä¼°ï¼‰ã€STOIï¼ˆçŸ­æœŸå®¢è§‚å¯æ‡‚åº¦ï¼‰å’ŒMOSï¼ˆå¹³å‡æ„è§å¾—åˆ†ï¼‰ç­‰æŒ‡æ ‡å„è‡ªæ•æ‰äº†è¯­éŸ³è´¨é‡çš„ä¸åŒæ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æŒ‡æ ‡é€šå¸¸æœ‰ä¸åŒçš„å°ºåº¦ã€å‡è®¾å’Œä¾èµ–å…³ç³»ï¼Œä½¿å¾—è”åˆä¼°è®¡å˜å¾—éå¸¸å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARECHOï¼ˆåŸºäºé“¾å‡è®¾ä¼˜åŒ–çš„è‡ªå›å½’è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªå›å½’ä¾èµ–å»ºæ¨¡çš„é€šç”¨è¯­éŸ³è¯„ä¼°ç³»ç»Ÿã€‚ARECHOçš„ä¸‰ä¸ªä¸»è¦åˆ›æ–°ç‚¹åœ¨äºï¼šï¼ˆ1ï¼‰å…¨é¢çš„è¯­éŸ³ä¿¡æ¯åˆ†è¯ç®¡é“ï¼›ï¼ˆ2ï¼‰åŠ¨æ€åˆ†ç±»å™¨é“¾ï¼Œæ˜ç¡®æ•æ‰è·¨æŒ‡æ ‡ä¾èµ–å…³ç³»ï¼›ï¼ˆ3ï¼‰ä¸¤æ­¥é¢å‘ç½®ä¿¡åº¦çš„è§£ç ç®—æ³•ï¼Œæé«˜æ¨ç†å¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§è¯„ä¼°åœºæ™¯ä¸­ï¼ŒARECHOæ˜¾è‘—ä¼˜äºåŸºå‡†æ¡†æ¶ï¼ŒåŒ…æ‹¬å¢å¼ºè¯­éŸ³åˆ†æã€è¯­éŸ³ç”Ÿæˆè¯„ä¼°å’Œå™ªå£°è¯­éŸ³è¯„ä¼°ã€‚æ­¤å¤–ï¼Œå…¶åŠ¨æ€ä¾èµ–å»ºæ¨¡é€šè¿‡æ•æ‰è·¨æŒ‡æ ‡å…³ç³»æé«˜äº†å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24518v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³ä¿¡å·åˆ†æé¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹å¤šä¸ªæ„ŸçŸ¥å’Œå®¢è§‚æŒ‡æ ‡çš„è¯­éŸ³è´¨é‡è¯„ä¼°å’Œè¯­éŸ³ç‰¹å¾æè¿°ä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³ç°å­˜è¯„ä¼°æ–¹æ³•ä¸­çš„ä¸åŒå°ºåº¦ã€å‡è®¾å’Œä¾èµ–æ€§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºé“¾çš„é€šç”¨è¯„ä¼°ç³»ç»ŸARECHOï¼Œç”¨äºåŸºäºè‡ªå›å½’ä¾èµ–å»ºæ¨¡çš„è¯­éŸ³è¯„ä¼°ã€‚ARECHOçš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šå…¨é¢çš„è¯­éŸ³ä¿¡æ¯æ ‡è®°åŒ–ç®¡é“ã€åŠ¨æ€åˆ†ç±»å™¨é“¾ä»¥åŠä¸¤æ­¥ä¿¡å¿ƒå¯¼å‘è§£ç ç®—æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒARECHOåœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºå‡†æ¡†æ¶ï¼ŒåŒ…æ‹¬å¢å¼ºè¯­éŸ³åˆ†æã€è¯­éŸ³ç”Ÿæˆè¯„ä¼°å’Œå™ªå£°è¯­éŸ³è¯„ä¼°ã€‚å…¶åŠ¨æ€ä¾èµ–å»ºæ¨¡æé«˜äº†å¯è§£é‡Šæ€§ï¼Œé€šè¿‡æ•æ‰æŒ‡æ ‡é—´çš„å…³ç³»å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ä¿¡å·åˆ†æé¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹å¤šä¸ªæ„ŸçŸ¥å’Œå®¢è§‚æŒ‡æ ‡çš„è¯­éŸ³è´¨é‡è¯„ä¼°å’Œæè¿°ä»»åŠ¡ä¸­ã€‚</li>
<li>ç°å­˜è¯„ä¼°æ–¹æ³•å¦‚PESQã€STOIå’ŒMOSç­‰æœ‰ä¸åŒçš„å°ºåº¦ã€å‡è®¾å’Œä¾èµ–æ€§ï¼Œè”åˆä¼°è®¡å…·æœ‰éš¾åº¦ã€‚</li>
<li>ARECHOæ˜¯ä¸€ä¸ªåŸºäºé“¾çš„é€šç”¨è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºè¯­éŸ³è¯„ä¼°ï¼ŒåŸºäºè‡ªå›å½’ä¾èµ–å»ºæ¨¡ã€‚</li>
<li>ARECHOå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå…¨é¢çš„è¯­éŸ³ä¿¡æ¯æ ‡è®°åŒ–ç®¡é“ã€åŠ¨æ€åˆ†ç±»å™¨é“¾å’Œä¸¤æ­¥ä¿¡å¿ƒå¯¼å‘è§£ç ç®—æ³•ã€‚</li>
<li>ARECHOåœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºå‡†æ¡†æ¶ï¼ŒåŒ…æ‹¬å¢å¼ºè¯­éŸ³åˆ†æã€è¯­éŸ³ç”Ÿæˆè¯„ä¼°å’Œå™ªå£°è¯­éŸ³è¯„ä¼°ã€‚</li>
<li>ARECHOçš„åŠ¨æ€ä¾èµ–å»ºæ¨¡æé«˜äº†è¯„ä¼°çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b9ca6869809e2ddab42c7e753ebd6cd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition"><a href="#SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition" class="headerlink" title="SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition"></a>SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition</h2><p><strong>Authors:Longjie Luo, Lin Li, Qingyang Hong</strong></p>
<p>Due to the lack of target speech annotations in real-recorded far-field conversational datasets, speech enhancement (SE) models are typically trained on simulated data. However, the trained models often perform poorly in real-world conditions, hindering their application in far-field speech recognition. To address the issue, we (a) propose direct sound estimation (DSE) to estimate the oracle direct sound of real-recorded data for SE; and (b) present a novel pseudo-supervised learning method, SuPseudo, which leverages DSE-estimates as pseudo-labels and enables SE models to directly learn from and adapt to real-recorded data, thereby improving their generalization capability. Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo. Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo, and our system significantly outperforms the previous state-of-the-art. A demo of our method can be found at <a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">https://EeLLJ.github.io/SuPseudo/</a>. </p>
<blockquote>
<p>ç”±äºçœŸå®å½•éŸ³çš„è¿œè·ç¦»å¯¹è¯æ•°æ®é›†ä¸­ç¼ºä¹ç›®æ ‡è¯­éŸ³æ³¨é‡Šï¼Œè¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹é€šå¸¸æ˜¯åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç„¶è€Œï¼Œè¿™äº›è®­ç»ƒè¿‡çš„æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„è¡¨ç°å¾€å¾€ä¸ä½³ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨è¿œè·ç¦»è¯­éŸ³è¯†åˆ«ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ï¼ˆaï¼‰æå‡ºç›´æ¥å£°éŸ³ä¼°è®¡ï¼ˆDSEï¼‰ï¼Œä»¥ä¼°è®¡çœŸå®å½•éŸ³æ•°æ®çš„ç†æƒ³ç›´è¾¾å£°éŸ³ï¼Œç”¨äºè¯­éŸ³å¢å¼ºï¼›ï¼ˆbï¼‰æå‡ºäº†ä¸€ç§æ–°å‹ä¼ªç›‘ç£å­¦ä¹ æ–¹æ³•SuPseudoï¼Œå®ƒåˆ©ç”¨DSEä¼°è®¡ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä½¿SEæ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»çœŸå®å½•éŸ³æ•°æ®ä¸­è¿›è¡Œå­¦ä¹ å’Œé€‚åº”ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªåä¸ºFARNETçš„SEæ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨SuPseudoã€‚åœ¨MISP2023è¯­æ–™åº“ä¸Šçš„å®éªŒè¯æ˜äº†SuPseudoçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æœ‰å…³æˆ‘ä»¬æ–¹æ³•çš„æ¼”ç¤ºï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/%E3%80%82">https://EeLLJ.github.io/SuPseudo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24450v1">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹è¿œåœºè¯­éŸ³è¯†åˆ«ä¸­çœŸå®å½•éŸ³æ•°æ®é›†ç›®æ ‡è¯­éŸ³æ³¨é‡Šç¼ºå¤±çš„é—®é¢˜ï¼Œç°æœ‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹é€šå¸¸åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨ç°å®ç¯å¢ƒä¸­çš„è¡¨ç°å¾€å¾€ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•ï¼šä¸€æ˜¯ç›´æ¥å£°éŸ³ä¼°è®¡ï¼ˆDSEï¼‰ï¼Œç”¨äºä¼°è®¡çœŸå®å½•éŸ³æ•°æ®çš„ç†æƒ³ç›´æ¥å£°éŸ³ç”¨äºSEï¼›äºŒæ˜¯ä¼ªç›‘ç£å­¦ä¹ æ–¹æ³•SuPseudoï¼Œå®ƒåˆ©ç”¨DSEä¼°è®¡ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä½¿SEæ¨¡å‹èƒ½ç›´æ¥å­¦ä¹ å’Œé€‚åº”çœŸå®å½•éŸ³æ•°æ®ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªåä¸ºFARNETçš„SEæ¨¡å‹ä»¥å……åˆ†åˆ©ç”¨SuPseudoã€‚åœ¨MISP2023è¯­æ–™åº“ä¸Šçš„å®éªŒè¯æ˜äº†SuPseudoçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚æœ‰å…³è¯¥æ–¹æ³•çš„æ¼”ç¤ºï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®å½•éŸ³çš„è¿œåœºè¯­éŸ³è¯†åˆ«æ•°æ®é›†ç¼ºä¹ç›®æ ‡è¯­éŸ³æ³¨é‡Šï¼Œå¯¼è‡´è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè®­ç»ƒåï¼Œç°å®ç¯å¢ƒä¸­çš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ç›´æ¥å£°éŸ³ä¼°è®¡ï¼ˆDSEï¼‰æ–¹æ³•ï¼Œç”¨äºä¼°è®¡çœŸå®å½•éŸ³æ•°æ®çš„ç†æƒ³ç›´æ¥å£°éŸ³ï¼Œä¸ºSEæä¾›åŸºç¡€ã€‚</li>
<li>å¼•å…¥äº†ä¼ªç›‘ç£å­¦ä¹ æ–¹æ³•SuPseudoï¼Œç»“åˆDSEä¼°è®¡ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œè®©SEæ¨¡å‹èƒ½ç›´æ¥ä»çœŸå®å½•éŸ³æ•°æ®å­¦ä¹ å¹¶é€‚åº”ï¼Œè¿›è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†åä¸ºFARNETçš„SEæ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨SuPseudoçš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨MISP2023è¯­æ–™åº“ä¸Šçš„å®éªŒè¯æ˜äº†SuPseudoæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç³»ç»Ÿæ€§èƒ½æ˜¾è‘—è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c19e63aa10b19abbb6a7753e4403e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc60436bae0475679a8ca26eaeee1da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5ee31987b8703bd3aea1af33be50171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19307c4333638b41bcbba90f5d8cb337.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge"><a href="#Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge" class="headerlink" title="Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge"></a>Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge</h2><p><strong>Authors:Longjie Luo, Shenghui Lu, Lin Li, Qingyang Hong</strong></p>
<p>This paper presents our system for the MISP-Meeting Challenge Track 2. The primary difficulty lies in the dataset, which contains strong background noise, reverberation, overlapping speech, and diverse meeting topics. To address these issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework comprising time alignment, level alignment, and signal-to-noise ratio filtering, to generate signal-level pseudo labels for real-recorded far-field audio data, thereby facilitating SE modelsâ€™ training; and (c) explored fine-tuning strategies, data augmentation, and multimodal information to enhance the performance of pre-trained Automatic Speech Recognition (ASR) models in meeting scenarios. Finally, our system achieved character error rates (CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative improvements of 64.8% and 52.6% over the baseline, securing second place. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬ä¸ºMISP-MeetingæŒ‘æˆ˜èµ›é“2è®¾è®¡çš„ç³»ç»Ÿã€‚ä¸»è¦éš¾ç‚¹åœ¨äºæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¼ºçƒˆçš„èƒŒæ™¯å™ªå£°ã€å›å£°ã€è¯­éŸ³é‡å ä»¥åŠå¤šæ ·çš„ä¼šè®®ä¸»é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ï¼ˆaï¼‰è®¾è®¡äº†G-SpatialNetï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹ï¼Œç”¨äºæ”¹è¿›å¼•å¯¼æºåˆ†ç¦»ï¼ˆGSSï¼‰ä¿¡å·ï¼›ï¼ˆbï¼‰æå‡ºTLSï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ—¶é—´å¯¹é½ã€æ°´å¹³å¯¹é½å’Œä¿¡å™ªæ¯”è¿‡æ»¤çš„æ¡†æ¶ï¼Œç”¨äºä¸ºçœŸå®å½•åˆ¶çš„è¿œè·ç¦»éŸ³é¢‘æ•°æ®ç”Ÿæˆä¿¡å·çº§ä¼ªæ ‡ç­¾ï¼Œä»è€Œæœ‰åŠ©äºSEæ¨¡å‹çš„è®­ç»ƒï¼›ï¼ˆcï¼‰æ¢ç´¢äº†å¾®è°ƒç­–ç•¥ã€æ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»¥æé«˜é¢„è®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨ä¼šè®®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨Devå’ŒEvalé›†ä¸Šçš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERsï¼‰åˆ†åˆ«ä¸º5.44%å’Œ9.52%ï¼Œç›¸å¯¹äºåŸºçº¿æœ‰64.8%å’Œ52.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œè·å¾—äº†ç¬¬äºŒåã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24446v1">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>æ‘˜è¦</strong><br>ç³»ç»Ÿè§£å†³MISPä¼šè®®æŒ‘æˆ˜èµ›é“ä¸­çš„ä¸»è¦éš¾ç‚¹ï¼Œé’ˆå¯¹å¼ºèƒŒæ™¯å™ªå£°ã€æ··å“ã€è¯­éŸ³é‡å åŠå¤šæ ·çš„ä¼šè®®ä¸»é¢˜ç­‰æ•°æ®é›†ä¸­çš„é—®é¢˜ï¼Œï¼ˆä¸€ï¼‰è®¾è®¡G-SpatialNetè¯­éŸ³å¢å¼ºæ¨¡å‹æ”¹è¿›å¯¼å‘æºåˆ†ç¦»ä¿¡å·ï¼›ï¼ˆäºŒï¼‰æå‡ºTLSæ¡†æ¶ï¼ŒåŒ…æ‹¬æ—¶é—´å¯¹é½ã€æ°´å¹³å¯¹é½å’Œä¿¡å™ªæ¯”è¿‡æ»¤ï¼Œä¸ºçœŸå®è¿œè·ç¦»éŸ³é¢‘æ•°æ®ç”Ÿæˆä¿¡å·çº§ä¼ªæ ‡ç­¾ï¼ŒåŠ©åŠ›è¯­éŸ³å¢å¼ºæ¨¡å‹çš„è®­ç»ƒï¼›ï¼ˆä¸‰ï¼‰æ¢ç´¢å¾®è°ƒç­–ç•¥ã€æ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ä¿¡æ¯ï¼Œæå‡ä¼šè®®åœºæ™¯ä¸‹é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€ç»ˆç³»ç»Ÿå®ç°å¼€å‘é›†å’Œè¯„ä¼°é›†ä¸Šçš„å­—ç¬¦é”™è¯¯ç‡åˆ†åˆ«ä¸º5.44%å’Œ9.52%ï¼Œç›¸è¾ƒäºåŸºçº¿æœ‰64.8%å’Œ52.6%çš„ç›¸å¯¹æå‡ï¼Œè·å¾—ç¬¬äºŒåã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>é’ˆå¯¹MISPä¼šè®®æŒ‘æˆ˜èµ›é“çš„æ•°æ®é›†éš¾ç‚¹ï¼ŒåŒ…æ‹¬å¼ºèƒŒæ™¯å™ªå£°ã€æ··å“ã€è¯­éŸ³é‡å å’Œå¤šæ ·çš„ä¼šè®®ä¸»é¢˜ã€‚</li>
<li>è®¾è®¡G-SpatialNetè¯­éŸ³å¢å¼ºæ¨¡å‹ä»¥æ”¹è¿›å¯¼å‘æºåˆ†ç¦»ä¿¡å·ã€‚</li>
<li>æå‡ºTLSæ¡†æ¶ï¼ŒåŒ…æ‹¬æ—¶é—´å¯¹é½ã€æ°´å¹³å¯¹é½å’Œä¿¡å™ªæ¯”è¿‡æ»¤æŠ€æœ¯ï¼Œç”Ÿæˆä¿¡å·çº§ä¼ªæ ‡ç­¾åŠ©åŠ›è¯­éŸ³å¢å¼ºæ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>æ¢ç´¢äº†å¤šç§ç­–ç•¥æ¥æå‡é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨ä¼šè®®åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¾®è°ƒç­–ç•¥ã€æ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ä¿¡æ¯åˆ©ç”¨ã€‚</li>
<li>ç³»ç»Ÿå®ç°å¼€å‘é›†å’Œè¯„ä¼°é›†çš„å­—ç¬¦é”™è¯¯ç‡æ˜¾è‘—ä¸‹é™ï¼Œç›¸è¾ƒäºåŸºçº¿æœ‰å¤§å¹…æ”¹è¿›ã€‚</li>
<li>è·å¾—æ¯”èµ›ç¬¬äºŒåï¼Œè¯æ˜ç³»ç»Ÿæ€§èƒ½ä¼˜å¼‚ã€‚</li>
<li>å¯¹æœªæ¥è§£å†³ç±»ä¼¼æŒ‘æˆ˜å…·æœ‰å¯ç¤ºä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-807b69db1c639c3d6ecbfa55e141ae6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2bc8b4a7e329bc94f044eb8f4c1104a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f4b540d6eddfd296bdb56b0aa80e67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3a4aaf3397ad6b1235b13ebc340afa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MOPSA-Mixture-of-Prompt-Experts-Based-Speaker-Adaptation-for-Elderly-Speech-Recognition"><a href="#MOPSA-Mixture-of-Prompt-Experts-Based-Speaker-Adaptation-for-Elderly-Speech-Recognition" class="headerlink" title="MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly   Speech Recognition"></a>MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly   Speech Recognition</h2><p><strong>Authors:Chengxi Deng, Xurong Xie, Shujie Hu, Mengzhe Geng, Yicong Jiang, Jiankun Zhao, Jiajun Deng, Guinan Li, Youjun Chen, Huimeng Wang, Haoning Xu, Mingyu Cui, Xunying Liu</strong></p>
<p>This paper proposes a novel Mixture of Prompt-Experts based Speaker Adaptation approach (MOPSA) for elderly speech recognition. It allows zero-shot, real-time adaptation to unseen speakers, and leverages domain knowledge tailored to elderly speakers. Top-K most distinctive speaker prompt clusters derived using K-means serve as experts. A router network is trained to dynamically combine clustered prompt-experts. Acoustic and language level variability among elderly speakers are modelled using separate encoder and decoder prompts for Whisper. Experiments on the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets suggest that online MOPSA adaptation outperforms the speaker-independent (SI) model by statistically significant word error rate (WER) or character error rate (CER) reductions of 0.86% and 1.47% absolute (4.21% and 5.40% relative). Real-time factor (RTF) speed-up ratios of up to 16.12 times are obtained over offline batch-mode adaptation. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºä¸“å®¶æ··åˆçš„è¯´è¯äººè‡ªé€‚åº”æ–¹æ³•ï¼ˆMOPSAï¼‰ï¼Œç”¨äºè€å¹´è¯­éŸ³è¯†åˆ«ã€‚å®ƒå…è®¸é›¶æ ·æœ¬å®æ—¶é€‚åº”æœªè§è¿‡çš„è¯´è¯äººï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹è€å¹´è¯´è¯äººçš„é¢†åŸŸçŸ¥è¯†ã€‚ä½¿ç”¨K-meansç®—æ³•ç”Ÿæˆçš„Kä¸ªæœ€å…·ç‰¹è‰²çš„è¯´è¯äººæç¤ºèšç±»ä½œä¸ºä¸“å®¶ã€‚è®­ç»ƒè·¯ç”±å™¨ç½‘ç»œä»¥åŠ¨æ€ç»„åˆèšç±»æç¤ºä¸“å®¶ã€‚ä½¿ç”¨å•ç‹¬çš„ç¼–ç å™¨å’Œè§£ç å™¨æç¤ºä¸ºWhisperå»ºç«‹è€å¹´è¯´è¯äººä¹‹é—´çš„å£°å­¦å’Œè¯­è¨€çº§åˆ«çš„å˜åŒ–æ¨¡å‹ã€‚åœ¨è‹±è¯­DementiaBank Pittå’Œå¹¿ä¸œè¯JCCOCC MoCAè€å¹´è¯­éŸ³æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨çº¿MOPSAè‡ªé€‚åº”æ–¹æ³•ä¼˜äºç‹¬ç«‹äºè¯´è¯äººçš„æ¨¡å‹ï¼Œç»å¯¹é™ä½è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æˆ–å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰åˆ†åˆ«ä¸º0.86%å’Œ1.47%ï¼ˆç›¸å¯¹é™ä½åˆ†åˆ«ä¸º4.21%å’Œ5.4%ï¼‰ï¼Œæœ€é«˜å¯è¾¾åˆ°16.12å€çš„å®æ—¶å› å­ï¼ˆRTFï¼‰åŠ é€Ÿæ¯”ç›¸æ¯”äºç¦»çº¿æ‰¹å¤„ç†æ¨¡å¼è‡ªé€‚åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24224v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºPromptä¸“å®¶æ··åˆçš„åœ¨çº¿å®æ—¶è€å¹´è¯­éŸ³è¯†åˆ«é€‚åº”æŠ€æœ¯ï¼ˆMOPSAï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°é›¶æ¥è§¦çŠ¶æ€ä¸‹çš„æœªçŸ¥æ¼”è®²è€…é€‚åº”ï¼Œä¸”ç»“åˆä¸“é—¨é’ˆå¯¹è€å¹´æ¼”è®²è€…çš„é¢†åŸŸçŸ¥è¯†è¿›è¡Œé€‚åº”æ€§ä¼˜åŒ–ã€‚é€šè¿‡ä½¿ç”¨Kå‡å€¼èšç±»æ–¹æ³•é€‰å‡ºæœ€å…·ç‰¹è‰²çš„å‰Kä¸ªæ¼”è®²æç¤ºä¸“å®¶ï¼Œå»ºç«‹ä¸“å®¶æ¨¡å‹ã€‚é€šè¿‡è®­ç»ƒè·¯ç”±å™¨ç½‘ç»œå®ç°ä¸“å®¶æ¨¡å‹çš„åŠ¨æ€ç»„åˆï¼Œé’ˆå¯¹è€å¹´äººåœ¨è¯­éŸ³å’Œè¯æ±‡æ°´å¹³çš„å·®å¼‚ä½¿ç”¨ç‰¹å®šçš„ç¼–ç å™¨ä¸è§£ç å™¨æç¤ºå¤„ç†è€³è¯­ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œä¸è¯´è¯äººç‹¬ç«‹æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨çº¿MOPSAè‡ªé€‚åº”åœ¨è‹±æ–‡ç—´å‘†é“¶è¡Œï¼ˆDementiaBank Pittï¼‰å’Œç²¤è¯­JCCOCCæµ‹è¯•è¯­æ–™åº“çš„è€å¹´è¯­éŸ³æ•°æ®é›†ä¸Šçš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰æˆ–å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰æ˜¾è‘—é™ä½äº†ç»å¯¹å€¼å’Œç›¸å¯¹å€¼ï¼Œåˆ†åˆ«ä¸ºç»å¯¹å‡å°‘çš„ç™¾åˆ†æ¯”ä¸º0.86%å’Œ1.47%ï¼ˆç›¸å¯¹å‡å°‘ç™¾åˆ†æ¯”ä¸º4.21%å’Œ5.40%ï¼‰ã€‚æ­¤å¤–ï¼Œç›¸å¯¹äºç¦»çº¿æ‰¹é‡æ¨¡å¼çš„é€‚åº”é€Ÿåº¦æå‡å¯è¾¾å®æ—¶ç³»æ•°ï¼ˆRTFï¼‰åŠ é€Ÿæ¯”ä¸ºé«˜è¾¾æœ€é«˜ä¸ºåœ¨å¹³å‡æ¯”è¾¾åˆ°è¾¾åˆ°æ¯”æé«˜é€Ÿæœ€é«˜æ—¶æœ€å¤§æœ€é«˜å¯æå‡åˆ°è¾¾æœ€å¤šæé«˜æœ€å¤§å€é€Ÿæœ€å¤šé«˜è¾¾èƒ½åˆ†åˆ«æ—¶æ¯”ç‡æ—¶çš„æ¯”çš„æ‰€æµ‹å¾—çš„æƒ…å†µä¸‹å¾—åˆ°çš„æ•°æ®è¯å®äº†è¿™ç‚¹å¯æä¾›ï¼Œä»æ¨¡æ‹Ÿçœ‹å‡ºä»¤äººç©ç›®çš„é«˜æ•ˆé€Ÿåº¦æå‡è‡³åˆ°åå››å€ä»¥ä¸Šæ•°æ®å…·æœ‰å¯é æ€§å¯è§‚æƒ…å†µä¸‹å¿«é€Ÿå¼€å‘å®é™…åº”ç”¨æ½œåŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºçš„MOPSAè‡ªé€‚åº”æŠ€æœ¯ä¸ºè€å¹´è¯­éŸ³è¯†åˆ«é¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong>ï¼š </p>
<ul>
<li>MOPSAæ–¹æ³•å®ç°äº†é›¶æ¥è§¦çŠ¶æ€ä¸‹çš„æœªçŸ¥æ¼”è®²è€…é€‚åº”ã€‚ </li>
<li>åˆ©ç”¨Kå‡å€¼èšç±»é€‰æ‹©æœ€å…·ç‰¹è‰²çš„æ¼”è®²æç¤ºä¸“å®¶ã€‚ </li>
<li>é‡‡ç”¨åŠ¨æ€è·¯ç”±å™¨ç½‘ç»œç»“åˆä¸“å®¶æ¨¡å‹å¤„ç†ä¸åŒè¯­éŸ³ç‰¹å¾çš„å·®å¼‚æ€§ä¿¡æ¯ã€‚ </li>
<li>åœ¨ä¸¤ä¸ªè€å¹´è¯­éŸ³æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå­—é”™è¯¯ç‡å’Œå­—ç¬¦é”™è¯¯ç‡æœ‰æ˜¾è‘—é™ä½ã€‚ </li>
<li>åœ¨çº¿è‡ªé€‚åº”é€Ÿåº¦è¾¾åˆ°è¾ƒé«˜çš„å®æ—¶ç³»æ•°åŠ é€Ÿæ¯”ï¼ˆRTFï¼‰ã€‚ </li>
<li>æ­¤ç ”ç©¶æ˜¾ç¤ºå‡ºå®é™…åº”ç”¨çš„å·¨å¤§æ½œåŠ›å¹¶æœ‰åˆ©äºåç»­æ·±å…¥ç ”ç©¶ä¸å®è·µè¿ç”¨ä¸å‘å±•é¢†åŸŸçš„ã€‚ä»å¦ä¸€æ–¹é¢å¼ºè°ƒè®ºç‚¹ä¸æŠ€æœ¯ç­–ç•¥çš„ç§¯ææ„ä¹‰ä»·å€¼å’Œè¶‹åŠ¿é¢„æµ‹æŠ€æœ¯åº”ç”¨çš„æœªæ¥å‘å±•æ–¹å‘ç­‰é‡è¦çš„çœ‹æ³•è¡¨è¾¾è¿›ä¸€æ­¥é‡è§†æœ¬ç ”ç©¶çš„æ·±è¿œå½±å“å¹¶ç»™å‡ºå»ºè®®æ–¹å‘å»ºè®®å¹¶æœŸæœ›åœ¨æœªæ¥èƒ½æœ‰æ›´å¤šç›¸å…³çš„ç ”ç©¶è¿›å±•æ¶‰åŠåœ¨ä¸åŒåœºåˆçš„è®¨è®ºæƒ…å¢ƒæä¾›ä¸€ç³»åˆ—ä¼˜ç§€äº§å“å’ŒæœåŠ¡è§£æ—¥ç›Šä¸¥å³»æŒ‘æˆ˜å¾—ä»¥å®Œæˆå£è¯­å¤„ç†æŠ€æœ¯ä¸æ–­åˆ›æ–°å’Œçªç ´ç­‰ä»»åŠ¡æ–¹é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a83ad5a64d220b1d4942f8fc2932a323.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e444f13fa342083cf55b67d22c27220.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ea87cfc2934037960269754a79bb3fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2c4fef9ec8cadfe981522fb5f36aa4d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FeatureSense-Protecting-Speaker-Attributes-in-Always-On-Audio-Sensing-System"><a href="#FeatureSense-Protecting-Speaker-Attributes-in-Always-On-Audio-Sensing-System" class="headerlink" title="FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing   System"></a>FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing   System</h2><p><strong>Authors:Bhawana Chhaglani, Sarmistha Sarna Gomasta, Yuvraj Agarwal, Jeremy Gummeson, Prashant Shenoy</strong></p>
<p>Audio is a rich sensing modality that is useful for a variety of human activity recognition tasks. However, the ubiquitous nature of smartphones and smart speakers with always-on microphones has led to numerous privacy concerns and a lack of trust in deploying these audio-based sensing systems. This paper addresses this critical challenge of preserving user privacy when using audio for sensing applications while maintaining utility. While prior work focuses primarily on protecting recoverable speech content, we show that sensitive speaker-specific attributes such as age and gender can still be inferred after masking speech and propose a comprehensive privacy evaluation framework to assess this speaker attribute leakage. We design and implement FeatureSense, an open-source library that provides a set of generalizable privacy-aware audio features that can be used for wide range of sensing applications. We present an adaptive task-specific feature selection algorithm that optimizes the privacy-utility-cost trade-off based on the application requirements. Through our extensive evaluation, we demonstrate the high utility of FeatureSense across a diverse set of sensing tasks. Our system outperforms existing privacy techniques by 60.6% in preserving user-specific privacy. This work provides a foundational framework for ensuring trust in audio sensing by enabling effective privacy-aware audio classification systems. </p>
<blockquote>
<p>éŸ³é¢‘æ˜¯ä¸€ç§ä¸°å¯Œçš„æ„ŸçŸ¥æ¨¡å¼ï¼Œå¯¹äºå„ç§äººç±»æ´»åŠ¨è¯†åˆ«ä»»åŠ¡éƒ½å¾ˆæœ‰ç”¨ã€‚ç„¶è€Œï¼Œå¸¦æœ‰å¸¸å¼€éº¦å…‹é£çš„æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½éŸ³ç®±çš„æ™®éæ€§å¼•å‘äº†äººä»¬å¯¹éšç§çš„æ‹…å¿§ï¼Œä»¥åŠå¯¹éƒ¨ç½²è¿™äº›åŸºäºéŸ³é¢‘çš„æ„ŸçŸ¥ç³»ç»Ÿçš„ä¸ä¿¡ä»»ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä½¿ç”¨éŸ³é¢‘è¿›è¡Œæ„ŸçŸ¥åº”ç”¨æ—¶ä¿æŠ¤ç”¨æˆ·éšç§çš„è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¿æŒå…¶å®ç”¨æ€§ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¿æŠ¤å¯æ¢å¤çš„è¯­éŸ³å†…å®¹ä¸Šï¼Œä½†æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å±è”½è¯­éŸ³åï¼Œä»ç„¶å¯ä»¥æ¨æ–­å‡ºå¹´é¾„å’Œæ€§åˆ«ç­‰æ•æ„Ÿçš„è¯´è¯äººç‰¹å®šå±æ€§ï¼Œå¹¶æå‡ºä¸€ä¸ªå…¨é¢çš„éšç§è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°è¿™ç§è¯´è¯äººå±æ€§æ³„éœ²ã€‚æˆ‘ä»¬è®¾è®¡å¹¶å®ç°äº†FeatureSenseï¼Œä¸€ä¸ªå¼€æºåº“ï¼Œæä¾›ä¸€å¥—é€šç”¨çš„éšç§æ„ŸçŸ¥éŸ³é¢‘ç‰¹å¾ï¼Œå¯ç”¨äºå¹¿æ³›çš„æ„ŸçŸ¥åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„ä»»åŠ¡ç‰¹å®šç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œæ ¹æ®åº”ç”¨è¦æ±‚ä¼˜åŒ–éšç§æ•ˆç”¨æˆæœ¬æƒè¡¡ã€‚é€šè¿‡æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬åœ¨å„ç§æ„ŸçŸ¥ä»»åŠ¡ä¸­å±•ç¤ºäº†FeatureSenseçš„é«˜å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¿æŠ¤ç”¨æˆ·ç‰¹å®šéšç§æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰éšç§æŠ€æœ¯è¾¾60.6%ã€‚è¿™é¡¹å·¥ä½œä¸ºç¡®ä¿éŸ³é¢‘æ„ŸçŸ¥çš„ä¿¡ä»»æä¾›äº†ä¸€ä¸ªåŸºç¡€æ¡†æ¶ï¼Œé€šè¿‡å®ç°æœ‰æ•ˆçš„éšç§æ„ŸçŸ¥éŸ³é¢‘åˆ†ç±»ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24115v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åœ¨ä½¿ç”¨éŸ³é¢‘æ„ŸçŸ¥åº”ç”¨æ—¶å¦‚ä½•ä¿æŠ¤ç”¨æˆ·éšç§çš„é—®é¢˜ã€‚æ–‡ç« æå‡ºä¸€ç§åä¸ºFeatureSenseçš„å¼€æºåº“ï¼Œå…¶ä¸­åŒ…å«ä¸€ç³»åˆ—é€šç”¨éšç§æ„ŸçŸ¥éŸ³é¢‘ç‰¹å¾ï¼Œæ—¨åœ¨é€‚åº”å¹¿æ³›çš„åº”ç”¨éœ€æ±‚ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿè‡ªé€‚åº”ä»»åŠ¡éœ€æ±‚è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œåœ¨éšç§ã€æ•ˆç”¨å’Œæˆæœ¬ä¹‹é—´æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒFeatureSenseåœ¨ä¸åŒæ„ŸçŸ¥ä»»åŠ¡ä¸­å…·æœ‰é«˜æ•ˆç”¨æ€§ï¼Œå¹¶åœ¨ç”¨æˆ·éšç§ä¿æŠ¤æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘æ˜¯ä¸€ç§ä¸°å¯Œçš„äººç±»æ´»åŠ¨æ„ŸçŸ¥æ¨¡æ€ï¼Œä½†æ™ºèƒ½è®¾å¤‡å’Œå§‹ç»ˆå¼€å¯çš„éº¦å…‹é£å¼•å‘äº†éšç§ä¿æŠ¤é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ä¿æŠ¤å¯æ¢å¤çš„è¯­éŸ³å†…å®¹ï¼Œä½†æ•æ„Ÿçš„ç”¨æˆ·ç‰¹å®šå±æ€§ï¼ˆå¦‚å¹´é¾„å’Œæ€§åˆ«ï¼‰åœ¨é®è”½è¯­éŸ³åä»ç„¶å¯ä»¥è¢«æ¨æ–­å‡ºæ¥ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»¼åˆçš„éšç§è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°è¿™ç§ç”¨æˆ·ç‰¹å®šå±æ€§çš„æ³„éœ²æƒ…å†µã€‚</li>
<li>è®¾è®¡ä¸å®ç°äº†FeatureSenseå¼€æºåº“ï¼Œæä¾›ä¸€å¥—é€šç”¨éšç§æ„ŸçŸ¥éŸ³é¢‘ç‰¹å¾ï¼Œé€‚ç”¨äºå„ç§æ„ŸçŸ¥åº”ç”¨ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”ä»»åŠ¡ç‰¹å®šç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œæ ¹æ®åº”ç”¨éœ€æ±‚ä¼˜åŒ–éšç§ã€æ•ˆç”¨å’Œæˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼ŒFeatureSenseåœ¨å¤šç§æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜æ•ˆç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a790dcb6496e995d102fa34ddb55e7a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59d1c712bc65d4253ef82e9b12a2b3a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df14e3a7a2e3c53923ed8fecab2a319b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-Emotion-Fool-Anti-spoofing"><a href="#Can-Emotion-Fool-Anti-spoofing" class="headerlink" title="Can Emotion Fool Anti-spoofing?"></a>Can Emotion Fool Anti-spoofing?</h2><p><strong>Authors:Aurosweta Mahapatra, Ismail Rasim Ulgen, Abinay Reddy Naini, Carlos Busso, Berrak Sisman</strong></p>
<p>Traditional anti-spoofing focuses on models and datasets built on synthetic speech with mostly neutral state, neglecting diverse emotional variations. As a result, their robustness against high-quality, emotionally expressive synthetic speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of emotional text-to-speech samples. Our analysis shows existing anti-spoofing models struggle with emotional synthetic speech, exposing risks of emotion-targeted attacks. Even trained on emotional data, the models underperform due to limited focus on emotional aspect and show performance disparities across emotions. This highlights the need for emotion-focused anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network. GEM performs effectively across all emotions and neutral state, improving defenses against spoofing attacks. We release the EmoSpoof-TTS Dataset: <a target="_blank" rel="noopener" href="https://emospoof-tts.github.io/Dataset/">https://emospoof-tts.github.io/Dataset/</a> </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æŠ—æ¬ºéª—æŠ€æœ¯ä¸»è¦å…³æ³¨åŸºäºä¸­æ€§çŠ¶æ€åˆæˆè¯­éŸ³çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå¿½è§†äº†æƒ…æ„Ÿå˜åŒ–çš„å¤šæ ·æ€§ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯¹äºé«˜è´¨é‡ã€æƒ…æ„Ÿä¸°å¯Œçš„åˆæˆè¯­éŸ³çš„ç¨³å¥æ€§å°šä¸ç¡®å®šã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥EmoSpoof-TTSâ€”â€”ä¸€ä¸ªæƒ…æ„Ÿæ–‡æœ¬åˆ°è¯­éŸ³æ ·æœ¬çš„è¯­æ–™åº“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œç°æœ‰çš„æŠ—æ¬ºéª—æ¨¡å‹åœ¨å¤„ç†æƒ…æ„Ÿåˆæˆè¯­éŸ³æ—¶é¢ä¸´å›°éš¾ï¼Œè¿™æš´éœ²äº†é’ˆå¯¹æƒ…æ„Ÿçš„æ”»å‡»é£é™©ã€‚å³ä½¿åœ¨æƒ…æ„Ÿæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ¨¡å‹ç”±äºç¼ºä¹å¯¹æƒ…æ„Ÿæ–¹é¢çš„é‡ç‚¹å…³æ³¨è€Œè¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”åœ¨å„ç§æƒ…æ„Ÿä¹‹é—´è¡¨ç°å‡ºæ€§èƒ½å·®å¼‚ã€‚è¿™å‡¸æ˜¾äº†åœ¨æ•°æ®é›†å’Œæ–¹æ³•è®ºä¸Šéƒ½éœ€è¦ä»¥æƒ…æ„Ÿä¸ºä¸­å¿ƒçš„æŠ—æ¬ºéª—èŒƒå¼ã€‚æˆ‘ä»¬æå‡ºäº†GEMï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«é—¨æ§ç½‘ç»œçš„æƒ…æ„Ÿä¸“ä¸šæ¨¡å‹é—¨æ§é›†åˆã€‚GEMåœ¨æ‰€æœ‰æƒ…æ„Ÿå’Œä¸­æ€§çŠ¶æ€ä¸‹éƒ½èƒ½æœ‰æ•ˆè¿è¡Œï¼Œæé«˜äº†å¯¹æ¬ºéª—æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒäº†EmoSpoof-TTSæ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://emospoof-tts.github.io/Dataset/">https://emospoof-tts.github.io/Dataset/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23962v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¼ ç»Ÿåæ¬ºéª—æŠ€æœ¯åœ¨å¤„ç†æƒ…æ„Ÿæ€§è¯­éŸ³åˆæˆæ–¹é¢çš„ä¸è¶³ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æƒ…æ„Ÿè¯­éŸ³åˆæˆè¯­æ–™åº“EmoSpoof-TTSã€‚åˆ†æè¡¨æ˜ï¼Œç°æœ‰åæ¬ºéª—æ¨¡å‹éš¾ä»¥åº”å¯¹æƒ…æ„Ÿåˆæˆè¯­éŸ³ï¼Œå­˜åœ¨æƒ…æ„Ÿç›®æ ‡æ”»å‡»çš„é£é™©ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºæƒ…æ„Ÿä¸“ä¸šåŒ–æ¨¡å‹çš„é—¨æ§é›†æˆæ–¹æ³•GEMï¼Œå¹¶é…ä»¥è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«é—¨æ§ç½‘ç»œï¼Œèƒ½æœ‰æ•ˆåº”å¯¹å„ç§æƒ…æ„Ÿå’Œä¸­æ€§çŠ¶æ€ä¸‹çš„æ¬ºéª—æ”»å‡»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿåæ¬ºéª—æŠ€æœ¯ä¸»è¦å…³æ³¨ä¸­æ€§çŠ¶æ€çš„åˆæˆè¯­éŸ³ï¼Œå¿½è§†äº†æƒ…æ„Ÿå¤šæ ·æ€§ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¯¹äºé«˜è´¨é‡çš„æƒ…æ„Ÿè¡¨è¾¾åˆæˆè¯­éŸ³çš„é²æ£’æ€§å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>æƒ…æ„Ÿåˆæˆè¯­éŸ³åˆ†ææ˜¾ç¤ºç°æœ‰åæ¬ºéª—æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œå­˜åœ¨æƒ…æ„Ÿç›®æ ‡æ”»å‡»é£é™©ã€‚</li>
<li>å³ä½¿ç»è¿‡æƒ…æ„Ÿæ•°æ®è®­ç»ƒï¼Œæ¨¡å‹åœ¨æƒ…æ„Ÿæ–¹é¢çš„å…³æ³¨ä»ç„¶æœ‰é™ï¼Œä¸åŒæƒ…æ„Ÿé—´çš„æ€§èƒ½å­˜åœ¨å·®å¼‚ã€‚</li>
<li>æå‡ºäº†åŸºäºæƒ…æ„Ÿä¸“æ³¨çš„åæ¬ºéª—æ¨¡å¼éœ€æ±‚ï¼ŒåŒ…æ‹¬æ•°æ®é›†å’Œæ–¹æ³•è®ºã€‚</li>
<li>å¼•å…¥äº†EmoSpoof-TTSæ•°æ®é›†ï¼Œç”¨äºå¢å¼ºå¯¹æƒ…æ„Ÿè¯­éŸ³åˆæˆçš„é˜²å¾¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ce5620bd53aacd9c83fa902dc9199bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-588faf6169be3c06821f6990319fce1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9b44663950772cdcdae76078533ac5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9cc257a6b9305cd0dd0e14443b40fc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10a50e1b669c8759ece9205fc26fe45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb05069022a30128e3728765eba16c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8640af454563e5c4427c9aff1415cae5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Auxiliary-Information-in-Text-to-Video-Retrieval-A-Review"><a href="#Leveraging-Auxiliary-Information-in-Text-to-Video-Retrieval-A-Review" class="headerlink" title="Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review"></a>Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review</h2><p><strong>Authors:Adriano Fragomeni, Dima Damen, Michael Wray</strong></p>
<p>Text-to-Video (T2V) retrieval aims to identify the most relevant item from a gallery of videos based on a userâ€™s text query. Traditional methods rely solely on aligning video and text modalities to compute the similarity and retrieve relevant items. However, recent advancements emphasise incorporating auxiliary information extracted from video and text modalities to improve retrieval performance and bridge the semantic gap between these modalities. Auxiliary information can include visual attributes, such as objects; temporal and spatial context; and textual descriptions, such as speech and rephrased captions. This survey comprehensively reviews 81 research papers on Text-to-Video retrieval that utilise such auxiliary information. It provides a detailed analysis of their methodologies; highlights state-of-the-art results on benchmark datasets; and discusses available datasets and their auxiliary information. Additionally, it proposes promising directions for future research, focusing on different ways to further enhance retrieval performance using this information. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ£€ç´¢æ—¨åœ¨æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æŸ¥è¯¢ä»è§†é¢‘åº“ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„é¡¹ç›®ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä»…ä¾èµ–äºå¯¹é½è§†é¢‘å’Œæ–‡æœ¬æ¨¡å¼æ¥è®¡ç®—ç›¸ä¼¼æ€§å’Œæ£€ç´¢ç›¸å…³é¡¹ç›®ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¿›å±•å¼ºè°ƒèå…¥ä»è§†é¢‘å’Œæ–‡æœ¬æ¨¡å¼ä¸­æå–çš„è¾…åŠ©ä¿¡æ¯ï¼Œä»¥æé«˜æ£€ç´¢æ€§èƒ½å¹¶ç¼©å°è¿™äº›æ¨¡å¼ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚è¾…åŠ©ä¿¡æ¯å¯ä»¥åŒ…æ‹¬è§†è§‰å±æ€§ï¼Œå¦‚ç‰©ä½“ï¼›æ—¶é—´å’Œç©ºé—´ä¸Šä¸‹æ–‡ï¼›ä»¥åŠæ–‡æœ¬æè¿°ï¼Œå¦‚è¯­éŸ³å’Œé‡æ–°è¡¨è¿°çš„æ ‡é¢˜ã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢å›é¡¾äº†81ç¯‡å…³äºåˆ©ç”¨æ­¤ç±»è¾…åŠ©ä¿¡æ¯è¿›è¡Œæ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢çš„ç ”ç©¶è®ºæ–‡ã€‚å®ƒæä¾›äº†å¯¹å…¶æ–¹æ³•çš„è¯¦ç»†åˆ†æï¼›çªå‡ºäº†åŸºå‡†æ•°æ®é›†ä¸Šçš„æœ€æ–°ç»“æœï¼›å¹¶è®¨è®ºäº†å¯ç”¨çš„æ•°æ®é›†åŠå…¶è¾…åŠ©ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºäº†æœªæ¥ç ”ç©¶çš„å‡ ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•åˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›ä¸€æ­¥æé«˜æ£€ç´¢æ€§èƒ½çš„ä¸åŒæ–¹å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23952v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æ–‡æœ¬ä¸è§†é¢‘æ£€ç´¢ç»¼åˆè€ƒå¯Ÿç ”ç©¶æŠ¥å‘Šç»¼è¿°äº†åŸºäºè¾…åŠ©ä¿¡æ¯çš„æ–‡æœ¬ä¸è§†é¢‘æ£€ç´¢çš„ç›¸å…³ç ”ç©¶è®ºæ–‡ï¼ŒåŒ…æ‹¬è§†è§‰å±æ€§å’Œæ–‡æœ¬æè¿°ç­‰è¾…åŠ©ä¿¡æ¯çš„ä½¿ç”¨ã€‚æŠ¥å‘Šè¯¦ç»†åˆ†æäº†è¿™äº›æ–¹æ³•çš„å®ç°æ–¹å¼ï¼Œçªå‡ºäº†åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æœ€æ–°æˆæœï¼Œå¹¶è®¨è®ºäº†ç°æœ‰æ•°æ®é›†åŠå…¶è¾…åŠ©ä¿¡æ¯ã€‚åŒæ—¶ï¼ŒæŠ¥å‘Šä¹ŸæŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå»ºè®®å¦‚ä½•åˆ©ç”¨è¾…åŠ©ä¿¡æ¯è¿›ä¸€æ­¥æé«˜æ£€ç´¢æ€§èƒ½ã€‚è¯¥æŠ¥å‘Šæ¶‰åŠçš„æ–¹æ³•æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€èåˆç¼©å°æ–‡æœ¬ä¸è§†é¢‘ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>T2Væ£€ç´¢æ—¨åœ¨æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æŸ¥è¯¢ä»è§†é¢‘åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„è§†é¢‘ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–è§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€çš„å¯¹é½æ¥è®¡ç®—ç›¸ä¼¼æ€§å’Œæ£€ç´¢ç›¸å…³é¡¹ç›®ã€‚</li>
<li>è¿‘å¹´æ¥çš„ç ”ç©¶å¼ºè°ƒåˆ©ç”¨ä»è§†é¢‘å’Œæ–‡æœ¬ä¸­æå–çš„è¾…åŠ©ä¿¡æ¯æ¥å¢å¼ºæ£€ç´¢æ€§èƒ½å’Œç¼©å°æ¨¡æ€é—´çš„è¯­ä¹‰å·®è·ã€‚</li>
<li>è¾…åŠ©ä¿¡æ¯åŒ…æ‹¬è§†è§‰å±æ€§ï¼ˆå¦‚å¯¹è±¡ï¼‰ã€æ—¶ç©ºä¸Šä¸‹æ–‡å’Œæ–‡æœ¬æè¿°ï¼ˆå¦‚è¯­éŸ³å’Œé‡æ–°è¡¨è¿°çš„æ ‡é¢˜ï¼‰ã€‚</li>
<li>è¯¥æŠ¥å‘Šè¯¦ç»†ç»¼è¿°äº†å…³äºå¦‚ä½•åˆ©ç”¨è¿™äº›è¾…åŠ©ä¿¡æ¯è¿›è¡ŒT2Væ£€ç´¢çš„81ç¯‡ç ”ç©¶è®ºæ–‡ã€‚</li>
<li>æŠ¥å‘Šè¯¦ç»†åˆ†æäº†è¿™äº›è®ºæ–‡çš„æ–¹æ³•è®ºï¼Œå¹¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçªå‡ºäº†æœ€æ–°æˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b2c79bf84e93ad73443b5641a7eeed3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533274df90f7971d03b6aeb19ca27ae0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164e1b1232f2cbc680ed7dca8aa1019d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EmotionTalk-An-Interactive-Chinese-Multimodal-Emotion-Dataset-With-Rich-Annotations"><a href="#EmotionTalk-An-Interactive-Chinese-Multimodal-Emotion-Dataset-With-Rich-Annotations" class="headerlink" title="EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich   Annotations"></a>EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich   Annotations</h2><p><strong>Authors:Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</strong></p>
<p>In recent years, emotion recognition plays a critical role in applications such as human-computer interaction, mental health monitoring, and sentiment analysis. While datasets for emotion analysis in languages such as English have proliferated, there remains a pressing need for high-quality, comprehensive datasets tailored to the unique linguistic, cultural, and multimodal characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an interactive Chinese multimodal emotion dataset with rich annotations. This dataset provides multimodal information from 19 actors participating in dyadic conversational settings, incorporating acoustic, visual, and textual modalities. It includes 23.6 hours of speech (19,250 utterances), annotations for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative, neutral, weakly positive, and positive) and 4-dimensional speech captions (speaker, speaking style, emotion and overall). The dataset is well-suited for research on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. To our knowledge, it represents the first high-quality and versatile Chinese dialogue multimodal emotion dataset, which is a valuable contribution to research on cross-cultural emotion analysis and recognition. Additionally, we conduct experiments on EmotionTalk to demonstrate the effectiveness and quality of the dataset. It will be open-source and freely available for all academic purposes. The dataset and codes will be made available at: <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/EmotionTalk">https://github.com/NKU-HLT/EmotionTalk</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæƒ…æ„Ÿè¯†åˆ«åœ¨äººæœºäº¤äº’ã€å¿ƒç†å¥åº·ç›‘æµ‹å’Œæƒ…æ„Ÿåˆ†æç­‰é¢†åŸŸçš„åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶è‹±è¯­æƒ…æ„Ÿåˆ†æçš„æ•°æ®é›†å·²ç»å¤§é‡æ¶Œç°ï¼Œä½†å¯¹äºå…·æœ‰ç‹¬ç‰¹è¯­è¨€ã€æ–‡åŒ–å’Œå¤šæ¨¡æ€ç‰¹å¾çš„ä¸­å›½ï¼Œä»è¿«åˆ‡éœ€è¦é«˜è´¨é‡çš„ç»¼åˆæ•°æ®é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>EmotionTalk</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰ä¸°å¯Œæ³¨é‡Šçš„äº¤äº’å¼ä¸­æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æä¾›äº†æ¥è‡ª19åæ¼”å‘˜åœ¨åŒäººå¯¹è¯ç¯å¢ƒä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œèåˆäº†å£°éŸ³ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ã€‚å®ƒåŒ…å«23.6å°æ—¶çš„è¯­éŸ³ï¼ˆ19,250ä¸ªé™ˆè¿°ï¼‰ã€7ä¸ªé™ˆè¿°çº§æƒ…æ„Ÿç±»åˆ«çš„æ³¨é‡Šï¼ˆå¿«ä¹ã€æƒŠè®¶ã€æ‚²ä¼¤ã€åŒæ¶ã€æ„¤æ€’ã€ææƒ§å’Œä¸­æ€§ï¼‰ã€5ç»´æƒ…æ„Ÿæ ‡ç­¾ï¼ˆè´Ÿé¢ã€è½»å¾®è´Ÿé¢ã€ä¸­æ€§ã€è½»å¾®æ­£é¢å’Œæ­£é¢ï¼‰å’Œ4ç»´è¯­éŸ³å­—å¹•ï¼ˆè¯´è¯è€…ã€è¯´è¯é£æ ¼ã€æƒ…æ„Ÿå’Œæ€»ä½“ï¼‰ã€‚è¯¥æ•°æ®é›†é€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ã€ç¼ºå¤±æ¨¡æ€æŒ‘æˆ˜å’Œè¯­éŸ³å­—å¹•ä»»åŠ¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªé«˜è´¨é‡ä¸”é€šç”¨çš„ä¸­æ–‡å¯¹è¯å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¯¹è·¨æ–‡åŒ–æƒ…æ„Ÿåˆ†æå’Œè¯†åˆ«ç ”ç©¶åšå‡ºäº†å®è´µçš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨EmotionTalkä¸Šè¿›è¡Œäº†å®éªŒï¼Œä»¥è¯æ˜è¯¥æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œè´¨é‡ã€‚è¯¥æ•°æ®é›†å°†å¼€æºå¹¶å…è´¹ä¾›æ‰€æœ‰å­¦æœ¯ç›®çš„ä½¿ç”¨ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/EmotionTalk">https://github.com/NKU-HLT/EmotionTalk</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23018v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºEmotionTalkçš„ä¸­æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª19åå‚ä¸è€…åœ¨å¯¹è¯ç¯å¢ƒä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å£°éŸ³ã€è§†é¢‘å’Œæ–‡å­—ã€‚æ•°æ®é›†åŒ…å«ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œå¦‚æƒ…æ„Ÿç±»åˆ«ã€æƒ…æ„Ÿç»´åº¦å’Œè¯­éŸ³å­—å¹•ç­‰ã€‚è¯¥æ•°æ®é›†é€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ï¼Œç¼ºå¤±æ¨¡æ€æŒ‘æˆ˜å’Œè¯­éŸ³å­—å¹•ä»»åŠ¡ç­‰ã€‚è¿™æ˜¯é¦–ä¸ªé«˜è´¨é‡ã€å¤šåŠŸèƒ½çš„ä¸­æ–‡å¯¹è¯å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¯¹è·¨æ–‡åŒ–æƒ…æ„Ÿåˆ†æå’Œè¯†åˆ«ç ”ç©¶å…·æœ‰å®è´µè´¡çŒ®ã€‚æ•°æ®é›†å°†å¼€æºå¹¶å…è´¹æä¾›ç»™æ‰€æœ‰å­¦æœ¯ç”¨é€”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmotionTalkæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡çš„å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«å£°éŸ³ã€è§†é¢‘å’Œæ–‡å­—ä¿¡æ¯ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿç±»åˆ«ã€æƒ…æ„Ÿç»´åº¦å’Œè¯­éŸ³å­—å¹•ç­‰ã€‚</li>
<li>æ•°æ®é›†é€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ã€‚</li>
<li>æ•°æ®é›†å…·æœ‰ç¼ºå¤±æ¨¡æ€æŒ‘æˆ˜çš„ç‰¹æ€§ï¼Œé€‚ç”¨äºå¤„ç†å®é™…åº”ç”¨ä¸­çš„ç¼ºå¤±æ•°æ®é—®é¢˜ã€‚</li>
<li>EmotionTalkæ˜¯é¦–ä¸ªé«˜è´¨é‡ã€å¤šåŠŸèƒ½çš„ä¸­æ–‡å¯¹è¯å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†å¯¹è·¨æ–‡åŒ–æƒ…æ„Ÿåˆ†æå’Œè¯†åˆ«ç ”ç©¶å…·æœ‰å®è´µè´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54f8936dd00e9ec5649721421c126e40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da46e6fee0874d1347830a4333e9940c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4076e5a17be011cd67b2957450086680.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Assessment-of-Pathological-Voices-via-Combined-Low-Level-Descriptors-and-Foundation-Model-Representations"><a href="#Towards-Robust-Assessment-of-Pathological-Voices-via-Combined-Low-Level-Descriptors-and-Foundation-Model-Representations" class="headerlink" title="Towards Robust Assessment of Pathological Voices via Combined Low-Level   Descriptors and Foundation Model Representations"></a>Towards Robust Assessment of Pathological Voices via Combined Low-Level   Descriptors and Foundation Model Representations</h2><p><strong>Authors:Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao</strong></p>
<p>Perceptual voice quality assessment is essential for diagnosing and monitoring voice disorders by providing standardized evaluations of vocal function. Traditionally, expert raters use standard scales such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are subjective and prone to inter-rater variability, motivating the need for automated, objective assessment methods. This study proposes Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to extract high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we also introduce VOQANet+, which integrates low-level speech descriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings into a hybrid representation. Unlike prior studies focused only on vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality Dataset (PVQD), we evaluate our models on both vowel-based and sentence-level speech (PVQD-S subset) to improve generalizability. Results show that sentence-based input outperforms vowel-based input, especially at the patient level, underscoring the value of longer utterances for capturing perceptual voice attributes. VOQANet consistently surpasses baseline methods in root mean squared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V and GRBAS dimensions, with VOQANet+ achieving even better performance. Additional experiments under noisy conditions show that VOQANet+ maintains high prediction accuracy and robustness, supporting its potential for real-world and telehealth deployment. </p>
<blockquote>
<p>æ„ŸçŸ¥è¯­éŸ³è´¨é‡è¯„ä¼°å¯¹äºé€šè¿‡æ ‡å‡†åŒ–è¯­éŸ³åŠŸèƒ½è¯„ä¼°æ¥è¯Šæ–­å’Œç›‘ç£è¯­éŸ³éšœç¢è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿä¸Šï¼Œä¸“å®¶è¯„ä¼°è€…ä½¿ç”¨æ ‡å‡†é‡è¡¨ï¼Œå¦‚å…±è¯†å¬è§‰æ„ŸçŸ¥è¯­éŸ³è¯„ä¼°ï¼ˆCAPE-Vï¼‰å’Œç­‰çº§ã€ç²—ç³™åº¦ã€å‘¼å¸å£°ã€è™šå¼±å’Œç´§å¼ ï¼ˆGRBASï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æŒ‡æ ‡æ˜¯ä¸»è§‚çš„å¹¶ä¸”å®¹æ˜“äº§ç”Ÿè¯„ä»·è€…ä¹‹é—´çš„å·®å¼‚ï¼Œå› æ­¤éœ€è¦è‡ªåŠ¨ã€å®¢è§‚çš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³è´¨é‡è¯„ä¼°ç½‘ç»œï¼ˆVOQANetï¼‰ï¼Œè¯¥ç½‘ç»œå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶åˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ä»åŸå§‹è¯­éŸ³ä¸­æå–é«˜çº§å£°éŸ³å’ŒéŸµå¾‹ä¿¡æ¯ã€‚ä¸ºäº†å¢å¼ºç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†VOQANet+ï¼Œå®ƒå°†ä½çº§åˆ«è¯­éŸ³æè¿°ç¬¦ï¼ˆå¦‚é¢¤æŠ–ã€æ³¢åŠ¨å’Œè°æ³¢ä¸å™ªå£°æ¯”ï¼ˆHNRï¼‰ï¼‰ä¸SFMåµŒå…¥åˆ°æ··åˆè¡¨ç¤ºä¸­ã€‚ä¸åŒäºä»¥å‰åªå…³æ³¨æ„ŸçŸ¥è¯­éŸ³è´¨é‡æ•°æ®é›†ï¼ˆPVQDï¼‰ä¸­çš„åŸºäºå…ƒéŸ³çš„å‘éŸ³ï¼ˆPVQD-Aå­é›†ï¼‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†åŸºäºå…ƒéŸ³å’Œå¥å­çº§åˆ«çš„è¯­éŸ³ï¼ˆPVQD-Så­é›†ï¼‰çš„è¯„ä¼°ï¼Œä»¥æé«˜å…¶é€šç”¨æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºå¥å­çš„è¾“å…¥ä¼˜äºåŸºäºå…ƒéŸ³çš„è¾“å…¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‚£è€…å±‚é¢ï¼Œè¿™å¼ºè°ƒäº†è¾ƒé•¿è¯­éŸ³ç‰‡æ®µåœ¨æ•æ‰æ„ŸçŸ¥è¯­éŸ³å±æ€§æ–¹é¢çš„ä»·å€¼ã€‚VOQANetåœ¨CAPE-Vå’ŒGRBASç»´åº¦çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å’Œçš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPCCï¼‰æ–¹é¢å§‹ç»ˆè¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œè€ŒVOQANet+ç”šè‡³å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„é™„åŠ å®éªŒè¡¨æ˜ï¼ŒVOQANet+ä¿æŒé«˜é¢„æµ‹ç²¾åº¦å’Œç¨³å¥æ€§ï¼Œæ”¯æŒå…¶åœ¨ç°å®ä¸–ç•Œå’Œè¿œç¨‹åŒ»ç–—éƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21356v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä¼°å¯¹äºé€šè¿‡æ ‡å‡†åŒ–çš„å—“éŸ³åŠŸèƒ½è¯„ä»·æ¥è¯Šæ–­å’Œæ²»ç–—è¯­éŸ³éšœç¢è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿä¸Šï¼Œä¸“å®¶è¯„åˆ†è€…ä½¿ç”¨è¯¸å¦‚CAPE-Vå’ŒGRBASç­‰æ ‡å‡†é‡è¡¨è¿›è¡Œè¯„ä¼°ï¼Œä½†è¿™äº›æŒ‡æ ‡å…·æœ‰ä¸»è§‚æ€§ï¼Œå­˜åœ¨è¯„åˆ†è€…é—´å˜å¼‚æ€§çš„ç¼ºç‚¹ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦å®¢è§‚è‡ªåŠ¨çš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„Voice Quality Assessment Networkï¼ˆVOQANetï¼‰ï¼Œè¯¥ç½‘ç»œå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ä»åŸå§‹è¯­éŸ³ä¸­æå–é«˜çº§å£°å­¦å’ŒéŸµå¾‹ä¿¡æ¯ã€‚ä¸ºæé«˜ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†VOQANet+ï¼Œå®ƒå°†ä½çº§è¯­éŸ³æè¿°ç¬¦ï¼ˆå¦‚æŠ–åŠ¨ã€å’å£°å’Œè°æ³¢å™ªå£°æ¯”ï¼ˆHNRï¼‰ï¼‰ä¸SFMåµŒå…¥ç›¸ç»“åˆå½¢æˆæ··åˆè¡¨ç¤ºã€‚ä¸ä»¥å¾€ä»…å…³æ³¨æ„ŸçŸ¥è¯­éŸ³è´¨é‡æ•°æ®é›†ï¼ˆPVQDï¼‰çš„å…ƒéŸ³å‘éŸ³å­é›†çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬å¯¹å…ƒéŸ³å‘éŸ³å’Œå¥å­æ°´å¹³çš„è¯­éŸ³ï¼ˆPVQD-Så­é›†ï¼‰è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå¥å­ä¸ºåŸºç¡€çš„è¾“å…¥ä¼˜äºå…ƒéŸ³ä¸ºåŸºç¡€çš„è¾“å…¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‚£è€…å±‚é¢ï¼Œè¿™å¼ºè°ƒäº†è¾ƒé•¿è¯è¯­åœ¨æ•æ‰æ„ŸçŸ¥è¯­éŸ³å±æ€§æ–¹é¢çš„ä»·å€¼ã€‚VOQANetåœ¨CAPE-Vå’ŒGRBASå„ç»´åº¦ä¸Šçš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å’Œçš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPCCï¼‰ä¸Šå‡è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œè€ŒVOQANet+çš„è¡¨ç°æ›´ä½³ã€‚åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é¢å¤–å®éªŒè¡¨æ˜ï¼ŒVOQANet+ä¿æŒäº†è¾ƒé«˜çš„é¢„æµ‹ç²¾åº¦å’Œç¨³å¥æ€§ï¼Œæ”¯æŒå…¶åœ¨ç°å®å’Œè¿œç¨‹åŒ»ç–—éƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä¼°æ˜¯è¯Šæ–­å’Œç®¡ç†è¯­éŸ³éšœç¢çš„é‡è¦å·¥å…·ï¼Œéœ€è¦å®¢è§‚ã€è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ–¹æ³•ä»¥å‡è½»ä¸»è§‚å˜å¼‚æ€§çš„å½±å“ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„VOQANetæ¨¡å‹ï¼Œå¯ä»¥æå–é«˜çº§å£°å­¦ç‰¹å¾ï¼Œå¯¹è¯­éŸ³è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¼•å…¥çš„VOQANet+æ¨¡å‹è¿›ä¸€æ­¥ç»“åˆäº†ä½çº§è¯­éŸ³æè¿°ç¬¦å’Œé«˜çº§ç‰¹å¾ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°å¥å­æ°´å¹³çš„è¯­éŸ³è¯„ä¼°ç›¸æ¯”å…ƒéŸ³æ°´å¹³çš„è¯„ä¼°æ›´å…·ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®ä¸–ç•Œçš„ç¯å¢ƒä¸‹ã€‚</li>
<li>VOQANetåœ¨å„ç§æµ‹è¯•æŒ‡æ ‡ä¸Šè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>VOQANet+åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é¢„æµ‹æ€§èƒ½ç¨³å®šï¼Œæ˜¾ç¤ºå‡ºåœ¨è¿œç¨‹åŒ»ç–—ç­‰å®é™…åœºæ™¯ä¸­åº”ç”¨çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea571ee9ba504e1b1396f800b8b57f80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2b10504e8e4a3d157bf2f71304a2189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b118dd181014e199f6b257b85c38a59d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e1afcdd4f66db0a59f1b3d1b51077fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c15b2150abb49da35ba3f959e61536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b108b54388ae0969801dfd1f4b2b690.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation"><a href="#EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation" class="headerlink" title="EASY: Emotion-aware Speaker Anonymization via Factorized Distillation"></a>EASY: Emotion-aware Speaker Anonymization via Factorized Distillation</h2><p><strong>Authors:Jixun Yao, Hexin Liu, Eng Siong Chng, Lei Xie</strong></p>
<p>Emotion plays a significant role in speech interaction, conveyed through tone, pitch, and rhythm, enabling the expression of feelings and intentions beyond words to create a more personalized experience. However, most existing speaker anonymization systems employ parallel disentanglement methods, which only separate speech into linguistic content and speaker identity, often neglecting the preservation of the original emotional state. In this study, we introduce EASY, an emotion-aware speaker anonymization framework. EASY employs a novel sequential disentanglement process to disentangle speaker identity, linguistic content, and emotional representation, modeling each speech attribute in distinct subspaces through a factorized distillation approach. By independently constraining speaker identity and emotional representation, EASY minimizes information leakage, enhancing privacy protection while preserving original linguistic content and emotional state. Experimental results on the VoicePrivacy Challenge official datasets demonstrate that our proposed approach outperforms all baseline systems, effectively protecting speaker privacy while maintaining linguistic content and emotional state. </p>
<blockquote>
<p>æƒ…æ„Ÿåœ¨è¯­éŸ³äº¤äº’ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œé€šè¿‡è¯­è°ƒã€éŸ³é«˜å’ŒèŠ‚å¥æ¥ä¼ è¾¾ï¼Œä½¿äººä»¬åœ¨è¨€è¯­ä¹‹å¤–èƒ½å¤Ÿè¡¨è¾¾æ„Ÿå—å’Œæ„å›¾ï¼Œä»è€Œåˆ›é€ æ›´åŠ ä¸ªæ€§åŒ–çš„ä½“éªŒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è¯´è¯äººåŒ¿ååŒ–ç³»ç»Ÿé‡‡ç”¨å¹¶è¡Œåˆ†ç¦»æ–¹æ³•ï¼Œä»…å°†è¯­éŸ³åˆ†ç¦»ä¸ºè¯­è¨€å†…å®¹å’Œè¯´è¯äººèº«ä»½ï¼Œå¾€å¾€å¿½è§†äº†åŸå§‹æƒ…æ„ŸçŠ¶æ€çš„ä¿ç•™ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æƒ…æ„Ÿæ„ŸçŸ¥è¯´è¯äººåŒ¿ååŒ–æ¡†æ¶EASYã€‚EASYé‡‡ç”¨æ–°é¢–çš„é¡ºåºåˆ†ç¦»è¿‡ç¨‹æ¥åˆ†ç¦»è¯´è¯äººèº«ä»½ã€è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿè¡¨ç¤ºï¼Œé€šè¿‡å› å­è’¸é¦æ³•å°†æ¯ç§è¯­éŸ³å±æ€§å»ºæ¨¡åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­ã€‚é€šè¿‡ç‹¬ç«‹çº¦æŸè¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿè¡¨ç¤ºï¼ŒEASYå‡å°‘äº†ä¿¡æ¯æ³„éœ²ï¼Œåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿ç•™äº†åŸå§‹çš„è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚åœ¨VoicePrivacy Challengeå®˜æ–¹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿ç³»ç»Ÿï¼Œåœ¨ä¿æŠ¤è¯´è¯äººéšç§çš„åŒæ—¶ï¼Œä¿æŒè¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15004v2">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>æ€»ç»“</strong><br>    è¯¥ç ”ç©¶ä»‹ç»äº†æƒ…æ„Ÿæ„ŸçŸ¥çš„è¯´è¯äººåŒ¿ååŒ–æ¡†æ¶â€”â€”EASYã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°çš„è¿ç»­è§£çº ç¼ è¿‡ç¨‹ï¼Œå°†è¯´è¯äººçš„èº«ä»½ã€è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿè¡¨è¾¾è¿›è¡Œåˆ†ç¦»ï¼Œå¹¶é€šè¿‡å› å­è’¸é¦çš„æ–¹æ³•å°†æ¯ä¸ªè¯­éŸ³å±æ€§å»ºæ¨¡åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­ã€‚é€šè¿‡ç‹¬ç«‹çº¦æŸè¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿè¡¨è¾¾ï¼ŒEASYå‡å°‘äº†ä¿¡æ¯æ³„éœ²ï¼Œå¢å¼ºäº†éšç§ä¿æŠ¤ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹çš„è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚åœ¨VoicePrivacy Challengeå®˜æ–¹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿ç³»ç»Ÿï¼Œæœ‰æ•ˆåœ°ä¿æŠ¤äº†è¯´è¯äººçš„éšç§ï¼ŒåŒæ—¶ä¿æŒäº†è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æƒ…æ„Ÿåœ¨è¯­éŸ³äº¤äº’ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œé€šè¿‡è¯­è°ƒã€éŸ³é«˜å’ŒèŠ‚å¥æ¥ä¼ è¾¾ã€‚</li>
<li>ç°æœ‰çš„å¤§å¤šæ•°è¯´è¯äººåŒ¿ååŒ–ç³»ç»Ÿåªå…³æ³¨å°†è¯­éŸ³åˆ†ç¦»ä¸ºè¯­è¨€å†…å®¹å’Œè¯´è¯äººèº«ä»½ï¼Œå¿½è§†äº†æƒ…æ„ŸçŠ¶æ€çš„ä¿æŒã€‚</li>
<li>EASYæ˜¯ä¸€ä¸ªæƒ…æ„Ÿæ„ŸçŸ¥çš„è¯´è¯äººåŒ¿ååŒ–æ¡†æ¶ï¼Œé‡‡ç”¨æ–°çš„è¿ç»­è§£çº ç¼ è¿‡ç¨‹æ¥åˆ†ç¦»è¯´è¯äººçš„èº«ä»½ã€è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>EASYé€šè¿‡å› å­è’¸é¦çš„æ–¹æ³•å»ºæ¨¡æ¯ä¸ªè¯­éŸ³å±æ€§ï¼Œå°†è¯´è¯äººçš„èº«ä»½å’Œæƒ…æ„Ÿè¡¨è¾¾ç‹¬ç«‹çº¦æŸï¼Œä»¥æœ€å°åŒ–ä¿¡æ¯æ³„éœ²ã€‚</li>
<li>EASYåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿ç•™äº†åŸå§‹çš„è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚</li>
<li>åœ¨VoicePrivacy Challengeå®˜æ–¹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEASYçš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºçº¿ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8562c7f7bb203476c37b82d30e0ea994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4447f99c345bfed7ffe7bc2027ab6804.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a02d0aef7ed82018b44e58b9315f761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1f849ec7fbb77f41f6ae55c4157ad98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6e996f0bc5868ad3725b83adc77094.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis"><a href="#TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis" class="headerlink" title="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"></a>TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao</strong></p>
<p>Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. Singing voice samples are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/">https://aaronz345.github.io/TCSinger2Demo/</a>. </p>
<blockquote>
<p>å¯å®šåˆ¶çš„å¤šè¯­è¨€é›¶æ ·æœ¬æ­Œå£°åˆæˆï¼ˆSVSï¼‰åœ¨éŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ç­‰æ–¹é¢å…·æœ‰å„ç§æ½œåœ¨åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SVSæ¨¡å‹è¿‡äºä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶å¯¼è‡´éŸ³ç´ å’ŒéŸ³ç¬¦ä¹‹é—´çš„è¿‡æ¸¡ä¸ä½³ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜ç¼ºä¹é€šè¿‡ä¸åŒæç¤ºè¿›è¡Œæœ‰æ•ˆçš„å¤šçº§é£æ ¼æ§åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TCSinger 2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡å¤šè¯­è¨€é›¶æ ·æœ¬SVSæ¨¡å‹ï¼Œå…·æœ‰åŸºäºå„ç§æç¤ºçš„é£æ ¼è¿ç§»å’Œé£æ ¼æ§åˆ¶ã€‚TCSinger 2ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼š1ï¼‰æ¨¡ç³Šè¾¹ç•Œå†…å®¹ï¼ˆBBCï¼‰ç¼–ç å™¨ï¼Œé¢„æµ‹æŒç»­æ—¶é—´ï¼Œæ‰©å±•å†…å®¹åµŒå…¥ï¼Œå¹¶å¯¹è¾¹ç•Œåº”ç”¨æ©ç ä»¥å®ç°å¹³æ»‘è¿‡æ¸¡ã€‚2ï¼‰è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ ä»æ­Œå£°ã€è¯­éŸ³å’Œæ–‡æœ¬æç¤ºä¸­æå–å¯¹é½è¡¨ç¤ºã€‚3ï¼‰åŸºäºæµçš„è‡ªå®šä¹‰å˜å‹å™¨ï¼Œåˆ©ç”¨Cus-MOEå’ŒF0ç›‘ç£ï¼Œæé«˜ç”Ÿæˆæ­Œå£°çš„åˆæˆè´¨é‡å’Œé£æ ¼å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSinger 2åœ¨å¤šä¸ªç›¸å…³ä»»åŠ¡çš„ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ­Œå£°æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://aaronz345.github.io/TCSinger2Demo/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14910v3">PDF</a> Accepted by Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªå¯æ‰©å±•çš„å¤šè¯­ç§é›¶æ ·æœ¬æ­Œå”±è¯­éŸ³åˆæˆï¼ˆSVSï¼‰æ¨¡å‹TCSinger 2è¢«æå‡ºï¼Œç”¨äºéŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ã€‚å®ƒåŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šæ¨¡ç³Šè¾¹ç•Œå†…å®¹ç¼–ç å™¨ã€è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨å’ŒåŸºäºæµçš„è‡ªå®šä¹‰è½¬æ¢å™¨ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰SVSæ¨¡å‹è¿‡åº¦ä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šçš„é—®é¢˜ï¼Œå®ç°äº†é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„æµç•…è¿‡æ¸¡å’ŒåŸºäºä¸åŒæç¤ºçš„å¤šçº§é£æ ¼æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSinger 2åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ç›¸å…³æ­Œå”±è¯­éŸ³æ ·æœ¬å¯åœ¨çº¿ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.TCSinger 2æ˜¯ä¸€ä¸ªå¤šè¯­ç§é›¶æ ·æœ¬æ­Œå”±è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œç”¨äºéŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ã€‚</p>
<p>2.ç°æœ‰SVSæ¨¡å‹å­˜åœ¨è¿‡åº¦ä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šçš„é—®é¢˜ï¼Œå¯¼è‡´é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½å—é™å’ŒéŸ³ç´ ä¸éŸ³ç¬¦é—´çš„è¿‡æ¸¡ä¸æµç•…ã€‚</p>
<p>3.TCSinger 2é€šè¿‡æ¨¡ç³Šè¾¹ç•Œå†…å®¹ç¼–ç å™¨è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®ç°äº†å¹³æ»‘è¿‡æ¸¡ã€‚</p>
<p>4.è¯¥æ¨¡å‹åŒ…å«è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨å’ŒåŸºäºæµçš„è‡ªå®šä¹‰è½¬æ¢å™¨ï¼Œä»¥æé«˜åˆæˆè´¨é‡å’Œé£æ ¼å»ºæ¨¡ã€‚</p>
<p>5.é€šè¿‡å¯¹æ¯”å®éªŒï¼ŒTCSinger 2åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜äºåŸºå‡†æ¨¡å‹çš„æ•ˆæœã€‚</p>
<p>6.è¯¥æ¨¡å‹æ”¯æŒå¤šè¯­ç§ï¼Œå…·æœ‰é£æ ¼è½¬ç§»å’Œé£æ ¼æ§åˆ¶åŠŸèƒ½ï¼ŒåŸºäºå„ç§æç¤ºå®ç°å¤šçº§æ§åˆ¶ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e7bbead0ddbc8e41cb84b96b439db84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e75c65f84e442410c70f3d18dd7f86b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99887b38acacc46968bc74cb19d1cd6e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e608cb554b61d11570a3fefe79d414fd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea PÃ©rez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar NÃ¶th, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>ç”±äºæ•°æ®ç¨€ç¼ºï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­é¢†åŸŸï¼Œé’ˆå¯¹è¨€è¯­éšœç¢è€…çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹è‹±è¯­è¨€è¯­éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ï¼Œç„¶åå°†å…¶åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­ç±»çš„è¨€è¯­éšœç¢è¯­éŸ³ã€‚ç”Ÿæˆçš„æ•°æ®éšåè¢«ç”¨äºå¾®è°ƒç”¨äºæ”¹è¿›è¨€è¯­éšœç¢è¯­éŸ³è¯†åˆ«çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹â€”â€”å¤§è§„æ¨¡å¤šè¯­è¨€è¯­éŸ³ï¼ˆMMSï¼‰ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶è½¬æ¢è¯´è¯äººå’ŒéŸµå¾‹çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†è¨€è¯­éšœç¢çš„ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v3">PDF</a> 5 pages, 1 figure, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä½¿ç”¨è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰å¯¹è¯­éŸ³è½¬æ¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚ç„¶åå°†å…¶åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­å‘éŸ³éšœç¢ç±»ä¼¼çš„è¯­éŸ³ã€‚ç”Ÿæˆçš„æ•°æ®ç”¨äºå¾®è°ƒå¤šè¯­è¨€ASRæ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶å®ç°è¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ç¨€ç¼ºä½¿å¾—å‘éŸ³éšœç¢è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰çš„è¯­éŸ³è½¬æ¢æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚</li>
<li>ä½¿ç”¨è¯¥æ¨¡å‹å°†å¥åº·çš„éè‹±è¯­è¯­éŸ³è½¬æ¢ä¸ºéè‹±è¯­å‘éŸ³éšœç¢ç±»ä¼¼çš„è¯­éŸ³ï¼Œç”Ÿæˆçš„æ•°æ®ç”¨äºè®­ç»ƒASRæ¨¡å‹ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œè¿™ç§è¯­éŸ³è½¬æ¢æŠ€æœ¯åœ¨å¤šç§è¯­è¨€ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ASRæ¨¡å‹å’Œå¢å¼ºæŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯ç»“åˆè¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢ï¼Œèƒ½æ›´çœŸå®åœ°æ¨¡æ‹Ÿå‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</li>
<li>å®¢è§‚å’Œä¸»è§‚åˆ†æè¯å®äº†ç”Ÿæˆè¯­éŸ³çš„é€¼çœŸåº¦å’Œæ¨¡æ‹Ÿå‘éŸ³éšœç¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95762dbc7935311d3ab99f1d0517fe87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851cefed71dd03055dee202aa3b93374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bff05fd4865b97d0af86cd3b7475e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569360d203bc2186acb89f7d6ccd725e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach"><a href="#Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach" class="headerlink" title="Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"></a>Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</h2><p><strong>Authors:Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 28% with less than a 2% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 4.6% improvement in fairness metrics with a drop of less than 3.6% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential when explicit demographic information is unavailable. </p>
<blockquote>
<p>è™½ç„¶å­ç¾¤ä½“å·®å¼‚å’Œæ€§èƒ½åè§åœ¨è®¡ç®—ç ”ç©¶ä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶ï¼Œä½†åœ¨åˆ†ç±»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§ä»ç„¶è¢«å¿½è§†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºéš¾ä»¥è·å¾—çš„æ˜ç¡®äººå£ç»Ÿè®¡æ ‡ç­¾ï¼ˆç”±äºéšç§æ‹…å¿§ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’Œk-meansèšç±»è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œä»¥å‡è½»SERä¸­çš„åè§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¼ªæ ‡ç­¾IDIå‡å°‘äº†å­ç¾¤ä½“å·®å¼‚ï¼Œå…¬å¹³åº¦æŒ‡æ ‡æé«˜äº†28%ä»¥ä¸Šï¼Œè€ŒSERå‡†ç¡®ç‡ä¸‹é™äº†ä¸åˆ°2%ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£çš„IDIåœ¨å…¬å¹³æŒ‡æ ‡ä¸Šæé«˜äº†4.6%ä»¥ä¸Šï¼Œè€ŒSERæ€§èƒ½ä¸‹é™äº†ä¸åˆ°3.6%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ— ç›‘ç£çš„IDIå§‹ç»ˆèƒ½å¤Ÿå‡è½»ç§æ—å’Œå¹´é¾„å·®å¼‚ï¼Œåœ¨ç¼ºä¹æ˜ç¡®äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µä¸‹æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14449v3">PDF</a> Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„ç±»åˆ«å…¬å¹³æ€§é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æ–¹æ³•ä¾èµ–éš¾ä»¥è·å–çš„æ˜ç¡®äººå£ç»Ÿè®¡æ ‡ç­¾çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†éšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’ŒK-meansèšç±»è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œä»¥å‡è½»SERä¸­çš„åè§ã€‚å®éªŒè¡¨æ˜ï¼Œä¼ªæ ‡ç­¾IDIå‡å°‘äº†å­ç¾¤å·®å¼‚ï¼Œå…¬å¹³åº¦æŒ‡æ ‡æé«˜äº†28%ä»¥ä¸Šï¼Œè€ŒSERå‡†ç¡®ç‡ä»…ä¸‹é™ä¸åˆ°2%ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£çš„IDIåœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šæé«˜äº†4.6%ä»¥ä¸Šï¼Œè€ŒSERæ€§èƒ½ä»…ä¸‹é™ä¸åˆ°3.6%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ— ç›‘ç£çš„IDIå¯ä»¥æŒç»­ç¼“è§£ç§æ—å’Œå¹´é¾„å·®å¼‚ï¼Œå±•ç¤ºå‡ºäº†å½“æ²¡æœ‰æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯æ—¶ï¼Œå…¶æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ä»ç„¶å­˜åœ¨å¯¹ç±»åˆ«å…¬å¹³æ€§çš„ç ”ç©¶ä¸è¶³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºéš¾ä»¥è·å–çš„äººå£ç»Ÿè®¡æ ‡ç­¾ï¼Œå¼•å…¥éšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>IDIæ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾è¿›è¡Œå·¥ä½œï¼Œé€šè¿‡ä¼ªæ ‡ç­¾å‡å°‘å­ç¾¤å·®å¼‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¼ªæ ‡ç­¾IDIèƒ½æ˜¾è‘—æé«˜å…¬å¹³æ€§æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„SERå‡†ç¡®ç‡ã€‚</li>
<li>æ— ç›‘ç£çš„IDIé€šè¿‡K-meansèšç±»è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œèƒ½åœ¨æ²¡æœ‰æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯æ—¶åº”ç”¨ã€‚</li>
<li>æ— ç›‘ç£IDIèƒ½æŒç»­ç¼“è§£ç§æ—å’Œå¹´é¾„å·®å¼‚ï¼Œæé«˜å…¬å¹³åº¦æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f55bf042f5634129bd8baec5c84fd98e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dace8852cb555f7b53f22c6c92bfd9bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae5faed4372daf6ae06502ac3760eb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbcbef08d692c08c629765fa04714f42.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Spatiotemporal-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment"><a href="#Spatiotemporal-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment" class="headerlink" title="Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of   Speech Conditions in Facial and Vocal Affective Alignment"></a>Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of   Speech Conditions in Facial and Vocal Affective Alignment</h2><p><strong>Authors:Von Ralph Dane Marquez Herbuela, Yukie Nagai</strong></p>
<p>Understanding how humans express and synchronize emotions across multiple communication channels particularly facial expressions and speech has significant implications for emotion recognition systems and human computer interaction. Motivated by the notion that non-overlapping speech promotes clearer emotional coordination, while overlapping speech disrupts synchrony, this study examines how these conversational dynamics shape the spatial and temporal alignment of arousal and valence across facial and vocal modalities. Using dyadic interactions from the IEMOCAP dataset, we extracted continuous emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech audio). Segments were categorized based on speech overlap, and emotional alignment was assessed using Pearson correlation, lag adjusted analysis, and Dynamic Time Warping (DTW). Across analyses, non overlapping speech was associated with more stable and predictable emotional synchrony than overlapping speech. While zero-lag correlations were low and not statistically different, non overlapping speech showed reduced variability, especially for arousal. Lag adjusted correlations and best-lag distributions revealed clearer, more consistent temporal alignment in these segments. In contrast, overlapping speech exhibited higher variability and flatter lag profiles, though DTW indicated unexpectedly tighter alignment suggesting distinct coordination strategies. Notably, directionality patterns showed that facial expressions more often preceded speech during turn-taking, while speech led during simultaneous vocalizations. These findings underscore the importance of conversational structure in regulating emotional communication and provide new insight into the spatial and temporal dynamics of multimodal affective alignment in real world interaction. </p>
<blockquote>
<p>ç†è§£äººç±»å¦‚ä½•åœ¨å¤šä¸ªæ²Ÿé€šæ¸ é“ï¼ˆå°¤å…¶æ˜¯é¢éƒ¨è¡¨æƒ…å’Œè¨€è¯­ï¼‰ä¸Šè¡¨è¾¾å’ŒåŒæ­¥æƒ…ç»ªï¼Œå¯¹äºæƒ…ç»ªè¯†åˆ«ç³»ç»Ÿå’Œäººæœºäº¤äº’æœ‰ç€é‡å¤§å¯ç¤ºã€‚æœ¬ç ”ç©¶å—åˆ°éé‡å æ€§è¨€è¯­èƒ½å¤Ÿä¿ƒè¿›æ›´æ¸…æ™°æƒ…æ„Ÿåè°ƒçš„è§‚å¿µçš„å¯å‘ï¼ŒåŒæ—¶é‡å æ€§è¨€è¯­ä¼šç ´ååŒæ­¥æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¿™äº›å¯¹è¯åŠ¨åŠ›å¦‚ä½•å½±å“é¢éƒ¨å’Œå£°éŸ³æ¨¡æ€çš„å…´å¥‹å’Œä»·å€¼çš„ç©ºé—´å’Œæ—¶é—´å¯¹é½ã€‚æˆ‘ä»¬ä½¿ç”¨IEMOCAPæ•°æ®é›†ä¸­çš„äºŒå…ƒäº¤äº’æ•°æ®ï¼Œé€šè¿‡EmoNetï¼ˆé¢éƒ¨è§†é¢‘ï¼‰å’ŒåŸºäºWav2Vec2çš„æ¨¡å‹ï¼ˆè¯­éŸ³éŸ³é¢‘ï¼‰æå–è¿ç»­çš„æƒ…ç»ªä¼°è®¡ã€‚æ ¹æ®è¯­éŸ³é‡å å¯¹ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä½¿ç”¨Pearsonç›¸å…³æ€§ã€æ»åè°ƒæ•´åˆ†æå’ŒåŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰æ¥è¯„ä¼°æƒ…ç»ªå¯¹é½ã€‚åˆ†æå‘ç°ï¼Œéé‡å æ€§è¨€è¯­ä¸æ›´ç¨³å®šå’Œå¯é¢„æµ‹çš„æƒ…æ„ŸåŒæ­¥ç›¸å…³ï¼Œè€Œé‡å æ€§è¨€è¯­åˆ™è¡¨ç°å‡ºæ›´é«˜çš„å¯å˜æ€§å’Œæ›´å¹³å¦çš„æ»ååˆ†å¸ƒã€‚ç„¶è€Œï¼ŒDTWæ„å¤–åœ°æ˜¾ç¤ºå‡ºæ›´ç´§å¯†çš„å¯¹é½ï¼Œè¿™è¡¨æ˜äº†ä¸åŒçš„åè°ƒç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ–¹å‘æ€§æ¨¡å¼æ˜¾ç¤ºï¼Œåœ¨è½®æµå‘è¨€æ—¶ï¼Œé¢éƒ¨è¡¨æƒ…å¾€å¾€å…ˆäºè¨€è¯­ï¼Œè€Œåœ¨åŒæ—¶å‘å£°æ—¶ï¼Œåˆ™æ˜¯è¨€è¯­é¢†å…ˆã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¯¹è¯ç»“æ„åœ¨è°ƒèŠ‚æƒ…æ„Ÿæ²Ÿé€šä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºç°å®äº’åŠ¨ä¸­å¤šæ¨¡æ€æƒ…æ„Ÿå¯¹é½çš„ç©ºé—´å’Œæ—¶é—´åŠ¨æ€æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13455v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººç±»åœ¨å¤šé€šé“æ²Ÿé€šä¸­å¦‚ä½•è¡¨è¾¾å’ŒåŒæ­¥æƒ…ç»ªï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨è¡¨æƒ…å’Œè¨€è¯­æ–¹é¢ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œéé‡å çš„è¯­éŸ³æœ‰åŠ©äºæ›´æ¸…æ™°çš„æƒ…æ„Ÿåè°ƒï¼Œè€Œé‡å çš„è¯­éŸ³ä¼šç ´ååŒæ­¥æ€§ã€‚é€šè¿‡å¯¹IEMOCAPæ•°æ®é›†çš„åŒäººäº’åŠ¨è¿›è¡Œç ”ç©¶ï¼Œåˆ†æä¸åŒæ²Ÿé€šæ–¹å¼å¯¹æƒ…æ„ŸåŒæ­¥çš„å½±å“ï¼Œå‘ç°éé‡å è¯­éŸ³ç›¸è¾ƒäºé‡å è¯­éŸ³èƒ½å¸¦æ¥æ›´ç¨³å®šå’Œå¯é¢„æµ‹çš„æƒ…æ„ŸåŒæ­¥ã€‚ç ”ç©¶æ­ç¤ºäº†é¢éƒ¨è¡¨æƒ…å’Œè¨€è¯­åœ¨æƒ…æ„Ÿæ²Ÿé€šä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€æƒ…æ„ŸåŒæ­¥çš„æ—¶ç©ºåŠ¨æ€æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åœ¨å¤šé€šé“æ²Ÿé€šä¸­è¡¨è¾¾å’ŒåŒæ­¥æƒ…ç»ªå…·æœ‰é‡è¦çš„ç ”ç©¶æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨è¡¨æƒ…å’Œè¨€è¯­æ–¹é¢ã€‚</li>
<li>éé‡å çš„è¯­éŸ³æœ‰åŠ©äºæ›´æ¸…æ™°çš„æƒ…æ„Ÿåè°ƒï¼Œè€Œé‡å çš„è¯­éŸ³å¯èƒ½ç ´åæƒ…æ„ŸåŒæ­¥ã€‚</li>
<li>é€šè¿‡IEMOCAPæ•°æ®é›†çš„ç ”ç©¶å‘ç°ï¼Œéé‡å è¯­éŸ³å¸¦æ¥æ›´ç¨³å®šå’Œå¯é¢„æµ‹çš„æƒ…æ„ŸåŒæ­¥è¡¨ç°ã€‚</li>
<li>æƒ…æ„ŸåŒæ­¥çš„åˆ†ææ–¹æ³•åŒ…æ‹¬çš®å°”é€Šç›¸å…³ç³»æ•°ã€æ»åè°ƒæ•´åˆ†æå’ŒåŠ¨æ€æ—¶é—´å¼¯æ›²ã€‚</li>
<li>éé‡å è¯­éŸ³åœ¨æƒ…ç»ªåŒæ­¥æ–¹é¢è¡¨ç°å‡ºè¾ƒä½çš„å¯å˜æ€§ï¼Œç‰¹åˆ«æ˜¯å…´å¥‹åº¦æ–¹é¢ã€‚</li>
<li>é¢éƒ¨è¡¨æƒ…åœ¨è½¬å‘æ—¶æ›´å¸¸å…ˆäºè¨€è¯­ï¼Œè€ŒåŒæ—¶å‘å£°æ—¶åˆ™ç›¸åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-959c46630bb56151d2caf7194fb47281.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e7bcee293262680c3989cd14efa2f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b228e76e9667901a275578b2c85402f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83e8a90d1094ed1ce546ebb9ca3e8b82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-136d687e719e04ac96145e56f44406b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e911bdd5f8e34cc01d36417fe65e7ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eea2203a811254a8e8f61c60811ce882.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ISDrama-Immersive-Spatial-Drama-Generation-through-Multimodal-Prompting"><a href="#ISDrama-Immersive-Spatial-Drama-Generation-through-Multimodal-Prompting" class="headerlink" title="ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting"></a>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao</strong></p>
<p>Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/ISDramaDemo">https://aaronz345.github.io/ISDramaDemo</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆä¸“æ³¨äºåŸºäºå¤šæ¨¡æ€æç¤ºåˆ›å»ºå…·æœ‰æˆå‰§è¯­è°ƒçš„è¿ç»­å¤šæ‰¬å£°å™¨åŒè€³è¯­éŸ³ï¼Œåœ¨ARã€VRç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦åŸºäºå¤šæ¨¡æ€è¾“å…¥å¯¹ç©ºé—´ä¿¡æ¯å’Œæˆå‰§è¯­è°ƒè¿›è¡ŒåŒæ—¶å»ºæ¨¡ï¼Œå¹¶ä¸”æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡å°è¯•è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ„å»ºäº†MRSDramaï¼Œå³é¦–ä¸ªå¤šæ¨¡æ€è®°å½•çš„ç©ºé—´æˆå‰§æ•°æ®é›†ï¼ŒåŒ…å«åŒè€³æˆå‰§éŸ³é¢‘ã€å‰§æœ¬ã€è§†é¢‘ã€å‡ ä½•å§¿åŠ¿å’Œæ–‡æœ¬æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ISDramaï¼Œå³é€šè¿‡å¤šæ¨¡æ€æç¤ºè¿›è¡Œæ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆçš„é¦–ä¸ªæ¨¡å‹ã€‚ISDramaä¸»è¦åŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š1ï¼‰åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€å§¿åŠ¿ç¼–ç å™¨ï¼Œè€ƒè™‘ç§»åŠ¨æ‰¬å£°å™¨äº§ç”Ÿçš„å¤šæ™®å‹’æ•ˆåº”ï¼Œä»å¤šæ¨¡æ€æç¤ºä¸­æå–ç»Ÿä¸€å§¿åŠ¿ä¿¡æ¯ã€‚2ï¼‰æ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨ï¼Œä¸€ä¸ªåŸºäºæµçš„mamba-transformeræ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡æˆå‰§ï¼Œå¹¶ç»“åˆæˆå‰§MOEé€‰æ‹©é€‚å½“çš„ä¸“å®¶ä»¥å¢å¼ºè¯­è°ƒå’Œå§¿åŠ¿æ§åˆ¶ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ä¸Šä¸‹æ–‡ä¸€è‡´çš„æ— ç›‘ç£åˆ†ç±»ç­–ç•¥ï¼Œä»¥è¿è´¯åœ°ç”Ÿæˆå®Œæ•´çš„æˆå‰§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒISDramaåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ç›¸å…³æ¼”ç¤ºå’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://aaronz345.github.io/ISDramaDemo%E8%AE%BF%E9%97%AE%E3%80%82">https://aaronz345.github.io/ISDramaDemoè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆçš„ç ”ç©¶ï¼Œé‡ç‚¹åˆ›å»ºåŸºäºå¤šæ¨¡æ€æç¤ºçš„è¿ç»­å¤šè®²è€…åŒè€³æˆå‰§è¯­éŸ³ã€‚è¯¥ç ”ç©¶æ„å»ºäº†MRSDramaæ•°æ®é›†ï¼Œå¹¶æå‡ºISDramaæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€æç¤ºè¿›è¡Œæ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆã€‚è¯¥æ¨¡å‹åŒ…æ‹¬å¤šæ¨¡æ€å§¿æ€ç¼–ç å™¨å’Œæ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡æˆå‰§å¹¶æ§åˆ¶è¯­è°ƒå’Œå§¿æ€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒISDramaåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆé›†ä¸­äºåˆ›å»ºåŸºäºå¤šæ¨¡æ€æç¤ºçš„è¿ç»­å¤šè®²è€…åŒè€³æˆå‰§è¯­éŸ³ï¼Œå¯åº”ç”¨äºARã€VRç­‰é¢†åŸŸã€‚</li>
<li>ç ”ç©¶é¢ä¸´åŒæ—¶å»ºæ¨¡ç©ºé—´ä¿¡æ¯å’Œæˆå‰§è¯­è°ƒçš„æŒ‘æˆ˜ï¼Œä¸”æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æ„å»ºäº†MRSDramaæ•°æ®é›†ï¼ŒåŒ…å«åŒè€³æˆå‰§éŸ³é¢‘ã€å‰§æœ¬ã€è§†é¢‘ã€å‡ ä½•å§¿æ€å’Œæ–‡æœ¬æç¤ºã€‚</li>
<li>æå‡ºISDramaæ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å§¿æ€ç¼–ç å™¨å’Œæ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨ã€‚</li>
<li>å¤šæ¨¡æ€å§¿æ€ç¼–ç å™¨é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œè€ƒè™‘ç§»åŠ¨è®²è€…å¸¦æ¥çš„å¤šæ™®å‹’æ•ˆåº”ï¼Œä»å¤šæ¨¡æ€æç¤ºä¸­æå–ç»Ÿä¸€å§¿æ€ä¿¡æ¯ã€‚</li>
<li>æ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨åŸºäºæµå¼çš„mamba-transformeræ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡æˆå‰§ï¼Œå¹¶å¼•å…¥Drama-MOEé€‰æ‹©é€‚å½“çš„ä¸“å®¶ä»¥å¢å¼ºè¯­è°ƒå’Œå§¿æ€æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7d0dde82d91263e54fa13503c0933f8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1790c3fd740147bdb039a455784460df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6899f8d47230be9d8e7f5d844befd36c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2317f687c69202ee624115138579b192.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph"><a href="#Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph" class="headerlink" title="Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph"></a>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph</h2><p><strong>Authors:Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li</strong></p>
<p>The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox">https://github.com/YiboZhao624/MetaTox</a>. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„é‡å¤§å…³æ³¨ã€‚å½“ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯’æ€§æ£€æµ‹æ—¶ï¼Œä¼šå‡ºç°ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†ä¼šå¯¼è‡´å‡é˜´æ€§ç»“æœï¼›2ï¼‰LLMå¯¹æœ‰æ¯’è¨€è®ºè¿‡äºæ•æ„Ÿï¼Œå¯¼è‡´å‡é˜³æ€§ç»“æœï¼Œé™åˆ¶è¨€è®ºè‡ªç”±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºMetaToxï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾ä¸Šçš„å›¾æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¸‰æ­¥éª¤çš„ç®¡é“ï¼Œåˆ©ç”¨LLMæå–æ¯’æ€§ä¿¡æ¯ï¼Œä»¥æ¯’æ€§åŸºå‡†æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æ£€ç´¢å’Œæ’åè¿‡ç¨‹æŸ¥è¯¢å›¾å½¢ï¼Œä»¥è¡¥å……å‡†ç¡®ä¸”ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå’Œæ·±å…¥çš„æ¡ˆä¾‹ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MetaToxæ–¹æ³•æ˜¾è‘—é™ä½äº†å‡é˜³æ€§ç‡ï¼ŒåŒæ—¶æé«˜äº†æ€»ä½“æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YiboZhao624/MetaToxè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15268v3">PDF</a> 8 pages of content</p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„å…³æ³¨ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯’æ€§æ£€æµ‹æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†å’Œè¿‡åº¦æ•æ„Ÿäºæ¯’æ€§è¨€è®ºå¯¼è‡´è¯¯æŠ¥ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMetaToxçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ä¸Šçš„å›¾æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚é€šè¿‡æ„å»ºå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡LLMæå–æ¯’æ€§ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æ¯’æ€§åŸºå‡†æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“è¿›è¡ŒæŸ¥è¯¢å’Œæ’åè¿‡ç¨‹ï¼Œä»¥è¡¥å……å‡†ç¡®ã€ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚å®éªŒå’Œæ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒMetaToxæ˜¾è‘—é™ä½äº†è¯¯æŠ¥ç‡ï¼Œæé«˜äº†æ•´ä½“æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å…³äºç½‘ç»œå†…å®¹æ¯’æ€§çš„å…³æ³¨ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯’æ€§æ£€æµ‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†å’Œè¿‡åº¦æ•æ„Ÿå¯¼è‡´è¯¯æŠ¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºMetaToxçš„æ–°æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ä¸Šçš„å›¾æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚</li>
<li>MetaToxé€šè¿‡æ„å»ºå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ï¼Œåˆ©ç”¨LLMæå–æ¯’æ€§ä¿¡æ¯ã€‚</li>
<li>MetaToxä½¿ç”¨æ¯’æ€§åŸºå‡†æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“è¿›è¡ŒæŸ¥è¯¢å’Œæ’åè¿‡ç¨‹ï¼Œä»¥è¡¥å……å‡†ç¡®ã€ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚</li>
<li>MetaToxæ˜¾è‘—é™ä½äº†è¯¯æŠ¥ç‡ï¼Œæé«˜äº†æ•´ä½“æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5561d951e9b963a721e630ce6153036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4117e7f489848b54039de48bf5f165d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505ad0de96990783c6cb239a97f90499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30f192e2c185a64a742a9bfec37da5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control"><a href="#TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control" class="headerlink" title="TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control"></a>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control</h2><p><strong>Authors:Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao</strong></p>
<p>Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S&amp;D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer. Singing voice samples can be accessed at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/">https://aaronz345.github.io/TCSingerDemo/</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬å”±è…”åˆæˆï¼ˆSVSï¼‰å¸¦æœ‰é£æ ¼è½¬æ¢å’Œé£æ ¼æ§åˆ¶åŠŸèƒ½ï¼Œæ—¨åœ¨ä»éŸ³é¢‘å’Œæ–‡å­—æç¤ºç”Ÿæˆå…·æœ‰æœªè§éŸ³è´¨å’Œé£æ ¼çš„é«˜è´¨é‡å”±è…”ï¼ˆåŒ…æ‹¬å”±æ³•ã€æƒ…æ„Ÿã€èŠ‚å¥ã€æŠ€å·§å’Œå‘éŸ³ï¼‰ã€‚ç„¶è€Œï¼Œå”±è…”é£æ ¼çš„å¤šé¢æ€§ç»™æœ‰æ•ˆçš„å»ºæ¨¡ã€è½¬æ¢å’Œæ§åˆ¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå½“å‰çš„SVSæ¨¡å‹å¾€å¾€ä¸èƒ½ä¸ºæœªè§è¿‡çš„æ­Œæ‰‹ç”Ÿæˆå¯Œæœ‰é£æ ¼ç»†å¾®å·®åˆ«çš„å”±è…”ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TCSingerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè·¨è¯­è¨€è¯­éŸ³å’Œå”±è…”é£æ ¼é£æ ¼è½¬æ¢çš„é›¶æ ·æœ¬SVSæ¨¡å‹ï¼Œä»¥åŠå¤šçº§é£æ ¼æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒTCSingeræå‡ºäº†ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰èšç±»é£æ ¼ç¼–ç å™¨é‡‡ç”¨èšç±»å‘é‡é‡åŒ–æ¨¡å‹ï¼Œå°†é£æ ¼ä¿¡æ¯ç¨³å®šåœ°å‡èšæˆä¸€ä¸ªç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼›2ï¼‰é£æ ¼å’ŒæŒç»­æ—¶é—´è¯­è¨€æ¨¡å‹ï¼ˆS&amp;D-LMï¼‰åŒæ—¶é¢„æµ‹é£æ ¼ä¿¡æ¯å’ŒéŸ³ç´ æŒç»­æ—¶é—´ï¼Œä¸¤è€…éƒ½å—ç›Šï¼›3ï¼‰é£æ ¼è‡ªé€‚åº”è§£ç å™¨é‡‡ç”¨ä¸€ç§æ–°çš„mel-styleè‡ªé€‚åº”å½’ä¸€åŒ–æ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¢å¼ºç»†èŠ‚çš„å”±è…”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSingeråœ¨åˆæˆè´¨é‡ã€æ­Œæ‰‹ç›¸ä¼¼åº¦å’Œé£æ ¼å¯æ§æ€§æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºå‡†æ¨¡å‹ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬é£æ ¼è½¬æ¢ã€å¤šçº§é£æ ¼æ§åˆ¶ã€è·¨è¯­è¨€é£æ ¼è½¬æ¢å’Œè¯­éŸ³åˆ°å”±è…”é£æ ¼è½¬æ¢ç­‰å„é¡¹ä»»åŠ¡ã€‚å”±è…”æ ·æœ¬å¯è®¿é—®<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/%E3%80%82">https://aaronz345.github.io/TCSingerDemo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15977v6">PDF</a> Accepted by EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶ä»‹ç»äº†é’ˆå¯¹è·¨è¯­è¨€è¯­éŸ³å’Œæ­Œå”±é£æ ¼è¿›è¡Œé›¶æ ·æœ¬æ­Œå”±å£°éŸ³åˆæˆï¼ˆSVSï¼‰çš„é£æ ¼è½¬ç§»å’Œå¤šå±‚æ¬¡é£æ ¼æ§åˆ¶çš„æ¨¡å‹TCSingerã€‚å®ƒé€šè¿‡ä¸‰ä¸ªä¸»è¦æ¨¡å—å®ç°ï¼šèšç±»é£æ ¼ç¼–ç å™¨ã€é£æ ¼ä¸æ—¶é•¿è¯­è¨€æ¨¡å‹ä»¥åŠé£æ ¼è‡ªé€‚åº”è§£ç å™¨ã€‚è¯¥æ¨¡å‹åœ¨åˆæˆè´¨é‡ã€æ­Œæ‰‹ç›¸ä¼¼æ€§å’Œé£æ ¼å¯æ§æ€§æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬é£æ ¼è½¬ç§»ã€å¤šå±‚æ¬¡é£æ ¼æ§åˆ¶ã€è·¨è¯­è¨€é£æ ¼è½¬ç§»å’Œè¯­éŸ³åˆ°æ­Œå”±é£æ ¼è½¬ç§»ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TCSingeræ˜¯ä¸€ä¸ªç”¨äºé£æ ¼è½¬ç§»çš„é›¶æ ·æœ¬æ­Œå”±å£°éŸ³åˆæˆï¼ˆSVSï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æœªè§è¿‡çš„éŸ³è´¨å’Œé£æ ¼çš„æ­Œå”±å£°éŸ³ã€‚</li>
<li>TCSingeré€šè¿‡ä¸‰ä¸ªä¸»è¦æ¨¡å—å®ç°ï¼šèšç±»é£æ ¼ç¼–ç å™¨ã€é£æ ¼ä¸æ—¶é•¿è¯­è¨€æ¨¡å‹å’Œé£æ ¼è‡ªé€‚åº”è§£ç å™¨ã€‚</li>
<li>èšç±»é£æ ¼ç¼–ç å™¨ä½¿ç”¨èšç±»å‘é‡é‡åŒ–æ¨¡å‹ï¼Œå°†é£æ ¼ä¿¡æ¯ç¨³å®šåœ°æµ“ç¼©åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚</li>
<li>é£æ ¼ä¸æ—¶é•¿è¯­è¨€æ¨¡å‹åŒæ—¶é¢„æµ‹é£æ ¼ä¿¡æ¯å’ŒéŸ³ç´ æ—¶é•¿ï¼Œå¯¹ä¸¤è€…éƒ½æœ‰åˆ©ã€‚</li>
<li>é£æ ¼è‡ªé€‚åº”è§£ç å™¨é‡‡ç”¨æ–°çš„mel-styleè‡ªé€‚åº”å½’ä¸€åŒ–æ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¢å¼ºç»†èŠ‚çš„æ­Œå”±å£°éŸ³ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSingeråœ¨åˆæˆè´¨é‡ã€æ­Œæ‰‹ç›¸ä¼¼æ€§å’Œé£æ ¼å¯æ§æ€§æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c34f2e70d85806fd10ebbab3b740b302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a3202a7091e1b4c968475e48fc1ace2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb2a5d55364652a98ef3c9cb82f2e29.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks"><a href="#GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks" class="headerlink" title="GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks"></a>GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks</h2><p><strong>Authors:Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao</strong></p>
<p>The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at <a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/">http://aaronz345.github.io/GTSingerDemo/</a>. We provide the dataset and the code for processing data and conducting benchmarks at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AaronZ345/GTSinger">https://huggingface.co/datasets/AaronZ345/GTSinger</a> and <a target="_blank" rel="noopener" href="https://github.com/AaronZ345/GTSinger">https://github.com/AaronZ345/GTSinger</a>. </p>
<blockquote>
<p>é«˜è´¨é‡å¤šä»»åŠ¡æ¼”å”±æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜æ˜¾è‘—é˜»ç¢äº†å¤šæ ·å¯æ§å’Œä¸ªæ€§åŒ–æ¼”å”±ä»»åŠ¡çš„å‘å±•ï¼Œå› ä¸ºç°æœ‰çš„æ¼”å”±æ•°æ®é›†å­˜åœ¨è´¨é‡ä½ã€è¯­è¨€åŠæ­Œæ‰‹å¤šæ ·æ€§æœ‰é™ã€ç¼ºä¹å¤šæŠ€æœ¯ä¿¡æ¯å’Œç°å®éŸ³ä¹ä¹è°±ä»¥åŠä»»åŠ¡é€‚ç”¨æ€§å·®ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GTSingerï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹å…¨çƒã€å¤šæŠ€æœ¯ã€å…è´¹ä½¿ç”¨çš„ã€é«˜è´¨é‡æ¼”å”±è¯­æ–™åº“ï¼Œé…æœ‰ç°å®éŸ³ä¹ä¹è°±ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¼”å”±ä»»åŠ¡ï¼Œä»¥åŠç›¸åº”çš„åŸºå‡†æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼Œ(1)æˆ‘ä»¬æ”¶é›†äº†80.59å°æ—¶çš„é«˜è´¨é‡æ¼”å”±å£°éŸ³ï¼Œå½¢æˆäº†æœ€å¤§çš„å½•éŸ³æ¼”å”±æ•°æ®é›†ï¼›(2)20ä½ä¸“ä¸šæ­Œæ‰‹è·¨è¶Šä¹ç§å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€ï¼Œå±•ç°äº†å¤šæ ·çš„éŸ³è‰²å’Œé£æ ¼ï¼›(3)æˆ‘ä»¬æä¾›äº†å…­ç§å¸¸ç”¨æ¼”å”±æŠ€æœ¯çš„å—æ§æ¯”è¾ƒå’ŒéŸ³ç´ çº§æ³¨é‡Šï¼Œæœ‰åŠ©äºæŠ€æœ¯å»ºæ¨¡å’Œæ§åˆ¶ï¼›(4)GTSingeræä¾›äº†ç°å®éŸ³ä¹ä¹è°±ï¼Œæœ‰åŠ©äºç°å®éŸ³ä¹ä½œæ›²ï¼›(5)æ¼”å”±å£°éŸ³é…æœ‰æ‰‹åŠ¨éŸ³ç´ åˆ°éŸ³é¢‘å¯¹é½ã€å…¨å±€é£æ ¼æ ‡ç­¾ï¼Œä»¥åŠç”¨äºå„ç§æ¼”å”±ä»»åŠ¡çš„16.16å°æ—¶é…å¥—è¯­éŸ³ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ–¹ä¾¿ä½¿ç”¨GTSingerï¼Œæˆ‘ä»¬è¿›è¡Œäº†å››é¡¹åŸºå‡†å®éªŒï¼šæŠ€æœ¯å¯æ§çš„æ¼”å”±å£°éŸ³åˆæˆã€æŠ€æœ¯è¯†åˆ«ã€é£æ ¼è½¬æ¢å’Œè¯­éŸ³åˆ°æ¼”å”±çš„è½¬æ¢ã€‚è¯­æ–™åº“å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/%E6%89%BE%E5%88%B0%E3%80%82%E6%88%91%E4%BB%AC%E5%9C%A8https://huggingface.co/datasets/AaronZ345/GTSinger%E5%92%8Chttps://github.com/AaronZ345/GTSinger%E6%8F%90%E4%BE%9B%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">http://aaronz345.github.io/GTSingerDemo/æ‰¾åˆ°ã€‚æˆ‘ä»¬åœ¨https://huggingface.co/datasets/AaronZ345/GTSingerå’Œhttps://github.com/AaronZ345/GTSingeræä¾›äº†æ•°æ®é›†å’Œæ•°æ®å¤„ç†åŠåŸºå‡†æµ‹è¯•çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13832v7">PDF</a> Accepted by NeurIPS 2024 (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GTSingerè¿™ä¸€å…¨çƒå¤§å‹ã€å¤šæŠ€å·§ã€å…è´¹ä½¿ç”¨çš„æ­Œå”±è¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“å…·æœ‰é«˜è´¨é‡ã€å¤šè¯­è¨€ã€åŒ…å«å¤šç§æ¼”å”±æŠ€å·§ä¸çœŸå®ä¹è°±ç­‰ç‰¹ç‚¹ï¼Œé€‚ç”¨äºå„ç§æ­Œå”±ä»»åŠ¡ã€‚ä¸ºè§£å†³é«˜è´¨é‡å¤šä»»åŠ¡æ­Œå”±æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ï¼ŒGTSingeræä¾›äº†è¶…è¿‡80å°æ—¶çš„æ­Œå”±å£°éŸ³æ•°æ®ï¼Œæ¶µç›–äº†å¤šç§é£æ ¼å’ŒæŠ€å·§ï¼Œå¹¶é…æœ‰éŸ³ä¹ä¹è°±å’ŒéŸ³é¢‘å¯¹é½ç­‰æŠ€æœ¯æ”¯æŒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†é’ˆå¯¹GTSingerè¿›è¡Œçš„å››é¡¹åŸºå‡†å®éªŒï¼ŒåŒ…æ‹¬å¯æ§æ¼”å”±å£°éŸ³åˆæˆã€æŠ€å·§è¯†åˆ«ã€é£æ ¼è½¬æ¢å’Œè¯­éŸ³è½¬å”±ç­‰ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å¯åœ¨æŒ‡å®šç½‘ç«™ä¸‹è½½ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GTSingeræ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¤§å‹æ­Œå”±è¯­æ–™åº“ï¼Œä¸ºè§£å†³é«˜è´¨é‡å¤šä»»åŠ¡æ­Œå”±æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜è€Œè®¾è®¡ã€‚</li>
<li>GTSingeråŒ…å«è¶…è¿‡80å°æ—¶çš„é«˜è´¨é‡æ­Œå”±å£°éŸ³æ•°æ®ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å½•éŸ³æ­Œå”±æ•°æ®é›†ã€‚</li>
<li>GTSingeræ¶µç›–äº†ä¹ç§å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€å’Œå¤šç§æ¼”å”±é£æ ¼ï¼Œæä¾›å¤šæ ·åŒ–çš„éŸ³è‰²å’Œé£æ ¼ã€‚</li>
<li>GTSingeræä¾›äº†å…­ç§å¸¸ç”¨æ¼”å”±æŠ€å·§çš„å¯¹æ¯”åˆ†æå’Œæ³¨é‡Šï¼Œæ”¯æŒæŠ€å·§å»ºæ¨¡å’Œæ§åˆ¶ã€‚</li>
<li>GTSingeræä¾›äº†çœŸå®çš„éŸ³ä¹ä¹è°±å’ŒéŸ³é¢‘å¯¹é½æŠ€æœ¯ï¼ŒåŠ©åŠ›éŸ³ä¹åˆ›ä½œå’Œç ”ç©¶ã€‚</li>
<li>GTSingeråŒ…æ‹¬æŠ€æœ¯å¯æ§çš„æ­Œå”±å£°éŸ³åˆæˆã€æŠ€å·§è¯†åˆ«ã€é£æ ¼è½¬æ¢å’Œè¯­éŸ³è½¬å”±ç­‰åŸºå‡†å®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-333000518bb4fe8b296a8865370abfbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0a829871655320f45719954820b9f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f36701794ff88abe07da6e61077bca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad95cdd657d117546409a35f0b263aff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0f1a2777fc489b76f96c568fb6e77be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b8c04cc16af9c2620048c99092a7307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d630db24fb3cff05a06b79fc3242dfbe.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9aa18e960c10d0cefd2f6bf80ee89dab.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  AdaHuman Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cc5f599d4203924725e20121808d44ef.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Contrast-Invariant Self-supervised Segmentation for Quantitative   Placental MRI
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
