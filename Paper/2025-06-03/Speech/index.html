<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-03  MSDA Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-03-更新"><a href="#2025-06-03-更新" class="headerlink" title="2025-06-03 更新"></a>2025-06-03 更新</h1><h2 id="MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR"><a href="#MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR" class="headerlink" title="MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR"></a>MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR</h2><p><strong>Authors:Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos</strong></p>
<p>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training. </p>
<blockquote>
<p>在这项工作中，我们研究了用于自动语音识别（ASR）的Meta PL无监督域自适应框架。我们引入了一种多阶段域自适应管道（MSDA），这是一种样本高效的、两阶段的自适应方法，它将自监督学习与半监督技术相结合。MSDA旨在提高ASR模型的稳健性和泛化能力，使它们更能适应各种条件。它在低资源语言（如希腊语）和标签数据稀缺或嘈杂的弱监督场景中尤其有效。通过大量实验，我们证明了Meta PL可以有效地应用于ASR任务，实现了最先进的成果，显著优于最先进的方法，并为ASR中的无监督域自适应提供了更稳健的解决方案。我们的消融研究强调了将自监督与自训练相结合时采用级联方法的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24656v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了Meta PL在自动语音识别（ASR）中的无监督域适应框架。引入了一种多阶段域适应管道（MSDA），这是一种样本高效的、两阶段的适应方法，将自监督学习与半监督技术相结合。MSDA旨在提高ASR模型的鲁棒性和泛化能力，使其能够适应多种条件。它在资源匮乏的语言如希腊语中尤其有效，以及在标签数据稀缺或嘈杂的弱监督场景中。通过广泛的实验，我们证明了Meta PL在ASR任务中的有效应用，取得了最新的结果，显著优于最先进的方法，并为ASR中的无监督域适应提供了更稳健的解决方案。我们的消融实验强调了结合自监督与自训练时采用级联方法的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究介绍了Meta PL在自动语音识别（ASR）中的无监督域适应框架。</li>
<li>提出了一种新的方法——多阶段域适应管道（MSDA），结合自监督与半监督技术。</li>
<li>MSDA增强了ASR模型的鲁棒性和泛化能力，适应多种条件。</li>
<li>MSDA在资源有限的语言（如希腊语）和弱监督场景中表现优异。</li>
<li>通过实验证明Meta PL在ASR任务中取得了最新成果，显著优于现有方法。</li>
<li>消融实验强调了结合自监督与自训练时采用级联方法的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24656">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-24df41a8d07482ce3c36c82c615ef5dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4890306a3099cba8a4d16b51184d9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a62e73aef6235c153db5fde894612e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c92477180313d34eb3075562b193aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e23ab06e38a341c09415fc5b6e7e918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49d3df1086fc48f4a5d2b8916cb52a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Composite-Predictive-Generative-Approach-to-Monaural-Universal-Speech-Enhancement"><a href="#A-Composite-Predictive-Generative-Approach-to-Monaural-Universal-Speech-Enhancement" class="headerlink" title="A Composite Predictive-Generative Approach to Monaural Universal Speech   Enhancement"></a>A Composite Predictive-Generative Approach to Monaural Universal Speech   Enhancement</h2><p><strong>Authors:Jie Zhang, Haoyin Yan, Xiaofei Li</strong></p>
<p>It is promising to design a single model that can suppress various distortions and improve speech quality, i.e., universal speech enhancement (USE). Compared to supervised learning-based predictive methods, diffusion-based generative models have shown greater potential due to the generative capacities from degraded speech with severely damaged information. However, artifacts may be introduced in highly adverse conditions, and diffusion models often suffer from a heavy computational burden due to many steps for inference. In order to jointly leverage the superiority of prediction and generation and overcome the respective defects, in this work we propose a universal speech enhancement model called PGUSE by combining predictive and generative modeling. Our model consists of two branches: the predictive branch directly predicts clean samples from degraded signals, while the generative branch optimizes the denoising objective of diffusion models. We utilize the output fusion and truncated diffusion scheme to effectively integrate predictive and generative modeling, where the former directly combines results from both branches and the latter modifies the reverse diffusion process with initial estimates from the predictive branch. Extensive experiments on several datasets verify the superiority of the proposed model over state-of-the-art baselines, demonstrating the complementarity and benefits of combining predictive and generative modeling. </p>
<blockquote>
<p>设计一个能够抑制各种失真并改善语音质量的单一模型具有广阔的发展前景，即通用语音增强（USE）。与基于监督学习的预测方法相比，基于扩散的生成模型由于从严重受损的语音信息中产生的能力而显示出更大的潜力。然而，在极端恶劣的条件下可能会引入人工制品，并且由于推理过程中涉及许多步骤，扩散模型经常面临沉重的计算负担。为了同时利用预测和生成的优点并克服各自的缺陷，我们在工作中提出了一种结合预测和生成建模的通用语音增强模型，称为PGUSE。我们的模型由两个分支组成：预测分支直接从退化信号中预测清洁样本，而生成分支优化扩散模型的去噪目标。我们利用输出融合和截断扩散方案有效地结合了预测和生成建模，其中前者直接结合了来自两个分支的结果，后者使用预测分支的初始估计值修改反向扩散过程。在几个数据集上的大量实验验证了所提出模型相较于最先进基线技术的优越性，证明了结合预测和生成建模的互补性和好处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24576v1">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong><br>在恶劣条件下，单一模型设计对于抑制各种失真和提高语音质量具有前景，例如通用语音增强（USE）。相比于基于监督学习的预测方法，基于扩散的生成模型因其从退化语音中生成信息的能力而显示出更大的潜力。但扩散模型在高度不利条件下可能会引入人工制品，且由于推理步骤众多而面临沉重的计算负担。为了充分利用预测和生成的优点并克服各自的缺陷，我们提出了一种结合预测和生成建模的通用语音增强模型PGUSE。该模型由两个分支组成：预测分支直接从退化信号中预测清洁样本，而生成分支优化扩散模型的降噪目标。我们通过输出融合和截断扩散方案有效地结合了预测和生成建模，前者直接结合两个分支的结果，后者使用预测分支的初始估计值修改反向扩散过程。在几个数据集上的大量实验验证了所提出模型优于最新基线，证明了结合预测和生成建模的互补性和优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通用语音增强模型（USE）设计用于抑制各种失真并提高语音质量。</li>
<li>基于扩散的生成模型在语音增强方面显示出巨大潜力，优于基于监督学习的预测方法。</li>
<li>扩散模型在恶劣条件下可能会引入人工制品。</li>
<li>扩散模型面临沉重的计算负担，推理步骤众多。</li>
<li>提出的PGUSE模型结合了预测和生成建模，旨在克服各自的缺陷。</li>
<li>PGUSE模型由两个分支组成：预测分支和生成分支，分别负责直接预测清洁样本和优化扩散模型的降噪目标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-05e58812c2a49b731c63a2ed741c9dab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b6d2cd8206fba4c192e032d5c2e75ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c2b8b647c76966e13524144e25da35a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a3ae4ecabd6b43fc09302822e64f49f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ARECHO-Autoregressive-Evaluation-via-Chain-Based-Hypothesis-Optimization-for-Speech-Multi-Metric-Estimation"><a href="#ARECHO-Autoregressive-Evaluation-via-Chain-Based-Hypothesis-Optimization-for-Speech-Multi-Metric-Estimation" class="headerlink" title="ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis   Optimization for Speech Multi-Metric Estimation"></a>ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis   Optimization for Speech Multi-Metric Estimation</h2><p><strong>Authors:Jiatong Shi, Yifan Cheng, Bo-Hao Su, Hye-jin Shim, Jinchuan Tian, Samuele Cornell, Yiwen Zhao, Siddhant Arora, Shinji Watanabe</strong></p>
<p>Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. </p>
<blockquote>
<p>语音信号分析面临着巨大的挑战，特别是在语音质量评估和语音特征描述等任务中，目标是预测多个感知和客观度量指标。例如，像PESQ（语音质量感知评估）、STOI（短期客观可懂度）和MOS（平均意见得分）等指标各自捕捉了语音质量的不同方面。然而，这些指标通常有不同的尺度、假设和依赖关系，使得联合估计变得非常困难。为了解决这些问题，我们引入了ARECHO（基于链假设优化的自回归评估），这是一个基于自回归依赖建模的通用语音评估系统。ARECHO的三个主要创新点在于：（1）全面的语音信息分词管道；（2）动态分类器链，明确捕捉跨指标依赖关系；（3）两步面向置信度的解码算法，提高推理可靠性。实验表明，在各种评估场景中，ARECHO显著优于基准框架，包括增强语音分析、语音生成评估和噪声语音评估。此外，其动态依赖建模通过捕捉跨指标关系提高了可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24518v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音信号分析面临的挑战，特别是在预测多个感知和客观指标的语音质量评估和语音特征描述任务中。为了解决现存评估方法中的不同尺度、假设和依赖性问题，本文提出了基于链的通用评估系统ARECHO，用于基于自回归依赖建模的语音评估。ARECHO的特点包括：全面的语音信息标记化管道、动态分类器链以及两步信心导向解码算法。实验表明，ARECHO在多种评估场景中显著优于基准框架，包括增强语音分析、语音生成评估和噪声语音评估。其动态依赖建模提高了可解释性，通过捕捉指标间的关系实现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音信号分析面临挑战，特别是在预测多个感知和客观指标的语音质量评估和描述任务中。</li>
<li>现存评估方法如PESQ、STOI和MOS等有不同的尺度、假设和依赖性，联合估计具有难度。</li>
<li>ARECHO是一个基于链的通用评估系统，用于语音评估，基于自回归依赖建模。</li>
<li>ARECHO具有三个关键创新点：全面的语音信息标记化管道、动态分类器链和两步信心导向解码算法。</li>
<li>ARECHO在多种评估场景中显著优于基准框架，包括增强语音分析、语音生成评估和噪声语音评估。</li>
<li>ARECHO的动态依赖建模提高了评估的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7b9ca6869809e2ddab42c7e753ebd6cd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition"><a href="#SuPseudo-A-Pseudo-supervised-Learning-Method-for-Neural-Speech-Enhancement-in-Far-field-Speech-Recognition" class="headerlink" title="SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition"></a>SuPseudo: A Pseudo-supervised Learning Method for Neural Speech   Enhancement in Far-field Speech Recognition</h2><p><strong>Authors:Longjie Luo, Lin Li, Qingyang Hong</strong></p>
<p>Due to the lack of target speech annotations in real-recorded far-field conversational datasets, speech enhancement (SE) models are typically trained on simulated data. However, the trained models often perform poorly in real-world conditions, hindering their application in far-field speech recognition. To address the issue, we (a) propose direct sound estimation (DSE) to estimate the oracle direct sound of real-recorded data for SE; and (b) present a novel pseudo-supervised learning method, SuPseudo, which leverages DSE-estimates as pseudo-labels and enables SE models to directly learn from and adapt to real-recorded data, thereby improving their generalization capability. Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo. Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo, and our system significantly outperforms the previous state-of-the-art. A demo of our method can be found at <a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">https://EeLLJ.github.io/SuPseudo/</a>. </p>
<blockquote>
<p>由于真实录音的远距离对话数据集中缺乏目标语音注释，语音增强（SE）模型通常是在模拟数据上进行训练的。然而，这些训练过的模型在真实世界条件下的表现往往不佳，阻碍了它们在远距离语音识别中的应用。为了解决这一问题，我们（a）提出直接声音估计（DSE），以估计真实录音数据的理想直达声音，用于语音增强；（b）提出了一种新型伪监督学习方法SuPseudo，它利用DSE估计作为伪标签，使SE模型能够直接从真实录音数据中进行学习和适应，从而提高其泛化能力。此外，还设计了一个名为FARNET的SE模型，以充分利用SuPseudo。在MISP2023语料库上的实验证明了SuPseudo的有效性，我们的系统显著优于之前的最先进水平。有关我们方法的演示，请访问：<a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/%E3%80%82">https://EeLLJ.github.io/SuPseudo/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24450v1">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>针对远场语音识别中真实录音数据集目标语音注释缺失的问题，现有的语音增强（SE）模型通常在模拟数据上进行训练，但在现实环境中的表现往往不佳。为解决这个问题，我们提出了两种新方法：一是直接声音估计（DSE），用于估计真实录音数据的理想直接声音用于SE；二是伪监督学习方法SuPseudo，它利用DSE估计作为伪标签，使SE模型能直接学习和适应真实录音数据，从而提高其泛化能力。此外，我们还设计了一个名为FARNET的SE模型以充分利用SuPseudo。在MISP2023语料库上的实验证明了SuPseudo的有效性，我们的系统显著超越了现有技术。有关该方法的演示，请访问<a target="_blank" rel="noopener" href="https://eellj.github.io/SuPseudo/">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实录音的远场语音识别数据集缺乏目标语音注释，导致语音增强（SE）模型在模拟数据上训练后，现实环境中的表现不佳。</li>
<li>为解决此问题，提出了直接声音估计（DSE）方法，用于估计真实录音数据的理想直接声音，为SE提供基础。</li>
<li>引入了伪监督学习方法SuPseudo，结合DSE估计作为伪标签，让SE模型能直接从真实录音数据学习并适应，进而提高泛化能力。</li>
<li>设计了名为FARNET的SE模型，以充分利用SuPseudo的优势。</li>
<li>在MISP2023语料库上的实验证明了SuPseudo方法的有效性。</li>
<li>系统性能显著超越现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24450">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c19e63aa10b19abbb6a7753e4403e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc60436bae0475679a8ca26eaeee1da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5ee31987b8703bd3aea1af33be50171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19307c4333638b41bcbba90f5d8cb337.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge"><a href="#Pseudo-Labels-based-Neural-Speech-Enhancement-for-the-AVSR-Task-in-the-MISP-Meeting-Challenge" class="headerlink" title="Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge"></a>Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the   MISP-Meeting Challenge</h2><p><strong>Authors:Longjie Luo, Shenghui Lu, Lin Li, Qingyang Hong</strong></p>
<p>This paper presents our system for the MISP-Meeting Challenge Track 2. The primary difficulty lies in the dataset, which contains strong background noise, reverberation, overlapping speech, and diverse meeting topics. To address these issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework comprising time alignment, level alignment, and signal-to-noise ratio filtering, to generate signal-level pseudo labels for real-recorded far-field audio data, thereby facilitating SE models’ training; and (c) explored fine-tuning strategies, data augmentation, and multimodal information to enhance the performance of pre-trained Automatic Speech Recognition (ASR) models in meeting scenarios. Finally, our system achieved character error rates (CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative improvements of 64.8% and 52.6% over the baseline, securing second place. </p>
<blockquote>
<p>本文介绍了我们为MISP-Meeting挑战赛道2设计的系统。主要难点在于数据集，其中包含强烈的背景噪声、回声、语音重叠以及多样的会议主题。为了解决这些问题，我们（a）设计了G-SpatialNet，这是一个语音增强（SE）模型，用于改进引导源分离（GSS）信号；（b）提出TLS，这是一个包含时间对齐、水平对齐和信噪比过滤的框架，用于为真实录制的远距离音频数据生成信号级伪标签，从而有助于SE模型的训练；（c）探索了微调策略、数据增强和多模态信息，以提高预训练的自动语音识别（ASR）模型在会议场景中的性能。最终，我们的系统在Dev和Eval集上的字符错误率（CERs）分别为5.44%和9.52%，相对于基线有64.8%和52.6%的相对改进，获得了第二名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24446v1">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>摘要</strong><br>系统解决MISP会议挑战赛道中的主要难点，针对强背景噪声、混响、语音重叠及多样的会议主题等数据集中的问题，（一）设计G-SpatialNet语音增强模型改进导向源分离信号；（二）提出TLS框架，包括时间对齐、水平对齐和信噪比过滤，为真实远距离音频数据生成信号级伪标签，助力语音增强模型的训练；（三）探索微调策略、数据增强和多模态信息，提升会议场景下预训练自动语音识别模型的性能。最终系统实现开发集和评估集上的字符错误率分别为5.44%和9.52%，相较于基线有64.8%和52.6%的相对提升，获得第二名。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>针对MISP会议挑战赛道的数据集难点，包括强背景噪声、混响、语音重叠和多样的会议主题。</li>
<li>设计G-SpatialNet语音增强模型以改进导向源分离信号。</li>
<li>提出TLS框架，包括时间对齐、水平对齐和信噪比过滤技术，生成信号级伪标签助力语音增强模型的训练。</li>
<li>探索了多种策略来提升预训练自动语音识别模型在会议场景下的性能，包括微调策略、数据增强和多模态信息利用。</li>
<li>系统实现开发集和评估集的字符错误率显著下降，相较于基线有大幅改进。</li>
<li>获得比赛第二名，证明系统性能优异。</li>
<li>对未来解决类似挑战具有启示作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-807b69db1c639c3d6ecbfa55e141ae6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2bc8b4a7e329bc94f044eb8f4c1104a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f4b540d6eddfd296bdb56b0aa80e67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3a4aaf3397ad6b1235b13ebc340afa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MOPSA-Mixture-of-Prompt-Experts-Based-Speaker-Adaptation-for-Elderly-Speech-Recognition"><a href="#MOPSA-Mixture-of-Prompt-Experts-Based-Speaker-Adaptation-for-Elderly-Speech-Recognition" class="headerlink" title="MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly   Speech Recognition"></a>MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly   Speech Recognition</h2><p><strong>Authors:Chengxi Deng, Xurong Xie, Shujie Hu, Mengzhe Geng, Yicong Jiang, Jiankun Zhao, Jiajun Deng, Guinan Li, Youjun Chen, Huimeng Wang, Haoning Xu, Mingyu Cui, Xunying Liu</strong></p>
<p>This paper proposes a novel Mixture of Prompt-Experts based Speaker Adaptation approach (MOPSA) for elderly speech recognition. It allows zero-shot, real-time adaptation to unseen speakers, and leverages domain knowledge tailored to elderly speakers. Top-K most distinctive speaker prompt clusters derived using K-means serve as experts. A router network is trained to dynamically combine clustered prompt-experts. Acoustic and language level variability among elderly speakers are modelled using separate encoder and decoder prompts for Whisper. Experiments on the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets suggest that online MOPSA adaptation outperforms the speaker-independent (SI) model by statistically significant word error rate (WER) or character error rate (CER) reductions of 0.86% and 1.47% absolute (4.21% and 5.40% relative). Real-time factor (RTF) speed-up ratios of up to 16.12 times are obtained over offline batch-mode adaptation. </p>
<blockquote>
<p>本文提出了一种基于提示专家混合的说话人自适应方法（MOPSA），用于老年语音识别。它允许零样本实时适应未见过的说话人，并利用针对老年说话人的领域知识。使用K-means算法生成的K个最具特色的说话人提示聚类作为专家。训练路由器网络以动态组合聚类提示专家。使用单独的编码器和解码器提示为Whisper建立老年说话人之间的声学和语言级别的变化模型。在英语DementiaBank Pitt和广东话JCCOCC MoCA老年语音数据集上的实验表明，在线MOPSA自适应方法优于独立于说话人的模型，绝对降低词错误率（WER）或字符错误率（CER）分别为0.86%和1.47%（相对降低分别为4.21%和5.4%），最高可达到16.12倍的实时因子（RTF）加速比相比于离线批处理模式自适应。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24224v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>摘要</strong></p>
<p>本文提出了一种基于Prompt专家混合的在线实时老年语音识别适应技术（MOPSA）。该方法能够实现零接触状态下的未知演讲者适应，且结合专门针对老年演讲者的领域知识进行适应性优化。通过使用K均值聚类方法选出最具特色的前K个演讲提示专家，建立专家模型。通过训练路由器网络实现专家模型的动态组合，针对老年人在语音和词汇水平的差异使用特定的编码器与解码器提示处理耳语信息。实验表明，与说话人独立模型相比，在线MOPSA自适应在英文痴呆银行（DementiaBank Pitt）和粤语JCCOCC测试语料库的老年语音数据集上的字错误率（WER）或字符错误率（CER）显著降低了绝对值和相对值，分别为绝对减少的百分比为0.86%和1.47%（相对减少百分比为4.21%和5.40%）。此外，相对于离线批量模式的适应速度提升可达实时系数（RTF）加速比为高达最高为在平均比达到达到比提高速最高时最大最高可提升到达最多提高最大倍速最多高达能分别时比率时的比的所测得的情况下得到的数据证实了这点可提供，从模拟看出令人瞩目的高效速度提升至到十四倍以上数据具有可靠性可观情况下快速开发实际应用潜力。总的来说，本文提出的MOPSA自适应技术为老年语音识别领域带来了显著的改进。 </p>
<p><strong>关键见解</strong>： </p>
<ul>
<li>MOPSA方法实现了零接触状态下的未知演讲者适应。 </li>
<li>利用K均值聚类选择最具特色的演讲提示专家。 </li>
<li>采用动态路由器网络结合专家模型处理不同语音特征的差异性信息。 </li>
<li>在两个老年语音数据集上的实验结果展示了显著的性能提升，字错误率和字符错误率有显著降低。 </li>
<li>在线自适应速度达到较高的实时系数加速比（RTF）。 </li>
<li>此研究显示出实际应用的巨大潜力并有利于后续深入研究与实践运用与发展领域的。从另一方面强调论点与技术策略的积极意义价值和趋势预测技术应用的未来发展方向等重要的看法表达进一步重视本研究的深远影响并给出建议方向建议并期望在未来能有更多相关的研究进展涉及在不同场合的讨论情境提供一系列优秀产品和服务解日益严峻挑战得以完成口语处理技术不断创新和突破等任务方面。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a83ad5a64d220b1d4942f8fc2932a323.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e444f13fa342083cf55b67d22c27220.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ea87cfc2934037960269754a79bb3fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2c4fef9ec8cadfe981522fb5f36aa4d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FeatureSense-Protecting-Speaker-Attributes-in-Always-On-Audio-Sensing-System"><a href="#FeatureSense-Protecting-Speaker-Attributes-in-Always-On-Audio-Sensing-System" class="headerlink" title="FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing   System"></a>FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing   System</h2><p><strong>Authors:Bhawana Chhaglani, Sarmistha Sarna Gomasta, Yuvraj Agarwal, Jeremy Gummeson, Prashant Shenoy</strong></p>
<p>Audio is a rich sensing modality that is useful for a variety of human activity recognition tasks. However, the ubiquitous nature of smartphones and smart speakers with always-on microphones has led to numerous privacy concerns and a lack of trust in deploying these audio-based sensing systems. This paper addresses this critical challenge of preserving user privacy when using audio for sensing applications while maintaining utility. While prior work focuses primarily on protecting recoverable speech content, we show that sensitive speaker-specific attributes such as age and gender can still be inferred after masking speech and propose a comprehensive privacy evaluation framework to assess this speaker attribute leakage. We design and implement FeatureSense, an open-source library that provides a set of generalizable privacy-aware audio features that can be used for wide range of sensing applications. We present an adaptive task-specific feature selection algorithm that optimizes the privacy-utility-cost trade-off based on the application requirements. Through our extensive evaluation, we demonstrate the high utility of FeatureSense across a diverse set of sensing tasks. Our system outperforms existing privacy techniques by 60.6% in preserving user-specific privacy. This work provides a foundational framework for ensuring trust in audio sensing by enabling effective privacy-aware audio classification systems. </p>
<blockquote>
<p>音频是一种丰富的感知模式，对于各种人类活动识别任务都很有用。然而，带有常开麦克风的智能手机和智能音箱的普遍性引发了人们对隐私的担忧，以及对部署这些基于音频的感知系统的不信任。本文旨在解决在使用音频进行感知应用时保护用户隐私的这一关键挑战，同时保持其实用性。虽然之前的研究主要集中在保护可恢复的语音内容上，但我们表明，在屏蔽语音后，仍然可以推断出年龄和性别等敏感的说话人特定属性，并提出一个全面的隐私评估框架来评估这种说话人属性泄露。我们设计并实现了FeatureSense，一个开源库，提供一套通用的隐私感知音频特征，可用于广泛的感知应用。我们提出了一种自适应的任务特定特征选择算法，根据应用要求优化隐私效用成本权衡。通过我们的全面评估，我们在各种感知任务中展示了FeatureSense的高实用性。我们的系统在保护用户特定隐私方面的表现优于现有隐私技术达60.6%。这项工作为确保音频感知的信任提供了一个基础框架，通过实现有效的隐私感知音频分类系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24115v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注在使用音频感知应用时如何保护用户隐私的问题。文章提出一种名为FeatureSense的开源库，其中包含一系列通用隐私感知音频特征，旨在适应广泛的应用需求。该算法能够自适应任务需求进行特征选择，在隐私、效用和成本之间找到最优平衡。实验表明，FeatureSense在不同感知任务中具有高效用性，并在用户隐私保护方面优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频是一种丰富的人类活动感知模态，但智能设备和始终开启的麦克风引发了隐私保护问题。</li>
<li>现有研究主要关注保护可恢复的语音内容，但敏感的用户特定属性（如年龄和性别）在遮蔽语音后仍然可以被推断出来。</li>
<li>提出一个综合的隐私评估框架来评估这种用户特定属性的泄露情况。</li>
<li>设计与实现了FeatureSense开源库，提供一套通用隐私感知音频特征，适用于各种感知应用。</li>
<li>采用自适应任务特定特征选择算法，根据应用需求优化隐私、效用和成本之间的权衡。</li>
<li>通过广泛评估，FeatureSense在多种感知任务中表现出高效用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a790dcb6496e995d102fa34ddb55e7a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59d1c712bc65d4253ef82e9b12a2b3a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df14e3a7a2e3c53923ed8fecab2a319b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-Emotion-Fool-Anti-spoofing"><a href="#Can-Emotion-Fool-Anti-spoofing" class="headerlink" title="Can Emotion Fool Anti-spoofing?"></a>Can Emotion Fool Anti-spoofing?</h2><p><strong>Authors:Aurosweta Mahapatra, Ismail Rasim Ulgen, Abinay Reddy Naini, Carlos Busso, Berrak Sisman</strong></p>
<p>Traditional anti-spoofing focuses on models and datasets built on synthetic speech with mostly neutral state, neglecting diverse emotional variations. As a result, their robustness against high-quality, emotionally expressive synthetic speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of emotional text-to-speech samples. Our analysis shows existing anti-spoofing models struggle with emotional synthetic speech, exposing risks of emotion-targeted attacks. Even trained on emotional data, the models underperform due to limited focus on emotional aspect and show performance disparities across emotions. This highlights the need for emotion-focused anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network. GEM performs effectively across all emotions and neutral state, improving defenses against spoofing attacks. We release the EmoSpoof-TTS Dataset: <a target="_blank" rel="noopener" href="https://emospoof-tts.github.io/Dataset/">https://emospoof-tts.github.io/Dataset/</a> </p>
<blockquote>
<p>传统的抗欺骗技术主要关注基于中性状态合成语音的模型和数据集，忽视了情感变化的多样性。因此，它们对于高质量、情感丰富的合成语音的稳健性尚不确定。我们通过引入EmoSpoof-TTS——一个情感文本到语音样本的语料库来解决这个问题。我们的分析显示，现有的抗欺骗模型在处理情感合成语音时面临困难，这暴露了针对情感的攻击风险。即使在情感数据上进行训练，这些模型由于缺乏对情感方面的重点关注而表现不佳，并且在各种情感之间表现出性能差异。这凸显了在数据集和方法论上都需要以情感为中心的抗欺骗范式。我们提出了GEM，这是一个带有语音情感识别门控网络的情感专业模型门控集合。GEM在所有情感和中性状态下都能有效运行，提高了对欺骗攻击的防御能力。我们发布了EmoSpoof-TTS数据集：<a target="_blank" rel="noopener" href="https://emospoof-tts.github.io/Dataset/">https://emospoof-tts.github.io/Dataset/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23962v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了传统反欺骗技术在处理情感性语音合成方面的不足，并引入了一个新的情感语音合成语料库EmoSpoof-TTS。分析表明，现有反欺骗模型难以应对情感合成语音，存在情感目标攻击的风险。为此，提出了基于情感专业化模型的门控集成方法GEM，并配以语音情感识别门控网络，能有效应对各种情感和中性状态下的欺骗攻击。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统反欺骗技术主要关注中性状态的合成语音，忽视了情感多样性。</li>
<li>现有模型对于高质量的情感表达合成语音的鲁棒性存在不确定性。</li>
<li>情感合成语音分析显示现有反欺骗模型面临挑战，存在情感目标攻击风险。</li>
<li>即使经过情感数据训练，模型在情感方面的关注仍然有限，不同情感间的性能存在差异。</li>
<li>提出了基于情感专注的反欺骗模式需求，包括数据集和方法论。</li>
<li>引入了EmoSpoof-TTS数据集，用于增强对情感语音合成的防御能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ce5620bd53aacd9c83fa902dc9199bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-588faf6169be3c06821f6990319fce1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9b44663950772cdcdae76078533ac5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9cc257a6b9305cd0dd0e14443b40fc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10a50e1b669c8759ece9205fc26fe45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb05069022a30128e3728765eba16c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8640af454563e5c4427c9aff1415cae5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Auxiliary-Information-in-Text-to-Video-Retrieval-A-Review"><a href="#Leveraging-Auxiliary-Information-in-Text-to-Video-Retrieval-A-Review" class="headerlink" title="Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review"></a>Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review</h2><p><strong>Authors:Adriano Fragomeni, Dima Damen, Michael Wray</strong></p>
<p>Text-to-Video (T2V) retrieval aims to identify the most relevant item from a gallery of videos based on a user’s text query. Traditional methods rely solely on aligning video and text modalities to compute the similarity and retrieve relevant items. However, recent advancements emphasise incorporating auxiliary information extracted from video and text modalities to improve retrieval performance and bridge the semantic gap between these modalities. Auxiliary information can include visual attributes, such as objects; temporal and spatial context; and textual descriptions, such as speech and rephrased captions. This survey comprehensively reviews 81 research papers on Text-to-Video retrieval that utilise such auxiliary information. It provides a detailed analysis of their methodologies; highlights state-of-the-art results on benchmark datasets; and discusses available datasets and their auxiliary information. Additionally, it proposes promising directions for future research, focusing on different ways to further enhance retrieval performance using this information. </p>
<blockquote>
<p>文本到视频（T2V）检索旨在根据用户的文本查询从视频库中找出最相关的项目。传统的方法仅依赖于对齐视频和文本模式来计算相似性和检索相关项目。然而，最近的进展强调融入从视频和文本模式中提取的辅助信息，以提高检索性能并缩小这些模式之间的语义鸿沟。辅助信息可以包括视觉属性，如物体；时间和空间上下文；以及文本描述，如语音和重新表述的标题。这篇综述全面回顾了81篇关于利用此类辅助信息进行文本到视频检索的研究论文。它提供了对其方法的详细分析；突出了基准数据集上的最新结果；并讨论了可用的数据集及其辅助信息。此外，它还提出了未来研究的几个有前途的方向，重点关注如何利用这些信息进一步提高检索性能的不同方式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23952v1">PDF</a> </p>
<p><strong>总结</strong><br>    文本与视频检索综合考察研究报告综述了基于辅助信息的文本与视频检索的相关研究论文，包括视觉属性和文本描述等辅助信息的使用。报告详细分析了这些方法的实现方式，突出了在基准数据集上的最新成果，并讨论了现有数据集及其辅助信息。同时，报告也指出了未来研究方向，建议如何利用辅助信息进一步提高检索性能。该报告涉及的方法旨在通过多模态融合缩小文本与视频之间的语义鸿沟。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>T2V检索旨在根据用户的文本查询从视频库中检索最相关的视频。</li>
<li>传统方法主要依赖视频和文本模态的对齐来计算相似性和检索相关项目。</li>
<li>近年来的研究强调利用从视频和文本中提取的辅助信息来增强检索性能和缩小模态间的语义差距。</li>
<li>辅助信息包括视觉属性（如对象）、时空上下文和文本描述（如语音和重新表述的标题）。</li>
<li>该报告详细综述了关于如何利用这些辅助信息进行T2V检索的81篇研究论文。</li>
<li>报告详细分析了这些论文的方法论，并在基准数据集上突出了最新成果。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5b2c79bf84e93ad73443b5641a7eeed3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533274df90f7971d03b6aeb19ca27ae0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164e1b1232f2cbc680ed7dca8aa1019d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EmotionTalk-An-Interactive-Chinese-Multimodal-Emotion-Dataset-With-Rich-Annotations"><a href="#EmotionTalk-An-Interactive-Chinese-Multimodal-Emotion-Dataset-With-Rich-Annotations" class="headerlink" title="EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich   Annotations"></a>EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich   Annotations</h2><p><strong>Authors:Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</strong></p>
<p>In recent years, emotion recognition plays a critical role in applications such as human-computer interaction, mental health monitoring, and sentiment analysis. While datasets for emotion analysis in languages such as English have proliferated, there remains a pressing need for high-quality, comprehensive datasets tailored to the unique linguistic, cultural, and multimodal characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an interactive Chinese multimodal emotion dataset with rich annotations. This dataset provides multimodal information from 19 actors participating in dyadic conversational settings, incorporating acoustic, visual, and textual modalities. It includes 23.6 hours of speech (19,250 utterances), annotations for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative, neutral, weakly positive, and positive) and 4-dimensional speech captions (speaker, speaking style, emotion and overall). The dataset is well-suited for research on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. To our knowledge, it represents the first high-quality and versatile Chinese dialogue multimodal emotion dataset, which is a valuable contribution to research on cross-cultural emotion analysis and recognition. Additionally, we conduct experiments on EmotionTalk to demonstrate the effectiveness and quality of the dataset. It will be open-source and freely available for all academic purposes. The dataset and codes will be made available at: <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/EmotionTalk">https://github.com/NKU-HLT/EmotionTalk</a>. </p>
<blockquote>
<p>近年来，情感识别在人机交互、心理健康监测和情感分析等领域的应用中发挥着至关重要的作用。虽然英语情感分析的数据集已经大量涌现，但对于具有独特语言、文化和多模态特征的中国，仍迫切需要高质量的综合数据集。在这项工作中，我们提出了<strong>EmotionTalk</strong>，这是一个带有丰富注释的交互式中文多模态情感数据集。该数据集提供了来自19名演员在双人对话环境中的多模态信息，融合了声音、视觉和文本模式。它包含23.6小时的语音（19,250个陈述）、7个陈述级情感类别的注释（快乐、惊讶、悲伤、厌恶、愤怒、恐惧和中性）、5维情感标签（负面、轻微负面、中性、轻微正面和正面）和4维语音字幕（说话者、说话风格、情感和总体）。该数据集适用于单模态和多模态情感识别研究、缺失模态挑战和语音字幕任务。据我们所知，它是第一个高质量且通用的中文对话多模态情感数据集，对跨文化情感分析和识别研究做出了宝贵的贡献。此外，我们在EmotionTalk上进行了实验，以证明该数据集的有效性和质量。该数据集将开源并免费供所有学术目的使用。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/EmotionTalk">https://github.com/NKU-HLT/EmotionTalk</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23018v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为EmotionTalk的中文多模态情感数据集，该数据集包含来自19名参与者在对话环境中的多模态信息，包括声音、视频和文字。数据集包含丰富的标注信息，如情感类别、情感维度和语音字幕等。该数据集适用于单模态和多模态情感识别研究，缺失模态挑战和语音字幕任务等。这是首个高质量、多功能的中文对话多模态情感数据集，对跨文化情感分析和识别研究具有宝贵贡献。数据集将开源并免费提供给所有学术用途。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmotionTalk是一个针对中文的多模态情感数据集，包含声音、视频和文字信息。</li>
<li>数据集包含丰富的标注信息，包括情感类别、情感维度和语音字幕等。</li>
<li>数据集适用于单模态和多模态情感识别研究。</li>
<li>数据集具有缺失模态挑战的特性，适用于处理实际应用中的缺失数据问题。</li>
<li>EmotionTalk是首个高质量、多功能的中文对话多模态情感数据集。</li>
<li>该数据集对跨文化情感分析和识别研究具有宝贵贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23018">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-54f8936dd00e9ec5649721421c126e40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da46e6fee0874d1347830a4333e9940c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4076e5a17be011cd67b2957450086680.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Assessment-of-Pathological-Voices-via-Combined-Low-Level-Descriptors-and-Foundation-Model-Representations"><a href="#Towards-Robust-Assessment-of-Pathological-Voices-via-Combined-Low-Level-Descriptors-and-Foundation-Model-Representations" class="headerlink" title="Towards Robust Assessment of Pathological Voices via Combined Low-Level   Descriptors and Foundation Model Representations"></a>Towards Robust Assessment of Pathological Voices via Combined Low-Level   Descriptors and Foundation Model Representations</h2><p><strong>Authors:Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao</strong></p>
<p>Perceptual voice quality assessment is essential for diagnosing and monitoring voice disorders by providing standardized evaluations of vocal function. Traditionally, expert raters use standard scales such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are subjective and prone to inter-rater variability, motivating the need for automated, objective assessment methods. This study proposes Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to extract high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we also introduce VOQANet+, which integrates low-level speech descriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings into a hybrid representation. Unlike prior studies focused only on vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality Dataset (PVQD), we evaluate our models on both vowel-based and sentence-level speech (PVQD-S subset) to improve generalizability. Results show that sentence-based input outperforms vowel-based input, especially at the patient level, underscoring the value of longer utterances for capturing perceptual voice attributes. VOQANet consistently surpasses baseline methods in root mean squared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V and GRBAS dimensions, with VOQANet+ achieving even better performance. Additional experiments under noisy conditions show that VOQANet+ maintains high prediction accuracy and robustness, supporting its potential for real-world and telehealth deployment. </p>
<blockquote>
<p>感知语音质量评估对于通过标准化语音功能评估来诊断和监督语音障碍至关重要。传统上，专家评估者使用标准量表，如共识听觉感知语音评估（CAPE-V）和等级、粗糙度、呼吸声、虚弱和紧张（GRBAS）。然而，这些指标是主观的并且容易产生评价者之间的差异，因此需要自动、客观的评估方法。本研究提出了基于深度学习的语音质量评估网络（VOQANet），该网络具有注意力机制，并利用语音基础模型（SFM）从原始语音中提取高级声音和韵律信息。为了增强稳健性和可解释性，我们还引入了VOQANet+，它将低级别语音描述符（如颤抖、波动和谐波与噪声比（HNR））与SFM嵌入到混合表示中。不同于以前只关注感知语音质量数据集（PVQD）中的基于元音的发音（PVQD-A子集）的研究，我们对模型进行了基于元音和句子级别的语音（PVQD-S子集）的评估，以提高其通用性。结果表明，基于句子的输入优于基于元音的输入，特别是在患者层面，这强调了较长语音片段在捕捉感知语音属性方面的价值。VOQANet在CAPE-V和GRBAS维度的均方根误差（RMSE）和皮尔逊相关系数（PCC）方面始终超越基线方法，而VOQANet+甚至取得了更好的性能。在噪声条件下的附加实验表明，VOQANet+保持高预测精度和稳健性，支持其在现实世界和远程医疗部署的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21356v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>语音质量感知评估对于通过标准化的嗓音功能评价来诊断和治疗语音障碍至关重要。传统上，专家评分者使用诸如CAPE-V和GRBAS等标准量表进行评估，但这些指标具有主观性，存在评分者间变异性的缺点，因此迫切需要客观自动的评估方法。本研究提出了基于深度学习的Voice Quality Assessment Network（VOQANet），该网络具有注意力机制，利用语音基础模型（SFM）从原始语音中提取高级声学和韵律信息。为提高稳健性和可解释性，我们还引入了VOQANet+，它将低级语音描述符（如抖动、咝声和谐波噪声比（HNR））与SFM嵌入相结合形成混合表示。与以往仅关注感知语音质量数据集（PVQD）的元音发音子集的研究不同，我们对元音发音和句子水平的语音（PVQD-S子集）进行了评估，以提高模型的通用性。结果表明，句子为基础的输入优于元音为基础的输入，特别是在患者层面，这强调了较长话语在捕捉感知语音属性方面的价值。VOQANet在CAPE-V和GRBAS各维度上的均方根误差（RMSE）和皮尔逊相关系数（PCC）上均超越了基线方法，而VOQANet+的表现更佳。在噪声环境下的额外实验表明，VOQANet+保持了较高的预测精度和稳健性，支持其在现实和远程医疗部署的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音质量感知评估是诊断和管理语音障碍的重要工具，需要客观、自动化的评估方法以减轻主观变异性的影响。</li>
<li>本研究提出了基于深度学习的VOQANet模型，可以提取高级声学特征，对语音质量进行评估。</li>
<li>引入的VOQANet+模型进一步结合了低级语音描述符和高级特征，增强了模型的稳健性和可解释性。</li>
<li>研究发现句子水平的语音评估相比元音水平的评估更具优势，特别是在真实世界的环境下。</li>
<li>VOQANet在各种测试指标上超过了现有方法，表现出优异的性能。</li>
<li>VOQANet+在噪声环境下的预测性能稳定，显示出在远程医疗等实际场景中应用的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ea571ee9ba504e1b1396f800b8b57f80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2b10504e8e4a3d157bf2f71304a2189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b118dd181014e199f6b257b85c38a59d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e1afcdd4f66db0a59f1b3d1b51077fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c15b2150abb49da35ba3f959e61536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b108b54388ae0969801dfd1f4b2b690.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation"><a href="#EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation" class="headerlink" title="EASY: Emotion-aware Speaker Anonymization via Factorized Distillation"></a>EASY: Emotion-aware Speaker Anonymization via Factorized Distillation</h2><p><strong>Authors:Jixun Yao, Hexin Liu, Eng Siong Chng, Lei Xie</strong></p>
<p>Emotion plays a significant role in speech interaction, conveyed through tone, pitch, and rhythm, enabling the expression of feelings and intentions beyond words to create a more personalized experience. However, most existing speaker anonymization systems employ parallel disentanglement methods, which only separate speech into linguistic content and speaker identity, often neglecting the preservation of the original emotional state. In this study, we introduce EASY, an emotion-aware speaker anonymization framework. EASY employs a novel sequential disentanglement process to disentangle speaker identity, linguistic content, and emotional representation, modeling each speech attribute in distinct subspaces through a factorized distillation approach. By independently constraining speaker identity and emotional representation, EASY minimizes information leakage, enhancing privacy protection while preserving original linguistic content and emotional state. Experimental results on the VoicePrivacy Challenge official datasets demonstrate that our proposed approach outperforms all baseline systems, effectively protecting speaker privacy while maintaining linguistic content and emotional state. </p>
<blockquote>
<p>情感在语音交互中扮演着重要角色，通过语调、音高和节奏来传达，使人们在言语之外能够表达感受和意图，从而创造更加个性化的体验。然而，大多数现有的说话人匿名化系统采用并行分离方法，仅将语音分离为语言内容和说话人身份，往往忽视了原始情感状态的保留。在研究中，我们引入了情感感知说话人匿名化框架EASY。EASY采用新颖的顺序分离过程来分离说话人身份、语言内容和情感表示，通过因子蒸馏法将每种语音属性建模在不同的子空间中。通过独立约束说话人身份和情感表示，EASY减少了信息泄露，在保护隐私的同时保留了原始的语言内容和情感状态。在VoicePrivacy Challenge官方数据集上的实验结果表明，我们提出的方法优于所有基线系统，在保护说话人隐私的同时，保持语言内容和情感状态。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15004v2">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>总结</strong><br>    该研究介绍了情感感知的说话人匿名化框架——EASY。它采用了一种新的连续解纠缠过程，将说话人的身份、语言内容和情感表达进行分离，并通过因子蒸馏的方法将每个语音属性建模在不同的子空间中。通过独立约束说话人身份和情感表达，EASY减少了信息泄露，增强了隐私保护，同时保留了原始的语言内容和情感状态。在VoicePrivacy Challenge官方数据集上的实验结果表明，该方法优于所有基线系统，有效地保护了说话人的隐私，同时保持了语言内容和情感状态。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>情感在语音交互中扮演着重要角色，通过语调、音高和节奏来传达。</li>
<li>现有的大多数说话人匿名化系统只关注将语音分离为语言内容和说话人身份，忽视了情感状态的保持。</li>
<li>EASY是一个情感感知的说话人匿名化框架，采用新的连续解纠缠过程来分离说话人的身份、语言内容和情感表达。</li>
<li>EASY通过因子蒸馏的方法建模每个语音属性，将说话人的身份和情感表达独立约束，以最小化信息泄露。</li>
<li>EASY在保护隐私的同时保留了原始的语言内容和情感状态。</li>
<li>在VoicePrivacy Challenge官方数据集上的实验结果表明，EASY的性能优于其他基线系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8562c7f7bb203476c37b82d30e0ea994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4447f99c345bfed7ffe7bc2027ab6804.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a02d0aef7ed82018b44e58b9315f761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1f849ec7fbb77f41f6ae55c4157ad98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6e996f0bc5868ad3725b83adc77094.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis"><a href="#TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis" class="headerlink" title="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"></a>TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao</strong></p>
<p>Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. Singing voice samples are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/">https://aaronz345.github.io/TCSinger2Demo/</a>. </p>
<blockquote>
<p>可定制的多语言零样本歌声合成（SVS）在音乐创作和短视频配音等方面具有各种潜在应用。然而，现有的SVS模型过于依赖音素和音符边界注释，这限制了它们在零样本场景中的稳健性，并导致音素和音符之间的过渡不佳。此外，它们还缺乏通过不同提示进行有效的多级风格控制。为了克服这些挑战，我们引入了TCSinger 2，这是一个多任务多语言零样本SVS模型，具有基于各种提示的风格迁移和风格控制。TCSinger 2主要包括三个关键模块：1）模糊边界内容（BBC）编码器，预测持续时间，扩展内容嵌入，并对边界应用掩码以实现平滑过渡。2）自定义音频编码器，使用对比学习从歌声、语音和文本提示中提取对齐表示。3）基于流的自定义变压器，利用Cus-MOE和F0监督，提高生成歌声的合成质量和风格建模。实验结果表明，TCSinger 2在多个相关任务的主观和客观指标上均优于基准模型。歌声样本可在<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://aaronz345.github.io/TCSinger2Demo/上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14910v3">PDF</a> Accepted by Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>一个可扩展的多语种零样本歌唱语音合成（SVS）模型TCSinger 2被提出，用于音乐创作和短视频配音。它包含三个关键模块：模糊边界内容编码器、自定义音频编码器和基于流的自定义转换器。该模型解决了现有SVS模型过度依赖音素和音符边界注释的问题，实现了零样本场景下的流畅过渡和基于不同提示的多级风格控制。实验结果表明，TCSinger 2在多项任务中均优于基准模型。相关歌唱语音样本可在线体验。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.TCSinger 2是一个多语种零样本歌唱语音合成模型，用于音乐创作和短视频配音。</p>
<p>2.现有SVS模型存在过度依赖音素和音符边界注释的问题，导致零样本场景下的性能受限和音素与音符间的过渡不流畅。</p>
<p>3.TCSinger 2通过模糊边界内容编码器解决了这一问题，实现了平滑过渡。</p>
<p>4.该模型包含自定义音频编码器和基于流的自定义转换器，以提高合成质量和风格建模。</p>
<p>5.通过对比实验，TCSinger 2在多项任务中表现出优于基准模型的效果。</p>
<p>6.该模型支持多语种，具有风格转移和风格控制功能，基于各种提示实现多级控制。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14910">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7e7bbead0ddbc8e41cb84b96b439db84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e75c65f84e442410c70f3d18dd7f86b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99887b38acacc46968bc74cb19d1cd6e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e608cb554b61d11570a3fefe79d414fd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>由于数据稀缺，特别是在非英语领域，针对言语障碍者的自动语音识别（ASR）仍然是一个挑战。为了解决这一问题，我们对英语言语障碍语音（UASpeech）进行微调，以编码说话人的特征和韵律扭曲，然后将其应用于将健康的非英语语音（FLEURS）转换为非英语类的言语障碍语音。生成的数据随后被用于微调用于改进言语障碍语音识别的多语言自动语音识别模型——大规模多语言语音（MMS）。在PC-GITA（西班牙语）、EasyCall（意大利语）和SSNCE（泰米尔语）上的评估表明，同时转换说话人和韵律的语音转换技术显著优于现成的MMS性能和传统的增强技术，如速度和节奏扰动。对生成数据的客观和主观分析进一步证实，生成的语音模拟了言语障碍的特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v3">PDF</a> 5 pages, 1 figure, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>这篇文本介绍了针对发音障碍语音的自动语音识别（ASR）的挑战，并提出了一种解决方案。通过使用英语发音障碍语音（UASpeech）对语音转换模型进行微调，以编码说话人的特征和韵律扭曲。然后将其应用于将健康的非英语语音（FLEURS）转换为非英语发音障碍类似的语音。生成的数据用于微调多语言ASR模型Massively Multilingual Speech（MMS），以提高对发音障碍语音的识别能力。在PC-GITA（西班牙语）、EasyCall（意大利语）和SSNCE（泰米尔语）上的评估表明，同时实现说话人和韵律转换的语音转换技术显著优于现成的MMS性能和传统的增强技术，如速度和节奏扰动。对生成数据的客观和主观分析进一步证实，生成的语音模拟了发音障碍的特征。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据稀缺使得发音障碍语音的自动语音识别（ASR）具有挑战性。</li>
<li>提出了一种基于英语发音障碍语音（UASpeech）的语音转换模型微调方法，以编码说话人的特征和韵律扭曲。</li>
<li>使用该模型将健康的非英语语音转换为非英语发音障碍类似的语音，生成的数据用于训练ASR模型。</li>
<li>评估显示，这种语音转换技术在多种语言上的性能优于传统的ASR模型和增强技术。</li>
<li>该技术结合说话人和韵律转换，能更真实地模拟发音障碍的特征。</li>
<li>客观和主观分析证实了生成语音的逼真度和模拟发音障碍的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-95762dbc7935311d3ab99f1d0517fe87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851cefed71dd03055dee202aa3b93374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bff05fd4865b97d0af86cd3b7475e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569360d203bc2186acb89f7d6ccd725e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach"><a href="#Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach" class="headerlink" title="Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"></a>Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</h2><p><strong>Authors:Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 28% with less than a 2% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 4.6% improvement in fairness metrics with a drop of less than 3.6% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential when explicit demographic information is unavailable. </p>
<blockquote>
<p>虽然子群体差异和性能偏见在计算研究中得到了越来越多的研究，但在分类语音情感识别（SER）中的公平性仍然被忽视。现有方法通常依赖于难以获得的明确人口统计标签（由于隐私担忧）。为了解决这一局限性，我们引入了一个隐式人口统计推断（IDI）模块，该模块利用预训练模型的伪标签和k-means聚类进行无监督学习，以减轻SER中的偏见。我们的实验表明，伪标签IDI减少了子群体差异，公平度指标提高了28%以上，而SER准确率下降了不到2%。此外，无监督的IDI在公平指标上提高了4.6%以上，而SER性能下降了不到3.6%。进一步的分析表明，无监督的IDI始终能够减轻种族和年龄差异，在缺乏明确人口统计信息的情况下显示出其潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14449v3">PDF</a> Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix</p>
<p><strong>Summary</strong></p>
<p>该文探讨了语音情感识别（SER）中的类别公平性问题，并指出现有方法依赖难以获取的明确人口统计标签的问题。为此，引入了隐式人口统计推断（IDI）模块，利用预训练模型的伪标签和K-means聚类进行无监督学习，以减轻SER中的偏见。实验表明，伪标签IDI减少了子群差异，公平度指标提高了28%以上，而SER准确率仅下降不到2%。此外，无监督的IDI在公平度指标上提高了4.6%以上，而SER性能仅下降不到3.6%。进一步的分析表明，无监督的IDI可以持续缓解种族和年龄差异，展示出了当没有明确的人口统计信息时，其潜在的应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）中的公平性问题日益受到关注，但仍然存在对类别公平性的研究不足。</li>
<li>现有方法依赖于难以获取的人口统计标签，引入隐式人口统计推断（IDI）模块来解决这一问题。</li>
<li>IDI模块利用预训练模型的伪标签进行工作，通过伪标签减少子群差异。</li>
<li>实验结果显示，伪标签IDI能显著提高公平性指标，同时保持较高的SER准确率。</li>
<li>无监督的IDI通过K-means聚类进行无监督学习，能在没有明确的人口统计信息时应用。</li>
<li>无监督IDI能持续缓解种族和年龄差异，提高公平度指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f55bf042f5634129bd8baec5c84fd98e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dace8852cb555f7b53f22c6c92bfd9bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae5faed4372daf6ae06502ac3760eb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbcbef08d692c08c629765fa04714f42.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Spatiotemporal-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment"><a href="#Spatiotemporal-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment" class="headerlink" title="Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of   Speech Conditions in Facial and Vocal Affective Alignment"></a>Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of   Speech Conditions in Facial and Vocal Affective Alignment</h2><p><strong>Authors:Von Ralph Dane Marquez Herbuela, Yukie Nagai</strong></p>
<p>Understanding how humans express and synchronize emotions across multiple communication channels particularly facial expressions and speech has significant implications for emotion recognition systems and human computer interaction. Motivated by the notion that non-overlapping speech promotes clearer emotional coordination, while overlapping speech disrupts synchrony, this study examines how these conversational dynamics shape the spatial and temporal alignment of arousal and valence across facial and vocal modalities. Using dyadic interactions from the IEMOCAP dataset, we extracted continuous emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech audio). Segments were categorized based on speech overlap, and emotional alignment was assessed using Pearson correlation, lag adjusted analysis, and Dynamic Time Warping (DTW). Across analyses, non overlapping speech was associated with more stable and predictable emotional synchrony than overlapping speech. While zero-lag correlations were low and not statistically different, non overlapping speech showed reduced variability, especially for arousal. Lag adjusted correlations and best-lag distributions revealed clearer, more consistent temporal alignment in these segments. In contrast, overlapping speech exhibited higher variability and flatter lag profiles, though DTW indicated unexpectedly tighter alignment suggesting distinct coordination strategies. Notably, directionality patterns showed that facial expressions more often preceded speech during turn-taking, while speech led during simultaneous vocalizations. These findings underscore the importance of conversational structure in regulating emotional communication and provide new insight into the spatial and temporal dynamics of multimodal affective alignment in real world interaction. </p>
<blockquote>
<p>理解人类如何在多个沟通渠道（尤其是面部表情和言语）上表达和同步情绪，对于情绪识别系统和人机交互有着重大启示。本研究受到非重叠性言语能够促进更清晰情感协调的观念的启发，同时重叠性言语会破坏同步性。本研究探讨了这些对话动力如何影响面部和声音模态的兴奋和价值的空间和时间对齐。我们使用IEMOCAP数据集中的二元交互数据，通过EmoNet（面部视频）和基于Wav2Vec2的模型（语音音频）提取连续的情绪估计。根据语音重叠对片段进行分类，并使用Pearson相关性、滞后调整分析和动态时间规整（DTW）来评估情绪对齐。分析发现，非重叠性言语与更稳定和可预测的情感同步相关，而重叠性言语则表现出更高的可变性和更平坦的滞后分布。然而，DTW意外地显示出更紧密的对齐，这表明了不同的协调策略。值得注意的是，方向性模式显示，在轮流发言时，面部表情往往先于言语，而在同时发声时，则是言语领先。这些发现强调了对话结构在调节情感沟通中的重要性，并为现实互动中多模态情感对齐的空间和时间动态提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13455v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了人类在多通道沟通中如何表达和同步情绪，特别是在面部表情和言语方面。研究指出，非重叠的语音有助于更清晰的情感协调，而重叠的语音会破坏同步性。通过对IEMOCAP数据集的双人互动进行研究，分析不同沟通方式对情感同步的影响，发现非重叠语音相较于重叠语音能带来更稳定和可预测的情感同步。研究揭示了面部表情和言语在情感沟通中的重要性，并为多模态情感同步的时空动态提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类在多通道沟通中表达和同步情绪具有重要的研究意义，特别是在面部表情和言语方面。</li>
<li>非重叠的语音有助于更清晰的情感协调，而重叠的语音可能破坏情感同步。</li>
<li>通过IEMOCAP数据集的研究发现，非重叠语音带来更稳定和可预测的情感同步表现。</li>
<li>情感同步的分析方法包括皮尔逊相关系数、滞后调整分析和动态时间弯曲。</li>
<li>非重叠语音在情绪同步方面表现出较低的可变性，特别是兴奋度方面。</li>
<li>面部表情在转向时更常先于言语，而同时发声时则相反。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-959c46630bb56151d2caf7194fb47281.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e7bcee293262680c3989cd14efa2f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b228e76e9667901a275578b2c85402f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83e8a90d1094ed1ce546ebb9ca3e8b82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-136d687e719e04ac96145e56f44406b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e911bdd5f8e34cc01d36417fe65e7ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eea2203a811254a8e8f61c60811ce882.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ISDrama-Immersive-Spatial-Drama-Generation-through-Multimodal-Prompting"><a href="#ISDrama-Immersive-Spatial-Drama-Generation-through-Multimodal-Prompting" class="headerlink" title="ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting"></a>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao</strong></p>
<p>Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/ISDramaDemo">https://aaronz345.github.io/ISDramaDemo</a>. </p>
<blockquote>
<p>多模态沉浸式空间戏剧生成专注于基于多模态提示创建具有戏剧语调的连续多扬声器双耳语音，在AR、VR等领域具有潜在应用。这项任务需要基于多模态输入对空间信息和戏剧语调进行同时建模，并且数据采集成本高昂。据我们所知，我们的工作是首次尝试解决这些挑战。我们构建了MRSDrama，即首个多模态记录的空间戏剧数据集，包含双耳戏剧音频、剧本、视频、几何姿势和文本提示。然后，我们提出了ISDrama，即通过多模态提示进行沉浸式空间戏剧生成的首个模型。ISDrama主要包含以下主要组件：1）基于对比学习的多模态姿势编码器，考虑移动扬声器产生的多普勒效应，从多模态提示中提取统一姿势信息。2）沉浸式戏剧转换器，一个基于流的mamba-transformer模型，用于生成高质量戏剧，并结合戏剧MOE选择适当的专家以增强语调和姿势控制。我们还设计了一种上下文一致的无监督分类策略，以连贯地生成完整的戏剧。实验结果表明，ISDrama在客观和主观指标上均优于基准模型。相关演示和数据集可通过<a target="_blank" rel="noopener" href="https://aaronz345.github.io/ISDramaDemo%E8%AE%BF%E9%97%AE%E3%80%82">https://aaronz345.github.io/ISDramaDemo访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态沉浸式空间戏剧生成的研究，重点创建基于多模态提示的连续多讲者双耳戏剧语音。该研究构建了MRSDrama数据集，并提出ISDrama模型，通过多模态提示进行沉浸式空间戏剧生成。该模型包括多模态姿态编码器和沉浸式戏剧转换器，能生成高质量戏剧并控制语调和姿态。实验结果显示，ISDrama在客观和主观指标上优于基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态沉浸式空间戏剧生成集中于创建基于多模态提示的连续多讲者双耳戏剧语音，可应用于AR、VR等领域。</li>
<li>研究面临同时建模空间信息和戏剧语调的挑战，且数据采集成本高昂。</li>
<li>构建了MRSDrama数据集，包含双耳戏剧音频、剧本、视频、几何姿态和文本提示。</li>
<li>提出ISDrama模型，包括多模态姿态编码器和沉浸式戏剧转换器。</li>
<li>多模态姿态编码器采用对比学习，考虑移动讲者带来的多普勒效应，从多模态提示中提取统一姿态信息。</li>
<li>沉浸式戏剧转换器基于流式的mamba-transformer模型，生成高质量戏剧，并引入Drama-MOE选择适当的专家以增强语调和姿态控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7d0dde82d91263e54fa13503c0933f8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1790c3fd740147bdb039a455784460df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6899f8d47230be9d8e7f5d844befd36c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2317f687c69202ee624115138579b192.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph"><a href="#Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph" class="headerlink" title="Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph"></a>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph</h2><p><strong>Authors:Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li</strong></p>
<p>The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox">https://github.com/YiboZhao624/MetaTox</a>. </p>
<blockquote>
<p>社交媒体平台的快速发展引发了人们对网络内容毒性的重大关注。当使用大型语言模型（LLM）进行毒性检测时，会出现两个关键挑战：1）缺乏特定领域的毒性知识会导致假阴性结果；2）LLM对有毒言论过于敏感，导致假阳性结果，限制言论自由。为了解决这些问题，我们提出了一种新方法，称为MetaTox，利用元毒性知识图上的图搜索来增强仇恨和毒性检测。首先，我们通过一个三步骤的管道，利用LLM提取毒性信息，以毒性基准数据集作为语料库，构建了一个全面的元毒性知识图。其次，我们通过检索和排名过程查询图形，以补充准确且相关的毒性知识。在多个数据集上进行的大量实验和深入的案例研究结果表明，我们的MetaTox方法显著降低了假阳性率，同时提高了总体毒性检测性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YiboZhao624/MetaTox获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15268v3">PDF</a> 8 pages of content</p>
<p><strong>Summary</strong></p>
<p>社交媒体平台的快速发展引发了人们对网络内容毒性的关注。使用大型语言模型（LLM）进行毒性检测时面临两大挑战：缺乏特定领域的毒性知识和过度敏感于毒性言论导致误报。为解决这些问题，提出了一种名为MetaTox的新方法，利用元毒性知识图谱上的图搜索来增强仇恨和毒性检测。通过构建全面的元毒性知识图谱，通过LLM提取毒性信息，并利用毒性基准数据集作为语料库进行查询和排名过程，以补充准确、相关的毒性知识。实验和案例研究表明，MetaTox显著降低了误报率，提高了整体毒性检测性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>社交媒体平台的快速发展引发了关于网络内容毒性的关注。</li>
<li>使用大型语言模型（LLM）进行毒性检测面临两大挑战：缺乏特定领域的毒性知识和过度敏感导致误报。</li>
<li>提出了一种名为MetaTox的新方法来解决这些问题，该方法利用元毒性知识图谱上的图搜索来增强仇恨和毒性检测。</li>
<li>MetaTox通过构建全面的元毒性知识图谱，利用LLM提取毒性信息。</li>
<li>MetaTox使用毒性基准数据集作为语料库进行查询和排名过程，以补充准确、相关的毒性知识。</li>
<li>MetaTox显著降低了误报率，提高了整体毒性检测性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15268">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5561d951e9b963a721e630ce6153036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4117e7f489848b54039de48bf5f165d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505ad0de96990783c6cb239a97f90499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30f192e2c185a64a742a9bfec37da5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control"><a href="#TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control" class="headerlink" title="TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control"></a>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control</h2><p><strong>Authors:Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao</strong></p>
<p>Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S&amp;D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer. Singing voice samples can be accessed at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/">https://aaronz345.github.io/TCSingerDemo/</a>. </p>
<blockquote>
<p>零样本唱腔合成（SVS）带有风格转换和风格控制功能，旨在从音频和文字提示生成具有未见音质和风格的高质量唱腔（包括唱法、情感、节奏、技巧和发音）。然而，唱腔风格的多面性给有效的建模、转换和控制带来了重大挑战。此外，当前的SVS模型往往不能为未见过的歌手生成富有风格细微差别的唱腔。为了应对这些挑战，我们引入了TCSinger，这是第一个用于跨语言语音和唱腔风格风格转换的零样本SVS模型，以及多级风格控制。具体来说，TCSinger提出了三个主要模块：1）聚类风格编码器采用聚类向量量化模型，将风格信息稳定地凝聚成一个紧凑的潜在空间；2）风格和持续时间语言模型（S&amp;D-LM）同时预测风格信息和音素持续时间，两者都受益；3）风格自适应解码器采用一种新的mel-style自适应归一化方法，生成具有增强细节的唱腔。实验结果表明，TCSinger在合成质量、歌手相似度和风格可控性方面优于所有基准模型，包括零样本风格转换、多级风格控制、跨语言风格转换和语音到唱腔风格转换等各项任务。唱腔样本可访问<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/%E3%80%82">https://aaronz345.github.io/TCSingerDemo/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15977v6">PDF</a> Accepted by EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>该研究介绍了针对跨语言语音和歌唱风格进行零样本歌唱声音合成（SVS）的风格转移和多层次风格控制的模型TCSinger。它通过三个主要模块实现：聚类风格编码器、风格与时长语言模型以及风格自适应解码器。该模型在合成质量、歌手相似性和风格可控性方面均优于基线模型，并实现了零样本风格转移、多层次风格控制、跨语言风格转移和语音到歌唱风格转移等任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TCSinger是一个用于风格转移的零样本歌唱声音合成（SVS）模型，能够生成具有未见过的音质和风格的歌唱声音。</li>
<li>TCSinger通过三个主要模块实现：聚类风格编码器、风格与时长语言模型和风格自适应解码器。</li>
<li>聚类风格编码器使用聚类向量量化模型，将风格信息稳定地浓缩到紧凑的潜在空间中。</li>
<li>风格与时长语言模型同时预测风格信息和音素时长，对两者都有利。</li>
<li>风格自适应解码器采用新的mel-style自适应归一化方法，生成具有增强细节的歌唱声音。</li>
<li>实验结果表明，TCSinger在合成质量、歌手相似性和风格可控性方面均优于基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c34f2e70d85806fd10ebbab3b740b302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a3202a7091e1b4c968475e48fc1ace2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb2a5d55364652a98ef3c9cb82f2e29.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks"><a href="#GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks" class="headerlink" title="GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks"></a>GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks</h2><p><strong>Authors:Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao</strong></p>
<p>The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at <a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/">http://aaronz345.github.io/GTSingerDemo/</a>. We provide the dataset and the code for processing data and conducting benchmarks at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AaronZ345/GTSinger">https://huggingface.co/datasets/AaronZ345/GTSinger</a> and <a target="_blank" rel="noopener" href="https://github.com/AaronZ345/GTSinger">https://github.com/AaronZ345/GTSinger</a>. </p>
<blockquote>
<p>高质量多任务演唱数据集稀缺的问题显著阻碍了多样可控和个性化演唱任务的发展，因为现有的演唱数据集存在质量低、语言及歌手多样性有限、缺乏多技术信息和现实音乐乐谱以及任务适用性差等问题。为了解决这些问题，我们推出了GTSinger，这是一个大型全球、多技术、免费使用的、高质量演唱语料库，配有现实音乐乐谱，适用于所有演唱任务，以及相应的基准测试。具体来说，(1)我们收集了80.59小时的高质量演唱声音，形成了最大的录音演唱数据集；(2)20位专业歌手跨越九种广泛使用的语言，展现了多样的音色和风格；(3)我们提供了六种常用演唱技术的受控比较和音素级注释，有助于技术建模和控制；(4)GTSinger提供了现实音乐乐谱，有助于现实音乐作曲；(5)演唱声音配有手动音素到音频对齐、全局风格标签，以及用于各种演唱任务的16.16小时配套语音。此外，为了方便使用GTSinger，我们进行了四项基准实验：技术可控的演唱声音合成、技术识别、风格转换和语音到演唱的转换。语料库和演示可在<a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/%E6%89%BE%E5%88%B0%E3%80%82%E6%88%91%E4%BB%AC%E5%9C%A8https://huggingface.co/datasets/AaronZ345/GTSinger%E5%92%8Chttps://github.com/AaronZ345/GTSinger%E6%8F%90%E4%BE%9B%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">http://aaronz345.github.io/GTSingerDemo/找到。我们在https://huggingface.co/datasets/AaronZ345/GTSinger和https://github.com/AaronZ345/GTSinger提供了数据集和数据处理及基准测试的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13832v7">PDF</a> Accepted by NeurIPS 2024 (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了GTSinger这一全球大型、多技巧、免费使用的歌唱语料库。该语料库具有高质量、多语言、包含多种演唱技巧与真实乐谱等特点，适用于各种歌唱任务。为解决高质量多任务歌唱数据集稀缺的问题，GTSinger提供了超过80小时的歌唱声音数据，涵盖了多种风格和技巧，并配有音乐乐谱和音频对齐等技术支持。此外，本文还介绍了针对GTSinger进行的四项基准实验，包括可控演唱声音合成、技巧识别、风格转换和语音转唱等。相关数据和代码可在指定网站下载使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GTSinger是一个全球性的大型歌唱语料库，为解决高质量多任务歌唱数据集稀缺的问题而设计。</li>
<li>GTSinger包含超过80小时的高质量歌唱声音数据，是迄今为止最大的录音歌唱数据集。</li>
<li>GTSinger涵盖了九种广泛使用的语言和多种演唱风格，提供多样化的音色和风格。</li>
<li>GTSinger提供了六种常用演唱技巧的对比分析和注释，支持技巧建模和控制。</li>
<li>GTSinger提供了真实的音乐乐谱和音频对齐技术，助力音乐创作和研究。</li>
<li>GTSinger包括技术可控的歌唱声音合成、技巧识别、风格转换和语音转唱等基准实验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-333000518bb4fe8b296a8865370abfbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0a829871655320f45719954820b9f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f36701794ff88abe07da6e61077bca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad95cdd657d117546409a35f0b263aff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0f1a2777fc489b76f96c568fb6e77be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b8c04cc16af9c2620048c99092a7307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d630db24fb3cff05a06b79fc3242dfbe.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9aa18e960c10d0cefd2f6bf80ee89dab.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-06-03  AdaHuman Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cc5f599d4203924725e20121808d44ef.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-03  Contrast-Invariant Self-supervised Segmentation for Quantitative   Placental MRI
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
