<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  AdaHuman Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-18862ca7cb0f1725e215a9b917dda0b5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="AdaHuman-Animatable-Detailed-3D-Human-Generation-with-Compositional-Multiview-Diffusion"><a href="#AdaHuman-Animatable-Detailed-3D-Human-Generation-with-Compositional-Multiview-Diffusion" class="headerlink" title="AdaHuman: Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion"></a>AdaHuman: Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion</h2><p><strong>Authors:Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal</strong></p>
<p>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes. </p>
<blockquote>
<p>å½“å‰ç”¨äºå›¾åƒåˆ°3DåŒ–èº«ç”Ÿæˆçš„æ–¹æ³•éš¾ä»¥ç”Ÿæˆé€‚ç”¨äºçœŸå®ä¸–ç•Œåº”ç”¨çš„é«˜åº¦è¯¦ç»†çš„åŠ¨ç”»åŒ–èº«ã€‚æˆ‘ä»¬å¼•å…¥äº†AdaHumanï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ é‡å¤–å›¾åƒç”Ÿæˆé«˜åº¦é€¼çœŸçš„åŠ¨ç”»3DåŒ–èº«ã€‚AdaHumanæœ‰ä¸¤ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å§¿æ€è°ƒèŠ‚çš„3Då…³èŠ‚æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä»»æ„å§¿æ€ä¸‹åˆæˆä¸€è‡´çš„å¤šè§†è§’å›¾åƒï¼Œå¹¶åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­è¿›è¡Œç›¸åº”çš„3Dé«˜æ–¯Splatsï¼ˆ3DGSï¼‰é‡å»ºï¼›ï¼ˆ2ï¼‰ç»„åˆå¼3DGSä¼˜åŒ–æ¨¡å—ï¼Œé€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç»†åŒ–å¢å¼ºå±€éƒ¨èº«ä½“éƒ¨ä½çš„ç»†èŠ‚ï¼Œå¹¶ä½¿ç”¨æ–°å‹è£å‰ªæ„ŸçŸ¥ç›¸æœºå°„çº¿å›¾æ— ç¼é›†æˆå®ƒä»¬ï¼Œç”Ÿæˆè¿è´¯çš„è¯¦ç»†3DåŒ–èº«ã€‚è¿™äº›ç»„ä»¶ä½¿å¾—AdaHumanèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„æ ‡å‡†åŒ–Aå§¿æ€åŒ–èº«ï¼Œæœ€å°é™åº¦åœ°å®ç°è‡ªæˆ‘é®æŒ¡ï¼Œå¹¶èƒ½å¤Ÿå®ç°ä¸ä»»ä½•è¾“å…¥åŠ¨ä½œè¿›è¡Œç»‘å®šå’ŒåŠ¨ç”»ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•å’Œé‡å¤–å›¾åƒä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAdaHumanåœ¨åŒ–èº«é‡å»ºå’Œé‡æ–°å®šä½æ–¹é¢å‡æ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚ä¸ºäº†ç ”ç©¶ç›®çš„ï¼Œæˆ‘ä»¬å°†å…¬å¼€æä¾›ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24877v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://nvlabs.github.io/AdaHuman">https://nvlabs.github.io/AdaHuman</a></p>
<p><strong>Summary</strong></p>
<p>AdaHumanæ˜¯ä¸€ä¸ªä»å•å¼ å›¾åƒç”Ÿæˆé«˜ä¿çœŸåŠ¨ç”»3Dè§’è‰²çš„æ–°æ¡†æ¶ã€‚å®ƒé‡‡ç”¨å§¿æ€æ¡ä»¶åŒ–ä¸‰ç»´å…³èŠ‚æ‰©æ•£æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯æ’å€¼é‡æ„æŠ€æœ¯ï¼Œé€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç»†åŒ–è¿‡ç¨‹ï¼Œæ— ç¼é›†æˆå„éƒ¨åˆ†ç»†èŠ‚ï¼Œç”Ÿæˆè¿è´¯ä¸”è¯¦ç»†çš„3Dè§’è‰²ã€‚AdaHumanèƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„æ ‡å‡†åŒ–Aå§¿æ€è§’è‰²ï¼Œæœ€å°é™åº¦åœ°å®ç°è‡ªæˆ‘é®æŒ¡ï¼Œå¹¶èƒ½ä¸ä»»ä½•è¾“å…¥åŠ¨ä½œè¿›è¡ŒåŒ¹é…å’ŒåŠ¨ç”»åŒ–ã€‚å…¶åœ¨å…¬å…±åŸºå‡†æµ‹è¯•å’ŒçœŸå®å›¾åƒä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå½“å‰çš„ä¸»æµæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaHumanä½¿ç”¨æ–°é¢–çš„æ¡†æ¶ä»å•å¼ å›¾åƒç”ŸæˆåŠ¨ç”»å¼3Dè§’è‰²ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«å§¿æ€æ¡ä»¶åŒ–ä¸‰ç»´å…³èŠ‚æ‰©æ•£æ¨¡å‹ï¼Œèƒ½åˆæˆä»»æ„å§¿æ€ä¸‹çš„å¤šè§†è§’å›¾åƒã€‚</li>
<li>AdaHumanåˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ’å€¼ï¼ˆ3DGSï¼‰è¿›è¡Œæ¯ä¸€æ­¥çš„æ‰©æ•£é‡æ„ã€‚</li>
<li>å¼•å…¥äº†å›¾åƒåˆ°å›¾åƒçš„ç»†åŒ–æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œå¹¶é‡‡ç”¨åˆ›æ–°çš„å†œä½œç‰©æ„ŸçŸ¥ç›¸æœºå°„çº¿æ˜ å°„æ— ç¼é›†æˆè¿™äº›ç»†èŠ‚ã€‚</li>
<li>AdaHumanå¯ä»¥ç”Ÿæˆæ ‡å‡†åŒ–çš„é«˜åº¦é€¼çœŸçš„Aå§¿æ€è§’è‰²ï¼Œæœ€å°åŒ–è‡ªæˆ‘é®æŒ¡ã€‚</li>
<li>AdaHumançš„è§’è‰²æ˜“äºè£…å¤‡å’ŒåŠ¨ç”»åŒ–ï¼Œèƒ½ä¸ä»»ä½•è¾“å…¥åŠ¨ä½œåŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ab9ddfefee8cd2d8f36acdf2f9bf0d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a77b746de3d7652c2ae2cfe485133f78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-170ee6183a05b1cc549d259f9390e0d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf7f1b7c9fe3bc0f4ae482d4ece2594.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning"><a href="#Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning" class="headerlink" title="Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning"></a>Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning</h2><p><strong>Authors:Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</strong></p>
<p>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs. </p>
<blockquote>
<p>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ–°æ–¹æ³•ï¼Œç”¨äºåˆ†è§£è¯­è¨€æ¨¡å‹çš„æ¿€æ´»è¿›è¡Œè§£é‡Šå’Œæ§åˆ¶ã€‚å®ƒä»¬å·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å™¨å›¾åƒç¼–ç å™¨å’Œå°å‹æ‰©æ•£æ¨¡å‹ã€‚æ¿€æ´»æ¨ç†æ—¶é—´åˆ†è§£ï¼ˆITDAï¼‰æ˜¯å­—å…¸å­¦ä¹ çš„ä¸€ä¸ªæ–°è¿‘æå‡ºçš„å˜ä½“ï¼Œå®ƒå°†å­—å…¸è§†ä¸ºæ¿€æ´»åˆ†å¸ƒä¸­çš„æ•°æ®ç‚¹é›†ï¼Œå¹¶é€šè¿‡æ¢¯åº¦è¿½æ±‚è¿›è¡Œé‡å»ºã€‚æˆ‘ä»¬å°†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å’ŒITDAåº”ç”¨äºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹Flux 1ï¼Œå¹¶é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“è€ƒè™‘ä¸¤è€…çš„åµŒå…¥è§£é‡Šæ€§ã€‚æˆ‘ä»¬å‘ç°SAEèƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œåœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨SAEç‰¹å¾é€šè¿‡æ¿€æ´»æ·»åŠ æ¥å¼•å¯¼å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬å‘ç°ITDAçš„è§£é‡Šæ€§ä¸SAEç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24360v1">PDF</a> 10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR   2025</p>
<p><strong>Summary</strong></p>
<p>ç¨€ç–è‡ªç¼–ç å™¨æ˜¯åˆ†è§£è¯­è¨€æ¨¡å‹æ¿€æ´»ä»¥è¿›è¡Œè§£é‡Šå’Œæ§åˆ¶çš„ä¸€ç§æœ‰å‰é€”çš„æ–°æ–¹æ³•ã€‚å·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å™¨å›¾åƒç¼–ç å™¨å’Œå°å‹æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹Flux 1ï¼Œå¹¶é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“æ¥è€ƒè™‘ä¸¤è€…çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬å‘ç°ç¨€ç–è‡ªç¼–ç å™¨èƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œå¹¶åœ¨å¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚æˆ‘ä»¬è¿˜èƒ½å¤Ÿä½¿ç”¨SAEç‰¹å¾é€šè¿‡æ¿€æ´»æ·»åŠ æ¥å¼•å¯¼å›¾åƒç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨åˆ†è§£è¯­è¨€æ¨¡å‹æ¿€æ´»æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>SAEèƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œä¼˜äºMLPç¥ç»å…ƒåœ¨è§£é‡ŠåµŒå…¥æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨SAEç‰¹å¾å¯ä»¥åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæ¿€æ´»æ·»åŠ ï¼Œå®ç°å¯¹å›¾åƒç”Ÿæˆçš„å¼•å¯¼ã€‚</li>
<li>ITDAï¼ˆæ¨ç†æ—¶é—´æ¿€æ´»åˆ†è§£ï¼‰åœ¨å¯è§£é‡Šæ€§ä¸SAEç›¸å½“ã€‚</li>
<li>SAEå’ŒITDAçš„åº”ç”¨ä¸ºè¯­è¨€æ¨¡å‹çš„ç†è§£å’Œæ§åˆ¶åœ¨è§£é‡Šæ€§å’Œæ§åˆ¶æ–¹é¢æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1c22696a928956243b2bbc08ee7e6ba3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5af5085d406ff9dd99f7a78d6879004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff2737bf419a0bcca28f1acc438e1a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-464854384adbd7779b5f8aa7619f4926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e45deecaa551bc541ad2a68085ab6fe3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46be4dcec21f8eb73a7ad696931c864f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InteractAnything-Zero-shot-Human-Object-Interaction-Synthesis-via-LLM-Feedback-and-Object-Affordance-Parsing"><a href="#InteractAnything-Zero-shot-Human-Object-Interaction-Synthesis-via-LLM-Feedback-and-Object-Affordance-Parsing" class="headerlink" title="InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM   Feedback and Object Affordance Parsing"></a>InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM   Feedback and Object Affordance Parsing</h2><p><strong>Authors:Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang</strong></p>
<p>Recent advances in 3D human-aware generation have made significant progress. However, existing methods still struggle with generating novel Human Object Interaction (HOI) from text, particularly for open-set objects. We identify three main challenges of this task: precise human-object relation reasoning, affordance parsing for any object, and detailed human interaction pose synthesis aligning description and object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework without training on specific datasets, leveraging the knowledge from large-scale pre-trained models. Specifically, the human-object relations are inferred from large language models (LLMs) to initialize object properties and guide the optimization process. Then we utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. The initial human pose is generated by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. Finally, we introduce a detailed optimization to generate fine-grained, precise, and natural interaction, enforcing realistic 3D contact between the 3D object and the involved body parts, including hands in grasping. This is achieved by distilling human-level feedback from LLMs to capture detailed human-object relations from the text instruction. Extensive experiments validate the effectiveness of our approach compared to prior works, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects. </p>
<blockquote>
<p>å…³äºä¸‰ç»´äººå½¢æ„ŸçŸ¥ç”Ÿæˆé¢†åŸŸæœ€è¿‘å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»éš¾ä»¥æ ¹æ®æ–‡æœ¬ç”Ÿæˆæ–°å‹çš„äºº-ç‰©äº¤äº’ï¼ˆHOIï¼‰ï¼Œå°¤å…¶æ˜¯å¼€æ”¾é›†ç‰©ä½“ã€‚æˆ‘ä»¬ç¡®å®šäº†è¯¥ä»»åŠ¡é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç²¾å‡†çš„äºº-ç‰©å…³ç³»æ¨ç†ã€ä»»æ„å¯¹è±¡çš„å¯æ“æ§æ€§è§£æä»¥åŠç¬¦åˆæè¿°å’Œç‰©ä½“å‡ ä½•ç‰¹æ€§çš„è¯¦ç»†äººç±»äº¤äº’å§¿æ€åˆæˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬ä¸‰ç»´HOIç”Ÿæˆæ¡†æ¶ï¼Œæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯å€ŸåŠ©å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨æ–­å‡ºäºº-ç‰©å…³ç³»ï¼Œä»¥åˆå§‹åŒ–ç‰©ä½“å±æ€§å¹¶å¼•å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹æ¥åˆ†ææœªè§è¿‡çš„ç‰©ä½“å¹¶æå–æ¥è§¦ç‚¹ï¼Œé¿å…äº†ç°æœ‰ä¸‰ç»´èµ„äº§çŸ¥è¯†çš„é™åˆ¶ã€‚åŸºäºè¾“å…¥æ–‡æœ¬å’Œç‰©ä½“å‡ ä½•ç‰¹æ€§ï¼Œé€šè¿‡å¤šè§†è§’SDSç”Ÿæˆåˆå§‹äººç±»å§¿æ€ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„ä¼˜åŒ–ï¼Œä»¥ç”Ÿæˆç²¾ç»†ã€ç²¾å‡†ã€è‡ªç„¶çš„äº¤äº’ï¼Œå¼ºåˆ¶è¦æ±‚ä¸‰ç»´ç‰©ä½“ä¸æ‰€æ¶‰åŠèº«ä½“éƒ¨ä½ä¹‹é—´çš„ç°å®æ¥è§¦ï¼ŒåŒ…æ‹¬æŠ“æ¡æ—¶çš„æ‰‹éƒ¨ã€‚è¿™æ˜¯é€šè¿‡ä»LLMä¸­æå–äººç±»åé¦ˆæ¥å®ç°ä»æ–‡æœ¬æŒ‡ä»¤ä¸­æ•è·è¯¦ç»†çš„äºº-ç‰©å…³ç³»ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸å…ˆå‰å·¥ä½œç›¸æ¯”çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤äº’çš„ç²¾ç»†æ€§è´¨å’Œå¤„ç†å¼€æ”¾é›†ä¸‰ç»´ç‰©ä½“çš„èƒ½åŠ›æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24315v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     è¿‘æœŸä¸‰ç»´äººç±»æ„ŸçŸ¥ç”ŸæˆæŠ€æœ¯å–å¾—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä»æ–‡æœ¬ç”Ÿæˆæ–°å‹äººæœºäº¤äº’ï¼ˆHOIï¼‰çš„éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾é›†å¯¹è±¡ä¸Šã€‚æœ¬æ–‡æå‡ºä¸€ç§é›¶æ ·æœ¬ä¸‰ç»´HOIç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡è¯­è¨€æ¨¡å‹æ¨æ–­äººç±»ä¸å¯¹è±¡çš„å…³ç³»ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹è§£ææœªè§è¿‡çš„å¯¹è±¡å¹¶æå–æ¥è§¦ç‚¹ï¼Œé¿å…ç°æœ‰ä¸‰ç»´èµ„äº§çŸ¥è¯†çš„é™åˆ¶ã€‚é€šè¿‡å¤šè§†è§’SDSç”Ÿæˆåˆå§‹äººç±»å§¿åŠ¿ï¼Œå¹¶å¼•å…¥è¯¦ç»†ä¼˜åŒ–ç”Ÿæˆç²¾ç»†ã€ç²¾ç¡®å’Œè‡ªç„¶äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸä¸‰ç»´äººç±»æ„ŸçŸ¥ç”ŸæˆæŠ€æœ¯æœ‰è¿›å±•ï¼Œä½†ä»é¢ä¸´ä»æ–‡æœ¬ç”Ÿæˆæ–°å‹äººæœºäº¤äº’çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾é›†å¯¹è±¡ä¸Šã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§é›¶æ ·æœ¬ä¸‰ç»´HOIç”Ÿæˆæ¡†æ¶ï¼Œæ— éœ€ç‰¹å®šæ•°æ®é›†è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨è¯­è¨€æ¨¡å‹æ¨æ–­äººç±»ä¸å¯¹è±¡çš„å…³ç³»ï¼Œåˆå§‹åŒ–å¯¹è±¡å±æ€§å¹¶å¼•å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹è§£ææœªè§è¿‡çš„å¯¹è±¡ï¼Œæå–æ¥è§¦ç‚¹ã€‚</li>
<li>é€šè¿‡å¤šè§†è§’SDSç”Ÿæˆåˆå§‹äººç±»å§¿åŠ¿ã€‚</li>
<li>å¼•å…¥è¯¦ç»†ä¼˜åŒ–ç”Ÿæˆç²¾ç»†ã€ç²¾ç¡®å’Œè‡ªç„¶äº¤äº’ï¼Œå®ç°ä¸‰ç»´å¯¹è±¡ä¸èº«ä½“éƒ¨ä½çš„ç°å®æ¥è§¦ï¼ŒåŒ…æ‹¬æ‰‹éƒ¨æŠ“æ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e6ebf99c2e96da34dd93f7fbf957195.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5c012b8b2227a828bfd2a5e9b149185.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-300f7c1cefb2632978a185a8f07fd73f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ea9fddedef7ca01137d6500e32adb6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Category-aware-EEG-image-generation-based-on-wavelet-transform-and-contrast-semantic-loss"><a href="#Category-aware-EEG-image-generation-based-on-wavelet-transform-and-contrast-semantic-loss" class="headerlink" title="Category-aware EEG image generation based on wavelet transform and   contrast semantic loss"></a>Category-aware EEG image generation based on wavelet transform and   contrast semantic loss</h2><p><strong>Authors:Enshang Zhang, Zhicheng Zhang, Takashi Hanakawa</strong></p>
<p>Reconstructing visual stimuli from EEG signals is a crucial step in realizing brain-computer interfaces. In this paper, we propose a transformer-based EEG signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating mechanism. Guided by the feature alignment and category-aware fusion losses, this encoder is used to extract features related to visual stimuli from EEG signals. Subsequently, with the aid of a pre-trained diffusion model, these features are reconstructed into visual stimuli. To verify the effectiveness of the model, we conducted EEG-to-image generation and classification tasks using the THINGS-EEG dataset. To address the limitations of quantitative analysis at the semantic level, we combined WordNet-based classification and semantic similarity metrics to propose a novel semantic-based score, emphasizing the ability of our model to transfer neural activities into visual representations. Experimental results show that our model significantly improves semantic alignment and classification accuracy, which achieves a maximum single-subject accuracy of 43%, outperforming other state-of-the-art methods. The source code and supplementary material is available at <a target="_blank" rel="noopener" href="https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main">https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main</a>. </p>
<blockquote>
<p>ä»è„‘ç”µå›¾ä¿¡å·é‡å»ºè§†è§‰åˆºæ¿€æ˜¯å®ç°è„‘æœºæ¥å£çš„å…³é”®æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨çš„è„‘ç”µå›¾ä¿¡å·ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ç»“åˆäº†ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å’Œé—¨æ§æœºåˆ¶ã€‚åœ¨ç‰¹å¾å¯¹é½å’Œç±»åˆ«æ„ŸçŸ¥èåˆæŸå¤±çš„æŒ‡å¯¼ä¸‹ï¼Œè¯¥ç¼–ç å™¨ç”¨äºä»è„‘ç”µå›¾ä¿¡å·ä¸­æå–ä¸è§†è§‰åˆºæ¿€ç›¸å…³çš„ç‰¹å¾ã€‚éšåï¼Œå€ŸåŠ©é¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›ç‰¹å¾è¢«é‡å»ºä¸ºè§†è§‰åˆºæ¿€ã€‚ä¸ºäº†éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨THINGS-EEGæ•°æ®é›†ä¸Šè¿›è¡Œäº†è„‘ç”µå›¾åˆ°å›¾åƒç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¯­ä¹‰å±‚é¢å®šé‡åˆ†æçš„é™åˆ¶ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºWordNetçš„åˆ†ç±»å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¯­ä¹‰çš„è¯„åˆ†ï¼Œå¼ºè°ƒæˆ‘ä»¬çš„æ¨¡å‹å°†ç¥ç»æ´»åŠ¨è½¬åŒ–ä¸ºè§†è§‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­ä¹‰å¯¹é½å’Œåˆ†ç±»ç²¾åº¦ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ï¼Œå®ç°äº†æœ€é«˜å•ç”¨æˆ·å‡†ç¡®ç‡ä¸º43%ï¼Œè¶…è¿‡äº†å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å’Œè¡¥å……ææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zes0v0inn/DWT_EEG_Reconstruction&#x2F;tree&#x2F;mainæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24301v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè„‘ç”µå›¾ä¿¡å·é‡å»ºè§†è§‰åˆºæ¿€æ˜¯å®ç°è„‘æœºæ¥å£çš„é‡è¦æ­¥éª¤ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»“åˆç¦»æ•£å°æ³¢å˜æ¢å’Œé—¨æ§æœºåˆ¶çš„åŸºäºè½¬æ¢å™¨çš„è„‘ç”µå›¾ä¿¡å·ç¼–ç å™¨ã€‚é€šè¿‡ç‰¹å¾å¯¹é½å’Œç±»åˆ«æ„ŸçŸ¥èåˆæŸå¤±æŒ‡å¯¼ï¼Œè¯¥ç¼–ç å™¨ä»è„‘ç”µå›¾ä¿¡å·ä¸­æå–ä¸è§†è§‰åˆºæ¿€ç›¸å…³çš„ç‰¹å¾ã€‚å€ŸåŠ©é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›ç‰¹å¾è¢«é‡å»ºä¸ºè§†è§‰åˆºæ¿€ã€‚åˆ©ç”¨THINGS-EEGæ•°æ®é›†è¿›è¡Œè„‘ç”µå›¾åˆ°å›¾åƒç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ä¸ºè§£å†³è¯­ä¹‰å±‚é¢å®šé‡åˆ†æçš„é™åˆ¶ï¼Œæœ¬ç ”ç©¶ç»“åˆWordNetåˆ†ç±»å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ï¼Œæå‡ºä¸€ç§æ–°çš„åŸºäºè¯­ä¹‰çš„è¯„åˆ†æ–¹æ³•ï¼Œå¼ºè°ƒæ¨¡å‹å°†ç¥ç»æ´»åŠ¨è½¬åŒ–ä¸ºè§†è§‰è¡¨å¾çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­ä¹‰å¯¹é½å’Œåˆ†ç±»ç²¾åº¦ä¸Šæ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°æœ€é«˜å•ä¸»ä½“å‡†ç¡®ç‡43%ï¼Œä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨å˜å‹å™¨æ¶æ„çš„ç¼–ç å™¨å¤„ç†EEGä¿¡å·ï¼Œç»“åˆç¦»æ•£å°æ³¢å˜æ¢å’Œé—¨æœºåˆ¶ï¼Œä»¥æå–ä¸è§†è§‰åˆºæ¿€ç›¸å…³çš„ç‰¹å¾ã€‚</li>
<li>ç¼–ç å™¨åœ¨ç‰¹å¾å¯¹é½å’Œç±»åˆ«æ„ŸçŸ¥èåˆæŸå¤±çš„æŒ‡å¯¼ä¸‹å·¥ä½œï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å°†æå–çš„ç‰¹å¾é‡å»ºä¸ºè§†è§‰åˆºæ¿€ï¼Œå®ç°è„‘æœºæ¥å£çš„é‡è¦æ­¥éª¤ã€‚</li>
<li>ä½¿ç”¨THINGS-EEGæ•°æ®é›†è¿›è¡ŒEEGåˆ°å›¾åƒç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡æ¥éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ºå…‹æœè¯­ä¹‰å±‚é¢å®šé‡åˆ†æçš„å±€é™æ€§ï¼Œç»“åˆWordNetåˆ†ç±»å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ï¼Œæå‡ºæ–°çš„è¯­ä¹‰è¯„åˆ†æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨è¯­ä¹‰å¯¹é½å’Œåˆ†ç±»ç²¾åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€é«˜å•ä¸»ä½“å‡†ç¡®ç‡43%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aeeded49f0a5239b834e17f48415bc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0dcb6864fef38b43430f488f06abbc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e200d5e44631f512582c7c3aaedc92e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b6017249bbf704e65cb1ec695b904c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-AI-for-Urban-Design-A-Stepwise-Approach-Integrating-Human-Expertise-with-Multimodal-Diffusion-Models"><a href="#Generative-AI-for-Urban-Design-A-Stepwise-Approach-Integrating-Human-Expertise-with-Multimodal-Diffusion-Models" class="headerlink" title="Generative AI for Urban Design: A Stepwise Approach Integrating Human   Expertise with Multimodal Diffusion Models"></a>Generative AI for Urban Design: A Stepwise Approach Integrating Human   Expertise with Multimodal Diffusion Models</h2><p><strong>Authors:Mingyi He, Yuebing Liang, Shenhao Wang, Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Li Tian, Jinhua Zhao</strong></p>
<p>Urban design is a multifaceted process that demands careful consideration of site-specific constraints and collaboration among diverse professionals and stakeholders. The advent of generative artificial intelligence (GenAI) offers transformative potential by improving the efficiency of design generation and facilitating the communication of design ideas. However, most existing approaches are not well integrated with human design workflows. They often follow end-to-end pipelines with limited control, overlooking the iterative nature of real-world design. This study proposes a stepwise generative urban design framework that integrates multimodal diffusion models with human expertise to enable more adaptive and controllable design processes. Instead of generating design outcomes in a single end-to-end process, the framework divides the process into three key stages aligned with established urban design workflows: (1) road network and land use planning, (2) building layout planning, and (3) detailed planning and rendering. At each stage, multimodal diffusion models generate preliminary designs based on textual prompts and image-based constraints, which can then be reviewed and refined by human designers. We design an evaluation framework to assess the fidelity, compliance, and diversity of the generated designs. Experiments using data from Chicago and New York City demonstrate that our framework outperforms baseline models and end-to-end approaches across all three dimensions. This study underscores the benefits of multimodal diffusion models and stepwise generation in preserving human control and facilitating iterative refinements, laying the groundwork for human-AI interaction in urban design solutions. </p>
<blockquote>
<p>åŸå¸‚è®¾è®¡æ˜¯ä¸€ä¸ªæ¶‰åŠå¤šæ–¹é¢çš„è¿‡ç¨‹ï¼Œéœ€è¦ä»”ç»†è€ƒè™‘ç‰¹å®šåœ°ç‚¹çš„é™åˆ¶å› ç´ ä»¥åŠä¸åŒä¸“ä¸šäººå£«å’Œåˆ©ç›Šç›¸å…³è€…ä¹‹é—´çš„åˆä½œã€‚ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„å‡ºç°ï¼Œé€šè¿‡æé«˜è®¾è®¡ç”Ÿæˆçš„æ•ˆç‡å’Œä¿ƒè¿›è®¾è®¡æ€æƒ³çš„äº¤æµï¼Œæä¾›äº†å˜é©çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¹¶æ²¡æœ‰å¾ˆå¥½åœ°ä¸äººç±»è®¾è®¡å·¥ä½œæµç¨‹ç›¸ç»“åˆã€‚å®ƒä»¬é€šå¸¸éµå¾ªç«¯åˆ°ç«¯çš„ç®¡é“ï¼Œæ§åˆ¶æœ‰é™ï¼Œå¿½è§†äº†ç°å®ä¸–ç•Œè®¾è®¡çš„è¿­ä»£æ€§è´¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†æ­¥ç”Ÿæˆçš„åŸå¸‚è®¾è®¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤šæ¨¡å¼æ‰©æ•£æ¨¡å‹ä¸äººç±»ä¸“ä¸šçŸ¥è¯†ç›¸ç»“åˆï¼Œä½¿è®¾è®¡è¿‡ç¨‹æ›´åŠ é€‚åº”å’Œå¯æ§ã€‚ä¸åŒäºåœ¨å•ä¸€ç«¯åˆ°ç«¯è¿‡ç¨‹ä¸­äº§ç”Ÿè®¾è®¡ç»“æœï¼Œè¯¥æ¡†æ¶å°†è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå…³é”®é˜¶æ®µï¼Œä¸æ—¢å®šçš„åŸå¸‚è®¾è®¡å·¥ä½œæµç¨‹ç›¸ä¸€è‡´ï¼šï¼ˆ1ï¼‰é“è·¯ç½‘ç»œå’ŒåœŸåœ°åˆ©ç”¨è§„åˆ’ï¼›ï¼ˆ2ï¼‰å»ºç­‘å¸ƒå±€è§„åˆ’ï¼›ï¼ˆ3ï¼‰è¯¦ç»†è§„åˆ’å’Œå‘ˆç°ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¤šæ¨¡å¼æ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºå’Œå›¾åƒçº¦æŸç”Ÿæˆåˆæ­¥è®¾è®¡ï¼Œç„¶åå¯ä»¥ç”±äººç±»è®¾è®¡å¸ˆè¿›è¡Œå®¡æŸ¥å’Œä¿®æ”¹ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°ç”Ÿæˆè®¾è®¡çš„ä¿çœŸåº¦ã€åˆè§„æ€§å’Œå¤šæ ·æ€§ã€‚ä½¿ç”¨èŠåŠ å“¥å’Œçº½çº¦å¸‚æ•°æ®çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰ä¸‰ä¸ªç»´åº¦ä¸Šéƒ½ä¼˜äºåŸºçº¿æ¨¡å‹å’Œç«¯åˆ°ç«¯æ–¹æ³•ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡å¼æ‰©æ•£æ¨¡å‹å’Œåˆ†æ­¥ç”Ÿæˆåœ¨ä¿æŒäººç±»æ§åˆ¶å’Œä¿ƒè¿›è¿­ä»£æ”¹è¿›æ–¹é¢çš„ç›Šå¤„ï¼Œä¸ºåŸå¸‚è®¾è®¡ä¸­çš„äººæœºäº¤äº’å¥ å®šäº†åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24260v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†æ­¥ç”Ÿæˆçš„åŸå¸‚è®¾è®¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ä¸äººç±»ä¸“ä¸šçŸ¥è¯†ï¼Œä½¿è®¾è®¡è¿‡ç¨‹æ›´åŠ é€‚åº”å¹¶å¯æ§ã€‚æ¡†æ¶å°†åŸå¸‚è®¾è®¡è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä½¿ç”¨å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹åŸºäºæ–‡æœ¬æç¤ºå’Œå›¾åƒçº¦æŸç”Ÿæˆåˆæ­¥è®¾è®¡ï¼Œç„¶åå¯ç”±äººç±»è®¾è®¡å¸ˆè¿›è¡Œå®¡æŸ¥å’Œä¿®æ”¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿çœŸåº¦ã€åˆè§„æ€§å’Œè®¾è®¡çš„å¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹å’Œç«¯åˆ°ç«¯æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚è®¾è®¡æ˜¯ä¸€ä¸ªæ¶‰åŠå¤šæ–¹å› ç´ å’Œä¸“ä¸šçš„å¤æ‚è¿‡ç¨‹ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘ç°åœºç‰¹å®šçº¦æŸå’Œå¤šæ–¹åˆä½œã€‚</li>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„å‡ºç°åœ¨åŸå¸‚è®¾è®¡ä¸­æä¾›äº†æ”¹å–„è®¾è®¡ç”Ÿæˆæ•ˆç‡å’Œä¿ƒè¿›è®¾è®¡ç†å¿µæ²Ÿé€šçš„å¯èƒ½æ€§ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°äººå·¥æ™ºèƒ½è®¾è®¡æµç¨‹æœªèƒ½å¾ˆå¥½åœ°èå…¥äººç±»è®¾è®¡å·¥ä½œæµç¨‹ï¼Œç¼ºä¹çµæ´»æ€§ä¸”éš¾ä»¥é€‚åº”è¿­ä»£è®¾è®¡çš„éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†æ­¥ç”Ÿæˆçš„åŸå¸‚è®¾è®¡æ¡†æ¶ï¼Œæ•´åˆäº†å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ä¸äººç±»ä¸“ä¸šçŸ¥è¯†ï¼Œä»¥åº”å¯¹ä»¥ä¸Šé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶å°†åŸå¸‚è®¾è®¡è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé“è·¯ç½‘ç»œåŠåœŸåœ°åˆ©ç”¨è§„åˆ’ã€å»ºç­‘å¸ƒå±€è§„åˆ’ã€è¯¦ç»†è§„åˆ’å’Œæ¸²æŸ“ã€‚</li>
<li>åœ¨æ¯ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹åŸºäºæ–‡æœ¬æç¤ºå’Œå›¾åƒçº¦æŸç”Ÿæˆåˆæ­¥è®¾è®¡ï¼Œä¸ºåç»­çš„äººç±»è®¾è®¡å¸ˆå®¡æŸ¥å’Œä¿®æ”¹æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-631ce3776205d86950d210c419dad034.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbac7c2b6bf958b88cc75ef8e5c17c23.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="STORK-Improving-the-Fidelity-of-Mid-NFE-Sampling-for-Diffusion-and-Flow-Matching-Models"><a href="#STORK-Improving-the-Fidelity-of-Mid-NFE-Sampling-for-Diffusion-and-Flow-Matching-Models" class="headerlink" title="STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow   Matching Models"></a>STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow   Matching Models</h2><p><strong>Authors:Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu</strong></p>
<p>Diffusion models (DMs) have demonstrated remarkable performance in high-fidelity image and video generation. Because high-quality generations with DMs typically require a large number of function evaluations (NFEs), resulting in slow sampling, there has been extensive research successfully reducing the NFE to a small range (&lt;10) while maintaining acceptable image quality. However, many practical applications, such as those involving Stable Diffusion 3.5, FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve superior results, and, despite the practical relevance, research on the effective sampling within this mid-NFE regime remains underexplored. In this work, we propose a novel, training-free, and structure-independent DM ODE solver called the Stabilized Taylor Orthogonal Rungeâ€“Kutta (STORK) method, based on a class of stiff ODE solvers with a Taylor expansion adaptation. Unlike prior work such as DPM-Solver, which is dependent on the semi-linear structure of the DM ODE, STORK is applicable to any DM sampling, including noise-based and flow matching-based models. Within the 20-50 NFE range, STORK achieves improved generation quality, as measured by FID scores, across unconditional pixel-level generation and conditional latent-space generation tasks using models like Stable Diffusion 3.5 and SANA. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK">https://github.com/ZT220501/STORK</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç”±äºä½¿ç”¨DMsç”Ÿæˆé«˜è´¨é‡å›¾åƒé€šå¸¸éœ€è¦å¤§é‡çš„å‡½æ•°è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œå¯¼è‡´é‡‡æ ·é€Ÿåº¦è¾ƒæ…¢ï¼Œå› æ­¤å·²æœ‰å¤§é‡ç ”ç©¶æˆåŠŸå°†NFEé™ä½åˆ°ä¸€ä¸ªè¾ƒå°çš„èŒƒå›´ï¼ˆ&lt;10ï¼‰åŒæ—¶ä¿æŒå¯æ¥å—çš„å›¾åƒè´¨é‡ã€‚ç„¶è€Œï¼Œè®¸å¤šå®é™…åº”ç”¨ï¼Œå¦‚æ¶‰åŠStable Diffusion 3.5ã€FLUXå’ŒSANAçš„åº”ç”¨ï¼Œé€šå¸¸åœ¨ä¸­æœŸNFEèŒƒå›´ï¼ˆ20-50 NFEï¼‰å†…è¿è¡Œï¼Œä»¥å–å¾—æ›´å¥½çš„ç»“æœã€‚å°½ç®¡è¿™ç§åšæ³•å…·æœ‰å®é™…æ„ä¹‰ï¼Œä½†å…³äºæ­¤ä¸­æœŸNFEèŒƒå›´å†…æœ‰æ•ˆé‡‡æ ·çš„ç ”ç©¶ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒä¸”ç‹¬ç«‹äºç»“æ„ä¹‹å¤–çš„DM ODEæ±‚è§£å™¨ï¼Œç§°ä¸ºç¨³å®šåŒ–çš„æ³°å‹’æ­£äº¤é¾™æ ¼-åº“å¡”ï¼ˆSTORKï¼‰æ–¹æ³•ï¼Œå®ƒåŸºäºä¸€ç±»å¸¦æ³°å‹’å±•å¼€é€‚åº”æ€§çš„åˆšæ€§çš„ODEæ±‚è§£å™¨ã€‚ä¸å…ˆå‰çš„å·¥ä½œï¼ˆå¦‚DPM-Solverï¼‰ä¸åŒï¼Œåè€…ä¾èµ–äºDM ODEçš„åŠçº¿æ€§ç»“æ„ï¼ŒSTORKå¯åº”ç”¨äºä»»ä½•DMé‡‡æ ·ï¼ŒåŒ…æ‹¬åŸºäºå™ªå£°å’ŒåŸºäºæµåŒ¹é…æ¨¡å‹çš„é‡‡æ ·ã€‚åœ¨20-50 NFEèŒƒå›´å†…ï¼ŒSTORKåœ¨æ— æ¡ä»¶åƒç´ çº§ç”Ÿæˆå’Œæœ‰æ¡ä»¶æ½œåœ¨ç©ºé—´ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨Stable Diffusion 3.5å’ŒSANAç­‰æ¨¡å‹æ—¶ï¼Œå®ç°äº†æ”¹è¿›çš„ç”Ÿæˆè´¨é‡ï¼Œè¯¥è´¨é‡ç”±FIDåˆ†æ•°è¡¡é‡ã€‚ä»£ç å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK%E3%80%82">https://github.com/ZT220501/STORKã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24210v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå…¶é€šå¸¸éœ€è¦å¤§é‡çš„åŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰æ¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå¯¼è‡´é‡‡æ ·é€Ÿåº¦æ…¢ï¼Œå› æ­¤ç ”ç©¶è€…ä»¬ä¸€ç›´è‡´åŠ›äºå‡å°‘NFEsè‡³è¾ƒå°èŒƒå›´å¹¶ä¿æŒå¯æ¥å—çš„å›¾åƒè´¨é‡ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å°†NFEæˆåŠŸé™è‡³ä¸ªä½æ•°ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚Stable Diffusion 3.5ã€FLUXå’ŒSANAç­‰ï¼Œé€šå¸¸åœ¨ä¸­æœŸNFEèŒƒå›´å†…ï¼ˆ20-50 NFEï¼‰è¿è¡Œä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒä¸”ç‹¬ç«‹äºç»“æ„ä¹‹å¤–çš„æ‰©æ•£æ¨¡å‹ODEæ±‚è§£å™¨â€”â€”ç¨³å®šæ³°å‹’æ­£äº¤é¾™æ ¼åº“å¡”ï¼ˆSTORKï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä¸€ç±»åˆšæ€§çš„ODEæ±‚è§£å™¨å¹¶èåˆäº†æ³°å‹’å±•å¼€é€‚åº”æŠ€æœ¯ã€‚ä¸ä¾èµ–DM ODEåŠçº¿æ€§ç»“æ„çš„DPM-Solverä¸åŒï¼ŒSTORKé€‚ç”¨äºä»»ä½•DMé‡‡æ ·ï¼ŒåŒ…æ‹¬åŸºäºå™ªå£°å’ŒæµåŒ¹é…æ¨¡å‹ã€‚åœ¨20-50 NFEèŒƒå›´å†…ï¼ŒSTORKåœ¨æ— æ¡ä»¶åƒç´ çº§ç”Ÿæˆå’Œæ¡ä»¶æ½œåœ¨ç©ºé—´ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æ›´é«˜çš„ç”Ÿæˆè´¨é‡ï¼Œä½¿ç”¨å¦‚Stable Diffusion 3.5å’ŒSANAç­‰æ¨¡å‹æ—¶ï¼Œå…¶FIDå¾—åˆ†æœ‰æ‰€æé«˜ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZT220501/STORKè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç›®å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å‡å°‘æ‰©æ•£æ¨¡å‹çš„NFEè‡³è¾ƒå°èŒƒå›´ä»¥ä¿æŒå›¾åƒè´¨é‡ï¼Œä½†ä¸­æœŸNFEèŒƒå›´å†…çš„æœ‰æ•ˆé‡‡æ ·ç ”ç©¶ä»è¾ƒå°‘ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ODEæ±‚è§£å™¨â€”â€”STORKæ–¹æ³•ï¼Œé€‚ç”¨äºä»»ä½•DMé‡‡æ ·ï¼ŒåŒ…æ‹¬åŸºäºå™ªå£°å’ŒæµåŒ¹é…æ¨¡å‹ã€‚</li>
<li>åœ¨ä¸­æœŸNFEèŒƒå›´å†…ï¼ˆ20-50 NFEï¼‰ï¼ŒSTORKæ–¹æ³•åœ¨æ— æ¡ä»¶åƒç´ çº§ç”Ÿæˆå’Œæ¡ä»¶æ½œåœ¨ç©ºé—´ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æ›´é«˜çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSTORKæ–¹æ³•åœ¨å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡FIDå¾—åˆ†ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>STORKæ–¹æ³•çš„å®ç°ä»£ç å·²å…¬å¼€æä¾›ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a28809a7741e5219671150a00c66b72d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cfadecfc4ad08528ba2fb5f8a8d4c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee12ff06a44e5353522589961d830844.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OpenUniï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åŸºå‡†ã€‚å—åˆ°å½“å‰ç»Ÿä¸€æ¨¡å‹å­¦ä¹ å®è·µçš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œä¸€ä¸ªè½»é‡çº§çš„åŸºäºå˜å‹å™¨çš„è¿æ¥å™¨ï¼Œå°†ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è¿æ¥èµ·æ¥ï¼Œä»è€Œæœ€å°åŒ–è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚é€šè¿‡é€‰æ‹©æœ€ç®€å•çš„æ¶æ„ï¼Œæˆ‘ä»¬è¯æ˜OpenUniå¯ä»¥ï¼š1ï¼‰ç”Ÿæˆé«˜è´¨é‡ä¸”ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒï¼›2ï¼‰åœ¨GenEvalã€DPG-Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼Œæ¿€æ´»çš„å‚æ•°åªæœ‰1.1Bå’Œ3.1Bã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œç¤¾åŒºå‘å±•ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>ä¸Šå‘å¸ƒäº†æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œæˆ‘ä»¬ç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬2300ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OpenUniï¼Œä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åŸºå‡†ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œè½»é‡çº§åŸºäºå˜å‹å™¨çš„è¿æ¥å™¨ï¼Œå°†ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹è¿æ¥èµ·æ¥ï¼Œé™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚OpenUnièƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€æŒ‡ä»¤å¯¹é½çš„å›¾åƒï¼Œå¹¶åœ¨GenEvalã€DPG-Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼Œä»…ä½¿ç”¨1.1Bå’Œ3.1Bæ¿€æ´»å‚æ•°ã€‚æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬2.3äº¿å›¾åƒæ–‡æœ¬å¯¹ï¼‰å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenUniæ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆç»Ÿä¸€æ¨¡å‹ã€‚</li>
<li>å®ƒé€šè¿‡æ¡¥æ¥ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>é‡‡ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œè½»é‡çº§åŸºäºå˜å‹å™¨çš„è¿æ¥å™¨æ¥é™ä½è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚</li>
<li>OpenUniå¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€æŒ‡ä»¤å¯¹é½çš„å›¾åƒã€‚</li>
<li>å®ƒåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¦‚GenEvalã€DPG-Benchå’ŒWISEã€‚</li>
<li>è¯¥æ¨¡å‹ä»…ä½¿ç”¨1.1Bå’Œ3.1Bæ¿€æ´»å‚æ•°ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d56dc7306e5f25558cee0e232e83a179.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4660fa82c63c1a45604f8604520060af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d31729f8a2776c5c424dbbd0e69f52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-215343cec7a0cfdac617ff0769b3ee6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d96e785d6599c1f0f5c00a115dab2434.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape"><a href="#Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape" class="headerlink" title="Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape"></a>Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape</h2><p><strong>Authors:Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu</strong></p>
<p>Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V&#x2F;T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45% end-to-end % and over 92% self-attention latency reduction on an H100 GPU at negligible overhead cost.   Code available online here: \href{<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention%7D%7Bhttps://github.com/cccrrrccc/Re-ttention%7D">https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}</a> </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡è§†è§‰å†…å®¹ï¼ˆå¦‚è§†é¢‘å’Œå›¾åƒï¼‰çš„é»˜è®¤æ¨¡å‹ã€‚ä¸€ä¸ªå·¨å¤§çš„ç“¶é¢ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­å¤æ‚æ€§éšåˆ†è¾¨ç‡å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ è€ŒäºŒæ¬¡æ–¹å¢é•¿ã€‚å‡è½»è¿™ä¸€è´Ÿæ‹…çš„ä¸€ç§é€»è¾‘æ–¹æ³•æ˜¯ç¨€ç–æ³¨æ„åŠ›ï¼Œå…¶ä¸­ä»…å°†ä¸€éƒ¨åˆ†æ ‡è®°æˆ–è¡¥ä¸åŒ…å«åœ¨è®¡ç®—ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨æé«˜çš„ç¨€ç–åº¦æ°´å¹³ä¸Šæ— æ³•ä¿æŒè§†è§‰è´¨é‡ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†Re-ttentionï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶ç©ºå†—ä½™æ€§ï¼Œå®ç°äº†é’ˆå¯¹è§†è§‰ç”Ÿæˆæ¨¡å‹çš„é«˜ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…‹æœäº†æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ¦‚ç‡å½’ä¸€åŒ–è½¬ç§»é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒRe-ttentionæ ¹æ®å…ˆå‰çš„softmaxåˆ†å¸ƒå†å²é‡æ–°è°ƒæ•´æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥åœ¨æé«˜çš„ç¨€ç–åº¦æ°´å¹³ä¸Šä¿æŒå…¨äºŒæ¬¡æ³¨æ„åŠ›çš„è§†è§‰è´¨é‡ã€‚åœ¨T2V&#x2F;T2Iæ¨¡å‹ï¼ˆå¦‚CogVideoXå’ŒPixArt DiTsï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-ttentionåœ¨æ¨ç†è¿‡ç¨‹ä¸­åªéœ€è¦3.1%çš„æ ‡è®°ï¼Œè¶…è¶Šäº†FastDiTAttnã€Sparse VideoGenå’ŒMInferenceç­‰å½“ä»£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹é‡å»¶è¿Ÿæ—¶é—´è¯æ˜ï¼Œè¯¥æ–¹æ³•å¯åœ¨H100 GPUä¸Šå®ç°è¶…è¿‡45%çš„ç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘å’Œè¶…è¿‡92%çš„è‡ªæ³¨æ„åŠ›å»¶è¿Ÿå‡å°‘ï¼ŒåŒæ—¶å‡ ä¹ä¸å¢åŠ é¢å¤–æˆæœ¬ã€‚ç›¸å…³ä»£ç å·²åœ¨çº¿å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention">https://github.com/cccrrrccc/Re-ttention</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22918v2">PDF</a> Submitted before obtaining agreement of all authors</p>
<p><strong>Summary</strong><br>     æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡è§†è§‰å†…å®¹ï¼ˆå¦‚è§†é¢‘å’Œå›¾åƒï¼‰çš„æ ‡å‡†æ¨¡å‹ã€‚å½“å‰é¢ä¸´çš„ä¸»è¦ç“¶é¢ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶å¤æ‚æ€§éšåˆ†è¾¨ç‡å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ è€Œå‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚ä¸ºå‡è½»è¿™ä¸€è´Ÿæ‹…ï¼Œäººä»¬å°è¯•é‡‡ç”¨ç¨€ç–æ³¨æ„åŠ›æ³•ï¼Œä»…åŒ…å«éƒ¨åˆ†ä»¤ç‰Œæˆ–è¡¥ä¸è¿›è¡Œè®¡ç®—ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨æé«˜çš„ç¨€ç–æ°´å¹³ä¸‹æ— æ³•ä¿æŒè§†è§‰è´¨é‡ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Re-ttentionï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶é—´å†—ä½™æ€§ï¼Œé€šè¿‡å…‹æœæ³¨æ„åŠ›æœºåˆ¶å†…çš„æ¦‚ç‡å½’ä¸€åŒ–åç§»ï¼Œä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹å®ç°äº†æé«˜çš„ç¨€ç–æ³¨æ„åŠ›ã€‚Re-ttentionæ ¹æ®å…ˆå‰çš„softmaxåˆ†å¸ƒå†å²é‡å¡‘æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥åœ¨æé«˜çš„ç¨€ç–æ°´å¹³ä¸Šä¿æŒå…¨äºŒæ¬¡æ³¨æ„åŠ›çš„è§†è§‰è´¨é‡ã€‚åœ¨T2V&#x2F;T2Iæ¨¡å‹ï¼ˆå¦‚CogVideoXå’ŒPixArt DiTsï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-ttentionåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…éœ€ä½¿ç”¨3.1%çš„ä»¤ç‰Œï¼Œè¶…è¶Šäº†FastDiTAttnã€Sparse VideoGenå’ŒMInferenceç­‰å½“ä»£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æµ‹é‡äº†å»¶è¿Ÿæ—¶é—´ï¼Œè¯æ˜è¯¥æ–¹æ³•å¯åœ¨H100 GPUä¸Šå®ç°è¶…è¿‡45%çš„ç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘å’Œè¶…è¿‡92%çš„è‡ªæ³¨æ„åŠ›å»¶è¿Ÿå‡å°‘ï¼Œä¸”å‡ ä¹ä¸å¢åŠ å¼€é”€ã€‚ç›¸å…³ä»£ç å·²åœ¨çº¿å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformers (DiT) æ˜¯ç”Ÿæˆé«˜è´¨é‡è§†è§‰å†…å®¹çš„ä¸»å¯¼æ¨¡å‹ã€‚</li>
<li>å½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚æ€§ï¼Œå®ƒéšåˆ†è¾¨ç‡å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ è€Œå¢é•¿ã€‚</li>
<li>ä¸ºå‡è½»è´Ÿæ‹…ï¼Œæå‡ºäº†ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œä½†ç°æœ‰æŠ€æœ¯åœ¨é«˜ç¨€ç–åº¦ä¸‹å­˜åœ¨è§†è§‰è´¨é‡æŸå¤±å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚</li>
<li>Re-ttention åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶é—´å†—ä½™æ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡é‡å¡‘æ³¨æ„åŠ›åˆ†æ•°æ¥åœ¨æé«˜ç¨€ç–åº¦ä¸‹ä¿æŒè§†è§‰è´¨é‡ã€‚</li>
<li>Re-ttention åœ¨T2V&#x2F;T2Iæ¨¡å‹ä¸Šçš„å®éªŒè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨ã€‚</li>
<li>Re-ttention èƒ½æ˜¾è‘—å‡å°‘ç«¯åˆ°ç«¯å’Œè‡ªæ³¨æ„åŠ›çš„å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a75cd7f47adc14567e99e44316bdc5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30216dafe9bcff6b84a9b4339723b5f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7905eafd41f579c842d54bda2d8a4f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71d2f80fd6570fad70cdaf1f531b0ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9478fd57d613165c33ec1defc3bccfb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiffDecompose-Layer-Wise-Decomposition-of-Alpha-Composited-Images-via-Diffusion-Transformers"><a href="#DiffDecompose-Layer-Wise-Decomposition-of-Alpha-Composited-Images-via-Diffusion-Transformers" class="headerlink" title="DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via   Diffusion Transformers"></a>DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via   Diffusion Transformers</h2><p><strong>Authors:Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song</strong></p>
<p>Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent&#x2F;transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/Wangzt1121/DiffDecompose">https://github.com/Wangzt1121/DiffDecompose</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è®¸å¤šç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡ç§»é™¤ï¼‰ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒåˆ†è§£æ–¹æ³•åœ¨è§£å†³åŠé€æ˜æˆ–é€æ˜å±‚é®æŒ¡é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ï¼Œè¿™æ˜¯ç”±äºé®ç½©å…ˆéªŒä¾èµ–ã€é™æ€å¯¹è±¡å‡è®¾ä»¥åŠæ•°æ®é›†ç¼ºä¹æ‰€å¯¼è‡´çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šé€å±‚åˆ†è§£Alphaåˆæˆå›¾åƒï¼Œæ—¨åœ¨ä»å•ä¸ªé‡å å›¾åƒä¸­æ¢å¤æ„æˆå±‚ï¼Œæ¡ä»¶ä¸ºåŠé€æ˜&#x2F;é€æ˜Alphaå±‚çš„éçº¿æ€§é®æŒ¡ã€‚ä¸ºäº†åº”å¯¹å±‚æ¨¡ç³Šã€æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®ç¨€ç¼ºç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†AlphaBlendæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºé€æ˜å’ŒåŠé€æ˜å±‚åˆ†è§£çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œæ”¯æŒå…­ä¸ªçœŸå®ä¸–ç•Œå­ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼ŒåŠé€æ˜ç‚«å…‰å»é™¤ã€åŠé€æ˜ç»†èƒåˆ†è§£ã€ç»ç’ƒå™¨çš¿åˆ†è§£ç­‰ï¼‰ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†DiffDecomposeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£Transformerçš„æ¡†æ¶ï¼Œå®ƒå­¦ä¹ åœ¨è¾“å…¥å›¾åƒã€è¯­ä¹‰æç¤ºå’Œæ··åˆç±»å‹æ¡ä»¶ä¸‹çš„å¯èƒ½å±‚åˆ†è§£çš„åéªŒæ¦‚ç‡ã€‚DiffDecomposeä¸æ˜¯ç›´æ¥å›å½’alphaé®ç½©ï¼Œè€Œæ˜¯è¿›è¡Œä¸Šä¸‹æ–‡åˆ†è§£ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€æ¯å±‚ç›‘ç£çš„æƒ…å†µä¸‹é¢„æµ‹ä¸€ä¸ªæˆ–å¤šä¸ªå±‚ï¼Œå¹¶å¼•å…¥äº†å±‚ä½ç½®ç¼–ç å…‹éš†ä»¥ä¿æŒå±‚ä¹‹é—´çš„åƒç´ çº§å¯¹åº”å…³ç³»ã€‚åœ¨æå‡ºçš„AlphaBlendæ•°æ®é›†å’Œå…¬å…±LOGOæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†DiffDecomposeçš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡æ¥å—åï¼Œä»£ç å’Œæ•°æ®é›†å°†å¯ä¾›ä½¿ç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/Wangzt1|">https://github.com/Wangzt1|</a> ä¼šå¢åŠ æ ‡ç‚¹ç¬¦å·å‰²è£‚è¯­å¢ƒæˆ–è€…ä¿¡æ¯åˆ†å‰²ä¸æ˜ç­‰è´Ÿé¢å½±å“ï¼Œæ‰€ä»¥åœ¨å®é™…ç¿»è¯‘è¿‡ç¨‹ä¸­ä¸€èˆ¬ä¼šå°½é‡ä¿æŒè¯­å¥çš„å®Œæ•´æ€§ã€‚ä¸‹é¢æ˜¯ç»è¿‡ç¿»è¯‘çš„æ–‡æœ¬ï¼š</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21541v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ†è§£æ–°æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŠé€æ˜æˆ–é€æ˜å›¾å±‚é®æŒ¡çš„åˆ†è§£é—®é¢˜ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼šé€å±‚åˆ†è§£Alphaåˆæˆå›¾åƒï¼Œæ—¨åœ¨ä»å•ä¸€é‡å å›¾åƒä¸­æ¢å¤æ„æˆå›¾å±‚ã€‚ä¸ºè§£å†³å›¾å±‚æ¨¡ç³Šã€é€šç”¨æ€§åŠæ•°æ®ç¨€ç¼ºç­‰æŒ‘æˆ˜ï¼Œæ–‡ç« é¦–å…ˆæ¨å‡ºäº†AlphaBlendæ•°æ®é›†ï¼Œæ”¯æŒå…­ä¸ªçœŸå®ä¸–ç•Œçš„å­ä»»åŠ¡ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£Transformerçš„DiffDecomposeæ¡†æ¶ï¼Œå­¦ä¹ æ ¹æ®è¾“å…¥å›¾åƒã€è¯­ä¹‰æç¤ºå’Œæ··åˆç±»å‹è¿›è¡Œå¯èƒ½çš„å›¾å±‚åˆ†è§£çš„åéªŒåˆ†å¸ƒã€‚DiffDecomposeé‡‡ç”¨ä¸Šä¸‹æ–‡åˆ†è§£æ–¹å¼ï¼Œæ— éœ€æ¯å±‚ç›‘ç£å³å¯é¢„æµ‹ä¸€å±‚æˆ–å¤šå±‚ï¼Œå¹¶å¼•å…¥å±‚ä½ç½®ç¼–ç å…‹éš†ä»¥ç»´æŒå±‚é—´çš„åƒç´ çº§å¯¹åº”å…³ç³»ã€‚åœ¨AlphaBlendå’Œå…¬å¼€LOGOæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†DiffDecomposeçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šé€å±‚åˆ†è§£Alphaåˆæˆçš„å›¾åƒï¼Œç›®æ ‡æ˜¯æ¢å¤ç”±åŠé€æ˜æˆ–é€æ˜å›¾å±‚é®æŒ¡æ„æˆçš„å›¾å±‚ã€‚</li>
<li>é’ˆå¯¹è¿™ä¸€ä»»åŠ¡ï¼Œæ–‡ç« æå‡ºäº†AlphaBlendæ•°æ®é›†ï¼Œæ”¯æŒå…­ä¸ªçœŸå®ä¸–ç•Œçš„å­ä»»åŠ¡ï¼Œä¸ºå›¾åƒåˆ†è§£æä¾›äº†ä¸°å¯Œçš„æ•°æ®èµ„æºã€‚</li>
<li>ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„DiffDecomposeæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å­¦ä¹ æ ¹æ®è¾“å…¥å›¾åƒã€è¯­ä¹‰æç¤ºå’Œæ··åˆç±»å‹è¿›è¡Œå¯èƒ½çš„å›¾å±‚åˆ†è§£çš„åéªŒåˆ†å¸ƒã€‚</li>
<li>DiffDecomposeé‡‡ç”¨ä¸Šä¸‹æ–‡åˆ†è§£æ–¹æ³•ï¼Œèƒ½é¢„æµ‹ä¸€å±‚æˆ–å¤šå±‚è€Œæ— éœ€æ¯å±‚éƒ½æœ‰ç›´æ¥ç›‘ç£ã€‚</li>
<li>å¼•å…¥äº†å±‚ä½ç½®ç¼–ç å…‹éš†æŠ€æœ¯ï¼Œä¿æŒå„å±‚ä¹‹é—´çš„åƒç´ çº§å¯¹åº”å…³ç³»ã€‚</li>
<li>åœ¨AlphaBlendå’Œå…¬å¼€LOGOæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffDecomposeæŠ€æœ¯æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>è®ºæ–‡æ¥å—åï¼Œå°†å…¬å¼€ä»£ç å’Œæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-54a2270aeefc9c9c9c45452a34aef8e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a025374db415838d9736299af6539419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62f0d30b704a0fef321d151c257ec5ab.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models"><a href="#T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models" class="headerlink" title="T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models"></a>T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models</h2><p><strong>Authors:Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li</strong></p>
<p>Recent advances in text-to-video (T2V) diffusion models have significantly enhanced the quality of generated videos. However, their ability to produce explicit or harmful content raises concerns about misuse and potential rights violations. Inspired by the success of unlearning techniques in erasing undesirable concepts from text-to-image (T2I) models, we extend unlearning to T2V models and propose a robust and precise unlearning method. Specifically, we adopt negatively-guided velocity prediction fine-tuning and enhance it with prompt augmentation to ensure robustness against LLM-refined prompts. To achieve precise unlearning, we incorporate a localization and a preservation regularization to preserve the modelâ€™s ability to generate non-target concepts. Extensive experiments demonstrate that our method effectively erases a specific concept while preserving the modelâ€™s generation capability for all other concepts, outperforming existing methods. We provide the unlearned models in \href{<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%7D%7Bhttps://github.com/VDIGPKU/T2VUnlearning.git%7D">https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}</a>. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬äº§ç”Ÿæ˜ç¡®æˆ–æœ‰å®³å†…å®¹çš„èƒ½åŠ›å¼•å‘äº†å…³äºè¯¯ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ã€‚å—æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä¸­æ¶ˆé™¤ä¸è‰¯æ¦‚å¿µçš„å»å­¦ä¹ æŠ€æœ¯æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬å°†å»å­¦ä¹ æ‰©å±•åˆ°T2Væ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¨³å¥ä¸”ç²¾ç¡®çš„å»å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è´Ÿå¯¼å‘é€Ÿåº¦é¢„æµ‹å¾®è°ƒï¼Œå¹¶é€šè¿‡æç¤ºå¢å¼ºæ¥ç¡®ä¿å…¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æç¤ºçš„ç¨³å¥æ€§ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„å»å­¦ä¹ ï¼Œæˆ‘ä»¬ç»“åˆäº†å®šä½å’Œä¿å­˜æ­£åˆ™åŒ–ï¼Œä»¥ä¿ç•™æ¨¡å‹ç”Ÿæˆéç›®æ ‡æ¦‚å¿µçš„èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ï¼Œä¿ç•™äº†æ¨¡å‹å¯¹æ‰€æœ‰å…¶ä»–æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬æä¾›å»å­¦ä¹ çš„æ¨¡å‹åœ¨<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%E3%80%82">https://github.com/VDIGPKU/T2VUnlearning.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17550v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•ï¼Œè¿™äº›æ¨¡å‹ç”Ÿæˆè§†é¢‘çš„è´¨é‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬äº§ç”Ÿæ˜ç¡®æˆ–æœ‰å®³å†…å®¹çš„èƒ½åŠ›å¼•å‘äº†å…³äºè¯¯ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ã€‚ç ”ç©¶å›¢é˜Ÿå—åˆ°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä¸­çš„å»è®­ç»ƒæŠ€æœ¯æˆåŠŸçš„å¯å‘ï¼Œå°†å»è®­ç»ƒæŠ€æœ¯æ‰©å±•åˆ°T2Væ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¨³å¥ä¸”ç²¾ç¡®çš„å»è®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¶ˆé™¤ç‰¹å®šæ¦‚å¿µï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹å¯¹å…¶ä»–æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸å…³æ¨¡å‹å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>T2Væ¨¡å‹äº§ç”Ÿæœ‰å®³å†…å®¹çš„èƒ½åŠ›å¼•å‘å…³æ³¨ã€‚</li>
<li>ç ”ç©¶äººå‘˜å—åˆ°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å»è®­ç»ƒæŠ€æœ¯æˆåŠŸçš„å¯å‘ï¼Œå°†å…¶æ‰©å±•åˆ°T2Væ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨³å¥ä¸”ç²¾ç¡®çš„å»è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è´Ÿå‘å¼•å¯¼é€Ÿåº¦é¢„æµ‹å¾®è°ƒå¹¶ç»“åˆæç¤ºå¢å¼ºæŠ€æœ¯ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å®ç°äº†ç²¾ç¡®å»è®­ç»ƒï¼Œé€šè¿‡å¼•å…¥å®šä½å’Œä¿ç•™æ­£åˆ™åŒ–æ¥ä¿ç•™æ¨¡å‹å¯¹éç›®æ ‡æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¶ˆé™¤ç‰¹å®šæ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒå¯¹å…¶ä»–æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af5426cf2508fedbb314a5d9ce4f328e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d9a9db4dfd0d331c8779326b3ac5ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8023591641a773830b258f6604362f8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85e0455019d3f902b9195b638488f108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b70de0b98b44af06b9fd700e8a7cba8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b86cdc1ec073f06b8e441d5ab9df478d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction"><a href="#AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction" class="headerlink" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction"></a>AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer">https://github.com/TencentARC/AnimeGamer</a>. </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒå’Œè§†é¢‘åˆæˆæŠ€æœ¯çš„è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆé¢†åŸŸå¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚ä¸€ä¸ªç‰¹åˆ«å¸å¼•äººçš„åº”ç”¨æ˜¯å°†åŠ¨æ¼«ç”µå½±ä¸­çš„è§’è‰²è½¬å˜ä¸ºå¯äº’åŠ¨çš„æ¸¸æˆå®ä½“ã€‚è¿™å…è®¸ç©å®¶é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸åœ¨åŠ¨æ€çš„åŠ¨æ¼«ä¸–ç•Œä¸­ï¼Œæ‰®æ¼”ä»–ä»¬æœ€å–œæ¬¢çš„è§’è‰²è¿›è¡Œç”Ÿæ´»æ¨¡æ‹Ÿã€‚è¿™ç±»æ¸¸æˆè¢«å®šä¹‰ä¸ºæ— é™æ¸¸æˆï¼Œå› ä¸ºå®ƒä»¬æ¶ˆé™¤äº†é¢„å®šçš„è¾¹ç•Œå’Œå›ºå®šçš„æ¸¸æˆè§„åˆ™ï¼Œç©å®¶å¯ä»¥é€šè¿‡å¼€æ”¾å¼çš„è¯­è¨€ä¸æ¸¸æˆä¸–ç•Œäº’åŠ¨ï¼Œå¹¶ä½“éªŒä¸æ–­æ¼”å˜çš„æ•…äº‹æƒ…èŠ‚å’Œç¯å¢ƒã€‚æœ€è¿‘ï¼Œä¸€ç§å¼€åˆ›æ€§çš„æ— é™åŠ¨æ¼«ç”Ÿæ´»æ¨¡æ‹Ÿæ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†å¤šè½®æ–‡æœ¬å¯¹è¯ç¿»è¯‘ä¸ºå›¾åƒç”Ÿæˆçš„è¯­è¨€æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œå®ƒå¿½ç•¥äº†å†å²è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¸¸æˆæ€§ä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œå®ƒåªèƒ½ç”Ÿæˆé™æ€å›¾åƒï¼Œæ— æ³•èå…¥åŠ¨æ€å…ƒç´ ï¼Œæ— æ³•ä¸ºç©å®¶æä¾›å¼•äººå…¥èƒœçš„æ¸¸æˆä½“éªŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AnimeGamerï¼Œå®ƒå»ºç«‹åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¹‹ä¸Šï¼Œç”¨äºç”Ÿæˆæ¯ä¸ªæ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬æç»˜è§’è‰²åŠ¨ä½œå’ŒçŠ¶æ€æ›´æ–°çš„åŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ¥ä»£è¡¨åŠ¨ç”»é•œå¤´ï¼Œå¯ä»¥ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å°†å…¶è§£ç ä¸ºé«˜è´¨é‡çš„è§†é¢‘ç‰‡æ®µã€‚é€šè¿‡è·å–å†å²åŠ¨ç”»é•œå¤´è¡¨ç¤ºä½œä¸ºä¸Šä¸‹æ–‡å¹¶é¢„æµ‹éšåçš„è¡¨ç¤ºï¼ŒAnimeGamerå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œæ»¡æ„åŠ¨æ€æ€§çš„æ¸¸æˆã€‚ä½¿ç”¨è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œäººç±»è¯„ä¼°çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer%E3%80%82">https://github.com/TencentARC/AnimeGamerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01014v2">PDF</a> Project released at: <a target="_blank" rel="noopener" href="https://howe125.github.io/AnimeGamer.github.io/">https://howe125.github.io/AnimeGamer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°å›¾åƒå’Œè§†é¢‘åˆæˆæŠ€æœ¯åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ¼«è§’è‰²è½¬åŒ–ä¸ºå¯äº’åŠ¨å®ä½“æ–¹é¢çš„åˆ›æ–°ã€‚ç©å®¶å¯é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸äºæ— é™åŠ¨æ¼«ä¸–ç•Œçš„åŠ¨æ€è§’è‰²æ¨¡æ‹Ÿä¸­ã€‚æ–‡ç« æŒ‡å‡ºäº†ä¸€ç§å°†å¤šè½®æ–‡æœ¬å¯¹è¯è½¬åŒ–ä¸ºå›¾åƒç”Ÿæˆè¯­è¨€æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œä½†å¿½è§†äº†å†å²è§†è§‰èƒŒæ™¯ï¼Œå¯¼è‡´æ¸¸æˆä½“éªŒä¸ä¸€è‡´ã€‚æœ¬ç ”ç©¶æå‡ºåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„AnimeGamerï¼Œèƒ½ç”Ÿæˆæ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬åŠ¨æ€åŠ¨ç”»é•œå¤´å’Œè§’è‰²çŠ¶æ€æ›´æ–°ã€‚å¼•å…¥æ–°å‹åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ¥å±•ç¤ºåŠ¨ç”»é•œå¤´ï¼Œå¹¶ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è§£ç æˆé«˜è´¨é‡è§†é¢‘ç‰‡æ®µã€‚é€šè¿‡è€ƒè™‘å†å²åŠ¨ç”»é•œå¤´å¹¶é¢„æµ‹åç»­è¡¨ç¤ºï¼ŒAnimeGamerèƒ½ç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œæ»¡æ„åŠ¨æ€æ€§çš„æ¸¸æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ¼«è§’è‰²è½¬åŒ–ä¸ºå¯äº’åŠ¨å®ä½“æŠ€æœ¯ä¸ºæ¸¸æˆé¢†åŸŸå¸¦æ¥æ–°çš„æœºé‡ã€‚</li>
<li>ç©å®¶å¯é€šè¿‡è¯­è¨€æŒ‡ä»¤æ²‰æµ¸äºåŠ¨æ€åŠ¨æ¼«ä¸–ç•Œä¸­çš„è§’è‰²æ¨¡æ‹Ÿä¸­ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•å¿½è§†äº†å†å²è§†è§‰èƒŒæ™¯ï¼Œå¯¼è‡´æ¸¸æˆä½“éªŒä¸ä¸€è‡´ã€‚</li>
<li>AnimeGameråŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ç”Ÿæˆæ¸¸æˆçŠ¶æ€åŠåŠ¨æ€åŠ¨ç”»é•œå¤´ã€‚</li>
<li>å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€è¡¨ç¤ºæ¥å±•ç¤ºåŠ¨ç”»é•œå¤´ï¼Œæé«˜æ¸¸æˆè´¨é‡ã€‚</li>
<li>AnimeGameré€šè¿‡è€ƒè™‘å†å²åŠ¨ç”»é•œå¤´é¢„æµ‹åç»­å†…å®¹ï¼Œå®ç°ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€æ€§çš„æ¸¸æˆä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-87cd988c8c24823de93269df32207128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd29e104e47e0ff54d6807a15db67d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696e28d03c6550c4c5c82861b2ad2d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f3546a6de895403e043b8e44e47447f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cef586b07bbc87c8a676a1da28206c4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Text-Driven-360-Degree-Panorama-Generation"><a href="#A-Survey-on-Text-Driven-360-Degree-Panorama-Generation" class="headerlink" title="A Survey on Text-Driven 360-Degree Panorama Generation"></a>A Survey on Text-Driven 360-Degree Panorama Generation</h2><p><strong>Authors:Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue</strong></p>
<p>The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at <a target="_blank" rel="noopener" href="https://littlewhitesea.github.io/Text-Driven-Pano-Gen/">https://littlewhitesea.github.io/Text-Driven-Pano-Gen/</a>. </p>
<blockquote>
<p>æ–‡æœ¬é©±åŠ¨çš„å…¨æ™¯ç”ŸæˆæŠ€æœ¯å‡ºç°ï¼Œä½¿å¾—å¯ä»¥ç›´æ¥ä»æ–‡æœ¬æè¿°ä¸­åˆæˆå…¨è§†è§’å…¨æ™¯å›¾åƒï¼Œè¿™æ ‡å¿—ç€æ²‰æµ¸å¼è§†è§‰å†…å®¹åˆ›ä½œé¢†åŸŸå‘ç”Ÿäº†é©å‘½æ€§çš„è¿›æ­¥ã€‚è¿™ä¸€åˆ›æ–°æå¤§åœ°ç®€åŒ–äº†ä¼ ç»Ÿåˆ¶ä½œæ­¤ç±»å†…å®¹çš„å¤æ‚è¿‡ç¨‹ã€‚æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åŠ é€Ÿäº†è¿™ä¸€æ–°å…´é¢†åŸŸçš„å¿«é€Ÿå‘å±•ã€‚è¿™ç¯‡ç»¼è¿°å¯¹æ–‡æœ¬é©±åŠ¨çš„å…¨è§†è§’å…¨æ™¯ç”ŸæˆæŠ€æœ¯è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œæ·±å…¥åˆ†æäº†æœ€å…ˆè¿›ç®—æ³•åŠå…¶åœ¨ç”Ÿæˆå…¨è§†è§’å…¨æ™¯çš„æŠ€æœ¯ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹å½“å‰çš„å±€é™æ€§è¿›è¡Œäº†æ‰¹åˆ¤æ€§å®¡è§†ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„å¸Œæœ›æ–¹å‘ã€‚ç›¸å…³èµ„æºå’Œç ”ç©¶è®ºæ–‡ç²¾é€‰çš„é¡¹ç›®é¡µé¢å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://littlewhitesea.github.io/Text-Driven-Pano-Gen/%E8%AE%BF%E9%97%AE%E3%80%82">https://littlewhitesea.github.io/Text-Driven-Pano-Gen/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14799v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬é©±åŠ¨çš„360åº¦å…¨æ™¯ç”ŸæˆæŠ€æœ¯ä¸ºä»æ–‡æœ¬æè¿°ç›´æ¥åˆæˆ360åº¦å…¨æ™¯å›¾åƒæä¾›äº†å¯èƒ½ï¼Œè¿™æ ‡å¿—ç€æ²‰æµ¸å¼è§†è§‰å†…å®¹åˆ›ä½œé¢†åŸŸçš„ä¸€æ¬¡å˜é©æ€§è¿›å±•ã€‚è¯¥æŠ€æœ¯çš„å‡ºç°ç®€åŒ–äº†ä¼ ç»Ÿå¤æ‚çš„å†…å®¹åˆ¶ä½œè¿‡ç¨‹ã€‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åŠ é€Ÿäº†è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿…é€Ÿå‘å±•ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†æ–‡æœ¬é©±åŠ¨çš„360åº¦å…¨æ™¯ç”ŸæˆæŠ€æœ¯ï¼Œæ·±å…¥åˆ†æäº†æœ€æ–°ç®—æ³•åŠå…¶åœ¨360åº¦ä¸‰ç»´åœºæ™¯ç”Ÿæˆä¸­çš„åº”ç”¨ï¼ŒåŒæ—¶æ‰¹åˆ¤æ€§åœ°æ¢è®¨äº†å½“å‰å±€é™æ€§å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³èµ„æºå’Œç ”ç©¶è®ºæ–‡å¯åœ¨[<a target="_blank" rel="noopener" href="https://littlewhitesea.github.io/Text-Driven-Pano-Gen/]%E6%9F%A5%E9%98%85%E3%80%82">https://littlewhitesea.github.io/Text-Driven-Pano-Gen/]æŸ¥é˜…ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬é©±åŠ¨çš„360åº¦å…¨æ™¯ç”ŸæˆæŠ€æœ¯å®ç°äº†ä»æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆï¼Œç®€åŒ–äº†ä¼ ç»Ÿçš„å†…å®¹åˆ¶ä½œè¿‡ç¨‹ã€‚</li>
<li>æœ€æ–°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•åŠ é€Ÿäº†è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ºæ²‰æµ¸å¼è§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥äº†å˜é©ã€‚</li>
<li>æ–‡ä¸­æ·±å…¥åˆ†æäº†å½“å‰æœ€æ–°çš„ç®—æ³•åŠå…¶åœ¨360åº¦ä¸‰ç»´åœºæ™¯ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚</li>
<li>å½“å‰æŠ€æœ¯è¿˜å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
<li>æ–‡ä¸­æä¾›äº†å…³äºè¯¥é¢†åŸŸçš„èµ„æºå’Œç ”ç©¶è®ºæ–‡çš„é“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc416061417c8cdbce08203d7c61045e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0267dcb4e126e94711a0e038f7a96060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d617da7a8cd53631721c78f1ef68be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cc87f30b49f56596ea943c097d39f5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3992cc225a538b8f306d518a748b9d3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a3e98130d7627097a7029a46817e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63aa688b0ce17574128fb971b5666477.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Masked-Autoencoders-Are-Effective-Tokenizers-for-Diffusion-Models"><a href="#Masked-Autoencoders-Are-Effective-Tokenizers-for-Diffusion-Models" class="headerlink" title="Masked Autoencoders Are Effective Tokenizers for Diffusion Models"></a>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</h2><p><strong>Authors:Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj</strong></p>
<p>Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released. </p>
<blockquote>
<p>æœ€æ–°çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›å±•å·²ç»è¯æ˜å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•é€šè¿‡tokenizerså­¦ä¹ å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ç‰¹æ€§ä»å¾…æ¢ç´¢ã€‚æˆ‘ä»¬ä»ç†è®ºå’Œå®è¯ä¸Šå‘ç°ï¼Œæ›´å¥½çš„ç”Ÿæˆè´¨é‡ä¸å…·æœ‰æ›´å¥½ç»“æ„çš„æ½œåœ¨åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ï¼Œä¾‹å¦‚å…·æœ‰æ›´å°‘çš„é«˜æ–¯æ··åˆæ¨¡å¼å¹¶ä¸”å…·æœ‰æ›´å¤šåˆ¤åˆ«ç‰¹å¾çš„åˆ†å¸ƒã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†MAETokï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ©æ¨¡å»ºæ¨¡çš„è‡ªç¼–ç å™¨ï¼ˆAEï¼‰ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé‡å»ºä¿çœŸåº¦çš„åŒæ—¶å­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„æ½œåœ¨ç©ºé—´ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬åˆ†æçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å˜åˆ†è‡ªç¼–ç å™¨å½¢å¼å¹¶éå¿…éœ€ï¼Œä»…ä½¿ç”¨AEçš„åˆ¤åˆ«æ€§æ½œåœ¨ç©ºé—´å³å¯åœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œä»…ä½¿ç”¨128ä¸ªtokenã€‚MAETokå®ç°äº†é‡è¦çš„å®é™…æ”¹è¿›ï¼Œå®ç°äº†1.69çš„gFIDï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æé«˜äº†76å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†31å€ï¼Œå¯ç”¨äºç”Ÿæˆ512x512çš„å›¾åƒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ½œåœ¨ç©ºé—´çš„ç»“æ„å¯¹äºæœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ï¼Œè€Œéå˜åˆ†çº¦æŸã€‚ä»£ç å’Œè®­ç»ƒæ¨¡å‹å‡å·²å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03444v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ½œæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å·²è¯æ˜å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…³äºä»¤ç‰Œå™¨æ½œåœ¨ç©ºé—´ç‰¹æ€§å¯¹äºæ½œæ‰©æ•£æ¨¡å‹çš„å­¦ä¹ å’Œç”Ÿæˆçš„å½±å“å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡å‘ç°ä¼˜è´¨ç”Ÿæˆä¸å…·æœ‰æ›´å¥½ç»“æ„å’Œæ›´å°‘é«˜æ–¯æ··åˆæ¨¡å¼çš„æ½œåœ¨åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MAETokï¼Œä¸€ç§åˆ©ç”¨æ©æ¨¡å»ºæ¨¡çš„è‡ªç¼–ç å™¨ï¼ˆAEï¼‰ï¼Œæ—¨åœ¨å­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„æ½œåœ¨ç©ºé—´å¹¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬åˆ†æçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å˜åˆ†è‡ªç¼–ç å™¨å½¢å¼å¹¶éå¿…éœ€ï¼Œä»…ä½¿ç”¨AEçš„åˆ¤åˆ«æ€§æ½œåœ¨ç©ºé—´å³å¯åœ¨ImageNetç”Ÿæˆä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚MAETokå–å¾—äº†æ˜¾è‘—çš„å®è·µæ”¹è¿›ï¼Œå®ç°äº†æ›´é«˜çš„ç”Ÿæˆå›¾åƒè´¨é‡å’Œæ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­å±•ç°æœ‰æ•ˆæ€§ã€‚</li>
<li>æ½œåœ¨ç©ºé—´çš„ç»“æ„å’Œæ€§è´¨å¯¹æ½œæ‰©æ•£æ¨¡å‹çš„å­¦ä¹ å’Œç”Ÿæˆè‡³å…³é‡è¦ã€‚</li>
<li>é«˜è´¨é‡ç”Ÿæˆçš„æ½œåœ¨åˆ†å¸ƒå…·æœ‰æ›´å°‘çš„é«˜æ–¯æ··åˆæ¨¡å¼å’Œæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚</li>
<li>MAETokåˆ©ç”¨æ©æ¨¡å»ºæ¨¡çš„è‡ªç¼–ç å™¨ï¼ˆAEï¼‰å­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>å®éªŒè¡¨æ˜å˜åˆ†è‡ªç¼–ç å™¨å½¢å¼å¹¶éå¿…éœ€ï¼Œä»…ä½¿ç”¨AEå³å¯å®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>MAETokåœ¨ImageNetç”Ÿæˆä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬ç”Ÿæˆå›¾åƒè´¨é‡ã€è®­ç»ƒé€Ÿåº¦å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-748771407e505232c49c97fce51fd11c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-456880a0b6c4eea926df19c333db81ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5db41eecd9d506fe7f4124155e2410d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b38b1538970db5b9dced9d05e419de3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18862ca7cb0f1725e215a9b917dda0b5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unpaired-Deblurring-via-Decoupled-Diffusion-Model"><a href="#Unpaired-Deblurring-via-Decoupled-Diffusion-Model" class="headerlink" title="Unpaired Deblurring via Decoupled Diffusion Model"></a>Unpaired Deblurring via Decoupled Diffusion Model</h2><p><strong>Authors:Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang</strong></p>
<p>Generative diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. In favor of their ability to supplement missing details and generate aesthetically pleasing contents, recent works have applied them to image deblurring via training an adapter on blurry-sharp image pairs to provide structural conditions for restoration. However, acquiring substantial amounts of realistic paired data is challenging and costly in real-world scenarios. On the other hand, relying solely on synthetic data often results in overfitting, leading to unsatisfactory performance when confronted with unseen blur patterns. To tackle this issue, we propose UID-Diff, a generative-diffusion-based model designed to enhance deblurring performance on unknown domains by decoupling structural features and blur patterns through joint training on three specially designed tasks. We employ two Q-Formers as structural features and blur patterns extractors separately. The features extracted by them will be used for the supervised deblurring task on synthetic data and the unsupervised blur-transfer task by leveraging unpaired blurred images from the target domain simultaneously. We further introduce a reconstruction task to make the structural features and blur patterns complementary. This blur-decoupled learning process enhances the generalization capabilities of UID-Diff when encountering unknown blur patterns. Experiments on real-world datasets demonstrate that UID-Diff outperforms existing state-of-the-art methods in blur removal and structural preservation in various challenging scenarios. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡æ•°æ®é›†çš„ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç”±äºå…¶èƒ½å¤Ÿè¡¥å……ç¼ºå¤±çš„ç»†èŠ‚å¹¶ç”Ÿæˆç¾è§‚çš„å†…å®¹ï¼Œæœ€è¿‘çš„ç ”ç©¶å°†å…¶åº”ç”¨äºå›¾åƒå»æ¨¡ç³Šï¼Œé€šè¿‡æ¨¡ç³Š-æ¸…æ™°å›¾åƒå¯¹è®­ç»ƒé€‚é…å™¨ï¼Œä¸ºæ¢å¤æä¾›ç»“æ„æ¡ä»¶ã€‚ç„¶è€Œï¼Œåœ¨ç°å®åœºæ™¯ä¸­è·å–å¤§é‡çœŸå®çš„é…å¯¹æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å¦ä¸€æ–¹é¢ï¼Œä»…ä¾èµ–åˆæˆæ•°æ®å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå½“é¢å¯¹æœªçŸ¥çš„æ¨¡ç³Šæ¨¡å¼æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¸å°½äººæ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UID-Diffï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç”Ÿæˆæ‰©æ•£çš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è”åˆè®­ç»ƒä¸‰ä¸ªä¸“é—¨è®¾è®¡çš„ä»»åŠ¡æ¥æé«˜æœªçŸ¥é¢†åŸŸçš„å»æ¨¡ç³Šæ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤ä¸ªQ-Formersåˆ†åˆ«ä½œä¸ºç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼æå–å™¨ã€‚å®ƒä»¬æå–çš„ç‰¹å¾å°†ç”¨äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å»æ¨¡ç³Šä»»åŠ¡ï¼Œå¹¶åŒæ—¶åˆ©ç”¨æ¥è‡ªç›®æ ‡åŸŸçš„æ— é…å¯¹æ¨¡ç³Šå›¾åƒè¿›è¡Œæ— ç›‘ç£æ¨¡ç³Šè½¬ç§»ä»»åŠ¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥é‡å»ºä»»åŠ¡ä½¿ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼äº’è¡¥ã€‚è¿™ç§æ¨¡ç³Šè§£è€¦å­¦ä¹ è¿‡ç¨‹æé«˜äº†UID-Diffåœ¨é‡åˆ°æœªçŸ¥æ¨¡ç³Šæ¨¡å¼æ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUID-Diffåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼Œæ¨¡ç³Šå»é™¤å’Œç»“æ„ä¿ç•™æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01522v2">PDF</a> We propose UID-Diff to integrate generative diffusion model into   unpaired deblurring tasks</p>
<p><strong>Summary</strong><br>     åŸºäºç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹çš„å›¾åƒå»æ¨¡ç³ŠæŠ€æœ¯é€šè¿‡è”åˆè®­ç»ƒä¸‰é¡¹ä»»åŠ¡ä»¥æå‡æœªçŸ¥é¢†åŸŸçš„å»æ¨¡ç³Šæ€§èƒ½ã€‚é€šè¿‡ä¸¤ä¸ªQ-Formersåˆ†åˆ«æå–ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼ï¼Œå¹¶ç”¨äºåˆæˆæ•°æ®çš„ç›‘ç£å»æ¨¡ç³Šä»»åŠ¡å’ŒåŸºäºæœªé…å¯¹æ¨¡ç³Šå›¾åƒçš„æ— ç›‘ç£æ¨¡ç³Šè½¬ç§»ä»»åŠ¡ã€‚åŒæ—¶å¼•å…¥é‡å»ºä»»åŠ¡ä½¿ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼äº’è¡¥ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUID-Diffåœ¨å¤šç§æŒ‘æˆ˜åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨å›¾åƒå»æ¨¡ç³Šæ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>æå‡ºUID-Diffæ¨¡å‹ï¼Œé€šè¿‡è”åˆè®­ç»ƒä¸‰é¡¹ä»»åŠ¡æå‡æœªçŸ¥é¢†åŸŸçš„å»æ¨¡ç³Šæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨ä¸¤ä¸ªQ-Formersåˆ†åˆ«æå–ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼ã€‚</li>
<li>ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼ç”¨äºåˆæˆæ•°æ®çš„ç›‘ç£å»æ¨¡ç³Šä»»åŠ¡å’ŒåŸºäºæœªé…å¯¹æ¨¡ç³Šå›¾åƒçš„æ— ç›‘ç£æ¨¡ç³Šè½¬ç§»ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥é‡å»ºä»»åŠ¡ä»¥ä¼˜åŒ–ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼çš„äº’è¡¥æ€§ã€‚</li>
<li>UID-Diffæ¨¡å‹åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b057e725bc541813c87bdcb6ec0da819.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-046d7925796731878ff06125b34a8712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8123cfc4bd9aed86d909b7c041132fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-743924d2527b165451625817a95d6d6c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space"><a href="#DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space" class="headerlink" title="DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space"></a>DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space</h2><p><strong>Authors:Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin BeneÅ¡, Wenshuo Chen, Albert Ali Salah, Itir Onal Ertugrul</strong></p>
<p>This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to 512$\times$512 resolution without using the latent diffusion paradigm and beats latent diffusion (using SD-VAE) with only 1&#x2F;4 training cost. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why â€˜image diffusion can be seen as spectral autoregressionâ€™, bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is <a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff">https://github.com/forever208/DCTdiff</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†é¢‘ç‡ç©ºé—´çš„å›¾åƒå»ºæ¨¡ï¼Œå¹¶ä»‹ç»äº†DCTdiffè¿™ä¸€ç«¯åˆ°ç«¯çš„æ‰©æ•£ç”ŸæˆèŒƒå¼ï¼Œè¯¥èŒƒå¼åœ¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç©ºé—´ä¸­å¯¹å›¾åƒè¿›è¡Œæœ‰æ•ˆå»ºæ¨¡ã€‚æˆ‘ä»¬ç ”ç©¶äº†DCTdiffçš„è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºäº†å…³é”®çš„è®¾è®¡å› ç´ ã€‚åœ¨ä¸åŒæ¡†æ¶ï¼ˆUViTã€DiTï¼‰ã€ç”Ÿæˆä»»åŠ¡å’Œå¤šç§æ‰©æ•£é‡‡æ ·å™¨ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDCTdiffåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºåƒç´ çš„æ‰©æ•£æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDCTdiffå¯ä»¥æ— ç¼æ‰©å±•åˆ°512Ã—512åˆ†è¾¨ç‡ï¼Œè€Œæ— éœ€ä½¿ç”¨æ½œåœ¨æ‰©æ•£èŒƒå¼ï¼Œå¹¶ä¸”ä»…ä»¥å››åˆ†ä¹‹ä¸€çš„è®­ç»ƒæˆæœ¬å‡»è´¥äº†æ½œåœ¨æ‰©æ•£ï¼ˆä½¿ç”¨SD-VAEï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬è¯´æ˜äº†DCTå›¾åƒå»ºæ¨¡çš„å‡ ä¸ªæœ‰è¶£ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æä¾›äº†ä¸ºä»€ä¹ˆâ€œå›¾åƒæ‰©æ•£å¯ä»¥è¢«è§†ä¸ºè°±è‡ªå›å½’â€çš„ç†è®ºè¯æ˜ï¼Œä»è€Œç¼©å°äº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚DCTdiffçš„æœ‰æ•ˆæ€§å’Œæ‰€å¼•å…¥çš„ç‰¹æ€§ä¸ºé¢‘ç‡ç©ºé—´çš„å›¾åƒå»ºæ¨¡æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff%E3%80%82">https://github.com/forever208/DCTdiffã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15032v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DCTdiffï¼Œè¿™æ˜¯ä¸€ç§åœ¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç©ºé—´ä¸­è¿›è¡Œå›¾åƒå»ºæ¨¡çš„ç«¯åˆ°ç«¯æ‰©æ•£ç”ŸæˆèŒƒå¼ã€‚å®ƒæ¢è®¨äº†DCTdiffçš„è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºäº†å…³é”®è®¾è®¡å› ç´ ã€‚å®éªŒè¡¨æ˜ï¼ŒDCTdiffåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºåƒç´ çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶èƒ½æ— ç¼æ‰©å±•åˆ°512x512åˆ†è¾¨ç‡ã€‚è¿˜æä¾›äº†ä¸€ç§å°†å›¾åƒæ‰©æ•£è§†ä¸ºè°±è‡ªå›å½’çš„ç†è®ºè¯æ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCTdiffæ˜¯ä¸€ç§åœ¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç©ºé—´è¿›è¡Œå›¾åƒå»ºæ¨¡çš„æ‰©æ•£ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>DCTdiffåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºåƒç´ åŸºç¡€ä¸Šçš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>DCTdiffå¯æ— ç¼æ‰©å±•è‡³é«˜åˆ†è¾¨ç‡ï¼Œå¦‚512x512ï¼Œæ— éœ€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ–¹æ³•ã€‚</li>
<li>DCTdiffç›¸å¯¹äºæ½œåœ¨æ‰©æ•£ï¼ˆä½¿ç”¨SD-VAEï¼‰å…·æœ‰æ›´ä½çš„è®­ç»ƒæˆæœ¬ã€‚</li>
<li>DCTå›¾åƒå»ºæ¨¡å…·æœ‰ä¸€äº›æœ‰è¶£ç‰¹æ€§ï¼Œå¦‚å›¾åƒæ‰©æ•£å¯è§†ä¸ºè°±è‡ªå›å½’çš„ç†è®ºè¯æ˜ã€‚</li>
<li>è¯¥è®ºæ–‡æ¢è®¨äº†DCTdiffçš„è®¾è®¡ç©ºé—´ï¼Œå¹¶æ­ç¤ºäº†å…¶å…³é”®è®¾è®¡å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0de78b128b8fda6f9b23f41b233eda94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63c8232df9805a1a30721f23d0948c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f711dcc62c558ba73c2d349de51ff9c5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TrojanEdit-Multimodal-Backdoor-Attack-Against-Image-Editing-Model"><a href="#TrojanEdit-Multimodal-Backdoor-Attack-Against-Image-Editing-Model" class="headerlink" title="TrojanEdit: Multimodal Backdoor Attack Against Image Editing Model"></a>TrojanEdit: Multimodal Backdoor Attack Against Image Editing Model</h2><p><strong>Authors:Ji Guo, Peihong Chen, Wenbo Jiang, Xiaolei Wen, Jiaming He, Jiachen Li, Guoming Lu, Aiguo Chen, Hongwei Li</strong></p>
<p>Multimodal diffusion models for image editing generate outputs conditioned on both textual instructions and visual inputs, aiming to modify target regions while preserving the rest of the image. Although diffusion models have been shown to be vulnerable to backdoor attacks, existing efforts mainly focus on unimodal generative models and fail to address the unique challenges in multimodal image editing. In this paper, we present the first study of backdoor attacks on multimodal diffusion-based image editing models. We investigate the use of both textual and visual triggers to embed a backdoor that achieves high attack success rates while maintaining the modelâ€™s normal functionality. However, we identify a critical modality bias. Simply combining triggers from different modalities leads the model to primarily rely on the stronger one, often the visual modality, which results in a loss of multimodal behavior and degrades editing quality. To overcome this issue, we propose TrojanEdit, a backdoor injection framework that dynamically adjusts the gradient contributions of each modality during training. This allows the model to learn a truly multimodal backdoor that activates only when both triggers are present. Extensive experiments on multiple image editing models show that TrojanEdit successfully integrates triggers from different modalities, achieving balanced multimodal backdoor learning while preserving clean editing performance and ensuring high attack effectiveness. </p>
<blockquote>
<p>åŸºäºå›¾åƒç¼–è¾‘çš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ç”Ÿæˆè¾“å‡ºï¼Œæ—¨åœ¨ä¿®æ”¹ç›®æ ‡åŒºåŸŸçš„åŒæ—¶ä¿ç•™å›¾åƒçš„å…¶ä½™éƒ¨åˆ†ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å·²æ˜¾ç¤ºå‡ºå®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€ç”Ÿæˆæ¨¡å‹ä¸Šï¼Œæœªèƒ½è§£å†³å¤šæ¨¡æ€å›¾åƒç¼–è¾‘ä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºå¤šæ¨¡æ€æ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¸Šçš„åé—¨æ”»å‡»è¿›è¡Œäº†é¦–æ¬¡ç ”ç©¶ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨æ–‡æœ¬å’Œè§†è§‰è§¦å‘å› ç´ æ¥åµŒå…¥åé—¨çš„æ–¹æ³•ï¼Œä»¥å®ç°é«˜æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ä¿æŒæ¨¡å‹çš„æ­£å¸¸åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§å…³é”®çš„æ¨¡æ€åè§ã€‚ç®€å•åœ°ç»“åˆä¸åŒæ¨¡æ€çš„è§¦å‘å™¨ä¼šå¯¼è‡´æ¨¡å‹ä¸»è¦ä¾èµ–äºæ›´å¼ºçš„æ¨¡æ€ï¼Œé€šå¸¸æ˜¯è§†è§‰æ¨¡æ€ï¼Œä»è€Œå¯¼è‡´å¤šæ¨¡æ€è¡Œä¸ºçš„ä¸§å¤±å’Œç¼–è¾‘è´¨é‡çš„ä¸‹é™ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TrojanEditï¼Œä¸€ç§åé—¨æ³¨å…¥æ¡†æ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„æ¢¯åº¦è´¡çŒ®ã€‚è¿™å…è®¸æ¨¡å‹å­¦ä¹ ä¸€ç§çœŸæ­£çš„å¤šæ¨¡æ€åé—¨ï¼Œåªæœ‰åœ¨ä¸¤ç§è§¦å‘å™¨éƒ½å­˜åœ¨æ—¶æ‰ä¼šè¢«æ¿€æ´»ã€‚åœ¨å¤šä¸ªå›¾åƒç¼–è¾‘æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTrojanEditæˆåŠŸåœ°å°†æ¥è‡ªä¸åŒæ¨¡æ€çš„è§¦å‘å™¨é›†æˆåœ¨ä¸€èµ·ï¼Œå®ç°äº†å¹³è¡¡çš„å¤šæ¨¡æ€åé—¨å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒäº†æ¸…æ´ç¼–è¾‘æ€§èƒ½ï¼Œå¹¶ç¡®ä¿é«˜æ”»å‡»æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14681v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒç¼–è¾‘ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ç”Ÿæˆè¾“å‡ºï¼Œæ—¨åœ¨ä¿®æ”¹ç›®æ ‡åŒºåŸŸåŒæ—¶ä¿ç•™å›¾åƒå…¶ä½™éƒ¨åˆ†ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å·²è¢«è¯æ˜å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€ç”Ÿæˆæ¨¡å‹ä¸Šï¼Œæœªèƒ½è§£å†³å¤šæ¨¡æ€å›¾åƒç¼–è¾‘ä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘çš„åé—¨æ”»å‡»é—®é¢˜ã€‚æˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨æ–‡æœ¬å’Œè§†è§‰è§¦å‘æ¥åµŒå…¥åé—¨çš„æ–¹æ³•ï¼Œåœ¨ä¿æŒæ¨¡å‹æ­£å¸¸åŠŸèƒ½çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§å…³é”®æ¨¡æ€åè§ï¼Œå³ç®€å•ç»“åˆä¸åŒæ¨¡æ€çš„è§¦å‘ä¼šä½¿æ¨¡å‹ä¸»è¦ä¾èµ–äºæ›´å¼ºå¤§çš„æ¨¡æ€ï¼ˆé€šå¸¸æ˜¯è§†è§‰æ¨¡æ€ï¼‰ï¼Œå¯¼è‡´å¤šæ¨¡æ€è¡Œä¸ºçš„ä¸§å¤±å’Œç¼–è¾‘è´¨é‡çš„ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TrojanEditï¼Œä¸€ç§åé—¨æ³¨å…¥æ¡†æ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„æ¢¯åº¦è´¡çŒ®ã€‚è¿™å…è®¸æ¨¡å‹å­¦ä¹ ä¸€ç§çœŸæ­£çš„å¤šæ¨¡æ€åé—¨ï¼Œåªæœ‰åœ¨ä¸¤ä¸ªè§¦å‘å™¨éƒ½å­˜åœ¨æ—¶æ‰ä¼šè¢«æ¿€æ´»ã€‚åœ¨å¤šä¸ªå›¾åƒç¼–è¾‘æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTrojanEditæˆåŠŸé›†æˆäº†æ¥è‡ªä¸åŒæ¨¡æ€çš„è§¦å‘å™¨ï¼Œå®ç°äº†å¹³è¡¡çš„å¤šæ¨¡æ€åé—¨å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒäº†æ¸…æ´ç¼–è¾‘æ€§èƒ½ï¼Œç¡®ä¿äº†é«˜æ”»å‡»æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒç¼–è¾‘ï¼Œå¯ä»¥åŸºäºæ–‡æœ¬æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ç”Ÿæˆè¾“å‡ºã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€ç”Ÿæˆæ¨¡å‹ä¸Šçš„åé—¨æ”»å‡»ï¼Œæœªæ¶µç›–å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>å•çº¯ç»“åˆä¸åŒæ¨¡æ€çš„è§¦å‘ä¼šå¯¼è‡´æ¨¡å‹ä¾èµ–äºå¼ºåŠ¿æ¨¡æ€ï¼ˆé€šå¸¸æ˜¯è§†è§‰æ¨¡æ€ï¼‰ï¼Œè¿›è€Œä¸§å¤±å¤šæ¨¡æ€è¡Œä¸ºå¹¶é™ä½ç¼–è¾‘è´¨é‡ã€‚</li>
<li>æå‡ºTrojanEditæ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ä¸åŒæ¨¡æ€çš„æ¢¯åº¦è´¡çŒ®æ¥è®­ç»ƒçœŸæ­£çš„å¤šæ¨¡æ€åé—¨ã€‚</li>
<li>TrojanEditå®ç°äº†ä¸åŒæ¨¡æ€è§¦å‘å™¨çš„é›†æˆï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ¸…æ´ç¼–è¾‘æ€§èƒ½çš„åŒæ—¶ç¡®ä¿é«˜æ”»å‡»æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTrojanEditèƒ½å¤Ÿå¹³è¡¡å¤šæ¨¡æ€åé—¨å­¦ä¹ ï¼ŒåŒæ—¶åœ¨å¤šä¸ªå›¾åƒç¼–è¾‘æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-527a752735d4124db40f932071515ba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75304da302ca06a1defb40dba42b6566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c9720327e07d5f2017e886fb4a0aa2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e08024a0b233a1cde52ece16ac76dcbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d586dad1c7b40c2b0bbb3d32de2df06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8847289f208dcb448bdd6bd33cce6d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931ac4b7d905e3a48896405a95645fb5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner"><a href="#CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner" class="headerlink" title="CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner"></a>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner</h2><p><strong>Authors:Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</strong></p>
<p>We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan">https://github.com/wyysf-98/CraftsMan</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œåä¸ºCraftsManã€‚è¯¥ç³»ç»Ÿå¯ä»¥ç”Ÿæˆå…·æœ‰å¤šæ ·åŒ–å½¢çŠ¶ã€è§„åˆ™ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†è¡¨é¢çš„é«˜ä¿çœŸä¸‰ç»´å‡ ä½•ä½“ï¼Œå¹¶ä¸”ä»¥äº¤äº’æ–¹å¼å¯¹å…¶è¿›è¡Œç»†åŒ–ã€‚å°½ç®¡ä¸‰ç»´ç”ŸæˆæŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ä¼˜åŒ–è¿‡ç¨‹å†—é•¿ã€ç½‘æ ¼æ‹“æ‰‘ä¸è§„åˆ™ã€è¡¨é¢å™ªå£°ä»¥åŠéš¾ä»¥é€‚åº”ç”¨æˆ·ç¼–è¾‘ç­‰é—®é¢˜ï¼Œä»è€Œé˜»ç¢äº†å®ƒä»¬åœ¨ä¸‰ç»´å»ºæ¨¡è½¯ä»¶ä¸­çš„å¹¿æ³›é‡‡ç”¨å’Œå®æ–½ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°å·¥åŒ çš„å¯å‘ï¼Œå·¥åŒ é€šå¸¸é¦–å…ˆå¤§è‡´å‹¾å‹’å‡ºä½œå“çš„æ•´ä½“è½®å»“ï¼Œç„¶åç»†åŒ–è¡¨é¢ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹åŸºäºé›†åˆçš„æ½œåœ¨ä¸‰ç»´è¡¨ç¤ºè¿›è¡Œæ“ä½œï¼Œä»¥åœ¨å‡ ç§’å†…ç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç•¥å‡ ä½•å½¢çŠ¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸€è¿‡ç¨‹ä»¥æ–‡æœ¬æç¤ºæˆ–å‚è€ƒå›¾åƒä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨å¼ºå¤§çš„å¤šè§†å›¾ï¼ˆMVï¼‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆç²—ç•¥å‡ ä½•å½¢çŠ¶çš„å¤šè§†å›¾ï¼Œç„¶åå°†å…¶è¾“å…¥æˆ‘ä»¬çš„MVæ¡ä»¶ä¸‰ç»´æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆä¸‰ç»´å‡ ä½•å½¢çŠ¶ï¼Œä»è€Œå¤§å¤§æé«˜äº†ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ä¹‹åï¼Œä½¿ç”¨åŸºäºæ³•çº¿çš„å‡ ä½•ç»†åŒ–å™¨æ¥æ˜¾ç€å¢å¼ºè¡¨é¢ç»†èŠ‚ã€‚æ­¤ç»†åŒ–è¿‡ç¨‹å¯ä»¥è‡ªåŠ¨è¿›è¡Œï¼Œä¹Ÿå¯ä»¥ä¸ç”¨æˆ·æä¾›çš„ç¼–è¾‘è¿›è¡Œäº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´èµ„äº§æ–¹é¢ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://craftsman3d.github.io/%EF%BC%8C%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/wyysf-98/CraftsMan">https://craftsman3d.github.io/ï¼Œä»£ç ï¼šhttps://github.com/wyysf-98/CraftsMan</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14979v4">PDF</a> HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan3D">https://github.com/wyysf-98/CraftsMan3D</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥é¡¹ç›®æå‡ºäº†ä¸€ç§åä¸ºCraftsMançš„æ–°å‹ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ï¼Œå…·æœ‰å¤šæ ·çš„å½¢çŠ¶ã€è§„åˆ™ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œå¹¶å…è®¸ä»¥äº¤äº’æ–¹å¼è°ƒæ•´å‡ ä½•å½¢çŠ¶ã€‚å®ƒé€šè¿‡åˆ©ç”¨è¿è¡Œåœ¨æ½œåœ¨ç©ºé—´ä¸Šçš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’å†…ç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç³™å‡ ä½•å½¢çŠ¶ã€‚è¯¥é¡¹ç›®è¿˜æä¾›äº†ä¸€ç§åŸºäºæ³•çº¿çš„å‡ ä½•ä¼˜åŒ–å™¨ï¼Œå¯æ˜¾è‘—æé«˜è¡¨é¢ç»†èŠ‚çš„è´¨é‡ï¼Œå¹¶å…è®¸è‡ªåŠ¨æˆ–äº¤äº’å¼åœ°è¿›è¡Œç”¨æˆ·ç¼–è¾‘ã€‚è¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´èµ„äº§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CraftsManæ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ã€‚</li>
<li>è¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤šæ ·å½¢çŠ¶ã€è§„åˆ™ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†è¡¨é¢ç»†èŠ‚çš„ä¸‰ç»´æ¨¡å‹ã€‚</li>
<li>CraftsManå…è®¸ä»¥äº¤äº’æ–¹å¼è°ƒæ•´å‡ ä½•å½¢çŠ¶ï¼Œæé«˜äº†ç”¨æˆ·çš„ç¼–è¾‘ä½“éªŒã€‚</li>
<li>è¯¥ç³»ç»Ÿé‡‡ç”¨åŸºäºæ½œåœ¨ç©ºé—´çš„3Dæ‰©æ•£æ¨¡å‹ï¼Œå¿«é€Ÿç”Ÿæˆç²—ç³™å‡ ä½•å½¢çŠ¶ã€‚</li>
<li>CraftsManåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹æé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨åŸºäºæ³•çº¿çš„å‡ ä½•ä¼˜åŒ–å™¨ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¡¨é¢ç»†èŠ‚çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9239fe621b943fddbed51b9a7c80c11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5bf449708d03a86e4c9efb4e4cd813d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35016a66d5990272953fc5bd05fc344f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a80358eb5cf508aebb5b903dc26a92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f5ed3f1bb0c5beaafe09ef3ebe654e8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><a href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation" class="headerlink" title="TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation"></a>TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation</h2><p><strong>Authors:Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p>
<p>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a â€œScreenwriterâ€, engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the â€œRehearsalâ€. Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the â€œFinal Performanceâ€. With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è¿›å±•èƒ½å¤Ÿä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡ã€ä»¤äººæƒŠå¹çš„å›¾åƒã€‚ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨éœ€æ±‚è¾ƒé«˜çš„å¤šè½®å›¾åƒç”Ÿæˆä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå³å¦‚ä½•åœ¨å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»¥åŠåœ¨å¤šä¸ªäº¤äº’å›åˆä¸­å¯¹åŒä¸€ä¸»é¢˜çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TheaterGenï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ï¼Œæä¾›äº†å¤šè½®å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p>åœ¨æ­¤æ¡†æ¶ä¸­ï¼ŒLLMså……å½“â€œç¼–å‰§â€ï¼Œè¿›è¡Œå¤šè½®äº¤äº’ï¼Œç”Ÿæˆå¹¶ç®¡ç†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æç¤ºå†Œï¼Œå…¶ä¸­åŒ…å«ç›®æ ‡å›¾åƒä¸­æ¯ä¸ªè§’è‰²çš„æç¤ºå’Œå¸ƒå±€è®¾è®¡ã€‚åŸºäºè¿™äº›ï¼ŒTheaterGenç”Ÿæˆè§’è‰²å›¾åƒåˆ—è¡¨å¹¶æå–æŒ‡å¯¼ä¿¡æ¯ï¼Œç±»ä¼¼äºâ€œæ’ç»ƒâ€ã€‚éšåï¼Œé€šè¿‡å°†æç¤ºå†Œå’ŒæŒ‡å¯¼ä¿¡æ¯èå…¥T2Iæ‰©æ•£æ¨¡å‹çš„åå‘å»å™ªè¿‡ç¨‹ï¼ŒTheaterGenç”Ÿæˆæœ€ç»ˆå›¾åƒï¼Œå°±åƒè¿›è¡Œâ€œæœ€ç»ˆè¡¨æ¼”â€ã€‚é€šè¿‡å¯¹æç¤ºå†Œå’Œè§’è‰²å›¾åƒçš„æœ‰æ•ˆç®¡ç†ï¼ŒTheaterGenæ˜¾è‘—æé«˜äº†åˆæˆå›¾åƒçš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚</p>
<p>æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ç”¨åŸºå‡†æµ‹è¯•CMIGBenchï¼ˆä¸€è‡´å¤šè½®å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼‰ï¼ŒåŒ…å«8000ä¸ªå¤šè½®æŒ‡ä»¤ã€‚ä¸åŒäºä¹‹å‰çš„å¤šè½®åŸºå‡†æµ‹è¯•ï¼ŒCMIGBenchä¸ä¼šé¢„å…ˆå®šä¹‰è§’è‰²ã€‚CMIGBenchåŒ…å«äº†æ•…äº‹ç”Ÿæˆå’Œå¤šè½®ç¼–è¾‘ä»»åŠ¡ï¼Œä»¥è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜¾ç¤ºï¼ŒTheaterGenæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚å®ƒåœ¨æœ€å…ˆè¿›çš„Mini DALLE 3æ¨¡å‹ä¸Šæé«˜äº†21%çš„å¹³å‡è§’è‰²é—´ç›¸ä¼¼åº¦å’Œ19%çš„å¹³å‡æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18919v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬ä¸­ä»‹ç»äº†æœ€æ–°æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ï¼Œä½†å¤šè½®å›¾åƒç”Ÿæˆä»é¢ä¸´ä¿æŒå›¾åƒä¸æ–‡æœ¬ä¹‹é—´è¯­ä¹‰ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†è®­ç»ƒå…æ¡†æ¶TheaterGenï¼Œé›†æˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå®ç°å¤šè½®å›¾åƒç”Ÿæˆã€‚é€šè¿‡æ ‡å‡†åŒ–æç¤ºä¹¦ç±ç®¡ç†è§’è‰²å›¾åƒå’ŒæŒ‡å¯¼ä¿¡æ¯ï¼Œæé«˜åˆæˆå›¾åƒçš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚åŒæ—¶å¼•å…¥CMIGBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ•…äº‹ç”Ÿæˆå’Œå¤šè½®ç¼–è¾‘ä»»åŠ¡è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTheaterGenæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†å‰æ²¿Mini DALLE 3æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·å¤‡ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>å¤šè½®å›¾åƒç”Ÿæˆåœ¨å®é™…åœºæ™¯ä¸­éœ€æ±‚é«˜ï¼Œä½†ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>TheaterGenæ¡†æ¶é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè§£å†³å¤šè½®å›¾åƒç”Ÿæˆçš„è¯­ä¹‰ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚</li>
<li>TheaterGené€šè¿‡æ ‡å‡†åŒ–æç¤ºä¹¦ç±ç®¡ç†è§’è‰²å›¾åƒå’ŒæŒ‡å¯¼ä¿¡æ¯ã€‚</li>
<li>CMIGBenchåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°å¤šè½®å›¾åƒç”Ÿæˆæ€§èƒ½ï¼ŒåŒ…æ‹¬æ•…äº‹ç”Ÿæˆå’Œå¤šè½®ç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºTheaterGenæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-304858bb178dc2e2427571760aa1fd33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2043c33ab09f9b1c4433b964015247c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-568b0d8a34639fe3e5425bc5cb460f4b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0f8389b7acacf76f1ffb4b87b162eef3.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  VideoCAD A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-19585b96692f0456684962692487eea5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  ErpGS Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
