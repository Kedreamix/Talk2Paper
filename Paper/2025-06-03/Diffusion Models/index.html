<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-03  AdaHuman Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-18862ca7cb0f1725e215a9b917dda0b5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    73 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-03-更新"><a href="#2025-06-03-更新" class="headerlink" title="2025-06-03 更新"></a>2025-06-03 更新</h1><h2 id="AdaHuman-Animatable-Detailed-3D-Human-Generation-with-Compositional-Multiview-Diffusion"><a href="#AdaHuman-Animatable-Detailed-3D-Human-Generation-with-Compositional-Multiview-Diffusion" class="headerlink" title="AdaHuman: Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion"></a>AdaHuman: Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion</h2><p><strong>Authors:Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal</strong></p>
<p>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes. </p>
<blockquote>
<p>当前用于图像到3D化身生成的方法难以生成适用于真实世界应用的高度详细的动画化身。我们引入了AdaHuman，这是一个新型框架，可以从单张野外图像生成高度逼真的动画3D化身。AdaHuman有两个主要创新点：（1）姿态调节的3D关节扩散模型，该模型在任意姿态下合成一致的多视角图像，并在每个扩散步骤中进行相应的3D高斯Splats（3DGS）重建；（2）组合式3DGS优化模块，通过图像到图像的细化增强局部身体部位的细节，并使用新型裁剪感知相机射线图无缝集成它们，生成连贯的详细3D化身。这些组件使得AdaHuman能够生成高度逼真的标准化A姿态化身，最小限度地实现自我遮挡，并能够实现与任何输入动作进行绑定和动画。在公共基准测试和野外图像上的广泛评估表明，AdaHuman在化身重建和重新定位方面均显著优于最新技术方法。为了研究目的，我们将公开提供代码和模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24877v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://nvlabs.github.io/AdaHuman">https://nvlabs.github.io/AdaHuman</a></p>
<p><strong>Summary</strong></p>
<p>AdaHuman是一个从单张图像生成高保真动画3D角色的新框架。它采用姿态条件化三维关节扩散模型和三维高斯插值重构技术，通过图像到图像的细化过程，无缝集成各部分细节，生成连贯且详细的3D角色。AdaHuman能生成高度逼真的标准化A姿态角色，最小限度地实现自我遮挡，并能与任何输入动作进行匹配和动画化。其在公共基准测试和真实图像上的表现显著优于当前的主流方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaHuman使用新颖的框架从单张图像生成动画式3D角色。</li>
<li>该框架包含姿态条件化三维关节扩散模型，能合成任意姿态下的多视角图像。</li>
<li>AdaHuman利用三维高斯插值（3DGS）进行每一步的扩散重构。</li>
<li>引入了图像到图像的细化模块来增强局部细节，并采用创新的农作物感知相机射线映射无缝集成这些细节。</li>
<li>AdaHuman可以生成标准化的高度逼真的A姿态角色，最小化自我遮挡。</li>
<li>AdaHuman的角色易于装备和动画化，能与任何输入动作匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24877">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7ab9ddfefee8cd2d8f36acdf2f9bf0d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a77b746de3d7652c2ae2cfe485133f78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-170ee6183a05b1cc549d259f9390e0d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf7f1b7c9fe3bc0f4ae482d4ece2594.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning"><a href="#Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning" class="headerlink" title="Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning"></a>Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning</h2><p><strong>Authors:Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</strong></p>
<p>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs. </p>
<blockquote>
<p>稀疏自动编码器是一种很有前途的新方法，用于分解语言模型的激活进行解释和控制。它们已成功应用于视觉转换器图像编码器和小型扩散模型。激活推理时间分解（ITDA）是字典学习的一个新近提出的变体，它将字典视为激活分布中的数据点集，并通过梯度追求进行重建。我们将稀疏自动编码器（SAE）和ITDA应用于大型文本到图像的扩散模型Flux 1，并通过引入视觉自动化解释管道考虑两者的嵌入解释性。我们发现SAE能够准确重建残差流嵌入，在解释性方面优于MLP神经元。我们能够使用SAE特征通过激活添加来引导图像生成。我们发现ITDA的解释性与SAE相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24360v1">PDF</a> 10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR   2025</p>
<p><strong>Summary</strong></p>
<p>稀疏自编码器是分解语言模型激活以进行解释和控制的一种有前途的新方法。已成功应用于视觉转换器图像编码器和小型扩散模型。我们将其应用于大型文本到图像的扩散模型Flux 1，并通过引入视觉自动化解释管道来考虑两者的可解释性。我们发现稀疏自编码器能够准确重建残差流嵌入，并在可解释性方面优于MLP神经元。我们还能够使用SAE特征通过激活添加来引导图像生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稀疏自编码器（SAE）在分解语言模型激活方面表现出良好的潜力，特别是在大型文本到图像的扩散模型中。</li>
<li>通过引入视觉自动化解释管道，可以进一步提高模型的可解释性。</li>
<li>SAE能够准确重建残差流嵌入，优于MLP神经元在解释嵌入方面的性能。</li>
<li>使用SAE特征可以在图像生成过程中进行激活添加，实现对图像生成的引导。</li>
<li>ITDA（推理时间激活分解）在可解释性与SAE相当。</li>
<li>SAE和ITDA的应用为语言模型的理解和控制在解释性和控制方面提供了新的视角和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1c22696a928956243b2bbc08ee7e6ba3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5af5085d406ff9dd99f7a78d6879004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff2737bf419a0bcca28f1acc438e1a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-464854384adbd7779b5f8aa7619f4926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e45deecaa551bc541ad2a68085ab6fe3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46be4dcec21f8eb73a7ad696931c864f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InteractAnything-Zero-shot-Human-Object-Interaction-Synthesis-via-LLM-Feedback-and-Object-Affordance-Parsing"><a href="#InteractAnything-Zero-shot-Human-Object-Interaction-Synthesis-via-LLM-Feedback-and-Object-Affordance-Parsing" class="headerlink" title="InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM   Feedback and Object Affordance Parsing"></a>InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM   Feedback and Object Affordance Parsing</h2><p><strong>Authors:Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang</strong></p>
<p>Recent advances in 3D human-aware generation have made significant progress. However, existing methods still struggle with generating novel Human Object Interaction (HOI) from text, particularly for open-set objects. We identify three main challenges of this task: precise human-object relation reasoning, affordance parsing for any object, and detailed human interaction pose synthesis aligning description and object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework without training on specific datasets, leveraging the knowledge from large-scale pre-trained models. Specifically, the human-object relations are inferred from large language models (LLMs) to initialize object properties and guide the optimization process. Then we utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. The initial human pose is generated by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. Finally, we introduce a detailed optimization to generate fine-grained, precise, and natural interaction, enforcing realistic 3D contact between the 3D object and the involved body parts, including hands in grasping. This is achieved by distilling human-level feedback from LLMs to capture detailed human-object relations from the text instruction. Extensive experiments validate the effectiveness of our approach compared to prior works, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects. </p>
<blockquote>
<p>关于三维人形感知生成领域最近取得了显著进展。然而，现有方法仍难以根据文本生成新型的人-物交互（HOI），尤其是开放集物体。我们确定了该任务面临三大挑战：精准的人-物关系推理、任意对象的可操控性解析以及符合描述和物体几何特性的详细人类交互姿态合成。在这项工作中，我们提出了一种新型的零样本三维HOI生成框架，无需在特定数据集上进行训练，而是借助大规模预训练模型的知识。具体而言，我们从大型语言模型（LLM）推断出人-物关系，以初始化物体属性并引导优化过程。然后，我们利用预训练的二维图像扩散模型来分析未见过的物体并提取接触点，避免了现有三维资产知识的限制。基于输入文本和物体几何特性，通过多视角SDS生成初始人类姿态。最后，我们进行了详细的优化，以生成精细、精准、自然的交互，强制要求三维物体与所涉及身体部位之间的现实接触，包括抓握时的手部。这是通过从LLM中提取人类反馈来实现从文本指令中捕获详细的人-物关系。大量实验验证了我们的方法与先前工作相比的有效性，特别是在交互的精细性质和处理开放集三维物体的能力方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24315v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     近期三维人类感知生成技术取得进展，但仍存在从文本生成新型人机交互（HOI）的难题，特别是在开放集对象上。本文提出一种零样本三维HOI生成框架，利用大规模预训练模型的知识，无需在特定数据集上进行训练。通过语言模型推断人类与对象的关系，利用预训练的二维图像扩散模型解析未见过的对象并提取接触点，避免现有三维资产知识的限制。通过多视角SDS生成初始人类姿势，并引入详细优化生成精细、精确和自然交互。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期三维人类感知生成技术有进展，但仍面临从文本生成新型人机交互的挑战，特别是在开放集对象上。</li>
<li>本文提出一种零样本三维HOI生成框架，无需特定数据集训练。</li>
<li>利用语言模型推断人类与对象的关系，初始化对象属性并引导优化过程。</li>
<li>采用预训练的二维图像扩散模型解析未见过的对象，提取接触点。</li>
<li>通过多视角SDS生成初始人类姿势。</li>
<li>引入详细优化生成精细、精确和自然交互，实现三维对象与身体部位的现实接触，包括手部抓握。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1e6ebf99c2e96da34dd93f7fbf957195.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5c012b8b2227a828bfd2a5e9b149185.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-300f7c1cefb2632978a185a8f07fd73f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ea9fddedef7ca01137d6500e32adb6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Category-aware-EEG-image-generation-based-on-wavelet-transform-and-contrast-semantic-loss"><a href="#Category-aware-EEG-image-generation-based-on-wavelet-transform-and-contrast-semantic-loss" class="headerlink" title="Category-aware EEG image generation based on wavelet transform and   contrast semantic loss"></a>Category-aware EEG image generation based on wavelet transform and   contrast semantic loss</h2><p><strong>Authors:Enshang Zhang, Zhicheng Zhang, Takashi Hanakawa</strong></p>
<p>Reconstructing visual stimuli from EEG signals is a crucial step in realizing brain-computer interfaces. In this paper, we propose a transformer-based EEG signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating mechanism. Guided by the feature alignment and category-aware fusion losses, this encoder is used to extract features related to visual stimuli from EEG signals. Subsequently, with the aid of a pre-trained diffusion model, these features are reconstructed into visual stimuli. To verify the effectiveness of the model, we conducted EEG-to-image generation and classification tasks using the THINGS-EEG dataset. To address the limitations of quantitative analysis at the semantic level, we combined WordNet-based classification and semantic similarity metrics to propose a novel semantic-based score, emphasizing the ability of our model to transfer neural activities into visual representations. Experimental results show that our model significantly improves semantic alignment and classification accuracy, which achieves a maximum single-subject accuracy of 43%, outperforming other state-of-the-art methods. The source code and supplementary material is available at <a target="_blank" rel="noopener" href="https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main">https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main</a>. </p>
<blockquote>
<p>从脑电图信号重建视觉刺激是实现脑机接口的关键步骤。在本文中，我们提出了一种基于变压器的脑电图信号编码器，该编码器结合了离散小波变换（DWT）和门控机制。在特征对齐和类别感知融合损失的指导下，该编码器用于从脑电图信号中提取与视觉刺激相关的特征。随后，借助预先训练的扩散模型，这些特征被重建为视觉刺激。为了验证模型的有效性，我们在THINGS-EEG数据集上进行了脑电图到图像生成和分类任务。为了解决语义层面定量分析的限制，我们结合了基于WordNet的分类和语义相似性度量，提出了一种新的基于语义的评分，强调我们的模型将神经活动转化为视觉表示的能力。实验结果表明，我们的模型在语义对齐和分类精度上有了显著提高，实现了最高单用户准确率为43%，超过了其他最先进的方法。源代码和补充材料可在<a target="_blank" rel="noopener" href="https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zes0v0inn/DWT_EEG_Reconstruction&#x2F;tree&#x2F;main找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24301v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于脑电图信号重建视觉刺激是实现脑机接口的重要步骤。本研究提出一种结合离散小波变换和门控机制的基于转换器的脑电图信号编码器。通过特征对齐和类别感知融合损失指导，该编码器从脑电图信号中提取与视觉刺激相关的特征。借助预训练的扩散模型，这些特征被重建为视觉刺激。利用THINGS-EEG数据集进行脑电图到图像生成和分类任务，验证了模型的有效性。为解决语义层面定量分析的限制，本研究结合WordNet分类和语义相似性度量，提出一种新的基于语义的评分方法，强调模型将神经活动转化为视觉表征的能力。实验结果表明，该模型在语义对齐和分类精度上显著提高，达到最高单主体准确率43%，优于其他最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究利用变压器架构的编码器处理EEG信号，结合离散小波变换和门机制，以提取与视觉刺激相关的特征。</li>
<li>编码器在特征对齐和类别感知融合损失的指导下工作，以提高模型性能。</li>
<li>利用预训练的扩散模型将提取的特征重建为视觉刺激，实现脑机接口的重要步骤。</li>
<li>使用THINGS-EEG数据集进行EEG到图像生成和分类任务来验证模型的有效性。</li>
<li>为克服语义层面定量分析的局限性，结合WordNet分类和语义相似性度量，提出新的语义评分方法。</li>
<li>实验结果显示，模型在语义对齐和分类精度上表现优异，达到最高单主体准确率43%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aeeded49f0a5239b834e17f48415bc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0dcb6864fef38b43430f488f06abbc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e200d5e44631f512582c7c3aaedc92e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b6017249bbf704e65cb1ec695b904c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-AI-for-Urban-Design-A-Stepwise-Approach-Integrating-Human-Expertise-with-Multimodal-Diffusion-Models"><a href="#Generative-AI-for-Urban-Design-A-Stepwise-Approach-Integrating-Human-Expertise-with-Multimodal-Diffusion-Models" class="headerlink" title="Generative AI for Urban Design: A Stepwise Approach Integrating Human   Expertise with Multimodal Diffusion Models"></a>Generative AI for Urban Design: A Stepwise Approach Integrating Human   Expertise with Multimodal Diffusion Models</h2><p><strong>Authors:Mingyi He, Yuebing Liang, Shenhao Wang, Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Li Tian, Jinhua Zhao</strong></p>
<p>Urban design is a multifaceted process that demands careful consideration of site-specific constraints and collaboration among diverse professionals and stakeholders. The advent of generative artificial intelligence (GenAI) offers transformative potential by improving the efficiency of design generation and facilitating the communication of design ideas. However, most existing approaches are not well integrated with human design workflows. They often follow end-to-end pipelines with limited control, overlooking the iterative nature of real-world design. This study proposes a stepwise generative urban design framework that integrates multimodal diffusion models with human expertise to enable more adaptive and controllable design processes. Instead of generating design outcomes in a single end-to-end process, the framework divides the process into three key stages aligned with established urban design workflows: (1) road network and land use planning, (2) building layout planning, and (3) detailed planning and rendering. At each stage, multimodal diffusion models generate preliminary designs based on textual prompts and image-based constraints, which can then be reviewed and refined by human designers. We design an evaluation framework to assess the fidelity, compliance, and diversity of the generated designs. Experiments using data from Chicago and New York City demonstrate that our framework outperforms baseline models and end-to-end approaches across all three dimensions. This study underscores the benefits of multimodal diffusion models and stepwise generation in preserving human control and facilitating iterative refinements, laying the groundwork for human-AI interaction in urban design solutions. </p>
<blockquote>
<p>城市设计是一个涉及多方面的过程，需要仔细考虑特定地点的限制因素以及不同专业人士和利益相关者之间的合作。生成式人工智能（GenAI）的出现，通过提高设计生成的效率和促进设计思想的交流，提供了变革的潜力。然而，大多数现有方法并没有很好地与人类设计工作流程相结合。它们通常遵循端到端的管道，控制有限，忽视了现实世界设计的迭代性质。本研究提出了一种分步生成的城市设计框架，该框架将多模式扩散模型与人类专业知识相结合，使设计过程更加适应和可控。不同于在单一端到端过程中产生设计结果，该框架将过程分为三个关键阶段，与既定的城市设计工作流程相一致：（1）道路网络和土地利用规划；（2）建筑布局规划；（3）详细规划和呈现。在每个阶段，多模式扩散模型根据文本提示和图像约束生成初步设计，然后可以由人类设计师进行审查和修改。我们设计了一个评估框架，以评估生成设计的保真度、合规性和多样性。使用芝加哥和纽约市数据的实验表明，我们的框架在所有三个维度上都优于基线模型和端到端方法。该研究强调了多模式扩散模型和分步生成在保持人类控制和促进迭代改进方面的益处，为城市设计中的人机交互奠定了基石。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24260v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究提出了一种分步生成的城市设计框架，该框架结合了多模态扩散模型与人类专业知识，使设计过程更加适应并可控。框架将城市设计过程分为三个阶段，每个阶段都使用多模态扩散模型基于文本提示和图像约束生成初步设计，然后可由人类设计师进行审查和修改。实验表明，该框架在保真度、合规性和设计的多样性方面优于基准模型和端到端方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>城市设计是一个涉及多方因素和专业的复杂过程，需要综合考虑现场特定约束和多方合作。</li>
<li>生成式人工智能（GenAI）的出现在城市设计中提供了改善设计生成效率和促进设计理念沟通的可能性。</li>
<li>当前大多数人工智能设计流程未能很好地融入人类设计工作流程，缺乏灵活性且难以适应迭代设计的需求。</li>
<li>研究提出了一种分步生成的城市设计框架，整合了多模态扩散模型与人类专业知识，以应对以上问题。</li>
<li>该框架将城市设计过程分为三个阶段：道路网络及土地利用规划、建筑布局规划、详细规划和渲染。</li>
<li>在每个阶段，利用多模态扩散模型基于文本提示和图像约束生成初步设计，为后续的人类设计师审查和修改提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24260">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-631ce3776205d86950d210c419dad034.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbac7c2b6bf958b88cc75ef8e5c17c23.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="STORK-Improving-the-Fidelity-of-Mid-NFE-Sampling-for-Diffusion-and-Flow-Matching-Models"><a href="#STORK-Improving-the-Fidelity-of-Mid-NFE-Sampling-for-Diffusion-and-Flow-Matching-Models" class="headerlink" title="STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow   Matching Models"></a>STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow   Matching Models</h2><p><strong>Authors:Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu</strong></p>
<p>Diffusion models (DMs) have demonstrated remarkable performance in high-fidelity image and video generation. Because high-quality generations with DMs typically require a large number of function evaluations (NFEs), resulting in slow sampling, there has been extensive research successfully reducing the NFE to a small range (&lt;10) while maintaining acceptable image quality. However, many practical applications, such as those involving Stable Diffusion 3.5, FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve superior results, and, despite the practical relevance, research on the effective sampling within this mid-NFE regime remains underexplored. In this work, we propose a novel, training-free, and structure-independent DM ODE solver called the Stabilized Taylor Orthogonal Runge–Kutta (STORK) method, based on a class of stiff ODE solvers with a Taylor expansion adaptation. Unlike prior work such as DPM-Solver, which is dependent on the semi-linear structure of the DM ODE, STORK is applicable to any DM sampling, including noise-based and flow matching-based models. Within the 20-50 NFE range, STORK achieves improved generation quality, as measured by FID scores, across unconditional pixel-level generation and conditional latent-space generation tasks using models like Stable Diffusion 3.5 and SANA. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK">https://github.com/ZT220501/STORK</a>. </p>
<blockquote>
<p>扩散模型（DMs）在高保真图像和视频生成方面表现出卓越的性能。由于使用DMs生成高质量图像通常需要大量的函数评估（NFE），导致采样速度较慢，因此已有大量研究成功将NFE降低到一个较小的范围（&lt;10）同时保持可接受的图像质量。然而，许多实际应用，如涉及Stable Diffusion 3.5、FLUX和SANA的应用，通常在中期NFE范围（20-50 NFE）内运行，以取得更好的结果。尽管这种做法具有实际意义，但关于此中期NFE范围内有效采样的研究仍然被探索得不够。在这项工作中，我们提出了一种新型、无需训练且独立于结构之外的DM ODE求解器，称为稳定化的泰勒正交龙格-库塔（STORK）方法，它基于一类带泰勒展开适应性的刚性的ODE求解器。与先前的工作（如DPM-Solver）不同，后者依赖于DM ODE的半线性结构，STORK可应用于任何DM采样，包括基于噪声和基于流匹配模型的采样。在20-50 NFE范围内，STORK在无条件像素级生成和有条件潜在空间生成任务中，使用Stable Diffusion 3.5和SANA等模型时，实现了改进的生成质量，该质量由FID分数衡量。代码可访问于<a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK%E3%80%82">https://github.com/ZT220501/STORK。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24210v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型（DMs）在高保真图像和视频生成方面表现出卓越的性能。然而，由于其通常需要大量的功能评估（NFEs）来生成高质量图像，导致采样速度慢，因此研究者们一直致力于减少NFEs至较小范围并保持可接受的图像质量。尽管已有研究将NFE成功降至个位数，但在实际应用中，如Stable Diffusion 3.5、FLUX和SANA等，通常在中期NFE范围内（20-50 NFE）运行以获得更好的结果。本文提出了一种新型、无需训练且独立于结构之外的扩散模型ODE求解器——稳定泰勒正交龙格库塔（STORK）方法，该方法基于一类刚性的ODE求解器并融合了泰勒展开适应技术。与依赖DM ODE半线性结构的DPM-Solver不同，STORK适用于任何DM采样，包括基于噪声和流匹配模型。在20-50 NFE范围内，STORK在无条件像素级生成和条件潜在空间生成任务上实现了更高的生成质量，使用如Stable Diffusion 3.5和SANA等模型时，其FID得分有所提高。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZT220501/STORK获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在高保真图像和视频生成中表现出卓越性能。</li>
<li>目前研究主要集中在减少扩散模型的NFE至较小范围以保持图像质量，但中期NFE范围内的有效采样研究仍较少。</li>
<li>本文提出了一种新型、无需训练的扩散模型ODE求解器——STORK方法，适用于任何DM采样，包括基于噪声和流匹配模型。</li>
<li>在中期NFE范围内（20-50 NFE），STORK方法在无条件像素级生成和条件潜在空间生成任务上实现了更高的生成质量。</li>
<li>与现有方法相比，STORK方法在图像质量评估指标FID得分上有所提升。</li>
<li>STORK方法的实现代码已公开提供，方便其他研究者使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a28809a7741e5219671150a00c66b72d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cfadecfc4ad08528ba2fb5f8a8d4c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee12ff06a44e5353522589961d830844.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>在这份报告中，我们介绍了OpenUni，这是一个简单、轻量级、完全开源的多模态理解和生成统一基准。受到当前统一模型学习实践的启发，我们采用了一种高效的训练策略，通过一组可学习的查询和一个轻量级的基于变压器的连接器，将现成的多模态大型语言模型（LLMs）和扩散模型连接起来，从而最小化训练复杂性和开销。通过选择最简单的架构，我们证明OpenUni可以：1）生成高质量且符合指令的图像；2）在GenEval、DPG-Bench和WISE等标准基准测试中实现卓越性能，激活的参数只有1.1B和3.1B。为了支持开放研究和社区发展，我们在<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>上发布了所有模型权重、训练代码和我们精选的训练数据集（包括2300万张图像文本对）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了OpenUni，一个简单、轻量级、完全开源的多模态理解和生成统一基准。它通过高效的训练策略，采用可学习的查询和轻量级基于变压器的连接器，将现成的多模态大型语言模型和扩散模型连接起来，降低了训练复杂性和开销。OpenUni能够生成高质量、指令对齐的图像，并在GenEval、DPG-Bench和WISE等标准基准测试中实现卓越性能，仅使用1.1B和3.1B激活参数。所有模型权重、训练代码和精选的训练数据集（包括2.3亿图像文本对）已在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenUni是一个简单、轻量级、完全开源的多模态理解和生成统一模型。</li>
<li>它通过桥接现成的多模态大型语言模型和扩散模型，实现了高效训练。</li>
<li>采用可学习的查询和轻量级基于变压器的连接器来降低训练复杂性和开销。</li>
<li>OpenUni可以生成高质量、指令对齐的图像。</li>
<li>它在标准基准测试中实现了卓越性能，如GenEval、DPG-Bench和WISE。</li>
<li>该模型仅使用1.1B和3.1B激活参数，表现出强大的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d56dc7306e5f25558cee0e232e83a179.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4660fa82c63c1a45604f8604520060af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d31729f8a2776c5c424dbbd0e69f52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-215343cec7a0cfdac617ff0769b3ee6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d96e785d6599c1f0f5c00a115dab2434.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape"><a href="#Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape" class="headerlink" title="Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape"></a>Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape</h2><p><strong>Authors:Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu</strong></p>
<p>Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V&#x2F;T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45% end-to-end % and over 92% self-attention latency reduction on an H100 GPU at negligible overhead cost.   Code available online here: \href{<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention%7D%7Bhttps://github.com/cccrrrccc/Re-ttention%7D">https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}</a> </p>
<blockquote>
<p>扩散Transformer（DiT）已成为生成高质量视觉内容（如视频和图像）的默认模型。一个巨大的瓶颈是注意力机制，其中复杂性随分辨率和视频长度的增加而二次方增长。减轻这一负担的一种逻辑方法是稀疏注意力，其中仅将一部分标记或补丁包含在计算中。然而，现有技术在极高的稀疏度水平上无法保持视觉质量，并可能产生不可忽略的计算开销。为了解决这一担忧，我们提出了Re-ttention，它通过利用扩散模型的时空冗余性，实现了针对视觉生成模型的高稀疏注意力机制，克服了注意力机制中的概率归一化转移问题。具体来说，Re-ttention根据先前的softmax分布历史重新调整注意力分数，以在极高的稀疏度水平上保持全二次注意力的视觉质量。在T2V&#x2F;T2I模型（如CogVideoX和PixArt DiTs）上的实验结果表明，Re-ttention在推理过程中只需要3.1%的标记，超越了FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，我们通过测量延迟时间证明，该方法可在H100 GPU上实现超过45%的端到端延迟减少和超过92%的自注意力延迟减少，同时几乎不增加额外成本。相关代码已在线发布：<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention">https://github.com/cccrrrccc/Re-ttention</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22918v2">PDF</a> Submitted before obtaining agreement of all authors</p>
<p><strong>Summary</strong><br>     扩散转换器（DiT）已成为生成高质量视觉内容（如视频和图像）的标准模型。当前面临的主要瓶颈是注意力机制，其复杂性随分辨率和视频长度的增加而呈二次方增长。为减轻这一负担，人们尝试采用稀疏注意力法，仅包含部分令牌或补丁进行计算。然而，现有技术在极高的稀疏水平下无法保持视觉质量，并可能产生不可忽略的计算开销。为解决这一问题，我们提出了Re-ttention，利用扩散模型的时间冗余性，通过克服注意力机制内的概率归一化偏移，为视觉生成模型实现了极高的稀疏注意力。Re-ttention根据先前的softmax分布历史重塑注意力分数，以在极高的稀疏水平上保持全二次注意力的视觉质量。在T2V&#x2F;T2I模型（如CogVideoX和PixArt DiTs）上的实验结果表明，Re-ttention在推理过程中仅需使用3.1%的令牌，超越了FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，我们测量了延迟时间，证明该方法可在H100 GPU上实现超过45%的端到端延迟减少和超过92%的自注意力延迟减少，且几乎不增加开销。相关代码已在线发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformers (DiT) 是生成高质量视觉内容的主导模型。</li>
<li>当前面临的主要挑战是注意力机制的复杂性，它随分辨率和视频长度的增加而增长。</li>
<li>为减轻负担，提出了稀疏注意力方法，但现有技术在高稀疏度下存在视觉质量损失和计算开销问题。</li>
<li>Re-ttention 利用扩散模型的时间冗余性来解决这个问题，通过重塑注意力分数来在极高稀疏度下保持视觉质量。</li>
<li>Re-ttention 在T2V&#x2F;T2I模型上的实验表现优越，显著减少了令牌使用。</li>
<li>Re-ttention 能显著减少端到端和自注意力的延迟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a75cd7f47adc14567e99e44316bdc5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30216dafe9bcff6b84a9b4339723b5f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7905eafd41f579c842d54bda2d8a4f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71d2f80fd6570fad70cdaf1f531b0ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9478fd57d613165c33ec1defc3bccfb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiffDecompose-Layer-Wise-Decomposition-of-Alpha-Composited-Images-via-Diffusion-Transformers"><a href="#DiffDecompose-Layer-Wise-Decomposition-of-Alpha-Composited-Images-via-Diffusion-Transformers" class="headerlink" title="DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via   Diffusion Transformers"></a>DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via   Diffusion Transformers</h2><p><strong>Authors:Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song</strong></p>
<p>Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent&#x2F;transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/Wangzt1121/DiffDecompose">https://github.com/Wangzt1121/DiffDecompose</a>. </p>
<blockquote>
<p>扩散模型在许多生成任务（如对象移除）中取得了巨大成功。然而，现有的图像分解方法在解决半透明或透明层遮挡问题时遇到了困难，这是由于遮罩先验依赖、静态对象假设以及数据集缺乏所导致的。在本文中，我们深入研究了一项新任务：逐层分解Alpha合成图像，旨在从单个重叠图像中恢复构成层，条件为半透明&#x2F;透明Alpha层的非线性遮挡。为了应对层模糊、泛化能力和数据稀缺等挑战，我们首先引入了AlphaBlend数据集，这是首个用于透明和半透明层分解的大规模高质量数据集，支持六个真实世界子任务（例如，半透明炫光去除、半透明细胞分解、玻璃器皿分解等）。基于该数据集，我们提出了DiffDecompose，这是一个基于扩散Transformer的框架，它学习在输入图像、语义提示和混合类型条件下的可能层分解的后验概率。DiffDecompose不是直接回归alpha遮罩，而是进行上下文分解，使模型能够在无需每层监督的情况下预测一个或多个层，并引入了层位置编码克隆以保持层之间的像素级对应关系。在提出的AlphaBlend数据集和公共LOGO数据集上的广泛实验验证了DiffDecompose的有效性。论文接受后，代码和数据集将可供使用。我们的代码可用链接：<a target="_blank" rel="noopener" href="https://github.com/Wangzt1|">https://github.com/Wangzt1|</a> 会增加标点符号割裂语境或者信息分割不明等负面影响，所以在实际翻译过程中一般会尽量保持语句的完整性。下面是经过翻译的文本：</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21541v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文研究了基于扩散模型的图像分解新技术，特别是针对半透明或透明图层遮挡的分解问题。文章引入了一个新的任务：逐层分解Alpha合成图像，旨在从单一重叠图像中恢复构成图层。为解决图层模糊、通用性及数据稀缺等挑战，文章首先推出了AlphaBlend数据集，支持六个真实世界的子任务。基于该数据集，提出了一种基于扩散Transformer的DiffDecompose框架，学习根据输入图像、语义提示和混合类型进行可能的图层分解的后验分布。DiffDecompose采用上下文分解方式，无需每层监督即可预测一层或多层，并引入层位置编码克隆以维持层间的像素级对应关系。在AlphaBlend和公开LOGO数据集上的实验验证了DiffDecompose的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>引入了一种新的任务：逐层分解Alpha合成的图像，目标是恢复由半透明或透明图层遮挡构成的图层。</li>
<li>针对这一任务，文章提出了AlphaBlend数据集，支持六个真实世界的子任务，为图像分解提供了丰富的数据资源。</li>
<li>介绍了基于扩散模型的DiffDecompose框架，该框架可以学习根据输入图像、语义提示和混合类型进行可能的图层分解的后验分布。</li>
<li>DiffDecompose采用上下文分解方法，能预测一层或多层而无需每层都有直接监督。</li>
<li>引入了层位置编码克隆技术，保持各层之间的像素级对应关系。</li>
<li>在AlphaBlend和公开LOGO数据集上的实验表明，DiffDecompose技术效果显著。</li>
<li>论文接受后，将公开代码和数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-54a2270aeefc9c9c9c45452a34aef8e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a025374db415838d9736299af6539419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62f0d30b704a0fef321d151c257ec5ab.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models"><a href="#T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models" class="headerlink" title="T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models"></a>T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models</h2><p><strong>Authors:Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li</strong></p>
<p>Recent advances in text-to-video (T2V) diffusion models have significantly enhanced the quality of generated videos. However, their ability to produce explicit or harmful content raises concerns about misuse and potential rights violations. Inspired by the success of unlearning techniques in erasing undesirable concepts from text-to-image (T2I) models, we extend unlearning to T2V models and propose a robust and precise unlearning method. Specifically, we adopt negatively-guided velocity prediction fine-tuning and enhance it with prompt augmentation to ensure robustness against LLM-refined prompts. To achieve precise unlearning, we incorporate a localization and a preservation regularization to preserve the model’s ability to generate non-target concepts. Extensive experiments demonstrate that our method effectively erases a specific concept while preserving the model’s generation capability for all other concepts, outperforming existing methods. We provide the unlearned models in \href{<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%7D%7Bhttps://github.com/VDIGPKU/T2VUnlearning.git%7D">https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}</a>. </p>
<blockquote>
<p>近期文本到视频（T2V）扩散模型的进步极大地提高了生成视频的质量。然而，它们产生明确或有害内容的能力引发了关于误用和潜在权利侵犯的担忧。受文本到图像（T2I）模型中消除不良概念的去学习技术成功的启发，我们将去学习扩展到T2V模型，并提出了一种稳健且精确的去学习方法。具体来说，我们采用负导向速度预测微调，并通过提示增强来确保其对大型语言模型优化提示的稳健性。为了实现精确的去学习，我们结合了定位和保存正则化，以保留模型生成非目标概念的能力。大量实验表明，我们的方法在消除特定概念的同时，保留了模型对所有其他概念的生成能力，优于现有方法。我们提供去学习的模型在<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%E3%80%82">https://github.com/VDIGPKU/T2VUnlearning.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17550v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了近期文本到视频（T2V）扩散模型的进展，这些模型生成视频的质量得到了显著提升。然而，它们产生明确或有害内容的能力引发了关于误用和潜在权利侵犯的担忧。研究团队受到文本到图像（T2I）模型中的去训练技术成功的启发，将去训练技术扩展到T2V模型，并提出了一种稳健且精确的去训练方法。实验证明该方法能有效消除特定概念，同时保留模型对其他概念的生成能力，优于现有方法。相关模型已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到视频（T2V）扩散模型的最新进展显著提高了视频生成的质量。</li>
<li>T2V模型产生有害内容的能力引发关注。</li>
<li>研究人员受到文本到图像（T2I）模型去训练技术成功的启发，将其扩展到T2V模型。</li>
<li>提出了一种稳健且精确的去训练方法，通过负向引导速度预测微调并结合提示增强技术，提高模型的稳健性。</li>
<li>实现了精确去训练，通过引入定位和保留正则化来保留模型对非目标概念的生成能力。</li>
<li>实验证明该方法能有效消除特定概念，同时保持对其他概念的生成能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17550">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af5426cf2508fedbb314a5d9ce4f328e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d9a9db4dfd0d331c8779326b3ac5ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8023591641a773830b258f6604362f8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85e0455019d3f902b9195b638488f108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b70de0b98b44af06b9fd700e8a7cba8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b86cdc1ec073f06b8e441d5ab9df478d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction"><a href="#AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction" class="headerlink" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction"></a>AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer">https://github.com/TencentARC/AnimeGamer</a>. </p>
<blockquote>
<p>近期图像和视频合成技术的进展为生成游戏领域带来了新的希望。一个特别吸引人的应用是将动漫电影中的角色转变为可互动的游戏实体。这允许玩家通过语言指令沉浸在动态的动漫世界中，扮演他们最喜欢的角色进行生活模拟。这类游戏被定义为无限游戏，因为它们消除了预定的边界和固定的游戏规则，玩家可以通过开放式的语言与游戏世界互动，并体验不断演变的故事情节和环境。最近，一种开创性的无限动漫生活模拟方法采用大型语言模型（LLMs）将多轮文本对话翻译为图像生成的语言指令。然而，它忽略了历史视觉上下文，导致游戏性不一致。此外，它只能生成静态图像，无法融入动态元素，无法为玩家提供引人入胜的游戏体验。在这项工作中，我们提出了AnimeGamer，它建立在多模态大型语言模型（MLLMs）之上，用于生成每个游戏状态，包括描绘角色动作和状态更新的动态动画镜头，如图1所示。我们引入了新型的动作感知多模态表示来代表动画镜头，可以使用视频扩散模型将其解码为高质量的视频片段。通过获取历史动画镜头表示作为上下文并预测随后的表示，AnimeGamer可以生成具有上下文一致性和满意动态性的游戏。使用自动化指标和人类评估的广泛评估表明，AnimeGamer在游戏体验的各个方面都优于现有方法。相关代码和检查点已发布在<a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer%E3%80%82">https://github.com/TencentARC/AnimeGamer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01014v2">PDF</a> Project released at: <a target="_blank" rel="noopener" href="https://howe125.github.io/AnimeGamer.github.io/">https://howe125.github.io/AnimeGamer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了最新图像和视频合成技术在游戏领域的应用，特别是在动漫角色转化为可互动实体方面的创新。玩家可通过语言指令沉浸于无限动漫世界的动态角色模拟中。文章指出了一种将多轮文本对话转化为图像生成语言指令的大型语言模型方法，但忽视了历史视觉背景，导致游戏体验不一致。本研究提出基于多模态大型语言模型的AnimeGamer，能生成游戏状态，包括动态动画镜头和角色状态更新。引入新型动作感知多模态表示来展示动画镜头，并使用视频扩散模型解码成高质量视频片段。通过考虑历史动画镜头并预测后续表示，AnimeGamer能生成具有上下文一致性和满意动态性的游戏。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动漫角色转化为可互动实体技术为游戏领域带来新的机遇。</li>
<li>玩家可通过语言指令沉浸于动态动漫世界中的角色模拟中。</li>
<li>现有大型语言模型方法忽视了历史视觉背景，导致游戏体验不一致。</li>
<li>AnimeGamer基于多模态大型语言模型，可生成游戏状态及动态动画镜头。</li>
<li>引入动作感知多模态表示来展示动画镜头，提高游戏质量。</li>
<li>AnimeGamer通过考虑历史动画镜头预测后续内容，实现上下文一致性和动态性的游戏体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01014">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-87cd988c8c24823de93269df32207128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd29e104e47e0ff54d6807a15db67d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696e28d03c6550c4c5c82861b2ad2d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f3546a6de895403e043b8e44e47447f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cef586b07bbc87c8a676a1da28206c4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Text-Driven-360-Degree-Panorama-Generation"><a href="#A-Survey-on-Text-Driven-360-Degree-Panorama-Generation" class="headerlink" title="A Survey on Text-Driven 360-Degree Panorama Generation"></a>A Survey on Text-Driven 360-Degree Panorama Generation</h2><p><strong>Authors:Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue</strong></p>
<p>The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at <a target="_blank" rel="noopener" href="https://littlewhitesea.github.io/Text-Driven-Pano-Gen/">https://littlewhitesea.github.io/Text-Driven-Pano-Gen/</a>. </p>
<blockquote>
<p>文本驱动的全景生成技术出现，使得可以直接从文本描述中合成全视角全景图像，这标志着沉浸式视觉内容创作领域发生了革命性的进步。这一创新极大地简化了传统制作此类内容的复杂过程。文本到图像的扩散模型的最新进展加速了这一新兴领域的快速发展。这篇综述对文本驱动的全视角全景生成技术进行了全面回顾，深入分析了最先进算法及其在生成全视角全景的技术中的应用。此外，我们还对当前的局限性进行了批判性审视，并提出了未来研究的希望方向。相关资源和研究论文精选的项目页面可通过<a target="_blank" rel="noopener" href="https://littlewhitesea.github.io/Text-Driven-Pano-Gen/%E8%AE%BF%E9%97%AE%E3%80%82">https://littlewhitesea.github.io/Text-Driven-Pano-Gen/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14799v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本驱动的360度全景生成技术为从文本描述直接合成360度全景图像提供了可能，这标志着沉浸式视觉内容创作领域的一次变革性进展。该技术的出现简化了传统复杂的内容制作过程。文本到图像扩散模型的最新进展加速了这一新兴领域的迅速发展。本文全面回顾了文本驱动的360度全景生成技术，深入分析了最新算法及其在360度三维场景生成中的应用，同时批判性地探讨了当前局限性和未来研究方向。相关资源和研究论文可在[<a target="_blank" rel="noopener" href="https://littlewhitesea.github.io/Text-Driven-Pano-Gen/]%E6%9F%A5%E9%98%85%E3%80%82">https://littlewhitesea.github.io/Text-Driven-Pano-Gen/]查阅。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本驱动的360度全景生成技术实现了从文本到图像的合成，简化了传统的内容制作过程。</li>
<li>最新文本到图像扩散模型的进展加速了该领域的快速发展。</li>
<li>该技术为沉浸式视觉内容创作带来了变革。</li>
<li>文中深入分析了当前最新的算法及其在360度三维场景生成中的应用。</li>
<li>当前技术还存在一定的局限性，需要进一步的研究和改进。</li>
<li>文中提供了关于该领域的资源和研究论文的链接。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc416061417c8cdbce08203d7c61045e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0267dcb4e126e94711a0e038f7a96060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d617da7a8cd53631721c78f1ef68be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cc87f30b49f56596ea943c097d39f5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3992cc225a538b8f306d518a748b9d3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a3e98130d7627097a7029a46817e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63aa688b0ce17574128fb971b5666477.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Masked-Autoencoders-Are-Effective-Tokenizers-for-Diffusion-Models"><a href="#Masked-Autoencoders-Are-Effective-Tokenizers-for-Diffusion-Models" class="headerlink" title="Masked Autoencoders Are Effective Tokenizers for Diffusion Models"></a>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</h2><p><strong>Authors:Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj</strong></p>
<p>Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released. </p>
<blockquote>
<p>最新的潜在扩散模型进展已经证明其在高分辨率图像合成中的有效性。然而，关于如何通过tokenizers学习和生成扩散模型的潜在空间特性仍待探索。我们从理论和实证上发现，更好的生成质量与具有更好结构的潜在分布密切相关，例如具有更少的高斯混合模式并且具有更多判别特征的分布。基于这些见解，我们提出了MAETok，这是一种利用掩模建模的自编码器（AE），能够在保持重建保真度的同时学习语义丰富的潜在空间。大量实验验证了我们分析的有效性，证明了变分自编码器形式并非必需，仅使用AE的判别性潜在空间即可在ImageNet生成任务上实现卓越性能，仅使用128个token。MAETok实现了重要的实际改进，实现了1.69的gFID，同时训练速度提高了76倍，推理速度提高了31倍，可用于生成512x512的图像。我们的研究表明，潜在空间的结构对于有效的扩散模型至关重要，而非变分约束。代码和训练模型均已发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03444v2">PDF</a> </p>
<p><strong>Summary</strong><br>     近期潜扩散模型的进步已证明其在高分辨率图像合成中的有效性。然而，关于令牌器潜在空间特性对于潜扩散模型的学习和生成的影响尚未被充分探索。本文发现优质生成与具有更好结构和更少高斯混合模式的潜在分布密切相关。基于此，我们提出了MAETok，一种利用掩模建模的自编码器（AE），旨在学习语义丰富的潜在空间并保持重建保真度。实验验证了我们分析的有效性，证明了变分自编码器形式并非必需，仅使用AE的判别性潜在空间即可在ImageNet生成上实现卓越性能。MAETok取得了显著的实践改进，实现了更高的生成图像质量和更快的训练和推理速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜扩散模型在图像合成中展现有效性。</li>
<li>潜在空间的结构和性质对潜扩散模型的学习和生成至关重要。</li>
<li>高质量生成的潜在分布具有更少的高斯混合模式和更具区分性的特征。</li>
<li>MAETok利用掩模建模的自编码器（AE）学习语义丰富的潜在空间。</li>
<li>实验表明变分自编码器形式并非必需，仅使用AE即可实现卓越性能。</li>
<li>MAETok在ImageNet生成上实现了显著的性能提升，包括生成图像质量、训练速度和推理速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-748771407e505232c49c97fce51fd11c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-456880a0b6c4eea926df19c333db81ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5db41eecd9d506fe7f4124155e2410d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b38b1538970db5b9dced9d05e419de3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18862ca7cb0f1725e215a9b917dda0b5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unpaired-Deblurring-via-Decoupled-Diffusion-Model"><a href="#Unpaired-Deblurring-via-Decoupled-Diffusion-Model" class="headerlink" title="Unpaired Deblurring via Decoupled Diffusion Model"></a>Unpaired Deblurring via Decoupled Diffusion Model</h2><p><strong>Authors:Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang</strong></p>
<p>Generative diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. In favor of their ability to supplement missing details and generate aesthetically pleasing contents, recent works have applied them to image deblurring via training an adapter on blurry-sharp image pairs to provide structural conditions for restoration. However, acquiring substantial amounts of realistic paired data is challenging and costly in real-world scenarios. On the other hand, relying solely on synthetic data often results in overfitting, leading to unsatisfactory performance when confronted with unseen blur patterns. To tackle this issue, we propose UID-Diff, a generative-diffusion-based model designed to enhance deblurring performance on unknown domains by decoupling structural features and blur patterns through joint training on three specially designed tasks. We employ two Q-Formers as structural features and blur patterns extractors separately. The features extracted by them will be used for the supervised deblurring task on synthetic data and the unsupervised blur-transfer task by leveraging unpaired blurred images from the target domain simultaneously. We further introduce a reconstruction task to make the structural features and blur patterns complementary. This blur-decoupled learning process enhances the generalization capabilities of UID-Diff when encountering unknown blur patterns. Experiments on real-world datasets demonstrate that UID-Diff outperforms existing state-of-the-art methods in blur removal and structural preservation in various challenging scenarios. </p>
<blockquote>
<p>基于大规模数据集的生成性扩散模型在图像合成方面取得了显著的进展。由于其能够补充缺失的细节并生成美观的内容，最近的研究将其应用于图像去模糊，通过模糊-清晰图像对训练适配器，为恢复提供结构条件。然而，在现实场景中获取大量真实的配对数据具有挑战性和成本高昂。另一方面，仅依赖合成数据往往会导致过拟合，当面对未知的模糊模式时，性能往往不尽人意。为了解决这一问题，我们提出了UID-Diff，这是一个基于生成扩散的模型，旨在通过联合训练三个专门设计的任务来提高未知领域的去模糊性能。我们采用两个Q-Formers分别作为结构特征和模糊模式提取器。它们提取的特征将用于合成数据上的监督去模糊任务，并同时利用来自目标域的无配对模糊图像进行无监督模糊转移任务。我们进一步引入重建任务使结构特征和模糊模式互补。这种模糊解耦学习过程提高了UID-Diff在遇到未知模糊模式时的泛化能力。在真实世界数据集上的实验表明，UID-Diff在各种具有挑战性的场景中，模糊去除和结构保留方面的性能超过了现有的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01522v2">PDF</a> We propose UID-Diff to integrate generative diffusion model into   unpaired deblurring tasks</p>
<p><strong>Summary</strong><br>     基于生成式扩散模型的图像去模糊技术通过联合训练三项任务以提升未知领域的去模糊性能。通过两个Q-Formers分别提取结构特征和模糊模式，并用于合成数据的监督去模糊任务和基于未配对模糊图像的无监督模糊转移任务。同时引入重建任务使结构特征和模糊模式互补。在真实数据集上的实验表明，UID-Diff在多种挑战场景中表现优于现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式扩散模型在图像合成领域取得了显著进展，尤其在图像去模糊方面展现出潜力。</li>
<li>提出UID-Diff模型，通过联合训练三项任务提升未知领域的去模糊性能。</li>
<li>使用两个Q-Formers分别提取结构特征和模糊模式。</li>
<li>结构特征和模糊模式用于合成数据的监督去模糊任务和基于未配对模糊图像的无监督模糊转移任务。</li>
<li>引入重建任务以优化结构特征和模糊模式的互补性。</li>
<li>UID-Diff模型在真实数据集上的实验表现优于现有最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b057e725bc541813c87bdcb6ec0da819.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-046d7925796731878ff06125b34a8712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8123cfc4bd9aed86d909b7c041132fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-743924d2527b165451625817a95d6d6c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space"><a href="#DCTdiff-Intriguing-Properties-of-Image-Generative-Modeling-in-the-DCT-Space" class="headerlink" title="DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space"></a>DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT   Space</h2><p><strong>Authors:Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Wenshuo Chen, Albert Ali Salah, Itir Onal Ertugrul</strong></p>
<p>This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to 512$\times$512 resolution without using the latent diffusion paradigm and beats latent diffusion (using SD-VAE) with only 1&#x2F;4 training cost. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why ‘image diffusion can be seen as spectral autoregression’, bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is <a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff">https://github.com/forever208/DCTdiff</a>. </p>
<blockquote>
<p>本文探讨了频率空间的图像建模，并介绍了DCTdiff这一端到端的扩散生成范式，该范式在离散余弦变换（DCT）空间中对图像进行有效建模。我们研究了DCTdiff的设计空间，揭示了关键的设计因素。在不同框架（UViT、DiT）、生成任务和多种扩散采样器上的实验表明，DCTdiff在生成质量和训练效率方面优于基于像素的扩散模型。值得注意的是，DCTdiff可以无缝扩展到512×512分辨率，而无需使用潜在扩散范式，并且仅以四分之一的训练成本击败了潜在扩散（使用SD-VAE）。最后，我们说明了DCT图像建模的几个有趣特性。例如，我们提供了为什么“图像扩散可以被视为谱自回归”的理论证明，从而缩小了扩散模型和自回归模型之间的差距。DCTdiff的有效性和所引入的特性为频率空间的图像建模提供了有前景的方向。代码地址为：<a target="_blank" rel="noopener" href="https://github.com/forever208/DCTdiff%E3%80%82">https://github.com/forever208/DCTdiff。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15032v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DCTdiff，这是一种在离散余弦变换（DCT）空间中进行图像建模的端到端扩散生成范式。它探讨了DCTdiff的设计空间，揭示了关键设计因素。实验表明，DCTdiff在生成质量和训练效率方面优于基于像素的扩散模型，并能无缝扩展到512x512分辨率。还提供了一种将图像扩散视为谱自回归的理论证明。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCTdiff是一种在离散余弦变换（DCT）空间进行图像建模的扩散生成方法。</li>
<li>DCTdiff在生成质量和训练效率方面优于像素基础上的扩散模型。</li>
<li>DCTdiff可无缝扩展至高分辨率，如512x512，无需使用潜在扩散方法。</li>
<li>DCTdiff相对于潜在扩散（使用SD-VAE）具有更低的训练成本。</li>
<li>DCT图像建模具有一些有趣特性，如图像扩散可视为谱自回归的理论证明。</li>
<li>该论文探讨了DCTdiff的设计空间，并揭示了其关键设计因素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0de78b128b8fda6f9b23f41b233eda94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63c8232df9805a1a30721f23d0948c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f711dcc62c558ba73c2d349de51ff9c5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TrojanEdit-Multimodal-Backdoor-Attack-Against-Image-Editing-Model"><a href="#TrojanEdit-Multimodal-Backdoor-Attack-Against-Image-Editing-Model" class="headerlink" title="TrojanEdit: Multimodal Backdoor Attack Against Image Editing Model"></a>TrojanEdit: Multimodal Backdoor Attack Against Image Editing Model</h2><p><strong>Authors:Ji Guo, Peihong Chen, Wenbo Jiang, Xiaolei Wen, Jiaming He, Jiachen Li, Guoming Lu, Aiguo Chen, Hongwei Li</strong></p>
<p>Multimodal diffusion models for image editing generate outputs conditioned on both textual instructions and visual inputs, aiming to modify target regions while preserving the rest of the image. Although diffusion models have been shown to be vulnerable to backdoor attacks, existing efforts mainly focus on unimodal generative models and fail to address the unique challenges in multimodal image editing. In this paper, we present the first study of backdoor attacks on multimodal diffusion-based image editing models. We investigate the use of both textual and visual triggers to embed a backdoor that achieves high attack success rates while maintaining the model’s normal functionality. However, we identify a critical modality bias. Simply combining triggers from different modalities leads the model to primarily rely on the stronger one, often the visual modality, which results in a loss of multimodal behavior and degrades editing quality. To overcome this issue, we propose TrojanEdit, a backdoor injection framework that dynamically adjusts the gradient contributions of each modality during training. This allows the model to learn a truly multimodal backdoor that activates only when both triggers are present. Extensive experiments on multiple image editing models show that TrojanEdit successfully integrates triggers from different modalities, achieving balanced multimodal backdoor learning while preserving clean editing performance and ensuring high attack effectiveness. </p>
<blockquote>
<p>基于图像编辑的多模态扩散模型根据文本指令和视觉输入生成输出，旨在修改目标区域的同时保留图像的其余部分。尽管扩散模型已显示出容易受到后门攻击的影响，但现有研究主要集中在单模态生成模型上，未能解决多模态图像编辑中的独特挑战。在本文中，我们对基于多模态扩散的图像编辑模型上的后门攻击进行了首次研究。我们研究了使用文本和视觉触发因素来嵌入后门的方法，以实现高攻击成功率的同时保持模型的正常功能。然而，我们发现了一种关键的模态偏见。简单地结合不同模态的触发器会导致模型主要依赖于更强的模态，通常是视觉模态，从而导致多模态行为的丧失和编辑质量的下降。为了克服这一问题，我们提出了TrojanEdit，一种后门注入框架，在训练过程中动态调整每个模态的梯度贡献。这允许模型学习一种真正的多模态后门，只有在两种触发器都存在时才会被激活。在多个图像编辑模型上的广泛实验表明，TrojanEdit成功地将来自不同模态的触发器集成在一起，实现了平衡的多模态后门学习，同时保持了清洁编辑性能，并确保高攻击效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14681v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态扩散模型用于图像编辑，能够根据文本指令和视觉输入生成输出，旨在修改目标区域同时保留图像其余部分。虽然扩散模型已被证明容易受到后门攻击，但现有研究主要集中在单模态生成模型上，未能解决多模态图像编辑中的独特挑战。本文首次研究了多模态扩散模型图像编辑的后门攻击问题。我们探讨了使用文本和视觉触发来嵌入后门的方法，在保持模型正常功能的同时实现了较高的攻击成功率。然而，我们发现了一种关键模态偏见，即简单结合不同模态的触发会使模型主要依赖于更强大的模态（通常是视觉模态），导致多模态行为的丧失和编辑质量的下降。为解决这一问题，我们提出了TrojanEdit，一种后门注入框架，在训练过程中动态调整每个模态的梯度贡献。这允许模型学习一种真正的多模态后门，只有在两个触发器都存在时才会被激活。在多个图像编辑模型上的广泛实验表明，TrojanEdit成功集成了来自不同模态的触发器，实现了平衡的多模态后门学习，同时保持了清洁编辑性能，确保了高攻击效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态扩散模型用于图像编辑，可以基于文本指令和视觉输入生成输出。</li>
<li>现有研究主要集中在单模态生成模型上的后门攻击，未涵盖多模态扩散模型中的独特挑战。</li>
<li>单纯结合不同模态的触发会导致模型依赖于强势模态（通常是视觉模态），进而丧失多模态行为并降低编辑质量。</li>
<li>提出TrojanEdit框架来解决这一问题，通过动态调整不同模态的梯度贡献来训练真正的多模态后门。</li>
<li>TrojanEdit实现了不同模态触发器的集成，能够在保持清洁编辑性能的同时确保高攻击效果。</li>
<li>实验结果表明，TrojanEdit能够平衡多模态后门学习，同时在多个图像编辑模型上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-527a752735d4124db40f932071515ba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75304da302ca06a1defb40dba42b6566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c9720327e07d5f2017e886fb4a0aa2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e08024a0b233a1cde52ece16ac76dcbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d586dad1c7b40c2b0bbb3d32de2df06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8847289f208dcb448bdd6bd33cce6d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931ac4b7d905e3a48896405a95645fb5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner"><a href="#CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner" class="headerlink" title="CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner"></a>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner</h2><p><strong>Authors:Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</strong></p>
<p>We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan">https://github.com/wyysf-98/CraftsMan</a> </p>
<blockquote>
<p>我们提出了一种新颖的三维建模系统，名为CraftsMan。该系统可以生成具有多样化形状、规则网格拓扑和精细表面的高保真三维几何体，并且以交互方式对其进行细化。尽管三维生成技术取得了重大进展，但现有方法仍然面临优化过程冗长、网格拓扑不规则、表面噪声以及难以适应用户编辑等问题，从而阻碍了它们在三维建模软件中的广泛采用和实施。我们的工作受到工匠的启发，工匠通常首先大致勾勒出作品的整体轮廓，然后细化表面细节。具体来说，我们采用了一种三维扩散模型，该模型在潜在空间中对基于集合的潜在三维表示进行操作，以在几秒内生成具有规则网格拓扑的粗略几何形状。特别是，这一过程以文本提示或参考图像为输入，并利用强大的多视图（MV）扩散模型生成粗略几何形状的多视图，然后将其输入我们的MV条件三维扩散模型以生成三维几何形状，从而大大提高了稳健性和通用性。之后，使用基于法线的几何细化器来显着增强表面细节。此细化过程可以自动进行，也可以与用户提供的编辑进行交互。大量实验表明，我们的方法在生成高质量的三维资产方面与现有方法相比具有更高的有效性。主页：<a target="_blank" rel="noopener" href="https://craftsman3d.github.io/%EF%BC%8C%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/wyysf-98/CraftsMan">https://craftsman3d.github.io/，代码：https://github.com/wyysf-98/CraftsMan</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14979v4">PDF</a> HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan3D">https://github.com/wyysf-98/CraftsMan3D</a></p>
<p><strong>Summary</strong></p>
<p>该项目提出了一种名为CraftsMan的新型三维建模系统，该系统能够生成高质量的三维模型，具有多样的形状、规则网格拓扑和精细的表面细节，并允许以交互方式调整几何形状。它通过利用运行在潜在空间上的三维扩散模型，能够在几秒内生成具有规则网格拓扑的粗糙几何形状。该项目还提供了一种基于法线的几何优化器，可显著提高表面细节的质量，并允许自动或交互式地进行用户编辑。该系统在生成高质量三维资产方面表现出显著效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CraftsMan是一个新型的三维建模系统，可以生成高质量的三维模型。</li>
<li>该系统能够生成具有多样形状、规则网格拓扑和精细表面细节的三维模型。</li>
<li>CraftsMan允许以交互方式调整几何形状，提高了用户的编辑体验。</li>
<li>该系统采用基于潜在空间的3D扩散模型，快速生成粗糙几何形状。</li>
<li>CraftsMan利用多视角扩散模型提高了模型的稳健性和泛化能力。</li>
<li>系统采用基于法线的几何优化器，可以显著提高表面细节的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9239fe621b943fddbed51b9a7c80c11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5bf449708d03a86e4c9efb4e4cd813d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35016a66d5990272953fc5bd05fc344f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a80358eb5cf508aebb5b903dc26a92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f5ed3f1bb0c5beaafe09ef3ebe654e8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><a href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation" class="headerlink" title="TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation"></a>TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation</h2><p><strong>Authors:Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p>
<p>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a “Screenwriter”, engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the “Rehearsal”. Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the “Final Performance”. With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity. </p>
<blockquote>
<p>近期扩散模型（Diffusion Models）的进展能够从文本生成高质量、令人惊叹的图像。然而，在真实场景中应用需求较高的多轮图像生成仍然面临挑战，即如何在图像和文本之间保持语义一致性，以及在多个交互回合中对同一主题的上下文一致性。为解决这一问题，我们引入了TheaterGen，这是一个无需训练的框架，它结合了大型语言模型（LLMs）和文本到图像（T2I）模型，提供了多轮图像生成的能力。</p>
</blockquote>
<p>在此框架中，LLMs充当“编剧”，进行多轮交互，生成并管理一个标准化的提示册，其中包含目标图像中每个角色的提示和布局设计。基于这些，TheaterGen生成角色图像列表并提取指导信息，类似于“排练”。随后，通过将提示册和指导信息融入T2I扩散模型的反向去噪过程，TheaterGen生成最终图像，就像进行“最终表演”。通过对提示册和角色图像的有效管理，TheaterGen显著提高了合成图像的语义和上下文一致性。</p>
<p>此外，我们引入了专用基准测试CMIGBench（一致多轮图像生成基准测试），包含8000个多轮指令。不同于之前的多轮基准测试，CMIGBench不会预先定义角色。CMIGBench包含了故事生成和多轮编辑任务，以进行全面评估。广泛的实验结果表显示，TheaterGen显著优于现有技术方法。它在最先进的Mini DALLE 3模型上提高了21%的平均角色间相似度和19%的平均文本-图像相似度。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18919v2">PDF</a> </p>
<p><strong>Summary</strong><br>     文本中介绍了最新扩散模型能生成高质量图像的能力，但多轮图像生成仍面临保持图像与文本之间语义一致性的挑战。为此，提出了训练免框架TheaterGen，集成大型语言模型和文本到图像模型，实现多轮图像生成。通过标准化提示书籍管理角色图像和指导信息，提高合成图像的语义和上下文一致性。同时引入CMIGBench基准测试，包含故事生成和多轮编辑任务进行全面评估。实验结果显示，TheaterGen显著优于现有方法，提升了前沿Mini DALLE 3模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型具备生成高质量图像的能力。</li>
<li>多轮图像生成在实际场景中需求高，但保持语义一致性是挑战。</li>
<li>TheaterGen框架集成大型语言模型和文本到图像模型，解决多轮图像生成的语义一致性挑战。</li>
<li>TheaterGen通过标准化提示书籍管理角色图像和指导信息。</li>
<li>CMIGBench基准测试用于全面评估多轮图像生成性能，包括故事生成和多轮编辑任务。</li>
<li>实验结果显示TheaterGen显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-304858bb178dc2e2427571760aa1fd33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2043c33ab09f9b1c4433b964015247c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-568b0d8a34639fe3e5425bc5cb460f4b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0f8389b7acacf76f1ffb4b87b162eef3.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-03  VideoCAD A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-19585b96692f0456684962692487eea5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-06-03  ErpGS Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
