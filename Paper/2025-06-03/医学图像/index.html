<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  VideoCAD A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0f8389b7acacf76f1ffb4b87b162eef3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="VideoCAD-A-Large-Scale-Video-Dataset-for-Learning-UI-Interactions-and-3D-Reasoning-from-CAD-Software"><a href="#VideoCAD-A-Large-Scale-Video-Dataset-for-Learning-UI-Interactions-and-3D-Reasoning-from-CAD-Software" class="headerlink" title="VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software"></a>VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software</h2><p><strong>Authors:Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed</strong></p>
<p>Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt at engineering UI interaction learning for precision tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order of magnitude higher complexity in UI interaction learning for real-world engineering tasks, having up to a 20x longer time horizon than other datasets. We show two important downstream applications of VideoCAD: learning UI interactions from professional precision 3D CAD tools and a visual question-answering (VQA) benchmark designed to evaluate multimodal large language modelsâ€™ (LLM) spatial reasoning and video understanding abilities. To learn the UI interactions, we propose VideoCADFormer - a state-of-the-art model in learning CAD interactions directly from video, which outperforms multiple behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”å¤æ‚çš„è¿‡ç¨‹ï¼Œéœ€è¦ç”¨æˆ·ä¸å¤æ‚çš„3Dç•Œé¢è¿›è¡Œç²¾ç¡®ã€é•¿æœŸçš„äº¤äº’ã€‚å°½ç®¡æœ€è¿‘äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ä»£ç†åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†è¿›å±•æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œä½†ç°æœ‰çš„å¤§å¤šæ•°æ•°æ®é›†å’Œæ–¹æ³•éƒ½é›†ä¸­åœ¨ç§»åŠ¨æˆ–Webåº”ç”¨ç¨‹åºä¸­çš„ç®€çŸ­ã€ä½å¤æ‚åº¦çš„ä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰ä¸“ä¸šå·¥ç¨‹å·¥å…·çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoCADï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•è¿›è¡Œé’ˆå¯¹ç²¾ç¡®ä»»åŠ¡çš„å·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒVideoCADæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡41Kä¸ªæ ‡æ³¨çš„CADæ“ä½œè§†é¢‘è®°å½•ï¼Œè¿™äº›è®°å½•æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶æ”¶é›†äººä¸ºè®¾è®¡çš„CADç•Œé¢çš„é«˜ä¿çœŸUIåŠ¨ä½œæ•°æ®ç”Ÿæˆçš„ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒVideoCADåœ¨é’ˆå¯¹ç°å®ä¸–ç•Œå·¥ç¨‹ä»»åŠ¡çš„UIäº¤äº’å­¦ä¹ æ–¹é¢æä¾›äº†æ›´é«˜å¤æ‚åº¦çš„æ•°æ®ï¼Œæ—¶é—´è·¨åº¦æ˜¯å…¶ä»–æ•°æ®é›†çš„20å€ã€‚æˆ‘ä»¬å±•ç¤ºäº†VideoCADçš„ä¸¤ä¸ªé‡è¦ä¸‹æ¸¸åº”ç”¨ï¼šä»ä¸“ä¸šçš„ç²¾å¯†ä¸‰ç»´CADå·¥å…·å­¦ä¹ UIäº¤äº’ä»¥åŠè®¾è®¡ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å­¦ä¹ UIäº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†VideoCADFormerâ€”â€”ä¸€ä¸ªç›´æ¥ä»è§†é¢‘å­¦ä¹ CADäº¤äº’çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å¤šä¸ªè¡Œä¸ºå…‹éš†åŸºå‡†æµ‹è¯•ã€‚VideoCADFormerä»¥åŠä»VideoCADè¡ç”Ÿçš„VQAåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰è§†é¢‘å‹ç”¨æˆ·ç•Œé¢ç†è§£çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½çš„éœ€æ±‚ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿æœŸä¾èµ–å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24838v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VideoCADï¼Œä¸€ä¸ªä¸ºç²¾å¯†ä»»åŠ¡è®¾è®¡çš„å·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ æ•°æ®é›†ã€‚VideoCADåŒ…å«è¶…è¿‡41Kä¸ªæ ‡æ³¨çš„CADæ“ä½œè§†é¢‘è®°å½•ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶ä»äººå·¥è®¾è®¡çš„CADä¸­æ”¶é›†é«˜ä¿çœŸUIåŠ¨ä½œæ•°æ®ã€‚ç›¸è¾ƒäºç°æœ‰æ•°æ®é›†ï¼ŒVideoCADåœ¨ç°å®ä¸–ç•Œå·¥ç¨‹ä»»åŠ¡çš„UIäº¤äº’å­¦ä¹ ä¸­å…·æœ‰æ›´é«˜çš„å¤æ‚æ€§ï¼Œæ—¶é—´è·¨åº¦æ˜¯å…¶ä»–æ•°æ®é›†çš„20å€ã€‚æœ¬æ–‡å±•ç¤ºäº†VideoCADçš„ä¸¤ä¸ªé‡è¦åº”ç”¨ï¼šä»ä¸“ä¸šç²¾å¯†3D CADå·¥å…·å­¦ä¹ UIäº¤äº’å’Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›çš„è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å­¦ä¹ UIäº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†VideoCADFormerï¼Œä¸€ä¸ªç›´æ¥ä»è§†é¢‘å­¦ä¹ CADäº¤äº’çš„å…ˆè¿›æ¨¡å‹ï¼Œè¶…è¶Šäº†å¤šä¸ªè¡Œä¸ºå…‹éš†åŸºçº¿ã€‚VideoCADå’ŒVideoCADFormerè¡ç”Ÿçš„VQAåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰è§†é¢‘å‹UIç†è§£çš„ä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿å‘¨æœŸä¾èµ–ç­‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VideoCADæ˜¯é¦–ä¸ªé’ˆå¯¹ç²¾å¯†ä»»åŠ¡è®¾è®¡çš„å·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ æ•°æ®é›†ã€‚</li>
<li>VideoCADåŒ…å«è¶…è¿‡41Kä¸ªæ ‡æ³¨çš„CADæ“ä½œè§†é¢‘ï¼Œå¤æ‚æ€§æ˜¾è‘—é«˜äºç°æœ‰æ•°æ®é›†ã€‚</li>
<li>VideoCADæ•°æ®é›†é€‚ç”¨äºä»ä¸“ä¸šç²¾å¯†3D CADå·¥å…·å­¦ä¹ UIäº¤äº’ã€‚</li>
<li>VideoCADä¸ºè§†è§‰é—®ç­”ä»»åŠ¡æä¾›äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›ã€‚</li>
<li>VideoCADæå‡ºäº†ç²¾ç¡®åŠ¨ä½œå®šä½çš„éœ€æ±‚ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿç†è§£å¤æ‚çš„ç”¨æˆ·æ“ä½œã€‚</li>
<li>å½“å‰è§†é¢‘å‹UIç†è§£çš„æŒ‘æˆ˜åŒ…æ‹¬å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3834c21cab465b52f3eeba0b4147c098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4278d9c16201e28a104396153f6ffd89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e21817674620dd382da44c8ab19696ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fa6f07e9de01acde011c0193e14e849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af09834b625542ab273a3d4f54eb9428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea7bcf153e240082e48927ebfd8ca02.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AFIRE-Accurate-and-Fast-Image-Reconstruction-Algorithm-for-Geometric-inconsistency-Multispectral-CT"><a href="#AFIRE-Accurate-and-Fast-Image-Reconstruction-Algorithm-for-Geometric-inconsistency-Multispectral-CT" class="headerlink" title="AFIRE: Accurate and Fast Image Reconstruction Algorithm for   Geometric-inconsistency Multispectral CT"></a>AFIRE: Accurate and Fast Image Reconstruction Algorithm for   Geometric-inconsistency Multispectral CT</h2><p><strong>Authors:Yu Gao, Chong Chen</strong></p>
<p>For nonlinear multispectral computed tomography (CT), accurate and fast image reconstruction is challenging when the scanning geometries under different X-ray energy spectra are inconsistent or mismatched. Motivated by this, we propose an accurate and fast algorithm named AFIRE to address such problem in the case of mildly full scan. We discover that the derivative operator (gradient) of the involved nonlinear mapping at some special points, for example, at zero, can be represented as a composition (block multiplication) of a diagonal operator (matrix) composed of X-ray transforms (projection matrices) and a very small-scale matrix. Based on the insights, the AFIRE is proposed respectively from the continuous, discrete and actual-use perspectives by leveraging the simplified Newton method. Under proper conditions, we establish the convergence theory of the proposed algorithm. Furthermore, numerical experiments are also carried out to verify that the proposed algorithm can accurately and effectively reconstruct the basis images in completely geometric-inconsistency dual-energy CT with noiseless and noisy projection data. Particularly, the proposed algorithm significantly outperforms some state-of-the-art methods in terms of accuracy and efficiency. Finally, the flexibility and extensibility of the proposed algorithm are also demonstrated. </p>
<blockquote>
<p>å¯¹äºéçº¿æ€§å¤šå…‰è°±è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼Œå½“ä¸åŒXå°„çº¿èƒ½é‡è°±ä¸‹çš„æ‰«æå‡ ä½•ç»“æ„ä¸ä¸€è‡´æˆ–ä¸åŒ¹é…æ—¶ï¼Œå®ç°å‡†ç¡®å¿«é€Ÿçš„å›¾åƒé‡å»ºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‡†ç¡®å¿«é€Ÿçš„ç®—æ³•ï¼Œåä¸ºAFIREï¼Œä»¥è§£å†³è½»å¾®å…¨æ‰«ææƒ…å†µä¸‹çš„æ­¤ç±»é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰€æ¶‰åŠéçº¿æ€§æ˜ å°„çš„å¯¼æ•°ç®—å­ï¼ˆæ¢¯åº¦ï¼‰åœ¨æŸäº›ç‰¹æ®Šç‚¹ï¼Œä¾‹å¦‚åœ¨é›¶ç‚¹ä¸Šï¼Œå¯ä»¥è¡¨ç¤ºä¸ºå¯¹è§’çº¿ç®—å­ï¼ˆç”±Xå°„çº¿å˜æ¢ï¼ˆæŠ•å½±çŸ©é˜µï¼‰ç»„æˆçš„çŸ©é˜µï¼‰å’Œä¸€ä¸ªå°è§„æ¨¡çŸ©é˜µçš„ä¹˜ç§¯ï¼ˆå—ç»„åˆï¼‰ã€‚åŸºäºè¿™äº›è§è§£ï¼Œåˆ©ç”¨ç®€åŒ–ç‰›é¡¿æ³•åˆ†åˆ«ä»è¿ç»­ã€ç¦»æ•£å’Œå®é™…ä½¿ç”¨ä¸‰ä¸ªè§’åº¦æå‡ºäº†AFIREç®—æ³•ã€‚åœ¨é€‚å½“çš„æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬å»ºç«‹äº†æ‰€æå‡ºç®—æ³•çš„æ”¶æ•›ç†è®ºã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†æ•°å€¼å®éªŒï¼ŒéªŒè¯äº†æ‰€æç®—æ³•åœ¨å‡ ä½•å®Œå…¨ä¸ä¸€è‡´çš„åŒèƒ½CTä¸­ï¼Œèƒ½å‡†ç¡®æœ‰æ•ˆåœ°é‡å»ºåŸºç¡€å›¾åƒï¼Œä¸”å¯¹äºæ— å™ªå£°å’Œæœ‰å™ªå£°çš„æŠ•å½±æ•°æ®å‡æœ‰æ•ˆã€‚ç‰¹åˆ«åœ°ï¼Œæ‰€æç®—æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¸€äº›æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ€åï¼Œæ‰€æç®—æ³•çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¹Ÿå¾—åˆ°äº†è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24793v1">PDF</a> 36 pages, 15 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹éçº¿æ€§å¤šå…‰è°±è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼Œå½“ä¸åŒXå°„çº¿èƒ½é‡å…‰è°±ä¸‹çš„æ‰«æå‡ ä½•ä½“ä¸ä¸€è‡´æˆ–ä¸åŒ¹é…æ—¶ï¼Œå®ç°å‡†ç¡®å¿«é€Ÿå›¾åƒé‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAFIREçš„ç²¾å‡†å¿«é€Ÿç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è½»å¾®å…¨æ‰«æçš„æƒ…å†µä¸‹è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚è¯¥ç®—æ³•åŸºäºç‰¹æ®Šç‚¹ï¼ˆå¦‚é›¶ç‚¹ï¼‰éçº¿æ€§æ˜ å°„çš„å¯¼æ•°è¿ç®—ç¬¦ï¼ˆæ¢¯åº¦ï¼‰å¯ä»¥è¢«è¡¨ç¤ºä¸ºå¯¹è§’è¿ç®—ç¬¦ï¼ˆç”±Xå°„çº¿å˜æ¢ï¼ˆæŠ•å½±çŸ©é˜µï¼‰ç»„æˆçš„å°è§„æ¨¡çŸ©é˜µçš„å—ä¹˜æ³•ï¼‰çš„è§è§£ã€‚ç»“åˆè¿ç»­ã€ç¦»æ•£å’Œå®é™…ä½¿ç”¨ç­‰è§’åº¦ï¼Œåˆ©ç”¨ç®€åŒ–ç‰›é¡¿æ³•æå‡ºäº†AFIREç®—æ³•ã€‚åœ¨é€‚å½“çš„æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬å»ºç«‹äº†è¯¥ç®—æ³•çš„æ”¶æ•›ç†è®ºã€‚æ•°å€¼å®éªŒéªŒè¯äº†è¯¥ç®—æ³•åœ¨å®Œå…¨å‡ ä½•ä¸ä¸€è‡´çš„åŒèƒ½CTçš„æ— å™ªå£°å’Œæœ‰å™ªå£°æŠ•å½±æ•°æ®ä¸­èƒ½ç²¾å‡†æœ‰æ•ˆåœ°é‡å»ºåŸºç¡€å›¾åƒã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯¥ç®—æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¸€äº›æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ€åï¼Œå±•ç¤ºäº†è¯¥ç®—æ³•çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éçº¿æ€§å¤šå…‰è°±CTåœ¨æ‰«æå‡ ä½•ä¸ä¸€è‡´æ—¶é¢ä¸´å‡†ç¡®å¿«é€Ÿå›¾åƒé‡å»ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åä¸ºAFIREçš„ç²¾å‡†å¿«é€Ÿç®—æ³•ï¼Œç”¨äºè§£å†³è½»å¾®å…¨æ‰«ææƒ…å†µä¸‹çš„è¿™ä¸€é—®é¢˜ã€‚</li>
<li>AFIREç®—æ³•åŸºäºç‰¹æ®Šç‚¹éçº¿æ€§æ˜ å°„çš„å¯¼æ•°è¿ç®—ç¬¦çš„è§è§£ã€‚</li>
<li>è¯¥ç®—æ³•ç»“åˆè¿ç»­ã€ç¦»æ•£å’Œå®é™…ä½¿ç”¨ç­‰è§’åº¦æå‡ºï¼Œåˆ©ç”¨ç®€åŒ–ç‰›é¡¿æ³•ã€‚</li>
<li>åœ¨é€‚å½“çš„æ¡ä»¶ä¸‹ï¼Œå»ºç«‹äº†AFIREç®—æ³•çš„æ”¶æ•›ç†è®ºã€‚</li>
<li>æ•°å€¼å®éªŒéªŒè¯äº†AFIREåœ¨åŒèƒ½CTä¸­çš„ç²¾å‡†é‡å»ºèƒ½åŠ›ï¼Œæ— è®ºæ˜¯æ— å™ªå£°è¿˜æ˜¯æœ‰å™ªå£°çš„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39812d099b4497ad04406e2e114bda7a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contrast-Invariant-Self-supervised-Segmentation-for-Quantitative-Placental-MRI"><a href="#Contrast-Invariant-Self-supervised-Segmentation-for-Quantitative-Placental-MRI" class="headerlink" title="Contrast-Invariant Self-supervised Segmentation for Quantitative   Placental MRI"></a>Contrast-Invariant Self-supervised Segmentation for Quantitative   Placental MRI</h2><p><strong>Authors:Xinliu Zhong, Ruiying Liu, Emily S. Nichols, Xuzhe Zhang, Andrew F. Laine, Emma G. Duerden, Yun Wang</strong></p>
<p>Accurate placental segmentation is essential for quantitative analysis of the placenta. However, this task is particularly challenging in T2*-weighted placental imaging due to: (1) weak and inconsistent boundary contrast across individual echoes; (2) the absence of manual ground truth annotations for all echo times; and (3) motion artifacts across echoes caused by fetal and maternal movement. In this work, we propose a contrast-augmented segmentation framework that leverages complementary information across multi-echo T2*-weighted MRI to learn robust, contrast-invariant representations. Our method integrates: (i) masked autoencoding (MAE) for self-supervised pretraining on unlabeled multi-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain adaptation across echo times; and (iii) global-local collaboration to align fine-grained features with global anatomical context. We further introduce a semantic matching loss to encourage representation consistency across echoes of the same subject. Experiments on a clinical multi-echo placental MRI dataset demonstrate that our approach generalizes effectively across echo times and outperforms both single-echo and naive fusion baselines. To our knowledge, this is the first work to systematically exploit multi-echo T2*-weighted MRI for placental segmentation. </p>
<blockquote>
<p>å‡†ç¡®çš„èƒç›˜åˆ†å‰²å¯¹äºèƒç›˜çš„å®šé‡åˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºT2*-åŠ æƒèƒç›˜æˆåƒçš„ä»¥ä¸‹ç‰¹ç‚¹ï¼Œè¿™ä¸€ä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼šï¼ˆ1ï¼‰å„ä¸ªå›å£°ä¹‹é—´çš„è¾¹ç•Œå¯¹æ¯”åº¦è¾ƒå¼±ä¸”ä¸ä¸€è‡´ï¼›ï¼ˆ2ï¼‰æ‰€æœ‰å›å£°æ—¶é—´å‡æ²¡æœ‰æ‰‹åŠ¨çœŸå®æ ‡æ³¨ï¼›ï¼ˆ3ï¼‰ç”±äºèƒå„¿å’Œæ¯äº²çš„ç§»åŠ¨ï¼Œå„å›å£°é—´å­˜åœ¨è¿åŠ¨ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹æ¯”å¢å¼ºåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šå›å£°T2*-åŠ æƒMRIä¸­çš„äº’è¡¥ä¿¡æ¯æ¥å­¦ä¹ ç¨³å¥ã€å¯¹æ¯”ä¸å˜çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†ï¼šï¼ˆiï¼‰ç”¨äºåœ¨æ— æ ‡ç­¾çš„å¤šå›å£°åˆ‡ç‰‡ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒçš„é¢ç½©è‡ªåŠ¨ç¼–ç ï¼ˆMAEï¼‰ï¼›ï¼ˆiiï¼‰ç”¨äºä¸åŒå›å£°æ—¶é—´è¿›è¡Œæ— ç›‘ç£åŸŸè‡ªé€‚åº”çš„é¢ç½©ä¼ªæ ‡ç­¾ï¼ˆMPLï¼‰ï¼›ï¼ˆiiiï¼‰å…¨å±€-å±€éƒ¨åä½œä»¥å¯¹ç²¾ç»†ç‰¹å¾è¿›è¡Œå…¨å±€è§£å‰–ä¸Šä¸‹æ–‡å¯¹é½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è¯­ä¹‰åŒ¹é…æŸå¤±ï¼Œä»¥é¼“åŠ±åŒä¸€å—è¯•è€…ä¸åŒå›å£°ä¹‹é—´çš„è¡¨ç¤ºä¸€è‡´æ€§ã€‚åœ¨ä¸´åºŠåŒ»å­¦å¤šå›å£°èƒç›˜MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒå›å£°æ—¶é—´ä¹‹é—´è¿›è¡Œæœ‰æ•ˆçš„æ³›åŒ–ï¼Œå¹¶ä¸”ä¼˜äºå•å›å£°å’Œç®€å•çš„èåˆåŸºçº¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°åˆ©ç”¨å¤šå›å£°T2*-åŠ æƒMRIè¿›è¡Œèƒç›˜åˆ†å‰²çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24739v1">PDF</a> 8 pages, 20 figures</p>
<p><strong>Summary</strong><br>     å‡†ç¡®è¿›è¡Œèƒç›˜åˆ†å‰²å¯¹äºèƒç›˜å®šé‡åˆ†æè‡³å…³é‡è¦ã€‚åœ¨T2*-åŠ æƒèƒç›˜æˆåƒä¸­ï¼Œç”±äºè¾¹ç•Œå¯¹æ¯”åº¦å¼±ã€ä¸åŒå›å£°é—´ç¼ºä¹æ‰‹åŠ¨çœŸå®æ ‡æ³¨ä»¥åŠèƒå„¿å’Œæ¯ä½“è¿åŠ¨å¼•èµ·çš„è¿åŠ¨ä¼ªå½±ï¼Œè¿™ä¸€ä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¯¹æ¯”å¢å¼ºåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šå›å£°T2*-åŠ æƒMRIä¸­çš„äº’è¡¥ä¿¡æ¯æ¥å­¦ä¹ ç¨³å¥ã€å¯¹æ¯”ä¸å˜çš„è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†ï¼šiï¼‰æ©ç è‡ªåŠ¨ç¼–ç ï¼ˆMAEï¼‰ç”¨äºåœ¨æ— æ ‡ç­¾çš„å¤šå›å£°åˆ‡ç‰‡ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼›iiï¼‰æ©ç ä¼ªæ ‡ç­¾ï¼ˆMPLï¼‰ç”¨äºä¸åŒå›å£°æ—¶é—´çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼›iiiï¼‰å…¨å±€å±€éƒ¨åä½œä»¥å°†ç²¾ç»†ç‰¹å¾ä¸å…¨å±€è§£å‰–ä¸Šä¸‹æ–‡å¯¹é½ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è¯­ä¹‰åŒ¹é…æŸå¤±ï¼Œä»¥é¼“åŠ±åŒä¸€å—è¯•è€…ä¸åŒå›å£°ä¹‹é—´çš„è¡¨ç¤ºä¸€è‡´æ€§ã€‚åœ¨ä¸´åºŠè¯•éªŒå¤šå›å£°èƒç›˜MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå›å£°æ—¶é—´ä¹‹é—´å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¼˜äºå•å›å£°å’Œç®€å•èåˆåŸºçº¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°åˆ©ç”¨å¤šå›å£°T2*-åŠ æƒMRIè¿›è¡Œèƒç›˜åˆ†å‰²çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®èƒç›˜åˆ†å‰²å¯¹èƒç›˜å®šé‡åˆ†æè‡³å…³é‡è¦ã€‚</li>
<li>T2*-åŠ æƒèƒç›˜æˆåƒä¸­çš„èƒç›˜åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦ç”±äºè¾¹ç•Œå¯¹æ¯”åº¦å¼±ã€ç¼ºä¹çœŸå®æ ‡æ³¨å’Œè¿åŠ¨ä¼ªå½±ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¯¹æ¯”å¢å¼ºåˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨å¤šå›å£°T2*-åŠ æƒMRIä¸­çš„ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆæ©ç è‡ªåŠ¨ç¼–ç ï¼ˆMAEï¼‰è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæ©ç ä¼ªæ ‡ç­¾ï¼ˆMPLï¼‰è¿›è¡ŒåŸŸè‡ªé€‚åº”ä»¥åŠå…¨å±€å±€éƒ¨åä½œã€‚</li>
<li>è¯­ä¹‰åŒ¹é…æŸå¤±è¢«å¼•å…¥ä»¥ç¡®ä¿ä¸åŒå›å£°é—´çš„è¡¨ç¤ºä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒå›å£°æ—¶é—´ä¹‹é—´å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶æ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°åˆ©ç”¨å¤šå›å£°T2*-åŠ æƒMRIè¿›è¡Œèƒç›˜åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc5f599d4203924725e20121808d44ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41bc33a69e63148f9dfebbc1e025e291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f078a55788b61427a46a15c0ba015725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a03a27496a6df2ba00e28b401277c97f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Decoupled-Competitive-Framework-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Decoupled-Competitive-Framework-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Decoupled Competitive Framework for Semi-supervised Medical Image   Segmentation"></a>Decoupled Competitive Framework for Semi-supervised Medical Image   Segmentation</h2><p><strong>Authors:Jiahe Chen, Jiahe Ying, Shen Wang, Jianwei Zheng</strong></p>
<p>Confronting the critical challenge of insufficiently annotated samples in medical domain, semi-supervised medical image segmentation (SSMIS) emerges as a promising solution. Specifically, most methodologies following the Mean Teacher (MT) or Dual Students (DS) architecture have achieved commendable results. However, to date, these approaches face a performance bottleneck due to two inherent limitations, \textit{e.g.}, the over-coupling problem within MT structure owing to the employment of exponential moving average (EMA) mechanism, as well as the severe cognitive bias between two students of DS structure, both of which potentially lead to reduced efficacy, or even model collapse eventually. To mitigate these issues, a Decoupled Competitive Framework (DCF) is elaborated in this work, which utilizes a straightforward competition mechanism for the update of EMA, effectively decoupling students and teachers in a dynamical manner. In addition, the seamless exchange of invaluable and precise insights is facilitated among students, guaranteeing a better learning paradigm. The DCF introduced undergoes rigorous validation on three publicly accessible datasets, which encompass both 2D and 3D datasets. The results demonstrate the superiority of our method over previous cutting-edge competitors. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/JiaheChen2002/DCF">https://github.com/JiaheChen2002/DCF</a>. </p>
<blockquote>
<p>é¢å¯¹åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ ·æœ¬ä¸è¶³çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚å…·ä½“æ¥è¯´ï¼Œå¤§å¤šæ•°éµå¾ªMean Teacherï¼ˆMTï¼‰æˆ–Dual Studentsï¼ˆDSï¼‰æ¶æ„çš„æ–¹æ³•éƒ½å–å¾—äº†å€¼å¾—èµæ‰¬çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œè¿™äº›æ–¹æ³•ç”±äºä¸¤ä¸ªå›ºæœ‰å±€é™è€Œé¢ä¸´æ€§èƒ½ç“¶é¢ˆï¼Œä¾‹å¦‚MTç»“æ„å†…çš„è¿‡åº¦è€¦åˆé—®é¢˜ï¼Œè¿™æ˜¯ç”±äºé‡‡ç”¨äº†æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æœºåˆ¶ï¼Œä»¥åŠDSç»“æ„ä¸­çš„ä¸¤ä¸ªå­¦ç”Ÿä¹‹é—´ä¸¥é‡çš„è®¤çŸ¥åå·®ã€‚è¿™ä¸¤ä¸ªé—®é¢˜éƒ½å¯èƒ½å¯¼è‡´æ•ˆç‡é™ä½ï¼Œç”šè‡³æœ€ç»ˆå¯¼è‡´æ¨¡å‹å´©æºƒã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡è¯¦ç»†é˜è¿°äº†ä¸€ä¸ªè§£è€¦ç«äº‰æ¡†æ¶ï¼ˆDCFï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç®€å•çš„ç«äº‰æœºåˆ¶æ¥æ›´æ–°EMAï¼Œä»¥åŠ¨æ€çš„æ–¹å¼æœ‰æ•ˆåœ°å°†å­¦ç”Ÿå’Œæ•™å¸ˆè§£è€¦ã€‚æ­¤å¤–ï¼Œå­¦ç”Ÿä¹‹é—´ä¿ƒè¿›äº†å®è´µè€Œç²¾ç¡®è§è§£çš„æ— ç¼äº¤æµï¼Œä¿è¯äº†æ›´å¥½çš„å­¦ä¹ èŒƒå¼ã€‚DCFåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ï¼ŒåŒ…æ‹¬2Då’Œ3Dæ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¹‹å‰çš„å°–ç«¯ç«äº‰å¯¹æ‰‹ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiaheChen2002/DCF%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JiaheChen2002/DCFä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24667v1">PDF</a> Published in ECAI 2024</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰é¢ä¸´ç¼ºä¹æ ‡æ³¨æ ·æœ¬çš„æŒ‘æˆ˜ï¼ŒMean Teacherï¼ˆMTï¼‰å’ŒDual Studentsï¼ˆDSï¼‰æ¶æ„çš„æ–¹æ³•è™½å–å¾—ä¸€å®šæˆæœï¼Œä½†å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºä¸€ç§Decoupled Competitive Frameworkï¼ˆDCFï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ç«äº‰æœºåˆ¶æ›´æ–°EMAï¼ŒåŠ¨æ€è§£è€¦å­¦ç”Ÿå’Œæ•™å¸ˆï¼Œå¹¶åœ¨å­¦ç”Ÿé—´è¿›è¡Œæœ‰ä»·å€¼çš„ä¿¡æ¯äº¤æ¢ï¼Œä¿è¯æ›´å¥½çš„å­¦ä¹ æ¨¡å¼ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå‰æ²¿ç«å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²(SSMIS)æ˜¯åº”å¯¹åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ ·æœ¬ä¸è¶³çš„æœ‰åŠ›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰ä¸»æµçš„Mean Teacherï¼ˆMTï¼‰å’ŒDual Studentsï¼ˆDSï¼‰æ¶æ„æ–¹æ³•å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>Decoupled Competitive Frameworkï¼ˆDCFï¼‰æ¡†æ¶è¢«æå‡ºä»¥è§£å†³MTå’ŒDSçš„é—®é¢˜ã€‚</li>
<li>DCFåˆ©ç”¨ç«äº‰æœºåˆ¶æ›´æ–°EMAï¼Œæœ‰æ•ˆè§£è€¦å­¦ç”Ÿå’Œæ•™å¸ˆã€‚</li>
<li>DCFæ¡†æ¶åœ¨å­¦ç”Ÿé—´è¿›è¡Œæœ‰ä»·å€¼çš„ä¿¡æ¯äº¤æ¢ï¼Œä¿è¯æ›´å¥½çš„å­¦ä¹ æ¨¡å¼ã€‚</li>
<li>DCFåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8efdd05654c5892ad7525b8674e54e03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9333561f5e3484d9f539efd378d445c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2b236320ba4fc7f153179ee6d4c8fc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf4f4d677af9712405fa3df302b04905.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Power-of-Intermediate-Domains-for-Mixed-Domain-Semi-Supervised-Medical-Image-Segmentation"><a href="#Unleashing-the-Power-of-Intermediate-Domains-for-Mixed-Domain-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Unleashing the Power of Intermediate Domains for Mixed Domain   Semi-Supervised Medical Image Segmentation"></a>Unleashing the Power of Intermediate Domains for Mixed Domain   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao</strong></p>
<p>Both limited annotation and domain shift are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately. However, the coexistence of limited annotation and domain shift is quite common, which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medical image Segmentation (MiDSS), where limited labeled data from a single domain and a large amount of unlabeled data from multiple domains. To tackle this issue, we propose the UST-RUN framework, which fully leverages intermediate domain information to facilitate knowledge transfer. We employ Unified Copy-paste (UCP) to construct intermediate domains, and propose a Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by merging pseudo-labels from intermediate samples. Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples. To generate more diverse intermediate samples, we further select reliable samples with high-quality pseudo-labels, which are then mixed with other unlabeled data. Additionally, we generate sophisticated intermediate samples with high-quality pseudo-labels for unreliable samples, ensuring effective knowledge transfer for them. Extensive experiments on four public datasets demonstrate the superiority of UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the Prostate dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MQinghe/UST-RUN">https://github.com/MQinghe/UST-RUN</a> </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œæ ‡æ³¨æ•°æ®æœ‰é™å’Œé¢†åŸŸåç§»æ˜¯ä¸¤ä¸ªæ™®éå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åŠç›‘ç£åˆ†å‰²å’Œæ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•åˆ†åˆ«è§£å†³è¿™äº›é—®é¢˜ä¸­çš„ä¸€ä¸ªã€‚ç„¶è€Œï¼Œæ ‡æ³¨æ•°æ®æœ‰é™å’Œé¢†åŸŸåç§»çš„å…±å­˜æ˜¯ç›¸å½“å¸¸è§çš„ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬å¼•å…¥ä¸€ç§æ–°é¢–ä¸”å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼šæ··åˆé¢†åŸŸåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMiDSSï¼‰ï¼Œå…¶ä¸­æ¶‰åŠå•ä¸ªé¢†åŸŸçš„æœ‰é™æ ‡æ³¨æ•°æ®å’Œå¤šä¸ªé¢†åŸŸçš„å¤§é‡æœªæ ‡æ³¨æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UST-RUNæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨ä¸­é—´é¢†åŸŸä¿¡æ¯æ¥ä¿ƒè¿›çŸ¥è¯†è½¬ç§»ã€‚æˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€å¤åˆ¶ç²˜è´´ï¼ˆUCPï¼‰æ¥æ„å»ºä¸­é—´é¢†åŸŸï¼Œå¹¶æå‡ºå¯¹ç§°å¼•å¯¼è®­ç»ƒç­–ç•¥ï¼ˆSymGDï¼‰æ¥é€šè¿‡åˆå¹¶ä¸­é—´æ ·æœ¬çš„ä¼ªæ ‡ç­¾æ¥ç›‘ç£æœªæ ‡æ³¨æ•°æ®ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹æ„ŸçŸ¥éšæœºå¹…åº¦æ··åˆï¼ˆTP-RAMï¼‰ï¼Œä»¥é€æ­¥å°†é£æ ¼è½¬æ¢ç»„ä»¶èå…¥ä¸­é—´æ ·æœ¬ã€‚ä¸ºäº†ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„ä¸­é—´æ ·æœ¬ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€‰æ‹©å…·æœ‰é«˜è´¨é‡ä¼ªæ ‡ç­¾çš„å¯é æ ·æœ¬ï¼Œå¹¶å°†å…¶ä¸å…¶ä»–æœªæ ‡æ³¨æ•°æ®æ··åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºä¸å¯é çš„æ ·æœ¬ç”Ÿæˆå…·æœ‰é«˜è´¨é‡ä¼ªæ ‡ç­¾çš„ç²¾ç»†ä¸­é—´æ ·æœ¬ï¼Œä»¥ç¡®ä¿å¯¹å®ƒä»¬è¿›è¡Œæœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†UST-RUNçš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUST-RUNåœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†æé«˜äº†1 ç»“é¢˜ä¸»è¦å›´ç»•åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸä¸­æ ‡æ³¨æ•°æ®æœ‰é™å’Œé¢†åŸŸåç§»è¿™ä¸¤ä¸ªæŒ‘æˆ˜å±•å¼€ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€åªèƒ½å•ç‹¬è§£å†³å…¶ä¸­ä¸€ä¸ªé—®é¢˜ï¼Œè€Œæˆ‘ä»¬çš„UST-RUNæ¡†æ¶åˆ™èƒ½åŒæ—¶åº”å¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é€šè¿‡æ„å»ºä¸­é—´é¢†åŸŸã€åˆ©ç”¨å¯¹ç§°å¼•å¯¼è®­ç»ƒç­–ç•¥ä»¥åŠå¼•å…¥éšæœºå¹…åº¦æ··åˆç­‰æŠ€æœ¯æ‰‹æ®µï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šï¼ŒDiceå¾—åˆ†æé«˜äº†é«˜è¾¾ åäºŒåˆ†ä¹‹ä¸€ç‚¹ä¹å›› ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨ <a target="_blank" rel="noopener" href="https://github.com/MQinghe/UST-RUN">https://github.com/MQinghe/UST-RUN</a> ä¾›å¤§å®¶å‚è€ƒå’Œä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24567v1">PDF</a> Accepted by IEEE TMI 2025. arXiv admin note: text overlap with   arXiv:2404.08951</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æœ‰é™æ ‡æ³¨å’Œé¢†åŸŸå·®å¼‚ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¼ ç»ŸåŠç›‘ç£åˆ†å‰²å’Œæ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•é€šå¸¸å•ç‹¬è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“è¿™ä¸¤ä¸ªé—®é¢˜åŒæ—¶å­˜åœ¨æ—¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„åœºæ™¯ï¼šæ··åˆåŸŸåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMiDSSï¼‰ï¼Œå…¶ä¸­æ¶‰åŠå•ä¸€é¢†åŸŸçš„æœ‰é™æ ‡æ³¨æ•°æ®ä»¥åŠå¤šä¸ªé¢†åŸŸçš„å¤§é‡æ— æ ‡æ³¨æ•°æ®ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UST-RUNæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨ä¸­é—´åŸŸä¿¡æ¯ä¿ƒè¿›çŸ¥è¯†è½¬ç§»ã€‚æˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€å¤åˆ¶ç²˜è´´ï¼ˆUCPï¼‰æŠ€æœ¯æ„å»ºä¸­é—´åŸŸï¼Œå¹¶æå‡ºå¯¹ç§°å¼•å¯¼è®­ç»ƒç­–ç•¥ï¼ˆSymGDï¼‰æ¥é€šè¿‡ä¸­é—´æ ·æœ¬çš„ä¼ªæ ‡ç­¾æ¥ç›‘ç£æ— æ ‡æ³¨æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è®­ç»ƒè¿‡ç¨‹æ„ŸçŸ¥éšæœºå¹…åº¦æ··åˆï¼ˆTP-RAMï¼‰ï¼Œé€æ­¥å°†é£æ ¼è½¬æ¢ç»„ä»¶èå…¥ä¸­é—´æ ·æœ¬ã€‚ä¸ºäº†ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„ä¸­é—´æ ·æœ¬ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€‰æ‹©é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾æ ·æœ¬ï¼Œå¹¶å°†å…¶ä¸å…¶ä»–æ— æ ‡æ³¨æ•°æ®æ··åˆã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ä¸ºä¸å¯é çš„æ ·æœ¬ç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾æ ·æœ¬ï¼Œç¡®ä¿å®ƒä»¬çš„çŸ¥è¯†è½¬ç§»æœ‰æ•ˆã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†UST-RUNçš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šï¼ŒDiceå¾—åˆ†æé«˜äº†12.94%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æœ‰é™æ ‡æ³¨å’Œé¢†åŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å•ç‹¬è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†MiDSSåœºæ™¯ä¸­å­˜åœ¨ä¸¤è€…çš„ç»“åˆã€‚</li>
<li>UST-RUNæ¡†æ¶è¢«æå‡ºä»¥åº”å¯¹è¿™ç§åœºæ™¯ï¼Œå……åˆ†åˆ©ç”¨ä¸­é—´åŸŸä¿¡æ¯ã€‚</li>
<li>UCPç”¨äºæ„å»ºä¸­é—´åŸŸï¼ŒSymGDç”¨äºç›‘ç£æ— æ ‡æ³¨æ•°æ®ã€‚</li>
<li>TP-RAMé€æ­¥èå…¥é£æ ¼è½¬æ¢ç»„ä»¶åˆ°ä¸­é—´æ ·æœ¬ä¸­ã€‚</li>
<li>é€‰æ‹©é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾æ ·æœ¬ä¸å…¶ä»–æ— æ ‡æ³¨æ•°æ®æ··åˆï¼Œæé«˜æ ·æœ¬å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23b535d0bad13fcc51e9f36c55a764b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfb61d9ae2cd6789e3b3392d119b9d56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee2ebcecc9edef67bafacb0490622927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e32cadc83f6022a44875516587c9f378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6024fd215af54aa431e7c1987935ee4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a18c4f11fbeef70189f7996b17ee4c2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Digital-twins-enable-full-reference-quality-assessment-of-photoacoustic-image-reconstructions"><a href="#Digital-twins-enable-full-reference-quality-assessment-of-photoacoustic-image-reconstructions" class="headerlink" title="Digital twins enable full-reference quality assessment of photoacoustic   image reconstructions"></a>Digital twins enable full-reference quality assessment of photoacoustic   image reconstructions</h2><p><strong>Authors:Janek GrÃ¶hl, Leonid Kunyansky, Jenni Poimala, Thomas R. Else, Francesca Di Cecio, Sarah E. Bohndiek, Ben T. Cox, Andreas Hauptmann</strong></p>
<p>Quantitative comparison of the quality of photoacoustic image reconstruction algorithms remains a major challenge. No-reference image quality measures are often inadequate, but full-reference measures require access to an ideal reference image. While the ground truth is known in simulations, it is unknown in vivo, or in phantom studies, as the reference depends on both the phantom properties and the imaging system. We tackle this problem by using numerical digital twins of tissue-mimicking phantoms and the imaging system to perform a quantitative calibration to reduce the simulation gap. The contributions of this paper are two-fold: First, we use this digital-twin framework to compare multiple state-of-the-art reconstruction algorithms. Second, among these is a Fourier transform-based reconstruction algorithm for circular detection geometries, which we test on experimental data for the first time. Our results demonstrate the usefulness of digital phantom twins by enabling assessment of the accuracy of the numerical forward model and enabling comparison of image reconstruction schemes with full-reference image quality assessment. We show that the Fourier transform-based algorithm yields results comparable to those of iterative time reversal, but at a lower computational cost. All data and code are publicly available on Zenodo: <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15388429">https://doi.org/10.5281/zenodo.15388429</a>. </p>
<blockquote>
<p>å¯¹äºå…‰å£°å›¾åƒé‡å»ºç®—æ³•è´¨é‡çš„å®šé‡æ¯”è¾ƒä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æªæ–½å¾€å¾€ä¸è¶³ï¼Œè€Œå…¨å‚è€ƒæªæ–½åˆ™éœ€è¦ç†æƒ³çš„å‚è€ƒå›¾åƒã€‚è™½ç„¶æ¨¡æ‹Ÿä¸­çš„çœŸå®æƒ…å†µå·²çŸ¥ï¼Œä½†åœ¨çœŸå®ç”Ÿç‰©ä½“å†…æˆ–å¹»å½±ç ”ç©¶ä¸­çš„çœŸå®æƒ…å†µæœªçŸ¥ï¼Œå› ä¸ºå‚è€ƒæƒ…å†µæ—¢å–å†³äºå¹»å½±çš„ç‰¹æ€§ä¹Ÿå–å†³äºæˆåƒç³»ç»Ÿã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç»„ç»‡æ¨¡æ‹Ÿå¹»å½±å’Œæˆåƒç³»ç»Ÿçš„æ•°å­—åŒèƒèƒè¿›è¡Œå®šé‡æ ¡å‡†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥ç¼©å°æ¨¡æ‹Ÿå·®è·ã€‚æœ¬æ–‡çš„è´¡çŒ®æœ‰ä¸¤æ–¹é¢ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°å­—åŒèƒèƒæ¡†æ¶æ¥æ¯”è¾ƒå¤šä¸ªæœ€æ–°é‡å»ºç®—æ³•ã€‚å…¶æ¬¡ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯åŸºäºå‚…é‡Œå¶å˜æ¢çš„åœ†å½¢æ£€æµ‹å‡ ä½•é‡å»ºç®—æ³•ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†å…¶æµ‹è¯•åœ¨çœŸå®æ•°æ®ä¸Šã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†æ•°å­—åŒèƒèƒå¹»å½±çš„å®ç”¨æ€§ï¼Œå¯ä»¥é€šè¿‡è¯„ä¼°æ•°å€¼å‰å‘æ¨¡å‹çš„å‡†ç¡®æ€§æ¥æ¯”è¾ƒå›¾åƒé‡å»ºæ–¹æ¡ˆä¸å…¨å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒåŸºäºå‚…é‡Œå¶å˜æ¢çš„ç®—æ³•çš„ç»“æœä¸è¿­ä»£æ—¶é—´åè½¬çš„ç»“æœç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å·²åœ¨Zenodoä¸Šå…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15388429">https://doi.org/10.5281/zenodo.15388429ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨æ•°å­—åŒèƒèƒæ¡†æ¶å¯¹å¤šç§å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå…¶ä¸­é¦–æ¬¡åœ¨å®éªŒæ•°æ®ä¸Šæµ‹è¯•äº†åŸºäºå‚…é‡Œå¶å˜æ¢çš„é‡å»ºç®—æ³•ã€‚è¯¥ç ”ç©¶è§£å†³äº†æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°çš„å±€é™æ€§ï¼Œé€šè¿‡æ•°å­—åŒèƒèƒæŠ€æœ¯å¯¹æˆåƒç³»ç»Ÿè¿›è¡Œå®šé‡æ ¡å‡†ï¼Œå‡å°‘æ¨¡æ‹Ÿå·®è·ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºå‚…é‡Œå¶å˜æ¢çš„ç®—æ³•è™½ç„¶è®¡ç®—æˆæœ¬è¾ƒä½ï¼Œä½†ç»“æœå¯ä¸è¿­ä»£æ—¶é—´åè½¬ç›¸æ¯”ã€‚æ‰€æœ‰æ•°æ®å’Œæ–¹æ³•åœ¨Zenodoå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®šé‡æ¯”è¾ƒå…‰å£°å›¾åƒé‡å»ºç®—æ³•çš„è´¨é‡ä»æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æªæ–½é€šå¸¸ä¸è¶³ï¼Œè€Œå…¨å‚è€ƒæªæ–½éœ€è¦ç†æƒ³å‚è€ƒå›¾åƒã€‚</li>
<li>æ•°å­—åŒ–åŒèƒèƒæ¡†æ¶ç”¨äºæ¯”è¾ƒå¤šç§å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•ã€‚</li>
<li>åŸºäºå‚…é‡Œå¶å˜æ¢çš„é‡å»ºç®—æ³•åœ¨æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>æ•°å­—åŒèƒèƒæŠ€æœ¯è§£å†³äº†æ¨¡æ‹Ÿå·®è·é—®é¢˜ï¼Œå¯é€šè¿‡å®šé‡æ ¡å‡†å¯¹æˆåƒç³»ç»Ÿè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åŸºäºå‚…é‡Œå¶å˜æ¢çš„ç®—æ³•ç»“æœä¸è¿­ä»£æ—¶é—´åè½¬ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f93850a4a63fd80fc438ef52c1c074ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04fdb9b582a041c78dc74b36a761bdd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd45b8c51fe1b152b205d3302c0e8efe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ACM-UNet-Adaptive-Integration-of-CNNs-and-Mamba-for-Efficient-Medical-Image-Segmentation"><a href="#ACM-UNet-Adaptive-Integration-of-CNNs-and-Mamba-for-Efficient-Medical-Image-Segmentation" class="headerlink" title="ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical   Image Segmentation"></a>ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical   Image Segmentation</h2><p><strong>Authors:Jing Huang, Yongkang Zhao, Yuhan Li, Zhitao Dai, Cheng Chen, Qiying Lai</strong></p>
<p>The U-shaped encoder-decoder architecture with skip connections has become a prevailing paradigm in medical image segmentation due to its simplicity and effectiveness. While many recent works aim to improve this framework by designing more powerful encoders and decoders, employing advanced convolutional neural networks (CNNs) for local feature extraction, Transformers or state space models (SSMs) such as Mamba for global context modeling, or hybrid combinations of both, these methods often struggle to fully utilize pretrained vision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To bridge this gap, we introduce ACM-UNet, a general-purpose segmentation framework that retains a simple UNet-like design while effectively incorporating pretrained CNNs and Mamba models through a lightweight adapter mechanism. This adapter resolves architectural incompatibilities and enables the model to harness the complementary strengths of CNNs and SSMs-namely, fine-grained local detail extraction and long-range dependency modeling. Additionally, we propose a hierarchical multi-scale wavelet transform module in the decoder to enhance feature fusion and reconstruction fidelity. Extensive experiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet achieves state-of-the-art performance while remaining computationally efficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the Synapse dataset with 17.93G FLOPs, showcasing its effectiveness and scalability. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/zyklcode/ACM-UNet">https://github.com/zyklcode/ACM-UNet</a>. </p>
<blockquote>
<p>å¸¦æœ‰è·³è¿‡è¿æ¥çš„Uå‹ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç”±äºå…¶ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸»æµèŒƒå¼ã€‚è™½ç„¶æœ€è¿‘è®¸å¤šå·¥ä½œæ—¨åœ¨é€šè¿‡è®¾è®¡æ›´å¼ºå¤§çš„ç¼–ç å™¨å’Œè§£ç å™¨ã€ä½¿ç”¨å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ã€ä½¿ç”¨Transformeræˆ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å¦‚Mambaè¿›è¡Œå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæˆ–ä¸¤è€…çš„æ··åˆç»„åˆæ¥æ”¹è¿›æ­¤æ¡†æ¶ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€éš¾ä»¥å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰ä¸»å¹²ï¼ˆä¾‹å¦‚ResNetã€ViTã€VMambaï¼‰ç½‘ç»œï¼Œè¿™æ˜¯ç”±äºç»“æ„ä¸åŒ¹é…é€ æˆçš„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ACM-UNetï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨åˆ†å‰²æ¡†æ¶ï¼Œå®ƒä¿ç•™äº†ç®€å•çš„UNetè®¾è®¡ï¼ŒåŒæ—¶é€šè¿‡è½»é‡çº§é€‚é…å™¨æœºåˆ¶æœ‰æ•ˆåœ°ç»“åˆäº†é¢„è®­ç»ƒçš„CNNå’ŒMambaæ¨¡å‹ã€‚è¯¥é€‚é…å™¨è§£å†³äº†æ¶æ„ä¸å…¼å®¹é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨CNNå’ŒSSMçš„äº’è¡¥ä¼˜åŠ¿â€”â€”å³ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚æå–å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è§£ç å™¨ä¸­æå‡ºäº†åˆ†å±‚å¤šå°ºåº¦å°æ³¢å˜æ¢æ¨¡å—ï¼Œä»¥å¢å¼ºç‰¹å¾èåˆå’Œé‡å»ºä¿çœŸåº¦ã€‚åœ¨Synapseå’ŒACDCåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒACM-UNetåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨Synapseæ•°æ®é›†ä¸Šè¾¾åˆ°äº†85.12%çš„Diceåˆ†æ•°å’Œ13.89mmçš„HD95ï¼ŒåŒæ—¶åªéœ€17.93G FLOPsï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/zyklcode/ACM-UNet%E3%80%82">https://github.com/zyklcode/ACM-UNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24481v1">PDF</a> 10 pages, 3 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºACM-UNetçš„é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨Uå‹ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡è½»é‡çº§é€‚é…å™¨æœºåˆ¶æœ‰æ•ˆç»“åˆäº†é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒMambaæ¨¡å‹ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç»“æ„ä¸åŒ¹é…çš„é—®é¢˜ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨CNNå’ŒSSMçš„äº’è¡¥ä¼˜åŠ¿ï¼Œå®ç°äº†ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚æå–å’Œé•¿è·ç¦»ä¾èµ–å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å±‚æ¬¡åŒ–çš„å¤šå°ºåº¦å°æ³¢å˜æ¢æ¨¡å—ï¼Œå¢å¼ºäº†ç‰¹å¾èåˆå’Œé‡å»ºçš„ä¿çœŸåº¦ã€‚åœ¨Synapseå’ŒACDCåŸºå‡†æµ‹è¯•ä¸­ï¼ŒACM-UNetå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ACM-UNetæ˜¯ä¸€ç§åŸºäºUå‹ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰ä¸»å¹²ç»“æ„ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è½»é‡çº§é€‚é…å™¨æœºåˆ¶ï¼ŒACM-UNetæœ‰æ•ˆç»“åˆäº†CNNå’ŒMambaæ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨ä¸¤è€…çš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>æ¡†æ¶ä¸­æå‡ºäº†å±‚æ¬¡åŒ–çš„å¤šå°ºåº¦å°æ³¢å˜æ¢æ¨¡å—ï¼Œå¢å¼ºäº†ç‰¹å¾èåˆå’Œé‡å»ºçš„ä¿çœŸåº¦ã€‚</li>
<li>ACM-UNetåœ¨Synapseå’ŒACDCåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ACM-UNetå®ç°äº†85.12%çš„Diceå¾—åˆ†å’Œ13.89mmçš„HD95å€¼ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67e6ab053f7f47f749f0abd65c24121c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb5659756da0ac231f15265df7370ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5eaed0f29215abebe7c6e7dbb75dfa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6894cec178a2eeb35e81df5229a875.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="pyMEAL-A-Multi-Encoder-Augmentation-Aware-Learning-for-Robust-and-Generalizable-Medical-Image-Translation"><a href="#pyMEAL-A-Multi-Encoder-Augmentation-Aware-Learning-for-Robust-and-Generalizable-Medical-Image-Translation" class="headerlink" title="pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and   Generalizable Medical Image Translation"></a>pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and   Generalizable Medical Image Translation</h2><p><strong>Authors:Abdul-mojeed Olabisi Ilyas, Adeleke Maradesa, Jamal Banzi, Jianpan Huang, Henry K. F. Mak, Kannie W. Y. Chan</strong></p>
<p>Medical imaging is critical for diagnostics, but clinical adoption of advanced AI-driven imaging faces challenges due to patient variability, image artifacts, and limited model generalization. While deep learning has transformed image analysis, 3D medical imaging still suffers from data scarcity and inconsistencies due to acquisition protocols, scanner differences, and patient motion. Traditional augmentation uses a single pipeline for all transformations, disregarding the unique traits of each augmentation and struggling with large data volumes.   To address these challenges, we propose a Multi-encoder Augmentation-Aware Learning (MEAL) framework that leverages four distinct augmentation variants processed through dedicated encoders. Three fusion strategies such as concatenation (CC), fusion layer (FL), and adaptive controller block (BD) are integrated to build multi-encoder models that combine augmentation-specific features before decoding. MEAL-BD uniquely preserves augmentation-aware representations, enabling robust, protocol-invariant feature learning.   As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic Resonance Imaging (MRI) translation study, MEAL-BD consistently achieved the best performance on both unseen- and predefined-test data. On both geometric transformations (like rotations and flips) and non-augmented inputs, MEAL-BD outperformed other competing methods, achieving higher mean peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) scores. These results establish MEAL as a reliable framework for preserving structural fidelity and generalizing across clinically relevant variability. By reframing augmentation as a source of diverse, generalizable features, MEAL supports robust, protocol-invariant learning, advancing clinically reliable medical imaging solutions. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒå¯¹äºè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å…ˆè¿›çš„AIé©±åŠ¨çš„æˆåƒåœ¨ä¸´åºŠåº”ç”¨ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‚£è€…å·®å¼‚ã€å›¾åƒä¼ªå½±å’Œæ¨¡å‹æ³›åŒ–æœ‰é™ç­‰é—®é¢˜ã€‚æ·±åº¦å­¦ä¹ å·²ç»æ”¹å˜äº†å›¾åƒåˆ†æé¢†åŸŸï¼Œä½†3DåŒ»å­¦æˆåƒä»ç„¶å› é‡‡é›†åè®®ã€æ‰«æä»ªå·®å¼‚ã€æ‚£è€…ç§»åŠ¨ç­‰å› ç´ è€Œé¢ä¸´æ•°æ®ç¨€ç¼ºå’Œä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¼ ç»Ÿå¢å¼ºæ–¹æ³•ä½¿ç”¨å•ä¸€ç®¡é“è¿›è¡Œæ‰€æœ‰è½¬æ¢ï¼Œå¿½ç•¥äº†æ¯ç§å¢å¼ºçš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†å¤§é‡æ•°æ®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šç¼–ç å™¨å¢å¼ºæ„ŸçŸ¥å­¦ä¹ ï¼ˆMEALï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å››ç§ä¸åŒçš„å¢å¼ºå˜ä½“ï¼Œå¹¶é€šè¿‡ä¸“ç”¨ç¼–ç å™¨è¿›è¡Œå¤„ç†ã€‚é›†æˆäº†ä¸‰ç§èåˆç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸²è”ï¼ˆCCï¼‰ã€èåˆå±‚ï¼ˆFLï¼‰å’Œè‡ªé€‚åº”æ§åˆ¶å™¨å—ï¼ˆBDï¼‰ï¼Œä»¥æ„å»ºå¤šç¼–ç å™¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨è§£ç ä¹‹å‰ç»„åˆå¢å¼ºç‰¹å®šçš„ç‰¹å¾ã€‚MEAL-BDç‹¬ç‰¹åœ°ä¿ç•™äº†å¢å¼ºæ„ŸçŸ¥è¡¨ç¤ºï¼Œå®ç°äº†ç¨³å¥çš„ã€åè®®ä¸å˜çš„ç‰¹å¾å­¦ä¹ ã€‚åœ¨CTåˆ°T1åŠ æƒMRIçš„è½¬æ¢ç ”ç©¶ä¸­ï¼ŒMEAL-BDåœ¨æœªè§è¿‡çš„å’Œé¢„å®šä¹‰æµ‹è¯•æ•°æ®ä¸Šå‡è¡¨ç°æœ€ä½³ã€‚åœ¨å‡ ä½•è½¬æ¢ï¼ˆå¦‚æ—‹è½¬å’Œç¿»è½¬ï¼‰å’Œéå¢å¼ºè¾“å…¥æ–¹é¢ï¼ŒMEAL-BDä¹Ÿä¼˜äºå…¶ä»–ç«äº‰æ–¹æ³•ï¼Œè·å¾—äº†æ›´é«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å¾—åˆ†ã€‚è¿™äº›ç»“æœè¯æ˜äº†MEALä½œä¸ºä¸€ä¸ªå¯é æ¡†æ¶åœ¨ä¿æŒç»“æ„ä¿çœŸåº¦å’Œæ³›åŒ–ä¸´åºŠç›¸å…³å˜å¼‚æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å°†å¢å¼ºé‡æ„ä¸ºå¤šæ ·åŒ–å’Œå¯æ³›åŒ–çš„ç‰¹å¾çš„æ¥æºï¼ŒMEALæ”¯æŒç¨³å¥çš„ã€åè®®ä¸å˜çš„å­¦ä¹ ï¼Œæ¨åŠ¨ä¸´åºŠå¯é çš„åŒ»å­¦æˆåƒè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24421v1">PDF</a> 36 pages, 9 figures, 2 tables</p>
<p><strong>æ‘˜è¦</strong><br>     åŒ»å­¦æˆåƒå¯¹äºè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ä¸´åºŠé‡‡ç”¨å…ˆè¿›çš„AIé©±åŠ¨æˆåƒé¢ä¸´æ‚£è€…å·®å¼‚æ€§ã€å›¾åƒä¼ªå½±å’Œæ¨¡å‹æ³›åŒ–æœ‰é™ç­‰æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ è™½å·²æ”¹å˜å›¾åƒåˆ†æé¢†åŸŸï¼Œä½†3DåŒ»å­¦æˆåƒä»å› é‡‡é›†åè®®ã€æ‰«æä»ªå·®å¼‚å’Œæ‚£è€…è¿åŠ¨ç­‰å› ç´ è€Œé¢ä¸´æ•°æ®ç¨€ç¼ºå’Œä¸ä¸€è‡´æ€§ç­‰é—®é¢˜ã€‚ä¼ ç»Ÿå¢å¼ºæ–¹æ³•ä½¿ç”¨å•ä¸€ç®¡é“è¿›è¡Œæ‰€æœ‰è½¬æ¢ï¼Œå¿½ç•¥äº†æ¯ç§å¢å¼ºçš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶éš¾ä»¥å¤„ç†å¤§é‡æ•°æ®ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç¼–ç å™¨å¢å¼ºæ„ŸçŸ¥å­¦ä¹ ï¼ˆMEALï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å››ç§ä¸åŒçš„å¢å¼ºå˜ä½“ï¼Œé€šè¿‡ä¸“ç”¨ç¼–ç å™¨è¿›è¡Œå¤„ç†ã€‚é›†æˆäº†ä¸‰ç§èåˆç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸²è”ï¼ˆCCï¼‰ã€èåˆå±‚ï¼ˆFLï¼‰å’Œè‡ªé€‚åº”æ§åˆ¶å™¨å—ï¼ˆBDï¼‰ï¼Œä»¥æ„å»ºå¤šç¼–ç å™¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨è§£ç ä¹‹å‰ç»“åˆå¢å¼ºç‰¹å®šåŠŸèƒ½ã€‚MEAL-BDç‹¬ç‰¹åœ°ä¿ç•™äº†å¢å¼ºæ„ŸçŸ¥è¡¨ç¤ºï¼Œèƒ½å¤Ÿå®ç°ç¨³å¥çš„åè®®ä¸å˜ç‰¹å¾å­¦ä¹ ã€‚åœ¨CTåˆ°T1åŠ æƒMRIçš„ç¿»è¯‘ç ”ç©¶ä¸­ï¼ŒMEAL-BDåœ¨æœªè§è¿‡å’Œé¢„å®šä¹‰æµ‹è¯•æ•°æ®ä¸Šå‡è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚åœ¨å‡ ä½•è½¬æ¢ï¼ˆå¦‚æ—‹è½¬å’Œç¿»è½¬ï¼‰å’Œéå¢å¼ºè¾“å…¥ä¸Šï¼ŒMEAL-BDå‡ä¼˜äºå…¶ä»–ç«äº‰æ–¹æ³•ï¼Œè·å¾—æ›´é«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰åˆ†æ•°ã€‚è¿™äº›ç»“æœè¯æ˜äº†MEALä½œä¸ºä¸€ä¸ªå¯é æ¡†æ¶åœ¨ä¿ç•™ç»“æ„ä¿çœŸåº¦å’Œæ³›åŒ–ä¸´åºŠç›¸å…³å˜åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å°†å¢å¼ºé‡æ„ä¸ºå¤šæ ·åŒ–å’Œå¯æ³›åŒ–çš„ç‰¹å¾æºï¼ŒMEALæ”¯æŒç¨³å¥çš„åè®®ä¸å˜å­¦ä¹ ï¼Œæ¨åŠ¨ä¸´åºŠå¯é çš„åŒ»å­¦æˆåƒè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒåœ¨ä¸´åºŠè¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ‚£è€…å·®å¼‚æ€§ã€å›¾åƒä¼ªå½±å’Œæ¨¡å‹æ³›åŒ–æœ‰é™ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å›¾åƒåˆ†æä¸­çš„åº”ç”¨å—åˆ°3DåŒ»å­¦æˆåƒæ•°æ®ç¨€ç¼ºå’Œä¸ä¸€è‡´æ€§çš„é™åˆ¶ã€‚</li>
<li>ä¼ ç»Ÿå¢å¼ºæ–¹æ³•å¿½è§†æ¯ç§å¢å¼ºçš„ç‹¬ç‰¹ç‰¹å¾ï¼Œéš¾ä»¥å¤„ç†å¤§é‡æ•°æ®ã€‚</li>
<li>MEALæ¡†æ¶åˆ©ç”¨å¤šç¼–ç å™¨ç»“åˆå››ç§å¢å¼ºå˜ä½“å’Œä¸“ç”¨ç¼–ç å™¨å¤„ç†ã€‚</li>
<li>MEALæ¡†æ¶é€šè¿‡èåˆç­–ç•¥ç»“åˆå¢å¼ºç‰¹å®šåŠŸèƒ½ï¼Œå¹¶ç‹¬ç‰¹åœ°ä¿ç•™å¢å¼ºæ„ŸçŸ¥è¡¨ç¤ºã€‚</li>
<li>åœ¨CTåˆ°MRIçš„ç¿»è¯‘ç ”ç©¶ä¸­ï¼ŒMEAL-BDè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce31087fc12afd568317533dbd64cc24.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Novel-Coronary-Artery-Registration-Method-Based-on-Super-pixel-Particle-Swarm-Optimization"><a href="#A-Novel-Coronary-Artery-Registration-Method-Based-on-Super-pixel-Particle-Swarm-Optimization" class="headerlink" title="A Novel Coronary Artery Registration Method Based on Super-pixel   Particle Swarm Optimization"></a>A Novel Coronary Artery Registration Method Based on Super-pixel   Particle Swarm Optimization</h2><p><strong>Authors:Peng Qi, Wenxi Qu, Tianliang Yao, Haonan Ma, Dylan Wintle, Yinyi Lai, Giorgos Papanastasiou, Chengjia Wang</strong></p>
<p>Percutaneous Coronary Intervention (PCI) is a minimally invasive procedure that improves coronary blood flow and treats coronary artery disease. Although PCI typically requires 2D X-ray angiography (XRA) to guide catheter placement at real-time, computed tomography angiography (CTA) may substantially improve PCI by providing precise information of 3D vascular anatomy and status. To leverage real-time XRA and detailed 3D CTA anatomy for PCI, accurate multimodal image registration of XRA and CTA is required, to guide the procedure and avoid complications. This is a challenging process as it requires registration of images from different geometrical modalities (2D -&gt; 3D and vice versa), with variations in contrast and noise levels. In this paper, we propose a novel multimodal coronary artery image registration method based on a swarm optimization algorithm, which effectively addresses challenges such as large deformations, low contrast, and noise across these imaging modalities. Our algorithm consists of two main modules: 1) preprocessing of XRA and CTA images separately, and 2) a registration module based on feature extraction using the Steger and Superpixel Particle Swarm Optimization algorithms. Our technique was evaluated on a pilot dataset of 28 pairs of XRA and CTA images from 10 patients who underwent PCI. The algorithm was compared with four state-of-the-art (SOTA) methods in terms of registration accuracy, robustness, and efficiency. Our method outperformed the selected SOTA baselines in all aspects. Experimental results demonstrate the significant effectiveness of our algorithm, surpassing the previous benchmarks and proposes a novel clinical approach that can potentially have merit for improving patient outcomes in coronary artery disease. </p>
<blockquote>
<p>ç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—ï¼ˆPCIï¼‰æ˜¯ä¸€ç§å¾®åˆ›æ‰‹æœ¯ï¼Œæ—¨åœ¨æ”¹å–„å† çŠ¶åŠ¨è„‰è¡€æµå¹¶æ²»ç–—å† çŠ¶åŠ¨è„‰ç–¾ç—…ã€‚è™½ç„¶PCIé€šå¸¸éœ€è¦å€ŸåŠ©äºŒç»´Xå°„çº¿è¡€ç®¡é€ å½±æœ¯ï¼ˆXRAï¼‰æ¥å®æ—¶å¼•å¯¼å¯¼ç®¡æ”¾ç½®ä½ç½®ï¼Œä½†è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±æœ¯ï¼ˆCTAï¼‰å¯ä»¥é€šè¿‡æä¾›ä¸‰ç»´è¡€ç®¡è§£å‰–ç»“æ„å’ŒçŠ¶æ€çš„ç²¾ç¡®ä¿¡æ¯ï¼Œå¤§å¹…æ”¹è¿›PCIçš„æ•ˆæœã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å®æ—¶XRAå’Œè¯¦ç»†çš„3D CTAè§£å‰–ç»“æ„ä¿¡æ¯ä»¥è¿›è¡ŒPCIæ‰‹æœ¯ï¼Œéœ€è¦å‡†ç¡®çš„XRAå’ŒCTAå¤šæ¨¡æ€å›¾åƒé…å‡†ï¼Œä»¥æŒ‡å¯¼æ‰‹æœ¯è¿‡ç¨‹å¹¶é¿å…å¹¶å‘ç—‡ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¿‡ç¨‹ï¼Œå› ä¸ºå®ƒéœ€è¦é…å‡†æ¥è‡ªä¸åŒå‡ ä½•æ¨¡æ€çš„å›¾åƒï¼ˆä¾‹å¦‚ä»äºŒç»´åˆ°ä¸‰ç»´ä»¥åŠåä¹‹ï¼‰ï¼Œå¹¶ä¼´éšå¯¹æ¯”åº¦å’Œå™ªå£°æ°´å¹³çš„å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¾¤ä¼˜åŒ–ç®—æ³•çš„å¤šæ¨¡æ€å† çŠ¶åŠ¨è„‰å›¾åƒé…å‡†æ–°æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¿™äº›æˆåƒæŠ€æœ¯ä¸­çš„å¤§å˜å½¢ã€ä½å¯¹æ¯”åº¦å’Œå™ªå£°ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç®—æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼š1ï¼‰åˆ†åˆ«å¯¹XRAå’ŒCTAå›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼›2ï¼‰åŸºäºç‰¹å¾æå–çš„æ³¨å†Œæ¨¡å—ï¼Œé‡‡ç”¨Stegerç®—æ³•å’Œè¶…åƒç´ ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•ã€‚æˆ‘ä»¬çš„æŠ€æœ¯åœ¨ç”±ç»å†è¿‡PCIæ‰‹æœ¯çš„10åæ‚£è€…çš„28å¯¹XRAå’ŒCTAå›¾åƒç»„æˆçš„å…ˆå¯¼æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¯¥ç®—æ³•ä¸å››ç§æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨é…å‡†ç²¾åº¦ã€ç¨³å¥æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°æ›´å¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨æ‰€æœ‰æ–¹é¢éƒ½è¶…è¿‡äº†ä¹‹å‰çš„æ–¹æ³•å¹¶å±•ç°äº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”æå‡ºäº†ä¸€ç§æ½œåœ¨çš„æ–°ä¸´åºŠæ–¹æ³•ï¼Œå¯èƒ½ä¼šå¯¹å† çŠ¶åŠ¨è„‰ç–¾ç—…æ‚£è€…çš„æ²»ç–—æ•ˆæœæœ‰æ‰€æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24351v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—ï¼ˆPCIï¼‰æ˜¯ä¸€ç§å¾®åˆ›æ‰‹æœ¯ï¼Œå¯æ”¹å–„å† çŠ¶åŠ¨è„‰è¡€æµå¹¶æ²»ç–—å† å¿ƒç—…ã€‚è™½ç„¶PCIé€šå¸¸éœ€è¦å€ŸåŠ©äºŒç»´Xå°„çº¿è¡€ç®¡é€ å½±æœ¯ï¼ˆXRAï¼‰å®æ—¶æŒ‡å¯¼å¯¼ç®¡æ”¾ç½®ï¼Œä½†è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±æœ¯ï¼ˆCTAï¼‰é€šè¿‡æä¾›ä¸‰ç»´è¡€ç®¡è§£å‰–ç»“æ„å’ŒçŠ¶æ€çš„ç²¾ç¡®ä¿¡æ¯ï¼Œå¯ä¸ºPCIå¸¦æ¥å®è´¨æ€§çš„æ”¹è¿›ã€‚ä¸ºäº†åˆ©ç”¨å®æ—¶çš„XRAå’Œè¯¦ç»†çš„3D CTAè§£å‰–ç»“æ„è¿›è¡ŒPCIï¼Œéœ€è¦å¯¹XRAå’ŒCTAè¿›è¡Œç²¾ç¡®çš„å¤šæ¨¡æ€å›¾åƒé…å‡†ï¼Œä»¥æŒ‡å¯¼æ‰‹æœ¯å¹¶é¿å…å¹¶å‘ç—‡ã€‚è¿™ä¸€è¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéœ€è¦é…å‡†æ¥è‡ªä¸åŒå‡ ä½•æ¨¡æ€ï¼ˆ2Dåˆ°3Dåä¹‹äº¦ç„¶ï¼‰çš„å›¾åƒï¼Œå¹¶ä¸”å…·æœ‰å¯¹æ¯”åº¦å’Œå™ªå£°æ°´å¹³çš„å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¾¤ä¼˜åŒ–ç®—æ³•çš„å¤šæ¨¡æ€å† çŠ¶åŠ¨è„‰å›¾åƒé…å‡†æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³ä¸åŒæˆåƒæ¨¡å¼çš„å¤§å˜å½¢ã€ä½å¯¹æ¯”åº¦å’Œå™ªå£°ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç®—æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼š1ï¼‰å¯¹XRAå’ŒCTAå›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼›2ï¼‰åŸºäºç‰¹å¾æå–çš„é…å‡†æ¨¡å—ï¼Œé‡‡ç”¨Stegerå’Œè¶…åƒç´ ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•ã€‚æˆ‘ä»¬çš„æŠ€æœ¯åœ¨ç”±10åæ¥å—PCIæ‰‹æœ¯çš„æ‚£è€…ç»„æˆçš„28å¯¹XRAå’ŒCTAå›¾åƒè¯•ç‚¹æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¯¥æ–¹æ³•åœ¨é…å‡†ç²¾åº¦ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢ä¸å››ç§æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ–¹é¢éƒ½ä¼˜äºæ‰€é€‰çš„åŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ ‡å‡†ï¼Œæå‡ºäº†ä¸€ç§å¯èƒ½æœ‰åŠ©äºæ”¹å–„å† å¿ƒç—…æ‚£è€…æ²»ç–—ç»“æœçš„æ–°ä¸´åºŠæ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—ï¼ˆPCIï¼‰é€šå¸¸ä¾èµ–äºŒç»´Xå°„çº¿è¡€ç®¡é€ å½±æœ¯ï¼ˆXRAï¼‰è¿›è¡Œå®æ—¶æŒ‡å¯¼ã€‚</li>
<li>è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±æœ¯ï¼ˆCTAï¼‰å¯æä¾›ä¸‰ç»´è¡€ç®¡è§£å‰–ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ”¹è¿›PCIã€‚</li>
<li>å¤šæ¨¡æ€å›¾åƒé…å‡†æ˜¯å®ç°XRAå’ŒCTAèåˆçš„å…³é”®æ­¥éª¤ï¼Œæœ‰åŠ©äºæŒ‡å¯¼æ‰‹æœ¯å¹¶é¿å…å¹¶å‘ç—‡ã€‚</li>
<li>é…å‡†è¿‡ç¨‹é¢ä¸´ä¸åŒå‡ ä½•æ¨¡æ€å›¾åƒé…å‡†çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å›¾åƒé—´çš„å¤§å˜å½¢ã€ä½å¯¹æ¯”åº¦å’Œå™ªå£°å·®å¼‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¾¤ä¼˜åŒ–ç®—æ³•çš„å¤šæ¨¡æ€å† çŠ¶åŠ¨è„‰å›¾åƒé…å‡†æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢„å¤„ç†å’Œç‰¹å¾æå–é…å‡†ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚</li>
<li>åœ¨è¯•ç‚¹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é…å‡†ç²¾åº¦ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-56cab63af7f7b4b7d84357e831e04e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7901ba1018c9a97f4f2c9a7da14be86e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb34900f246ce985a121bbfb7b46b724.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62c3a0a900daa6426c25681037339928.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EgoExOR-An-Ego-Exo-Centric-Operating-Room-Dataset-for-Surgical-Activity-Understanding"><a href="#EgoExOR-An-Ego-Exo-Centric-Operating-Room-Dataset-for-Surgical-Activity-Understanding" class="headerlink" title="EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity   Understanding"></a>EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity   Understanding</h2><p><strong>Authors:Ege Ã–zsoy, Arda Mamur, Felix Tristram, Chantal Pellegrini, Magdalena Wysocki, Benjamin Busam, Nassir Navab</strong></p>
<p>Operating rooms (ORs) demand precise coordination among surgeons, nurses, and equipment in a fast-paced, occlusion-heavy environment, necessitating advanced perception models to enhance safety and efficiency. Existing datasets either provide partial egocentric views or sparse exocentric multi-view context, but do not explore the comprehensive combination of both. We introduce EgoExOR, the first OR dataset and accompanying benchmark to fuse first-person and third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D cameras, and ultrasound imagery. Its detailed scene graph annotations, covering 36 entities and 22 relations (568,235 triplets), enable robust modeling of clinical interactions, supporting tasks like action recognition and human-centric perception. We evaluate the surgical scene graph generation performance of two adapted state-of-the-art models and offer a new baseline that explicitly leverages EgoExORâ€™s multimodal and multi-perspective signals. This new dataset and benchmark set a new foundation for OR perception, offering a rich, multimodal resource for next-generation clinical perception. </p>
<blockquote>
<p>æ‰‹æœ¯å®¤ï¼ˆORï¼‰éœ€è¦åœ¨å¿«èŠ‚å¥ã€é®æŒ¡ä¸¥é‡çš„ç¯å¢ƒä¸­ï¼Œç²¾ç¡®åè°ƒå¤–ç§‘åŒ»ç”Ÿã€æŠ¤å£«å’Œè®¾å¤‡ä¹‹é—´çš„é…åˆï¼Œè¿™è¦æ±‚å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹æ¥æé«˜å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚ç°æœ‰çš„æ•°æ®é›†è¦ä¹ˆæä¾›éƒ¨åˆ†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§’ï¼Œè¦ä¹ˆæä¾›ç¨€ç–çš„å¤–è§†å¤šè§†è§’ä¸Šä¸‹æ–‡ï¼Œä½†å¹¶ä¸æ¢ç´¢ä¸¤è€…çš„ç»¼åˆç»“åˆã€‚æˆ‘ä»¬æ¨å‡ºäº†EgoExORï¼Œè¿™æ˜¯é¦–ä¸ªæ‰‹æœ¯å®¤æ•°æ®é›†ä»¥åŠèåˆç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†è§’çš„é…å¥—åŸºå‡†æµ‹è¯•ã€‚EgoExORæ¶µç›–äº†æ¨¡æ‹Ÿè„Šæ¤æ‰‹æœ¯çš„94åˆ†é’Ÿï¼ˆä»¥15å¸§&#x2F;ç§’çš„é€Ÿç‡æ‹æ‘„84,553å¸§ï¼‰ï¼ŒåŒ…æ‹¬è¶…å£°å¼•å¯¼ä¸‹çš„é’ˆæ’å…¥å’Œå¾®åˆ›è„Šæ¤æ‰‹æœ¯ã€‚å®ƒæ•´åˆäº†æ¥è‡ªå¯ç©¿æˆ´çœ¼é•œçš„è‡ªæˆ‘ä¸­å¿ƒæ•°æ®ï¼ˆRGBã€å‡è§†ã€æ‰‹è¿½è¸ªã€éŸ³é¢‘ï¼‰ã€æ¥è‡ªRGB-Dç›¸æœºçš„å¤–éƒ¨RGBå’Œæ·±åº¦æ•°æ®ï¼Œä»¥åŠè¶…å£°å›¾åƒã€‚å…¶è¯¦ç»†çš„åœºæ™¯å›¾æ³¨é‡Šæ¶µç›–äº†36ä¸ªå®ä½“å’Œ22ä¸ªå…³ç³»ï¼ˆå…±568,235ä¸ªä¸‰å…ƒç»„ï¼‰ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ¨¡æ‹Ÿä¸´åºŠäº’åŠ¨ï¼Œæ”¯æŒåŠ¨ä½œè¯†åˆ«å’Œäººç±»ä¸­å¿ƒæ„ŸçŸ¥ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ä¸ªé€‚åº”å…ˆè¿›çŠ¶æ€æ¨¡å‹çš„æ‰‹æœ¯åœºæ™¯å›¾ç”Ÿæˆæ€§èƒ½ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºçº¿ï¼Œè¯¥åŸºçº¿æ˜ç¡®åˆ©ç”¨äº†EgoExORçš„å¤šæ¨¡æ€å’Œå¤šè§†è§’ä¿¡å·ã€‚è¿™ä¸ªæ–°çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸ºæ‰‹æœ¯å®¤æ„ŸçŸ¥è®¾å®šäº†æ–°çš„åŸºç¡€ï¼Œä¸ºä¸‹ä¸€ä»£ä¸´åºŠæ„ŸçŸ¥æä¾›äº†ä¸°å¯Œã€å¤šæ¨¡æ€çš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24287v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰‹æœ¯å®¤æ„ŸçŸ¥é¢†åŸŸçš„æ–°æ•°æ®é›†EgoExORåŠå…¶åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†èåˆäº†ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†è§’ï¼Œé›†æˆäº†å¯ç©¿æˆ´çœ¼é•œçš„RGBã€è§†çº¿è¿½è¸ªã€æ‰‹éƒ¨è¿½è¸ªå’ŒéŸ³é¢‘æ•°æ®ï¼Œä»¥åŠRGB-Dç›¸æœºçš„å¤–éƒ¨RGBå’Œæ·±åº¦æ•°æ®ï¼Œä»¥åŠè¶…å£°å›¾åƒã€‚å®ƒè¯¦ç»†çš„åœºæ™¯å›¾æ³¨è§£æ”¯æŒåŠ¨ä½œè¯†åˆ«å’Œä»¥äººä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥ä»»åŠ¡ï¼Œä¸ºæ‰‹æœ¯å®¤æ„ŸçŸ¥ç ”ç©¶æä¾›äº†æ–°çš„ä¸°å¯Œå¤šæ¨¡æ€èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EgoExORæ˜¯é¦–ä¸ªèåˆç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†è§’çš„æ‰‹æœ¯æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº†å¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¦‚RGBã€éŸ³é¢‘ã€æ·±åº¦æ•°æ®ç­‰ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–äº†ä¸¤é¡¹æ¨¡æ‹Ÿæ‰‹æœ¯ï¼šè¶…å£°å¼•å¯¼ä¸‹çš„é’ˆæ’å…¥å’Œå¾®åˆ›è„Šæ¤æ‰‹æœ¯ã€‚</li>
<li>è¯¦ç»†çš„åœºæ™¯å›¾æ³¨è§£èƒ½å¤Ÿæ”¯æŒåŠ¨ä½œè¯†åˆ«å’Œä»¥äººä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹è¯¥æ•°æ®é›†çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å’Œå¤šè§†è§’ä¿¡å·ã€‚</li>
<li>EgoExORä¸ºæ‰‹æœ¯å®¤æ„ŸçŸ¥ç ”ç©¶æä¾›äº†æ–°çš„åŸºç¡€ï¼Œæœ‰åŠ©äºæé«˜æ‰‹æœ¯çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-021dcad7e411940f13f13fd7d477767e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11ef1780d17bb4f59a03c272ca236aa2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Beyond-the-LUMIR-challenge-The-pathway-to-foundational-registration-models"><a href="#Beyond-the-LUMIR-challenge-The-pathway-to-foundational-registration-models" class="headerlink" title="Beyond the LUMIR challenge: The pathway to foundational registration   models"></a>Beyond the LUMIR challenge: The pathway to foundational registration   models</h2><p><strong>Authors:Junyu Chen, Shuwen Wei, Joel Honkamaa, Pekka Marttinen, Hang Zhang, Min Liu, Yichao Zhou, Zuopeng Tan, Zhuoyuan Wang, Yi Wang, Hongchao Zhou, Shunbo Hu, Yi Zhang, Qian Tao, Lukas FÃ¶rner, Thomas Wendler, Bailiang Jian, Benedikt Wiestler, Tim Hable, Jin Kim, Dan Ruan, Frederic Madesta, Thilo Sentker, Wiebke Heyer, Lianrui Zuo, Yuwei Dai, Jing Wu, Jerry L. Prince, Harrison Bai, Yong Du, Yihao Liu, Alessa Hering, Reuben Dorent, Lasse Hansen, Mattias P. Heinrich, Aaron Carass</strong></p>
<p>Medical image challenges have played a transformative role in advancing the field, catalyzing algorithmic innovation and establishing new performance standards across diverse clinical applications. Image registration, a foundational task in neuroimaging pipelines, has similarly benefited from the Learn2Reg initiative. Building on this foundation, we introduce the Large-scale Unsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation benchmark designed to assess and advance unsupervised brain MRI registration. Distinct from prior challenges that leveraged anatomical label maps for supervision, LUMIR removes this dependency by providing over 4,000 preprocessed T1-weighted brain MRIs for training without any label maps, encouraging biologically plausible deformation modeling through self-supervision. In addition to evaluating performance on 590 held-out test subjects, LUMIR introduces a rigorous suite of zero-shot generalization tasks, spanning out-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted), disease populations (e.g., Alzheimerâ€™s disease), acquisition protocols (e.g., 9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and over 4,000 image pairs were included for evaluation. Performance was assessed using both segmentation-based metrics (Dice coefficient, 95th percentile Hausdorff distance) and landmark-based registration accuracy (target registration error). Across both in-domain and zero-shot tasks, deep learning-based methods consistently achieved state-of-the-art accuracy while producing anatomically plausible deformation fields. The top-performing deep learning-based models demonstrated diffeomorphic properties and inverse consistency, outperforming several leading optimization-based methods, and showing strong robustness to most domain shifts, the exception being a drop in performance on out-of-domain contrasts. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæŒ‘æˆ˜åœ¨æ¨åŠ¨é¢†åŸŸå‘å±•æ–¹é¢å‘æŒ¥äº†å˜é©æ€§ä½œç”¨ï¼Œå‚¬ç”Ÿäº†ç®—æ³•åˆ›æ–°ï¼Œå¹¶é’ˆå¯¹ä¸åŒä¸´åºŠåº”ç”¨å»ºç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚ä½œä¸ºç¥ç»æˆåƒç®¡é“ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå›¾åƒé…å‡†ä¹Ÿå—ç›ŠäºLearn2Regå€¡è®®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¤§è§„æ¨¡æ— ç›‘ç£è„‘MRIå›¾åƒé…å‡†ï¼ˆLUMIRï¼‰æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€é¡¹æ—¨åœ¨è¯„ä¼°å’Œæ¨è¿›æ— ç›‘ç£è„‘MRIé…å‡†çš„ä¸‹ä¸€ä»£åŸºå‡†æµ‹è¯•ã€‚ä¸åŒäºä»¥å‰ä¾èµ–è§£å‰–æ ‡ç­¾å›¾è¿›è¡Œç›‘ç£çš„æŒ‘æˆ˜ï¼ŒLUMIRé€šè¿‡æä¾›4000å¤šå¼ é¢„å¤„ç†çš„T1åŠ æƒè„‘MRIç”¨äºè®­ç»ƒï¼Œè€Œæ— éœ€ä»»ä½•æ ‡ç­¾å›¾ï¼Œé¼“åŠ±é€šè¿‡è‡ªç›‘ç£è¿›è¡Œç”Ÿç‰©å­¦ä¸Šåˆç†çš„å˜å½¢å»ºæ¨¡ã€‚é™¤äº†å¯¹590ä¸ªä¿ç•™æµ‹è¯•å¯¹è±¡çš„æ€§èƒ½è¿›è¡Œè¯„ä¼°å¤–ï¼ŒLUMIRè¿˜å¼•å…¥äº†ä¸€ç³»åˆ—ä¸¥æ ¼çš„é›¶æ ·æœ¬æ³›åŒ–ä»»åŠ¡ï¼Œæ¶µç›–åŸŸå¤–æˆåƒæ¨¡å¼ï¼ˆä¾‹å¦‚FLAIRã€T2åŠ æƒã€T2 * åŠ æƒï¼‰ã€ç–¾ç—…ç¾¤ä½“ï¼ˆä¾‹å¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼‰ã€é‡‡é›†åè®®ï¼ˆä¾‹å¦‚9.4T MRIï¼‰å’Œç‰©ç§ï¼ˆä¾‹å¦‚çŒ•çŒ´å¤§è„‘ï¼‰ã€‚è¯„ä¼°å…±æ¶‰åŠ1158ä¸ªä¸»ä½“å’Œ4000å¤šä¸ªå›¾åƒå¯¹ã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨åŸºäºåˆ†å‰²çš„æŒ‡æ ‡ï¼ˆDiceç³»æ•°ã€ç¬¬95ä¸ªç™¾åˆ†ä½Hausdorffè·ç¦»ï¼‰å’ŒåŸºäºåœ°æ ‡çš„é…å‡†ç²¾åº¦ï¼ˆç›®æ ‡æ³¨å†Œè¯¯å·®ï¼‰ã€‚åœ¨åŸŸå†…ä»»åŠ¡å’Œé›¶æ ·æœ¬ä»»åŠ¡ä¸­ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å§‹ç»ˆè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶äº§ç”Ÿäº†ç”Ÿç‰©å­¦ä¸Šåˆç†çš„å˜å½¢åœºã€‚è¡¨ç°æœ€ä½³çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹è¡¨ç°å‡ºå¾®åˆ†åŒèƒšå±æ€§å’Œé€†å‘ä¸€è‡´æ€§ï¼Œä¼˜äºå‡ ç§åŸºäºä¼˜åŒ–çš„é¢†å…ˆæ–¹æ³•ï¼Œå¹¶ä¸”å¯¹å¤§å¤šæ•°åŸŸåç§»è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œä½†åœ¨åŸŸå¤–å¯¹æ¯”ä¸‹æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒæŒ‘æˆ˜åœ¨æ¨åŠ¨é¢†åŸŸå‘å±•æ–¹é¢æ‰®æ¼”äº†é‡è¦è§’è‰²ï¼Œä¿ƒè¿›äº†ç®—æ³•åˆ›æ–°å¹¶å»ºç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚åŸºäºLearn2Regå€¡è®®ï¼Œå›¾åƒé…å‡†è¿™ä¸€ç¥ç»æˆåƒç®¡é“ä¸­çš„åŸºç¡€ä»»åŠ¡ä¹Ÿä»ä¸­å—ç›Šã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¤§è§„æ¨¡æ— ç›‘ç£è„‘MRIå›¾åƒé…å‡†ï¼ˆLUMIRï¼‰æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å’Œæ¨è¿›æ— ç›‘ç£è„‘MRIé…å‡†çš„æ–°ä¸€ä»£åŸºå‡†ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºè§£å‰–æ ‡ç­¾å›¾è¿›è¡Œç›‘ç®¡çš„æŒ‘æˆ˜ä¸åŒï¼ŒLUMIRé€šè¿‡æä¾›è¶…è¿‡4000å¼ é¢„å¤„ç†çš„T1åŠ æƒè„‘MRIå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œé¼“åŠ±é€šè¿‡è‡ªæˆ‘ç›‘ç£è¿›è¡Œç”Ÿç‰©å­¦ä¸Šåˆç†çš„å˜å½¢å»ºæ¨¡ã€‚é™¤äº†å¯¹590åå—è¯•è€…è¿›è¡Œæµ‹è¯•å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸¥æ ¼çš„æ— å°„å‡»æ³›åŒ–ä»»åŠ¡ï¼Œæ¶µç›–åŸŸå¤–æˆåƒæ¨¡å¼ï¼ˆå¦‚FLAIRã€T2åŠ æƒã€T2 *åŠ æƒï¼‰ã€ç–¾ç—…ç¾¤ä½“ï¼ˆå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—‡ï¼‰ã€é‡‡é›†åè®®ï¼ˆå¦‚9.4T MRIï¼‰å’Œç‰©ç§ï¼ˆå¦‚çŒ•çŒ´å¤§è„‘ï¼‰ã€‚è¯„ä¼°ä½¿ç”¨åŸºäºåˆ†å‰²çš„æŒ‡æ ‡ï¼ˆDiceç³»æ•°ã€ç¬¬95ç™¾åˆ†ä½Hausdorffè·ç¦»ï¼‰å’ŒåŸºäºåœ°æ ‡çš„æ³¨å†Œå‡†ç¡®æ€§ï¼ˆç›®æ ‡æ³¨å†Œè¯¯å·®ï¼‰ã€‚æ— è®ºåœ¨åŸŸå†…ä»»åŠ¡è¿˜æ˜¯é›¶å°„å‡»ä»»åŠ¡ä¸­ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•éƒ½æŒç»­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶äº§ç”Ÿäº†è§£å‰–å­¦ä¸Šåˆç†çš„å˜å½¢åœºã€‚è¡¨ç°æœ€ä½³çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å±•ç°å‡ºå¾®åˆ†åŒèƒšæ€§è´¨å’Œé€†ä¸€è‡´æ€§ï¼Œä¼˜äºå‡ ç§é¢†å…ˆçš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶å¯¹å¤§å¤šæ•°é¢†åŸŸå˜åŒ–è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œé™¤äº†åœ¨é¢†åŸŸå¤–çš„å¯¹æ¯”ç»“æœæ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæŒ‘æˆ˜æ¨åŠ¨äº†åŒ»å­¦å›¾åƒé¢†åŸŸçš„è¿›æ­¥ï¼Œä¿ƒè¿›äº†ç®—æ³•åˆ›æ–°å¹¶æé«˜äº†æ€§èƒ½æ ‡å‡†ã€‚</li>
<li>LUMIRæŒ‘æˆ˜æ—¨åœ¨è¯„ä¼°å’Œæ¨è¿›æ— ç›‘ç£è„‘MRIé…å‡†æŠ€æœ¯ã€‚</li>
<li>LUMIRæŒ‘æˆ˜é€šè¿‡æä¾›æ— æ ‡ç­¾çš„T1åŠ æƒè„‘MRIå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œé¼“åŠ±è‡ªæˆ‘ç›‘ç£çš„å˜å½¢å»ºæ¨¡ã€‚</li>
<li>LUMIRæŒ‘æˆ˜æ¶µç›–äº†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–çš„æˆåƒæ¨¡å¼ã€ç–¾ç—…ç¾¤ä½“ã€é‡‡é›†åè®®å’Œç‰©ç§ã€‚</li>
<li>åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒé…å‡†ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶äº§ç”Ÿäº†åˆç†çš„å˜å½¢åœºã€‚</li>
<li>é¡¶çº§æ·±åº¦å­¦ä¹ æ¨¡å‹å±•ç°å‡ºå¾®åˆ†åŒèƒšæ€§è´¨å’Œé€†ä¸€è‡´æ€§ï¼Œä¼˜äºä¸€äº›ä¼˜åŒ–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b633cba52a806a41001c3b6c28354b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ffac5304fecdb9b942edb7a7307b678.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab984dc55e3086554d3fc727b3c64dd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sparsity-Driven-Parallel-Imaging-Consistency-for-Improved-Self-Supervised-MRI-Reconstruction"><a href="#Sparsity-Driven-Parallel-Imaging-Consistency-for-Improved-Self-Supervised-MRI-Reconstruction" class="headerlink" title="Sparsity-Driven Parallel Imaging Consistency for Improved   Self-Supervised MRI Reconstruction"></a>Sparsity-Driven Parallel Imaging Consistency for Improved   Self-Supervised MRI Reconstruction</h2><p><strong>Authors:YaÅŸar Utku AlÃ§alar, Mehmet AkÃ§akaya</strong></p>
<p>Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self-supervised learning has gained prominence. However, its application at high acceleration rates frequently introduces artifacts, compromising image fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL networks via carefully-designed perturbations. In particular, we enhance the k-space masking idea of conventional self-supervised learning with a novel consistency term that assesses the modelâ€™s ability to accurately predict the added perturbations in a sparse domain, leading to more reliable and artifact-free reconstructions. The results obtained from the fastMRI knee and brain datasets show that the proposed training strategy effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates, outperforming state-of-the-art self-supervised methods both visually and quantitatively. </p>
<blockquote>
<p>ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ ï¼ˆPD-DLï¼‰æ¨¡å‹å·²è¢«è¯æ˜æ˜¯ç”¨äºæ”¹è¿›å¿«é€ŸMRIæ‰«æé‡å»ºçš„å¼ºå¤§æ–¹æ³•ã€‚ä¸ºäº†åœ¨æ— æ³•ä½¿ç”¨å®Œå…¨é‡‡æ ·å‚è€ƒæ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œè‡ªç›‘ç£å­¦ä¹ å·²å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå…¶åœ¨é«˜åŠ é€Ÿç‡ä¸‹çš„åº”ç”¨ç»å¸¸ä¼šäº§ç”Ÿä¼ªå½±ï¼Œä»è€Œå½±å“å›¾åƒä¿çœŸåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç²¾å¿ƒè®¾è®¡æ‰°åŠ¨æ¥è®­ç»ƒPD-DLç½‘ç»œçš„æ–°æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åŸºäºä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ çš„kç©ºé—´æ©è”½æ€æƒ³ï¼ŒåŠ å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ä¸€è‡´æ€§æœ¯è¯­ï¼Œè¯¥æœ¯è¯­è¯„ä¼°æ¨¡å‹åœ¨ç¨€ç–åŸŸä¸­å‡†ç¡®é¢„æµ‹æ·»åŠ æ‰°åŠ¨çš„èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å¯é ä¸”æ— ä¼ªå½±çš„é‡å»ºã€‚ä»fastMRIçš„è†å…³èŠ‚å’Œå¤§è„‘æ•°æ®é›†è·å¾—çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è®­ç»ƒç­–ç•¥æœ‰æ•ˆåœ°å‡å°‘äº†æ··å ä¼ªå½±å¹¶å‡è½»äº†é«˜åŠ é€Ÿç‡ä¸‹çš„å™ªå£°æ”¾å¤§é—®é¢˜ï¼Œåœ¨è§†è§‰å’Œå®šé‡è¯„ä¼°ä¸Šéƒ½ä¼˜äºæœ€æ–°çš„è‡ªç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24136v1">PDF</a> IEEE International Conference on Image Processing (ICIP), 2025</p>
<p><strong>Summary</strong><br>     ç‰©ç†é©±åŠ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“åˆç²¾å¿ƒè®¾è®¡çš„æ‰°åŠ¨ï¼Œé€šè¿‡è¯„ä¼°æ¨¡å‹åœ¨ç¨€ç–åŸŸé¢„æµ‹æ·»åŠ æ‰°åŠ¨çš„èƒ½åŠ›ï¼Œå¢å¼ºäº†å¸¸è§„è‡ªç›‘ç£å­¦ä¹ çš„kç©ºé—´æ©è”½æ€æƒ³ï¼Œå®ç°äº†æ›´å¯é ä¸”æ— ä¼ªå½±çš„é‡å»ºã€‚æ­¤æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†å¿«é€ŸMRIæ‰«æé‡å»ºä¸­çš„æ··å ä¼ªå½±ï¼Œå¹¶åœ¨é«˜åŠ é€Ÿç‡ä¸‹é™ä½äº†å™ªå£°æ”¾å¤§é—®é¢˜ï¼Œè¶…è¶Šäº†ç°æœ‰è‡ªç›‘ç£æ–¹æ³•åœ¨è§†è§‰å’Œæ•°é‡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PD-DLæ¨¡å‹ç”¨äºæ”¹è¿›å¿«é€ŸMRIæ‰«æé‡å»ºã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ åœ¨è®­ç»ƒPD-DLæ¨¡å‹æ—¶é¢ä¸´é«˜åŠ é€Ÿç‡ä¸‹çš„ä¼ªå½±é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è®­ç»ƒç­–ç•¥ï¼Œç»“åˆç²¾å¿ƒè®¾è®¡çš„æ‰°åŠ¨æ¥å¢å¼ºPD-DLç½‘ç»œã€‚</li>
<li>åœ¨kç©ºé—´æ©è”½åŸºç¡€ä¸ŠåŠ å…¥ä¸€è‡´æ€§è¯„ä¼°ï¼Œä»¥é¢„æµ‹ç¨€ç–åŸŸä¸­çš„æ·»åŠ æ‰°åŠ¨ã€‚</li>
<li>è¯¥ç­–ç•¥æœ‰æ•ˆå‡å°‘æ··å ä¼ªå½±ï¼Œé™ä½é«˜åŠ é€Ÿç‡ä¸‹çš„å™ªå£°æ”¾å¤§é—®é¢˜ã€‚</li>
<li>åœ¨fastMRIçš„è†å…³èŠ‚å’Œå¤§è„‘æ•°æ®é›†ä¸Šçš„ç»“æœä¼˜äºç°æœ‰è‡ªç›‘ç£æ–¹æ³•ï¼Œå®ç°è§†è§‰å’Œæ•°é‡ä¸Šçš„è¶…è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b002b4e38e7db83721f2fa9ecc48ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13a3eb34483135dc90cb7fa3c083e91c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-345586d93331898a7a15c0a69b103a0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-742612d226496fb2a714fd03edc936c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-defbde8c845c788ef665aa19aed532b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a706ed1d3076b6404f241bd5a0894d4a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improved-Accuracy-in-Pelvic-Tumor-Resections-Using-a-Real-Time-Vision-Guided-Surgical-System"><a href="#Improved-Accuracy-in-Pelvic-Tumor-Resections-Using-a-Real-Time-Vision-Guided-Surgical-System" class="headerlink" title="Improved Accuracy in Pelvic Tumor Resections Using a Real-Time   Vision-Guided Surgical System"></a>Improved Accuracy in Pelvic Tumor Resections Using a Real-Time   Vision-Guided Surgical System</h2><p><strong>Authors:Vahid Danesh, Paul Arauz, Maede Boroji, Andrew Zhu, Mia Cottone, Elaine Gould, Fazel A. Khan, Imin Kao</strong></p>
<p>Pelvic bone tumor resections remain significantly challenging due to complex three-dimensional anatomy and limited surgical visualization. Current navigation systems and patient-specific instruments, while accurate, present limitations including high costs, radiation exposure, workflow disruption, long production time, and lack of reusability. This study evaluates a real-time vision-guided surgical system combined with modular jigs to improve accuracy in pelvic bone tumor resections. A vision-guided surgical system combined with modular cutting jigs and real-time optical tracking was developed and validated. Five female pelvis sawbones were used, with each hemipelvis randomly assigned to either the vision-guided and modular jig system or traditional freehand method. A total of twenty resection planes were analyzed for each method. Accuracy was assessed by measuring distance and angular deviations from the planned resection planes. The vision-guided and modular jig system significantly improved resection accuracy compared to the freehand method, reducing the mean distance deviation from 2.07 $\pm$ 1.71 mm to 1.01 $\pm$ 0.78 mm (p&#x3D;0.0193). In particular, all specimens resected using the vision-guided system exhibited errors of less than 3 mm. Angular deviations also showed significant improvements with roll angle deviation reduced from 15.36 $\pm$ 17.57$^\circ$ to 4.21 $\pm$ 3.46$^\circ$ (p&#x3D;0.0275), and pitch angle deviation decreased from 6.17 $\pm$ 4.58$^\circ$ to 1.84 $\pm$ 1.48$^\circ$ (p&lt;0.001). The proposed vision-guided and modular jig system significantly improves the accuracy of pelvic bone tumor resections while maintaining workflow efficiency. This cost-effective solution provides real-time guidance without the need for referencing external monitors, potentially improving surgical outcomes in complex pelvic bone tumor cases. </p>
<blockquote>
<p>ç›†è…”éª¨è‚¿ç˜¤åˆ‡é™¤æœ¯ä»å­˜åœ¨å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå…¶å¤æ‚çš„ä¸‰ç»´è§£å‰–ç»“æ„å’Œæœ‰é™çš„æ‰‹æœ¯è§†é‡ã€‚å°½ç®¡ç›®å‰çš„å¯¼èˆªç³»ç»Ÿå’Œä¸“ç”¨æ‰‹æœ¯å™¨æ¢°ç›¸å½“ç²¾ç¡®ï¼Œä½†ä»å­˜åœ¨åŒ…æ‹¬é«˜æˆæœ¬ã€è¾å°„æš´éœ²ã€å·¥ä½œæµç¨‹å¹²æ‰°ã€ç”Ÿäº§æ—¶é—´é•¿ä»¥åŠä¸å¯é‡å¤ä½¿ç”¨ç­‰å±€é™æ€§ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸€ç§ç»“åˆæ¨¡å—åŒ–å¤¹å…·çš„å®æ—¶è§†è§‰å¼•å¯¼æ‰‹æœ¯ç³»ç»Ÿï¼Œä»¥æé«˜ç›†è…”éª¨è‚¿ç˜¤åˆ‡é™¤æœ¯çš„ç²¾ç¡®åº¦ã€‚å¼€å‘å¹¶éªŒè¯äº†ä¸€ç§ç»“åˆæ¨¡å—åŒ–åˆ‡å‰²å¤¹å…·å’Œå®æ—¶å…‰å­¦è¿½è¸ªçš„è§†è§‰å¼•å¯¼æ‰‹æœ¯ç³»ç»Ÿã€‚ç ”ç©¶ä½¿ç”¨äº†5ä¸ªå¥³æ€§éª¨ç›†é”¯çŠ¶éª¨æ ·æœ¬ï¼Œæ¯ä¸ªåŠéª¨ç›†è¢«éšæœºåˆ†é…è‡³è§†è§‰å¼•å¯¼ä¸æ¨¡å—åŒ–å¤¹å…·ç³»ç»Ÿç»„æˆ–ä¼ ç»Ÿè‡ªç”±æ‰‹æ³•ç»„ã€‚æ¯ç§æ–¹æ³•åˆ†æçš„æ€»åˆ‡é™¤å¹³é¢æœ‰20ä¸ªã€‚é€šè¿‡æµ‹é‡ä¸è®¡åˆ’åˆ‡é™¤å¹³é¢çš„è·ç¦»å’Œè§’åº¦åå·®æ¥è¯„ä¼°å‡†ç¡®æ€§ã€‚ä¸è‡ªç”±æ‰‹æ³•ç›¸æ¯”ï¼Œè§†è§‰å¼•å¯¼ä¸æ¨¡å—åŒ–å¤¹å…·ç³»ç»Ÿæ˜¾è‘—æé«˜äº†åˆ‡é™¤å‡†ç¡®æ€§ï¼Œå¹³å‡è·ç¦»åå·®ä»2.07Â±1.71æ¯«ç±³å‡å°‘åˆ°1.01Â±0.78æ¯«ç±³ï¼ˆp&#x3D;0.0193ï¼‰ã€‚ç‰¹åˆ«æ˜¯ä½¿ç”¨è§†è§‰å¼•å¯¼ç³»ç»Ÿçš„æ‰€æœ‰æ ‡æœ¬è¯¯å·®å‡å°äº3æ¯«ç±³ã€‚è§’åº¦åå·®ä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹å–„ï¼Œæ»šåŠ¨è§’åº¦åå·®ä»15.36Â±17.57Â°å‡å°‘åˆ°4.21Â±3.46Â°ï¼Œä¿¯ä»°è§’åº¦åå·®ä»6.17Â±4.58Â°å‡å°‘åˆ°1.84Â±1.48Â°ï¼ˆp&lt;0.001ï¼‰ã€‚æ‹Ÿè®®çš„è§†è§‰å¼•å¯¼å’Œæ¨¡å—åŒ–å¤¹å…·ç³»ç»Ÿå¯åœ¨ä¿æŒå·¥ä½œæµç¨‹æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æé«˜ç›†è…”éª¨è‚¿ç˜¤åˆ‡é™¤æœ¯çš„å‡†ç¡®æ€§ã€‚è¿™ç§ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆæä¾›äº†å®æ—¶æŒ‡å¯¼ï¼Œè€Œæ— éœ€å‚è€ƒå¤–éƒ¨ç›‘è§†å™¨ï¼Œæœ‰å¯èƒ½åœ¨å¤æ‚çš„ç›†è…”éª¨è‚¿ç˜¤ç—…ä¾‹ä¸­æ”¹å–„æ‰‹æœ¯ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23984v1">PDF</a> 9 Pages, 5 figures, Submitted to Journal of Orthopaedic Research</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ç ”ç©¶äº†éª¨ç›†è‚¿ç˜¤åˆ‡é™¤æ‰‹æœ¯ä¸­çš„ä¸€é¡¹æ–°æŠ€æœ¯ã€‚ç”±äºéª¨ç›†çš„ä¸‰ç»´å¤æ‚ç»“æ„å’Œæ‰‹æœ¯å¯è§†åŒ–çš„å±€é™æ€§ï¼Œè¯¥æ‰‹æœ¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å®æ—¶è§†è§‰å¯¼èˆªæ‰‹æœ¯ç³»ç»Ÿç»“åˆæ¨¡å—åŒ–å¤¹å…·ï¼Œæ—¨åœ¨æé«˜éª¨ç›†è‚¿ç˜¤åˆ‡é™¤çš„ç²¾å‡†åº¦ã€‚å¯¹æ¯”ä¼ ç»Ÿçš„æ‰‹å·¥æ“ä½œï¼Œè¿™ä¸€æ–°æŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æé«˜æ‰‹æœ¯çš„å‡†ç¡®åº¦ï¼Œå…·æœ‰æ›´å¤§çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éª¨ç›†è‚¿ç˜¤åˆ‡é™¤æ‰‹æœ¯é¢ä¸´å¤æ‚çš„ä¸‰ç»´è§£å‰–ç»“æ„å’Œæœ‰é™çš„æ‰‹æœ¯å¯è§†åŒ–æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¯¼èˆªç³»ç»Ÿå’Œæ‚£è€…ä¸“ç”¨ä»ªå™¨è™½å‡†ç¡®ï¼Œä½†å­˜åœ¨æˆæœ¬é«˜ã€è¾å°„æš´éœ²ã€å·¥ä½œæµç¨‹ä¸­æ–­ã€ç”Ÿäº§æ—¶é—´é•¿å’Œä¸å¯é‡å¤ä½¿ç”¨ç­‰å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å®æ—¶è§†è§‰å¯¼èˆªæ‰‹æœ¯ç³»ç»Ÿç»“åˆæ¨¡å—åŒ–å¤¹å…·ï¼Œæ—¨åœ¨æé«˜éª¨ç›†è‚¿ç˜¤åˆ‡é™¤çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„æ‰‹å·¥æ“ä½œç›¸æ¯”ï¼Œè§†è§‰å¯¼èˆªå’Œæ¨¡å—åŒ–å¤¹å…·ç³»ç»Ÿæ˜¾è‘—æé«˜äº†åˆ‡é™¤å‡†ç¡®æ€§ï¼Œå°†å¹³å‡è·ç¦»åå·®ä»2.07Â±1.71æ¯«ç±³å‡å°‘åˆ°1.01Â±0.78æ¯«ç±³ã€‚</li>
<li>æ‰€æœ‰ä½¿ç”¨è§†è§‰å¯¼èˆªç³»ç»Ÿçš„æ ‡æœ¬çš„è¯¯å·®å‡å°äº3æ¯«ç±³ã€‚</li>
<li>è§’åº¦åå·®ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œæ»šåŠ¨è§’åº¦åå·®ä»15.36Â±17.57Â°å‡å°‘åˆ°4.21Â±3.46Â°ï¼Œä¿¯ä»°è§’åº¦åå·®ä»6.17Â±4.58Â°å‡å°‘åˆ°1.84Â±1.48Â°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b38de95335c0b5f2e1603fe85acf90a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38628fc1e821a41efc22d4b22eeda9f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3e7a3a0e18573f968339fdb9158e267.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30cd5e9c899a4e9a3b08b8587966fe72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54af4f2919ed3a190b7bcd0a8dc07de4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward"><a href="#CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward" class="headerlink" title="CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward"></a>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward</h2><p><strong>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</strong></p>
<p>In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAD-Coderè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬åˆ°CADï¼ˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼‰çš„é—®é¢˜é‡æ–°å®šä¹‰ä¸ºCadQueryè„šæœ¬çš„ç”Ÿæˆé—®é¢˜ã€‚CadQueryæ˜¯ä¸€ç§åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ã€‚è¿™ç§è¡¨ç¤ºæ³•èƒ½å¤Ÿå®ç°ç›´æ¥çš„å‡ ä½•éªŒè¯ã€æ›´ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ä»¥åŠä¸ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ— ç¼é›†æˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ ç®¡é“ï¼šï¼ˆ1ï¼‰åœ¨æˆå¯¹çš„æ–‡æœ¬-CadQueryæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼›ï¼ˆ2ï¼‰ä½¿ç”¨ç¾¤ä½“å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç”±åŒ…æ‹¬å‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±åœ¨å†…çš„CADç‰¹å®šå¥–åŠ±è¿›è¡ŒæŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹æ¥æ”¹å–„æ¨¡å‹æ¨ç†ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«11ä¸‡ä¸ªæ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1500ä¸ªCoTæ ·æœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCAD-Coderä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆå’Œå¤æ‚çš„CADæ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†çš„æœ€æ–°æŠ€æœ¯è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19713v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAD-Coderæ¡†æ¶èƒ½å°†æ–‡æœ¬è½¬åŒ–ä¸ºCADè®¾è®¡ï¼Œé€šè¿‡ç”ŸæˆCadQueryè„šæœ¬å®ç°ã€‚è¯¥æ¡†æ¶æ”¯æŒç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ï¼Œå¹¶èƒ½æ— ç¼é›†æˆç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ºæé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ç²¾åº¦ï¼Œç ”ç©¶æå‡ºäº†åŒ…å«ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼Œå¹¶å¼•å…¥äº†é›†å›¢å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ€è€ƒé“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼ŒCAD-Coderèƒ½ä½¿LLMsç›´æ¥æ ¹æ®è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆã€å¤æ‚çš„CADæ¨¡å‹ï¼Œæ¨åŠ¨æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-Coderå°†æ–‡æœ¬è½¬åŒ–ä¸ºCADè®¾è®¡çš„èƒ½åŠ›æ˜¯é€šè¿‡ç”ŸæˆCadQueryè„šæœ¬å®ç°çš„ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ã€‚</li>
<li>CAD-Coderæ”¯æŒç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ï¼Œå¹¶èƒ½æ— ç¼é›†æˆç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“åŒ…æ‹¬ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ç²¾åº¦ã€‚</li>
<li>é›†å›¢å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰è¢«å¼•å…¥ä»¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>æ€è€ƒé“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹ç”¨äºæé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«110Kæ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1.5Kæ€è€ƒé“¾æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8a02cd23da62dd9c0d5a1bc79c5a8f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f227d4738b014e8188273432dfbe8a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e3a0b1472798b3cf897d12a1f99e67.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="U2-BENCH-Benchmarking-Large-Vision-Language-Models-on-Ultrasound-Understanding"><a href="#U2-BENCH-Benchmarking-Large-Vision-Language-Models-on-Ultrasound-Understanding" class="headerlink" title="U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound   Understanding"></a>U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound   Understanding</h2><p><strong>Authors:Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</strong></p>
<p>Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging. </p>
<blockquote>
<p>è¶…å£°æ˜¯å…¨çƒå«ç”Ÿä¿å¥ä¸­å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ä¹‹ä¸€ï¼Œå¯¹å…¶è§£è¯»å´å› æ“ä½œå‘˜ã€å™ªå£°å’Œè§£å‰–ç»“æ„ç­‰å› ç´ å¯¼è‡´çš„å›¾åƒè´¨é‡å·®å¼‚è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è‡ªç„¶å’ŒåŒ»ç–—é¢†åŸŸçš„å¤šæ¨¡å¼åŠŸèƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨è¶…å£°æ–¹é¢çš„è¡¨ç°å´é²œæœ‰ç ”ç©¶ã€‚æˆ‘ä»¬æ¨å‡ºU2-BENCHï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°LVLMsåœ¨è¶…å£°ç†è§£æ–¹é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚U2-BENCHèšåˆäº†æ¶µç›–15ä¸ªè§£å‰–åŒºåŸŸçš„7,241ä¸ªç—…ä¾‹ï¼Œå¹¶å®šä¹‰äº†8ä¸ªä»¥ä¸´åºŠä¸ºåŸºç¡€çš„çš„ä»»åŠ¡ï¼Œå¦‚è¯Šæ–­ã€è§†å›¾è¯†åˆ«ã€ç—…ç¶å®šä½ã€ä¸´åºŠä»·å€¼è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆç­‰ï¼Œæ¶‰åŠ50ç§è¶…å£°åº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†20é¡¹æœ€å…ˆè¿›çš„LVLMsï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºçš„ã€é€šç”¨å’Œä¸“ç”¨çš„ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œåœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œä¸´åºŠè¯­è¨€ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚U2-BENCHå»ºç«‹äº†ä¸€ä¸ªä¸¥æ ¼ç»Ÿä¸€çš„æµ‹è¯•å¹³å°ï¼Œä»¥è¯„ä¼°å’ŒåŠ é€ŸåŒ»ç–—è¶…å£°æˆåƒè¿™ä¸€ç‹¬ç‰¹å¤šæ¨¡å¼é¢†åŸŸçš„LVLMç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17779v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹è¶…å£°æ³¢å½±åƒç†è§£çš„é¦–ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•U2-BENCHã€‚U2-BENCHè¦†ç›–äº†è¶…å£°æ³¢å›¾åƒçš„åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§£å‰–å­¦åŒºåŸŸå’Œä¸´åºŠåº”ç”¨åœºæ™¯ã€‚æ–‡ç« è¯„ä¼°äº†å¤šæ¬¾å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…å£°æ³¢å›¾åƒç†è§£æ–¹é¢çš„æ€§èƒ½ï¼Œå‘ç°å®ƒä»¬åœ¨å›¾åƒçº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œä¸´åºŠè¯­è¨€ç”Ÿæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’ŒåŠ é€ŸåŒ»å­¦è¶…å£°æˆåƒé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶æä¾›äº†ä¸¥è°¨å’Œç»Ÿä¸€çš„å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U2-BENCHæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¶…å£°æ³¢ç†è§£æ–¹é¢çš„é¦–ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒæ¶µç›–äº†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’Œæ–‡æœ¬ç”Ÿæˆï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§£å‰–å­¦åŒºåŸŸå’Œä¸´åºŠåº”ç”¨åœºæ™¯ã€‚</li>
<li>U2-BENCHå¯¹å¤šæ¬¾å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹åœ¨å›¾åƒçº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>åœ¨ç©ºé—´æ¨ç†å’Œä¸´åºŠè¯­è¨€ç”Ÿæˆæ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>U2-BENCHä¸ºè¯„ä¼°å’ŒåŠ é€ŸåŒ»å­¦è¶…å£°æˆåƒé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶æä¾›äº†ä¸¥è°¨å’Œç»Ÿä¸€çš„å¹³å°ã€‚</li>
<li>U2-BENCHåŒ…æ‹¬å¤§é‡ç—…ä¾‹æ•°æ®ï¼Œå¯ç”¨äºçœŸå®ä¸–ç•ŒåŒ»å­¦è¶…å£°å½±åƒåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2898d0677c881c6d415f8fba58b398c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ee9f732c9dec75ae933e707088f6231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5c0ffcdbd3c60b6da8c05668dc6409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d4e35f6c8fff3b706799de808d3f2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f8389b7acacf76f1ffb4b87b162eef3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis"><a href="#A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis" class="headerlink" title="A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis"></a>A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis</h2><p><strong>Authors:Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif</strong></p>
<p>Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of â€œpositiveâ€ and â€œnegativeâ€ samples, where positive pairs (e.g., variation of the same image&#x2F;object) are brought together in the embedding space, and negative pairs (e.g., views from different images&#x2F;objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å­¦ä¹ æ½œåœ¨çš„æ¨¡å¼å¹¶ä»æ— æ ‡ç­¾æ•°æ®ä¸­æå–åˆ¤åˆ«ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå…¶ä¸­æ­£æ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚ï¼ŒåŒä¸€å›¾åƒ&#x2F;å¯¹è±¡çš„å˜ä½“ï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­èšé›†åœ¨ä¸€èµ·ï¼Œè€Œè´Ÿæ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªä¸åŒå›¾åƒ&#x2F;å¯¹è±¡çš„è§†å›¾ï¼‰åˆ™è¢«æ¨å¼€ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œå›¾åƒæ–‡æœ¬åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œä¸”ä¸éœ€è¦ä¾èµ–å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…¨é¢è®¨è®ºäº†ä¸æ–‡æœ¬-å›¾åƒæ¨¡å‹ç›¸å…³çš„å¯¹æ¯”å­¦ä¹ çš„æœ¯è¯­ã€æœ€æ–°å‘å±•ä»¥åŠåº”ç”¨ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿‘å¹´æ¥æ–‡æœ¬-å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„æ¨¡å‹ç»“æ„å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»äº†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æœ€æ–°æŠ€æœ¯ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åŸºäºæ–‡æœ¬-å›¾åƒçš„æœ€æ–°æœ€å…ˆè¿›çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11101v3">PDF</a> 38 pages, 8 figures, survey paper</p>
<p><strong>Summary</strong><br>     è‡ªç›‘ç£å­¦ä¹ é€šè¿‡ä»éæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨æ¨¡å¼å’Œæå–åˆ¤åˆ«ç‰¹å¾ï¼Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå°†æ­£æ ·æœ¬å¯¹æ‹‰è¿‘åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶å°†è´Ÿæ ·æœ¬å¯¹æ¨å¼€ã€‚æ­¤æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†ææ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æœ¬æ–‡å…¨é¢æ¢è®¨äº†æ–‡æœ¬-å›¾åƒæ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ æœ¯è¯­ã€æœ€æ–°å‘å±•å’Œåº”ç”¨ï¼Œæ¦‚è¿°äº†è¿‘å¹´æ¥çš„å¯¹æ¯”å­¦ä¹ æ–‡æœ¬-å›¾åƒæ¨¡å‹çš„æ–¹æ³•ï¼ŒæŒ‰æ¨¡å‹ç»“æ„åˆ†ç±»ï¼Œå¹¶ä»‹ç»äº†æœ€æ–°çš„æŠ€æœ¯è¿›å±•ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡éæ ‡è®°æ•°æ®ä¸­çš„æ¨¡å¼å’Œç‰¹å¾è¿›è¡Œå­¦ä¹ ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ åœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­å¼•å…¥æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æ¦‚å¿µï¼Œé€šè¿‡æ‹‰è¿‘æ­£æ ·æœ¬å¯¹å’Œæ¨å¼€è´Ÿæ ·æœ¬å¯¹æ¥è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†ææ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ•ˆæœï¼Œå°¤å…¶åœ¨å‡å°‘æ ‡æ³¨æ•°æ®ä¾èµ–æ–¹é¢ã€‚</li>
<li>æ–‡æœ¬-å›¾åƒæ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•å…¨é¢æ¢è®¨äº†è¯¥é¢†åŸŸçš„æœ¯è¯­ã€æœ€æ–°å‘å±•å’Œåº”ç”¨ã€‚</li>
<li>æ–‡æœ¬-å›¾åƒæ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•æŒ‰æ¨¡å‹ç»“æ„è¿›è¡Œäº†åˆ†ç±»ã€‚</li>
<li>è¿‘å¹´æ¥çš„æŠ€æœ¯è¿›å±•åŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¨¡å‹æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b4bc219995a9306c13e1ff75bbd5344.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LesionDiffusion-Towards-Text-controlled-General-Lesion-Synthesis"><a href="#LesionDiffusion-Towards-Text-controlled-General-Lesion-Synthesis" class="headerlink" title="LesionDiffusion: Towards Text-controlled General Lesion Synthesis"></a>LesionDiffusion: Towards Text-controlled General Lesion Synthesis</h2><p><strong>Authors:Henrui Tian, Wenhui Lei, Linrui Dai, Hanyu Chen, Xiaofan Zhang</strong></p>
<p>Fully-supervised lesion recognition methods in medical imaging face challenges due to the reliance on large annotated datasets, which are expensive and difficult to collect. To address this, synthetic lesion generation has become a promising approach. However, existing models struggle with scalability, fine-grained control over lesion attributes, and the generation of complex structures. We propose LesionDiffusion, a text-controllable lesion synthesis framework for 3D CT imaging that generates both lesions and corresponding masks. By utilizing a structured lesion report template, our model provides greater control over lesion attributes and supports a wider variety of lesion types. We introduce a dataset of 1,505 annotated CT scans with paired lesion masks and structured reports, covering 14 lesion types across 8 organs. LesionDiffusion consists of two components: a lesion mask synthesis network (LMNet) and a lesion inpainting network (LINet), both guided by lesion attributes and image features. Extensive experiments demonstrate that LesionDiffusion significantly improves segmentation performance, with strong generalization to unseen lesion types and organs, outperforming current state-of-the-art models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HengruiTianSJTU/LesionDiffusion">https://github.com/HengruiTianSJTU/LesionDiffusion</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œå…¨ç›‘ç£ç—…ç¶è¯†åˆ«æ–¹æ³•é¢ä¸´ç€ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œè€Œè¿™äº›æ•°æ®é›†çš„æ”¶é›†æ—¢æ˜‚è´µåˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆæˆç—…ç¶ç”Ÿæˆå·²ç»æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å¯æ‰©å±•æ€§ã€å¯¹ç—…ç¶å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»¥åŠå¤æ‚ç»“æ„çš„ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†LesionDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3D CTæˆåƒçš„æ–‡æœ¬å¯æ§ç—…ç¶åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆç—…ç¶å’Œç›¸åº”çš„æ©è†œã€‚é€šè¿‡åˆ©ç”¨ç»“æ„åŒ–ç—…ç¶æŠ¥å‘Šæ¨¡æ¿ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æä¾›äº†å¯¹ç—…ç¶å±æ€§æ›´å¤§çš„æ§åˆ¶åŠ›ï¼Œå¹¶æ”¯æŒæ›´å¤šç§ç±»çš„ç—…ç¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«1505ä¸ªæ ‡æ³¨CTæ‰«æçš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ‰«æéƒ½æœ‰é…å¯¹çš„ç—…ç¶æ©è†œå’Œç»“æ„åŒ–æŠ¥å‘Šï¼Œè¦†ç›–8ä¸ªå™¨å®˜ä¸­çš„14ç§ç—…ç¶ã€‚LesionDiffusionç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç—…ç¶æ©è†œåˆæˆç½‘ç»œï¼ˆLMNetï¼‰å’Œç—…ç¶å¡«å……ç½‘ç»œï¼ˆLINetï¼‰ï¼Œä¸¤è€…å‡ç”±ç—…ç¶å±æ€§å’Œå›¾åƒç‰¹å¾å¼•å¯¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLesionDiffusionæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œå¯¹æœªè§è¿‡çš„ç—…ç¶ç±»å‹å’Œå™¨å®˜å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HengruiTianSJTU/LesionDiffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HengruiTianSJTU/LesionDiffusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00741v4">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„ç—…ç¶è¯†åˆ«æ–¹æ³•é¢ä¸´ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ï¼Œè¿™æ—¢è€—è´¹æˆæœ¬åˆéš¾ä»¥æ”¶é›†ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œåˆæˆç—…ç¶ç”Ÿæˆæˆä¸ºäº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œç°æœ‰æ¨¡å‹åœ¨å¯æ‰©å±•æ€§ã€å¯¹ç—…ç¶å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»¥åŠå¤æ‚ç»“æ„çš„ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†LesionDiffusionï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº3D CTæˆåƒçš„æ–‡æœ¬å¯æ§ç—…ç¶åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆç—…ç¶åŠå…¶å¯¹åº”çš„æ©è†œã€‚é€šè¿‡åˆ©ç”¨ç»“æ„åŒ–ç—…ç¶æŠ¥å‘Šæ¨¡æ¿ï¼Œè¯¥æ¨¡å‹å®ç°å¯¹ç—…ç¶å±æ€§çš„æ›´å¥½æ§åˆ¶å¹¶æ”¯æŒæ›´å¤šç§ç±»çš„ç—…ç¶ã€‚å®éªŒè¡¨æ˜ï¼ŒLesionDiffusionæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œå¯¹æœªè§è¿‡çš„ç—…ç¶ç±»å‹å’Œå™¨å®˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­çš„å…¨ç›‘ç£ç—…ç¶è¯†åˆ«æ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œå­˜åœ¨æˆæœ¬å’Œæ”¶é›†éš¾åº¦é—®é¢˜ã€‚</li>
<li>åˆæˆç—…ç¶ç”Ÿæˆæ˜¯åº”å¯¹æ­¤æŒ‘æˆ˜çš„æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¯æ‰©å±•æ€§ã€å¯¹ç—…ç¶å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»¥åŠå¤æ‚ç»“æ„çš„ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>LesionDiffusionæ˜¯ä¸€ä¸ªç”¨äº3D CTæˆåƒçš„æ–‡æœ¬å¯æ§ç—…ç¶åˆæˆæ¡†æ¶ï¼Œèƒ½ç”Ÿæˆç—…ç¶åŠå…¶æ©è†œã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç»“æ„åŒ–ç—…ç¶æŠ¥å‘Šæ¨¡æ¿å®ç°å¯¹ç—…ç¶å±æ€§çš„æ›´å¥½æ§åˆ¶ï¼Œå¹¶æ”¯æŒå¤šç§ç—…ç¶ç±»å‹ã€‚</li>
<li>LesionDiffusionæ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼Œå¯¹æœªè§è¿‡çš„ç—…ç¶ç±»å‹å’Œå™¨å®˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc53d73727ec065fa8230246a51c58c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b601c0c98bc3f6e70d620d8dad67123.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-402ce4f3de7dc83322b8332fbfca0a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5af67a85513b1f808fbde0ecc1e73a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Exploring-the-mysterious-high-ionization-source-powering-Ne-V-in-high-z-analog-SBS0335-052-E-with-JWST-MIRI"><a href="#Exploring-the-mysterious-high-ionization-source-powering-Ne-V-in-high-z-analog-SBS0335-052-E-with-JWST-MIRI" class="headerlink" title="Exploring the mysterious high-ionization source powering [Ne V] in   high-z analog SBS0335-052 E with JWST&#x2F;MIRI"></a>Exploring the mysterious high-ionization source powering [Ne V] in   high-z analog SBS0335-052 E with JWST&#x2F;MIRI</h2><p><strong>Authors:Matilde Mingozzi, Macarena Garcia Del Valle-Espinosa, Bethan L. James, Ryan J. Rickards Vaught, Matthew Hayes, Ricardo O. AmorÃ­n, Claus Leitherer, Alessandra Aloisi, Leslie Hunt, David Law, Chris Richardson, Aidan Pidgeon, Karla Z. Arellano-CÃ³rdova, Danielle A. Berg, John Chisholm, Svea Hernandez, Logan Jones, Nimisha Kumari, Crystal L. Martin, Swara Ravindranath, Livia Vallini, Xinfeng Xu</strong></p>
<p>Nearby blue compact dwarf galaxies (BCDs) share similar properties with objects from the Epoch of Reionization revealed by JWST, in terms of low stellar mass, low metallicity and high specific star-formation rate. Thus, they represent ideal local laboratories for detailed multi-wavelength studies to understand their properties and the mechanisms shaping them. We report the first JWST MIRI&#x2F;MRS observations of the BCD SBS 0335-052 E, analyzing MIR emission lines tracing different levels of ionization (e.g., [NeII], [SIV], [NeIII], [OIV], [NeV]) of the ionized gas. SBS 0335-052 E MIR emission is characterized by a bright point source, located in one of the youngest and most embedded stellar clusters ($t\sim3$ Myr, $A_V\sim15$), and underlying extended high-ionization emission (i.e., [OIV], [NeV]) from the surroundings of the older and less dusty stellar clusters ($t&lt; 20 $ Myr, $A_V\sim8$). From the comparison with state-of-the-art models, we can exclude shocks, X-ray binaries, and old stellar populations as the main sources of the high ionization. Interestingly, a 4-16% contribution of a $\sim10^5$ M$_\odot$ intermediate massive black hole (IMBH) is needed to justify the strong [NeV]&#x2F;[NeII] and would be consistent with optical&#x2F;UV line ratios from previous studies. However, even IMBH models cannot explain the strongest [OIV]&#x2F;[NeIII]. Also, star-forming models (regardless of including X-ray binaries) struggle to reproduce even the lower ionization line ratios (e.g., [SIV]&#x2F;[NeII]) typically observed in BCDs. Overall, while current models suggest the need to account for an accreting IMBH in this high-$z$ analog, limitations still exist in predicting high-ionization emission lines (I.P. $&gt;54$ eV) when modeling these low-metallicity environments, thus other sources of ionization cannot be fully ruled out. </p>
<blockquote>
<p>é‚»è¿‘çš„è“è‰²è‡´å¯†çŸ®æ˜Ÿç³»ï¼ˆBCDsï¼‰ä¸JWSTæ­ç¤ºçš„å†ç”µç¦»æ—¶ä»£ç‰©ä½“å…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§ï¼Œå…·æœ‰ä½æ’æ˜Ÿè´¨é‡ã€ä½é‡‘å±ä¸°åº¦å’Œé«˜ç‰¹å®šæ’æ˜Ÿå½¢æˆç‡ã€‚å› æ­¤ï¼Œå®ƒä»¬ä½œä¸ºç†æƒ³çš„æœ¬åœ°å®éªŒå®¤ï¼Œæœ‰åŠ©äºè¿›è¡Œè¯¦ç»†çš„å¤šæ³¢é•¿ç ”ç©¶ï¼Œä»¥äº†è§£å®ƒä»¬çš„ç‰¹æ€§å’Œå½¢æˆæœºåˆ¶ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†BCD SBS 0335-052 Eçš„é¦–æ‰¹JWST MIRI&#x2F;MRSè§‚æµ‹ç»“æœï¼Œåˆ†æMIRå‘å°„çº¿è¿½è¸ªä¸åŒç¨‹åº¦çš„ç”µç¦»ï¼ˆä¾‹å¦‚ï¼Œ[NeII]ã€[SIV]ã€[NeIII]ã€[OIV]ã€[NeV]ï¼‰çš„ç”µç¦»æ°”ä½“ã€‚SBS 0335-052 Eçš„MIRå‘å°„ç‰¹å¾è¡¨ç°ä¸ºä¸€ä¸ªæ˜äº®çš„ç‚¹æºï¼Œä½äºæœ€å¹´è½»ä¸”æœ€åµŒå…¥çš„æ’æ˜Ÿå›¢ä¹‹ä¸€ï¼ˆt<del>3Myrï¼ŒAv</del>15ï¼‰ï¼Œä»¥åŠæ¥è‡ªè¾ƒè€ä¸”å°˜åŸƒè¾ƒå°‘çš„æ’æ˜Ÿå›¢å‘¨å›´çš„é«˜ç”µç¦»å‘å°„ï¼ˆä¾‹å¦‚ï¼Œ[OIV]ã€[NeV]ï¼‰ï¼ˆt&lt;20Myrï¼ŒAv~8ï¼‰ã€‚é€šè¿‡ä¸æœ€æ–°æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥æ’é™¤å†²å‡»æ³¢ã€Xå°„çº¿åŒæ˜Ÿå’Œè€æ’æ˜Ÿç¾¤ä½œä¸ºä¸»è¦çš„é«˜ç”µç¦»æ¥æºã€‚æœ‰è¶£çš„æ˜¯ï¼Œ[NeV]&#x2F;[NeII]çš„æ¯”ç‡éœ€è¦ä¸€ä¸ªçº¦å æ€»é‡å¤§çº¦ä¸ºè¾¾åˆ°ç™¾è¿›äº”MBHçš„å¤§è´¨é‡çš„é»‘æ´æ—¶è¯´æ˜ç»“è®ºå¤§çº¦æ˜¯ç™¾åˆ†ä¹‹ä¸€è‡´å½“å‚ä¸å›½å®¶æ˜¯å¦è¿™ä¹ˆé¡¹æ›´æœ‰é€»è¾‘çš„æ½œå¯¹çš„ç‰¹è´¨æ›´é‡è¦æ˜Ÿæ¼”æ®å·²æå‡ºäº†è¿›è¡Œç¤ºè€…æ‰€éœ€ä»¥åŠå’Œè¿‡å»ç ”ç©¶å¾—å‡ºçš„å…‰å­¦ç´«å¤–çº¿çš„æ¯”ä¾‹å€¼åŸºæœ¬å»åˆä¸è¿‡å³ä¾¿å¦‚æ­¤ä¹Ÿæœ‰é«˜è´¨ç”šè‡³æˆ‘ä»¬çš„ç ”ç©¶ä¸­è¿™äº›åŒ…æ‹¬å¤„äºå³ä¾¿ç†å³ä½¿Xå°„çº¿åŒæ˜Ÿæ¨¡å‹ä¹Ÿä¾ç„¶éš¾ä»¥è§£é‡ŠBCDä¸­é€šå¸¸è§‚å¯Ÿåˆ°çš„è¾ƒä½ç”µç¦»çº¿æ¯”ç‡ï¼ˆä¾‹å¦‚ï¼Œ[SIV]&#x2F;[NeII]ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œè™½ç„¶å½“å‰æ¨¡å‹æš—ç¤ºéœ€è¦åœ¨æ­¤é«˜çº¢ç§»ç±»ä¼¼ç‰©ä¸­è€ƒè™‘å­˜åœ¨æ­£åœ¨å¸ç§¯çš„IMBHï¼Œä½†åœ¨æ¨¡æ‹Ÿè¿™äº›ä½é‡‘å±ä¸°åº¦ç¯å¢ƒæ—¶é¢„æµ‹é«˜ç”µç¦»å‘å°„çº¿ï¼ˆIP&gt; 54 eVï¼‰ä»å­˜åœ¨å±€é™æ€§ï¼Œå› æ­¤ä¸èƒ½å®Œå…¨æ’é™¤å…¶ä»–ç”µç¦»æºçš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07662v2">PDF</a> Accepted for publication in ApJ</p>
<p><strong>Summary</strong></p>
<p>åœ¨é™„è¿‘çš„å°å‹è“çŸ®æ˜Ÿç³»ï¼ˆBCDsï¼‰ä¸­ï¼Œå‘ç°äº†ä¸JWSTæ­ç¤ºçš„å†ç”µç¦»æ—¶ä»£ç‰©ä½“ç›¸ä¼¼çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬ä½æ’æ˜Ÿè´¨é‡ã€ä½é‡‘å±ä¸°åº¦å’Œé«˜ç‰¹å®šæ’æ˜Ÿå½¢æˆç‡ã€‚å› æ­¤ï¼Œå®ƒä»¬æˆä¸ºè¿›è¡Œæ·±å…¥äº†è§£å…¶å±æ€§å’Œå¡‘é€ æœºåˆ¶çš„è¯¦ç»†å¤šæ³¢é•¿ç ”ç©¶çš„ç†æƒ³æœ¬åœ°å®éªŒå®¤ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨JWST MIRI&#x2F;MRSå¯¹BCD SBS 0335-052 Eçš„é¦–æ¬¡è§‚æµ‹ï¼Œåˆ†æè¿½è¸ªä¸åŒç”µç¦»æ°´å¹³çš„MIRå‘å°„çº¿ï¼ˆä¾‹å¦‚ï¼Œ[NeII]ã€[SIV]ã€[NeIII]ã€[OIV]ã€[NeV]ï¼‰ã€‚SBS 0335-052 Eçš„MIRå‘å°„ç‰¹ç‚¹ä¸ºä¸€ä¸ªæ˜äº®çš„ç‚¹æºï¼Œä½äºæœ€å¹´è½»ã€æœ€åµŒå…¥çš„æ’æ˜Ÿå›¢ä¹‹ä¸€ï¼ˆå¹´é¾„çº¦3Myrï¼Œæ¶ˆå…‰çº¦15ï¼‰ï¼Œä»¥åŠæ¥è‡ªè¾ƒè€å’Œè¾ƒå°‘å°˜åŸƒçš„æ’æ˜Ÿå›¢å‘¨å›´çš„é«˜ç”µç¦»å‘å°„ï¼ˆä¾‹å¦‚ï¼Œ[OIV]ã€[NeV]ï¼Œå¹´é¾„&lt;20Myrï¼Œæ¶ˆå…‰çº¦8ï¼‰ã€‚é€šè¿‡ä¸æœ€æ–°æ¨¡å‹çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥æ’é™¤å†²å‡»æ³¢ã€Xå°„çº¿åŒæ˜Ÿå’Œè€æ’æ˜Ÿç¾¤ä½“ä½œä¸ºé«˜ç”µç¦»çš„ä¸»è¦æ¥æºã€‚æœ‰è¶£çš„æ˜¯ï¼Œéœ€è¦ä¸€ä¸ªçº¦å æ€»é‡4-16%çš„çº¦10^5 Mâ˜‰çš„ä¸­é—´è´¨é‡é»‘æ´ï¼ˆIMBHï¼‰æ¥è§£é‡Šå¼ºçƒˆçš„[NeV]&#x2F;[NeII]ï¼Œè¿™ä¸å…ˆå‰çš„å…‰å­¦&#x2F;ç´«å¤–çº¿æ¯”ç‡ç ”ç©¶ä¸€è‡´ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯IMBHæ¨¡å‹ä¹Ÿæ— æ³•è§£é‡Šæœ€å¼ºçš„[OIV]&#x2F;[NeIII]ã€‚åŒæ—¶ï¼Œæ’æ˜Ÿå½¢æˆæ¨¡å‹ï¼ˆæ— è®ºæ˜¯å¦åŒ…å«Xå°„çº¿åŒæ˜Ÿï¼‰éƒ½éš¾ä»¥å¤åˆ¶åœ¨BCDsä¸­é€šå¸¸è§‚å¯Ÿåˆ°çš„è¾ƒä½ç”µç¦»çº¿æ¯”ç‡ï¼ˆä¾‹å¦‚ï¼Œ[SIV]&#x2F;[NeII]ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œè™½ç„¶å½“å‰æ¨¡å‹éœ€è¦åœ¨é«˜zç±»ä¼¼ç‰©ä¸­è€ƒè™‘IMBHçš„å‚ä¸ï¼Œä½†åœ¨æ¨¡æ‹Ÿè¿™äº›ä½é‡‘å±ä¸°åº¦ç¯å¢ƒæ—¶ï¼Œé¢„æµ‹é«˜ç”µç¦»å‘å°„çº¿ï¼ˆIP&gt; 54 eVï¼‰ä»å­˜åœ¨å±€é™æ€§ï¼Œå› æ­¤ä¸èƒ½å®Œå…¨æ’é™¤å…¶ä»–ç”µç¦»æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é™„è¿‘çš„å°å‹è“çŸ®æ˜Ÿç³»ï¼ˆBCDsï¼‰ä¸JWSTæ­ç¤ºçš„å†ç”µç¦»æ—¶ä»£ç‰©ä½“å…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§ã€‚</li>
<li>JWST MIRI&#x2F;MRSè§‚æµ‹æ­ç¤ºäº†SBS 0335-052 Eçš„MIRå‘å°„ç‰¹æ€§ã€‚</li>
<li>ç‚¹æºä½äºå¹´è½»æ’æ˜Ÿå›¢ä¸­ï¼Œå…·æœ‰é«˜çš„ç‰¹å®šæ˜Ÿå½¢æˆç‡ã€‚</li>
<li>é«˜ç”µç¦»å‘å°„æ¥æºäºè¾ƒè€æ’æ˜Ÿå›¢å‘¨å›´åŒºåŸŸã€‚</li>
<li>éœ€è¦è€ƒè™‘ä¸­é—´è´¨é‡é»‘æ´ï¼ˆIMBHï¼‰ä½œä¸ºé«˜ç”µç¦»çš„å¯èƒ½æ¥æºä¹‹ä¸€ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨é¢„æµ‹é«˜ç”µç¦»å‘å°„çº¿æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b22badf8114374fb0a91b8069f88c776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8102e562979bb9118ff05c67dfce707.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc3eaf0c7c876365699da636ffd3613a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84968e835aa89265357c6d3c500cb4d8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-475eda727dd89aed026cd543c204fb32.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Speech Token Prediction via Compressed-to-fine Language Modeling for   Speech Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18862ca7cb0f1725e215a9b917dda0b5.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  AdaHuman Animatable Detailed 3D Human Generation with Compositional   Multiview Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
