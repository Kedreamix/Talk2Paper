<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Open CaptchaWorld A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e8a968a21b03aa19e3527fc3184a28a5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="Open-CaptchaWorld-A-Comprehensive-Web-based-Platform-for-Testing-and-Benchmarking-Multimodal-LLM-Agents"><a href="#Open-CaptchaWorld-A-Comprehensive-Web-based-Platform-for-Testing-and-Benchmarking-Multimodal-LLM-Agents" class="headerlink" title="Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents"></a>Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents</h2><p><strong>Authors:Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen</strong></p>
<p>CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL. </p>
<blockquote>
<p>CAPTCHAä¸€ç›´æ˜¯ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²ç½‘ç»œä»£ç†çš„å…³é”®ç“¶é¢ˆï¼Œç»å¸¸é˜»æ­¢å®ƒä»¬å®Œæˆç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚è™½ç„¶ç°ä»£çš„å¤šæ¨¡å¼LLMä»£ç†åœ¨é™æ€æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†å®ƒä»¬å¤„ç†åƒCAPTCHAè¿™æ ·çš„äº¤äº’å¼ã€å¤šæ­¥éª¤æ¨ç†æŒ‘æˆ˜çš„èƒ½åŠ›å´å¾ˆå°‘å¾—åˆ°æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Open CaptchaWorldï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°MLLMé©±åŠ¨ä»£ç†çš„è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›è€Œè®¾è®¡çš„ç½‘ç»œåŸºå‡†æµ‹è¯•å¹³å°å’Œå·¥å…·ï¼Œé€šè¿‡å¤šæ ·åŒ–å’ŒåŠ¨æ€çš„CAPTCHAè°œé¢˜æ¥å®ç°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†20ç§ç°ä»£CAPTCHAç±»å‹ï¼Œæ€»å…±225ä¸ªCAPTCHAï¼Œé€šè¿‡æˆ‘ä»¬æå‡ºçš„æ–°æŒ‡æ ‡è¿›è¡Œæ³¨é‡Šï¼šCAPTCHAæ¨ç†æ·±åº¦ï¼Œè¯¥æŒ‡æ ‡é‡åŒ–äº†è§£ç­”æ¯ä¸ªè°œé¢˜æ‰€éœ€çš„è®¤çŸ¥å’Œè¡ŒåŠ¨æ­¥éª¤æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»å§‹ç»ˆå–å¾—è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œè€Œæœ€å…ˆè¿›çš„MLLMä»£ç†åˆ™é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼ŒBrowser-Use Openai-o3çš„æˆåŠŸç‡æœ€é«˜åªæœ‰40.0%ï¼Œè¿œä½äºäººç±»çš„è¡¨ç°æ°´å¹³93.3%ã€‚è¿™å‡¸æ˜¾äº†Open CaptchaWorldä½œä¸ºä¸€ä¸ªåŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ï¼Œå¯ä»¥è¯Šæ–­å½“å‰å¤šæ¨¡å¼ä»£ç†çš„å±€é™æ€§ï¼Œå¹¶å¼•å¯¼å¼€å‘æ›´ç¨³å¥çš„å¤šæ¨¡å¼æ¨ç†ç³»ç»Ÿã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ä»¥ä¸‹URLç½‘å€æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24878v1">PDF</a> Code at: <a target="_blank" rel="noopener" href="https://github.com/MetaAgentX/OpenCaptchaWorld">https://github.com/MetaAgentX/OpenCaptchaWorld</a></p>
<p><strong>Summary</strong><br>    æ¨å‡ºé¦–ä¸ªé’ˆå¯¹å¤šæ¨¡æ€LLMä»£ç†äººçš„ç½‘ç»œåŸºå‡†æµ‹è¯•å¹³å°Open CaptchaWorldï¼Œé€šè¿‡225ä¸ªä¸åŒç±»å‹çš„CAPTCHAè°œé¢˜è¯„ä¼°å…¶è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾æ˜¾ç¤ºäººç±»è¡¨ç°ä¼˜å¼‚ï¼Œè€Œå½“å‰æœ€å…ˆè¿›çš„MLLMä»£ç†äººè§£å†³ç‡æœ€é«˜ä»…è¾¾40%ï¼Œè¿œä½äºäººç±»æ°´å¹³ã€‚Open CaptchaWorldæˆä¸ºè¯Šæ–­å½“å‰å¤šæ¨¡æ€ä»£ç†äººå±€é™æ€§çš„é‡è¦åŸºå‡†ï¼Œå¹¶å¼•å¯¼å¼€å‘æ›´ç¨³å¥çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAPTCHAsæ˜¯éƒ¨ç½²ç½‘ç»œä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œç»å¸¸é˜»æ­¢å…¶å®Œæˆç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚</li>
<li>ç°ä»£å¤šæ¨¡æ€LLMä»£ç†åœ¨é™æ€æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†äº¤äº’å¼ã€å¤šæ­¥éª¤æ¨ç†æŒ‘æˆ˜æ–¹é¢èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ã€‚</li>
<li>Open CaptchaWorldæ˜¯é¦–ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šæ¨¡æ€LLMä»£ç†çš„è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›çš„ç½‘ç»œåŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å¹³å°åŒ…å«225ä¸ªä¸åŒç±»å‹çš„CAPTCHAè°œé¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šCAPTCHAæ¨ç†æ·±åº¦ï¼Œç”¨äºé‡åŒ–è§£å†³æ¯ä¸ªè°œé¢˜æ‰€éœ€çš„è®¤çŸ¥å’ŒåŠ¨ä½œæ­¥éª¤æ•°é‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œäººç±»è¡¨ç°æ¥è¿‘å®Œç¾ï¼Œè€Œæœ€å…ˆè¿›çš„MLLMä»£ç†äººçš„è§£å†³ç‡è¿œä½äºäººç±»æ°´å¹³ï¼Œæœ€é«˜ä»…è¾¾40%ã€‚</li>
<li>Open CaptchaWorldæ˜¯ä¸€ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•ï¼Œæœ‰åŠ©äºè¯Šæ–­å½“å‰å¤šæ¨¡æ€ä»£ç†äººçš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-009651660876cbeec6e6e1401afd8206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4fe6293ba2dfb864fa1e30a62597693.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66cb91fb17442e40d24f022304115fb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccdf52c7b86623bc15244d7304c29830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a968a21b03aa19e3527fc3184a28a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MoDoMoDo-Multi-Domain-Data-Mixtures-for-Multimodal-LLM-Reinforcement-Learning"><a href="#MoDoMoDo-Multi-Domain-Data-Mixtures-for-Multimodal-LLM-Reinforcement-Learning" class="headerlink" title="MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement   Learning"></a>MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement   Learning</h2><p><strong>Authors:Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained modelâ€™s accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œæœ€è¿‘å‡ºç°åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹åï¼Œå…¶åœ¨å…·æœ‰ç»“æ„åŒ–ã€å¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å¸¦æ¥äº†å·¨å¤§çš„æœºä¼šï¼Œä½†ç”±äºéœ€è¦å¾®å¦™çš„è§†è§‰ã€é€»è¾‘å’Œç©ºé—´èƒ½åŠ›çš„è§†è§‰è¯­è¨€ä»»åŠ¡çš„å¹¿æ³›æ€§å’Œå¼‚è´¨æ€§ï¼Œå…¶åº”ç”¨å˜å¾—å¤æ‚ã€‚å› æ­¤ï¼Œä½¿ç”¨RLVRåœ¨å¤šæ•°æ®é›†ä¸Šè®­ç»ƒMLLMå¯èƒ½æ˜¯æœ‰ç›Šçš„ï¼Œä½†åˆ›å»ºäº†æ¥è‡ªä¸åŒæ•°æ®é›†ä¹‹é—´äº¤äº’çš„ç›¸äº’å†²çªç›®æ ‡æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æœ€ä½³çš„æ•°æ®é›†æ··åˆç­–ç•¥æ¥æé«˜æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºMultimodal LLM RLVRå¼•å…¥äº†ç³»ç»Ÿçš„åè®­ç»ƒæ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¥æ ¼çš„æ•°æ®æ··åˆé—®é¢˜å…¬å¼å’ŒåŸºå‡†å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ€RLVRæ¡†æ¶ï¼Œç”¨äºå¤šæ•°æ®é›†çš„åè®­ç»ƒï¼Œé€šè¿‡æ•´ç†åŒ…å«ä¸åŒå¯éªŒè¯çš„è§†å¬è¯­è¨€é—®é¢˜çš„æ•°æ®é›†ï¼Œå¹¶å¯ç”¨å…·æœ‰ä¸åŒå¯éªŒè¯å¥–åŠ±çš„å¤šåŸŸåœ¨çº¿RLå­¦ä¹ ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®æ··åˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥ä»æ•°æ®æ··åˆåˆ†å¸ƒä¸­å­¦ä¹ é¢„æµ‹RLå¾®è°ƒç»“æœï¼Œä»è€Œä¼˜åŒ–æœ€ä½³çš„æ··åˆã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œå½“ç»“åˆæ··åˆé¢„æµ‹ç­–ç•¥æ—¶ï¼Œå¤šåŸŸRLVRè®­ç»ƒå¯ä»¥æ˜¾ç€æé«˜MLLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æœ€ä½³æ··åˆä¸é‡‡ç”¨å‡åŒ€æ•°æ®æ··åˆè¿›è¡Œåè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæé«˜äº†å…¶åœ¨è¶…å‡ºåˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡5.24%ï¼Œä¸é¢„å¾®è°ƒåŸºå‡†ç›¸æ¯”ï¼Œæé«˜äº†æ€»å‡†ç¡®ç‡20.74%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24871v1">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://modomodo-rl.github.io/">https://modomodo-rl.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åæˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œå…¶åœ¨å…·æœ‰ç»“æ„åŒ–ã€å¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å­˜åœ¨å¹¿æ³›çš„æœºé‡ï¼Œä½†ç”±äºéœ€è¦å¾®å¦™çš„è§†è§‰ã€é€»è¾‘å’Œç©ºé—´èƒ½åŠ›ï¼Œè§†è§‰è¯­è¨€ä»»åŠ¡çš„å¹¿æ³›æ€§å’Œå¼‚è´¨æ€§ä½¿å…¶å¤æ‚åŒ–ã€‚å¯¹å¤šä¸ªæ•°æ®é›†ä½¿ç”¨RLVRè®­ç»ƒMLLMå¯èƒ½æœ‰ç›Šï¼Œä½†ä¸åŒæ•°æ®é›†ä¹‹é—´äº¤äº’äº§ç”Ÿçš„ç›®æ ‡å†²çªçªæ˜¾äº†éœ€è¦ä¼˜åŒ–æ•°æ®é›†æ··åˆç­–ç•¥ä»¥æé«˜æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºMultimodal LLM RLVRå¼•å…¥äº†ä¸€ç§ç³»ç»Ÿçš„åè®­ç»ƒæ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¥æ ¼çš„æ•°æ®æ··åˆé—®é¢˜å…¬å¼å’ŒåŸºå‡†å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ï¼šï¼ˆ1ï¼‰å¼€å‘äº†ä¸€ç§å¤šæ¨¡æ€RLVRæ¡†æ¶ï¼Œç”¨äºé€šè¿‡åŒ…å«ä¸åŒå¯éªŒè¯çš„è§†è§‰è¯­è¨€é—®é¢˜æ¥ç­›é€‰æ•°æ®é›†çš„æ•°æ®é›†è®­ç»ƒåçš„è®­ç»ƒé˜¶æ®µï¼›(2) æå‡ºäº†ä¸€ç§æ•°æ®æ··åˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé¢„æµ‹æ•°æ®æ··åˆåˆ†å¸ƒä¸‹çš„RLç²¾ç»†è°ƒæ•´ç»“æœï¼Œä»è€Œä¼˜åŒ–æœ€ä½³æ··åˆã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œå½“ç»“åˆæ··åˆé¢„æµ‹ç­–ç•¥æ—¶ï¼Œå¤šåŸŸRLVRè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜MLLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚ä¸æˆ‘ä»¬ä»¥å‰å¾®è°ƒåŸºå‡†ç›¸æ¯”ï¼Œæœ€ä½³æ··åˆå¯ä»¥æé«˜æ¨¡å‹åœ¨è¶…å‡ºåˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡å¹³å‡æé«˜5.24%ï¼Œå¹¶ä¸”ç›¸å¯¹äºç»Ÿä¸€æ•°æ®æ··åˆçš„åè®­ç»ƒæ¨¡å‹å¹³å‡æé«˜äº†é«˜è¾¾é«˜è¾¾é«˜è¾¾20.74%ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æœ€ä½³æ··åˆç­–ç•¥ä¸ºæœªæ¥çš„å¤šæ¨¡æ€LLMè®­ç»ƒæä¾›äº†æ–°çš„è§†è§’å’Œæ½œåœ¨çš„æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åè®­ç»ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰ç»“æ„åŒ–ç­”æ¡ˆçš„ä»»åŠ¡ä¸Šã€‚</li>
<li>å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè§†è§‰è¯­è¨€ä»»åŠ¡çš„å¹¿æ³›æ€§å’Œå¼‚è´¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9625bffcac259fd4cb6696f87565b13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60ef819a3443b217f2c0c7108ce1a507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f8ab7915e69f7c548212d74efff16f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-096ebf3e83b7be1947c58dff11275cc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59515d968849f0665ebb498fd7d87148.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SiLVR-A-Simple-Language-based-Video-Reasoning-Framework"><a href="#SiLVR-A-Simple-Language-based-Video-Reasoning-Framework" class="headerlink" title="SiLVR: A Simple Language-based Video Reasoning Framework"></a>SiLVR: A Simple Language-based Video Reasoning Framework</h2><p><strong>Authors:Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius</strong></p>
<p>Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio&#x2F;speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CeeZh/SILVR">https://github.com/CeeZh/SILVR</a>. </p>
<blockquote>
<p>æœ€è¿‘æµ‹è¯•æ—¶é—´ä¼˜åŒ–çš„è¿›å±•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†æ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè§£å†³æ•°å­¦å’Œç¼–ç ä¸­çš„é«˜åº¦å¤æ‚é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä»æœ‰å¾ˆå¤§å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç®€å•è¯­è¨€çš„è§†é¢‘æ¨ç†æ¡†æ¶SiLVRï¼Œå®ƒå°†å¤æ‚çš„è§†é¢‘ç†è§£åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒSiLVRä½¿ç”¨å¤šæ„Ÿå®˜è¾“å…¥ï¼ˆå¦‚çŸ­è§†é¢‘å­—å¹•å’ŒéŸ³é¢‘&#x2F;è¯­éŸ³å­—å¹•ï¼‰å°†åŸå§‹è§†é¢‘è½¬æ¢ä¸ºåŸºäºè¯­è¨€çš„è¡¨ç¤ºå½¢å¼ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†è¯­è¨€æè¿°è¾“å…¥åˆ°å¼ºå¤§çš„æ¨ç†LLMä¸­ï¼Œä»¥è§£å†³å¤æ‚çš„è§†é¢‘è¯­è¨€ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†å¤„ç†é•¿ä¸Šä¸‹æ–‡å¤šæ„Ÿå®˜è¾“å…¥ï¼Œæˆ‘ä»¬é‡‡ç”¨è‡ªé€‚åº”ä»¤ç‰Œç¼©å‡æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŠ¨æ€ç¡®å®šä»¤ç‰Œçš„æ—¶åºç²’åº¦ã€‚æˆ‘ä»¬ç®€å•ã€æ¨¡å—åŒ–ã€æ— éœ€è®­ç»ƒçš„è§†é¢‘æ¨ç†æ¡†æ¶åœ¨Video-MMEï¼ˆé•¿ï¼‰ã€Video-MMMUï¼ˆç†è§£ï¼‰ã€Video-MMLUã€CGBenchå’ŒEgoLifeä¸Šå–å¾—äº†æœ€ä½³æŠ¥å‘Šç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä»¥è§†é¢‘æ¨ç†èƒ½åŠ›ä¸ºç ”ç©¶é‡ç‚¹çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æœªå¯¹è§†é¢‘è¿›è¡Œæ˜ç¡®è®­ç»ƒï¼Œå¼ºå¤§çš„æ¨ç†LLMå¯ä»¥æœ‰æ•ˆåœ°èšåˆæ¥è‡ªè§†é¢‘ã€è¯­éŸ³å’ŒéŸ³é¢‘çš„å¤šæ„Ÿå®˜è¾“å…¥ä¿¡æ¯ï¼Œç”¨äºå¤„ç†å¤æ‚çš„æ—¶åºã€å› æœã€é•¿ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†è·å–è§†é¢‘æ¨ç†ä»»åŠ¡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CeeZh/SILVR">https://github.com/CeeZh/SILVR</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24869v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶çš„ä¼˜åŒ–è¿›æ­¥æ˜¾è‘—ï¼Œèƒ½å¤Ÿè§£å†³æ•°å­¦å’Œç¼–ç ä¸­çš„é«˜åº¦å¤æ‚é—®é¢˜ã€‚ä½†å¯¹äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰æ¥è¯´ï¼Œå…¶åœ¨å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ä»æœ‰æ˜¾è‘—å·®è·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºç®€å•è¯­è¨€è§†é¢‘æ¨ç†æ¡†æ¶SiLVRï¼Œå°†å¤æ‚è§†é¢‘ç†è§£åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤šæ„Ÿå®˜è¾“å…¥å°†åŸå§‹è§†é¢‘è½¬æ¢ä¸ºè¯­è¨€è¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µå°†è¯­è¨€æè¿°è¾“å…¥å¼ºå¤§çš„æ¨ç†LLMæ¥è§£å†³å¤æ‚è§†é¢‘è¯­è¨€ç†è§£ä»»åŠ¡ã€‚é‡‡ç”¨è‡ªé€‚åº”ä»¤ç‰Œç¼©å‡æ–¹æ¡ˆå¤„ç†é•¿ä¸Šä¸‹æ–‡å¤šæ„Ÿå®˜è¾“å…¥ï¼Œå¹¶å®ç°äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œå®è¯ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¼ºå¤§çš„æ¨ç†LLMå¯ä»¥æœ‰æ•ˆåœ°èšåˆæ¥è‡ªè§†é¢‘ã€è¯­éŸ³å’ŒéŸ³é¢‘çš„å¤šæ„Ÿå®˜è¾“å…¥ä¿¡æ¯ï¼Œä»¥å®Œæˆå¤æ‚çš„æ—¶ç©ºã€å› æœã€é•¿æœŸä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†è·å–è§†é¢‘æ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æµ‹è¯•æ—¶çš„ä¼˜åŒ–å¢å¼ºäº†å…¶è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸã€‚</li>
<li>å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰åœ¨è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>SiLVRæ¡†æ¶è¢«æå‡ºä»¥è§£å†³MLLMåœ¨è§†é¢‘ç†è§£ä¸Šçš„æŒ‘æˆ˜ï¼Œå…¶é€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹å°†å¤æ‚è§†é¢‘è½¬åŒ–ä¸ºè¯­è¨€è¡¨ç¤ºã€‚</li>
<li>SiLVRä½¿ç”¨å¤šæ„Ÿå®˜è¾“å…¥å¦‚çŸ­è§†é¢‘å­—å¹•å’ŒéŸ³é¢‘&#x2F;è¯­éŸ³å­—å¹•æ¥ç”Ÿæˆè¯­è¨€è¡¨ç¤ºã€‚</li>
<li>è‡ªé€‚åº”ä»¤ç‰Œç¼©å‡æ–¹æ¡ˆè¢«ç”¨äºå¤„ç†é•¿ä¸Šä¸‹æ–‡å¤šæ„Ÿå®˜è¾“å…¥ã€‚</li>
<li>SiLVRæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cadbe5ccc6164cfa0d21226763504bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56eb0df568b23487df4470ab919bf070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d31ae7f0dad7a59bdbebd3000bef912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb3803b646f36104015bf8631de0b7c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fbbc9524b8a08a326278fb49d58ce58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-208a31d122d4e2d68248ddf651141907.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Harnessing-Negative-Signals-Reinforcement-Distillation-from-Teacher-Data-for-LLM-Reasoning"><a href="#Harnessing-Negative-Signals-Reinforcement-Distillation-from-Teacher-Data-for-LLM-Reasoning" class="headerlink" title="Harnessing Negative Signals: Reinforcement Distillation from Teacher   Data for LLM Reasoning"></a>Harnessing Negative Signals: Reinforcement Distillation from Teacher   Data for LLM Reasoning</h2><p><strong>Authors:Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi</strong></p>
<p>Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAIâ€™s o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples â€“ valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDIâ€™s superiority over baseline Rejection Sampling SFT or SFT combined with DPO&#x2F;SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data. </p>
<blockquote>
<p>è¿‘æœŸæ¨¡å‹è’¸é¦æŠ€æœ¯çš„è¿›å±•è¡¨æ˜ï¼Œæ¥è‡ªé«˜çº§æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek-R1ã€OpenAIçš„o1ï¼‰çš„æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å°†å¤æ‚çš„æ¨ç†èƒ½åŠ›ä¼ é€’ç»™æ›´å°ã€æ›´é«˜æ•ˆçš„å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œæ ‡å‡†å®è·µé‡‡ç”¨æ‹’ç»é‡‡æ ·ï¼Œä¸¢å¼ƒé”™è¯¯çš„æ¨ç†ç¤ºä¾‹â€”â€”è¿™äº›æœ‰ä»·å€¼ä½†å´ç»å¸¸è¢«å¿½è§†çš„æ•°æ®ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿè’¸é¦æ¨ç†ç—•è¿¹ï¼Œä»¥æœ€å¤§åŒ–ç¦»çº¿è®¾ç½®ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰è¿™ä¸€ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»æ­£é¢ç—•è¿¹ä¸­å­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µåˆ™è¿›ä¸€æ­¥ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„REDIç›®æ ‡ï¼Œé€šè¿‡æ­£é¢å’Œè´Ÿé¢ç—•è¿¹å¯¹æ¨¡å‹è¿›è¡Œç²¾ç‚¼ã€‚è¿™ä¸€æ–°é¢–ç›®æ ‡æ˜¯ä¸€ä¸ªç®€å•ã€æ— å‚è€ƒçš„æŸå¤±å‡½æ•°ï¼Œåœ¨è’¸é¦ç¯å¢ƒä¸­ä¼˜äºDPOå’ŒSimPOç­‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒREDIåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºåŸºäºæ‹’ç»é‡‡æ ·çš„SFTæˆ–ç»“åˆDPO&#x2F;SimPOçš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…åœ¨å…¬å¼€å¼€æ”¾çš„Open-R1æ•°æ®é›†ä¸Šè®­ç»ƒäº†13.1ä¸‡ä¸ªæ­£è´Ÿæ ·æœ¬çš„Qwen-REDI-1.5Bæ¨¡å‹ï¼Œåœ¨MATH-500ï¼ˆpass@1ï¼‰ä¸Šçš„å¾—åˆ†è¾¾åˆ°83.1%ã€‚å…¶æ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šäº†DeepSeek-R1-Distill-Qwen-1.5Bï¼ˆä¸€ä¸ªåœ¨80ä¸‡ä¸“æœ‰æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼‰åœ¨å„ç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œä¸ºä½¿ç”¨å…¬å¼€æ•°æ®ç¦»çº¿è®­ç»ƒçš„1.5Bæ¨¡å‹å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24850v1">PDF</a> 27 pages, 10 figures. Code available at   <a target="_blank" rel="noopener" href="https://github.com/Tim-Siu/reinforcement-distillation">https://github.com/Tim-Siu/reinforcement-distillation</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°æ¨¡å‹è’¸é¦æŠ€æœ¯æ˜¾ç¤ºï¼Œé«˜çº§æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ã€OpenAIçš„o1ï¼‰çš„æ•°æ®å¯æœ‰æ•ˆå°†å¤æ‚æ¨ç†èƒ½åŠ›è½¬ç§»è‡³æ›´å°ã€æ›´é«˜æ•ˆçš„å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œæ ‡å‡†å®è·µé‡‡ç”¨æ‹’ç»é‡‡æ ·ï¼Œä¸¢å¼ƒé”™è¯¯çš„æ¨ç†ç¤ºä¾‹ï¼Œè¿™äº›å®è´µä½†å¸¸å¸¸æœªè¢«å……åˆ†åˆ©ç”¨çš„æ•°æ®ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³å…³é”®é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿè’¸é¦æ¨ç†è½¨è¿¹ï¼Œä»¥æœ€å¤§åŒ–ç¦»çº¿è®¾ç½®ä¸­LLMçš„æ¨ç†æ€§èƒ½ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­¦ä¹ æ­£é¢è½¨è¿¹ã€‚ç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„REDIç›®æ ‡ï¼Œè¿›ä¸€æ­¥ç²¾ç‚¼æ¨¡å‹ï¼ŒåŒæ—¶åˆ©ç”¨æ­£é¢å’Œè´Ÿé¢è½¨è¿¹ã€‚æ­¤æ–°é¢–ç›®æ ‡æ˜¯ä¸€ä¸ªç®€å•ã€æ— å‚è€ƒçš„æŸå¤±å‡½æ•°ï¼Œåœ¨æ­¤è’¸é¦ç¯å¢ƒä¸­ä¼˜äºDPOå’ŒSimPOç­‰æ—¢å®šæ–¹æ³•ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒREDIåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºåŸºäºæ‹’ç»é‡‡æ ·çš„SFTæˆ–ç»“åˆDPO&#x2F;SimPOçš„SFTã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…å¯¹Open-R1æ•°æ®é›†çš„131kæ­£è´Ÿæ ·æœ¬è¿›è¡Œåè®­ç»ƒçš„Qwen-REDI-1.5Bæ¨¡å‹ï¼Œåœ¨MATH-500ä¸Šçš„å¾—åˆ†ä¸º83.1%ï¼ˆpass@1ï¼‰ï¼Œå…¶æ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šäº†åœ¨800kä¸“æœ‰æ•°æ®ä¸Šåè®­ç»ƒçš„DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹ï¼Œåœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„ç¦»çº¿åè®­ç»ƒ1.5Bæ¨¡å‹çš„æœ€ä½³çŠ¶æ€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…ˆè¿›æ¨¡å‹çš„æ•°æ®å¯æœ‰æ•ˆè½¬ç§»å¤æ‚æ¨ç†èƒ½åŠ›è‡³å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>æ ‡å‡†å®è·µå¸¸é€šè¿‡æ‹’ç»é‡‡æ ·ä¸¢å¼ƒé”™è¯¯æ¨ç†ç¤ºä¾‹ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¿½ç•¥äº†æœ‰ä»·å€¼çš„æ•°æ®ã€‚</li>
<li>å¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿè’¸é¦æ¨ç†è½¨è¿¹çš„é—®é¢˜ã€‚</li>
<li>REDIçš„ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­¦ä¹ æ­£é¢è½¨è¿¹ï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨æ­£é¢å’Œè´Ÿé¢è½¨è¿¹è¿›è¡Œç²¾ç‚¼ã€‚</li>
<li>REDIæ¡†æ¶é‡‡ç”¨ä¸€ä¸ªæ–°é¢–çš„æ— å‚è€ƒæŸå¤±å‡½æ•°ä½œä¸ºè’¸é¦ç›®æ ‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•å¦‚DPOå’ŒSimPOã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºREDIåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a06cf9f1954536ebfd01ce819e76e6d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfada3cc54a7880cfb5b7c750a401ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd3bc2c4f08724c802fc6180be6681d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f86a506141d1ea850b8e5262e9267fa7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Chameleon-A-Flexible-Data-mixing-Framework-for-Language-Model-Pretraining-and-Finetuning"><a href="#Chameleon-A-Flexible-Data-mixing-Framework-for-Language-Model-Pretraining-and-Finetuning" class="headerlink" title="Chameleon: A Flexible Data-mixing Framework for Language Model   Pretraining and Finetuning"></a>Chameleon: A Flexible Data-mixing Framework for Language Model   Pretraining and Finetuning</h2><p><strong>Authors:Wanyun Xie, Francesco Tonin, Volkan Cevher</strong></p>
<p>Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LIONS-EPFL/Chameleon">https://github.com/LIONS-EPFL/Chameleon</a>. </p>
<blockquote>
<p>è®­ç»ƒæ•°æ®æ··åˆç‰©å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½äº§ç”Ÿå¾ˆå¤§å½±å“ã€‚ç°æœ‰çš„é¢†åŸŸé‡æƒæ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„æƒé‡è®¡ç®—ï¼Œå¹¶åœ¨å¼•å…¥æ–°æ•°æ®æ—¶éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªçµæ´»é«˜æ•ˆçš„æ•°æ®æ··åˆæ¡†æ¶â€”â€”å˜è‰²é¾™ï¼ˆChameleonï¼‰ï¼Œå®ƒåˆ©ç”¨æ æ†åˆ†æ•°æ¥è¡¡é‡å­¦ä¹ åµŒå…¥ç©ºé—´å†…çš„é¢†åŸŸé‡è¦æ€§ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨é¢†åŸŸåµŒå…¥ä¹‹ä¸Šæ„å»ºé¢†åŸŸäº²å’ŒçŸ©é˜µã€‚ç”±æ­¤äº§ç”Ÿçš„æ æ†åˆ†æ•°ç¡®å®šäº†ä¸€ç§æ··åˆç‰©ï¼Œè¯¥æ··åˆç‰©ä¼šæé«˜åœ¨åµŒå…¥ç©ºé—´ä¸­å…±äº«å…±åŒè¡¨ç¤ºçš„é¢†åŸŸçš„æƒé‡ã€‚è¿™ç§è¡¨è¿°æ–¹å¼å¯ä»¥é€šè¿‡è®¡ç®—æ–°çš„é¢†åŸŸåµŒå…¥æ¥ç›´æ¥è¿ç§»åˆ°æ–°çš„æ•°æ®ä¸Šã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§å…³é”®åœºæ™¯ä¸­å±•ç¤ºäº†æ”¹è¿›ï¼šï¼ˆiï¼‰æˆ‘ä»¬è®¡ç®—çš„æƒé‡åœ¨é¢„è®­ç»ƒåŸŸä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”è®¡ç®—æˆæœ¬åªæœ‰å…¶ä¸€å°éƒ¨åˆ†ï¼›ï¼ˆiiï¼‰å˜è‰²é¾™èƒ½å¤Ÿé€‚åº”æ•°æ®å˜åŒ–ï¼Œæ— éœ€ä»£ç†é‡æ–°è®­ç»ƒï¼Œåœ¨è½¬ç§»åˆ°æ–°æ•°æ®æ—¶æé«˜äº†å°‘æ ·æœ¬æ¨ç†çš„å‡†ç¡®æ€§ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¾®è°ƒé¢†åŸŸçš„é‡æƒæ•ˆç‡å¾ˆé«˜ï¼Œåœ¨æ‰€æœ‰å¾®è°ƒé¢†åŸŸçš„æµ‹è¯•å›°æƒ‘åº¦ä¸Šå‡ä¼˜äºå‡åŒ€æ··åˆç‰©ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LIONS-EPFL/Chameleon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LIONS-EPFL/Chameleonæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24844v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒæ•°æ®æ··åˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚ç°æœ‰é¢†åŸŸé‡æƒæ–¹æ³•é€šå¸¸ä¾èµ–æ˜‚è´µçš„æƒé‡è®¡ç®—ï¼Œå¹¶åœ¨å¼•å…¥æ–°æ•°æ®æ—¶éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†çµæ´»é«˜æ•ˆçš„æ•°æ®æ··åˆæ¡†æ¶Chameleonï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ æ†å¾—åˆ†åœ¨å­¦ä¹ çš„åµŒå…¥ç©ºé—´ä¸­é‡åŒ–é¢†åŸŸé‡è¦æ€§ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºé¢†åŸŸäº²å’ŒçŸ©é˜µåŸºäºé¢†åŸŸåµŒå…¥ã€‚è¯±å¯¼çš„æ æ†å¾—åˆ†ç¡®å®šäº†ä¸€ç§æ··åˆï¼Œè¿™ç§æ··åˆä¼šæé«˜åœ¨åµŒå…¥ç©ºé—´ä¸­å…±äº«å…±åŒè¡¨ç¤ºçš„é¢†åŸŸçš„æƒé‡ã€‚è¿™ç§è¡¨è¿°å…è®¸é€šè¿‡è®¡ç®—æ–°é¢†åŸŸåµŒå…¥ç›´æ¥è½¬ç§»åˆ°æ–°æ•°æ®ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬è®¡ç®—å‡ºçš„æƒé‡èƒ½åœ¨é¢„è®­ç»ƒé¢†åŸŸä¸Šæé«˜æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ä½¿ç”¨æ›´å°‘çš„è®¡ç®—é‡ï¼›Chameleonèƒ½å¤Ÿé€‚åº”æ•°æ®å˜åŒ–è€Œæ— éœ€ä»£ç†é‡æ–°è®­ç»ƒï¼Œåœ¨è½¬ç§»åˆ°æ–°æ•°æ®æ—¶æå‡äº†å°‘æ ·æœ¬æ¨ç†çš„å‡†ç¡®æ€§ï¼›æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¾®è°ƒä¸­å®ç°äº†æœ‰æ•ˆçš„é¢†åŸŸé‡æƒï¼Œåœ¨æ‰€æœ‰å¾®è°ƒé¢†åŸŸçš„æµ‹è¯•å›°æƒ‘åº¦ä¸Šå‡ä¼˜äºå‡åŒ€æ··åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒæ•°æ®æ··åˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç°æœ‰é¢†åŸŸé‡æƒæ–¹æ³•æˆæœ¬é«˜ä¸”éœ€è¦é¢‘ç¹é‡æ–°è®­ç»ƒã€‚</li>
<li>Chameleonæ¡†æ¶åˆ©ç”¨æ æ†å¾—åˆ†åœ¨åµŒå…¥ç©ºé—´ä¸­é‡åŒ–é¢†åŸŸé‡è¦æ€§ã€‚</li>
<li>Chameleoné€šè¿‡æ„å»ºé¢†åŸŸäº²å’ŒçŸ©é˜µæ¥å¤„ç†æ•°æ®æ··åˆã€‚</li>
<li>Chameleonèƒ½æé«˜å…±äº«å…±åŒè¡¨ç¤ºçš„é¢†åŸŸçš„æƒé‡ã€‚</li>
<li>Chameleonå…è®¸ç›´æ¥è½¬ç§»åˆ°æ–°æ•°æ®ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>åœ¨å®éªŒæµ‹è¯•ä¸­ï¼ŒChameleonåœ¨é¢„è®­ç»ƒã€é€‚åº”æ–°æ•°æ®å’Œå¾®è°ƒé¢†åŸŸä¸Šå‡è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2e28166b52d662ff267cf25163dca4a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d2d5648f664307425391b811aab6a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a497cf8a6fecea08ebb9cee9e45c0cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bb6026b8397d37a2fb8d7cdf8af3d0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-577df307be6dba536c7e91363465ddfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f28b213e7960b927ea568d0fd4439d01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-650d0427e32ba606f2419e605780e0e5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Vision-LLMs-Are-Bad-at-Hierarchical-Visual-Understanding-and-LLMs-Are-the-Bottleneck"><a href="#Vision-LLMs-Are-Bad-at-Hierarchical-Visual-Understanding-and-LLMs-Are-the-Bottleneck" class="headerlink" title="Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are   the Bottleneck"></a>Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are   the Bottleneck</h2><p><strong>Authors:Yuwen Tan, Yuan Qing, Boqing Gong</strong></p>
<p>This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMsâ€™ hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMsâ€™ bottleneck effect to some extent because the VQA tasks improve the LLMâ€™s hierarchical consistency more than the vision LLMâ€™s. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge. </p>
<blockquote>
<p>æœ¬æ–‡æ­ç¤ºäº†è®¸å¤šæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¼ºä¹å…³äºæˆ‘ä»¬è§†è§‰ä¸–ç•Œçš„åˆ†å±‚çŸ¥è¯†ï¼Œç”šè‡³ä¸äº†è§£å·²ç»ç¡®ç«‹çš„ç”Ÿç‰©åˆ†ç±»å­¦ã€‚è¿™ä¸€ç¼ºé™·ä½¿å¾—LLMæˆä¸ºé™åˆ¶è§†è§‰LLMåˆ†å±‚è§†è§‰ç†è§£ï¼ˆä¾‹å¦‚ï¼Œèƒ½è¯†åˆ«é³ƒé³é±¼ä½†ä¸è®¤è¯†è„Šæ¤åŠ¨ç‰©ï¼‰çš„ç“¶é¢ˆã€‚æˆ‘ä»¬ä½¿ç”¨ä»å…­ä¸ªåˆ†ç±»æ³•å’Œå››ä¸ªå›¾åƒæ•°æ®é›†ä¸­æ„å»ºçš„çº¦ä¸€ç™¾ä¸‡é“å››é€‰ä¸€çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡å¾—å‡ºè¿™äº›å‘ç°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬çš„VQAä»»åŠ¡å¯¹è§†è§‰LLMè¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå†æ¬¡è¯å®äº†LLMçš„ç“¶é¢ˆæ•ˆåº”ï¼Œå› ä¸ºVQAä»»åŠ¡æé«˜äº†LLMçš„å±‚æ¬¡ä¸€è‡´æ€§ï¼Œä½†è¶…è¿‡äº†è§†è§‰LLMçš„å±‚æ¬¡ã€‚æˆ‘ä»¬çŒœæƒ³ï¼Œåœ¨LLMæ‹¥æœ‰ç›¸åº”çš„åˆ†ç±»å­¦çŸ¥è¯†ä¹‹å‰ï¼Œæ— æ³•ä½¿å…¶å®Œå…¨ç†è§£è§†è§‰æ¦‚å¿µçš„å±‚æ¬¡ç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24840v1">PDF</a> 28 pages, 13 figures</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†ç›®å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¼ºä¹å¯¹æˆ‘ä»¬è§†è§‰ä¸–ç•Œçš„å±‚æ¬¡çŸ¥è¯†ï¼Œç”šè‡³å¯¹å·²ç»ç¡®ç«‹çš„ç”Ÿç‰©åˆ†ç±»å­¦ä¹Ÿä¸äº†è§£ã€‚è¿™ä¸€ç¼ºé™·ä½¿å¾—LLMæˆä¸ºè§†è§‰LLMçš„å±‚æ¬¡åŒ–è§†è§‰ç†è§£ï¼ˆä¾‹å¦‚ï¼Œè¯†åˆ«æµ·é²‚é±¼ä½†ä¸çŸ¥é“å®ƒæ˜¯è„Šæ¤åŠ¨ç‰©ï¼‰çš„ç“¶é¢ˆã€‚é€šè¿‡å¯¹å…­ç§åˆ†ç±»æ³•å’Œå››ç§å›¾åƒæ•°æ®é›†æ„å»ºçš„çº¦ä¸€ç™¾ä¸‡ä¸ªå››é€‰ä¸€çš„è§†è§‰é—®ç­”ä»»åŠ¡çš„ç ”ç©¶å¾—å‡ºäº†è¿™äº›å‘ç°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬çš„VQAä»»åŠ¡å¯¹è§†è§‰LLMè¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå†æ¬¡ç¡®è®¤äº†LLMçš„ç“¶é¢ˆæ•ˆåº”ï¼Œå› ä¸ºVQAä»»åŠ¡æé«˜äº†LLMçš„å±‚æ¬¡ä¸€è‡´æ€§ï¼Œä½†è¶…è¿‡äº†å¯¹è§†è§‰LLMçš„æé«˜ã€‚æˆ‘ä»¬æ¨æµ‹ï¼Œé™¤éLLMæ‹¥æœ‰ç›¸åº”çš„åˆ†ç±»çŸ¥è¯†ï¼Œå¦åˆ™æ— æ³•ä½¿å…¶å®Œå…¨å±‚æ¬¡åŒ–åœ°ç†è§£è§†è§‰æ¦‚å¿µã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…ˆè¿›çš„LLMç¼ºä¹è§†è§‰ä¸–ç•Œçš„å±‚æ¬¡çŸ¥è¯†ï¼Œå¯¹ç”Ÿç‰©åˆ†ç±»å­¦ä¹Ÿä¸äº†è§£ã€‚</li>
<li>LLMæˆä¸ºè§†è§‰LLMåœ¨å±‚æ¬¡åŒ–è§†è§‰ç†è§£æ–¹é¢çš„ç“¶é¢ˆã€‚</li>
<li>é€šè¿‡çº¦ä¸€ç™¾ä¸‡ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡å‘ç°è¿™ä¸€ç¼ºé™·ï¼Œè¿™äº›ä»»åŠ¡æ˜¯åŸºäºå…­ç§åˆ†ç±»æ³•å’Œå››ä¸ªå›¾åƒæ•°æ®é›†æ„å»ºçš„ã€‚</li>
<li>ä½¿ç”¨VQAä»»åŠ¡å¾®è°ƒè§†è§‰LLMåœ¨ä¸€å®šç¨‹åº¦ä¸Šå†æ¬¡ç¡®è®¤äº†LLMçš„ç“¶é¢ˆæ•ˆåº”ã€‚</li>
<li>VQAä»»åŠ¡æé«˜äº†LLMçš„å±‚æ¬¡ä¸€è‡´æ€§ï¼Œä½†å¯¹è§†è§‰LLMçš„æå‡æœ‰é™ã€‚</li>
<li>LLMæ— æ³•å®Œå…¨å±‚æ¬¡åŒ–åœ°ç†è§£è§†è§‰æ¦‚å¿µï¼Œé™¤éå®ƒä»¬æ‹¥æœ‰ç›¸åº”çš„åˆ†ç±»çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b4c143eef133e83a9d1626a064282f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4088d1e685245791d36d79d87237d061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d7f33ebd0f297385a8d8f353f3b4c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0777777d758e73ec951692bf5a44dec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf651ff7ba788c9848c6932cfc3b943d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9118d0abb2418986e99cfdb5141fd11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef5c0b05f14635c581a1f417346d0c28.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VideoCAD-A-Large-Scale-Video-Dataset-for-Learning-UI-Interactions-and-3D-Reasoning-from-CAD-Software"><a href="#VideoCAD-A-Large-Scale-Video-Dataset-for-Learning-UI-Interactions-and-3D-Reasoning-from-CAD-Software" class="headerlink" title="VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software"></a>VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software</h2><p><strong>Authors:Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed</strong></p>
<p>Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt at engineering UI interaction learning for precision tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order of magnitude higher complexity in UI interaction learning for real-world engineering tasks, having up to a 20x longer time horizon than other datasets. We show two important downstream applications of VideoCAD: learning UI interactions from professional precision 3D CAD tools and a visual question-answering (VQA) benchmark designed to evaluate multimodal large language modelsâ€™ (LLM) spatial reasoning and video understanding abilities. To learn the UI interactions, we propose VideoCADFormer - a state-of-the-art model in learning CAD interactions directly from video, which outperforms multiple behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”å¤æ‚çš„è¿‡ç¨‹ï¼Œéœ€è¦ç”¨æˆ·ä¸å¤æ‚çš„3Dç•Œé¢è¿›è¡Œç²¾ç¡®ã€é•¿æ—¶é—´çš„äº¤äº’ã€‚å°½ç®¡æœ€è¿‘äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰æ™ºèƒ½ä½“çš„è¿›æ­¥æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•éƒ½ä¸“æ³¨äºç§»åŠ¨æˆ–ç½‘é¡µåº”ç”¨ç¨‹åºä¸­çš„ç®€çŸ­ã€ä½å¤æ‚åº¦ä»»åŠ¡ï¼Œæœªèƒ½æ•æ‰åˆ°ä¸“ä¸šå·¥ç¨‹å·¥å…·çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoCADï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ä¸ºç²¾å¯†ä»»åŠ¡è¿›è¡Œå·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒVideoCADæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡41Kä¸ªæ ‡æ³¨çš„CADæ“ä½œè§†é¢‘è®°å½•ï¼Œè¿™äº›è®°å½•æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶ä»äººä¸ºè®¾è®¡çš„CADä¸­æ”¶é›†é«˜ä¿çœŸUIåŠ¨ä½œæ•°æ®ç”Ÿæˆçš„ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒVideoCADåœ¨ç°å®ä¸–ç•Œå·¥ç¨‹ä»»åŠ¡çš„UIäº¤äº’å­¦ä¹ ä¸­æä¾›äº†æ›´é«˜å¤æ‚åº¦çš„æ•°æ®ï¼Œæ—¶é—´è·¨åº¦æ¯”å…¶ä»–æ•°æ®é›†é•¿è¾¾20å€ã€‚æˆ‘ä»¬å±•ç¤ºäº†VideoCADçš„ä¸¤ä¸ªé‡è¦ä¸‹æ¸¸åº”ç”¨ï¼šä»ä¸“ä¸šç²¾å¯†3D CADå·¥å…·å­¦ä¹ UIäº¤äº’ä»¥åŠè®¾è®¡ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å­¦ä¹ UIäº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†VideoCADFormerâ€”â€”ä¸€ä¸ªç›´æ¥ä»è§†é¢‘å­¦ä¹ CADäº¤äº’çš„æœ€æ–°æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å¤šä¸ªè¡Œä¸ºå…‹éš†åŸºçº¿ã€‚VideoCADFormerå’Œä»VideoCADè¡ç”Ÿçš„VQAåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰è§†é¢‘å‹ç”¨æˆ·ç•Œé¢ç†è§£çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿æœŸä¾èµ–å…³ç³»çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24838v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>CADè®¾è®¡è¿‡ç¨‹è€—æ—¶ä¸”å¤æ‚ï¼Œéœ€è¦ç”¨æˆ·ä¸ç²¾ç»†çš„3Dç•Œé¢è¿›è¡Œç²¾ç¡®çš„é•¿å‘¨æœŸäº¤äº’ã€‚è™½ç„¶AIé©±åŠ¨çš„UIä»£ç†å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦å…³æ³¨ç§»åŠ¨æˆ–Webåº”ç”¨ç¨‹åºä¸­çš„çŸ­ã€ä½å¤æ‚æ€§ä»»åŠ¡ï¼Œæœªèƒ½æ•æ‰åˆ°ä¸“ä¸šå·¥ç¨‹å·¥å…·çš„éœ€æ±‚ã€‚æœ¬ç ”ç©¶æ¨å‡ºVideoCADï¼Œé¦–æ¬¡å°è¯•å·¥ç¨‹UIäº¤äº’å­¦ä¹ ç²¾ç¡®ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒVideoCADæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡4.1ä¸‡ä»½ç»æ³¨é‡Šçš„CADæ“ä½œè§†é¢‘è®°å½•ï¼Œç”±ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ç”Ÿæˆï¼Œç”¨äºä»äººå·¥CADè®¾è®¡ä¸­æ”¶é›†é«˜ä¿çœŸUIåŠ¨ä½œæ•°æ®ã€‚ç›¸è¾ƒäºç°æœ‰æ•°æ®é›†ï¼ŒVideoCADåœ¨ç°å®ä¸–ç•Œå·¥ç¨‹ä»»åŠ¡çš„UIäº¤äº’å­¦ä¹ ä¸­æä¾›äº†æ›´é«˜å¤æ‚åº¦çš„æ•°æ®ï¼Œæ—¶é—´è·¨åº¦æ˜¯å…¶ä»–æ•°æ®é›†çš„20å€ã€‚æˆ‘ä»¬å±•ç¤ºäº†VideoCADçš„ä¸¤ä¸ªé‡è¦ä¸‹æ¸¸åº”ç”¨ï¼šä»ä¸“ä¸šç²¾ç¡®3D CADå·¥å…·å­¦ä¹ UIäº¤äº’å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†å­¦ä¹ UIäº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†VideoCADFormerâ€”â€”ç›´æ¥ä»è§†é¢‘å­¦ä¹ CADäº¤äº’çš„å…ˆè¿›æ¨¡å‹ï¼Œå…¶è¡¨ç°ä¼˜äºå¤šä¸ªè¡Œä¸ºå…‹éš†åŸºçº¿ã€‚VideoCADFormerå’ŒåŸºäºVideoCADçš„VQAåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰è§†é¢‘å‹UIç†è§£çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿å‘¨æœŸä¾èµ–å…³ç³»ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VideoCADæ˜¯é¦–ä¸ªé’ˆå¯¹å·¥ç¨‹UIäº¤äº’å­¦ä¹ çš„æ•°æ®é›†ï¼Œä¸“ä¸ºç²¾ç¡®ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>VideoCADåŒ…å«è¶…è¿‡4.1ä¸‡ä»½ç»æ³¨é‡Šçš„CADæ“ä½œè§†é¢‘ï¼Œå¤æ‚åº¦é«˜äºç°æœ‰æ•°æ®é›†ã€‚</li>
<li>VideoCADæä¾›äº†ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›çš„VQAåŸºå‡†æµ‹è¯•ã€‚</li>
<li>VideoCADFormeræ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç›´æ¥ä»è§†é¢‘ä¸­å­¦ä¹ CADäº¤äº’ã€‚</li>
<li>å½“å‰è§†é¢‘å‹UIç†è§£é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿å‘¨æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>VideoCADæ•°æ®é›†å’ŒVideoCADFormeræ¨¡å‹æœ‰åŠ©äºæ¨åŠ¨CADè®¾è®¡é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3834c21cab465b52f3eeba0b4147c098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4278d9c16201e28a104396153f6ffd89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e21817674620dd382da44c8ab19696ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fa6f07e9de01acde011c0193e14e849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af09834b625542ab273a3d4f54eb9428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea7bcf153e240082e48927ebfd8ca02.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improving-Reliability-and-Explainability-of-Medical-Question-Answering-through-Atomic-Fact-Checking-in-Retrieval-Augmented-LLMs"><a href="#Improving-Reliability-and-Explainability-of-Medical-Question-Answering-through-Atomic-Fact-Checking-in-Retrieval-Augmented-LLMs" class="headerlink" title="Improving Reliability and Explainability of Medical Question Answering   through Atomic Fact Checking in Retrieval-Augmented LLMs"></a>Improving Reliability and Explainability of Medical Question Answering   through Atomic Fact Checking in Retrieval-Augmented LLMs</h2><p><strong>Authors:Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken</strong></p>
<p>Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&amp;A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&amp;A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å±•ç°å‡ºä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†ï¼Œä½†å®¹æ˜“å‡ºç°å¹»è§‰å’Œä¸å‡†ç¡®çš„å¼•ç”¨ï¼Œè¿™å¯¹å®ƒä»¬åœ¨ä¸´åºŠä¸Šçš„åº”ç”¨å’Œåˆè§„æ€§æ„æˆæŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•ï¼Œå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆæ³•ï¼Œé€šè¿‡åŸºäºæºæ–‡æ¡£çš„ç­”æ¡ˆæ¥éƒ¨åˆ†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†å¹»è§‰å’Œè¾ƒä½çš„äº‹å®å±‚é¢è§£é‡Šæ€§ä»ç„¶å­˜åœ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24830v1">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»å­¦é¢†åŸŸå±•ç°å‡ºä¸°å¯Œçš„çŸ¥è¯†ï¼Œä½†å­˜åœ¨æ˜“äº§ç”Ÿå¹»è§‰å’Œå¼•ç”¨ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œè¿™å¯¹å…¶åœ¨ä¸´åºŠåº”ç”¨å’Œæ³•è§„éµå¾ªæ–¹é¢å¸¦æ¥æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯è™½èƒ½éƒ¨åˆ†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†å¹»è§‰å’Œäº‹å®å±‚é¢è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŸå­äº‹å®æ ¸æŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç”¨äºåŒ»å­¦é•¿æ–‡æœ¬é—®ç­”çš„LLMçš„å¯é æ€§å’Œè§£é‡Šæ€§ã€‚è¯¥æ–¹æ³•å°†LLMç”Ÿæˆçš„å›ç­”åˆ†è§£æˆç¦»æ•£çš„å¯éªŒè¯å•ä½ï¼Œå³åŸå­äº‹å®ï¼Œæ¯ä¸ªåŸå­äº‹å®éƒ½ä¼šä¸æƒå¨åŒ»å­¦å‡†åˆ™çŸ¥è¯†åº“è¿›è¡Œç‹¬ç«‹éªŒè¯ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿé’ˆå¯¹æ€§çº æ­£é”™è¯¯å¹¶ç›´æ¥è¿½æº¯è‡³æºæ–‡çŒ®ï¼Œä»è€Œæé«˜åŒ»å­¦é—®ç­”çš„å‡†ç¡®åº¦å’Œè§£é‡Šæ€§ã€‚ç»åŒ»å­¦ä¸“å®¶å¤šè¯»è€…è¯„ä¼°å’Œè‡ªåŠ¨åŒ–å¼€æ”¾é—®ç­”åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨äº‹å®å‡†ç¡®åº¦å’Œè§£é‡Šæ€§æ–¹é¢æœ‰æ˜¾è‘—æé«˜ï¼Œæ•´ä½“å›ç­”æ”¹å–„ç‡æœ€é«˜è¾¾40%ï¼Œå¹»è§‰æ£€æµ‹ç‡é«˜è¾¾50%ã€‚æœ¬æ¡†æ¶å°†æ¯ä¸ªåŸå­äº‹å®è¿½æº¯è‡³æœ€ç›¸å…³çš„æ•°æ®åº“ç‰‡æ®µï¼Œä¸ºç”Ÿæˆå›ç­”æä¾›äº†ç²¾ç»†ã€é€æ˜çš„è§£é‡Šï¼Œè§£å†³äº†å½“å‰åŒ»å­¦äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„ä¸»è¦ç¼ºå£ã€‚æœ¬ç ”ç©¶æ˜¯å‘ä¸´åºŠåº”ç”¨ä¸­æ›´å¯é ã€æ›´å€¼å¾—ä¿¡èµ–çš„LLMåº”ç”¨è¿ˆå‡ºé‡è¦ä¸€æ­¥ï¼Œæ»¡è¶³äº†ä¸´åºŠåº”ç”¨çš„å…³é”®å…ˆå†³æ¡ä»¶ï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©åŒ»ç–—ä¿å¥é¢†åŸŸå¸¦æ¥æ›´å¤§ä¿¡å¿ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°å‡ºä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†ï¼Œä½†å­˜åœ¨å¹»è§‰å’Œå¼•ç”¨ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯è™½èƒ½è§£å†³éƒ¨åˆ†é—®é¢˜ï¼Œä½†å¹»è§‰å’Œäº‹å®è§£é‡Šæ€§ä¸è¶³ä»ç„¶å­˜åœ¨ã€‚</li>
<li>æ–°å‹åŸå­äº‹å®æ ¸æŸ¥æ¡†æ¶æ—¨åœ¨æé«˜LLMåœ¨åŒ»å­¦é•¿æ–‡æœ¬é—®ç­”ä¸­çš„å¯é æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>æ¡†æ¶å°†å›ç­”åˆ†è§£æˆç¦»æ•£çš„å¯éªŒè¯å•ä½ï¼ˆåŸå­äº‹å®ï¼‰ï¼Œå¹¶ä¸æƒå¨åŒ»å­¦å‡†åˆ™çŸ¥è¯†åº“è¿›è¡Œç‹¬ç«‹éªŒè¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æé«˜åŒ»å­¦é—®ç­”çš„å‡†ç¡®åº¦å’Œè§£é‡Šæ€§ï¼Œé€šè¿‡è¿½æº¯è‡³æºæ–‡çŒ®å‡å°‘é”™è¯¯ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œæ¡†æ¶æ•´ä½“å›ç­”æ”¹å–„ç‡æœ€é«˜è¾¾40%ï¼Œå¹»è§‰æ£€æµ‹ç‡é«˜è¾¾50%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a1337d51591e53c75b44a2cff6c03fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dc9509ded34cd0c52487f1358545f12.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LegalEval-Q-A-New-Benchmark-for-The-Quality-Evaluation-of-LLM-Generated-Legal-Text"><a href="#LegalEval-Q-A-New-Benchmark-for-The-Quality-Evaluation-of-LLM-Generated-Legal-Text" class="headerlink" title="LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated   Legal Text"></a>LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated   Legal Text</h2><p><strong>Authors:Li yunhan, Wu gengshen</strong></p>
<p>As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.   Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/lyxx3rd/LegalEval-Q">https://github.com/lyxx3rd/LegalEval-Q</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹åº”ç”¨ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œå½“å‰çš„è¯„ä¼°åŸºå‡†å¾€å¾€ä¸»è¦å…³æ³¨äº‹å®å‡†ç¡®æ€§ï¼Œè€Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†é‡è¦çš„è¯­è¨€è´¨é‡æ–¹é¢ï¼Œå¦‚æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå›å½’æ¨¡å‹ï¼ŒåŸºäºæ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­æ¥è¯„ä¼°æ³•å¾‹æ–‡æœ¬çš„è´¨é‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€å¥—ä¸“é—¨çš„æ³•å¾‹é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¤è¯„ä¼°æ¡†æ¶åˆ†æäº†49ä¸ªLLMã€‚æˆ‘ä»¬çš„åˆ†æå¾—å‡ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œæ¨¡å‹è´¨é‡åœ¨14äº¿å‚æ•°æ—¶è¾¾åˆ°ç¨³å®šæ°´å¹³ï¼Œä»…åœ¨72äº¿å‚æ•°æ—¶è§‚å¯Ÿåˆ°2.7%çš„è¾¹é™…æ”¹è¿›ã€‚å…¶æ¬¡ï¼Œå·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„å½±å“å¾®ä¹å…¶å¾®ï¼Œå¦‚ç»Ÿè®¡æ˜¾è‘—æ€§é˜ˆå€¼é«˜äº0.016æ‰€ç¤ºã€‚ç¬¬ä¸‰ï¼Œæ¨ç†æ¨¡å‹å§‹ç»ˆä¼˜äºåŸºç¡€æ¶æ„ã€‚æˆ‘ä»¬ç ”ç©¶çš„é‡å¤§æˆæœä¹‹ä¸€æ˜¯å‘å¸ƒæ’ååˆ—è¡¨å’Œå¸•ç´¯æ‰˜åˆ†æï¼Œè¿™çªæ˜¾äº†Qwen3ç³»åˆ—åœ¨æˆæœ¬æ€§èƒ½æƒè¡¡æ–¹é¢çš„æœ€ä½³é€‰æ‹©ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºæ³•å¾‹LLMå»ºç«‹äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œè¿˜æ­ç¤ºäº†å½“å‰è®­ç»ƒæ•°æ®æ”¹è¿›æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://github.com/lyxx3rd/LegalEval-Q%E3%80%82]">https://github.com/lyxx3rd/LegalEval-Qã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24826v1">PDF</a> 10 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹åº”ç”¨ä¸­çš„è¯„ä»·ç°çŠ¶ï¼Œå½“å‰è¯„ä»·åŸºå‡†ä¸»è¦é›†ä¸­åœ¨äº‹å®å‡†ç¡®æ€§ä¸Šï¼Œè€Œå¿½è§†äº†è¯­è¨€è´¨é‡çš„å…³é”®è¦ç´ å¦‚æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ç­‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚é¦–å…ˆï¼Œé€šè¿‡å›å½’æ¨¡å‹è¯„ä¼°æ³•å¾‹æ–‡æœ¬è´¨é‡ï¼›å…¶æ¬¡ï¼Œåˆ¶å®šä¸“ä¸šæ³•å¾‹é—®é¢˜é›†ï¼›æœ€åï¼Œå¯¹49ä¸ªLLMè¿›è¡Œåˆ†æè¯„ä»·ã€‚ç ”ç©¶å‘ç°æ¨¡å‹è´¨é‡åœ¨è¾¾åˆ°ä¸€å®šå‚æ•°è§„æ¨¡åè¶‹äºç¨³å®šï¼Œå·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦å¯¹æ€§èƒ½å½±å“ç”šå¾®ï¼Œè€Œæ¨ç†æ¨¡å‹è¡¨ç°è¾ƒå¥½ã€‚æœ¬ç ”ç©¶ä¸ä»…å»ºç«‹äº†æ ‡å‡†åŒ–çš„æ³•å¾‹LLMè¯„ä¼°åè®®ï¼Œè¿˜æ­ç¤ºäº†å½“å‰è®­ç»ƒæ•°æ®ä¼˜åŒ–æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰LLMåœ¨æ³•å¾‹åº”ç”¨ä¸­çš„è¯„ä»·ä¸»è¦å…³æ³¨äº‹å®å‡†ç¡®æ€§ï¼Œå¿½è§†äº†è¯­è¨€è´¨é‡çš„å…³é”®è¦ç´ ã€‚</li>
<li>æå‡ºçš„è¯„ä¼°æ¡†æ¶åŒ…å«å›å½’æ¨¡å‹è¯„ä¼°æ³•å¾‹æ–‡æœ¬è´¨é‡ã€åˆ¶å®šä¸“ä¸šæ³•å¾‹é—®é¢˜é›†ä»¥åŠå¯¹LLMè¿›è¡Œåˆ†æè¯„ä»·ã€‚</li>
<li>æ¨¡å‹è´¨é‡åœ¨è¾¾åˆ°ä¸€å®šå‚æ•°è§„æ¨¡åè¶‹äºç¨³å®šï¼Œå¢åŠ å‚æ•°å¸¦æ¥çš„æ€§èƒ½æå‡æœ‰é™ã€‚</li>
<li>å·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦å¯¹LLMæ€§èƒ½çš„å½±å“ä¸æ˜¾è‘—ã€‚</li>
<li>æ¨ç†æ¨¡å‹åœ¨æ³•å¾‹æ–‡æœ¬å¤„ç†ä¸­è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>ç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–çš„æ³•å¾‹LLMè¯„ä¼°åè®®ã€‚</li>
<li>æ­ç¤ºäº†å½“å‰è®­ç»ƒæ•°æ®ä¼˜åŒ–æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcbb2426d253944b81ef9cd2f47ce01c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5dc8a930a4cb58427f7ae9ac7ae1f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ae0c0ff6711db4b56a37e52e67e2a27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccf65b425f01e3802b56a87bade04035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81649b87e74944ea1c0c51985a43c3ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7253c0e9555e994dd75b7d86ecb21161.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PhySense-Principle-Based-Physics-Reasoning-Benchmarking-for-Large-Language-Models"><a href="#PhySense-Principle-Based-Physics-Reasoning-Benchmarking-for-Large-Language-Models" class="headerlink" title="PhySense: Principle-Based Physics Reasoning Benchmarking for Large   Language Models"></a>PhySense: Principle-Based Physics Reasoning Benchmarking for Large   Language Models</h2><p><strong>Authors:Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo</strong></p>
<p>Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿…é€Ÿè¿›æ­¥ï¼Œè¶Šæ¥è¶Šèƒ½å¤Ÿè§£å†³å¤æ‚çš„ç§‘å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰©ç†å­¦é—®é¢˜ã€‚å°½ç®¡å–å¾—äº†è¿™æ ·çš„è¿›å±•ï¼Œä½†å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸æ— æ³•æ¨¡ä»¿äººç±»ä¸“å®¶çš„ç®€æ´ã€åŸºäºåŸåˆ™çš„é€»è¾‘æ¨ç†ï¼Œè€Œæ˜¯ç”Ÿæˆå†—é•¿ä¸”æ™¦æ¶©çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç§å·®å¼‚çªæ˜¾äº†å®ƒä»¬åœ¨åº”ç”¨æ ¸å¿ƒç‰©ç†åŸç†è¿›è¡Œé«˜æ•ˆã€å¯è§£é‡Šçš„é—®é¢˜è§£å†³èƒ½åŠ›ä¸Šçš„é‡è¦å·®è·ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhySenseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåŸåˆ™çš„ç‰©ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œä¸“å®¶ä½¿ç”¨æŒ‡å¯¼åŸåˆ™å¯ä»¥è½»æ¾è§£å†³ï¼Œè€Œå¯¹äºæ²¡æœ‰å…ˆè¿›è¡ŒåŸåˆ™æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´å´å…·æœ‰æ¬ºéª—æ€§éš¾åº¦ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå°–ç«¯çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œæç¤ºç±»å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬æ— æ³•ä¸ç±»ä¼¼ä¸“å®¶çš„æ¨ç†è·¯å¾„ä¿æŒä¸€è‡´ï¼Œè¿™ä¸ºå¼€å‘å…·æœ‰é«˜æ•ˆã€ç¨³å¥å’Œå¯è§£é‡Šçš„åŸåˆ™åŸºç¡€çš„AIç³»ç»Ÿæä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24823v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚çš„ç§‘å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰©ç†é—®é¢˜æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€æ— æ³•æ¨¡ä»¿äººç±»ä¸“å®¶çš„ç®€æ´ã€åŸºäºåŸåˆ™çš„é€»è¾‘æ¨ç†ï¼Œè€Œæ˜¯ç”Ÿæˆå†—é•¿ä¸”æ™¦æ¶©çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhySenseï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºäºåŸåˆ™çš„ç‰©ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ˜“äºä¸“å®¶é€šè¿‡éµå¾ªåŸåˆ™è§£å†³ï¼Œä½†å¯¹ç¼ºä¹åŸåˆ™ä¼˜å…ˆæ¨ç†çš„LLMæ¥è¯´å´å…·æœ‰æ¬ºéª—æ€§éš¾åº¦ã€‚å¯¹å¤šä¸ªå…ˆè¿›çš„LLMå’Œæç¤ºç±»å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬æ— æ³•æŒ‰ç…§ä¸“å®¶å¼çš„æ¨ç†è·¯å¾„è¿›è¡Œå¯¹é½ï¼Œè¿™ä¸ºå¼€å‘å…·æœ‰é«˜æ•ˆã€ç¨³å¥å’Œå¯è§£é‡Šçš„åŸºäºåŸåˆ™çš„ç§‘å­¦æ¨ç†çš„AIç³»ç»Ÿæä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è§£å†³å¤æ‚çš„ç§‘å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰©ç†é—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>LLMsåœ¨æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„ç®€æ´ã€åŸºäºåŸåˆ™çš„é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨å·®è·ã€‚</li>
<li>PhySenseæ˜¯ä¸€ä¸ªæ–°çš„åŸºäºåŸåˆ™çš„ç‰©ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨éµå¾ªæ ¸å¿ƒç‰©ç†åŸåˆ™æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>PhySenseå¯¹äºäººç±»ä¸“å®¶æ¥è¯´æ˜“äºè§£å†³ï¼Œä½†å¯¹LLMsæ¥è¯´å…·æœ‰æ¬ºéª—æ€§éš¾åº¦ã€‚</li>
<li>å¯¹å¤šä¸ªå…ˆè¿›çš„LLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨éµå¾ªä¸“å®¶å¼çš„æ¨ç†è·¯å¾„æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>è¿™ä¸€ç ”ç©¶ä¸ºå¼€å‘å…·æœ‰é«˜æ•ˆã€ç¨³å¥å’Œå¯è§£é‡Šçš„åŸºäºåŸåˆ™çš„ç§‘å­¦æ¨ç†çš„AIç³»ç»Ÿæä¾›äº†è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-953ce6e80ca8b285f0cdf1972d16e70a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14fb9c3a93e4ed46b9a5d345a9d7ea1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5fabb27633a788b9b70414a544084fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-317f3d2cf62e9d3390999e2749dcd768.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Draw-ALL-Your-Imagine-A-Holistic-Benchmark-and-Agent-Framework-for-Complex-Instruction-based-Image-Generation"><a href="#Draw-ALL-Your-Imagine-A-Holistic-Benchmark-and-Agent-Framework-for-Complex-Instruction-based-Image-Generation" class="headerlink" title="Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for   Complex Instruction-based Image Generation"></a>Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for   Complex Instruction-based Image Generation</h2><p><strong>Authors:Yucheng Zhou, Jiahao Yuan, Qianning Wang</strong></p>
<p>Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a modelâ€™s ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at <a target="_blank" rel="noopener" href="https://github.com/yczhou001/LongBench-T2I">https://github.com/yczhou001/LongBench-T2I</a>. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªå¯¹è±¡ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„å¤æ‚æŒ‡ä»¤æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ç°æœ‰çš„è¯„ä¼°T2Iæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨äºæ–‡æœ¬ä¸å›¾åƒçš„ä¸€èˆ¬å¯¹é½ï¼Œæ— æ³•æ•æ‰åˆ°å¤æ‚ã€å¤šè§’åº¦æç¤ºçš„ç»†å¾®è¦æ±‚ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LongBench-T2Iï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåœ¨å¤æ‚æŒ‡ä»¤ä¸‹è¯„ä¼°T2Iæ¨¡å‹è€Œè®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚LongBench-T2IåŒ…å«500ä¸ªç²¾å¿ƒè®¾è®¡æç¤ºï¼Œæ¶µç›–ä¹ä¸ªä¸åŒçš„è§†è§‰è¯„ä¼°ç»´åº¦ï¼Œèƒ½å¤Ÿå¯¹æ¨¡å‹éµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚é™¤äº†åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼ˆPlan2Genï¼‰ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–æ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¿ƒè¿›å¤æ‚æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶æ— ç¼é›†æˆåˆ°ç°æœ‰çš„T2Iæ¨¡å‹ä¸­ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£é‡Šå’Œåˆ†è§£å¤æ‚çš„æç¤ºï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ç”±äºç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚CLIPScoreï¼‰æ— æ³•å……åˆ†æ•æ‰å¤æ‚æŒ‡ä»¤çš„ç»†å¾®å·®åˆ«ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°å·¥å…·åŒ…ï¼Œè¯¥å·¥å…·åŒ…ä½¿ç”¨ä¸€å¥—å¤šç»´åº¦æŒ‡æ ‡è‡ªåŠ¨è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æ•°æ®å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yczhou001/LongBench-T2I%E3%80%82">https://github.com/yczhou001/LongBench-T2Iã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24787v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œå½“é¢å¯¹æ¶‰åŠå¤šä¸ªç‰©ä½“ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„å¤æ‚æŒ‡ä»¤æ—¶ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€è¡¨ç°ä¸ä½³ã€‚å½“å‰è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æ–‡æœ¬ä¸å›¾åƒçš„æ€»ä½“å¯¹é½ï¼Œå¿½ç•¥äº†å¤æ‚ã€å¤šé¢æç¤ºçš„ç»†å¾®è¦æ±‚ã€‚é’ˆå¯¹è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºLongBench-T2IåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è¡¨ç°ã€‚LongBench-T2IåŒ…å«500ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ¶µç›–ä¹ä¸ªä¸åŒçš„è§†è§‰è¯„ä¼°ç»´åº¦ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹éµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼ˆPlan2Genï¼‰ï¼Œå¯åœ¨æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¿ƒè¿›å¤æ‚æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶å¯æ— ç¼é›†æˆç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šå’Œåˆ†è§£å¤æ‚æç¤ºï¼Œæ›´æœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ç”±äºç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚CLIPScoreæ— æ³•å……åˆ†æ•æ‰å¤æ‚æŒ‡ä»¤çš„ç»†å¾®å·®åˆ«ï¼Œæˆ‘ä»¬æ¨å‡ºä¸€ä¸ªè¯„ä¼°å·¥å…·åŒ…ï¼Œä½¿ç”¨å¤šç»´åº¦æŒ‡æ ‡è‡ªåŠ¨è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yczhou001/LongBench-T2I%E3%80%82">https://github.com/yczhou001/LongBench-T2Iã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯è™½ç„¶èƒ½ä»æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªç‰©ä½“ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„å¤æ‚æŒ‡ä»¤æ—¶è¡¨ç°æ¬ ä½³ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æ–‡æœ¬ä¸å›¾åƒçš„æ€»ä½“å¯¹é½ï¼Œå¿½ç•¥äº†å¤æ‚æç¤ºçš„ç»†å¾®è¦æ±‚ã€‚</li>
<li>LongBench-T2IåŸºå‡†æµ‹è¯•æ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è¡¨ç°ï¼ŒåŒ…å«500ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œä¹ä¸ªè§†è§‰è¯„ä¼°ç»´åº¦ã€‚</li>
<li>æ¨å‡ºPlan2Genä»£ç†æ¡†æ¶ï¼Œå¯åœ¨æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹ä¿ƒè¿›å¤æ‚æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>Plan2Genæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šå’Œåˆ†è§£å¤æ‚æç¤ºï¼Œæ›´æœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚CLIPScoreæ— æ³•å……åˆ†æ•æ‰å¤æ‚æŒ‡ä»¤çš„ç»†å¾®å·®åˆ«ï¼Œå› æ­¤éœ€è¦æ–°çš„è¯„ä¼°å·¥å…·åŒ…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c75e07c44904b0bd498b3666580ea9fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74670ff27b4c7f743f2bffd4c7fb4445.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48018f5e1011d120cc61d3071abaed70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1abd90fdf717f929901bcb7517f5df6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-660cdd55f25104c7122cebdb4a0c0154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a7b641d54b682bf51e5de601d994df4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Revisiting-Epistemic-Markers-in-Confidence-Estimation-Can-Markers-Accurately-Reflect-Large-Language-Modelsâ€™-Uncertainty"><a href="#Revisiting-Epistemic-Markers-in-Confidence-Estimation-Can-Markers-Accurately-Reflect-Large-Language-Modelsâ€™-Uncertainty" class="headerlink" title="Revisiting Epistemic Markers in Confidence Estimation: Can Markers   Accurately Reflect Large Language Modelsâ€™ Uncertainty?"></a>Revisiting Epistemic Markers in Confidence Estimation: Can Markers   Accurately Reflect Large Language Modelsâ€™ Uncertainty?</h2><p><strong>Authors:Jiayu Liu, Qing Zong, Weiqi Wang, Yangqiu Song</strong></p>
<p>As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g., â€œfairly confidentâ€) instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/MarCon">https://github.com/HKUST-KnowComp/MarCon</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œå‡†ç¡®è¯„ä¼°å…¶ä¿¡å¿ƒè‡³å…³é‡è¦ã€‚äººç±»é€šå¸¸é€šè¿‡è®¤è¯†æ ‡è®°ï¼ˆä¾‹å¦‚â€œç›¸å½“æœ‰ä¿¡å¿ƒâ€ï¼‰è€Œä¸æ˜¯æ•°å€¼æ¥è¡¨è¾¾ä¿¡å¿ƒã€‚ç„¶è€Œï¼Œç”±äºä¸å„ç§æ ‡è®°ç›¸å…³çš„ä¸ç¡®å®šæ€§çš„é‡åŒ–å›°éš¾ï¼Œå°šä¸æ¸…æ¥šLLMæ˜¯å¦ä¸€è‡´åœ°ä½¿ç”¨è¿™äº›æ ‡è®°æ¥åæ˜ å…¶å†…åœ¨ä¿¡å¿ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ ‡è®°ä¿¡å¿ƒå®šä¹‰ä¸ºæ¨¡å‹ä½¿ç”¨è®¤è¯†æ ‡è®°æ—¶çš„è§‚å¯Ÿå‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¼€æºå’Œä¸“æœ‰LLMåœ¨åˆ†å¸ƒå¼å†…å’Œåˆ†å¸ƒå¼å¤–çš„å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šå…¶ç¨³å®šæ€§çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¿™äº›æ ‡è®°åœ¨åŒä¸€åˆ†å¸ƒå†…å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œä½†å®ƒä»¬åœ¨åˆ†å¸ƒå¼å¤–çš„ä¿¡å¿ƒå´æ˜¯ä¸ä¸€è‡´çš„ã€‚è¿™äº›å‘ç°å¼•å‘äº†äººä»¬å¯¹è®¤è¯†æ ‡è®°åœ¨ä¿¡å¿ƒä¼°è®¡æ–¹é¢çš„å¯é æ€§çš„é‡å¤§æ‹…å¿§ï¼Œå¹¶å¼ºè°ƒéœ€è¦åœ¨æ ‡è®°ä¿¡å¿ƒå’Œå®é™…æ¨¡å‹ä¸ç¡®å®šæ€§ä¹‹é—´å®ç°æ›´å¥½çš„å¯¹é½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/MarCon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HKUST-KnowComp/MarConæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24778v1">PDF</a> ACL2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸåº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œè¯„ä¼°å…¶ç½®ä¿¡åº¦è‡³å…³é‡è¦ã€‚äººç±»é€šè¿‡è®¤è¯†æ€§æ ‡è®°ï¼ˆå¦‚â€œç›¸å½“æœ‰ä¿¡å¿ƒâ€ï¼‰è€Œéæ•°å€¼æ¥è¡¨è¾¾ä¿¡å¿ƒã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šLLMæ˜¯å¦ä¸€è‡´ä½¿ç”¨è¿™äº›æ ‡è®°æ¥åæ˜ å…¶å†…åœ¨ä¿¡å¿ƒï¼Œè¿™å½’å› äºä¸å„ç§æ ‡è®°ç›¸å…³çš„ä¸ç¡®å®šæ€§çš„é‡åŒ–éš¾åº¦ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶é¦–å…ˆå®šä¹‰æ ‡è®°ä¿¡å¿ƒä¸ºæ¨¡å‹ä½¿ç”¨è®¤è¯†æ€§æ ‡è®°æ—¶çš„è§‚å¯Ÿå‡†ç¡®ç‡ã€‚ç ”ç©¶è¯„ä¼°äº†å¼€æºå’Œä¸“æœ‰LLMåœ¨å†…å¤–åˆ†å¸ƒè®¾ç½®å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸­çš„ç¨³å®šæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ ‡è®°åœ¨ç›¸åŒåˆ†å¸ƒå†…é€šç”¨æ€§è‰¯å¥½ï¼Œä½†åœ¨å¤–éƒ¨åˆ†å¸ƒåœºæ™¯ä¸­å…¶ä¿¡å¿ƒä¸ä¸€è‡´ã€‚è¿™å¼•å‘å¯¹è®¤è¯†æ€§æ ‡è®°ç”¨äºç½®ä¿¡åº¦ä¼°è®¡å¯é æ€§çš„æ‹…å¿§ï¼Œå¹¶å¼ºè°ƒéœ€è¦æ”¹è¿›æ ‡è®°ä¿¡å¿ƒå’Œå®é™…æ¨¡å‹ä¸ç¡®å®šæ€§ä¹‹é—´çš„å¯¹é½ã€‚ç›¸å…³ç ”ç©¶ä»£ç å·²ä¸Šä¼ è‡³ï¼š<a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/MarCon">https://github.com/HKUST-KnowComp/MarCon</a> ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ä½¿å¾—å‡†ç¡®è¯„ä¼°å…¶ç½®ä¿¡åº¦è‡³å…³é‡è¦ã€‚</li>
<li>äººç±»é€šè¿‡è®¤è¯†æ€§æ ‡è®°è¡¨è¾¾ä¿¡å¿ƒï¼Œä½†LLMæ˜¯å¦ä¸€è‡´ä½¿ç”¨è¿™äº›æ ‡è®°æ¥åæ˜ å…¶å†…åœ¨ä¿¡å¿ƒå°šä¸æ¸…æ¥šã€‚</li>
<li>ç ”ç©¶å®šä¹‰æ ‡è®°ä¿¡å¿ƒä¸ºæ¨¡å‹ä½¿ç”¨è®¤è¯†æ€§æ ‡è®°æ—¶çš„è§‚å¯Ÿå‡†ç¡®ç‡ã€‚</li>
<li>è¯„ä¼°äº†LLMåœ¨å†…å¤–åˆ†å¸ƒè®¾ç½®å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸­çš„ç¨³å®šæ€§ã€‚</li>
<li>æ ‡è®°åœ¨ç›¸åŒåˆ†å¸ƒå†…é€šç”¨æ€§è‰¯å¥½ï¼Œä½†åœ¨å¤–éƒ¨åˆ†å¸ƒåœºæ™¯ä¸­å…¶ä¿¡å¿ƒä¸ä¸€è‡´ã€‚</li>
<li>è®¤è¯†æ€§æ ‡è®°ç”¨äºç½®ä¿¡åº¦ä¼°è®¡çš„å¯é æ€§å¼•å‘æ‹…å¿§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-666e86fa466d6c484182fdb417c552a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e08bb0d84f85c06a98191325f0b05bcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3350acc966b1a788e6c3a9c2ed26a5ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0139ea960fbf12f4a20bb3860421d6c1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AFLoRA-Adaptive-Federated-Fine-Tuning-of-Large-Language-Models-with-Resource-Aware-Low-Rank-Adaption"><a href="#AFLoRA-Adaptive-Federated-Fine-Tuning-of-Large-Language-Models-with-Resource-Aware-Low-Rank-Adaption" class="headerlink" title="AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with   Resource-Aware Low-Rank Adaption"></a>AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with   Resource-Aware Low-Rank Adaption</h2><p><strong>Authors:Yajie Zhou, Xiaoyi Pang, Zhibo Wang</strong></p>
<p>Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global modelâ€™s performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world. </p>
<blockquote>
<p>è”é‚¦å¾®è°ƒï¼ˆFederated Fine-tuningï¼‰ä½œä¸ºä¸€ç§ä½¿ç”¨åˆ†æ•£æ•°æ®é€‚åº”åŸºç¡€æ¨¡å‹ä»¥æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡çš„æ–¹æ³•ï¼Œå±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦åœ¨å…·æœ‰å¼‚è´¨ä¸”å—é™æ•°æ®å’Œç³»ç»Ÿèµ„æºçš„å®¢æˆ·ç«¯ä¸Šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œè¿™ä½¿å¾—å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™ç§ç¯å¢ƒä¸‹ï¼Œå…¨çƒæ¨¡å‹çš„æ€§èƒ½å¾€å¾€å—åˆ°æœ€å¼±å®¢æˆ·çš„é™åˆ¶ï¼Œå¹¶ä¸”ç”±äºæœ¬åœ°æ•°æ®çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰ç‰¹æ€§è€Œè¿›ä¸€æ­¥é™ä½ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰å‚æ•°é«˜æ•ˆæŠ€æœ¯æ¥å‡å°‘é€šä¿¡å’Œè®¡ç®—å¼€é”€ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•åŒæ—¶ç¡®ä¿ä½ç§©æ›´æ–°çš„å‡†ç¡®èšåˆå’Œé™ä½ç³»ç»Ÿæˆæœ¬ï¼Œä»è€Œé˜»ç¢äº†æ•´ä½“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AFLoRAï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºLLMçš„è‡ªé€‚åº”å’Œè½»é‡çº§çš„è”é‚¦å¾®è°ƒæ¡†æ¶ã€‚AFLoRAé€šè¿‡è§£è€¦å…±äº«å’Œå®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°æ¥é™ä½å¼€é”€å¹¶æé«˜èšåˆå‡†ç¡®æ€§ï¼Œé‡‡ç”¨åŸºäºå¯¹è§’çŸ©é˜µçš„ç§©ç¼©å‡æ¥æ›´å¥½åœ°åˆ©ç”¨æœ¬åœ°èµ„æºï¼Œå¹¶é‡‡ç”¨ç§©æ„ŸçŸ¥èšåˆå’Œå…¬å…±æ•°æ®ç»†åŒ–æ¥åŠ å¼ºæ•°æ®å¼‚è´¨æ€§ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAFLoRAåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œä¸ºç°å®ä¸–ç•Œä¸­å¼‚æ„ç¯å¢ƒä¸‹LLMçš„æœ‰æ•ˆé€‚åº”æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24773v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è”é‚¦å¾®è°ƒæ˜¯ä¸€ç§é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨å®¢æˆ·ç«¯è¿›è¡Œå¾®è°ƒæ—¶ï¼Œç”±äºæ•°æ®å’Œç³»ç»Ÿèµ„æºçš„å¼‚æ„æ€§å’Œå±€é™æ€§ï¼Œè®¡ç®—å’Œé€šä¿¡éœ€æ±‚å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AFLoRAï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”çš„è½»é‡çº§è”é‚¦å¾®è°ƒæ¡†æ¶ã€‚å®ƒé€šè¿‡è§£è€¦å…±äº«å’Œå®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°ã€é‡‡ç”¨åŸºäºå¯¹è§’çŸ©é˜µçš„æ’åå‰ªæä»¥åŠå…¬å…±æ•°æ®çš„ç²¾ç»†åŒ–æ’åæ„ŸçŸ¥èšåˆç­‰æŠ€æœ¯æ¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å¾®è°ƒæ˜¯ä¸€ç§é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´è®¡ç®—å’Œé€šä¿¡çš„æŒ‘æˆ˜ã€‚</li>
<li>ç”±äºæ•°æ®çš„å¼‚æ„æ€§å’Œæœ‰é™èµ„æºï¼Œå…¨çƒæ¨¡å‹çš„æ€§èƒ½é€šå¸¸å—åˆ°æœ€å¼±å®¢æˆ·çš„é™åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰å‚æ•°é«˜æ•ˆæŠ€æœ¯ï¼Œä½†éš¾ä»¥åœ¨å‡†ç¡®èšåˆä½ç§©æ›´æ–°å’Œæ§åˆ¶ç³»ç»Ÿæˆæœ¬ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚</li>
<li>AFLoRAæ˜¯ä¸€ä¸ªè‡ªé€‚åº”çš„è½»é‡çº§è”é‚¦å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚å®ƒé€šè¿‡è§£è€¦å…±äº«å’Œå®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°ã€é‡‡ç”¨åŸºäºå¯¹è§’çŸ©é˜µçš„æ’åå‰ªæä»¥åŠå…¬å…±æ•°æ®çš„ç²¾ç»†åŒ–æ’åæ„ŸçŸ¥èšåˆç­‰æŠ€æœ¯æ¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>AFLoRAé€šè¿‡å‡å°‘è®¡ç®—å’Œé€šä¿¡å¼€é”€ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨æœ¬åœ°èµ„æºã€‚</li>
<li>AFLoRAåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¼‚æ„ç¯å¢ƒä¸­æœ‰æ•ˆé€‚åº”LLMæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e7acc2d6c7d95cd81c08851ae0e1e2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c1e4b63bd684eaf6fd154871fcfbe4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53147aeaa102a6c9c4b3b54720ca864f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AutoChemSchematic-AI-A-Closed-Loop-Physics-Aware-Agentic-Framework-for-Auto-Generating-Chemical-Process-and-Instrumentation-Diagrams"><a href="#AutoChemSchematic-AI-A-Closed-Loop-Physics-Aware-Agentic-Framework-for-Auto-Generating-Chemical-Process-and-Instrumentation-Diagrams" class="headerlink" title="AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams"></a>AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams</h2><p><strong>Authors:Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana</strong></p>
<p>Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&amp;D timelines from lab discovery to plant deployment. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥åŠ é€Ÿäº†æ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™çš„å‘ç°ã€‚ç„¶è€Œï¼Œå°†è¿™äº›å‘ç°è½¬åŒ–ä¸ºå·¥ä¸šè§„æ¨¡çš„ç”Ÿäº§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œå› ä¸ºè¿™éœ€è¦å¼€å‘å…¨æ–°çš„åŒ–å­¦åˆ¶é€ å·¥è‰ºã€‚å°½ç®¡å®ƒä»¬åœ¨è§„æ¨¡åŒ–åŒ–å­¦è¿‡ç¨‹ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†ç›®å‰çš„AIæ–¹æ³•æ— æ³•è‡ªåŠ¨ç”Ÿæˆå·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰æˆ–å·¥è‰ºæµç¨‹è¯´æ˜ï¼ˆPIDsï¼‰ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªé—­ç¯ã€äº†è§£ç‰©ç†çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å·¥ä¸šå¯è¡Œæ€§çš„å·¥è‰ºæµç¨‹å›¾å’Œè¯´æ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é’ˆå¯¹åŒ–å­¦å·¥è‰ºé—®ç­”ä»»åŠ¡è®­ç»ƒçš„é¢†åŸŸä¸“ä¸šåŒ–å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸åŸºäºåŸºæœ¬åŸç†çš„ä»¿çœŸï¼Œåˆ©ç”¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŒ…å«1020+åŒ–å­¦ç‰©è´¨çš„å·¥è‰ºæµç¨‹å’Œä»ªå™¨æè¿°å±‚æ¬¡çŸ¥è¯†å›¾è°±ï¼›ï¼ˆ2ï¼‰å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œæ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰åœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒé¢†åŸŸä¸“ä¸šåŒ–SLMï¼›ï¼ˆ3ï¼‰åŸºäºDWSIMçš„æ¨¡æ‹Ÿå™¨è¿›è¡Œé—­ç¯éªŒè¯ä»¥ç¡®ä¿å¯è¡Œæ€§ã€‚ä¸ºäº†æé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†å…ˆè¿›çš„æ¨ç†æ—¶é—´ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬FlashAttentionã€å‰ç»è§£ç ã€å¸¦KVç¼“å­˜é‡åŒ–çš„PagedAttentionä»¥åŠæµ‹è¯•æ—¶é—´æ¨ç†ç¼©æ”¾ï¼Œå¹¶ç‹¬ç«‹åº”ç”¨ç»“æ„ä¿®å‰ªæŠ€æœ¯ï¼ˆå®½åº¦å’Œæ·±åº¦ï¼‰ï¼Œä»¥é‡è¦æ€§å¯å‘å¼ä¸ºæŒ‡å¯¼ï¼Œä»¥æœ€å°çš„ç²¾åº¦æŸå¤±å‡å°‘æ¨¡å‹å¤§å°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†ç»æ¨¡æ‹Ÿå™¨éªŒè¯çš„é«˜ä¿çœŸåº¦å·¥è‰ºæµç¨‹æè¿°ï¼Œåœ¨æ­£ç¡®æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°æœªè§è¿‡çš„åŒ–å­¦ç‰©è´¨ã€‚é€šè¿‡è¿æ¥AIé©±åŠ¨çš„è®¾è®¡ä¸å·¥ä¸šè§„æ¨¡å¯è¡Œæ€§ï¼Œè¿™é¡¹å·¥ä½œæ˜¾è‘—ç¼©çŸ­äº†ä»å®éªŒå®¤å‘ç°åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24584v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç”Ÿæˆå¼AIæŠ€æœ¯åœ¨å‘ç°æ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™æ–¹é¢çš„è¿›å±•è¿…é€Ÿï¼Œä½†å°†è¿™äº›å‘ç°è½¬åŒ–ä¸ºå·¥ä¸šè§„æ¨¡ç”Ÿäº§ä»æ˜¯å…³é”®ç“¶é¢ˆï¼Œéœ€è¦å¼€å‘å…¨æ–°çš„åŒ–å­¦åˆ¶é€ å·¥è‰ºã€‚å½“å‰AIæ–¹æ³•æ— æ³•è‡ªåŠ¨ç”Ÿæˆå·¥ç¨‹çº¦æŸä¸‹çš„å·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰å’Œå·¥è‰ºæµç¨‹æŒ‡ç¤ºå›¾ï¼ˆPIDsï¼‰ï¼Œè€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é—­ç¯ã€ç‰©ç†æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆå…·æœ‰å·¥ä¸šå¯è¡Œæ€§çš„PFDså’ŒPIDsã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸“ä¸šé¢†åŸŸçš„å°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œé‡‡ç”¨ç¬¬ä¸€æ€§åŸåˆ™æ¨¡æ‹Ÿï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°ï¼šä¸€æ˜¯åŒ…å«è¶…è¿‡1020ç§åŒ–å­¦ç‰©è´¨çš„æµç¨‹ä¸ä»ªå™¨æè¿°å±‚æ¬¡çŸ¥è¯†å›¾è°±ï¼›äºŒæ˜¯å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œæ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰å¯¹ç‰¹å®šé¢†åŸŸçš„SLMè¿›è¡Œå¾®è°ƒï¼›ä¸‰æ˜¯åŸºäºDWSIMçš„æ¨¡æ‹Ÿå¾ªç¯éªŒè¯ä»¥ç¡®ä¿å¯è¡Œæ€§ã€‚ä¸ºæé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†FlashAttentionã€å‰ç»è§£ç ã€åˆ†é¡µæ³¨æ„åŠ›ä¸KVç¼“å­˜é‡åŒ–ç­‰é«˜çº§æ¨ç†æ—¶é—´ä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶ç‹¬ç«‹åº”ç”¨ç»“æ„ä¿®å‰ªæŠ€æœ¯ï¼ˆå®½åº¦å’Œæ·±åº¦ï¼‰ä»¥åœ¨ä¿æŒç²¾åº¦æŸå¤±æœ€å°çš„æƒ…å†µä¸‹å‡å°æ¨¡å‹å¤§å°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å·¥è‰ºæµç¨‹æè¿°ä¸æ¨¡æ‹Ÿå™¨éªŒè¯é«˜åº¦ä¸€è‡´ï¼Œåœ¨æ­£ç¡®æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶å¯æ³›åŒ–åˆ°æœªè§è¿‡çš„åŒ–å­¦ç‰©è´¨ã€‚è¯¥å·¥ä½œé€šè¿‡æ¡¥æ¥AIé©±åŠ¨çš„è®¾è®¡ä¸å·¥ä¸šè§„æ¨¡å¯è¡Œæ€§ï¼Œæ˜¾è‘—ç¼©çŸ­äº†ä»å®éªŒå®¤å‘ç°åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIåœ¨åŒ–å­¦é¢†åŸŸå‘ç°æ–°å‹ç‰©è´¨ææ–™æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†å·¥ä¸šåŒ–ç”Ÿäº§é¢ä¸´ç“¶é¢ˆï¼Œéœ€è¦æ–°åŒ–å­¦åˆ¶é€ å·¥è‰ºã€‚</li>
<li>å½“å‰AIæ— æ³•è‡ªåŠ¨ç”Ÿæˆå·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰å’Œå·¥è‰ºæµç¨‹æŒ‡ç¤ºå›¾ï¼ˆPIDsï¼‰ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè¯­è¨€æ¨¡å‹å’Œç‰©ç†æ¨¡æ‹Ÿçš„é—­ç¯æ¡†æ¶æ¥è‡ªåŠ¨ç”Ÿæˆè¿™äº›å›¾ã€‚</li>
<li>æ¡†æ¶åŒ…å«å±‚æ¬¡çŸ¥è¯†å›¾è°±ã€å¤šé˜¶æ®µè®­ç»ƒç®¡é“å’ŒåŸºäºDWSIMçš„æ¨¡æ‹ŸéªŒè¯ç­‰æŠ€æœ¯ã€‚</li>
<li>ä¼˜åŒ–æŠ€æœ¯åŒ…æ‹¬é«˜çº§æ¨ç†æ—¶é—´ä¼˜åŒ–å’Œç»“æ„ä¿®å‰ªæŠ€æœ¯ï¼Œä»¥æé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶ç”Ÿæˆçš„å·¥è‰ºæµç¨‹æè¿°ä¸æ¨¡æ‹Ÿå™¨éªŒè¯é«˜åº¦ä¸€è‡´ï¼Œä¸”ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶å¯æ³›åŒ–åˆ°æœªè§è¿‡çš„åŒ–å­¦ç‰©è´¨ï¼Œæ˜¾è‘—ç¼©çŸ­ä»å®éªŒå®¤åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66311291fd1b9171776026f61304a67c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8124b1329bf708d3356c5d77642bf90d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3187f64448014e03deaf4a610f26608.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Advantageous-Parameter-Expansion-Training-Makes-Better-Large-Language-Models"><a href="#Advantageous-Parameter-Expansion-Training-Makes-Better-Large-Language-Models" class="headerlink" title="Advantageous Parameter Expansion Training Makes Better Large Language   Models"></a>Advantageous Parameter Expansion Training Makes Better Large Language   Models</h2><p><strong>Authors:Naibin Gu, Yilong Chen, Zhenyu Zhang, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang</strong></p>
<p>Although scaling up the number of trainable parameters in both pre-training and fine-tuning can effectively improve the performance of large language models, it also leads to increased computational overhead. When delving into the parameter difference, we find that a subset of parameters, termed advantageous parameters, plays a crucial role in determining model performance. Further analysis reveals that stronger models tend to possess more such parameters. In this paper, we propose Advantageous Parameter EXpansion Training (APEX), a method that progressively expands advantageous parameters into the space of disadvantageous ones, thereby increasing their proportion and enhancing training effectiveness. Further theoretical analysis from the perspective of matrix effective rank explains the performance gains of APEX. Extensive experiments on both instruction tuning and continued pre-training demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning while using only 52% of the trainable parameters. In continued pre-training, APEX achieves the same perplexity level as conventional training with just 33% of the training data, and yields significant improvements on downstream tasks. </p>
<blockquote>
<p>è™½ç„¶é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­å¢åŠ å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å¯ä»¥æœ‰æ•ˆæé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å®ƒä¹Ÿå¯¼è‡´äº†è®¡ç®—å¼€é”€çš„å¢åŠ ã€‚åœ¨æ·±å…¥ç ”ç©¶å‚æ•°å·®å¼‚æ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¸€éƒ¨åˆ†å‚æ•°ï¼Œç§°ä¸ºä¼˜åŠ¿å‚æ•°ï¼Œåœ¨å†³å®šæ¨¡å‹æ€§èƒ½æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ€§èƒ½æ›´å¼ºçš„æ¨¡å‹å¾€å¾€æ‹¥æœ‰æ›´å¤šçš„æ­¤ç±»å‚æ•°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¼˜åŠ¿å‚æ•°æ‰©å±•è®­ç»ƒï¼ˆAPEXï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€æ­¥å°†ä¼˜åŠ¿å‚æ•°æ‰©å±•åˆ°åŠ£åŠ¿å‚æ•°çš„ç©ºé—´ï¼Œä»è€Œå¢åŠ å…¶æ¯”ä¾‹å¹¶æé«˜è®­ç»ƒæ•ˆæœã€‚ä»çŸ©é˜µæœ‰æ•ˆç§©çš„è§’åº¦è¿›ä¸€æ­¥ç†è®ºåˆ†æï¼Œè§£é‡Šäº†APEXçš„æ€§èƒ½æå‡ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒä¸æŒç»­é¢„è®­ç»ƒæ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒæ–¹é¢ï¼ŒAPEXä¼˜äºå…¨å‚æ•°è°ƒæ•´ï¼ŒåŒæ—¶åªä½¿ç”¨52%çš„å¯è®­ç»ƒå‚æ•°ã€‚åœ¨æŒç»­é¢„è®­ç»ƒæ–¹é¢ï¼ŒAPEXä½¿ç”¨ä»…33%çš„è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ä¸ä¼ ç»Ÿè®­ç»ƒç›¸åŒçš„å›°æƒ‘åº¦æ°´å¹³ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24241v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¯ä»¥é€šè¿‡å¢åŠ é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µçš„å¯è®­ç»ƒå‚æ•°æ•°é‡æ¥æé«˜ï¼Œä½†è¿™ä¹Ÿä¼šå¢åŠ è®¡ç®—å¼€é”€ã€‚ç ”ç©¶å‘ç°ï¼Œè¢«ç§°ä¸ºä¼˜åŠ¿å‚æ•°çš„å‚æ•°å­é›†å¯¹æ¨¡å‹æ€§èƒ½èµ·ç€å…³é”®ä½œç”¨ã€‚æ›´å¼ºå£®çš„æ¨¡å‹é€šå¸¸æ‹¥æœ‰æ›´å¤šçš„ä¼˜åŠ¿å‚æ•°ã€‚æœ¬æ–‡æå‡ºäº†ä¼˜åŠ¿å‚æ•°æ‰©å±•è®­ç»ƒï¼ˆAPEXï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä¼˜åŠ¿å‚æ•°é€æ­¥æ‰©å±•åˆ°ä¸åˆ©å‚æ•°çš„èŒƒå›´å†…ï¼Œä»è€Œæé«˜å…¶æ¯”ä¾‹å¹¶å¢å¼ºè®­ç»ƒæ•ˆæœã€‚ä»çŸ©é˜µæœ‰æ•ˆç§©çš„è§’åº¦è¿›ä¸€æ­¥åˆ†æäº†APEXçš„æ€§èƒ½æå‡åŸå› ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒæ–¹é¢ï¼ŒAPEXåœ¨ä»…ä½¿ç”¨52%çš„å¯è®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºå…¨å‚æ•°å¾®è°ƒã€‚åœ¨æŒç»­é¢„è®­ç»ƒæ–¹é¢ï¼ŒAPEXä»…ä½¿ç”¨33%çš„è®­ç»ƒæ•°æ®å°±è¾¾åˆ°äº†ä¸ä¼ ç»Ÿè®­ç»ƒç›¸åŒçš„å›°æƒ‘åº¦æ°´å¹³ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¢åŠ å¯è®­ç»ƒå‚æ•°æ•°é‡å¯ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—å¼€é”€ã€‚</li>
<li>ä¼˜åŠ¿å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½èµ·å…³é”®ä½œç”¨ï¼Œæ›´å¼ºå£®çš„æ¨¡å‹æ‹¥æœ‰æ›´å¤šä¼˜åŠ¿å‚æ•°ã€‚</li>
<li>APEXæ–¹æ³•é€šè¿‡é€æ­¥æ‰©å±•ä¼˜åŠ¿å‚æ•°åˆ°ä¸åˆ©å‚æ•°çš„èŒƒå›´å†…ï¼Œæé«˜è®­ç»ƒæ•ˆæœã€‚</li>
<li>APEXæ–¹æ³•ä»çŸ©é˜µæœ‰æ•ˆç§©çš„è§’åº¦è¿›è¡Œç†è®ºè§£é‡Šï¼Œè¯æ˜äº†å…¶æ€§èƒ½æå‡çš„åŸå› ã€‚</li>
<li>åœ¨æŒ‡ä»¤å¾®è°ƒæ–¹é¢ï¼ŒAPEXåœ¨å‡å°‘å¯è®­ç»ƒå‚æ•°ä½¿ç”¨çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºå…¨å‚æ•°å¾®è°ƒã€‚</li>
<li>åœ¨æŒç»­é¢„è®­ç»ƒæ–¹é¢ï¼ŒAPEXåœ¨å‡å°‘è®­ç»ƒæ•°æ®ä½¿ç”¨çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸ä¼ ç»Ÿè®­ç»ƒç›¸åŒçš„å›°æƒ‘åº¦æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-476a17160ba929649bde0ec6a7dd5218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7e8135e33be7245ccd65b9b0004643.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6faf0d319da34bc4df8b0c38b456b1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36695b7975a550707de2dc0a42172a5d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Light-as-Deception-GPT-driven-Natural-Relighting-Against-Vision-Language-Pre-training-Models"><a href="#Light-as-Deception-GPT-driven-Natural-Relighting-Against-Vision-Language-Pre-training-Models" class="headerlink" title="Light as Deception: GPT-driven Natural Relighting Against   Vision-Language Pre-training Models"></a>Light as Deception: GPT-driven Natural Relighting Against   Vision-Language Pre-training Models</h2><p><strong>Authors:Ying Yang, Jie Zhang, Xiao Lv, Di Lin, Tao Xiang, Qing Guo</strong></p>
<p>While adversarial attacks on vision-and-language pretraining (VLP) models have been explored, generating natural adversarial samples crafted through realistic and semantically meaningful perturbations remains an open challenge. Existing methods, primarily designed for classification tasks, struggle when adapted to VLP models due to their restricted optimization spaces, leading to ineffective attacks or unnatural artifacts. To address this, we propose \textbf{LightD}, a novel framework that generates natural adversarial samples for VLP models via semantically guided relighting. Specifically, LightD leverages ChatGPT to propose context-aware initial lighting parameters and integrates a pretrained relighting model (IC-light) to enable diverse lighting adjustments. LightD expands the optimization space while ensuring perturbations align with scene semantics. Additionally, gradient-based optimization is applied to the reference lighting image to further enhance attack effectiveness while maintaining visual naturalness. The effectiveness and superiority of the proposed LightD have been demonstrated across various VLP models in tasks such as image captioning and visual question answering. </p>
<blockquote>
<p>è™½ç„¶è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»å·²è¢«æ¢ç´¢ï¼Œä½†é€šè¿‡ç°å®å’Œè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ‰°åŠ¨ç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå½“é€‚åº”åˆ°VLPæ¨¡å‹æ—¶ï¼Œç”±äºä¼˜åŒ–ç©ºé—´æœ‰é™ï¼Œå¯¼è‡´æ”»å‡»æ•ˆæœä¸ä½³æˆ–äº§ç”Ÿä¸è‡ªç„¶çš„ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{LightD}ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è¯­ä¹‰å¼•å¯¼ç…§æ˜ç”ŸæˆVLPæ¨¡å‹è‡ªç„¶å¯¹æŠ—æ ·æœ¬çš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒLightDåˆ©ç”¨ChatGPTæå‡ºä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„åˆå§‹ç…§æ˜å‚æ•°ï¼Œå¹¶é›†æˆä¸€ä¸ªé¢„è®­ç»ƒçš„ç…§æ˜æ¨¡å‹ï¼ˆIC-lightï¼‰ä»¥å®ç°å„ç§ç…§æ˜è°ƒæ•´ã€‚LightDæ‰©å¤§äº†ä¼˜åŒ–ç©ºé—´ï¼ŒåŒæ—¶ç¡®ä¿æ‰°åŠ¨ä¸åœºæ™¯è¯­ä¹‰ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œå¯¹å‚è€ƒç…§æ˜å›¾åƒåº”ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜æ”»å‡»æ•ˆæœçš„åŒæ—¶ä¿æŒè§†è§‰è‡ªç„¶æ€§ã€‚LightDçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§åœ¨å„ç§VLPæ¨¡å‹çš„ä»»åŠ¡ï¼ˆå¦‚å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ï¼‰ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24227v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¯¹æŠ—æ ·æœ¬åœ¨è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLPï¼‰ä¸­çš„æ”»å‡»å·²æœ‰æ‰€ç ”ç©¶ï¼Œä½†ç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ä»æ˜¯å¼€æ”¾æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¸ºåˆ†ç±»ä»»åŠ¡è®¾è®¡ï¼Œéš¾ä»¥é€‚åº”VLPæ¨¡å‹ï¼Œä¼˜åŒ–ç©ºé—´å—é™å¯¼è‡´æ”»å‡»æ— æ•ˆæˆ–å‡ºç°ä¸è‡ªç„¶ä¼ªå½±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLightDæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼é‡å…‰ç…§ç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚LightDåˆ©ç”¨ChatGPTæå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆå§‹å…‰ç…§å‚æ•°ï¼Œé›†æˆé¢„è®­ç»ƒé‡å…‰ç…§æ¨¡å‹IC-lightå®ç°å¤šæ ·åŒ–å…‰ç…§è°ƒæ•´ã€‚è¯¥æ¡†æ¶æ‰©å¤§äº†ä¼˜åŒ–ç©ºé—´ï¼Œç¡®ä¿æ‰°åŠ¨ä¸åœºæ™¯è¯­ä¹‰ä¸€è‡´ã€‚æ¢¯åº¦ä¼˜åŒ–å‚è€ƒç…§æ˜å›¾åƒï¼Œæé«˜æ”»å‡»æœ‰æ•ˆæ€§åŒæ—¶ä¿æŒè§†è§‰è‡ªç„¶æ€§ã€‚LightDåœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ ·æœ¬åœ¨è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLPï¼‰ä¸­çš„æ”»å‡»ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥ç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ï¼Œä¼˜åŒ–ç©ºé—´å—é™ã€‚</li>
<li>LightDæ¡†æ¶é€šè¿‡è¯­ä¹‰å¼•å¯¼é‡å…‰ç…§ç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚</li>
<li>LightDåˆ©ç”¨ChatGPTæå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆå§‹å…‰ç…§å‚æ•°ã€‚</li>
<li>é›†æˆé¢„è®­ç»ƒé‡å…‰ç…§æ¨¡å‹IC-lightå®ç°å¤šæ ·åŒ–å…‰ç…§è°ƒæ•´ã€‚</li>
<li>LightDæ‰©å¤§äº†ä¼˜åŒ–ç©ºé—´ï¼Œç¡®ä¿æ‰°åŠ¨ä¸åœºæ™¯è¯­ä¹‰ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f656e04c1fc2bab4524f6719b7370e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-215f3ffcdc5d46c616516f098c75b008.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c034fbbdd804e92c611772b4eb3cbc2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8000aa8d0fda4d44fdf27e8b2f2f0b83.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DSR-Bench-Evaluating-the-Structural-Reasoning-Abilities-of-LLMs-via-Data-Structures"><a href="#DSR-Bench-Evaluating-the-Structural-Reasoning-Abilities-of-LLMs-via-Data-Structures" class="headerlink" title="DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via   Data Structures"></a>DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via   Data Structures</h2><p><strong>Authors:Yu He, Yingxi Li, Colin White, Ellen Vitercik</strong></p>
<p>Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoningâ€“that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMsâ€™ structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²äºæ¶‰åŠæ•°æ®æ“ä½œçš„åŸºç¡€ç°å®ä¸–ç•Œä»»åŠ¡ä¸­ã€‚è¿™äº›ä»»åŠ¡çš„æ ¸å¿ƒè¦æ±‚æ˜¯è¿›è¡Œç»“æ„æ¨ç†çš„èƒ½åŠ›ï¼Œå³ç†è§£å’Œæ¨ç†æ•°æ®å…³ç³»ã€‚ä¾‹å¦‚ï¼Œå®¢æˆ·è¯·æ±‚éœ€è¦æ—¶åºæ’åºï¼Œå¯ä»¥é€šè¿‡é˜Ÿåˆ—ç­‰æ•°æ®ç»“æ„æ¥è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºé«˜çº§ã€åº”ç”¨é©±åŠ¨çš„è¯„ä»·ï¼Œå¹¶æ²¡æœ‰å­¤ç«‹è¿™ç§åŸºæœ¬èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ•°æ®ç»“æ„è¯„ä¼°LLMçš„ç»“æ„æ¨ç†èƒ½åŠ›ï¼Œæä¾›æ•°æ®å…³ç³»çš„å¯è§£é‡Šè¡¨ç¤ºã€‚DSR-BenchåŒ…æ‹¬20ä¸ªæ•°æ®ç»“æ„ã€35ä¸ªæ“ä½œå’Œ4140ä¸ªé—®é¢˜å®ä¾‹ï¼ŒæŒ‰å±‚æ¬¡ç»“æ„ç»„ç»‡ï¼Œç”¨äºç²¾ç»†åˆ†ææ¨ç†å±€é™æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç®¡é“æ˜¯å®Œå…¨è‡ªåŠ¨åŒ–å’Œç¡®å®šçš„ï¼Œæ¶ˆé™¤äº†ä¸»è§‚äººä¸ºæˆ–æ¨¡å‹åŸºç¡€çš„åˆ¤æ–­ã€‚å…¶åˆæˆæ€§è´¨è¿˜ç¡®ä¿äº†å¯æ‰©å±•æ€§ï¼Œå¹¶é™ä½äº†æ•°æ®æ±¡æŸ“é£é™©ã€‚æˆ‘ä»¬å¯¹ä¹ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚åˆ†æè¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨åŸºæœ¬çš„å¤šå±æ€§å’Œå¤šè·³æ¨ç†æ–¹é¢è¡¨ç°æŒ£æ‰ã€‚æ­¤å¤–ï¼Œè™½ç„¶ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨å¤æ‚å’Œæ··åˆç»“æ„ä¸Šä»ç„¶å¾ˆè„†å¼±ï¼Œå…¶ä¸­æœ€ä½³æ¨¡å‹åœ¨æŒ‘æˆ˜å­é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä»…ä¸º47%ã€‚å…³é”®çš„æ˜¯ï¼Œæ¨¡å‹åœ¨å¤šç»´æ•°æ®å’Œè‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°æ–¹é¢çš„è¡¨ç°å¾€å¾€ä¸ä½³ï¼Œè¿™çªæ˜¾äº†ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24069v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…ä»»åŠ¡ä¸­è¢«å¹¿æ³›åº”ç”¨ï¼Œè¦æ±‚å¯¹æ•°æ®è¿›è¡Œç»“æ„æ€§æ¨ç†ï¼Œä½†ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é«˜çº§åº”ç”¨è¯„ä¼°ä¸Šï¼Œç¼ºä¹å¯¹è¯¥æ ¸å¿ƒèƒ½åŠ›çš„ç‹¬ç«‹è¯„ä¼°ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSR-BenchåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè¯„ä¼°LLMå¤„ç†æ•°æ®å…³ç³»çš„èƒ½åŠ›ã€‚è¯¥æµ‹è¯•åŒ…æ‹¬20ä¸ªæ•°æ®ç»“æ„ã€35ä¸ªæ“ä½œå’Œ4,140ä¸ªé—®é¢˜å®ä¾‹ï¼Œå¹¶ç»„ç»‡äº†ç²¾ç»†çš„æ¨ç†é™åˆ¶åˆ†æã€‚æˆ‘ä»¬çš„è¯„ä¼°æµç¨‹å®Œå…¨è‡ªåŠ¨åŒ–ä¸”ç¡®å®šæ€§å¼ºï¼Œæ¶ˆé™¤äº†ä¸»è§‚äººä¸ºæˆ–æ¨¡å‹åŸºç¡€çš„åˆ¤æ–­ã€‚æ­¤å¤–ï¼Œå…¶åˆæˆæ€§è´¨ç¡®ä¿äº†å¯æ‰©å±•æ€§å¹¶é™ä½äº†æ•°æ®æ±¡æŸ“é£é™©ã€‚æˆ‘ä»¬å¯¹ä¹ç§æœ€æ–°LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨åŸºæœ¬å¤šå±æ€§å’Œå¤šæ¨ç†æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè€Œæ¨ç†å¯¼å‘æ¨¡å‹è™½ç„¶åœ¨å¤æ‚å’Œæ··åˆç»“æ„ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†åœ¨æŒ‘æˆ˜å­é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä»…ä¸º47%ã€‚æ¨¡å‹åœ¨å¤„ç†å¤šç»´æ•°æ®å’Œè‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°æ—¶è¡¨ç°ä¸ä½³ï¼Œå‡¸æ˜¾äº†å®é™…éƒ¨ç½²ä¸­çš„å…³é”®å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨ç°å®ä»»åŠ¡ä¸­éœ€è¦å¤„ç†æ•°æ®æ“çºµï¼Œå…¶æ ¸å¿ƒè¦æ±‚åŒ…æ‹¬ç»“æ„æ€§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é«˜çº§åº”ç”¨è¯„ä¼°ï¼Œç¼ºä¹é’ˆå¯¹LLMç»“æ„æ€§æ¨ç†èƒ½åŠ›çš„ç‹¬ç«‹è¯„ä¼°ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†DSR-BenchåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè¯„ä¼°LLMå¤„ç†æ•°æ®å…³ç³»çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šç§æ•°æ®ç»“æ„å’Œæ“ä½œã€‚</li>
<li>DSR-Benchè¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–ä¸”ç¡®å®šæ€§å¼ºï¼Œæ¶ˆé™¤äº†ä¸»è§‚åˆ¤æ–­ï¼Œå¹¶ç¡®ä¿æµ‹è¯•çš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>ä¹ç§æœ€æ–°LLMåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä¸€ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨åŸºæœ¬å¤šå±æ€§å’Œå¤šæ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ¨ç†å¯¼å‘æ¨¡å‹åœ¨å¤æ‚å’Œæ··åˆç»“æ„ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æŒ‘æˆ˜å­é›†ä¸Šçš„å¹³å‡å¾—åˆ†è¾ƒä½ï¼Œæ˜¾ç¤ºå…¶å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6ce9e802d3963f5fbbbdc24bef4b3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10cb66a901c2d0edfece7d67c1381383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d1ca5d38791ee0c0f2114a60f192f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5450150ec019815032c89e6679faca0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diversity-of-Transformer-Layers-One-Aspect-of-Parameter-Scaling-Laws"><a href="#Diversity-of-Transformer-Layers-One-Aspect-of-Parameter-Scaling-Laws" class="headerlink" title="Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws"></a>Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</h2><p><strong>Authors:Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe</strong></p>
<p>Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layerâ€™s output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layersâ€™ outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study. </p>
<blockquote>
<p>Transformeræ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç°å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»å¯¼æ¶æ„ã€‚æœ€è¿‘å…³äºå‚æ•°ç¼©æ”¾å®šå¾‹çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¢åŠ å‚æ•°è§„æ¨¡ï¼Œå¯ä»¥æé«˜å…¶ä»»åŠ¡è§£å†³æ€§èƒ½ã€‚å°½ç®¡é€šè¿‡è§£ææ®‹å·®æµï¼Œæœ€è¿‘çš„æœºæ¢°å¯è§£é‡Šæ€§ç ”ç©¶æ·±åŒ–äº†æˆ‘ä»¬å¯¹Transformerå†…éƒ¨è¡Œä¸ºçš„ç†è§£ï¼Œä½†è¿™äº›å†…éƒ¨æœºåˆ¶ä¸å‚æ•°ç¼©æ”¾å®šå¾‹ä¹‹é—´çš„å…³ç³»ä»ä¸æ¸…æ¥šã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å…³æ³¨å±‚åŠå…¶å¤§å°ï¼Œè¿™äº›ä¸»è¦æ˜¯å†³å®šTransformerå‚æ•°è§„æ¨¡çš„å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹æ®‹å·®æµä¸­çš„å±‚è¿›è¡Œç†è®ºä¸Šçš„åç½®å¤šæ ·æ€§åˆ†è§£ç ”ç©¶ã€‚åˆ†è§£å°†ï¼ˆiï¼‰åç½®ï¼ˆæ¯å±‚è¾“å‡ºä¸åŸºå‡†å€¼çš„è¯¯å·®ï¼‰å’Œï¼ˆiiï¼‰å¤šæ ·æ€§ï¼ˆæŒ‡ç¤ºæ¯å±‚è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼‰åˆ†å¼€ã€‚åœ¨è¯¥ç†è®ºä¸‹åˆ†æTransformeræ˜¾ç¤ºï¼Œå½“å„ä¸ªå±‚åšå‡ºçš„é¢„æµ‹æ¥è¿‘æ­£ç¡®ç­”æ¡ˆå¹¶ä¸”å½¼æ­¤ä¿æŒå¤šæ ·æ€§æ—¶ï¼Œæ€§èƒ½ä¼šæé«˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“å„å±‚è¾“å‡ºè¿œç¦»åŸºå‡†å€¼æ—¶ï¼Œå¤šæ ·æ€§å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥ä¿¡æ¯ç†è®ºå¤šæ ·æ€§å¹¶å±•ç¤ºæˆ‘ä»¬çš„ä¸»è¦å‘ç°ï¼šåªæœ‰å½“è¿™äº›å±‚è¡¨ç°ä¸åŒå³å…·æœ‰å¤šæ ·æ€§æ—¶ï¼Œå¢åŠ å±‚æ•°æ‰ä¼šæé«˜æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†å¢åŠ å±‚æ•°æ‰€å¸¦æ¥æ€§èƒ½å¢ç›Šçš„äºšæ¨¡æ€§ï¼šéšç€é¢å¤–å±‚çš„å¢åŠ ï¼Œè¾¹é™…æ”¹è¿›ä¼šå‡å°‘ï¼Œè¿™ä¸å‚æ•°ç¼©æ”¾å®šå¾‹é¢„æµ‹çš„æ—¥å¿—æ”¶æ•›ç›¸ä¸€è‡´ã€‚åœ¨å¤šä¸ªè¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šå¯¹å„ç§LLMè¿›è¡Œçš„å®éªŒå®è¯è¯å®äº†æœ¬ç ”ç©¶ä¸­å¾—å‡ºçš„ç†è®ºå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ¶æ„ä¸­çš„å±‚æ•°ä¸å…¶å¤§å°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„å½±å“ã€‚æ–‡ç« é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ­ç¤ºäº†å±‚å†…æœºåˆ¶ä¸å‚æ•°ç¼©æ”¾å®šå¾‹ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å„å±‚é¢„æµ‹æ¥è¿‘æ­£ç¡®ç­”æ¡ˆå¹¶ä¿æŒç›¸äº’å¤šæ ·æ€§æ—¶ï¼Œæ€§èƒ½ä¼šæé«˜ã€‚å¢åŠ å±‚æ•°èƒ½æé«˜æ€§èƒ½ï¼Œä½†è¿™äº›å±‚å¿…é¡»è¡¨ç°ä¸åŒï¼Œå³å…·æœ‰å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œéšç€å±‚æ•°çš„å¢åŠ ï¼Œæ€§èƒ½æ”¹è¿›è¡¨ç°å‡ºå­æ¨¡å—æ€§ï¼Œè¾¹é™…æ”¹è¿›éšç€é¢å¤–å±‚çš„å¢åŠ è€Œå‡å°‘ï¼Œè¿™ä¸å‚æ•°ç¼©æ”¾å®šå¾‹é¢„æµ‹çš„æ—¥å¿—æ”¶æ•›ç›¸ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå·²æˆä¸ºLLMçš„ä¸»å¯¼æ¶æ„ã€‚</li>
<li>å‚æ•°è§„æ¨¡æ‰©å¤§èƒ½æé«˜Transformerçš„ä»»åŠ¡è§£å†³æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯¹Transformerçš„æ®‹ç•™æµä¸­çš„å±‚è¿›è¡Œç†è®ºç ”ç©¶ï¼Œå‘ç°æ€§èƒ½å’Œå±‚å†…æœºåˆ¶å’Œè§„æ¨¡æœ‰å…³ã€‚</li>
<li>æ€§èƒ½å’Œå±‚å†…æœºåˆ¶çš„é¢„æµ‹æ¥è¿‘æ­£ç¡®ç­”æ¡ˆä»¥åŠå„å±‚è¾“å‡ºçš„å¤šæ ·æ€§æœ‰å…³ã€‚</li>
<li>å¢åŠ å±‚æ•°èƒ½æé«˜æ€§èƒ½ï¼Œä½†è¿™äº›å±‚å¿…é¡»è¡¨ç°ä¸åŒï¼ˆå³å…·æœ‰å¤šæ ·æ€§ï¼‰ã€‚</li>
<li>éšç€å±‚æ•°çš„å¢åŠ ï¼Œæ€§èƒ½æ”¹è¿›è¡¨ç°å‡ºå­æ¨¡å—æ€§ï¼Œè¾¹é™…æ”¹è¿›é€æ¸å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c1dbd2aa0df7f45a5bbf42bbfd9cf0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bac8c29aef1c60964a5397cd0448351d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba36d72424e22e26073ee4a0fbdda7a7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Controllable-Multi-property-Multi-objective-Molecule-Optimization"><a href="#Large-Language-Models-for-Controllable-Multi-property-Multi-objective-Molecule-Optimization" class="headerlink" title="Large Language Models for Controllable Multi-property Multi-objective   Molecule Optimization"></a>Large Language Models for Controllable Multi-property Multi-objective   Molecule Optimization</h2><p><strong>Authors:Vishal Dey, Xiao Hu, Xia Ning</strong></p>
<p>In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through <a target="_blank" rel="noopener" href="https://github.com/ninglab/GeLLMO-C">https://github.com/ninglab/GeLLMO-C</a>. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œä¸­çš„è¯ç‰©è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œåˆ†å­ä¼˜åŒ–éœ€è¦é€‰æ‹©æ€§æ”¹å–„å¤šä¸ªåˆ†å­å±æ€§ä»¥è¾¾åˆ°è¯ç‰©ç›¸å…³çš„æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–å·²ç»è¾¾åˆ°æ­¤ç±»æ ‡å‡†çš„å±æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®¡ç®—æ–¹æ³•å’ŒæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ— æ³•æ•æ‰åˆ°è¿™ç§å¾®å¦™çš„å±æ€§ç‰¹å®šç›®æ ‡ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†C-MuMOInstructï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºå…·æœ‰æ˜ç¡®å±æ€§ç‰¹å®šç›®æ ‡çš„å¤šå±æ€§ä¼˜åŒ–çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚åˆ©ç”¨C-MuMOInstructï¼Œæˆ‘ä»¬å¼€å‘äº†GeLLMO-Csç³»åˆ—æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œæœ‰é’ˆå¯¹æ€§çš„å±æ€§ç‰¹å®šä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨5ä¸ªå†…éƒ¨å’Œ5ä¸ªå¤–éƒ¨ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGeLLMO-Cså§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒæˆåŠŸç‡æé«˜äº†é«˜è¾¾126%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGeLLMO-Csåœ¨æ–°å‹ä¼˜åŒ–ä»»åŠ¡å’Œæœªè§è¿‡çš„æŒ‡ä»¤ä¸Šå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶å°„å‡»æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ºæ”¯æŒå…·æœ‰å±æ€§ç‰¹å®šç›®æ ‡çš„ç°å®å¤šæ ·ä¼˜åŒ–æä¾›äº†åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€æ­¥ã€‚å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ninglab/GeLLMO-C%E8%AE%BF%E9%97%AEC-MuMOInstruct%E5%92%8C%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/ninglab/GeLLMO-Cè®¿é—®C-MuMOInstructå’Œç›¸å…³ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹çœŸå®è¯ç‰©è®¾è®¡ä¸­çš„åˆ†å­ä¼˜åŒ–é—®é¢˜ï¼Œç°æœ‰è®¡ç®—æ–¹æ³•å’ŒæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ— æ³•æ•æ‰å¤æ‚çš„å±æ€§ç‰¹å®šç›®æ ‡ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†C-MuMOInstructæ•°æ®é›†ï¼Œä¸“æ³¨äºå…·æœ‰æ˜ç¡®å±æ€§ç‰¹å®šç›®æ ‡çš„å¤šå±æ€§ä¼˜åŒ–ã€‚åˆ©ç”¨C-MuMOInstructæ•°æ®é›†ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†GeLLMO-Csç³»åˆ—æŒ‡ä»¤è°ƒæ•´LLMï¼Œèƒ½å¤Ÿè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å±æ€§ç‰¹å®šä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒGeLLMO-Csåœ¨5ä¸ªå†…éƒ¨å’Œ5ä¸ªå¤–éƒ¨ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡é«˜å‡ºåŸºçº¿è¾¾126%ï¼Œå¹¶åœ¨æœªè§è¿‡çš„æŒ‡ä»¤å’Œæ–°å‹ä¼˜åŒ–ä»»åŠ¡ä¸Šå±•ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ºæ”¯æŒçœŸå®ã€å¤šæ ·åŒ–çš„å±æ€§ç‰¹å®šä¼˜åŒ–æä¾›äº†åŸºç¡€LLMçš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®è¯ç‰©è®¾è®¡ä¸­çš„åˆ†å­ä¼˜åŒ–éœ€è¦åŒæ—¶æé«˜å¤šä¸ªåˆ†å­å±æ€§è‡³è¯ç‰©å­¦ç›¸å…³æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–å·²è¾¾æ ‡å±æ€§çš„ç¨³å®šæ€§ã€‚</li>
<li>ç°æœ‰è®¡ç®—æ–¹æ³•å’ŒæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ— æ³•æœ‰æ•ˆå¤„ç†è¿™ç§å¤æ‚çš„å±æ€§ç‰¹å®šä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>C-MuMOInstructæ•°æ®é›†çš„å¼•å…¥ï¼Œä¸“æ³¨äºå…·æœ‰æ˜ç¡®å±æ€§ç‰¹å®šç›®æ ‡çš„å¤šå±æ€§ä¼˜åŒ–ã€‚</li>
<li>GeLLMO-Csç³»åˆ—æŒ‡ä»¤è°ƒæ•´LLMè¢«å¼€å‘å‡ºæ¥ï¼Œèƒ½å¤Ÿæ‰§è¡Œæœ‰é’ˆå¯¹æ€§çš„å±æ€§ç‰¹å®šä¼˜åŒ–ã€‚</li>
<li>GeLLMO-Csåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡ç°æœ‰åŸºçº¿ï¼ŒæˆåŠŸç‡é«˜å‡ºè¾¾126%ã€‚</li>
<li>GeLLMO-Cså±•ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æœªè§è¿‡çš„æŒ‡ä»¤å’Œæ–°å‹ä¼˜åŒ–ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d360f62b0a9b12df08dd10f4f26f8ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4523cea023581bcebd8bcb9f2a9c02f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18dc2e136f2f0cdfcfe5c8ae24dc8414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a91ff2e98d1e0501ce17308f8f38c988.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FLAT-LLM-Fine-grained-Low-rank-Activation-Space-Transformation-for-Large-Language-Model-Compression"><a href="#FLAT-LLM-Fine-grained-Low-rank-Activation-Space-Transformation-for-Large-Language-Model-Compression" class="headerlink" title="FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression"></a>FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression</h2><p><strong>Authors:Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang</strong></p>
<p>Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis (PCA), and employ an importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç„¶è€Œå®ƒä»¬çš„é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æ„æˆäº†æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„ä½ç§©åˆ†è§£æ–¹æ³•ä¸ºå®ç°ç»“æ„å‹ç¼©æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ï¼Œä½†å®ƒä»¬å¾€å¾€å­˜åœ¨ç²¾åº¦ä¸‹é™ã€æ ¡å‡†ç¨‹åºæ˜‚è´µçš„é—®é¢˜ï¼Œå¹¶å¯¼è‡´æ¨¡å‹æ¶æ„æ•ˆç‡ä½ä¸‹ï¼Œé˜»ç¢äº†ç°å®ä¸–ç•Œçš„æ¨ç†é€Ÿåº¦æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FLAT-LLMï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿä¸”å‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•ï¼ŒåŸºäºæ¿€æ´»ç©ºé—´ä¸­çš„ç²¾ç»†ç²’åº¦ä½ç§©å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¤´å‘ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è®¡ç®—çš„æˆªæ–­ç‰¹å¾å‘é‡è½¬æ¢æƒé‡ï¼Œä»è€Œå‡å°‘éšè—ç»´åº¦ï¼Œå¹¶åŸºäºé‡è¦æ€§çš„åº¦é‡è‡ªé€‚åº”åœ°åœ¨è§£ç å™¨ä¹‹é—´åˆ†é…ç­‰çº§ã€‚FLAT-LLMå®ç°äº†é«˜æ•ˆæœ‰æ•ˆçš„æƒé‡å‹ç¼©ï¼Œæ— éœ€æ¢å¤å¾®è°ƒï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆæ ¡å‡†ã€‚åœ¨4ä¸ªæ¨¡å‹å’Œ11ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFLAT-LLMåœ¨æ³›åŒ–å’Œä¸‹æ¸¸æ€§èƒ½ä¸Šè¶…è¿‡äº†ç»“æ„å‰ªæåŸºå‡†çº¿ï¼ŒåŒæ—¶åœ¨åŸºäºåˆ†è§£çš„æ–¹æ³•ä¸Šå®ç°äº†æ¨ç†é€Ÿåº¦çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23966v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMé¢ä¸´è®¡ç®—èµ„æºå’Œå†…å­˜çš„æŒ‘æˆ˜ï¼Œç°æœ‰ç»“æ„å‹ç¼©æ–¹æ³•å­˜åœ¨ç²¾åº¦ä¸‹é™å’Œæ ¡å‡†è¿‡ç¨‹å¤æ‚çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¿€æ´»ç©ºé—´ç²¾ç»†ç²’åº¦ä½ç§©å˜æ¢çš„å¿«é€Ÿå‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•FLAT-LLMã€‚å®ƒé€šè¿‡å¤´ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è®¡ç®—æˆªæ–­ç‰¹å¾å‘é‡ï¼Œå¯¹æƒé‡è¿›è¡Œå˜æ¢ï¼Œå¹¶åŸºäºé‡è¦æ€§åº¦é‡è‡ªé€‚åº”åˆ†é…è§£ç å™¨ä¹‹é—´çš„ç§©ã€‚è¯¥æ–¹æ³•æ— éœ€æ¢å¤å¾®è°ƒå³å¯å®ç°é«˜æ•ˆæœ‰æ•ˆçš„æƒé‡å‹ç¼©ï¼Œå¯åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆæ ¡å‡†ã€‚åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFLAT-LLMåœ¨æ³›åŒ–å’Œä¸‹æ¸¸æ€§èƒ½ä¸Šä¼˜äºç»“æ„å‰ªæåŸºçº¿æ–¹æ³•ï¼Œå¹¶æä¾›æ¯”åŸºäºåˆ†è§£çš„æ–¹æ³•æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´èµ„æºå—é™ç¯å¢ƒä¸­çš„è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç»“æ„å‹ç¼©æ–¹æ³•å­˜åœ¨ç²¾åº¦ä¸‹é™å’Œæ ¡å‡†å¤æ‚çš„é—®é¢˜ã€‚</li>
<li>FLAT-LLMæ˜¯ä¸€ç§åŸºäºæ¿€æ´»ç©ºé—´ç²¾ç»†ç²’åº¦ä½ç§©å˜æ¢çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•ã€‚</li>
<li>FLAT-LLMé€šè¿‡å¤´ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¿›è¡Œæƒé‡å˜æ¢ã€‚</li>
<li>FLAT-LLMé‡‡ç”¨é‡è¦æ€§åº¦é‡è‡ªé€‚åº”åˆ†é…è§£ç å™¨ä¹‹é—´çš„ç§©ã€‚</li>
<li>FLAT-LLMæ— éœ€æ¢å¤å¾®è°ƒå³å¯å®ç°é«˜æ•ˆæœ‰æ•ˆçš„æƒé‡å‹ç¼©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-139034430b5198b4cbf27a0e62d0b7d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a036fd34fc011d593588ac5124f062a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-224bf5f2b0bc2321f5c0e72be0d73a31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-814b389a08a67162d0d7c0fb9bc532dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8b9ef47877f16d808c2b4989bc8450.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-aa72a0d2b3a94241e899ba49bae8f062.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Open CaptchaWorld A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c64add917f44249521790e958396869.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Agent-X Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
