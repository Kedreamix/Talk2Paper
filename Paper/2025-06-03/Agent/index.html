<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Open CaptchaWorld A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-aa72a0d2b3a94241e899ba49bae8f062.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="Open-CaptchaWorld-A-Comprehensive-Web-based-Platform-for-Testing-and-Benchmarking-Multimodal-LLM-Agents"><a href="#Open-CaptchaWorld-A-Comprehensive-Web-based-Platform-for-Testing-and-Benchmarking-Multimodal-LLM-Agents" class="headerlink" title="Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents"></a>Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents</h2><p><strong>Authors:Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen</strong></p>
<p>CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL. </p>
<blockquote>
<p>CAPTCHAä¸€ç›´æ˜¯ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²ç½‘ç»œä»£ç†çš„å…³é”®ç“¶é¢ˆï¼Œç»å¸¸é˜»æ­¢å®ƒä»¬å®Œæˆç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚è™½ç„¶ç°ä»£çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨é™æ€æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†åƒCAPTCHAè¿™æ ·çš„äº¤äº’å¼å¤šæ­¥éª¤æ¨ç†æŒ‘æˆ˜æ–¹é¢çš„èƒ½åŠ›å´å¾ˆå°‘å¾—åˆ°æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Open CaptchaWorldï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è¯„ä¼°ç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨ä»£ç†çš„è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›çš„ç½‘ç»œåŸºå‡†å’Œå¹³å°ï¼Œé€šè¿‡å¤šæ ·åŒ–å’ŒåŠ¨æ€çš„CAPTCHAè°œé¢˜æ¥å®ç°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†20ç§ç°ä»£CAPTCHAç±»å‹ï¼Œæ€»å…±225ä¸ªCAPTCHAï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†æ¥è¿›è¡Œæ³¨é‡Šï¼šCAPTCHAæ¨ç†æ·±åº¦ï¼Œè¿™ä¸ªåº¦é‡æ ‡å‡†é‡åŒ–äº†è§£å†³æ¯ä¸ªè°œé¢˜æ‰€éœ€çš„è®¤çŸ¥å’ŒåŠ¨ä½œæ­¥éª¤æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»å§‹ç»ˆå–å¾—è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œè€Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†å´é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå…¶ä¸­Browser-Use Openai-o3çš„æˆåŠŸç‡æœ€é«˜åªæœ‰40.0%ï¼Œè¿œè¿œä½äºäººç±»93.3%çš„è¡¨ç°ã€‚è¿™çªå‡ºäº†Open CaptchaWorldä½œä¸ºè¯Šæ–­å½“å‰å¤šæ¨¡å¼ä»£ç†é™åˆ¶å’Œå¼•å¯¼å¼€å‘æ›´ç¨³å¥çš„å¤šæ¨¡å¼æ¨ç†ç³»ç»Ÿçš„é‡è¦åŸºå‡†ã€‚ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨è¿™ä¸ªURLä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24878v1">PDF</a> Code at: <a target="_blank" rel="noopener" href="https://github.com/MetaAgentX/OpenCaptchaWorld">https://github.com/MetaAgentX/OpenCaptchaWorld</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºOpen CaptchaWorldçš„è¯„æµ‹ï¼Œç°ä»£å¤šæ¨¡æ€LLMä»£ç†åœ¨è§£å†³CAPTCHAè°œé¢˜æ—¶å­˜åœ¨è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›çš„ç“¶é¢ˆã€‚è¯¥å¹³å°åŒ…å«å¤šç§ç±»å‹çš„CAPTCHAè°œé¢˜ï¼Œå¹¶æå‡ºæ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”CAPTCHAæ¨ç†æ·±åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡äººç±»è¡¨ç°æ¥è¿‘å®Œç¾ï¼Œä½†æœ€å…ˆè¿›çš„LLMä»£ç†æˆåŠŸç‡æœ€é«˜ä»…ä¸º40%ï¼Œè¿œä½äºäººç±»æ°´å¹³ã€‚è¿™ä¸ºè¯Šæ–­å½“å‰å¤šæ¨¡æ€ä»£ç†çš„å±€é™æ€§å¹¶å¼•å¯¼å¼€å‘æ›´ç¨³å¥çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAPTCHAså·²æˆä¸ºéƒ¨ç½²Webä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œé˜»ç¢äº†ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ä»»åŠ¡çš„å®Œæˆã€‚</li>
<li>ç°ä»£å¤šæ¨¡æ€LLMä»£ç†åœ¨é™æ€æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†äº¤äº’å¼å¤šæ­¥éª¤æ¨ç†æŒ‘æˆ˜å¦‚CAPTCHAsæ–¹é¢èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ã€‚</li>
<li>Open CaptchaWorldæ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€LLMä»£ç†çš„è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›çš„ç½‘é¡µåŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>è¯¥å¹³å°åŒ…å«20ç§ç°ä»£CAPTCHAç±»å‹ï¼Œæ€»è®¡225ä¸ªCAPTCHAï¼Œå¹¶æå‡ºäº†æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”CAPTCHAæ¨ç†æ·±åº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡äººç±»è¡¨ç°æ¥è¿‘å®Œç¾ï¼Œä½†æœ€å…ˆè¿›çš„LLMä»£ç†åœ¨è§£å†³CAPTCHAè°œé¢˜æ–¹é¢çš„è¡¨ç°æ˜¾è‘—æŒ£æ‰ï¼ŒæˆåŠŸç‡æœ€é«˜ä»…ä¸º40%ã€‚</li>
<li>ä¸äººç±»æ€§èƒ½ç›¸æ¯”ï¼ŒLLMä»£ç†çš„è¡¨ç°è¿œä½äºäººç±»æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-009651660876cbeec6e6e1401afd8206.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4fe6293ba2dfb864fa1e30a62597693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66cb91fb17442e40d24f022304115fb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccdf52c7b86623bc15244d7304c29830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a968a21b03aa19e3527fc3184a28a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Agent-X-Evaluating-Deep-Multimodal-Reasoning-in-Vision-Centric-Agentic-Tasks"><a href="#Agent-X-Evaluating-Deep-Multimodal-Reasoning-in-Vision-Centric-Agentic-Tasks" class="headerlink" title="Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks"></a>Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks</h2><p><strong>Authors:Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan</strong></p>
<p>Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents multi-step and deep reasoning capabilities in real-world, multimodal settings. Agent- X features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models. Our data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/Agent-X">https://github.com/mbzuai-oryx/Agent-X</a> </p>
<blockquote>
<p>æ·±åº¦æ¨ç†æ˜¯è§£å†³å¤æ‚ä»»åŠ¡çš„åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œéœ€è¦è¿ç»­çš„å¤šæ¨¡å¼ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä½¿ç”¨å®Œå…¨åˆæˆã€å•è½®æ¬¡çš„æŸ¥è¯¢è¿›è¡Œè¯„ä¼°ï¼Œè§†è§‰æ¨¡å¼æœ‰é™ï¼Œå¹¶ä¸”ç¼ºä¹ä¸€ä¸ªæ¡†æ¶æ¥è¯„ä¼°ç°å®ä¸–ç•Œç¯å¢ƒä¸­æ‰€éœ€çš„å¤šä¸ªæ­¥éª¤çš„æ¨ç†è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Agent-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“åœ¨å¤šæ­¥éª¤å’Œæ·±åº¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„ç°å®ä¸–ç•Œã€å¤šæ¨¡å¼è®¾ç½®ã€‚Agent-Xæ‹¥æœ‰828é¡¹çœŸå®è§†è§‰ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€å¤šå›¾åƒæ¯”è¾ƒã€è§†é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†å…­å¤§æ™ºèƒ½ç¯å¢ƒï¼šé€šç”¨è§†è§‰æ¨ç†ã€ç½‘é¡µæµè§ˆã€å®‰å…¨ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è¿åŠ¨å’Œæ•°å­¦æ¨ç†ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¦æ±‚æ™ºèƒ½ä½“åœ¨è¿™äº›å¤šæ ·åŒ–çš„ç¯å¢ƒä¸­å°†å·¥å…·ä½¿ç”¨ä¸æ˜ç¡®ã€åˆ†æ­¥éª¤çš„å†³ç­–ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç²¾ç»†çš„ã€æ­¥éª¤çº§çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å’Œé€»è¾‘æ€§ï¼Œä»¥åŠåœ¨æ•´ä¸ªä»»åŠ¡ä¸­å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTã€åŒå­åº§å’ŒQwenç³»åˆ—æ¨¡å‹ï¼Œåœ¨è§£å†³å¤šæ­¥éª¤è§†è§‰ä»»åŠ¡æ–¹é¢ä»é¢ä¸´å›°éš¾ï¼Œå…¨ç¨‹æˆåŠŸç‡ä½äº50%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰è§†è§‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“åœ¨é€»è¾‘æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸Šçš„å…³é”®ç“¶é¢ˆï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“æ¨ç†æ¨¡å‹çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/Agent-X">https://github.com/mbzuai-oryx/Agent-X</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24876v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¸ºè§£å†³è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­éœ€è¦å¤šæ­¥éª¤æ·±åº¦æ¨ç†çš„é—®é¢˜ï¼Œç°æœ‰è¯„ä¼°åŸºå‡†å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºAgent-Xå¤§å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ä»£ç†çš„å¤šæ­¥éª¤æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æµ‹è¯•åŒ…å«çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€åœºæ™¯ï¼Œæ¶µç›–å…­å¤§ä»£ç†ç¯å¢ƒï¼Œè¦æ±‚ä»£ç†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­æ•´åˆå·¥å…·ä½¿ç”¨å’Œæ˜ç¡®åˆ†æ­¥å†³ç­–ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ç²¾ç»†çš„ã€æ­¥éª¤çº§çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°æ¯ä¸€æ­¥æ¨ç†çš„æ­£ç¡®æ€§å’Œé€»è¾‘æ€§ï¼Œä»¥åŠæ•´ä¸ªä»»åŠ¡ä¸­å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œåœ¨å®Œæˆå¤šæ­¥éª¤è§†è§‰ä»»åŠ¡æ—¶çš„æˆåŠŸç‡ä¹Ÿä¸åˆ°50%ï¼Œè¿™çªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡æ¨ç†æ–¹é¢çš„ç“¶é¢ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­çš„å¤šæ­¥éª¤æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Agent-XåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨çœŸå®ä¸–ç•Œå¤šæ¨¡æ€åœºæ™¯ä¸­çš„å¤šæ­¥éª¤æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Agent-XåŒ…å«å…­å¤§ä»£ç†ç¯å¢ƒï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡ç±»å‹ã€‚</li>
<li>è¯¥æµ‹è¯•è¦æ±‚ä»£ç†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­æ•´åˆå·¥å…·ä½¿ç”¨ä¸æ˜ç¡®åˆ†æ­¥å†³ç­–ã€‚</li>
<li>æå‡ºäº†ç²¾ç»†çš„ã€æ­¥éª¤çº§çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°æ¯ä¸€æ­¥æ¨ç†çš„æ­£ç¡®æ€§å’Œé€»è¾‘æ€§ã€‚</li>
<li>å³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼Œåœ¨å®Œæˆå¤šæ­¥éª¤è§†è§‰ä»»åŠ¡æ—¶çš„æˆåŠŸç‡ä¹Ÿä¸åˆ°50%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60d65302a1333ad37a1a7b9cbed55f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aba625aae8519a40f2dd7047e11a53e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2241f78966b3e7e570c118504777c876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c8001b01cf8a933a66954e44fbae171.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1463efbd6ab814f7dd6069e1793e5d00.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Draw-ALL-Your-Imagine-A-Holistic-Benchmark-and-Agent-Framework-for-Complex-Instruction-based-Image-Generation"><a href="#Draw-ALL-Your-Imagine-A-Holistic-Benchmark-and-Agent-Framework-for-Complex-Instruction-based-Image-Generation" class="headerlink" title="Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for   Complex Instruction-based Image Generation"></a>Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for   Complex Instruction-based Image Generation</h2><p><strong>Authors:Yucheng Zhou, Jiahao Yuan, Qianning Wang</strong></p>
<p>Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a modelâ€™s ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at <a target="_blank" rel="noopener" href="https://github.com/yczhou001/LongBench-T2I">https://github.com/yczhou001/LongBench-T2I</a>. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªå¯¹è±¡ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„å¤æ‚æŒ‡ä»¤æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ç°æœ‰çš„è¯„ä¼°T2Iæ¨¡å‹çš„ä¸»è¦åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬ä¸å›¾åƒçš„ä¸€èˆ¬å¯¹é½ä¸Šï¼Œæ— æ³•æ•æ‰å¤æ‚ã€å¤šè§’åº¦æç¤ºçš„ç»†å¾®è¦æ±‚ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LongBench-T2Iï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåœ¨å¤æ‚æŒ‡ä»¤ä¸‹è¯„ä¼°T2Iæ¨¡å‹è€Œè®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚LongBench-T2IåŒ…å«500ä¸ªç²¾å¿ƒè®¾è®¡æç¤ºï¼Œæ¶µç›–ä¹ä¸ªä¸åŒçš„è§†è§‰è¯„ä¼°ç»´åº¦ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ¨¡å‹éµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚é™¤äº†åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼ˆPlan2Genï¼‰ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–æ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¿ƒè¿›å¤æ‚æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶æ— ç¼é›†æˆç°æœ‰çš„T2Iæ¨¡å‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šå’Œåˆ†è§£å¤æ‚æç¤ºï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ç”±äºç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚CLIPScoreï¼‰æ— æ³•å……åˆ†æ•æ‰å¤æ‚æŒ‡ä»¤çš„ç»†å¾®å·®åˆ«ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°å·¥å…·åŒ…ï¼Œè¯¥å·¥å…·åŒ…ä½¿ç”¨ä¸€ç³»åˆ—å¤šç»´æŒ‡æ ‡è‡ªåŠ¨è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æ•°æ®å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yczhou001/LongBench-T2I%E3%80%82">https://github.com/yczhou001/LongBench-T2Iã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24787v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºLongBench-T2Içš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°T2Iæ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«500ä¸ªç²¾å¿ƒè®¾è®¡æç¤ºï¼Œæ¶µç›–ä¹ä¸ªä¸åŒçš„è§†è§‰è¯„ä¼°ç»´åº¦ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä¸€ä¸ªåä¸ºPlan2Gençš„ä»£ç†æ¡†æ¶ï¼Œå¯æ¨åŠ¨å¤æ‚æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚æœ€åï¼Œç”±äºç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¸è¶³ä»¥æ•æ‰å¤æ‚æŒ‡ä»¤çš„ç»†å¾®å·®åˆ«ï¼Œå› æ­¤å¼•å…¥äº†è‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·åŒ…ï¼Œä½¿ç”¨å¤šç»´åº¦æŒ‡æ ‡å¯¹ç”Ÿæˆçš„å›¾åƒè´¨é‡è¿›è¡Œè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”ŸæˆæŠ€æœ¯å–å¾—æœ€æ–°è¿›å±•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æ–‡æœ¬ä¸å›¾åƒçš„ä¸€èˆ¬å¯¹é½ï¼Œéš¾ä»¥è¯„ä¼°æ¨¡å‹å¤„ç†å¤æ‚ã€å¤šæ–¹é¢æç¤ºçš„èƒ½åŠ›ã€‚</li>
<li>LongBench-T2IåŸºå‡†æµ‹è¯•åŒ…å«500ä¸ªå¤æ‚æç¤ºï¼Œæ¶µç›–ä¹ä¸ªè§†è§‰è¯„ä¼°ç»´åº¦ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹éµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>Plan2Genä»£ç†æ¡†æ¶æœ‰åŠ©äºæ ¹æ®å¤æ‚æŒ‡ä»¤è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œæ— éœ€é¢å¤–è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚CLIPScoreä¸è¶³ä»¥æ•æ‰å¤æ‚æŒ‡ä»¤çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>å¼•å…¥è‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·åŒ…ï¼Œä½¿ç”¨å¤šç»´åº¦æŒ‡æ ‡å¯¹ç”Ÿæˆçš„å›¾åƒè´¨é‡è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c75e07c44904b0bd498b3666580ea9fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74670ff27b4c7f743f2bffd4c7fb4445.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48018f5e1011d120cc61d3071abaed70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1abd90fdf717f929901bcb7517f5df6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-660cdd55f25104c7122cebdb4a0c0154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a7b641d54b682bf51e5de601d994df4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AutoChemSchematic-AI-A-Closed-Loop-Physics-Aware-Agentic-Framework-for-Auto-Generating-Chemical-Process-and-Instrumentation-Diagrams"><a href="#AutoChemSchematic-AI-A-Closed-Loop-Physics-Aware-Agentic-Framework-for-Auto-Generating-Chemical-Process-and-Instrumentation-Diagrams" class="headerlink" title="AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams"></a>AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams</h2><p><strong>Authors:Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana</strong></p>
<p>Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&amp;D timelines from lab discovery to plant deployment. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›å±•åŠ é€Ÿäº†æ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™çš„å‘ç°ï¼›ç„¶è€Œï¼Œå°†è¿™äº›å‘ç°è½¬åŒ–ä¸ºå·¥ä¸šè§„æ¨¡ç”Ÿäº§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼Œå› ä¸ºè¿™éœ€è¦å¼€å‘å…¨æ–°çš„åŒ–å­¦åˆ¶é€ å·¥è‰ºã€‚å°½ç®¡å®ƒä»¬åœ¨è§„æ¨¡åŒ–åŒ–å­¦è¿‡ç¨‹ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†ç›®å‰çš„AIæ–¹æ³•æ— æ³•è‡ªåŠ¨ç”ŸæˆPFDsæˆ–PIDsã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é—­ç¯ã€ç‰©ç†æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆå·¥ä¸šå¯è¡Œçš„PFDså’ŒPIDsã€‚è¯¥æ¡†æ¶å°†é¢†åŸŸä¸“ä¸šåŒ–çš„å°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼ˆç”¨äºåŒ–å­¦è¿‡ç¨‹é—®ç­”ä»»åŠ¡ï¼‰ä¸ç¬¬ä¸€æ€§åŸåˆ™æ¨¡æ‹Ÿç›¸ç»“åˆï¼Œåˆ©ç”¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŒ…å«1020+åŒ–å­¦ç‰©è´¨çš„å·¥è‰ºæµç¨‹å’Œä»ªå™¨æè¿°åˆ†å±‚çŸ¥è¯†å›¾è°±ï¼›ï¼ˆ2ï¼‰å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œæ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰åœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒé¢†åŸŸä¸“ä¸šåŒ–çš„SLMï¼›ï¼ˆ3ï¼‰åŸºäºDWSIMçš„æ¨¡æ‹Ÿå™¨è¿›è¡Œé—­ç¯éªŒè¯ï¼Œä»¥ç¡®ä¿å¯è¡Œæ€§ã€‚ä¸ºäº†æé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†å…ˆè¿›çš„æ¨ç†æ—¶é—´ä¼˜åŒ–ï¼ŒåŒ…æ‹¬FlashAttentionã€å‰ç»è§£ç ã€å¸¦KVç¼“å­˜é‡åŒ–çš„PagedAttentionä»¥åŠæµ‹è¯•æ—¶é—´æ¨ç†ç¼©æ”¾ï¼Œå¹¶ç‹¬ç«‹åº”ç”¨ç»“æ„ä¿®å‰ªæŠ€æœ¯ï¼ˆå®½åº¦å’Œæ·±åº¦ï¼‰ï¼Œä»¥é‡è¦æ€§å¯å‘å¼ä¸ºæŒ‡å¯¼ï¼Œä»¥å°½é‡å‡å°‘ç²¾åº¦æŸå¤±æ¥å‡å°æ¨¡å‹è§„æ¨¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†æ¨¡æ‹Ÿå™¨éªŒè¯çš„é«˜ä¿çœŸåº¦è¿‡ç¨‹æè¿°ï¼Œåœ¨æ­£ç¡®æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°æœªè§è¿‡çš„åŒ–å­¦ç‰©è´¨ã€‚é€šè¿‡æ¡¥æ¢AIé©±åŠ¨çš„è®¾è®¡ä¸å·¥ä¸šè§„æ¨¡å¯è¡Œæ€§ä¹‹é—´çš„è”ç³»ï¼Œè¿™é¡¹å·¥ä½œæ˜¾è‘—ç¼©çŸ­äº†ä»å®éªŒå®¤å‘ç°åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24584v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€è¿‘ç”Ÿæˆå¼AIçš„è¿›å±•åŠ é€Ÿäº†æ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™çš„å‘ç°ï¼Œä½†å°†è¿™äº›å‘ç°è½¬åŒ–ä¸ºå·¥ä¸šè§„æ¨¡ç”Ÿäº§ä»æ˜¯å…³é”®ç“¶é¢ˆï¼Œéœ€è¦å¼€å‘å…¨æ–°çš„åŒ–å­¦åˆ¶é€ å·¥è‰ºã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é—­ç¯ã€ç‰©ç†æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆç¬¦åˆå·¥ä¸šè¦æ±‚çš„PFDså’ŒPIDsã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸“ä¸šé¢†åŸŸçš„å°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸åŸºäºç¬¬ä¸€åŸç†çš„ä»¿çœŸï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°è‡ªåŠ¨åŒ–ç”Ÿæˆï¼šåŒ…å«è¶…è¿‡1020ç§åŒ–å­¦å“çš„å·¥è‰ºæµå’Œä»ªå™¨æè¿°å±‚æ¬¡çŸ¥è¯†å›¾è°±ã€å¤šé˜¶æ®µè®­ç»ƒç®¡é“ä»¥åŠDWSIMä»¿çœŸéªŒè¯ã€‚ä¸ºæé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ï¼Œè¯¥æ¡†æ¶è¿˜é‡‡ç”¨äº†é«˜çº§æ¨ç†æ—¶é—´ä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶å¯¹æ¨¡å‹ç»“æ„è¿›è¡Œäº†ä¿®å‰ªï¼Œä»¥å‡å°‘æ¨¡å‹å¤§å°å¹¶ä¿æŒå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å·¥è‰ºæè¿°ä¸ä»¿çœŸéªŒè¯é«˜åº¦ä¸€è‡´ï¼Œåœ¨æ­£ç¡®æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„åŒ–å­¦å“ã€‚è¿™ä¸€å·¥ä½œé€šè¿‡æ¡¥æ¢AIé©±åŠ¨çš„è®¾è®¡ä¸å·¥ä¸šè§„æ¨¡å¯è¡Œæ€§ï¼Œæ˜¾è‘—ç¼©çŸ­äº†ä»å®éªŒå®¤å‘ç°åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIåœ¨åŒ–å­¦å‘ç°å’Œææ–™é¢†åŸŸæœ‰é‡å¤§è¿›å±•ï¼Œä½†å·¥ä¸šè§„æ¨¡ç”Ÿäº§çš„è½¬åŒ–ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦å…¨æ–°çš„åŒ–å­¦åˆ¶é€ å·¥è‰ºæ¥æ»¡è¶³å·¥ä¸šç”Ÿäº§çš„è§„æ¨¡åŒ–éœ€æ±‚ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ç»“åˆäº†SLMå’ŒåŸºäºç¬¬ä¸€åŸç†çš„ä»¿çœŸï¼Œå®ç°è‡ªåŠ¨åŒ–ç”Ÿæˆå·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰å’Œç®¡é“ä»ªè¡¨æµç¨‹å›¾ï¼ˆPIDsï¼‰ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå±‚æ¬¡çŸ¥è¯†å›¾è°±ã€å¤šé˜¶æ®µè®­ç»ƒç®¡é“å’ŒDWSIMä»¿çœŸéªŒè¯ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨äº†å¤šç§æ¨ç†æ—¶é—´ä¼˜åŒ–æŠ€æœ¯å’Œæ¨¡å‹ç»“æ„ä¿®å‰ªï¼Œä»¥æé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å·¥è‰ºæè¿°ä¸ä»¿çœŸéªŒè¯é«˜åº¦ä¸€è‡´ï¼Œå…·æœ‰ä¼˜å¼‚çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66311291fd1b9171776026f61304a67c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8124b1329bf708d3356c5d77642bf90d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3187f64448014e03deaf4a610f26608.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NexusSum-Hierarchical-LLM-Agents-for-Long-Form-Narrative-Summarization"><a href="#NexusSum-Hierarchical-LLM-Agents-for-Long-Form-Narrative-Summarization" class="headerlink" title="NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization"></a>NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization</h2><p><strong>Authors:Hyuntak Kim, Byung-Hak Kim</strong></p>
<p>Summarizing long-form narrativesâ€“such as books, movies, and TV scriptsâ€“requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipelineâ€“without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains. </p>
<blockquote>
<p>æ‘˜è¦é•¿æœŸå™äº‹å†…å®¹ï¼ˆå¦‚ä¹¦ç±ã€ç”µå½±å’Œç”µè§†å‰§æœ¬ï¼‰éœ€è¦æ•æ‰å¤æ‚çš„æƒ…èŠ‚çº¿ã€è§’è‰²äº’åŠ¨å’Œä¸»é¢˜è¿è´¯æ€§ï¼Œè¿™æ˜¯ä¸€é¡¹å¯¹ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†NexusSumï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå™äº‹æ‘˜è¦çš„å¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–ã€é¡ºåºåŒ–çš„ç®¡é“å¤„ç†é•¿ç¯‡æ–‡æœ¬ï¼Œæ— éœ€å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å¯¹è¯åˆ°æè¿°çš„è½¬æ¢ï¼šä¸€ç§é’ˆå¯¹å™äº‹çš„é¢„å¤„ç†æ–¹å¼ï¼Œå°†è§’è‰²å¯¹è¯å’Œæè¿°æ€§æ–‡æœ¬æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œæé«˜è¿è´¯æ€§ã€‚ï¼ˆ2ï¼‰åˆ†å±‚å¤šå¤§å‹è¯­è¨€æ¨¡å‹æ‘˜è¦ï¼šä¸€ç§ç»“æ„åŒ–æ‘˜è¦ç®¡é“ï¼Œä¼˜åŒ–äº†å—å¤„ç†å¹¶æ§åˆ¶è¾“å‡ºé•¿åº¦ï¼Œä»¥äº§ç”Ÿå‡†ç¡®ã€é«˜è´¨é‡çš„æ‘˜è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å™äº‹æ‘˜è¦æ–¹é¢å»ºç«‹äº†æœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œåœ¨ä¹¦ç±ã€ç”µå½±å’Œç”µè§†å‰§æœ¬ä¸Šå®ç°äº†é«˜è¾¾BERTScoreï¼ˆF1ï¼‰çš„30.0%çš„æå‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†å¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ç¯‡å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ ·åŒ–çš„å™äº‹é¢†åŸŸæä¾›äº†å¯æ‰©å±•çš„ç»“æ„åŒ–æ‘˜è¦æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24575v1">PDF</a> Accepted to the main track of ACL 2025</p>
<p><strong>Summary</strong>ï¼š<br>ä»‹ç»äº†ä¸€ç§åä¸ºNexusSumçš„å¤šä»£ç†LLMæ¡†æ¶ï¼Œç”¨äºå™äº‹æ‘˜è¦ï¼Œèƒ½å¤Ÿå¤„ç†é•¿æ–‡æœ¬ï¼Œé€šè¿‡ç»“æ„åŒ–ã€é¡ºåºçš„ç®¡é“è¿›è¡Œï¼Œæ— éœ€å¾®è°ƒã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¯¹è¯åˆ°æè¿°çš„è½¬æ¢å’Œåˆ†å±‚å¤šLLMæ‘˜è¦æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯æé«˜äº†å™äº‹æ‘˜è¦çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶åœ¨ä¹¦ç±ã€ç”µå½±å’Œç”µè§†å‰§æœ¬çš„å™äº‹æ‘˜è¦ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>NexusSumæ˜¯ä¸€ä¸ªå¤šä»£ç†LLMæ¡†æ¶ï¼Œç”¨äºå™äº‹æ‘˜è¦ï¼Œå¤„ç†é•¿æ–‡æœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¯¹è¯åˆ°æè¿°çš„è½¬æ¢å’Œåˆ†å±‚å¤šLLMæ‘˜è¦æŠ€æœ¯ã€‚</li>
<li>å¯¹è¯åˆ°æè¿°çš„è½¬æ¢å°†è§’è‰²å¯¹è¯å’Œæè¿°æ€§æ–‡æœ¬æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œæé«˜è¿è´¯æ€§ã€‚</li>
<li>åˆ†å±‚å¤šLLMæ‘˜è¦æŠ€æœ¯ä¼˜åŒ–äº†å—å¤„ç†å¹¶æ§åˆ¶è¾“å‡ºé•¿åº¦ï¼Œæé«˜äº†æ‘˜è¦çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¹¦ç±ã€ç”µå½±å’Œç”µè§†å‰§æœ¬çš„å™äº‹æ‘˜è¦ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œæé«˜äº†BERTScoreï¼ˆF1ï¼‰è¾¾30.0%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b007773fc6b02d222304b80444acfdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f824fca7af035a0563c6eb0da77a507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f688f1dd7c800b3af3ea67e2f04f722b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f49301801048a3ff94b7e62349f98ca8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CREFT-Sequential-Multi-Agent-LLM-for-Character-Relation-Extraction"><a href="#CREFT-Sequential-Multi-Agent-LLM-for-Character-Relation-Extraction" class="headerlink" title="CREFT: Sequential Multi-Agent LLM for Character Relation Extraction"></a>CREFT: Sequential Multi-Agent LLM for Character Relation Extraction</h2><p><strong>Authors:Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim</strong></p>
<p>Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review â€“ offering substantial benefits to the entertainment, publishing, and educational sectors. </p>
<blockquote>
<p>ç†è§£å¤æ‚çš„è§’è‰²å…³ç³»å¯¹äºå™äº‹åˆ†æå’Œé«˜æ•ˆçš„å‰§æœ¬è¯„ä¼°è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„æå–æ–¹æ³•å¾€å¾€éš¾ä»¥å¤„ç†å…·æœ‰å¾®å¦™äº¤äº’çš„é•¿ç¯‡å™äº‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CREFTï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ä¸“é—¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººçš„æ–°å‹åºåˆ—æ¡†æ¶ã€‚é¦–å…ˆï¼ŒCREFTé€šè¿‡çŸ¥è¯†è’¸é¦æ„å»ºåŸºç¡€è§’è‰²å›¾ï¼Œç„¶åè¿­ä»£åœ°ä¼˜åŒ–è§’è‰²æ„æˆã€å…³ç³»æå–ã€è§’è‰²è¯†åˆ«ä»¥åŠç¾¤ç»„åˆ†é…ã€‚åœ¨ç²¾é€‰çš„éŸ©å‰§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCREFTåœ¨å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå•ä»£ç†LLMåŸºçº¿ã€‚é€šè¿‡ç³»ç»Ÿåœ°å¯è§†åŒ–è§’è‰²ç½‘ç»œï¼ŒCREFTç®€åŒ–äº†å™äº‹ç†è§£ï¼ŒåŠ é€Ÿäº†å‰§æœ¬å®¡æŸ¥ï¼Œä¸ºå¨±ä¹ã€å‡ºç‰ˆå’Œæ•™è‚²è¡Œä¸šæä¾›äº†å®è´¨æ€§ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹è¯æ–‡æœ¬çš„å™äº‹åˆ†æä¸å‰§æœ¬è¯„ä»·è¦æ±‚æ·±å…¥ç†è§£è§’è‰²é—´çš„å…³ç³»ï¼Œç„¶è€Œç°æœ‰æå–æ–¹æ³•åœ¨å¤„ç†é•¿å™äº‹æ–‡æœ¬æ—¶å¾€å¾€éš¾ä»¥æŠŠæ¡å¾®å¦™çš„äº’åŠ¨å…³ç³»ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºCREFTï¼Œä¸€ä¸ªåˆ©ç”¨ä¸“é—¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ„å»ºçš„æ–°åºåˆ—æ¡†æ¶ã€‚CREFTé¦–å…ˆé€šè¿‡çŸ¥è¯†è’¸é¦æ„å»ºåŸºç¡€è§’è‰²å›¾è°±ï¼Œç„¶åè¿­ä»£åœ°ç²¾ç‚¼è§’è‰²æ„æˆã€å…³ç³»æŠ½å–ã€è§’è‰²è¯†åˆ«åŠç¾¤ç»„åˆ†é…ã€‚åœ¨ç²¾å¿ƒæŒ‘é€‰çš„éŸ©å‰§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCREFTåœ¨å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºå•ä¸€ä»£ç†çš„LLMåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡ç³»ç»Ÿå¯è§†åŒ–è§’è‰²ç½‘ç»œï¼ŒCREFTç®€åŒ–äº†å™äº‹ç†è§£å¹¶åŠ é€Ÿäº†å‰§æœ¬å®¡æŸ¥ï¼Œä¸ºå¨±ä¹ã€å‡ºç‰ˆå’Œæ•™è‚²è¡Œä¸šå¸¦æ¥äº†å®è´¨æ€§ç›Šå¤„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç†è§£è§’è‰²é—´å¤æ‚å…³ç³»æ˜¯å™äº‹åˆ†æå’Œå‰§æœ¬è¯„ä¼°çš„å…³é”®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿å½¢å¼å™äº‹æ–‡æœ¬æ—¶ï¼Œå¾€å¾€éš¾ä»¥å¤„ç†å¾®å¦™çš„äº’åŠ¨å…³ç³»ã€‚</li>
<li>CREFTæ˜¯ä¸€ä¸ªæ–°çš„åºåˆ—æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>CREFTé€šè¿‡çŸ¥è¯†è’¸é¦æ„å»ºåŸºç¡€è§’è‰²å›¾è°±ï¼Œå¹¶è¿­ä»£åœ°ä¼˜åŒ–å¤šä¸ªç¯èŠ‚ã€‚</li>
<li>åœ¨éŸ©å‰§æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒCREFTåœ¨å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ä¸Šä¼˜äºLLMåŸºçº¿æ¨¡å‹ã€‚</li>
<li>CREFTé€šè¿‡ç³»ç»Ÿå¯è§†åŒ–è§’è‰²ç½‘ç»œï¼Œç®€åŒ–äº†å™äº‹ç†è§£å¹¶åŠ é€Ÿäº†å‰§æœ¬å®¡æŸ¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24076f2847bc62fc205f02bb477a9976.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06024b9e8f5df9dedb55810069e9ec4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33e0cc5fa26a001b53d5c885aa7c4b3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a1786322648227c51e445d5fead192.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RMoA-Optimizing-Mixture-of-Agents-through-Diversity-Maximization-and-Residual-Compensation"><a href="#RMoA-Optimizing-Mixture-of-Agents-through-Diversity-Maximization-and-Residual-Compensation" class="headerlink" title="RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and   Residual Compensation"></a>RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and   Residual Compensation</h2><p><strong>Authors:Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao</strong></p>
<p>Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNetâ€™s residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mindhunter01/RMoA">https://github.com/mindhunter01/RMoA</a>. </p>
<blockquote>
<p>å°½ç®¡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤šä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°é«˜è®¡ç®—å¼€é”€ã€ä¿¡æ¯ä¸¢å¤±å’Œç¨³å¥æ€§çš„é™åˆ¶ã€‚å—ResNetæ®‹å·®å­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Residual Mixture-of-Agentsï¼ˆRMoAï¼‰ï¼Œé€šè¿‡é›†æˆæ®‹å·®è¿æ¥æ¥ä¼˜åŒ–æ•ˆç‡å’Œå¯é æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–æ¨¡å‹å“åº”çš„ä¿¡æ¯åˆ©ç”¨ï¼ŒåŒæ—¶æœ€å°åŒ–è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§åœ°è®¾è®¡äº†ä¸€ç§åŸºäºåµŒå…¥çš„å¤šæ ·æ€§é€‰æ‹©æœºåˆ¶ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼æ€§è¿›è¡Œè´ªå©ªåœ°é€‰æ‹©å“åº”ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£è¿­ä»£ä¿¡æ¯é€€åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªResidual Extraction Agentï¼Œé€šè¿‡æ•æ‰è·¨å±‚å“åº”å·®å¼‚æ¥ä¿ç•™è·¨å±‚å¢é‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆä¸€ä¸ªResidual Aggregation Agentè¿›è¡Œåˆ†å±‚ä¿¡æ¯é›†æˆã€‚å¦å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶ï¼Œæ ¹æ®æ®‹å·®æ”¶æ•›æƒ…å†µåŠ¨æ€åœæ­¢å¤„ç†ï¼Œè¿›ä¸€æ­¥æé«˜æ¨ç†æ•ˆç‡ã€‚RMoAåœ¨è·¨å¯¹é½ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šä»»åŠ¡ç†è§£ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mindhunter01/RMoA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mindhunter01/RMoAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24442v1">PDF</a> Accepted by ACL 2025 (Findings)</p>
<p><strong>æ€»ç»“</strong></p>
<p>åŸºäºResNetçš„æ®‹å·®å­¦ä¹ ç†å¿µï¼Œæå‡ºäº†Residual Mixture-of-Agentsï¼ˆRMoAï¼‰ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥ä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡å’Œå¯é æ€§ï¼Œå¹¶è®¾è®¡äº†åŸºäºåµŒå…¥çš„å¤šæ ·æ€§é€‰æ‹©æœºåˆ¶ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼æ€§é€‰æ‹©å“åº”ã€‚ä¸ºç¼“è§£è¿­ä»£ä¿¡æ¯é€€åŒ–ï¼Œå¼•å…¥Residual Extraction Agentå’ŒResidual Aggregation Agentåˆ†åˆ«è¿›è¡Œè·¨å±‚ä¿¡æ¯å¢é‡æ•è·å’Œå±‚æ¬¡ä¿¡æ¯æ•´åˆã€‚æ­¤å¤–ï¼Œæå‡ºäº†è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶ï¼Œæ ¹æ®æ®‹å·®æ”¶æ•›æƒ…å†µåŠ¨æ€ç»ˆæ­¢å¤„ç†ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚RMoAåœ¨è·¨å¯¹é½ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šä»»åŠ¡ç†è§£ç­‰æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RMoAç³»ç»Ÿç»“åˆäº†æ®‹å·®è¿æ¥ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šä»»åŠ¡æ—¶çš„è®¡ç®—æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>åŸºäºåµŒå…¥çš„å¤šæ ·æ€§é€‰æ‹©æœºåˆ¶ç”¨äºæœ€å¤§åŒ–æ¨¡å‹å“åº”çš„ä¿¡æ¯åˆ©ç”¨ç‡ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>Residual Extraction Agentå’ŒResidual Aggregation Agentåˆ†åˆ«è§£å†³äº†è·¨å±‚ä¿¡æ¯å¢é‡æ•è·å’Œå±‚æ¬¡ä¿¡æ¯æ•´åˆçš„é—®é¢˜ã€‚</li>
<li>RMoAé€šè¿‡å¼•å…¥è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶ï¼Œæ ¹æ®æ®‹å·®æ”¶æ•›æƒ…å†µåŠ¨æ€è°ƒæ•´å¤„ç†è¿‡ç¨‹ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
<li>RMoAåœ¨å¤šä¸ªä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼ŒåŒ…æ‹¬è·¨å¯¹é½ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šä»»åŠ¡ç†è§£ã€‚</li>
<li>RMoAç³»ç»Ÿæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-99a3cfff061ea71962aa83ad96618fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d13ceaf26f28f41a6b240867ca3559af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5519d852f24504ffdb27035f6675b0a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unifying-Language-Agent-Algorithms-with-Graph-based-Orchestration-Engine-for-Reproducible-Agent-Research"><a href="#Unifying-Language-Agent-Algorithms-with-Graph-based-Orchestration-Engine-for-Reproducible-Agent-Research" class="headerlink" title="Unifying Language Agent Algorithms with Graph-based Orchestration Engine   for Reproducible Agent Research"></a>Unifying Language Agent Algorithms with Graph-based Orchestration Engine   for Reproducible Agent Research</h2><p><strong>Authors:Qianqian Zhang, Jiajia Liao, Heting Ying, Yibo Ma, Haozhan Shen, Jingcheng Li, Peng Liu, Lu Zhang, Chunxin Fang, Kyusong Lee, Ruochen Xu, Tiancheng Zhao</strong></p>
<p>Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„è¯­è¨€ä»£ç†åœ¨ç†è§£ã€æ¨ç†å’Œæ‰§è¡Œå¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€å‘ç¨³å¥çš„ä»£ç†é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼šå¤§é‡çš„å·¥ç¨‹å¼€é”€ã€ç¼ºä¹æ ‡å‡†åŒ–ç»„ä»¶ä»¥åŠå…¬å¹³æ¯”è¾ƒçš„ä¸è¶³è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºAgentå›¾çš„æ¨ç†ä¸è¯„ä¼°ç¼–æ’ï¼ˆAGORAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»å¯æ‰©å±•çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä¸€ç§æ¨¡å—åŒ–æ¶æ„ï¼Œå…·æœ‰åŸºäºå›¾çš„æµç¨‹å¼•æ“ã€é«˜æ•ˆçš„å†…å­˜ç®¡ç†å’Œæ¸…æ™°çš„ç»„ä»¶æŠ½è±¡ï¼›ï¼ˆ2ï¼‰ä¸€ç³»åˆ—å…ˆè¿›çš„å¯é‡ç”¨ä»£ç†ç®—æ³•ï¼Œå®ç°æœ€å…ˆè¿›çš„æ¨ç†æ–¹æ³•ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªç»´åº¦ä¸Šè¿›è¡Œç³»ç»Ÿçš„æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨æ•°å­¦æ¨ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯„ä¼°äº†ä¸åŒLLMä¸­çš„å¤šç§ä»£ç†ç®—æ³•ï¼Œæ­ç¤ºäº†å®ƒä»¬ç›¸å¯¹ä¼˜åŠ¿å’Œé€‚ç”¨æ€§çš„é‡è¦è§è§£ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤æ‚çš„æ¨ç†æ–¹æ³•å¯ä»¥å¢å¼ºä»£ç†èƒ½åŠ›ï¼Œä½†åƒâ€œæ€ç»´é“¾â€è¿™æ ·çš„ç®€å•æ–¹æ³•å¾€å¾€è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶ä¸”è®¡ç®—å¼€é”€æ›´ä½ã€‚AGORAä¸ä»…ç®€åŒ–äº†è¯­è¨€ä»£ç†çš„å¼€å‘ï¼Œè€Œä¸”é€šè¿‡æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ä¸ºå¯é‡å¤çš„ä»£ç†ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24354v1">PDF</a> Accepted by ACL 2025 Demo</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€ä»£ç†åœ¨ç†è§£ã€æ¨ç†å’Œæ‰§è¡Œå¤æ‚ä»»åŠ¡æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€å‘ç¨³å¥çš„ä»£ç†é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å·¨å¤§çš„å·¥ç¨‹å¼€é”€ã€ç¼ºä¹æ ‡å‡†åŒ–ç»„ä»¶å’Œä¸å……åˆ†çš„è¯„ä¼°æ¡†æ¶ç­‰ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºAgentå›¾çš„æ¨ç†ä¸è¯„ä¼°ç¼–æ’ï¼ˆAGORAï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®è§£å†³è¿™äº›é—®é¢˜ï¼šæ¨¡å—åŒ–æ¶æ„ã€å¯é‡ç”¨çš„ä»£ç†ç®—æ³•å¥—ä»¶å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…ç®€åŒ–äº†è¯­è¨€ä»£ç†çš„å¼€å‘ï¼Œè¿˜ä¸ºæ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å»ºç«‹äº†è¯­è¨€ä»£ç†ç ”ç©¶çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AGORAæ¡†æ¶åŸºäºå›¾çš„å·¥ä½œæµå¼•æ“ï¼Œå®ç°äº†æ¨¡å—åŒ–æ¶æ„ã€é«˜æ•ˆå†…å­˜ç®¡ç†å’Œæ¸…æ™°çš„ç»„ä»¶æŠ½è±¡ã€‚</li>
<li>AGORAåŒ…å«ä¸€ç³»åˆ—å¯é‡ç”¨çš„ä»£ç†ç®—æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ¨ç†æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ•°å­¦æ¨ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œè¯„ä¼°äº†ä¸åŒLLMä¸­çš„ä»£ç†ç®—æ³•ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤æ‚çš„æ¨ç†æ–¹æ³•å¯ä»¥å¢å¼ºä»£ç†èƒ½åŠ›ï¼Œä½†ç®€å•çš„å¦‚â€œæ€ç»´é“¾â€æ–¹æ³•å¸¸å¸¸å±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—å¼€é”€è¾ƒä½ã€‚</li>
<li>AGORAæ¡†æ¶ç®€åŒ–äº†è¯­è¨€ä»£ç†çš„å¼€å‘ï¼Œä¸ºè¯­è¨€ä»£ç†ç ”ç©¶å»ºç«‹äº†å¯é‡å¤æ€§çš„åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e6b5bacc8844dcf2070ff1d3e2eb9e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dd133fcf20da35c1cb26cbf64a8967e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5778d55ffb8dceb04bb99e54e4a16c3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a019e7ed2b7232a5101017321f13b57c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97321cf52ccbe0b2e32c2c10f9de7616.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5720198c84ea74b5c70aab4cd591d6b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="R3DM-Enabling-Role-Discovery-and-Diversity-Through-Dynamics-Models-in-Multi-agent-Reinforcement-Learning"><a href="#R3DM-Enabling-Role-Discovery-and-Diversity-Through-Dynamics-Models-in-Multi-agent-Reinforcement-Learning" class="headerlink" title="R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in   Multi-agent Reinforcement Learning"></a>R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in   Multi-agent Reinforcement Learning</h2><p><strong>Authors:Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali</strong></p>
<p>Multi-agent reinforcement learning (MARL) has achieved significant progress in large-scale traffic control, autonomous vehicles, and robotics. Drawing inspiration from biological systems where roles naturally emerge to enable coordination, role-based MARL methods have been proposed to enhance cooperation learning for complex tasks. However, existing methods exclusively derive roles from an agentâ€™s past experience during training, neglecting their influence on its future trajectories. This paper introduces a key insight: an agentâ€™s role should shape its future behavior to enable effective coordination. Hence, we propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel role-based MARL framework that learns emergent roles by maximizing the mutual information between agentsâ€™ roles, observed trajectories, and expected future behaviors. R3DM optimizes the proposed objective through contrastive learning on past trajectories to first derive intermediate roles that shape intrinsic rewards to promote diversity in future behaviors across different roles through a learned dynamics model. Benchmarking on SMAC and SMACv2 environments demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving multi-agent coordination to increase win rates by up to 20%. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤§è§„æ¨¡äº¤é€šæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å—ç”Ÿç‰©ç³»ç»Ÿå¯å‘ï¼Œå…¶ä¸­çš„è§’è‰²è‡ªç„¶å‡ºç°ä»¥å®ç°åè°ƒï¼ŒåŸºäºè§’è‰²çš„MARLæ–¹æ³•å·²è¢«æå‡ºä»¥å¢å¼ºå¤æ‚ä»»åŠ¡çš„åˆä½œå­¦ä¹ ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…æ ¹æ®æ™ºèƒ½ä½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡å»ç»éªŒæ¥æ¨å¯¼è§’è‰²ï¼Œå¿½è§†äº†å®ƒä»¬å¯¹æœªæ¥è½¨è¿¹çš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…³é”®è§‚ç‚¹ï¼šæ™ºèƒ½ä½“çš„è§’è‰²åº”è¯¥å¡‘é€ å…¶æœªæ¥è¡Œä¸ºï¼Œä»¥å®ç°æœ‰æ•ˆçš„åè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡åŠ¨æ€æ¨¡å‹å‘ç°è§’è‰²å’Œå¤šæ ·æ€§ï¼ˆR3DMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºè§’è‰²çš„MARLæ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–æ™ºèƒ½ä½“è§’è‰²ã€è§‚å¯Ÿåˆ°çš„è½¨è¿¹å’Œé¢„æœŸçš„æœªæ¥è¡Œä¸ºä¹‹é—´çš„äº’ä¿¡æ¯æ¥å­¦ä¹ æ–°å…´è§’è‰²ã€‚R3DMé€šè¿‡å¯¹è¿‡å»è½¨è¿¹çš„å¯¹æ¯”å­¦ä¹ æ¥ä¼˜åŒ–æ‰€æå‡ºçš„ç›®æ ‡ï¼Œé¦–å…ˆå¾—å‡ºä¸­é—´è§’è‰²ï¼Œå½¢æˆå†…åœ¨å¥–åŠ±ï¼Œé€šè¿‡å­¦åˆ°çš„åŠ¨æ€æ¨¡å‹ä¿ƒè¿›æœªæ¥ä¸åŒè§’è‰²è¡Œä¸ºçš„å¤šæ ·æ€§ã€‚åœ¨SMACå’ŒSMACv2ç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒR3DMä¼˜äºæœ€æ–°çš„MARLæ–¹æ³•ï¼Œé€šè¿‡æé«˜å¤šæ™ºèƒ½ä½“åè°ƒï¼Œå°†èƒœç‡æé«˜é«˜è¾¾20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24265v1">PDF</a> 21 pages, To appear in the International Conference of Machine   Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤§å‹äº¤é€šæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å—ç”Ÿç‰©ç³»ç»Ÿä¸­è‡ªç„¶æ¶Œç°çš„è§’è‰²åè°ƒå¯äº‹ï¼ŒåŸºäºè§’è‰²çš„MARLæ–¹æ³•å·²è¢«æå‡ºä»¥æå‡å¤æ‚ä»»åŠ¡çš„åˆä½œå­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…æ ¹æ®æ™ºèƒ½ä½“çš„è¿‡å»ç»éªŒæ¥è¡ç”Ÿè§’è‰²ï¼Œå¿½è§†äº†å…¶åœ¨æœªæ¥è½¨è¿¹ä¸Šçš„å½±å“ã€‚æœ¬æ–‡çš„å…³é”®è§è§£æ˜¯ï¼šæ™ºèƒ½ä½“çš„è§’è‰²åº”å¡‘é€ å…¶æœªæ¥è¡Œä¸ºä»¥å®ç°æœ‰æ•ˆçš„åè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡åŠ¨æ€æ¨¡å‹å‘ç°å¤šæ ·æ€§çš„è§’è‰²ï¼ˆR3DMï¼‰ï¼Œä¸€ç§æ–°å‹åŸºäºè§’è‰²çš„MARLæ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–æ™ºèƒ½ä½“è§’è‰²ã€è§‚å¯Ÿåˆ°çš„è½¨è¿¹å’Œé¢„æœŸçš„æœªæ¥è¡Œä¸ºä¹‹é—´çš„äº’ä¿¡æ¯æ¥å­¦ä¹ æ¶Œç°çš„è§’è‰²ã€‚R3DMé€šè¿‡å¯¹è¿‡å»è½¨è¿¹çš„å¯¹æ¯”å­¦ä¹ æ¥ä¼˜åŒ–ç›®æ ‡ï¼Œé¦–å…ˆæ¨å¯¼å‡ºä¸­é—´è§’è‰²ï¼Œå½¢æˆå†…åœ¨å¥–åŠ±ï¼Œå¹¶é€šè¿‡å­¦ä¹ åˆ°çš„åŠ¨æ€æ¨¡å‹ä¿ƒè¿›æœªæ¥ä¸åŒè§’è‰²è¡Œä¸ºçš„å¤šæ ·æ€§ã€‚åœ¨SMACå’ŒSMACv2ç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒR3DMä¼˜äºæœ€æ–°çš„MARLæ–¹æ³•ï¼Œæé«˜äº†å¤šæ™ºèƒ½ä½“çš„åè°ƒæ€§èƒ½ï¼Œèƒœç‡æé«˜äº†é«˜è¾¾20%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤§å‹äº¤é€šæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—è¿›å±•ã€‚</li>
<li>åŸºäºè§’è‰²çš„MARLæ–¹æ³•å—ç”Ÿç‰©ç³»ç»Ÿä¸­è‡ªç„¶æ¶Œç°çš„è§’è‰²å¯å‘ï¼Œæ—¨åœ¨æå‡å¤æ‚ä»»åŠ¡çš„åˆä½œå­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰MARLæ–¹æ³•å¿½è§†æ™ºèƒ½ä½“è§’è‰²å¯¹æœªæ¥è½¨è¿¹çš„å½±å“ã€‚</li>
<li>æœ¬æ–‡æå‡ºR3DMæ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–æ™ºèƒ½ä½“è§’è‰²ã€è½¨è¿¹å’Œé¢„æœŸæœªæ¥è¡Œä¸ºé—´çš„äº’ä¿¡æ¯æ¥å­¦ä¹ è§’è‰²ã€‚</li>
<li>R3DMé€šè¿‡å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–ç›®æ ‡ï¼Œç»“åˆåŠ¨æ€æ¨¡å‹å‘ç°å¤šæ ·æ€§è§’è‰²ã€‚</li>
<li>R3DMåœ¨SMACå’ŒSMACv2ç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–MARLæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-017ff17fdfb85b18c9366f5f03d0d9f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ff932d8586bb5d60ce4356cec598e10.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Learning-API-Functionality-from-Demonstrations-for-Tool-based-Agents"><a href="#Learning-API-Functionality-from-Demonstrations-for-Tool-based-Agents" class="headerlink" title="Learning API Functionality from Demonstrations for Tool-based Agents"></a>Learning API Functionality from Demonstrations for Tool-based Agents</h2><p><strong>Authors:Bhrij Patel, Ashish Jagmohan, Aditya Vempaty</strong></p>
<p>Digital tool-based agents that invoke external Application Programming Interfaces (APIs) often rely on documentation to understand API functionality. However, such documentation is frequently missing, outdated, privatized, or inconsistent-hindering the development of reliable, general-purpose agents. In this work, we propose learning API functionality directly from demonstrations as a new paradigm applicable in scenarios without documentation. Using existing API benchmarks, we collect demonstrations from both expert API-based agents and from self-exploration. To understand what information demonstrations must convey for successful task completion, we extensively study how the number of demonstrations and the use of LLM-generated summaries and evaluations affect the task success rate of the API-based agent. Our experiments across 3 datasets and 5 models show that learning functionality from demonstrations remains a non-trivial challenge, even for state-of-the-art LLMs. We find that providing explicit function calls and natural language critiques significantly improves the agentâ€™s task success rate due to more accurate parameter filling. We analyze failure modes, identify sources of error, and highlight key open challenges for future work in documentation-free, self-improving, API-based agents. </p>
<blockquote>
<p>åŸºäºæ•°å­—å·¥å…·çš„å¤–éƒ¨åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ä»£ç†é€šå¸¸ä¾èµ–äºæ–‡æ¡£æ¥äº†è§£APIçš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ–‡æ¡£ç»å¸¸ç¼ºå¤±ã€è¿‡æ—¶ã€ç§æœ‰åŒ–æˆ–ä¸ä¸€è‡´ï¼Œé˜»ç¢äº†å¯é ã€é€šç”¨ä»£ç†çš„å¼€å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒä¾‹ï¼Œå³ç›´æ¥ä»æ¼”ç¤ºä¸­å­¦ä¹ APIåŠŸèƒ½ï¼Œé€‚ç”¨äºæ²¡æœ‰æ–‡æ¡£çš„åœºæ™¯ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„APIåŸºå‡†æµ‹è¯•ï¼Œæ”¶é›†ä¸“ä¸šAPIä»£ç†çš„æ¼”ç¤ºä»¥åŠè‡ªæˆ‘æ¢ç´¢çš„æ¼”ç¤ºã€‚ä¸ºäº†äº†è§£ä¸ºäº†æˆåŠŸå®Œæˆä»»åŠ¡ï¼Œæ¼”ç¤ºå¿…é¡»ä¼ è¾¾å“ªäº›ä¿¡æ¯ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶æ¼”ç¤ºçš„æ•°é‡å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦å’Œè¯„ä¼°å¯¹åŸºäºAPIçš„ä»£ç†ä»»åŠ¡æˆåŠŸç‡çš„å½±å“ã€‚æˆ‘ä»¬åœ¨3ä¸ªæ•°æ®é›†å’Œ5ä¸ªæ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä»æ¼”ç¤ºä¸­å­¦ä¹ åŠŸèƒ½ä»ç„¶æ˜¯ä¸€ä¸ªéå¹³å‡¡çš„æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯å¯¹äºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬å‘ç°ï¼Œæä¾›æ˜ç¡®çš„å‡½æ•°è°ƒç”¨å’Œè‡ªç„¶è¯­è¨€çš„æ‰¹è¯„å¯ä»¥æ˜¾è‘—æé«˜ä»£ç†çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå› ä¸ºå¯ä»¥æ›´å‡†ç¡®åœ°å¡«å……å‚æ•°ã€‚æˆ‘ä»¬åˆ†æäº†å¤±è´¥æ¨¡å¼ï¼Œè¯†åˆ«äº†é”™è¯¯æ¥æºï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥å·¥ä½œåœ¨æ— æ–‡æ¡£ã€è‡ªæˆ‘æ”¹è¿›ã€åŸºäºAPIçš„ä»£ç†æ–¹é¢çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24197v1">PDF</a> 18 Pages, 13 Figures, 5 Tables</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ•°å­—å·¥å…·å‹ä»£ç†åœ¨è°ƒç”¨å¤–éƒ¨åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰æ—¶é¢ä¸´çš„æ–‡æ¡£ç¼ºå¤±ã€è¿‡æ—¶ã€ç§æœ‰åŒ–æˆ–ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼â€”â€”ç›´æ¥ä»æ¼”ç¤ºä¸­å­¦ä¹ APIåŠŸèƒ½ã€‚ç ”ç©¶é€šè¿‡ç°æœ‰APIåŸºå‡†æµ‹è¯•æ”¶é›†äº†ä¸“ä¸šAPIä»£ç†å’Œè‡ªæˆ‘æ¢ç´¢çš„æ¼”ç¤ºï¼Œå¹¶æ¢è®¨äº†æ¼”ç¤ºæ•°é‡åŠå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦å’Œè¯„ä¼°å¯¹APIä»£ç†ä»»åŠ¡å®Œæˆç‡çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹æƒ…å†µä¸‹ï¼Œä»æ¼”ç¤ºä¸­å­¦ä¹ åŠŸèƒ½ä»æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æ˜ç¡®çš„åŠŸèƒ½è°ƒç”¨å’Œè‡ªç„¶è¯­è¨€çš„è¯„ä»·èƒ½æ˜¾è‘—æé«˜ä»£ç†çš„ä»»åŠ¡å®Œæˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—å·¥å…·å‹ä»£ç†åœ¨è°ƒç”¨APIæ—¶é¢ä¸´æ–‡æ¡£ç¼ºå¤±ã€è¿‡æ—¶ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºç›´æ¥ä»æ¼”ç¤ºä¸­å­¦ä¹ APIåŠŸèƒ½çš„æ–°èŒƒå¼ä»¥é€‚åº”æ— æ–‡æ¡£åœºæ™¯ã€‚</li>
<li>é€šè¿‡ç°æœ‰APIåŸºå‡†æµ‹è¯•æ”¶é›†æ¼”ç¤ºï¼Œå¹¶ç ”ç©¶æ¼”ç¤ºæ•°é‡å¯¹APIä»£ç†ä»»åŠ¡å®Œæˆç‡çš„å½±å“ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦å’Œè¯„ä¼°åœ¨APIå­¦ä¹ ä»»åŠ¡ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>æ˜ç¡®çš„åŠŸèƒ½è°ƒç”¨å’Œè‡ªç„¶è¯­è¨€çš„è¯„ä»·èƒ½æ˜¾è‘—æé«˜ä»£ç†çš„ä»»åŠ¡å®Œæˆç‡ã€‚</li>
<li>åˆ†æå’Œè¯†åˆ«äº†å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¤±è´¥æ¨¡å¼å’Œå…³é”®é”™è¯¯æ¥æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3ad93a5a47db4ecf5e46e6f8305501a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0e38908bc7ca212ab25a17b33a5661c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa72a0d2b3a94241e899ba49bae8f062.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Just-Follow-MLLM-Plans-Robust-and-Efficient-Planning-for-Open-world-Agents"><a href="#Donâ€™t-Just-Follow-MLLM-Plans-Robust-and-Efficient-Planning-for-Open-world-Agents" class="headerlink" title="Donâ€™t Just Follow MLLM Plans: Robust and Efficient Planning for   Open-world Agents"></a>Donâ€™t Just Follow MLLM Plans: Robust and Efficient Planning for   Open-world Agents</h2><p><strong>Authors:Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, Jungseul Ok</strong></p>
<p>Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMsâ€™ inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOAâ€™s robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches. </p>
<blockquote>
<p>åœ¨ä¸å¯é¢„æµ‹ã€äº¤äº’æ€§çš„ç¯å¢ƒä¸­ï¼Œå¼€å‘èƒ½å¤ŸæŒæ¡å¤æ‚ã€å¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§„åˆ’æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæœ‰é—®é¢˜çš„å†…éƒ¨çŸ¥è¯†æˆ–åšå‡ºä¸åˆ‡å®é™…çš„ç¯å¢ƒå‡è®¾ã€‚å°½ç®¡æœ€è¿‘æœ‰å·¥ä½œæ¢ç´¢å­¦ä¹ è§„åˆ’çŸ¥è¯†ï¼Œä½†ç”±äºéƒ¨åˆ†ä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–ä¸åˆ‡å®é™…çš„è®¾ç½®ï¼Œå®ƒä»¬ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚äº‹å®ä¸Šï¼Œå…ˆå‰çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†èƒ½å¤Ÿåœ¨ç°å®ç¯å¢ƒä¸­ä»å¤´è·å¾—è§„åˆ’çŸ¥è¯†çš„ä»£ç†çš„å‘å±•ã€‚è™½ç„¶å®ç°è¿™ç§èƒ½åŠ›æ˜¯å¿…è¦çš„ï¼Œä½†å®ƒå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦çš„æ˜¯åœ¨èå…¥LLMä¸å‡†ç¡®çŸ¥è¯†æ—¶å®ç°ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæ•ˆç‡å¯¹äºå®ç”¨æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå­¦ä¹ å¯èƒ½éœ€è¦ç¦æ­¢æ¢ç´¢ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å‘å¼€æ”¾ä¸–ç•Œä»£ç†çš„ç¨³å¥é«˜æ•ˆè§„åˆ’ï¼ˆREPOAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜çš„æ–°å‹æ¡†æ¶ã€‚REPOAå…·æœ‰ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šè‡ªé€‚åº”ä¾èµ–å­¦ä¹ ã€ç²¾ç»†çš„æ•…éšœæ„ŸçŸ¥æ“ä½œå†…å­˜ï¼Œä»¥å¢å¼ºå¯¹ä¸å‡†ç¡®çŸ¥è¯†çš„ç¨³å¥æ€§ï¼Œä»¥åŠåŸºäºéš¾åº¦çš„æ¢ç´¢ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå»ºç«‹çš„å¼€æ”¾ä¸–ç•Œæµ‹è¯•åºŠä¸Šè¿›è¡Œçš„è¯„ä¼°è¯æ˜äº†REPOAçš„ç¨³å¥é«˜æ•ˆè§„åˆ’ï¼Œå±•ç¤ºäº†å…¶æˆåŠŸè·å–å…ˆå‰æ–¹æ³•æ— æ³•è¾¾åˆ°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åæœŸæ¸¸æˆé“å…·çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦ä»‹ç»äº†å¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚å¤šå˜ã€ä¸å¯é¢„æµ‹çš„ç¯å¢ƒä¸­å®Œæˆå¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†äººçš„æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å¸¸ä¾èµ–äºå†…éƒ¨çŸ¥è¯†çš„ç¼ºé™·æˆ–åšå‡ºä¸åˆ‡å®é™…çš„ç¯å¢ƒå‡è®¾ã€‚æœ€è¿‘çš„ç ”ç©¶è™½ç„¶å¼€å§‹æ¢ç´¢å­¦ä¹ è§„åˆ’çŸ¥è¯†ï¼Œä½†ä»å—é™äºä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–ä¸åˆ‡å®é™…çš„è®¾ç½®ã€‚æ–‡ç« å¼ºè°ƒï¼Œä»ç°å®ç¯å¢ƒä¸­ç›´æ¥è·å–è§„åˆ’çŸ¥è¯†çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†å®ç°è¿™ä¸€èƒ½åŠ›éœ€è¦è§£å†³é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å®ç°ç¨³å¥æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†Robust and Efficient Planning for Open-world Agentsï¼ˆREPOAï¼‰æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šè‡ªé€‚åº”ä¾èµ–å­¦ä¹ ã€ç²¾ç»†åŒ–çš„å¤±è´¥æ„ŸçŸ¥æ“ä½œå†…å­˜å’Œéš¾åº¦å¯¼å‘çš„æ¢ç´¢ã€‚åœ¨å¼€æ”¾ä¸–ç•Œæµ‹è¯•å¹³å°çš„è¯„ä¼°ä¸­ï¼ŒREPOAå±•ç°å‡ºç¨³å¥ä¸”é«˜æ•ˆçš„è§„åˆ’èƒ½åŠ›ï¼ŒæˆåŠŸè·å–äº†å…ˆå‰æ–¹æ³•æ— æ³•è§¦åŠçš„å¤æ‚åæœŸä»»åŠ¡ç‰©å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘èƒ½åœ¨å¤æ‚å¤šå˜ã€ä¸å¯é¢„æµ‹çš„ç¯å¢ƒä¸­å®Œæˆå¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†äººæ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¾èµ–å†…éƒ¨çŸ¥è¯†çš„ç¼ºé™·æˆ–åšå‡ºä¸åˆ‡å®é™…çš„ç¯å¢ƒå‡è®¾çš„é—®é¢˜ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢å­¦ä¹ è§„åˆ’çŸ¥è¯†ï¼Œä½†ä»å—é™äºä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–ä¸åˆ‡å®é™…çš„è®¾ç½®ã€‚</li>
<li>ç›´æ¥ä»ç°å®ç¯å¢ƒä¸­è·å–è§„åˆ’çŸ¥è¯†çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†å®ç°è¿™ä¸€èƒ½åŠ›éœ€è¦è§£å†³ç¨³å¥æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>REPOAæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼šè‡ªé€‚åº”ä¾èµ–å­¦ä¹ ã€ç²¾ç»†åŒ–çš„å¤±è´¥æ„ŸçŸ¥æ“ä½œå†…å­˜å’Œéš¾åº¦å¯¼å‘çš„æ¢ç´¢ã€‚</li>
<li>REPOAæ¡†æ¶é€šè¿‡å¢å¼ºç¨³å¥æ€§å’Œæ•ˆç‡ï¼ŒæˆåŠŸåœ¨å¼€æ”¾ä¸–ç•Œæµ‹è¯•å¹³å°å®Œæˆå¤æ‚åæœŸä»»åŠ¡ç‰©å“çš„è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7171b52d3b44e94128d261a96ac10ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-876e2fe92932c301d1d64d50d8627849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efeb7f17e2d6998c4a18ea2bec7f4118.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FieldWorkArena-Agentic-AI-Benchmark-for-Real-Field-Work-Tasks"><a href="#FieldWorkArena-Agentic-AI-Benchmark-for-Real-Field-Work-Tasks" class="headerlink" title="FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks"></a>FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks</h2><p><strong>Authors:Atsunori Moteki, Shoichi Masui, Fan Yang, Yueqi Song, Yonatan Bisk, Graham Neubig, Ikuo Kusajima, Yasuto Watanabe, Hiroyuki Ishida, Jun Takahashi, Shan Jiang</strong></p>
<p>This paper proposes FieldWorkArena, a benchmark for agentic AI targeting real-world field work. With the recent increase in demand for agentic AI, they are required to monitor and report safety and health incidents, as well as manufacturing-related incidents, that may occur in real-world work environments. Existing agentic AI benchmarks have been limited to evaluating web tasks and are insufficient for evaluating agents in real-world work environments, where complexity increases significantly. In this paper, we define a new action space that agentic AI should possess for real world work environment benchmarks and improve the evaluation function from previous methods to assess the performance of agentic AI in diverse real-world tasks. The dataset consists of videos captured on-site and documents actually used in factories and warehouses, and tasks were created based on interviews with on-site workers and managers. Evaluation results confirmed that performance evaluation considering the characteristics of Multimodal LLM (MLLM) such as GPT-4o is feasible. Additionally, the effectiveness and limitations of the proposed new evaluation method were identified. The complete dataset (HuggingFace) and evaluation program (GitHub) can be downloaded from the following website: <a target="_blank" rel="noopener" href="https://en-documents.research.global.fujitsu.com/fieldworkarena/">https://en-documents.research.global.fujitsu.com/fieldworkarena/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†FieldWorkArenaï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç°å®å·¥ä½œç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“AIçš„åŸºå‡†æµ‹è¯•ã€‚éšç€å¯¹æ™ºèƒ½ä½“AIéœ€æ±‚çš„ä¸æ–­å¢åŠ ï¼Œä»–ä»¬éœ€è¦ç›‘æ§å’ŒæŠ¥å‘Šç°å®å·¥ä½œç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿçš„å®‰å…¨å’Œå¥åº·äº‹ä»¶ï¼Œä»¥åŠä¸åˆ¶é€ ç›¸å…³çš„äº‹ä»¶ã€‚ç°æœ‰çš„æ™ºèƒ½ä½“AIåŸºå‡†æµ‹è¯•ä»…é™äºè¯„ä¼°ç½‘ç»œä»»åŠ¡ï¼Œå¯¹äºè¯„ä¼°ç°å®å·¥ä½œç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“æ¥è¯´æ˜¯ä¸å¤Ÿçš„ï¼Œç°å®å·¥ä½œç¯å¢ƒä¸­çš„å¤æ‚æ€§å¤§å¤§å¢åŠ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŠ¨ä½œç©ºé—´ï¼Œæ™ºèƒ½ä½“AIåº”è¯¥å…·å¤‡è¿™ä¸€ç©ºé—´ä»¥åº”å¯¹ç°å®å·¥ä½œç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ”¹è¿›äº†ä¹‹å‰çš„è¯„ä¼°åŠŸèƒ½ï¼Œä»¥è¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤šç§ç°å®ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ•°æ®é›†åŒ…å«ç°åœºæ‹æ‘„çš„è§†é¢‘å’Œå·¥å‚ä»“åº“å®é™…ä½¿ç”¨çš„æ–‡ä»¶ï¼Œä»»åŠ¡çš„åˆ›å»ºåŸºäºç°åœºå·¥äººå’Œç®¡ç†äººå‘˜çš„è®¿è°ˆã€‚è¯„ä¼°ç»“æœè¯å®ï¼Œè€ƒè™‘åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰çš„ç‰¹æ€§æ¥è¿›è¡Œæ€§èƒ½è¯„ä¼°æ˜¯å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œè¿˜ç¡®å®šäº†æ‰€æå‡ºçš„æ–°è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå±€é™æ€§ã€‚å®Œæ•´çš„æ•°æ®é›†ï¼ˆHuggingFaceï¼‰å’Œè¯„ä¼°ç¨‹åºï¼ˆGitHubï¼‰å¯ä»ä»¥ä¸‹ç½‘ç«™ä¸‹è½½ï¼š<a target="_blank" rel="noopener" href="https://en-documents.research.global.fujitsu.com/fieldworkarena/%E3%80%82">https://en-documents.research.global.fujitsu.com/fieldworkarena/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19662v2">PDF</a> 6 pages, 2 figures, 4 tables</p>
<p><strong>Summary</strong><br>     æ­¤è®ºæ–‡æå‡ºFieldWorkArenaï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç°å®å·¥ä½œç¯å¢ƒä¸­çš„ä»£ç†æ™ºèƒ½AIçš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚é‰´äºå¯¹ä»£ç†æ™ºèƒ½AIéœ€æ±‚çš„å¢åŠ ï¼Œè¯¥å¹³å°èƒ½å¤Ÿç›‘æ§å¹¶æŠ¥å‘Šå¯èƒ½å‘ç”Ÿçš„çœŸå®å·¥ä½œç¯å¢ƒçš„å¥åº·å’Œå®‰å…¨é—®é¢˜ï¼Œä»¥åŠåˆ¶é€ ä¸šç›¸å…³çš„äº‹æ•…ã€‚ç°æœ‰ä»£ç†æ™ºèƒ½AIåŸºå‡†æµ‹è¯•ä»…é™äºè¯„ä¼°ç½‘ç»œä»»åŠ¡ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°çœŸå®å·¥ä½œç¯å¢ƒä¸­çš„ä»£ç†æ™ºèƒ½ã€‚è®ºæ–‡å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŠ¨ä½œç©ºé—´ï¼Œå¹¶æ”¹è¿›äº†è¯„ä¼°å‡½æ•°ï¼Œä»¥è¯„ä¼°ä»£ç†æ™ºèƒ½åœ¨å¤šç§çœŸå®ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ•°æ®é›†åŒ…å«ç°åœºæ‹æ‘„çš„è§†é¢‘å’Œå·¥å‚ä»“åº“å®é™…ä½¿ç”¨çš„æ–‡æ¡£ï¼Œä»»åŠ¡æ˜¯åŸºäºç°åœºå·¥äººå’Œç®¡ç†äººå‘˜çš„è®¿è°ˆåˆ›å»ºçš„ã€‚è¯„ä¼°ç»“æœè¯æ˜äº†è€ƒè™‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¦‚GPT-4oçš„ç‰¹æ€§è¿›è¡Œæ€§èƒ½è¯„ä¼°çš„å¯è¡Œæ€§ï¼ŒåŒæ—¶ç¡®å®šäº†æ–°è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FieldWorkArenaæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¸“ä¸ºä»£ç†æ™ºèƒ½AIåœ¨ç°å®å·¥ä½œç¯å¢ƒä¸­çš„è¡¨ç°è®¾è®¡ã€‚</li>
<li>è¯¥å¹³å°èƒ½å¤Ÿåº”å¯¹çœŸå®å·¥ä½œç¯å¢ƒä¸­å¯èƒ½å‡ºç°çš„å¥åº·å’Œå®‰å…¨é—®é¢˜ï¼Œä»¥åŠåˆ¶é€ ä¸šç›¸å…³çš„äº‹æ•…ã€‚</li>
<li>ç°æœ‰ä»£ç†æ™ºèƒ½AIçš„åŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºç½‘ç»œä»»åŠ¡ï¼Œæ— æ³•å……åˆ†è¯„ä¼°å…¶åœ¨çœŸå®å·¥ä½œç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</li>
<li>è®ºæ–‡å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŠ¨ä½œç©ºé—´å¹¶æ”¹è¿›äº†è¯„ä¼°å‡½æ•°ï¼Œä»¥ä¾¿æ›´å‡†ç¡®åœ°è¯„ä¼°ä»£ç†æ™ºèƒ½åœ¨å¤šç§çœŸå®ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç°åœºæ‹æ‘„çš„è§†é¢‘å’ŒçœŸå®å·¥ä½œç¯å¢ƒçš„æ–‡æ¡£ï¼Œä»»åŠ¡æ˜¯åŸºäºå¯¹ç°åœºå·¥ä½œäººå‘˜å’Œç®¡ç†è€…çš„è®¿è°ˆåˆ›å»ºçš„ã€‚</li>
<li>è¯„ä¼°ç»“æœè¯æ˜äº†è€ƒè™‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç‰¹æ€§çš„æ€§èƒ½è¯„ä¼°çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-973e58c3b6bf2058a1f6128d3a07087e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d43047f3583ed72f2bc59805de05e559.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb443f857d525f31ee6cd199f6facca9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c660f83050997f90cd08a447fff9eda7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64af24d2e0765d7322b89405d10084b0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BAR-A-Backward-Reasoning-based-Agent-for-Complex-Minecraft-Tasks"><a href="#BAR-A-Backward-Reasoning-based-Agent-for-Complex-Minecraft-Tasks" class="headerlink" title="BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks"></a>BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks</h2><p><strong>Authors:Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei</strong></p>
<p>Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agentâ€™s initial state. However, this forward reasoning paradigm doesnâ€™t work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agentâ€™s initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨éµå¾ªäººç±»æŒ‡ä»¤å’Œè‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºäº†å®Œæˆä»»åŠ¡ï¼Œä»£ç†äººéœ€è¦é€šè¿‡è§„åˆ’å°†ä»»åŠ¡åˆ†è§£ä¸ºå¯è½»æ¾æ‰§è¡Œçš„æ­¥éª¤ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡ä»ä»£ç†äººçš„åˆå§‹çŠ¶æ€æ¨æ–­åº”æ‰§è¡Œå“ªäº›ä¸‹ä¸€æ­¥éª¤æ¥è¿›è¡Œè§„åˆ’ã€‚ç„¶è€Œï¼Œè¿™ç§æ­£å‘æ¨ç†æ¨¡å¼å¯¹äºå¤æ‚ä»»åŠ¡å¹¶ä¸å¥æ•ˆã€‚æˆ‘ä»¬æè®®åœ¨Minecraftè¿™ä¸ªåŸºäºç°å®ä¸–ç•Œåœºæ™¯æ¨¡æ‹Ÿå¤æ‚ä»»åŠ¡çš„è™šæ‹Ÿç¯å¢ƒä¸­ç ”ç©¶è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæ­£å‘æ¨ç†çš„å¤±è´¥æ˜¯ç”±äºä»£ç†åˆå§‹çŠ¶æ€ä¸ä»»åŠ¡ç›®æ ‡ä¹‹é—´å­˜åœ¨çš„å·¨å¤§æ„ŸçŸ¥å·®è·æ‰€å¯¼è‡´çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨é€†å‘æ¨ç†ï¼Œä»ç»ˆç«¯çŠ¶æ€å¼€å§‹è¿›è¡Œè§„åˆ’ï¼Œè¿™å¯ä»¥ç›´æ¥ä¸€æ­¥å®ç°ä»»åŠ¡ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºé€†å‘æ¨ç†çš„ä»£ç†ï¼ˆBARï¼‰ã€‚å®ƒé…å¤‡äº†ä¸€ä¸ªé€’å½’ç›®æ ‡åˆ†è§£æ¨¡å—ã€ä¸€ä¸ªçŠ¶æ€ä¸€è‡´æ€§ç»´æŠ¤æ¨¡å—å’Œä¸€ä¸ªé˜¶æ®µè®°å¿†æ¨¡å—ï¼Œä»ç»ˆç«¯çŠ¶æ€å¼€å§‹è¿›è¡Œç¨³å¥ã€ä¸€è‡´å’Œé«˜æ•ˆçš„è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBARä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ‰€æå‡ºæ¨¡å—æ•ˆæœæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14079v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨å®Œæˆå¤æ‚ä»»åŠ¡æ—¶é¢ä¸´è§„åˆ’æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡åå‘æ¨ç†æ¥è§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ä¸ªåä¸ºBARçš„æ™ºèƒ½ä½“æ¨¡å‹ï¼Œå®ƒèƒ½ä»ä»»åŠ¡ç»ˆç‚¹é€†å‘è¿›è¡Œè§„åˆ’ã€‚BARç»“åˆäº†é€’å½’ç›®æ ‡åˆ†è§£æ¨¡å—ã€çŠ¶æ€ä¸€è‡´æ€§ç»´æŠ¤æ¨¡å—å’Œé˜¶æ®µè®°å¿†æ¨¡å—ï¼Œä»¥å®ç°é«˜æ•ˆã€ç¨³å¥çš„è§„åˆ’ã€‚å®éªŒè¯æ˜ï¼ŒBARä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„è§„åˆ’èƒ½åŠ›æœ‰é™ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦ä»æ™ºèƒ½ä½“çš„åˆå§‹çŠ¶æ€å‡ºå‘è¿›è¡Œæ­£å‘æ¨ç†ï¼Œå¯¹äºå¤æ‚ä»»åŠ¡éš¾ä»¥å¥æ•ˆã€‚</li>
<li>æå‡ºä½¿ç”¨åå‘æ¨ç†åœ¨Minecraftè™šæ‹Ÿç¯å¢ƒä¸­è§£å†³æ­¤é—®é¢˜ï¼Œä»ä»»åŠ¡ç»ˆç‚¹å¼€å§‹è§„åˆ’èƒ½æ›´ç›´æ¥å®ç°ç›®æ ‡ã€‚</li>
<li>BARæ¨¡å‹å…·å¤‡é€’å½’ç›®æ ‡åˆ†è§£ã€çŠ¶æ€ä¸€è‡´æ€§ç»´æŠ¤å’Œé˜¶æ®µè®°å¿†ä¸‰å¤§æ¨¡å—ï¼Œæœ‰åŠ©äºé«˜æ•ˆç¨³å¥çš„è§„åˆ’ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b3bf9d2ffd26c5568a2a5ac7a1eaf1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cf566b0f3acba0e6f7546dd12091b04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28804291cbac0b3536cd89f6d0c5879.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7f0cb1b255b01caeed570b4b80abf50.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ReSo-A-Reward-driven-Self-organizing-LLM-based-Multi-Agent-System-for-Reasoning-Tasks"><a href="#ReSo-A-Reward-driven-Self-organizing-LLM-based-Multi-Agent-System-for-Reasoning-Tasks" class="headerlink" title="ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for   Reasoning Tasks"></a>ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for   Reasoning Tasks</h2><p><strong>Authors:Heng Zhou, Hejia Geng, Xiangyuan Xue, Li Kang, Yiran Qin, Zhiyong Wang, Zhenfei Yin, Lei Bai</strong></p>
<p>Multi-agent systems (MAS) have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving; however, current MAS frameworks suffer from poor flexibility and scalability with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process centered on our Collaborative Reward Model that provides fine-grained reward signals to optimize MAS cooperation. We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå·²å‡ºç°åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼›ç„¶è€Œï¼Œå½“å‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶åœ¨ä¼˜åŒ–ç­–ç•¥ä¸Šç¼ºä¹çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReSoï¼Œå®ƒå°†ä»»åŠ¡å›¾ç”Ÿæˆä¸ä»¥å¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©è¿‡ç¨‹ç›¸ç»“åˆï¼Œä»¥æˆ‘ä»¬çš„åä½œå¥–åŠ±æ¨¡å‹ä¸ºä¸­å¿ƒï¼Œä¸ºä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆä½œæä¾›ç²¾ç»†çš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¸éœ€è¦äººå·¥æ³¨é‡Šçš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReSoçš„è¡¨ç°ä¸ç°æœ‰æ–¹æ³•ç›¸åŒ¹é…æˆ–è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œåœ¨æ•°å­¦å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMath-MASï¼‰ä¸Šè¾¾åˆ°33.7%çš„å‡†ç¡®ç‡ï¼Œåœ¨SciBench-MASä¸Šè¾¾åˆ°32.3%çš„å‡†ç¡®ç‡ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å®Œå…¨å¤±è´¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02390v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å½“å‰MASæ¡†æ¶ç¼ºä¹çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¼˜åŒ–ç­–ç•¥å°šå¾…å®Œå–„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºReSoç³»ç»Ÿï¼Œé€šè¿‡ä»»åŠ¡å›¾ç”Ÿæˆä¸åŸºäºåä½œå¥–åŠ±æ¨¡å‹çš„å¥–åŠ±é©±åŠ¨ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©è¿‡ç¨‹æ¥ä¼˜åŒ–MASçš„åˆä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ— éœ€äººå·¥æ ‡æ³¨çš„è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶æ¥ç”ŸæˆMASåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReSoç³»ç»Ÿçš„è¡¨ç°ä¸ç°æœ‰æ–¹æ³•ç›¸å½“æˆ–æ›´èƒœä¸€ç­¹ï¼Œåœ¨Math-MASå’ŒSciBench-MASä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°33.7%å’Œ32.3%ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å®Œå…¨å¤±è´¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šã€‚</li>
<li>å½“å‰MASæ¡†æ¶å­˜åœ¨çµæ´»æ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ï¼Œä»¥åŠä¼˜åŒ–ç­–ç•¥çš„ä¸è¶³ã€‚</li>
<li>ReSoç³»ç»Ÿé€šè¿‡ä»»åŠ¡å›¾ç”Ÿæˆå’Œå¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©ä¼˜åŒ–MASåˆä½œã€‚</li>
<li>ReSoç³»ç»Ÿå¼•å…¥è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”ŸæˆMASåŸºå‡†æµ‹è¯•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>ReSoç³»ç»Ÿåœ¨Math-MASå’ŒSciBench-MASä¸Šçš„å‡†ç¡®ç‡é«˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8f9997dffc1f54e8663ec336e60ba7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-884b85d200d4d519a7839dbbf73ee5b1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Explorer-Scaling-Exploration-driven-Web-Trajectory-Synthesis-for-Multimodal-Web-Agents"><a href="#Explorer-Scaling-Exploration-driven-Web-Trajectory-Synthesis-for-Multimodal-Web-Agents" class="headerlink" title="Explorer: Scaling Exploration-driven Web Trajectory Synthesis for   Multimodal Web Agents"></a>Explorer: Scaling Exploration-driven Web Trajectory Synthesis for   Multimodal Web Agents</h2><p><strong>Authors:Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, Ahmed Awadallah</strong></p>
<p>Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„æˆåŠŸæ¿€å‘äº†è‡ªä¸»å®Œæˆå¤æ‚ç½‘ç»œä»»åŠ¡çš„æ™ºèƒ½ä»£ç†åº”ç”¨çš„å‰æ™¯ã€‚è™½ç„¶å¼€æºLMMä»£ç†åœ¨ç¦»çº¿è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨æ›´ç°å®çš„åœ¨çº¿ç¯å¢ƒä¸­ï¼Œå®ƒä»¬çš„æ€§èƒ½ä»ç„¶è¿œè¿œè½åäºäººç±»æ°´å¹³çš„èƒ½åŠ›ã€‚ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆæ˜¯ç¼ºä¹è·¨ä¸åŒé¢†åŸŸçš„å¤šæ ·ä¸”å¤§è§„æ¨¡è½¨è¿¹çº§æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®çš„æ”¶é›†æˆæœ¬å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å‘å¯åˆæˆçš„æœ€å¤§ä¸”æœ€å…·å¤šæ ·æ€§çš„è½¨è¿¹çº§æ•°æ®é›†æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡9.4ä¸‡æ¡æˆåŠŸçš„å¤šæ¨¡æ€ç½‘ç»œè½¨è¿¹ï¼Œæ¶µç›–4.9ä¸‡ä¸ªå”¯ä¸€URLã€72ä¸‡ä¸ªæˆªå›¾å’Œ3.3äº¿ä¸ªç½‘ç»œå…ƒç´ ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ©ç”¨å¹¿æ³›çš„ç½‘ç»œæ¢ç´¢å’Œç²¾ç»†åŒ–æ¥è·å¾—å¤šæ ·çš„ä»»åŠ¡æ„å›¾ã€‚æ¯æ¡æˆåŠŸè½¨è¿¹çš„å¹³å‡æˆæœ¬ä¸º28ç¾åˆ†ï¼Œä½¿å¾—ç¤¾åŒºä¸­çš„å¹¿å¤§ç”¨æˆ·éƒ½èƒ½è´Ÿæ‹…å¾—èµ·ã€‚åˆ©ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†Explorerï¼Œè¿™æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€ç½‘ç»œä»£ç†ï¼Œåœ¨ç¦»çº¿åœ¨çº¿ç½‘ç»œä»£ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Mind2Web-Liveã€Multimodal-Mind2Webå’ŒMiniWob++ï¼‰ä¸­è¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒå¼ºè°ƒäº†æ•°æ®è§„æ¨¡æ‰©å±•æ˜¯æé«˜ç½‘ç»œä»£ç†èƒ½åŠ›çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶ä½¿æ›´å¤§è§„æ¨¡ä¸Šçš„æœ€æ–°LMMä»£ç†ç ”ç©¶æ›´åŠ ä¾¿æ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11357v4">PDF</a> ACL 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»£ç†åœ¨è‡ªä¸»å®Œæˆå¤æ‚ç½‘ç»œä»»åŠ¡æ–¹é¢å±•ç°å‡ºæˆåŠŸåº”ç”¨å‰æ™¯ã€‚å°½ç®¡å¼€æºLMMä»£ç†åœ¨ç¦»çº¿è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ›´ç°å®çš„åœ¨çº¿ç¯å¢ƒä¸­ï¼Œå…¶æ€§èƒ½ä»ç„¶è¿œè¿œè½åäºäººç±»æ°´å¹³ã€‚æœ¬æ–‡è§£å†³æ­¤æŒ‘æˆ˜ï¼Œé€šè¿‡å¼€å‘å¯ä¼¸ç¼©é…æ–¹åˆæˆè¿„ä»Šä¸ºæ­¢æœ€å¤§ä¸”æœ€å¤šå…ƒè½¨è¿¹çº§æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡9.4ä¸‡æ¡æˆåŠŸå¤šæ¨¡æ€ç½‘ç»œè½¨è¿¹ï¼Œè·¨è¶Š4.9ä¸‡ä¸ªå”¯ä¸€URLã€72ä¸‡ä¸ªæˆªå›¾å’Œ33ä¸‡ä¸ªç½‘é¡µå…ƒç´ ã€‚è®­ç»ƒåŸºäºæ­¤æ•°æ®é›†çš„å¤šæ¨¡æ€ç½‘ç»œä»£ç†Explorerï¼Œåœ¨ç¦»çº¿åŠåœ¨çº¿ç½‘ç»œä»£ç†åŸºå‡†æµ‹è¯•å¦‚Mind2Web-Liveã€Multimodal-Mind2Webå’ŒMiniWob++ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå‡¸æ˜¾æ•°æ®è§„æ¨¡æ‰©å¤§æ˜¯æå‡ç½‘ç»œä»£ç†èƒ½åŠ›çš„é‡è¦é©±åŠ¨åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä»£ç†åœ¨å¤æ‚ç½‘ç»œä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œä½†åœ¨çº¿ç¯å¢ƒæ€§èƒ½ä»å¾…æé«˜ã€‚</li>
<li>ç°æœ‰æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹è·¨ä¸åŒé¢†åŸŸçš„å¤§å‹è½¨è¿¹çº§æ•°æ®é›†ï¼Œä¸”æ”¶é›†æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡åˆæˆæœ€å¤§ã€æœ€å¤šå…ƒè½¨è¿¹çº§æ•°æ®é›†è§£å†³æ­¤é—®é¢˜ï¼ŒåŒ…å«å¤šæ ·åŒ–ä»»åŠ¡æ„å›¾ã€‚</li>
<li>æ•°æ®é›†åˆæˆå¹³å‡æˆæœ¬ä½ï¼Œä½¿å¾—æ›´å¤šç”¨æˆ·å¯è´Ÿæ‹…ã€‚</li>
<li>åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„Explorerä»£ç†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒç»“æœå¼ºè°ƒæ•°æ®è§„æ¨¡æ‰©å¤§å¯¹æå‡ç½‘ç»œä»£ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ec7ba74d70a21bd832a7dc24f9145e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a18bed7a76daea6c5694b61ce112da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1df30ed6d5f3370119593c0faa3b3324.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26c92d046135907f211871d9d944ce2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7b58576d5bb73315cef221dd6f6e36e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b71604385b28e278626994e5fbc8e74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6172f81822a1fb1c62708c4bbaea3e43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74e116562ef6afa6f3e7b910cb9cc723.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="KBQA-o1-Agentic-Knowledge-Base-Question-Answering-with-Monte-Carlo-Tree-Search"><a href="#KBQA-o1-Agentic-Knowledge-Base-Question-Answering-with-Monte-Carlo-Tree-Search" class="headerlink" title="KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree   Search"></a>KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree   Search</h2><p><strong>Authors:Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan</strong></p>
<p>Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic explorationâ€™s performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B modelâ€™s GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. Our code is publicly available. </p>
<blockquote>
<p>çŸ¥è¯†åº“é—®ç­”ï¼ˆKBQAï¼‰æ—¨åœ¨åˆ©ç”¨å¤§è§„æ¨¡ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆKBï¼‰å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ‰€å‘å±•ï¼ŒKBQAä»ç„¶é¢ä¸´çŸ¥è¯†åº“æ„è¯†è–„å¼±ã€æœ‰æ•ˆæ€§ä¸æ•ˆç‡ä¸å¹³è¡¡ã€é«˜åº¦ä¾èµ–æ³¨é‡Šæ•°æ®ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†KBQA-o1ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ–°å‹æ™ºèƒ½KBQAæ–¹æ³•ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åŸºäºReActçš„ä»£ç†è¿‡ç¨‹ï¼Œç”¨äºé€æ­¥ç”Ÿæˆé€»è¾‘å½¢å¼å¹¶è¿›è¡ŒçŸ¥è¯†åº“ç¯å¢ƒæ¢ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨MCTSï¼ˆä¸€ç§å—ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹é©±åŠ¨çš„å¯å‘å¼æœç´¢æ–¹æ³•ï¼‰æ¥å¹³è¡¡æ™ºèƒ½æ¢ç´¢çš„æ€§èƒ½å’Œæœç´¢ç©ºé—´ã€‚é€šè¿‡å¯å‘å¼æ¢ç´¢ï¼ŒKBQA-o1å¯ä»¥ç”Ÿæˆé«˜è´¨é‡æ³¨é‡Šï¼Œé€šè¿‡å¢é‡å¾®è°ƒè¿›ä¸€æ­¥æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKBQA-o1ä¼˜äºä»¥å‰çš„ä½èµ„æºKBQAæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºæœ‰é™çš„æ³¨é‡Šæ•°æ®ï¼ŒKBQA-o1å°†Llama-3.1-8Bæ¨¡å‹çš„GrailQA F1æ€§èƒ½æé«˜åˆ°78.5%ï¼Œè€Œä¹‹å‰çš„æœ€ä½³æ–¹æ³•GPT-3.5-turboçš„è¯¥æ€§èƒ½æŒ‡æ ‡ä¸º48.5%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18922v4">PDF</a> Accepted by ICML 2025 main conference</p>
<p><strong>Summary</strong><br>     çŸ¥è¯†åº“é—®ç­”ï¼ˆKBQAï¼‰æ—¨åœ¨åˆ©ç”¨å¤§è§„æ¨¡ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆKBï¼‰å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚å°½ç®¡æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ï¼ŒKBQAä»é¢ä¸´çŸ¥è¯†åº“æ„è¯†å¼±ã€æ•ˆç‡å’Œæ•ˆæœä¸å¹³è¡¡ã€é«˜åº¦ä¾èµ–æ³¨é‡Šæ•°æ®ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ–°å‹æ™ºèƒ½KBQAæ–¹æ³•â€”â€”KBQA-o1ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åŸºäºReActçš„ä»£ç†è¿‡ç¨‹ï¼Œç”¨äºé€æ­¥ç”Ÿæˆé€»è¾‘å½¢å¼å¹¶è¿›è¡ŒçŸ¥è¯†åº“ç¯å¢ƒæ¢ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨MCTSè¿™ç§å—ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹é©±åŠ¨çš„ç­–ç•¥å¯å‘å¼æœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æ™ºèƒ½æ¢ç´¢çš„æ€§èƒ½å’Œæœç´¢ç©ºé—´çš„å¹³è¡¡ã€‚é€šè¿‡å¯å‘å¼æ¢ç´¢ï¼ŒKBQA-o1å¯ä»¥ç”Ÿæˆé«˜è´¨é‡æ³¨é‡Šï¼Œé€šè¿‡å¢é‡å¾®è°ƒè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKBQA-o1åœ¨æœ‰é™æ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¼˜äºä¹‹å‰çš„ä½èµ„æºKBQAæ–¹æ³•ï¼Œå°†Llama-3.1-8Bæ¨¡å‹çš„GrailQA F1æ€§èƒ½æé«˜åˆ°78.5%ï¼Œè€Œä¹‹å‰æœ€ä½³æ–¹æ³•çš„GPT-3.5-turboæ¨¡å‹ä»…ä¸º48.5%ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. KBQAé¢ä¸´çŸ¥è¯†åº“æ„è¯†å¼±ã€æ•ˆç‡å’Œæ•ˆæœä¸å¹³è¡¡ã€ä¾èµ–æ³¨é‡Šæ•°æ®ç­‰æŒ‘æˆ˜ã€‚<br>     2. KBQA-o1æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºä»£ç†çš„KBQAæ–¹æ³•ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚<br>     3. KBQA-o1å¼•å…¥ReAct-basedä»£ç†è¿‡ç¨‹ï¼Œç”¨äºé€æ­¥ç”Ÿæˆé€»è¾‘å½¢å¼å¹¶è¿›è¡ŒKBç¯å¢ƒæ¢ç´¢ã€‚<br>     4. MCTSå¹³è¡¡äº†æ™ºèƒ½æ¢ç´¢çš„æ€§èƒ½å’Œæœç´¢ç©ºé—´ã€‚<br>     5. KBQA-o1å¯ä»¥é€šè¿‡ç”Ÿæˆé«˜è´¨é‡æ³¨é‡Šï¼Œè¿›ä¸€æ­¥é€šè¿‡å¢é‡å¾®è°ƒæé«˜æ€§èƒ½ã€‚<br>     6. KBQA-o1åœ¨æœ‰é™æ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„KBQAæ–¹æ³•ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1bba9b4115fb35f4a998757df86f2c13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a6eb69b609fdfa59a24cd60cd65b220.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f613419b707245de6415f437be40c24c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94d85a678797b37aa1bc87a4bae3f0e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69ba8d082e5d4040e1aa68da06ed3009.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Improving-Parallel-Program-Performance-with-LLM-Optimizers-via-Agent-System-Interfaces"><a href="#Improving-Parallel-Program-Performance-with-LLM-Optimizers-via-Agent-System-Interfaces" class="headerlink" title="Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces"></a>Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interfaces</h2><p><strong>Authors:Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken</strong></p>
<p>Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes. </p>
<blockquote>
<p>ç°ä»£ç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºé«˜æ€§èƒ½è®¡ç®—æ¥è¿›è¡Œå¤æ‚çš„å»ºæ¨¡å’Œæ¨¡æ‹Ÿã€‚æé«˜å¹¶è¡Œç¨‹åºæ€§èƒ½çš„å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•å°†ä»»åŠ¡é«˜æ•ˆæ˜ å°„åˆ°å¤„ç†å™¨å¹¶å°†æ•°æ®æ˜ å°„åˆ°å†…å­˜ï¼Œè¿™ä¸€è¿‡ç¨‹ç”±ç§°ä¸ºæ˜ å°„å™¨çš„å¤æ‚ä½çº§ç³»ç»Ÿä»£ç å†³å®šã€‚å¼€å‘é«˜æ€§èƒ½æ˜ å°„å™¨éœ€è¦æ•°å¤©çš„æ‰‹åŠ¨è°ƒæ•´ï¼Œè¿™å¯¹æ²¡æœ‰ç³»ç»Ÿä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸç§‘å­¦å®¶æ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§éšœç¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½¿ç”¨ç”Ÿæˆä¼˜åŒ–è‡ªåŠ¨åŒ–æ˜ å°„å™¨å¼€å‘çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸°å¯Œçš„åé¦ˆï¼Œè€Œä¸ä»…ä»…æ˜¯æ ‡é‡æ€§èƒ½æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†Agent-Systemæ¥å£ï¼Œå®ƒåŒ…æ‹¬ä¸€ç§é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰ï¼Œä»¥æ¶ˆé™¤ç³»ç»Ÿä»£ç çš„åº•å±‚å¤æ‚æ€§ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªç»“æ„åŒ–æœç´¢ç©ºé—´ï¼Œä»¥åŠAutoGuideï¼Œä¸€ç§å°†åŸå§‹æ‰§è¡Œè¾“å‡ºè§£é‡Šä¸ºå¯æ“ä½œåé¦ˆçš„æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚OpenTunerï¼‰ä¸åŒï¼Œåè€…ä»…ä¾èµ–äºæ ‡é‡åé¦ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä¸­æ‰¾åˆ°æ›´ä¼˜çš„æ˜ å°„å™¨ã€‚ä»…éœ€10æ¬¡è¿­ä»£ï¼Œå³ä½¿åœ¨1000æ¬¡è¿­ä»£åï¼Œå®ƒçš„æ€§èƒ½ä¹Ÿä¼˜äºOpenTunerï¼Œè¾¾åˆ°äº†3.8å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰¾åˆ°çš„æ˜ å°„å™¨åœ¨ä¹ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä¸“å®¶ç¼–å†™çš„æ˜ å°„å™¨ï¼Œæœ€é«˜åŠ é€Ÿ1.34å€ï¼Œå¹¶å°†è°ƒæ•´æ—¶é—´ä»æ•°å¤©ç¼©çŸ­åˆ°æ•°åˆ†é’Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15625v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£ç§‘å­¦å‘ç°å¯¹é«˜æ€§èƒ½è®¡ç®—çš„ä¾èµ–ï¼Œä»¥åŠæé«˜å¹¶è¡Œç¨‹åºæ€§èƒ½çš„å…³é”®æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–å¼€å‘é«˜æ€§èƒ½æ˜ å°„å™¨çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ç”Ÿæˆå¼ä¼˜åŒ–å’Œä¸°å¯Œçš„åé¦ˆæœºåˆ¶ï¼ŒåŒ…æ‹¬Agent-Systemæ¥å£å’ŒAutoGuideæœºåˆ¶ã€‚ä¸ä¼ ç»Ÿä¾èµ–å•ä¸€æ ‡é‡åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨æ›´å°‘çš„è¿­ä»£æ¬¡æ•°ä¸­æ‰¾åˆ°æ›´ä¼˜çš„æ˜ å°„å™¨ï¼Œå°†è°ƒä¼˜æ—¶é—´ä»å‡ å¤©ç¼©çŸ­åˆ°å‡ åˆ†é’Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£ç§‘å­¦å‘ç°ä¾èµ–äºé«˜æ€§èƒ½è®¡ç®—è¿›è¡Œå¤æ‚å»ºæ¨¡å’Œä»¿çœŸã€‚</li>
<li>é«˜æ•ˆåœ°å°†ä»»åŠ¡å’Œæ•°æ®å¤„ç†æ˜ å°„åˆ°å¤„ç†å™¨æ˜¯æå‡å¹¶è¡Œç¨‹åºæ€§èƒ½çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶è‡ªåŠ¨åŒ–å¼€å‘é«˜æ€§èƒ½æ˜ å°„å™¨ï¼Œé‡‡ç”¨ç”Ÿæˆå¼ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«Agent-Systemæ¥å£å’ŒAutoGuideæœºåˆ¶ï¼Œå¯æŠ½è±¡åº•å±‚ç³»ç»Ÿä»£ç çš„å¤æ‚æ€§å¹¶å®šä¹‰ç»“æ„åŒ–æœç´¢ç©ºé—´ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸°å¯Œçš„åé¦ˆæœºåˆ¶ï¼Œä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–æ ‡é‡åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä¸­æ‰¾åˆ°æ€§èƒ½æ›´ä¼˜çš„æ˜ å°„å™¨ï¼Œç”šè‡³è¶…è¶Šä¸“å®¶ç¼–å†™çš„æ˜ å°„å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89c5c7f00541ccd500880fdb0de280d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a93c70201fabe0bcf919c5691cbd2028.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-307fb2c99d98b19169ffd886b5525650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6639de392e35d620ae8dc6ff90daf6bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Agent-Security-Bench-ASB-Formalizing-and-Benchmarking-Attacks-and-Defenses-in-LLM-based-Agents"><a href="#Agent-Security-Bench-ASB-Formalizing-and-Benchmarking-Attacks-and-Defenses-in-LLM-based-Agents" class="headerlink" title="Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and   Defenses in LLM-based Agents"></a>Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and   Defenses in LLM-based Agents</h2><p><strong>Authors:Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang</strong></p>
<p>Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 27 different types of attack&#x2F;defense methods, and 7 evaluation metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory poisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and 11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal critical vulnerabilities in different stages of agent operation, including system prompt, user prompt handling, tool usage, and memory retrieval, with the highest average attack success rate of 84.30%, but limited effectiveness shown in current defenses, unveiling important works to be done in terms of agent security for the community. We also introduce a new metric to evaluate the agentsâ€™ capability to balance utility and security. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/agiresearch/ASB">https://github.com/agiresearch/ASB</a>. </p>
<blockquote>
<p>è™½ç„¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨å·¥å…·å’Œè®°å¿†æœºåˆ¶æ¥è§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½å¼•å…¥å…³é”®çš„å®‰å…¨æ¼æ´ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®å¹¶æ²¡æœ‰å…¨é¢è¯„ä¼°é’ˆå¯¹åŸºäºLLMçš„ä»£ç†çš„æ”»å‡»å’Œé˜²å¾¡æªæ–½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Agent Security Benchï¼ˆASBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§„èŒƒåŒ–ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°åŸºäºLLMçš„ä»£ç†çš„æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•çš„å…¨é¢æ¡†æ¶ï¼ŒåŒ…æ‹¬10ä¸ªåœºæ™¯ï¼ˆä¾‹å¦‚ç”µå­å•†åŠ¡ã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èï¼‰ã€é’ˆå¯¹è¿™äº›åœºæ™¯çš„10ä¸ªä»£ç†ã€è¶…è¿‡400ä¸ªå·¥å…·ã€27ç§ä¸åŒç±»å‹çš„æ”»å‡»&#x2F;é˜²å¾¡æ–¹æ³•ä»¥åŠ7ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚åŸºäºASBï¼Œæˆ‘ä»¬å¯¹10ç§æç¤ºæ³¨å…¥æ”»å‡»ã€ä¸€ç§å†…å­˜ä¸­æ¯’æ”»å‡»ã€ä¸€ç§æ–°å‹çš„è®¡åˆ’æ€ç»´åé—¨æ”»å‡»ã€4ç§æ··åˆæ”»å‡»ä»¥åŠé’ˆå¯¹13ç§LLMéª¨å¹²çš„11ç§ç›¸åº”é˜²å¾¡æªæ–½è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç»“æœæ­ç¤ºäº†ä»£ç†æ“ä½œä¸åŒé˜¶æ®µçš„å…³é”®æ¼æ´ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæç¤ºã€ç”¨æˆ·æç¤ºå¤„ç†ã€å·¥å…·ä½¿ç”¨å’Œè®°å¿†æ£€ç´¢ï¼Œæœ€é«˜å¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾åˆ°84.30%ï¼Œè€Œå½“å‰é˜²å¾¡æªæ–½çš„æœ‰æ•ˆæ€§æœ‰é™ï¼Œè¿™æ­ç¤ºäº†ç¤¾åŒºåœ¨ä»£ç†å®‰å…¨æ–¹é¢è¿˜æœ‰é‡è¦å·¥ä½œè¦åšã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°ä»£ç†åœ¨å®ç”¨æ€§å’Œå®‰å…¨æ€§ä¹‹é—´çš„å¹³è¡¡èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/agiresearch/ASB%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/agiresearch/ASBæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02644v4">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡æ—¶å¯èƒ½å­˜åœ¨çš„å…³é”®å®‰å…¨æ¼æ´ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†Agent Security Benchï¼ˆASBï¼‰æ¡†æ¶ï¼Œç”¨äºå½¢å¼åŒ–ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°LLMä»£ç†çš„æ”»å‡»å’Œé˜²å¾¡æªæ–½ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†ASBçš„è®¾è®¡å’ŒåŠŸèƒ½ï¼Œå¹¶é€šè¿‡å®éªŒæ­ç¤ºäº†ä»£ç†æ“ä½œä¸åŒé˜¶æ®µçš„ä¸¥é‡æ¼æ´ï¼Œå¦‚ç³»ç»Ÿæç¤ºã€ç”¨æˆ·æç¤ºå¤„ç†ã€å·¥å…·ä½¿ç”¨å’Œå†…å­˜æ£€ç´¢ç­‰ã€‚åŒæ—¶æå‡ºäº†è¯„ä»·ä»£ç†æ•ˆç”¨å’Œå®‰å…¨å¹³è¡¡çš„æ–°æŒ‡æ ‡ã€‚æœ‰å…³ä»£ç å¯è®¿é—®ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†äººé¢ä¸´å…³é”®å®‰å…¨æ¼æ´é—®é¢˜ã€‚</li>
<li>Agent Security Benchï¼ˆASBï¼‰æ¡†æ¶ç”¨äºè¯„ä¼°LLMä»£ç†çš„å®‰å…¨æ€§èƒ½ã€‚</li>
<li>ASBæ¶µç›–å¤šç§åœºæ™¯ã€ä»£ç†äººã€å·¥å…·ã€æ”»å‡»&#x2F;é˜²å¾¡æ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>åŸºå‡†æµ‹è¯•ç»“æœæ­ç¤ºäº†ä»£ç†æ“ä½œä¸åŒé˜¶æ®µçš„ä¸¥é‡æ¼æ´ã€‚</li>
<li>å½“å‰é˜²å¾¡æªæ–½çš„æœ‰æ•ˆæ€§æœ‰é™ï¼Œéœ€åŠ å¼ºä»£ç†å®‰å…¨ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†è¯„ä»·ä»£ç†æ•ˆç”¨å’Œå®‰å…¨å¹³è¡¡çš„æ–°æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68f7af7b1a75a01464d3df0a35685c68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93471f2f5bc0057346af104cbf07b944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6afad9b83c8d71ae6838eef48af202d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GUICourse-From-General-Vision-Language-Models-to-Versatile-GUI-Agents"><a href="#GUICourse-From-General-Vision-Language-Models-to-Versatile-GUI-Agents" class="headerlink" title="GUICourse: From General Vision Language Models to Versatile GUI Agents"></a>GUICourse: From General Vision Language Models to Versatile GUI Agents</h2><p><strong>Authors:Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, Maosong Sun</strong></p>
<p>Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at <a target="_blank" rel="noopener" href="https://github.com/yiye3/GUICourse">https://github.com/yiye3/GUICourse</a>. </p>
<blockquote>
<p>åˆ©ç”¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è¿›è¡Œäººæœºäº¤äº’å¯¹äºè®¿é—®å¹¿æ³›çš„æ•°å­—å·¥å…·è‡³å…³é‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœ€æ–°è¿›å±•çªæ˜¾äº†å¼€å‘é€šç”¨ä»£ç†æ¥å¸®åŠ©äººç±»å®ŒæˆGUIå¯¼èˆªä»»åŠ¡çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰VLMé¢ä¸´åŸºç¡€èƒ½åŠ›ï¼ˆOCRå’Œæ¥åœ°èƒ½åŠ›ï¼‰å’ŒGUIçŸ¥è¯†ï¼ˆGUIå…ƒç´ çš„åŠŸèƒ½å’Œæ§åˆ¶æ–¹æ³•ï¼‰æ–¹é¢çš„æŒ‘æˆ˜ï¼Œé˜»ç¢äº†å®ƒä»¬æˆä¸ºå®ç”¨çš„GUIä»£ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è´¡çŒ®äº†GUICourseï¼Œè¿™æ˜¯ä¸€å¥—ç”¨äºä»é€šç”¨VLMè®­ç»ƒåŸºäºè§†è§‰çš„GUIä»£ç†çš„æ•°æ®é›†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†GUIEnvæ•°æ®é›†ï¼Œä»¥åŠ å¼ºVLMçš„OCRå’Œæ¥åœ°èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†GUIActå’ŒGUIChatæ•°æ®é›†ï¼Œä»¥ä¸°å¯Œä»–ä»¬å¯¹GUIç»„ä»¶å’Œäº¤äº’çš„çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GUIä»£ç†åœ¨å¸¸è§çš„GUIä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶åŸºçº¿VLMã€‚å³ä½¿æ˜¯å°å‹çš„GUIä»£ç†ï¼ˆå…·æœ‰3.1Bå‚æ•°ï¼‰ä»ç„¶å¯ä»¥åœ¨å•æ­¥å’Œå¤šæ­¥GUIä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ¶ˆèç ”ç©¶åˆ†æäº†è¯¥ä»£ç†è®­ç»ƒé˜¶æ®µçš„ä¸åŒå˜åŒ–ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ•°æ®é›†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yiye3/GUICourse%E3%80%82">https://github.com/yiye3/GUICourseã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–è¯´æ˜</strong>ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11317v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„äººæœºäº¤äº’å¯¹äºè®¿é—®å¹¿æ³›çš„æ•°å­—å·¥å…·è‡³å…³é‡è¦ã€‚æœ€è¿‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå¼€å‘é€šç”¨ä»£ç†å®ŒæˆGUIå¯¼èˆªä»»åŠ¡çš„æ½œåŠ›å·¨å¤§ã€‚ç„¶è€Œï¼Œå½“å‰VLMé¢ä¸´åŸºç¡€èƒ½åŠ›ï¼ˆå¦‚å…‰å­¦å­—ç¬¦è¯†åˆ«å’Œå®šä½ï¼‰å’ŒGUIçŸ¥è¯†ï¼ˆå¦‚GUIç»„ä»¶çš„åŠŸèƒ½å’Œæ§åˆ¶æ–¹æ³•ï¼‰çš„æŒ‘æˆ˜ï¼Œé˜»ç¢äº†å®ƒä»¬æˆä¸ºå®ç”¨çš„GUIä»£ç†ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºGUICourseå¥—ä»¶ï¼Œç”¨äºä»é€šç”¨VLMè®­ç»ƒè§†è§‰åŸºç¡€GUIä»£ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥GUIEnvæ•°æ®é›†åŠ å¼ºVLMçš„å…‰å­¦å­—ç¬¦è¯†åˆ«å’Œå®šä½èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥GUIActå’ŒGUIChatæ•°æ®é›†ä¸°å¯Œå…¶å¯¹GUIç»„ä»¶å’Œäº¤äº’çš„çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GUIä»£ç†åœ¨å¸¸è§GUIä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºç¡€VLMã€‚ç”šè‡³å°å‹GUIä»£ç†ï¼ˆ3.1Bå‚æ•°ï¼‰åœ¨å•æ­¥å’Œå¤šæ­¥GUIä»»åŠ¡ä¸Šä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIåœ¨æ•°å­—å·¥å…·è®¿é—®ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>VLMåœ¨å¼€å‘GUIå¯¼èˆªä»»åŠ¡ä»£ç†æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰VLMé¢ä¸´OCRå’Œå®šä½èƒ½åŠ›ä»¥åŠGUIçŸ¥è¯†çš„æŒ‘æˆ˜ã€‚</li>
<li>GUICourseå¥—ä»¶åŒ…æ‹¬GUIEnvã€GUIActå’ŒGUIChatæ•°æ®é›†ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>GUIä»£ç†åœ¨å¸¸è§GUIä»»åŠ¡ä¸Šçš„æ€§èƒ½ç»è¿‡å®éªŒéªŒè¯ä¼˜äºåŸºç¡€VLMã€‚</li>
<li>å°å‹GUIä»£ç†ä¹Ÿèƒ½æœ‰æ•ˆå¤„ç†å•æ­¥å’Œå¤šæ­¥GUIä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de6fb6a5d3ff2d23470e048a56192674.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24128f9910b7240121d35ada16de3121.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30b9f58fb937927b0fe8b7e25ea432dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c04912a519f65f12af860e9b566fbd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83146464c28444df7e3e8fd38e9d5b44.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7152e85b345e5d4ffe44920b1368a8e0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Chameleon A MatMul-Free Temporal Convolutional Network Accelerator for   End-to-End Few-Shot and Continual Learning from Sequential Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e8a968a21b03aa19e3527fc3184a28a5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Open CaptchaWorld A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
