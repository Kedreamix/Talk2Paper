<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Agent-X Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3c64add917f44249521790e958396869.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="Agent-X-Evaluating-Deep-Multimodal-Reasoning-in-Vision-Centric-Agentic-Tasks"><a href="#Agent-X-Evaluating-Deep-Multimodal-Reasoning-in-Vision-Centric-Agentic-Tasks" class="headerlink" title="Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks"></a>Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks</h2><p><strong>Authors:Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan</strong></p>
<p>Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents multi-step and deep reasoning capabilities in real-world, multimodal settings. Agent- X features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models. Our data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/Agent-X">https://github.com/mbzuai-oryx/Agent-X</a> </p>
<blockquote>
<p>æ·±åº¦æ¨ç†æ˜¯è§£å†³å¤æ‚ä»»åŠ¡çš„åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œéœ€è¦è¿ç»­çš„å¤šæ¨¡å¼ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä½¿ç”¨å®Œå…¨åˆæˆã€å•è½®æŸ¥è¯¢è¿›è¡Œè¯„ä¼°ï¼Œå…·æœ‰æœ‰é™çš„è§†è§‰æ¨¡å¼ï¼Œå¹¶ä¸”ç¼ºä¹è¯„ä¼°å¤šä¸ªæ­¥éª¤ä¸­çš„æ¨ç†è´¨é‡çš„æ¡†æ¶ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­æ˜¯å¿…éœ€çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Agent-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»£ç†åœ¨ç°å®ä¸–ç•Œå¤šæ¨¡å¼è®¾ç½®ä¸­çš„å¤šæ­¥éª¤å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚Agent-Xæ‹¥æœ‰828ä¸ªä»£ç†ä»»åŠ¡ï¼Œå…·æœ‰çœŸå®è§†è§‰ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬å›¾åƒã€å¤šå›¾åƒæ¯”è¾ƒã€è§†é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†å…­å¤§ä»£ç†ç¯å¢ƒï¼šé€šç”¨è§†è§‰æ¨ç†ã€ç½‘é¡µæµè§ˆã€å®‰å…¨å’Œç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è¿åŠ¨å’Œæ•°å­¦æ¨ç†ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¦æ±‚ä»£ç†åœ¨è¿™äº›ä¸åŒçš„ç¯å¢ƒä¸­å°†å·¥å…·ä½¿ç”¨ä¸æ˜ç¡®çš„åˆ†æ­¥éª¤å†³ç­–ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç²¾ç»†çš„ã€æ­¥éª¤çº§çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å’Œé€»è¾‘æ€§ï¼Œä»¥åŠåœ¨æ•´ä¸ªä»»åŠ¡ä¸­å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTã€åŒå­åº§å’ŒQwenç³»åˆ—ï¼Œåœ¨è§£å†³å¤šæ­¥éª¤è§†è§‰ä»»åŠ¡æ—¶ä¹Ÿé¢ä¸´å›°éš¾ï¼Œå…¨ç¨‹æˆåŠŸå®Œæˆç‡ä½äº50%ã€‚è¿™äº›å‘ç°çªå‡ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„å…³é”®ç“¶é¢ˆï¼Œå¹¶ç¡®å®šäº†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»£ç†æ¨ç†æ¨¡å‹æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/Agent-X">https://github.com/mbzuai-oryx/Agent-X</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24876v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œæ·±åº¦æ¨ç†æ˜¯è§£å†³å¤æ‚ä»»åŠ¡çš„å…³é”®ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤å’Œå¤šæ¨¡å¼ç†è§£çš„åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä½¿ç”¨å…¨åˆæˆã€å•å›åˆæŸ¥è¯¢å’Œæœ‰é™çš„è§†è§‰æ¨¡å¼è¿›è¡Œè¯„ä¼°ï¼Œç¼ºä¹è¯„ä¼°å¤šæ­¥éª¤æ¨ç†è´¨é‡çš„æ¡†æ¶ï¼Œæ— æ³•æ»¡è¶³çœŸå®ä¸–ç•Œçš„éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Agent-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“åœ¨å¤šæ­¥éª¤æ·±åº¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æµ‹è¯•åŒ…å«828ä¸ªçœŸå®è§†è§‰ç¯å¢ƒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€å¤šå›¾åƒå¯¹æ¯”ã€è§†é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¦æ±‚æ™ºèƒ½ä½“åœ¨è¿™äº›ä¸åŒçš„ç¯å¢ƒä¸­æ•´åˆå·¥å…·ä½¿ç”¨å’Œæ˜ç¡®çš„é€æ­¥å†³ç­–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç²¾ç»†çš„ã€æ­¥éª¤çº§çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å’Œé€»è¾‘æ€§ï¼Œä»¥åŠåœ¨æ•´ä¸ªä»»åŠ¡ä¸­å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå¦‚GPTã€åŒå­åº§å’ŒQwenç³»åˆ—ï¼Œåœ¨è§£å†³å¤šæ­¥éª¤è§†è§‰ä»»åŠ¡æ—¶ä¹Ÿé¢ä¸´å›°éš¾ï¼Œå…¨ç¨‹æˆåŠŸç‡ä½äº50%ã€‚è¿™çªæ˜¾äº†å½“å‰è§†è§‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“æ¨ç†æ¨¡å‹çš„ç“¶é¢ˆï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦æ¨ç†åœ¨è§£å†³ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šæ­¥éª¤å¤æ‚ä»»åŠ¡ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦ä½¿ç”¨åˆæˆæ•°æ®å’Œå•å›åˆæŸ¥è¯¢è¿›è¡Œè¯„ä¼°ï¼Œç¼ºä¹çœŸå®ä¸–ç•Œçš„å¤šæ­¥éª¤æ¨ç†è´¨é‡è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>Agent-Xæ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤šæ­¥éª¤æ·±åº¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>Agent-XåŒ…å«çœŸå®è§†è§‰ç¯å¢ƒçš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ç­‰ã€‚</li>
<li>Agent-Xè¦æ±‚æ™ºèƒ½ä½“åœ¨å¤šç§ç¯å¢ƒä¸­æ•´åˆå·¥å…·ä½¿ç”¨å’Œé€æ­¥å†³ç­–ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶æ³¨é‡æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å’Œé€»è¾‘æ€§ï¼Œä»¥åŠå·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60d65302a1333ad37a1a7b9cbed55f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aba625aae8519a40f2dd7047e11a53e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2241f78966b3e7e570c118504777c876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c8001b01cf8a933a66954e44fbae171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1463efbd6ab814f7dd6069e1793e5d00.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ReasonGen-R1-CoT-for-Autoregressive-Image-generation-models-through-SFT-and-RL"><a href="#ReasonGen-R1-CoT-for-Autoregressive-Image-generation-models-through-SFT-and-RL" class="headerlink" title="ReasonGen-R1: CoT for Autoregressive Image generation models through SFT   and RL"></a>ReasonGen-R1: CoT for Autoregressive Image generation models through SFT   and RL</h2><p><strong>Authors:Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu</strong></p>
<p>Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based â€œthinkingâ€ skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms&#x2F;reasongen. </p>
<blockquote>
<p>å°½ç®¡åŸºäºæ€ç»´é“¾æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå–å¾—äº†çªç ´ï¼Œä½†å®ƒä»¬èå…¥ç”Ÿæˆå¼è§†è§‰æ¨¡å‹çš„ç ”ç©¶ä»ç„¶ä¸å¤Ÿå……åˆ†ã€‚æˆ‘ä»¬å¼•å…¥äº†ReasonGen-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡åœ¨æ–°ç”Ÿæˆçš„åŸºäºæ–‡æœ¬æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œå°†ä¸€ä¸ªè‡ªå›å½’å›¾åƒç”Ÿæˆå™¨èµ‹äºˆæ˜ç¡®çš„åŸºäºæ–‡æœ¬çš„â€œæ€è€ƒâ€æŠ€èƒ½ï¼Œç„¶åä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å¯¹å…¶è¾“å‡ºè¿›è¡Œæ”¹è¿›ã€‚ä¸ºäº†èƒ½å¤Ÿè®©æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒä¹‹å‰é€šè¿‡æ–‡æœ¬è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬è‡ªåŠ¨ç”Ÿæˆå¹¶å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸è§†è§‰æç¤ºé…å¯¹çš„äººå·¥æ„å»ºæ¨ç†è¯­æ–™åº“ï¼Œå®ç°å¯¹ç‰©ä½“å¸ƒå±€ã€é£æ ¼å’Œåœºæ™¯ç»„æˆçš„å¯æ§è§„åˆ’ã€‚æˆ‘ä»¬çš„GRPOç®—æ³•ä½¿ç”¨æ¥è‡ªé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¥–åŠ±ä¿¡å·æ¥è¯„ä¼°æ€»ä½“è§†è§‰è´¨é‡ï¼Œå¹¶åœ¨æ¯æ¬¡æ›´æ–°ä¸­ä¼˜åŒ–ç­–ç•¥ã€‚åœ¨GenEvalã€DPGå’ŒT2IåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒReasonGen-R1æŒç»­ä¼˜äºå¼ºåŠ²çš„åŸºçº¿æ¨¡å‹å’Œä¹‹å‰çš„æœ€æ–°æ¨¡å‹ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®aka.ms&#x2F;reasongenã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24875v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºReasonGen-R1çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå®ƒå°†é“¾å¼æ€ç»´æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œåº”ç”¨äºç”Ÿæˆå¼è§†è§‰æ¨¡å‹ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åœ¨æ–°ç”Ÿæˆçš„åŸºäºæ–‡æœ¬ç†æ€§çš„ç›‘ç£å¾®è°ƒæ•°æ®ä¸Šèµ‹äºˆè‡ªå›å½’å›¾åƒç”Ÿæˆå™¨æ˜ç¡®çš„åŸºäºæ–‡æœ¬çš„â€œæ€è€ƒâ€æŠ€èƒ½ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–æ¥ä¼˜åŒ–è¾“å‡ºã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡æœ¬è¿›è¡Œæ¨ç†å†ç”Ÿæˆå›¾åƒï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆä¸è§†è§‰æç¤ºé…å¯¹çš„æ•°æ®é›†ï¼Œä»¥å®ç°å¯¹è±¡å¸ƒå±€ã€é£æ ¼å’Œåœºæ™¯ç»„æˆçš„å—æ§è§„åˆ’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒReasonGen-R1åœ¨GenEvalã€DPGå’ŒT2IåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹å’Œå…ˆå‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ReasonGen-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é“¾å¼æ€ç»´æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œåº”ç”¨äºç”Ÿæˆå¼è§†è§‰æ¨¡å‹ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒèµ‹äºˆè‡ªå›å½’å›¾åƒç”Ÿæˆå™¨æ˜ç¡®çš„åŸºäºæ–‡æœ¬çš„â€œæ€è€ƒâ€æŠ€èƒ½ï¼Œä½¿ç”¨æ–°ç”Ÿæˆçš„åŸºäºæ–‡æœ¬ç†æ€§çš„æ•°æ®é›†ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µä½¿ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ä¼˜åŒ–è¾“å‡ºï¼Œè¯¥ç®—æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ¥è¯„ä¼°æ•´ä½“è§†è§‰è´¨é‡ï¼Œå¹¶åœ¨æ¯æ¬¡æ›´æ–°ä¸­ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°é€šè¿‡æ–‡æœ¬è¿›è¡Œæ¨ç†å†ç”Ÿæˆå›¾åƒçš„è¿‡ç¨‹ã€‚</li>
<li>è‡ªåŠ¨ç”Ÿæˆä¸è§†è§‰æç¤ºé…å¯¹çš„æ•°æ®é›†ï¼Œå®ç°å¯¹è±¡å¸ƒå±€ã€é£æ ¼å’Œåœºæ™¯ç»„æˆçš„å—æ§è§„åˆ’ã€‚</li>
<li>ReasonGen-R1åœ¨GenEvalã€DPGå’ŒT2IåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b99cc8f669a49af463045581cc55027e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-debe48d58b784601f6665116c06c313d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12d1ec7c0d090cd20bf6d259edd203a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43f02e83ac37ad95ab0cea9104727f8f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ProxyThinker-Test-Time-Guidance-through-Small-Visual-Reasoners"><a href="#ProxyThinker-Test-Time-Guidance-through-Small-Visual-Reasoners" class="headerlink" title="ProxyThinker: Test-Time Guidance through Small Visual Reasoners"></a>ProxyThinker: Test-Time Guidance through Small Visual Reasoners</h2><p><strong>Authors:Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, Vicente Ordonez</strong></p>
<p>Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MrZilinXiao/ProxyThinker">https://github.com/MrZilinXiao/ProxyThinker</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è¿›å±•æ¨åŠ¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è§†è§‰æ¨ç†èƒ½åŠ›çš„è¾¹ç•Œã€‚ç„¶è€Œï¼Œä½¿ç”¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è®­ç»ƒLVLMsçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œç»™æ¨¡å‹è§„æ¨¡æ‰©å±•å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ProxyThinkerï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ—¶é—´æŠ€æœ¯ï¼Œèƒ½å¤Ÿè®©å¤§å‹æ¨¡å‹ä»å°å‹ã€æ…¢æ€è€ƒçš„è§†è§‰æ¨ç†å™¨ç»§æ‰¿è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚é€šè¿‡ä»åŸºå‡†æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¸­å‡å»RFTæ¨ç†å™¨çš„è¾“å‡ºåˆ†å¸ƒï¼ŒProxyThinkerä¿®æ”¹äº†è§£ç åŠ¨æ€ï¼Œå¹¶æˆåŠŸæ¿€å‘äº†ä»¥è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘æ ¡æ­£ç­‰å‡ºç°çš„å¤æ‚è¡Œä¸ºæ‰€å±•ç¤ºçš„æ…¢æ€è€ƒæ¨ç†ã€‚ProxyThinkeråœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºé—´ã€æ•°å­¦å’Œå¤šå­¦ç§‘æ¨ç†è§†è§‰åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆæå‡æ€§èƒ½ï¼Œä½¿æœªè°ƒæ•´çš„åŸºå‡†æ¨¡å‹èƒ½å¤Ÿä¸å…¨å°ºå¯¸RFTæ¨¡å‹çš„æ€§èƒ½ç›¸ç«äº‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®ç°é€šè¿‡å¹¶è¡ŒæŠ€æœ¯é«˜æ•ˆåœ°åè°ƒäº†å¤šä¸ªè¯­è¨€æ¨¡å‹ï¼Œä¸ä»¥å‰çš„è§£ç æ—¶é—´æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾38å€çš„å¿«é€Ÿæ¨ç†ï¼Œä¸ºProxyThinkerçš„å®é™…éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/MrZilinXiao/ProxyThinker%E3%80%82">https://github.com/MrZilinXiao/ProxyThinkerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24872v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ çš„å‘å±•æ¨åŠ¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è§†è§‰æ¨ç†èƒ½åŠ›çš„å‰æ²¿ï¼Œä½†è®­ç»ƒè¿™äº›æ¨¡å‹ä½¿ç”¨å¼ºåŒ–å¾®è°ƒåœ¨è®¡ç®—ä¸Šä»£ä»·é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ProxyThinkerï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ—¶é—´æŠ€æœ¯ï¼Œèƒ½è®©å¤§å‹æ¨¡å‹ç»§æ‰¿å°å‹æ…¢é€Ÿè§†è§‰æ¨ç†è€…çš„è§†è§‰æ¨ç†èƒ½åŠ›è€Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚é€šè¿‡ä»åŸºç¡€æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¸­å‡å»ç»è¿‡å¼ºåŒ–å¾®è°ƒæ¨ç†è€…çš„è¾“å‡ºåˆ†å¸ƒï¼ŒProxyThinkeræˆåŠŸæ¿€å‘äº†è‡ªæˆ‘éªŒè¯å’Œæ ¡æ­£ç­‰å¤æ‚è¡Œä¸ºæ‰€è¡¨ç°å‡ºçš„æ…¢é€Ÿæ¨ç†ã€‚ProxyThinkeråœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆæå‡äº†æ€§èƒ½ï¼Œä½¿å¾—æœªç»è°ƒæ•´çš„åŸºç¡€æ¨¡å‹èƒ½å¤Ÿä¸å…¨å°ºå¯¸å¼ºåŒ–å¾®è°ƒæ¨¡å‹ç›¸ç«äº‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„å®ç°ä½¿ç”¨å¹¶è¡ŒæŠ€æœ¯é«˜æ•ˆåœ°åè°ƒå¤šä¸ªè¯­è¨€æ¨¡å‹ï¼Œä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”å¯å®ç°æœ€é«˜è¾¾38å€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œä¸ºProxyThinkerçš„å®é™…éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ¨åŠ¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚</li>
<li>è®­ç»ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½¿ç”¨å¼ºåŒ–å¾®è°ƒåœ¨è®¡ç®—ä¸Šä»£ä»·é«˜æ˜‚ï¼Œå­˜åœ¨è§„æ¨¡åŒ–æŒ‘æˆ˜ã€‚</li>
<li>ProxyThinkeræ˜¯ä¸€ç§æ¨ç†æ—¶é—´æŠ€æœ¯ï¼Œå…è®¸å¤§å‹æ¨¡å‹ç»§æ‰¿å°å‹æ…¢é€Ÿè§†è§‰æ¨ç†æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ProxyThinkeré€šè¿‡è°ƒæ•´è§£ç åŠ¨æ€æ¿€å‘å¤æ‚è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘éªŒè¯å’Œæ ¡æ­£ã€‚</li>
<li>ProxyThinkeræé«˜äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œä½¿åŸºç¡€æ¨¡å‹èƒ½ä¸å…¨å°ºå¯¸å¼ºåŒ–å¾®è°ƒæ¨¡å‹ç«äº‰ã€‚</li>
<li>ProxyThinkerå®ç°äº†é«˜æ•ˆçš„å¤šä¸ªè¯­è¨€æ¨¡å‹å¹¶è¡Œåè°ƒï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-520fc3f692b0006db5d8ec7e87f5421d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e803eab2c3fd8da97d017186c77f8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6405cc9b964ce6c24ca6965369c2041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78500c238b65833f98fcf18a408a9e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1c4c90c372af9eeda627718b8b7c4c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dd7e0e9087efe58056b14ae37828a76.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MoDoMoDo-Multi-Domain-Data-Mixtures-for-Multimodal-LLM-Reinforcement-Learning"><a href="#MoDoMoDo-Multi-Domain-Data-Mixtures-for-Multimodal-LLM-Reinforcement-Learning" class="headerlink" title="MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement   Learning"></a>MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement   Learning</h2><p><strong>Authors:Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained modelâ€™s accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline. </p>
<blockquote>
<p>ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æœ€è¿‘ä½œä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åçš„ä¸€ç§å¼ºå¤§èŒƒå¼è€Œå‡ºç°ï¼Œå…¶åœ¨å…·æœ‰ç»“æ„åŒ–å’Œå¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å¸¦æ¥äº†å·¨å¤§çš„æœºä¼šï¼Œä½†ç”±äºéœ€è¦å¾®å¦™çš„è§†è§‰ã€é€»è¾‘å’Œç©ºé—´èƒ½åŠ›çš„è§†è§‰è¯­è¨€ä»»åŠ¡çš„å¹¿æ³›æ€§å’Œå¼‚è´¨æ€§ï¼Œä½¿å…¶å¤æ‚åŒ–ã€‚å› æ­¤ï¼Œä½¿ç”¨RLVRåœ¨å¤šæ•°æ®é›†ä¸Šè®­ç»ƒMLLMå¯èƒ½æœ‰ç›Šï¼Œä½†åˆ›å»ºäº†æ¥è‡ªä¸åŒæ•°æ®é›†ä¹‹é—´äº¤äº’çš„ç›¸äº’å†²çªçš„ç›®æ ‡æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æœ€ä½³çš„æ•°æ®é›†æ··åˆç­–ç•¥æ¥æé«˜æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºå¤šæ¨¡æ€LLM RLVRå¼•å…¥äº†ç³»ç»Ÿçš„åè®­ç»ƒæ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¥æ ¼çš„æ•°æ®æ··åˆé—®é¢˜å…¬å¼å’ŒåŸºå‡†å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ€RLVRæ¡†æ¶ï¼Œç”¨äºå¤šæ•°æ®é›†çš„åè®­ç»ƒï¼Œé€šè¿‡æ•´ç†åŒ…å«ä¸åŒå¯éªŒè¯çš„è§†è§‰è¯­è¨€é—®é¢˜çš„æ•°æ®é›†ï¼Œå¹¶å¯ç”¨å…·æœ‰ä¸åŒå¯éªŒè¯å¥–åŠ±çš„å¤šåŸŸåœ¨çº¿RLå­¦ä¹ ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®æ··åˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥å­¦ä¹ é¢„æµ‹RLå¾®è°ƒç»“æœï¼Œä»è€Œä¼˜åŒ–æœ€ä½³æ··åˆã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œå½“å¤šåŸŸRLVRè®­ç»ƒä¸æ··åˆé¢„æµ‹ç­–ç•¥ç›¸ç»“åˆæ—¶ï¼Œå¯ä»¥æ˜¾ç€æé«˜MLLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æœ€ä½³ç»„åˆåœ¨è¶…å‡ºåˆ†å¸ƒçš„åŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸ç»Ÿä¸€æ•°æ®æ··åˆåè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œå¹³å‡æé«˜äº†5.24%çš„ç²¾åº¦ï¼Œä¸å¾®è°ƒå‰çš„åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œæ€»æé«˜äº†20.74%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24871v1">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://modomodo-rl.github.io/">https://modomodo-rl.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç»“æ„åŒ–å’Œå¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡æ–¹é¢ã€‚è™½ç„¶å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å…·æœ‰æ˜¾è‘—çš„æœºä¼šï¼Œä½†è¿™ä¸€é¢†åŸŸä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šéœ€è¦æ›´åŠ å¤æ‚çš„æ··åˆç­–ç•¥æ¥å¤„ç†å„ç§æ•°æ®é›†çš„ç›®æ ‡å†²çªé—®é¢˜ã€‚è¯¥æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€LLMçš„RLVRç³»ç»Ÿæ€§åè®­ç»ƒæ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬å¯¹æ•°æ®æ··åˆé—®é¢˜çš„ä¸¥è°¨è¡¨è¿°å’ŒåŸºå‡†å®ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼€å‘åŒ…å«ä¸åŒå¯éªŒè¯è§†è§‰è¯­è¨€é—®é¢˜çš„æ•°æ®é›†å’Œå¤šåŸŸåœ¨çº¿RLå­¦ä¹ æŠ€æœ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œç»“åˆæ•°æ®æ··åˆé¢„æµ‹ç­–ç•¥çš„å¤šåŸŸRLVRè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜MLLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚ä¸å‡åŒ€æ•°æ®æ··åˆç›¸æ¯”ï¼Œæœ€ä½³æ•°æ®æ··åˆç­–ç•¥å¯ä»¥æé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–çš„å‡†ç¡®æ€§å¹³å‡è¾¾5.24%ï¼Œä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œæ€»æé«˜è¾¾20.74%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤„ç†ç»“æ„åŒ–ç­”æ¡ˆä»»åŠ¡æ—¶ã€‚</li>
<li>å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†éœ€è§£å†³è§†è§‰è¯­è¨€ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¼‚è´¨æ€§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹MLLMçš„RLVRç³»ç»Ÿæ€§åè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¥è°¨çš„æ•°æ®æ··åˆé—®é¢˜è¡¨è¿°å’ŒåŸºå‡†å®ç°ã€‚</li>
<li>é€šè¿‡å¼€å‘åŒ…å«ä¸åŒå¯éªŒè¯è§†è§‰è¯­è¨€é—®é¢˜çš„æ•°æ®é›†å’Œå¤šåŸŸåœ¨çº¿RLå­¦ä¹ æŠ€æœ¯æ¥è§£å†³æ•°æ®æ··åˆé—®é¢˜ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå¤šåŸŸRLVRè®­ç»ƒç»“åˆæ•°æ®æ··åˆé¢„æµ‹ç­–ç•¥èƒ½æ˜¾è‘—æé«˜MLLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€ä½³æ•°æ®æ··åˆç­–ç•¥èƒ½æ˜¾è‘—æé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºå‡åŒ€æ•°æ®æ··åˆæå‡å¹³å‡è¾¾5.24%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9625bffcac259fd4cb6696f87565b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ef819a3443b217f2c0c7108ce1a507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f8ab7915e69f7c548212d74efff16f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-096ebf3e83b7be1947c58dff11275cc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59515d968849f0665ebb498fd7d87148.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SiLVR-A-Simple-Language-based-Video-Reasoning-Framework"><a href="#SiLVR-A-Simple-Language-based-Video-Reasoning-Framework" class="headerlink" title="SiLVR: A Simple Language-based Video Reasoning Framework"></a>SiLVR: A Simple Language-based Video Reasoning Framework</h2><p><strong>Authors:Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius</strong></p>
<p>Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio&#x2F;speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CeeZh/SILVR">https://github.com/CeeZh/SILVR</a>. </p>
<blockquote>
<p>æœ€è¿‘æµ‹è¯•æ—¶ä¼˜åŒ–çš„è¿›å±•ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè§£å†³æ•°å­¦å’Œç¼–ç ä¸­çš„é«˜åº¦å¤æ‚é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤šæ¨¡å¼LLMï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä»ç„¶å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„è§†é¢‘è¯­è¨€ä»»åŠ¡æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç®€å•è¯­è¨€çš„è§†é¢‘æ¨ç†æ¡†æ¶SiLVRï¼Œå®ƒå°†å¤æ‚çš„è§†é¢‘ç†è§£åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒSiLVRä½¿ç”¨å¤šæ„Ÿå®˜è¾“å…¥ï¼Œå¦‚çŸ­è§†é¢‘å­—å¹•å’ŒéŸ³é¢‘&#x2F;è¯­éŸ³å­—å¹•ï¼Œå°†åŸå§‹è§†é¢‘è½¬æ¢ä¸ºåŸºäºè¯­è¨€çš„è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†è¯­è¨€æè¿°è¾“å…¥åˆ°å¼ºå¤§çš„æ¨ç†LLMä¸­ï¼Œä»¥è§£å†³å¤æ‚çš„è§†é¢‘è¯­è¨€ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„å¤šæ„Ÿå®˜è¾“å…¥ï¼Œæˆ‘ä»¬é‡‡ç”¨è‡ªé€‚åº”ä»¤ç‰Œç¼©å‡æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŠ¨æ€ç¡®å®šä»¤ç‰Œçš„æ—¶åºç²’åº¦ã€‚æˆ‘ä»¬ç®€å•ã€æ¨¡å—åŒ–ä¸”æ— éœ€è®­ç»ƒçš„è§†é¢‘æ¨ç†æ¡†æ¶åœ¨Video-MMEï¼ˆé•¿ï¼‰ã€Video-MMMUï¼ˆç†è§£ï¼‰ã€Video-MMLUã€CGBenchå’ŒEgoLifeä¸Šå–å¾—äº†æœ€ä½³æŠ¥å‘Šç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹è§†é¢‘æ¨ç†èƒ½åŠ›çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æœªå¯¹è§†é¢‘è¿›è¡Œæ˜ç¡®è®­ç»ƒï¼Œä½†å¼ºå¤§çš„æ¨ç†LLMå¯ä»¥æœ‰æ•ˆåœ°èšåˆæ¥è‡ªè§†é¢‘ã€è¯­éŸ³å’ŒéŸ³é¢‘çš„å¤šæ„Ÿå®˜è¾“å…¥ä¿¡æ¯ï¼Œç”¨äºå¤æ‚çš„æ—¶åºã€å› æœã€é•¿ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†è·å–è§†é¢‘æ¨ç†ä»»åŠ¡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CeeZh/SILVR">https://github.com/CeeZh/SILVR</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24869v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•æ—¶é—´ä¼˜åŒ–æ–¹é¢çš„æœ€æ–°è¿›å±•ä½¿å¾—è§£å†³æ•°å­¦å’Œç¼–ç ä¸­çš„é«˜åº¦å¤æ‚é—®é¢˜æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå¤šåª’ä½“LLMsçš„æ¨ç†èƒ½åŠ›åœ¨å¤„ç†å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡æ—¶ä»å­˜åœ¨æ˜¾è‘—æ»åã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºç®€å•è¯­è¨€çš„è§†é¢‘æ¨ç†æ¡†æ¶SiLVRï¼Œå°†å¤æ‚çš„è§†é¢‘ç†è§£åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤šåª’ä½“è¾“å…¥å°†åŸå§‹è§†é¢‘è½¬æ¢ä¸ºåŸºäºè¯­è¨€çš„è¡¨ç¤ºå½¢å¼ï¼›ç¬¬äºŒé˜¶æ®µå°†è¯­è¨€æè¿°è¾“å…¥å¼ºå¤§çš„æ¨ç†LLMæ¥è§£å†³å¤æ‚çš„è§†é¢‘è¯­è¨€ç†è§£ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è‡ªé€‚åº”ä»¤ç‰Œç¼©å‡æ–¹æ¡ˆèƒ½å¤Ÿå¤„ç†é•¿ä¸Šä¸‹æ–‡å¤šåª’ä½“è¾“å…¥ï¼ŒåŠ¨æ€ç¡®å®šä»¤ç‰Œçš„æ—¶åºç²’åº¦ã€‚SiLVRåœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æˆç»©ï¼Œå®è¯ç ”ç©¶è¡¨æ˜ï¼Œå¼ºå¤§çš„æ¨ç†LLMèƒ½å¤Ÿæœ‰æ•ˆåœ°èšåˆæ¥è‡ªè§†é¢‘ã€è¯­éŸ³å’ŒéŸ³é¢‘çš„å¤šæ„Ÿå®˜è¾“å…¥ä¿¡æ¯ï¼Œä»¥å®Œæˆå¤æ‚çš„æ—¶ç©ºã€å› æœã€é•¿æœŸçŸ¥è¯†å’Œè·å–è§†é¢‘æ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•æ—¶é—´ä¼˜åŒ–æ–¹é¢å–å¾—è¿›å±•ï¼Œå¢å¼ºäº†è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>å¤šåª’ä½“LLMsåœ¨å¤„ç†å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡æ—¶ä»å­˜åœ¨æ¨ç†èƒ½åŠ›æ»åçš„é—®é¢˜ã€‚</li>
<li>SiLVRæ¡†æ¶æ˜¯ä¸€ç§åŸºäºç®€å•è¯­è¨€çš„è§†é¢‘æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¤šåª’ä½“è¾“å…¥çš„è§†é¢‘è¯­è¨€è½¬æ¢å’Œå¼ºå¤§çš„æ¨ç†LLMçš„è¯­è¨€æè¿°è¾“å…¥ã€‚</li>
<li>è‡ªé€‚åº”ä»¤ç‰Œç¼©å‡æ–¹æ¡ˆèƒ½å¤Ÿå¤„ç†é•¿ä¸Šä¸‹æ–‡å¤šåª’ä½“è¾“å…¥ã€‚</li>
<li>SiLVRåœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æˆç»©ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œå¼ºå¤§çš„æ¨ç†LLMèƒ½å¤Ÿèšåˆå¤šæ„Ÿå®˜è¾“å…¥ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5cadbe5ccc6164cfa0d21226763504bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56eb0df568b23487df4470ab919bf070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d31ae7f0dad7a59bdbebd3000bef912.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb3803b646f36104015bf8631de0b7c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fbbc9524b8a08a326278fb49d58ce58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-208a31d122d4e2d68248ddf651141907.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ProRL-Prolonged-Reinforcement-Learning-Expands-Reasoning-Boundaries-in-Large-Language-Models"><a href="#ProRL-Prolonged-Reinforcement-Learning-Expands-Reasoning-Boundaries-in-Large-Language-Models" class="headerlink" title="ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in   Large Language Models"></a>ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in   Large Language Models</h2><p><strong>Authors:Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong</strong></p>
<p>Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a modelâ€™s reasoning capabilities or merely amplifies high-reward outputs already latent in the base modelâ€™s distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: <a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B">https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹è¿›å±•çªå‡ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ä¸å¯éªŒè¯å¥–åŠ±å¯¹é½æ¨¡å‹çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯å¦çœŸæ­£æ‰©å±•äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ–è€…åªæ˜¯æ”¾å¤§äº†åŸºç¡€æ¨¡å‹ä¸­å·²ç»å­˜åœ¨çš„é«˜å¥–åŠ±è¾“å‡ºï¼Œä»¥åŠæŒç»­æ‰©å¤§å¼ºåŒ–å­¦ä¹ çš„è®¡ç®—è§„æ¨¡æ˜¯å¦å¯é åœ°å¯¼è‡´æ¨ç†æ€§èƒ½çš„æå‡ï¼Œè¿™äº›é—®é¢˜ä»ç„¶å­˜åœ¨äº‰è®®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¯æ˜é•¿æœŸå¼ºåŒ–å­¦ä¹ ï¼ˆProRLï¼‰è®­ç»ƒå¯ä»¥æ­ç¤ºåŸºç¡€æ¨¡å‹æ— æ³•è®¿é—®çš„æ–°æ¨ç†ç­–ç•¥ï¼Œä»è€ŒæŒ‘æˆ˜äº†æµè¡Œçš„å‡è®¾ï¼Œå³ä½¿åœ¨å¤§èŒƒå›´é‡‡æ ·ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹è®­ç»ƒæ–¹æ³•ProRLï¼Œå®ƒç»“åˆäº†KLæ•£åº¦æ§åˆ¶ã€å‚è€ƒç­–ç•¥é‡ç½®å’Œä¸€ç³»åˆ—å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å‹åœ¨ä¸€ç³»åˆ—pass@kè¯„ä¼°ä¸­å§‹ç»ˆä¼˜äºåŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å®Œå…¨å¤±è´¥çš„åœºæ™¯ï¼Œæ— è®ºå°è¯•æ¬¡æ•°å¦‚ä½•ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæ¨ç†è¾¹ç•Œçš„æ”¹è¿›ä¸åŸºç¡€æ¨¡å‹çš„ä»»åŠ¡èƒ½åŠ›å’Œè®­ç»ƒæŒç»­æ—¶é—´å¯†åˆ‡ç›¸å…³ï¼Œè¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ å¯ä»¥éšç€æ—¶é—´çš„æ¨ç§»æ¢ç´¢å¹¶å¡«å……æ–°çš„è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚è¿™äº›å‘ç°ä¸ºæˆ‘ä»¬æä¾›äº†åœ¨ä½•ç§æ¡ä»¶ä¸‹å¼ºåŒ–å­¦ä¹ æœ‰æ„ä¹‰åœ°æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ¨ç†è¾¹ç•Œçš„æ–°è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å…³äºé•¿æœŸå¼ºåŒ–å­¦ä¹ çš„æ¨ç†å·¥ä½œå¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬å‘å¸ƒæ¨¡å‹æƒé‡ä»¥æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-%Qwen-1.5B">https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24864v1">PDF</a> 26 pages, 17 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä¸­å¿ƒè¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡é€šè¿‡ProRLè®­ç»ƒæ–¹æ³•æ­ç¤ºäº†RLåœ¨å‘æ˜åŸºç¡€æ¨¡å‹æ— æ³•è·å–çš„æ–°æ¨ç†ç­–ç•¥ä¸Šçš„ä¼˜åŠ¿ã€‚ç»éªŒåˆ†æè¡¨æ˜ï¼ŒRLè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­è¡¨ç°å‡ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€æ¨¡å‹å®Œå…¨å¤±è´¥çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡å»ºç«‹äº†å¯¹æœªæ¥é•¿å‘¨æœŸæ¨ç†å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ (RL)è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆæ–¹æ³•ï¼Œå¯ä»¥å‘æ˜åŸºç¡€è¯­è¨€æ¨¡å‹ä¸­æ½œåœ¨çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ProRLè®­ç»ƒæ–¹æ³•ç»“åˆäº†KLæ•£åº¦æ§åˆ¶ã€å‚è€ƒç­–ç•¥é‡ç½®å’Œå¤šæ ·åŒ–ä»»åŠ¡ï¼Œèƒ½æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RLè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨åŸºç¡€æ¨¡å‹å¤±è´¥çš„æƒ…å†µä¸‹ã€‚</li>
<li>æ¨ç†è¾¹ç•Œçš„æ”¹è¿›ä¸åŸºç¡€æ¨¡å‹çš„ä»»åŠ¡èƒ½åŠ›å’Œè®­ç»ƒæ—¶é•¿ç´§å¯†ç›¸å…³ã€‚</li>
<li>RLèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¢ç´¢å¹¶å¡«å……æ–°çš„è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚</li>
<li>æœ¬æ–‡æ­ç¤ºäº†RLåœ¨æ‰©å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¾¹ç•Œçš„æ¡ä»¶ä¸‹çš„ä½œç”¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14a9160302923233a3d88dd407d98604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69d726d6207bda1bb90cace933946c8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3648c13ba916bbd24d669689eddab5e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ca5af15152defe89de1172e94057048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f3c557c9fc70366a4042dde5b9bcdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f13c37415cbb796adb8dbcf0ffcf91b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AlphaOne-Reasoning-Models-Thinking-Slow-and-Fast-at-Test-Time"><a href="#AlphaOne-Reasoning-Models-Thinking-Slow-and-Fast-at-Test-Time" class="headerlink" title="AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time"></a>AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time</h2><p><strong>Authors:Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, Huan Zhang</strong></p>
<p>This paper presents AlphaOne ($\alpha$1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. $\alpha$1 first introduces $\alpha$ moment, which represents the scaled thinking phase with a universal parameter $\alpha$. Within this scaled pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate $\alpha$1â€™s superior reasoning capability and efficiency. Project page: <a target="_blank" rel="noopener" href="https://alphaone-project.github.io/">https://alphaone-project.github.io/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†AlphaOneï¼ˆÎ±1ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨æµ‹è¯•æ—¶å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ¨ç†è¿›åº¦è¿›è¡Œè°ƒåˆ¶çš„é€šç”¨æ¡†æ¶ã€‚Î±1é¦–å…ˆå¼•å…¥äº†Î±æ—¶åˆ»ï¼Œè¿™ä»£è¡¨äº†ä¸€ä¸ªé€šç”¨å‚æ•°Î±çš„ç¼©æ”¾æ€è€ƒé˜¶æ®µã€‚åœ¨è¿™ä¸ªç¼©æ”¾çš„é¢„Î±æ—¶åˆ»é˜¶æ®µï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿæ¨ç†è¿‡æ¸¡ç¬¦å·çš„æ’å…¥ï¼ŒåŠ¨æ€å®‰æ’ç¼“æ…¢æ€è€ƒçš„è½¬å˜ï¼Œä½œä¸ºä¸€ä¸ªä¼¯åŠªåˆ©éšæœºè¿‡ç¨‹ã€‚åœ¨Î±æ—¶åˆ»ä¹‹åï¼ŒÎ±1é€šè¿‡æ€è€ƒç»“æŸç¬¦å·ç¡®å®šæ€§åœ°ç»ˆæ­¢ç¼“æ…¢æ€è€ƒï¼Œä»è€Œä¿ƒè¿›å¿«é€Ÿæ¨ç†å’Œé«˜æ•ˆçš„ç­”æ¡ˆç”Ÿæˆã€‚è¿™ç§æ–¹æ³•é€šè¿‡å®ç°çµæ´»ä¸”å¯†é›†çš„æ…¢åˆ°å¿«çš„æ¨ç†è°ƒåˆ¶ï¼Œç»Ÿä¸€å¹¶æ¦‚æ‹¬äº†ç°æœ‰çš„å•è°ƒç¼©æ”¾æ–¹æ³•ã€‚åœ¨å„ç§æ•°å­¦ã€ç¼–ç¨‹å’Œç§‘å­¦é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®è¯ç ”ç©¶è¯æ˜äº†Î±1çš„å“è¶Šæ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://alphaone-project.github.io/">https://alphaone-project.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AlphaOneæ¡†æ¶ç”¨äºåœ¨æµ‹è¯•æ—¶å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œè°ƒåˆ¶ã€‚å®ƒå¼•å…¥äº†Î±æ—¶åˆ»æ¥ä»£è¡¨é€šè¿‡é€šç”¨å‚æ•°Î±è¿›è¡Œç¼©æ”¾æ€è€ƒé˜¶æ®µã€‚åœ¨æ­¤ç¼©æ”¾é¢„Î±æ—¶åˆ»é˜¶æ®µï¼Œå®ƒé€šè¿‡å¯¹æ¨ç†è¿‡æ¸¡æ ‡è®°çš„æ’å…¥è¿›è¡Œä¼¯åŠªåˆ©éšæœºè¿‡ç¨‹å»ºæ¨¡æ¥å®ç°åŠ¨æ€å®‰æ’æ…¢é€Ÿæ€è€ƒè¿‡æ¸¡ã€‚åœ¨Î±æ—¶åˆ»åï¼ŒAlphaOneé€šè¿‡ç»ˆæ­¢æ€è€ƒæ ‡è®°ç¡®å®šæ€§åœ°æ¨åŠ¨å¿«é€Ÿæ¨ç†å’Œé«˜æ•ˆç­”æ¡ˆç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»Ÿä¸€å¹¶æ¨å¹¿äº†ç°æœ‰çš„å•è°ƒç¼©æ”¾æ–¹æ³•ï¼Œå®ç°äº†çµæ´»ä¸”å¯†é›†çš„æ…¢åˆ°å¿«çš„æ¨ç†è°ƒåˆ¶ã€‚å¯¹å¤šä¸ªåŸºå‡†æµ‹è¯•é¢†åŸŸçš„å¹¿æ³›å®è¯ç ”ç©¶è¯æ˜äº†AlphaOneåœ¨æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlphaOneæ˜¯ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºåœ¨æµ‹è¯•æ—¶è°ƒæ•´å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥Î±æ—¶åˆ»ä»£è¡¨é€šè¿‡é€šç”¨å‚æ•°Î±è¿›è¡Œçš„ç¼©æ”¾æ€è€ƒé˜¶æ®µã€‚</li>
<li>åœ¨é¢„Î±æ—¶åˆ»é˜¶æ®µï¼Œé€šè¿‡ä¼¯åŠªåˆ©éšæœºè¿‡ç¨‹å»ºæ¨¡æ¨ç†è¿‡æ¸¡æ ‡è®°çš„æ’å…¥ï¼Œå®ç°åŠ¨æ€å®‰æ’æ…¢é€Ÿæ€è€ƒè¿‡æ¸¡ã€‚</li>
<li>AlphaOneé€šè¿‡ç»ˆæ­¢æ€è€ƒæ ‡è®°æ¥æ¨åŠ¨å¿«é€Ÿæ¨ç†å’Œé«˜æ•ˆç­”æ¡ˆç”Ÿæˆã€‚</li>
<li>AlphaOneç»Ÿä¸€å¹¶æ¨å¹¿äº†ç°æœ‰çš„å•è°ƒç¼©æ”¾æ–¹æ³•ï¼Œå…è®¸çµæ´»ä¸”å¯†é›†çš„æ…¢åˆ°å¿«çš„æ¨ç†è°ƒåˆ¶ã€‚</li>
<li>AlphaOneæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55f2398b916caeeff8ced4bcf7de1d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd4512b8fdc83734e1b6891adf2b5299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-566638336e8663f7507d43696f4022a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e153848035322ca8baf614a386fe3309.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Accelerated-Sampling-from-Masked-Diffusion-Models-via-Entropy-Bounded-Unmasking"><a href="#Accelerated-Sampling-from-Masked-Diffusion-Models-via-Entropy-Bounded-Unmasking" class="headerlink" title="Accelerated Sampling from Masked Diffusion Models via Entropy Bounded   Unmasking"></a>Accelerated Sampling from Masked Diffusion Models via Entropy Bounded   Unmasking</h2><p><strong>Authors:Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, Brian Karrer</strong></p>
<p>Recent masked diffusion models (MDMs) have shown competitive performance compared to autoregressive models (ARMs) for language modeling. While most literature has focused on performance enhancing sampling procedures, efficient sampling from MDMs has been scarcely explored. We make the observation that often a given sequence of partially masked tokens determines the values of multiple unknown tokens deterministically, meaning that a single prediction of a masked model holds additional information unused by standard sampling procedures. Based on this observation, we introduce EB-Sampler, a simple drop-in replacement for existing samplers, utilizing an Entropy Bounded unmasking procedure that dynamically unmasks multiple tokens in one function evaluation with predefined approximate error tolerance. We formulate the EB-Sampler as part of a broad family of adaptive samplers for which we provide an error analysis that motivates our algorithmic choices. EB-Sampler accelerates sampling from current state of the art MDMs by roughly 2-3x on standard coding and math reasoning benchmarks without loss in performance. We also validate the same procedure works well on smaller reasoning tasks including maze navigation and Sudoku, tasks ARMs often struggle with. </p>
<blockquote>
<p>è¿‘æœŸçš„æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å±•ç°äº†ä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰ç›¸ç«äº‰çš„æ€§èƒ½ã€‚å°½ç®¡å¤§å¤šæ•°æ–‡çŒ®éƒ½é›†ä¸­äºæé«˜é‡‡æ ·ç¨‹åºæ€§èƒ½ï¼Œä½†å¾ˆå°‘æ¢ç´¢MDMsçš„é«˜æ•ˆé‡‡æ ·ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç»™å®šçš„éƒ¨åˆ†æ©ç ä»¤ç‰Œåºåˆ—ç»å¸¸ç¡®å®šæ€§åœ°å†³å®šäº†å¤šä¸ªæœªçŸ¥ä»¤ç‰Œçš„å€¼ï¼Œè¿™æ„å‘³ç€æ©ç æ¨¡å‹çš„ä¸€ä¸ªé¢„æµ‹åŒ…å«äº†æ ‡å‡†é‡‡æ ·ç¨‹åºæœªä½¿ç”¨çš„é¢å¤–ä¿¡æ¯ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†EB-Samplerï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç°æœ‰é‡‡æ ·å™¨çš„æ›¿ä»£äº§å“ï¼Œå®ƒåˆ©ç”¨ç†µæœ‰ç•Œè§£æ©ç ç¨‹åºï¼Œåœ¨ä¸€ä¸ªå‡½æ•°è¯„ä¼°ä¸­åŠ¨æ€è§£æ©å¤šä¸ªä»¤ç‰Œï¼Œå…·æœ‰é¢„è®¾çš„è¿‘ä¼¼è¯¯å·®å®¹å¿åº¦ã€‚æˆ‘ä»¬å°†EB-Sampleråˆ¶å®šä¸ºå¹¿æ³›è‡ªé€‚åº”é‡‡æ ·å™¨å®¶æ—çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶æä¾›äº†è¯¯å·®åˆ†æï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„ç®—æ³•é€‰æ‹©ã€‚EB-Sampleråœ¨ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹ï¼ŒåŠ é€Ÿäº†å¯¹å½“å‰æœ€å…ˆè¿›çš„MDMsçš„é‡‡æ ·ï¼Œåœ¨æ ‡å‡†ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„é€Ÿåº¦å¤§çº¦æé«˜äº†2-3å€ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†è¯¥ç›¸åŒç¨‹åºåœ¨å°è§„æ¨¡æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬è¿·å®«å¯¼èˆªå’Œæ•°ç‹¬ç­‰ARMsç»å¸¸é¢ä¸´å›°éš¾çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œæ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å±•ç°å‡ºä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰ç›¸å½“çš„ç«äº‰åŠ›ã€‚å°½ç®¡å¤šæ•°æ–‡çŒ®èšç„¦äºæé«˜é‡‡æ ·ç¨‹åºæ€§èƒ½ï¼Œä½†å¾ˆå°‘æœ‰äººæ¢ç´¢MDMsçš„é«˜æ•ˆé‡‡æ ·ã€‚åŸºäºè§‚å¯Ÿåˆ°çš„ç‰¹å®šæ©ç åºåˆ—å¯ç¡®å®šæ€§åœ°å†³å®šå¤šä¸ªæœªçŸ¥æ ‡è®°çš„å€¼è¿™ä¸€äº‹å®ï¼Œæˆ‘ä»¬å¼•å…¥äº†EB-Samplerï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç°æœ‰é‡‡æ ·å™¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒé‡‡ç”¨ç†µè¾¹ç•Œè§£æ©è¿‡ç¨‹ï¼Œå¯åœ¨ä¸€æ¬¡å‡½æ•°è¯„ä¼°ä¸­åŠ¨æ€åœ°æ­ç¤ºå¤šä¸ªæ ‡è®°çš„è¿‘ä¼¼è¯¯å·®å®¹å¿åº¦ã€‚æˆ‘ä»¬åˆ¶å®šäº†å¹¿æ³›çš„è‡ªé€‚åº”é‡‡æ ·å™¨å®¶æ—ä¸­çš„EB-Samplerï¼Œå¹¶æä¾›äº†è¯¯å·®åˆ†ææ¥æ”¯æŒæˆ‘ä»¬çš„ç®—æ³•é€‰æ‹©ã€‚åœ¨æ ‡å‡†ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEB-Samplerå°†å½“å‰æœ€å…ˆè¿›çš„MDMsçš„é‡‡æ ·é€Ÿåº¦æé«˜äº†å¤§çº¦2-3å€ï¼ŒåŒæ—¶ä¸æŸå¤±æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†åŒæ ·çš„ç¨‹åºåœ¨è¿·å®«å¯¼èˆªå’Œæ•°ç‹¬ç­‰å°å‹æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œè¿™äº›ä»»åŠ¡å¯¹äºARMsæ¥è¯´å¸¸å¸¸æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰ä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°ç›¸å½“ã€‚</li>
<li>ç°æœ‰æ–‡çŒ®å¤šå…³æ³¨æ€§èƒ½æå‡é‡‡æ ·ç¨‹åºï¼Œä½†å¯¹MDMsçš„é«˜æ•ˆé‡‡æ ·ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>EB-Sampleræ˜¯åŸºäºè§‚å¯Ÿåˆ°ç‰¹å®šæ©ç åºåˆ—å¯å†³å®šå¤šä¸ªæœªçŸ¥æ ‡è®°å€¼çš„å‘ç°è€Œè®¾è®¡çš„ã€‚</li>
<li>EB-Sampleræ˜¯ç°æœ‰é‡‡æ ·å™¨çš„ç®€å•æ›¿ä»£æ–¹æ¡ˆï¼Œé‡‡ç”¨ç†µè¾¹ç•Œè§£æ©è¿‡ç¨‹ã€‚</li>
<li>EB-Sampleråœ¨æ ‡å‡†ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­åŠ é€Ÿé‡‡æ ·é€Ÿåº¦è¾¾2-3å€ã€‚</li>
<li>EB-Sampleråœ¨å°å‹æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶é€‚ç”¨äºè¿·å®«å¯¼èˆªå’Œæ•°ç‹¬ç­‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eba7f27438bcef7af7e3ad8c59b68260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57987f45d79beb8c6bcae922c66cfea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1820e62982b0fa700f1f9dea6930d06f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e45d9339e08889901e49575ec9db21f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Harnessing-Negative-Signals-Reinforcement-Distillation-from-Teacher-Data-for-LLM-Reasoning"><a href="#Harnessing-Negative-Signals-Reinforcement-Distillation-from-Teacher-Data-for-LLM-Reasoning" class="headerlink" title="Harnessing Negative Signals: Reinforcement Distillation from Teacher   Data for LLM Reasoning"></a>Harnessing Negative Signals: Reinforcement Distillation from Teacher   Data for LLM Reasoning</h2><p><strong>Authors:Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi</strong></p>
<p>Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAIâ€™s o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples â€“ valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDIâ€™s superiority over baseline Rejection Sampling SFT or SFT combined with DPO&#x2F;SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data. </p>
<blockquote>
<p>æœ€è¿‘æ¨¡å‹è’¸é¦é¢†åŸŸçš„è¿›å±•è¡¨æ˜ï¼Œæ¥è‡ªå…ˆè¿›æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek-R1ã€OpenAIçš„o1ï¼‰çš„æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å°†å¤æ‚çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°æ›´å°ã€æ›´é«˜æ•ˆçš„å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œæ ‡å‡†å®è·µé‡‡ç”¨æ‹’ç»é‡‡æ ·ï¼Œä¸¢å¼ƒé”™è¯¯çš„æ¨ç†ç¤ºä¾‹â€”â€”è¿™äº›ç¤ºä¾‹è™½ç„¶å®è´µï¼Œä½†å¾€å¾€æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æ­£é¢å’Œè´Ÿé¢çš„è’¸é¦æ¨ç†ç—•è¿¹ï¼Œä»¥æœ€å¤§åŒ–ç¦»çº¿è®¾ç½®ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰è¿™ä¸€ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»æ­£é¢ç—•è¿¹ä¸­å­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µåˆ™è¿›ä¸€æ­¥ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„REDIç›®æ ‡ï¼Œé€šè¿‡æ­£é¢å’Œè´Ÿé¢ç—•è¿¹å¯¹æ¨¡å‹è¿›è¡Œç²¾ç‚¼ã€‚è¿™ä¸€æ–°é¢–ç›®æ ‡æ˜¯ä¸€ä¸ªç®€å•ã€æ— å‚è€ƒçš„æŸå¤±å‡½æ•°ï¼Œåœ¨è’¸é¦ç¯å¢ƒä¸­ä¼˜äºDPOå’ŒSimPOç­‰æ—¢å®šæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡æ–¹é¢ï¼ŒREDIç›¸è¾ƒäºåŸºçº¿æ‹’ç»é‡‡æ ·SFTæˆ–ç»“åˆDPO&#x2F;SimPOçš„SFTå…·æœ‰ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…å¯¹æ¥è‡ªå…¬å¼€Open-R1æ•°æ®é›†çš„13.1ä¸‡æ­£é¢å’Œè´Ÿé¢ç¤ºä¾‹è¿›è¡Œè®­ç»ƒçš„Qwen-REDI-1.5Bæ¨¡å‹ï¼Œåœ¨MATH-500ï¼ˆpass@1ï¼‰ä¸Šå¾—åˆ†è¾¾åˆ°83.1%ã€‚å…¶æ€§èƒ½åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¸DeepSeek-R1-Distill-Qwen-1.5Bï¼ˆä¸€ä¸ªç”¨80ä¸‡ä¸“æœ‰æ•°æ®åè®­ç»ƒçš„æ¨¡å‹ï¼‰ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ï¼Œä¸ºä½¿ç”¨å…¬å¼€æ•°æ®ç¦»çº¿åè®­ç»ƒçš„1.5Bæ¨¡å‹å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24850v1">PDF</a> 27 pages, 10 figures. Code available at   <a target="_blank" rel="noopener" href="https://github.com/Tim-Siu/reinforcement-distillation">https://github.com/Tim-Siu/reinforcement-distillation</a></p>
<p><strong>æ‘˜è¦</strong><br>å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æœ‰æ•ˆè½¬ç§»è‡³å°æ¨¡å‹ã€‚é€šå¸¸ä¸¢å¼ƒé”™è¯¯æ¨ç†æ ·æœ¬ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿé¢æ¨ç†ç—•è¿¹æå‡æ¨¡å‹ç¦»çº¿æ¨ç†æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæå‡ºå¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µå­¦ä¹ ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­¦ä¹ æ­£é¢ç—•è¿¹ï¼›ç¬¬äºŒé˜¶æ®µä½¿ç”¨æ­£è´Ÿé¢ç—•è¿¹è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚REDIç›®æ ‡å‡½æ•°ç®€å•æ— å‚è€ƒï¼Œåœ¨è’¸é¦ç¯å¢ƒä¸‹ä¼˜äºDPOå’ŒSimPOç­‰æ–¹æ³•ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒREDIåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºæ‹’ç»é‡‡æ ·SFTæˆ–ç»“åˆDPO&#x2F;SimPOçš„æ–¹æ³•ã€‚ä½¿ç”¨å…¬å¼€Open-R1æ•°æ®é›†çš„æ­£è´Ÿé¢ä¾‹å­è®­ç»ƒçš„Qwen-REDI-1.5Bæ¨¡å‹ï¼Œåœ¨MATH-500ä»»åŠ¡ä¸Šå¾—åˆ†83.1%ï¼Œæ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šä½¿ç”¨ä¸“æœ‰æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œæˆä¸ºä½¿ç”¨å…¬å¼€æ•°æ®ç¦»çº¿è®­ç»ƒçš„1.5Bæ¨¡å‹çš„æ–°åŸºå‡†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¨¡å‹è’¸é¦çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºï¼Œé«˜çº§æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼ŒOpenAIçš„o1ï¼‰çš„æ•°æ®å¯æœ‰æ•ˆè½¬ç§»å¤æ‚æ¨ç†èƒ½åŠ›è‡³å°å‹ã€é«˜æ•ˆçš„å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>æ‹’ç»é‡‡æ ·æ˜¯æ ‡å‡†å®è·µï¼Œä¸¢å¼ƒé”™è¯¯çš„æ¨ç†ä¾‹å­ï¼Œè¿™äº›ä¾‹å­è™½ç„¶æœ‰ä»·å€¼ä½†ç»å¸¸è¢«å¿½è§†ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿé¢æ¨ç†ç—•è¿¹æå‡æ¨¡å‹ç¦»çº¿æ¨ç†æ€§èƒ½ï¼Œæå‡ºå¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰æ¡†æ¶ã€‚</li>
<li>REDIæ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­¦ä¹ æ­£é¢ç—•è¿¹ï¼›ç¬¬äºŒé˜¶æ®µç»“åˆæ­£è´Ÿé¢ç—•è¿¹è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>REDIæ¡†æ¶é‡‡ç”¨ä¸€ç§æ–°çš„æ— å‚è€ƒæŸå¤±å‡½æ•°ï¼Œå³REDIç›®æ ‡å‡½æ•°ï¼Œå®ƒåœ¨è’¸é¦ç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œä¼˜äºDPOå’ŒSimPOç­‰æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒREDIåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºäºæ‹’ç»é‡‡æ ·çš„SFTæˆ–ç»“åˆDPO&#x2F;SimPOçš„æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨å…¬å¼€æ•°æ®é›†Open-R1çš„æ­£è´Ÿé¢ä¾‹å­è®­ç»ƒçš„Qwen-REDI-1.5Bæ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„åŸºå‡†æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a06cf9f1954536ebfd01ce819e76e6d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfada3cc54a7880cfb5b7c750a401ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd3bc2c4f08724c802fc6180be6681d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f86a506141d1ea850b8e5262e9267fa7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VideoCAD-A-Large-Scale-Video-Dataset-for-Learning-UI-Interactions-and-3D-Reasoning-from-CAD-Software"><a href="#VideoCAD-A-Large-Scale-Video-Dataset-for-Learning-UI-Interactions-and-3D-Reasoning-from-CAD-Software" class="headerlink" title="VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software"></a>VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and   3D Reasoning from CAD Software</h2><p><strong>Authors:Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed</strong></p>
<p>Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt at engineering UI interaction learning for precision tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order of magnitude higher complexity in UI interaction learning for real-world engineering tasks, having up to a 20x longer time horizon than other datasets. We show two important downstream applications of VideoCAD: learning UI interactions from professional precision 3D CAD tools and a visual question-answering (VQA) benchmark designed to evaluate multimodal large language modelsâ€™ (LLM) spatial reasoning and video understanding abilities. To learn the UI interactions, we propose VideoCADFormer - a state-of-the-art model in learning CAD interactions directly from video, which outperforms multiple behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”å¤æ‚çš„è¿‡ç¨‹ï¼Œéœ€è¦ç”¨æˆ·ä¸å¤æ‚çš„3Dç•Œé¢è¿›è¡Œç²¾ç¡®ã€é•¿æœŸçš„äº¤äº’ã€‚å°½ç®¡æœ€è¿‘äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰æ™ºèƒ½ä½“çš„è¿›æ­¥æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•éƒ½ä¸“æ³¨äºç§»åŠ¨æˆ–ç½‘é¡µåº”ç”¨ä¸­çš„çŸ­æœŸã€ä½å¤æ‚æ€§ä»»åŠ¡ï¼Œæ— æ³•æ•æ‰ä¸“ä¸šå·¥ç¨‹å·¥å…·çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoCADï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ä¸ºç²¾å¯†ä»»åŠ¡è¿›è¡Œå·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒVideoCADæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡41Kä¸ªç»æ³¨é‡Šçš„CADæ“ä½œè§†é¢‘è®°å½•ï¼Œè¿™äº›è®°å½•æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶ä»äººä¸ºè®¾è®¡çš„CADä¸­æ”¶é›†é«˜ä¿çœŸUIåŠ¨ä½œæ•°æ®ç”Ÿæˆçš„ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒVideoCADåœ¨ç°å®ä¸–ç•Œå·¥ç¨‹ä»»åŠ¡çš„UIäº¤äº’å­¦ä¹ ä¸­æä¾›äº†æ›´é«˜å¤æ‚åº¦çš„æ•°æ®ï¼Œæ—¶é—´è·¨åº¦æ¯”å…¶ä»–æ•°æ®é›†é•¿è¾¾20å€ã€‚æˆ‘ä»¬å±•ç¤ºäº†VideoCADçš„ä¸¤ä¸ªé‡è¦ä¸‹æ¸¸åº”ç”¨ï¼šä»ä¸“ä¸šç²¾å¯†3D CADå·¥å…·å­¦ä¹ UIäº¤äº’å’Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å­¦ä¹ UIäº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†VideoCADFormerâ€”â€”ä¸€ä¸ªç›´æ¥ä»è§†é¢‘ä¸­å­¦ä¹ CADäº¤äº’çš„æœ€æ–°æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å¤šä¸ªè¡Œä¸ºå…‹éš†åŸºçº¿ã€‚VideoCADFormerä»¥åŠä»VideoCADè¡ç”Ÿçš„VQAåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰è§†é¢‘å‹ç”¨æˆ·ç•Œé¢ç†è§£çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿æœŸä¾èµ–å…³ç³»çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24838v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Computer-Aided Designï¼ˆCADï¼‰è¿‡ç¨‹ä¸­ç”¨æˆ·ç•Œé¢çš„å¤æ‚æ€§å’Œæ—¶é—´æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚é’ˆå¯¹æ­¤ï¼Œç ”ç©¶è€…å¼•å…¥äº†VideoCADï¼Œä¸€ä¸ªé’ˆå¯¹ç²¾å¯†ä»»åŠ¡è¿›è¡Œå·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ çš„å¤§å‹åˆæˆæ•°æ®é›†ã€‚VideoCADåŒ…å«è¶…è¿‡41Kä¸ªç»è¿‡æ³¨é‡Šçš„CADæ“ä½œè§†é¢‘è®°å½•ï¼Œå…·æœ‰æ¯”å…¶ä»–æ•°æ®é›†é«˜å‡ºä¸€ä¸ªæ•°é‡çº§çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†VideoCADçš„ä¸¤ä¸ªé‡è¦ä¸‹æ¸¸åº”ç”¨ï¼šä»ä¸“ä¸šç²¾å¯†3D CADå·¥å…·å­¦ä¹ ç”¨æˆ·ç•Œé¢äº¤äº’ä»¥åŠè®¾è®¡ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›çš„è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å­¦ä¹ ç”¨æˆ·ç•Œé¢äº¤äº’ï¼Œç ”ç©¶è€…æå‡ºäº†VideoCADFormeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è§†é¢‘ä¸­å­¦ä¹ CADäº¤äº’ï¼Œå¹¶è¶…è¶Šäº†å¤šä¸ªè¡Œä¸ºå…‹éš†åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoCADæ˜¯ä¸€ä¸ªå¤§å‹åˆæˆæ•°æ®é›†ï¼Œä¸“æ³¨äºå·¥ç¨‹ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ ï¼Œé’ˆå¯¹ç²¾å¯†ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>VideoCADåŒ…å«è¶…è¿‡41Kä¸ªæ³¨é‡Šçš„è§†é¢‘è®°å½•ï¼Œå±•ç¤ºCADæ“ä½œï¼Œå…·æœ‰æ˜¾è‘—çš„é«˜å¤æ‚æ€§å’Œé•¿æ—¶é—´è·¨åº¦ã€‚</li>
<li>VideoCADæä¾›äº†ä¸¤ä¸ªé‡è¦çš„ä¸‹æ¸¸åº”ç”¨ï¼šä»ä¸“ä¸šç²¾å¯†3D CADå·¥å…·å­¦ä¹ UIäº¤äº’å’Œä¸€ä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VideoCADæ­ç¤ºäº†å½“å‰è§†é¢‘ç”¨æˆ·ç•Œé¢ç†è§£çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŠ¨ä½œå®šä½ã€å¤šæ¨¡æ€å’Œç©ºé—´æ¨ç†ä»¥åŠé•¿æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>VideoCADçš„å¤æ‚æ€§å’Œå¤§è§„æ¨¡æ•°æ®é›†å¯ä»¥ä¿ƒè¿›AIé©±åŠ¨çš„ç”¨æˆ·ç•Œé¢ä»£ç†çš„å‘å±•å’Œæ”¹è¿›ã€‚</li>
<li>VideoCADå’ŒVideoCADFormeræ¨¡å‹çš„æå‡ºä¸ºå·¥ç¨‹å·¥å…·ä¸­çš„ç”¨æˆ·ç•Œé¢äº¤äº’å­¦ä¹ æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3834c21cab465b52f3eeba0b4147c098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4278d9c16201e28a104396153f6ffd89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e21817674620dd382da44c8ab19696ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fa6f07e9de01acde011c0193e14e849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af09834b625542ab273a3d4f54eb9428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea7bcf153e240082e48927ebfd8ca02.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LegalEval-Q-A-New-Benchmark-for-The-Quality-Evaluation-of-LLM-Generated-Legal-Text"><a href="#LegalEval-Q-A-New-Benchmark-for-The-Quality-Evaluation-of-LLM-Generated-Legal-Text" class="headerlink" title="LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated   Legal Text"></a>LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated   Legal Text</h2><p><strong>Authors:Li yunhan, Wu gengshen</strong></p>
<p>As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.   Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/lyxx3rd/LegalEval-Q">https://github.com/lyxx3rd/LegalEval-Q</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹åº”ç”¨ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œå½“å‰çš„è¯„ä¼°åŸºå‡†é€šå¸¸ä¸»è¦å…³æ³¨äº‹å®å‡†ç¡®æ€§ï¼Œè€Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†é‡è¦çš„è¯­è¨€è´¨é‡æ–¹é¢ï¼Œä¾‹å¦‚æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå›å½’æ¨¡å‹ï¼ŒåŸºäºæ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­æ¥è¯„ä¼°æ³•å¾‹æ–‡æœ¬çš„è´¨é‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€å¥—ä¸“é—¨çš„æ³•å¾‹é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¤è¯„ä¼°æ¡†æ¶åˆ†æäº†49ä¸ªLLMã€‚æˆ‘ä»¬çš„åˆ†æå¾—å‡ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œæ¨¡å‹è´¨é‡åœ¨14äº¿å‚æ•°æ—¶è¾¾åˆ°ç¨³å®šæ°´å¹³ï¼Œä»…åœ¨72äº¿å‚æ•°æ—¶æ³¨æ„åˆ°æœ‰2.7%çš„è½»å¾®æ”¹è¿›ã€‚å…¶æ¬¡ï¼Œå·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„å½±å“å¾®ä¹å…¶å¾®ï¼Œå¦‚ç»Ÿè®¡æ˜¾è‘—æ€§é˜ˆå€¼é«˜äº0.016æ‰€ç¤ºã€‚ç¬¬ä¸‰ï¼Œæ¨ç†æ¨¡å‹å§‹ç»ˆä¼˜äºåŸºç¡€æ¶æ„ã€‚æˆ‘ä»¬ç ”ç©¶çš„ä¸€ä¸ªé‡è¦æˆæœæ˜¯å‘å¸ƒæ’ååˆ—è¡¨å’Œå¸•ç´¯æ‰˜åˆ†æï¼Œè¿™çªå‡ºäº†Qwen3ç³»åˆ—åœ¨æˆæœ¬æ€§èƒ½æƒè¡¡æ–¹é¢çš„æœ€ä½³é€‰æ‹©ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºæ³•å¾‹LLMå»ºç«‹äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œè¿˜æ­ç¤ºäº†å½“å‰è®­ç»ƒæ•°æ®æ”¹è¿›æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/lyxx3rd/LegalEval-Q">lyxx3rd&#x2F;LegalEval-Q</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24826v1">PDF</a> 10 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹åº”ç”¨ä¸­çš„è¯„ä»·ç°çŠ¶ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶æ¥è§£å†³å½“å‰ä¸»è¦èšç„¦äºäº‹å®å‡†ç¡®æ€§è€Œå¿½è§†è¯­è¨€è´¨é‡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šå¼€å‘å›å½’æ¨¡å‹è¯„ä¼°æ³•å¾‹æ–‡æœ¬è´¨é‡ã€åˆ›å»ºä¸“ä¸šæ³•å¾‹é—®é¢˜é›†ã€ä½¿ç”¨æ­¤æ¡†æ¶åˆ†æ49ä¸ªLLMã€‚ç ”ç©¶å‘ç°æ¨¡å‹è´¨é‡åœ¨14äº¿å‚æ•°æ—¶è¶‹äºç¨³å®šï¼Œå·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦å½±å“ç”šå¾®ï¼Œè€Œæ¨ç†æ¨¡å‹è¡¨ç°è¾ƒåŸºç¡€æ¶æ„æ›´ä¼˜ã€‚ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹åº”ç”¨ä¸­çš„è¯„ä»·ç›®å‰ä¸»è¦å…³æ³¨äº‹å®å‡†ç¡®æ€§ï¼Œä½†å¿½ç•¥äº†è¯­è¨€è´¨é‡å¦‚æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ä½¿ç”¨ç­‰æ–¹é¢çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ­¥éª¤çš„æ–°è¯„ä¼°æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåŸºäºæ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ä½¿ç”¨çš„å›å½’æ¨¡å‹æ¥è¯„ä¼°æ³•å¾‹æ–‡æœ¬è´¨é‡ã€‚</li>
<li>åˆ›å»ºäº†ä¸“é—¨ç”¨äºæ³•å¾‹é—®é¢˜çš„é›†åˆã€‚</li>
<li>å¯¹49ä¸ªLLMè¿›è¡Œäº†åˆ†æï¼Œå‘ç°æ¨¡å‹è´¨é‡åœ¨è¾¾åˆ°ä¸€å®šå‚æ•°æ•°é‡åè¶‹äºç¨³å®šï¼Œå·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦å¯¹æ¨¡å‹æ€§èƒ½å½±å“è¾ƒå°ã€‚</li>
<li>æ¨ç†æ¨¡å‹çš„è¡¨ç°ä¼˜äºåŸºç¡€æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcbb2426d253944b81ef9cd2f47ce01c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5dc8a930a4cb58427f7ae9ac7ae1f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ae0c0ff6711db4b56a37e52e67e2a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf65b425f01e3802b56a87bade04035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81649b87e74944ea1c0c51985a43c3ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7253c0e9555e994dd75b7d86ecb21161.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PhySense-Principle-Based-Physics-Reasoning-Benchmarking-for-Large-Language-Models"><a href="#PhySense-Principle-Based-Physics-Reasoning-Benchmarking-for-Large-Language-Models" class="headerlink" title="PhySense: Principle-Based Physics Reasoning Benchmarking for Large   Language Models"></a>PhySense: Principle-Based Physics Reasoning Benchmarking for Large   Language Models</h2><p><strong>Authors:Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo</strong></p>
<p>Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿…é€Ÿè¿›æ­¥ï¼Œè¶Šæ¥è¶Šèƒ½å¤Ÿè§£å†³å¤æ‚çš„ç§‘å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰©ç†é—®é¢˜ã€‚å°½ç®¡å–å¾—äº†è¿™ä¸€è¿›å±•ï¼Œä½†å½“å‰çš„LLMé€šå¸¸æ— æ³•æ¨¡ä»¿äººç±»ä¸“å®¶çš„ç®€æ´ã€åŸºäºåŸåˆ™æ¨ç†çš„ç‰¹ç‚¹ï¼Œè€Œæ˜¯ç”Ÿæˆå†—é•¿ä¸”ä¸é€æ˜çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç§å·®å¼‚å‡¸æ˜¾äº†å®ƒä»¬åœ¨åº”ç”¨æ ¸å¿ƒç‰©ç†åŸç†è¿›è¡Œæœ‰æ•ˆå’Œå¯è§£é‡Šçš„é—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›å­˜åœ¨å…³é”®å·®è·ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhySenseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ–°å‹åŸåˆ™çš„ç‰©ç†å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜“äºä¸“å®¶ä½¿ç”¨æŒ‡å¯¼åŸåˆ™è§£å†³ï¼Œä½†å¯¹æ²¡æœ‰é¦–å…ˆéµå¾ªåŸåˆ™çš„LLMæ¥è¯´å´å…·æœ‰æ¬ºéª—æ€§éš¾åº¦ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæœ€å…ˆè¿›LLMå’Œæç¤ºç±»å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬æ— æ³•ä¸ä¸“å®¶å¼çš„æ¨ç†è·¯å¾„ä¿æŒä¸€è‡´ï¼Œè¿™ä¸ºå¼€å‘å…·æœ‰é«˜æ•ˆã€ç¨³å¥å’Œå¯è§£é‡ŠåŸºäºåŸåˆ™çš„ç§‘å­¦æ¨ç†çš„AIç³»ç»Ÿæä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24823v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚çš„ç§‘å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰©ç†é—®é¢˜æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€æ— æ³•åƒäººç±»ä¸“å®¶ä¸€æ ·è¿›è¡Œç®€æ´ã€åŸºäºåŸåˆ™çš„æ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhySenseï¼Œä¸€ä¸ªåŸºäºåŸåˆ™çš„ç‰©ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å¯¼åŸåˆ™è®©ä¸“å®¶è½»æ¾è§£å†³ï¼Œä½†å¯¹LLMsæ¥è¯´å´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯„ä¼°å‘ç°ï¼Œæœ€å…ˆè¿›çš„LLMså’Œæç¤ºç±»å‹éƒ½æ— æ³•ä¸ä¸“å®¶çº§çš„æ¨ç†è·¯å¾„å¯¹é½ï¼Œè¿™ä¸ºå¼€å‘å…·æœ‰é«˜æ•ˆã€ç¨³å¥å’Œå¯è§£é‡Šæ€§çš„åŸºäºåŸåˆ™çš„ç§‘å­¦æ¨ç†çš„AIç³»ç»Ÿæä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚çš„ç§‘å­¦é—®é¢˜æ–¹é¢å–å¾—äº†è¿›å±•ï¼ŒåŒ…æ‹¬ç‰©ç†é—®é¢˜ã€‚</li>
<li>LLMsåœ¨æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„åŸºäºåŸåˆ™çš„ç®€æ´æ¨ç†æ–¹é¢å­˜åœ¨å·®è·ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†PhySenseï¼Œä¸€ä¸ªåŸºäºåŸåˆ™çš„ç‰©ç†æ¨ç†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>PhySenseæµ‹è¯•æ—¨åœ¨è®©ä¸“å®¶é€šè¿‡å¼•å¯¼åŸåˆ™è½»æ¾è§£å†³ï¼Œä½†å¯¹LLMsæ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¯¹å¤šä¸ªæœ€å…ˆè¿›çš„LLMså’Œæç¤ºç±»å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬æ— æ³•ä¸ä¸“å®¶çº§çš„æ¨ç†è·¯å¾„å¯¹é½ã€‚</li>
<li>è¿™ä¸€å‘ç°ä¸ºå¼€å‘å…·æœ‰é«˜æ•ˆã€ç¨³å¥å’Œå¯è§£é‡Šæ€§çš„åŸºäºåŸåˆ™çš„ç§‘å­¦æ¨ç†çš„AIç³»ç»Ÿæä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-953ce6e80ca8b285f0cdf1972d16e70a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14fb9c3a93e4ed46b9a5d345a9d7ea1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5fabb27633a788b9b70414a544084fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-317f3d2cf62e9d3390999e2749dcd768.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Video-Reasoning-with-Focused-Thinking"><a href="#Reinforcing-Video-Reasoning-with-Focused-Thinking" class="headerlink" title="Reinforcing Video Reasoning with Focused Thinking"></a>Reinforcing Video Reasoning with Focused Thinking</h2><p><strong>Authors:Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua</strong></p>
<p>Recent advancements in reinforcement learning, particularly through Group Relative Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group variance), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we propose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state-of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4% accuracy on CLEVRER (18.8% improvement over Video-R1) and 65.8% on MMVU. Our codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/longmalongma/TW-GRPO%7D%7Bhttps://github.com/longmalongma/TW-GRPO%7D">https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åº”ç”¨ï¼Œå·²ç»æ˜¾è‘—æ”¹è¿›äº†ç”¨äºå¤æ‚æ¨ç†ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸¤ä¸ªå…³é”®å±€é™ï¼š1ï¼‰å®ƒä»¬ç»å¸¸äº§ç”Ÿä¸èšç„¦ã€å†—é•¿çš„æ¨ç†é“¾ï¼Œæ©ç›–äº†é‡è¦çš„æ—¶ç©ºçº¿ç´¢ï¼›2ï¼‰äºŒå…ƒå¥–åŠ±æ— æ³•è€ƒè™‘éƒ¨åˆ†æ­£ç¡®çš„ç­”æ¡ˆï¼Œå¯¼è‡´å¥–åŠ±æ–¹å·®é«˜å’Œå­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TW-GRPOï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºè§†è§‰æ¨ç†çš„æ–°æ¡†æ¶ï¼Œå…·æœ‰èšç„¦æ€è€ƒå’Œå¯†é›†çš„å¥–åŠ±ç²’åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä»¤ç‰ŒåŠ æƒæœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†å…·æœ‰é«˜ä¿¡æ¯å¯†åº¦çš„ä»¤ç‰Œï¼ˆç”±ç»„å†…æ–¹å·®ä¼°è®¡ï¼‰ï¼ŒåŒæ—¶æŠ‘åˆ¶å†—ä½™ä»¤ç‰Œï¼Œå¦‚é€šç”¨æ¨ç†å‰ç¼€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†é€‰æ‹©é¢˜æ”¹ä¸ºå¤šé€‰é¢˜é—®ç­”ä»»åŠ¡æ¥æ”¹é©å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå…¶ä¸­è½¯å¥–åŠ±èƒ½å¤Ÿé€šè¿‡åŒºåˆ†éƒ¨åˆ†æ­£ç¡®æ€§æ¥è¿›è¡Œæ›´ç²¾ç»†çš„æ¢¯åº¦ä¼°è®¡ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†é—®ç­”åè½¬ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯ä»¥ä»ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„å¤šé€‰æ‹©æ ·æœ¬ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘æ¨ç†å’Œé€šç”¨ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTW-GRPOåœ¨CLEVRERä¸Šå®ç°äº†50.4%çš„å‡†ç¡®ç‡ï¼ˆç›¸å¯¹äºVideo-R1æé«˜äº†18.8%ï¼‰ï¼Œåœ¨MMVUä¸Šå®ç°äº†65.8%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/longmalongma/TW-GRPO">https://github.com/longmalongma/TW-GRPO</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24718v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•å·²ç»æå¤§åœ°æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿˜å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šä¸€æ˜¯ç”Ÿæˆçš„æ¨ç†é“¾é€šå¸¸ç¼ºä¹é‡ç‚¹ä¸”å†—é•¿ï¼Œéš¾ä»¥å‡¸æ˜¾æ—¶ç©ºçº¿ç´¢ï¼›äºŒæ˜¯äºŒå…ƒå¥–åŠ±æœºåˆ¶æ— æ³•åº”å¯¹éƒ¨åˆ†æ­£ç¡®çš„æƒ…å†µï¼Œå¯¼è‡´å¥–åŠ±æ–¹å·®å¤§ä¸”å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TW-GRPOæ¡†æ¶ï¼Œé€šè¿‡æ ‡è®°æƒé‡æœºåˆ¶å¼ºåŒ–è§†è§‰æ¨ç†çš„èšç„¦æ€è€ƒï¼Œå¹¶ç»†åŒ–å¥–åŠ±ç²’åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è§†é¢‘æ¨ç†å’Œé€šç”¨ç†è§£ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå¦‚åœ¨CLEVRERä¸Šè¾¾åˆ°äº†50.4%çš„å‡†ç¡®ç‡ï¼ˆç›¸è¾ƒäºVideo-R1æå‡18.8%ï¼‰ï¼Œåœ¨MMVUä¸Šè¾¾åˆ°äº†65.8%ã€‚ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³<a target="_blank" rel="noopener" href="https://github.com/longmalongma/TW-GRPO%E3%80%82">https://github.com/longmalongma/TW-GRPOã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šç”Ÿæˆæ¨ç†é“¾ä¸èšç„¦å’Œå¥–åŠ±æœºåˆ¶å¯¹éƒ¨åˆ†æ­£ç¡®æƒ…å†µå¤„ç†ä¸è¶³ã€‚</li>
<li>TW-GRPOæ¡†æ¶é€šè¿‡æ ‡è®°æƒé‡æœºåˆ¶æå‡è§†è§‰æ¨ç†çš„èšç„¦æ€è€ƒï¼Œå¹¶ç»†åŒ–å¥–åŠ±ç²’åº¦ä»¥åº”å¯¹ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TW-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå¦‚CLEVRERå’ŒMMVUã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºVideo-R1åœ¨CLEVRERä¸Šæœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶å…·æœ‰æ½œåŠ›æ¨åŠ¨è§†è§‰æ¨ç†ä»»åŠ¡çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25a172b1002f381cf1155d755e461d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3a23e9723f93aa2127b7aea300fbe70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac4778bd9a31fd705d2799d400969bfc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FinMME-Benchmark-Dataset-for-Financial-Multi-Modal-Reasoning-Evaluation"><a href="#FinMME-Benchmark-Dataset-for-Financial-Multi-Modal-Reasoning-Evaluation" class="headerlink" title="FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation"></a>FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation</h2><p><strong>Authors:Junyu Luo, Zhizhuo Kou, Liming Yang, Xiao Luo, Jinsheng Huang, Zhiping Xiao, Jingshu Peng, Chengzhong Liu, Jiaming Ji, Xuanzhe Liu, Sirui Han, Ming Zhang, Yike Guo</strong></p>
<p>Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/luojunyu/FinMME">https://huggingface.co/datasets/luojunyu/FinMME</a> and <a target="_blank" rel="noopener" href="https://github.com/luo-junyu/FinMME">https://github.com/luo-junyu/FinMME</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»å†äº†å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œåœ¨é‡‘èé¢†åŸŸï¼Œæœ‰æ•ˆä¸”ä¸“ä¸šçš„å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†æ˜æ˜¾ç¼ºä¹ã€‚ä¸ºäº†æ¨åŠ¨é‡‘èé¢†åŸŸMLLMsçš„å‘å±•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FinMMEï¼Œå®ƒæ¶µç›–äº†è¶…è¿‡11,000ä¸ªé«˜è´¨é‡é‡‘èç ”ç©¶æ ·æœ¬ï¼Œæ¶‰åŠ18ä¸ªé‡‘èé¢†åŸŸå’Œ6ä¸ªèµ„äº§ç±»åˆ«ï¼ŒåŒ…å«10ç§ä¸»è¦å›¾è¡¨ç±»å‹å’Œ21ç§å­ç±»å‹ã€‚æˆ‘ä»¬é€šè¿‡20åæ³¨é‡Šè€…å’Œç²¾å¿ƒè®¾è®¡çš„éªŒè¯æœºåˆ¶ç¡®ä¿æ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†FinScoreè¯„ä¼°ç³»ç»Ÿï¼Œå®ƒç»“åˆäº†å¹»è§‰æƒ©ç½šå’Œå¤šç»´åº¦èƒ½åŠ›è¯„ä¼°ï¼Œä»¥æä¾›ä¸åè§çš„è¯„ä¼°ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4oåœ¨FinMMEä¸Šçš„è¡¨ç°ä¹Ÿä¸å°½äººæ„ï¼Œè¿™å‡¸æ˜¾äº†å®ƒçš„æŒ‘æˆ˜æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•è¡¨ç°å‡ºé«˜åº¦çš„ç¨³å¥æ€§ï¼Œåœ¨ä¸åŒæç¤ºä¸‹çš„é¢„æµ‹å˜åŒ–ç‡ä¿æŒåœ¨1%ä»¥ä¸‹ï¼Œä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”æ˜¾ç¤ºå‡ºæ›´é«˜çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œè¯„ä¼°åè®®å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/luojunyu/FinMME">https://huggingface.co/datasets/luojunyu/FinMME</a>å’Œ<a target="_blank" rel="noopener" href="https://github.com/luo-junyu/FinMME%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/luo-junyu/FinMMEä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24714v1">PDF</a> ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>é‡‘èé¢†åŸŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•è¿…çŒ›ï¼Œä½†ç¼ºä¹æœ‰æ•ˆå’Œä¸“ä¸šçš„å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºFinMMEæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1.1ä¸‡é«˜è´¨é‡é‡‘èç ”ç©¶æ ·æœ¬ï¼Œè¦†ç›–å¤šä¸ªé‡‘èé¢†åŸŸå’Œèµ„äº§ç±»åˆ«ï¼ŒåŒæ—¶é…å¤‡å›¾è¡¨æ•°æ®ã€‚ä¸ºç¡®ä¿æ•°æ®è´¨é‡ï¼Œæˆ‘ä»¬è®¾ç«‹äº†äºŒåä½æ ‡æ³¨å‘˜å¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡éªŒè¯æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†FinScoreè¯„ä¼°ç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…å«å¹»è§‰æƒ©ç½šå’Œå¤šç»´åº¦èƒ½åŠ›è¯„ä¼°ï¼Œä»¥æä¾›ä¸åè§çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€æ–°æŠ€æœ¯æ¨¡å‹å¦‚GPT-4oåœ¨FinMMEä¸Šçš„è¡¨ç°ä¹Ÿä¸å°½äººæ„ï¼Œå‡¸æ˜¾å…¶æŒ‘æˆ˜æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•å±•ç°å‡ºé«˜ç¨³å¥æ€§ï¼Œé¢„æµ‹å˜åŒ–åœ¨ä¸åŒæç¤ºä¸‹ä½äºç™¾åˆ†ä¹‹ä¸€ï¼Œç›¸è¾ƒäºç°æœ‰æ•°æ®é›†å±•ç°å‡ºæ›´é«˜çš„å¯é æ€§ã€‚æ•°æ®é›†å’Œè¯„ä¼°åè®®å¯åœ¨ç›¸å…³ç½‘ç«™è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èé¢†åŸŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•å¿«é€Ÿï¼Œä½†ç¼ºä¹ä¸“é—¨çš„è¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>FinMMEæ•°æ®é›†åŒ…å«é«˜è´¨é‡é‡‘èç ”ç©¶æ ·æœ¬ï¼Œè¦†ç›–å¤šä¸ªé‡‘èé¢†åŸŸå’Œèµ„äº§ç±»åˆ«ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡å¤§é‡æ ‡æ³¨å‘˜å’ŒéªŒè¯æœºåˆ¶ç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>FinScoreè¯„ä¼°ç³»ç»ŸåŒ…å«å¹»è§‰æƒ©ç½šå’Œå¤šç»´åº¦èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå…ˆè¿›æ¨¡å‹å¦‚GPT-4åœ¨FinMMEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œè¯´æ˜å…¶æŒ‘æˆ˜æ€§ã€‚</li>
<li>FinMMEåŸºå‡†æµ‹è¯•å±•ç°å‡ºé«˜ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-567bcfdd66f3bab0e5be70cff6ba2726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89804467829ed3fb243c8f3e90adea04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1b63aa63a0b5c8bead4c038962249fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61f862a25c42b8a9020c549c9dc24b9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-962a4e9bbc63303d32c38cf8289b297d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e379c18d0cd7f357e1d1f91f998beaf3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Causal-aware-Large-Language-Models-Enhancing-Decision-Making-Through-Learning-Adapting-and-Acting"><a href="#Causal-aware-Large-Language-Models-Enhancing-Decision-Making-Through-Learning-Adapting-and-Acting" class="headerlink" title="Causal-aware Large Language Models: Enhancing Decision-Making Through   Learning, Adapting and Acting"></a>Causal-aware Large Language Models: Enhancing Decision-Making Through   Learning, Adapting and Acting</h2><p><strong>Authors:Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai</strong></p>
<p>Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a <code>learning-adapting-acting&quot; paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game </code>Crafterâ€ validate the effectiveness of our proposed method. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºåœ¨æ¨¡å‹ä¸­å­˜å‚¨äº†å¤§é‡çš„çŸ¥è¯†ï¼Œåœ¨å†³ç­–åˆ¶å®šæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›é¢„è®­ç»ƒæ¨¡å‹å¾€å¾€ç¼ºä¹æ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”æ–°ç¯å¢ƒï¼Œè¿›ä¸€æ­¥é˜»ç¢äº†å®ƒä»¬åœ¨å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å—åˆ°äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œæå‡ºäº†å› æœæ„ŸçŸ¥LLMã€‚å®ƒå°†ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰é›†æˆåˆ°å†³ç­–è¿‡ç¨‹ä¸­ï¼Œä»¥â€œå­¦ä¹ -é€‚åº”-è¡ŒåŠ¨â€çš„èŒƒå¼å¯¹ç¯å¢ƒç»“æ„è¿›è¡Œå»ºæ¨¡ã€æ›´æ–°å’Œåˆ©ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å­¦ä¹ é˜¶æ®µï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨LLMæå–ç¯å¢ƒç‰¹å®šçš„å› æœå®ä½“åŠå…¶å› æœå…³ç³»æ¥åˆå§‹åŒ–ç¯å¢ƒç»“æ„å› æœæ¨¡å‹ã€‚éšååœ¨é€‚åº”é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡å…³äºç¯å¢ƒçš„å¤–éƒ¨åé¦ˆæ¥æ›´æ–°ç»“æ„å› æœæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå› æœå¹²é¢„çš„æ¦‚å¿µã€‚æœ€åï¼Œåœ¨è¡ŒåŠ¨é˜¶æ®µï¼Œå› æœæ„ŸçŸ¥LLMåˆ©ç”¨ç»“æ„åŒ–å› æœçŸ¥è¯†æ¥æ›´æœ‰æ•ˆåœ°åˆ¶å®šæ”¿ç­–ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»£ç†æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸Šè¿°è¿‡ç¨‹ä¼šåå¤è¿›è¡Œä»¥å­¦ä¹ å› æœå…³ç³»çŸ¥è¯†ï¼Œæœ€ç»ˆä½¿å› æœæ„ŸçŸ¥LLMå®ç°å¯¹ç¯å¢ƒçš„æ›´å‡†ç¡®ç†è§£å¹¶åšå‡ºæ›´æœ‰æ•ˆçš„å†³ç­–ã€‚åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆâ€œCrafterâ€ä¸­çš„22ä¸ªä¸åŒä»»åŠ¡è¿›è¡Œçš„å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24710v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong>ï¼šåŸºäºäººç±»è®¤çŸ¥è¿‡ç¨‹ï¼Œæå‡ºå› æœæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCausal-aware LLMsï¼‰ï¼Œé€šè¿‡å°†ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰èå…¥å†³ç­–è¿‡ç¨‹ï¼Œæ¨¡æ‹Ÿã€æ›´æ–°å’Œåˆ©ç”¨ç¯å¢ƒç»“æ„çŸ¥è¯†ã€‚æ¨¡å‹ç»å†â€œå­¦ä¹ -é€‚åº”-è¡ŒåŠ¨â€ä¸‰ä¸ªé˜¶æ®µï¼Œä»ç¯å¢ƒä¸­æå–å› æœå®ä½“å’Œå…³ç³»è¿›è¡Œåˆå§‹åŒ–ï¼Œé€šè¿‡å¤–éƒ¨åé¦ˆæ›´æ–°ç»“æ„å› æœæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ åˆ¶å®šé«˜æ•ˆç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨æ¸¸æˆç¯å¢ƒä¸­å¯æœ‰æ•ˆæé«˜å†³ç­–æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†³ç­–è¿‡ç¨‹ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç¼ºä¹æ¨ç†èƒ½åŠ›å’Œé€‚åº”æ–°ç¯å¢ƒçš„èƒ½åŠ›ã€‚</li>
<li>å› æœæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCausal-aware LLMsï¼‰é€šè¿‡ç»“åˆç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨â€œå­¦ä¹ -é€‚åº”-è¡ŒåŠ¨â€æ¨¡å¼è¿›è¡Œæ“ä½œï¼ŒåŒ…æ‹¬åˆ©ç”¨LLMæå–ç¯å¢ƒç‰¹å®šå› æœå®ä½“å’Œå…³ç³»è¿›è¡Œåˆå§‹åŒ–ã€‚</li>
<li>é€šè¿‡å¤–éƒ¨åé¦ˆæ›´æ–°ç»“æ„å› æœæ¨¡å‹ï¼Œé‡‡ç”¨å› æœå¹²é¢„çš„æ€æƒ³ã€‚</li>
<li>Causal-aware LLMsåˆ©ç”¨ç»“æ„åŒ–å› æœçŸ¥è¯†åˆ¶å®šæ›´é«˜æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»£ç†å®ç°ã€‚</li>
<li>æ¨¡å‹å¯è¿­ä»£å­¦ä¹ å› æœçŸ¥è¯†ï¼Œæ›´å‡†ç¡®åœ°ç†è§£ç¯å¢ƒå¹¶åšå‡ºæ›´é«˜æ•ˆçš„å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d2580486d9e1f7dd66ab010078c6fd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d24ee7438fbad59f6874b790da8ad78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4fee982b5e28294fc5b8c4ef20ebf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18a02d7d0029716f9fe93ddbbab0eb9d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Soft-Reasoning-Navigating-Solution-Spaces-in-Large-Language-Models-through-Controlled-Embedding-Exploration"><a href="#Soft-Reasoning-Navigating-Solution-Spaces-in-Large-Language-Models-through-Controlled-Embedding-Exploration" class="headerlink" title="Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration"></a>Soft Reasoning: Navigating Solution Spaces in Large Language Models   through Controlled Embedding Exploration</h2><p><strong>Authors:Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui</strong></p>
<p>Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºæœ‰é™çš„å¤šæ ·æ€§å’Œä½æ•ˆçš„æœç´¢è€Œéš¾ä»¥è¿›è¡Œå¤æ‚çš„æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†Soft Reasoningï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåµŒå…¥çš„æœç´¢æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç¬¬ä¸€ä¸ªè¯çš„åµŒå…¥æ¥å¼•å¯¼ç”Ÿæˆã€‚å®ƒç»“åˆäº†ï¼ˆ1ï¼‰åµŒå…¥æ‰°åŠ¨ä»¥å®ç°å—æ§æ¢ç´¢ï¼Œä»¥åŠï¼ˆ2ï¼‰è´å¶æ–¯ä¼˜åŒ–ï¼Œé€šè¿‡éªŒè¯å™¨æŒ‡å¯¼çš„ç›®æ ‡æ¥ä¼˜åŒ–åµŒå…¥ï¼Œå¹³è¡¡æ¢ç´¢å’Œå¼€å‘ã€‚è¿™ç§æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼ŒåŒæ—¶é¿å…äº†ä¾èµ–å¯å‘å¼æœç´¢ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—èƒ½åŠ›æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæ›´é«˜çš„æ­£ç¡®æ€§ï¼Œæˆä¸ºäº†ä¸€ç§å¯æ‰©å±•ä¸”ä¸å—æ¨¡å‹ç±»å‹é™åˆ¶çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24688v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºSoft Reasoningçš„åŸºäºåµŒå…¥çš„æœç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶é¢ä¸´çš„å±€é™æ€§å’Œä½æ•ˆæœç´¢é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–ç¬¬ä¸€ä¸ªä»¤ç‰Œçš„åµŒå…¥æ¥å¼•å¯¼ç”Ÿæˆï¼Œå¹¶ç»“åˆåµŒå…¥æ‰°åŠ¨å’Œè´å¶æ–¯ä¼˜åŒ–ï¼Œé€šè¿‡éªŒè¯å™¨å¼•å¯¼çš„ç›®æ ‡æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚æ­¤æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼ŒåŒæ—¶é¿å…äº†ä¾èµ–å¯å‘å¼æœç´¢ï¼Œå®éªŒè¯æ˜å…¶èƒ½åœ¨ä¿è¯æ­£ç¡®æ€§çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ï¼Œæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”æ¨¡å‹æ— å…³çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶é¢ä¸´å¤šæ ·æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>Soft Reasoningæ˜¯ä¸€ç§åŸºäºåµŒå…¥çš„æœç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>Soft Reasoningé€šè¿‡ä¼˜åŒ–ç¬¬ä¸€ä¸ªä»¤ç‰Œçš„åµŒå…¥æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†åµŒå…¥æ‰°åŠ¨å’Œè´å¶æ–¯ä¼˜åŒ–æŠ€æœ¯ã€‚</li>
<li>Soft Reasoningé€šè¿‡éªŒè¯å™¨å¼•å¯¼çš„ç›®æ ‡æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab98372d1fe7b6ef715d823b6d7933b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e130195e4342a0de1d4cc134ee0d2709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e2d6f29fa32ef064f0a918d726244b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df09bf3de0a8a1e8faf6a32e501648ba.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Disentangling-Language-and-Culture-for-Evaluating-Multilingual-Large-Language-Models"><a href="#Disentangling-Language-and-Culture-for-Evaluating-Multilingual-Large-Language-Models" class="headerlink" title="Disentangling Language and Culture for Evaluating Multilingual Large   Language Models"></a>Disentangling Language and Culture for Evaluating Multilingual Large   Language Models</h2><p><strong>Authors:Jiahao Ying, Wei Tang, Yiran Zhao, Yixin Cao, Yu Rong, Wenxuan Zhang</strong></p>
<p>This paper introduces a Dual Evaluation Framework to comprehensively assess the multilingual capabilities of LLMs. By decomposing the evaluation along the dimensions of linguistic medium and cultural context, this framework enables a nuanced analysis of LLMsâ€™ ability to process questions within both native and cross-cultural contexts cross-lingually. Extensive evaluations are conducted on a wide range of models, revealing a notable â€œCulturalLinguistic Synergyâ€ phenomenon, where models exhibit better performance when questions are culturally aligned with the language. This phenomenon is further explored through interpretability probing, which shows that a higher proportion of specific neurons are activated in a languageâ€™s cultural context. This activation proportion could serve as a potential indicator for evaluating multilingual performance during model training. Our findings challenge the prevailing notion that LLMs, primarily trained on English data, perform uniformly across languages and highlight the necessity of culturally and linguistically model evaluations. Our code can be found at <a target="_blank" rel="noopener" href="https://yingjiahao14/">https://yingjiahao14</a>. github.io&#x2F;Dual-Evaluation&#x2F;. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŒè¯„ä¼°æ¡†æ¶ï¼Œä»¥å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ²¿è¯­è¨€åª’ä»‹å’Œæ–‡åŒ–èƒŒæ™¯è¿™ä¸¤ä¸ªç»´åº¦è¿›è¡Œåˆ†è§£è¯„ä¼°ï¼Œèƒ½å¤Ÿç»†è‡´åœ°åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ¬åœŸå’Œè·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è·¨è¯­è¨€å¤„ç†é—®é¢˜çš„èƒ½åŠ›ã€‚å¯¹ä¸€ç³»åˆ—æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œæ­ç¤ºäº†ä¸€ç§æ˜æ˜¾çš„â€œæ–‡åŒ–è¯­è¨€ååŒâ€ç°è±¡ï¼Œå³å½“é—®é¢˜ä¸æ–‡åŒ–ä¸è¯­è¨€ç›¸åŒ¹é…æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½è¡¨ç°æ›´å¥½ã€‚é€šè¿‡å¯è§£é‡Šæ€§æ¢æµ‹è¿›ä¸€æ­¥æ¢ç´¢äº†è¿™ä¸€ç°è±¡ï¼Œç»“æœæ˜¾ç¤ºï¼Œåœ¨æŸç§è¯­è¨€çš„æ–‡åŒ–èƒŒæ™¯ä¸‹ï¼Œæ›´é«˜æ¯”ä¾‹çš„ç‰¹å®šç¥ç»å…ƒè¢«æ¿€æ´»ã€‚è¿™ç§æ¿€æ´»æ¯”ä¾‹å¯ä½œä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°å¤šè¯­è¨€èƒ½åŠ›çš„ä¸€ä¸ªæ½œåœ¨æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‚£äº›ä¸»è¦ä»¥è‹±è¯­æ•°æ®è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¹¶éåœ¨æ‰€æœ‰è¯­è¨€ä¸­éƒ½è¡¨ç°ä¸€è‡´ï¼Œè¿™å¼ºè°ƒäº†è¿›è¡Œæ–‡åŒ–å’Œè¯­è¨€ä¸Šçš„æ¨¡å‹è¯„ä¼°çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://yingjiahao14.github.io/Dual-Evaluation/">https://yingjiahao14.github.io/Dual-Evaluation/</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24635v1">PDF</a> Accepted to ACL 2025 (Main Conference)</p>
<p><strong>Summary</strong><br>è¿™ç¯‡è®ºæ–‡æå‡ºä¸€ä¸ªåŒè¯„ä»·æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚æ¡†æ¶é€šè¿‡å¯¹è¯­è¨€åª’ä»‹å’Œæ–‡åŒ–èƒŒæ™¯ç»´åº¦çš„è¯„ä¼°ï¼Œç²¾ç»†åˆ†æLLMsåœ¨æœ¬åœŸå’Œè·¨æ–‡åŒ–ç¯å¢ƒä¸­å¤„ç†é—®é¢˜çš„èƒ½åŠ›ã€‚å¯¹ä¸€ç³»åˆ—æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°å‘ç°äº†ä¸€ç§åä¸ºâ€œæ–‡åŒ–è¯­è¨€ååŒâ€çš„ç°è±¡ï¼Œå³å½“é—®é¢˜ä¸è¯­è¨€æ–‡åŒ–èƒŒæ™¯ç›¸ç¬¦æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½æ›´ä½³ã€‚é€šè¿‡è§£é‡Šæ€§æ¢æµ‹è¿›ä¸€æ­¥æ¢ç´¢äº†è¿™ä¸€ç°è±¡ï¼Œå‘ç°ç‰¹å®šç¥ç»å…ƒåœ¨æŸç§è¯­è¨€çš„æ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ¿€æ´»æ¯”ä¾‹æ›´é«˜ï¼Œè¿™å¯ä»¥ä½œä¸ºè¯„ä¼°æ¨¡å‹è®­ç»ƒæœŸé—´å¤šè¯­è¨€èƒ½åŠ›çš„ä¸€ä¸ªæ½œåœ¨æŒ‡æ ‡ã€‚è¿™é¡¹ç ”ç©¶æŒ‘æˆ˜äº†ä»¥å¾€è®¤ä¸ºLLMsåœ¨è‹±è¯­æ•°æ®è®­ç»ƒåèƒ½åœ¨å„ç§è¯­è¨€ä¸­è¡¨ç°ä¸€è‡´çš„è§‚å¿µï¼Œå¹¶å¼ºè°ƒäº†æ–‡åŒ–å’Œè¯­è¨€è¯„ä»·çš„é‡è¦æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://yingjiahao14.github.io/Dual-Evaluation/">ç½‘ç«™é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŒè¯„ä»·æ¡†æ¶ï¼Œç”¨äºå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ä»è¯­è¨€åª’ä»‹å’Œæ–‡åŒ–èƒŒæ™¯ä¸¤ä¸ªç»´åº¦å¯¹LLMsè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†ä¸€ç§åä¸ºâ€œæ–‡åŒ–è¯­è¨€ååŒâ€çš„ç°è±¡ï¼Œå³æ¨¡å‹åœ¨æ–‡åŒ–èƒŒæ™¯ä¸è¯­è¨€ç›¸ç¬¦çš„é—®é¢˜ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
<li>é€šè¿‡è§£é‡Šæ€§æ¢æµ‹ï¼Œå‘ç°ç‰¹å®šç¥ç»å…ƒåœ¨ç‰¹å®šè¯­è¨€æ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ¿€æ´»æ¯”ä¾‹è¾ƒé«˜ã€‚</li>
<li>è¿™ä¸ªæ¿€æ´»æ¯”ä¾‹å¯ä»¥ä½œä¸ºè¯„ä¼°æ¨¡å‹å¤šè¯­è¨€èƒ½åŠ›çš„ä¸€ä¸ªæ½œåœ¨æŒ‡æ ‡ã€‚</li>
<li>ç ”ç©¶æŒ‘æˆ˜äº†LLMsåœ¨å¤šç§è¯­è¨€ä¸­çš„ä¸€è‡´æ€§èƒ½è¡¨ç°çš„è§‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d2665366589a16d4acb7fd4efafb55c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f595cdf83a411ff31b1085a4d2305066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68ea8fd32afdf5ef14cd6460124256f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c64add917f44249521790e958396869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-555973bf26924079bebb00f5a8caf69a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d15b037ac18245e97423c30fe7dfe059.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Thought-Efficient-Reasoning-via-Bidirectional-Compression-for-Low-Resource-Settings"><a href="#A-Thought-Efficient-Reasoning-via-Bidirectional-Compression-for-Low-Resource-Settings" class="headerlink" title="A*-Thought: Efficient Reasoning via Bidirectional Compression for   Low-Resource Settings"></a>A*-Thought: Efficient Reasoning via Bidirectional Compression for   Low-Resource Settings</h2><p><strong>Authors:Xiaoang Xu, Shuo Wang, Xu Han, Zhenghao Liu, Huijia Wu, Peipei Li, Zhiyuan Liu, Maosong Sun, Zhaofeng He</strong></p>
<p>Large Reasoning Models (LRMs) achieve superior performance by extending the thought length. However, a lengthy thinking trajectory leads to reduced efficiency. Most of the existing methods are stuck in the assumption of overthinking and attempt to reason efficiently by compressing the Chain-of-Thought, but this often leads to performance degradation. To address this problem, we introduce A*-Thought, an efficient tree search-based unified framework designed to identify and isolate the most essential thoughts from the extensive reasoning chains produced by these models. It formulates the reasoning process of LRMs as a search tree, where each node represents a reasoning span in the giant reasoning space. By combining the A* search algorithm with a cost function specific to the reasoning path, it can efficiently compress the chain of thought and determine a reasoning path with high information density and low cost. In addition, we also propose a bidirectional importance estimation mechanism, which further refines this search process and enhances its efficiency beyond uniform sampling. Extensive experiments on several advanced math tasks show that A*-Thought effectively balances performance and efficiency over a huge search space. Specifically, A*-Thought can improve the performance of QwQ-32B by 2.39$\times$ with low-budget and reduce the length of the output token by nearly 50% with high-budget. The proposed method is also compatible with several other LRMs, demonstrating its generalization capability. The code can be accessed at: <a target="_blank" rel="noopener" href="https://github.com/AI9Stars/AStar-Thought">https://github.com/AI9Stars/AStar-Thought</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ‰©å±•æ€ç»´é•¿åº¦å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ€è€ƒè½¨è¿¹è¿‡é•¿ä¼šå¯¼è‡´æ•ˆç‡é™ä½ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½é™·å…¥äº†è¿‡åº¦æ€è€ƒçš„å‡è®¾ï¼Œå¹¶è¯•å›¾é€šè¿‡å‹ç¼©æ€ç»´é“¾æ¥é«˜æ•ˆåœ°è¿›è¡Œæ¨ç†ï¼Œä½†è¿™å¸¸å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†A*-Thoughtï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé«˜æ•ˆæ ‘æœç´¢çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä»è¿™äº›æ¨¡å‹äº§ç”Ÿçš„å¹¿æ³›æ¨ç†é“¾ä¸­è¯†åˆ«å’Œéš”ç¦»æœ€æœ¬è´¨çš„æƒ³æ³•ã€‚å®ƒå°†LRMsçš„æ¨ç†è¿‡ç¨‹å…¬å¼åŒ–ä¸ºæœç´¢æ ‘ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨å·¨å¤§æ¨ç†ç©ºé—´ä¸­çš„ä¸€ä¸ªæ¨ç†è·¨åº¦ã€‚é€šè¿‡ç»“åˆA<em>æœç´¢ç®—æ³•å’Œé’ˆå¯¹æ¨ç†è·¯å¾„çš„æˆæœ¬å‡½æ•°ï¼Œå®ƒå¯ä»¥é«˜æ•ˆåœ°å‹ç¼©æ€ç»´é“¾ï¼Œå¹¶ç¡®å®šå…·æœ‰é«˜ä¿¡æ¯å¯†åº¦å’Œä½æˆæœ¬çš„æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŒå‘é‡è¦æ€§ä¼°è®¡æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†æœç´¢è¿‡ç¨‹ï¼Œæé«˜äº†å…¶æ•ˆç‡ï¼Œè¶…è¿‡äº†å‡åŒ€é‡‡æ ·ã€‚åœ¨å‡ ä¸ªé«˜çº§æ•°å­¦ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒA</em>-Thoughtåœ¨å·¨å¤§çš„æœç´¢ç©ºé—´ä¸­æœ‰æ•ˆåœ°å¹³è¡¡äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒA*-Thoughtå¯ä»¥åœ¨ä½é¢„ç®—çš„æƒ…å†µä¸‹ä½¿QwQ-3 2Bçš„æ€§èƒ½æé«˜2.39å€ï¼Œå¹¶åœ¨é«˜é¢„ç®—çš„æƒ…å†µä¸‹å°†è¾“å‡ºä»¤ç‰Œé•¿åº¦å‡å°‘è¿‘50%ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿˜ä¸å…¶ä»–å‡ ç§LRMså…¼å®¹ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/AI9Stars/AStar-Thought">https://github.com/AI9Stars/AStar-Thought</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24550v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ‰©å±•æ€ç»´é•¿åº¦å®ç°å“è¶Šæ€§èƒ½ï¼Œä½†è¿‡é•¿çš„æ€è€ƒè½¨è¿¹ä¼šå¯¼è‡´æ•ˆç‡é™ä½ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šé™·å…¥è¿‡åº¦æ€è€ƒçš„å‡è®¾ï¼Œå°è¯•é€šè¿‡å‹ç¼©æ€ç»´é“¾æ¥é«˜æ•ˆæ¨ç†ï¼Œä½†è¿™å¾€å¾€å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†A*-Thoughtï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ ‘æœç´¢çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä»LRMsäº§ç”Ÿçš„ä¼—å¤šæ¨ç†é“¾ä¸­è¯†åˆ«å’Œéš”ç¦»æœ€æœ¬è´¨çš„æƒ³æ³•ã€‚å®ƒå°†LRMsçš„æ¨ç†è¿‡ç¨‹å…¬å¼åŒ–ä¸ºä¸€ä¸ªæœç´¢æ ‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨å·¨å¤§æ¨ç†ç©ºé—´ä¸­çš„ä¸€ä¸ªæ¨ç†è·¨åº¦ã€‚é€šè¿‡ç»“åˆA<em>æœç´¢ç®—æ³•å’Œé’ˆå¯¹æ¨ç†è·¯å¾„çš„æˆæœ¬å‡½æ•°ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å‹ç¼©æ€ç»´é“¾ï¼Œå¹¶ç¡®å®šå…·æœ‰é«˜ä¿¡æ¯å¯†åº¦å’Œä½æˆæœ¬çš„æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŒå‘é‡è¦æ€§ä¼°è®¡æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†æœç´¢è¿‡ç¨‹ï¼Œæé«˜äº†æ•ˆç‡ã€‚åœ¨å¤šä¸ªé«˜çº§æ•°å­¦ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒA</em>-Thoughtåœ¨å·¨å¤§çš„æœç´¢ç©ºé—´ä¸­æœ‰æ•ˆåœ°å¹³è¡¡äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒA*-Thoughtå¯ä»¥åœ¨ä½é¢„ç®—ä¸‹æé«˜QwQ-32Bçš„æ€§èƒ½2.39å€ï¼Œå¹¶åœ¨é«˜é¢„ç®—ä¸‹å°†è¾“å‡ºä»¤ç‰Œé•¿åº¦å‡å°‘è¿‘50%ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿˜ä¸å…¶ä»–å‡ ç§LRMså…¼å®¹ï¼Œå±•ç¤ºäº†å…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ‰©å±•æ€ç»´é•¿åº¦å®ç°é«˜æ€§èƒ½ï¼Œä½†è¿‡é•¿çš„æ€è€ƒè½¨è¿¹ä¼šé™ä½æ•ˆç‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°è¯•é€šè¿‡å‹ç¼©æ€ç»´é“¾æ¥ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œä½†å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>A*-Thoughtæ˜¯ä¸€ä¸ªåŸºäºæ ‘æœç´¢çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è¯†åˆ«å’Œéš”ç¦»LRMsä¸­çš„å…³é”®æ€ç»´ã€‚</li>
<li>A*-Thoughtå°†æ¨ç†è¿‡ç¨‹å…¬å¼åŒ–ä¸ºä¸€ä¸ªæœç´¢æ ‘ï¼Œç»“åˆA*æœç´¢ç®—æ³•å’Œæˆæœ¬å‡½æ•°è¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚</li>
<li>åŒå‘é‡è¦æ€§ä¼°è®¡æœºåˆ¶è¿›ä¸€æ­¥æé«˜äº†A*-Thoughtçš„æœç´¢æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>A*-Thoughtåœ¨å¤šä¸ªé«˜çº§æ•°å­¦ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„æœ‰æ•ˆå¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52d00dcedda57bfcc7da541e1f17deed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67a199933ba89454024b39a6d92db5ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbd8e541c30d915b9f58f56e269811a5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TimeHC-RL-Temporal-aware-Hierarchical-Cognitive-Reinforcement-Learning-for-Enhancing-LLMsâ€™-Social-Intelligence"><a href="#TimeHC-RL-Temporal-aware-Hierarchical-Cognitive-Reinforcement-Learning-for-Enhancing-LLMsâ€™-Social-Intelligence" class="headerlink" title="TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning   for Enhancing LLMsâ€™ Social Intelligence"></a>TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning   for Enhancing LLMsâ€™ Social Intelligence</h2><p><strong>Authors:Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu</strong></p>
<p>Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMsâ€™ cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMsâ€™ social intelligence. In our experiments, we systematically explore improving LLMsâ€™ social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMsâ€™ social intelligence has uncovered several valuable insights. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦æ·±å…¥æ€è€ƒçš„æ•°å­¦å’Œç¼–ç¨‹ç­‰æ™ºåŠ›ç›¸å…³é¢†åŸŸä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œä»åè®­ç»ƒçš„è§’åº¦æ¥å¢å¼ºLLMåœ¨ç¤¾ä¼šé¢†åŸŸä¸­çš„è®¤çŸ¥èƒ½åŠ›å‘å±•ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬è®¤è¯†åˆ°ï¼Œç¤¾ä¼šä¸–ç•Œéµå¾ªä¸€ä¸ªç‹¬ç‰¹çš„æ—¶é—´çº¿ï¼Œéœ€è¦æ›´ä¸°å¯Œçš„è®¤çŸ¥æ¨¡å¼ç»„åˆï¼ŒåŒ…æ‹¬ç›´è§‰ååº”ï¼ˆç³»ç»Ÿä¸€ï¼‰å’Œè¡¨å±‚æ€ç»´åˆ°æ·±æ€ç†Ÿè™‘çš„ç†æ€§æ€ç»´ï¼ˆç³»ç»ŸäºŒï¼‰ï¼Œè¿™ä¸åŒäºä¸»è¦ä¾èµ–äºç³»ç»ŸäºŒè®¤çŸ¥çš„æ•°å­¦é¢†åŸŸï¼ˆé€šè¿‡å°å¿ƒè°¨æ…çš„é€æ­¥æ¨ç†ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶é—´æ„ŸçŸ¥åˆ†å±‚è®¤çŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆTimeHC-RLï¼‰ï¼Œä»¥æé«˜LLMçš„ç¤¾ä¼šæ™ºèƒ½ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†æé«˜LLMç¤¾ä¼šæ™ºåŠ›çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡äº”ç§å…¶ä»–åè®­ç»ƒæ¨¡å¼å’Œä¸¤ç§æµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å¼åœ¨å…«ä¸ªå…·æœ‰ä¸åŒæ•°æ®æ¨¡å¼çš„æ•°æ®é›†ä¸ŠéªŒè¯äº†TimeHC-RLæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„TimeHC-RLæ–¹æ³•ä¼˜äºå¹¿æ³›é‡‡ç”¨çš„ç³»ç»ŸäºŒå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å®ƒä¸ºå…·æœ‰7Bå‚æ•°çš„åŸºå‡†æ¨¡å‹æä¾›äº†ç¿…è†€ï¼Œä½¿å…¶èƒ½å¤ŸåŒ¹æ•ŒDeepSeek-R1å’ŒOpenAI-O3ç­‰å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä»åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´å¹²é¢„çš„è§’åº¦æ¥ç³»ç»Ÿåœ°æ¢ç´¢æé«˜LLMç¤¾ä¼šæ™ºåŠ›çš„æ–¹æ³•å·²ç»æ­ç¤ºäº†ä¸€äº›å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24500v1">PDF</a> 22 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦æ·±æ€ç†Ÿè™‘çš„IQç›¸å…³é¢†åŸŸï¼ˆå¦‚æ•°å­¦å’Œç¼–ç ï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç¤¾ä¼šé¢†åŸŸçš„è®¤çŸ¥å‘å±•æ–¹é¢ï¼Œå°¤å…¶æ˜¯ä»åè®­ç»ƒè§’åº¦ï¼Œä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶åºåˆ†å±‚è®¤çŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆTimeHC-RLï¼‰æ¥æå‡LLMsçš„ç¤¾ä¼šæ™ºèƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒTimeHC-RLæ–¹æ³•èƒ½æœ‰æ•ˆæå‡LLMsçš„ç¤¾ä¼šæ™ºèƒ½ï¼Œå¹¶ä¼˜äºå¹¿æ³›é‡‡ç”¨çš„System 2 RLæ–¹æ³•ã€‚è¯¥æ–¹æ³•èµ‹äºˆäº†7Béª¨å¹²æ¨¡å‹ç¿…è†€ï¼Œä½¿å…¶æ€§èƒ½ä¸é«˜çº§æ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O3ç›¸åª²ç¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦æ·±æ€çš„IQé¢†åŸŸå·²å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†å…¶åœ¨ç¤¾ä¼šé¢†åŸŸçš„è®¤çŸ¥å‘å±•ä»å¾…æ¢ç´¢ã€‚</li>
<li>ç¤¾ä¼šé¢†åŸŸéœ€è¦ä¸°å¯Œçš„è®¤çŸ¥æ¨¡å¼ç»„åˆï¼ŒåŒ…æ‹¬ç›´è§‰ååº”ï¼ˆSystem 1ï¼‰å’Œè¡¨é¢å±‚æ¬¡çš„æ€è€ƒä»¥åŠæ·±æ€ç†Ÿè™‘ï¼ˆSystem 2ï¼‰ã€‚</li>
<li>å¼•å…¥æ—¶åºåˆ†å±‚è®¤çŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆTimeHC-RLï¼‰ä»¥å¢å¼ºLLMsçš„ç¤¾ä¼šæ™ºèƒ½ã€‚</li>
<li>TimeHC-RLæ–¹æ³•é€šè¿‡äº”ç§åè®­ç»ƒæ¨¡å¼å’Œä¸¤ç§æµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å¼åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>TimeHC-RLæ–¹æ³•ä¼˜äºå¹¿æ³›é‡‡ç”¨çš„System 2 RLæ–¹æ³•ã€‚</li>
<li>TimeHC-RLä½¿7Béª¨å¹²æ¨¡å‹æ€§èƒ½å¤§å¹…æå‡ï¼Œå¯ä¸é«˜çº§æ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O3ç›¸åª²ç¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4907fcd2e524847d58628d0708f65bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81a10085014b34e379cbd1a45c421324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5e9002e39d5f654a9c6b3ff1ce1f494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0546d5463dde55748cf64b2323ffbdc8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Effective-Code-Integrated-Reasoning"><a href="#Towards-Effective-Code-Integrated-Reasoning" class="headerlink" title="Towards Effective Code-Integrated Reasoning"></a>Towards Effective Code-Integrated Reasoning</h2><p><strong>Authors:Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen</strong></p>
<p>In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of modelâ€™s capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/CIR">https://github.com/RUCAIBox/CIR</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»£ç é›†æˆæ¨ç†ï¼ˆCode-Integrated Reasoningï¼ŒCIRï¼‰ã€‚åœ¨æ­¤åœºæ™¯ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨å¿…è¦æ—¶ç”Ÿæˆä»£ç ï¼Œå¹¶é€šè¿‡ä»£ç è§£é‡Šå™¨æ‰§è¡Œæ¥é›†æˆåé¦ˆã€‚ä¸ºäº†è·å–è¿™ç§èƒ½åŠ›ï¼Œæ¨¡å‹å¿…é¡»å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨å¤–éƒ¨ä»£ç å·¥å…·ï¼Œè¿™å¯ä»¥é€šè¿‡å·¥å…·å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰é€šè¿‡äº¤äº’å­¦ä¹ æ¥æ”¯æŒã€‚å°½ç®¡æœ‰å…¶ä¼˜ç‚¹ï¼Œä½†å·¥å…·å¢å¼ºå‹RLä»ç„¶å¯èƒ½é­å—å­¦ä¹ åŠ¨æ€ä¸­çš„æ½œåœ¨ä¸ç¨³å®šæ€§çš„å›°æ‰°ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æé«˜å·¥å…·å¢å¼ºå‹RLåœ¨ä»£ç é›†æˆæ¨ç†ä¸­çš„è®­ç»ƒæ•ˆæœå’Œç¨³å®šæ€§çš„ç³»ç»Ÿæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†å¹³è¡¡æ¢ç´¢ä¸ç¨³å®šæ€§çš„å¢å¼ºè®­ç»ƒç­–ç•¥ï¼Œåœ¨é€æ­¥æ„å»ºå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åŒæ—¶æé«˜æ¨ç†æ€§èƒ½ã€‚åœ¨äº”ä¸ªä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªç«äº‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä»£ç é›†æˆæ¨ç†çš„æœºåˆ¶å’Œæ•ˆæœè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†ä¸€äº›å…³é”®è§è§£ï¼Œä¾‹å¦‚æ¨¡å‹èƒ½åŠ›è¾¹ç•Œçš„æ‰©å±•ä»¥åŠé€šè¿‡ä»£ç é›†æˆåŒæ—¶æé«˜æ¨ç†æ•ˆç‡ã€‚æ‰€æœ‰é‡ç°æ­¤å·¥ä½œçš„æ•°æ®å’Œä»£ç éƒ½å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/CIR%E3%80%82">https://github.com/RUCAIBox/CIRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24480v1">PDF</a> Technical Report on Slow Thinking with LLMs: Code-Integrated   Reasoning</p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæ¢è®¨äº†é›†æˆä»£ç çš„æ¨ç†èƒ½åŠ›ã€‚å½“æ¨¡å‹éœ€è¦ç”Ÿæˆä»£ç æ—¶ï¼Œå®ƒé€šè¿‡æ‰§è¡Œä»£ç è§£é‡Šå™¨æ¥é›†æˆåé¦ˆã€‚ä¸ºäº†è·å–è¿™ç§èƒ½åŠ›ï¼Œæ¨¡å‹å¿…é¡»å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨å¤–éƒ¨ä»£ç å·¥å…·ï¼Œè¿™æ˜¯é€šè¿‡å·¥å…·å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ”¯æŒæ¥å®ç°çš„ã€‚ä¸ºè§£å†³å·¥å…·å¢å¼ºå‹RLå¯èƒ½å­˜åœ¨çš„æ½œåœ¨å­¦ä¹ ä¸ç¨³å®šé—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æé«˜å·¥å…·å¢å¼ºå‹RLè®­ç»ƒæ•ˆæœå’Œç¨³å®šæ€§çš„ç³»ç»Ÿæ–¹æ³•ã€‚é€šè¿‡å¼€å‘å¹³è¡¡æ¢ç´¢å’Œç¨³å®šæ€§çš„å¢å¼ºè®­ç»ƒç­–ç•¥ï¼Œé€æ­¥æ„å»ºå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜æ¨ç†æ€§èƒ½ã€‚åœ¨äº”ä¸ªä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªç«äº‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæ·±å…¥åˆ†æäº†ä»£ç é›†æˆæ¨ç†çš„æœºåˆ¶å’Œæ•ˆæœï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›è¾¹ç•Œçš„æ‰©å±•å’Œé€šè¿‡ä»£ç é›†æˆåŒæ—¶æé«˜æ¨ç†æ•ˆç‡çš„å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å…·å¤‡ç”Ÿæˆä»£ç å¹¶é›†æˆåé¦ˆçš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨å¤–éƒ¨ä»£ç å·¥å…·çš„æ”¯æŒæ˜¯é€šè¿‡å·¥å…·å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°çš„ã€‚</li>
<li>å·¥å…·å¢å¼ºå‹RLå­˜åœ¨æ½œåœ¨çš„å­¦ä¹ ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æé«˜å·¥å…·å¢å¼ºå‹RLè®­ç»ƒæ•ˆæœå’Œç¨³å®šæ€§çš„ç³»ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬å¹³è¡¡æ¢ç´¢å’Œç¨³å®šæ€§çš„å¢å¼ºè®­ç»ƒç­–ç•¥ã€‚</li>
<li>æ¨¡å‹åœ¨äº”ä¸ªä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå¤šä¸ªç«äº‰åŸºå‡†ã€‚</li>
<li>æ¨¡å‹èƒ½åŠ›è¾¹ç•Œå¾—åˆ°æ‰©å±•ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-173ae46a3c779d0457f55a30f4763392.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2069ce73d6993ff67d51cf1cd34ba69.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e8a968a21b03aa19e3527fc3184a28a5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Open CaptchaWorld A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-02/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-85c8f303c9e344a58f81a1749c5d524f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-02  Benchmarking and Rethinking Knowledge Editing for Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
