<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Chameleon A MatMul-Free Temporal Convolutional Network Accelerator for   End-to-End Few-Shot and Continual Learning from Sequential Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7152e85b345e5d4ffe44920b1368a8e0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-03-æ›´æ–°"><a href="#2025-06-03-æ›´æ–°" class="headerlink" title="2025-06-03 æ›´æ–°"></a>2025-06-03 æ›´æ–°</h1><h2 id="Chameleon-A-MatMul-Free-Temporal-Convolutional-Network-Accelerator-for-End-to-End-Few-Shot-and-Continual-Learning-from-Sequential-Data"><a href="#Chameleon-A-MatMul-Free-Temporal-Convolutional-Network-Accelerator-for-End-to-End-Few-Shot-and-Continual-Learning-from-Sequential-Data" class="headerlink" title="Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for   End-to-End Few-Shot and Continual Learning from Sequential Data"></a>Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for   End-to-End Few-Shot and Continual Learning from Sequential Data</h2><p><strong>Authors:Douwe den Blanken, Charlotte Frenkel</strong></p>
<p>On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequential data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5% area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on-chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of-the-art inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$ higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way 5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3% on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 $\mu$W. </p>
<blockquote>
<p>åœ¨è®¾å¤‡è¾¹ç¼˜è¿›è¡Œåœ¨çº¿å­¦ä¹ å¯ä»¥å®ç°ä½å»¶è¿Ÿã€éšç§ä¸ªæ€§åŒ–çš„ä½“éªŒï¼ŒåŒæ—¶æé«˜é•¿æœŸç¨³å¥æ€§å¹¶é™ä½ç»´æŠ¤æˆæœ¬ã€‚ç„¶è€Œï¼Œå®ç°å¯æ‰©å±•çš„ã€ä½åŠŸè€—çš„ç«¯åˆ°ç«¯ç‰‡ä¸Šå­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ä»æœ‰é™æ ·æœ¬çš„çœŸå®åºåˆ—æ•°æ®ä¸­å­¦ä¹ ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚å®é™…ä¸Šï¼Œæ”¯æŒåå‘ä¼ æ’­çš„åŠ é€Ÿå™¨ä»¥æ¨ç†æ•ˆç‡ä¸ºä»£ä»·ä¼˜åŒ–äº†å­¦ä¹ æ€§èƒ½ï¼Œè€Œç®€åŒ–çš„å­¦ä¹ ç®—æ³•å¾€å¾€æ— æ³•è¾¾åˆ°å¯æ¥å—çš„å‡†ç¡®åº¦ç›®æ ‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Chameleonï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ï¼ˆ1ï¼‰ç»Ÿä¸€çš„å­¦ä¹ å’Œæ¨ç†æ¶æ„æ”¯æŒå°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ã€æŒç»­å­¦ä¹ ï¼ˆCLï¼‰å’Œæ¨ç†ï¼Œå…¶é¢ç§¯ä»…æ¯”æ¨ç†é€»è¾‘å¢åŠ äº†0.5%ã€‚ï¼ˆ2ï¼‰åˆ©ç”¨æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNsï¼‰æœ‰æ•ˆåœ°æ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–å…³ç³»ï¼Œå®ç°äº†å¯¹åºåˆ—æ•°æ®çš„ç«¯åˆ°ç«¯ç‰‡ä¸ŠFSLå’ŒCLçš„é¦–æ¬¡æ¼”ç¤ºä»¥åŠå¯¹16kHzåŸå§‹éŸ³é¢‘çš„æ¨ç†ã€‚ï¼ˆ3ï¼‰åŒæ¨¡å¼ã€æ— çŸ©é˜µä¹˜æ³•è®¡ç®—é˜µåˆ—å…è®¸ä¸æœ€å…ˆè¿›çš„ä»…ç”¨äºæ¨ç†çš„å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰åŠ é€Ÿå™¨çš„åŠŸè€—ç›¸åŒ¹é…ï¼Œæˆ–è€…å®ç°4.3å€æ›´é«˜çš„å³°å€¼GOPsã€‚é‡‡ç”¨40çº³ç±³CMOSå·¥è‰ºåˆ¶é€ çš„Chameleonåœ¨Omniglotæ•°æ®é›†ä¸Šåˆ›ä¸‹äº†ç«¯åˆ°ç«¯ç‰‡ä¸ŠFSLçš„æ–°å‡†ç¡®ç‡çºªå½•ï¼ˆ5åˆ†ç±»ï¼Œæ¯åˆ†ç±»åªç”¨ä¸€ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹å‡†ç¡®ç‡ä¸º96.8%ï¼Œæ¯åˆ†ç±»ç”¨äº”ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹å‡†ç¡®ç‡ä¸º98.8%ï¼‰ï¼Œåœ¨æŒç»­å­¦ä¹ æ–¹é¢ï¼ˆå­¦ä¹ 250ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«ç”¨åä¸ªæ ·æœ¬æ—¶çš„æœ€ç»ˆå‡†ç¡®ç‡ä¸º82.2%ï¼‰ï¼ŒåŒæ—¶åœ¨Googleè¯­éŸ³å‘½ä»¤æ•°æ®é›†ä¸Šçš„æ¨ç†å‡†ç¡®ç‡è¾¾åˆ°äº†93.3%ï¼Œåœ¨æç«¯è¾¹ç¼˜çš„åŠŸç‡é¢„ç®—ä¸º3.1å¾®ç“¦çš„æƒ…å†µä¸‹å®ç°äº†ä¸Šè¿°æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24852v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºChameleonæ–¹æ¡ˆæ¥è§£å†³å®é™…åº”ç”¨ä¸­é¢ä¸´çš„éš¾é¢˜ã€‚å®ƒå®ç°åœ¨è®¾å¤‡ä¸Šå¿«é€Ÿå“åº”å°‘é‡æ ·æœ¬ä¿¡æ¯è¿›è¡Œè‡ªé€‚åº”å­¦ä¹ å’Œä¼˜åŒ–é¢„æµ‹æ•ˆæœçš„åŒæ—¶æ§åˆ¶èƒ½é‡æ¶ˆè€—ä»¥åŠå‡å°åç»­ç»´æŠ¤è´¹ç”¨ã€‚ç‰¹åˆ«æ˜¯å¤„ç†åŠ¨æ€å¤æ‚è¿ç»­åœºæ™¯ï¼ˆä¾‹å¦‚æµåª’ä½“è¯­éŸ³æ§åˆ¶æˆ–å®æ—¶ç›‘æ§å›¾åƒä»»åŠ¡ç­‰ï¼‰ï¼Œè¿‡å»ç³»ç»Ÿè¯•å›¾ä¾èµ–å¢è®¾åŠ é€Ÿå™¨å’Œç®—æ³•å¤æ‚æ€§æé«˜æ¥æé«˜æ€§èƒ½ï¼Œå´éš¾ä»¥åœ¨å®æ—¶è¿ç»­æ•°æ®æµæƒ…å†µä¸‹å‡†ç¡®å’Œå¿«é€Ÿåœ°è°ƒæ•´å…¶çŠ¶æ€ä»¥åŠå¤§å¹…é™ä½è®¡ç®—æˆæœ¬ï¼Œæ•…å­˜åœ¨ä¸€å®šçš„æŒ‘æˆ˜ã€‚æœ¬æ–¹æ¡ˆçš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬é‡‡ç”¨ä¸€ä½“åŒ–çš„æ¨ç†ä¸æ·±åº¦å­¦ä¹ æ¡†æ¶å…¼å®¹æ—¶åºæ•°æ®æµçš„å˜åŒ–æœºåˆ¶å¹¶æ”¯æŒå…³é”®æ•°æ®æµçš„æœ€ç›´è§‚æŠ½è±¡å•å…ƒèåˆçš„ç­–ç•¥ä¼˜åŒ–æ–¹å¼ï¼ˆä¾‹å¦‚å›¾å½¢å¤„ç†å•å…ƒï¼‰ï¼Œé€šè¿‡å¼•å…¥å·ç§¯ç¥ç»ç½‘ç»œæŠ€æœ¯å®ç°æ—¶åºæ•°æ®çš„é•¿æœŸä¾èµ–å…³ç³»çš„é«˜æ•ˆæ•æ‰ï¼Œä»¥åŠé‡‡ç”¨åŒæ¨¡å¼çŸ©é˜µä¹˜æ³•æ— ä¾èµ–çš„è®¡ç®—é˜µåˆ—å…è®¸å³°å€¼è¿ç®—é€Ÿåº¦çš„å¤§å¹…æå‡æˆ–èŠ‚èƒ½æ“ä½œä»¥é€‚åº”ä½åŠŸè€—åº”ç”¨çš„éœ€æ±‚ã€‚å®éªŒç»“æœå±•ç¤ºå…¶åœ¨å¤šä¸ªåœºæ™¯ä¸­è¶…è¶Šå½“å‰é¢†å…ˆæ–¹æ¡ˆçš„æ€§èƒ½è¡¨ç°ã€‚ä¾‹å¦‚åœ¨Omniglotæ•°æ®é›†ä¸Šæ”¯æŒå°‘é‡çš„å­¦ä¹ æ ·æœ¬å°±å¯ä»¥å®ç°96.8%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶Google Speech Commandsæ•°æ®é›†ä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡ä¹Ÿä¿æŒåœ¨ç›¸å½“é«˜çš„æ°´å¹³ã€‚åŒæ—¶ï¼ŒChameleonæ–¹æ¡ˆå®ç°äº†ä½åŠŸè€—ä¸‹çš„é«˜æ€§èƒ½è¡¨ç°ï¼Œåœ¨æç«¯ä½åŠŸè€—é¢„ç®—ä¸‹ä»èƒ½ä¿æŒå‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºChameleonæ–¹æ¡ˆï¼Œè§£å†³äº†åœ¨æœ‰é™æ ·æœ¬ä¸‹å®ç°å¿«é€Ÿå“åº”å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œæ”¯æŒä¸ªæ€§åŒ–å®šåˆ¶ä¸”é™ä½ç»´æŠ¤æˆæœ¬ã€‚</li>
<li>é‡‡ç”¨ç»Ÿä¸€çš„å­¦ä¹ ä¸æ¨ç†æ¶æ„ï¼Œå®ç°å¿«é€Ÿå­¦ä¹ é€‚åº”å¹¶æ”¯æŒæ¨ç†æ“ä½œã€‚å¼•å…¥TCNç½‘ç»œæŠ€æœ¯å®ç°é•¿æœŸä¾èµ–çš„é«˜æ•ˆæ•æ‰ã€‚åœ¨å®æ—¶æ•°æ®æµä¸Šè¿›è¡Œç«¯å¯¹ç«¯å­¦ä¹ å’Œæ¨ç†æ¼”ç¤ºï¼Œå®ç°äº†é¦–ä¸ªåŸºäºæ—¶åºæ•°æ®çš„å°‘æ ·æœ¬å­¦ä¹ æ¼”ç¤ºå’Œæ¨ç†æ¡ˆä¾‹åº”ç”¨ç­‰å…³é”®ç‚¹çš„æ— ç¼å¤„ç†æ“ä½œèƒ½åŠ›æ˜¾è‘—ä¼˜äºä¹‹å‰å·²æœ‰æ¨¡å‹çš„ç­–ç•¥å®ç°ï¼Œä»¥æ­¤å…‹æœæ—¢æœ‰éš¾ç‚¹å¹¶ä¿æŒé«˜é€Ÿçš„è¿è¡Œè¡¨ç°æ°´å‡†ã€‚åŒæ—¶å±•ç¤ºäº†åœ¨éŸ³é¢‘æ•°æ®ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŒæ¨¡å¼è®¡ç®—é˜µåˆ—è®¾è®¡å…è®¸çµæ´»è°ƒæ•´è®¡ç®—æ€§èƒ½ä¸åŠŸè€—ä¹‹é—´çš„å¹³è¡¡ï¼Œæ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©æ€§èƒ½æå‡æˆ–ä½åŠŸè€—æ¨¡å¼ï¼Œé€‚ç”¨äºä¸åŒçš„è®¡ç®—éœ€æ±‚ã€‚å…·æœ‰é™ä½èƒ½è€—çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—èƒ½åŠ›çš„é«˜æ•ˆåˆ©ç”¨ï¼Œå±•ç°äº†æ˜æ˜¾çš„èƒ½æ•ˆä¼˜åŠ¿å¹¶é€‚åº”å¹¿æ³›çš„ç¡¬ä»¶å¹³å°ç¯å¢ƒç‰¹æ€§ä¼˜åŒ–ï¼Œå¯ä»¥æå¤§ç¨‹åº¦ä¸Šæ¨åŠ¨ç®—æ³•è¿è¡Œé€Ÿåº¦ä¸æ•ˆèƒ½çš„ä¼˜åŒ–å‘å±•æ€åŠ¿ä¸ç»¼åˆä»·å€¼èƒ½åŠ›åº”ç”¨ç­‰çªç ´æå‡ä¼˜åŒ–æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e58d329937631f0cbf498e6002686467.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28021ac460e56aa88c31c6b0381a897f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8ee86a8316ec851be53cdd4349e77c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e48db166e3282c7781377ae3e6902967.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef2ea800a6b3925154e252658da5f8e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dccaf0fa97e22db16c61b7745c02997.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Chameleon-A-Flexible-Data-mixing-Framework-for-Language-Model-Pretraining-and-Finetuning"><a href="#Chameleon-A-Flexible-Data-mixing-Framework-for-Language-Model-Pretraining-and-Finetuning" class="headerlink" title="Chameleon: A Flexible Data-mixing Framework for Language Model   Pretraining and Finetuning"></a>Chameleon: A Flexible Data-mixing Framework for Language Model   Pretraining and Finetuning</h2><p><strong>Authors:Wanyun Xie, Francesco Tonin, Volkan Cevher</strong></p>
<p>Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LIONS-EPFL/Chameleon">https://github.com/LIONS-EPFL/Chameleon</a>. </p>
<blockquote>
<p>è®­ç»ƒæ•°æ®æ··åˆç‰©å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ã€‚ç°æœ‰çš„é¢†åŸŸé‡æƒæ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„æƒé‡è®¡ç®—ï¼Œå¹¶åœ¨å¼•å…¥æ–°æ•°æ®æ—¶éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªçµæ´»é«˜æ•ˆçš„æ•°æ®æ··åˆæ¡†æ¶Chameleonï¼Œå®ƒåˆ©ç”¨æ æ†åˆ†æ•°æ¥é‡åŒ–å­¦ä¹ åµŒå…¥ç©ºé—´ä¸­çš„é¢†åŸŸé‡è¦æ€§ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨åŸŸåµŒå…¥ä¸Šæ„å»ºåŸŸäº²å’ŒçŸ©é˜µã€‚æ‰€å¾—åˆ°çš„æ æ†åˆ†æ•°ç¡®å®šäº†ä¸€ç§æ··åˆç‰©ï¼Œè¯¥æ··åˆç‰©ä¼šæé«˜åœ¨åµŒå…¥ç©ºé—´ä¸­å…±äº«å…±åŒè¡¨ç¤ºçš„é¢†åŸŸçš„æƒé‡ã€‚è¿™ç§è¡¨è¿°å…è®¸é€šè¿‡è®¡ç®—æ–°åŸŸåµŒå…¥ç›´æ¥è½¬ç§»åˆ°æ–°æ•°æ®ä¸Šã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§å…³é”®åœºæ™¯ä¸­è¯æ˜äº†æ”¹è¿›ï¼šï¼ˆiï¼‰æˆ‘ä»¬è®¡ç®—çš„æƒé‡åœ¨é¢„è®­ç»ƒåŸŸä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åªä½¿ç”¨äº†ç°æœ‰æ–¹æ³•çš„ä¸€éƒ¨åˆ†è®¡ç®—é‡ï¼›ï¼ˆiiï¼‰Chameleonèƒ½å¤Ÿé€‚åº”æ•°æ®å˜åŒ–è€Œæ— éœ€é€šè¿‡é‡æ–°è®­ç»ƒæå‡è¡¨ç°ï¼Œè½¬ç§»åˆ°æ–°æ•°æ®æ—¶æé«˜äº†å°‘é‡æ•°æ®çš„æ¨ç†å‡†ç¡®åº¦ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¾®è°ƒæ—¶çš„é¢†åŸŸé‡æƒæ•ˆç‡å¾ˆé«˜ï¼Œåœ¨æ‰€æœ‰å¾®è°ƒé¢†åŸŸä¸Šçš„ä¸€è‡´æ”¹è¿›æµ‹è¯•å›°æƒ‘åº¦ä¼˜äºå‡åŒ€æ··åˆç‰©ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LIONS-EPFL/Chameleon%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LIONS-EPFL/Chameleonæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24844v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Chameleonæ•°æ®æ··åˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ æ†å¾—åˆ†æ¥é‡åŒ–åœ¨æ‰€å­¦åµŒå…¥ç©ºé—´ä¸­çš„é¢†åŸŸé‡è¦æ€§ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºé¢†åŸŸäº²å’ŒçŸ©é˜µå’Œè®¡ç®—æ æ†å¾—åˆ†æ¥ç¡®å®šæ•°æ®æ··åˆçš„æ¯”ä¾‹ï¼Œå¼ºè°ƒåœ¨åµŒå…¥ç©ºé—´ä¸­å…±äº«å…±åŒè¡¨ç¤ºçš„é¢†åŸŸçš„æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼ŒChameleonåœ¨é¢„è®­ç»ƒã€é€‚åº”æ–°æ•°æ®å’Œå¾®è°ƒé¢†åŸŸç­‰ä¸‰ç§åœºæ™¯ä¸‹éƒ½è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chameleonæ¡†æ¶é€šè¿‡æ•°æ®æ··åˆæé«˜è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨æ æ†å¾—åˆ†é‡åŒ–é¢†åŸŸåœ¨åµŒå…¥ç©ºé—´ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æ„å»ºé¢†åŸŸäº²å’ŒçŸ©é˜µæ¥ç¡®å®šæ•°æ®æ··åˆæ¯”ä¾‹ã€‚</li>
<li>æ¡†æ¶èƒ½è‡ªé€‚åº”æ–°æ•°æ®ï¼Œé€šè¿‡è®¡ç®—æ–°é¢†åŸŸåµŒå…¥è¿›è¡Œç›´æ¥è¿ç§»ã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé¢†åŸŸè®¡ç®—æƒé‡æ—¶ï¼ŒChameleonä½¿ç”¨è¾ƒå°‘çš„è®¡ç®—èµ„æºå³èƒ½æå‡æ€§èƒ½ã€‚</li>
<li>Chameleonåœ¨é€‚åº”æ–°æ•°æ®å’Œå¾®è°ƒé¢†åŸŸæ—¶å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œæé«˜äº†å°‘æ ·æœ¬æ¨ç†çš„å‡†ç¡®åº¦å¹¶é™ä½äº†æµ‹è¯•å›°æƒ‘åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e28166b52d662ff267cf25163dca4a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d2d5648f664307425391b811aab6a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a497cf8a6fecea08ebb9cee9e45c0cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bb6026b8397d37a2fb8d7cdf8af3d0c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-577df307be6dba536c7e91363465ddfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f28b213e7960b927ea568d0fd4439d01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-650d0427e32ba606f2419e605780e0e5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Lightweight-Relational-Embedding-in-Task-Interpolated-Few-Shot-Networks-for-Enhanced-Gastrointestinal-Disease-Classification"><a href="#Lightweight-Relational-Embedding-in-Task-Interpolated-Few-Shot-Networks-for-Enhanced-Gastrointestinal-Disease-Classification" class="headerlink" title="Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks   for Enhanced Gastrointestinal Disease Classification"></a>Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks   for Enhanced Gastrointestinal Disease Classification</h2><p><strong>Authors:Xinliu Zhong, Leo Hwa Liang, Angela S. Koh, Yeo Si Yong</strong></p>
<p>Traditional diagnostic methods like colonoscopy are invasive yet critical tools necessary for accurately diagnosing colorectal cancer (CRC). Detection of CRC at early stages is crucial for increasing patient survival rates. However, colonoscopy is dependent on obtaining adequate and high-quality endoscopic images. Prolonged invasive procedures are inherently risky for patients, while suboptimal or insufficient images hamper diagnostic accuracy. These images, typically derived from video frames, often exhibit similar patterns, posing challenges in discrimination. To overcome these challenges, we propose a novel Deep Learning network built on a Few-Shot Learning architecture, which includes a tailored feature extractor, task interpolation, relational embedding, and a bi-level routing attention mechanism. The Few-Shot Learning paradigm enables our model to rapidly adapt to unseen fine-grained endoscopic image patterns, and the task interpolation augments the insufficient images artificially from varied instrument viewpoints. Our relational embedding approach discerns critical intra-image features and captures inter-image transitions between consecutive endoscopic frames, overcoming the limitations of Convolutional Neural Networks (CNNs). The integration of a light-weight attention mechanism ensures a concentrated analysis of pertinent image regions. By training on diverse datasets, the modelâ€™s generalizability and robustness are notably improved for handling endoscopic images. Evaluated on Kvasir dataset, our model demonstrated superior performance, achieving an accuracy of 90.1%, precision of 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current state-of-the-art methods, presenting a promising solution to the challenges of invasive colonoscopy by optimizing CRC detection through advanced image analysis. </p>
<blockquote>
<p>ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å¦‚ç»“è‚ é•œæ£€æŸ¥æ˜¯ä¾µå…¥æ€§çš„ï¼Œä½†å¯¹äºå‡†ç¡®è¯Šæ–­ç»“è‚ ç™Œï¼ˆCRCï¼‰ä»æ˜¯å¿…è¦çš„å…³é”®å·¥å…·ã€‚æ—©æœŸå‘ç°ç»“è‚ ç™Œå¯¹äºæé«˜æ‚£è€…å­˜æ´»ç‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç»“è‚ é•œæ£€æŸ¥ä¾èµ–äºè·å¾—å……è¶³ä¸”é«˜è´¨é‡çš„å†…çª¥é•œå›¾åƒã€‚é•¿æ—¶é—´çš„ä¾µå…¥æ€§æ‰‹æœ¯å¯¹æ‚£è€…æ¥è¯´å­˜åœ¨å›ºæœ‰é£é™©ï¼Œè€Œæ¬¡ä¼˜æˆ–ä¸å……è¶³çš„å›¾åƒåˆ™ä¼šå½±å“è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚è¿™äº›å›¾åƒé€šå¸¸æ¥æºäºè§†é¢‘å¸§ï¼Œå‘ˆç°å‡ºç›¸ä¼¼çš„æ¨¡å¼ï¼Œç»™é‰´åˆ«å¸¦æ¥æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œè¯¥ç½‘ç»œå»ºç«‹åœ¨å°æ ·å­¦ä¹ æ¶æ„ä¹‹ä¸Šï¼ŒåŒ…æ‹¬å®šåˆ¶çš„ç‰¹å¾æå–å™¨ã€ä»»åŠ¡æ’å€¼ã€å…³ç³»åµŒå…¥å’ŒåŒçº§è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ã€‚å°æ ·å­¦ä¹ èŒƒå¼ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè¿…é€Ÿé€‚åº”æœªè§è¿‡çš„ç²¾ç»†å†…çª¥é•œå›¾åƒæ¨¡å¼ï¼Œä»»åŠ¡æ’å€¼åˆ™ä»å„ç§ä»ªå™¨è§†è§’äººå·¥å¢å¼ºä¸è¶³å›¾åƒã€‚æˆ‘ä»¬çš„å…³ç³»åµŒå…¥æ–¹æ³•èƒ½å¤Ÿè¾¨åˆ«å›¾åƒå†…çš„å…³é”®ç‰¹å¾ï¼Œå¹¶æ•æ‰è¿ç»­å†…çª¥é•œå¸§ä¹‹é—´çš„å›¾åƒé—´è¿‡æ¸¡ï¼Œä»è€Œå…‹æœå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å±€é™æ€§ã€‚è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶çš„é›†æˆç¡®ä¿äº†ç›¸å…³å›¾åƒåŒºåŸŸçš„é›†ä¸­åˆ†æã€‚é€šè¿‡å¤šæ ·æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹çš„é€šç”¨æ€§å’Œç¨³å¥æ€§å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼Œä»¥å¤„ç†å†…çª¥é•œå›¾åƒã€‚åœ¨Kvasiræ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†90.1%çš„å‡†ç¡®ç‡ã€0.845çš„ç²¾ç¡®åº¦ã€0.942çš„å¬å›ç‡å’Œ0.891çš„F1åˆ†æ•°ã€‚è¿™è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–CRCæ£€æµ‹çš„é«˜çº§å›¾åƒåˆ†æï¼Œä¸ºè§£å†³ä¾µå…¥æ€§ç»“è‚ é•œæ£€æŸ¥çš„æŒ‘æˆ˜æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24792v1">PDF</a> 6 pages, 15 figures</p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ æ¶æ„çš„æ¨¡å‹ï¼Œç”¨äºç»“ç›´è‚ ç™Œçš„æ—©æœŸè¯Šæ–­ã€‚æ¨¡å‹åŒ…æ‹¬ç‰¹å¾æå–å™¨ã€ä»»åŠ¡æ’å€¼ã€å…³ç³»åµŒå…¥å’ŒåŒçº§è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿè¿…é€Ÿé€‚åº”å†…çª¥é•œå›¾åƒæ¨¡å¼ï¼Œè§£å†³å›¾åƒè´¨é‡ä¸è¶³å’Œæ¨¡å¼è¯†åˆ«å›°éš¾çš„é—®é¢˜ã€‚åœ¨Kvasiræ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸ºç»“ç›´è‚ ç™Œæ£€æµ‹æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨¡å‹åŸºäºæ·±åº¦å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ æ¶æ„æ„å»ºï¼Œç”¨äºè§£å†³ç»“ç›´è‚ ç™Œæ—©æœŸè¯Šæ–­ä¸­å†…çª¥é•œå›¾åƒçš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç‰¹å¾æå–å™¨ã€ä»»åŠ¡æ’å€¼ã€å…³ç³»åµŒå…¥å’ŒåŒçº§è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ç­‰æŠ€æœ¯ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿè¿…é€Ÿé€‚åº”å†…çª¥é•œå›¾åƒæ¨¡å¼ï¼Œè§£å†³å›¾åƒè´¨é‡ä¸è¶³å’Œæ¨¡å¼è¯†åˆ«å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡åœ¨Kvasiræ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸ºç»“ç›´è‚ ç™Œæ£€æµ‹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4adc99fc5fd87073adcc3d5b1d59f500.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8de0ef22aa42fa4e110aba85975786c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e73b70ffa05a76e2a8a4d0b44dce91d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-028ba605650bf5aeb6c690b177985d88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d34983e62330ea66b465416f3cb76592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437c7ee180fa58b708df4f82cb3d220a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Large-Language-Models-for-Cryptanalysis-and-Mismatched-Generalization"><a href="#Benchmarking-Large-Language-Models-for-Cryptanalysis-and-Mismatched-Generalization" class="headerlink" title="Benchmarking Large Language Models for Cryptanalysis and   Mismatched-Generalization"></a>Benchmarking Large Language Models for Cryptanalysis and   Mismatched-Generalization</h2><p><strong>Authors:Utsav Maskey, Chencheng Zhu, Usman Naseem</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis a critical area for data security and encryption has not yet been thoroughly explored in LLM evaluations. To address this gap, we evaluate cryptanalytic potential of state of the art LLMs on encrypted texts generated using a range of cryptographic algorithms. We introduce a novel benchmark dataset comprising diverse plain texts spanning various domains, lengths, writing styles, and topics paired with their encrypted versions. Using zero-shot and few shot settings, we assess multiple LLMs for decryption accuracy and semantic comprehension across different encryption schemes. Our findings reveal key insights into the strengths and limitations of LLMs in side-channel communication while raising concerns about their susceptibility to jailbreaking attacks. This research highlights the dual-use nature of LLMs in security contexts and contributes to the ongoing discussion on AI safety and security. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆæ–¹å¼ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œå¯¹äºæ•°æ®å®‰å…¨å’ŒåŠ å¯†è‡³å…³é‡è¦çš„å¯†ç åˆ†æé¢†åŸŸåœ¨LLMè¯„ä¼°ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆç«¯çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä½¿ç”¨å¤šç§åŠ å¯†ç®—æ³•ç”Ÿæˆçš„åŠ å¯†æ–‡æœ¬çš„å¯†ç åˆ†ææ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å¤šç§é¢†åŸŸçš„æ™®é€šæ–‡æœ¬åŠå…¶åŠ å¯†ç‰ˆæœ¬é…å¯¹çš„æ–°å‹åŸºå‡†æ•°æ®é›†ã€‚é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒåŠ å¯†æ–¹æ¡ˆä¸‹å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„è§£å¯†å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¾§ä¿¡é“é€šä¿¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶å¼•å‘äº†å¯¹å…¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„æ‹…å¿§ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­çš„åŒé‡ç”¨é€”æ€§è´¨ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨å’Œä¿æŠ¤çš„æŒç»­è®¨è®ºåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24621v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆæ–¹å¼ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œå¯¹äºæ•°æ®å®‰å…¨å’ŒåŠ å¯†è‡³å…³é‡è¦çš„å¯†ç åˆ†æé¢†åŸŸåœ¨LLMè¯„ä¼°ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä½¿ç”¨å¤šç§åŠ å¯†ç®—æ³•ç”Ÿæˆçš„åŠ å¯†æ–‡æœ¬çš„å¯†ç åˆ†ææ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å¤šç§é¢†åŸŸçš„æ™®é€šæ–‡æœ¬åŠå…¶åŠ å¯†ç‰ˆæœ¬çš„æ–°å‹åŸºå‡†æ•°æ®é›†ã€‚é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒåŠ å¯†æ–¹æ¡ˆä¸‹çš„è§£å¯†å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¾§ä¿¡é“é€šä¿¡ä¸­çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¯¹å…¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»æå‡ºäº†æ‹…å¿§ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­çš„åŒé‡ç”¨é€”æ€§è´¨ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨å’Œå®‰å…¨æ€§è®¨è®ºåšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„è¿›å±•å·²åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¯†ç åˆ†æåœ¨LLMè¯„ä¼°ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°å‹åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«æ™®é€šæ–‡æœ¬åŠå…¶ä½¿ç”¨å¤šç§åŠ å¯†ç®—æ³•ç”Ÿæˆçš„åŠ å¯†ç‰ˆæœ¬ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®è¯„ä¼°äº†LLMsåœ¨å¤šç§åŠ å¯†æ–¹æ¡ˆä¸‹çš„è§£å¯†èƒ½åŠ›å’Œè¯­ä¹‰ç†è§£ã€‚</li>
<li>LLMsåœ¨ä¾§ä¿¡é“é€šä¿¡ä¸­å…·æœ‰æ½œåŠ›å’Œå±€é™æ€§ã€‚</li>
<li>LLMså®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œè¿™å¼•å‘äº†å…³äºå…¶å®‰å…¨æ€§çš„æ‹…å¿§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32521c2eb028948a2b3f63673f098c92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015df6221d4f1234261f9fd6de2ca4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-242ffafaccfe294a8e6f684fa40546e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d01d346e0784b80ffd54d75c42b59a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d05d03f4a284abe06386a926fa13e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7152e85b345e5d4ffe44920b1368a8e0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Foundation-Models-for-Zero-Shot-Biometric-Tasks"><a href="#Benchmarking-Foundation-Models-for-Zero-Shot-Biometric-Tasks" class="headerlink" title="Benchmarking Foundation Models for Zero-Shot Biometric Tasks"></a>Benchmarking Foundation Models for Zero-Shot Biometric Tasks</h2><p><strong>Authors:Redwan Sony, Parisa Farmanifard, Hamzeh Alzwairy, Nitish Shukla, Arun Ross</strong></p>
<p>The advent of foundation models, particularly Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of artificial intelligence, enabling remarkable generalization across diverse tasks with minimal or no supervision. Yet, their potential in biometric recognition and analysis remains relatively underexplored. In this work, we introduce a comprehensive benchmark that evaluates the zero-shot and few-shot performance of state-of-the-art publicly available VLMs and MLLMs across six biometric tasks spanning the face and iris modalities: face verification, soft biometric attribute prediction (gender and race), iris recognition, presentation attack detection (PAD), and face manipulation detection (morphs and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments show that embeddings from these foundation models can be used for diverse biometric tasks with varying degrees of success. For example, in the case of face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW) dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1 percent FMR on the IITD-R-Full dataset was 97.55 percent without any fine-tuning. Further, we show that applying a simple classifier head to these embeddings can help perform DeepFake detection for faces, Presentation Attack Detection (PAD) for irides, and extract soft biometric attributes like gender and ethnicity from faces with reasonably high accuracy. This work reiterates the potential of pretrained models in achieving the long-term vision of Artificial General Intelligence. </p>
<blockquote>
<p>éšç€åŸºç¡€æ¨¡å‹çš„å…´èµ·ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ï¼Œäººå·¥æ™ºèƒ½çš„è¾¹ç•Œå¾—åˆ°äº†é‡æ–°å®šä¹‰ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸­å®ç°æ˜¾è‘—æ³›åŒ–ï¼Œè€Œæ— éœ€æˆ–åªéœ€å¾ˆå°‘çš„ç›‘ç£ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç”Ÿç‰©è¯†åˆ«åˆ†æå’Œåº”ç”¨ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†æœ€å‰æ²¿çš„å…¬å¼€å¯ç”¨çš„VLMså’ŒMLLMsåœ¨å…­ä¸ªè·¨è¶Šé¢éƒ¨å’Œè™¹è†œæ¨¡æ€çš„ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡çš„é›¶æ ·æœ¬å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼šé¢éƒ¨éªŒè¯ã€è½¯ç”Ÿç‰©è¯†åˆ«å±æ€§é¢„æµ‹ï¼ˆæ€§åˆ«å’Œç§æ—ï¼‰ã€è™¹è†œè¯†åˆ«ã€å‘ˆç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰å’Œé¢éƒ¨æ“ä½œæ£€æµ‹ï¼ˆå½¢æ€å’Œæ·±åº¦ä¼ªé€ ï¼‰ã€‚æœ¬æ¬¡è¯„ä¼°å…±ä½¿ç”¨äº†40ä¸ªVLMsã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›åŸºç¡€æ¨¡å‹çš„åµŒå…¥å¯ç”¨äºå„ç§ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ï¼Œå¹¶å–å¾—ä¸åŒç¨‹åº¦çš„æˆåŠŸã€‚ä¾‹å¦‚ï¼Œåœ¨é¢éƒ¨éªŒè¯æ–¹é¢ï¼Œåœ¨é‡å¤–çœŸå®åœºæ™¯ä¸‹çš„é¢éƒ¨æ•°æ®é›†ï¼ˆLabeled Face in the Wild (LFW)ï¼‰ä¸Šï¼Œä»¥1%çš„è¯¯è¯†ç‡ï¼ˆFalse Match Rate, FMRï¼‰è·å¾—äº†é«˜è¾¾96.77%çš„çœŸåŒ¹é…ç‡ï¼ˆTrue Match Rate, TMRï¼‰ï¼Œä¸”æ— éœ€å¾®è°ƒã€‚åœ¨è™¹è†œè¯†åˆ«æ–¹é¢ï¼Œåœ¨IITD-R-Fullæ•°æ®é›†ä¸Šï¼Œä»¥1%çš„è¯¯è¯†ç‡è·å¾—äº†é«˜è¾¾97.55%çš„çœŸåŒ¹é…ç‡ï¼ŒåŒæ ·æ— éœ€å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨è¿™äº›åµŒå…¥ä¹‹ä¸Šåº”ç”¨ç®€å•çš„åˆ†ç±»å™¨å¤´éƒ¨æœ‰åŠ©äºæ£€æµ‹äººè„¸æ·±åº¦ä¼ªé€ ã€æ£€æµ‹è™¹è†œçš„å‘ˆç°æ”»å‡»ï¼Œå¹¶ä»äººè„¸ä¸­æå–æ€§åˆ«å’Œç§æ—ç­‰è½¯ç”Ÿç‰©è¯†åˆ«å±æ€§ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚è¿™é¡¹å·¥ä½œå†æ¬¡å¼ºè°ƒäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨å®ç°äººå·¥æ™ºèƒ½é€šç”¨åŒ–é•¿æœŸæ„¿æ™¯æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24214v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒæ¨¡å‹å¦‚Vision-Language Models (VLMs)å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•ï¼Œäººå·¥æ™ºèƒ½çš„è¾¹ç•Œå¾—åˆ°äº†é‡æ–°å®šä¹‰ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­å±•ç°å‡ºå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©è¯†åˆ«å’Œåˆ†æé¢†åŸŸã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€é¡¹å…¨é¢çš„åŸºå‡†æµ‹è¯•è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨å…­ä¸ªç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼ŒåŒ…æ‹¬é¢éƒ¨å’Œè™¹è†œè¯†åˆ«ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹çš„åµŒå…¥å¯ç”¨äºå¤šç§ç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ï¼Œå¹¶å–å¾—äº†ä¸åŒç¨‹åº¦çš„æˆåŠŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ¨¡å‹å¦‚VLMså’ŒMLLMsæ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨ç”Ÿç‰©è¯†åˆ«é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå®Œå…¨æŒ–æ˜ã€‚</li>
<li>æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹åœ¨å…­ä¸ªç”Ÿç‰©è¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹çš„åµŒå…¥å¯ç”¨äºå¤šç§ä»»åŠ¡ï¼Œå¹¶å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>åœ¨é¢éƒ¨éªŒè¯ä»»åŠ¡ä¸­ï¼ŒLFWæ•°æ®é›†ä¸Šçš„True Match Rateè¾¾åˆ°äº†96.77%ï¼Œåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨è™¹è†œè¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒIITD-R-Fullæ•°æ®é›†ä¸Šçš„è¡¨ç°åŒæ ·å‡ºè‰²ï¼ŒéªŒè¯äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ç”Ÿç‰©è¯†åˆ«é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f60eea53400300380846b63c01009c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0003787a7bcac445ce6c92f5eb816c48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-370a87f266fb22d6ad8104c590cba166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-061e55a2380c28886a613fe13fb2f102.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33dad1b9250d66e0a0512c1de8be917b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Provably-Improving-Generalization-of-Few-Shot-Models-with-Synthetic-Data"><a href="#Provably-Improving-Generalization-of-Few-Shot-Models-with-Synthetic-Data" class="headerlink" title="Provably Improving Generalization of Few-Shot Models with Synthetic Data"></a>Provably Improving Generalization of Few-Shot Models with Synthetic Data</h2><p><strong>Authors:Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Tran Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than</strong></p>
<p>Few-shot image classification remains challenging due to the scarcity of labeled training examples. Augmenting them with synthetic data has emerged as a promising way to alleviate this issue, but models trained on synthetic samples often face performance degradation due to the inherent gap between real and synthetic distributions. To address this limitation, we develop a theoretical framework that quantifies the impact of such distribution discrepancies on supervised learning, specifically in the context of image classification. More importantly, our framework suggests practical ways to generate good synthetic samples and to train a predictor with high generalization ability. Building upon this framework, we propose a novel theoretical-based algorithm that integrates prototype learning to optimize both data partitioning and model training, effectively bridging the gap between real few-shot data and synthetic data. Extensive experiments results show that our approach demonstrates superior performance compared to state-of-the-art methods, outperforming them across multiple datasets. </p>
<blockquote>
<p>ç”±äºç¼ºå°‘æ ‡è®°çš„è®­ç»ƒæ ·æœ¬ï¼Œå°æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚é€šè¿‡åˆæˆæ•°æ®å¢å¼ºæ ·æœ¬å·²ç»æˆä¸ºç¼“è§£è¿™ä¸ªé—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†åœ¨åˆæˆæ ·æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹å¾€å¾€é¢ä¸´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå› ä¸ºçœŸå®å’Œåˆæˆåˆ†å¸ƒä¹‹é—´å­˜åœ¨å›ºæœ‰çš„å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹åˆ†å¸ƒå·®å¼‚å¯¹ç›‘ç£å­¦ä¹ çš„å½±å“è¿›è¡Œäº†é‡åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒåˆ†ç±»çš„ä¸Šä¸‹æ–‡ä¸­ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æå‡ºäº†ç”Ÿæˆè‰¯å¥½åˆæˆæ ·æœ¬å’Œè®­ç»ƒå…·æœ‰å‡ºè‰²æ³›åŒ–èƒ½åŠ›çš„é¢„æµ‹å™¨çš„å®ç”¨æ–¹æ³•ã€‚åŸºäºè¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç†è®ºçš„æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•ç»“åˆäº†åŸå‹å­¦ä¹ æ¥ä¼˜åŒ–æ•°æ®åˆ†åŒºå’Œæ¨¡å‹è®­ç»ƒï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†çœŸå®å°æ ·æœ¬æ•°æ®å’Œåˆæˆæ•°æ®ä¹‹é—´çš„å·®è·ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šå®ƒä»¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24190v1">PDF</a> ICML 2025. Our code will be released soon</p>
<p><strong>æ€»ç»“</strong></p>
<p>å°‘é‡æ ‡æ³¨è®­ç»ƒæ ·æœ¬çš„å›¾åƒåˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä½¿ç”¨åˆæˆæ•°æ®æ‰©å¢æ˜¯ä¸€ç§è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•ï¼Œä½†æ¨¡å‹åœ¨åˆæˆæ ·æœ¬ä¸Šçš„è®­ç»ƒå¾€å¾€ä¼šå‡ºç°æ€§èƒ½ä¸‹é™ï¼Œè¿™æ˜¯ç”±äºçœŸå®å’Œåˆæˆåˆ†å¸ƒä¹‹é—´å­˜åœ¨å›ºæœ‰å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œå®šé‡åˆ†æäº†è¿™ç§åˆ†å¸ƒå·®å¼‚å¯¹ç›‘ç£å­¦ä¹ çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒåˆ†ç±»çš„æƒ…å¢ƒä¸­ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†ç”Ÿæˆè‰¯å¥½åˆæˆæ ·æœ¬å’Œè®­ç»ƒå…·æœ‰é«˜é€šç”¨åŒ–èƒ½åŠ›çš„é¢„æµ‹å™¨çš„å®ç”¨æ–¹æ³•ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆåŸå‹å­¦ä¹ çš„æ–°ç®—æ³•ï¼Œä¼˜åŒ–æ•°æ®åˆ†åŒºå’Œæ¨¡å‹è®­ç»ƒï¼Œæœ‰æ•ˆå¼¥åˆäº†çœŸå®å°‘é‡æ•°æ®å’Œåˆæˆæ•°æ®ä¹‹é—´çš„å·®è·ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šå®ƒä»¬ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å°‘é‡æ ‡æ³¨è®­ç»ƒæ ·æœ¬çš„å›¾åƒåˆ†ç±»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨åˆæˆæ•°æ®æ‰©å¢å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>æ¨¡å‹åœ¨åˆæˆæ ·æœ¬ä¸Šçš„è®­ç»ƒå­˜åœ¨æ€§èƒ½ä¸‹é™çš„é£é™©ï¼ŒåŸå› æ˜¯çœŸå®å’Œåˆæˆæ•°æ®åˆ†å¸ƒä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
<li>æˆ‘ä»¬å»ºç«‹äº†ç†è®ºæ¡†æ¶æ¥åˆ†æè¿™ç§åˆ†å¸ƒå·®å¼‚å¯¹ç›‘ç£å­¦ä¹ çš„å½±å“ã€‚</li>
<li>è¯¥æ¡†æ¶æä¾›äº†ç”Ÿæˆè‰¯å¥½åˆæˆæ ·æœ¬å’Œè®­ç»ƒé«˜é€šç”¨é¢„æµ‹å™¨çš„æ–¹æ³•ã€‚</li>
<li>åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆåŸå‹å­¦ä¹ çš„æ–°ç®—æ³•ï¼Œä¼˜åŒ–æ•°æ®åˆ†åŒºå’Œæ¨¡å‹è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-429b39989878119a7631d86823005f7a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Proxy-FDA-Proxy-based-Feature-Distribution-Alignment-for-Fine-tuning-Vision-Foundation-Models-without-Forgetting"><a href="#Proxy-FDA-Proxy-based-Feature-Distribution-Alignment-for-Fine-tuning-Vision-Foundation-Models-without-Forgetting" class="headerlink" title="Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning   Vision Foundation Models without Forgetting"></a>Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning   Vision Foundation Models without Forgetting</h2><p><strong>Authors:Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald, Josh Susskind</strong></p>
<p>Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of concept forgetting on other tasks. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method Proxy-FDA that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. Experiments show that Proxy-FDA significantly reduces concept forgetting during fine-tuning, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDAâ€™s benefits in various fine-tuning settings (end-to-end, few-shot and continual tuning) and across different tasks like image classification, captioning and VQA. </p>
<blockquote>
<p>é¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šçš„è§†è§‰åŸºç¡€æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œæ¦‚å¿µè¿›è¡Œäº†ä¸°å¯Œçš„è¡¨ç¤ºï¼Œé€šè¿‡å¾®è°ƒå¯ä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå•ä¸€ä»»åŠ¡çš„å¾®è°ƒå¸¸å¸¸ä¼šå¯¼è‡´å…¶ä»–ä»»åŠ¡çš„æ¦‚å¿µé—å¿˜é—®é¢˜ã€‚æœ€è¿‘çš„ç¨³å¥å¾®è°ƒæ–¹æ³•æ—¨åœ¨å‡è½»å¯¹å…ˆå‰çŸ¥è¯†çš„é—å¿˜ï¼ŒåŒæ—¶ä¸å½±å“å¾®è°ƒæ€§èƒ½ã€‚çŸ¥è¯†é€šå¸¸é€šè¿‡åŒ¹é…åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹æƒé‡æˆ–ç‰¹å¾å¯¹æ¥ä¿ç•™ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„ç‚¹å¯¹ç‚¹åŒ¹é…å¯èƒ½è¿‡äºå¼ºçƒˆï¼Œæ²¡æœ‰æ˜ç¡®æ„è¯†åˆ°ç‰¹å¾é‚»åŸŸç»“æ„ä¹Ÿç¼–ç äº†ä¸°å¯Œçš„çŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æ–¹æ³•Proxy-FDAï¼Œå®ƒæ˜¾å¼åœ°ä¿ç•™ç‰¹å¾ç©ºé—´ä¸­çš„ç»“æ„çŸ¥è¯†ã€‚Proxy-FDAæ‰§è¡Œç‰¹å¾åˆ†å¸ƒå¯¹é½ï¼ˆä½¿ç”¨æœ€è¿‘é‚»å›¾ï¼‰åœ¨é¢„è®­ç»ƒçš„å’Œå¾®è°ƒåçš„ç‰¹å¾ç©ºé—´ä¹‹é—´ï¼Œé€šè¿‡å¯¹é½ä»¥åŠåŠ¨æ€ç”Ÿæˆä¿¡æ¯ä»£ç†æ¥æé«˜æ•°æ®å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒProxy-FDAåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾è‘—å‡å°‘äº†æ¦‚å¿µé—å¿˜ï¼Œæˆ‘ä»¬å‘ç°é—å¿˜ä¸åˆ†å¸ƒè·ç¦»åº¦é‡ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼ˆä¸L2è·ç¦»ç›¸æ¯”ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜äº†Proxy-FDAåœ¨å„ç§å¾®è°ƒè®¾ç½®ï¼ˆç«¯åˆ°ç«¯ã€å°æ ·æœ¬å’Œè¿ç»­è°ƒæ•´ï¼‰å’Œä¸åŒä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ï¼‰ä¸­çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24088v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šçš„è§†è§‰åŸºç¡€æ¨¡å‹ç¼–ç äº†ä¸°å¯Œçš„ç°å®ä¸–ç•Œæ¦‚å¿µè¡¨ç¤ºï¼Œé€šè¿‡å¾®è°ƒé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´å…¶ä»–ä»»åŠ¡çš„æ¦‚å¿µé—å¿˜é—®é¢˜ã€‚æœ€è¿‘æå‡ºçš„ç¨³å¥å¾®è°ƒæ–¹æ³•æ—¨åœ¨å‡è½»å¯¹å…ˆå‰çŸ¥è¯†çš„é—å¿˜ï¼ŒåŒæ—¶ä¸å½±å“å¾®è°ƒæ€§èƒ½ã€‚çŸ¥è¯†é€šå¸¸é€šè¿‡åŒ¹é…åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹æƒé‡æˆ–ç‰¹å¾å¯¹æ¥ä¿ç•™ã€‚ç„¶è€Œï¼Œè¿™ç§ç‚¹å¯¹ç‚¹åŒ¹é…å¯èƒ½è¿‡äºå¼ºçƒˆï¼Œæ²¡æœ‰æ˜ç¡®åœ°æ„è¯†åˆ°ç‰¹å¾é‚»åŸŸç»“æ„ä¹Ÿç¼–ç äº†ä¸°å¯Œçš„çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æ–¹æ³•â€”â€”Proxy-FDAï¼Œå®ƒæ˜¾å¼åœ°ä¿ç•™ç‰¹å¾ç©ºé—´ä¸­çš„ç»“æ„çŸ¥è¯†ã€‚Proxy-FDAæ‰§è¡Œç‰¹å¾åˆ†å¸ƒå¯¹é½ï¼ˆä½¿ç”¨æœ€è¿‘é‚»å›¾ï¼‰å’Œé¢„è®­ç»ƒä¸å¾®è°ƒç‰¹å¾ç©ºé—´ä¹‹é—´å¯¹é½ï¼Œé€šè¿‡åŠ¨æ€ç”Ÿæˆä¿¡æ¯ä»£ç†æ¥æé«˜æ•°æ®å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒProxy-FDAåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾è‘—å‡å°‘äº†æ¦‚å¿µé—å¿˜ï¼Œæˆ‘ä»¬å‘ç°é—å¿˜ä¸åˆ†å¸ƒè·ç¦»åº¦é‡ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼ˆä¸L2è·ç¦»ç›¸æ¯”ï¼‰ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†Proxy-FDAåœ¨å„ç§å¾®è°ƒè®¾ç½®ï¼ˆç«¯åˆ°ç«¯ã€å°æ ·æœ¬å’ŒæŒç»­è°ƒæ•´ï¼‰å’Œä¸åŒä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ï¼‰ä¸­çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šå¯ç¼–ç ä¸°å¯Œçš„ç°å®ä¸–ç•Œæ¦‚å¿µè¡¨ç¤ºã€‚</li>
<li>å¾®è°ƒåŸºç¡€æ¨¡å‹ä¼šå¯¼è‡´æ¦‚å¿µé—å¿˜é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„ç¨³å¥å¾®è°ƒæ–¹æ³•æ—¨åœ¨å‡è½»å¯¹å…ˆå‰çŸ¥è¯†çš„é—å¿˜ï¼ŒåŒæ—¶ä¿æŒå¾®è°ƒæ€§èƒ½ã€‚</li>
<li>çŸ¥è¯†ä¿ç•™é€šå¸¸é€šè¿‡åŒ¹é…æ¨¡å‹æƒé‡æˆ–ç‰¹å¾å¯¹æ¥å®ç°ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½è¿‡äºå¼ºçƒˆã€‚</li>
<li>Proxy-FDAæ–¹æ³•é€šè¿‡æ˜¾å¼åœ°ä¿ç•™ç‰¹å¾ç©ºé—´ä¸­çš„ç»“æ„çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>Proxy-FDAä½¿ç”¨æœ€è¿‘é‚»å›¾æ‰§è¡Œç‰¹å¾åˆ†å¸ƒå¯¹é½ï¼Œå¹¶é€šè¿‡åŠ¨æ€ç”Ÿæˆä¿¡æ¯ä»£ç†æé«˜æ•°æ®å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cbe9ce85827d4c6054481f211ff7f69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-daa84e109fd760ce347a958c32fce9a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faf7699a505c48a65c96ba6d42fd7165.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2a7f0f30c3d648d1606938e27bc0820.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought"><a href="#VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought" class="headerlink" title="VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought"></a>VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought</h2><p><strong>Authors:Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang</strong></p>
<p>Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual \textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and \textbf{R}easoning), a framework that equips an MLLM with the ability to (i) decide \emph{when} additional visual evidence is needed, (ii) determine \emph{where} to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization (R-GRPO)}, a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ¨ç†çš„MLLMsï¼ˆMasked Language Modelingï¼‰åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†ä¸€å®šçš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦åŠ¨æ€å’Œè¿­ä»£åœ°å…³æ³¨å¹¶é‡æ–°è®¿é—®è§†è§‰åŒºåŸŸï¼Œä»¥å®ç°æ–‡æœ¬æ¨ç†åœ¨è§†è§‰è¯æ®ä¸­çš„ç²¾ç¡®å®šä½ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>VLM-R$^3$<strong>ï¼ˆé…å¤‡åŒºåŸŸè¯†åˆ«å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç®€ç§°VLM-R$^3$ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒä¸ºMLLMæä¾›äº†ä»¥ä¸‹èƒ½åŠ›ï¼šï¼ˆiï¼‰å†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ï¼Œï¼ˆiiï¼‰ç¡®å®šåœ¨å›¾åƒä¸­çš„å®šä½ä½ç½®ï¼Œï¼ˆiiiï¼‰æ— ç¼åœ°å°†ç›¸å…³çš„å­å›¾åƒå†…å®¹é‡æ–°ç¼–ç»‡æˆè¿è´¯çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯</strong>åŒºåŸŸæ¡ä»¶å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆR-GRPOï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆä¾‹å¦‚è£å‰ªã€æ”¾å¤§ï¼‰å¹¶å°†å¾—åˆ°çš„è§†è§‰ä¸Šä¸‹æ–‡é›†æˆåˆ°éšåçš„æ¨ç†æ­¥éª¤ä¸­ã€‚ä¸ºäº†å¼•å¯¼ç­–ç•¥ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªé€‚åº¦ä½†ç²¾å¿ƒç­–åˆ’çš„Visuo-Lingual Interleaved Rationaleï¼ˆVLIRï¼‰è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“æä¾›äº†å…³äºåŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬ä¾æ®çš„æ­¥éª¤çº§ç›‘ç£ã€‚åœ¨MathVistaã€ScienceQAå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVLM-R$^3$åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¾®å¦™çš„ç©ºé—´æ¨ç†æˆ–ç²¾ç»†çš„è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šï¼Œå…¶æ”¶ç›Šæœ€å¤§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16192v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ¡†æ¶VLM-R$^3$ç»“åˆäº†è§†è§‰è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°äº†åŠ¨æ€è¿­ä»£å…³æ³¨å›¾åƒåŒºåŸŸï¼Œç²¾ç¡®åœ°å°†æ–‡æœ¬æ¨ç†ä¸è§†è§‰è¯æ®å¯¹æ¥ã€‚é€šè¿‡åŒºåŸŸæ¡ä»¶å¼ºåŒ–æ”¿ç­–ä¼˜åŒ–ï¼ˆR-GRPOï¼‰è®­ç»ƒæ¨¡å¼ï¼Œæ¨¡å‹å¯è‡ªä¸»ç­›é€‰ä¿¡æ¯åŒºåŸŸã€æ‰§è¡Œé€‚å½“å˜æ¢å¹¶èå…¥åç»­æ¨ç†æ­¥éª¤ã€‚é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„Visuo-Lingual Interleaved Rationaleè¯­æ–™åº“è¿›è¡Œæ”¿ç­–å¼•å¯¼ã€‚åœ¨MathVistaã€ScienceQAç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†å’Œç»†å¾®è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLM-R$^3$ç»“åˆäº†è§†è§‰è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°åŠ¨æ€è¿­ä»£å…³æ³¨å›¾åƒåŒºåŸŸã€‚</li>
<li>é€šè¿‡R-GRPOè®­ç»ƒæ¨¡å¼ï¼Œæ¨¡å‹å¯è‡ªä¸»ç­›é€‰ä¿¡æ¯åŒºåŸŸå¹¶æ‰§è¡Œé€‚å½“å˜æ¢ã€‚</li>
<li>VLM-R$^3$åŒ…å«Visuo-Lingual Interleaved Rationaleè¯­æ–™åº“è¿›è¡Œæ”¿ç­–å¼•å¯¼ã€‚</li>
<li>VLM-R$^3$åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†å’Œè§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨å¿…è¦æ—¶è·å–é¢å¤–çš„è§†è§‰è¯æ®å¹¶ç²¾å‡†å®šä½å›¾åƒå†…å®¹ã€‚</li>
<li>VLM-R$^3$èƒ½æ— ç¼å°†ç›¸å…³çš„å­å›¾åƒå†…å®¹èå…¥é“¾å¼æ€ç»´è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a436ebc8a1fb336805b4d7637d8a6a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3efa3118bcc66eac357d292593cbeb63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ade60abc3bdc5760f02256ca2c95a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2199ea2b2d0aacf723ef9262927e083.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Sundial-A-Family-of-Highly-Capable-Time-Series-Foundation-Models"><a href="#Sundial-A-Family-of-Highly-Capable-Time-Series-Foundation-Models" class="headerlink" title="Sundial: A Family of Highly Capable Time Series Foundation Models"></a>Sundial: A Family of Highly Capable Time Series Foundation Models</h2><p><strong>Authors:Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long</strong></p>
<p>We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patchâ€™s distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundialâ€™s pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/thuml/Sundial">https://github.com/thuml/Sundial</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Sundialï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŸç”Ÿã€çµæ´»ä¸”å¯æ‰©å±•çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ã€‚ä¸ºäº†é¢„æµ‹ä¸‹ä¸€ä¸ªè¡¥ä¸çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµåŒ¹é…çš„TimeFlow Lossï¼Œå®ƒä¿ƒè¿›äº†åœ¨è¿ç»­å€¼æ—¶é—´åºåˆ—ä¸Šå¯¹Transformerè¿›è¡ŒåŸç”Ÿé¢„è®­ç»ƒï¼Œæ— éœ€è¿›è¡Œç¦»æ•£ä»¤ç‰ŒåŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨ä»»æ„é•¿åº¦çš„æ—¶åºæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ— éœ€æŒ‡å®šä»»ä½•å…ˆéªŒåˆ†å¸ƒï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„é¢„æµ‹ç»“æœï¼Œç›¸è¾ƒäºä½¿ç”¨å‚æ•°å¯†åº¦çš„æ–¹æ³•ï¼Œåœ¨è¡¨ç¤ºå­¦ä¹ ä¸­å®ç°äº†æ›´é«˜çš„çµæ´»æ€§ã€‚é’ˆå¯¹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹Transformerè¿›è¡Œäº†å¾®å°ä½†å…³é”®æ€§çš„è°ƒæ•´ï¼Œå¹¶æ•´ç†äº†åŒ…å«åäº¿æ—¶é—´ç‚¹çš„TimeBenchæ•°æ®é›†ï¼Œä¸»è¦ç”±ç°å®ä¸–ç•Œæ•°æ®é›†å’Œåˆæˆæ•°æ®ç»„æˆã€‚é€šè¿‡TimeFlow Losså‡è½»æ¨¡å¼å´©æºƒé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨TimeBenchä¸Šå¯¹ä¸€ç³»åˆ—Sundialæ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„æ¨¡å‹å®¹é‡å’Œæ³›åŒ–æ€§èƒ½ã€‚é™¤äº†å‡ºè‰²çš„å¯æ‰©å±•æ€§ä¹‹å¤–ï¼ŒSundialåœ¨ç‚¹é¢„æµ‹å’Œæ¦‚ç‡é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå³æ—¶æ¨ç†é€Ÿåº¦æå¿«ï¼Œå³é›¶æ ·æœ¬é¢„æµ‹ä»…éœ€å‡ æ¯«ç§’ã€‚æˆ‘ä»¬ç›¸ä¿¡Sundialå¼€åˆ›æ€§çš„ç”Ÿæˆé¢„æµ‹èƒ½åŠ›å¯ä»¥æé«˜ç°å®ä¸–ç•Œå†³ç­–ä¸­çš„æ¨¡å‹å¯é æ€§ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/thuml/Sundial%E3%80%82">https://github.com/thuml/Sundialã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00816v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Sundialç³»åˆ—åŸç”Ÿã€çµæ´»ã€å¯æ‰©å±•çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ã€‚æå‡ºåŸºäºæµåŒ¹é…çš„TimeFlow Lossé¢„æµ‹ä¸‹ä¸€ä¸ªè¡¥ä¸çš„åˆ†å¸ƒï¼Œå®ç°äº†åœ¨æ— éœ€ç¦»æ•£æ ‡è®°åŒ–çš„æƒ…å†µä¸‹å¯¹è¿ç»­å€¼æ—¶é—´åºåˆ—è¿›è¡ŒåŸç”Ÿé¢„è®­ç»ƒã€‚æ¨¡å‹åœ¨ä»»æ„é•¿åº¦çš„æ—¶é—´åºåˆ—ä¸Šé¢„è®­ç»ƒï¼Œæ— éœ€æŒ‡å®šä»»ä½•å…ˆéªŒåˆ†å¸ƒï¼Œå¯ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„é¢„æµ‹ç»“æœï¼Œæé«˜äº†è¡¨ç¤ºå­¦ä¹ çš„çµæ´»æ€§ã€‚é€šè¿‡ä½¿ç”¨æœ€å°ä½†å…³é”®çš„Transformeré€‚åº”ç­–ç•¥å’Œå¤§è§„æ¨¡æ•°æ®é›†TimeBenchï¼Œæˆ‘ä»¬è®­ç»ƒäº†Sundialæ¨¡å‹å®¶æ—ï¼Œè¯¥æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”åœ¨ç‚¹å’Œæ¦‚ç‡é¢„æµ‹æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€ä½³æ•ˆæœï¼Œè¿˜å…·æœ‰å³æ—¶æ¨ç†é€Ÿåº¦ï¼Œå¯åœ¨æ¯«ç§’å†…å®ç°é›¶å°„å‡»é¢„æµ‹ã€‚æˆ‘ä»¬è®¤ä¸ºSundialå¼€åˆ›æ€§çš„ç”Ÿæˆé¢„æµ‹èƒ½åŠ›èƒ½å¤Ÿæé«˜å®é™…å†³ç­–ä¸­çš„æ¨¡å‹å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sundialæ˜¯ä¸€ä¸ªåŸç”Ÿã€çµæ´»ã€å¯æ‰©å±•çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å®¶æ—ã€‚</li>
<li>æå‡ºTimeFlow Lossæ–¹æ³•é¢„æµ‹ä¸‹ä¸€ä¸ªè¡¥ä¸çš„åˆ†å¸ƒï¼Œè¯¥æ–¹æ³•åŸºäºæµåŒ¹é…ä¸”é€‚ç”¨äºè¿ç»­å€¼æ—¶é—´åºåˆ—çš„é¢„è®­ç»ƒã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»æ„é•¿åº¦çš„æ—¶é—´åºåˆ—ä¸Šé¢„è®­ç»ƒï¼Œæ— éœ€æŒ‡å®šå…ˆéªŒåˆ†å¸ƒï¼Œå¹¶èƒ½ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„é¢„æµ‹ç»“æœã€‚</li>
<li>åˆ©ç”¨æœ€å°ä½†å…³é”®çš„Transformeré€‚åº”ç­–ç•¥å’Œå¤§è§„æ¨¡æ•°æ®é›†TimeBenchè®­ç»ƒäº†Sundialæ¨¡å‹å®¶æ—ã€‚</li>
<li>Sundialå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œåœ¨ç‚¹å’Œæ¦‚ç‡é¢„æµ‹æ–¹é¢å‡è¾¾åˆ°äº†æœ€ä½³æ•ˆæœï¼Œä¸”èƒ½å®ç°å¿«é€Ÿçš„å³æ—¶æ¨ç†ã€‚</li>
<li>è¯¥æ¨¡å‹å±•ç°å‡ºå¯é çš„ç”Ÿæˆé¢„æµ‹èƒ½åŠ›ï¼Œæœ‰æœ›æå‡å®é™…å†³ç­–ä¸­çš„æ¨¡å‹å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7edd13f3413defff0e12300053b140fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34a4e7b61c5d3ce97e00dc715ce529cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-626e7450f4bf0496fb315c173dd1c8f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4d397ba0b78a313b6215552616c6c00.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="QPO-Query-dependent-Prompt-Optimization-via-Multi-Loop-Offline-Reinforcement-Learning"><a href="#QPO-Query-dependent-Prompt-Optimization-via-Multi-Loop-Offline-Reinforcement-Learning" class="headerlink" title="QPO: Query-dependent Prompt Optimization via Multi-Loop Offline   Reinforcement Learning"></a>QPO: Query-dependent Prompt Optimization via Multi-Loop Offline   Reinforcement Learning</h2><p><strong>Authors:Yilun Kong, Hangyu Mao, Qi Zhao, Bin Zhang, Jingqing Ruan, Li Shen, Yongzhe Chang, Xueqian Wang, Rui Zhao, Dacheng Tao</strong></p>
<p>Prompt engineering has demonstrated remarkable success in enhancing the performance of large language models (LLMs) across diverse tasks. However, most existing prompt optimization methods only focus on the task-level performance, overlooking the importance of query-preferred prompts, which leads to suboptimal performances. Additionally, these methods rely heavily on frequent interactions with LLMs to obtain feedback for guiding the optimization process, incurring substantial redundant interaction costs. In this paper, we introduce Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline reinforcement learning to iteratively fine-tune a small pretrained language model to generate optimal prompts tailored to the input queries, thus significantly improving the prompting effect on the large target LLM. We derive insights from offline prompting demonstration data, which already exists in large quantities as a by-product of benchmarking diverse prompts on open-sourced tasks, thereby circumventing the expenses of online interactions. Furthermore, we continuously augment the offline dataset with the generated prompts in each loop, as the prompts from the fine-tuned model are supposed to outperform the source prompts in the original dataset. These iterative loops bootstrap the model towards generating optimal prompts. Experiments on various LLM scales and diverse NLP and math tasks demonstrate the efficacy and cost-efficiency of our method in both zero-shot and few-shot scenarios. </p>
<blockquote>
<p>æç¤ºå·¥ç¨‹åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æç¤ºä¼˜åŒ–æ–¹æ³•åªå…³æ³¨ä»»åŠ¡çº§åˆ«çš„æ€§èƒ½ï¼Œå¿½è§†äº†æŸ¥è¯¢åå¥½æç¤ºçš„é‡è¦æ€§ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•è¿˜ä¸¥é‡ä¾èµ–äºä¸LLMçš„é¢‘ç¹äº¤äº’æ¥è·å¾—åé¦ˆä»¥æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ï¼Œäº§ç”Ÿäº†å¤§é‡çš„å†—ä½™äº¤äº’æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŸ¥è¯¢ä¾èµ–æç¤ºä¼˜åŒ–ï¼ˆQPOï¼‰ï¼Œå®ƒåˆ©ç”¨å¤šå¾ªç¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¥è¿­ä»£å¾®è°ƒä¸€ä¸ªå°çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé’ˆå¯¹è¾“å…¥æŸ¥è¯¢çš„å®šåˆ¶æœ€ä¼˜æç¤ºï¼Œä»è€Œæ˜¾è‘—æé«˜åœ¨å¤§ç›®æ ‡LLMä¸Šçš„æç¤ºæ•ˆæœã€‚æˆ‘ä»¬ä»ç¦»çº¿æç¤ºæ¼”ç¤ºæ•°æ®ä¸­è·å¾—è§è§£ï¼Œè¿™äº›æ•°æ®ä½œä¸ºåœ¨å¼€æºä»»åŠ¡ä¸Šè¯„ä¼°å„ç§æç¤ºçš„å‰¯äº§å“è€Œå¤§é‡å­˜åœ¨ï¼Œä»è€Œé¿å…äº†åœ¨çº¿äº¤äº’çš„è´¹ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨æ¯ä¸ªå¾ªç¯ä¸­ä¸æ–­æ‰©å……ç¦»çº¿æ•°æ®é›†ä¸ç”Ÿæˆçš„æç¤ºï¼Œå› ä¸ºå¾®è°ƒæ¨¡å‹äº§ç”Ÿçš„æç¤ºåº”è¯¥ä¼˜äºåŸå§‹æ•°æ®é›†ä¸­çš„æºæç¤ºã€‚è¿™äº›è¿­ä»£å¾ªç¯ä½¿æ¨¡å‹æœç€ç”Ÿæˆæœ€ä½³æç¤ºçš„æ–¹å‘å‘å±•ã€‚åœ¨å„ç§è§„æ¨¡çš„LLMå’Œä¸åŒçš„NLPå’Œæ•°å­¦ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨é›¶å‡»å’Œå°‘å‡»åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10504v2">PDF</a> Transactions on Machine Learning Research (TMLR)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æŸ¥è¯¢ä¾èµ–çš„æç¤ºä¼˜åŒ–ï¼ˆQPOï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å¤šå¾ªç¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¥å¾®è°ƒå°å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé’ˆå¯¹è¾“å…¥æŸ¥è¯¢é‡èº«å®šåˆ¶çš„æœ€ä¼˜æç¤ºã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†åœ¨å¤§ç›®æ ‡è¯­è¨€æ¨¡å‹ä¸Šçš„æç¤ºæ•ˆæœï¼Œè€Œä¸”é€šè¿‡åˆ©ç”¨ç¦»çº¿æç¤ºæ¼”ç¤ºæ•°æ®æ¥é™ä½æˆæœ¬ï¼Œå¹¶æŒç»­ç”¨ç”Ÿæˆæç¤ºæ‰©å……ç¦»çº¿æ•°æ®é›†ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç¤ºå·¥ç¨‹åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ–¹é¢è¡¨ç°æ˜¾è‘—ã€‚</li>
<li>ç°æœ‰æç¤ºä¼˜åŒ–æ–¹æ³•ä¸»è¦å…³æ³¨ä»»åŠ¡çº§åˆ«æ€§èƒ½ï¼Œå¿½ç•¥äº†æŸ¥è¯¢åå¥½æç¤ºçš„é‡è¦æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>QPOæ–¹æ³•åˆ©ç”¨å¤šå¾ªç¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¥å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç”Ÿæˆé’ˆå¯¹è¾“å…¥æŸ¥è¯¢çš„æœ€ä¼˜æç¤ºã€‚</li>
<li>QPOé€šè¿‡åˆ©ç”¨ç¦»çº¿æç¤ºæ¼”ç¤ºæ•°æ®æ¥é™ä½åœ¨çº¿äº¤äº’æˆæœ¬ã€‚</li>
<li>QPOæ–¹æ³•æŒç»­ä½¿ç”¨ç”Ÿæˆæç¤ºæ‰©å……ç¦»çº¿æ•°æ®é›†ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>QPOåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œæˆæœ¬æ•ˆç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e9d3caf10850800b0f33bfed83c3124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93facc229f5172e9a31f93ae6cf4aaef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a66f5eff5840b3048c2d8b16ec1d49b0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-03/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Segmenting France Across Four Centuries
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-aa72a0d2b3a94241e899ba49bae8f062.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Open CaptchaWorld A Comprehensive Web-based Platform for Testing and   Benchmarking Multimodal LLM Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23394.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
