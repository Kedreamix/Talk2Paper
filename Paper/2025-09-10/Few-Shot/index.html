<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  UNH at CheckThat! 2025 Fine-tuning Vs Prompting in Claim Extraction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-18d2b7afb6a0919fa0f40ed6f97182ac.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-10-æ›´æ–°"><a href="#2025-09-10-æ›´æ–°" class="headerlink" title="2025-09-10 æ›´æ–°"></a>2025-09-10 æ›´æ–°</h1><h2 id="UNH-at-CheckThat-2025-Fine-tuning-Vs-Prompting-in-Claim-Extraction"><a href="#UNH-at-CheckThat-2025-Fine-tuning-Vs-Prompting-in-Claim-Extraction" class="headerlink" title="UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction"></a>UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</h2><p><strong>Authors:Joe Wilder, Nikhil Kadapala, Benji Xu, Mohammed Alsaadi, Aiden Parsons, Mitchell Rogers, Palash Agarwal, Adam Hassick, Laura Dietz</strong></p>
<p>We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower. </p>
<blockquote>
<p>æˆ‘ä»¬å‚ä¸äº†CheckThat! Task 2çš„è‹±è¯­ä»»åŠ¡ï¼Œæ¢ç´¢äº†å„ç§æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å°‘é‡æç¤ºå’Œä¸åŒLLMå®¶æ—çš„å¾®è°ƒï¼Œæ—¨åœ¨ä»ç¤¾äº¤åª’ä½“æ®µè½ä¸­æå–å€¼å¾—æ£€æŸ¥çš„å£°æ˜ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒFLAN-T5æ¨¡å‹è·å¾—äº†æœ€ä½³METEORåˆ†æ•°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å³ä½¿ä½¿ç”¨å…¶ä»–æ–¹æ³•è·å¾—çš„METEORåˆ†æ•°è¾ƒä½ï¼Œæœ‰æ—¶ä¹Ÿèƒ½æå–å‡ºæ›´é«˜è´¨é‡çš„å£°æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06883v1">PDF</a> 16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,   Madrid, Spain</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨CheckThat! Task 2 Englishä»»åŠ¡ä¸­ï¼Œé€šè¿‡ä¸åŒçš„LLMå®¶æ—æ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬æç¤ºå’Œå¾®è°ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä»ç¤¾äº¤åª’ä½“æ®µè½ä¸­æå–å€¼å¾—éªŒè¯çš„å£°æ˜ã€‚æœ€ä½³METEORå¾—åˆ†æ˜¯é€šè¿‡å¾®è°ƒFLAN-T5æ¨¡å‹å®ç°çš„ï¼Œä½†å…¶ä»–æ–¹æ³•æœ‰æ—¶ä¹Ÿèƒ½æå–å‡ºæ›´é«˜è´¨é‡çš„å£°æ˜ï¼Œå³ä½¿å…¶METEORå¾—åˆ†è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚ä¸CheckThat! Task 2 Englishä»»åŠ¡ï¼Œä¸“æ³¨äºä»ç¤¾äº¤åª’ä½“æ®µè½ä¸­æå–å€¼å¾—éªŒè¯çš„å£°æ˜ã€‚</li>
<li>æ¢ç´¢äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬æç¤ºå’Œé’ˆå¯¹ä¸åŒLLMå®¶æ—çš„å¾®è°ƒã€‚</li>
<li>é€šè¿‡å¾®è°ƒFLAN-T5æ¨¡å‹è·å¾—äº†æœ€ä½³METEORå¾—åˆ†ã€‚</li>
<li>é«˜è´¨é‡çš„å£°æ˜æœ‰æ—¶å¯ä»¥é€šè¿‡å…¶ä»–æ–¹æ³•æå–ã€‚</li>
<li>METEORå¾—åˆ†å¹¶éå”¯ä¸€è¡¡é‡å£°æ˜è´¨é‡çš„æŒ‡æ ‡ã€‚</li>
<li>åœ¨å¤„ç†ç¤¾äº¤åª’ä½“æ–‡æœ¬æ—¶ï¼Œéœ€è¦è€ƒè™‘å¤šç§ç­–ç•¥æ¥æé«˜æå–å£°æ˜è´¨é‡çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a08c44171d8bf2408c5404011e15858.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80f948f42e1ab2369c1f0fcf1e6b8cb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b91223234efc402d456fedb81b4bb8c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="O-3-Afford-One-Shot-3D-Object-to-Object-Affordance-Grounding-for-Generalizable-Robotic-Manipulation"><a href="#O-3-Afford-One-Shot-3D-Object-to-Object-Affordance-Grounding-for-Generalizable-Robotic-Manipulation" class="headerlink" title="O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for   Generalizable Robotic Manipulation"></a>O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for   Generalizable Robotic Manipulation</h2><p><strong>Authors:Tongxuan Tian, Xuhui Kang, Yen-Ling Kuo</strong></p>
<p>Grounding object affordance is fundamental to robotic manipulation as it establishes the critical link between perception and action among interacting objects. However, prior works predominantly focus on predicting single-object affordance, overlooking the fact that most real-world interactions involve relationships between pairs of objects. In this work, we address the challenge of object-to-object affordance grounding under limited data contraints. Inspired by recent advances in few-shot learning with 2D vision foundation models, we propose a novel one-shot 3D object-to-object affordance learning approach for robotic manipulation. Semantic features from vision foundation models combined with point cloud representation for geometric understanding enable our one-shot learning pipeline to generalize effectively to novel objects and categories. We further integrate our 3D affordance representation with large language models (LLMs) for robotics manipulation, significantly enhancing LLMsâ€™ capability to comprehend and reason about object interactions when generating task-specific constraint functions. Our experiments on 3D object-to-object affordance grounding and robotic manipulation demonstrate that our O$^3$Afford significantly outperforms existing baselines in terms of both accuracy and generalization capability. </p>
<blockquote>
<p>å¯¹è±¡åŠŸèƒ½è®¤çŸ¥åœ¨æœºå™¨äººæ“æ§ä¸­èµ·åˆ°åŸºç¡€æ€§ä½œç”¨ï¼Œå› ä¸ºå®ƒå»ºç«‹äº†äº¤äº’å¯¹è±¡é—´æ„ŸçŸ¥ä¸è¡ŒåŠ¨çš„å…³é”®è”ç³»ã€‚ç„¶è€Œï¼Œæ—©æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­äºé¢„æµ‹å•ä¸ªå¯¹è±¡çš„åŠŸèƒ½è®¤çŸ¥ï¼Œå¿½ç•¥äº†ç°å®ä¸­å¤§å¤šæ•°äº’åŠ¨éƒ½æ¶‰åŠä¸¤ä¸ªå¯¹è±¡é—´å…³ç³»çš„äº‹å®ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨æœ‰é™æ•°æ®çº¦æŸä¸‹çš„å¯¹è±¡é—´åŠŸèƒ½è®¤çŸ¥å®šä½æŒ‘æˆ˜ã€‚å—è¿‘æœŸå°‘é‡å­¦ä¹ 2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é’ˆå¯¹æœºå™¨äººæ“æ§çš„ä¸€æ¬¡æ€§3Då¯¹è±¡é—´åŠŸèƒ½è®¤çŸ¥å­¦ä¹ æ–¹æ³•ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„è¯­ä¹‰ç‰¹å¾ä¸ç‚¹äº‘è¡¨ç¤ºç›¸ç»“åˆè¿›è¡Œå‡ ä½•ç†è§£ï¼Œä½¿æˆ‘ä»¬çš„å•æ¬¡å­¦ä¹ ç®¡é“èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ–°å‹å¯¹è±¡å’Œç±»åˆ«ä¸Šã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ä¸‰ç»´åŠŸèƒ½è®¤çŸ¥è¡¨ç°ä¸ç”¨äºæœºå™¨äººæ“æ§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›ä¸€æ­¥æ•´åˆï¼Œæ˜¾è‘—æé«˜äº†LLMåœ¨ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„çº¦æŸå‡½æ•°æ—¶ç†è§£å’Œæ¨ç†å¯¹è±¡äº¤äº’çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ç»´å¯¹è±¡é—´åŠŸèƒ½è®¤çŸ¥å®šä½å’Œæœºå™¨äººæ“æ§ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„O$^3$Affordåœ¨ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†æµ‹è¯•æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06233v1">PDF</a> Conference on Robot Learning (CoRL) 2025. Project website:   <a target="_blank" rel="noopener" href="https://o3afford.github.io/">https://o3afford.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡å…³æ³¨æœºå™¨äººæ“ä½œä¸­çš„å¯¹è±¡é—´å¯è´Ÿæ‹…æ€§ï¼ˆobject-to-object affordanceï¼‰é—®é¢˜ï¼Œå¼ºè°ƒåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå¯¹è±¡é—´çš„äº¤äº’æ˜¯å¸¸æ€ã€‚é’ˆå¯¹æ•°æ®æœ‰é™çš„æƒ…å†µï¼Œç ”ç©¶å›¢é˜Ÿå—äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹çš„å¯å‘ï¼Œæå‡ºä¸€ç§é’ˆå¯¹æœºå™¨äººæ“ä½œçš„ä¸€æ¬¡æ€§ä¸‰ç»´å¯¹è±¡é—´å¯è´Ÿæ‹…æ€§å­¦ä¹ ï¼ˆone-shot 3D object-to-object affordance learningï¼‰æ–¹æ³•ã€‚ç»“åˆè§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾ä¸ç‚¹äº‘è¡¨ç¤ºæ³•ï¼Œæ­¤æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°æ¨å¹¿è‡³æ–°å¯¹è±¡å’Œç±»åˆ«ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å›¢é˜Ÿè¿˜æ•´åˆäº†ä»–ä»¬çš„ä¸‰ç»´å¯è´Ÿæ‹…æ€§è¡¨ç¤ºæ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ˜¾è‘—æé«˜äº†LLMsåœ¨ç”Ÿæˆç‰¹å®šä»»åŠ¡çº¦æŸå‡½æ•°æ—¶å¯¹å¯¹è±¡äº¤äº’çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œä»–ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´å¯¹è±¡é—´å¯è´Ÿæ‹…æ€§å’Œæœºå™¨äººæ“ä½œæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººæ“ä½œä¸­å¯¹è±¡é—´å¯è´Ÿæ‹…æ€§çš„é‡è¦æ€§è¢«å¼ºè°ƒï¼Œåæ˜ äº†ç°å®ä¸–ç•Œä¸­å¯¹è±¡äº¤äº’çš„æ™®éæ€§ã€‚</li>
<li>é¢å¯¹æ•°æ®é™åˆ¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä¸€æ¬¡æ€§ä¸‰ç»´å¯¹è±¡é—´å¯è´Ÿæ‹…æ€§å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç»“åˆè§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾å’Œç‚¹äº‘è¡¨ç¤ºæ³•ï¼Œæé«˜å¯¹æ–°å¯¹è±¡å’Œç±»åˆ«çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›å¢å¼ºã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨å•ä¸ªå¯¹è±¡çš„å¯è´Ÿæ‹…æ€§é¢„æµ‹ï¼Œæ›´æ³¨é‡å¯¹è±¡é—´çš„äº¤äº’å…³ç³»ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç»´å¯¹è±¡é—´å¯è´Ÿæ‹…æ€§å’Œæœºå™¨äººæ“ä½œæ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-235aea26d90aac22c345ffed15b5c0f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9764c3d4ed3ba11f38548bf5c45b1963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccb70f1116c90504de51f102c49d283b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8cfd53add7717f00d82f1a682d72d6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Query-Intent-Detection-via-Relation-Aware-Prompt-Learning"><a href="#Few-Shot-Query-Intent-Detection-via-Relation-Aware-Prompt-Learning" class="headerlink" title="Few-Shot Query Intent Detection via Relation-Aware Prompt Learning"></a>Few-Shot Query Intent Detection via Relation-Aware Prompt Learning</h2><p><strong>Authors:Liang Zhang, Yuan Li, Shijie Zhang, Zheng Zhang, Xitong Li</strong></p>
<p>Intent detection is a crucial component of modern conversational systems, since accurately identifying user intent at the beginning of a conversation is essential for generating effective responses. Recent efforts have focused on studying this problem under a challenging few-shot scenario. These approaches primarily leverage large-scale unlabeled dialogue text corpora to pretrain language models through various pretext tasks, followed by fine-tuning for intent detection with very limited annotations. Despite the improvements achieved, existing methods have predominantly focused on textual data, neglecting to effectively capture the crucial structural information inherent in conversational systems, such as the query-query relation and query-answer relation. To address this gap, we propose SAID, a novel framework that integrates both textual and relational structure information in a unified manner for model pretraining for the first time. Building on this framework, we further propose a novel mechanism, the query-adaptive attention network (QueryAdapt), which operates at the relation token level by generating intent-specific relation tokens from well-learned query-query and query-answer relations explicitly, enabling more fine-grained knowledge transfer. Extensive experimental results on two real-world datasets demonstrate that SAID significantly outperforms state-of-the-art methods. </p>
<blockquote>
<p>æ„å›¾æ£€æµ‹æ˜¯ç°ä»£å¯¹è¯ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå› ä¸ºåœ¨å¯¹è¯å¼€å§‹æ—¶å‡†ç¡®è¯†åˆ«ç”¨æˆ·æ„å›¾å¯¹äºç”Ÿæˆæœ‰æ•ˆçš„å“åº”è‡³å…³é‡è¦ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„è¿™ä¸ªé—®é¢˜ä¸Šã€‚è¿™äº›æ–¹æ³•ä¸»è¦åˆ©ç”¨å¤§è§„æ¨¡çš„æœªæ ‡æ³¨å¯¹è¯æ–‡æœ¬è¯­æ–™åº“ï¼Œé€šè¿‡å„ç§å‰åºä»»åŠ¡æ¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç„¶åä½¿ç”¨éå¸¸æœ‰é™çš„æ³¨é‡Šè¿›è¡Œæ„å›¾æ£€æµ‹çš„å¾®è°ƒã€‚å°½ç®¡å–å¾—äº†æ”¹è¿›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æ•°æ®ä¸Šï¼Œå¿½è§†äº†æœ‰æ•ˆæ•è·å¯¹è¯ç³»ç»Ÿä¸­å›ºæœ‰çš„å…³é”®ç»“æ„ä¿¡æ¯ï¼Œå¦‚æŸ¥è¯¢-æŸ¥è¯¢å…³ç³»å’ŒæŸ¥è¯¢-ç­”æ¡ˆå…³ç³»ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†SAIDï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œé¦–æ¬¡ä»¥ç»Ÿä¸€çš„æ–¹å¼å°†æ–‡æœ¬å’Œå…³ç³»ç»“æ„ä¿¡æ¯æ•´åˆåœ¨ä¸€èµ·è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶ï¼Œå³æŸ¥è¯¢è‡ªé€‚åº”æ³¨æ„åŠ›ç½‘ç»œï¼ˆQueryAdaptï¼‰ï¼Œå®ƒåœ¨å…³ç³»ä»¤ç‰Œçº§åˆ«æ“ä½œï¼Œé€šè¿‡ä»å­¦ä¹ è‰¯å¥½çš„æŸ¥è¯¢-æŸ¥è¯¢å’ŒæŸ¥è¯¢-ç­”æ¡ˆå…³ç³»ä¸­ç”Ÿæˆç‰¹å®šçš„æ„å›¾å…³ç³»ä»¤ç‰Œï¼Œå®ç°æ›´ç²¾ç»†çš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒSAIDæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†æ„å›¾æ£€æµ‹åœ¨ç°ä»£å¯¹è¯ç³»ç»Ÿä¸­çš„é‡è¦æ€§å’Œç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SAIDï¼Œè¯¥æ¡†æ¶é¦–æ¬¡å°†æ–‡æœ¬å’Œå…³ç³»ç»“æ„ä¿¡æ¯ç»Ÿä¸€èµ·æ¥è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶QueryAdaptï¼Œé€šè¿‡åœ¨å…³ç³»ä»¤ç‰Œçº§åˆ«ç”Ÿæˆç‰¹å®šæ„å›¾çš„å…³ç³»ä»¤ç‰Œï¼Œå®ç°äº†æ›´ç²¾ç»†çš„çŸ¥è¯†è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAIDåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å›¾æ£€æµ‹æ˜¯ç°ä»£å¯¹è¯ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå¯¹ç”Ÿæˆæœ‰æ•ˆå“åº”è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤§è§„æ¨¡æœªæ ‡æ³¨å¯¹è¯æ–‡æœ¬è¯­æ–™åº“è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡å¯¹æœ‰é™æ³¨é‡Šè¿›è¡Œå¾®è°ƒæ¥è¿›è¡Œæ„å›¾æ£€æµ‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬æ•°æ®ï¼Œå¿½ç•¥äº†å¯¹è¯ç³»ç»Ÿä¸­å›ºæœ‰çš„å…³é”®ç»“æ„ä¿¡æ¯ï¼Œå¦‚æŸ¥è¯¢-æŸ¥è¯¢å…³ç³»å’ŒæŸ¥è¯¢-ç­”æ¡ˆå…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶SAIDï¼Œé¦–æ¬¡ç»Ÿä¸€äº†æ–‡æœ¬å’Œå…³ç³»ç»“æ„ä¿¡æ¯è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒã€‚<br>5.SAIæå‡ºäº†ä¸€ä¸ªæŸ¥è¯¢è‡ªé€‚åº”æ³¨æ„åŠ›ç½‘ç»œï¼ˆQueryAdaptï¼‰çš„æ–°æœºåˆ¶ï¼Œé€šè¿‡åœ¨å…³ç³»ä»¤ç‰Œçº§åˆ«ç”Ÿæˆç‰¹å®šæ„å›¾çš„å…³ç³»ä»¤ç‰Œæ¥å®ç°æ›´ç²¾ç»†çš„çŸ¥è¯†è¿ç§»ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSAIDåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f82ea3b40e4008934bdb42fe723e87a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c30fdb194b50f38ec3802c856ac80e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba826478a3ab83f5d7851dcd06b26563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72317f7a705976055c382078ce2588f1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics"><a href="#Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics" class="headerlink" title="Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot   Approach for Pharmacokinetics"></a>Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot   Approach for Pharmacokinetics</h2><p><strong>Authors:CÃ©sar Ali Ojeda Marin, Wilhelm Huisinga, Purity Kavwele, Niklas Hartung</strong></p>
<p>Accurate dose-response forecasting under sparse sampling is central to precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect Transformer (AICMET) model, a transformer-based latent-variable framework that unifies mechanistic compartmental priors with amortized in-context Bayesian inference. AICMET is pre-trained on hundreds of thousands of synthetic pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters of compartment models, endowing the model with strong inductive biases and enabling zero-shot adaptation to new compounds. At inference time, the decoder conditions on the collective context of previously profiled trial participants, generating calibrated posterior predictions for newly enrolled patients after a few early drug concentration measurements. This capability collapses traditional model-development cycles from weeks to hours while preserving some degree of expert modelling. Experiments across public datasets show that AICMET attains state-of-the-art predictive accuracy and faithfully quantifies inter-patient variability â€“ outperforming both nonlinear mixed-effects baselines and recent neural ODE variants. Our results highlight the feasibility of transformer-based, population-aware neural architectures as offering a new alternative for bespoke pharmacokinetic modeling pipelines, charting a path toward truly population-aware personalized dosing regimens. </p>
<blockquote>
<p>åœ¨ç¨€ç–é‡‡æ ·ä¸‹è¿›è¡Œå‡†ç¡®çš„å‰‚é‡ååº”é¢„æµ‹æ˜¯ç²¾å‡†è¯ç‰©æ²»ç–—çš„æ ¸å¿ƒã€‚æˆ‘ä»¬æå‡ºäº†å¹³å‡ä¸Šä¸‹æ–‡æ··åˆæ•ˆåº”å˜å‹å™¨ï¼ˆAICMETï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ½œåœ¨å˜é‡æ¡†æ¶ï¼Œå®ƒå°†æœºæ¢°å®¤æ¨¡å‹å…ˆéªŒä¸å¹³å‡ä¸Šä¸‹æ–‡è´å¶æ–¯æ¨ç†ç»Ÿä¸€èµ·æ¥ã€‚AICMETåœ¨æ•°ä»¥ä¸‡è®¡åˆæˆè¯ç‰©ä»£è°¢åŠ¨åŠ›å­¦è½¨è¿¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶ä¸­Ornstein-Uhlenbeckä¸ºå®¤æ¨¡å‹çš„å‚æ•°æä¾›äº†å…ˆéªŒçŸ¥è¯†ï¼Œä¸ºæ¨¡å‹æä¾›äº†å¼ºçƒˆçš„å½’çº³åè§ï¼Œå¹¶å®ç°äº†å¯¹æ–°åŒ–åˆç‰©çš„é›¶æ ·æœ¬é€‚åº”ã€‚åœ¨æ¨ç†æ—¶ï¼Œè§£ç å™¨æ ¹æ®å…ˆå‰å—è¯•è€…çš„é›†ä½“ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶åŒ–ï¼Œåœ¨æ–°æ‹›å‹Ÿçš„æ‚£è€…è¿›è¡Œäº†å‡ æ¬¡æ—©æœŸè¯ç‰©æµ“åº¦æµ‹é‡åç”Ÿæˆæ ¡å‡†çš„åéªŒé¢„æµ‹ã€‚è¿™ç§èƒ½åŠ›å°†ä¼ ç»Ÿçš„æ¨¡å‹å¼€å‘å‘¨æœŸä»å‡ å‘¨ç¼©çŸ­åˆ°å‡ å°æ—¶ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€å®šç¨‹åº¦çš„ä¸“å®¶å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAICMETè¾¾åˆ°äº†æœ€å…ˆè¿›çš„é¢„æµ‹ç²¾åº¦ï¼Œå¹¶å¿ å®é‡åŒ–äº†æ‚£è€…é—´çš„å˜å¼‚æ€§ï¼Œè¶…è¶Šäº†éçº¿æ€§æ··åˆæ•ˆåº”åŸºå‡†å’Œæœ€æ–°çš„ç¥ç»ODEå˜ä½“ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†åŸºäºå˜å‹å™¨çš„ã€å…·æœ‰äººç¾¤æ„è¯†çš„ç¥ç»ç½‘ç»œæ¶æ„çš„å¯è¡Œæ€§ï¼Œä¸ºå®šåˆ¶è¯ç‰©ä»£è°¢åŠ¨åŠ›å­¦å»ºæ¨¡ç®¡é“æä¾›äº†æ–°çš„é€‰æ‹©ï¼Œä¸ºçœŸæ­£å…·æœ‰äººç¾¤æ„è¯†ä¸ªæ€§åŒ–ç»™è¯æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15659v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p> AICMETæ¨¡å‹æ˜¯ä¸€ç§åŸºäºè½¬åŒ–å™¨çš„æ½œå˜é‡æ¡†æ¶ï¼Œå®ƒå°†æœºæ¢°å®¤æ¨¡å‹å…ˆéªŒä¸æ‘Šé”€ä¸Šä¸‹æ–‡è´å¶æ–¯æ¨ç†ç›¸ç»“åˆï¼Œå®ç°äº†ç¨€ç–é‡‡æ ·ä¸‹çš„ç²¾å‡†å‰‚é‡ååº”é¢„æµ‹ã€‚è¯¥æ¨¡å‹ç»è¿‡æ•°åä¸‡æ¡åˆæˆè¯ç‰©ä»£è°¢è½¨è¿¹çš„é¢„è®­ç»ƒï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„å½’çº³åç½®èƒ½åŠ›ï¼Œå¯é€‚åº”æ–°åŒ–åˆç‰©ã€‚åœ¨æ¨æ–­æ—¶ï¼Œè§£ç å™¨æ ¹æ®å…ˆå‰å—è¯•è€…çš„é›†ä½“ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œåœ¨å°‘æ•°æ—©æœŸè¯ç‰©æµ“åº¦æµ‹é‡åç”Ÿæˆæ–°æ‚£è€…çš„æ ¡å‡†åé¢„æµ‹ã€‚è¿™å¤§å¤§ç¼©çŸ­äº†ä¼ ç»Ÿæ¨¡å‹å¼€å‘å‘¨æœŸï¼ŒåŒæ—¶ä¿æŒäº†ä¸“å®¶å»ºæ¨¡çš„ç¨‹åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒAICMETè¾¾åˆ°äº†æœ€å…ˆè¿›çš„é¢„æµ‹ç²¾åº¦ï¼Œå¹¶å¿ å®é‡åŒ–äº†æ‚£è€…é—´çš„å˜å¼‚æ€§ï¼Œè¶…è¶Šäº†éçº¿æ€§æ··åˆæ•ˆåº”åŸºå‡†å’Œæœ€æ–°çš„ç¥ç»ODEå˜ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AICMETæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºè½¬åŒ–å™¨çš„æ½œå˜é‡æ¡†æ¶ï¼Œç»“åˆäº†æœºæ¢°å®¤æ¨¡å‹å…ˆéªŒå’Œæ‘Šé”€ä¸Šä¸‹æ–‡è´å¶æ–¯æ¨ç†ã€‚</li>
<li>æ¨¡å‹ç»è¿‡æ•°åä¸‡æ¡åˆæˆè¯ç‰©ä»£è°¢è½¨è¿¹çš„é¢„è®­ç»ƒï¼Œå…·å¤‡å¼ºå¤§çš„å½’çº³åç½®èƒ½åŠ›ï¼Œå¯é€‚åº”æ–°åŒ–åˆç‰©ã€‚</li>
<li>AICMETå¯ä»¥åœ¨å°‘é‡æ—©æœŸè¯ç‰©æµ“åº¦æµ‹é‡åç”Ÿæˆæ–°æ‚£è€…çš„æ ¡å‡†é¢„æµ‹ã€‚</li>
<li>æ¨¡å‹å®ç°äº†ç¨€ç–é‡‡æ ·ä¸‹çš„ç²¾å‡†å‰‚é‡ååº”é¢„æµ‹ï¼Œç¼©çŸ­äº†ä¼ ç»Ÿæ¨¡å‹å¼€å‘å‘¨æœŸã€‚</li>
<li>AICMETè¾¾åˆ°äº†æœ€å…ˆè¿›çš„é¢„æµ‹ç²¾åº¦ï¼Œå¹¶å¿ å®é‡åŒ–äº†æ‚£è€…é—´çš„å˜å¼‚æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹è¶…è¶Šäº†ç°æœ‰çš„éçº¿æ€§æ··åˆæ•ˆåº”åŸºå‡†å’Œç¥ç»ODEå˜ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa5eb8dcdb98c1facb245756d68f6e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427289b8515724057751f1b12cc861d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1932375e8571222d9abfca1f35f18b34.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-Generalization-and-Personalization-in-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning"><a href="#Bridging-Generalization-and-Personalization-in-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning" class="headerlink" title="Bridging Generalization and Personalization in Human Activity   Recognition via On-Device Few-Shot Learning"></a>Bridging Generalization and Personalization in Human Activity   Recognition via On-Device Few-Shot Learning</h2><p><strong>Authors:Pixi Kang, Julian Moosmann, Mengxi Liu, Bo Zhou, Michele Magno, Paul Lukowicz, Sizhen Bian</strong></p>
<p>Human Activity Recognition (HAR) with different sensing modalities requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73%, 17.38%, and 3.70%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization. The related framework is open sourced for further research\footnote{<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023%7D">https://github.com/kangpx/onlineTiny2023}</a>. </p>
<blockquote>
<p>äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰é‡‡ç”¨ä¸åŒæ„ŸçŸ¥æ¨¡å¼ï¼Œéœ€è¦å¼ºå¤§çš„è·¨ç”¨æˆ·æ³›åŒ–èƒ½åŠ›å’Œé«˜æ•ˆçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„HARæ¨¡å‹åœ¨é¢å¯¹ç”¨æˆ·ç‰¹å®šå˜åŒ–æ—¶å¾€å¾€æ— æ³•æ³›åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è®¾å¤‡ç«¯å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨HARä¸­æ¶èµ·æ³›åŒ–ä¸ä¸ªæ€§åŒ–ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªè·¨ç”¨æˆ·çš„é€šç”¨è¡¨ç¤ºï¼Œç„¶åä»…é€šè¿‡å°‘é‡æ ‡è®°æ ·æœ¬è¿…é€Ÿé€‚åº”æ–°ç”¨æˆ·ï¼Œç›´æ¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ›´æ–°è½»é‡çº§åˆ†ç±»å™¨å±‚ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å…·æœ‰æœ€å°è®¡ç®—å’Œå†…å­˜æˆæœ¬ç¨³å¥çš„è®¾å¤‡ç«¯å­¦ä¹ ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œéƒ¨ç½²çš„å®é™…é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨èƒ½æ•ˆé«˜çš„RISC-V GAP9å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆRecGymã€QVAR-Gestureã€Ultrasound-Gestureï¼‰ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œéƒ¨ç½²åçš„é€‚åº”åˆ†åˆ«æé«˜äº†3.73%ã€17.38%å’Œ3.70%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°æ ·æœ¬è®¾å¤‡ç«¯å­¦ä¹ èƒ½å¤Ÿé€šè¿‡æ— ç¼ç»“åˆæ³›åŒ–å’Œä¸ªæ€§åŒ–ï¼Œå®ç°å¯æ‰©å±•çš„ã€ç”¨æˆ·æ„ŸçŸ¥çš„å’ŒèŠ‚èƒ½çš„å¯ç©¿æˆ´äººç±»æ´»åŠ¨è¯†åˆ«ã€‚ç›¸å…³æ¡†æ¶å·²å¼€æºä¾›è¿›ä¸€æ­¥ç ”ç©¶\footnote{<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023%7D%E3%80%82">https://github.com/kangpx/onlineTiny2023}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15413v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰çš„ä¸åŒæ„ŸçŸ¥æ¨¡å¼ï¼Œæå‡ºä¸€ç§æ–°å‹è®¾å¤‡ç«¯å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œèåˆè·¨ç”¨æˆ·çš„ä¸€èˆ¬åŒ–å’Œä¸ªä½“ä¸ªæ€§åŒ–éœ€æ±‚ã€‚æ¡†æ¶å…ˆè®­ç»ƒè·¨ç”¨æˆ·çš„é€šç”¨è¡¨ç¤ºï¼Œå†åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æ–°ç”¨æˆ·è¿›è¡Œå¿«é€Ÿé€‚åº”ï¼Œç›´æ¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ›´æ–°è½»é‡çº§åˆ†ç±»å™¨å±‚ã€‚è¯¥æ–¹æ³•å®ç°ç¨³å¥çš„è®¾å¤‡ç«¯å­¦ä¹ ï¼Œè®¡ç®—ä¸å†…å­˜æˆæœ¬ä½ï¼Œé€‚ç”¨äºå®é™…éƒ¨ç½²ã€‚åœ¨RISC-V GAP9å¾®æ§åˆ¶å™¨ä¸Šå®ç°æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºéƒ¨ç½²åé€‚åº”æ€§æé«˜å‡†ç¡®ç‡ã€‚è¯æ˜å°‘æ ·æœ¬å­¦ä¹ å¯å®ç°å¯æ‰©å±•ã€ç”¨æˆ·å‹å¥½å’ŒèŠ‚èƒ½çš„å¯ç©¿æˆ´äººç±»æ´»åŠ¨è¯†åˆ«ï¼Œèåˆä¸€èˆ¬åŒ–å’Œä¸ªæ€§åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºäººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰ã€‚</li>
<li>æ¡†æ¶èåˆäº†è·¨ç”¨æˆ·çš„ä¸€èˆ¬åŒ–å’Œä¸ªä½“ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚</li>
<li>é€šè¿‡è®­ç»ƒé€šç”¨è¡¨ç¤ºå’Œå¿«é€Ÿé€‚åº”æ–°ç”¨æˆ·çš„å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼Œå®ç°ç¨³å¥å­¦ä¹ ã€‚</li>
<li>ç›´æ¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ›´æ–°è½»é‡çº§åˆ†ç±»å™¨å±‚ã€‚</li>
<li>æ¡†æ¶é€‚ç”¨äºå®é™…éƒ¨ç½²ï¼Œè®¡ç®—ä¸å†…å­˜æˆæœ¬ä½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œéƒ¨ç½²åçš„é€‚åº”æ€§æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c3f391fd2161314d52a7cea7847d4e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-177563e2614d7637f02aafcadad75ddf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c85be1d1894f00f3fce99ca979eedd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47eaf1277fb857ff07ab2ab1af34b59c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="COLLAGE-Adaptive-Fusion-based-Retrieval-for-Augmented-Policy-Learning"><a href="#COLLAGE-Adaptive-Fusion-based-Retrieval-for-Augmented-Policy-Learning" class="headerlink" title="COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning"></a>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</h2><p><strong>Authors:Sateesh Kumar, Shivin Dass, Georgios Pavlakos, Roberto MartÃ­n-MartÃ­n</strong></p>
<p>In this work, we study the problem of data retrieval for few-shot imitation learning: selecting data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and can introduce detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task-specific combination of multiple cues. COLLAGE follows a simple, flexible, and efficient recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on how well a policy trained on each subset predicts actions in the target demonstrations. These weights are then used to perform importance sampling during policy training, sampling data more densely or sparsely according to estimated relevance. COLLAGE is general and feature-agnostic, allowing it to combine any number of subsets selected by any retrieval heuristic, and to identify which subsets provide the greatest benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches by 5.1% in simulation across 10 tasks, and by 16.6% in the real world across 6 tasks, where we perform retrieval from the large-scale DROID dataset. More information at <a target="_blank" rel="noopener" href="https://robin-lab.cs.utexas.edu/COLLAGE">https://robin-lab.cs.utexas.edu/COLLAGE</a> . </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°æ ·æ¨¡ä»¿å­¦ä¹ çš„æ•°æ®æ£€ç´¢é—®é¢˜ï¼šä»å¤§å‹æ•°æ®é›†ä¸­é€‰æ‹©æ•°æ®æ¥è®­ç»ƒé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ç­–ç•¥ï¼Œä»…ä½¿ç”¨å°‘æ•°ç›®æ ‡æ¼”ç¤ºã€‚å…ˆå‰çš„æ–¹æ³•ä½¿ç”¨å•ä¸€ç‰¹å¾è·ç¦»å¯å‘å¼æ–¹æ³•è¿›è¡Œæ•°æ®æ£€ç´¢ï¼Œå‡è®¾æœ€ä½³çš„æ¼”ç¤ºæ˜¯é‚£äº›åœ¨è§†è§‰ã€è¯­ä¹‰æˆ–è¿åŠ¨ç©ºé—´ä¸­æœ€æ¥è¿‘ç›®æ ‡ç¤ºä¾‹çš„æ¼”ç¤ºã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åªæ•è·äº†éƒ¨åˆ†ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å¯èƒ½å¼•å…¥æœ‰å®³çš„æ¼”ç¤ºï¼Œä¾‹å¦‚ç”±äºåœºæ™¯å¸ƒå±€ç›¸ä¼¼è€Œä»æ— å…³çš„ä»»åŠ¡ä¸­æ£€ç´¢æ•°æ®ï¼Œæˆ–ä»å…·æœ‰ä¸åŒç›®æ ‡çš„ä»»åŠ¡ä¸­é€‰æ‹©ç›¸ä¼¼çš„è¿åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†COLLAGEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå°æ ·æ¨¡ä»¿å­¦ä¹ ä¸­çš„é›†ä½“æ•°æ®èšåˆæ–¹æ³•ã€‚å®ƒä½¿ç”¨è‡ªé€‚åº”çš„åæœŸèåˆæœºåˆ¶ï¼Œæ ¹æ®é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¤šä¸ªçº¿ç´¢ç»„åˆæ¥æŒ‡å¯¼ç›¸å…³æ¼”ç¤ºçš„é€‰æ‹©ã€‚COLLAGEéµå¾ªç®€å•ã€çµæ´»å’Œé«˜æ•ˆçš„æ–¹æ³•ï¼šå®ƒä¸ºä½¿ç”¨å•ä¸€ç‰¹å¾ï¼ˆä¾‹å¦‚å¤–è§‚ã€å½¢çŠ¶æˆ–è¯­è¨€ç›¸ä¼¼æ€§ï¼‰é¢„å…ˆé€‰æ‹©çš„æ•°æ®å­é›†åˆ†é…æƒé‡ï¼ŒåŸºäºåœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒçš„ç­–ç•¥å¯¹ç›®æ ‡æ¼”ç¤ºçš„é¢„æµ‹æ•ˆæœã€‚è¿™äº›æƒé‡ç„¶åè¢«ç”¨äºåœ¨ç­–ç•¥è®­ç»ƒæœŸé—´æ‰§è¡Œé‡è¦æ€§é‡‡æ ·ï¼Œæ ¹æ®ä¼°è®¡çš„ç›¸å…³æ€§æ›´å¯†é›†æˆ–æ›´ç¨€ç–åœ°é‡‡æ ·æ•°æ®ã€‚COLLAGEæ˜¯é€šç”¨çš„ï¼Œä¸ç‰¹å¾æ— å…³ï¼Œèƒ½å¤Ÿç»“åˆä»»ä½•æ•°é‡çš„ç”±ä»»ä½•æ£€ç´¢å¯å‘å¼é€‰æ‹©çš„æ•°æ®å­é›†ï¼Œå¹¶ç¡®å®šå“ªäº›å­é›†å¯¹ç›®æ ‡ä»»åŠ¡æä¾›æœ€å¤§çš„å¥½å¤„ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œä¸æœ€æ–°æ£€ç´¢å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCOLLAGEåœ¨æ¨¡æ‹Ÿçš„10ä¸ªä»»åŠ¡ä¸Šæé«˜äº†5.1%çš„æ€§èƒ½ï¼Œåœ¨çœŸå®ä¸–ç•Œçš„6ä¸ªä»»åŠ¡ä¸Šæé«˜äº†16.6%çš„æ€§èƒ½ï¼Œå…¶ä¸­æˆ‘ä»¬ä»å¤§è§„æ¨¡DROIDæ•°æ®é›†ä¸­è¿›è¡Œæ£€ç´¢ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®<a target="_blank" rel="noopener" href="https://robin-lab.cs.utexas.edu/COLLAGE%E3%80%82">https://robin-lab.cs.utexas.edu/COLLAGEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01131v2">PDF</a> Accepted at the Conference on Robot Learning (CoRL), 2025. Project   page: <a target="_blank" rel="noopener" href="https://robin-lab.cs.utexas.edu/COLLAGE">https://robin-lab.cs.utexas.edu/COLLAGE</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°æ ·å­¦ä¹ ä¸­çš„æ•°æ®é‡‡é›†é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä»…ä¾èµ–å•ä¸€ç‰¹å¾è·ç¦»å¯å‘å¼è¿›è¡Œæ•°æ®æ£€ç´¢çš„å±€é™æ€§ï¼Œæå‡ºäº†COLLAGEæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è‡ªé€‚åº”åæœŸèåˆæœºåˆ¶ï¼Œæ ¹æ®ç‰¹å®šä»»åŠ¡ç»„åˆå¤šç§çº¿ç´¢æ¥æŒ‡å¯¼ç›¸å…³æ¼”ç¤ºçš„é€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼ŒCOLLAGEåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸­çš„å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ£€ç´¢å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯ï¼šå°æ ·å­¦ä¹ ä¸­çš„æ•°æ®é‡‡é›†é—®é¢˜ï¼Œå³å¦‚ä½•ä»å¤§é‡æ•°æ®é›†ä¸­é€‰æ‹©ç”¨äºè®­ç»ƒé«˜æ€§èƒ½ç­–ç•¥çš„æ•°æ®ã€‚</li>
<li>é—®é¢˜ç°çŠ¶ï¼šç°æœ‰æ–¹æ³•åŸºäºå•ä¸€ç‰¹å¾è·ç¦»å¯å‘å¼è¿›è¡Œæ•°æ®æ£€ç´¢ï¼Œå­˜åœ¨ä¿¡æ¯æ•è·ä¸å…¨å’Œå¼•å…¥ä¸ç›¸å…³æ¼”ç¤ºçš„é£é™©ã€‚</li>
<li>æ–¹æ³•ä»‹ç»ï¼šæå‡ºCOLLAGEæ–¹æ³•ï¼Œé‡‡ç”¨è‡ªé€‚åº”åæœŸèåˆæœºåˆ¶ï¼Œæ ¹æ®ä»»åŠ¡ç‰¹å®šç»„åˆå¤šç§çº¿ç´¢æ¥æŒ‡å¯¼æ¼”ç¤ºé€‰æ‹©ã€‚</li>
<li>COLLAGEç‰¹ç‚¹ï¼šç®€å•ã€çµæ´»ã€é«˜æ•ˆï¼Œå¯ç»“åˆä»»ä½•æ•°é‡çš„å­é›†å’Œä»»ä½•æ£€ç´¢å¯å‘å¼æ–¹æ³•ï¼Œå¹¶ç¡®å®šå“ªäº›å­é›†å¯¹ç›®æ ‡ä»»åŠ¡æœ€æœ‰ç›Šã€‚</li>
<li>å®éªŒç»“æœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å¤šä¸ªä»»åŠ¡ä¸Šï¼ŒCOLLAGEä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ•°æ®æ¥æºï¼šä»å¤§å‹æ•°æ®é›†DROIDä¸­è¿›è¡Œæ•°æ®æ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8ddf6f8b46bcbe7736be9c235a77dca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-159e42a3a5a5a08595a6e20e9b6af480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8253e157ab2ade01fb3705f28ed06ecc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-893c81e4705e5f4f9666f02a8d4c7c72.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CodeMixBench-Evaluating-Code-Mixing-Capabilities-of-LLMs-Across-18-Languages"><a href="#CodeMixBench-Evaluating-Code-Mixing-Capabilities-of-LLMs-Across-18-Languages" class="headerlink" title="CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18   Languages"></a>CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18   Languages</h2><p><strong>Authors:Yilun Yang, Yekun Chai</strong></p>
<p>Code-mixing, the practice of switching between languages within a conversation, poses unique challenges for traditional NLP. Existing benchmarks are limited by their narrow language pairs and tasks, failing to adequately assess large language modelsâ€™ (LLMs) code-mixing abilities. Despite the recognized importance of code-mixing for multilingual users, research on LLMs in this context remains sparse. Additionally, current techniques for synthesizing code-mixed data are underdeveloped to generate code-mixing. In response, we introduce CodeMixBench, a comprehensive benchmark covering eight tasks, including three specific to LLMs and five traditional NLP tasks, and 18 languages across seven language families. We also propose a new method for generating large-scale synthetic code-mixed texts by combining word substitution with GPT-4 prompting. Our evaluation reveals consistent underperformance of LLMs on code-mixed datasets involving different language families. Enhancements in training data size, model scale, and few-shot learning could improve their performance. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/Jeromeyluck/CodeMixBench">https://github.com/Jeromeyluck/CodeMixBench</a>. </p>
<blockquote>
<p>ä»£ç æ··åˆï¼ˆcode-mixingï¼‰æŒ‡çš„æ˜¯åœ¨å¯¹è¯ä¸­åˆ‡æ¢ä¸åŒè¯­è¨€çš„å®è·µï¼Œè¿™ä¸ºä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å—é™äºå…¶ç‹­çª„çš„è¯­è¨€å¯¹å’Œä»»åŠ¡èŒƒå›´ï¼Œæ— æ³•å……åˆ†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç æ··åˆèƒ½åŠ›ã€‚å°½ç®¡ä»£ç æ··åˆå¯¹äºå¤šè¯­è¨€ç”¨æˆ·çš„é‡è¦æ€§å·²ç»å¾—åˆ°è®¤è¯†ï¼Œä½†åœ¨æ­¤èƒŒæ™¯ä¸‹çš„LLMç ”ç©¶ä»ç„¶ç¨€å°‘ã€‚æ­¤å¤–ï¼Œå½“å‰ç”Ÿæˆä»£ç æ··åˆæ•°æ®çš„åˆæˆæŠ€æœ¯å°šä¸æˆç†Ÿã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CodeMixBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å…«ä¸ªä»»åŠ¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸“é—¨é’ˆå¯¹LLMsçš„ä»»åŠ¡å’Œäº”ä¸ªä¼ ç»ŸNLPä»»åŠ¡ï¼Œæ¶‰åŠä¸ƒä¸ªè¯­è¨€å®¶æ—çš„18ç§è¯­è¨€ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¯æ›¿æ¢å’ŒGPT-4æç¤ºæ¥ç”Ÿæˆå¤§è§„æ¨¡çš„ä»£ç æ··åˆæ–‡æœ¬ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°ï¼Œåœ¨ä¸åŒè¯­è¨€å®¶æ—çš„ä»£ç æ··åˆæ•°æ®é›†ä¸Šï¼ŒLLMsçš„è¡¨ç°æŒç»­ä¸ä½³ã€‚é€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®å¤§å°ã€æ¨¡å‹è§„æ¨¡å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå¯ä»¥æ”¹å–„å…¶æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jeromeyluck/CodeMixBench%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Jeromeyluck/CodeMixBenchè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18791v2">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¼ ç»ŸNLPä¸­ä»£ç æ··åˆï¼ˆå³åœ¨å¯¹è¯ä¸­åˆ‡æ¢è¯­è¨€ï¼‰å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å› è¯­è¨€å¯¹å’Œä»»åŠ¡èŒƒå›´æœ‰é™ï¼Œæ— æ³•å……åˆ†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç æ··åˆèƒ½åŠ›ã€‚è™½ç„¶ä»£ç æ··åˆå¯¹äºå¤šè¯­è¨€ç”¨æˆ·çš„é‡è¦æ€§å¾—åˆ°äº†è®¤å¯ï¼Œä½†å…³äºLLMsåœ¨æ­¤èƒŒæ™¯ä¸‹çš„ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚æ­¤å¤–ï¼Œå½“å‰ç”Ÿæˆä»£ç æ··åˆæ•°æ®çš„åˆæˆæŠ€æœ¯å°šä¸æˆç†Ÿã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†CodeMixBenchåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å…«é¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¸‰é¡¹é’ˆå¯¹LLMsçš„ä»»åŠ¡å’Œäº”é¡¹ä¼ ç»ŸNLPä»»åŠ¡ï¼Œæ¶‰åŠ18ç§è¯­è¨€å’Œä¸ƒä¸ªè¯­è¨€å®¶æ—ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¯æ›¿æ¢å’ŒGPT-4æç¤ºæ¥ç”Ÿæˆå¤§è§„æ¨¡çš„ä»£ç æ··åˆæ–‡æœ¬ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠä¸åŒè¯­è¨€å®¶æ—çš„æ··åˆä»£ç é›†æ—¶è¡¨ç°ä¸ä½³ã€‚å¢åŠ è®­ç»ƒæ•°æ®é‡ã€æ‰©å¤§æ¨¡å‹è§„æ¨¡ä»¥åŠé‡‡ç”¨å°æ ·æœ¬å­¦ä¹ å¯æœ›æ”¹å–„å…¶è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç æ··åˆï¼ˆè¯­è¨€åˆ‡æ¢ï¼‰åœ¨ä¼ ç»ŸNLPä¸­æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç æ··åˆèƒ½åŠ›è¯„ä¼°ä¸è¶³ã€‚</li>
<li>CodeMixBenchåŸºå‡†æµ‹è¯•æ¶µç›–å…«é¡¹ä»»åŠ¡å’Œå¤šä¸ªè¯­è¨€å®¶æ—ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆè¯æ›¿æ¢å’ŒGPT-4æç¤ºç”Ÿæˆå¤§è§„æ¨¡ä»£ç æ··åˆæ–‡æœ¬çš„æ–°æ–¹æ³•ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠä¸åŒè¯­è¨€å®¶æ—çš„æ··åˆä»£ç é›†æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¢åŠ è®­ç»ƒæ•°æ®é‡ã€æ‰©å¤§æ¨¡å‹è§„æ¨¡ä»¥åŠé‡‡ç”¨å°æ ·æœ¬å­¦ä¹ å¯èƒ½æœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa0d7c7bb529eda234a8ae54e35b3604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7732dd7c4f99f020ac3027a0f30402d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e37de542597dbe52006613ca2fd9f94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e75df294a63ab1a84dd80922ec61228.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SenseCF-LLM-Prompted-Counterfactuals-for-Intervention-and-Sensor-Data-Augmentation"><a href="#SenseCF-LLM-Prompted-Counterfactuals-for-Intervention-and-Sensor-Data-Augmentation" class="headerlink" title="SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data   Augmentation"></a>SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data   Augmentation</h2><p><strong>Authors:Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh</strong></p>
<p>Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: github.com&#x2F;shovito66&#x2F;SenseCF. </p>
<blockquote>
<p>å› æœè§£é‡Šï¼ˆCFsï¼‰é€šè¿‡å¼ºè°ƒæ”¹å˜ç»“æœæ‰€éœ€çš„æœ€å°å˜åŒ–ï¼Œä¸ºæœºå™¨å­¦ä¹ çš„é¢„æµ‹æä¾›äº†ä»¥äººç±»ä¸ºä¸­å¿ƒçš„æ´å¯Ÿã€‚å› æ­¤ï¼Œå› æœè§£é‡Šå¯ä»¥ç”¨ä½œï¼ˆiï¼‰å¼‚å¸¸é¢„é˜²çš„å¹²é¢„æªæ–½å’Œï¼ˆiiï¼‰å¢å¼ºæ•°æ®ä»¥è®­ç»ƒç¨³å¥æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«æ˜¯GPT-4o-miniï¼Œåœ¨é›¶æ ·æœ¬å’Œä¸‰æ ·æœ¬ç¯å¢ƒä¸­ç”Ÿæˆå› æœè§£é‡Šã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šç”¨äºå‹åŠ›é¢„æµ‹çš„AI-Readiæ——èˆ°æ•°æ®é›†å’Œç”¨äºå¿ƒè„ç—…æ£€æµ‹çš„å…¬å¼€æ•°æ®é›†ã€‚ä¸ä¼ ç»Ÿçš„DiCEã€CFNOWå’ŒNICEç­‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬åŸºäºå°‘é‡æ ·æœ¬çš„LLMæ–¹æ³•å®ç°äº†é«˜è¾¾99%çš„å¯ä¿¡åº¦å’Œé«˜è¾¾0.99çš„å¼ºæœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”ç¨€ç–æ€§å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LLMç”Ÿæˆçš„å› æœè§£é‡Šä½œä¸ºå¢å¼ºæ ·æœ¬æé«˜äº†ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ€§èƒ½ï¼ˆå¹³å‡å‡†ç¡®ç‡æé«˜5%ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚è¿™è¯æ˜äº†åŸºäºæç¤ºçš„ç”ŸæˆæŠ€æœ¯å¢å¼ºä¸´åºŠå’Œç”Ÿç†é¢„æµ‹ä»»åŠ¡çš„å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§çš„æ½œåŠ›ã€‚ä»£ç åº“ï¼šgithub.com&#x2F;shovito66&#x2F;SenseCFã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05541v2">PDF</a> Accepted at the IEEE-EMBS International Conference on Body Sensor   Networks (IEEE-EMBS BSN) 2025, LA, CA, USA</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆè®¡æ•°å™¨äº‹å®è§£é‡Šï¼ˆCFsï¼‰çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœºå™¨å­¦ä¹ çš„é¢„æµ‹è§£é‡Šä¸­ã€‚ç ”ç©¶é›†ä¸­åœ¨é€šè¿‡GPT-4o-miniç”ŸæˆCFsï¼Œå¹¶åœ¨é›¶é•œå¤´å’Œä¸‰é•œå¤´è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ã€‚è¯¥ç ”ç©¶åœ¨AI-Readiæ——èˆ°æ•°æ®é›†å’Œå…¬å…±å¿ƒè„ç—…æ£€æµ‹æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å®ç°äº†é«˜å¯ä¿¡åº¦ã€æœ‰æ•ˆæ€§å’Œç«äº‰æ€§ç¨€ç–æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LLMç”Ÿæˆçš„CFsä½œä¸ºå¢å¼ºæ ·æœ¬å¯ä»¥æé«˜ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹ã€‚è¿™å±•ç¤ºäº†åŸºäºæç¤ºçš„ç”ŸæˆæŠ€æœ¯åœ¨ä¸´åºŠå’Œç”Ÿç†é¢„æµ‹ä»»åŠ¡ä¸­æé«˜è§£é‡Šæ€§å’Œç¨³å¥æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡æ•°å™¨äº‹å®è§£é‡Šï¼ˆCFsï¼‰å¯ä»¥çªå‡ºæ˜¾ç¤ºæ”¹å˜ç»“æœæ‰€éœ€çš„æœ€å°å˜åŒ–ï¼Œä¸ºæœºå™¨å­¦ä¹ çš„é¢„æµ‹æä¾›ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§è§£ã€‚</li>
<li>LLMsï¼Œç‰¹åˆ«æ˜¯GPT-4o-miniï¼Œè¢«ç”¨äºç”ŸæˆCFsï¼Œå¹¶åœ¨é›¶é•œå¤´å’Œä¸‰é•œå¤´è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åœ¨AI-Readiæ——èˆ°æ•°æ®é›†å’Œå…¬å…±å¿ƒè„ç—…æ£€æµ‹æ•°æ®é›†ä¸Šï¼ŒLLMæ–¹æ³•å®ç°äº†é«˜å¯ä¿¡åº¦ã€æœ‰æ•ˆæ€§å’Œç«äº‰æ€§ç¨€ç–æ€§ã€‚</li>
<li>ä½¿ç”¨LLMç”Ÿæˆçš„CFsä½œä¸ºå¢å¼ºæ ·æœ¬ï¼Œå¯ä»¥æé«˜ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹ã€‚</li>
<li>LLMæ–¹æ³•æ˜¾ç¤ºå‡ºåœ¨è§£é‡Šæ€§å’Œç¨³å¥æ€§æ–¹é¢æå‡ä¸´åºŠå’Œç”Ÿç†é¢„æµ‹ä»»åŠ¡çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç åº“ä½äºgithub.com&#x2F;shovito66&#x2F;SenseCFã€‚</li>
<li>CFså¯ç”¨äºå¼‚å¸¸é¢„é˜²ï¼ˆä½œä¸ºå¹²é¢„æªæ–½ï¼‰å’Œè®­ç»ƒç¨³å¥æ¨¡å‹ï¼ˆä½œä¸ºå¢å¼ºæ•°æ®ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0db7f58f8d58c1bb14b9529be7ca96f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c322beb11a6bfcbaf1b893b8a579df1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc9beefb7375992db1f8e4dde7b2926f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0801efdcf57229b1ff12c66fa00cbdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-396d3f656eb34d1832c2e27351febd9d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Synthetic-data-enables-context-aware-bioacoustic-sound-event-detection"><a href="#Synthetic-data-enables-context-aware-bioacoustic-sound-event-detection" class="headerlink" title="Synthetic data enables context-aware bioacoustic sound event detection"></a>Synthetic data enables context-aware bioacoustic sound event detection</h2><p><strong>Authors:Benjamin Hoffman, David Robinson, Marius Miron, Vittorio Baglione, Daniela Canestrari, Damian Elias, Eva Trapote, Felix Effenberger, Maddie Cusimano, Masato Hagiwara, Olivier Pietquin</strong></p>
<p>We propose a methodology for training foundation models that enhances their in-context learning capabilities within the domain of bioacoustic signal processing. We use synthetically generated training data, introducing a domain-randomization-based pipeline that constructs diverse acoustic scenes with temporally strong labels. We generate over 8.8 thousand hours of strongly-labeled audio and train a query-by-example, transformer-based model to perform few-shot bioacoustic sound event detection. Our second contribution is a public benchmark of 13 diverse few-shot bioacoustics tasks. Our model outperforms previously published methods, and improves relative to other training-free methods by $64%$. We demonstrate that this is due to increase in model size and data scale, as well as algorithmic improvements. We make our trained model available via an API, to provide ecologists and ethologists with a training-free tool for bioacoustic sound event detection. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å…¶åœ¨ç”Ÿç‰©å£°ä¿¡å·å¤„ç†é¢†åŸŸçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆç”Ÿæˆçš„è®­ç»ƒæ•°æ®ï¼Œå¼•å…¥åŸºäºåŸŸéšæœºåŒ–çš„ç®¡é“ï¼Œæ„å»ºå…·æœ‰æ—¶é—´ä¸Šå¼ºæ ‡ç­¾çš„å¤šæ ·åŒ–å£°éŸ³åœºæ™¯ã€‚æˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡8800å°æ—¶çš„å¼ºæ ‡ç­¾éŸ³é¢‘ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæŸ¥è¯¢ç¤ºä¾‹çš„å˜å‹å™¨æ¨¡å‹ï¼Œä»¥æ‰§è¡Œå°‘æ•°ç”Ÿç‰©å£°éŸ³äº‹ä»¶æ£€æµ‹ã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯åŒ…å«13ä¸ªå¤šæ ·å°‘æ•°ç”Ÿç‰©å£°å­¦ä»»åŠ¡çš„å…¬å¼€åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºå…ˆå‰å‘å¸ƒçš„æ–¹æ³•ï¼Œä¸å…¶ä»–æ— è®­ç»ƒæ–¹æ³•çš„ç›¸å¯¹æ”¹è¿›å¹…åº¦è¾¾åˆ°64%ã€‚æˆ‘ä»¬è¯æ˜è¿™æ˜¯ç”±äºæ¨¡å‹å¤§å°å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ ä»¥åŠç®—æ³•çš„æ”¹è¿›æ‰€è‡´ã€‚æˆ‘ä»¬é€šè¿‡APIæä¾›ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œä¸ºç”Ÿæ€å­¦å®¶å’ŒåŠ¨ç‰©è¡Œä¸ºå­¦å®¶æä¾›æ— éœ€è®­ç»ƒçš„ç”Ÿç‰©å£°éŸ³äº‹ä»¶æ£€æµ‹å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00296v2">PDF</a> </p>
<p><strong>Summary</strong>:<br>æå‡ºä¸€ç§è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼Œå¢å¼ºå…¶åœ¨ç”Ÿç‰©å£°ä¿¡å·å¤„ç†é¢†åŸŸçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚é€šè¿‡åˆæˆç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå¼•å…¥åŸºäºåŸŸéšæœºåŒ–çš„ç®¡é“æ„å»ºå¤šæ ·çš„å£°éŸ³åœºæ™¯ï¼Œå¹¶å¸¦æœ‰æ—¶é—´å¼ºæ ‡ç­¾ã€‚è®­ç»ƒåŸºäºæŸ¥è¯¢ç¤ºä¾‹çš„è½¬æ¢æ¨¡å‹è¿›è¡Œå°‘é‡ç”Ÿç‰©å£°éŸ³äº‹ä»¶æ£€æµ‹ã€‚åŒæ—¶æä¾›å…¬å¼€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹APIï¼Œç”¨äºç”Ÿæ€å­¦å’ŒåŠ¨ç‰©è¡Œä¸ºå­¦ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æå‡ºä¸€ç§å¢å¼ºåŸºç¡€æ¨¡å‹åœ¨ç”Ÿç‰©å£°ä¿¡å·å¤„ç†é¢†åŸŸä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨åŸºäºåŸŸéšæœºåŒ–çš„ç®¡é“æ„å»ºå¤šæ ·å£°éŸ³åœºæ™¯ï¼Œå¹¶å¸¦æœ‰æ—¶é—´å¼ºæ ‡ç­¾ã€‚</li>
<li>è®­ç»ƒäº†åŸºäºæŸ¥è¯¢ç¤ºä¾‹çš„è½¬æ¢æ¨¡å‹è¿›è¡Œå°‘é‡ç”Ÿç‰©å£°éŸ³äº‹ä»¶æ£€æµ‹ã€‚</li>
<li>å…¬å¼€äº†åŒ…å«13ç§ä¸åŒç”Ÿç‰©å£°å­¦ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¨¡å‹è¡¨ç°ä¼˜äºå…ˆå‰å‘å¸ƒçš„æ–¹æ³•ï¼Œç›¸è¾ƒäºå…¶ä»–æ— è®­ç»ƒæ–¹æ³•æé«˜äº†64%ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ ä»¥åŠç®—æ³•çš„æ”¹è¿›æ˜¯æ€§èƒ½æå‡çš„ä¸»è¦åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25e69acaf800401803a3b903c4dbe3c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db35b05e606f52c934e81438ce40c696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82dc227aec1a131a28fd733ff2bb04c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d825219da907cddd78327bdafde75e47.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Knowledge-Editing-through-Chain-of-Thought"><a href="#Knowledge-Editing-through-Chain-of-Thought" class="headerlink" title="Knowledge Editing through Chain-of-Thought"></a>Knowledge Editing through Chain-of-Thought</h2><p><strong>Authors:Changyue Wang, Weihang Su, Qingyao Ai, Yichen Tang, Yiqun Liu</strong></p>
<p>Knowledge Editing is a technique that updates large language models (LLMs) with new information to maintain their world knowledge. This approach avoids the need to rebuild the model from scratch, thereby addressing the high costs associated with frequent retraining. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the modelâ€™s original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks. In response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating. The code and data of EditCoT are available at: <a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT">https://github.com/bebr2/EditCoT</a> . </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘æ˜¯ä¸€ç§æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥ç»´æŒå…¶ä¸–ç•ŒçŸ¥è¯†çš„æ–°ä¿¡æ¯çš„æŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä»é›¶å¼€å§‹é‡å»ºæ¨¡å‹çš„éœ€è¦ï¼Œä»è€Œè§£å†³äº†ä¸é¢‘ç¹é‡æ–°è®­ç»ƒç›¸å…³çš„é«˜æˆæœ¬é—®é¢˜ã€‚åœ¨è¿™äº›ä¹‹ä¸­ï¼Œä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼å› å…¶èƒ½å¤Ÿåœ¨é›†æˆæ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹åŠŸèƒ½è€Œè„±é¢–è€Œå‡ºã€‚å°½ç®¡å…¶å…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¾€å¾€æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¸»è¦å…³æ³¨ä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†ä¸‰å…ƒç»„çš„å¤šè·³é—®ç­”ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹å°‘æ•°æç¤ºçš„ä»»åŠ¡åˆ†è§£çš„ä¾èµ–ï¼Œä½¿å®ƒä»¬åœ¨è·¨ä¸åŒä»»åŠ¡æ³›åŒ–æ—¶å˜å¾—ä¸ç¨³å®šä¸”æ•ˆæœè¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EditCoTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»æœ‰æ•ˆåœ°æ›´æ–°å„ç§ä»»åŠ¡çš„LLMï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚EditCoTé€šè¿‡ä¸ºç»™å®šè¾“å…¥ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œç„¶åä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„CoTç¼–è¾‘å™¨è¿­ä»£ä¼˜åŒ–è¿™ä¸€æ€ç»´é“¾è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤šç§è¯­è¨€å’Œä»»åŠ¡çš„å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸Šå¯¹EditCoTè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ³›åŒ–èƒ½åŠ›ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæ ‡å¿—ç€çŸ¥è¯†æ›´æ–°é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§è¿›å±•ã€‚EditCoTçš„ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bebr2/EditCoTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17727v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯æ˜¯ä¸€ç§æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°ä¿¡æ¯ä»¥ç»´æŒå…¶ä¸–ç•ŒçŸ¥è¯†çš„æ–¹æ³•ã€‚è¯¥æŠ€æœ¯æ— éœ€ä»é›¶å¼€å§‹é‡å»ºæ¨¡å‹ï¼Œä»è€Œé™ä½äº†é¢‘ç¹é‡æ–°è®­ç»ƒçš„é«˜æˆæœ¬ã€‚å…¶ä¸­ï¼Œä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼åœ¨é›†æˆæ–°çŸ¥è¯†çš„åŒæ—¶ä¿æŒæ¨¡å‹çš„åŸå§‹åŠŸèƒ½æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚é’ˆå¯¹ç°æœ‰ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„å±€é™æ€§å’Œä¸è¶³ï¼Œæå‡ºäº†EditCoTè¿™ä¸€æ–°çš„çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿçµæ´»ã€é«˜æ•ˆåœ°æ›´æ–°å„ç§ä»»åŠ¡ä¸‹çš„LLMï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚EditCoTé€šè¿‡ç”Ÿæˆç»™å®šè¾“å…¥çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œç„¶åä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„CoTç¼–è¾‘å™¨è¿›è¡Œè¿­ä»£æ”¹è¿›æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè¯­è¨€å’Œä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹EditCoTè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ã€é€šç”¨æ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€çŸ¥è¯†æ›´æ–°é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯ç”¨äºæ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œé¿å…é‡å»ºæ¨¡å‹çš„æ˜‚è´µæˆæœ¬ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼èƒ½å¤Ÿé›†æˆæ–°çŸ¥è¯†åŒæ—¶ä¿æŒæ¨¡å‹çš„åŸå§‹åŠŸèƒ½ã€‚</li>
<li>ç°æœ‰ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å…·æœ‰ä»»åŠ¡ç‰¹å®šæ€§ï¼Œå¯¹å¤šè·³é—®ç­”ä»»åŠ¡ä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†ä¸‰å…ƒç»„ï¼Œä¸”åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢è¡¨ç°ä¸ç¨³å®šã€‚</li>
<li>EditCoTæ˜¯ä¸€ä¸ªæ–°çš„çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»ã€é«˜æ•ˆåœ°åœ¨å„ç§ä»»åŠ¡ä¸‹æ›´æ–°LLMï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>EditCoTé€šè¿‡ç”Ÿæˆæ€ç»´é“¾å¹¶ä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„ç¼–è¾‘å™¨è¿›è¡Œè¿­ä»£æ”¹è¿›æ¥å®ç°å…¶æ•ˆæœã€‚</li>
<li>EditCoTåœ¨å¤šä¸ªè¯­è¨€å’Œä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e1be402f259b9db3173465e40bb6e235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b5acde3ec834054f975346c33a9b527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6f4d6b655eec9c4a0c97406b80add8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6009917a93f8f9ce7d3730d61f980cf6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Affective-Computing-in-the-Era-of-Large-Language-Models-A-Survey-from-the-NLP-Perspective"><a href="#Affective-Computing-in-the-Era-of-Large-Language-Models-A-Survey-from-the-NLP-Perspective" class="headerlink" title="Affective Computing in the Era of Large Language Models: A Survey from   the NLP Perspective"></a>Affective Computing in the Era of Large Language Models: A Survey from   the NLP Perspective</h2><p><strong>Authors:Yiqun Zhang, Xiaocui Yang, Xingle Xu, Zeran Gao, Yijie Huang, Shiyi Mu, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song, Ge Yu</strong></p>
<p>Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU&#x2F;AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-&#x2F;Prompt-Tuning), Prompt Engineering (zero&#x2F;few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable&#x2F;programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU&#x2F;AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems. </p>
<blockquote>
<p>æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰èåˆäº†è®¡ç®—æœºç§‘å­¦ã€å¿ƒç†å­¦å’Œè®¤çŸ¥ç§‘å­¦ï¼Œä½¿æœºå™¨èƒ½å¤Ÿåœ¨ç¤¾äº¤åª’ä½“ã€é‡‘èã€åŒ»ç–—ä¿å¥å’Œæ•™è‚²ç­‰é¢†åŸŸè¯†åˆ«ã€è§£é‡Šå’Œæ¨¡æ‹Ÿäººç±»æƒ…ç»ªã€‚æƒ…æ„Ÿè®¡ç®—é€šå¸¸å›´ç»•ä¸¤ä¸ªä»»åŠ¡å®¶æ—ï¼šæƒ…æ„Ÿç†è§£ï¼ˆAUï¼‰å’Œæƒ…æ„Ÿç”Ÿæˆï¼ˆAGï¼‰ã€‚è™½ç„¶ç»è¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åœ¨AUæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬å¾€å¾€è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œåœ¨AGæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆå¤šæ ·ä¸”æƒ…æ„Ÿæ°å½“çš„ååº”æ–¹é¢ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ˆä¾‹å¦‚ChatGPTå’ŒLLaMAï¼‰çš„å‡ºç°ï¼Œé€šè¿‡æä¾›ä¸Šä¸‹æ–‡å­¦ä¹ ã€æ›´å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ›´å¼ºçš„åºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œå‚¬åŒ–äº†ä¸€ç§èŒƒå¼è½¬å˜ã€‚è¿™ç¯‡ç»¼è¿°æ–‡ç« ä»‹ç»äº†LLMæ—¶ä»£çš„æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰çš„NLPç›¸å…³æ¦‚è¿°ã€‚æˆ‘ä»¬ï¼ˆiï¼‰æ•´åˆäº†ä¼ ç»Ÿçš„ACä»»åŠ¡å’ŒåŸºäºLLMçš„åˆæ­¥ç ”ç©¶ï¼›ï¼ˆiiï¼‰å›é¡¾äº†æé«˜AU&#x2F;AGçš„é€‚åº”æŠ€æœ¯ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒï¼ˆå…¨é¢å’Œå‚æ•°é«˜æ•ˆçš„æ–¹æ³•ï¼Œå¦‚LoRAã€P-&#x2F;Prompt-Tuningï¼‰ã€æç¤ºå·¥ç¨‹ï¼ˆé›¶&#x2F;å°‘æ ·æœ¬ã€æ€ç»´é“¾ã€åŸºäºä»£ç†çš„æç¤ºï¼‰å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¯¹äºåè€…ï¼Œæˆ‘ä»¬æ€»ç»“äº†äººç±»åå¥½å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€å¯éªŒè¯&#x2F;ç¨‹åºåŒ–å¥–åŠ±ï¼ˆRLVRï¼‰å’Œäººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰ï¼Œè¿™äº›æä¾›åå¥½æˆ–è§„åˆ™åŸºç¡€çš„ä¼˜åŒ–ä¿¡å·ï¼Œæœ‰åŠ©äºå¼•å¯¼AU&#x2F;AGæœå‘åŒç†å¿ƒã€å®‰å…¨å’Œè§„åˆ’ï¼Œå®ç°æ›´ç²¾ç»†æˆ–å¤šç›®æ ‡æ§åˆ¶ã€‚ä¸ºäº†è¯„ä¼°è¿›å±•ï¼Œæˆ‘ä»¬æ±‡æ€»äº†AUå’ŒAGçš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°å®è·µã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ä»ä¼¦ç†ã€æ•°æ®è´¨é‡å’Œå®‰å…¨åˆ°ç¨³å¥è¯„ä¼°å’Œèµ„æºæ•ˆç‡çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡ç»¼è¿°èƒ½æ¾„æ¸…æƒ…æ„Ÿè®¡ç®—çš„æ™¯è§‚ï¼Œå¹¶ä¸ºæ„å»ºçŸ¥æƒ…ã€å¯é å’Œè´Ÿè´£ä»»çš„LLMç³»ç»Ÿæä¾›å®ç”¨æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04638v2">PDF</a> Compared with the previous version, reinforcement learning has been   added (as a new section), including RLHF, RLVR, and RLAIF</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ã€‚æ–‡ç« æ¦‚è¿°äº†æƒ…æ„Ÿç†è§£ï¼ˆAUï¼‰å’Œæƒ…æ„Ÿç”Ÿæˆï¼ˆAGï¼‰ä¸¤å¤§ä»»åŠ¡å®¶æ—ï¼ŒæŒ‡å‡ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨AUæ–¹é¢çš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨AGæ–¹é¢çš„è¡¨ç°ä»éœ€æé«˜ã€‚æ–‡ç« è¿˜ä»‹ç»äº†é€‚åº”æŠ€æœ¯ï¼Œå¦‚æŒ‡ä»¤è°ƒæ•´ã€æç¤ºå·¥ç¨‹å¼ºåŒ–å­¦ä¹ ç­‰ï¼Œå¹¶è®¨è®ºäº†æŒ‘æˆ˜å’Œç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰ç»“åˆäº†è®¡ç®—æœºç§‘å­¦ã€å¿ƒç†å­¦å’Œè®¤çŸ¥ç§‘å­¦ï¼Œä½¿æœºå™¨èƒ½å¤Ÿè¯†åˆ«ã€è§£é‡Šå’Œæ¨¡æ‹Ÿäººç±»æƒ…æ„Ÿã€‚</li>
<li>ACä¸»è¦å…³æ³¨ä¸¤å¤§ä»»åŠ¡ï¼šæƒ…æ„Ÿç†è§£ï¼ˆAUï¼‰å’Œæƒ…æ„Ÿç”Ÿæˆï¼ˆAGï¼‰ã€‚</li>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨AUæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨AGæ–¹é¢çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æä¾›äº†ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„åºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œæ¨åŠ¨äº†ACçš„å‘å±•ã€‚</li>
<li>é€‚åº”æŠ€æœ¯å¦‚æŒ‡ä»¤è°ƒæ•´ã€æç¤ºå·¥ç¨‹å’Œå¼ºåŒ–å­¦ä¹ å¯æ”¹å–„AU&#x2F;AGã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åå¥½ã€å¯éªŒè¯çš„ç¨‹åºå¥–åŠ±å’ŒAIåé¦ˆç­‰æ–¹é¢ä¸ºAU&#x2F;AGæä¾›äº†ä¼˜åŒ–ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e857ad323223ac9a623f94148f11695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7867f1f6972e6556f8670a2bd9a50351.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a679dd33a4c1c1d723984da07c289230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4d33db6781b2f631b33be3d3ac03a09.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MedualTime-A-Dual-Adapter-Language-Model-for-Medical-Time-Series-Text-Multimodal-Learning"><a href="#MedualTime-A-Dual-Adapter-Language-Model-for-Medical-Time-Series-Text-Multimodal-Learning" class="headerlink" title="MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text   Multimodal Learning"></a>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text   Multimodal Learning</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Meng Zhao, Fugee Tsung</strong></p>
<p>The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTimeâ€™s transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. <a target="_blank" rel="noopener" href="https://github.com/start2020/MedualTime">https://github.com/start2020/MedualTime</a> </p>
<blockquote>
<p>æœ€è¿‘è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•èµ·äº†åŒ»å­¦æ—¶é—´åºåˆ—æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå¯¹æ¯”å­¦ä¹ å’ŒåŸºäºæç¤ºçš„LMæ–¹æ³•å¾€å¾€å­˜åœ¨åè§ï¼Œç»å¸¸å°†æ—¶é—´åºåˆ—æ¨¡æ€è§†ä¸ºä¸»è¦è§’è‰²ï¼Œè€Œå°†æ–‡æœ¬æ¨¡æ€è§†ä¸ºæ¬¡è¦ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•å½’ç±»ä¸ºæ—¶é—´ä¸»å¯¼èŒƒå¼ï¼Œè¿™å¯èƒ½å¿½ç•¥äº†æ–‡æœ¬æ¨¡æ€ä¸­åµŒå…¥çš„ç‹¬ç‰¹ä¸”å…³é”®çš„ä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œå¦‚ä¸´åºŠæŠ¥å‘Šï¼Œå› æ­¤æ— æ³•å……åˆ†åˆ©ç”¨ä¸åŒæ¨¡æ€çš„ç›¸äº’ä¼˜åŠ¿å’Œäº’è¡¥æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬-æ—¶é—´å¤šæ¨¡æ€å­¦ä¹ èŒƒå¼ï¼Œä½¿ä»»ä¸€æ¨¡æ€éƒ½å¯ä»¥ä½œä¸ºä¸»è¦æ¨¡æ€ï¼ŒåŒæ—¶å—åˆ°å¦ä¸€æ¨¡æ€çš„å¢å¼ºï¼Œä»è€Œæœ‰æ•ˆåœ°æ•è·ç‰¹å®šäºæ¨¡æ€çš„ä¿¡æ¯å¹¶ä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†MedualTimeï¼Œä¸€ç§ç”±åŒé€‚é…å™¨ç»„æˆçš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶å®ç°æ—¶é—´ä¸»å¯¼å’Œæ–‡æœ¬ä¸»å¯¼å»ºæ¨¡ã€‚åœ¨æ¯ä¸ªé€‚é…å™¨ä¸­ï¼Œæˆ‘ä»¬å°†è½»é‡çº§é€‚é…ä»¤ç‰Œæ³¨å…¥LMçš„é¡¶å±‚ï¼Œä»¥é¼“åŠ±é«˜çº§æ¨¡æ€èåˆã€‚åŒé€‚é…å™¨å…±äº«çš„LMç®¡é“ä¸ä»…å®ç°äº†é€‚é…å™¨å¯¹é½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„å¾®è°ƒï¼Œå‡å°‘äº†è®¡ç®—èµ„æºã€‚å®é™…ä¸Šï¼ŒMedualTimeåœ¨åŒ»ç–—æ•°æ®ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨ç›‘ç£è®¾ç½®ä¸‹å®ç°äº†8%çš„å‡†ç¡®æ€§å’Œ12%çš„F1å€¼æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼ŒMedualTimeçš„å¯è½¬ç§»æ€§é€šè¿‡ä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦åŒ»ç–—æ•°æ®çš„å°‘é‡æ ‡ç­¾è½¬ç§»å®éªŒå¾—åˆ°äº†éªŒè¯ã€‚<a target="_blank" rel="noopener" href="https://github.com/start2020/MedualTime">https://github.com/start2020/MedualTime</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06620v4">PDF</a> 9 pages, 6 figure, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦æ—¶é—´åºåˆ—æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰å¯¹æ¯”å­¦ä¹ å’Œæç¤ºé©±åŠ¨çš„è¯­è¨€æ¨¡å‹æ–¹æ³•ä¸»è¦å…³æ³¨æ—¶é—´åºåˆ—æ¨¡æ€è€Œå¿½è§†æ–‡æœ¬æ¨¡æ€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬æ—¶é—´å¤šæ¨¡æ€å­¦ä¹ èŒƒå¼ã€‚è¯¥èŒƒå¼å…è®¸ä»»ä¸€æ¨¡æ€ä½œä¸ºä¸»è¦æ¨¡æ€ï¼ŒåŒæ—¶è¢«å¦ä¸€æ¨¡æ€å¢å¼ºï¼Œä»è€Œæœ‰æ•ˆæ•è·æ¨¡æ€ç‰¹å®šä¿¡æ¯å¹¶ä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚å…·ä½“åœ°ï¼Œè®¾è®¡äº†ä¸€ç§åä¸ºMedualTimeçš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åŒé€‚é…å™¨å®ç°æ—¶é—´åºåˆ—ä¸»å¯¼å’Œæ–‡æœ¬ä¸»å¯¼å»ºæ¨¡çš„åŒæ—¶è¿›è¡Œã€‚è¯¥æ¨¡å‹åœ¨åŒ»å­¦æ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨ç›‘ç£è®¾ç½®ä¸‹å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡æé«˜8%ï¼ŒF1å¾—åˆ†æé«˜12%ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„åŒ»å­¦æ•°æ®å°‘é‡æ ‡ç­¾è¿ç§»å®éªŒéªŒè¯äº†MedualTimeçš„è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦æ—¶é—´åºåˆ—æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­å–å¾—è¿…é€Ÿè¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨åè§ï¼Œå€¾å‘äºä»¥æ—¶é—´åºåˆ—ä¸ºä¸»ï¼Œæ–‡æœ¬ä¸ºè¾…ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ–‡æœ¬æ—¶é—´å¤šæ¨¡æ€å­¦ä¹ èŒƒå¼ï¼Œä¿ƒè¿›ä¸åŒæ¨¡æ€ä¹‹é—´çš„äº’è¡¥æ€§ã€‚</li>
<li>ä»‹ç»äº†åä¸ºMedualTimeçš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åŒé€‚é…å™¨å®ç°å¤šç§ä¸»å¯¼æ¨¡å¼å»ºæ¨¡ã€‚</li>
<li>MedualTimeæ¨¡å‹åœ¨åŒ»å­¦æ•°æ®ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå‡†ç¡®ç‡å’ŒF1å¾—åˆ†å‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>MedualTimeæ¨¡å‹å…·æœ‰è‰¯å¥½çš„è¿ç§»èƒ½åŠ›ï¼Œå¯é€šè¿‡å°‘é‡æ ‡ç­¾è¿ç§»å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67c6bac4c94e343e5452df9b3188afb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0772bb37285c7a52df532b27de117ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddacb92fbb53370daeb96321fdfb31f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e542828625906d6e26ec5cb7e893b3db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e86f6ddb75be1efbc8cf2f8e842a32de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18d2b7afb6a0919fa0f40ed6f97182ac.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-a-General-Time-Series-Forecasting-Model-with-Unified-Representation-and-Adaptive-Transfer"><a href="#Towards-a-General-Time-Series-Forecasting-Model-with-Unified-Representation-and-Adaptive-Transfer" class="headerlink" title="Towards a General Time Series Forecasting Model with Unified   Representation and Adaptive Transfer"></a>Towards a General Time Series Forecasting Model with Unified   Representation and Adaptive Transfer</h2><p><strong>Authors:Yihang Wang, Yuying Qiu, Peng Chen, Kai Zhao, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</strong></p>
<p>With the growing availability of multi-domain time series data, there is an increasing demand for general forecasting models pre-trained on multi-source datasets to support diverse downstream prediction scenarios. Existing time series foundation models primarily focus on scaling up pre-training datasets and model sizes to enhance generalization performance. In this paper, we take a different approach by addressing two critical aspects of general forecasting models: (1) how to derive unified representations from heterogeneous multi-domain time series data, and (2) how to effectively capture domain-specific features to enable adaptive transfer across various downstream scenarios. To address the first aspect, we propose Decomposed Frequency Learning as the pre-training task, which leverages frequency-based masking and reconstruction to decompose coupled semantic information in time series, resulting in unified representations across domains. For the second aspect, we introduce the Time Series Register, which captures domain-specific representations during pre-training and enhances adaptive transferability to downstream tasks. Our model achieves the state-of-the-art forecasting performance on seven real-world benchmarks, demonstrating remarkable few-shot and zero-shot capabilities. </p>
<blockquote>
<p>éšç€å¤šé¢†åŸŸæ—¶é—´åºåˆ—æ•°æ®çš„æ—¥ç›Šæ™®åŠï¼Œå¯¹é¢„è®­ç»ƒåœ¨å¤šæºæ•°æ®é›†ä¸Šçš„é€šç”¨é¢„æµ‹æ¨¡å‹çš„éœ€æ±‚ä¹Ÿè¶Šæ¥è¶Šé«˜ï¼Œä»¥æ”¯æŒå„ç§ä¸‹æ¸¸é¢„æµ‹åœºæ™¯ã€‚ç°æœ‰çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨æ‰©å¤§é¢„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹è§„æ¨¡ä¸Šï¼Œä»¥æé«˜æ³›åŒ–æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡è§£å†³é€šç”¨é¢„æµ‹æ¨¡å‹çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰å¦‚ä½•ä»å¼‚è´¨å¤šé¢†åŸŸæ—¶é—´åºåˆ—æ•°æ®ä¸­å¯¼å‡ºç»Ÿä¸€è¡¨ç¤ºï¼Œä»¥åŠï¼ˆ2ï¼‰å¦‚ä½•æœ‰æ•ˆåœ°æ•è·ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ï¼Œä»¥å®ç°è·¨å„ç§ä¸‹æ¸¸åœºæ™¯çš„é€‚åº”æ€§è¿ç§»ã€‚ä¸ºè§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ†è§£é¢‘ç‡å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œåˆ©ç”¨åŸºäºé¢‘ç‡çš„æ©è”½å’Œé‡å»ºæ¥åˆ†è§£æ—¶é—´åºåˆ—ä¸­è€¦åˆçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿè·¨é¢†åŸŸçš„ç»Ÿä¸€è¡¨ç¤ºã€‚å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶é—´åºåˆ—å¯„å­˜å™¨ï¼Œå®ƒåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ•è·ç‰¹å®šé¢†åŸŸçš„è¡¨ç¤ºï¼Œå¹¶å¢å¼ºäº†å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§è¿ç§»èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ï¼Œå±•ç°äº†å‡ºè‰²çš„å°æ ·æœ¬å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17478v3">PDF</a> Accepted by the Forty-second International Conference on Machine   Learning (ICML2025)</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šæºæ•°æ®é¢„è®­ç»ƒçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šé¢†åŸŸæ—¶é—´åºåˆ—æ•°æ®çš„ç»Ÿä¸€è¡¨ç¤ºå’Œç‰¹å®šé¢†åŸŸç‰¹å¾çš„æœ‰æ•ˆæ•æ‰é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨åˆ†è§£é¢‘ç‡å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡å’Œæ—¶é—´åºåˆ—å¯„å­˜å™¨æŠ€æœ¯ï¼Œæ¨¡å‹å®ç°äº†è·¨ä¸åŒä¸‹æ¸¸åœºæ™¯çš„é€‚åº”æ€§è¿ç§»ï¼Œå¹¶åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€å¤šé¢†åŸŸæ—¶é—´åºåˆ—æ•°æ®çš„å¢é•¿ï¼Œå¯¹åŸºäºå¤šæºæ•°æ®é›†é¢„è®­ç»ƒçš„é€šç”¨é¢„æµ‹æ¨¡å‹çš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚</li>
<li>ç°æœ‰æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸»è¦å…³æ³¨å¦‚ä½•æ‰©å¤§é¢„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹è§„æ¨¡ä»¥æé«˜æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡è§£å†³äº†ä¸€èˆ¬é¢„æµ‹æ¨¡å‹ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•ä»å¼‚è´¨å¤šé¢†åŸŸæ—¶é—´åºåˆ—æ•°æ®ä¸­æå–ç»Ÿä¸€è¡¨ç¤ºä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°æ•è·é¢†åŸŸç‰¹å®šç‰¹å¾ä»¥å®ç°è·¨å„ç§ä¸‹æ¸¸åœºæ™¯çš„é€‚åº”æ€§è¿ç§»ã€‚</li>
<li>ä¸ºè§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæå‡ºäº†åŸºäºé¢‘ç‡æ©è”½å’Œé‡å»ºçš„åˆ†è§£é¢‘ç‡å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œä»¥åˆ†è§£æ—¶é—´åºåˆ—ä¸­çš„è€¦åˆè¯­ä¹‰ä¿¡æ¯ï¼Œå¾—åˆ°è·¨é¢†åŸŸçš„ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
<li>ä¸ºè§£å†³ç¬¬äºŒä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†æ—¶é—´åºåˆ—å¯„å­˜å™¨ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ•è·ç‰¹å®šé¢†åŸŸçš„è¡¨ç¤ºï¼Œæé«˜äº†å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§è¿ç§»èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8db14ec5ea23ed23c419f0ef6a72bdd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-337d2eed7223c39378ed27833e599408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea0f00aca770f0e68f6953796b1d28d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05a2feab32ff830f7b9774daacd8dd5e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de9edfe9af32be5f431e2a1433e0388c.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  HD 143811 AB b A Directly Imaged Planet Orbiting a Spectroscopic Binary   in Sco-Cen
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c7733bda78cecbaf3ae9405d71c3fc36.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Slice-100K A Multimodal Dataset for Extrusion-based 3D Printing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
