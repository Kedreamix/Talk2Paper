<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  BIR-Adapter A Low-Complexity Diffusion Model Adapter for Blind Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b40eba6c3bb19616fac277665b6d627d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-10-æ›´æ–°"><a href="#2025-09-10-æ›´æ–°" class="headerlink" title="2025-09-10 æ›´æ–°"></a>2025-09-10 æ›´æ–°</h1><h2 id="BIR-Adapter-A-Low-Complexity-Diffusion-Model-Adapter-for-Blind-Image-Restoration"><a href="#BIR-Adapter-A-Low-Complexity-Diffusion-Model-Adapter-for-Blind-Image-Restoration" class="headerlink" title="BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image   Restoration"></a>BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image   Restoration</h2><p><strong>Authors:Cem Eteke, Alexander Griessel, Wolfgang Kellerer, Eckehard Steinbach</strong></p>
<p>This paper introduces BIR-Adapter, a low-complexity blind image restoration adapter for diffusion models. The BIR-Adapter enables the utilization of the prior of pre-trained large-scale diffusion models on blind image restoration without training any auxiliary feature extractor. We take advantage of the robustness of pretrained models. We extract features from degraded images via the model itself and extend the self-attention mechanism with these degraded features. We introduce a sampling guidance mechanism to reduce hallucinations. We perform experiments on synthetic and real-world degradations and demonstrate that BIR-Adapter achieves competitive or better performance compared to state-of-the-art methods while having significantly lower complexity. Additionally, its adapter-based design enables integration into other diffusion models, enabling broader applications in image restoration tasks. We showcase this by extending a super-resolution-only model to perform better under additional unknown degradations. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº† BIR-Adapterï¼Œä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„ä½å¤æ‚åº¦ç›²å›¾åƒæ¢å¤é€‚é…å™¨ã€‚BIR-Adapter å®ç°äº†åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹å¯¹ç›²å›¾åƒæ¢å¤çš„å…ˆéªŒä¿¡æ¯ï¼Œæ— éœ€è®­ç»ƒä»»ä½•è¾…åŠ©ç‰¹å¾æå–å™¨ã€‚æˆ‘ä»¬å……åˆ†åˆ©ç”¨äº†é¢„è®­ç»ƒæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡å‹æœ¬èº«ä»é€€åŒ–å›¾åƒä¸­æå–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿™äº›é€€åŒ–ç‰¹å¾æ‰©å±•è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é‡‡æ ·å¼•å¯¼æœºåˆ¶æ¥å‡å°‘å¹»è§‰ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„é€€åŒ–ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ BIR-Adapter åœ¨å…·æœ‰æ˜¾è‘—é™ä½å¤æ‚æ€§çš„åŒæ—¶ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…¶åŸºäºé€‚é…å™¨çš„è®¾è®¡å¯ä»¥é›†æˆåˆ°å…¶ä»–æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­å…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å°†ä¸€ä¸ªä»…ç”¨äºè¶…åˆ†è¾¨ç‡çš„æ¨¡å‹æ‰©å±•åˆ°åœ¨é¢å¤–çš„æœªçŸ¥é€€åŒ–ä¸‹è¡¨ç°æ›´å¥½æ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06904v1">PDF</a> 20 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BIR-Adapterï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„ä½å¤æ‚åº¦ç›²å›¾åƒæ¢å¤é€‚é…å™¨ã€‚BIR-Adapteråˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ï¼Œæ— éœ€è®­ç»ƒä»»ä½•è¾…åŠ©ç‰¹å¾æå–å™¨å³å¯è¿›è¡Œç›²å›¾åƒæ¢å¤ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç›´æ¥ä»é€€åŒ–å›¾åƒä¸­æå–ç‰¹å¾ï¼Œå¹¶æ‰©å±•è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥åˆ©ç”¨è¿™äº›é€€åŒ–ç‰¹å¾ã€‚é€šè¿‡å¼•å…¥é‡‡æ ·æŒ‡å¯¼æœºåˆ¶å‡å°‘å¹»è±¡ã€‚å®éªŒè¡¨æ˜ï¼ŒBIR-Adapteråœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œé€€åŒ–æ•°æ®ä¸Šè¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½æˆ–æ›´ä¼˜ç§€çš„æ€§èƒ½ï¼Œä¸”å¤æ‚åº¦æ˜¾è‘—é™ä½ã€‚æ­¤å¤–ï¼Œå…¶åŸºäºé€‚é…å™¨çš„è®¾è®¡å¯ä¸å…¶ä»–æ‰©æ•£æ¨¡å‹é›†æˆï¼Œæ‰©å¤§äº†åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­çš„åº”ç”¨èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œå°†ä¸€ä¸ªä»…ç”¨äºè¶…åˆ†è¾¨ç‡çš„æ¨¡å‹æ‰©å±•åˆ°åœ¨æœªçŸ¥é€€åŒ–æ¡ä»¶ä¸‹è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BIR-Adapteræ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„ç›²å›¾åƒæ¢å¤é€‚é…å™¨ï¼Œå…·æœ‰ä½å¤æ‚åº¦ã€‚</li>
<li>å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ï¼Œæ— éœ€è®­ç»ƒé¢å¤–çš„ç‰¹å¾æå–å™¨ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç›´æ¥ä»é€€åŒ–å›¾åƒä¸­æå–ç‰¹å¾ã€‚</li>
<li>æ‰©å±•äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥åˆ©ç”¨é€€åŒ–ç‰¹å¾ï¼Œå¹¶å¼•å…¥é‡‡æ ·æŒ‡å¯¼æœºåˆ¶å‡å°‘å¹»è±¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒBIR-Adapteråœ¨åˆæˆå’ŒçœŸå®é€€åŒ–æ•°æ®ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¤æ‚åº¦æ›´ä½ã€‚</li>
<li>BIR-AdapteråŸºäºé€‚é…å™¨çš„è®¾è®¡å¯ä¸å…¶ä»–æ‰©æ•£æ¨¡å‹é›†æˆï¼Œæé«˜å›¾åƒæ¢å¤ä»»åŠ¡çš„é€‚ç”¨èŒƒå›´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-74f1665dbe3b8843129644978711504d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8aaea6c69a195f13b9f38d66d842060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c17c48161dbe41fe29c63f667f38cdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68c569feae1e851eb0ef88affc1c3a0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35920db4b21ecff75b56bb5d839b2302.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward"><a href="#UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward" class="headerlink" title="UMO: Scaling Multi-Identity Consistency for Image Customization via   Matching Reward"></a>UMO: Scaling Multi-Identity Consistency for Image Customization via   Matching Reward</h2><p><strong>Authors:Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He</strong></p>
<p>Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With â€œmulti-to-multi matchingâ€ paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: <a target="_blank" rel="noopener" href="https://github.com/bytedance/UMO">https://github.com/bytedance/UMO</a> </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒå®šåˆ¶æŠ€æœ¯çš„è¿›å±•å±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œå› å…¶æ›´å¼ºå¤§çš„å®šåˆ¶èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºäººç±»å¯¹äººè„¸æ›´ä¸ºæ•æ„Ÿï¼Œå› æ­¤åœ¨å¤šå‚è€ƒå›¾åƒä¸­ä¿æŒèº«ä»½ä¸€è‡´å¹¶é¿å…èº«ä»½æ··æ·†ä»æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å®šåˆ¶æ¨¡å‹çš„èº«ä»½å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UMOï¼ˆUnified Multi-identity Optimizationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŒé«˜ä¿çœŸèº«ä»½ä¿ç•™ï¼Œå¹¶é€šè¿‡å¯æ‰©å±•æ€§ç¼“è§£èº«ä»½æ··æ·†ã€‚é‡‡ç”¨â€œå¤šå¯¹å¤šåŒ¹é…â€èŒƒå¼ï¼ŒUMOå°†å¤šèº«ä»½ç”Ÿæˆé‡æ–°æ„å»ºä¸ºå…¨çƒåˆ†é…ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä¸ºç°æœ‰çš„å›¾åƒå®šåˆ¶æ–¹æ³•ä¸€èˆ¬é€šè¿‡æ‰©æ•£æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ é‡Šæ”¾å¤šèº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºå¸®åŠ©è®­ç»ƒUMOï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯ä¼¸ç¼©çš„å®šåˆ¶æ•°æ®é›†ï¼ŒåŒ…å«å¤šå‚è€ƒå›¾åƒï¼Œæ—¢æœ‰åˆæˆéƒ¨åˆ†ä¹Ÿæœ‰çœŸå®éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°æŒ‡æ ‡æ¥è¡¡é‡èº«ä»½æ··æ·†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUMOä¸ä»…æ˜¾è‘—æé«˜èº«ä»½ä¸€è‡´æ€§ï¼Œè€Œä¸”åœ¨å¤šç§å›¾åƒå®šåˆ¶æ–¹æ³•ä¸­å‡å°‘äº†èº«ä»½æ··æ·†ï¼Œåœ¨èº«ä»½ä¿ç•™çš„ç»´åº¦ä¸Šæˆä¸ºå¼€æºæ–¹æ³•ä¸­çš„æœ€æ–°å…ˆè¿›æŠ€æœ¯ã€‚ä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/bytedance/UMO">https://github.com/bytedance/UMO</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06818v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://bytedance.github.io/UMO/">https://bytedance.github.io/UMO/</a> Code and model:   <a target="_blank" rel="noopener" href="https://github.com/bytedance/UMO">https://github.com/bytedance/UMO</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå›¾åƒå®šåˆ¶æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä½†ä»é¢ä¸´èº«ä»½ä¸€è‡´æ€§ä¿æŠ¤å’Œé¿å…èº«ä»½æ··æ·†çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šèº«ä»½ä¼˜åŒ–æ¡†æ¶UMOï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£æ¨¡å‹ä¸Šç»´æŠ¤é«˜ä¿çœŸèº«ä»½å¹¶ç¼“è§£èº«ä»½æ··æ·†é—®é¢˜ã€‚UMOå°†å¤šèº«ä»½ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå…¨å±€åˆ†é…ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥â€œå¤šå¯¹å¤šåŒ¹é…â€èŒƒå¼æ¥æé«˜ç°æœ‰å›¾åƒå®šåˆ¶æ–¹æ³•çš„èº«ä»½ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªç”¨äºUMOè®­ç»ƒçš„å¯æ‰©å±•å®šåˆ¶åŒ–æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„èº«ä»½æ··æ·†åº¦é‡æ ‡å‡†ã€‚å®éªŒè¯æ˜ï¼ŒUMOä¸ä»…åœ¨æé«˜èº«ä»½ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å“è¶Šï¼Œè€Œä¸”åœ¨å¤šç§å›¾åƒå®šåˆ¶æ–¹æ³•ä¸­å‡å°‘äº†èº«ä»½æ··æ·†ï¼Œæˆä¸ºå¼€æºæ–¹æ³•ä¸­èº«ä»½ä¿ç•™ç»´åº¦çš„æœ€æ–°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒå®šåˆ¶æŠ€æœ¯çš„å‘å±•å¸¦æ¥äº†å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶åœ¨æ›´å¼ºçš„å®šåˆ¶åŒ–èƒ½åŠ›æ–¹é¢ã€‚</li>
<li>èº«ä»½ä¸€è‡´æ€§çš„ä¿æŠ¤å’Œé¿å…èº«ä»½æ··æ·†æ˜¯å½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>UMOæ¡†æ¶æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£æ¨¡å‹ä¸Šç»´æŠ¤é«˜ä¿çœŸèº«ä»½å¹¶ç¼“è§£èº«ä»½æ··æ·†é—®é¢˜ã€‚</li>
<li>UMOé‡‡ç”¨â€œå¤šå¯¹å¤šåŒ¹é…â€èŒƒå¼ï¼Œå°†å¤šèº«ä»½ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå…¨å±€åˆ†é…ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>UMOçš„å¼€å‘ä¾èµ–äºä¸€ä¸ªç”¨äºè®­ç»ƒçš„å¯æ‰©å±•å®šåˆ¶åŒ–æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«åˆæˆå’ŒçœŸå®å›¾åƒã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†æ¥è¡¡é‡èº«ä»½æ··æ·†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-feceb6b01b48a180df58ebb0a7d95289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4483a7da13b9614694a1134a976db7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-972ae78ca33fe0a1b7cba4475d0c8770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5df18d21247ba96a400ecffb55d6638d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2cb1b950af1222fa36895c8b0dba4e8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-Rigging-to-Waving-3D-Guided-Diffusion-for-Natural-Animation-of-Hand-Drawn-Characters"><a href="#From-Rigging-to-Waving-3D-Guided-Diffusion-for-Natural-Animation-of-Hand-Drawn-Characters" class="headerlink" title="From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of   Hand-Drawn Characters"></a>From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of   Hand-Drawn Characters</h2><p><strong>Authors:Jie Zhou, Linzi Qu, Miu-Ling Lam, Hongbo Fu</strong></p>
<p>Hand-drawn character animation is a vibrant field in computer graphics, presenting challenges in achieving geometric consistency while conveying expressive motion. Traditional skeletal animation methods maintain geometric consistency but struggle with complex non-rigid elements like flowing hair and skirts, leading to unnatural deformation. Conversely, video diffusion models synthesize realistic dynamics but often create geometric distortions in stylized drawings due to domain gaps. This work proposes a hybrid animation system that combines skeletal animation and video diffusion. Initially, coarse images are generated from characters retargeted with skeletal animations for geometric guidance. These images are then enhanced in texture and secondary dynamics using video diffusion priors, framing this enhancement as an inpainting task. A domain-adapted diffusion model refines user-masked regions needing improvement, especially for secondary dynamics. To enhance motion realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in the denoising process, incorporating features from a pre-trained diffusion model enriched with human motion priors. Additionally, to tackle unnatural deformations from low-poly single-mesh character modeling, we present a Hair Layering Modeling (HLM) technique that uses segmentation maps to separate hair from the body, allowing for more natural animation of long-haired characters. Extensive experiments show that our system outperforms state-of-the-art methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>æ‰‹ç»˜å­—ç¬¦åŠ¨ç”»æ˜¯è®¡ç®—æœºå›¾å½¢å­¦ä¸­çš„ä¸€ä¸ªå……æ»¡ç”Ÿæœºçš„é¢†åŸŸï¼Œå®ƒåœ¨å®ç°å‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œè¿˜é¢ä¸´ç€è¡¨è¾¾è¿åŠ¨çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿéª¨éª¼åŠ¨ç”»æ–¹æ³•èƒ½å¤Ÿä¿æŒå‡ ä½•ä¸€è‡´æ€§ï¼Œä½†åœ¨å¤„ç†æµåŠ¨å¤´å‘å’Œè£™å­ç­‰å¤æ‚çš„éåˆšæ€§å…ƒç´ æ—¶å´æ„Ÿåˆ°å›°éš¾ï¼Œå¯¼è‡´å˜å½¢ä¸è‡ªç„¶ã€‚ç›¸åï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåˆæˆé€¼çœŸçš„åŠ¨æ€å›¾åƒï¼Œä½†ç”±äºé¢†åŸŸå·®å¼‚ï¼Œå¾€å¾€åœ¨é£æ ¼åŒ–çš„ç»˜ç”»ä¸­äº§ç”Ÿå‡ ä½•å¤±çœŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆéª¨éª¼åŠ¨ç”»å’Œè§†é¢‘æ‰©æ•£çš„æ··åˆåŠ¨ç”»ç³»ç»Ÿã€‚é¦–å…ˆï¼Œåˆ©ç”¨éª¨éª¼åŠ¨ç”»å¯¹è§’è‰²è¿›è¡Œé‡å®šå‘ï¼Œç”Ÿæˆç²—ç•¥å›¾åƒï¼Œä»¥æä¾›å‡ ä½•æŒ‡å¯¼ã€‚ç„¶åï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£å…ˆéªŒå¢å¼ºè¿™äº›å›¾åƒçš„çº¹ç†å’Œæ¬¡è¦åŠ¨åŠ›å­¦ï¼Œå°†è¿™ä¸€å¢å¼ºè¿‡ç¨‹è§†ä¸ºä¸€ç§å¡«å……ä»»åŠ¡ã€‚åŸŸé€‚åº”æ‰©æ•£æ¨¡å‹ä¼šç²¾ç»†åŒ–ç”¨æˆ·æ©ç½©çš„åŒºåŸŸï¼Œå°¤å…¶æ˜¯éœ€è¦æ”¹è¿›çš„æ¬¡è¦åŠ¨åŠ›å­¦åŒºåŸŸã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è¿åŠ¨é€¼çœŸåº¦ï¼Œæˆ‘ä»¬åœ¨å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ç§æ¬¡è¦åŠ¨åŠ›å­¦æ³¨å…¥ï¼ˆSDIï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„äººç±»è¿åŠ¨å…ˆéªŒç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ä½å¤šè¾¹å½¢å•ç½‘æ ¼è§’è‰²å»ºæ¨¡å¸¦æ¥çš„ä¸è‡ªç„¶å˜å½¢é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤´å‘åˆ†å±‚å»ºæ¨¡ï¼ˆHLMï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨åˆ†å‰²å›¾å°†å¤´å‘ä¸èº«ä½“åˆ†å¼€ï¼Œä»è€Œå®ç°é•¿å‘è§’è‰²çš„æ›´è‡ªç„¶åŠ¨ç”»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06573v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆéª¨éª¼åŠ¨ç”»å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ··åˆåŠ¨ç”»ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé¦–å…ˆé€šè¿‡éª¨éª¼åŠ¨ç”»å¯¹è§’è‰²è¿›è¡Œå‡ ä½•æŒ‡å¯¼ç”Ÿæˆç²—å›¾åƒï¼Œç„¶ååˆ©ç”¨è§†é¢‘æ‰©æ•£å…ˆéªŒå¢å¼ºçº¹ç†å’Œæ¬¡è¦åŠ¨åŠ›å­¦ï¼Œå°†å…¶è§†ä¸ºä¿®å¤ä»»åŠ¡ã€‚é€šè¿‡åŸŸé€‚åº”æ‰©æ•£æ¨¡å‹å¯¹ç”¨æˆ·éœ€è¦æ”¹è¿›çš„åŒºåŸŸè¿›è¡Œç»†åŒ–å¤„ç†ï¼Œå°¤å…¶æ˜¯æ¬¡è¦åŠ¨åŠ›å­¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è¿åŠ¨çœŸå®æ„Ÿï¼Œæˆ‘ä»¬åœ¨å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†æ¬¡è¦åŠ¨åŠ›å­¦æ³¨å…¥ï¼ˆSDIï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èå…¥äººç±»è¿åŠ¨å…ˆéªŒçš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ä½å¤šè¾¹å½¢å•ç½‘æ ¼è§’è‰²å»ºæ¨¡å¸¦æ¥çš„ä¸è‡ªç„¶å˜å½¢é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤´å‘åˆ†å±‚å»ºæ¨¡ï¼ˆHLMï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨åˆ†å‰²å›¾å°†å¤´å‘ä¸èº«ä½“åˆ†ç¦»ï¼Œä½¿é•¿å‘è§’è‰²çš„åŠ¨ç”»æ›´åŠ è‡ªç„¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ··åˆåŠ¨ç”»ç³»ç»Ÿç»“åˆäº†éª¨éª¼åŠ¨ç”»å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°å‡ ä½•ä¸€è‡´æ€§å’Œè¡¨è¾¾æ€§è¿åŠ¨ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡éª¨éª¼åŠ¨ç”»ç”Ÿæˆè§’è‰²çš„ç²—å›¾åƒï¼Œä¸ºåŠ¨ç”»æä¾›å‡ ä½•æŒ‡å¯¼ã€‚</li>
<li>åˆ©ç”¨è§†é¢‘æ‰©æ•£å…ˆéªŒå¢å¼ºå›¾åƒçš„çº¹ç†å’Œæ¬¡è¦åŠ¨åŠ›å­¦ï¼Œå°†å…¶è¡¨è¿°ä¸ºä¿®å¤ä»»åŠ¡ã€‚</li>
<li>åŸŸé€‚åº”æ‰©æ•£æ¨¡å‹ç”¨äºç»†åŒ–ç”¨æˆ·éœ€è¦æ”¹è¿›çš„åŒºåŸŸï¼Œå°¤å…¶æ˜¯æ¬¡è¦åŠ¨åŠ›å­¦æ–¹é¢çš„æ”¹è¿›ã€‚</li>
<li>å¼•å…¥æ¬¡è¦åŠ¨åŠ›å­¦æ³¨å…¥ï¼ˆSDIï¼‰ç­–ç•¥ï¼Œæé«˜è¿åŠ¨çœŸå®æ„Ÿï¼Œç»“åˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œäººç±»è¿åŠ¨å…ˆéªŒã€‚</li>
<li>æå‡ºå¤´å‘åˆ†å±‚å»ºæ¨¡ï¼ˆHLMï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨åˆ†å‰²å›¾æ”¹å–„é•¿å‘è§’è‰²çš„åŠ¨ç”»è‡ªç„¶åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨åŠ¨ç”»è´¨é‡å’Œç°å®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bffb59928915b425cf68f3fe3db9509a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17f59f5438eddab380b0d2dc17b1571b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107587a16dbe380904a1c59a0c18f8e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24273181fd5910854eb8de6d71d5fefd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84c300f46303d92824cd119120edf37c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c5aa64c978d03e50496cf04627c814.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9f0d355750a634b0f0d64b456bae5f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-363801aa73efa8e2d7f34ec5504bd550.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement"><a href="#TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement" class="headerlink" title="TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement"></a>TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement</h2><p><strong>Authors:Jibai Lin, Bo Ma, Yating Yang, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang, Xi Zhou</strong></p>
<p>Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired â€œwinningâ€ (balanced preservation-compliance) and â€œlosingâ€ (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDEâ€™s superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDEâ€™s versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE">https://github.com/KomJay520/TIDE</a>. </p>
<blockquote>
<p>ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆï¼ˆSDIGï¼‰æ—¨åœ¨æ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ“ä½œå›¾åƒä¸­çš„ç‰¹å®šä¸»é¢˜ï¼Œè¿™å¯¹äºæ¨è¿›æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚SDIGéœ€è¦åœ¨ä¿æŒä¸»é¢˜èº«ä»½å’Œéµå®ˆåŠ¨æ€ç¼–è¾‘æŒ‡ä»¤ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œç°æœ‰æ–¹æ³•å¯¹æ­¤æŒ‘æˆ˜è§£å†³ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›®æ ‡æŒ‡ä»¤æ‰©æ•£å¢å¼ºï¼ˆTIDEï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›®æ ‡ç›‘ç£å’Œåå¥½å­¦ä¹ è§£å†³è¿™ä¸€å¹³è¡¡é—®é¢˜ï¼Œè€Œæ— éœ€è¿›è¡Œæµ‹è¯•æ—¶å¾®è°ƒã€‚TIDEé¦–åˆ›ç›®æ ‡ç›‘ç£ä¸‰å…ƒç»„å¯¹é½ï¼Œä½¿ç”¨ï¼ˆå‚è€ƒå›¾åƒã€æŒ‡ä»¤ã€ç›®æ ‡å›¾åƒï¼‰ä¸‰å…ƒç»„å¯¹ä¸»é¢˜é€‚åº”åŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›´æ¥ä¸»é¢˜æ‰©æ•£ï¼ˆDSDï¼‰ç›®æ ‡ï¼Œç”¨é…å¯¹çš„â€œè·èƒœâ€ï¼ˆå¹³è¡¡ä¿ç•™å’Œåˆè§„æ€§ï¼‰å’Œâ€œå¤±è´¥â€ï¼ˆå¤±çœŸï¼‰ç›®æ ‡è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›ç›®æ ‡é€šè¿‡å®šé‡æŒ‡æ ‡ç³»ç»Ÿç”Ÿæˆå¹¶è¿›è¡Œè¯„ä¼°ã€‚è¿™ä¸ºå®ç°æœ€ä¼˜ä¿ç•™åˆè§„å¹³è¡¡æä¾›äº†éšå¼å¥–åŠ±å»ºæ¨¡ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTIDEåœ¨ç”Ÿæˆå¿ å®äºä¸»é¢˜çš„è¾“å‡ºå¹¶ä¿æŒæŒ‡ä»¤åˆè§„æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨å¤šä¸ªå®šé‡æŒ‡æ ‡ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚TIDEçš„é€šç”¨æ€§è¿›ä¸€æ­¥ä½“ç°åœ¨å…¶åœ¨ç»“æ„æ¡ä»¶ç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å›¾åƒæ’å€¼ç­‰å¤šæ ·åŒ–ä»»åŠ¡çš„æˆåŠŸåº”ç”¨ä¸Šã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE">https://github.com/KomJay520/TIDE</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06499v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬æŒ‡ä»¤é©±åŠ¨å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTIDEï¼ˆç›®æ ‡æŒ‡å¯¼æ‰©æ•£å¢å¼ºï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨ç»´æŒä¸»é¢˜èº«ä»½ä¸éµå¾ªåŠ¨æ€ç¼–è¾‘æŒ‡ä»¤ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡ç›®æ ‡ç›‘ç£ä¸åå¥½å­¦ä¹ ï¼Œæ— éœ€æµ‹è¯•æ—¶å¾®è°ƒå³å¯å®ç°ã€‚TIDEé‡‡ç”¨ç›®æ ‡ç›‘ç£çš„ä¸‰é‡å¯¹å‡†æŠ€æœ¯ï¼Œä½¿ç”¨ï¼ˆå‚è€ƒå›¾åƒã€æŒ‡ä»¤ã€ç›®æ ‡å›¾åƒï¼‰ä¸‰é‡ç»“æ„æ¥æ¨¡æ‹Ÿä¸»é¢˜é€‚åº”åŠ¨æ€ã€‚é€šè¿‡é‡‡ç”¨ç›´æ¥ä¸»é¢˜æ‰©æ•£ï¼ˆDSDï¼‰ç›®æ ‡ï¼Œè®­ç»ƒæ¨¡å‹ä»¥å¹³è¡¡ä¿å­˜å’Œåˆè§„æ€§çš„â€œè·èƒœâ€å’Œâ€œå¤±è´¥â€ç›®æ ‡ï¼Œå¹¶é€šè¿‡å®šé‡æŒ‡æ ‡è¿›è¡Œç³»ç»Ÿçš„ç”Ÿæˆä¸è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜TIDEåœ¨ç”Ÿæˆå¿ å®äºä¸»é¢˜ä¸”ç¬¦åˆæŒ‡ä»¤çš„è¾“å‡ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸”åœ¨å¤šç§å®šé‡æŒ‡æ ‡ä¸Šä¼˜äºåŸºå‡†æ–¹æ³•ã€‚TIDEçš„é€šç”¨æ€§è¿˜ä½“ç°åœ¨å…¶åœ¨ç»“æ„æ¡ä»¶ç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å›¾åƒæ’å€¼ç­‰ä»»åŠ¡çš„æˆåŠŸåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIDEæ¡†æ¶è§£å†³äº†åœ¨æ–‡æœ¬æŒ‡ä»¤é©±åŠ¨å›¾åƒç”Ÿæˆä¸­ç»´æŒä¸»é¢˜èº«ä»½ä¸éµå¾ªåŠ¨æ€ç¼–è¾‘æŒ‡ä»¤ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚</li>
<li>TIDEé‡‡ç”¨ç›®æ ‡ç›‘ç£çš„ä¸‰é‡å¯¹å‡†æŠ€æœ¯ï¼Œå®ç°ä¸»é¢˜é€‚åº”åŠ¨æ€çš„å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ç›´æ¥ä¸»é¢˜æ‰©æ•£ï¼ˆDSDï¼‰ç›®æ ‡ï¼Œè®­ç»ƒæ¨¡å‹ä»¥å¹³è¡¡ä¿å­˜å’Œåˆè§„æ€§ã€‚</li>
<li>TIDEé€šè¿‡ç³»ç»Ÿçš„ç”Ÿæˆä¸è¯„ä¼°ï¼Œé‡‡ç”¨å®šé‡æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜TIDEåœ¨ç”Ÿæˆå¿ å®äºä¸»é¢˜ä¸”ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>TIDEåœ¨å¤šç§å®šé‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e488aa9ca74eb9f9f7d15570f8eda5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98381ae38909c2778d2e7c6f1b553a68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acb18ee69b3d68a29d12abbf27c31b8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb794b77b221060b1ddf32f16be8145c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1650c731f8b7315f3f6a588f949be7c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VQualA-2025-Challenge-on-Image-Super-Resolution-Generated-Content-Quality-Assessment-Methods-and-Results"><a href="#VQualA-2025-Challenge-on-Image-Super-Resolution-Generated-Content-Quality-Assessment-Methods-and-Results" class="headerlink" title="VQualA 2025 Challenge on Image Super-Resolution Generated Content   Quality Assessment: Methods and Results"></a>VQualA 2025 Challenge on Image Super-Resolution Generated Content   Quality Assessment: Methods and Results</h2><p><strong>Authors:Yixiao Li, Xin Li, Chris Wei Zhou, Shuo Xing, Hadi Amirpour, Xiaoshuai Hao, Guanghui Yue, Baoquan Zhao, Weide Liu, Xiaoyuan Yang, Zhengzhong Tu, Xinyu Li, Chuanbiao Song, Chenqi Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Xiaoyan Sun, Shishun Tian, Dongyang Yan, Weixia Zhang, Junlin Chen, Wei Sun, Zhihua Wang, Zhuohang Shi, Zhizun Luo, Hang Ouyang, Tianxin Xiao, Fan Yang, Zhaowang Wu, Kaixin Deng</strong></p>
<p>This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lighting-YXLI/ISRGen-QA">https://github.com/Lighting-YXLI/ISRGen-QA</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå›¾åƒè¶…åˆ†è¾¨ç‡ç”Ÿæˆå†…å®¹è´¨é‡è¯„ä¼°ï¼ˆISRGen-QAï¼‰æ•°æ®é›†æ„å»ºçš„ISRGC-QæŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜ä½œä¸ºICCV 2025ç ”è®¨ä¼šè§†è§‰è´¨é‡è¯„ä¼°ï¼ˆVQualAï¼‰ç«èµ›çš„ä¸€éƒ¨åˆ†è€Œç»„ç»‡ã€‚ä¸ç°æœ‰çš„è¶…åˆ†è¾¨ç‡å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆSR-IQAï¼‰æ•°æ®é›†ä¸åŒï¼ŒISRGen-QAæ›´ä¾§é‡äºç”±æœ€æ–°ç”Ÿæˆæ–¹æ³•ï¼ˆåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼‰ç”Ÿæˆçš„SRå›¾åƒã€‚æ­¤æŒ‘æˆ˜çš„ä¸»è¦ç›®æ ‡æ˜¯åˆ†æç°ä»£è¶…åˆ†è¾¨ç‡æŠ€æœ¯å¼•å…¥çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶æœ‰æ•ˆåœ°è¯„ä¼°å…¶æ„ŸçŸ¥è´¨é‡ã€‚å…±æœ‰108åå‚ä¸è€…æ³¨å†Œå‚åŠ æ­¤æ¬¡æŒ‘æˆ˜ï¼Œå…¶ä¸­4æ”¯é˜Ÿä¼ä¸ºæœ€ç»ˆæµ‹è¯•é˜¶æ®µæäº¤äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆå’Œæƒ…å†µä»‹ç»è¡¨ã€‚è¿™äº›æäº¤åœ¨ISRGen-QAæ•°æ®é›†ä¸Šå±•ç°äº†æœ€æ–°å‰æ²¿çš„æ€§èƒ½ã€‚è¯¥é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lighting-YXLI/ISRGen-QA%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Lighting-YXLI/ISRGen-QAå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06413v1">PDF</a> 11 pages, 12 figures, VQualA ICCV Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ISRGC-QæŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜åŸºäºå›¾åƒè¶…åˆ†è¾¨ç‡ç”Ÿæˆå†…å®¹è´¨é‡è¯„ä¼°ï¼ˆISRGen-QAï¼‰æ•°æ®é›†ï¼Œä½œä¸ºICCV 2025ç ”è®¨ä¼šè§†è§‰è´¨é‡è¯„ä¼°ï¼ˆVQualAï¼‰ç«èµ›çš„ä¸€éƒ¨åˆ†ã€‚æŒ‘æˆ˜çš„é‡ç‚¹åœ¨äºåˆ†æç°ä»£è¶…åˆ†è¾¨ç‡æŠ€æœ¯å¼•å…¥çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶æœ‰æ•ˆè¯„ä¼°å…¶æ„ŸçŸ¥è´¨é‡ã€‚å…±æœ‰108æ”¯é˜Ÿä¼æŠ¥åå‚åŠ ï¼Œæœ€ç»ˆæœ‰4æ”¯é˜Ÿä¼åœ¨æµ‹è¯•é˜¶æ®µæäº¤äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆå’Œäº‹å®èµ„æ–™ï¼Œå±•ç°äº†åœ¨ISRGen-QAæ•°æ®é›†ä¸Šçš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é¡¹ç›®å…¬å¼€å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Lighting-YXLI/ISRGen-QA%E3%80%82">https://github.com/Lighting-YXLI/ISRGen-QAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ISRGC-QæŒ‘æˆ˜åŸºäºISRGen-QAæ•°æ®é›†æ„å»ºï¼Œä¸“æ³¨äºç°ä»£ç”ŸæˆæŠ€æœ¯ï¼ˆå¦‚GANså’Œæ‰©æ•£æ¨¡å‹ï¼‰ç”Ÿæˆçš„SRå›¾åƒè´¨é‡è¯„ä¼°ã€‚</li>
<li>æ­¤æŒ‘æˆ˜çš„ä¸»è¦ç›®æ ‡æ˜¯åˆ†æç°ä»£è¶…åˆ†è¾¨ç‡æŠ€æœ¯çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶æœ‰æ•ˆè¯„ä¼°å…¶æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>ä¸€å…±æœ‰108æ”¯é˜Ÿä¼æ³¨å†Œå‚åŠ äº†æ­¤æŒ‘æˆ˜ã€‚</li>
<li>æœ€ç»ˆæœ‰4æ”¯é˜Ÿä¼åœ¨æµ‹è¯•é˜¶æ®µæäº¤äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆå’Œäº‹å®èµ„æ–™ã€‚</li>
<li>è¿™äº›æäº¤çš„æ–¹æ¡ˆåœ¨ISRGen-QAæ•°æ®é›†ä¸Šå±•ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è¯¥é¡¹ç›®å…¬å¼€å¯è®¿é—®ï¼Œæ–¹ä¾¿å…¬ä¼—äº†è§£å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69db8fc6e5fba152d214ff7759154469.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f3b7549aa66d7ea8aeb4725a8f0ecd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab363c74204ebf4785ef3a1bd4267b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0f6d43d021146896a07d3cec559fa90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d6acd2eaa6b441ec4ab27fc931d8eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebafa6c9b5f1d9f017537c09f1f083ee.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Strategy-Guided-Diffusion-via-Sparse-Masking-Temporal-Reweighting-Distribution-Correction"><a href="#Multi-Strategy-Guided-Diffusion-via-Sparse-Masking-Temporal-Reweighting-Distribution-Correction" class="headerlink" title="Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting   Distribution Correction"></a>Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting   Distribution Correction</h2><p><strong>Authors:Zekun Zhou, Yanru Gong, Liu Shi, Qiegen Liu</strong></p>
<p>Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºçš„ç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒç§¯åˆ†åˆ†å¸ƒä¼°è®¡å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”±ç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼çš„è”åˆè®­ç»ƒæœºåˆ¶ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚åŸºäºç³»ç»Ÿçš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éšæ—¶é—´å˜åŒ– çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨ç”±çº¯å™ªå£°é€æ¸å˜ä¸ºçœŸå®å›¾åƒçš„æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ¸æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚é‡‡ç”¨çº¿æ€§å›å½’æ ¡æ­£å·²çŸ¥æ•°æ®å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œä»¥åœ¨å¤šä¸ªå­é¢‘ç‡åˆ†é‡ä¸Šæ‰§è¡Œå…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œä»è€Œæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°äº†é«˜è´¨é‡å›¾åƒé‡å»ºã€‚åœ¨å…¬å…±å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨è¡¨æ˜ï¼Œä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨PSNRä¸Šæé«˜äº†2.58 dBï¼Œåœ¨SSIMä¸Šæé«˜äº†2.37%ï¼Œåœ¨MSEä¸Šé™ä½äº†0.236ã€‚é‡å»ºçš„å›¾åƒåœ¨ç»“æ„ä¸€è‡´æ€§ã€ç»†èŠ‚æ¢å¤å’Œä¼ªå½±æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05992v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºçš„ç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒç§¯åˆ†åˆ†å¸ƒä¼°è®¡å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ã€‚é€šè¿‡è®¾è®¡è”åˆè®­ç»ƒæœºåˆ¶å’Œç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼ï¼Œæ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚æå‡ºåŠ¨æ€è°ƒæ•´æƒé‡çš„æ—¶é—´å˜åŒ–ç¨€ç–æ¡ä»¶é‡åŠ æƒå¼•å¯¼ç­–ç•¥ï¼Œåœ¨ä»çº¯å™ªå£°åˆ°çœŸå®å›¾åƒçš„æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­é€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚é‡‡ç”¨çº¿æ€§å›å½’æ ¡æ­£å·²çŸ¥å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œè¿›è¡Œè·¨å¤šä¸ªå­é¢‘ç‡ç»„ä»¶çš„å…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œå®ç°é«˜è´¨é‡å›¾åƒé‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å¼€å’ŒçœŸå®æ•°æ®é›†ä¸Šå‡å–å¾—æœ€ä½³æ•ˆæœï¼Œä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒPSNRæé«˜2.58 dBï¼ŒSSIMæé«˜2.37%ï¼ŒMSEé™ä½0.236ã€‚é‡å»ºçš„å›¾åƒåœ¨ç»“æ„ä¸€è‡´æ€§ã€ç»†èŠ‚æ¢å¤å’Œä¼ªå½±æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSTRIDEçš„ç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒé›†æˆåˆ†å¸ƒä¼°è®¡å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å’Œç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼è®¾è®¡ï¼Œæ¨¡å‹èƒ½å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚</li>
<li>æå‡ºæ—¶é—´å˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒç­–ç•¥ï¼Œä»¥åœ¨å»å™ªè¿‡ç¨‹ä¸­é€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨çº¿æ€§å›å½’æ ¡æ­£æ•°æ®åˆ†å¸ƒåç§»ï¼Œå‡å°‘æŒ‡å¯¼è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œæé«˜æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒ…æ‹¬PSNRã€SSIMå’ŒMSEç­‰æŒ‡æ ‡çš„æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2eb9393ad650011c0e2761f7b460752c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab165bb069aecbff4bd7b67289198e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bc9bc101b1141f2f23bc51e672363d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61369320df9cda5ce266b5ab1687fe9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6375f0b02934465ae1dbd273aa601f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance"><a href="#Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance" class="headerlink" title="Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance"></a>Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance</h2><p><strong>Authors:Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel</strong></p>
<p>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brainâ€™s three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimerâ€™s disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§æ¡ä»¶ä¸‹ç”Ÿæˆ2Då›¾åƒçš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨2Dæ–¹é¢çš„å‡ºè‰²è¡¨ç°å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºä¸°å¯Œä¸”æ˜“äºè·å–çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ”¯æ’‘ã€‚å…³é”®çš„æ˜¯ï¼Œ3Dé¢†åŸŸçš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è¿˜ä¸å­˜åœ¨ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚å› æ­¤ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä»…æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„3Dåäº‹å®åŒ»å­¦å›¾åƒæ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°æ¢ç´¢ã€‚å¡«è¡¥è¿™ä¸€ç©ºç™½å°†èƒ½å¤Ÿæ¨åŠ¨å¼ºå¤§çš„ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ï¼Œå¦‚ä¸ªæ€§åŒ–åäº‹å®è§£é‡Šã€ç–¾ç—…è¿›å±•æƒ…æ™¯æ¨¡æ‹Ÿä»¥åŠé€šè¿‡è¯¦ç»†å¯è§†åŒ–å‡è®¾åŒ»å­¦çŠ¶å†µå¢å¼ºåŒ»å­¦åŸ¹è®­ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å¼•å…¥ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æç¤ºç”Ÿæˆç”±åˆæˆæ‚£è€…çš„é«˜åˆ†è¾¨ç‡3Dåäº‹å®åŒ»å­¦å›¾åƒæŒ‡å¯¼çš„åŒ»å­¦å›¾åƒï¼Œæœç€è§£å†³è¿™ä¸€æŒ‘æˆ˜è¿ˆå‡ºäº†æœ‰æ„ä¹‰çš„ä¸€æ­¥ã€‚æˆ‘ä»¬é‡‡ç”¨æœ€å…ˆè¿›çš„3Dæ‰©æ•£æ¨¡å‹ï¼Œå€ŸåŠ©Simple Diffusionçš„å¢å¼ºåŠŸèƒ½å¹¶è¿›è¡Œæ‰©å±•ï¼Œå¹¶å¢åŠ é™„åŠ æ¡ä»¶æ¥æ”¹å–„æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†è¯­è¨€æŒ‡å¯¼çš„æœ¬åœ°3Dæ‰©æ•£æ¨¡å‹ä¸“é—¨åº”ç”¨äºç¥ç»æˆåƒæ•°æ®ï¼Œå…¶ä¸­å¿ å®çš„ä¸‰ç»´å»ºæ¨¡å¯¹äºè¡¨ç¤ºå¤§è„‘çš„ä¸‰ç»´ç»“æ„è‡³å…³é‡è¦ã€‚é€šè¿‡ä¸¤ä¸ªä¸åŒçš„ç¥ç»MRIæ•°æ®é›†çš„ç»“æœï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸæ¨¡æ‹Ÿäº†å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ä¸­çš„ä¸åŒåäº‹å®ç—…å˜è´Ÿè·ä»¥åŠé˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„è®¤çŸ¥çŠ¶æ€ï¼Œåœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„åŒæ—¶ä¿æŒåˆæˆåŒ»å­¦å›¾åƒçš„ä¸»ä½“ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¥ å®šäº†åŸºäºæç¤ºçš„ç–¾ç—…è¿›å±•åˆ†æåœ¨3DåŒ»å­¦æˆåƒä¸­çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05978v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3Dæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶è¿›å±•ã€‚ç”±äºç¼ºå°‘é€‚ç”¨äº3Dçš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡3Då›¾åƒæ–¹é¢å­˜åœ¨å±€é™ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨å…ˆè¿›çš„3Dæ‰©æ•£æ¨¡å‹å¹¶ç»“åˆå¢å¼ºæ¡ä»¶æŠ€æœ¯ï¼Œç”Ÿæˆç”±è‡ªç„¶è¯­è¨€æç¤ºå¼•å¯¼çš„é«˜è´¨é‡3DåŒ»å­¦å›¾åƒã€‚è¯¥ç ”ç©¶æˆåŠŸåº”ç”¨äºç¥ç»ç³»ç»Ÿæˆåƒæ•°æ®ï¼Œå¯æ¨¡æ‹Ÿå¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è™šæ„ç—…å˜ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”ä¿ç•™æ‚£è€…ç‰¹æ€§çš„åŒ»å­¦å›¾åƒã€‚è¿™æ ‡å¿—ç€ä¸ºè¯­è¨€é©±åŠ¨çš„ç–¾ç—…è¿›å±•åˆ†æåœ¨3DåŒ»å­¦æˆåƒé¢†åŸŸå¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡3Då›¾åƒæ–¹é¢çš„å±€é™æ€§æºäºç¼ºä¹ç›¸åº”çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹ç¥ç»ç³»ç»Ÿæˆåƒæ•°æ®çš„è¯­è¨€å¼•å¯¼åŸç”Ÿ3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å¢å¼ºæ¡ä»¶æŠ€æœ¯ï¼Œæé«˜äº†æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</li>
<li>æˆåŠŸåº”ç”¨äºå¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è™šæ„ç—…å˜æ¨¡æ‹Ÿï¼Œç”Ÿæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè¯­è¨€é©±åŠ¨çš„ç–¾ç—…è¿›å±•åˆ†æåœ¨3DåŒ»å­¦æˆåƒé¢†åŸŸæä¾›äº†åšå®åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33d85c9938d3d69c2e7985a5ead7914c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84788cd2c6aaec08fb6b9eec83a92e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea24de25c382ba42b67043adb35d3b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-897e1f7c4fd0dbd99a0e12217777eed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe4a36818c6a34cca0fd5b3099d757ce.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SuMa-A-Subspace-Mapping-Approach-for-Robust-and-Effective-Concept-Erasure-in-Text-to-Image-Diffusion-Models"><a href="#SuMa-A-Subspace-Mapping-Approach-for-Robust-and-Effective-Concept-Erasure-in-Text-to-Image-Diffusion-Models" class="headerlink" title="SuMa: A Subspace Mapping Approach for Robust and Effective Concept   Erasure in Text-to-Image Diffusion Models"></a>SuMa: A Subspace Mapping Approach for Robust and Effective Concept   Erasure in Text-to-Image Diffusion Models</h2><p><strong>Authors:Kien Nguyen, Anh Tran, Cuong Pham</strong></p>
<p>The rapid growth of text-to-image diffusion models has raised concerns about their potential misuse in generating harmful or unauthorized contents. To address these issues, several Concept Erasure methods have been proposed. However, most of them fail to achieve both robustness, i.e., the ability to robustly remove the target concept., and effectiveness, i.e., maintaining image quality. While few recent techniques successfully achieve these goals for NSFW concepts, none could handle narrow concepts such as copyrighted characters or celebrities. Erasing these narrow concepts is critical in addressing copyright and legal concerns. However, erasing them is challenging due to their close distances to non-target neighboring concepts, requiring finer-grained manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel method specifically designed to achieve both robustness and effectiveness in easing these narrow concepts. SuMa first derives a target subspace representing the concept to be erased and then neutralizes it by mapping it to a reference subspace that minimizes the distance between the two. This mapping ensures the target concept is robustly erased while preserving image quality. We conduct extensive experiments with SuMa across four tasks: subclass erasure, celebrity erasure, artistic style erasure, and instance erasure and compare the results with current state-of-the-art methods. Our method achieves image quality comparable to approaches focused on effectiveness, while also yielding results that are on par with methods targeting completeness. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹å®ƒä»¬å¯èƒ½æ»¥ç”¨ç”Ÿæˆæœ‰å®³æˆ–æœªç»æˆæƒå†…å®¹çš„å…³æ³¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå·²ç»æå‡ºäº†å‡ ç§æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¶ä¸­å¤§å¤šæ•°æ–¹æ³•æ— æ³•è¾¾åˆ°ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§çš„å¹³è¡¡ï¼Œå³åŒæ—¶æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µå¹¶ä¿æŒå›¾åƒè´¨é‡ã€‚è™½ç„¶æœ€è¿‘çš„ä¸€äº›æŠ€æœ¯æˆåŠŸåœ°å®ç°äº†å¯¹éå®‰å…¨å†…å®¹æ¦‚å¿µçš„è¿™äº›ç›®æ ‡ï¼Œä½†æ²¡æœ‰ä¸€ç§èƒ½å¤Ÿå¤„ç†ç‹­çª„çš„æ¦‚å¿µï¼Œå¦‚ç‰ˆæƒè§’è‰²æˆ–åäººã€‚æ¶ˆé™¤è¿™äº›ç‹­çª„çš„æ¦‚å¿µæ˜¯è§£å†³ç‰ˆæƒå’Œæ³•å¾‹é—®é¢˜çš„å…³é”®ã€‚ç„¶è€Œï¼Œç”±äºå®ƒä»¬ä¸ç›®æ ‡ç›¸é‚»æ¦‚å¿µçš„è¿‘è·ç¦»ï¼Œæ¶ˆé™¤å®ƒä»¬å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ›´ç²¾ç»†çš„æ“çºµã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­ç©ºé—´æ˜ å°„ï¼ˆSuMaï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä»¥è§£å†³è¿™äº›ç‹­çª„æ¦‚å¿µçš„é—®é¢˜ã€‚SuMaé¦–å…ˆæ¨å¯¼å‡ºä»£è¡¨è¦æ¶ˆé™¤æ¦‚å¿µçš„ç›®æ ‡å­ç©ºé—´ï¼Œç„¶åå°†å…¶æ˜ å°„åˆ°å‚è€ƒå­ç©ºé—´è¿›è¡Œä¸­å’Œï¼Œä»¥æœ€å°åŒ–ä¸¤è€…ä¹‹é—´çš„è·ç¦»ã€‚è¿™ç§æ˜ å°„ç¡®ä¿ç›®æ ‡æ¦‚å¿µè¢«ç¨³å¥åœ°æ¶ˆé™¤ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„SuMaå®éªŒï¼ŒåŒ…æ‹¬å­ç±»æ¶ˆé™¤ã€åäººæ¶ˆé™¤ã€è‰ºæœ¯é£æ ¼æ¶ˆé™¤å’Œå®ä¾‹æ¶ˆé™¤ï¼Œå¹¶ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ¯”è¾ƒäº†ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„å›¾åƒè´¨é‡ä¸æ³¨é‡æœ‰æ•ˆæ€§çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä¸æ³¨é‡å®Œæ•´æ€§çš„æ–¹æ³•çš„ç»“æœæŒå¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05625v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨æ»¥ç”¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆæœ‰å®³æˆ–æœªç»æˆæƒå†…å®¹çš„é£é™©ã€‚é’ˆå¯¹æ­¤ï¼Œæå‡ºäº†è‹¥å¹²æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•æ— æ³•åœ¨é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ€æ–°æŠ€æœ¯è™½æˆåŠŸå¤„ç†ä¸é€‚å½“å†…å®¹çš„æ¦‚å¿µï¼Œä½†æ— æ³•å¤„ç†å¦‚ç‰ˆæƒè§’è‰²æˆ–åäººç­‰ç»†åˆ†æ¦‚å¿µã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å­ç©ºé—´æ˜ å°„ï¼ˆSuMaï¼‰ï¼Œæ—¨åœ¨åŒæ—¶å®ç°é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä»¥æ¶ˆé™¤è¿™äº›ç»†åˆ†æ¦‚å¿µã€‚SuMaé€šè¿‡æ˜ å°„ç›®æ ‡å­ç©ºé—´å’Œå‚è€ƒå­ç©ºé—´æ¥ä¸­å’Œç›®æ ‡æ¦‚å¿µï¼Œç¡®ä¿ç¨³å¥åœ°æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå¢é•¿å¼•å‘äº†å…³äºå…¶æ½œåœ¨æ»¥ç”¨çš„å…³æ³¨ã€‚</li>
<li>å¤§å¤šæ•°æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•æ— æ³•åœ¨é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>æœ€æ–°æŠ€æœ¯æˆåŠŸå¤„ç†ä¸é€‚å½“å†…å®¹çš„æ¦‚å¿µï¼Œä½†æ— æ³•å¤„ç†ç‰ˆæƒè§’è‰²æˆ–åäººç­‰ç»†åˆ†æ¦‚å¿µã€‚</li>
<li>å­ç©ºé—´æ˜ å°„ï¼ˆSuMaï¼‰æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶å®ç°é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>SuMaé€šè¿‡æ˜ å°„ç›®æ ‡å­ç©ºé—´å’Œå‚è€ƒå­ç©ºé—´æ¥ä¸­å’Œç›®æ ‡æ¦‚å¿µã€‚</li>
<li>SuMaèƒ½ç¨³å¥åœ°æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f62fed1d7777335d798305ba9602b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a14bf5b9bc05037470371f9bd7b0769.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed3db82a2cb9bcb923e87e720df242b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b40eba6c3bb19616fac277665b6d627d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82c8b3412f99dfd320f37b77e6b94bc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05fe6a9818477428f1d507794376488b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Image-to-Brain-Signal-Generation-with-Cross-Attention-Mechanisms-for-Visual-Prostheses"><a href="#Diffusion-Based-Image-to-Brain-Signal-Generation-with-Cross-Attention-Mechanisms-for-Visual-Prostheses" class="headerlink" title="Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention   Mechanisms for Visual Prostheses"></a>Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention   Mechanisms for Visual Prostheses</h2><p><strong>Authors:Ganxi Xu, Jinyi Long, Jia Zhang</strong></p>
<p>Visual prostheses have shown great potential in restoring vision for blind individuals. However, while researchers have successfully utilized M&#x2F;EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process-converting images to M&#x2F;EEG signals in the brain encoding stage-remains largely unexplored. Thus, we present the first image-to-brain signal (M&#x2F;EEG) framework based on denoising diffusion probabilistic models enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. Furthermore, we evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Additionally, we pioneer the visualization of M&#x2F;EEG topographies across all subjects in both datasets, providing intuitive demonstrations of intra-subject and inter-subject variations in brain signals. </p>
<blockquote>
<p>è§†è§‰å‡ä½“åœ¨æ¢å¤ç›²äººçš„è§†è§‰æ–¹é¢å·²æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚è™½ç„¶ç ”ç©¶è€…å·²æˆåŠŸåˆ©ç”¨è„‘ç”µå›¾ï¼ˆM&#x2F;EEGï¼‰ä¿¡å·åœ¨è§†è§‰å‡ä½“çš„è„‘è§£ç é˜¶æ®µè¯±å‘è§†è§‰æ„ŸçŸ¥ï¼Œä½†äº’è¡¥è¿‡ç¨‹â€”â€”å°†å›¾åƒè½¬æ¢ä¸ºå¤§è„‘ç¼–ç é˜¶æ®µçš„M&#x2F;EEGä¿¡å·â€”â€”ä»å¤§å¤šæœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„å›¾åƒåˆ°è„‘ä¿¡å·ï¼ˆM&#x2F;EEGï¼‰æ¡†æ¶ï¼Œå¹¶å€ŸåŠ©è·¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº†å¢å¼ºã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šé¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨ï¼Œç”¨äºä»è¾“å…¥å›¾åƒä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºï¼›ä»¥åŠé‡‡ç”¨è·¨æ³¨æ„åŠ›å¢å¼ºçš„U-Netæ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ é€šè¿‡è¿­ä»£å»å™ªé‡å»ºç”Ÿç‰©å­¦ä¸Šåˆç†çš„è„‘ä¿¡å·ã€‚ä¸åŒäºä¾èµ–ç®€å•æ‹¼æ¥è¿›è¡Œæ¡ä»¶è®¾å®šçš„ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬çš„è·¨æ³¨æ„åŠ›æ¨¡å—å®ç°äº†è§†è§‰ç‰¹å¾å’Œè„‘ä¿¡å·è¡¨ç¤ºä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œæœ‰åŠ©äºç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç²¾ç»†å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤šæ¨¡å¼æ•°æ®é›†ï¼ˆTHINGS-EEG2å’ŒTHINGS-MEGï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œä»¥è¯æ˜å…¶åœ¨ç”Ÿæˆç”Ÿç‰©å­¦ä¸Šåˆç†çš„è„‘ä¿¡å·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜ç‡å…ˆåœ¨è¿™ä¸¤ä¸ªæ•°æ®é›†çš„æ‰€æœ‰å—è¯•è€…ä¸­å±•ç¤ºè„‘ç”µå›¾ï¼ˆM&#x2F;EEGï¼‰åœ°å½¢å›¾ï¼Œä¸ºè„‘ä¿¡å·çš„å—è¯•è€…å†…éƒ¨å’Œå—è¯•è€…ä¹‹é—´çš„å·®å¼‚æä¾›äº†ç›´è§‚æ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00787v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰å‡ä½“çš„æ½œåŠ›åŠå…¶åœ¨æ¢å¤ç›²äººè§†è§‰æ–¹é¢çš„åº”ç”¨ã€‚é’ˆå¯¹è§†è§‰å‡ä½“åœ¨å¤§è„‘è§£ç é˜¶æ®µçš„æˆåŠŸåº”ç”¨ï¼Œæå‡ºåœ¨å¤§è„‘ç¼–ç é˜¶æ®µå°†å›¾åƒè½¬æ¢ä¸ºM&#x2F;EEGä¿¡å·çš„è¿‡ç¨‹å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ä¸äº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„é¦–ä¸ªå›¾åƒåˆ°è„‘ä¿¡å·ï¼ˆM&#x2F;EEGï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’Œå¢å¼ºäº¤å‰æ³¨æ„åŠ›çš„U-Netæ‰©æ•£æ¨¡å‹ã€‚å‰è€…ä»è¾“å…¥å›¾åƒä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºï¼Œåè€…å­¦ä¹ é€šè¿‡è¿­ä»£å»å™ªé‡å»ºç”Ÿç‰©åˆç†çš„è„‘ä¿¡å·ã€‚é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¨¡å—å®ç°è§†è§‰ç‰¹å¾å’Œè„‘ä¿¡å·è¡¨ç¤ºä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œä¿ƒè¿›ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç²¾ç»†å¯¹é½ã€‚åœ¨ä¸¤ä¸ªå¤šæ¨¡å¼æ•°æ®é›†ï¼ˆTHINGS-EEG2å’ŒTHINGS-MEGï¼‰ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ç”Ÿæˆç”Ÿç‰©åˆç†è„‘ä¿¡å·çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ç‡å…ˆå¯è§†åŒ–M&#x2F;EEGåœ°å½¢å›¾ï¼Œæä¾›ç›´è§‚å±•ç¤ºä¸åŒå—è¯•è€…é—´çš„è„‘ä¿¡å·å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å‡ä½“çš„ç ”ç©¶å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¢å¤ç›²äººçš„è§†è§‰æ–¹é¢ã€‚</li>
<li>è™½ç„¶ç ”ç©¶è€…å·²ç»æˆåŠŸåˆ©ç”¨M&#x2F;EEGä¿¡å·åœ¨è§†è§‰å‡ä½“çš„è§£ç é˜¶æ®µæ¿€å‘è§†è§‰æ„ŸçŸ¥ï¼Œä½†ç¼–ç é˜¶æ®µçš„å›¾åƒåˆ°M&#x2F;EEGä¿¡å·çš„è½¬æ¢è¿‡ç¨‹å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>æå‡ºé¦–ä¸ªåŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å›¾åƒåˆ°è„‘ä¿¡å·ï¼ˆM&#x2F;EEGï¼‰æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…å«é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’Œå¢å¼ºäº¤å‰æ³¨æ„åŠ›çš„U-Netæ‰©æ•£æ¨¡å‹ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>CLIPè§†è§‰ç¼–ç å™¨å¯ä»¥ä»è¾“å…¥å›¾åƒä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>å¢å¼ºäº¤å‰æ³¨æ„åŠ›çš„U-Netæ‰©æ•£æ¨¡å‹å­¦ä¹ é€šè¿‡è¿­ä»£å»å™ªé‡å»ºç”Ÿç‰©åˆç†çš„è„‘ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-784edd5737e060035f6d90c6b2c16256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb93a3f3a912b3b1864c08b38a47e0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6deb89808ad0f2bf808e5c8c195e4593.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a1511ad046f3833a29cefa097170f98.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="4D-Visual-Pre-training-for-Robot-Learning"><a href="#4D-Visual-Pre-training-for-Robot-Learning" class="headerlink" title="4D Visual Pre-training for Robot Learning"></a>4D Visual Pre-training for Robot Learning</h2><p><strong>Authors:Chengkai Hou, Yanjie Ze, Yankai Fu, Zeyu Gao, Songbo Hu, Yue Yu, Shanghang Zhang, Huazhe Xu</strong></p>
<p>General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: <a target="_blank" rel="noopener" href="https://4d-visual-pretraining.github.io/">https://4d-visual-pretraining.github.io/</a> </p>
<blockquote>
<p>ä»ç½‘ç»œè§„æ¨¡æ•°æ®é›†å­¦ä¹ åˆ°çš„é€šç”¨è§†è§‰è¡¨ç¤ºåœ¨æœºå™¨äººé¢†åŸŸçš„æ“ä½œä»»åŠ¡ä¸Šè¿‘å¹´æ¥å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½¿å¾—æ•°æ®é«˜æ•ˆçš„æœºå™¨äººå­¦ä¹ æˆä¸ºå¯èƒ½ï¼›ç„¶è€Œï¼Œè¿™äº›é¢„è®­ç»ƒçš„è¡¨ç¤ºä¸»è¦åŸºäºäºŒç»´å›¾åƒï¼Œå¿½ç•¥äº†ä¸–ç•Œæœ¬èº«çš„ä¸‰ç»´ç‰¹æ€§ã€‚ç„¶è€Œï¼Œç”±äºå¤§è§„æ¨¡ä¸‰ç»´æ•°æ®çš„ç¨€ç¼ºï¼Œä»ç½‘ç»œæ•°æ®é›†ä¸­æå–é€šç”¨ä¸‰ç»´è¡¨ç¤ºä»ç„¶å¾ˆå›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ç§èƒ½å¤Ÿæ”¹è¿›æ‰€æœ‰ä¸‰ç»´è¡¨ç¤ºçš„é€šç”¨è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¡†æ¶ç§°ä¸ºFVPï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç°å®ä¸–ç•Œæœºå™¨äººå­¦ä¹ çš„æ–°å‹å››ç»´è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ã€‚FVPå°†è§†è§‰é¢„è®­ç»ƒç›®æ ‡è®¾å®šä¸ºä¸‹ä¸€ä¸ªç‚¹äº‘é¢„æµ‹é—®é¢˜ï¼Œå°†é¢„æµ‹æ¨¡å‹å»ºæ¨¡ä¸ºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨æ›´å¤§çš„å…¬å…±æ•°æ®é›†ä¸Šç›´æ¥è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒã€‚åœ¨åäºŒä¸ªç°å®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ä¸­ï¼ŒFVPå°†3Dæ‰©æ•£ç­–ç•¥ï¼ˆDP3ï¼‰çš„å¹³å‡æˆåŠŸç‡æé«˜äº†28%ã€‚FVPé¢„è®­ç»ƒçš„DP3åœ¨æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFVPçš„æœ‰æ•ˆæ€§é€‚ç”¨äºå„ç§ç‚¹äº‘ç¼–ç å™¨å’Œæ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬å°†FVPåº”ç”¨äºæ›´å¤§çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æœºå™¨äººæ¨¡å‹RDT-1Bï¼Œæé«˜äº†å…¶åœ¨å„ç§æœºå™¨äººä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://4d-visual-pretraining.github.io/">https://4d-visual-pretraining.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17230v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘å¹´æ¥ï¼Œä»ç½‘é¡µè§„æ¨¡æ•°æ®é›†å­¦ä¹ åˆ°çš„é€šç”¨è§†è§‰è¡¨ç¤ºåœ¨æœºå™¨äººæ‰§è¡Œä»»åŠ¡ä¸Šå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å¤§å¤šæ•°é¢„è®­ç»ƒè¡¨ç¤ºä¾§é‡äº2Då›¾åƒï¼Œå¿½ç•¥äº†ä¸–ç•Œçš„å›ºæœ‰3Dæ€§è´¨ã€‚é’ˆå¯¹ä»ç½‘é¡µæ•°æ®é›†æå–é€šç”¨3Dè¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FVPï¼ˆå››ç»´è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ï¼‰ï¼Œä½œä¸ºä¸€ç§æ”¹è¿›æ‰€æœ‰3Dè¡¨ç¤ºçš„é€šç”¨è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ã€‚FVPå°†è§†è§‰é¢„è®­ç»ƒç›®æ ‡è®¾å®šä¸ºä¸‹ä¸€ä¸ªç‚¹äº‘é¢„æµ‹é—®é¢˜ï¼Œå°†é¢„æµ‹æ¨¡å‹å»ºæ¨¡ä¸ºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨æ›´å¤§çš„å…¬å…±æ•°æ®é›†ä¸Šç›´æ¥è¿›è¡Œé¢„è®­ç»ƒã€‚åœ¨åäºŒä¸ªçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­ï¼ŒFVPæé«˜äº†3Dæ‰©æ•£ç­–ç•¥ï¼ˆDP3ï¼‰çš„å¹³å‡æˆåŠŸç‡28%ï¼Œä¸”åœ¨æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFVPåœ¨å¤šç§ç‚¹äº‘ç¼–ç å™¨å’Œæ•°æ®é›†ä¸Šçš„æ•ˆæœè‰¯å¥½ã€‚æœ€åï¼Œæˆ‘ä»¬å°†FVPåº”ç”¨äºæ›´å¤§çš„Vision-Language-Actionæœºå™¨äººæ¨¡å‹RDT-1Bï¼Œæé«˜äº†å…¶åœ¨å„ç§æœºå™¨äººä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨è§†è§‰è¡¨ç¤ºåœ¨æœºå™¨äººæ‰§è¡Œä»»åŠ¡ä¸Šå–å¾—å·¨å¤§æˆåŠŸï¼Œä½†å¤§å¤šæ•°é¢„è®­ç»ƒè¡¨ç¤ºå±€é™äº2Då›¾åƒï¼Œå¿½ç•¥äº†ä¸–ç•Œçš„3Dæ€§è´¨ã€‚</li>
<li>æå‡ºFVPæ¡†æ¶ï¼Œä¸€ä¸ªç”¨äºæ”¹è¿›æ‰€æœ‰3Dè¡¨ç¤ºçš„é€šç”¨è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>FVPå°†è§†è§‰é¢„è®­ç»ƒç›®æ ‡è®¾å®šä¸ºä¸‹ä¸€ä¸ªç‚¹äº‘é¢„æµ‹é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>FVPåœ¨åäºŒä¸ªçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­æé«˜äº†3Dæ‰©æ•£ç­–ç•¥ï¼ˆDP3ï¼‰çš„å¹³å‡æˆåŠŸç‡28%ã€‚</li>
<li>FVPåœ¨æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>FVPé€‚ç”¨äºå¤šç§ç‚¹äº‘ç¼–ç å™¨å’Œæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6d46fd332a9f71232a5d6f06739fa14f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca22dd841a9088e87e58e3a87dbc4333.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-770b4a0994b5ebd6b0587d2f7761a8ee.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible   Image Fusion"></a>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible   Image Fusion</h2><p><strong>Authors:Xiaoyang Zhang, Zhen Hua, Yakun Ju, Wei Zhou, Jun Liu, Alex C. Kot</strong></p>
<p>Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion modelâ€™s coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at <a target="_blank" rel="noopener" href="https://github.com/boshizhang123/SGDFuse">https://github.com/boshizhang123/SGDFuse</a>. </p>
<blockquote>
<p>çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆï¼ˆIVIFï¼‰æ—¨åœ¨å°†çº¢å¤–å›¾åƒä¸­çš„çƒ­è¾å°„ä¿¡æ¯ä¸å¯è§å…‰å›¾åƒä¸­çš„ä¸°å¯Œçº¹ç†ç»†èŠ‚ç›¸ç»“åˆï¼Œä»¥æé«˜ä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç”±äºç¼ºä¹åœºæ™¯çš„æ·±åº¦è¯­ä¹‰ç†è§£è€Œæ— æ³•ä¿ç•™å…³é”®ç›®æ ‡ï¼ŒåŒæ—¶èåˆè¿‡ç¨‹æœ¬èº«ä¹Ÿå¯èƒ½å¼•å…¥ä¼ªå½±å’Œç»†èŠ‚æŸå¤±ï¼Œä¸¥é‡æŸå®³å›¾åƒè´¨é‡å’Œä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SGDFuseï¼Œä¸€ä¸ªç”±Segment Anything Modelï¼ˆSAMï¼‰å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸå’Œè¯­ä¹‰æ„ŸçŸ¥çš„å›¾åƒèåˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨SAMç”Ÿæˆçš„é«˜è´¨é‡è¯­ä¹‰æ©è†œä½œä¸ºæ˜¾å¼å…ˆéªŒï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹å¼•å¯¼èåˆè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾çš„åˆæ­¥èåˆï¼Œç„¶ååˆ©ç”¨SAMçš„è¯­ä¹‰æ©è†œä¸åˆæ­¥èåˆå›¾åƒä¸€èµ·ä½œä¸ºæ¡ä»¶ï¼Œé©±åŠ¨æ‰©æ•£æ¨¡å‹çš„ä»ç²—åˆ°ç»†çš„é™å™ªç”Ÿæˆã€‚è¿™ç¡®ä¿äº†èåˆè¿‡ç¨‹ä¸ä»…å…·æœ‰æ˜ç¡®çš„è¯­ä¹‰æ–¹å‘æ€§ï¼Œè€Œä¸”è¿˜ä¿è¯äº†æœ€ç»ˆç»“æœçš„é«˜ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSGDFuseåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä»¥åŠé€‚åº”ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¸ºè§£å†³å›¾åƒèåˆçš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚SGDFuseçš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/boshizhang123/SGDFuse%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/boshizhang123/SGDFuseè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05264v2">PDF</a> Submitted to Information Fusion</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºSGDFuseæ–¹æ³•ï¼Œåˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„è¯­ä¹‰æ©è†œä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼èåˆè¿‡ç¨‹çš„ä¼˜åŒ–ï¼Œå®ç°é«˜è´¨é‡å’Œè¯­ä¹‰æ„ŸçŸ¥çš„å›¾åƒèåˆã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µæ“ä½œï¼Œåˆæ­¥èåˆå¤šæ¨¡æ€ç‰¹å¾ï¼Œç„¶åä½¿ç”¨è¯­ä¹‰æ©è†œä¸åˆæ­¥èåˆå›¾åƒä½œä¸ºæ¡ä»¶ï¼Œé©±åŠ¨æ‰©æ•£æ¨¡å‹çš„ä»ç²—åˆ°ç»†çš„é™å™ªç”Ÿæˆã€‚ç¡®ä¿èåˆè¿‡ç¨‹å…·æœ‰æ˜ç¡®çš„è¯­ä¹‰æ–¹å‘æ€§å¹¶ä¿è¯äº†æœ€ç»ˆç»“æœçš„ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¢å¤–å’Œå¯è§å…‰å›¾åƒèåˆï¼ˆIVIFï¼‰ç»“åˆäº†çº¢å¤–å›¾åƒçš„çƒ­è¾å°„ä¿¡æ¯ä¸å¯è§å›¾åƒçš„ä¸°å¯Œçº¹ç†ç»†èŠ‚ï¼Œæé«˜ä¸‹æ¸¸è§†è§‰ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹æ·±åº¦è¯­ä¹‰ç†è§£ï¼Œéš¾ä»¥ä¿ç•™å…³é”®ç›®æ ‡ï¼Œèåˆè¿‡ç¨‹å¯èƒ½å¼•å…¥ä¼ªå½±å’Œç»†èŠ‚æŸå¤±ã€‚</li>
<li>SGDFuseæ–¹æ³•åˆ©ç”¨SAMç”Ÿæˆçš„è¯­ä¹‰æ©è†œä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼èåˆè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚</li>
<li>SGDFuseé‡‡ç”¨ä¸¤é˜¶æ®µæ“ä½œï¼Œåˆæ­¥èåˆå¤šæ¨¡æ€ç‰¹å¾ï¼Œç„¶åä½¿ç”¨è¯­ä¹‰æ©è†œä¸åˆæ­¥èåˆå›¾åƒä½œä¸ºæ¡ä»¶ï¼Œé©±åŠ¨æ‰©æ•£æ¨¡å‹çš„é™å™ªç”Ÿæˆã€‚</li>
<li>SGDFuseç¡®ä¿äº†èåˆè¿‡ç¨‹çš„è¯­ä¹‰æ–¹å‘æ€§å’Œç»“æœçš„é«˜ä¿çœŸåº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜SGDFuseåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä»¥åŠä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5fd349cdacf8a9d4a6011aa42a6e4adb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5527fa59cd1bd7122186d1bd4b5893ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75a82aeaaac55c514af6a3bbe164a229.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Test-Time-Scaling-of-Diffusion-Models-via-Noise-Trajectory-Search"><a href="#Test-Time-Scaling-of-Diffusion-Models-via-Noise-Trajectory-Search" class="headerlink" title="Test-Time Scaling of Diffusion Models via Noise Trajectory Search"></a>Test-Time Scaling of Diffusion Models via Noise Trajectory Search</h2><p><strong>Authors:Vignav Ramesh, Morteza Mardani</strong></p>
<p>The iterative and stochastic nature of diffusion models enables test-time scaling, whereby spending additional compute during denoising generates higher-fidelity samples. Increasing the number of denoising steps is the primary scaling axis, but this yields quickly diminishing returns. Instead optimizing the noise trajectoryâ€“the sequence of injected noise vectorsâ€“is promising, as the specific noise realizations critically affect sample quality; but this is challenging due to a high-dimensional search space, complex noise-outcome interactions, and costly trajectory evaluations. We address this by first casting diffusion as a Markov Decision Process (MDP) with a terminal reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to be meaningful but impractical. To balance performance and efficiency, we then resort to a relaxation of MDP, where we view denoising as a sequence of independent contextual bandits. This allows us to introduce an $\epsilon$-greedy search algorithm that globally explores at extreme timesteps and locally exploits during the intermediate steps where de-mixing occurs. Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for class-conditioned&#x2F;text-to-image generation, exceeding baselines by up to $164%$ and matching&#x2F;exceeding MCTS performance. To our knowledge, this is the first practical method for test-time noise trajectory optimization of arbitrary (non-differentiable) rewards. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„è¿­ä»£æ€§å’Œéšæœºæ€§ä½¿å…¶èƒ½å¤Ÿå®ç°æµ‹è¯•æ—¶ç¼©æ”¾ï¼Œå³åœ¨å»å™ªè¿‡ç¨‹ä¸­èŠ±è´¹é¢å¤–çš„è®¡ç®—èµ„æºä¼šäº§ç”Ÿæ›´é«˜ä¿çœŸåº¦çš„æ ·æœ¬ã€‚å¢åŠ å»å™ªæ­¥éª¤çš„æ•°é‡æ˜¯ä¸»è¦ç¼©æ”¾è½´ï¼Œä½†è¿™æ ·åšäº§ç”Ÿçš„å›æŠ¥è¿…é€Ÿå‡å°‘ã€‚ä¼˜åŒ–å™ªå£°è½¨è¿¹â€”â€”æ³¨å…¥çš„å™ªå£°å‘é‡çš„åºåˆ—â€”â€”æ˜¯æœ‰å‰é€”çš„ï¼Œå› ä¸ºç‰¹å®šçš„å™ªå£°å®ç°ä¸¥é‡å½±å“æ ·æœ¬è´¨é‡ï¼›ç„¶è€Œï¼Œç”±äºé«˜ç»´æœç´¢ç©ºé—´ã€å¤æ‚çš„å™ªå£°ç»“æœäº¤äº’å’Œæ˜‚è´µçš„è½¨è¿¹è¯„ä¼°ï¼Œè¿™ä½¿å¾—é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆå°†æ‰©æ•£è½¬åŒ–ä¸ºå…·æœ‰ç»ˆç«¯å¥–åŠ±çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å±•ç¤ºæ ‘æœç´¢æ–¹æ³•ï¼ˆå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼‰è™½ç„¶æœ‰æ„ä¹‰ä½†ä¸å®ç”¨ã€‚ä¸ºäº†å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬è½¬è€Œé‡‡å–å¯¹MDPçš„æ”¾æ¾æ–¹æ³•ï¼Œå°†å»å™ªè§†ä¸ºä¸€ç³»åˆ—ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡banditã€‚è¿™å…è®¸æˆ‘ä»¬å¼•å…¥ä¸€ç§Îµè´ªå©ªæœç´¢ç®—æ³•ï¼Œåœ¨æç«¯æ—¶é—´æ­¥é•¿æ—¶è¿›è¡Œå…¨å±€æ¢ç´¢ï¼Œåœ¨å‘ç”Ÿè§£æ··çš„ä¸­é—´æ­¥éª¤ä¸­è¿›è¡Œå±€éƒ¨åˆ©ç”¨ã€‚åœ¨EDMå’ŒStable Diffusionä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨ç±»åˆ«æ¡ä»¶&#x2F;æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¶…è¿‡åŸºçº¿æœ€å¤šè¾¾164%ï¼Œå¹¶åŒ¹é…æˆ–è¶…è¿‡MCTSçš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯æµ‹è¯•æ—¶é’ˆå¯¹ä»»æ„ï¼ˆä¸å¯å¾®åˆ†ï¼‰å¥–åŠ±è¿›è¡Œå™ªå£°è½¨è¿¹ä¼˜åŒ–çš„ç¬¬ä¸€ç§å®ç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03164v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„è¿­ä»£æ€§å’Œéšæœºæ€§ä½¿å¾—æµ‹è¯•æ—¶ç¼©æ”¾æˆä¸ºå¯èƒ½ï¼Œé€šè¿‡å¢åŠ å»å™ªè¿‡ç¨‹ä¸­çš„è®¡ç®—é‡å¯ä»¥ç”Ÿæˆæ›´é«˜ä¿çœŸåº¦çš„æ ·æœ¬ã€‚æœ¬æ–‡ä¸»è¦æ¢è®¨äº†é€šè¿‡ä¼˜åŒ–å™ªå£°è½¨è¿¹æ¥æé«˜æ ·æœ¬è´¨é‡çš„æ–¹æ³•ï¼Œä½†ç”±äºæœç´¢ç©ºé—´é«˜ç»´ã€å™ªå£°ä¸ç»“æœäº’åŠ¨å¤æ‚ã€è½¨è¿¹è¯„ä¼°æˆæœ¬é«˜æ˜‚ï¼Œè¿™ä¸€ä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶å°†æ‰©æ•£è¿‡ç¨‹è½¬åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥Îµè´ªå©ªæœç´¢ç®—æ³•ï¼Œåœ¨å…¨å±€å’Œå±€éƒ¨ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç±»æ¡ä»¶&#x2F;æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œè¶…è¶ŠåŸºçº¿æœ€å¤šè¾¾164%ï¼Œå¹¶åŒ¹é…æˆ–è¶…è¶Šè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æ€§èƒ½ã€‚è¿™æ˜¯é¦–æ¬¡å®ç°æµ‹è¯•æ—¶é’ˆå¯¹ä»»æ„ï¼ˆä¸å¯å¾®ï¼‰å¥–åŠ±çš„å™ªå£°è½¨è¿¹ä¼˜åŒ–çš„å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾é€šè¿‡å¢åŠ å»å™ªæ­¥éª¤æé«˜æ ·æœ¬ä¿çœŸåº¦ï¼Œä½†æ”¶ç›Šé€’å‡ã€‚</li>
<li>å™ªå£°è½¨è¿¹ä¼˜åŒ–æ˜¯æé«˜æ ·æœ¬è´¨é‡çš„å…³é”®ï¼Œä½†é¢ä¸´é«˜ç»´æœç´¢ç©ºé—´ã€å¤æ‚äº’åŠ¨å’Œè¯„ä¼°æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>å°†æ‰©æ•£è¿‡ç¨‹è½¬åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æœ‰æ„ä¹‰ä½†ä¸å¤ªå®ç”¨ã€‚</li>
<li>é€šè¿‡å°†å»å™ªè§†ä¸ºä¸€ç³»åˆ—ç‹¬ç«‹ä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜ï¼Œå¼•å…¥Îµè´ªå©ªæœç´¢ç®—æ³•åœ¨å…¨å±€å’Œå±€éƒ¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç±»æ¡ä»¶&#x2F;æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰åŸºçº¿ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¯é¦–æ¬¡å®ç°æµ‹è¯•æ—¶é’ˆå¯¹ä»»æ„ï¼ˆä¸å¯å¾®ï¼‰å¥–åŠ±çš„å™ªå£°è½¨è¿¹ä¼˜åŒ–çš„å®ç”¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f84970cb87317dcb91710cc8c6e2c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8d8d3ff391dfadd6bdff05ed433e03e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images"><a href="#LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images" class="headerlink" title="LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images"></a>LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images</h2><p><strong>Authors:Leyang Wang, Joice Lin</strong></p>
<p>The success of modern machine learning, particularly in facial translation networks, is highly dependent on the availability of high-quality, paired, large-scale datasets. However, acquiring sufficient data is often challenging and costly. Inspired by the recent success of diffusion models in high-quality image synthesis and advancements in Large Language Models (LLMs), we propose a novel framework called LLM-assisted Paired Image Generation (LaPIG). This framework enables the construction of comprehensive, high-quality paired visible and thermal images using captions generated by LLMs. Our method encompasses three parts: visible image synthesis with ArcFace embedding, thermal image translation using Latent Diffusion Models (LDMs), and caption generation with LLMs. Our approach not only generates multi-view paired visible and thermal images to increase data diversity but also produces high-quality paired data while maintaining their identity information. We evaluate our method on public datasets by comparing it with existing methods, demonstrating the superiority of LaPIG. </p>
<blockquote>
<p>ç°ä»£æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨ç¿»è¯‘ç½‘ç»œæ–¹é¢çš„æˆåŠŸï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹ã€å¤§è§„æ¨¡æ•°æ®é›†çš„å¯è·å¾—æ€§ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿçš„æ•°æ®é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å—æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„æœ€æ–°æˆåŠŸå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨LLMç”Ÿæˆçš„æè¿°æ¥æ„å»ºå…¨é¢ã€é«˜è´¨é‡çš„é…å¯¹å¯è§å…‰å’Œçƒ­çº¢å¤–å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ArcFaceåµŒå…¥çš„å¯è§å›¾åƒåˆæˆã€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„çƒ­çº¢å¤–å›¾åƒç¿»è¯‘ï¼Œä»¥åŠä½¿ç”¨LLMçš„æè¿°ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ç”Ÿæˆå¤šè§†è§’é…å¯¹å¯è§å…‰å’Œçƒ­çº¢å¤–å›¾åƒï¼Œä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œä¸”è¿˜ç”Ÿæˆé«˜è´¨é‡é…å¯¹æ•°æ®ï¼ŒåŒæ—¶ä¿æŒå…¶èº«ä»½ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼Œè¯æ˜äº†LaPIGçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16376v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£æœºå™¨å­¦ä¹ åœ¨é¢éƒ¨ç¿»è¯‘ç½‘ç»œæ–¹é¢çš„æˆåŠŸé«˜åº¦ä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†çš„å¯è·å¾—æ€§ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿçš„æ•°æ®å¸¸å¸¸å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å—æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆå’Œè‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›å±•çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”LLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨LLMsç”Ÿæˆçš„æ ‡é¢˜ï¼Œæ„å»ºå…¨é¢ã€é«˜è´¨é‡é…å¯¹çš„å¯è§å…‰å’Œçƒ­çº¢å¤–å›¾åƒã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ArcFaceåµŒå…¥çš„å¯è§å›¾åƒåˆæˆã€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„çƒ­çº¢å¤–å›¾åƒç¿»è¯‘ï¼Œä»¥åŠä½¿ç”¨LLMsçš„æ ‡é¢˜ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä¸ä»…ç”Ÿæˆå¤šè§†è§’é…å¯¹å¯è§å…‰å’Œçƒ­çº¢å¤–å›¾åƒä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œä¸”ç”Ÿæˆé«˜è´¨é‡é…å¯¹æ•°æ®çš„åŒæ—¶ä¿æŒèº«ä»½ä¿¡æ¯ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLaPIGç›¸æ¯”ç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æœºå™¨å­¦ä¹ çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>è·å–è¶³å¤Ÿçš„æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>LaPIGæ¡†æ¶ç»“åˆäº†æ‰©æ•£æ¨¡å‹ã€LLMså’Œå›¾åƒåˆæˆæŠ€æœ¯ã€‚</li>
<li>LaPIGèƒ½å¤Ÿæ„å»ºå…¨é¢ã€é«˜è´¨é‡çš„é…å¯¹å¯è§å’Œçº¢å¤–å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬å¯è§å›¾åƒåˆæˆã€çƒ­å›¾åƒç¿»è¯‘å’Œæ ‡é¢˜ç”Ÿæˆä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ã€‚</li>
<li>LaPIGä¸ä»…æé«˜äº†æ•°æ®å¤šæ ·æ€§ï¼Œè¿˜èƒ½ç”Ÿæˆé«˜è´¨é‡ä¸”ä¿æŒèº«ä»½ä¿¡æ¯çš„é…å¯¹æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2565bef95767e7d76d749523051ee2f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31b6c38ab4f675043ab5a17779bbbca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b78173cdb01b03f3a9314f192627e972.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90401d4625728ae7acdf9a9974208e75.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TASR-Timestep-Aware-Diffusion-Model-for-Image-Super-Resolution"><a href="#TASR-Timestep-Aware-Diffusion-Model-for-Image-Super-Resolution" class="headerlink" title="TASR: Timestep-Aware Diffusion Model for Image Super-Resolution"></a>TASR: Timestep-Aware Diffusion Model for Image Super-Resolution</h2><p><strong>Authors:Qinwei Lin, Xiaopeng Sun, Yu Gao, Yujie Zhong, Dengjie Li, Zheng Zhao, Haoqian Wang</strong></p>
<p>Diffusion models have recently achieved outstanding results in the field of image super-resolution. These methods typically inject low-resolution (LR) images via ControlNet.In this paper, we first explore the temporal dynamics of information infusion through ControlNet, revealing that the input from LR images predominantly influences the initial stages of the denoising process. Leveraging this insight, we introduce a novel timestep-aware diffusion model that adaptively integrates features from both ControlNet and the pre-trained Stable Diffusion (SD). Our method enhances the transmission of LR information in the early stages of diffusion to guarantee image fidelity and stimulates the generation ability of the SD model itself more in the later stages to enhance the detail of generated images. To train this method, we propose a timestep-aware training strategy that adopts distinct losses at varying timesteps and acts on disparate modules. Experiments on benchmark datasets demonstrate the effectiveness of our method. Code: <a target="_blank" rel="noopener" href="https://github.com/SleepyLin/TASR">https://github.com/SleepyLin/TASR</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡ControlNetæ³¨å…¥ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†ControlNetä¸­ä¿¡æ¯æ³¨å…¥çš„æ—¶é—´åŠ¨æ€ï¼Œå‘ç°ä½åˆ†è¾¨ç‡å›¾åƒçš„è¾“å…¥ä¸»è¦å½±å“å»å™ªè¿‡ç¨‹çš„åˆå§‹é˜¶æ®µã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ—¶é—´æ­¥æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è‡ªé€‚åº”åœ°èåˆäº†ControlNetå’Œé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ‰©æ•£æ—©æœŸé˜¶æ®µä½åˆ†è¾¨ç‡ä¿¡æ¯çš„ä¼ è¾“ï¼Œä»¥ä¿è¯å›¾åƒçš„çœŸå®æ€§ï¼Œå¹¶åœ¨åæœŸæ›´å¤šåœ°åˆºæ¿€äº†SDæ¨¡å‹æœ¬èº«çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„ç»†èŠ‚ã€‚ä¸ºäº†è®­ç»ƒè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´æ­¥æ„ŸçŸ¥çš„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ä¸åŒçš„æ—¶é—´æ­¥é•¿é‡‡ç”¨ä¸åŒçš„æŸå¤±ï¼Œå¹¶å¯¹ä¸åŒçš„æ¨¡å—èµ·ä½œç”¨ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/SleepyLin/TASR">https://github.com/SleepyLin/TASR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03355v2">PDF</a> Accepted to ACM MM2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶ControlNetçš„ä¿¡æ¯æ³¨å…¥æ—¶é—´åŠ¨æ€ï¼Œå‘ç°ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥ä¸»è¦å½±å“å»å™ªè¿‡ç¨‹çš„åˆå§‹é˜¶æ®µã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ—¶é—´æ­¥é•¿æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è‡ªé€‚åº”åœ°ç»“åˆäº†ControlNetå’Œé¢„è®­ç»ƒç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨æ‰©æ•£çš„æ—©æœŸé˜¶æ®µåŠ å¼ºä½åˆ†è¾¨ç‡ä¿¡æ¯çš„ä¼ è¾“ï¼Œä¿è¯å›¾åƒä¿çœŸåº¦ï¼Œå¹¶åœ¨åæœŸæ›´å¤šåœ°æ¿€å‘SDæ¨¡å‹æœ¬èº«çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„ç»†èŠ‚ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¸¸ç”¨ControlNetæ³¨å…¥ä½åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†ControlNetçš„ä¿¡æ¯æ³¨å…¥æ—¶é—´åŠ¨æ€ï¼Œå‘ç°LRå›¾åƒè¾“å…¥å¯¹å»å™ªè¿‡ç¨‹åˆå§‹é˜¶æ®µæœ‰ä¸»è¦å½±å“ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ—¶é—´æ­¥é•¿æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è‡ªé€‚åº”ç»“åˆControlNetå’Œé¢„è®­ç»ƒç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ‰©æ•£çš„æ—©æœŸé˜¶æ®µåŠ å¼ºä½åˆ†è¾¨ç‡ä¿¡æ¯ä¼ è¾“ï¼Œä¿è¯å›¾åƒä¿çœŸåº¦ã€‚</li>
<li>åœ¨åæœŸï¼Œè¯¥æ–¹æ³•æ›´å¤šåœ°æ¿€å‘SDæ¨¡å‹æœ¬èº«çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„ç»†èŠ‚ã€‚</li>
<li>ä¸ºè®­ç»ƒæ­¤æ–¹æ³•ï¼Œæå‡ºäº†æ—¶é—´æ­¥é•¿æ„ŸçŸ¥è®­ç»ƒç­–ç•¥ï¼Œé‡‡ç”¨ä¸åŒæ—¶é—´æ­¥é•¿çš„æŸå¤±å’Œä¸åŒæ¨¡å—è¿›è¡Œæ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c9b092636db1ba58c6257e94d0f9b527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21a05d906397396f1a1defe9154575ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3288633fbc4dc8218496b2ef37d3358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bbceb3db6e19db736017e58f86aa2f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53f032eec98fef567008af75c4f4cad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42638b4bfb76443f121924c56b1a0419.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Show-o-One-Single-Transformer-to-Unify-Multimodal-Understanding-and-Generation"><a href="#Show-o-One-Single-Transformer-to-Unify-Multimodal-Understanding-and-Generation" class="headerlink" title="Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation"></a>Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation</h2><p><strong>Authors:Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou</strong></p>
<p>We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting&#x2F;extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o">https://github.com/showlab/Show-o</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ä¸€ä½“å¼è½¬æ¢å™¨ï¼Œå³Show-oï¼Œå®ƒèåˆäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸åŒäºå®Œå…¨çš„è‡ªå›å½’æ¨¡å‹ï¼ŒShow-oèåˆäº†è‡ªå›å½’å’Œï¼ˆç¦»æ•£ï¼‰æ‰©æ•£å»ºæ¨¡ï¼Œä»¥è‡ªé€‚åº”åœ°å¤„ç†å„ç§å’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚è¯¥ç»Ÿä¸€æ¨¡å‹çµæ´»æ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤&#x2F;å¤–æ¨ä»¥åŠæ··åˆæ¨¡æ€ç”Ÿæˆã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸ç°æœ‰é’ˆå¯¹ç†è§£æˆ–ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„å…·æœ‰ç›¸åŒæˆ–æ›´å¤šå‚æ•°çš„æ¨¡å‹ç›¸æ¯”è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚è¿™å……åˆ†å±•ç¤ºäº†å…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o%E3%80%82">https://github.com/showlab/Show-oã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12528v7">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè½¬æ¢å™¨â€”â€”Show-oã€‚å®ƒç»“åˆäº†è‡ªå›å½’å’Œç¦»æ•£æ‰©æ•£å»ºæ¨¡æŠ€æœ¯ï¼Œå¯çµæ´»å¤„ç†å„ç§æ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºï¼Œæ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤&#x2F;å¤–æ¨å’Œæ··åˆæ¨¡æ€ç”Ÿæˆç­‰ã€‚Show-oçš„æ€§èƒ½ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ï¼Œæ˜¾ç¤ºå…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Show-oæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè½¬æ¢å™¨ã€‚</li>
<li>å®ƒç»“åˆäº†è‡ªå›å½’å’Œç¦»æ•£æ‰©æ•£å»ºæ¨¡æŠ€æœ¯ã€‚</li>
<li>Show-oå¯çµæ´»å¤„ç†å„ç§æ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚</li>
<li>æ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰ã€‚</li>
<li>Show-oçš„æ€§èƒ½ä¸ç°æœ‰æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>Show-oæœ‰æ½œåŠ›æˆä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3110729eb89bef38c319823d70524616.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-421d31a20a83055ad534c795d6a88560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d099062b50d06e74e00215e7fb957b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2181d28072b89b4c778bf648205a094.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de4097afa976540a8634e2399e8e9880.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c80b9d85122e3b2777c2599cb8075d9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ManiCM-Real-time-3D-Diffusion-Policy-via-Consistency-Model-for-Robotic-Manipulation"><a href="#ManiCM-Real-time-3D-Diffusion-Policy-via-Consistency-Model-for-Robotic-Manipulation" class="headerlink" title="ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic   Manipulation"></a>ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic   Manipulation</h2><p><strong>Authors:Guanxing Lu, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, Wenbo Ding, Yansong Tang</strong></p>
<p>Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories. Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations. To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference. Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory. To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold. We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»è¢«éªŒè¯å¯ä»¥ä»è‡ªç„¶å›¾åƒåˆ°è¿åŠ¨è½¨è¿¹ç”Ÿæˆå¤æ‚çš„åˆ†å¸ƒã€‚æœ€è¿‘çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨3Dæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç”±äºå¤šæ¬¡å»å™ªæ­¥éª¤ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´è§‚å¯Ÿä¸‹ï¼Œå®ƒä»¬é­å—ä¸¥é‡çš„è¿è¡Œæ—¶é—´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®æ—¶æœºå™¨äººæ“ä½œæ¨¡å‹ManiCMï¼Œå¯¹æ‰©æ•£è¿‡ç¨‹æ–½åŠ ä¸€è‡´æ€§çº¦æŸï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸€æ¬¡æ¨ç†ä¸­ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æœºå™¨äººåŠ¨ä½œç©ºé—´ä¸­åˆ¶å®šäº†åŸºäºç‚¹äº‘è¾“å…¥çš„è¿ç»­æ‰©æ•£è¿‡ç¨‹ï¼Œè¦æ±‚ä»ODEè½¨è¿¹çš„ä»»ä½•ä¸€ç‚¹ç›´æ¥å¯¹åŸå§‹åŠ¨ä½œè¿›è¡Œå»å™ªã€‚ä¸ºäº†æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç›´æ¥é¢„æµ‹åŠ¨ä½œæ ·æœ¬ï¼Œè€Œä¸æ˜¯åœ¨è§†è§‰ç¤¾åŒºå†…é¢„æµ‹å™ªå£°ï¼Œä»¥åœ¨ä½ç»´åŠ¨ä½œæµå½¢ä¸­å®ç°å¿«é€Ÿæ”¶æ•›ã€‚æˆ‘ä»¬åœ¨Adroitå’ŒMetaworldçš„31ä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°äº†ManiCMï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æ¨ç†é€Ÿåº¦æé«˜äº†10å€ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å¹³å‡æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01586v3">PDF</a> <a target="_blank" rel="noopener" href="https://manicm-fast.github.io/">https://manicm-fast.github.io/</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒç”Ÿæˆå’Œè¿åŠ¨è½¨è¿¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚é’ˆå¯¹å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººæ“æ§ä»»åŠ¡ä¸­å­˜åœ¨çš„é«˜ç»´è§‚å¯Ÿå’Œå¤šæ¬¡å»å™ªå¯¼è‡´çš„è¿è¡Œæ—¶æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºManiCMçš„å®æ—¶æœºå™¨äººæ“æ§æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¯¹æ‰©æ•£è¿‡ç¨‹æ–½åŠ ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°äº†ä¸€æ­¥æ¨æ–­æœºå™¨äººåŠ¨ä½œã€‚é€šè¿‡åœ¨æœºå™¨äººåŠ¨ä½œç©ºé—´å†…æ„å»ºä¸€è‡´æ€§æ‰©æ•£è¿‡ç¨‹å¹¶è®¾è®¡ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹å¯ç›´æ¥ä»ODEè½¨è¿¹çš„ä»»ä½•ç‚¹å»å™ªåŸå§‹åŠ¨ä½œæ ·æœ¬ï¼Œä»è€Œæé«˜äº†æ”¶æ•›é€Ÿåº¦å¹¶ä¿æŒäº†åŠ¨ä½œæµå½¢çš„ä½ç»´åº¦ã€‚åœ¨Adroitå’ŒMetaworldçš„31ä¸ªæœºå™¨äººæ“æ§ä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡æ¨ç†é€Ÿåº¦ä¸ŠåŠ å¿«äº†10å€ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å¹³å‡æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²åœ¨å¤šä¸ªé¢†åŸŸè¯æ˜å…¶ç”Ÿæˆå¤æ‚åˆ†å¸ƒçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººæ“æ§ä»»åŠ¡ä¸­å­˜åœ¨è¿è¡Œæ—¶æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºManiCMçš„å®æ—¶æœºå™¨äººæ“æ§æ¨¡å‹ï¼Œé€šè¿‡æ–½åŠ ä¸€è‡´æ€§çº¦æŸå®ç°äº†ä¸€æ­¥æ¨æ–­æœºå™¨äººåŠ¨ä½œã€‚</li>
<li>ManiCMæ¨¡å‹åœ¨æœºå™¨äººåŠ¨ä½œç©ºé—´å†…æ„å»ºä¸€è‡´æ€§æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡äº†ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç›´æ¥é¢„æµ‹åŠ¨ä½œæ ·æœ¬ä»¥æé«˜æ”¶æ•›é€Ÿåº¦å¹¶ä¿æŒåŠ¨ä½œæµå½¢çš„ä½ç»´åº¦ã€‚</li>
<li>åœ¨å¤šä¸ªæœºå™¨äººæ“æ§ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒManiCMæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å¹¶ä¿æŒäº†é«˜æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cc86fcb8adbddf641dca575cc190eaa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37674fbe665b4faf2b50bd1ef28ed39e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91f2b82ae3b2be266834842e23583ac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-001f40574df3fbbfb733d1bad84cbcde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbec512fe62d3a5932d3c9a462daa1e6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e38fbdca613045530f6170c1981968cb.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in   Panoramic Radiographs using Federated, Centralized and Local Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5458ca39a46349987e7b12316bf2a67d.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Real-time Photorealistic Mapping for Situational Awareness in Robot   Teleoperation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
