<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-10  BIR-Adapter A Low-Complexity Diffusion Model Adapter for Blind Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b40eba6c3bb19616fac277665b6d627d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-10-更新"><a href="#2025-09-10-更新" class="headerlink" title="2025-09-10 更新"></a>2025-09-10 更新</h1><h2 id="BIR-Adapter-A-Low-Complexity-Diffusion-Model-Adapter-for-Blind-Image-Restoration"><a href="#BIR-Adapter-A-Low-Complexity-Diffusion-Model-Adapter-for-Blind-Image-Restoration" class="headerlink" title="BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image   Restoration"></a>BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image   Restoration</h2><p><strong>Authors:Cem Eteke, Alexander Griessel, Wolfgang Kellerer, Eckehard Steinbach</strong></p>
<p>This paper introduces BIR-Adapter, a low-complexity blind image restoration adapter for diffusion models. The BIR-Adapter enables the utilization of the prior of pre-trained large-scale diffusion models on blind image restoration without training any auxiliary feature extractor. We take advantage of the robustness of pretrained models. We extract features from degraded images via the model itself and extend the self-attention mechanism with these degraded features. We introduce a sampling guidance mechanism to reduce hallucinations. We perform experiments on synthetic and real-world degradations and demonstrate that BIR-Adapter achieves competitive or better performance compared to state-of-the-art methods while having significantly lower complexity. Additionally, its adapter-based design enables integration into other diffusion models, enabling broader applications in image restoration tasks. We showcase this by extending a super-resolution-only model to perform better under additional unknown degradations. </p>
<blockquote>
<p>本文介绍了 BIR-Adapter，一种用于扩散模型的低复杂度盲图像恢复适配器。BIR-Adapter 实现了利用预训练的大型扩散模型对盲图像恢复的先验信息，无需训练任何辅助特征提取器。我们充分利用了预训练模型的稳健性。我们通过模型本身从退化图像中提取特征，并利用这些退化特征扩展自注意力机制。我们引入了一种采样引导机制来减少幻觉。我们在合成和真实世界的退化上进行了实验，结果表明 BIR-Adapter 在具有显著降低复杂性的同时，与最先进的方法相比具有竞争力或更好的性能。此外，其基于适配器的设计可以集成到其他扩散模型中，从而在图像恢复任务中具有更广泛的应用。我们通过将一个仅用于超分辨率的模型扩展到在额外的未知退化下表现更好来展示这一点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06904v1">PDF</a> 20 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了BIR-Adapter，这是一种用于扩散模型的低复杂度盲图像恢复适配器。BIR-Adapter利用预训练的大型扩散模型的先验信息，无需训练任何辅助特征提取器即可进行盲图像恢复。通过利用预训练模型的稳健性，直接从退化图像中提取特征，并扩展自注意力机制以利用这些退化特征。通过引入采样指导机制减少幻象。实验表明，BIR-Adapter在合成和真实世界退化数据上表现出具有竞争力的性能或更优秀的性能，且复杂度显著降低。此外，其基于适配器的设计可与其他扩散模型集成，扩大了在图像恢复任务中的应用范围。例如，将一个仅用于超分辨率的模型扩展到在未知退化条件下表现更佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BIR-Adapter是一种用于扩散模型的盲图像恢复适配器，具有低复杂度。</li>
<li>它利用预训练的大型扩散模型的先验信息，无需训练额外的特征提取器。</li>
<li>通过利用预训练模型的稳健性，直接从退化图像中提取特征。</li>
<li>扩展了自注意力机制以利用退化特征，并引入采样指导机制减少幻象。</li>
<li>实验表明，BIR-Adapter在合成和真实退化数据上表现出竞争力，复杂度更低。</li>
<li>BIR-Adapter基于适配器的设计可与其他扩散模型集成，提高图像恢复任务的适用范围。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06904">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-74f1665dbe3b8843129644978711504d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8aaea6c69a195f13b9f38d66d842060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c17c48161dbe41fe29c63f667f38cdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68c569feae1e851eb0ef88affc1c3a0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35920db4b21ecff75b56bb5d839b2302.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward"><a href="#UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward" class="headerlink" title="UMO: Scaling Multi-Identity Consistency for Image Customization via   Matching Reward"></a>UMO: Scaling Multi-Identity Consistency for Image Customization via   Matching Reward</h2><p><strong>Authors:Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He</strong></p>
<p>Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With “multi-to-multi matching” paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: <a target="_blank" rel="noopener" href="https://github.com/bytedance/UMO">https://github.com/bytedance/UMO</a> </p>
<blockquote>
<p>近期图像定制技术的进展展现出广阔的应用前景，因其更强大的定制能力。然而，由于人类对人脸更为敏感，因此在多参考图像中保持身份一致并避免身份混淆仍是一项重大挑战，这限制了定制模型的身份可扩展性。为解决此问题，我们提出了UMO（Unified Multi-identity Optimization）框架，旨在保持高保真身份保留，并通过可扩展性缓解身份混淆。采用“多对多匹配”范式，UMO将多身份生成重新构建为全球分配优化问题，并为现有的图像定制方法一般通过扩散模型的强化学习释放多身份一致性。为帮助训练UMO，我们开发了一个可伸缩的定制数据集，包含多参考图像，既有合成部分也有真实部分。此外，我们还提出了一种新指标来衡量身份混淆。大量实验表明，UMO不仅显著提高身份一致性，而且在多种图像定制方法中减少了身份混淆，在身份保留的维度上成为开源方法中的最新先进技术。代码和模型：<a target="_blank" rel="noopener" href="https://github.com/bytedance/UMO">https://github.com/bytedance/UMO</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06818v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://bytedance.github.io/UMO/">https://bytedance.github.io/UMO/</a> Code and model:   <a target="_blank" rel="noopener" href="https://github.com/bytedance/UMO">https://github.com/bytedance/UMO</a></p>
<p><strong>Summary</strong></p>
<p>近期图像定制技术取得了进展，具有广泛的应用前景，但仍面临身份一致性保护和避免身份混淆的挑战。针对此问题，本文提出了一种统一的多身份优化框架UMO，旨在通过强化学习在扩散模型上维护高保真身份并缓解身份混淆问题。UMO将多身份生成重新定义为全局分配优化问题，并引入“多对多匹配”范式来提高现有图像定制方法的身份一致性。此外，本研究还建立了一个用于UMO训练的可扩展定制化数据集，并提出了一个全新的身份混淆度量标准。实验证明，UMO不仅在提高身份一致性方面表现卓越，而且在多种图像定制方法中减少了身份混淆，成为开源方法中身份保留维度的最新最佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像定制技术的发展带来了广泛的应用前景，尤其在更强的定制化能力方面。</li>
<li>身份一致性的保护和避免身份混淆是当前面临的主要挑战。</li>
<li>UMO框架旨在通过强化学习在扩散模型上维护高保真身份并缓解身份混淆问题。</li>
<li>UMO采用“多对多匹配”范式，将多身份生成重新定义为全局分配优化问题。</li>
<li>UMO的开发依赖于一个用于训练的可扩展定制化数据集，该数据集包含合成和真实图像。</li>
<li>研究人员提出了一种新的度量标准来衡量身份混淆。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06818">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-feceb6b01b48a180df58ebb0a7d95289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4483a7da13b9614694a1134a976db7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-972ae78ca33fe0a1b7cba4475d0c8770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5df18d21247ba96a400ecffb55d6638d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2cb1b950af1222fa36895c8b0dba4e8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-Rigging-to-Waving-3D-Guided-Diffusion-for-Natural-Animation-of-Hand-Drawn-Characters"><a href="#From-Rigging-to-Waving-3D-Guided-Diffusion-for-Natural-Animation-of-Hand-Drawn-Characters" class="headerlink" title="From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of   Hand-Drawn Characters"></a>From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of   Hand-Drawn Characters</h2><p><strong>Authors:Jie Zhou, Linzi Qu, Miu-Ling Lam, Hongbo Fu</strong></p>
<p>Hand-drawn character animation is a vibrant field in computer graphics, presenting challenges in achieving geometric consistency while conveying expressive motion. Traditional skeletal animation methods maintain geometric consistency but struggle with complex non-rigid elements like flowing hair and skirts, leading to unnatural deformation. Conversely, video diffusion models synthesize realistic dynamics but often create geometric distortions in stylized drawings due to domain gaps. This work proposes a hybrid animation system that combines skeletal animation and video diffusion. Initially, coarse images are generated from characters retargeted with skeletal animations for geometric guidance. These images are then enhanced in texture and secondary dynamics using video diffusion priors, framing this enhancement as an inpainting task. A domain-adapted diffusion model refines user-masked regions needing improvement, especially for secondary dynamics. To enhance motion realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in the denoising process, incorporating features from a pre-trained diffusion model enriched with human motion priors. Additionally, to tackle unnatural deformations from low-poly single-mesh character modeling, we present a Hair Layering Modeling (HLM) technique that uses segmentation maps to separate hair from the body, allowing for more natural animation of long-haired characters. Extensive experiments show that our system outperforms state-of-the-art methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>手绘字符动画是计算机图形学中的一个充满生机的领域，它在实现几何一致性的同时，还面临着表达运动的挑战。传统骨骼动画方法能够保持几何一致性，但在处理流动头发和裙子等复杂的非刚性元素时却感到困难，导致变形不自然。相反，视频扩散模型能够合成逼真的动态图像，但由于领域差异，往往在风格化的绘画中产生几何失真。本文提出了一种结合骨骼动画和视频扩散的混合动画系统。首先，利用骨骼动画对角色进行重定向，生成粗略图像，以提供几何指导。然后，利用视频扩散先验增强这些图像的纹理和次要动力学，将这一增强过程视为一种填充任务。域适应扩散模型会精细化用户掩罩的区域，尤其是需要改进的次要动力学区域。为了进一步提高运动逼真度，我们在去噪过程中引入了一种次要动力学注入（SDI）策略，该策略结合了预训练扩散模型的人类运动先验特征。此外，为了解决低多边形单网格角色建模带来的不自然变形问题，我们提出了一种头发分层建模（HLM）技术，利用分割图将头发与身体分开，从而实现长发角色的更自然动画。大量实验表明，我们的系统在定量和定性评估上均优于最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06573v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该文介绍了一种结合骨骼动画和视频扩散模型的混合动画系统。该系统首先通过骨骼动画对角色进行几何指导生成粗图像，然后利用视频扩散先验增强纹理和次要动力学，将其视为修复任务。通过域适应扩散模型对用户需要改进的区域进行细化处理，尤其是次要动力学。为了进一步提高运动真实感，我们在去噪过程中引入了次要动力学注入（SDI）策略，利用预训练扩散模型融入人类运动先验的特征。此外，为了解决低多边形单网格角色建模带来的不自然变形问题，提出了一种头发分层建模（HLM）技术，利用分割图将头发与身体分离，使长发角色的动画更加自然。实验表明，该系统在定量和定性评估方面均优于现有先进技术。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>混合动画系统结合了骨骼动画和视频扩散模型，以实现几何一致性和表达性运动。</li>
<li>系统通过骨骼动画生成角色的粗图像，为动画提供几何指导。</li>
<li>利用视频扩散先验增强图像的纹理和次要动力学，将其表述为修复任务。</li>
<li>域适应扩散模型用于细化用户需要改进的区域，尤其是次要动力学方面的改进。</li>
<li>引入次要动力学注入（SDI）策略，提高运动真实感，结合预训练扩散模型和人类运动先验。</li>
<li>提出头发分层建模（HLM）技术，利用分割图改善长发角色的动画自然度。</li>
<li>实验结果显示，该系统在动画质量和现实性方面显著优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bffb59928915b425cf68f3fe3db9509a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17f59f5438eddab380b0d2dc17b1571b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107587a16dbe380904a1c59a0c18f8e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24273181fd5910854eb8de6d71d5fefd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84c300f46303d92824cd119120edf37c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c5aa64c978d03e50496cf04627c814.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9f0d355750a634b0f0d64b456bae5f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-363801aa73efa8e2d7f34ec5504bd550.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement"><a href="#TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement" class="headerlink" title="TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement"></a>TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement</h2><p><strong>Authors:Jibai Lin, Bo Ma, Yating Yang, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang, Xi Zhou</strong></p>
<p>Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired “winning” (balanced preservation-compliance) and “losing” (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDE’s superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDE’s versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE">https://github.com/KomJay520/TIDE</a>. </p>
<blockquote>
<p>主题驱动图像生成（SDIG）旨在根据文本指令操作图像中的特定主题，这对于推进文本到图像的扩散模型至关重要。SDIG需要在保持主题身份和遵守动态编辑指令之间找到平衡，现有方法对此挑战解决不足。在本文中，我们引入了目标指令扩散增强（TIDE）框架，通过目标监督和偏好学习解决这一平衡问题，而无需进行测试时微调。TIDE首创目标监督三元组对齐，使用（参考图像、指令、目标图像）三元组对主题适应动态进行建模。该方法利用直接主题扩散（DSD）目标，用配对的“获胜”（平衡保留和合规性）和“失败”（失真）目标训练模型，这些目标通过定量指标系统生成并进行评估。这为实现最优保留合规平衡提供了隐式奖励建模。在标准基准测试上的实验结果表明，TIDE在生成忠实于主题的输出并保持指令合规方面表现出卓越性能，在多个定量指标上优于基线方法。TIDE的通用性进一步体现在其在结构条件生成、图像到图像生成和文本图像插值等多样化任务的成功应用上。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE">https://github.com/KomJay520/TIDE</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06499v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对文本指令驱动图像生成的任务，提出了一种名为TIDE（目标指导扩散增强）的框架。该框架解决了在维持主题身份与遵循动态编辑指令之间的平衡问题，通过目标监督与偏好学习，无需测试时微调即可实现。TIDE采用目标监督的三重对准技术，使用（参考图像、指令、目标图像）三重结构来模拟主题适应动态。通过采用直接主题扩散（DSD）目标，训练模型以平衡保存和合规性的“获胜”和“失败”目标，并通过定量指标进行系统的生成与评估。实验结果证明TIDE在生成忠实于主题且符合指令的输出方面表现出卓越性能，且在多种定量指标上优于基准方法。TIDE的通用性还体现在其在结构条件生成、图像到图像生成、文本图像插值等任务的成功应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIDE框架解决了在文本指令驱动图像生成中维持主题身份与遵循动态编辑指令之间的平衡问题。</li>
<li>TIDE采用目标监督的三重对准技术，实现主题适应动态的建模。</li>
<li>通过采用直接主题扩散（DSD）目标，训练模型以平衡保存和合规性。</li>
<li>TIDE通过系统的生成与评估，采用定量指标来衡量模型性能。</li>
<li>实验结果证明TIDE在生成忠实于主题且符合指令的图像方面表现出卓越性能。</li>
<li>TIDE在多种定量指标上优于其他基准方法，显示出其优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e488aa9ca74eb9f9f7d15570f8eda5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98381ae38909c2778d2e7c6f1b553a68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acb18ee69b3d68a29d12abbf27c31b8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb794b77b221060b1ddf32f16be8145c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1650c731f8b7315f3f6a588f949be7c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VQualA-2025-Challenge-on-Image-Super-Resolution-Generated-Content-Quality-Assessment-Methods-and-Results"><a href="#VQualA-2025-Challenge-on-Image-Super-Resolution-Generated-Content-Quality-Assessment-Methods-and-Results" class="headerlink" title="VQualA 2025 Challenge on Image Super-Resolution Generated Content   Quality Assessment: Methods and Results"></a>VQualA 2025 Challenge on Image Super-Resolution Generated Content   Quality Assessment: Methods and Results</h2><p><strong>Authors:Yixiao Li, Xin Li, Chris Wei Zhou, Shuo Xing, Hadi Amirpour, Xiaoshuai Hao, Guanghui Yue, Baoquan Zhao, Weide Liu, Xiaoyuan Yang, Zhengzhong Tu, Xinyu Li, Chuanbiao Song, Chenqi Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Xiaoyan Sun, Shishun Tian, Dongyang Yan, Weixia Zhang, Junlin Chen, Wei Sun, Zhihua Wang, Zhuohang Shi, Zhizun Luo, Hang Ouyang, Tianxin Xiao, Fan Yang, Zhaowang Wu, Kaixin Deng</strong></p>
<p>This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lighting-YXLI/ISRGen-QA">https://github.com/Lighting-YXLI/ISRGen-QA</a>. </p>
<blockquote>
<p>本文介绍了基于图像超分辨率生成内容质量评估（ISRGen-QA）数据集构建的ISRGC-Q挑战，该挑战作为ICCV 2025研讨会视觉质量评估（VQualA）竞赛的一部分而组织。与现有的超分辨率图像质量评估（SR-IQA）数据集不同，ISRGen-QA更侧重于由最新生成方法（包括生成对抗网络（GANs）和扩散模型）生成的SR图像。此挑战的主要目标是分析现代超分辨率技术引入的独特特征，并有效地评估其感知质量。共有108名参与者注册参加此次挑战，其中4支队伍为最终测试阶段提交了有效解决方案和情况介绍表。这些提交在ISRGen-QA数据集上展现了最新前沿的性能。该项目可在<a target="_blank" rel="noopener" href="https://github.com/Lighting-YXLI/ISRGen-QA%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Lighting-YXLI/ISRGen-QA公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06413v1">PDF</a> 11 pages, 12 figures, VQualA ICCV Workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了ISRGC-Q挑战，该挑战基于图像超分辨率生成内容质量评估（ISRGen-QA）数据集，作为ICCV 2025研讨会视觉质量评估（VQualA）竞赛的一部分。挑战的重点在于分析现代超分辨率技术引入的独特特征，并有效评估其感知质量。共有108支队伍报名参加，最终有4支队伍在测试阶段提交了有效解决方案和事实资料，展现了在ISRGen-QA数据集上的最新技术水平。项目公开可访问于：<a target="_blank" rel="noopener" href="https://github.com/Lighting-YXLI/ISRGen-QA%E3%80%82">https://github.com/Lighting-YXLI/ISRGen-QA。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ISRGC-Q挑战基于ISRGen-QA数据集构建，专注于现代生成技术（如GANs和扩散模型）生成的SR图像质量评估。</li>
<li>此挑战的主要目标是分析现代超分辨率技术的独特特征，并有效评估其感知质量。</li>
<li>一共有108支队伍注册参加了此挑战。</li>
<li>最终有4支队伍在测试阶段提交了有效解决方案和事实资料。</li>
<li>这些提交的方案在ISRGen-QA数据集上展现了最新技术水平。</li>
<li>该项目公开可访问，方便公众了解和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69db8fc6e5fba152d214ff7759154469.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f3b7549aa66d7ea8aeb4725a8f0ecd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab363c74204ebf4785ef3a1bd4267b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0f6d43d021146896a07d3cec559fa90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d6acd2eaa6b441ec4ab27fc931d8eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebafa6c9b5f1d9f017537c09f1f083ee.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Strategy-Guided-Diffusion-via-Sparse-Masking-Temporal-Reweighting-Distribution-Correction"><a href="#Multi-Strategy-Guided-Diffusion-via-Sparse-Masking-Temporal-Reweighting-Distribution-Correction" class="headerlink" title="Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting   Distribution Correction"></a>Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting   Distribution Correction</h2><p><strong>Authors:Zekun Zhou, Yanru Gong, Liu Shi, Qiegen Liu</strong></p>
<p>Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression. </p>
<blockquote>
<p>扩散模型在图像处理任务中表现出了显著的生成能力。我们提出了一种用于稀疏视图CT重建的稀疏条件时间加权积分分布估计引导扩散模型（STRIDE）。具体来说，我们设计了一种由稀疏条件概率引导的联合训练机制，以促进模型有效地学习缺失投影视图补全和全局信息建模。基于系统的理论分析，我们提出了一种随时间变化 的稀疏条件重加权指导策略，在由纯噪声逐渐变为真实图像的渐进去噪过程中动态调整权重，使模型能够逐渐感知稀疏视图信息。采用线性回归校正已知数据和生成数据之间的分布偏移，减轻指导过程中产生的不一致性。此外，我们构建了一个双网络并行架构，以在多个子频率分量上执行全局校正和优化，从而有效地提高了模型在细节恢复和结构保持方面的能力，最终实现了高质量图像重建。在公共和真实数据集上的实验结果表表明，与表现最佳的基线方法相比，所提出的方法在PSNR上提高了2.58 dB，在SSIM上提高了2.37%，在MSE上降低了0.236。重建的图像在结构一致性、细节恢复和伪影抑制方面表现出良好的泛化能力和鲁棒性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05992v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在图像处理任务中展现出惊人的生成能力。本文提出一种用于稀疏视图CT重建的稀疏条件时间加权积分分布估计引导扩散模型（STRIDE）。通过设计联合训练机制和稀疏条件概率引导，模型能更有效地学习缺失投影视图补全和全局信息建模。提出动态调整权重的时间变化稀疏条件重加权引导策略，在从纯噪声到真实图像的渐进去噪过程中逐步感知稀疏视图信息。采用线性回归校正已知和生成数据之间的分布偏移，减轻指导过程中的不一致性。构建双网络并行架构，进行跨多个子频率组件的全局校正和优化，有效提高模型在细节恢复和结构保持方面的能力，实现高质量图像重建。实验结果表明，该方法在公开和真实数据集上均取得最佳效果，与最佳基线方法相比，PSNR提高2.58 dB，SSIM提高2.37%，MSE降低0.236。重建的图像在结构一致性、细节恢复和伪影抑制方面表现出优异的泛化和鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像处理中表现出强大的生成能力。</li>
<li>提出了一种名为STRIDE的稀疏条件时间加权集成分布估计引导扩散模型，用于稀疏视图CT重建。</li>
<li>通过联合训练机制和稀疏条件概率引导设计，模型能学习缺失投影视图补全和全局信息建模。</li>
<li>提出时间变化的稀疏条件重加权策略，以在去噪过程中逐步感知稀疏视图信息。</li>
<li>采用线性回归校正数据分布偏移，减少指导过程中的不一致性。</li>
<li>构建双网络并行架构，提高模型在细节恢复和结构保持方面的能力。</li>
<li>实验结果表明，该方法在图像重建任务上取得了显著成果，包括PSNR、SSIM和MSE等指标的改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05992">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2eb9393ad650011c0e2761f7b460752c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab165bb069aecbff4bd7b67289198e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bc9bc101b1141f2f23bc51e672363d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61369320df9cda5ce266b5ab1687fe9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6375f0b02934465ae1dbd273aa601f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance"><a href="#Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance" class="headerlink" title="Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance"></a>Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance</h2><p><strong>Authors:Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel</strong></p>
<p>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brain’s three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimer’s disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging. </p>
<blockquote>
<p>视觉语言模型在各种条件下生成2D图像的能力令人印象深刻。然而，这些模型在2D方面的出色表现很大程度上是由于丰富且易于获取的预训练基础模型的支撑。关键的是，3D领域的预训练基础模型还不存在，这极大地限制了该领域的进展。因此，视觉语言模型在仅根据自然语言描述生成高分辨率的3D反事实医学图像方面的潜力尚未得到探索。填补这一空白将能够推动强大的临床和研究应用，如个性化反事实解释、疾病进展情景模拟以及通过详细可视化假设医学状况增强医学培训。我们的工作通过引入一个框架，该框架能够根据自由形式的语言提示生成由合成患者的高分辨率3D反事实医学图像指导的医学图像，朝着解决这一挑战迈出了有意义的一步。我们采用最先进的3D扩散模型，借助Simple Diffusion的增强功能并进行扩展，并增加附加条件来改善文本对齐和图像质量。据我们所知，这是首次将语言指导的本地3D扩散模型专门应用于神经成像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。通过两个不同的神经MRI数据集的结果，我们的框架成功模拟了多发性硬化症（MS）中的不同反事实病变负荷以及阿尔茨海默病中的认知状态，在生成高质量图像的同时保持合成医学图像的主体保真度。我们的研究结果奠定了基于提示的疾病进展分析在3D医学成像中的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05978v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对3D扩散模型的研究进展。由于缺少适用于3D的预训练基础模型，当前视觉语言模型在生成高质量3D图像方面存在局限。研究团队引入了一种框架，利用先进的3D扩散模型并结合增强条件技术，生成由自然语言提示引导的高质量3D医学图像。该研究成功应用于神经系统成像数据，可模拟多发性硬化症和阿尔茨海默病的虚构病变，生成高质量且保留患者特性的医学图像。这标志着为语言驱动的疾病进展分析在3D医学成像领域奠定了坚实基础。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>当前视觉语言模型在生成高质量3D图像方面的局限性源于缺乏相应的预训练基础模型。</li>
<li>研究引入了首个专门针对神经系统成像数据的语言引导原生3D扩散模型框架。</li>
<li>该框架结合了增强条件技术，提高了文本对齐和图像质量。</li>
<li>成功应用于多发性硬化症和阿尔茨海默病的虚构病变模拟，生成高质量医学图像。</li>
<li>该研究为语言驱动的疾病进展分析在3D医学成像领域提供了坚实基础。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05978">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33d85c9938d3d69c2e7985a5ead7914c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84788cd2c6aaec08fb6b9eec83a92e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea24de25c382ba42b67043adb35d3b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-897e1f7c4fd0dbd99a0e12217777eed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe4a36818c6a34cca0fd5b3099d757ce.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SuMa-A-Subspace-Mapping-Approach-for-Robust-and-Effective-Concept-Erasure-in-Text-to-Image-Diffusion-Models"><a href="#SuMa-A-Subspace-Mapping-Approach-for-Robust-and-Effective-Concept-Erasure-in-Text-to-Image-Diffusion-Models" class="headerlink" title="SuMa: A Subspace Mapping Approach for Robust and Effective Concept   Erasure in Text-to-Image Diffusion Models"></a>SuMa: A Subspace Mapping Approach for Robust and Effective Concept   Erasure in Text-to-Image Diffusion Models</h2><p><strong>Authors:Kien Nguyen, Anh Tran, Cuong Pham</strong></p>
<p>The rapid growth of text-to-image diffusion models has raised concerns about their potential misuse in generating harmful or unauthorized contents. To address these issues, several Concept Erasure methods have been proposed. However, most of them fail to achieve both robustness, i.e., the ability to robustly remove the target concept., and effectiveness, i.e., maintaining image quality. While few recent techniques successfully achieve these goals for NSFW concepts, none could handle narrow concepts such as copyrighted characters or celebrities. Erasing these narrow concepts is critical in addressing copyright and legal concerns. However, erasing them is challenging due to their close distances to non-target neighboring concepts, requiring finer-grained manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel method specifically designed to achieve both robustness and effectiveness in easing these narrow concepts. SuMa first derives a target subspace representing the concept to be erased and then neutralizes it by mapping it to a reference subspace that minimizes the distance between the two. This mapping ensures the target concept is robustly erased while preserving image quality. We conduct extensive experiments with SuMa across four tasks: subclass erasure, celebrity erasure, artistic style erasure, and instance erasure and compare the results with current state-of-the-art methods. Our method achieves image quality comparable to approaches focused on effectiveness, while also yielding results that are on par with methods targeting completeness. </p>
<blockquote>
<p>文本到图像扩散模型的快速发展引发了人们对它们可能滥用生成有害或未经授权内容的关注。为了解决这些问题，已经提出了几种概念消除方法。然而，其中大多数方法无法达到稳健性和有效性的平衡，即同时消除目标概念并保持图像质量。虽然最近的一些技术成功地实现了对非安全内容概念的这些目标，但没有一种能够处理狭窄的概念，如版权角色或名人。消除这些狭窄的概念是解决版权和法律问题的关键。然而，由于它们与目标相邻概念的近距离，消除它们具有挑战性，需要更精细的操纵。在本文中，我们引入了子空间映射（SuMa），这是一种专门设计的方法，旨在实现稳健性和有效性，以解决这些狭窄概念的问题。SuMa首先推导出代表要消除概念的目标子空间，然后将其映射到参考子空间进行中和，以最小化两者之间的距离。这种映射确保目标概念被稳健地消除，同时保持图像质量。我们在四个任务上进行了广泛的SuMa实验，包括子类消除、名人消除、艺术风格消除和实例消除，并与当前最先进的方法比较了结果。我们的方法产生的图像质量与注重有效性的方法相当，同时与注重完整性的方法的结果持平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05625v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注文本转图像扩散模型的潜在滥用问题，特别是生成有害或未经授权内容的风险。针对此，提出了若干概念消除方法，但大多数方法无法在鲁棒性和有效性之间取得平衡。最新技术虽成功处理不适当内容的概念，但无法处理如版权角色或名人等细分概念。本文介绍了一种新的方法——子空间映射（SuMa），旨在同时实现鲁棒性和有效性，以消除这些细分概念。SuMa通过映射目标子空间和参考子空间来中和目标概念，确保稳健地消除目标概念，同时保持图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转图像扩散模型的快速增长引发了关于其潜在滥用的关注。</li>
<li>大多数概念消除方法无法在鲁棒性和有效性之间取得平衡。</li>
<li>最新技术成功处理不适当内容的概念，但无法处理版权角色或名人等细分概念。</li>
<li>子空间映射（SuMa）是一种新的方法，旨在同时实现鲁棒性和有效性。</li>
<li>SuMa通过映射目标子空间和参考子空间来中和目标概念。</li>
<li>SuMa能稳健地消除目标概念，同时保持图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f62fed1d7777335d798305ba9602b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a14bf5b9bc05037470371f9bd7b0769.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed3db82a2cb9bcb923e87e720df242b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b40eba6c3bb19616fac277665b6d627d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82c8b3412f99dfd320f37b77e6b94bc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05fe6a9818477428f1d507794376488b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Image-to-Brain-Signal-Generation-with-Cross-Attention-Mechanisms-for-Visual-Prostheses"><a href="#Diffusion-Based-Image-to-Brain-Signal-Generation-with-Cross-Attention-Mechanisms-for-Visual-Prostheses" class="headerlink" title="Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention   Mechanisms for Visual Prostheses"></a>Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention   Mechanisms for Visual Prostheses</h2><p><strong>Authors:Ganxi Xu, Jinyi Long, Jia Zhang</strong></p>
<p>Visual prostheses have shown great potential in restoring vision for blind individuals. However, while researchers have successfully utilized M&#x2F;EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process-converting images to M&#x2F;EEG signals in the brain encoding stage-remains largely unexplored. Thus, we present the first image-to-brain signal (M&#x2F;EEG) framework based on denoising diffusion probabilistic models enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. Furthermore, we evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Additionally, we pioneer the visualization of M&#x2F;EEG topographies across all subjects in both datasets, providing intuitive demonstrations of intra-subject and inter-subject variations in brain signals. </p>
<blockquote>
<p>视觉假体在恢复盲人的视觉方面已显示出巨大潜力。虽然研究者已成功利用脑电图（M&#x2F;EEG）信号在视觉假体的脑解码阶段诱发视觉感知，但互补过程——将图像转换为大脑编码阶段的M&#x2F;EEG信号——仍大多未被探索。因此，我们首次提出了基于去噪扩散概率模型的图像到脑信号（M&#x2F;EEG）框架，并借助跨注意力机制进行了增强。我们的框架包含两个主要组成部分：预训练的CLIP视觉编码器，用于从输入图像中提取丰富的语义表示；以及采用跨注意力增强的U-Net扩散模型，学习通过迭代去噪重建生物学上合理的脑信号。不同于依赖简单拼接进行条件设定的传统生成模型，我们的跨注意力模块实现了视觉特征和脑信号表示之间的动态交互，有助于生成过程中的精细对齐。此外，我们在两个多模式数据集（THINGS-EEG2和THINGS-MEG）上评估了我们的框架，以证明其在生成生物学上合理的脑信号方面的有效性。我们还率先在这两个数据集的所有受试者中展示脑电图（M&#x2F;EEG）地形图，为脑信号的受试者内部和受试者之间的差异提供了直观演示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00787v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉假体的潜力及其在恢复盲人视觉方面的应用。针对视觉假体在大脑解码阶段的成功应用，提出在大脑编码阶段将图像转换为M&#x2F;EEG信号的过程尚未得到充分探索。因此，本文提出了基于去噪扩散概率模型与交叉注意力机制的首个图像到脑信号（M&#x2F;EEG）框架。该框架包含两个关键组件：预训练的CLIP视觉编码器和增强交叉注意力的U-Net扩散模型。前者从输入图像中提取丰富的语义表示，后者学习通过迭代去噪重建生物合理的脑信号。通过交叉注意力模块实现视觉特征和脑信号表示之间的动态交互，促进生成过程中的精细对齐。在两个多模式数据集（THINGS-EEG2和THINGS-MEG）上评估了该框架生成生物合理脑信号的有效性，并率先可视化M&#x2F;EEG地形图，提供直观展示不同受试者间的脑信号差异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉假体的研究具有巨大的潜力，特别是在恢复盲人的视觉方面。</li>
<li>虽然研究者已经成功利用M&#x2F;EEG信号在视觉假体的解码阶段激发视觉感知，但编码阶段的图像到M&#x2F;EEG信号的转换过程尚未得到充分研究。</li>
<li>提出首个基于去噪扩散概率模型和交叉注意力机制的图像到脑信号（M&#x2F;EEG）框架。</li>
<li>框架包含预训练的CLIP视觉编码器和增强交叉注意力的U-Net扩散模型两个关键组件。</li>
<li>CLIP视觉编码器可以从输入图像中提取丰富的语义表示。</li>
<li>增强交叉注意力的U-Net扩散模型学习通过迭代去噪重建生物合理的脑信号。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00787">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-784edd5737e060035f6d90c6b2c16256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb93a3f3a912b3b1864c08b38a47e0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6deb89808ad0f2bf808e5c8c195e4593.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a1511ad046f3833a29cefa097170f98.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="4D-Visual-Pre-training-for-Robot-Learning"><a href="#4D-Visual-Pre-training-for-Robot-Learning" class="headerlink" title="4D Visual Pre-training for Robot Learning"></a>4D Visual Pre-training for Robot Learning</h2><p><strong>Authors:Chengkai Hou, Yanjie Ze, Yankai Fu, Zeyu Gao, Songbo Hu, Yue Yu, Shanghang Zhang, Huazhe Xu</strong></p>
<p>General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: <a target="_blank" rel="noopener" href="https://4d-visual-pretraining.github.io/">https://4d-visual-pretraining.github.io/</a> </p>
<blockquote>
<p>从网络规模数据集学习到的通用视觉表示在机器人领域的操作任务上近年来取得了巨大的成功，使得数据高效的机器人学习成为可能；然而，这些预训练的表示主要基于二维图像，忽略了世界本身的三维特性。然而，由于大规模三维数据的稀缺，从网络数据集中提取通用三维表示仍然很困难。因此，我们正在寻找一种能够改进所有三维表示的通用视觉预训练框架作为替代方案。我们的框架称为FVP，这是一个用于现实世界机器人学习的新型四维视觉预训练框架。FVP将视觉预训练目标设定为下一个点云预测问题，将预测模型建模为扩散模型，并在更大的公共数据集上直接进行模型预训练。在十二个现实世界的操作任务中，FVP将3D扩散策略（DP3）的平均成功率提高了28%。FVP预训练的DP3在模仿学习方法中达到了最先进的性能。此外，FVP的有效性适用于各种点云编码器和数据集。最后，我们将FVP应用于更大的视觉语言行动机器人模型RDT-1B，提高了其在各种机器人任务上的性能。我们的项目页面可访问于：<a target="_blank" rel="noopener" href="https://4d-visual-pretraining.github.io/">https://4d-visual-pretraining.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17230v2">PDF</a> </p>
<p><strong>Summary</strong><br>     近年来，从网页规模数据集学习到的通用视觉表示在机器人执行任务上取得了巨大成功，但大多数预训练表示侧重于2D图像，忽略了世界的固有3D性质。针对从网页数据集提取通用3D表示的挑战，我们提出了FVP（四维视觉预训练框架），作为一种改进所有3D表示的通用视觉预训练框架。FVP将视觉预训练目标设定为下一个点云预测问题，将预测模型建模为扩散模型，并在更大的公共数据集上直接进行预训练。在十二个真实世界操作任务中，FVP提高了3D扩散策略（DP3）的平均成功率28%，且在模仿学习方法中达到最佳性能。此外，FVP在多种点云编码器和数据集上的效果良好。最后，我们将FVP应用于更大的Vision-Language-Action机器人模型RDT-1B，提高了其在各种机器人任务上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通用视觉表示在机器人执行任务上取得巨大成功，但大多数预训练表示局限于2D图像，忽略了世界的3D性质。</li>
<li>提出FVP框架，一个用于改进所有3D表示的通用视觉预训练框架。</li>
<li>FVP将视觉预训练目标设定为下一个点云预测问题，并采用扩散模型进行预测。</li>
<li>FVP在十二个真实世界操作任务中提高了3D扩散策略（DP3）的平均成功率28%。</li>
<li>FVP在模仿学习方法中达到最佳性能。</li>
<li>FVP适用于多种点云编码器和数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17230">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6d46fd332a9f71232a5d6f06739fa14f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca22dd841a9088e87e58e3a87dbc4333.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-770b4a0994b5ebd6b0587d2f7761a8ee.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible   Image Fusion"></a>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible   Image Fusion</h2><p><strong>Authors:Xiaoyang Zhang, Zhen Hua, Yakun Ju, Wei Zhou, Jun Liu, Alex C. Kot</strong></p>
<p>Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model’s coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at <a target="_blank" rel="noopener" href="https://github.com/boshizhang123/SGDFuse">https://github.com/boshizhang123/SGDFuse</a>. </p>
<blockquote>
<p>红外与可见光图像融合（IVIF）旨在将红外图像中的热辐射信息与可见光图像中的丰富纹理细节相结合，以提高下游视觉任务的感知能力。然而，现有方法往往由于缺乏场景的深度语义理解而无法保留关键目标，同时融合过程本身也可能引入伪影和细节损失，严重损害图像质量和任务性能。为了解决这些问题，本文提出了SGDFuse，一个由Segment Anything Model（SAM）引导的条件扩散模型，实现高保真和语义感知的图像融合。我们的方法的核心是利用SAM生成的高质量语义掩膜作为显式先验，通过条件扩散模型引导融合过程的优化。具体来说，该框架分为两个阶段：首先进行多模态特征的初步融合，然后利用SAM的语义掩膜与初步融合图像一起作为条件，驱动扩散模型的从粗到细的降噪生成。这确保了融合过程不仅具有明确的语义方向性，而且还保证了最终结果的高保真度。大量实验表明，SGDFuse在主观和客观评估以及适应下游任务方面均达到最佳性能，为解决图像融合的核心挑战提供了强大的解决方案。SGDFuse的代码可在[<a target="_blank" rel="noopener" href="https://github.com/boshizhang123/SGDFuse%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/boshizhang123/SGDFuse获取。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05264v2">PDF</a> Submitted to Information Fusion</p>
<p><strong>Summary</strong><br>     本论文提出SGDFuse方法，利用Segment Anything Model（SAM）生成的语义掩膜作为先验知识，指导融合过程的优化，实现高质量和语义感知的图像融合。该方法通过两阶段操作，初步融合多模态特征，然后使用语义掩膜与初步融合图像作为条件，驱动扩散模型的从粗到细的降噪生成。确保融合过程具有明确的语义方向性并保证了最终结果的保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>红外和可见光图像融合（IVIF）结合了红外图像的热辐射信息与可见图像的丰富纹理细节，提高下游视觉任务感知能力。</li>
<li>现有方法缺乏深度语义理解，难以保留关键目标，融合过程可能引入伪影和细节损失。</li>
<li>SGDFuse方法利用SAM生成的语义掩膜作为先验知识，指导融合过程的优化。</li>
<li>SGDFuse采用两阶段操作，初步融合多模态特征，然后使用语义掩膜与初步融合图像作为条件，驱动扩散模型的降噪生成。</li>
<li>SGDFuse确保了融合过程的语义方向性和结果的高保真度。</li>
<li>实验表明SGDFuse在主观和客观评估以及下游任务适应性方面达到最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5fd349cdacf8a9d4a6011aa42a6e4adb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5527fa59cd1bd7122186d1bd4b5893ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75a82aeaaac55c514af6a3bbe164a229.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Test-Time-Scaling-of-Diffusion-Models-via-Noise-Trajectory-Search"><a href="#Test-Time-Scaling-of-Diffusion-Models-via-Noise-Trajectory-Search" class="headerlink" title="Test-Time Scaling of Diffusion Models via Noise Trajectory Search"></a>Test-Time Scaling of Diffusion Models via Noise Trajectory Search</h2><p><strong>Authors:Vignav Ramesh, Morteza Mardani</strong></p>
<p>The iterative and stochastic nature of diffusion models enables test-time scaling, whereby spending additional compute during denoising generates higher-fidelity samples. Increasing the number of denoising steps is the primary scaling axis, but this yields quickly diminishing returns. Instead optimizing the noise trajectory–the sequence of injected noise vectors–is promising, as the specific noise realizations critically affect sample quality; but this is challenging due to a high-dimensional search space, complex noise-outcome interactions, and costly trajectory evaluations. We address this by first casting diffusion as a Markov Decision Process (MDP) with a terminal reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to be meaningful but impractical. To balance performance and efficiency, we then resort to a relaxation of MDP, where we view denoising as a sequence of independent contextual bandits. This allows us to introduce an $\epsilon$-greedy search algorithm that globally explores at extreme timesteps and locally exploits during the intermediate steps where de-mixing occurs. Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for class-conditioned&#x2F;text-to-image generation, exceeding baselines by up to $164%$ and matching&#x2F;exceeding MCTS performance. To our knowledge, this is the first practical method for test-time noise trajectory optimization of arbitrary (non-differentiable) rewards. </p>
<blockquote>
<p>扩散模型的迭代性和随机性使其能够实现测试时缩放，即在去噪过程中花费额外的计算资源会产生更高保真度的样本。增加去噪步骤的数量是主要缩放轴，但这样做产生的回报迅速减少。优化噪声轨迹——注入的噪声向量的序列——是有前途的，因为特定的噪声实现严重影响样本质量；然而，由于高维搜索空间、复杂的噪声结果交互和昂贵的轨迹评估，这使得面临挑战。我们通过首先将扩散转化为具有终端奖励的马尔可夫决策过程（MDP）来解决这个问题，并展示树搜索方法（如蒙特卡洛树搜索（MCTS））虽然有意义但不实用。为了平衡性能和效率，我们转而采取对MDP的放松方法，将去噪视为一系列独立的上下文bandit。这允许我们引入一种ε贪婪搜索算法，在极端时间步长时进行全局探索，在发生解混的中间步骤中进行局部利用。在EDM和Stable Diffusion上的实验显示，在类别条件&#x2F;文本到图像生成方面达到了最新水平，超过基线最多达164%，并匹配或超过MCTS的性能。据我们所知，这是测试时针对任意（不可微分）奖励进行噪声轨迹优化的第一种实用方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03164v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型的迭代性和随机性使得测试时缩放成为可能，通过增加去噪过程中的计算量可以生成更高保真度的样本。本文主要探讨了通过优化噪声轨迹来提高样本质量的方法，但由于搜索空间高维、噪声与结果互动复杂、轨迹评估成本高昂，这一任务极具挑战性。本研究将扩散过程转化为马尔可夫决策过程，并引入ε贪婪搜索算法，在全局和局部之间取得平衡，实现性能与效率的优化。实验结果显示，该方法在类条件&#x2F;文本到图像生成任务上达到领先水平，超越基线最多达164%，并匹配或超越蒙特卡洛树搜索的性能。这是首次实现测试时针对任意（不可微）奖励的噪声轨迹优化的实用方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的测试时缩放通过增加去噪步骤提高样本保真度，但收益递减。</li>
<li>噪声轨迹优化是提高样本质量的关键，但面临高维搜索空间、复杂互动和评估成本高的挑战。</li>
<li>将扩散过程转化为马尔可夫决策过程（MDP）有意义但不太实用。</li>
<li>通过将去噪视为一系列独立上下文相关的问题，引入ε贪婪搜索算法在全局和局部之间取得平衡。</li>
<li>实验结果表明，该方法在类条件&#x2F;文本到图像生成任务上表现优异，超越现有基线。</li>
<li>该方法是首次实现测试时针对任意（不可微）奖励的噪声轨迹优化的实用方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f84970cb87317dcb91710cc8c6e2c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8d8d3ff391dfadd6bdff05ed433e03e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images"><a href="#LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images" class="headerlink" title="LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images"></a>LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images</h2><p><strong>Authors:Leyang Wang, Joice Lin</strong></p>
<p>The success of modern machine learning, particularly in facial translation networks, is highly dependent on the availability of high-quality, paired, large-scale datasets. However, acquiring sufficient data is often challenging and costly. Inspired by the recent success of diffusion models in high-quality image synthesis and advancements in Large Language Models (LLMs), we propose a novel framework called LLM-assisted Paired Image Generation (LaPIG). This framework enables the construction of comprehensive, high-quality paired visible and thermal images using captions generated by LLMs. Our method encompasses three parts: visible image synthesis with ArcFace embedding, thermal image translation using Latent Diffusion Models (LDMs), and caption generation with LLMs. Our approach not only generates multi-view paired visible and thermal images to increase data diversity but also produces high-quality paired data while maintaining their identity information. We evaluate our method on public datasets by comparing it with existing methods, demonstrating the superiority of LaPIG. </p>
<blockquote>
<p>现代机器学习，特别是在面部翻译网络方面的成功，在很大程度上依赖于高质量、配对、大规模数据集的可获得性。然而，获取足够的数据通常具有挑战性和成本高昂。受扩散模型在高质量图像合成和大型语言模型（LLM）方面的最新成功启发，我们提出了一种名为LLM辅助配对图像生成（LaPIG）的新型框架。该框架能够利用LLM生成的描述来构建全面、高质量的配对可见光和热红外图像。我们的方法包括三部分：使用ArcFace嵌入的可见图像合成、使用潜在扩散模型（LDM）的热红外图像翻译，以及使用LLM的描述生成。我们的方法不仅生成多视角配对可见光和热红外图像，以增加数据多样性，而且还生成高质量配对数据，同时保持其身份信息的完整性。我们在公共数据集上评估了我们的方法，通过与现有方法进行对比，证明了LaPIG的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16376v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了现代机器学习在面部翻译网络方面的成功高度依赖于高质量、配对的大规模数据集的可获得性。然而，获取足够的数据常常具有挑战性和成本高昂。受扩散模型在高质量图像合成和自然语言模型（LLMs）进展的启发，本文提出了一种新型框架——LLM辅助配对图像生成（LaPIG）。该框架能够利用LLMs生成的标题，构建全面、高质量配对的可见光和热红外图像。该方法包括三部分：使用ArcFace嵌入的可见图像合成、使用潜在扩散模型（LDMs）的热红外图像翻译，以及使用LLMs的标题生成。该方法不仅生成多视角配对可见光和热红外图像以增加数据多样性，而且生成高质量配对数据的同时保持身份信息。在公共数据集上的评估表明，LaPIG相比现有方法具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代机器学习的成功在很大程度上依赖于高质量、配对的大规模数据集。</li>
<li>获取足够的数据具有挑战性和成本高昂。</li>
<li>LaPIG框架结合了扩散模型、LLMs和图像合成技术。</li>
<li>LaPIG能够构建全面、高质量的配对可见和红外图像。</li>
<li>该方法包括可见图像合成、热图像翻译和标题生成三个主要部分。</li>
<li>LaPIG不仅提高了数据多样性，还能生成高质量且保持身份信息的配对数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16376">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2565bef95767e7d76d749523051ee2f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31b6c38ab4f675043ab5a17779bbbca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b78173cdb01b03f3a9314f192627e972.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90401d4625728ae7acdf9a9974208e75.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TASR-Timestep-Aware-Diffusion-Model-for-Image-Super-Resolution"><a href="#TASR-Timestep-Aware-Diffusion-Model-for-Image-Super-Resolution" class="headerlink" title="TASR: Timestep-Aware Diffusion Model for Image Super-Resolution"></a>TASR: Timestep-Aware Diffusion Model for Image Super-Resolution</h2><p><strong>Authors:Qinwei Lin, Xiaopeng Sun, Yu Gao, Yujie Zhong, Dengjie Li, Zheng Zhao, Haoqian Wang</strong></p>
<p>Diffusion models have recently achieved outstanding results in the field of image super-resolution. These methods typically inject low-resolution (LR) images via ControlNet.In this paper, we first explore the temporal dynamics of information infusion through ControlNet, revealing that the input from LR images predominantly influences the initial stages of the denoising process. Leveraging this insight, we introduce a novel timestep-aware diffusion model that adaptively integrates features from both ControlNet and the pre-trained Stable Diffusion (SD). Our method enhances the transmission of LR information in the early stages of diffusion to guarantee image fidelity and stimulates the generation ability of the SD model itself more in the later stages to enhance the detail of generated images. To train this method, we propose a timestep-aware training strategy that adopts distinct losses at varying timesteps and acts on disparate modules. Experiments on benchmark datasets demonstrate the effectiveness of our method. Code: <a target="_blank" rel="noopener" href="https://github.com/SleepyLin/TASR">https://github.com/SleepyLin/TASR</a> </p>
<blockquote>
<p>扩散模型在图像超分辨率领域取得了显著成果。这些方法通常通过ControlNet注入低分辨率（LR）图像。在本文中，我们首先探索了ControlNet中信息注入的时间动态，发现低分辨率图像的输入主要影响去噪过程的初始阶段。利用这一见解，我们引入了一种新型的时间步感知扩散模型，该模型自适应地融合了ControlNet和预训练的稳定扩散（SD）的特征。我们的方法提高了扩散早期阶段低分辨率信息的传输，以保证图像的真实性，并在后期更多地刺激了SD模型本身的生成能力，提高了生成图像的细节。为了训练这种方法，我们提出了一种时间步感知的训练策略，该策略在不同的时间步长采用不同的损失，并对不同的模块起作用。在基准数据集上的实验证明了我们的方法的有效性。代码地址：<a target="_blank" rel="noopener" href="https://github.com/SleepyLin/TASR">https://github.com/SleepyLin/TASR</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03355v2">PDF</a> Accepted to ACM MM2025</p>
<p><strong>Summary</strong></p>
<p>扩散模型在图像超分辨率领域取得了显著成果。本文通过研究ControlNet的信息注入时间动态，发现低分辨率图像输入主要影响去噪过程的初始阶段。基于此，提出了一种新型的时间步长感知扩散模型，该模型自适应地结合了ControlNet和预训练稳定扩散（SD）的特征。该方法在扩散的早期阶段加强低分辨率信息的传输，保证图像保真度，并在后期更多地激发SD模型本身的生成能力，以提高生成图像的细节。<strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像超分辨率领域表现出卓越性能，常用ControlNet注入低分辨率图像。</li>
<li>本文探索了ControlNet的信息注入时间动态，发现LR图像输入对去噪过程初始阶段有主要影响。</li>
<li>引入了一种新的时间步长感知扩散模型，该模型自适应结合ControlNet和预训练稳定扩散（SD）的特征。</li>
<li>该方法在扩散的早期阶段加强低分辨率信息传输，保证图像保真度。</li>
<li>在后期，该方法更多地激发SD模型本身的生成能力，提高生成图像的细节。</li>
<li>为训练此方法，提出了时间步长感知训练策略，采用不同时间步长的损失和不同模块进行操作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03355">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c9b092636db1ba58c6257e94d0f9b527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21a05d906397396f1a1defe9154575ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3288633fbc4dc8218496b2ef37d3358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bbceb3db6e19db736017e58f86aa2f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53f032eec98fef567008af75c4f4cad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42638b4bfb76443f121924c56b1a0419.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Show-o-One-Single-Transformer-to-Unify-Multimodal-Understanding-and-Generation"><a href="#Show-o-One-Single-Transformer-to-Unify-Multimodal-Understanding-and-Generation" class="headerlink" title="Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation"></a>Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation</h2><p><strong>Authors:Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou</strong></p>
<p>We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting&#x2F;extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o">https://github.com/showlab/Show-o</a>. </p>
<blockquote>
<p>我们提出了一种统一的一体式转换器，即Show-o，它融合了多模态理解和生成。不同于完全的自回归模型，Show-o融合了自回归和（离散）扩散建模，以自适应地处理各种和混合模态的输入和输出。该统一模型灵活支持广泛的视觉语言任务，包括视觉问答、文本到图像生成、文本引导的图像修复&#x2F;外推以及混合模态生成。在各种基准测试中，其性能与现有针对理解或生成任务设计的具有相同或更多参数的模型相比表现相当或更好。这充分展示了其作为下一代基础模型的潜力。相关代码和模型已发布在<a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o%E3%80%82">https://github.com/showlab/Show-o。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12528v7">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种统一的多模态理解和生成转换器——Show-o。它结合了自回归和离散扩散建模技术，可灵活处理各种混合模态的输入和输出，支持广泛的视觉语言任务，包括视觉问答、文本到图像生成、文本引导的图像修复&#x2F;外推和混合模态生成等。Show-o的性能与现有模型相比具有竞争力，甚至在某些方面表现更优秀，显示其作为下一代基础模型的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Show-o是一个统一的多模态理解和生成转换器。</li>
<li>它结合了自回归和离散扩散建模技术。</li>
<li>Show-o可灵活处理各种混合模态的输入和输出。</li>
<li>支持广泛的视觉语言任务，包括视觉问答、文本到图像生成等。</li>
<li>Show-o的性能与现有模型相当，甚至在某些方面表现更优秀。</li>
<li>Show-o有潜力成为下一代基础模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12528">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3110729eb89bef38c319823d70524616.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-421d31a20a83055ad534c795d6a88560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d099062b50d06e74e00215e7fb957b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2181d28072b89b4c778bf648205a094.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de4097afa976540a8634e2399e8e9880.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c80b9d85122e3b2777c2599cb8075d9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ManiCM-Real-time-3D-Diffusion-Policy-via-Consistency-Model-for-Robotic-Manipulation"><a href="#ManiCM-Real-time-3D-Diffusion-Policy-via-Consistency-Model-for-Robotic-Manipulation" class="headerlink" title="ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic   Manipulation"></a>ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic   Manipulation</h2><p><strong>Authors:Guanxing Lu, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, Wenbo Ding, Yansong Tang</strong></p>
<p>Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories. Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations. To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference. Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory. To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold. We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate. </p>
<blockquote>
<p>扩散模型已经被验证可以从自然图像到运动轨迹生成复杂的分布。最近的基于扩散的方法在3D机器人操作任务中表现出令人印象深刻的性能，但由于多次去噪步骤，特别是在高维观察下，它们遭受严重的运行时间效率低下的问题。为此，我们提出了一种实时机器人操作模型ManiCM，对扩散过程施加一致性约束，使模型能够在一次推理中生成机器人动作。具体来说，我们在机器人动作空间中制定了基于点云输入的连续扩散过程，要求从ODE轨迹的任何一点直接对原始动作进行去噪。为了模拟这一过程，我们设计了一种一致性蒸馏技术，直接预测动作样本，而不是在视觉社区内预测噪声，以在低维动作流形中实现快速收敛。我们在Adroit和Metaworld的31个机器人操作任务上评估了ManiCM，结果表明我们的方法平均推理速度提高了10倍，同时保持了具有竞争力的平均成功率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01586v3">PDF</a> <a target="_blank" rel="noopener" href="https://manicm-fast.github.io/">https://manicm-fast.github.io/</a></p>
<p><strong>Summary</strong><br>扩散模型在自然图像生成和运动轨迹生成方面表现出良好的效果。针对当前扩散模型在机器人操控任务中存在的高维观察和多次去噪导致的运行时效率低下的问题，我们提出了一种名为ManiCM的实时机器人操控模型。该模型对扩散过程施加一致性约束，实现了一步推断机器人动作。通过在机器人动作空间内构建一致性扩散过程并设计一致性蒸馏技术，该模型可直接从ODE轨迹的任何点去噪原始动作样本，从而提高了收敛速度并保持了动作流形的低维度。在Adroit和Metaworld的31个机器人操控任务上的评估结果表明，我们的方法在平均推理速度上加快了10倍，同时保持了具有竞争力的平均成功率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已在多个领域证明其生成复杂分布的有效性。</li>
<li>当前扩散模型在机器人操控任务中存在运行时效率低下的问题。</li>
<li>提出了一种名为ManiCM的实时机器人操控模型，通过施加一致性约束实现了一步推断机器人动作。</li>
<li>ManiCM模型在机器人动作空间内构建一致性扩散过程。</li>
<li>设计了一致性蒸馏技术，直接预测动作样本以提高收敛速度并保持动作流形的低维度。</li>
<li>在多个机器人操控任务上的评估显示，ManiCM显著提高了推理速度并保持了高成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01586">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cc86fcb8adbddf641dca575cc190eaa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37674fbe665b4faf2b50bd1ef28ed39e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91f2b82ae3b2be266834842e23583ac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-001f40574df3fbbfb733d1bad84cbcde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbec512fe62d3a5932d3c9a462daa1e6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e38fbdca613045530f6170c1981968cb.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-09-10  Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in   Panoramic Radiographs using Federated, Centralized and Local Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5458ca39a46349987e7b12316bf2a67d.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-10  Real-time Photorealistic Mapping for Situational Awareness in Robot   Teleoperation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
