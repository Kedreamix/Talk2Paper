<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Barlow-Swin Toward a novel siamese-based segmentation architecture   using Swin-Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1cb373310dfc8352e2c29af0110b3f5c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-10-æ›´æ–°"><a href="#2025-09-10-æ›´æ–°" class="headerlink" title="2025-09-10 æ›´æ–°"></a>2025-09-10 æ›´æ–°</h1><h2 id="Barlow-Swin-Toward-a-novel-siamese-based-segmentation-architecture-using-Swin-Transformers"><a href="#Barlow-Swin-Toward-a-novel-siamese-based-segmentation-architecture-using-Swin-Transformers" class="headerlink" title="Barlow-Swin: Toward a novel siamese-based segmentation architecture   using Swin-Transformers"></a>Barlow-Swin: Toward a novel siamese-based segmentation architecture   using Swin-Transformers</h2><p><strong>Authors:Morteza Kiani Haftlang, Mohammadhossein Malmir, Foroutan Parand, Umberto Michelucci, Safouane El Ghazouali</strong></p>
<p>Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoderâ€™s ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: <a target="_blank" rel="noopener" href="https://github.com/mkianih/Barlow-Swin">https://github.com/mkianih/Barlow-Swin</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹å’Œæç»˜ç—…ç†åŒºåŸŸæ–¹é¢ã€‚è™½ç„¶U-Netç­‰å·ç§¯æ¶æ„å·²æˆä¸ºæ­¤ç±»ä»»åŠ¡çš„æ ‡é…ï¼Œä½†å…¶æœ‰é™çš„æ„Ÿå—é‡é™åˆ¶äº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚æœ€è¿‘å°†å˜å‹å™¨é›†æˆèµ·æ¥çš„åŠªåŠ›è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½†å¾€å¾€å¯¼è‡´æ·±åº¦è¿‡å¤§ã€è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ¨¡å‹ï¼Œä¸é€‚åˆå®æ—¶ä½¿ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“ä¸ºå®æ—¶äºŒè¿›åˆ¶åŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„ç«¯åˆ°ç«¯è½»é‡åŒ–æ–°å‹æ¶æ„ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†ç±»ä¼¼Swin Transformerçš„ç¼–ç å™¨å’Œç±»ä¼¼U-Netçš„è§£ç å™¨ï¼Œé€šè¿‡è·³è¿‡è·¯å¾„è¿æ¥ï¼Œä»¥ä¿ç•™ç©ºé—´ç»†èŠ‚å¹¶æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ç°æœ‰çš„Swin Transformeræˆ–U-Netè®¾è®¡ä¸åŒï¼Œæˆ‘ä»¬çš„æ¶æ„æ˜¾è‘—æ›´æµ…ä¸”æ•ˆç‡æ›´é«˜ã€‚ä¸ºäº†æ”¹å–„ç¼–ç å™¨åœ¨ä¸ä¾èµ–å¤§é‡æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ æœ‰æ„ä¹‰ç‰¹å¾çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨Barlow Twinså¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å‡å°‘å­¦ä¹ ç‰¹å¾ä¸­çš„ä¸å¿…è¦é‡å¤ï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨é‡è¦æ¨¡å¼ã€‚åœ¨æ­¤é¢„è®­ç»ƒä¹‹åï¼Œæˆ‘ä»¬å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æˆ‘ä»¬çš„ç‰¹å®šä»»åŠ¡ã€‚åœ¨åŸºå‡†äºŒè¿›åˆ¶åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‚æ•°è®¡æ•°å¤§å¹…å‡å°‘ä¸”æ¨ç†é€Ÿåº¦æ›´å¿«çš„æƒ…å†µä¸‹å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ï¼Œè¿™ä½¿å…¶æˆä¸ºåœ¨å®æ—¶å’Œèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­éƒ¨ç½²çš„å®é™…å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä»£ç å¯åœ¨Githubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mkianih/Barlow-Swin%E3%80%82">https://github.com/mkianih/Barlow-Swinã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06885v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å®æ—¶äºŒå…ƒåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹ç«¯åˆ°ç«¯è½»é‡åŒ–æ¶æ„ã€‚è¯¥æ¨¡å‹ç»“åˆSwin Transformeré£æ ¼çš„ç¼–ç å™¨å’ŒU-Neté£æ ¼çš„è§£ç å™¨ï¼Œé€šè¿‡è·³è¿‡è·¯å¾„è¿æ¥ä»¥ä¿ç•™ç©ºé—´ç»†èŠ‚å¹¶æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é‡‡ç”¨Barlow Twinsè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æé«˜ç¼–ç å™¨å­¦ä¹ æœ‰æ„ä¹‰ç‰¹å¾çš„èƒ½åŠ›ï¼Œå¹¶åœ¨é¢„è®­ç»ƒåå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‡å°‘å‚æ•°æ•°é‡å’ŒåŠ å¿«æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ï¼Œä½¿å…¶æˆä¸ºåœ¨å®æ—¶å’Œèµ„æºæœ‰é™çš„ä¸´åºŠç¯å¢ƒä¸­éƒ¨ç½²çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶ç”¨äºç—…ç†åŒºåŸŸçš„æ£€æµ‹å’Œåˆ’åˆ†ã€‚</li>
<li>è™½ç„¶U-Netç­‰å·ç§¯æ¶æ„å·²æˆä¸ºæ­¤ç±»ä»»åŠ¡çš„æ ‡å‡†ï¼Œä½†å…¶æœ‰é™çš„æ„Ÿå—é‡é™åˆ¶äº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</li>
<li>æœ€è¿‘å°†transformeré›†æˆèµ·æ¥çš„åŠªåŠ›è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†å¾€å¾€å¯¼è‡´æ¨¡å‹è¿‡äºå¤æ‚ï¼Œä¸é€‚åˆå®æ—¶ä½¿ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯è½»é‡åŒ–æ¶æ„ï¼Œç»“åˆSwin Transformeré£æ ¼çš„ç¼–ç å™¨å’ŒU-Neté£æ ¼çš„è§£ç å™¨ï¼Œä»¥è¿›è¡Œå®æ—¶äºŒå…ƒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡è·³è¿‡è·¯å¾„è¿æ¥ä¿ç•™ç©ºé—´ç»†èŠ‚å¹¶æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨Barlow Twinsè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æé«˜ç¼–ç å™¨å­¦ä¹ ç‰¹å¾çš„èƒ½åŠ›ï¼Œå¹¶åœ¨é¢„è®­ç»ƒåè¿›è¡Œå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cdf9c2fc4f335816c43186c0efe4e9dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7023b5fccc2e712390cf97e6a3e50b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b226dcf05aa04cfacb13dfa223c1225a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automated-Radiographic-Total-Sharp-Score-ARTSS-in-Rheumatoid-Arthritis-A-Solution-to-Reduce-Inter-Intra-Reader-Variation-and-Enhancing-Clinical-Practice"><a href="#Automated-Radiographic-Total-Sharp-Score-ARTSS-in-Rheumatoid-Arthritis-A-Solution-to-Reduce-Inter-Intra-Reader-Variation-and-Enhancing-Clinical-Practice" class="headerlink" title="Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid   Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing   Clinical Practice"></a>Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid   Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing   Clinical Practice</h2><p><strong>Authors:Hajar Moradmand, Lei Ren</strong></p>
<p>Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp&#x2F;Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions. </p>
<blockquote>
<p>è¯„ä¼°ç±»é£æ¹¿æ€§å…³èŠ‚ç‚ï¼ˆRAï¼‰çš„ä¸¥é‡ç¨‹åº¦å¯¹äºæ²»ç–—æ–¹æ¡ˆçš„åˆ¶å®šè‡³å…³é‡è¦ï¼Œé€šå¸¸é‡‡ç”¨Total Sharp&#x2F;Van Der Heijde Scoreï¼ˆTSSï¼‰è¿›è¡Œè¯„åˆ†ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨è¯„åˆ†è€—æ—¶ä¸”ä¸»è§‚æ€§è¾ƒå¼ºã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§Automated Radiographic Sharp Scoringï¼ˆARTSSï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥åˆ†æå…¨æ‰‹Xå°„çº¿å›¾åƒï¼Œæ—¨åœ¨å‡å°‘è§‚å¯Ÿè€…ä¹‹é—´çš„å·®å¼‚å’Œè§‚å¯Ÿè€…åœ¨å¤šæ¬¡è¯„åˆ†é—´çš„å·®å¼‚ã€‚è¿™é¡¹ç ”ç©¶ç‹¬ç‰¹åœ°è§£å†³äº†å…³èŠ‚æ¶ˆå¤±å’Œå¯å˜é•¿åº¦å›¾åƒåºåˆ—çš„æ‚£è€…é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª970åæ‚£è€…çš„æ•°æ®æ¥å¼€å‘ARTSSæ¡†æ¶ï¼Œåˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼šIï¼‰ä½¿ç”¨ResNet50å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†å’Œé‡æ–°å®šå‘ï¼›IIï¼‰ä½¿ç”¨UNet 3å¯¹æ‰‹éƒ¨è¿›è¡Œåˆ†å‰²ï¼›IIIï¼‰ä½¿ç”¨YOLOv7è¿›è¡Œå…³èŠ‚è¯†åˆ«ï¼›IVï¼‰ä½¿ç”¨è¯¸å¦‚VGG16ã€VGG19ã€ResNet50ã€DenseNet201ã€EfficientNetB0å’ŒVision Transformerï¼ˆViTï¼‰ç­‰æ¨¡å‹è¿›è¡ŒTSSé¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨äº¤é›†æ¯”ï¼ˆIoUï¼‰ã€å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆMAPï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å’ŒHuberæŸå¤±æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚ä»¥ä¸¤ä½æ”¾å°„ç§‘åŒ»ç”Ÿå¹³å‡çš„TSSä½œä¸ºçœŸå®å€¼ã€‚æ¨¡å‹è®­ç»ƒé‡‡ç”¨ä¸‰æŠ˜äº¤å‰éªŒè¯ï¼Œæ¯æŠ˜åŒ…å«452ä¸ªè®­ç»ƒæ ·æœ¬å’Œ227ä¸ªéªŒè¯æ ·æœ¬ï¼Œå¤–éƒ¨æµ‹è¯•åŒ…æ‹¬291ä¸ªæœªè§è¿‡çš„å—è¯•è€…ã€‚æˆ‘ä»¬çš„å…³èŠ‚è¯†åˆ«æ¨¡å‹è¾¾åˆ°äº†99%çš„å‡†ç¡®ç‡ã€‚è¡¨ç°æœ€ä½³çš„ViTæ¨¡å‹åœ¨TSSé¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¾ƒä½çš„HuberæŸå¤±ä¸º0.87ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–RAè¯„åˆ†ä¸­çš„æ½œåŠ›ï¼Œè¿™å¯èƒ½ä¼šæå¤§åœ°æé«˜ä¸´åºŠå®è·µæ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†å…³èŠ‚æ¶ˆå¤±å’Œå¯å˜å…³èŠ‚æ•°é‡çš„é—®é¢˜ï¼ŒèŠ‚çœäº†æ—¶é—´ï¼Œå‡å°‘äº†è§‚å¯Ÿè€…é—´å’Œè§‚å¯Ÿè€…è‡ªèº«å¤šæ¬¡è¯„åˆ†é—´çš„å·®å¼‚ï¼Œæé«˜äº†æ”¾å°„ç§‘åŒ»ç”Ÿå‡†ç¡®è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå¹¶å¸®åŠ©é£æ¹¿ç§‘åŒ»ç”Ÿåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06854v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æå‡ºä¸€ç§è‡ªåŠ¨ç±»é£æ¹¿æ€§å…³èŠ‚ç‚è¯„åˆ†ï¼ˆARTSSï¼‰æ¡†æ¶ï¼Œç”¨äºåˆ†æå…¨æ‰‹Xå°„çº¿å›¾åƒï¼Œä»¥è¯„ä¼°ç±»é£æ¹¿æ€§å…³èŠ‚ç‚ï¼ˆRAï¼‰çš„ä¸¥é‡æ€§ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å‡å°‘è§‚å¯Ÿè€…ä¹‹é—´çš„å·®å¼‚å’Œè§‚å¯Ÿè€…å†…éƒ¨çš„å·®å¼‚ï¼Œæé«˜è¯„ä¼°æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ã€æ‰‹éƒ¨åˆ†å‰²ã€å…³èŠ‚è¯†åˆ«å’Œæ€»å°–é”åº¦é¢„æµ‹ã€‚æœ€ä½³æ¨¡å‹ä¸ºVision Transformerï¼ˆViTï¼‰ï¼Œåœ¨é¢„æµ‹æ€»å°–é”åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ä½HuberæŸå¤±å€¼0.87ã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–RAè¯„åˆ†æ–¹é¢çš„æ½œåŠ›ï¼Œèƒ½æ˜¾è‘—æé«˜ä¸´åºŠå®è·µæ•ˆç‡ï¼Œè§£å†³å…³èŠ‚æ¶ˆå¤±å’Œå…³èŠ‚æ•°é‡å˜åŒ–çš„é—®é¢˜ï¼ŒèŠ‚çœæ—¶é—´ï¼Œæé«˜è¯„ä¼°å‡†ç¡®æ€§ï¼Œå¸®åŠ©é£æ¹¿ç—…å­¦å®¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨ç±»é£æ¹¿æ€§å…³èŠ‚ç‚è¯„åˆ†ï¼ˆARTSSï¼‰æ¡†æ¶ï¼Œç”¨äºåˆ†æå…¨æ‰‹Xå°„çº¿å›¾åƒã€‚</li>
<li>ARTSSæ¡†æ¶æ—¨åœ¨å‡å°‘è¯„åˆ†è¿‡ç¨‹ä¸­çš„ä¸»è§‚æ€§å’Œæ—¶é—´æ¶ˆè€—ã€‚</li>
<li>ç ”ç©¶è¿‡ç¨‹ä¸­æ¶‰åŠå›¾åƒé¢„å¤„ç†ã€æ‰‹éƒ¨åˆ†å‰²ã€å…³èŠ‚è¯†åˆ«å’Œæ€»å°–é”åº¦é¢„æµ‹ç­‰å¤šä¸ªé˜¶æ®µã€‚</li>
<li>æœ€ä½³æ¨¡å‹ä¸ºVision Transformerï¼ˆViTï¼‰ï¼Œåœ¨é¢„æµ‹æ€»å°–é”åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ä½HuberæŸå¤±å€¼ã€‚</li>
<li>æ­¤æ¡†æ¶èƒ½å¤Ÿå¤„ç†å…³èŠ‚æ¶ˆå¤±å’Œå…³èŠ‚æ•°é‡å˜åŒ–çš„æƒ…å†µï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li>
<li>ARTSSæ¡†æ¶çš„åº”ç”¨èƒ½æé«˜è¯„ä¼°å‡†ç¡®æ€§ï¼Œå¸®åŠ©é£æ¹¿ç—…å­¦å®¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-575be83ced329f70212ab017658e5f7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef96befc30cd522feb5822fd85d3535a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b752709ba3383c84aa59a7718682ba5f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Curia-A-Multi-Modal-Foundation-Model-for-Radiology"><a href="#Curia-A-Multi-Modal-Foundation-Model-for-Radiology" class="headerlink" title="Curia: A Multi-Modal Foundation Model for Radiology"></a>Curia: A Multi-Modal Foundation Model for Radiology</h2><p><strong>Authors:Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, ThÃ©o Danielou, LÃ©o Alberge, LÃ©o Machado, Daniel Tordjman, Julie Dupuis, Korentin Le Floch, Jean Du Terrail, Mariam Moshiri, Laurent Dercle, Tom Boeken, Jules Gregory, Maxime Ronot, FranÃ§ois Legou, Pascal Roux, Marc Sapoval, Pierre Manceron, Paul HÃ©rent</strong></p>
<p>AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base modelâ€™s weights at <a target="_blank" rel="noopener" href="https://huggingface.co/raidium/curia">https://huggingface.co/raidium/curia</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½è¾…åŠ©æ”¾å°„å­¦è§£è¯»ä¸»è¦åŸºäºç‹­çª„ã€å•ä¸€ä»»åŠ¡çš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¯¹äºè¦†ç›–å¹¿æ³›çš„æˆåƒæ¨¡å¼ã€ç–¾ç—…å’Œæ”¾å°„å­¦å‘ç°æ¥è¯´å¹¶ä¸å®ç”¨ã€‚åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰æœ‰æœ›åœ¨è·¨æ¨¡æ€å’Œä½æ•°æ®ç¯å¢ƒä¸­å®ç°å¹¿æ³›é€šç”¨åŒ–ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ½œåŠ›åœ¨æ”¾å°„å­¦é¢†åŸŸä¸€ç›´æœªèƒ½å¾—åˆ°å……åˆ†å‘æŒ¥ã€‚æˆ‘ä»¬å¼•å…¥äº†Curiaï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤§å‹åŒ»é™¢æ•°å¹´çš„æ¨ªæ–­é¢æˆåƒè¾“å‡ºä¸Šè®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯æ¶µç›–ç°å®ä¸–ç•Œæ•°æ®çš„æœ€å¤§æ­¤ç±»è¯­æ–™åº“ï¼ŒåŒ…å«15ä¸‡ä»½æ£€æŸ¥ï¼ˆ130TBï¼‰ã€‚åœ¨ä¸€ä¸ªæ–°ç¼–åˆ¶çš„åŒ…å«19é¡¹ä»»åŠ¡çš„å¤–éƒ¨éªŒè¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCuriaèƒ½å‡†ç¡®è¯†åˆ«å™¨å®˜ï¼Œæ£€æµ‹å¦‚è„‘å‡ºè¡€å’Œå¿ƒè‚Œæ¢—æ­»ç­‰çŠ¶å†µï¼Œå¹¶åœ¨è‚¿ç˜¤åˆ†æœŸä¸­é¢„æµ‹ç»“æœã€‚Curiaè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æ”¾å°„å­¦å®¶å’Œæœ€æ–°åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨è·¨æ¨¡æ€å’Œä½æ•°æ®ç¯å¢ƒä¸­è¡¨ç°å‡ºäº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„çªå‘ç‰¹æ€§ã€‚ä¸ºäº†åŠ é€Ÿè¿›å±•ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/raidium/curia%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9D%83%E9%87%8D%E3%80%82">https://huggingface.co/raidium/curiaä¸Šå‘å¸ƒäº†åŸºç¡€æ¨¡å‹çš„æƒé‡ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06830v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§å‹çœŸå®ä¸–ç•ŒåŒ»å­¦å½±åƒæ•°æ®è®­ç»ƒçš„æ”¾å°„å­¦åŸºç¡€æ¨¡å‹Curiaã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè·¨å¤šç§æˆåƒæ¨¡æ€å’Œç¼ºä¹æ•°æ®çš„ç¯å¢ƒè¿›è¡Œå¹¿æ³›çš„æ¨å¹¿ï¼Œå¯è¯†åˆ«å™¨å®˜ã€æ£€æµ‹ç–¾ç—…å¹¶é¢„æµ‹è‚¿ç˜¤åˆ†æœŸç­‰ä»»åŠ¡ã€‚å…¶æ€§èƒ½å·²ç»è¾¾åˆ°æˆ–è¶…è¿‡äº†æ”¾å°„å­¦å®¶å’Œæœ€è¿‘çš„åŸºç¡€æ¨¡å‹ï¼Œå±•ç°å‡ºåœ¨ä¸´åºŠä¸Šæœ‰é‡è¦ä»·å€¼çš„è·¨æ¨¡æ€å’Œå°‘æ•°æ®ç¯å¢ƒä¸‹çš„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨æ”¾å°„å­¦è§£è¯»ä¸­çš„åº”ç”¨ä¸»è¦åŸºäºç‹­çª„çš„å•ä»»åŠ¡æ¨¡å‹ï¼Œéš¾ä»¥è¦†ç›–å¹¿æ³›çš„æˆåƒæ–¹å¼ã€ç–¾ç—…å’Œæ”¾å°„å­¦å‘ç°ã€‚</li>
<li>åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å…·æœ‰åœ¨å¤šç§æ¨¡æ€å’Œä½æ•°æ®ç¯å¢ƒä¸‹è¿›è¡Œå¹¿æ³›æ¨å¹¿çš„æ½œåŠ›ã€‚</li>
<li>Curiaæ˜¯ä¸€ä¸ªåœ¨å¤§å‹åŒ»é™¢å¤šå¹´æ¥çš„å…¨éƒ¨æ¨ªæ–­é¢å½±åƒæ•°æ®ä¸Šè®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ŒåŒ…å«15ä¸‡ç§æ£€æŸ¥ã€130TBçš„æ•°æ®ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„çœŸå®ä¸–ç•Œæ•°æ®é›†åˆã€‚</li>
<li>Curiaåœ¨å¤–éƒ¨éªŒè¯çš„19é¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å™¨å®˜ã€æ£€æµ‹ç–¾ç—…å¦‚è„‘å‡ºè¡€å’Œå¿ƒè‚Œæ¢—å¡ï¼Œå¹¶é¢„æµ‹è‚¿ç˜¤åˆ†æœŸã€‚</li>
<li>Curiaçš„æ€§èƒ½å·²ç»è¾¾åˆ°æˆ–è¶…è¿‡äº†æ”¾å°„å­¦å®¶å’Œæœ€è¿‘çš„å…¶ä»–åŸºç¡€æ¨¡å‹ã€‚</li>
<li>Curiaå±•ç°å‡ºè·¨æ¨¡æ€å’Œå°‘æ•°æ®ç¯å¢ƒä¸‹çš„ä¸´åºŠæ˜¾è‘—ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5db74efbc9e96c3bd4936debf548192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b16a7b1396729a4fe694eb7b305a6aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8088ef6aeeb1f60eac7c896c5b2675c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50e5bbd6d208d5338b9bcb9e460822d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MRI-Based-Brain-Tumor-Detection-through-an-Explainable-EfficientNetV2-and-MLP-Mixer-Attention-Architecture"><a href="#MRI-Based-Brain-Tumor-Detection-through-an-Explainable-EfficientNetV2-and-MLP-Mixer-Attention-Architecture" class="headerlink" title="MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2   and MLP-Mixer-Attention Architecture"></a>MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2   and MLP-Mixer-Attention Architecture</h2><p><strong>Authors:Mustafa Yurdakul, Åakir TaÅŸdemir</strong></p>
<p>Brain tumors are serious health problems that require early diagnosis due to their high mortality rates. Diagnosing tumors by examining Magnetic Resonance Imaging (MRI) images is a process that requires expertise and is prone to error. Therefore, the need for automated diagnosis systems is increasing day by day. In this context, a robust and explainable Deep Learning (DL) model for the classification of brain tumors is proposed. In this study, a publicly available Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI images of three tumor types was used. First, the classification performance of nine well-known CNN architectures was evaluated to determine the most effective backbone. Among these, EfficientNetV2 demonstrated the best performance and was selected as the backbone for further development. Subsequently, an attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to enhance its classification capability. The performance of the final model was comprehensively compared with basic CNNs and the methods in the literature. Additionally, Grad-CAM visualization was used to interpret and validate the decision-making process of the proposed model. The proposed modelâ€™s performance was evaluated using the five-fold cross-validation method. The proposed model demonstrated superior performance with 99.50% accuracy, 99.47% precision, 99.52% recall and 99.49% F1 score. The results obtained show that the model outperforms the studies in the literature. Moreover, Grad-CAM visualizations demonstrate that the model effectively focuses on relevant regions of MRI images, thus improving interpretability and clinical reliability. A robust deep learning model for clinical decision support systems has been obtained by combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy and interpretability in brain tumor classification. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤æ˜¯ä¸¥é‡çš„å¥åº·é—®é¢˜ï¼Œç”±äºå…¶é«˜æ­»äº¡ç‡ï¼Œéœ€è¦æ—©æœŸè¯Šæ–­ã€‚é€šè¿‡æ£€æŸ¥ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒæ¥è¯Šæ–­è‚¿ç˜¤æ˜¯ä¸€ä¸ªéœ€è¦ä¸“ä¸šçŸ¥è¯†ä¸”å®¹æ˜“å‡ºé”™çš„è¿‡ç¨‹ã€‚å› æ­¤ï¼Œå¯¹è‡ªåŠ¨è¯Šæ–­ç³»ç»Ÿçš„éœ€æ±‚æ¯å¤©éƒ½åœ¨å¢åŠ ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»çš„ç¨³å¥ä¸”å¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ã€‚æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªå…¬å¼€çš„Figshareæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸‰ç§è‚¿ç˜¤ç±»å‹çš„3064å¼ T1åŠ æƒå¢å¼ºè„‘MRIå›¾åƒã€‚é¦–å…ˆï¼Œè¯„ä¼°äº†ä¹ç§çŸ¥åCNNæ¶æ„çš„åˆ†ç±»æ€§èƒ½ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„ä¸»å¹²ç½‘ç»œã€‚å…¶ä¸­ï¼ŒEfficientNetV2è¡¨ç°æœ€ä½³ï¼Œè¢«é€‰ä¸ºè¿›ä¸€æ­¥å‘å±•çš„ä¸»å¹²ç½‘ç»œã€‚éšåï¼Œå°†æ³¨æ„åŠ›æœºåˆ¶çš„MLP-Mixeræ¶æ„é›†æˆåˆ°EfficientNetV2ä¸­ï¼Œä»¥å¢å¼ºå…¶åˆ†ç±»èƒ½åŠ›ã€‚å°†æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ä¸åŸºæœ¬CNNå’Œæ–‡çŒ®ä¸­çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œä½¿ç”¨Grad-CAMå¯è§†åŒ–æ¥è§£é‡Šå’ŒéªŒè¯æ‰€æå‡ºæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚æ‰€æå‡ºæ¨¡å‹çš„æ€§èƒ½é‡‡ç”¨äº”å€äº¤å‰éªŒè¯æ³•è¿›è¡Œè¯„ä¼°ã€‚æ‰€æå‡ºæ¨¡å‹çš„æ€§èƒ½å“è¶Šï¼Œå‡†ç¡®åº¦ä¸º99.50%ï¼Œç²¾ç¡®åº¦ä¸º99.47%ï¼Œå¬å›ç‡ä¸º99.52%ï¼ŒF1åˆ†æ•°ä¸º99.49%ã€‚æ‰€è·å¾—çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ–‡çŒ®ä¸­çš„è¡¨ç°æ›´èƒœä¸€ç­¹ã€‚æ­¤å¤–ï¼ŒGrad-CAMå¯è§†åŒ–æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å…³æ³¨MRIå›¾åƒçš„ç›¸å…³åŒºåŸŸï¼Œä»è€Œæé«˜å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯é æ€§ã€‚é€šè¿‡ç»“åˆEfficientNetV2å’ŒåŸºäºæ³¨æ„åŠ›çš„MLP-Mixerï¼Œè·å¾—äº†ä¸€ä¸ªç”¨äºä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„ç¨³å¥æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä¸­å…·æœ‰é«˜ç²¾åº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06713v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“åˆEfficientNetV2å’Œæ³¨æ„åŠ›æœºåˆ¶MLP-Mixerï¼Œå¯¹è„‘è‚¿ç˜¤åˆ†ç±»å®ç°é«˜å‡†ç¡®åº¦ï¼ˆ99.5%ï¼‰å’Œé«˜å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤çš„é«˜æ­»äº¡ç‡ä½¿å…¶æ—©æœŸå‡†ç¡®è¯Šæ–­å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è„‘è‚¿ç˜¤è¯Šæ–­ä¸­å…·æœ‰æ½œåŠ›ï¼Œå°¤å…¶æ˜¯è‡ªåŠ¨åŒ–è¯Šæ–­ç³»ç»Ÿã€‚</li>
<li>EfficientNetV2åœ¨å¤šç§çŸ¥åCNNæ¶æ„ä¸­è¡¨ç°æœ€ä½³ï¼Œè¢«é€‰ä¸ºè¿›ä¸€æ­¥ç ”ç©¶çš„åŸºç¡€æ¶æ„ã€‚</li>
<li>å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶çš„MLP-Mixeræ¶æ„ä»¥å¢å¼ºEfficientNetV2çš„åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡Grad-CAMå¯è§†åŒ–å±•ç°äº†å†³ç­–è¿‡ç¨‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ¨¡å‹è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼ˆ99.5%ï¼‰ï¼Œä¼˜äºç°æœ‰æ–‡çŒ®ä¸­çš„å…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66f263ff69004624d0219d98a32f6969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0c20383402a2bc4cd4eb61b39abca92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9e725be25c8bdd419095a34c2ec8adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741f304db6a369b8289662825c360f74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a53ec720f6a47b2e04a1cf632bc7339.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MM-DINOv2-Adapting-Foundation-Models-for-Multi-Modal-Medical-Image-Analysis"><a href="#MM-DINOv2-Adapting-Foundation-Models-for-Multi-Modal-Medical-Image-Analysis" class="headerlink" title="MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image   Analysis"></a>MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image   Analysis</h2><p><strong>Authors:Daniel Scholz, Ayhan Can Erdur, Viktoria Ehm, Anke Meyer-Baese, Jan C. Peeken, Daniel Rueckert, Benedikt Wiestler</strong></p>
<p>Vision foundation models like DINOv2 demonstrate remarkable potential in medical imaging despite their origin in natural image domains. However, their design inherently works best for uni-modal image analysis, limiting their effectiveness for multi-modal imaging tasks that are common in many medical fields, such as neurology and oncology. While supervised models perform well in this setting, they fail to leverage unlabeled datasets and struggle with missing modalities, a frequent challenge in clinical settings. To bridge these gaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the pre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our approach incorporates multi-modal patch embeddings, enabling vision foundation models to effectively process multi-modal imaging data. To address missing modalities, we employ full-modality masking, which encourages the model to learn robust cross-modality relationships. Furthermore, we leverage semi-supervised learning to harness large unlabeled datasets, enhancing both the accuracy and reliability of medical predictions. Applied to glioma subtype classification from multi-sequence brain MRI, our method achieves a Matthews Correlation Coefficient (MCC) of 0.6 on an external test set, surpassing state-of-the-art supervised approaches by +11.1%. Our work establishes a scalable and robust solution for multi-modal medical imaging tasks, leveraging powerful vision foundation models pre-trained on natural images while addressing real-world clinical challenges such as missing data and limited annotations. </p>
<blockquote>
<p>DINOv2ç­‰è§†è§‰åŸºç¡€æ¨¡å‹è™½ç„¶åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œå°½ç®¡å®ƒä»¬æœ€åˆæ˜¯åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸå¼€å‘çš„ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®¾è®¡æœ¬è´¨ä¸Šæœ€é€‚åˆå•æ¨¡æ€å›¾åƒåˆ†æï¼Œåœ¨åŒ»å­¦é¢†åŸŸå¸¸è§çš„å¤šæ¨¡æ€æˆåƒä»»åŠ¡ä¸­æ•ˆæœä¸ä½³ï¼Œä¾‹å¦‚åœ¨ç¥ç»å­¦å’Œè‚¿ç˜¤å­¦ä¸­ã€‚å°½ç®¡ç›‘ç£æ¨¡å‹åœ¨è¿™ç§ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬æ— æ³•åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™åœ¨ä¸´åºŠç¯å¢ƒä¸­æ˜¯å¸¸è§çš„ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MM-DINOv2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–ä¸”é«˜æ•ˆæ¡†æ¶ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹DINOv2é€‚åº”äºå¤šæ¨¡æ€åŒ»å­¦æˆåƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šæ¨¡æ€è¡¥ä¸åµŒå…¥ï¼Œä½¿è§†è§‰åŸºç¡€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šæ¨¡æ€æˆåƒæ•°æ®ã€‚ä¸ºäº†è§£å†³ç¼ºå¤±æ¨¡æ€çš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…¨æ¨¡æ€æ©è”½æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é¼“åŠ±æ¨¡å‹å­¦ä¹ ç¨³å¥çš„è·¨æ¨¡æ€å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨åŠç›‘ç£å­¦ä¹ æ¥åˆ©ç”¨å¤§é‡çš„æœªæ ‡è®°æ•°æ®é›†ï¼Œæé«˜åŒ»ç–—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚åœ¨é€šè¿‡å¤šåºåˆ—è„‘éƒ¨MRIè¿›è¡Œèƒ¶è´¨ç˜¤äºšå‹åˆ†ç±»çš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†å¤–éƒ¨æµ‹è¯•é›†ä¸Šçš„Matthewsç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰ä¸º0.6ï¼Œç›¸è¾ƒäºæœ€æ–°çš„ç›‘ç£æ–¹æ³•æé«˜äº†+11.1%ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¤šæ¨¡æ€åŒ»å­¦æˆåƒä»»åŠ¡å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨åœ¨å¤©ç„¶å›¾åƒä¸Šé¢„å…ˆè®­ç»ƒçš„å¼ºå¤§è§†è§‰åŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶è§£å†³ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠæŒ‘æˆ˜ï¼Œå¦‚ç¼ºå¤±æ•°æ®å’Œæœ‰é™çš„æ³¨é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06617v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å°½ç®¡èµ·æºäºè‡ªç„¶å›¾åƒé¢†åŸŸï¼ŒDINOv2ç­‰è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®¾è®¡æœ¬è´¨ä¸Šæœ€é€‚åˆå•æ¨¡æ€å›¾åƒåˆ†æï¼Œåœ¨åŒ»å­¦é¢†åŸŸå¸¸è§çš„å¤šæ¨¡æ€æˆåƒä»»åŠ¡ä¸­æ•ˆæœæœ‰é™ï¼Œå¦‚ç¥ç»å­¦å’Œè‚¿ç˜¤å­¦ç­‰é¢†åŸŸã€‚è™½ç„¶ç›‘ç£æ¨¡å‹åœ¨è¿™ç§è®¾ç½®ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬æ— æ³•åˆ©ç”¨æœªæ ‡è®°æ•°æ®é›†ï¼Œå¹¶ä¸”éš¾ä»¥åº”å¯¹ä¸´åºŠç¯å¢ƒä¸­å¸¸è§çš„ç¼ºå¤±æ¨¡æ€æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MM-DINOv2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–ä¸”é«˜æ•ˆæ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹DINOv2é€‚åº”äºå¤šæ¨¡æ€åŒ»å­¦æˆåƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šæ¨¡æ€è¡¥ä¸åµŒå…¥ï¼Œä½¿è§†è§‰åŸºç¡€æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€æˆåƒæ•°æ®ã€‚ä¸ºè§£å†³ç¼ºå¤±æ¨¡æ€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å…¨æ¨¡æ€æ©è”½ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ç¨³å¥çš„è·¨æ¨¡æ€å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨åŠç›‘ç£å­¦ä¹ åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®é›†ï¼Œæé«˜åŒ»ç–—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚åœ¨æ¥è‡ªå¤šåºåˆ—è„‘éƒ¨MRIçš„èƒ¶è´¨ç˜¤äºšå‹åˆ†ç±»åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°0.6çš„Matthewsç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰ï¼Œæ¯”æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•é«˜å‡º+11.1%ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¤šæ¨¡æ€åŒ»å­¦æˆåƒä»»åŠ¡å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨åœ¨å¤©ç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„å¼ºå¤§è§†è§‰åŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶è§£å†³ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠæŒ‘æˆ˜ï¼Œå¦‚ç¼ºå¤±æ•°æ®å’Œæœ‰é™æ³¨é‡Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DINOv2ç­‰è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œå°½ç®¡å®ƒä»¬æœ€åˆæ˜¯ä¸ºè‡ªç„¶å›¾åƒè®¾è®¡çš„ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€å›¾åƒåˆ†ææ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨å¤šæ¨¡æ€åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„æ•ˆæœæœ‰é™ã€‚</li>
<li>å¤šæ¨¡æ€æˆåƒåœ¨åŒ»å­¦é¢†åŸŸå¾ˆå¸¸è§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¥ç»å­¦å’Œè‚¿ç˜¤å­¦ç­‰é¢†åŸŸã€‚</li>
<li>ç›‘ç£æ¨¡å‹éš¾ä»¥åˆ©ç”¨æœªæ ‡è®°æ•°æ®é›†å’Œåº”å¯¹ç¼ºå¤±æ¨¡æ€çš„æŒ‘æˆ˜ã€‚</li>
<li>MM-DINOv2æ¡†æ¶æ˜¯ç¬¬ä¸€ä¸ªå°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹é€‚åº”äºå¤šæ¨¡æ€åŒ»å­¦æˆåƒçš„ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€è¡¥ä¸åµŒå…¥å’Œå…¨é¢æ¨¡æ€æ©è”½è§£å†³å¤šæ¨¡æ€æ•°æ®åˆ†æå’Œç¼ºå¤±æ¨¡æ€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7cc3f8140e0c1f61ab75be89dcaa746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e0ebd8e851d8b913a059ce58408f62.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Contrastive-Anatomy-Contrast-Disentanglement-A-Domain-General-MRI-Harmonization-Method"><a href="#Contrastive-Anatomy-Contrast-Disentanglement-A-Domain-General-MRI-Harmonization-Method" class="headerlink" title="Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI   Harmonization Method"></a>Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI   Harmonization Method</h2><p><strong>Authors:Daniel Scholz, Ayhan Can Erdur, Robbie Holland, Viktoria Ehm, Jan C. Peeken, Benedikt Wiestler, Daniel Rueckert</strong></p>
<p>Magnetic resonance imaging (MRI) is an invaluable tool for clinical and research applications. Yet, variations in scanners and acquisition parameters cause inconsistencies in image contrast, hindering data comparability and reproducibility across datasets and clinical studies. Existing scanner harmonization methods, designed to address this challenge, face limitations, such as requiring traveling subjects or struggling to generalize to unseen domains. We propose a novel approach using a conditioned diffusion autoencoder with a contrastive loss and domain-agnostic contrast augmentation to harmonize MR images across scanners while preserving subject-specific anatomy. Our method enables brain MRI synthesis from a single reference image. It outperforms baseline techniques, achieving a +7% PSNR improvement on a traveling subjects dataset and +18% improvement on age regression in unseen. Our model provides robust, effective harmonization of brain MRIs to target scanners without requiring fine-tuning. This advancement promises to enhance comparability, reproducibility, and generalizability in multi-site and longitudinal clinical studies, ultimately contributing to improved healthcare outcomes. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­å…·æœ‰æé«˜çš„ä»·å€¼ã€‚ç„¶è€Œï¼Œæ‰«æä»ªå’Œé‡‡é›†å‚æ•°çš„å·®å¼‚å¯¼è‡´å›¾åƒå¯¹æ¯”åº¦çš„ä¸ä¸€è‡´æ€§ï¼Œé˜»ç¢äº†ä¸åŒæ•°æ®é›†å’Œä¸´åºŠç ”ç©¶ä¹‹é—´çš„æ•°æ®å¯æ¯”æ€§å’Œå¯é‡å¤æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜è€Œè®¾è®¡çš„ç°æœ‰æ‰«æä»ªè°è°ƒæ–¹æ³•é¢ä¸´ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦ç§»åŠ¨å—è¯•è€…æˆ–éš¾ä»¥æ¨å¹¿åˆ°æœªè§é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¡ä»¶æ‰©æ•£è‡ªç¼–ç å™¨çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¯¹æ¯”æŸå¤±å’Œé¢†åŸŸæ— å…³çš„å¯¹æ¯”å¢å¼ºæŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¿ç•™ç‰¹å®šä¸»ä½“è§£å‰–ç»“æ„çš„åŒæ—¶ï¼Œå®ç°è·¨æ‰«æä»ªçš„MRå›¾åƒè°è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ä»å•ä¸ªå‚è€ƒå›¾åƒåˆæˆå¤§è„‘MRIã€‚åœ¨ç§»åŠ¨å—è¯•è€…æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢è¾ƒåŸºçº¿æŠ€æœ¯æé«˜äº†7%ï¼Œåœ¨æœªè§è¿‡çš„å¹´é¾„å›å½’æµ‹è¯•ä¸­æé«˜äº†18%ã€‚æˆ‘ä»¬çš„æ¨¡å‹æä¾›äº†ä¸€ç§ç¨³å¥æœ‰æ•ˆçš„è„‘MRIè°è°ƒæ–¹æ³•ï¼Œæ— éœ€å¾®è°ƒå³å¯é’ˆå¯¹ç›®æ ‡æ‰«æä»ªã€‚è¿™ä¸€è¿›å±•æœ‰æœ›å¢å¼ºå¤šç«™ç‚¹å’Œçºµå‘ä¸´åºŠç ”ç©¶çš„å¯æ¯”æ€§ã€å¯é‡å¤æ€§å’Œæ³›åŒ–æ€§ï¼Œæœ€ç»ˆä¸ºæ”¹å–„åŒ»ç–—ç»“æœåšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06592v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­å…·æœ‰å¾ˆå¤§ä»·å€¼ã€‚ä¸åŒæ‰«æä»ªå’Œé‡‡é›†å‚æ•°é€ æˆçš„å›¾åƒå¯¹æ¯”å·®å¼‚ï¼Œç»™æ•°æ®åœ¨ä¸åŒæ•°æ®é›†å’Œä¸´åºŠç ”ç©¶ä¸­çš„å¯æ¯”æ€§å’Œå¯é‡å¤æ€§å¸¦æ¥æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç°æœ‰æ‰«æä»ªè°ƒå’Œæ–¹æ³•å­˜åœ¨å±€é™ï¼Œå¦‚éœ€è¦ç§»åŠ¨å—è¯•è€…æˆ–éš¾ä»¥æ¨å¹¿åˆ°æœªè§é¢†åŸŸã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ä½¿ç”¨æ¡ä»¶æ‰©æ•£è‡ªç¼–ç å™¨ä¸å¯¹æ¯”æŸå¤±å’Œé¢†åŸŸæ— å…³çš„å¯¹æ¯”å¢å¼ºç›¸ç»“åˆçš„æ–°æ–¹æ³•ï¼Œå®ç°å¯¹ä¸åŒæ‰«æä»ªä¸‹MRå›¾åƒçš„ä¸€è‡´åŒ–å¤„ç†ï¼ŒåŒæ—¶ä¿ç•™ä¸ªä½“ç‰¹å®šè§£å‰–ç»“æ„ã€‚è¯¥æ–¹æ³•å¯ä»å•ä¸€å‚è€ƒå›¾åƒåˆæˆå¤§è„‘MRIã€‚ç›¸è¾ƒäºåŸºçº¿æŠ€æœ¯ï¼Œåœ¨ç§»åŠ¨å—è¯•è€…æ•°æ®é›†ä¸Šå®ç°äº†å³°å€¼ä¿¡å·å™ªå£°æ¯”ï¼ˆPSNRï¼‰æå‡7%ï¼Œåœ¨æœªè§å¹´é¾„å›å½’ä»»åŠ¡ä¸Šæå‡18%ã€‚è¯¥æ–¹æ³•ä¸ºå¤§è„‘MRIå‘ç›®æ ‡æ‰«æä»ªçš„ç¨³å¥ã€æœ‰æ•ˆè°ƒå’Œæä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€å¾®è°ƒå³å¯å®ç°ã€‚æ­¤è¿›æ­¥æœ‰æœ›å¢å¼ºå¤šç«™ç‚¹å’Œçºµå‘ä¸´åºŠç ”ç©¶çš„å¯æ¯”æ€§ã€å¯é‡å¤æ€§å’Œæ³›åŒ–æ€§ï¼Œæœ€ç»ˆä¸ºæ”¹å–„åŒ»ç–—ç»“æœåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIåœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œä½†ä¸åŒæ‰«æä»ªå’Œé‡‡é›†å‚æ•°å¯¼è‡´å›¾åƒå¯¹æ¯”å·®å¼‚ï¼Œå½±å“æ•°æ®æ¯”è¾ƒå’Œå†ç°æ€§ã€‚</li>
<li>ç°æœ‰æ‰«æä»ªè°ƒå’Œæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç§»åŠ¨å—è¯•è€…éœ€æ±‚å’Œéš¾ä»¥æ¨å¹¿è‡³æœªè§é¢†åŸŸã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨æ¡ä»¶æ‰©æ•£è‡ªç¼–ç å™¨å’Œå¯¹æ¯”æŸå¤±ï¼Œç»“åˆé¢†åŸŸæ— å…³çš„å¯¹æ¯”å¢å¼ºï¼Œä»¥è°ƒå’Œä¸åŒæ‰«æä»ªä¸‹çš„MRå›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿ç•™ä¸ªä½“ç‰¹å®šè§£å‰–ç»“æ„çš„åŒæ—¶ï¼Œä»å•ä¸€å‚è€ƒå›¾åƒåˆæˆå¤§è„‘MRIã€‚</li>
<li>ä¸åŸºçº¿æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç§»åŠ¨å—è¯•è€…æ•°æ®é›†ä¸Šå®ç°äº†PSNRçš„æå‡ï¼Œå¹¶åœ¨æœªè§å¹´é¾„å›å½’ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºå¤§è„‘MRIçš„ç¨³å¥ã€æœ‰æ•ˆè°ƒå’Œæä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå¤šç§æ‰«æä»ªï¼Œæ— éœ€å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a18ed848afcb0b9f4ffb4d22015308ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-395bc3333069c2cb6d1a2c6f0213f041.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Predicting-Brain-Tumor-Response-to-Therapy-using-a-Hybrid-Deep-Learning-and-Radiomics-Approach"><a href="#Predicting-Brain-Tumor-Response-to-Therapy-using-a-Hybrid-Deep-Learning-and-Radiomics-Approach" class="headerlink" title="Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning   and Radiomics Approach"></a>Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning   and Radiomics Approach</h2><p><strong>Authors:Daniil Tikhonov, Matheus Scatolin, Mohor Banerjee, Qiankun Ji, Ahmed Jaheen, Mostafa Salem, Abdelrahman Elsayed, Hu Wang, Sarim Hashmi, Mohammad Yaqub</strong></p>
<p>Accurate evaluation of the response of glioblastoma to therapy is crucial for clinical decision-making and patient management. The Response Assessment in Neuro-Oncology (RANO) criteria provide a standardized framework to assess patientsâ€™ clinical response, but their application can be complex and subject to observer variability. This paper presents an automated method for classifying the intervention response from longitudinal MRI scans, developed to predict tumor response during therapy as part of the BraTS 2025 challenge. We propose a novel hybrid framework that combines deep learning derived feature extraction and an extensive set of radiomics and clinically chosen features. Our approach utilizes a fine-tuned ResNet-18 model to extract features from 2D regions of interest across four MRI modalities. These deep features are then fused with a rich set of more than 4800 radiomic and clinically driven features, including 3D radiomics of tumor growth and shrinkage masks, volumetric changes relative to the nadir, and tumor centroid shift. Using the fused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a Macro F1 score of 0.50 in the 4-class response prediction task (Complete Response, Partial Response, Stable Disease, Progressive Disease). Our results highlight that synergizing learned image representations with domain-targeted radiomic features provides a robust and effective solution for automated treatment response assessment in neuro-oncology. </p>
<blockquote>
<p>å¯¹èƒ¶è´¨æ¯ç»†èƒç˜¤æ²»ç–—ååº”çš„å‡†ç¡®è¯„ä¼°å¯¹äºä¸´åºŠå†³ç­–å’Œæ‚£è€…ç®¡ç†è‡³å…³é‡è¦ã€‚ç¥ç»è‚¿ç˜¤å­¦ååº”è¯„ä¼°ï¼ˆRANOï¼‰æ ‡å‡†æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ¡†æ¶æ¥è¯„ä¼°æ‚£è€…çš„ä¸´åºŠååº”ï¼Œä½†å…¶åœ¨åº”ç”¨è¿‡ç¨‹ä¸­å¯èƒ½è¾ƒä¸ºå¤æ‚ä¸”å­˜åœ¨è§‚å¯Ÿè€…é—´çš„å·®å¼‚ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åˆ†ç±»å¹²é¢„ååº”çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºçºµå‘MRIæ‰«æå¼€å‘ï¼Œæ—¨åœ¨ä½œä¸ºBraTS 2025æŒ‘æˆ˜èµ›çš„ä¸€éƒ¨åˆ†æ¥é¢„æµ‹æ²»ç–—è¿‡ç¨‹ä¸­çš„è‚¿ç˜¤ååº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¡†æ¶ï¼Œç»“åˆäº†æ·±åº¦å­¦ä¹ ç‰¹å¾æå–å’Œä¸€ç»„ä¸°å¯Œçš„æ”¾å°„å­¦åŠä¸´åºŠé€‰æ‹©ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç»è¿‡å¾®è°ƒåçš„ResNet-18æ¨¡å‹æ¥ä»å››ç§MRIæ¨¡å¼çš„2Dæ„Ÿå…´è¶£åŒºåŸŸä¸­æå–ç‰¹å¾ã€‚è¿™äº›æ·±åº¦ç‰¹å¾ç„¶åä¸è¶…è¿‡4800ä¸ªæ”¾å°„å­¦åŠä¸´åºŠé©±åŠ¨ç‰¹å¾é›†èåˆï¼ŒåŒ…æ‹¬è‚¿ç˜¤çš„3Dæ”¾å°„å­¦å¢é•¿å’Œèç¼©æ©æ¨¡ã€ç›¸å¯¹äºæœ€ä½ç‚¹çš„ä½“ç§¯å˜åŒ–ä»¥åŠè‚¿ç˜¤è´¨å¿ƒä½ç§»ã€‚ä½¿ç”¨èåˆç‰¹å¾é›†ï¼ŒCatBooståˆ†ç±»å™¨åœ¨4ç±»ååº”é¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†å¹³å‡ROC AUCä¸º0.81å’Œå®è§‚F1åˆ†æ•°ä¸º0.50ï¼ˆå®Œå…¨ååº”ã€éƒ¨åˆ†ååº”ã€ç¨³å®šæ€§ç–¾ç—…ã€è¿›å±•æ€§ç–¾ç—…ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œå°†å­¦ä¹ åˆ°çš„å›¾åƒè¡¨ç¤ºä¸é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ”¾å°„å­¦ç‰¹å¾ç›¸ç»“åˆï¼Œä¸ºç¥ç»è‚¿ç˜¤å­¦ä¸­è‡ªåŠ¨æ²»ç–—ååº”è¯„ä¼°æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06511v1">PDF</a> Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ ç‰¹å¾æå–å’Œå¤§é‡æ”¾å°„å­¦åŠä¸´åºŠç‰¹å¾çš„æ··åˆæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åˆ†ç±»èƒ¶è´¨æ¯ç»†èƒç˜¤å¯¹æ²»ç–—çš„ååº”ã€‚è¯¥ç ”ç©¶åˆ©ç”¨ç²¾ç»†è°ƒæ•´çš„ResNet-18æ¨¡å‹ä»MRIçš„å››ç§æ¨¡æ€ä¸­æå–ç‰¹å¾ï¼Œå¹¶ä¸è¶…è¿‡4800ä¸ªæ”¾å°„å­¦åŠä¸´åºŠç‰¹å¾èåˆã€‚èåˆç‰¹å¾é›†é€šè¿‡CatBooståˆ†ç±»å™¨å®ç°å¹³å‡ROC AUCä¸º0.81å’Œå®è§‚F1åˆ†æ•°ä¸º0.50çš„4ç±»ååº”é¢„æµ‹ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†å­¦ä¹ åˆ°çš„å›¾åƒè¡¨ç¤ºä¸é’ˆå¯¹é¢†åŸŸçš„æ”¾å°„å­¦ç‰¹å¾ç›¸ç»“åˆï¼Œä¸ºç¥ç»è‚¿ç˜¤å­¦ä¸­çš„è‡ªåŠ¨æ²»ç–—ååº”è¯„ä¼°æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠå†³ç­–å’Œæ‚£è€…ç®¡ç†ä¸­ï¼Œå‡†ç¡®è¯„ä¼°èƒ¶è´¨æ¯ç»†èƒç˜¤å¯¹æ²»ç–—çš„ååº”è‡³å…³é‡è¦ã€‚</li>
<li>RANOæ ‡å‡†æä¾›äº†ä¸€ä¸ªè¯„ä¼°æ‚£è€…ä¸´åºŠååº”çš„æ ‡å‡†åŒ–æ¡†æ¶ï¼Œä½†å…¶åº”ç”¨å¤æ‚ä¸”å­˜åœ¨è§‚å¯Ÿè€…å·®å¼‚ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œç»“åˆæ·±åº¦å­¦ä¹ ç‰¹å¾æå–å’Œå¤§é‡æ”¾å°„å­¦åŠä¸´åºŠç‰¹å¾è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ã€‚</li>
<li>åˆ©ç”¨ResNet-18æ¨¡å‹ä»MRIçš„å››ç§æ¨¡æ€ä¸­æå–ç‰¹å¾ã€‚</li>
<li>ç‰¹å¾é›†èåˆäº†è¶…è¿‡4800ä¸ªæ”¾å°„å­¦åŠä¸´åºŠç‰¹å¾ï¼ŒåŒ…æ‹¬è‚¿ç˜¤ç”Ÿé•¿å’Œèç¼©æ©æ¨¡çš„3Dæ”¾å°„å­¦ç‰¹å¾ã€ç›¸å¯¹äºæœ€ä½ç‚¹çš„ä½“ç§¯å˜åŒ–å’Œè‚¿ç˜¤è´¨å¿ƒåç§»ç­‰ã€‚</li>
<li>CatBooståˆ†ç±»å™¨åœ¨4ç±»ååº”é¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå¹³å‡ROC AUCä¸º0.81ï¼Œå®è§‚F1åˆ†æ•°ä¸º0.50ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f137cbe11813dc273c479cfb252336e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f6ae41aa4921c7644923ed4ace52f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac9aa7480ea6c2390216b5084d82a5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00de836efbba696a3f5500d9bf626500.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Information-Divergence-for-Robust-Semi-Supervised-Fetal-Ultrasound-Image-Segmentation"><a href="#Leveraging-Information-Divergence-for-Robust-Semi-Supervised-Fetal-Ultrasound-Image-Segmentation" class="headerlink" title="Leveraging Information Divergence for Robust Semi-Supervised Fetal   Ultrasound Image Segmentation"></a>Leveraging Information Divergence for Robust Semi-Supervised Fetal   Ultrasound Image Segmentation</h2><p><strong>Authors:Fangyijie Wang, GuÃ©nolÃ© Silvestre, Kathleen M. Curran</strong></p>
<p>Maternal-fetal Ultrasound is the primary modality for monitoring fetal development, yet automated segmentation remains challenging due to the scarcity of high-quality annotations. To address this limitation, we propose a semi-supervised learning framework that leverages information divergence for robust fetal ultrasound segmentation. Our method employs a lightweight convolutional network (1.47M parameters) and a Transformer-based network, trained jointly with labelled data through standard supervision and with unlabelled data via cross-supervision. To encourage consistent and confident predictions, we introduce an information divergence loss that combines per-pixel Kullback-Leibler divergence and Mutual Information Gap, effectively reducing prediction disagreement between the two models. In addition, we apply mixup on unlabelled samples to further enhance robustness. Experiments on two fetal ultrasound datasets demonstrate that our approach consistently outperforms seven state-of-the-art semi-supervised methods. When only 5% of training data is labelled, our framework improves the Dice score by 2.39%, reduces the 95% Hausdorff distance by 14.90, and decreases the Average Surface Distance by 4.18. These results highlight the effectiveness of leveraging information divergence for annotation-efficient and robust medical image segmentation. Our code is publicly available on GitHub. </p>
<blockquote>
<p>æ¯ä½“èƒå„¿è¶…å£°æ³¢æ˜¯ç›‘æµ‹èƒå„¿å‘è‚²çš„ä¸»è¦æ–¹å¼ï¼Œä½†ç”±äºé«˜è´¨é‡æ ‡æ³¨çš„ç¼ºä¹ï¼Œè‡ªåŠ¨åŒ–åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¿¡æ¯æ•£åº¦è¿›è¡Œç¨³å¥çš„èƒå„¿è¶…å£°åˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ä¸ªè½»é‡çº§çš„å·ç§¯ç½‘ç»œï¼ˆ147ä¸‡ä¸ªå‚æ•°ï¼‰å’Œä¸€ä¸ªåŸºäºTransformerçš„ç½‘ç»œï¼Œé€šè¿‡æ ‡å‡†ç›‘ç£ä¸æ ‡è®°æ•°æ®è”åˆè®­ç»ƒï¼Œå¹¶é€šè¿‡äº’ç›‘ç£çš„æ–¹å¼ä¸æœªæ ‡è®°æ•°æ®è®­ç»ƒã€‚ä¸ºäº†é¼“åŠ±ä¸€è‡´å’Œè‡ªä¿¡çš„é¢„æµ‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¿¡æ¯æ•£åº¦æŸå¤±ï¼Œå®ƒå°†åƒç´ çº§çš„Kullback-Leibleræ•£åº¦å’Œäº’ä¿¡æ¯å·®è·ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„é¢„æµ‹åˆ†æ­§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœªæ ‡è®°æ ·æœ¬åº”ç”¨äº†mixupï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç¨³å¥æ€§ã€‚åœ¨ä¸¤ä¸ªèƒå„¿è¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€ç›´ä¼˜äºä¸ƒç§æœ€å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ã€‚å½“åªæœ‰5%çš„è®­ç»ƒæ•°æ®è¢«æ ‡è®°æ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æé«˜äº†Diceå¾—åˆ†2.39%ï¼Œå°†95%çš„Hausdorffè·ç¦»å‡å°‘äº†14.90%ï¼Œå¹¶é™ä½äº†å¹³å‡è¡¨é¢è·ç¦»4.18%ã€‚è¿™äº›ç»“æœçªå‡ºäº†åˆ©ç”¨ä¿¡æ¯æ•£åº¦è¿›è¡Œæ ‡æ³¨æœ‰æ•ˆå’Œç¨³å¥åŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06495v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºä¿¡æ¯æ•£åº¦çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç¨³å¥çš„èƒå„¿è¶…å£°åˆ†å‰²ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è½»é‡çº§å·ç§¯ç½‘ç»œå’ŒåŸºäºTransformerçš„ç½‘ç»œï¼Œé€šè¿‡æ ‡å‡†ç›‘ç£è®­ç»ƒæ ‡ç­¾æ•°æ®ï¼Œå¹¶é€šè¿‡äº¤å‰ç›‘ç£è®­ç»ƒæœªæ ‡è®°æ•°æ®ã€‚å¼•å…¥ä¿¡æ¯æ•£åº¦æŸå¤±é¼“åŠ±ä¸€è‡´å’Œå¯é çš„é¢„æµ‹ï¼Œå¹¶æœ‰æ•ˆå‡å°‘æ¨¡å‹ä¹‹é—´çš„é¢„æµ‹åˆ†æ­§ã€‚åœ¨èƒå„¿è¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨5%çš„è®­ç»ƒæ•°æ®æ ‡æ³¨æ—¶ï¼Œç›¸è¾ƒäºå…¶ä»–ä¸ƒç§å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æ­¤æ¡†æ¶æé«˜äº†Diceå¾—åˆ†ï¼Œå‡å°‘äº†Hausdorffè·ç¦»å’Œå¹³å‡è¡¨é¢è·ç¦»ï¼Œè¯æ˜äº†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ ‡æ³¨æ•ˆç‡å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºèƒå„¿è¶…å£°å›¾åƒçš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚</li>
<li>ç»“åˆè½»é‡çº§å·ç§¯ç½‘ç»œå’ŒåŸºäºTransformerçš„ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡æ ‡å‡†ç›‘ç£å’Œäº¤å‰ç›‘ç£æ–¹å¼è®­ç»ƒæ•°æ®å’Œæœªæ ‡è®°æ•°æ®ã€‚</li>
<li>å¼•å…¥ä¿¡æ¯æ•£åº¦æŸå¤±ä»¥å‡å°‘æ¨¡å‹é—´çš„é¢„æµ‹åˆ†æ­§ã€‚</li>
<li>åœ¨èƒå„¿è¶…å£°æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®æ—¶ï¼Œè¯¥æ–¹æ³•ä¾ç„¶è¡¨ç°å‡ºè‰¯å¥½æ•ˆæœã€‚</li>
<li>å…¬å¼€ä»£ç å¹¶å·²åœ¨GitHubä¸Šæä¾›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cb373310dfc8352e2c29af0110b3f5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dbe313ea1639a7f5eb63cfbd91814ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-540873664816b2e88e366af2959bcc7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-690c976cdf2da7f3ae3b7aa0c58c4cbd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text4Seg-Advancing-Image-Segmentation-via-Generative-Language-Modeling"><a href="#Text4Seg-Advancing-Image-Segmentation-via-Generative-Language-Modeling" class="headerlink" title="Text4Seg++: Advancing Image Segmentation via Generative Language   Modeling"></a>Text4Seg++: Advancing Image Segmentation via Generative Language   Modeling</h2><p><strong>Authors:Mengcheng Lan, Chaofeng Chen, Jiaxing Xu, Zongrui Li, Yiping Ke, Xudong Jiang, Yingchen Yu, Yunqing Zhao, Song Bai</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks. However, effectively integrating image segmentation into these models remains a significant challenge. In this work, we propose a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. We first introduce image-wise semantic descriptors, a patch-aligned textual representation of segmentation masks that integrates naturally into the language modeling pipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74% and accelerating inference by $3\times$, without compromising performance. Building upon this, our initial framework Text4Seg achieves strong segmentation performance across a wide range of vision tasks. To further improve granularity and compactness, we propose box-wise semantic descriptors, which localizes regions of interest using bounding boxes and represents region masks via structured mask tokens called semantic bricks. This leads to our refined model, Text4Seg++, which formulates segmentation as a next-brick prediction task, combining precision, scalability, and generative efficiency. Comprehensive experiments on natural and remote sensing datasets show that Text4Seg++ consistently outperforms state-of-the-art models across diverse benchmarks without any task-specific fine-tuning, while remaining compatible with existing MLLM backbones. Our work highlights the effectiveness, scalability, and generalizability of text-driven image segmentation within the MLLM framework. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°è¿™äº›æ¨¡å‹ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬é®ç½©èŒƒå¼ï¼Œå°†å›¾åƒåˆ†å‰²è§†ä¸ºæ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼Œæ— éœ€é¢å¤–çš„è§£ç å™¨ï¼Œå¹¶æ˜¾è‘—ç®€åŒ–äº†åˆ†å‰²è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºè¯­ä¹‰æè¿°ç¬¦ï¼Œè¿™æ˜¯åˆ†å‰²é®ç½©çš„ä¸€ç§æ–°æ–‡æœ¬è¡¨ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªå›¾åƒå—éƒ½æ˜ å°„åˆ°å…¶ç›¸åº”çš„æ–‡æœ¬æ ‡ç­¾ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥å›¾åƒçº§è¯­ä¹‰æè¿°ç¬¦ï¼Œè¿™æ˜¯åˆ†å‰²é®ç½©çš„å—å¯¹é½æ–‡æœ¬è¡¨ç¤ºï¼Œè‡ªç„¶åœ°èå…¥è¯­è¨€å»ºæ¨¡æµç¨‹ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¡Œè¿è¡Œé•¿åº¦ç¼–ç ï¼ˆR-RLEï¼‰ï¼Œå®ƒå‹ç¼©äº†å†—ä½™çš„æ–‡æœ¬åºåˆ—ï¼Œåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†è¯­ä¹‰æè¿°ç¬¦çš„é•¿åº¦å‡å°‘äº†74%ï¼Œå¹¶å°†æ¨ç†é€Ÿåº¦æé«˜äº†3å€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æœ€åˆçš„æ¡†æ¶Text4Segåœ¨å¹¿æ³›çš„è§†è§‰ä»»åŠ¡ä¸­å®ç°äº†å¼ºå¤§çš„åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é¢—ç²’åº¦å’Œç´§å‡‘æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ¡†çº§è¯­ä¹‰æè¿°ç¬¦ï¼Œå…¶ä½¿ç”¨è¾¹ç•Œæ¡†å®šä½æ„Ÿå…´è¶£åŒºåŸŸï¼Œå¹¶é€šè¿‡ç§°ä¸ºè¯­ä¹‰ç –çš„ç»“æ„åŒ–é®ç½©ä»¤ç‰Œè¡¨ç¤ºåŒºåŸŸé®ç½©ã€‚è¿™å¯¼è‡´äº†æˆ‘ä»¬æ”¹è¿›åçš„æ¨¡å‹Text4Seg++ï¼Œå®ƒå°†åˆ†å‰²åˆ¶å®šä¸ºä¸‹ä¸€ä¸ªç –å—é¢„æµ‹ä»»åŠ¡ï¼Œç»“åˆäº†ç²¾åº¦ã€å¯æ‰©å±•æ€§å’Œç”Ÿæˆæ•ˆç‡ã€‚åœ¨å¤©ç„¶å’Œé¥æ„Ÿæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒText4Seg++åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œä¸”æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼ŒåŒæ—¶ä¸ç°æœ‰çš„MLLMéª¨å¹²ç½‘å…¼å®¹ã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†MLLMæ¡†æ¶å†…æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ†å‰²çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ä»¥åŠæ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06321v1">PDF</a> Extended version of our conference paper arXiv:2410.09855</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå°†å›¾åƒåˆ†å‰²è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼Œé€šè¿‡è¯­ä¹‰æè¿°ç¬¦å®ç°æ–‡æœ¬ä¸å›¾åƒçš„æ˜ å°„ã€‚å¼•å…¥å›¾åƒçº§è¯­ä¹‰æè¿°ç¬¦å’Œè¡Œçº§è¿è¡Œé•¿åº¦ç¼–ç ï¼ˆR-RLEï¼‰æé«˜æ•ˆç‡ï¼Œæ„å»ºåˆå§‹æ¡†æ¶Text4Segã€‚ä¸ºè¿›ä¸€æ­¥æé«˜ç²¾åº¦å’Œæ•ˆç‡ï¼Œæå‡ºç›’çº§è¯­ä¹‰æè¿°ç¬¦ï¼Œå®ç°åŒºåŸŸæ©è†œçš„è¡¨ç¤ºå’Œç»“æ„åŒ–æ©ç çš„ä»¤ç‰ŒåŒ–ã€‚æ”¹è¿›åçš„æ¨¡å‹Text4Seg++åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œå¹¶åœ¨è‡ªç„¶å’Œé¥æ„Ÿæ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°æ¨¡å‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ†å‰²åœ¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ä¸­çš„æœ‰æ•ˆæ€§ã€å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå°†å›¾åƒåˆ†å‰²è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†è¯­ä¹‰æè¿°ç¬¦ï¼Œå®ç°äº†æ–‡æœ¬ä¸å›¾åƒçš„æ˜ å°„ã€‚</li>
<li>é€šè¿‡å›¾åƒçº§è¯­ä¹‰æè¿°ç¬¦å’Œè¡Œçº§è¿è¡Œé•¿åº¦ç¼–ç ï¼ˆR-RLEï¼‰æé«˜äº†æ•ˆç‡ã€‚</li>
<li>åˆå§‹æ¡†æ¶Text4Segåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ºè¿›ä¸€æ­¥æé«˜ç²¾åº¦å’Œæ•ˆç‡ï¼Œæå‡ºäº†ç›’çº§è¯­ä¹‰æè¿°ç¬¦å’Œè¯­ä¹‰ç –çš„æ¦‚å¿µã€‚</li>
<li>æ”¹è¿›åçš„æ¨¡å‹Text4Seg++åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c6d2abda858261cc3b44e658d6e65ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-470ed8bf3aa6d3e51f6a22c45578da6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3596fea815596ae7b897ca32e3209344.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9260ff9efdc0d1cb7bbfba3bbb99eae5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d5815891fcef3fe72a47b73a60efb9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eac7323a3f7a735fe0ccd8de63f2e1f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b1119f1a8f6c045dae93960c7d93ef7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reasoning-Language-Model-for-Personalized-Lung-Cancer-Screening"><a href="#Reasoning-Language-Model-for-Personalized-Lung-Cancer-Screening" class="headerlink" title="Reasoning Language Model for Personalized Lung Cancer Screening"></a>Reasoning Language Model for Personalized Lung Cancer Screening</h2><p><strong>Authors:Chuang Niu, Ge Wang</strong></p>
<p>Accurate risk assessment in lung cancer screening is critical for enabling early cancer detection and minimizing unnecessary invasive procedures. The Lung CT Screening Reporting and Data System (Lung-RADS) has been widely used as the standard framework for patient management and follow-up. Nevertheless, Lung-RADS faces trade-offs between sensitivity and specificity, as it stratifies risk solely based on lung nodule characteristics without incorporating various risk factors. Here we propose a reasoning language model (RLM) to integrate radiology findings with longitudinal medical records for individualized lung cancer risk assessment. Through a systematic study including dataset construction and distillation, supervised fine-tuning, reinforcement learning, and comprehensive evaluation, our model makes significant improvements in risk prediction performance on datasets in the national lung screening trial. Notably, RLM can decompose the risk evaluation task into sub-components, analyze the contributions of diverse risk factors, and synthesize them into a final risk score computed using our data-driven system equation. Our approach improves both predictive accuracy and monitorability through the chain of thought reasoning process, thereby facilitating clinical translation into lung cancer screening. </p>
<blockquote>
<p>åœ¨è‚ºç™Œç­›æŸ¥ä¸­è¿›è¡Œå‡†ç¡®çš„é£é™©è¯„ä¼°å¯¹äºå®ç°æ—©æœŸç™Œç—‡æ£€æµ‹å¹¶å°½é‡å‡å°‘ä¸å¿…è¦çš„ä¾µå…¥æ€§ç¨‹åºè‡³å…³é‡è¦ã€‚è‚ºéƒ¨CTç­›æŸ¥æŠ¥å‘Šå’Œæ•°æ®ç³»ç»Ÿï¼ˆLung-RADSï¼‰å·²å¹¿æ³›åº”ç”¨äºæ‚£è€…ç®¡ç†å’Œéšè®¿çš„æ ‡å‡†æ¡†æ¶ã€‚ç„¶è€Œï¼ŒLung-RADSåœ¨çµæ•åº¦å’Œç‰¹å¼‚åº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå› ä¸ºå®ƒä»…ä»…åŸºäºè‚ºç»“èŠ‚ç‰¹å¾è¿›è¡Œé£é™©åˆ†å±‚ï¼Œè€Œæ²¡æœ‰çº³å…¥å„ç§é£é™©å› ç´ ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰ï¼Œä»¥æ•´åˆæ”¾å°„å­¦æ£€æŸ¥ç»“æœä¸çºµå‘åŒ»ç–—è®°å½•ï¼Œè¿›è¡Œä¸ªæ€§åŒ–çš„è‚ºç™Œé£é™©è¯„ä¼°ã€‚é€šè¿‡åŒ…æ‹¬æ•°æ®é›†æ„å»ºå’Œè’¸é¦ã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œç»¼åˆè¯„ä¼°çš„ç³»ç»Ÿç ”ç©¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å›½å®¶è‚ºéƒ¨ç­›æŸ¥è¯•éªŒçš„æ•°æ®é›†ä¸Šé£é™©é¢„æµ‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRLMå¯ä»¥å°†é£é™©è¯„ä¼°ä»»åŠ¡åˆ†è§£ä¸ºå­ç»„ä»¶ï¼Œåˆ†æå„ç§é£é™©å› ç´ çš„è´¡çŒ®ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é©±åŠ¨ç³»ç»Ÿæ–¹ç¨‹å°†å…¶åˆå¹¶æˆæœ€ç»ˆé£é™©åˆ†æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ€ç»´æ¨ç†è¿‡ç¨‹æé«˜äº†é¢„æµ‹ç²¾åº¦å’Œå¯ç›‘æ§æ€§ï¼Œä»è€Œä¿ƒè¿›äº†ä¸´åºŠåº”ç”¨äºè‚ºç™Œç­›æŸ¥çš„è½¬åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06169v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è‚ºç™Œç­›æŸ¥ä¸­å‡†ç¡®çš„é£é™©è¯„ä¼°å¯¹äºå®ç°æ—©æœŸç™Œç—‡æ£€æµ‹å¹¶æœ€å°åŒ–ä¸å¿…è¦çš„ä¾µå…¥æ€§ç¨‹åºè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§èåˆæ”¾å°„å­¦æ£€æŸ¥ç»“æœä¸çºµå‘åŒ»ç–—è®°å½•çš„æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰ï¼Œç”¨äºä¸ªæ€§åŒ–è‚ºç™Œé£é™©è¯„ä¼°ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•°æ®é›†æ„å»ºã€è’¸é¦ã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œç»¼åˆè¯„ä»·çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œåœ¨å›½å®¶çº§è‚ºç™Œç­›æŸ¥è¯•éªŒæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜é£é™©é¢„æµ‹æ€§èƒ½ã€‚RLMèƒ½åˆ†è§£é£é™©è¯„ä¼°ä»»åŠ¡ï¼Œåˆ†æå„ç§é£é™©å› ç´ çš„è´¡çŒ®ï¼Œå¹¶é€šè¿‡æ•°æ®é©±åŠ¨çš„ç³»ç»Ÿæ–¹ç¨‹åˆæˆæœ€ç»ˆé£é™©åˆ†æ•°ï¼Œæé«˜é¢„æµ‹ç²¾åº¦å’Œç›‘æ§èƒ½åŠ›ï¼Œæœ‰åŠ©äºä¸´åºŠåº”ç”¨äºè‚ºç™Œç­›æŸ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œç­›æŸ¥ä¸­çš„é£é™©è¯„ä¼°å¯¹äºæ—©æœŸç™Œç—‡æ£€æµ‹å’Œå‡å°‘ä¸å¿…è¦çš„ä¾µå…¥æ€§ç¨‹åºè‡³å…³é‡è¦ã€‚</li>
<li>Lung-RADSä½œä¸ºæ‚£è€…ç®¡ç†å’Œéšè®¿çš„æ ‡å‡†æ¡†æ¶ï¼Œä½†åœ¨é£é™©è¯„ä¼°æ–¹é¢å­˜åœ¨çµæ•åº¦å’Œç‰¹å¼‚åº¦çš„æƒè¡¡ã€‚</li>
<li>æå‡ºäº†èåˆæ”¾å°„å­¦æ£€æŸ¥ç»“æœä¸çºµå‘åŒ»ç–—è®°å½•çš„æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰è¿›è¡Œä¸ªæ€§åŒ–è‚ºç™Œé£é™©è¯„ä¼°ã€‚</li>
<li>RLMé€šè¿‡åˆ†è§£é£é™©è¯„ä¼°ä»»åŠ¡ï¼Œåˆ†æå¤šç§é£é™©å› ç´ ï¼Œå¹¶é€šè¿‡æ•°æ®é©±åŠ¨çš„ç³»ç»Ÿæ–¹ç¨‹åˆæˆæœ€ç»ˆé£é™©åˆ†æ•°ã€‚</li>
<li>RLMåœ¨å›½å®¶çº§è‚ºç™Œç­›æŸ¥è¯•éªŒæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜é£é™©é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>RLMèƒ½æé«˜é¢„æµ‹ç²¾åº¦å’Œç›‘æ§èƒ½åŠ›ï¼Œæœ‰åŠ©äºä¸´åºŠåº”ç”¨äºè‚ºç™Œç­›æŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0a3339eae3036689caead58425a85ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a59003574f57ed38667dc90bb8a98531.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedSeqFT-Sequential-Fine-tuning-Foundation-Models-for-3D-Medical-Image-Segmentation"><a href="#MedSeqFT-Sequential-Fine-tuning-Foundation-Models-for-3D-Medical-Image-Segmentation" class="headerlink" title="MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image   Segmentation"></a>MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image   Segmentation</h2><p><strong>Authors:Yiwen Ye, Yicheng Wu, Xiangde Luo, He Zhang, Ziyang Chen, Ting Dang, Yanning Zhang, Yong Xia</strong></p>
<p>Foundation models have become a promising paradigm for advancing medical image analysis, particularly for segmentation tasks where downstream applications often emerge sequentially. Existing fine-tuning strategies, however, remain limited: parallel fine-tuning isolates tasks and fails to exploit shared knowledge, while multi-task fine-tuning requires simultaneous access to all datasets and struggles with incremental task integration. To address these challenges, we propose MedSeqFT, a sequential fine-tuning framework that progressively adapts pre-trained models to new tasks while refining their representational capacity. MedSeqFT introduces two core components: (1) Maximum Data Similarity (MDS) selection, which identifies downstream samples most representative of the original pre-training distribution to preserve general knowledge, and (2) Knowledge and Generalization Retention Fine-Tuning (K&amp;G RFT), a LoRA-based knowledge distillation scheme that balances task-specific adaptation with the retention of pre-trained knowledge. Extensive experiments on two multi-task datasets covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently outperforms state-of-the-art fine-tuning strategies, yielding substantial performance gains (e.g., an average Dice improvement of 3.0%). Furthermore, evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT enhances transferability, particularly for tumor segmentation. Visual analyses of loss landscapes and parameter variations further highlight the robustness of MedSeqFT. These results establish sequential fine-tuning as an effective, knowledge-retentive paradigm for adapting foundation models to evolving clinical tasks. Code will be released. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºä¸€ä¸ªå‰æ™¯å…‰æ˜çš„æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç»å¸¸ä¾æ¬¡å‡ºç°ä¸‹æ¸¸åº”ç”¨çš„åˆ†å‰²ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¾®è°ƒç­–ç•¥ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼šå¹¶è¡Œå¾®è°ƒéš”ç¦»ä»»åŠ¡å¹¶ä¸”æ— æ³•åˆ©ç”¨å…±äº«çŸ¥è¯†ï¼Œè€Œå¤šä»»åŠ¡å¾®è°ƒéœ€è¦åŒæ—¶è®¿é—®æ‰€æœ‰æ•°æ®é›†å¹¶ä¸”éš¾ä»¥è¿›è¡Œå¢é‡ä»»åŠ¡é›†æˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedSeqFTï¼Œè¿™æ˜¯ä¸€ä¸ªé¡ºåºå¾®è°ƒæ¡†æ¶ï¼Œå¯ä»¥é€æ­¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ä»¥åº”å¯¹æ–°ä»»åŠ¡ï¼ŒåŒæ—¶æé«˜å…¶ä»£è¡¨æ€§èƒ½åŠ›ã€‚MedSeqFTå¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰æœ€å¤§æ•°æ®ç›¸ä¼¼æ€§ï¼ˆMDSï¼‰é€‰æ‹©ï¼Œå®ƒç¡®å®šäº†æœ€èƒ½ä»£è¡¨åŸå§‹é¢„è®­ç»ƒåˆ†å¸ƒçš„ä¸‹æ¸¸æ ·æœ¬ï¼Œä»¥ä¿ç•™ä¸€èˆ¬çŸ¥è¯†ï¼›ï¼ˆ2ï¼‰çŸ¥è¯†ä¸æ³›åŒ–ä¿ç•™å¾®è°ƒï¼ˆK&amp;G RFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLoRAçš„çŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œå¹³è¡¡äº†é’ˆå¯¹ä»»åŠ¡çš„é€‚åº”æ€§å’Œä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚åœ¨ä¸¤ä¸ªå¤šä»»åŠ¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒæ¶µç›–äº†åä¸ª3Dåˆ†å‰²ä»»åŠ¡ï¼Œè¯æ˜MedSeqFTå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„å¾®è°ƒç­–ç•¥ï¼Œäº§ç”Ÿäº†å®è´¨æ€§çš„æ€§èƒ½æå‡ï¼ˆä¾‹å¦‚ï¼Œå¹³å‡Diceç³»æ•°æé«˜äº†3.0%ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤ä¸ªæœªè§ä»»åŠ¡ï¼ˆCOVID-19-20å’Œè‚¾è„ï¼‰ä¸Šçš„è¯„ä¼°è¯å®ï¼ŒMedSeqFTæé«˜äº†å¯è¿ç§»æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè‚¿ç˜¤åˆ†å‰²ã€‚æŸå¤±æ™¯è§‚å’Œå‚æ•°å˜åŒ–çš„è§†è§‰åˆ†æè¿›ä¸€æ­¥çªå‡ºäº†MedSeqFTçš„ç¨³å¥æ€§ã€‚è¿™äº›ç»“æœè¯æ˜äº†é¡ºåºå¾®è°ƒæ˜¯ä¸€ç§æœ‰æ•ˆä¸”çŸ¥è¯†ä¿ç•™æ€§å¼ºçš„æ¨¡å¼ï¼Œé€‚ç”¨äºå°†åŸºç¡€æ¨¡å‹é€‚åº”äºä¸æ–­å‘å±•çš„ä¸´åºŠä»»åŠ¡ã€‚ä»£ç å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06096v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œé‡‡ç”¨åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰è¿›è¡Œç ”ç©¶çš„ç°çŠ¶åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰å¾®è°ƒç­–ç•¥ï¼ˆfine-tuningï¼‰çš„å±€é™æ€§ï¼Œæå‡ºMedSeqFTæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼šæœ€å¤§æ•°æ®ç›¸ä¼¼æ€§ï¼ˆMDSï¼‰é€‰æ‹©å’ŒçŸ¥è¯†åŠæ³›åŒ–èƒ½åŠ›ä¿ç•™å¾®è°ƒï¼ˆK&amp;G RFTï¼‰ã€‚MedSeqFTå¯å®ç°åŸºç¡€æ¨¡å‹çš„æ¸è¿›é€‚åº”ï¼Œå¹¶ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†çš„ä¸€èˆ¬æ€§ã€‚é€šè¿‡å®éªŒè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚MedSeqFTç›¸å¯¹äºå½“å‰ä¸»æµçš„å¾®è°ƒç­–ç•¥å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œå®ç°äº†å·¨å¤§çš„æ€§èƒ½æå‡ï¼ˆä¾‹å¦‚å¹³å‡Diceç³»æ•°æé«˜3.0%ï¼‰ã€‚å¯¹ä¸¤ä¸ªæœªè§ä»»åŠ¡ï¼ˆCOVID-19-20å’Œè‚¾è„åˆ†å‰²ï¼‰çš„è¯„ä¼°ä¹ŸéªŒè¯äº†å…¶å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ã€‚ä»£ç å³å°†å‘å¸ƒã€‚è¯¥æ‘˜è¦ç®€æ˜æ‰¼è¦åœ°æ€»ç»“äº†æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>åŸºç¡€æ¨¡å‹å·²æˆä¸ºæ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æè¿›æ­¥çš„æœ‰æœ›èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œä¸‹æ¸¸åº”ç”¨å¸¸å¸¸é™†ç»­å‡ºç°ã€‚</li>
<li>ç°æœ‰å¾®è°ƒç­–ç•¥å­˜åœ¨å±€é™æ€§ï¼šå¹³è¡Œå¾®è°ƒå­¤ç«‹ä»»åŠ¡ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨å…±äº«çŸ¥è¯†ï¼Œå¤šä»»åŠ¡å¾®è°ƒåˆ™éœ€è¦åŒæ—¶è®¿é—®æ‰€æœ‰æ•°æ®é›†å¹¶åœ¨é›†æˆæ–°å¢ä»»åŠ¡æ—¶é¢ä¸´å›°éš¾ã€‚</li>
<li>MedSeqFTæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡æ¸è¿›é€‚åº”æ–°ä»»åŠ¡å¹¶å®Œå–„å…¶è¡¨å¾èƒ½åŠ›æ¥ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹ã€‚æ¡†æ¶åŒ…å«ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼šæœ€å¤§æ•°æ®ç›¸ä¼¼æ€§é€‰æ‹©å’ŒçŸ¥è¯†åŠæ³›åŒ–èƒ½åŠ›ä¿ç•™å¾®è°ƒã€‚</li>
<li>å®éªŒè¯æ˜MedSeqFTåœ¨å¤šä»»åŠ¡æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¾®è°ƒç­–ç•¥ï¼Œå®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼ˆå¦‚å¹³å‡Diceç³»æ•°æé«˜3.0%ï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d15f395bc3b16bd5305fa72cae5069e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78a8cc56402a3596894b62e30a2b59f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64403b54f9a70a361b6ab2d78f98b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63d4a9bd73dca7a13971bb851c6ada94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53212d4f3deee30a933417ab8cefd845.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40932ecab319d3a6067420603bee3e24.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="High-Quality-Tomographic-Image-Reconstruction-Integrating-Neural-Networks-and-Mathematical-Optimization"><a href="#High-Quality-Tomographic-Image-Reconstruction-Integrating-Neural-Networks-and-Mathematical-Optimization" class="headerlink" title="High-Quality Tomographic Image Reconstruction Integrating Neural   Networks and Mathematical Optimization"></a>High-Quality Tomographic Image Reconstruction Integrating Neural   Networks and Mathematical Optimization</h2><p><strong>Authors:Anuraag Mishra, Andrea Gilch, Benjamin Apeleo Zubiri, Jan Rolfes, Frauke Liers</strong></p>
<p>In this work, we develop a novel technique for reconstructing images from projection-based nano- and microtomography. Our contribution focuses on enhancing reconstruction quality, particularly for specimen composed of homogeneous material phases connected by sharp edges. This is accomplished by training a neural network to identify edges within subpictures. The trained network is then integrated into a mathematical optimization model, to reduce artifacts from previous reconstructions. To this end, the optimization approach favors solutions according to the learned predictions, however may also determine alternative solutions if these are strongly supported by the raw data. Hence, our technique successfully incorporates knowledge about the homogeneity and presence of sharp edges in the sample and thereby eliminates blurriness. Our results on experimental datasets show significant enhancements in interface sharpness and material homogeneity compared to benchmark algorithms. Thus, our technique produces high-quality reconstructions, showcasing its potential for advancing tomographic imaging techniques. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæŠ•å½±çº³ç±³å’Œæ˜¾å¾®å±‚ææˆåƒé‡å»ºå›¾åƒçš„æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦é›†ä¸­åœ¨æé«˜é‡å»ºè´¨é‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”±é”åˆ©è¾¹ç¼˜è¿æ¥è€Œæˆçš„å‡åŒ€ææ–™ç›¸ç»„æˆçš„æ ·æœ¬ã€‚è¿™æ˜¯é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œæ¥è¯†åˆ«å­å›¾åƒä¸­çš„è¾¹ç¼˜æ¥å®ç°çš„ã€‚ç„¶åï¼Œå°†è®­ç»ƒå¥½çš„ç½‘ç»œé›†æˆåˆ°æ•°å­¦ä¼˜åŒ–æ¨¡å‹ä¸­ï¼Œä»¥å‡å°‘å…ˆå‰é‡å»ºäº§ç”Ÿçš„ä¼ªå½±ã€‚ä¸ºæ­¤ï¼Œä¼˜åŒ–æ–¹æ³•ä¼šæ ¹æ®å­¦ä¹ åˆ°çš„é¢„æµ‹æ¥é€‰æ‹©è§£å†³æ–¹æ¡ˆï¼Œä½†å¦‚æœè¿™äº›é¢„æµ‹å¾—åˆ°åŸå§‹æ•°æ®çš„å¼ºçƒˆæ”¯æŒï¼Œä¹Ÿå¯èƒ½ä¼šç¡®å®šå…¶ä»–è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯æˆåŠŸåœ°å°†æ ·æœ¬ä¸­çš„å‡åŒ€æ€§å’Œé”åˆ©è¾¹ç¼˜çš„çŸ¥è¯†çº³å…¥å…¶ä¸­ï¼Œä»è€Œæ¶ˆé™¤äº†æ¨¡ç³Šæ€§ã€‚æˆ‘ä»¬åœ¨å®éªŒæ•°æ®é›†ä¸Šçš„ç»“æœæ˜¾ç¤ºï¼Œä¸åŸºå‡†ç®—æ³•ç›¸æ¯”ï¼Œç•Œé¢æ¸…æ™°åº¦å’Œææ–™å‡åŒ€æ€§æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯äº§ç”Ÿäº†é«˜è´¨é‡çš„é‡å»ºå›¾åƒï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›å±‚ææˆåƒæŠ€æœ¯æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06082v1">PDF</a> 36 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæŠ•å½±çº³ç±³å’Œå¾®å±‚ææˆåƒæŠ€æœ¯çš„æ–°å‹å›¾åƒé‡å»ºæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯æ—¨åœ¨æé«˜é‡å»ºè´¨é‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”±å‡åŒ€ææ–™ç›¸é€šè¿‡é”è¾¹è¿æ¥ç»„æˆçš„æ ·æœ¬ã€‚é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œæ¥è¯†åˆ«å­å›¾åƒä¸­çš„è¾¹ç¼˜ï¼Œå¹¶å°†å…¶é›†æˆåˆ°æ•°å­¦ä¼˜åŒ–æ¨¡å‹ä¸­ï¼Œä»¥å‡å°‘å…ˆå‰é‡å»ºä¸­çš„ä¼ªå½±ã€‚ä¼˜åŒ–æ–¹æ³•æ ¹æ®å­¦ä¹ é¢„æµ‹é€‰æ‹©è§£å†³æ–¹æ¡ˆï¼Œä½†å¦‚æœåŸå§‹æ•°æ®å¼ºçƒˆæ”¯æŒï¼Œä¹Ÿå¯èƒ½ä¼šç¡®å®šå…¶ä»–è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œè¯¥æŠ€æœ¯æˆåŠŸç»“åˆäº†æ ·æœ¬çš„å‡è´¨æ€§å’Œé”è¾¹å­˜åœ¨çŸ¥è¯†ï¼Œæ¶ˆé™¤äº†æ¨¡ç³Šæ€§ã€‚å¯¹å®éªŒæ•°æ®é›†çš„ç»“æœæ˜¾ç¤ºï¼Œä¸åŸºå‡†ç®—æ³•ç›¸æ¯”ï¼Œç•Œé¢æ¸…æ™°åº¦å’Œææ–™å‡è´¨æ€§æœ‰äº†æ˜¾è‘—æé«˜ã€‚å› æ­¤ï¼Œè¯¥æŠ€æœ¯äº§ç”Ÿé«˜è´¨é‡çš„é‡å»ºå›¾åƒï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›å±‚ææˆåƒæŠ€æœ¯æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘äº†åŸºäºæŠ•å½±çº³ç±³å’Œå¾®å±‚ææˆåƒçš„æ–°å‹å›¾åƒé‡å»ºæŠ€æœ¯ã€‚</li>
<li>æŠ€æœ¯é‡ç‚¹åœ¨äºæé«˜é‡å»ºè´¨é‡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç”±å‡åŒ€ææ–™ç›¸å’Œé”è¾¹ç»„æˆçš„æ ·æœ¬ã€‚</li>
<li>é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œè¯†åˆ«å­å›¾åƒä¸­çš„è¾¹ç¼˜ï¼Œé›†æˆåˆ°æ•°å­¦ä¼˜åŒ–æ¨¡å‹ä¸­ã€‚</li>
<li>ä¼˜åŒ–æ¨¡å‹èƒ½å‡å°‘ä¼ªå½±ï¼Œå¹¶æ ¹æ®å­¦ä¹ é¢„æµ‹é€‰æ‹©è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æŠ€æœ¯ç»“åˆäº†æ ·æœ¬çš„å‡è´¨æ€§å’Œé”è¾¹å­˜åœ¨çš„çŸ¥è¯†ï¼Œæ¶ˆé™¤äº†å›¾åƒæ¨¡ç³Šã€‚</li>
<li>å®éªŒæ•°æ®é›†ç»“æœæ˜¾ç¤ºï¼Œä¸åŸºå‡†ç®—æ³•ç›¸æ¯”ï¼Œç•Œé¢æ¸…æ™°åº¦å’Œææ–™å‡è´¨æ€§æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47a5e8af3b67744c4473fd78087edcbb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Strategy-Guided-Diffusion-via-Sparse-Masking-Temporal-Reweighting-Distribution-Correction"><a href="#Multi-Strategy-Guided-Diffusion-via-Sparse-Masking-Temporal-Reweighting-Distribution-Correction" class="headerlink" title="Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting   Distribution Correction"></a>Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting   Distribution Correction</h2><p><strong>Authors:Zekun Zhou, Yanru Gong, Liu Shi, Qiegen Liu</strong></p>
<p>Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºçš„ç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒç§¯åˆ†åˆ†å¸ƒä¼°è®¡å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”±ç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼çš„è”åˆè®­ç»ƒæœºåˆ¶ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚åŸºäºç³»ç»Ÿçš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´å˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨ä»çº¯å™ªå£°åˆ°çœŸå®å›¾åƒçš„æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚é‡‡ç”¨çº¿æ€§å›å½’æ¥æ ¡æ­£å·²çŸ¥æ•°æ®å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œä»¥åœ¨å¤šä¸ªå­é¢‘ç‡åˆ†é‡ä¸Šæ‰§è¡Œå…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œä»è€Œæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°äº†é«˜è´¨é‡å›¾åƒé‡å»ºã€‚åœ¨å…¬å…±å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéƒ½è¡¨æ˜ï¼Œæ‰€ææ–¹æ³•è¾ƒæœ€ä½³åŸºçº¿æ–¹æ³•åœ¨PSNRä¸Šæé«˜äº†2.58åˆ†è´ï¼ŒSSIMæé«˜äº†2.37%ï¼ŒMSEé™ä½äº†0.236ã€‚é‡å»ºçš„å›¾åƒåœ¨ç»“æ„ä¸€è‡´æ€§ã€ç»†èŠ‚æ¢å¤å’Œä¼ªå½±æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05992v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–æ¡ä»¶æ—¶åºåŠ æƒé›†æˆåˆ†å¸ƒä¼°è®¡å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ï¼Œç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚è¯¥æ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾çš„è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ï¼Œå¹¶å¼•å…¥æ—¶åºå˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ã€‚åŒæ—¶ï¼Œé‡‡ç”¨çº¿æ€§å›å½’æ ¡æ­£å·²çŸ¥å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œæ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„è¿›è¡Œå…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œæé«˜æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œå®ç°é«˜è´¨é‡å›¾åƒé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºç¨€ç–æ¡ä»¶æ—¶åºé‡åŠ æƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾çš„è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚</li>
<li>å¼•å…¥æ—¶åºå˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ã€‚</li>
<li>é‡‡ç”¨çº¿æ€§å›å½’æ ¡æ­£åˆ†å¸ƒåç§»ï¼Œæé«˜æ¨¡å‹æŒ‡å¯¼è¿‡ç¨‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œè¿›è¡Œå…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œæé«˜ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒèƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœåœ¨å…¬å…±å’ŒçœŸå®æ•°æ®é›†ä¸Šå‡æ˜¾ç¤ºå‡ºæ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒPSNRæé«˜2.58 dBï¼ŒSSIMå¢åŠ 2.37%ï¼ŒMSEé™ä½0.236ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2eb9393ad650011c0e2761f7b460752c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab165bb069aecbff4bd7b67289198e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bc9bc101b1141f2f23bc51e672363d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61369320df9cda5ce266b5ab1687fe9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6375f0b02934465ae1dbd273aa601f7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance"><a href="#Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance" class="headerlink" title="Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance"></a>Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance</h2><p><strong>Authors:Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel</strong></p>
<p>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brainâ€™s three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimerâ€™s disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§æ¡ä»¶ä¸‹ç”Ÿæˆ2Då›¾åƒçš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼›ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨2Dä¸­çš„å‡ºè‰²è¡¨ç°å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºå¤§é‡å¯è½»æ¾è·å–çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ”¯æ’‘ã€‚å…³é”®çš„æ˜¯ï¼Œ3Dé¢†åŸŸçš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¹¶ä¸å­˜åœ¨ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚å› æ­¤ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆä»…åŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„é«˜åˆ†è¾¨ç‡3Dåäº‹å®åŒ»å­¦å›¾åƒçš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚è§£å†³è¿™ä¸€å·®è·å°†èƒ½å¤Ÿæ¨åŠ¨å¼ºå¤§çš„ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ï¼Œå¦‚ä¸ªæ€§åŒ–åäº‹å®è§£é‡Šã€æ¨¡æ‹Ÿç–¾ç—…è¿›å±•æƒ…æ™¯ä»¥åŠé€šè¿‡è¯¦ç»†å¯è§†åŒ–å‡è®¾åŒ»å­¦çŠ¶å†µå¢å¼ºåŒ»å­¦åŸ¹è®­ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å¼•å…¥ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆç”±è‡ªç”±å½¢å¼è¯­è¨€æç¤ºå¼•å¯¼çš„é«˜åˆ†è¾¨ç‡3Dåäº‹å®åŒ»å­¦å›¾åƒï¼Œæœç€è§£å†³è¿™ä¸€æŒ‘æˆ˜è¿ˆå‡ºäº†æœ‰æ„ä¹‰çš„ä¸€æ­¥ã€‚æˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„3Dæ‰©æ•£æ¨¡å‹å¹¶èå…¥äº†ç®€å•æ‰©æ•£å¢å¼ºæ¡ä»¶ï¼Œä»¥æé«˜æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†è¯­è¨€å¼•å¯¼çš„åŸç”Ÿ3Dæ‰©æ•£æ¨¡å‹ä¸“é—¨åº”ç”¨äºç¥ç»æˆåƒæ•°æ®ï¼Œå¿ å®çš„ä¸‰ç»´å»ºæ¨¡å¯¹äºè¡¨ç¤ºå¤§è„‘çš„çš„ä¸‰ç»´ç»“æ„è‡³å…³é‡è¦ã€‚é€šè¿‡ä¸¤ä¸ªä¸åŒçš„ç¥ç»MRIæ•°æ®é›†çš„ç»“æœï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸæ¨¡æ‹Ÿäº†å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„å„ç§åäº‹å®ç—…ç¶è´Ÿè·ä»¥åŠé˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„è®¤çŸ¥çŠ¶æ€ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„åŒæ—¶ï¼Œä¿æŒåˆæˆåŒ»å­¦å›¾åƒä¸­ä¸»ä½“çš„ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸º3DåŒ»å­¦æˆåƒä¸­çš„æç¤ºé©±åŠ¨ç–¾ç—…è¿›å±•åˆ†æå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05978v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”ŸæˆäºŒç»´å›¾åƒæ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼Œå¼ºè°ƒäº†ç¼ºä¹ç›¸åº”çš„ä¸‰ç»´é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¯¹ä¸‰ç»´é¢†åŸŸçš„é™åˆ¶ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œèƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è™šæ„åŒ»ç–—å›¾åƒï¼Œæ˜¯æœç€è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„é‡è¦ä¸€æ­¥ã€‚è¯¥ç ”ç©¶é€‚åº”äº†å…ˆè¿›çš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†æ”¹è¿›ï¼Œé¦–æ¬¡å±•ç¤ºäº†åº”ç”¨äºç¥ç»æˆåƒæ•°æ®çš„è¯­è¨€å¼•å¯¼åŸç”Ÿä¸‰ç»´æ‰©æ•£æ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆåŠŸæ¨¡æ‹Ÿäº†å¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è™šæ„ç—…å˜è´Ÿè·å’Œè®¤çŸ¥çŠ¶æ€ï¼Œä¸ºä¸‰ç»´åŒ»å­¦å½±åƒçš„ç—…æƒ…è¿›å±•åˆ†æå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”ŸæˆäºŒç»´å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹ç›¸åº”çš„ä¸‰ç»´é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹é™åˆ¶äº†å…¶åœ¨ä¸‰ç»´é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œèƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è™šæ„åŒ»ç–—å›¾åƒã€‚</li>
<li>ç ”ç©¶é€‚åº”äº†å…ˆè¿›çš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†æ”¹è¿›ï¼Œä»¥æé«˜å›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†è¯­è¨€å¼•å¯¼çš„åŸç”Ÿä¸‰ç»´æ‰©æ•£æ¨¡å‹åº”ç”¨äºç¥ç»æˆåƒæ•°æ®ã€‚</li>
<li>è¯¥æ¡†æ¶æˆåŠŸæ¨¡æ‹Ÿäº†å¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è™šæ„ç—…å˜è´Ÿè·å’Œè®¤çŸ¥çŠ¶æ€ã€‚</li>
<li>ç”Ÿæˆçš„ä¸‰ç»´åŒ»ç–—å›¾åƒè´¨é‡é«˜ï¼Œä¸”ä¿æŒäº†ä¸ªä½“ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33d85c9938d3d69c2e7985a5ead7914c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84788cd2c6aaec08fb6b9eec83a92e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea24de25c382ba42b67043adb35d3b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-897e1f7c4fd0dbd99a0e12217777eed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe4a36818c6a34cca0fd5b3099d757ce.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dual-Interaction-Network-with-Cross-Image-Attention-for-Medical-Image-Segmentation"><a href="#Dual-Interaction-Network-with-Cross-Image-Attention-for-Medical-Image-Segmentation" class="headerlink" title="Dual Interaction Network with Cross-Image Attention for Medical Image   Segmentation"></a>Dual Interaction Network with Cross-Image Attention for Medical Image   Segmentation</h2><p><strong>Authors:Jeonghyun Noh, Wangsu Jeon, Jinsun Park</strong></p>
<p>Medical image segmentation is a crucial method for assisting professionals in diagnosing various diseases through medical imaging. However, various factors such as noise, blurriness, and low contrast often hinder the accurate diagnosis of diseases. While numerous image enhancement techniques can mitigate these issues, they may also alter crucial information needed for accurate diagnosis in the original image. Conventional image fusion strategies, such as feature concatenation can address this challenge. However, they struggle to fully leverage the advantages of both original and enhanced images while suppressing the side effects of the enhancements. To overcome the problem, we propose a dual interactive fusion module (DIFM) that effectively exploits mutual complementary information from the original and enhanced images. DIFM employs cross-attention bidirectionally to simultaneously attend to corresponding spatial information across different images, subsequently refining the complementary features via global spatial attention. This interaction leverages low- to high-level features implicitly associated with diverse structural attributes like edges, blobs, and object shapes, resulting in enhanced features that embody important spatial characteristics. In addition, we introduce a multi-scale boundary loss based on gradient extraction to improve segmentation accuracy at object boundaries. Experimental results on the ACDC and Synapse datasets demonstrate the superiority of the proposed method quantitatively and qualitatively. Code available at: <a target="_blank" rel="noopener" href="https://github.com/JJeong-Gari/DIN">https://github.com/JJeong-Gari/DIN</a> </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯é€šè¿‡åŒ»å­¦å½±åƒè¾…åŠ©ä¸“ä¸šäººå£«è¯Šæ–­å„ç§ç–¾ç—…çš„é‡è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œå™ªå£°ã€æ¨¡ç³Šå’Œä½å¯¹æ¯”åº¦ç­‰å› ç´ ç»å¸¸é˜»ç¢ç–¾ç—…çš„å‡†ç¡®è¯Šæ–­ã€‚è™½ç„¶æœ‰è®¸å¤šå›¾åƒå¢å¼ºæŠ€æœ¯å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½ä¼šæ”¹å˜åŸå§‹å›¾åƒä¸­ç”¨äºå‡†ç¡®è¯Šæ–­çš„å…³é”®ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„å›¾åƒèåˆç­–ç•¥ï¼Œå¦‚ç‰¹å¾ä¸²è”ï¼Œå¯ä»¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å……åˆ†åˆ©ç”¨åŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒçš„ä¼˜åŠ¿æ—¶é‡åˆ°äº†å›°éš¾ï¼ŒåŒæ—¶ä¹ŸæŠ‘åˆ¶äº†å¢å¼ºçš„å‰¯ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒé‡äº¤äº’èåˆæ¨¡å—ï¼ˆDIFMï¼‰ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°åˆ©ç”¨åŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ã€‚DIFMé‡‡ç”¨åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥åŒæ—¶å…³æ³¨ä¸åŒå›¾åƒä¸­ç›¸åº”çš„ç©ºé—´ä¿¡æ¯ï¼Œç„¶åé€šè¿‡å…¨å±€ç©ºé—´æ³¨æ„åŠ›å¯¹äº’è¡¥ç‰¹å¾è¿›è¡Œç»†åŒ–ã€‚è¿™ç§äº¤äº’åˆ©ç”¨ä¸è¾¹ç¼˜ã€æ–‘å—å’Œå¯¹è±¡å½¢çŠ¶ç­‰å¤šæ ·ç»“æ„å±æ€§ç›¸å…³çš„ä½è‡³é«˜å±‚æ¬¡çš„ç‰¹å¾ï¼Œäº§ç”ŸåŒ…å«é‡è¦ç©ºé—´ç‰¹å¾çš„å¢å¼ºç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ¢¯åº¦æå–çš„å¤šå°ºåº¦è¾¹ç•ŒæŸå¤±ï¼Œä»¥æé«˜å¯¹è±¡è¾¹ç•Œçš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨ACDCå’ŒSynapseæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœä»å®šé‡å’Œå®šæ€§ä¸¤ä¸ªæ–¹é¢è¯æ˜äº†æ‰€ææ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/JJeong-Gari/DIN">https://github.com/JJeong-Gari/DIN</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05953v1">PDF</a> 16pages</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ååŠ©ä¸“ä¸šäººå‘˜è¿›è¡Œç–¾ç—…è¯Šæ–­çš„é‡è¦æ–¹æ³•ï¼Œä½†å™ªå£°ã€æ¨¡ç³Šå’Œä½å¯¹æ¯”åº¦ç­‰å› ç´ ä¼šå½±å“è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ä¸ºæ”¹å–„è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºä¸€ç§åŒé‡äº¤äº’èåˆæ¨¡å—ï¼ˆDIFMï¼‰ï¼Œè¯¥æ¨¡å—èƒ½æœ‰æ•ˆåˆ©ç”¨åŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œé€šè¿‡è·¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶å…³æ³¨ä¸åŒå›¾åƒä¸­å¯¹åº”çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶ç»ç”±å…¨å±€ç©ºé—´æ³¨æ„åŠ›å¯¹äº’è¡¥ç‰¹å¾è¿›è¡Œç²¾ç‚¼ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥åŸºäºæ¢¯åº¦æå–çš„å¤šå°ºåº¦è¾¹ç•ŒæŸå¤±ï¼Œä»¥æé«˜å¯¹è±¡è¾¹ç•Œçš„åˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ACDCå’ŒSynapseæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„å®šé‡å’Œå®šæ€§æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ç–¾ç—…è¯Šæ–­ä¸­èµ·é‡è¦ä½œç”¨ï¼Œä½†å™ªå£°ã€æ¨¡ç³Šå’Œä½å¯¹æ¯”åº¦ä¼šå½±å“è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>åŒé‡äº¤äº’èåˆæ¨¡å—ï¼ˆDIFMï¼‰èƒ½æœ‰æ•ˆåˆ©ç”¨åŸå§‹å’Œå¢å¼ºå›¾åƒçš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>DIFMé€šè¿‡è·¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨ä¸åŒå›¾åƒä¸­å¯¹åº”çš„ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>DIFMé€šè¿‡å…¨å±€ç©ºé—´æ³¨æ„åŠ›å¯¹äº’è¡¥ç‰¹å¾è¿›è¡Œç²¾ç‚¼ã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦è¾¹ç•ŒæŸå¤±ä»¥æé«˜å¯¹è±¡è¾¹ç•Œçš„åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>å®éªŒç»“æœåœ¨ACDCå’ŒSynapseæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec687adbce76db47ff59bcc38d17a686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e00a471041f261601b94e7ffdd3b888.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1de1bcd4c09af64a2d8d9428f245d6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eeba9f8dc877ead654989afa6c7a5b6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Brain-Tumor-Detection-Through-Diverse-CNN-Architectures-in-IoT-Healthcare-Industries-Fast-R-CNN-U-Net-Transfer-Learning-Based-CNN-and-Fully-Connected-CNN"><a href="#Brain-Tumor-Detection-Through-Diverse-CNN-Architectures-in-IoT-Healthcare-Industries-Fast-R-CNN-U-Net-Transfer-Learning-Based-CNN-and-Fully-Connected-CNN" class="headerlink" title="Brain Tumor Detection Through Diverse CNN Architectures in IoT   Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and   Fully Connected CNN"></a>Brain Tumor Detection Through Diverse CNN Architectures in IoT   Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and   Fully Connected CNN</h2><p><strong>Authors:Mohsen Asghari Ilani, Yaser M. Banad</strong></p>
<p>Artificial intelligence (AI)-powered deep learning has advanced brain tumor diagnosis in Internet of Things (IoT)-healthcare systems, achieving high accuracy with large datasets. Brain health is critical to human life, and accurate diagnosis is essential for effective treatment. Magnetic Resonance Imaging (MRI) provides key data for brain tumor detection, serving as a major source of big data for AI-driven image classification. In this study, we classified glioma, meningioma, and pituitary tumors from MRI images using Region-based Convolutional Neural Network (R-CNN) and UNet architectures. We also applied Convolutional Neural Networks (CNN) and CNN-based transfer learning models such as Inception-V3, EfficientNetB4, and VGG19. Model performance was assessed using F-score, recall, precision, and accuracy. The Fast R-CNN achieved the best results with 99% accuracy, 98.5% F-score, 99.5% Area Under the Curve (AUC), 99.4% recall, and 98.5% precision. Combining R-CNN, UNet, and transfer learning enables earlier diagnosis and more effective treatment in IoT-healthcare systems, improving patient outcomes. IoT devices such as wearable monitors and smart imaging systems continuously collect real-time data, which AI algorithms analyze to provide immediate insights for timely interventions and personalized care. For external cohort cross-dataset validation, EfficientNetB2 achieved the strongest performance among fine-tuned EfficientNet models, with 92.11% precision, 92.11% recall&#x2F;sensitivity, 95.96% specificity, 92.02% F1-score, and 92.23% accuracy. These findings underscore the robustness and reliability of AI models in handling diverse datasets, reinforcing their potential to enhance brain tumor classification and patient care in IoT healthcare environments. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ·±åº¦å­¦ä¹ åœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰åŒ»ç–—ç³»ç»Ÿä¸­æ¨åŠ¨äº†è„‘è‚¿ç˜¤è¯Šæ–­çš„è¿›æ­¥ï¼Œé€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†å®ç°äº†é«˜å‡†ç¡®æ€§ã€‚å¤§è„‘å¥åº·å¯¹äººç±»çš„ç”Ÿå‘½è‡³å…³é‡è¦ï¼Œè€Œå‡†ç¡®çš„è¯Šæ–­åˆ™æ˜¯æœ‰æ•ˆæ²»ç–—çš„å…³é”®ã€‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸ºè„‘è‚¿ç˜¤çš„æ£€æµ‹æä¾›äº†å…³é”®æ•°æ®ï¼Œæˆä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨å›¾åƒåˆ†ç±»çš„å¤§æ•°æ®æ¥æºä¹‹ä¸€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºåŒºåŸŸçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆR-CNNï¼‰å’ŒUNetæ¶æ„å¯¹èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤è¿›è¡ŒMRIå›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŸºäºCNNçš„è¿ç§»å­¦ä¹ æ¨¡å‹ï¼Œå¦‚Inception-V3ã€EfficientNetB4å’ŒVGG19ã€‚ä½¿ç”¨Fåˆ†æ•°ã€å¬å›ç‡ã€ç²¾ç¡®åº¦å’Œå‡†ç¡®åº¦æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚Fast R-CNNå–å¾—äº†æœ€ä½³ç»“æœï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†99%ï¼ŒFåˆ†æ•°ä¸º98.5%ï¼Œæ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸º99.5%ï¼Œå¬å›ç‡ä¸º99.4%ï¼Œç²¾ç¡®åº¦ä¸º98.5%ã€‚ç»“åˆR-CNNã€UNetå’Œè¿ç§»å­¦ä¹ èƒ½å¤Ÿåœ¨ç‰©è”ç½‘åŒ»ç–—ç³»ç»Ÿä¸­å®ç°æ—©æœŸè¯Šæ–­å’Œæ²»ç–—ï¼Œæé«˜æ‚£è€…æ²»ç–—æ•ˆæœã€‚ç‰©è”ç½‘è®¾å¤‡å¦‚å¯ç©¿æˆ´ç›‘æµ‹ä»ªå’Œæ™ºèƒ½æˆåƒç³»ç»Ÿä¸æ–­æ”¶é›†å®æ—¶æ•°æ®ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•åˆ†æè¿™äº›æ•°æ®ä»¥æä¾›å³æ—¶è§è§£ï¼Œä¸ºåŠæ—¶å¹²é¢„å’Œä¸ªæ€§åŒ–æŠ¤ç†æä¾›æŒ‡å¯¼ã€‚å¯¹äºå¤–éƒ¨é˜Ÿåˆ—è·¨æ•°æ®é›†éªŒè¯ï¼Œç»è¿‡å¾®è°ƒåçš„EfficientNetB2æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå…¶ç²¾ç¡®åº¦è¾¾åˆ°äº†92.11%ï¼Œå¬å›ç‡&#x2F;çµæ•åº¦ä¸º92.11%ï¼Œç‰¹å¼‚æ€§ä¸º95.96%ï¼ŒF1åˆ†æ•°ä¸º92.02%ï¼Œå‡†ç¡®ç‡ä¸º92.23%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ•°æ®é›†æ–¹é¢çš„ç¨³å¥æ€§å’Œå¯é æ€§ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨ç‰©è”ç½‘åŒ»ç–—ç¯å¢ƒä¸­æé«˜è„‘è‚¿ç˜¤åˆ†ç±»å’Œæ‚£è€…æŠ¤ç†æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05821v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºäººå·¥æ™ºèƒ½æ·±åº¦å­¦ä¹ çš„ç‰©è”ç½‘åŒ»ç–—ç³»ç»Ÿä¸­è„‘è‚¿ç˜¤è¯Šæ–­æŠ€æœ¯å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†å®ç°é«˜å‡†ç¡®ç‡ã€‚æœ¬ç ”ç©¶åˆ©ç”¨åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œï¼ˆR-CNNï¼‰å’ŒUNetæ¶æ„å¯¹èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤è¿›è¡ŒMRIå›¾åƒåˆ†ç±»ï¼ŒåŒæ—¶åº”ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŸºäºCNNçš„è¿ç§»å­¦ä¹ æ¨¡å‹å¦‚Inception-V3ã€EfficientNetB4å’ŒVGG19ã€‚å…¶ä¸­Fast R-CNNè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º99%ï¼Œç»“åˆR-CNNã€UNetå’Œè¿ç§»å­¦ä¹ èƒ½åœ¨ç‰©è”ç½‘åŒ»ç–—ç³»ç»Ÿä¸­å®ç°æ—©æœŸè¯Šæ–­å’Œæœ‰æ•ˆæ²»ç–—ï¼Œæ”¹å–„æ‚£è€…é¢„åã€‚æ­¤å¤–ï¼Œç‰©è”ç½‘è®¾å¤‡å¦‚å¯ç©¿æˆ´ç›‘æŠ¤ä»ªå’Œæ™ºèƒ½æˆåƒç³»ç»Ÿå¯å®æ—¶æ”¶é›†æ•°æ®ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•åˆ†æè¿™äº›æ•°æ®ï¼Œä¸ºåŠæ—¶å¹²é¢„å’Œä¸ªæ€§åŒ–æŠ¤ç†æä¾›å³æ—¶è§è§£ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºEfficientNetB2æ¨¡å‹åœ¨å¤–éƒ¨é˜Ÿåˆ—è·¨æ•°æ®é›†éªŒè¯ä¸­è¡¨ç°æœ€ä½³ï¼Œçªæ˜¾äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å¤„ç†å¤šæ ·æ•°æ®é›†æ–¹é¢çš„ç¨³å¥æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰åœ¨ç‰©è”ç½‘åŒ»ç–—ç¯å¢ƒä¸­æå‡è„‘è‚¿ç˜¤åˆ†ç±»å’Œæ‚£è€…æŠ¤ç†çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äººå·¥æ™ºèƒ½æ·±åº¦å­¦ä¹ åœ¨ç‰©è”ç½‘åŒ»ç–—ç³»ç»Ÿçš„è„‘è‚¿ç˜¤è¯Šæ–­ä¸­å–å¾—é«˜å‡†ç¡®ç‡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨R-CNNå’ŒUNetè¿›è¡ŒMRIå›¾åƒåˆ†ç±»ï¼ŒåŒºåˆ†èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ã€‚</li>
<li>Fast R-CNNè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡é«˜è¾¾99%ã€‚</li>
<li>è¿ç§»å­¦ä¹ æ¨¡å‹å¦‚Inception-V3ã€EfficientNetB4å’ŒVGG19ä¹Ÿåº”ç”¨äºç ”ç©¶ã€‚</li>
<li>ç‰©è”ç½‘è®¾å¤‡å®æ—¶æ”¶é›†æ•°æ®ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•æä¾›å³æ—¶åˆ†æã€‚</li>
<li>EfficientNetB2æ¨¡å‹åœ¨è·¨æ•°æ®é›†éªŒè¯ä¸­è¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºäººå·¥æ™ºèƒ½æ¨¡å‹å¤„ç†å¤šæ ·æ•°æ®é›†çš„ç¨³å¥æ€§ã€‚</li>
<li>è¿™äº›æŠ€æœ¯ç»“åˆæœ‰åŠ©äºæ—©æœŸè¯Šæ–­å’Œæœ‰æ•ˆæ²»ç–—ï¼Œæ”¹å–„æ‚£è€…é¢„åã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fea88d1858cea40ae383da70eebc684d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-186dbe3ae10d267509f091d5aede7849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3e8a97f3070ff9d837fb5fc949b56d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c24d8021f60676104a51c2d1e725df33.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Probabilistic-Segment-Anything-Model-for-Ambiguity-Aware-Medical-Image-Segmentation"><a href="#A-Probabilistic-Segment-Anything-Model-for-Ambiguity-Aware-Medical-Image-Segmentation" class="headerlink" title="A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image   Segmentation"></a>A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image   Segmentation</h2><p><strong>Authors:Tyler Ward, Abdullah Imran</strong></p>
<p>Recent advances in promptable segmentation, such as the Segment Anything Model (SAM), have enabled flexible, high-quality mask generation across a wide range of visual domains. However, SAM and similar models remain fundamentally deterministic, producing a single segmentation per object per prompt, and fail to capture the inherent ambiguity present in many real-world tasks. This limitation is particularly troublesome in medical imaging, where multiple plausible segmentations may exist due to annotation uncertainty or inter-expert variability. In this paper, we introduce Probabilistic SAM, a probabilistic extension of SAM that models a distribution over segmentations conditioned on both the input image and prompt. By incorporating a latent variable space and training with a variational objective, our model learns to generate diverse and plausible segmentation masks reflecting the variability in human annotations. The architecture integrates a prior and posterior network into the SAM framework, allowing latent codes to modulate the prompt embeddings during inference. The latent space allows for efficient sampling during inference, enabling uncertainty-aware outputs with minimal overhead. We evaluate Probabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate its ability to produce diverse outputs that align with expert disagreement, outperforming existing probabilistic baselines on uncertainty-aware metrics. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/tbwa233/Probabilistic-SAM/">https://github.com/tbwa233/Probabilistic-SAM/</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæç¤ºåˆ†å‰²é¢†åŸŸçš„è¿›å±•ï¼Œå¦‚â€œä»»æ„åˆ†å‰²æ¨¡å‹â€ï¼ˆSAMï¼‰ï¼Œå·²ç»èƒ½å¤Ÿåœ¨å¹¿æ³›çš„è§†è§‰é¢†åŸŸå®ç°çµæ´»ã€é«˜è´¨é‡çš„é¢å…·ç”Ÿæˆã€‚ç„¶è€Œï¼ŒSAMå’Œç±»ä¼¼æ¨¡å‹ä»ç„¶æœ¬è´¨ä¸Šæ˜¯ç¡®å®šæ€§çš„ï¼Œæ¯æ¬¡æç¤ºåªå¯¹ä¸€ä¸ªå¯¹è±¡äº§ç”Ÿä¸€ä¸ªåˆ†å‰²ç»“æœï¼Œæ— æ³•æ•æ‰è®¸å¤šç°å®ä»»åŠ¡ä¸­å­˜åœ¨çš„å›ºæœ‰æ¨¡ç³Šæ€§ã€‚è¿™ä¸€å±€é™æ€§åœ¨åŒ»å­¦å½±åƒä¸­å°¤å…¶éº»çƒ¦ï¼Œç”±äºæ³¨é‡Šçš„ä¸ç¡®å®šæ€§æˆ–ä¸“å®¶ä¹‹é—´çš„å·®å¼‚ï¼Œå¯èƒ½å­˜åœ¨å¤šä¸ªåˆç†çš„åˆ†å‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¦‚ç‡SAMï¼Œè¿™æ˜¯SAMçš„æ¦‚ç‡æ‰©å±•ï¼Œå®ƒå¯¹è¾“å…¥å›¾åƒå’Œæç¤ºå»ºç«‹åˆ†å‰²åˆ†å¸ƒçš„æ¡ä»¶æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥æ½œåœ¨å˜é‡ç©ºé—´å¹¶ä½¿ç”¨å˜åˆ†ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šäº†ç”Ÿæˆåæ˜ äººç±»æ³¨é‡Šå˜å¼‚æ€§çš„å¤šæ ·åŒ–å’Œåˆç†åˆ†å‰²é¢å…·ã€‚è¯¥æ¶æ„å°†å…ˆéªŒç½‘ç»œå’ŒåéªŒç½‘ç»œé›†æˆåˆ°SAMæ¡†æ¶ä¸­ï¼Œå…è®¸æ½œåœ¨ä»£ç åœ¨æ¨ç†è¿‡ç¨‹ä¸­è°ƒåˆ¶æç¤ºåµŒå…¥ã€‚æ½œåœ¨ç©ºé—´å…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œæœ‰æ•ˆé‡‡æ ·ï¼Œä»¥æœ€ä½çš„å¼€é”€å®ç°å…·æœ‰ä¸ç¡®å®šæ€§çš„è¾“å‡ºã€‚æˆ‘ä»¬åœ¨å…¬å…±LIDC-IDRIè‚ºç»“èŠ‚æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ¦‚ç‡SAMï¼Œè¯æ˜äº†å…¶äº§ç”Ÿä¸ä¸“å®¶åˆ†æ­§ç›¸ç¬¦çš„å¤šæ ·è¾“å‡ºçš„èƒ½åŠ›ï¼Œåœ¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„æ¦‚ç‡åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š[<a target="_blank" rel="noopener" href="https://github.com/tbwa233/Probabilistic-SAM/%E3%80%82]">https://github.com/tbwa233/Probabilistic-SAM/ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05809v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹â€”â€”æ¦‚ç‡æ€§SAMï¼ˆProbabilistic Segment Anything Modelï¼‰è¢«æå‡ºã€‚è¯¥æ¨¡å‹å…‹æœäº†ä¼ ç»Ÿæ¨¡å‹çš„ç¡®å®šæ€§é—®é¢˜ï¼Œèƒ½å¯¹åŒä¸€å¯¹è±¡çš„å¤šä¸ªå¯èƒ½åˆ†å‰²ç»“æœå»ºæ¨¡ï¼Œé€‚åº”äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å› æ ‡æ³¨ä¸ç¡®å®šæ€§æˆ–ä¸“å®¶é—´å·®å¼‚äº§ç”Ÿçš„å›ºæœ‰æ¨¡ç³Šæ€§ã€‚å®ƒé€šè¿‡å¼•å…¥æ½œåœ¨å˜é‡ç©ºé—´å’Œå˜åˆ†ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆåæ˜ äººç±»æ ‡æ³¨å·®å¼‚çš„å¤šæ ·åŒ–å’Œåˆç†åˆ†å‰²æ©è†œã€‚æ¨¡å‹æ¶æ„å°†å…ˆéªŒç½‘ç»œå’ŒåéªŒç½‘ç»œèå…¥SAMæ¡†æ¶ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æ½œåœ¨ä»£ç è°ƒåˆ¶æç¤ºåµŒå…¥ã€‚æ½œåœ¨ç©ºé—´æé«˜äº†æ¨ç†é˜¶æ®µçš„é‡‡æ ·æ•ˆç‡ï¼Œå®ç°äº†å…·æœ‰æœ€å°å¼€é”€çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥è¾“å‡ºã€‚åœ¨å…¬å…±LIDC-IDRIè‚ºç»“èŠ‚æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæ¦‚ç‡æ€§SAMèƒ½å¤Ÿäº§ç”Ÿä¸ä¸“å®¶åˆ†æ­§ä¸€è‡´çš„å¤šæ ·åŒ–è¾“å‡ºï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ¦‚ç‡åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚ç‡æ€§SAMæ¨¡å‹è§£å†³äº†ä¼ ç»ŸåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå¯¹åŒä¸€å¯¹è±¡çš„å¤šä¸ªå¯èƒ½åˆ†å‰²ç»“æœå»ºæ¨¡ï¼Œä»¥é€‚åº”åŒ»å­¦å›¾åƒä¸­çš„æ ‡æ³¨ä¸ç¡®å®šæ€§å’Œä¸“å®¶é—´å·®å¼‚ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ½œåœ¨å˜é‡ç©ºé—´å’Œå˜åˆ†ç›®æ ‡è®­ç»ƒï¼Œæ¨¡å‹èƒ½ç”Ÿæˆåæ˜ äººç±»æ ‡æ³¨å·®å¼‚çš„å¤šæ ·åŒ–ä¸”åˆç†çš„åˆ†å‰²æ©è†œã€‚</li>
<li>æ¨¡å‹æ¶æ„ç»“åˆäº†å…ˆéªŒç½‘ç»œå’ŒåéªŒç½‘ç»œï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æ½œåœ¨ä»£ç è°ƒåˆ¶æç¤ºåµŒå…¥ï¼Œæé«˜äº†çµæ´»æ€§ã€‚</li>
<li>æ½œåœ¨ç©ºé—´æé«˜äº†é‡‡æ ·æ•ˆç‡ï¼Œå®ç°äº†å…·æœ‰æœ€å°å¼€é”€çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥è¾“å‡ºã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæ¦‚ç‡æ€§SAMåœ¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9b130360af620952de3ed33ebf06eb77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-450274dae488a6608b96efe28be70ddd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffc79f79cb9a76ce1128c40fcaac1ac3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b1bc52beeb899150313f6b026f64d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-764c486dafa6e443cfde56c8691a507b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MeshMetrics-A-Precise-Implementation-of-Distance-Based-Image-Segmentation-Metrics"><a href="#MeshMetrics-A-Precise-Implementation-of-Distance-Based-Image-Segmentation-Metrics" class="headerlink" title="MeshMetrics: A Precise Implementation of Distance-Based Image   Segmentation Metrics"></a>MeshMetrics: A Precise Implementation of Distance-Based Image   Segmentation Metrics</h2><p><strong>Authors:GaÅ¡per Podobnik, TomaÅ¾ Vrtovec</strong></p>
<p>The surge of research in image segmentation has yielded remarkable performance gains but also exposed a reproducibility crisis. A major contributor is performance evaluation, where both selection and implementation of metrics play critical roles. While recent efforts have improved the former, the reliability of metric implementation has received far less attention. Pitfalls in distance-based metric implementation can lead to considerable discrepancies between common open-source tools, for instance, exceeding 100 mm for the Hausdorff distance and 30%pt for the normalized surface distance for the same pair of segmentations. To address these pitfalls, we introduce MeshMetrics, a mesh-based framework that provides a more precise computation of distance-based metrics than conventional grid-based approaches. Through theoretical analysis and empirical validation, we demonstrate that MeshMetrics achieves higher accuracy and precision than established tools, and is substantially less affected by discretization artifacts, such as distance quantization. We release MeshMetrics as an open-source Python package, available at <a target="_blank" rel="noopener" href="https://github.com/gasperpodobnik/MeshMetrics">https://github.com/gasperpodobnik/MeshMetrics</a>. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²ç ”ç©¶çš„çƒ­æ½®å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†ä¹Ÿæš´éœ²å‡ºäº†å¯é‡å¤æ€§çš„å±æœºã€‚å…¶ä¸­ä¸»è¦çš„è´¡çŒ®æ¥è‡ªäºæ€§èƒ½è¯„ä¼°ï¼Œå…¶ä¸­æŒ‡æ ‡çš„é€‰æ‹©å’Œå®æ–½éƒ½æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è™½ç„¶æœ€è¿‘çš„åŠªåŠ›æ”¹è¿›äº†å‰è€…ï¼Œä½†æŒ‡æ ‡å®æ–½çš„å¯é æ€§å´è¢«å¿½è§†äº†ã€‚åŸºäºè·ç¦»çš„æŒ‡æ ‡å®æ–½ä¸­çš„é™·é˜±ä¼šå¯¼è‡´å¸¸è§å¼€æºå·¥å…·ä¹‹é—´å‡ºç°å·¨å¤§å·®å¼‚ï¼Œä¾‹å¦‚å¯¹äºåŒä¸€å¯¹åˆ†å‰²ï¼Œè±ªæ–¯å¤šå¤«è·ç¦»è¶…è¿‡100æ¯«ç±³ï¼Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»è¶…è¿‡30%ã€‚ä¸ºäº†è§£å†³è¿™äº›é™·é˜±ï¼Œæˆ‘ä»¬å¼•å…¥äº†MeshMetricsï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç½‘æ ¼çš„æ¡†æ¶ï¼Œå®ƒä¸ºåŸºäºè·ç¦»çš„æŒ‡æ ‡æä¾›äº†æ¯”ä¼ ç»Ÿçš„åŸºäºç½‘æ ¼çš„æ–¹æ³•æ›´ç²¾ç¡®çš„è®¡ç®—ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜äº†MeshMetricsçš„å‡†ç¡®æ€§å’Œç²¾åº¦é«˜äºç°æœ‰å·¥å…·ï¼Œå¹¶ä¸”å—åˆ°ç¦»æ•£åŒ–ä¼ªå½±çš„å½±å“è¾ƒå°ï¼Œä¾‹å¦‚è·ç¦»é‡åŒ–ã€‚æˆ‘ä»¬å°†MeshMetricsä½œä¸ºä¸€ä¸ªå¼€æºPythonåŒ…å‘å¸ƒï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gasperpodobnik/MeshMetrics%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gasperpodobnik/MeshMetricsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒåˆ†å‰²é¢†åŸŸç ”ç©¶ä¸­å‡ºç°çš„å¯é‡å¤æ€§é—®é¢˜ï¼Œä¸»è¦æºäºæ€§èƒ½è¯„ä¼°ä¸­çš„æŒ‡æ ‡é€‰æ‹©å’Œå®ç°ã€‚å°½ç®¡é€‰æ‹©æ–¹é¢çš„æ”¹è¿›å·²ç»å–å¾—äº†ä¸€å®šæˆæœï¼Œä½†æŒ‡æ ‡å®ç°çš„å¯é æ€§å´è¢«å¿½è§†ã€‚æ–‡ä¸­æŒ‡å‡ºè·ç¦»åº¦é‡å®ç°ä¸­çš„é™·é˜±å¯èƒ½å¯¼è‡´å¸¸è§å¼€æºå·¥å…·é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†MeshMetricsæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç½‘æ ¼çš„æ–¹æ³•ï¼Œå¯ä»¥æ¯”ä¼ ç»Ÿçš„åŸºäºç½‘æ ¼çš„æ–¹æ³•æ›´ç²¾ç¡®åœ°è®¡ç®—è·ç¦»åº¦é‡ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼Œè¯æ˜MeshMetricsçš„å‡†ç¡®æ€§å’Œç²¾åº¦é«˜äºç°æœ‰å·¥å…·ï¼Œå¹¶ä¸”å—åˆ°ç¦»æ•£åŒ–ä¼ªå½±çš„å½±å“è¾ƒå°ã€‚æœ€åï¼ŒMeshMetricsä½œä¸ºå¼€æºPythonåŒ…å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²é¢†åŸŸå­˜åœ¨å¯é‡å¤æ€§é—®é¢˜ï¼Œæ€§èƒ½è¯„ä¼°ä¸­çš„æŒ‡æ ‡é€‰æ‹©å’Œå®ç°æ˜¯å…³é”®å› ç´ ã€‚</li>
<li>è·ç¦»åº¦é‡å®ç°ä¸­çš„é™·é˜±å¯èƒ½å¯¼è‡´å¸¸è§å¼€æºå·¥å…·é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>MeshMetricsæ¡†æ¶åŸºäºç½‘æ ¼ï¼Œèƒ½æ›´ç²¾ç¡®åœ°è®¡ç®—è·ç¦»åº¦é‡ã€‚</li>
<li>MeshMetricsçš„å‡†ç¡®æ€§å’Œç²¾åº¦é«˜äºç°æœ‰å·¥å…·ã€‚</li>
<li>MeshMetricså—åˆ°ç¦»æ•£åŒ–ä¼ªå½±çš„å½±å“è¾ƒå°ã€‚</li>
<li>MeshMetricsä½œä¸ºå¼€æºPythonåŒ…å‘å¸ƒï¼Œä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5b2fea2e97a6216a58e4867d9e70019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e865d92e338d7b616ab372b412f98b1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9e224d5f3fa551eaa3b33caf6b361fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b23304e5c06a675e2c3cbae4ac46d00b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-238fcee24a709bc911eddc62b3be655f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0e7e8d73455a1e5ff8c6132ad6108b9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Segmentation-and-Tracking-of-Eruptive-Solar-Phenomena-with-Convolutional-Neural-Networks"><a href="#Segmentation-and-Tracking-of-Eruptive-Solar-Phenomena-with-Convolutional-Neural-Networks" class="headerlink" title="Segmentation and Tracking of Eruptive Solar Phenomena with Convolutional   Neural Networks"></a>Segmentation and Tracking of Eruptive Solar Phenomena with Convolutional   Neural Networks</h2><p><strong>Authors:Oleg Stepanyuk, Kamen Kozarev</strong></p>
<p>Solar eruptive events are complex phenomena, which most often include coronal mass ejections (CME), CME-driven compressive and shock waves, flares, and filament eruptions. CMEs are large eruptions of magnetized plasma from the Sunâ€™s outer atmosphere or corona, that propagate outward into the interplanetary space. Over the last several decades a large amount of remote solar eruption observational data has become available from ground-based and space-borne instruments. This has recently required the development of software approaches for automated characterisation of eruptive features. Most solar feature detection and tracking algorithms currently in use have restricted applicability and complicated processing chains, while complexity in engineering machine learning (ML) training sets limit the use of data-driven approaches for tracking or solar eruptive related phenomena. Recently, we introduced Wavetrack - a general algorithmic method for smart characterization and tracking of solar eruptive features. The method, based on a-trous wavelet decomposition, intensity rankings and a set of filtering techniques, allows to simplify and automate image processing and feature tracking. Previously, we applied the method successfully to several types of remote solar observations. Here we present the natural evolution of this approach. We discuss various aspects of applying Machine Learning (ML) techniques towards segmentation of high-dynamic range heliophysics observations. We trained Convolutional Neural Network (CNN) image segmentation models using feature masks obtained from the Wavetrack code. We present results from pre-trained models for segmentation of solar eruptive features and demonstrate their performance on a set of CME events based on SDO&#x2F;AIA instrument data. </p>
<blockquote>
<p>å¤ªé˜³çˆ†å‘äº‹ä»¶æ˜¯å¤æ‚çš„ç°è±¡ï¼Œé€šå¸¸åŒ…æ‹¬æ—¥å†•ç‰©è´¨æŠ›å°„ï¼ˆCMEï¼‰ã€CMEé©±åŠ¨çš„å‹ç¼©æ³¢å’Œå†²å‡»æ³¢ã€è€€æ–‘å’Œä¸çŠ¶ä½“çˆ†å‘ã€‚æ—¥å†•ç‰©è´¨æŠ›å°„æ˜¯å¤ªé˜³å¤–å±‚å¤§æ°”æˆ–æ—¥å†•ä¸­çš„å¤§å‹ç£åŒ–ç­‰ç¦»å­ä½“å–·å‘ï¼Œå‘å¤–ä¼ æ’­åˆ°è¡Œæ˜Ÿé™…ç©ºé—´ã€‚è¿‡å»å‡ åå¹´ï¼Œä»åœ°é¢å’Œå¤ªç©ºä»ªå™¨è·å¾—äº†å¤§é‡å¤ªé˜³çˆ†å‘çš„è¿œç¨‹è§‚æµ‹æ•°æ®ã€‚è¿™æœ€è¿‘è¦æ±‚å¼€å‘ç”¨äºè‡ªåŠ¨è¡¨å¾çˆ†å‘ç‰¹å¾çš„è½¯ä»¶æ–¹æ³•ã€‚ç›®å‰ä½¿ç”¨çš„å¤§å¤šæ•°å¤ªé˜³ç‰¹å¾æ£€æµ‹å’Œè·Ÿè¸ªç®—æ³•çš„åº”ç”¨èŒƒå›´æœ‰é™ä¸”å¤„ç†é“¾å¤æ‚ï¼Œè€Œå·¥ç¨‹æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰è®­ç»ƒé›†çš„å¤æ‚æ€§é™åˆ¶äº†æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨è·Ÿè¸ªæˆ–å¤ªé˜³çˆ†å‘ç›¸å…³ç°è±¡ä¸­çš„åº”ç”¨ã€‚æœ€è¿‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†Wavetrackâ€”â€”ä¸€ç§ç”¨äºæ™ºèƒ½è¡¨å¾å’Œè·Ÿè¸ªå¤ªé˜³çˆ†å‘ç‰¹å¾çš„é€šç”¨ç®—æ³•æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºa-trouså°æ³¢åˆ†è§£ã€å¼ºåº¦æ’åå’Œä¸€ç³»åˆ—æ»¤æ³¢æŠ€æœ¯ï¼Œå¯ä»¥ç®€åŒ–å’Œè‡ªåŠ¨åŒ–å›¾åƒå¤„ç†å’Œç‰¹å¾è·Ÿè¸ªã€‚ä»¥å‰ï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸåœ°å°†è¯¥æ–¹æ³•åº”ç”¨äºå¤šç§ç±»å‹çš„è¿œç¨‹å¤ªé˜³è§‚æµ‹ã€‚è¿™é‡Œæˆ‘ä»¬ä»‹ç»è¯¥æ–¹æ³•çš„è‡ªç„¶æ¼”å˜ã€‚æˆ‘ä»¬è®¨è®ºäº†å°†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æŠ€æœ¯åº”ç”¨äºé«˜åŠ¨æ€èŒƒå›´å¤ªé˜³ç‰©ç†å­¦è§‚æµ‹åˆ†å‰²çš„å„ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬ä½¿ç”¨Wavetrackä»£ç è·å¾—çš„ç‰¹å¾æ©è†œè®­ç»ƒäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹å¯¹å¤ªé˜³çˆ†å‘ç‰¹å¾è¿›è¡Œåˆ†å‰²çš„ç»“æœï¼Œå¹¶åŸºäºSDO&#x2F;AIAä»ªå™¨æ•°æ®æ¼”ç¤ºäº†å®ƒä»¬åœ¨CMEäº‹ä»¶é›†ä¸Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05623v1">PDF</a> submitted to JGR: Machine Learning and Computation</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†å¤ªé˜³çˆ†å‘äº‹ä»¶ï¼ŒåŒ…æ‹¬æ—¥å†•ç‰©è´¨æŠ›å°„ã€é©±åŠ¨å‹ç¼©æ³¢å’Œå†²å‡»æ³¢ã€è€€æ–‘å’Œä¸çŠ¶ä½“çˆ†å‘ç­‰ã€‚éšç€è¿œç¨‹å¤ªé˜³çˆ†å‘è§‚æµ‹æ•°æ®çš„å¢åŠ ï¼Œéœ€è¦å¼€å‘è½¯ä»¶æ–¹æ³•è‡ªåŠ¨è¡¨å¾çˆ†å‘ç‰¹å¾ã€‚å¼•å…¥äº†ä¸€ç§åŸºäºå°æ³¢åˆ†è§£ã€å¼ºåº¦æ’åå’Œä¸€ç³»åˆ—è¿‡æ»¤æŠ€æœ¯çš„é€šç”¨ç®—æ³•æ–¹æ³•Wavetrackï¼Œç”¨äºæ™ºèƒ½è¡¨å¾å’Œè·Ÿè¸ªå¤ªé˜³çˆ†å‘ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜è®¨è®ºäº†åº”ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯åˆ†å‰²é«˜åŠ¨æ€èŒƒå›´å¤ªé˜³ç‰©ç†è§‚æµ‹çš„å„ä¸ªæ–¹é¢ï¼Œå¹¶ä½¿ç”¨Wavetrackä»£ç è·å¾—çš„ç‰¹å¾æ©è†œè®­ç»ƒäº†å·ç§¯ç¥ç»ç½‘ç»œå›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤ªé˜³çˆ†å‘äº‹ä»¶åŒ…æ‹¬æ—¥å†•ç‰©è´¨æŠ›å°„ã€å‹ç¼©æ³¢å’Œå†²å‡»æ³¢ã€è€€æ–‘å’Œä¸çŠ¶ä½“çˆ†å‘ç­‰ã€‚</li>
<li>å¤§é‡è¿œç¨‹å¤ªé˜³çˆ†å‘è§‚æµ‹æ•°æ®éœ€è¦å¼€å‘è½¯ä»¶æ–¹æ³•è¿›è¡Œè‡ªåŠ¨è¡¨å¾ã€‚</li>
<li>Wavetrackæ˜¯ä¸€ç§åŸºäºå°æ³¢åˆ†è§£çš„é€šç”¨ç®—æ³•æ–¹æ³•ï¼Œç”¨äºæ™ºèƒ½è¡¨å¾å’Œè·Ÿè¸ªå¤ªé˜³çˆ†å‘ç‰¹å¾ã€‚</li>
<li>Wavetrackå·²æˆåŠŸåº”ç”¨äºå¤šç§è¿œç¨‹å¤ªé˜³è§‚æµ‹ç±»å‹ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ–¹æ³•å¯ç”¨äºåˆ†å‰²é«˜åŠ¨æ€èŒƒå›´å¤ªé˜³ç‰©ç†è§‚æµ‹æ•°æ®ã€‚</li>
<li>ä½¿ç”¨Wavetrackä»£ç è·å¾—çš„ç‰¹å¾æ©è†œè®­ç»ƒäº†å·ç§¯ç¥ç»ç½‘ç»œå›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c9a0b458b2cd3607d2d8fe0dc4185f7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Veriserum-A-dual-plane-fluoroscopic-dataset-with-knee-implant-phantoms-for-deep-learning-in-medical-imaging"><a href="#Veriserum-A-dual-plane-fluoroscopic-dataset-with-knee-implant-phantoms-for-deep-learning-in-medical-imaging" class="headerlink" title="Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms   for deep learning in medical imaging"></a>Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms   for deep learning in medical imaging</h2><p><strong>Authors:Jinhao Wang, Florian Vogl, Pascal SchÃ¼tz, SaÅ¡a Ä†ukoviÄ‡, William R. Taylor</strong></p>
<p>Veriserum is an open-source dataset designed to support the training of deep learning registration for dual-plane fluoroscopic analysis. It comprises approximately 110,000 X-ray images of 10 knee implant pair combinations (2 femur and 5 tibia implants) captured during 1,600 trials, incorporating poses associated with daily activities such as level gait and ramp descent. Each image is annotated with an automatically registered ground-truth pose, while 200 images include manually registered poses for benchmarking.   Key features of Veriserum include dual-plane images and calibration tools. The dataset aims to support the development of applications such as 2D&#x2F;3D image registration, image segmentation, X-ray distortion correction, and 3D reconstruction. Freely accessible, Veriserum aims to advance computer vision and medical imaging research by providing a reproducible benchmark for algorithm development and evaluation. The Veriserum dataset used in this study is publicly available via <a target="_blank" rel="noopener" href="https://movement.ethz.ch/data-repository/veriserum.html">https://movement.ethz.ch/data-repository/veriserum.html</a>, with the data stored at ETH Z&quot;urich Research Collections: <a target="_blank" rel="noopener" href="https://doi.org/10.3929/ethz-b-000701146">https://doi.org/10.3929/ethz-b-000701146</a>. </p>
<blockquote>
<p>Veriserumæ˜¯ä¸€ä¸ªå¼€æºæ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒåŒå¹³é¢é€è§†åˆ†ææ·±åº¦å­¦ä¹ æ³¨å†Œçš„è®­ç»ƒã€‚å®ƒåŒ…å«äº†å¤§çº¦11ä¸‡å¼ Xå…‰å›¾åƒï¼Œå›¾åƒä¸­æœ‰10ç§è†å…³èŠ‚æ¤å…¥ç‰©ç»„åˆï¼ˆ2ç§è‚¡éª¨æ¤å…¥ç‰©å’Œ5ç§èƒ«éª¨æ¤å…¥ç‰©ï¼‰ï¼Œè¿™äº›å›¾åƒæ˜¯åœ¨1600æ¬¡è¯•éªŒè¿‡ç¨‹ä¸­æ•è·çš„ï¼Œå¹¶èå…¥äº†ä¸æ—¥å¸¸æ´»åŠ¨ç›¸å…³çš„å§¿åŠ¿ï¼Œå¦‚æ°´å¹³æ­¥æ€å’Œæ–œå¡ä¸‹é™ã€‚æ¯å¼ å›¾åƒéƒ½æ ‡æ³¨äº†è‡ªåŠ¨æ³¨å†Œçš„åœ°é¢çœŸå®å§¿åŠ¿ï¼Œè€Œ200å¼ å›¾åƒåˆ™åŒ…æ‹¬æ‰‹åŠ¨æ³¨å†Œçš„å§¿åŠ¿ï¼Œä»¥ä¾›åŸºå‡†æµ‹è¯•ã€‚Veriserumçš„å…³é”®ç‰¹å¾åŒ…æ‹¬åŒå¹³é¢å›¾åƒå’Œæ ¡å‡†å·¥å…·ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æ”¯æŒåº”ç”¨ç¨‹åºçš„å¼€å‘ï¼Œä¾‹å¦‚2D&#x2F;3Då›¾åƒæ³¨å†Œã€å›¾åƒåˆ†å‰²ã€Xå°„çº¿å¤±çœŸæ ¡æ­£å’Œ3Dé‡å»ºã€‚Veriserumå¯è‡ªç”±è®¿é—®ï¼Œæ—¨åœ¨é€šè¿‡ä¸ºç®—æ³•å¼€å‘å’Œè¯„ä¼°æä¾›å¯é‡å¤çš„åŸºå‡†æµ‹è¯•ï¼Œæ¨åŠ¨è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒç ”ç©¶çš„å‘å±•ã€‚æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„Veriserumæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://movement.ethz.ch/data-repository/veriserum.html%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%EF%BC%8C%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%9C%A8ETH%E8%8B%8F%E9%BB%8E%E4%B8%96%E7%A0%94%E7%A9%B6%E6%94%B6%E8%97%8F%E4%B8%AD%EF%BC%9Ahttps://doi.org/10.3929/ethz-b-000701146%E3%80%82">https://movement.ethz.ch/data-repository/veriserum.htmlå…¬å¼€è·å–ï¼Œæ•°æ®å­˜å‚¨åœ¨ETHè‹é»ä¸–ç ”ç©¶æ”¶è—ä¸­ï¼šhttps://doi.org/10.3929/ethz-b-000701146ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05483v1">PDF</a> This work has been accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>Veriserumæ˜¯ä¸€æ¬¾å¼€æºæ•°æ®é›†ï¼ŒåŒ…å«çº¦11ä¸‡å¼ Xå…‰å›¾åƒï¼Œç”¨äºæ”¯æŒæ·±åº¦å­¦ä¹ æ³¨å†Œç”¨äºåŒå¹³é¢é€è§†åˆ†æçš„åŸ¹è®­ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¯æ—¥æ´»åŠ¨å§¿æ€ï¼ˆå¦‚æ°´å¹³æ­¥æ€å’Œæ–œå¡ä¸‹é™ï¼‰çš„1,600æ¬¡è¯•éªŒæœŸé—´æ•è·çš„10ç§è†å…³èŠ‚æ¤å…¥ç‰©ç»„åˆï¼ˆ2ç§è‚¡éª¨å’Œ5ç§èƒ«éª¨æ¤å…¥ç‰©ï¼‰çš„å½±åƒã€‚æ¯ä¸€å¼ å›¾åƒéƒ½å¸¦æœ‰è‡ªåŠ¨æ³¨å†Œçš„åœ°é¢çœŸå®å§¿æ€æ³¨é‡Šï¼Œå…¶ä¸­200å¼ å›¾åƒè¿˜åŒ…æ‹¬æ‰‹åŠ¨æ³¨å†Œçš„å§¿æ€ä»¥ä¾›åŸºå‡†æµ‹è¯•ã€‚Veriserumæ•°æ®é›†æ—¨åœ¨æ”¯æŒè¯¸å¦‚2D&#x2F;3Då›¾åƒæ³¨å†Œã€å›¾åƒåˆ†å‰²ã€Xå…‰å¤±çœŸæ ¡æ­£å’Œ3Dé‡å»ºç­‰åº”ç”¨ç¨‹åºçš„å¼€å‘ï¼Œå¹¶æ—¨åœ¨é€šè¿‡æä¾›å¯é‡å¤çš„åŸºå‡†æµ‹è¯•æ¥ä¿ƒè¿›è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒç ”ç©¶çš„å‘å±•ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://movement.ethz.ch/data-repository/veriserum.html%E8%AE%BF%E9%97%AE%E3%80%82">https://movement.ethz.ch/data-repository/veriserum.htmlè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Veriserumæ˜¯ä¸€ä¸ªå¼€æºæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡ç”¨äºæ·±åº¦å­¦ä¹ æ³¨å†Œçš„Xå…‰å›¾åƒã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§è†å…³èŠ‚æ¤å…¥ç‰©çš„ç»„åˆå›¾åƒï¼Œæ¨¡æ‹Ÿæ—¥å¸¸æ´»åŠ¨å§¿æ€ã€‚</li>
<li>å›¾åƒå¸¦æœ‰è‡ªåŠ¨æ³¨å†Œçš„åœ°é¢çœŸå®å§¿æ€æ³¨é‡Šï¼Œéƒ¨åˆ†å›¾åƒè¿˜å¸¦æœ‰æ‰‹åŠ¨æ³¨å†Œçš„å§¿æ€ç”¨äºåŸºå‡†æµ‹è¯•ã€‚</li>
<li>Veriserumæ”¯æŒå¤šç§åŒ»å­¦æˆåƒåº”ç”¨ï¼Œå¦‚2D&#x2F;3Då›¾åƒæ³¨å†Œã€å›¾åƒåˆ†å‰²ã€Xå…‰å¤±çœŸæ ¡æ­£å’Œ3Dé‡å»ºã€‚</li>
<li>æ•°æ®é›†æ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒç ”ç©¶çš„å‘å±•ã€‚</li>
<li>Veriserumæ•°æ®é›†å¯é€šè¿‡ç‰¹å®šé“¾æ¥å…¬å¼€è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-72a33779707daa3a7e7604dcf96e9a19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8225fa607dcfba5b25a09318c85b94f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35aaa5df75405f9811409b4992b6c76c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f9b9342864fdd56a48c28237f9470c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0668847655546a6fddc485b70ec83e61.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a296031b95733198ee4bf176732e6ce9.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Multimodal Fine-grained Context Interaction Graph Modeling for   Conversational Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e38fbdca613045530f6170c1981968cb.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in   Panoramic Radiographs using Federated, Centralized and Local Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
