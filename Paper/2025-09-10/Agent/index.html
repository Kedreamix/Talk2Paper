<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  AxelSMOTE An Agent-Based Oversampling Algorithm for Imbalanced   Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-71bb0a6592c54b405fa40a2e9e64055b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    68 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-10-æ›´æ–°"><a href="#2025-09-10-æ›´æ–°" class="headerlink" title="2025-09-10 æ›´æ–°"></a>2025-09-10 æ›´æ–°</h1><h2 id="AxelSMOTE-An-Agent-Based-Oversampling-Algorithm-for-Imbalanced-Classification"><a href="#AxelSMOTE-An-Agent-Based-Oversampling-Algorithm-for-Imbalanced-Classification" class="headerlink" title="AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced   Classification"></a>AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced   Classification</h2><p><strong>Authors:Sukumar Kishanthan, Asela Hevapathige</strong></p>
<p>Class imbalance in machine learning poses a significant challenge, as skewed datasets often hinder performance on minority classes. Traditional oversampling techniques, which are commonly used to alleviate class imbalance, have several drawbacks: they treat features independently, lack similarity-based controls, limit sample diversity, and fail to manage synthetic variety effectively. To overcome these issues, we introduce AxelSMOTE, an innovative agent-based approach that views data instances as autonomous agents engaging in complex interactions. Based on Axelrodâ€™s cultural dissemination model, AxelSMOTE implements four key innovations: (1) trait-based feature grouping to preserve correlations; (2) a similarity-based probabilistic exchange mechanism for meaningful interactions; (3) Beta distribution blending for realistic interpolation; and (4) controlled diversity injection to avoid overfitting. Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms state-of-the-art sampling methods while maintaining computational efficiency. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®é›†çš„åæ–œå¾€å¾€ä¼šé™ä½å¯¹å°‘æ•°ç±»åˆ«çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„è¿‡é‡‡æ ·æŠ€æœ¯é€šå¸¸ç”¨äºç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†å­˜åœ¨å‡ ä¸ªç¼ºç‚¹ï¼šå®ƒä»¬ç‹¬ç«‹å¤„ç†ç‰¹å¾ï¼Œç¼ºä¹åŸºäºç›¸ä¼¼æ€§çš„æ§åˆ¶ï¼Œé™åˆ¶æ ·æœ¬å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•æœ‰æ•ˆåœ°ç®¡ç†åˆæˆå¤šæ ·æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AxelSMOTEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆ›æ–°çš„ä»£ç†æ–¹æ³•ï¼Œå®ƒå°†æ•°æ®å®ä¾‹è§†ä¸ºå‚ä¸å¤æ‚äº¤äº’çš„è‡ªæ²»ä»£ç†ã€‚åŸºäºAxelrodçš„æ–‡åŒ–ä¼ æ’­æ¨¡å‹ï¼ŒAxelSMOTEå®ç°äº†å››ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰åŸºäºç‰¹å¾çš„åˆ†ç»„æ¥ä¿ç•™ç›¸å…³æ€§ï¼›ï¼ˆ2ï¼‰åŸºäºç›¸ä¼¼æ€§çš„æ¦‚ç‡äº¤æ¢æœºåˆ¶ä»¥å®ç°æœ‰æ„ä¹‰çš„äº¤äº’ï¼›ï¼ˆ3ï¼‰Betaåˆ†å¸ƒæ··åˆä»¥å®ç°çœŸå®æ’å€¼ï¼›ï¼ˆ4ï¼‰æ§åˆ¶å¤šæ ·æ€§æ³¨å…¥ä»¥é¿å…è¿‡åº¦æ‹Ÿåˆã€‚åœ¨å…«ä¸ªä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAxelSMOTEåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºæœ€æ–°çš„é‡‡æ ·æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06875v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœºå™¨å­¦ä¹ ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚ä¼ ç»Ÿè¿‡é‡‡æ ·æŠ€æœ¯åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ—¶å­˜åœ¨è¯¸å¤šç¼ºé™·ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºä»£ç†çš„åˆ›æ–°æ–¹æ³•AxelSMOTEï¼Œè¯¥æ–¹æ³•å°†æ•°æ®å®ä¾‹è§†ä¸ºè‡ªä¸»ä»£ç†ï¼Œè¿›è¡Œå¤æ‚äº¤äº’ã€‚AxelSMOTEå®ç°äº†å››ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼ŒåŒ…æ‹¬åŸºäºç‰¹å¾çš„åˆ†ç»„ã€åŸºäºç›¸ä¼¼åº¦çš„æ¦‚ç‡äº¤æ¢æœºåˆ¶ã€Betaåˆ†å¸ƒæ··åˆä»¥åŠæ§åˆ¶å¤šæ ·æ€§æ³¨å…¥ã€‚å®éªŒè¯æ˜ï¼ŒAxelSMOTEåœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›çš„é‡‡æ ·æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜åœ¨æœºå™¨å­¦ä¹ ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
<li>ä¼ ç»Ÿè¿‡é‡‡æ ·æŠ€æœ¯å­˜åœ¨è¯¸å¤šç¼ºé™·ï¼Œå¦‚ç‹¬ç«‹å¤„ç†ç‰¹å¾ã€ç¼ºä¹ç›¸ä¼¼åº¦æ§åˆ¶ã€æ ·æœ¬å¤šæ ·æ€§æœ‰é™ç­‰ã€‚</li>
<li>AxelSMOTEæ˜¯ä¸€ç§åŸºäºä»£ç†çš„åˆ›æ–°æ–¹æ³•ï¼Œå¼•å…¥å¤æ‚äº¤äº’çš„æ•°æ®å¤„ç†æ–¹å¼ã€‚</li>
<li>AxelSMOTEå®ç°äº†å››ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŸºäºç‰¹å¾çš„åˆ†ç»„ã€æ¦‚ç‡äº¤æ¢æœºåˆ¶ã€Betaåˆ†å¸ƒæ··åˆä»¥åŠæ§åˆ¶å¤šæ ·æ€§æ³¨å…¥ã€‚</li>
<li>AxelSMOTEåœ¨å¤šä¸ªä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„é‡‡æ ·æ–¹æ³•ã€‚</li>
<li>AxelSMOTEèƒ½å¤Ÿä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æä¾›æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f76b3957a284ffc6cb8585b97f59494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52fab75edb73efd292b5e7437283b431.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-939ab730db216dc6c0274d15b1a977b7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="T-araVLN-Translator-for-Agricultural-Robotic-Agents-on-Vision-and-Language-Navigation"><a href="#T-araVLN-Translator-for-Agricultural-Robotic-Agents-on-Vision-and-Language-Navigation" class="headerlink" title="T-araVLN: Translator for Agricultural Robotic Agents on   Vision-and-Language Navigation"></a>T-araVLN: Translator for Agricultural Robotic Agents on   Vision-and-Language Navigation</h2><p><strong>Authors:Xiaobei Zhao, Xingqi Lyu, Xiang Li</strong></p>
<p>Agricultural robotic agents have been becoming powerful helpers in a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling agents navigate to the target position following the natural language instructions. AgriVLN effectively understands the simple instructions, however, often misunderstands the complicated instructions. To bridge this gap, we propose the method of Translator for Agricultural Robotic Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction Translator module translates the original instruction to be both refined and precise. Being evaluated on the A2A benchmark, our T-araVLN effectively improves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating the state-of-the-art performance in the agricultural domain. Code: <a target="_blank" rel="noopener" href="https://github.com/AlexTraveling/T-araVLN">https://github.com/AlexTraveling/T-araVLN</a>. </p>
<blockquote>
<p>å†œä¸šæœºå™¨äººä»£ç†åœ¨å¹¿æ³›çš„å†œä¸šä»»åŠ¡ä¸­å·²æˆä¸ºå¼ºå¤§çš„åŠ©æ‰‹ï¼Œç„¶è€Œä»ç„¶ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ“ä½œæˆ–ä¸å¯ç§»åŠ¨çš„é“è·¯è¿›è¡Œç§»åŠ¨ã€‚AgriVLNæ–¹æ³•å’ŒA2AåŸºå‡†ç‡å…ˆå°†è§†è§‰å’Œè¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ‰©å±•åˆ°å†œä¸šé¢†åŸŸï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®ã€‚AgriVLNèƒ½æœ‰æ•ˆåœ°ç†è§£ç®€å•æŒ‡ä»¤ï¼Œä½†å¸¸å¸¸è¯¯è§£å¤æ‚æŒ‡ä»¤ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†å†œä¸šæœºå™¨äººä»£ç†è§†è§‰å’Œè¯­è¨€å¯¼èˆªç¿»è¯‘å™¨æ–¹æ³•ï¼ˆT-araVLNï¼‰ï¼Œå…¶ä¸­çš„æŒ‡ä»¤ç¿»è¯‘æ¨¡å—å°†åŸå§‹æŒ‡ä»¤ç¿»è¯‘ä¸ºæ—¢ç²¾ç»†åˆç²¾ç¡®çš„æŒ‡ä»¤ã€‚åœ¨A2AåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°ï¼Œæˆ‘ä»¬çš„T-araVLNæ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†æˆåŠŸç‡ä»0.47åˆ°0.63ï¼Œå¹¶å°†å®šä½è¯¯å·®ä»2.91ç±³å‡å°‘åˆ°2.28ç±³ï¼Œæ˜¾ç¤ºå‡ºåœ¨å†œä¸šé¢†åŸŸçš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/AlexTraveling/T-araVLN%E3%80%82">https://github.com/AlexTraveling/T-araVLNã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06644v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å†œä¸šæœºå™¨äººå·²ç»åœ¨å¹¿æ³›çš„å†œä¸šä»»åŠ¡ä¸­æˆä¸ºæœ‰åŠ›çš„åŠ©æ‰‹ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ“ä½œæˆ–ä¸å¯ç§»åŠ¨çš„é“è·¯è¿›è¡Œç§»åŠ¨ã€‚AgriVLNæ–¹æ³•å’ŒA2AåŸºå‡†ç‡å…ˆå°†è§†è§‰å’Œè¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ‰©å±•åˆ°å†œä¸šé¢†åŸŸï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®ã€‚å°½ç®¡AgriVLNå¯ä»¥æœ‰æ•ˆåœ°ç†è§£ç®€å•æŒ‡ä»¤ï¼Œä½†ç»å¸¸è¯¯è§£å¤æ‚æŒ‡ä»¤ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘å†œä¸šæœºå™¨äººä»£ç†çš„è§†è§‰å’Œè¯­è¨€å¯¼èˆªç¿»è¯‘æ–¹æ³•ï¼ˆT-araVLNï¼‰ï¼Œå…¶ä¸­çš„æŒ‡ä»¤ç¿»è¯‘æ¨¡å—å°†åŸå§‹æŒ‡ä»¤ç¿»è¯‘ä¸ºæ—¢ç²¾ç¡®åˆç²¾ç‚¼çš„æŒ‡ä»¤ã€‚åœ¨A2AåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„T-araVLNæ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†æˆåŠŸç‡ä»0.47åˆ°0.63ï¼Œå¹¶å°†å¯¼èˆªè¯¯å·®ä»2.91ç±³å‡å°‘åˆ°2.28ç±³ï¼Œå±•ç°å‡ºå†œä¸šé¢†åŸŸçš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†œä¸šæœºå™¨äººå·²ç»åœ¨å„ç§å†œä¸šä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä½†åœ¨ç§»åŠ¨æ–¹é¢ä»é¢ä¸´ä¾èµ–æ‰‹åŠ¨æ“ä½œæˆ–ä¸å¯ç§»åŠ¨è¿è¾“æ–¹å¼çš„é—®é¢˜ã€‚</li>
<li>AgriVLNæ–¹æ³•å’ŒA2AåŸºå‡†é¦–æ¬¡å°†è§†è§‰å’Œè¯­è¨€å¯¼èˆªæŠ€æœ¯åº”ç”¨äºå†œä¸šé¢†åŸŸï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå¯¼èˆªã€‚</li>
<li>AgriVLNåœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶å­˜åœ¨è¯¯è§£çš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†T-araVLNæ–¹æ³•ï¼Œå…¶ä¸­çš„æŒ‡ä»¤ç¿»è¯‘æ¨¡å—èƒ½å¤Ÿç²¾ç‚¼å’Œç²¾ç¡®ç¿»è¯‘åŸå§‹æŒ‡ä»¤ã€‚</li>
<li>åœ¨A2AåŸºå‡†æµ‹è¯•ä¸Šï¼ŒT-araVLNæ–¹æ³•æ˜¾è‘—æé«˜äº†æˆåŠŸç‡å’Œå‡å°‘äº†å¯¼èˆªè¯¯å·®ã€‚</li>
<li>T-araVLNæ–¹æ³•å±•ç¤ºäº†å†œä¸šé¢†åŸŸæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30e53ffd8010096eb920828a861b7078.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82c851c2d53535d623132b39c0225955.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44a53439ad48fbaeb29b3cd60a02a425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f627b7c4d2c41d121ac244d9c44722.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dc147de57df71a751eec849d853c33e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents"><a href="#WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents" class="headerlink" title="WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents"></a>WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents</h2><p><strong>Authors:Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen, Pengyu Zhao, Junxian He</strong></p>
<p>The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en&#x2F;zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŒƒå¼è¶Šæ¥è¶Šè½¬å‘ä»£ç†åº”ç”¨ï¼Œå…¶ä¸­ç½‘é¡µæµè§ˆèƒ½åŠ›å¯¹äºä»å„ç§åœ¨çº¿æ¥æºæ£€ç´¢ä¿¡æ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºç½‘ç»œä»£ç†è¦ä¹ˆåœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ‰é™çš„ä¿¡æ¯æœç´¢èƒ½åŠ›ï¼Œè¦ä¹ˆç¼ºä¹é€æ˜çš„å®ç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°å…³é”®æŒ‘æˆ˜åœ¨äºç¼ºä¹ä¿¡æ¯æœç´¢çš„æŒ‘æˆ˜æ€§æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†WebExplorerï¼šä¸€ç§åŸºäºæ¨¡å‹æ¢ç´¢å’Œæ•°æ®è¿­ä»£ã€ä»é•¿åˆ°çŸ­çš„æŸ¥è¯¢æ¼”å˜çš„æœ‰ç³»ç»Ÿçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•åˆ›å»ºäº†éœ€è¦å¤šæ­¥éª¤æ¨ç†å’Œå¤æ‚ç½‘ç»œå¯¼èˆªçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æŸ¥è¯¢ç­”æ¡ˆå¯¹ã€‚é€šè¿‡åˆ©ç”¨æˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å¼€å‘äº†å…ˆè¿›çš„ç½‘ç»œä»£ç†WebExplorer-8Bï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒåé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒæœ€å¤šåŒ…å«é«˜è¾¾12ä¸‡å­—çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œé•¿è¾¾10ä¸‡æ¬¡çš„å·¥å…·è°ƒç”¨ï¼Œå¯å®ç°é•¿æœŸé—®é¢˜è§£å†³ã€‚åœ¨å„ç§ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•ä¸­ï¼ŒWebExplorer-8Båœ¨å…¶è§„æ¨¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½œä¸ºè§„æ¨¡ä¸º8äº¿å‚æ•°çš„æ¨¡å‹ï¼ŒWebExplorer-8Båœ¨ç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒåèƒ½å¤Ÿåœ¨å¹³å‡è¶…è¿‡16è½®æœç´¢ä¸­æœ‰æ•ˆæŸ¥æ‰¾ä¿¡æ¯ï¼Œåœ¨BrowseComp-en&#x2F;zhä¸Šçš„å‡†ç¡®æ€§é«˜äºWebSailor-72Bï¼Œå¹¶åœ¨WebWalkerQAå’ŒFRAMESä¸Šè¾¾åˆ°äº†è¿„ä»Šä¸ºæ­¢å‚æ•°åœ¨ç™¾äº¿è§„æ¨¡å†…çš„æ¨¡å‹çš„æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚é™¤äº†è¿™äº›ä¿¡æ¯æœç´¢ä»»åŠ¡ä¹‹å¤–ï¼Œå³ä½¿åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿåœ¨HLEåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•åœ¨é¢å‘é•¿æœŸè§†é‡çš„ç½‘ç»œä»£ç†æ–¹å‘ä¸Šçš„å®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å‘ä»£ç†åº”ç”¨æ—¶çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä¿¡æ¯æ£€ç´¢æ–¹é¢çš„ä¸è¶³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºWebExplorerçš„ç³»ç»ŸåŒ–æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹æ¢ç´¢å’ŒæŸ¥è¯¢çš„è¿­ä»£æ¼”åŒ–ï¼Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§é—®ç­”å¯¹ã€‚åŸºäºæ­¤æ•°æ®é›†è®­ç»ƒå‡ºçš„WebExplorer-8Bæ¨¡å‹ï¼Œæ”¯æŒé•¿è¯­å¢ƒå’Œå¤šæ¬¡å·¥å…·è°ƒç”¨ï¼Œèƒ½åœ¨å¤šç§ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç†åº”ç”¨ä¸­é¢ä¸´ä¿¡æ¯æ£€ç´¢èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>WebExploreræ–¹æ³•é€šè¿‡æ¨¡å‹æ¢ç´¢å’ŒæŸ¥è¯¢è¿­ä»£æ¼”åŒ–è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>WebExplorer-8Bæ¨¡å‹åŸºäºWebExploreræ–¹æ³•å¼€å‘ï¼Œæ”¯æŒé•¿è¯­å¢ƒå’Œå¤šæ¬¡å·¥å…·è°ƒç”¨ã€‚</li>
<li>WebExplorer-8Bæ¨¡å‹åœ¨å¤šç§ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”æ•°æ®ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>WebExplorer-8Bæ¨¡å‹åœ¨æµè§ˆæ¯”è¾ƒä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡é«˜äºWebSailor-72Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33b3f1cd6494766e0e8a09139e1df15a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f39e4d468f72de3e92c153b68b4b955.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dad0a05ae16265106b1a7181ac268551.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers"><a href="#Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers" class="headerlink" title="Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers"></a>Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers</h2><p><strong>Authors:Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao</strong></p>
<p>The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08% and 41.4% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°è‡ªåŠ¨åŒ–å®šç†è¯æ˜ä¸­å·²æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»æ ¹æœ¬ä¸Šå—åˆ°è®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ—¶è®¡ç®—æ‰©å±•æŒ‘æˆ˜çš„é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†<code>BFS-Prover-V2</code>ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³è¿™ä¸€åŒé‡æ‰©å±•é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªä¸»è¦çš„åˆ›æ–°ç‚¹ã€‚ç¬¬ä¸€ä¸ªæ˜¯æ–°å‹çš„å¤šè½®ç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¸æ–­æé«˜è®­ç»ƒæ—¶LLMé€æ­¥è¯æ˜çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å—åˆ°AlphaZeroåŸåˆ™çš„å¯å‘ï¼Œé‡‡ç”¨å¤šé˜¶æ®µä¸“å®¶è¿­ä»£ç®¡é“ï¼Œå…·æœ‰è‡ªé€‚åº”æˆ˜æœ¯çº§æ•°æ®è¿‡æ»¤å’Œå®šæœŸé‡æ–°è®­ç»ƒçš„åŠŸèƒ½ï¼Œä»¥å…‹æœé€šå¸¸é™åˆ¶é•¿æœŸRLçš„æ€§èƒ½å¹³å°é—®é¢˜ã€‚ç¬¬äºŒä¸ªåˆ›æ–°ç‚¹æ˜¯ä¸€ä¸ªå¢å¼ºè§„åˆ’çš„å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨æ¨ç†æ—¶é—´æ‰©å±•äº†æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¶æ„é‡‡ç”¨é€šç”¨æ¨ç†æ¨¡å‹ä½œä¸ºé«˜çº§è§„åˆ’å™¨ï¼Œå°†å¤æ‚çš„å®šç†è¿­ä»£åœ°åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•çš„å­ç›®æ ‡ã€‚è¿™ç§åˆ†å±‚æ–¹æ³•å¤§å¤§å‡å°‘äº†æœç´¢ç©ºé—´ï¼Œä½¿ä¸€ç»„å¹¶è¡Œè¯æ˜æ™ºèƒ½ä½“èƒ½å¤Ÿåˆ©ç”¨å…±äº«è¯æ˜ç¼“å­˜è¿›è¡Œé«˜æ•ˆåä½œã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§åŒé‡æ‰©å±•æ–¹æ³•å¯ä»¥åœ¨å»ºç«‹çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°ç»“æœã€‚<code>BFS-Prover-V2</code>åœ¨MiniF2Få’ŒProofNetæµ‹è¯•é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†95.08%å’Œ41.4%çš„å‡†ç¡®ç‡ã€‚è™½ç„¶æœ¬å·¥ä½œåœ¨å½¢å¼æ•°å­¦é¢†åŸŸå¾—åˆ°äº†å±•ç¤ºï¼Œä½†æœ¬å·¥ä½œä¸­æå‡ºçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†æŠ€æœ¯å…·æœ‰æ›´å¹¿æ³›çš„å…´è¶£ï¼Œå¹¶å¯åº”ç”¨äºå…¶ä»–éœ€è¦é•¿æœŸå¤šè½®æ¨ç†å’Œå¤æ‚æœç´¢çš„é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06493v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ä¸­çš„åº”ç”¨å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†é¢ä¸´ç€è®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ—¶è®¡ç®—æ‰©å±•çš„åŒé‡æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†\texttt{BFS-Prover-V2}ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è¿™ä¸€åŒé‡æ‰©å±•é—®é¢˜çš„ç³»ç»Ÿã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šä¸€æ˜¯åœ¨è®­ç»ƒæ—¶é‡‡ç”¨æ–°å‹çš„å¤šå›åˆç¦»çº¿RLæ¡†æ¶ï¼ŒæŒç»­æé«˜LLMé€æ­¥è¯æ˜çš„æ€§èƒ½ï¼›äºŒæ˜¯é‡‡ç”¨è§„åˆ’å¢å¼ºçš„å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„ï¼Œåœ¨æ¨ç†æ—¶æ‰©å±•æ¨ç†èƒ½åŠ›ã€‚å‰è€…å—AlphaZeroåŸç†å¯å‘ï¼Œåˆ©ç”¨å¤šé˜¶æ®µä¸“å®¶è¿­ä»£ç®¡é“ï¼Œå…·æœ‰è‡ªé€‚åº”æˆ˜æœ¯çº§æ•°æ®è¿‡æ»¤å’Œå®šæœŸå†è®­ç»ƒåŠŸèƒ½ï¼Œä»¥å…‹æœæ€§èƒ½ç“¶é¢ˆï¼›åè€…é‡‡ç”¨é€šç”¨æ¨ç†æ¨¡å‹ä½œä¸ºé«˜çº§è§„åˆ’å™¨ï¼Œå°†å¤æ‚å®šç†åˆ†è§£ä¸ºä¸€ç³»åˆ—ç®€å•å­ç›®æ ‡ã€‚è¿™ç§å±‚æ¬¡æ–¹æ³•å¤§å¤§å‡å°‘äº†æœç´¢ç©ºé—´ï¼Œä½¿ä¸€ç»„å¹¶è¡Œè¯æ˜æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡å…±äº«è¯æ˜ç¼“å­˜è¿›è¡Œæœ‰æ•ˆåä½œã€‚åœ¨å½¢å¼æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™ç§åŒé‡æ‰©å±•æ–¹æ³•å–å¾—äº†æœ€æ–°ç»“æœã€‚\texttt{BFS-Prover-V2}åœ¨MiniF2Få’ŒProofNetæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°95.08%å’Œ41.4%ã€‚è™½ç„¶æ­¤å·¥ä½œæ˜¯åœ¨å½¢å¼æ•°å­¦é¢†åŸŸå±•ç¤ºçš„ï¼Œä½†æå‡ºçš„RLå’Œæ¨ç†æŠ€æœ¯å…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦é•¿æœŸå¤šå›åˆæ¨ç†å’Œå¤æ‚æœç´¢çš„é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´è®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†è®¡ç®—çš„åŒé‡æ‰©å±•æŒ‘æˆ˜ã€‚</li>
<li>\texttt{BFS-Prover-V2}ç³»ç»Ÿé€šè¿‡ä¸¤é¡¹ä¸»è¦åˆ›æ–°è§£å†³è¿™äº›é—®é¢˜ï¼šä¸€æ˜¯é‡‡ç”¨æ–°å‹å¤šå›åˆç¦»çº¿RLæ¡†æ¶æé«˜LLMæ€§èƒ½ï¼›äºŒæ˜¯ä½¿ç”¨è§„åˆ’å¢å¼ºçš„å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„æ‰©å±•æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RLæ¡†æ¶å—AlphaZeroåŸç†å¯å‘ï¼Œåˆ©ç”¨å¤šé˜¶æ®µä¸“å®¶è¿­ä»£ç®¡é“å’Œè‡ªé€‚åº”æˆ˜æœ¯çº§æ•°æ®è¿‡æ»¤åŠå®šæœŸå†è®­ç»ƒæ¥å…‹æœæ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>æ¨ç†æ¶æ„é‡‡ç”¨é€šç”¨æ¨ç†æ¨¡å‹ä½œä¸ºé«˜çº§è§„åˆ’å™¨ï¼Œå°†å¤æ‚å®šç†åˆ†è§£ä¸ºç®€å•å­ç›®æ ‡ï¼Œå‡å°‘æœç´¢ç©ºé—´ã€‚</li>
<li>\texttt{BFS-Prover-V2}åœ¨å½¢å¼æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨MiniF2Få’ŒProofNetæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾95.08%å’Œ41.4%ã€‚</li>
<li>è™½ç„¶æ¡ˆä¾‹ç ”ç©¶é›†ä¸­åœ¨å½¢å¼æ•°å­¦é¢†åŸŸï¼Œä½†æå‡ºçš„RLå’Œæ¨ç†æŠ€æœ¯å…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯åº”ç”¨äºå…¶ä»–éœ€è¦é•¿æœŸå¤šå›åˆæ¨ç†å’Œå¤æ‚æœç´¢çš„é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-311a0b11fe3c032a66dc832b4d174b77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-459dd296e9e333de4c5bb14063f466c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcea538174a061d8af1fc5656996afa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3879198f5edb99f799e0a1ec14d23908.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tree-of-Agents-Improving-Long-Context-Capabilities-of-Large-Language-Models-through-Multi-Perspective-Reasoning"><a href="#Tree-of-Agents-Improving-Long-Context-Capabilities-of-Large-Language-Models-through-Multi-Perspective-Reasoning" class="headerlink" title="Tree of Agents: Improving Long-Context Capabilities of Large Language   Models through Multi-Perspective Reasoning"></a>Tree of Agents: Improving Long-Context Capabilities of Large Language   Models through Multi-Perspective Reasoning</h2><p><strong>Authors:Song Yu, Xiaofei Xu, Ke Deng, Li Li, Lin Tian</strong></p>
<p>Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Aireduce952/Tree-of-Agents">https://github.com/Aireduce952/Tree-of-Agents</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶é¢ä¸´æŒç»­æŒ‘æˆ˜ï¼Œæœ€æ˜¾è‘—çš„æ˜¯ä¸­é—´ä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œå³é•¿è¾“å…¥ä¸­çš„ä¸­é—´ä¿¡æ¯å¾€å¾€å¾—ä¸åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸€äº›å‡å°‘è¾“å…¥çš„æ–¹æ³•æœ‰ä¸¢å¼ƒå…³é”®ä¿¡æ¯çš„é£é™©ï¼Œè€Œå¦ä¸€äº›æ‰©å±•ä¸Šä¸‹æ–‡çª—å£çš„æ–¹æ³•åˆ™å¸¸å¸¸å¯¼è‡´æ³¨æ„åŠ›åˆ†æ•£ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Agentæ ‘ï¼ˆTOAï¼‰è¿™ä¸€å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œå®ƒå°†è¾“å…¥åˆ†å‰²æˆç”±ç‹¬ç«‹æ™ºèƒ½ä½“å¤„ç†çš„å—ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ç”Ÿæˆå…¶å±€éƒ¨è®¤çŸ¥ï¼Œç„¶åæ™ºèƒ½ä½“æ²¿ç€æ ‘çŠ¶ç»“æ„è·¯å¾„åŠ¨æ€äº¤æ¢ä¿¡æ¯è¿›è¡ŒååŒæ¨ç†ã€‚TOAä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ¢ç©¶ä¸åŒçš„æ¨ç†é¡ºåºä»¥å®ç°å¤šè§’åº¦ç†è§£ï¼Œæœ‰æ•ˆåœ°å‡è½»ä½ç½®åè§å¹¶å‡å°‘å¹»è§‰ã€‚ä¸ºäº†æé«˜å¤„ç†æ•ˆç‡ï¼Œæˆ‘ä»¬ç»“åˆäº†å‰ç¼€å“ˆå¸Œç¼“å­˜å’Œè‡ªé€‚åº”ä¿®å‰ªç­–ç•¥ï¼Œåœ¨APIå¼€é”€ç›¸å½“çš„æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒè¡¨æ˜ï¼Œç”±ç´§å‡‘LLaMA3.1-8Bé©±åŠ¨çš„TOAæ˜¾è‘—ä¼˜äºå¤šä¸ªåŸºçº¿ï¼Œå¹¶åœ¨å„ç§é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šå±•ç°äº†ä¸æœ€æ–°çš„æ›´å¤§è§„æ¨¡å•†ä¸šæ¨¡å‹ï¼ˆå¦‚Gemini1.5-proï¼‰ç›¸å½“çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Aireduce952/Tree-of-Agents%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Aireduce952/Tree-of-Agentsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06436v1">PDF</a> 19 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬ä»»åŠ¡æ—¶é¢ä¸´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä¸­é—´éƒ¨åˆ†çš„ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶â€”â€”Tree of Agentsï¼ˆTOAï¼‰ã€‚è¯¥æ¡†æ¶å°†è¾“å…¥åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œç”±ç‹¬ç«‹æ™ºèƒ½ä½“è¿›è¡Œå¤„ç†ã€‚æ™ºèƒ½ä½“é—´åŠ¨æ€äº¤æ¢ä¿¡æ¯ï¼Œå®ç°åä½œæ¨ç†ã€‚TOAé€šè¿‡ä¸åŒçš„æ¨ç†é¡ºåºå’Œå¤šè§’åº¦ç†è§£ï¼Œå‡å°‘åè§å’Œè¯¯åˆ¤ã€‚ç»“åˆå‰ç¼€å“ˆå¸Œç¼“å­˜å’Œè‡ªé€‚åº”å‰ªæç­–ç•¥ï¼Œæé«˜äº†å¤„ç†æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒTOAåœ¨å¤šç§é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸å°å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”æ€§èƒ½æ˜¾è‘—ï¼Œä¸å¤§å‹å•†ä¸šæ¨¡å‹ç›¸æ¯”è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬ä»»åŠ¡æ—¶å­˜åœ¨ä¸­é—´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚</li>
<li>Tree of Agentsï¼ˆTOAï¼‰æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TOAå°†è¾“å…¥åˆ†å‰²æˆç‰‡æ®µï¼Œç”±ç‹¬ç«‹æ™ºèƒ½ä½“å¤„ç†ï¼Œå¹¶é€šè¿‡åŠ¨æ€ä¿¡æ¯äº¤æ¢å®ç°åä½œæ¨ç†ã€‚</li>
<li>TOAé€šè¿‡ä¸åŒçš„æ¨ç†é¡ºåºå’Œå¤šè§’åº¦ç†è§£ï¼Œå‡å°‘åè§å’Œè¯¯åˆ¤ã€‚</li>
<li>TOAç»“åˆäº†å‰ç¼€å“ˆå¸Œç¼“å­˜å’Œè‡ªé€‚åº”å‰ªæç­–ç•¥ï¼Œæé«˜äº†å¤„ç†æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒTOAåœ¨å¤šç§é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3ada1f6e6d1396c764a03b111ee7549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-449acbce828e47bbc7b4c6973b82ee5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24dddc01e1b905335cab2b56d30c2f34.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HECATE-An-ECS-based-Framework-for-Teaching-and-Developing-Multi-Agent-Systems"><a href="#HECATE-An-ECS-based-Framework-for-Teaching-and-Developing-Multi-Agent-Systems" class="headerlink" title="HECATE: An ECS-based Framework for Teaching and Developing Multi-Agent   Systems"></a>HECATE: An ECS-based Framework for Teaching and Developing Multi-Agent   Systems</h2><p><strong>Authors:Arthur Casals, Anarosa A. F. BrandÃ£o</strong></p>
<p>This paper introduces HECATE, a novel framework based on the Entity-Component-System (ECS) architectural pattern that bridges the gap between distributed systems engineering and MAS development. HECATE is built using the Entity-Component-System architectural pattern, leveraging data-oriented design to implement multiagent systems. This approach involves engineering multiagent systems (MAS) from a distributed systems (DS) perspective, integrating agent concepts directly into the DS domain. This approach simplifies MAS development by (i) reducing the need for specialized agent knowledge and (ii) leveraging familiar DS patterns and standards to minimize the agent-specific knowledge required for engineering MAS. We present the frameworkâ€™s architecture, core components, and implementation approach, demonstrating how it supports different agent models. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†HECATEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå®ä½“ç»„ä»¶ç³»ç»Ÿï¼ˆECSï¼‰æ¶æ„æ¨¡å¼çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ç¼©å°åˆ†å¸ƒå¼ç³»ç»Ÿå·¥ç¨‹å’ŒMASå¼€å‘ä¹‹é—´çš„å·®è·ã€‚HECATEé‡‡ç”¨å®ä½“ç»„ä»¶ç³»ç»Ÿæ¶æ„æ¨¡å¼æ„å»ºï¼Œåˆ©ç”¨é¢å‘æ•°æ®çš„è®¾è®¡æ¥å®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥æ–¹æ³•ä»åˆ†å¸ƒå¼ç³»ç»Ÿï¼ˆDSï¼‰çš„è§’åº¦æ¥æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ï¼Œç›´æ¥å°†æ™ºèƒ½ä½“æ¦‚å¿µé›†æˆåˆ°DSé¢†åŸŸã€‚è¿™ç§æ–¹æ³•é€šè¿‡ï¼ˆiï¼‰å‡å°‘ç‰¹æ®Šæ™ºèƒ½ä½“çŸ¥è¯†çš„éœ€æ±‚ä»¥åŠï¼ˆiiï¼‰åˆ©ç”¨ç†Ÿæ‚‰çš„DSæ¨¡å¼å’Œæ ‡å‡†æ¥å‡å°‘å·¥ç¨‹MASæ‰€éœ€çš„ç‰¹å®šæ™ºèƒ½ä½“çŸ¥è¯†ï¼Œä»è€Œç®€åŒ–äº†MASçš„å¼€å‘ã€‚æˆ‘ä»¬ä»‹ç»äº†è¯¥æ¡†æ¶çš„æ¶æ„ã€æ ¸å¿ƒç»„ä»¶å’Œå®ç°æ–¹æ³•ï¼Œå±•ç¤ºäº†å®ƒå¦‚ä½•æ”¯æŒä¸åŒçš„æ™ºèƒ½ä½“æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06431v1">PDF</a> Submitted to ECAI-2025</p>
<p><strong>Summary</strong></p>
<p>HECATEæ˜¯ä¸€ä¸ªåŸºäºå®ä½“ç»„ä»¶ç³»ç»Ÿï¼ˆECSï¼‰æ¶æ„æ¨¡å¼çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ç¼©å°åˆ†å¸ƒå¼ç³»ç»Ÿå·¥ç¨‹å’ŒMASå¼€å‘ä¹‹é—´çš„å·®è·ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ•°æ®å¯¼å‘è®¾è®¡æ¥å®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡æ•´åˆæ™ºèƒ½ä½“æ¦‚å¿µåˆ°DSé¢†åŸŸæ¥ç®€åŒ–MASå¼€å‘ï¼Œå‡å°‘äº†å¯¹ä¸“ä¸šæ™ºèƒ½ä½“çŸ¥è¯†çš„éœ€æ±‚ï¼Œå¹¶å€ŸåŠ©ç†Ÿæ‚‰çš„DSæ¨¡å¼å’Œæ ‡å‡†æ¥æœ€å°åŒ–å·¥ç¨‹MASæ‰€éœ€çš„ç‰¹å®šçŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HECATEæ˜¯ä¸€ä¸ªåŸºäºå®ä½“ç»„ä»¶ç³»ç»Ÿï¼ˆECSï¼‰æ¶æ„æ¨¡å¼çš„æ¡†æ¶ã€‚</li>
<li>å®ƒæ—¨åœ¨ç¼©å°åˆ†å¸ƒå¼ç³»ç»Ÿå·¥ç¨‹å’ŒMASå¼€å‘ä¹‹é—´çš„å·®è·ã€‚</li>
<li>HECATEåˆ©ç”¨æ•°æ®å¯¼å‘è®¾è®¡æ¥å®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡æ•´åˆæ™ºèƒ½ä½“æ¦‚å¿µåˆ°DSé¢†åŸŸæ¥ç®€åŒ–MASå¼€å‘ã€‚</li>
<li>è¯¥æ¡†æ¶å‡å°‘äº†å¯¹ä¸“ä¸šæ™ºèƒ½ä½“çŸ¥è¯†çš„éœ€æ±‚ã€‚</li>
<li>HECATEå€ŸåŠ©ç†Ÿæ‚‰çš„DSæ¨¡å¼å’Œæ ‡å‡†æ¥ç®€åŒ–MASå¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fecfc0a4b511ced02190869d70862cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-119fae9b1291aa3bd308ab5299e817d3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MAPF-HD-Multi-Agent-Path-Finding-in-High-Density-Environments"><a href="#MAPF-HD-Multi-Agent-Path-Finding-in-High-Density-Environments" class="headerlink" title="MAPF-HD: Multi-Agent Path Finding in High-Density Environments"></a>MAPF-HD: Multi-Agent Path Finding in High-Density Environments</h2><p><strong>Authors:Hiroya Makino, Seigo Ito</strong></p>
<p>Multi-agent path finding (MAPF) involves planning efficient paths for multiple agents to move simultaneously while avoiding collisions. In typical warehouse environments, agents are often sparsely distributed along aisles. However, increasing the agent density can improve space efficiency. When the agent density is high, we must optimize the paths not only for goal-assigned agents but also for those obstructing them. This study proposes a novel MAPF framework for high-density environments (MAPF-HD). Several studies have explored MAPF in similar settings using integer linear programming (ILP). However, ILP-based methods require substantial computation time to optimize all agent paths simultaneously. Even in small grid-based environments with fewer than $100$ cells, these computations can incur tens to hundreds of seconds. These high computational costs render these methods impractical for large-scale applications such as automated warehouses and valet parking. To address these limitations, we introduce the phased null-agent swapping (PHANS) method. PHANS employs a heuristic approach to incrementally swap positions between agents and empty vertices. This method solves the MAPF-HD problem within seconds to tens of seconds, even in large environments containing more than $700$ cells. The proposed method can potentially improve efficiency in various real-world applications such as warehouse logistics, traffic management, or crowd control. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs">https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs</a>. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰æ¶‰åŠä¸ºå¤šä¸ªæ™ºèƒ½ä½“è§„åˆ’é«˜æ•ˆè·¯å¾„ï¼Œä½¿å…¶å¯ä»¥åŒæ—¶ç§»åŠ¨å¹¶é¿å…ç¢°æ’ã€‚åœ¨å…¸å‹çš„ä»“åº“ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“é€šå¸¸æ²¿é€šé“ç¨€ç–åˆ†å¸ƒã€‚ç„¶è€Œï¼Œå¢åŠ æ™ºèƒ½ä½“çš„å¯†åº¦å¯ä»¥æé«˜ç©ºé—´æ•ˆç‡ã€‚å½“æ™ºèƒ½ä½“å¯†åº¦è¾ƒé«˜æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»ä¸ºå·²åˆ†é…ç›®æ ‡çš„æ™ºèƒ½ä½“ä»¥åŠé˜»ç¢ä»–ä»¬çš„æ™ºèƒ½ä½“ä¼˜åŒ–è·¯å¾„ã€‚æœ¬ç ”ç©¶é’ˆå¯¹é«˜å¯†åº¦ç¯å¢ƒæå‡ºäº†ä¸€ç§æ–°çš„MAPFæ¡†æ¶ï¼ˆMAPF-HDï¼‰ã€‚å·²æœ‰ä¸€äº›ç ”ç©¶åœ¨ç±»ä¼¼ç¯å¢ƒä¸­ä½¿ç”¨æ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰æ¢ç´¢äº†MAPFã€‚ç„¶è€Œï¼ŒåŸºäºILPçš„æ–¹æ³•éœ€è¦å¤§é‡çš„è®¡ç®—æ—¶é—´æ¥åŒæ—¶ä¼˜åŒ–æ‰€æœ‰æ™ºèƒ½ä½“çš„è·¯å¾„ã€‚å³ä½¿åœ¨æ‹¥æœ‰å°‘äº100ä¸ªå•å…ƒçš„å°å‹ç½‘æ ¼ç¯å¢ƒä¸­ï¼Œè¿™äº›è®¡ç®—ä¹Ÿéœ€è¦èŠ±è´¹æ•°åè‡³æ•°ç™¾ç§’çš„æ—¶é—´ã€‚è¿™äº›é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ä½¿å¾—è¿™äº›æ–¹æ³•å¯¹äºå¤§è§„æ¨¡åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨åŒ–ä»“åº“å’Œä»£å®¢æ³Šè½¦ï¼‰æ¥è¯´ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†é˜¶æ®µç©ºæ™ºèƒ½ä½“äº¤æ¢ï¼ˆPHANSï¼‰æ–¹æ³•ã€‚PHANSé‡‡ç”¨å¯å‘å¼æ–¹æ³•é€æ­¥äº¤æ¢æ™ºèƒ½ä½“ä¸ç©ºé¡¶ç‚¹ä¹‹é—´çš„ä½ç½®ã€‚å³ä½¿åœ¨åŒ…å«è¶…è¿‡700ä¸ªå•å…ƒçš„å¤§å‹ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ä»¥åœ¨å‡ ç§’åˆ°å‡ åç§’å†…è§£å†³MAPF-HDé—®é¢˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•æœ‰å¯èƒ½æé«˜ä»“åº“ç‰©æµã€äº¤é€šç®¡ç†æˆ–äººç¾¤æ§åˆ¶ç­‰å„ç§ç°å®åº”ç”¨ä¸­çš„æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06374v1">PDF</a> 9 pages, 12 figures</p>
<p><strong>Summary</strong><br>åœ¨ä»“åº“ç­‰å¤§è§„æ¨¡åº”ç”¨åœºæ™¯ä¸­ï¼Œå¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰å¯¹äºæé«˜ç©ºé—´æ•ˆç‡è‡³å…³é‡è¦ã€‚é¢å¯¹é«˜å¯†åº¦æ™ºèƒ½ä½“ç¯å¢ƒï¼Œç°æœ‰çš„åŸºäºæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰çš„æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å®ç°å¿«é€Ÿä¼˜åŒ–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹é«˜å¯†åº¦ç¯å¢ƒçš„MAPFæ–°æ¡†æ¶ï¼ˆMAPF-HDï¼‰ï¼Œå¹¶å¼•å…¥äº†åˆ†é˜¶æ®µç©ºæ™ºèƒ½ä½“äº¤æ¢ï¼ˆPHANSï¼‰æ–¹æ³•ã€‚PHANSé‡‡ç”¨å¯å‘å¼æ–¹æ³•é€æ­¥äº¤æ¢æ™ºèƒ½ä½“ä¸ç©ºä½ç½®ï¼Œèƒ½åœ¨å‡ ç§’åˆ°å‡ åç§’å†…è§£å†³å¤§å‹ç¯å¢ƒä¸­çš„MAPF-HDé—®é¢˜ï¼Œæœ‰æœ›æé«˜ä»“åº“ç‰©æµã€äº¤é€šç®¡ç†æˆ–äººç¾¤æ§åˆ¶ç­‰åº”ç”¨çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰åœ¨é«˜å¯†åº¦æ™ºèƒ½ä½“ç¯å¢ƒä¸­å°¤ä¸ºé‡è¦ï¼Œéœ€ä¼˜åŒ–æ‰€æœ‰æ™ºèƒ½ä½“çš„è·¯å¾„ã€‚</li>
<li>ç°æœ‰åŸºäºæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰çš„æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸é€‚ç”¨äºå¤§è§„æ¨¡åº”ç”¨åœºæ™¯ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹é«˜å¯†åº¦ç¯å¢ƒçš„MAPFæ–°æ¡†æ¶ï¼ˆMAPF-HDï¼‰ã€‚</li>
<li>å¼•å…¥äº†åˆ†é˜¶æ®µç©ºæ™ºèƒ½ä½“äº¤æ¢ï¼ˆPHANSï¼‰æ–¹æ³•ï¼Œé‡‡ç”¨å¯å‘å¼æ–¹æ³•é€æ­¥ä¼˜åŒ–æ™ºèƒ½ä½“è·¯å¾„ã€‚</li>
<li>PHANSæ–¹æ³•èƒ½åœ¨å‡ ç§’åˆ°å‡ åç§’å†…è§£å†³å¤§å‹ç¯å¢ƒä¸­çš„MAPF-HDé—®é¢˜ã€‚</li>
<li>PHANSæ–¹æ³•å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œå¯åº”ç”¨äºä»“åº“ç‰©æµã€äº¤é€šç®¡ç†ã€äººç¾¤æ§åˆ¶ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91aff2963a778d9d7ddb2793809d804a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af321f8685e27e83742ca4b806209e09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a14a9908e80cc31369dad4d1d53fc92e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e521080050eb111eac0604dc5a54f652.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11dbfa93be64a578dadad192bef713cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d08fc9ccb5bf0d7fc55c5e078bf1622.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SFR-DeepResearch-Towards-Effective-Reinforcement-Learning-for-Autonomously-Reasoning-Single-Agents"><a href="#SFR-DeepResearch-Towards-Effective-Reinforcement-Learning-for-Autonomously-Reasoning-Single-Agents" class="headerlink" title="SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents"></a>SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents</h2><p><strong>Authors:Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, Shafiq Joty</strong></p>
<p>Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (&#96;&#96;thinkingâ€™â€™) models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanityâ€™s Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies. </p>
<blockquote>
<p>èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚ã€äº¤ç»‡çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ï¼Œå°¤å…¶æ˜¯éšç€é¢å‘æ¨ç†çš„ï¼ˆâ€œæ€è€ƒâ€ï¼‰æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚è¿™äº›èƒ½åŠ›æ˜¯è§£é”è®¸å¤šé‡è¦åº”ç”¨çš„å…³é”®ã€‚å…¶ä¸­ä¸€ä¸ªåº”ç”¨æ˜¯æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰ï¼Œå®ƒéœ€è¦åœ¨å¤šä¸ªæ¥æºä¹‹é—´è¿›è¡Œå¹¿æ³›æœç´¢å’Œæ¨ç†ã€‚æœ¬æ–‡çš„å·¥ä½œé‡ç‚¹æ˜¯ä¸ºDRå¼€å‘æœ¬åœ°è‡ªä¸»å•ä»£ç†æ¨¡å‹ï¼Œå…·æœ‰æœ€å°çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚ä¸å¤šä»£ç†ç³»ç»Ÿä¸åŒï¼Œå¤šä»£ç†ç³»ç»Ÿä¸­çš„ä»£ç†æ‰®æ¼”é¢„å…ˆå®šä¹‰çš„è§’è‰²ï¼Œå¹¶åœ¨é™æ€å·¥ä½œæµä¸­çš„æ¯ä¸ªæ­¥éª¤ä¸­è¢«å‘ŠçŸ¥è¦åšä»€ä¹ˆï¼Œè€Œè‡ªä¸»å•ä»£ç†åˆ™æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€åœ°ç¡®å®šå…¶ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†é’ˆå¯¹åŸºç¡€æˆ–æŒ‡ä»¤å¾®è°ƒLLMçš„è®­ç»ƒé…æ–¹ï¼Œä½†æˆ‘ä»¬ä¸“æ³¨äºå¯¹æ¨ç†ä¼˜åŒ–æ¨¡å‹çš„æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºä»£ç†æŠ€èƒ½ï¼ŒåŒæ—¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„RLé…æ–¹ï¼Œä½¿ç”¨å®Œå…¨åˆæˆæ•°æ®ï¼Œå¹¶åº”ç”¨äºå„ç§å¼€æºLLMã€‚æˆ‘ä»¬æœ€å¥½çš„å˜ä½“SFR-DR-20Båœ¨â€œäººç±»æœ€åçš„è€ƒè¯•â€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³é”®çš„åˆ†æå®éªŒï¼Œä»¥æä¾›æ›´å¤šå…³äºæˆ‘ä»¬æ–¹æ³•è®ºçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06283v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èåˆå¤æ‚äº¤ç»‡çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢æˆä¸ºäººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡ç‚¹ï¼Œç‰¹åˆ«æ˜¯éšç€é¢å‘æ¨ç†çš„â€œæ€è€ƒâ€æ¨¡å‹çš„æœ€æ–°å‘å±•ã€‚æœ¬æ–‡ä¸“æ³¨äºä¸ºæ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰å¼€å‘è‡ªä¸»çš„å•ä¸€ä»£ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰æœ€å°çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚ä¸åŒäºå¤šä»£ç†ç³»ç»Ÿï¼Œè‡ªä¸»å•ä¸€ä»£ç†èƒ½åŸºäºä¸Šä¸‹æ–‡åŠ¨æ€ç¡®å®šå…¶ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å¯¼ã€‚æœ¬æ–‡èšç„¦äºæŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¨ç†ä¼˜åŒ–æ¨¡å‹ï¼Œä»¥æé«˜ä»£ç†æŠ€èƒ½å¹¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„å…¨åˆæˆæ•°æ®çš„RLé…æ–¹ï¼Œå¹¶åº”ç”¨äºå„ç§å¼€æºLLMã€‚æœ€ä½³å˜ä½“SFR-DR-20Båœ¨äººç±»æœ€åè€ƒè¯•åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨ç ”ç©¶å¦‚ä½•é›†æˆå¤æ‚çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè¿™æ˜¯é¢å‘åº”ç”¨çš„é‡è¦æ–¹å‘ã€‚</li>
<li>è‡ªä¸»å•ä¸€ä»£ç†æ¨¡å‹è¢«å¼€å‘ç”¨äºæ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰ï¼Œèƒ½æœ€å°ç¨‹åº¦åœ°ä½¿ç”¨ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚</li>
<li>ä¸å¤šä»£ç†ç³»ç»Ÿä¸åŒï¼Œè‡ªä¸»å•ä¸€ä»£ç†èƒ½åŸºäºä¸Šä¸‹æ–‡åŠ¨æ€å†³ç­–ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å¯¼ã€‚</li>
<li>é€šè¿‡å¯¹æ¨ç†ä¼˜åŒ–æ¨¡å‹è¿›è¡ŒæŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæé«˜äº†ä»£ç†æŠ€èƒ½å¹¶ä¿ç•™äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ä¸ªç®€å•çš„å…¨åˆæˆæ•°æ®çš„å¼ºåŒ–å­¦ä¹ é…æ–¹ï¼Œé€‚ç”¨äºå„ç§å¼€æºLLMã€‚</li>
<li>æœ€ä½³æ¨¡å‹SFR-DR-20Båœ¨äººç±»æœ€åè€ƒè¯•åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡ä¸º28.7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3286312ad1f0e5fe82d0c2b56d68d124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-762a81b044fa30a731e76b1e1c36c50e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REMI-A-Novel-Causal-Schema-Memory-Architecture-for-Personalized-Lifestyle-Recommendation-Agents"><a href="#REMI-A-Novel-Causal-Schema-Memory-Architecture-for-Personalized-Lifestyle-Recommendation-Agents" class="headerlink" title="REMI: A Novel Causal Schema Memory Architecture for Personalized   Lifestyle Recommendation Agents"></a>REMI: A Novel Causal Schema Memory Architecture for Personalized   Lifestyle Recommendation Agents</h2><p><strong>Authors:Vishal Raman, Vijai Aravindh R, Abhijith Ragav</strong></p>
<p>Personalized AI assistants often struggle to incorporate complex personal data and causal knowledge, leading to generic advice that lacks explanatory power. We propose REMI, a Causal Schema Memory architecture for a multimodal lifestyle agent that integrates a personal causal knowledge graph, a causal reasoning engine, and a schema based planning module. The idea is to deliver explainable, personalized recommendations in domains like fashion, personal wellness, and lifestyle planning. Our architecture uses a personal causal graph of the userâ€™s life events and habits, performs goal directed causal traversals enriched with external knowledge and hypothetical reasoning, and retrieves adaptable plan schemas to generate tailored action plans. A Large Language Model orchestrates these components, producing answers with transparent causal explanations. We outline the CSM system design and introduce new evaluation metrics for personalization and explainability, including Personalization Salience Score and Causal Reasoning Accuracy, to rigorously assess its performance. Results indicate that CSM based agents can provide more context aware, user aligned recommendations compared to baseline LLM agents. This work demonstrates a novel approach to memory augmented, causal reasoning in personalized agents, advancing the development of transparent and trustworthy AI lifestyle assistants. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–äººå·¥æ™ºèƒ½åŠ©ç†åœ¨èå…¥å¤æ‚çš„ä¸ªäººæ•°æ®å’Œå› æœçŸ¥è¯†æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´æä¾›ç¼ºä¹è§£é‡ŠåŠ›çš„é€šç”¨å»ºè®®ã€‚æˆ‘ä»¬æå‡ºäº†REMIï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ¨¡å¼ç”Ÿæ´»æ–¹å¼ä»£ç†çš„å› æœæ¨¡å¼è®°å¿†æ¶æ„ï¼Œå®ƒé›†æˆäº†ä¸ªäººå› æœçŸ¥è¯†å›¾è°±ã€å› æœæ¨ç†å¼•æ“å’ŒåŸºäºæ¨¡å¼çš„è§„åˆ’æ¨¡å—ã€‚æˆ‘ä»¬çš„ç†å¿µæ˜¯åœ¨æ—¶å°šã€ä¸ªäººå¥åº·å’Œç”Ÿæ´»è§„åˆ’ç­‰é¢†åŸŸæä¾›å¯è§£é‡Šçš„ã€ä¸ªæ€§åŒ–çš„å»ºè®®ã€‚æˆ‘ä»¬çš„æ¶æ„ä½¿ç”¨äº†ç”¨æˆ·çš„ç”Ÿå¹³äº‹ä»¶å’Œä¹ æƒ¯çš„å› æœå›¾ï¼Œæ‰§è¡Œä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„å› æœéå†ï¼Œå¹¶ä¸°å¯Œå¤–éƒ¨çŸ¥è¯†å’Œå‡è®¾æ¨ç†ï¼Œæ£€ç´¢å¯é€‚åº”çš„æ¨¡å¼å›¾ä»¥ç”Ÿæˆå®šåˆ¶çš„è¡ŒåŠ¨è®¡åˆ’ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åè°ƒè¿™äº›ç»„ä»¶ï¼Œäº§ç”Ÿå…·æœ‰é€æ˜å› æœè§£é‡Šçš„ç­”æ¡ˆã€‚æˆ‘ä»¬æ¦‚è¿°äº†CSMç³»ç»Ÿè®¾è®¡çš„è¦ç‚¹ï¼Œå¹¶ä¸ºä¸ªæ€§åŒ–å’Œå¯è§£é‡Šæ€§å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–æ˜¾è‘—æ€§å¾—åˆ†å’Œå› æœæ¨ç†å‡†ç¡®æ€§ï¼Œä»¥ä¸¥æ ¼è¯„ä¼°å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºCSMçš„ä»£ç†å¯ä»¥æä¾›æ¯”åŸºäºLLMçš„ä»£ç†æ›´è´´è¿‘ä¸Šä¸‹æ–‡å’Œç”¨æˆ·éœ€æ±‚çš„å»ºè®®ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†åœ¨ä¸ªæ€§åŒ–ä»£ç†ä¸­å¢å¼ºè®°å¿†å’Œå› æœæ¨ç†çš„æ–°æ–¹æ³•ï¼Œæ¨åŠ¨äº†é€æ˜å’Œå¯é çš„äººå·¥æ™ºèƒ½ç”Ÿæ´»åŠ©ç†çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06269v1">PDF</a> 8 pages, 2 figures, Accepted at the OARS Workshop, KDD 2025, Paper   link: <a target="_blank" rel="noopener" href="https://oars-workshop.github.io/papers/Raman2025.pdf">https://oars-workshop.github.io/papers/Raman2025.pdf</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨ä¸ªæ€§åŒ–AIåŠ©ç†åœ¨é¢å¯¹å¤æ‚ä¸ªäººæ•°æ®å’Œå› æœçŸ¥è¯†æ—¶çš„æŒ‘æˆ˜ï¼Œæå‡ºREMIçš„å› æœæ¨¡å¼è®°å¿†æ¶æ„ï¼Œç”¨äºå¤šæ¨¡å¼ç”Ÿæ´»æ–¹å¼ä»£ç†ã€‚è¯¥æ¶æ„ç»“åˆä¸ªäººå› æœçŸ¥è¯†å›¾è°±ã€å› æœæ¨ç†å¼•æ“å’ŒåŸºäºæ¨¡å¼çš„è§„åˆ’æ¨¡å—ï¼Œæ—¨åœ¨æä¾›æ—¶å°šã€ä¸ªäººå¥åº·å’Œç”Ÿæ´»è§„åˆ’ç­‰é¢†åŸŸçš„å¯è§£é‡Šã€ä¸ªæ€§åŒ–å»ºè®®ã€‚ä½¿ç”¨ç”¨æˆ·çš„ç”Ÿå‘½äº‹ä»¶å’Œä¹ æƒ¯çš„å› æœå›¾ï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†å’Œå‡è®¾æ¨ç†è¿›è¡Œç›®æ ‡å¯¼å‘çš„å› æœéå†ï¼Œæ£€ç´¢é€‚åº”æ€§çš„è®¡åˆ’æ¨¡å¼ç”Ÿæˆä¸ªæ€§åŒ–çš„è¡ŒåŠ¨è®¡åˆ’ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åè°ƒè¿™äº›ç»„ä»¶ï¼Œäº§ç”Ÿå…·æœ‰é€æ˜å› æœè§£é‡Šçš„ç­”æ¡ˆã€‚æœ¬æ–‡ä»‹ç»CSMç³»ç»Ÿè®¾è®¡ï¼Œå¹¶å¼•å…¥æ–°çš„è¯„ä¼°æŒ‡æ ‡ä»¥è¯„ä¼°ä¸ªæ€§åŒ–ç¨‹åº¦å’Œè§£é‡Šæ€§ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–æ˜¾è‘—åˆ†æ•°å’Œå› æœæ¨ç†å‡†ç¡®æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºCSMçš„ä»£ç†å¯ä»¥æä¾›æ›´åŸºäºä¸Šä¸‹æ–‡å’Œç”¨æˆ·å¯¹é½çš„å»ºè®®ï¼Œç›¸è¾ƒäºåŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è¡¨ç°æ›´ä¼˜ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†åœ¨ä¸ªæ€§åŒ–ä»£ç†ä¸­å®ç°å¢å¼ºè®°å¿†å’Œå› æœæ¨ç†çš„æ–°æ–¹æ³•ï¼Œæ¨åŠ¨é€æ˜åº¦å’Œå¯ä¿¡èµ–çš„AIç”Ÿæ´»åŠ©ç†çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰AIåŠ©ç†åœ¨æ•´åˆå¤æ‚ä¸ªäººæ•°æ®å’Œå› æœçŸ¥è¯†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´å»ºè®®ç¼ºä¹è§£é‡ŠåŠ›ã€‚</li>
<li>REMIæ¶æ„ç»“åˆäº†ä¸ªäººå› æœçŸ¥è¯†å›¾è°±ã€å› æœæ¨ç†å¼•æ“å’ŒåŸºäºæ¨¡å¼çš„è§„åˆ’æ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ¶æ„æ—¨åœ¨åœ¨æ—¶å°šã€ä¸ªäººå¥åº·å’Œç”Ÿæ´»è§„åˆ’ç­‰é¢†åŸŸæä¾›å¯è§£é‡Šã€ä¸ªæ€§åŒ–çš„å»ºè®®ã€‚</li>
<li>ä½¿ç”¨ç”¨æˆ·çš„ç”Ÿå‘½äº‹ä»¶å’Œä¹ æƒ¯çš„å› æœå›¾è¿›è¡Œç›®æ ‡å¯¼å‘çš„å› æœéå†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºåè°ƒå„ä¸ªç»„ä»¶ï¼Œäº§ç”Ÿå…·æœ‰é€æ˜å› æœè§£é‡Šçš„ç­”æ¡ˆã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ä¸ªæ€§åŒ–æ˜¾è‘—åˆ†æ•°å’Œå› æœæ¨ç†å‡†ç¡®æ€§ä»¥è¯„ä¼°ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e48fbcccfeb5f351090436536c0fb485.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94b7d90a247d56115e2231838c3b1872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53c102d2a991d3c9b67efa0c519a2238.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61b5eab75713fac100b9a6cc478b7c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f978b8e064b6e989be6a22d2281670c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PillagerBench-Benchmarking-LLM-Based-Agents-in-Competitive-Minecraft-Team-Environments"><a href="#PillagerBench-Benchmarking-LLM-Based-Agents-in-Competitive-Minecraft-Team-Environments" class="headerlink" title="PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft   Team Environments"></a>PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft   Team Environments</h2><p><strong>Authors:Olivier Schipper, Yudi Zhang, Yali Du, Mykola Pechenizkiy, Meng Fang</strong></p>
<p>LLM-based agents have shown promise in various cooperative and strategic reasoning tasks, but their effectiveness in competitive multi-agent environments remains underexplored. To address this gap, we introduce PillagerBench, a novel framework for evaluating multi-agent systems in real-time competitive team-vs-team scenarios in Minecraft. It provides an extensible API, multi-round testing, and rule-based built-in opponents for fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based multi-agent system that facilitates teamwork through human-readable tactics, learns causal dependencies, and adapts to opponent strategies. Our evaluation demonstrates that TactiCrafter outperforms baseline approaches and showcases adaptive learning through self-play. Additionally, we analyze its learning process and strategic evolution over multiple game episodes. To encourage further research, we have open-sourced PillagerBench, fostering advancements in multi-agent AI for competitive environments. </p>
<blockquote>
<p>åŸºäºLLMçš„ä»£ç†åœ¨å„ç§åˆä½œå’Œç­–ç•¥æ¨ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ç«äº‰æ€§çš„å¤šä»£ç†ç¯å¢ƒä¸­ï¼Œå…¶æœ‰æ•ˆæ€§ä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PillagerBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåœ¨Minecraftä¸­å®æ—¶ç«æŠ€å›¢é˜Ÿå¯¹æŠ—å›¢é˜Ÿåœºæ™¯ä¸­è¯„ä¼°å¤šä»£ç†ç³»ç»Ÿçš„æ–°å‹æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„APIã€å¤šè½®æµ‹è¯•ä»¥åŠåŸºäºè§„åˆ™çš„å†…ç½®å¯¹æ‰‹ï¼Œä»¥å®ç°å…¬å¹³ã€å¯é‡å¤çš„æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†TactiCrafterï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼Œå®ƒé€šè¿‡äººç±»å¯è¯»çš„æˆ˜æœ¯ä¿ƒè¿›å›¢é˜Ÿåˆä½œï¼Œå­¦ä¹ å› æœä¾èµ–å…³ç³»ï¼Œå¹¶é€‚åº”å¯¹æ‰‹çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒTactiCrafterä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘æ¸¸æˆå±•ç¤ºè‡ªé€‚åº”å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶åœ¨å¤šä¸ªæ¸¸æˆç‰‡æ®µä¸­çš„å­¦ä¹ è¿‡ç¨‹ä»¥åŠç­–ç•¥æ¼”å˜ã€‚ä¸ºäº†é¼“åŠ±è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å·²å¼€æºPillagerBenchï¼Œä¿ƒè¿›ç«äº‰ç¯å¢ƒä¸­å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06235v1">PDF</a> for the source code, see <a target="_blank" rel="noopener" href="https://github.com/aialt/PillagerBench">https://github.com/aialt/PillagerBench</a></p>
<p><strong>Summary</strong></p>
<p>LLM-basedä»£ç†åœ¨å¤šç§åˆä½œå’Œç­–ç•¥æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨ç«äº‰å¤šä»£ç†ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œç ”ç©¶è€…æ¨å‡ºPillagerBenchæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åœ¨Minecraftä¸­å®æ—¶ç«äº‰å›¢é˜Ÿå¯¹æŠ—åœºæ™¯çš„å¤šä»£ç†ç³»ç»Ÿã€‚è¯¥æ¡†æ¶æä¾›å¯æ‰©å±•APIã€å¤šè½®æµ‹è¯•ï¼Œä»¥åŠåŸºäºè§„åˆ™çš„å†…ç½®å¯¹æ‰‹ï¼Œä»¥å®ç°å…¬å¹³ã€å¯é‡å¤çš„æ¯”è¾ƒã€‚åŒæ—¶ï¼Œæå‡ºTactiCrafterç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŸºäºLLMï¼Œé€šè¿‡äººç±»å¯é˜…è¯»æˆ˜æœ¯ä¿ƒè¿›å›¢é˜Ÿåˆä½œï¼Œå­¦ä¹ å› æœå…³ç³»ï¼Œå¹¶é€‚åº”å¯¹æ‰‹ç­–ç•¥ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒTactiCrafterä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå±•ç¤ºé€šè¿‡è‡ªæˆ‘åšå¼ˆçš„é€‚åº”æ€§å­¦ä¹ ã€‚æ­¤å¤–ï¼Œåˆ†æå…¶åœ¨å¤šä¸ªæ¸¸æˆé›†ä¸­çš„å­¦ä¹ è¿‡ç¨‹åŠç­–ç•¥æ¼”å˜ã€‚ä¸ºé¼“åŠ±è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå·²å¼€æºPillagerBenchï¼Œä¿ƒè¿›ç«äº‰ç¯å¢ƒä¸­å¤šä»£ç†äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-basedä»£ç†åœ¨åˆä½œå’Œç­–ç•¥æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>åœ¨ç«äº‰å¤šä»£ç†ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§éœ€è¦æ›´å¤šæ¢ç´¢ã€‚</li>
<li>PillagerBenchæ¡†æ¶ç”¨äºè¯„ä¼°å®æ—¶ç«äº‰å›¢é˜Ÿå¯¹æŠ—åœºæ™¯ä¸­çš„å¤šä»£ç†ç³»ç»Ÿã€‚</li>
<li>PillagerBenchæä¾›å¯æ‰©å±•APIã€å¤šè½®æµ‹è¯•åŠå†…ç½®å¯¹æ‰‹ã€‚</li>
<li>TactiCrafteræ˜¯åŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼Œèƒ½ä¿ƒè¿›å›¢é˜Ÿåˆä½œï¼Œå­¦ä¹ å› æœå…³ç³»å¹¶é€‚åº”å¯¹æ‰‹ç­–ç•¥ã€‚</li>
<li>TactiCrafterä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå±•ç°é€‚åº”æ€§å­¦ä¹ å¹¶é€šè¿‡è‡ªæˆ‘åšå¼ˆä¸æ–­è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1f19b014a5a6099bb21d085bac680a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c2c38f0956d4a3dc9b17bddbb0ab12d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceaf10c974e8632c0fc6a5061061ea06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9857f2d40bc6202fc9cbfe6618cec84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee50fdf2bf13e542c89cf422d46378d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-459489bf90799eb0cc8163698811f3a4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TalkToAgent-A-Human-centric-Explanation-of-Reinforcement-Learning-Agents-with-Large-Language-Models"><a href="#TalkToAgent-A-Human-centric-Explanation-of-Reinforcement-Learning-Agents-with-Large-Language-Models" class="headerlink" title="TalkToAgent: A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models"></a>TalkToAgent: A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models</h2><p><strong>Authors:Haechang Kim, Hao Chen, Can Li, Jong Min Lee</strong></p>
<p>Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agentâ€™s actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agentâ€™s actions and contextualized their meaning within the problem domain. </p>
<blockquote>
<p>å¯è§£é‡Šæ€§å¼ºåŒ–å­¦ä¹ ï¼ˆXRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œåœ¨æå‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†çš„é€æ˜åº¦æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºXRLç»“æœçš„æœ‰é™å¯ç†è§£æ€§å’Œå½“å‰XRLæ–¹æ³•çš„å­¤ç«‹è¦†ç›–ï¼Œä½¿å¾—å¤æ‚RLç­–ç•¥ä¸é¢†åŸŸä¸“å®¶ä¹‹é—´å­˜åœ¨å·®è·ï¼Œä½¿ç”¨æˆ·å¯¹ä½¿ç”¨å“ªäº›å·¥å…·æ„Ÿåˆ°ä¸ç¡®å®šã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TalkToAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œä¸ºRLç­–ç•¥æä¾›äº¤äº’å¼è‡ªç„¶è¯­è¨€è§£é‡Šã€‚è¯¥æ¶æ„åŒ…å«äº”ä¸ªä¸“ä¸šLLMä»£ç†ï¼ˆåè°ƒå™¨ã€è§£é‡Šå™¨ã€ç¼–ç å‘˜ã€è¯„ä¼°å™¨å’Œè°ƒè¯•å™¨ï¼‰ï¼Œä½¿TalkToAgentèƒ½å¤Ÿè‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³çš„XRLå·¥å…·ï¼Œå¹¶æ ¹æ®å…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šæ¾„æ¸…ä»£ç†çš„è¡ŒåŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»å®šæ€§è¡Œä¸ºæè¿°æˆ–ç”šè‡³æ–°çš„åŸºäºè§„åˆ™çš„ç­–ç•¥ä¸­æ¨å¯¼å‡ºæ›¿ä»£åœºæ™¯ï¼Œæ‰©å±•äº†ä¹‹å‰çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬åœ¨è‘—åçš„éçº¿æ€§æ§åˆ¶åŸºå‡†â€”â€”å››ç½è¿‡ç¨‹æ§åˆ¶é—®é¢˜ä¸ŠéªŒè¯äº†TalkToAgentã€‚ç»“æœè¡¨æ˜ï¼ŒTalkToAgentèƒ½å¤ŸæˆåŠŸåœ°å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ï¼Œä¸”å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œç¼–ç å™¨å’Œè°ƒè¯•å™¨ä¹‹é—´çš„äº¤äº’æœ€å°åŒ–äº†åäº‹å®ç”Ÿæˆä¸­çš„æ•…éšœã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯å®ï¼ŒTalkToAgentæœ‰æ•ˆåœ°è§£é‡Šäº†ä»£ç†çš„è¡ŒåŠ¨ï¼Œå¹¶åœ¨é—®é¢˜åŸŸå†…å¯¹å…¶æ„ä¹‰è¿›è¡Œäº†è¯­å¢ƒåŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04809v2">PDF</a> 31 pages total</p>
<p><strong>Summary</strong>ï¼šå‡ºç°äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå³é€šè¿‡Explainable Reinforcement Learningï¼ˆXRLï¼‰æé«˜å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†çš„é€æ˜åº¦ã€‚ä¸ºè§£å†³å½“å‰XRLç»“æœç†è§£æœ‰é™å’Œç°æœ‰XRLæ–¹æ³•å­¤ç«‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TalkToAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œå¯ä¸ºRLç­–ç•¥æä¾›äº¤äº’å¼è‡ªç„¶è¯­è¨€è§£é‡Šã€‚è¯¥æ¶æ„åŒ…å«äº”ä¸ªä¸“ä¸šLLMä»£ç†ï¼Œå¯è‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³çš„XRLå·¥å…·ï¼Œå¹¶æ¾„æ¸…ä»£ç†å…³äºå…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šçš„ä¸¾åŠ¨ã€‚æˆ‘ä»¬éªŒè¯äº†TalkToAgentåœ¨å¤„ç†å››é‡æ°´ç®±è¿‡ç¨‹æ§åˆ¶é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜TalkToAgentèƒ½å¤Ÿå‡†ç¡®åœ°å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ï¼Œä¸”ç¼–ç å™¨å’Œè°ƒè¯•å™¨ä¹‹é—´çš„äº’åŠ¨å‡å°‘äº†åäº‹å®ç”Ÿæˆä¸­çš„å¤±è´¥ã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯å®TalkToAgentæœ‰æ•ˆåœ°è§£é‡Šäº†ä»£ç†çš„è¡ŒåŠ¨å¹¶å°†å…¶ç½®äºé—®é¢˜åŸŸä¸­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>XRLæé«˜äº†RLä»£ç†çš„é€æ˜åº¦ã€‚</li>
<li>TalkToAgentæ˜¯ä¸€ä¸ªå¤šä»£ç†LLMæ¡†æ¶ï¼Œä¸ºRLç­–ç•¥æä¾›äº¤äº’å¼è‡ªç„¶è¯­è¨€è§£é‡Šã€‚</li>
<li>TalkToAgentåŒ…å«äº”ä¸ªä¸“ä¸šLLMä»£ç†ï¼Œå¯è‡ªåŠ¨æ˜ å°„ç”¨æˆ·æŸ¥è¯¢åˆ°ç›¸å…³XRLå·¥å…·ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½æ¾„æ¸…ä»£ç†å…³äºå…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šçš„è¡Œä¸ºã€‚</li>
<li>åœ¨å››é‡æ°´ç®±è¿‡ç¨‹æ§åˆ¶é—®é¢˜ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒTalkToAgentèƒ½å‡†ç¡®æ˜ å°„ç”¨æˆ·æŸ¥è¯¢åˆ°XRLä»»åŠ¡ã€‚</li>
<li>ç¼–ç å™¨å’Œè°ƒè¯•å™¨ä¹‹é—´çš„äº’åŠ¨å‡å°‘äº†åäº‹å®ç”Ÿæˆä¸­çš„å¤±è´¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2cc606d190358a6e6a6331a506c524a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819d41a158cf77cade41ad1f3bb840c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d554d24483eb08956d8b48d3635cb8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf8b5023176e5fb67538773d748254d0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Preacher-Paper-to-Video-Agentic-System"><a href="#Preacher-Paper-to-Video-Agentic-System" class="headerlink" title="Preacher: Paper-to-Video Agentic System"></a>Preacher: Paper-to-Video Agentic System</h2><p><strong>Authors:Jingwei Liu, Ling Yang, Hao Luo, Fan Wang, Hongyan Li, Mengdi Wang</strong></p>
<p>The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a topdown approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/Paper2Video">https://github.com/Gen-Verse/Paper2Video</a> </p>
<blockquote>
<p>è¿™ç¯‡è®ºæ–‡å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è§†é¢‘æ‘˜è¦ï¼Œå°†å…³é”®æ¦‚å¿µã€æ–¹æ³•å’Œç»“è®ºè½¬åŒ–ä¸ºæ˜“äºè®¿é—®ã€ç»„ç»‡è‰¯å¥½çš„æ ¼å¼ã€‚å°½ç®¡æœ€æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€å›ºå®šçš„è§†é¢‘æ—¶é•¿çº¦æŸã€æœ‰é™çš„é£æ ¼å¤šæ ·æ€§ä»¥åŠæ— æ³•è¡¨ç¤ºç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Preacherï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè®ºæ–‡åˆ°è§†é¢‘çš„æ™ºèƒ½ç³»ç»Ÿã€‚Preacheré‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•åˆ†è§£ã€æ€»ç»“å’Œé‡æ„è®ºæ–‡ï¼Œç„¶åè¿›è¡Œè‡ªä¸‹è€Œä¸Šçš„è§†é¢‘ç”Ÿæˆï¼Œå°†å„ç§è§†é¢‘ç‰‡æ®µåˆæˆä¸€ä¸ªè¿è´¯çš„æ‘˜è¦ã€‚ä¸ºäº†å¯¹é½è·¨æ¨¡æ€è¡¨ç¤ºï¼Œæˆ‘ä»¬å®šä¹‰äº†å…³é”®åœºæ™¯å¹¶å¼•å…¥äº†æ¸è¿›å¼æ€ç»´é“¾ï¼ˆP-CoTï¼‰è¿›è¡Œç²¾ç»†ã€è¿­ä»£çš„è§„åˆ’ã€‚PreacheræˆåŠŸåœ°åœ¨äº”ä¸ªç ”ç©¶é¢†åŸŸç”Ÿæˆäº†é«˜è´¨é‡çš„è§†é¢‘æ‘˜è¦ï¼Œå±•ç°å‡ºè¶…è¶Šå½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¸“é•¿ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/Paper2Video%E5%8F%97%E7%BD%B2%E5%B8%83%E3%80%82">https://github.com/Gen-Verse/Paper2Videoå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09632v6">PDF</a> ICCV 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/Paper2Video">https://github.com/Gen-Verse/Paper2Video</a></p>
<p><strong>Summary</strong><br>è®ºæ–‡è½¬è§†é¢‘ä»»åŠ¡æ˜¯å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºç»“æ„åŒ–è§†é¢‘æ‘˜è¦ï¼Œå°†å…³é”®æ¦‚å¿µã€æ–¹æ³•å’Œç»“è®ºè½¬åŒ–ä¸ºå¯è®¿é—®ã€ç»„ç»‡è‰¯å¥½çš„æ ¼å¼ã€‚ä¸ºè§£å†³å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚æœ‰é™ä¸Šä¸‹æ–‡çª—å£ã€è§†é¢‘æŒç»­æ—¶é—´é™åˆ¶ã€é£æ ¼å•ä¸€ä»¥åŠæ— æ³•è¡¨è¾¾é¢†åŸŸç‰¹å®šçŸ¥è¯†ç­‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†Preacherç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„æ–¹å¼åˆ†è§£ã€æ€»ç»“å’Œé‡æ„è®ºæ–‡ï¼Œç„¶åé€šè¿‡è‡ªä¸‹è€Œä¸Šçš„è§†é¢‘ç”Ÿæˆæ–¹å¼ï¼Œå°†å¤šæ ·åŒ–çš„è§†é¢‘ç‰‡æ®µåˆæˆä¸€ä¸ªè¿è´¯çš„æ‘˜è¦ã€‚ä¸ºå®ç°è·¨æ¨¡æ€è¡¨ç¤ºçš„å¯¹é½ï¼Œæˆ‘ä»¬å®šä¹‰äº†å…³é”®åœºæ™¯å¹¶å¼•å…¥äº†æ¸è¿›æ€ç»´é“¾ï¼ˆP-CoTï¼‰è¿›è¡Œç²¾ç»†çš„è¿­ä»£è§„åˆ’ã€‚PreacheræˆåŠŸç”Ÿæˆäº†äº”ä¸ªç ”ç©¶é¢†åŸŸçš„é«˜è´¨é‡è§†é¢‘æ‘˜è¦ï¼Œå±•ç°äº†è¶…è¶Šç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¸“ä¸šèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡è½¬è§†é¢‘ä»»åŠ¡æ—¨åœ¨å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºç»“æ„åŒ–è§†é¢‘æ‘˜è¦ï¼Œæé«˜ä¿¡æ¯çš„å¯è®¿é—®æ€§å’Œç»„ç»‡æ€§ã€‚</li>
<li>å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹å­˜åœ¨å¤šç§å±€é™æ€§ï¼Œå¦‚ä¸Šä¸‹æ–‡çª—å£æœ‰é™ã€è§†é¢‘æ—¶é•¿çº¦æŸã€é£æ ¼å•ä¸€ä»¥åŠæ— æ³•è¡¨è¾¾é¢†åŸŸç‰¹å®šçŸ¥è¯†ç­‰ã€‚</li>
<li>Preacheræ˜¯é¦–ä¸ªè®ºæ–‡è½¬è§†é¢‘çš„æ™ºèƒ½ç³»ç»Ÿï¼Œé‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•å¤„ç†è®ºæ–‡ï¼Œå¹¶è¾…ä»¥è‡ªä¸‹è€Œä¸Šçš„è§†é¢‘ç”Ÿæˆã€‚</li>
<li>Preacheré€šè¿‡åˆ†è§£ã€æ€»ç»“å’Œé‡æ„è®ºæ–‡ï¼Œæœ‰æ•ˆæç‚¼å…³é”®ä¿¡æ¯ã€‚</li>
<li>ä¸ºå®ç°è·¨æ¨¡æ€è¡¨ç¤ºçš„å¯¹é½ï¼ŒPreacherå®šä¹‰äº†å…³é”®åœºæ™¯å¹¶å¼•å…¥äº†æ¸è¿›æ€ç»´é“¾ï¼ˆP-CoTï¼‰è¿›è¡Œç²¾ç»†çš„è¿­ä»£è§„åˆ’ã€‚</li>
<li>Preacherç³»ç»ŸæˆåŠŸç”Ÿæˆäº†äº”ä¸ªç ”ç©¶é¢†åŸŸçš„é«˜è´¨é‡è§†é¢‘æ‘˜è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4dcf613d1cdd87fbd949e2b09db55a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c35f78f977b7b562e643467f781ae79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aabd58efacb3021bc328b855ffb6879f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-796ef1f12ffd673831080a00b4094a7f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Reasoning-for-Cardiovascular-Imaging-Phenotype-Analysis"><a href="#Multi-Agent-Reasoning-for-Cardiovascular-Imaging-Phenotype-Analysis" class="headerlink" title="Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis"></a>Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis</h2><p><strong>Authors:Weitong Zhang, Mengyun Qiao, Chengqi Zang, Steven Niederer, Paul M Matthews, Wenjia Bai, Bernhard Kainz</strong></p>
<p>Identifying associations between imaging phenotypes, disease risk factors, and clinical outcomes is essential for understanding disease mechanisms. However, traditional approaches rely on human-driven hypothesis testing and selection of association factors, often overlooking complex, non-linear dependencies among imaging phenotypes and other multi-modal data. To address this, we introduce Multi-agent Exploratory Synergy for the Heart (MESHAgents): a framework that leverages large language models as agents to dynamically elicit, surface, and decide confounders and phenotypes in association studies. Specifically, we orchestrate a multi-disciplinary team of AI agents, which spontaneously generate and converge on insights through iterative, self-organizing reasoning. The framework dynamically synthesizes statistical correlations with multi-expert consensus, providing an automated pipeline for phenome-wide association studies (PheWAS). We demonstrate the systemâ€™s capabilities through a population-based study of imaging phenotypes of the heart and aorta. MESHAgents autonomously uncovered correlations between imaging phenotypes and a wide range of non-imaging factors, identifying additional confounder variables beyond standard demographic factors. Validation on diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve performance comparable to expert-selected phenotypes, with mean AUC differences as small as $-0.004_{\pm0.010}$ on disease classification tasks. Notably, the recall score improves for 6 out of 9 disease types. Our framework provides clinically relevant imaging phenotypes with transparent reasoning, offering a scalable alternative to expert-driven methods. </p>
<blockquote>
<p>è¯†åˆ«æˆåƒè¡¨å‹ã€ç–¾ç—…é£é™©å› ç´ å’Œä¸´åºŠç»“æœä¹‹é—´çš„å…³è”å¯¹äºç†è§£ç–¾ç—…æœºåˆ¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºäººä¸ºé©±åŠ¨çš„å‡è®¾æ£€éªŒå’Œå…³è”å› ç´ çš„é€‰æ‹©ï¼Œå¾€å¾€å¿½ç•¥äº†æˆåƒè¡¨å‹å’Œå…¶ä»–å¤šæ¨¡å¼æ•°æ®ä¹‹é—´å¤æ‚ã€éçº¿æ€§çš„ä¾èµ–å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¿ƒè„å¤šæ™ºèƒ½ä½“æ¢ç´¢ååŒï¼ˆMESHAgentsï¼‰ï¼šä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä½“æ¥åŠ¨æ€æå–ã€æ˜¾ç¤ºå’Œå†³å®šå…³è”ç ”ç©¶ä¸­çš„æ··æ‚å› ç´ å’Œè¡¨å‹çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åè°ƒäº†ä¸€ä¸ªè·¨å­¦ç§‘çš„AIæ™ºèƒ½ä½“å›¢é˜Ÿï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½è‡ªå‘åœ°ç”Ÿæˆè§è§£å¹¶é€šè¿‡è¿­ä»£ã€è‡ªæˆ‘ç»„ç»‡æ¨ç†è¾¾æˆå…±è¯¥æ™ºèƒ½ä½“å›¢é˜Ÿèƒ½å¤Ÿè‡ªå‘äº§ç”Ÿè§è§£ï¼Œå¹¶é€šè¿‡è¿­ä»£å’Œè‡ªæˆ‘ç»„ç»‡æ¨ç†è¾¾æˆå…±é€šè¿‡å¯¹æ··æ‚å› ç´ çš„ç ”ç©¶å’Œå¤šé¢†åŸŸä¸“å®¶çš„å…±è¯†ï¼Œæ¡†æ¶åŠ¨æ€åˆæˆç»Ÿè®¡ç›¸å…³æ€§ï¼Œä¸ºå…¨ç°è±¡å…³è”ç ”ç©¶ï¼ˆPheWASï¼‰æä¾›è‡ªåŠ¨åŒ–ç®¡é“ã€‚æˆ‘ä»¬é€šè¿‡ä¸€é¡¹åŸºäºäººç¾¤çš„å¿ƒè„å’Œå¤§åŠ¨è„‰æˆåƒè¡¨å‹ç ”ç©¶æ¥å±•ç¤ºç³»ç»Ÿçš„èƒ½åŠ›ã€‚MESHAgentsè‡ªä¸»åœ°å‘ç°äº†æˆåƒè¡¨å‹ä¸ä¸€ç³»åˆ—éæˆåƒå› ç´ ä¹‹é—´çš„å…³è”ï¼Œé™¤äº†æ ‡å‡†çš„äººå£å­¦å› ç´ å¤–ï¼Œè¿˜ç¡®å®šäº†é¢å¤–çš„æ··æ‚å˜é‡ã€‚åœ¨è¯Šæ–­ä»»åŠ¡ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒMESHAgentså‘ç°çš„è¡¨å‹åœ¨ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä¸“å®¶é€‰æ‹©çš„è¡¨å‹ç›¸å½“ï¼Œå¹³å‡AUCå·®å¼‚ä»…ä¸ºÂ±0.01èŒƒå›´å†…ï¼Œæœ‰9ç§ç–¾ç—…ä¸­æœ‰å…­ç§ç–¾ç—…çš„å¬å›ç‡æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†å…·æœ‰é€æ˜æ¨ç†çš„ä¸´åºŠç›¸å…³æˆåƒè¡¨å‹ï¼Œä¸ºä¸“å®¶é©±åŠ¨çš„æ–¹æ³•æä¾›äº†å¯æ›¿ä»£çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03460v2">PDF</a> accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæˆåƒè¡¨å‹ã€ç–¾ç—…é£é™©å› ç´ ä¸ä¸´åºŠç»“æœä¹‹é—´çš„å…³è”ï¼Œå¯¹äºç†è§£ç–¾ç—…æœºåˆ¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºäººä¸ºé©±åŠ¨çš„å‡è®¾æ£€éªŒå’Œå…³è”å› ç´ çš„é€‰æ‹©ï¼Œå¾€å¾€å¿½ç•¥äº†æˆåƒè¡¨å‹ä¹‹é—´ä»¥åŠå…¶ä»–å¤šæ¨¡å¼æ•°æ®ä¹‹é—´å¤æ‚çš„éçº¿æ€§ä¾èµ–å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥Multi-agent Exploratory Synergy for the Heartï¼ˆMESHAgentsï¼‰ï¼šä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºä»£ç†çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°å¼•å‘ã€æ˜¾ç°å¹¶å†³å®šå…³è”ç ”ç©¶ä¸­çš„æ··æ·†å› ç´ å’Œè¡¨å‹ã€‚é€šè¿‡äººå£åŸºç¡€çš„å¿ƒå’Œä¸»åŠ¨è„‰æˆåƒè¡¨å‹ç ”ç©¶ï¼ŒMESHAgentsè‡ªä¸»åœ°æ­ç¤ºäº†æˆåƒè¡¨å‹ä¸ä¸€ç³»åˆ—éæˆåƒå› ç´ ä¹‹é—´çš„è”ç³»ï¼Œç¡®å®šäº†è¶…å‡ºæ ‡å‡†äººå£ç»Ÿè®¡å› ç´ çš„é¢å¤–æ··æ·†å˜é‡ã€‚åœ¨è¯Šæ–­ä»»åŠ¡ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒMESHAgentså‘ç°çš„è¡¨å‹åœ¨ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸ä¸“å®¶é€‰æ‹©çš„è¡¨å‹ç›¸å½“ï¼Œå¹³å‡AUCå·®å¼‚ä»…ä¸º$-0.004_{\pm0.010}$ï¼Œä¸”9ç§ç–¾ç—…ä¸­æœ‰6ç§çš„å¬å›ç‡æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†å…·æœ‰é€æ˜æ¨ç†çš„ä¸´åºŠç›¸å…³æˆåƒè¡¨å‹ï¼Œä¸ºä¸“å®¶é©±åŠ¨çš„æ–¹æ³•æä¾›äº†å¯ä¼¸ç¼©çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å…³è”ç ”ç©¶ä¸­å¿½ç•¥äº†æˆåƒè¡¨å‹ä¸å…¶ä»–æ•°æ®é—´çš„å¤æ‚éçº¿æ€§å…³ç³»ã€‚</li>
<li>MESHAgentsæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºä»£ç†ï¼Œèƒ½åŠ¨æ€ç¡®å®šå…³è”ç ”ç©¶ä¸­çš„æ··æ·†å› ç´ å’Œè¡¨å‹ã€‚</li>
<li>é€šè¿‡äººå£åŸºç¡€ç ”ç©¶ï¼ŒMESHAgentså‘ç°äº†æˆåƒè¡¨å‹ä¸éæˆåƒå› ç´ ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>MESHAgentsç¡®å®šçš„è¡¨å‹åœ¨ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸ä¸“å®¶é€‰æ‹©çš„è¡¨å‹ç›¸å½“ã€‚</li>
<li>MESHAgentsèƒ½æé«˜è¯Šæ–­ä»»åŠ¡çš„å¬å›ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›ç–¾ç—…ç±»å‹ä¸Šã€‚</li>
<li>è¯¥æ¡†æ¶æä¾›çš„æˆåƒè¡¨å‹å…·æœ‰é€æ˜æ¨ç†ï¼Œä¸ºä¸“å®¶é©±åŠ¨çš„æ–¹æ³•æä¾›äº†å¯ä¼¸ç¼©çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-520ee24fc6af693b78f12418b7576e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be4b38e6d21ecfb4273ec290b2beab5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91d051e2d0e7bb05ed9ce051f7297653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61d813d0c15c026c5cf0b84952bc0592.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ChatCFD-An-LLM-Driven-Agent-for-End-to-End-CFD-Automation-with-Domain-Specific-Structured-Reasoning"><a href="#ChatCFD-An-LLM-Driven-Agent-for-End-to-End-CFD-Automation-with-Domain-Specific-Structured-Reasoning" class="headerlink" title="ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with   Domain-Specific Structured Reasoning"></a>ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with   Domain-Specific Structured Reasoning</h2><p><strong>Authors:E Fan, Kang Hu, Zhuowen Wu, Jiangyang Ge, Jiawei Miao, Yuzhi Zhang, He Sun, Weizong Wang, Tianhan Zhang</strong></p>
<p>Computational Fluid Dynamics (CFD) is essential for advancing scientific and engineering fields but is hindered by operational complexity, high expertise requirements, and limited accessibility. This paper introduces ChatCFD, an automated agent system for OpenFOAM simulations that processes multi-modal inputs (e.g., research papers, meshes) via an interactive interface, leveraging DeepSeek-R1 and DeepSeek-V3 large language models, a multi-agent architecture, and OpenFOAM knowledge. Its four-stage pipeline (Knowledge Base Construction, User Input Processing, Case File Generation, and Execution and Error Reflection) enables iterative trial-reflection-refinement for intricate setups, supporting diverse physical models and external meshes. Validation on 205 benchmark tutorial cases, 110 perturbed variants, and 2 literature-derived cases shows ChatCFDâ€™s 82.1 percent operational success rate on basic cases, outperforming MetaOpenFOAM (6.2 percent) and Foam-Agent (42.3 percent), and 60-80 percent on literature-derived complex cases. Turbulence model studies show a 40 percent success rate for common models versus 10 percent for rare ones like RNG k-epsilon. Physics coupling analyses reveal higher resource demands for multi-physics-coupled cases, while LLM bias toward simpler setups introduces persistent errors, such as dimensional inconsistency. Ablation studies highlight the efficacy of RAG-based modules and reflection mechanisms. By automating hypothesis testing and parameter exploration, ChatCFD accelerates scientific discovery in fluid mechanics and engineering, addressing LLM limitations through structured design and showing strong potential as a modular component in MCP-based agent networks for collaborative multi-agent systems, paving the way for scalable AI-driven CFD innovation. The code for ChatCFD is available at <a target="_blank" rel="noopener" href="https://github.com/ConMoo/ChatCFD">https://github.com/ConMoo/ChatCFD</a>. </p>
<blockquote>
<p>è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰å¯¹äºæ¨åŠ¨ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸçš„å‘å±•è‡³å…³é‡è¦ï¼Œä½†å—åˆ°æ“ä½œå¤æ‚ã€ä¸“ä¸šè¦æ±‚é«˜ã€å¯è®¿é—®æ€§æœ‰é™çš„é˜»ç¢ã€‚æœ¬æ–‡ä»‹ç»äº†ChatCFDï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºOpenFOAMæ¨¡æ‹Ÿçš„è‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿï¼Œå®ƒé€šè¿‡äº¤äº’å¼æ¥å£å¤„ç†å¤šæ¨¡å¼è¾“å…¥ï¼ˆä¾‹å¦‚ç ”ç©¶è®ºæ–‡ã€ç½‘æ ¼ï¼‰ï¼Œåˆ©ç”¨DeepSeek-R1å’ŒDeepSeek-V3å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šä»£ç†æ¶æ„å’ŒOpenFOAMçŸ¥è¯†ã€‚å…¶å››é˜¶æ®µç®¡é“ï¼ˆçŸ¥è¯†åº“æ„å»ºã€ç”¨æˆ·è¾“å…¥å¤„ç†ã€æ¡ˆä¾‹æ–‡ä»¶ç”Ÿæˆã€æ‰§è¡Œå’Œé”™è¯¯åé¦ˆï¼‰ä¸ºå¤æ‚è®¾ç½®æä¾›äº†è¿­ä»£è¯•éªŒ-åæ€-æ”¹è¿›çš„æ”¯æŒï¼Œæ”¯æŒå¤šç§ç‰©ç†æ¨¡å‹å’Œå¤–éƒ¨ç½‘æ ¼ã€‚å¯¹205ä¸ªåŸºå‡†æ•™ç¨‹æ¡ˆä¾‹ã€110ä¸ªæ‰°åŠ¨å˜ä½“ä»¥åŠ2ä¸ªæ–‡çŒ®è¡ç”Ÿæ¡ˆä¾‹çš„éªŒè¯æ˜¾ç¤ºï¼ŒChatCFDåœ¨åŸºæœ¬æ¡ˆä¾‹ä¸Šçš„æ“ä½œæˆåŠŸç‡ä¸º82.1%ï¼Œè¶…è¿‡äº†MetaOpenFOAMï¼ˆ6.2%ï¼‰å’ŒFoam-Agentï¼ˆ42.3%ï¼‰ï¼Œåœ¨æ–‡çŒ®è¡ç”Ÿå¤æ‚æ¡ˆä¾‹ä¸Šçš„æˆåŠŸç‡ä¸º60-80%ã€‚æ¹æµæ¨¡å‹ç ”ç©¶è¡¨æ˜ï¼Œå¸¸è§æ¨¡å‹çš„æˆåŠŸç‡ä¸º40%ï¼Œè€Œç¨€æœ‰æ¨¡å‹å¦‚RNG k-epsilonçš„æˆåŠŸç‡ä¸º10%ã€‚ç‰©ç†è€¦åˆåˆ†æè¡¨æ˜ï¼Œå¤šç‰©ç†è€¦åˆæ¡ˆä¾‹çš„èµ„æºéœ€æ±‚æ›´é«˜ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ›´ç®€å•è®¾ç½®çš„åå¥½ä¼šå¯¼è‡´æŒä¹…æ€§é”™è¯¯ï¼Œå¦‚å°ºå¯¸ä¸ä¸€è‡´ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†åŸºäºRAGçš„æ¨¡å—å’Œåé¦ˆæœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å‡è®¾æ£€éªŒå’Œå‚æ•°æ¢ç´¢ï¼ŒChatCFDåŠ é€Ÿäº†æµä½“åŠ›å­¦å’Œå·¥ç¨‹ä¸­çš„ç§‘å­¦å‘ç°ï¼Œé€šè¿‡ç»“æ„åŒ–è®¾è®¡è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºä½œä¸ºæ¨¡å—åŒ–ç»„ä»¶åœ¨åŸºäºMCPçš„å¤šä»£ç†ç½‘ç»œä¸­çš„å¼ºå¤§æ½œåŠ›ï¼Œä¸ºå¯æ‰©å±•çš„AIé©±åŠ¨CFDåˆ›æ–°å¥ å®šäº†åŸºç¡€ã€‚ChatCFDçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ConMoo/ChatCFD">https://github.com/ConMoo/ChatCFD</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02019v2">PDF</a> 19 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>CFDï¼ˆè®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼‰åœ¨ç§‘å­¦ä¸å·¥ç¨‹é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†å—åˆ°æ“ä½œå¤æ‚ã€ä¸“ä¸šè¦æ±‚é«˜å’Œå¯è®¿é—®æ€§æœ‰é™çš„é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†ChatCFDç³»ç»Ÿï¼Œå®ƒé€šè¿‡å¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚ç ”ç©¶è®ºæ–‡ã€ç½‘æ ¼ï¼‰å’Œå¤šæ™ºèƒ½ä½“æ¶æ„æ¥è‡ªåŠ¨åŒ–OpenFOAMæ¨¡æ‹Ÿè¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿçš„ç®¡é“åˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼Œèƒ½å¤Ÿå®ç°å¤æ‚è®¾ç½®çš„è¿­ä»£è¯•éªŒã€åæ€å’Œæ”¹è¿›ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼ŒChatCFDåœ¨åŸºæœ¬æ¡ˆä¾‹ä¸Šçš„æ“ä½œæˆåŠŸç‡ä¸º82.1%ï¼Œä¼˜äºMetaOpenFOAMï¼ˆ6.2%ï¼‰å’ŒFoam-Agentï¼ˆ42.3%ï¼‰ï¼Œåœ¨å¤æ‚æ–‡çŒ®æ¡ˆä¾‹ä¸Šçš„æˆåŠŸç‡ä¸º60-80%ã€‚ç„¶è€Œï¼Œå¯¹äºå¤æ‚çš„ç‰©ç†æ¨¡å‹å’Œå¤šç§ç‰©ç†ç°è±¡çš„è€¦åˆåˆ†æéœ€æ±‚è¾ƒé«˜èµ„æºï¼ŒåŒæ—¶å¤§å‹è¯­è¨€æ¨¡å‹å¯¹äºå¤æ‚è®¾ç½®ä¸‹çš„é”™è¯¯å¯èƒ½åå‘ç®€åŒ–ï¼Œä½†ä»å±•ç¤ºäº†åœ¨æ¨¡å—åŒ–æ™ºèƒ½ä½“ç½‘ç»œå’Œå¯æ‰©å±•çš„AIé©±åŠ¨çš„CFDåˆ›æ–°æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚å¦‚éœ€äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œå¯è®¿é—®æˆ‘ä»¬çš„GitHubä»£ç ä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/ConMoo/ChatCFD%E3%80%82">https://github.com/ConMoo/ChatCFDã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ChatCFDæ˜¯ä¸€ä¸ªåŸºäºOpenFOAMçš„è‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿï¼Œç”¨äºå¤„ç†å¤šæ¨¡æ€è¾“å…¥å¹¶ç®€åŒ–è®¡ç®—æµä½“åŠ¨åŠ›å­¦çš„æ¨¡æ‹Ÿè¿‡ç¨‹ã€‚</li>
<li>ChatCFDé€šè¿‡å››ä¸ªé˜¶æ®µçš„ç®¡é“å®ç°å¤æ‚è®¾ç½®çš„è¿­ä»£è¯•éªŒã€åæ€å’Œæ”¹è¿›ã€‚</li>
<li>ChatCFDåœ¨åŸºæœ¬æ¡ˆä¾‹ä¸Šçš„æ“ä½œæˆåŠŸç‡è¾ƒé«˜ï¼Œä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</li>
<li>åœ¨å¤„ç†å¤æ‚ç‰©ç†æ¨¡å‹å’Œå¤šç§ç‰©ç†ç°è±¡çš„è€¦åˆåˆ†ææ—¶ï¼ŒChatCFDå±•ç°äº†å…¶é«˜æ•ˆæ€§ä½†ä¹Ÿæœ‰æ‰€å—é™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚è®¾ç½®æ—¶å¯èƒ½å‡ºç°åå·®ï¼Œéœ€è¦ç»“æ„åŒ–è®¾è®¡æ¥å‡å°‘è¯¯å·®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3af26f181e7a4ad53856f66ea548e05e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea76bfd92920ba11392f33c694ba0e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1b562b04a9a8dc18b7bbf5a2f47c436.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Game-Theory-and-Multi-Agent-Reinforcement-Learning-for-Zonal-Ancillary-Markets"><a href="#Game-Theory-and-Multi-Agent-Reinforcement-Learning-for-Zonal-Ancillary-Markets" class="headerlink" title="Game Theory and Multi-Agent Reinforcement Learning for Zonal Ancillary   Markets"></a>Game Theory and Multi-Agent Reinforcement Learning for Zonal Ancillary   Markets</h2><p><strong>Authors:Francesco Morri, HÃ©lÃ¨ne Le Cadre, Pierre Gruet, Luce Brotcorne</strong></p>
<p>We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones. </p>
<blockquote>
<p>æˆ‘ä»¬é‡‡ç”¨éåˆä½œåšå¼ˆç†è®ºæ¥åˆ»ç”»åŒºåŸŸè¾…åŠ©å¸‚åœºè€¦åˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†è¾…åŠ©å¸‚åœºåˆ¶å®šä¸ºä¸€ä¸ªå¤šé¢†å¯¼è€…å•ä¸€è·Ÿéšè€…çš„åŒå±‚é—®é¢˜ï¼Œéšåå°†å…¶è½¬åŒ–ä¸ºå¸¦æœ‰ä¾§çº¦æŸå’Œéå‡¸å¯è¡Œé›†çš„å¹¿ä¹‰çº³ä»€åšå¼ˆã€‚æˆ‘ä»¬ç¡®å®šäº†å‡è¡¡å­˜åœ¨çš„æ¡ä»¶ï¼Œå¹¶è¯æ˜è¯¥æ¸¸æˆæ˜¯å¹¿ä¹‰æ½œåœ¨åšå¼ˆç»“æ„ã€‚ä¸ºäº†è®¡ç®—å¸‚åœºå‡è¡¡ï¼Œæˆ‘ä»¬ä¾èµ–ä¸¤ç§ç²¾ç¡®æ–¹æ³•ï¼šä¸€ç§æ•´ä½“ä¼˜åŒ–æ–¹æ³•å’ŒGauss-Seidelæœ€ä½³ååº”æ³•ï¼Œæˆ‘ä»¬å°†å®ƒä»¬ä¸å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¯”è¾ƒã€‚åœ¨æ¥è‡ªå¾·å›½å’Œå¥¥åœ°åˆ©çš„çœŸå®æ•°æ®è¿›è¡Œçš„æ¨¡æ‹Ÿè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ å…·æœ‰æœ€å°çš„æ”¶æ•›é€Ÿç‡ï¼Œä½†éœ€è¦é¢„å…ˆè®­ç»ƒï¼Œè€Œæœ€ä½³ååº”æ³•æ˜¯æœ€æ…¢çš„ã€‚åœ¨ç»æµæ–¹é¢ï¼Œä¸ç²¾ç¡®æ–¹æ³•ç›¸æ¯”ï¼Œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯¼è‡´è¾ƒå°çš„å¸‚åœºæˆæœ¬ï¼Œä½†æˆæœ¬è¾ƒé«˜çš„åˆ©ç›Šç›¸å…³è€…åˆ©æ¶¦åˆ†é…å˜åŠ¨æ€§æ›´å¤§ã€‚æ­¤å¤–ï¼ŒåŒºåŸŸä¹‹é—´çš„æ›´å¼ºè€¦åˆå¾€å¾€é™ä½äº†è¾ƒå¤§åŒºåŸŸçš„æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03288v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨éåˆä½œåšå¼ˆç†è®ºç ”ç©¶äº†åŒºåŸŸè¾…åŠ©å¸‚åœºè€¦åˆçš„ç‰¹æ€§ã€‚æ–‡ç« å°†è¾…åŠ©å¸‚åœºå»ºæ¨¡ä¸ºä¸€ä¸ªå¤šé¢†å¯¼å•è·Ÿéšè€…çš„åŒå±‚é—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥è½¬åŒ–ä¸ºå¸¦æœ‰ä¾§çº¦æŸå’Œéå‡¸å¯è¡Œé›†çš„å¹¿ä¹‰çº³ä»€æ¸¸æˆã€‚æ–‡ç« ç¡®å®šäº†å‡è¡¡å­˜åœ¨çš„æ¡ä»¶ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¸¸æˆçš„å¹¿ä¹‰åŠ¿èƒ½æ¸¸æˆç»“æ„ã€‚ä¸ºäº†è®¡ç®—å¸‚åœºå‡è¡¡ï¼Œæ–‡ç« é‡‡ç”¨äº†ä¸¤ç§ç²¾ç¡®æ–¹æ³•ï¼šä¸€ä½“åŒ–ä¼˜åŒ–æ–¹æ³•å’Œé«˜æ–¯-èµ›å¾·å°”æœ€ä½³ååº”æ–¹æ³•ï¼Œå¹¶ä¸å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œäº†æ¯”è¾ƒã€‚åŸºäºå¾·å›½å’Œå¥¥åœ°åˆ©å®é™…æ•°æ®çš„æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ è™½ç„¶è®­ç»ƒæˆæœ¬é«˜ä¸”æ”¶ç›Šåˆ†é…å­˜åœ¨è¾ƒé«˜ä¸ç¡®å®šæ€§ï¼Œä½†æ”¶æ•›é€Ÿåº¦æœ€å¿«ä¸”å¸‚åœºæˆæœ¬è¾ƒä½ã€‚æ­¤å¤–ï¼ŒåŒºåŸŸé—´æ›´å¼ºçš„è€¦åˆè¶‹åŠ¿æœ‰åŠ©äºé™ä½å¤§å‹åŒºåŸŸçš„å¸‚åœºæˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡åˆ©ç”¨éåˆä½œåšå¼ˆç†è®ºç ”ç©¶äº†åŒºåŸŸè¾…åŠ©å¸‚åœºè€¦åˆé—®é¢˜ï¼Œå»ºç«‹äº†åŸºäºå¤šå±‚åšå¼ˆçš„å¸‚åœºæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¯¹å¸‚åœºçš„å¹¿ä¹‰çº³ä»€æ¸¸æˆç»“æ„åˆ†æï¼Œç¡®å®šäº†å‡è¡¡å­˜åœ¨çš„æ¡ä»¶ã€‚</li>
<li>æ–‡ç« é‡‡ç”¨äº†ä¸¤ç§ç²¾ç¡®æ–¹æ³•æ¥è®¡ç®—å¸‚åœºå‡è¡¡ï¼šä¸€ä½“åŒ–ä¼˜åŒ–æ–¹æ³•å’Œé«˜æ–¯-èµ›å¾·å°”æœ€ä½³ååº”æ–¹æ³•ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æ¨¡æ‹Ÿä¸­å±•ç°å‡ºæœ€å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œä½†è®­ç»ƒæˆæœ¬é«˜ä¸”æ”¶ç›Šåˆ†é…å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>ä¸ç²¾ç¡®æ–¹æ³•ç›¸æ¯”ï¼Œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨å¸‚åœºæˆæœ¬æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>åŒºåŸŸé—´æ›´å¼ºçš„è€¦åˆè¶‹åŠ¿æœ‰åŠ©äºé™ä½å¤§å‹åŒºåŸŸçš„å¸‚åœºæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0528de0aacf4bd92d1b2f03e0c379c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aab8256745b380e5ac4da3bf59ae8742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4299302c89b0ca456ceb150f398abaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71bb0a6592c54b405fa40a2e9e64055b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ResearchArena-Benchmarking-Large-Language-Modelsâ€™-Ability-to-Collect-and-Organize-Information-as-Research-Agents"><a href="#ResearchArena-Benchmarking-Large-Language-Modelsâ€™-Ability-to-Collect-and-Organize-Information-as-Research-Agents" class="headerlink" title="ResearchArena: Benchmarking Large Language Modelsâ€™ Ability to Collect   and Organize Information as Research Agents"></a>ResearchArena: Benchmarking Large Language Modelsâ€™ Ability to Collect   and Organize Information as Research Agents</h2><p><strong>Authors:Hao Kang, Chenyan Xiong</strong></p>
<p>Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMsâ€™ capabilities in conducting academic surveys â€“ a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papersâ€™ relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, though recent reasoning models such as DeepSeek-R1 show slightly better zero-shot performance. These results underscore significant opportunities for advancing LLMs in autonomous research. We open-source the code to construct the ResearchArena benchmark at <a target="_blank" rel="noopener" href="https://github.com/cxcscmu/ResearchArena">https://github.com/cxcscmu/ResearchArena</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„åˆ†æä»»åŠ¡ï¼Œå¦‚å¼€å±•ç ”ç©¶è°ƒæŸ¥æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ResearchArenaï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMè¿›è¡Œå­¦æœ¯è°ƒæŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•â€”â€”å­¦æœ¯ç ”ç©¶ä¸­çš„åŸºç¡€æ­¥éª¤ã€‚ResearchArenaå°†è¿‡ç¨‹æ¨¡æ‹Ÿä¸ºä¸‰ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ä¿¡æ¯å‘ç°ï¼Œè¯†åˆ«ç›¸å…³æ–‡çŒ®ï¼›ï¼ˆ2ï¼‰ä¿¡æ¯é€‰æ‹©ï¼Œè¯„ä¼°è®ºæ–‡çš„ç›¸å…³æ€§å’Œå½±å“åŠ›ï¼›ï¼ˆ3ï¼‰ä¿¡æ¯ç»„ç»‡ï¼Œå°†çŸ¥è¯†æ„å»ºæˆå±‚æ¬¡æ¡†æ¶ï¼Œå¦‚æ€ç»´å¯¼å›¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ€ç»´å¯¼å›¾æ„å»ºè¢«è§†ä¸ºé™„åŠ ä»»åŠ¡ï¼Œåæ˜ äº†å…¶åœ¨è°ƒæŸ¥æŠ¥å‘Šä¸­çš„è¾…åŠ©ä½œç”¨ã€‚ä¸ºäº†æ”¯æŒè¿™äº›è¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«1.2äº¿ç¯‡å…¨æ–‡å­¦æœ¯è®ºæ–‡å’Œ7900ç¯‡è°ƒæŸ¥æŠ¥å‘Šçš„ç¦»çº¿ç¯å¢ƒã€‚ä¸ºç¡®ä¿éµå®ˆä¼¦ç†è§„å®šï¼Œæˆ‘ä»¬ä¸é‡æ–°åˆ†å‘å—ç‰ˆæƒä¿æŠ¤çš„ææ–™ï¼›ç›¸åï¼Œæˆ‘ä»¬æä¾›ä»£ç ä»¥ä»è¯­ä¹‰å­¦è€…å¼€æ”¾ç ”ç©¶è¯­æ–™åº“ï¼ˆS2ORCï¼‰æ„å»ºç¯å¢ƒã€‚åˆæ­¥è¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºLLMçš„æ–¹æ³•ç›¸æ¯”äºç®€å•çš„å…³é”®è¯æ£€ç´¢æ–¹æ³•è¡¨ç°è¾ƒå·®ï¼Œå°½ç®¡æœ€è¿‘çš„æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1æ˜¾ç¤ºå‡ºç•¥å¥½çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æ¨è¿›LLMåœ¨è‡ªä¸»ç ”ç©¶æ–¹é¢çš„å·¨å¤§æœºä¼šã€‚æˆ‘ä»¬å…¬å¼€äº†æ„å»ºResearchArenaåŸºå‡†æµ‹è¯•çš„ä»£ç å¦‚ä¸‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/cxcscmu/ResearchArena">https://github.com/cxcscmu/ResearchArena</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10291v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºResearchArenaçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿›è¡Œå­¦æœ¯è°ƒæŸ¥æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥å¹³å°æ¨¡æ‹Ÿäº†å­¦æœ¯è°ƒæŸ¥çš„ä¸‰ä¸ªé˜¶æ®µï¼šä¿¡æ¯å‘ç°ã€ä¿¡æ¯ç­›é€‰å’Œä¿¡æ¯ç»„ç»‡ã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼ŒLLMæ–¹æ³•ç›¸è¾ƒäºç®€å•çš„å…³é”®è¯æ£€ç´¢æ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œä½†æœ€æ–°æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°ç¨å¥½ã€‚è¿™ä¸ºLLMsåœ¨è‡ªä¸»ç ”ç©¶æ–¹é¢çš„è¿›æ­¥æä¾›äº†é‡è¦æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ResearchArenaæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­¦æœ¯è°ƒæŸ¥æ–¹é¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>LLMsåœ¨å­¦æœ¯è°ƒæŸ¥çš„å¤šä¸ªé˜¶æ®µè¡¨ç°å‡ºæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿¡æ¯å‘ç°ã€ç­›é€‰å’Œç»„ç»‡ã€‚</li>
<li>ä¿¡æ¯ç»„ç»‡é˜¶æ®µä¸­çš„æ€ç»´å¯¼å›¾æ„å»ºè¢«è§†ä¸ºé™„åŠ ä»»åŠ¡ï¼Œåæ˜ å…¶åœ¨è°ƒæŸ¥æŠ¥å‘Šå†™ä½œä¸­çš„è¾…åŠ©ä½œç”¨ã€‚</li>
<li>ä¸ºæ”¯æŒè¯„ä¼°ï¼Œæ„å»ºäº†åŒ…å«1.2äº¿ç¯‡å…¨æ–‡å­¦æœ¯è®ºæ–‡å’Œ7900ç¯‡è°ƒæŸ¥æŠ¥å‘Šçš„ç¦»çº¿ç¯å¢ƒã€‚</li>
<li>LLMæ–¹æ³•ç›¸è¾ƒäºç®€å•çš„å…³é”®è¯æ£€ç´¢æ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œä½†æœ€æ–°æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>LLMsåœ¨è‡ªä¸»ç ”ç©¶æ–¹é¢æœ‰å·¨å¤§è¿›æ­¥ç©ºé—´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ac576e7d54079a17b49287cfd17579a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f88d20c53bcbbe757ccf476874070c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6641d7a0470e2ed419ed67dab65de9a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08f049bc352f90852f7b8f12bdce25d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10d7307d2f2f815f2f5c1852f8f79d04.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents"><a href="#Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents" class="headerlink" title="Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents"></a>Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents</h2><p><strong>Authors:Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu</strong></p>
<p>Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population. </p>
<blockquote>
<p>ä»æµè¡Œçš„å¯ç©¿æˆ´è·Ÿè¸ªå™¨ä¸­è·å–ä¸ªæ€§åŒ–æ´å¯Ÿéœ€è¦å¤æ‚çš„æ•°å€¼æ¨ç†ï¼Œè¿™æŒ‘æˆ˜äº†æ ‡å‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œéœ€è¦åŸºäºå·¥å…·çš„æ–¹æ³•ï¼Œå¦‚ä»£ç ç”Ÿæˆã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººä¸ºè¿™ç§å¤§è§„æ¨¡åˆ†ææä¾›äº†å‰æ™¯å¹¿é˜”ä½†å°šæœªå¼€å‘çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸ªäººå¥åº·æ´å¯Ÿä»£ç†ï¼ˆPHIAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢æ¥åˆ†æè§£è¯»è¡Œä¸ºå¥åº·æ•°æ®çš„ç³»ç»Ÿã€‚ä¸ºäº†æµ‹è¯•å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬åˆ›å»ºå¹¶åˆ†äº«äº†åŒ…å«è¶…è¿‡4000ä¸ªå¥åº·æ´å¯Ÿé—®é¢˜æ•°æ®é›†ã€‚ä¸ºæœŸ650å°æ—¶çš„äººç±»ä¸“å®¶è¯„ä¼°æ˜¾ç¤ºï¼ŒPHIAåœ¨å®¢è§‚æ•°å€¼é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—è¶…è¿‡å¼ºå¤§çš„ä»£ç ç”ŸæˆåŸºçº¿ï¼Œè¾¾åˆ°äº†84%ï¼Œå¯¹äºå¼€æ”¾å¼é—®é¢˜ï¼Œè·å¾—äº†83%çš„å¥½è¯„ï¼ŒåŒæ—¶è·å¾—æœ€é«˜è´¨é‡è¯„çº§çš„å¯èƒ½æ€§æ˜¯åŸæ¥çš„ä¸¤å€ã€‚è¿™é¡¹å·¥ä½œå¯ä»¥é€šè¿‡å¸®åŠ©ä¸ªäººç†è§£ä»–ä»¬çš„æ•°æ®æ¥æ¨åŠ¨è¡Œä¸ºå¥åº·çš„å‘å±•ï¼Œä¸ºæ›´å¹¿æ³›çš„äººç¾¤å¼€å¯ä¸€ä¸ªæ˜“äºè®¿é—®ã€ä¸ªæ€§åŒ–å’Œæ•°æ®é©±åŠ¨çš„å¥åº·æ–°æ—¶ä»£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06464v4">PDF</a> 53 pages, 7 main figures, 2 main tables, accepted to Nature   Communications</p>
<p><strong>Summary</strong></p>
<p>ç©¿æˆ´å¼è¿½è¸ªå™¨æ‰€æœé›†çš„ä¸ªäººå¥åº·æ•°æ®ï¼Œéœ€è¦å¤æ‚çš„æ•°å€¼åˆ†ææ‰èƒ½è½¬åŒ–ä¸ºä¸ªäººåŒ–çš„æ´è§ï¼Œè¿™æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥ä¸ªäººå¥åº·æ´è§ä»£ç†ï¼ˆPHIAï¼‰ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨å¤šæ­¥éª¤æ¨ç†ä¸ä»£ç ç”ŸæˆåŠä¿¡æ¯æ£€ç´¢æŠ€æœ¯æ¥åˆ†æå¹¶è§£è¯»è¡Œä¸ºå¥åº·æ•°æ®ã€‚æµ‹è¯•æ˜¾ç¤ºï¼ŒPHIAåœ¨å®¢è§‚æ•°å€¼é—®é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†84%å’Œè·å¾—83%çš„å¥½è¯„ï¼Œä¸”å…¶è¾¾åˆ°æœ€é«˜è´¨é‡è¯„çº§çš„å¯èƒ½æ€§æ˜¯åŸºçº¿æ¨¡å‹çš„ä¸¤å€ã€‚æ­¤ç ”ç©¶å°†æ¨åŠ¨è¡Œä¸ºå¥åº·é¢†åŸŸçš„å‘å±•ï¼Œä½¿ä¸ªäººèƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£è‡ªèº«æ•°æ®ï¼Œä¸ºæ›´å¹¿æ³›çš„äººç¾¤å¸¦æ¥å¯è®¿é—®çš„ã€ä¸ªæ€§åŒ–çš„ã€æ•°æ®é©±åŠ¨çš„ç¦ç¥‰æ–°æ—¶ä»£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©¿æˆ´å¼è¿½è¸ªå™¨çš„æ•°æ®éœ€è¦å¤æ‚çš„æ•°å€¼åˆ†æä»¥è½¬åŒ–ä¸ºä¸ªäººæ´è§ã€‚</li>
<li>ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¿™äº›æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥ä¸ªäººå¥åº·æ´è§ä»£ç†ï¼ˆPHIAï¼‰ï¼Œåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢æŠ€æœ¯è¿›è¡Œåˆ†æã€‚</li>
<li>PHIAåœ¨å®¢è§‚æ•°å€¼é—®é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜çš„æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>PHIAç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œè·å¾—æ›´é«˜çš„ç”¨æˆ·å¥½è¯„ï¼Œä¸”æ›´æœ‰å¯èƒ½è·å¾—æœ€é«˜è´¨é‡è¯„çº§ã€‚</li>
<li>æ­¤ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨è¡Œä¸ºå¥åº·é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d81515ed8e6c789178fa82c361768b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d074e2a4ce10aa11798e1a41e8ac44b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36957c2f0aa9c11b1af2beb0c69c1ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-435501c4677f83889704c7a3156abe03.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-10/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c7733bda78cecbaf3ae9405d71c3fc36.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Slice-100K A Multimodal Dataset for Extrusion-based 3D Printing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4d1eeb0f0bc50c018a456e5ed79a9bf5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Beyond Two-Stage Training Cooperative SFT and RL for LLM Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
