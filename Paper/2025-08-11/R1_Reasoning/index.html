<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-11  CX-Mind A Pioneering Multimodal Large Language Model for Interleaved   Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f2521a48ab6d958e0c2842dd6e3b642c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-11-æ›´æ–°"><a href="#2025-08-11-æ›´æ–°" class="headerlink" title="2025-08-11 æ›´æ–°"></a>2025-08-11 æ›´æ–°</h1><h2 id="CX-Mind-A-Pioneering-Multimodal-Large-Language-Model-for-Interleaved-Reasoning-in-Chest-X-ray-via-Curriculum-Guided-Reinforcement-Learning"><a href="#CX-Mind-A-Pioneering-Multimodal-Large-Language-Model-for-Interleaved-Reasoning-in-Chest-X-ray-via-Curriculum-Guided-Reinforcement-Learning" class="headerlink" title="CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved   Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning"></a>CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved   Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning</h2><p><strong>Authors:Wenjie Li, Yujie Zhang, Haoran Sun, Yueqi Li, Fanrui Zhang, Mengzhe Xu, Victoria Borja Clausich, Sade Mellin, Renhao Yang, Chenrun Wang, Jethro Zih-Shuo Wang, Shiyi Yao, Gen Li, Yidong Xu, Hanyu Wang, Yilin Huang, Angela Lin Wang, Chen Shi, Yin Zhang, Jianan Guo, Luqi Yang, Renxuan Li, Yang Xu, Jiawei Liu, Yao Zhang, Lei Liu, Carlos GutiÃ©rrez SanRomÃ¡n, Lei Wang</strong></p>
<p>Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on â€œone-timeâ€ diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind, the first generative model to achieve interleaved â€œthink-answerâ€ reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æˆåƒæ˜¯åœ¨ä¸´åºŠå®è·µä¸­æœ€å¹¿æ³›ä½¿ç”¨çš„è¯Šæ–­æ–¹æ³•ä¹‹ä¸€ï¼Œæ¶µç›–äº†å¹¿æ³›çš„è¯Šæ–­ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒåŸºäºæ¨ç†çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„å¹¿æ³›åº”ç”¨æé«˜äº†è¯Šæ–­æ•ˆç‡å’Œè§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ä¸»è¦ä¾èµ–äºâ€œä¸€æ¬¡æ€§â€è¯Šæ–­æ–¹æ³•ï¼Œç¼ºä¹æ¨ç†è¿‡ç¨‹çš„å¯éªŒè¯ç›‘ç£ã€‚è¿™å¸¦æ¥äº†åŒ…æ‹¬æ¨ç†è¿‡ç¨‹å†—é•¿ã€å¥–åŠ±ç¨€ç–å’Œé¢‘ç¹å‡ºç°å¹»è§‰ç­‰å¤šä»»åŠ¡CXRè¯Šæ–­çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CX-Mindï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®ç°CXRä»»åŠ¡çš„äº¤æ›¿â€œæ€è€ƒ-å›ç­”â€æ¨ç†çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”±åŸºäºè¯¾ç¨‹çš„å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯çš„è¿‡ç¨‹å¥–åŠ±ï¼ˆCuRL-VPRï¼‰é©±åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«708473å¼ å›¾åƒå’Œ2619148ä¸ªæ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†CX-Setï¼Œä»¥åŠç”±ä¸´åºŠæŠ¥å‘Šç›‘ç£çš„42828ä¸ªé«˜è´¨é‡äº¤æ›¿æ¨ç†æ•°æ®ç‚¹ã€‚åœ¨Group Relative Policy Optimizationæ¡†æ¶ä¸‹åˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œä¼˜åŒ–ï¼šé¦–å…ˆé€šè¿‡å°é—­åŸŸä»»åŠ¡ç¨³å®šåŸºæœ¬æ¨ç†ï¼Œç„¶åè½¬ç§»åˆ°å¼€æ”¾åŸŸè¯Šæ–­ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„æ¡ä»¶è¿‡ç¨‹å¥–åŠ±æ¥ç»•è¿‡å¯¹é¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCX-Mindåœ¨è§†è§‰ç†è§£ã€æ–‡æœ¬ç”Ÿæˆå’Œæ—¶ç©ºå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨é¢†åŸŸMLLMsï¼Œåœ¨åŒç±»CXRç‰¹å®šæ¨¡å‹ä¸Šå¹³å‡æ€§èƒ½æå‡25.1%ã€‚åœ¨çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®é›†ï¼ˆRui-CXRï¼‰ä¸Šï¼ŒCX-Mindåœ¨14ç§ç–¾ç—…ä¸Šçš„å¹³å‡å¬å›ç‡@1å¤§å¤§è¶…è¿‡äº†ç¬¬äºŒåï¼Œå¤šä¸­å¿ƒä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶åœ¨å¤šä¸ªç»´åº¦ä¸Šçš„ä¸´åºŠå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†CXRæˆåƒåœ¨ä¸´åºŠå®è·µä¸­çš„å¹¿æ³›åº”ç”¨ä»¥åŠå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†CX-Mindæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å®ç°äº¤äº’å¼â€œæ€è€ƒ-å›ç­”â€æ¨ç†çš„ç”Ÿæˆæ¨¡å‹ã€‚CX-Mindæ¨¡å‹é‡‡ç”¨åŸºäºè¯¾ç¨‹çš„å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯çš„è¿‡ç¨‹å¥–åŠ±ï¼ˆCuRL-VPRï¼‰ï¼Œå¹¶é€šè¿‡æ„å»ºCX-Setæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCX-Mindåœ¨è§†è§‰ç†è§£ã€æ–‡æœ¬ç”Ÿæˆå’Œæ—¶ç©ºå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨é¢†åŸŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åœ¨çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®é›†ä¸Šï¼ŒCX-Mindåœ¨å¤šç–¾ç—…è¯Šæ–­ä¸Šçš„è¡¨ç°è¶…è¿‡å…¶ä»–æ¨¡å‹ï¼Œå¾—åˆ°äº†å¤šä¸­å¿ƒä¸“å®¶è¯„ä¼°çš„è¿›ä¸€æ­¥ç¡®è®¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CXRæˆåƒåœ¨ä¸´åºŠå®è·µä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†å­˜åœ¨å¤šä»»åŠ¡è¯Šæ–­çš„æŒ‘æˆ˜ï¼Œå¦‚æ¨ç†è¿‡ç¨‹å†—é•¿ã€å¥–åŠ±ç¨€ç–å’Œé¢‘ç¹å‡ºç°å¹»è§‰ã€‚</li>
<li>CX-Mindæ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†äº¤äº’å¼â€œæ€è€ƒ-å›ç­”â€æ¨ç†ï¼Œé€‚ç”¨äºCXRä»»åŠ¡ã€‚</li>
<li>CX-Mindé‡‡ç”¨åŸºäºè¯¾ç¨‹çš„å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯çš„è¿‡ç¨‹å¥–åŠ±ï¼ˆCuRL-VPRï¼‰è¿›è¡Œè®­ç»ƒã€‚</li>
<li>CX-Mindåœ¨è§†è§‰ç†è§£ã€æ–‡æœ¬ç”Ÿæˆå’Œæ—¶ç©ºå¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰æ¨¡å‹å¹³å‡æ€§èƒ½æå‡25.1%ã€‚</li>
<li>CX-Mindåœ¨çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡å…¶ä»–æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç–¾ç—…è¯Šæ–­æ–¹é¢ã€‚</li>
<li>å¤šä¸­å¿ƒä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥ç¡®è®¤äº†CX-Mindçš„ä¸´åºŠå®ç”¨æ€§ã€‚</li>
<li>CX-Setæ•°æ®é›†æ˜¯æ„å»ºCX-Mindæ¨¡å‹çš„å…³é”®ï¼ŒåŒ…å«å¤§é‡å›¾åƒå’Œæ ·æœ¬ï¼Œä»¥åŠç”±ä¸´åºŠæŠ¥å‘Šç›‘ç£çš„é«˜è´¨é‡äº¤äº’å¼æ¨ç†æ•°æ®ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-be16796df8b8f3b19af59e3746fea063.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fc86184f690a19599db9a9a0af307c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding"><a href="#LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding" class="headerlink" title="LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding"></a>LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</strong></p>
<p>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">https://github.com/LLM-VLM-GSL/LLaDA-MedV</a>. </p>
<blockquote>
<p>è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰é•¿æœŸä»¥æ¥åœ¨ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚æœ€è¿‘ï¼ŒåƒLLaDAè¿™æ ·çš„æ©è†œæ‰©æ•£æ¨¡å‹è¢«è¯æ˜æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä»é²œæœ‰æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¼˜åŒ–çš„é¦–ä¸ªå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹LLaDA-MedVã€‚\LLaDA-MedVåœ¨å¼€æ”¾å¼çš„ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸­ç›¸å¯¹äºLLaVA-Medå’ŒLLaDA-Våˆ†åˆ«å®ç°äº†7.855%å’Œ1.867%çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•é›†çš„é—­å¼å­é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼šVQA-RADçš„84.93%ï¼ŒSLAKEçš„92.31%ï¼Œä»¥åŠPathVQAçš„95.15%ã€‚æ­¤å¤–ï¼Œä¸LLaVA-Medçš„è¯¦ç»†æ¯”è¾ƒè¡¨æ˜ï¼ŒLLaDA-MedVèƒ½å¤Ÿé€šè¿‡æ˜ç¡®æ§åˆ¶å“åº”é•¿åº¦æ¥ç”Ÿæˆæ›´é•¿çš„å“åº”ï¼Œä»è€Œäº§ç”Ÿæ›´ä¸°å¯Œçš„è¾“å‡ºä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œé‡ç‚¹æ¢è®¨äº†åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV%E4%B8%8A%E3%80%82">https://github.com/LLM-VLM-GSL/LLaDA-MedVä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaDA-MedVæ˜¯ä¸“ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£è®¾è®¡çš„è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´è¿›è¡Œä¼˜åŒ–ã€‚ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒLLaDA-MedVåœ¨å¼€æ”¾å‹ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸Šå–å¾—äº†ç›¸å¯¹æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•é›†çš„å°é—­å¼å­é›†ä¸Šè¾¾åˆ°æœ€æ–°æœ€é«˜å‡†ç¡®åº¦ã€‚æ­¤å¤–ï¼ŒLLaDA-MedVèƒ½å¤Ÿç”Ÿæˆè¾ƒé•¿çš„å“åº”ï¼Œå…·æœ‰æ˜ç¡®çš„å“åº”é•¿åº¦æ§åˆ¶åŠ›ï¼Œæä¾›æ›´å¤šä¿¡æ¯ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„å…³é”®è¦ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-MedVæ˜¯é¦–ä¸ªé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LLaDA-MedVåœ¨å¼€æ”¾å‹ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>LLaDA-MedVåœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•é›†çš„å°é—­å¼å­é›†ä¸Šè¾¾åˆ°æœ€æ–°æœ€é«˜å‡†ç¡®åº¦ã€‚</li>
<li>LLaDA-MedVèƒ½å¤Ÿç”Ÿæˆè¾ƒé•¿çš„å“åº”ï¼Œå…·æœ‰æ˜ç¡®çš„å“åº”é•¿åº¦æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>ä¸LLaVA-Medç›¸æ¯”ï¼ŒLLaDA-MedVåœ¨ç”Ÿæˆå“åº”æ—¶è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>æ·±å…¥æ¢è®¨äº†è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„å…³é”®è¦ç´ ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-28f50ce6fbe5010fdd218a258d6057b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdde17c0c7023c6476ca6684b57542b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc842df081978fe43604c02fe95adac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c9c5b7226586101902537bd928e01f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a50d503fd1515247626c76e4676af3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-Query-to-Logic-Ontology-Driven-Multi-Hop-Reasoning-in-LLMs"><a href="#From-Query-to-Logic-Ontology-Driven-Multi-Hop-Reasoning-in-LLMs" class="headerlink" title="From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs"></a>From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs</h2><p><strong>Authors:Haonan Bian, Yutao Qi, Rui Yang, Yuanxi Che, Jiaqian Wang, Heming Xia, Ranran Zhen</strong></p>
<p>Large Language Models (LLMs), despite their success in question answering, exhibit limitations in complex multi-hop question answering (MQA) tasks that necessitate non-linear, structured reasoning. This limitation stems from their inability to adequately capture deep conceptual relationships between entities. To overcome this challenge, we present <strong>ORACLE</strong> (<strong>O</strong>ntology-driven <strong>R</strong>easoning <strong>A</strong>nd <strong>C</strong>hain for <strong>L</strong>ogical <strong>E</strong>ucidation), a training-free framework that combines LLMsâ€™ generative capabilities with the structural benefits of knowledge graphs. Our approach operates through three stages: (1) dynamic construction of question-specific knowledge ontologies using LLMs, (2) transformation of these ontologies into First-Order Logic reasoning chains, and (3) systematic decomposition of the original query into logically coherent sub-questions. Experimental results on several standard MQA benchmarks show that our framework achieves highly competitive performance, rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses further confirm the effectiveness of each component, while demonstrating that our method generates more logical and interpretable reasoning chains than existing approaches. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨éœ€è¦éçº¿æ€§ã€ç»“æ„åŒ–æ¨ç†çš„å¤æ‚å¤šè·³é—®ç­”ï¼ˆMQAï¼‰ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬ä»è¡¨ç°å‡ºå±€é™æ€§ã€‚è¿™ä¸€å±€é™æ€§æºäºå®ƒä»¬æ— æ³•å……åˆ†æ•æ‰å®ä½“ä¹‹é—´çš„æ·±å±‚æ¦‚å¿µå…³ç³»ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>ORACLE</strong>ï¼ˆ<strong>O</strong>ntologyé©±åŠ¨çš„<strong>R</strong>easoning <strong>A</strong>nd <strong>C</strong>hainç”¨äº<strong>L</strong>ogical <strong>E</strong>ucidationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†LLMçš„ç”Ÿæˆèƒ½åŠ›ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šä¸€æ˜¯åˆ©ç”¨LLMåŠ¨æ€æ„å»ºé—®é¢˜ç‰¹å®šçš„çŸ¥è¯†æœ¬ä½“ï¼›äºŒæ˜¯å°†è¿™äº›æœ¬ä½“è½¬åŒ–ä¸ºä¸€é˜¶é€»è¾‘æ¨ç†é“¾ï¼›ä¸‰æ˜¯å°†åŸå§‹æŸ¥è¯¢ç³»ç»Ÿåœ°åˆ†è§£ä¸ºé€»è¾‘è¿è´¯çš„å­é—®é¢˜ã€‚åœ¨å¤šä¸ªæ ‡å‡†MQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚DeepSeek-R1ä¸ç›¸ä¸Šä¸‹ã€‚è¯¦ç»†çš„åˆ†æè¿›ä¸€æ­¥è¯å®äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„é€»è¾‘æ¨ç†é“¾æ¯”ç°æœ‰æ–¹æ³•æ›´å…·é€»è¾‘æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01424v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†æ•æ‰å®ä½“é—´çš„æ·±å±‚æ¦‚å¿µå…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒå…è´¹çš„æ¡†æ¶<strong>ORACLE</strong>ï¼Œç»“åˆè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¼˜åŠ¿ï¼Œé€šè¿‡æ„å»ºé—®é¢˜ç‰¹å®šçŸ¥è¯†æœ¬ä½“ã€è½¬æ¢ä¸ºä¸€é˜¶é€»è¾‘æ¨ç†é“¾å’Œåˆ†è§£åŸå§‹æŸ¥è¯¢ä¸ºé€»è¾‘è¿è´¯çš„å­é—®é¢˜ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ ‡å‡†å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é«˜åº¦ç«äº‰åŠ›ï¼Œä¸å½“å‰æœ€å‰æ²¿æ¨¡å‹å¦‚DeepSeek-R1ç›¸åŒ¹æ•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æ•æ‰å®ä½“é—´çš„æ·±å±‚æ¦‚å¿µå…³ç³»ã€‚</li>
<li><strong>ORACLE</strong>æ¡†æ¶é€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’ŒçŸ¥è¯†å›¾è°±çš„ç»“æ„ä¼˜åŠ¿æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li><strong>ORACLE</strong>æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ„å»ºé—®é¢˜ç‰¹å®šçŸ¥è¯†æœ¬ä½“ã€è½¬æ¢ä¸ºé€»è¾‘æ¨ç†é“¾å’Œåˆ†è§£åŸå§‹æŸ¥è¯¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œ<strong>ORACLE</strong>æ¡†æ¶åœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li><strong>ORACLE</strong>æ¡†æ¶ç”Ÿæˆçš„ç†ç”±é“¾æ¯”ç°æœ‰æ–¹æ³•æ›´é€»è¾‘å’Œå¯è§£é‡Šã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºè®­ç»ƒå…è´¹æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå¯é™ä½æ¨¡å‹è®­ç»ƒæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3308594a669ba97603e17b81b26c2ad6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0da194cf710913d26bad64228b88317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58d5789b1ce1572d3d155b9156c9e18c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f62f3814ad7a4c3c904a12c9c822835.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f92c0cdca4e6318b8e615a9307be898e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fb5133706369bb6cc52560c0f06e3e8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization"><a href="#RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization" class="headerlink" title="RL-PLUS: Countering Capability Boundary Collapse of LLMs in   Reinforcement Learning with Hybrid-policy Optimization"></a>RL-PLUS: Countering Capability Boundary Collapse of LLMs in   Reinforcement Learning with Hybrid-policy Optimization</h2><p><strong>Authors:Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, Ge Li</strong></p>
<p>Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLMâ€™s immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLMâ€™s problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºRLVRæœ¬è´¨ä¸Šæ˜¯ä¸€ç§åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œç»“åˆLLMåºå¤§çš„åŠ¨ä½œç©ºé—´å’Œç¨€ç–å¥–åŠ±ï¼Œä½¿å¾—å®ƒåœ¨çªç ´LLMçš„å†…åœ¨èƒ½åŠ›è¾¹ç•Œæ–¹é¢é‡åˆ°å›°éš¾ã€‚å…³é”®çš„æ˜¯ï¼ŒRLVRå¯èƒ½å¯¼è‡´èƒ½åŠ›è¾¹ç•Œå´©æºƒï¼Œç¼©å°LLMçš„é—®é¢˜è§£å†³èŒƒå›´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RL-PLUSï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹LLMçš„æ–°å‹æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒé€šè¿‡å†…éƒ¨åˆ©ç”¨å’Œå¤–éƒ¨æ•°æ®çš„ååŒä½œç”¨ï¼Œå®ç°æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è¶…è¶ŠåŸºç¡€æ¨¡å‹çš„è¾¹ç•Œã€‚RL-PLUSé›†æˆäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå³å¤šé‡é‡è¦æ€§é‡‡æ ·ï¼Œä»¥è§£å†³å¤–éƒ¨æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ï¼Œä»¥åŠåŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹èµ°å‘é«˜ä»·å€¼ã€æœªæ¢ç´¢çš„æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºåˆ†æå’Œå¤§é‡å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚ä¸ç°æœ‰çš„RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒRL-PLUSåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼›åœ¨å…­ä¸ªåˆ†å¸ƒå¤–æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼›åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­å®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ”¶ç›Šï¼Œå¹³å‡ç›¸å¯¹æ”¹è¿›é«˜è¾¾69.2%ã€‚æ­¤å¤–ï¼ŒPass@kæ›²çº¿çš„åˆ†æè¡¨æ˜ï¼ŒRL-PLUSæœ‰æ•ˆåœ°è§£å†³äº†èƒ½åŠ›è¾¹ç•Œå´©æºƒé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00222v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒéš¾ä»¥çªç ´åŸºç¡€LLMçš„å†…åœ¨èƒ½åŠ›è¾¹ç•Œï¼Œä¸»è¦æ˜¯å› ä¸ºå…¶æœ¬è´¨ä¸Šçš„åœ¨ç­–ç•¥ä¸LLMçš„åºå¤§åŠ¨ä½œç©ºé—´å’Œç¨€ç–å¥–åŠ±ä¹‹é—´çš„çŸ›ç›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RL-PLUSï¼Œè¿™æ˜¯ä¸€ç§ä¸ºLLMè®¾è®¡çš„æ–°å‹æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒé€šè¿‡å†…éƒ¨å‰¥å‰Šä¸å¤–éƒ¨æ•°æ®çš„ååŒï¼Œå®ç°äº†æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å¹¶è¶…è¶Šäº†åŸºç¡€æ¨¡å‹çš„è¾¹ç•Œã€‚RL-PLUSåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šé‡é‡è¦æ€§é‡‡æ ·ï¼Œè§£å†³å¤–éƒ¨æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ï¼›åŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹èµ°å‘é«˜ä»·å€¼ã€æœªæ¢ç´¢çš„æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºåˆ†æå’Œå¤§é‡å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚ä¸ç°æœ‰çš„RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒRL-PLUSåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å…­ä¸ªéå¸¸è§„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§æ¨¡å‹å®¶æ—ä¸­å®ç°äº†å¹³å‡ç›¸å¯¹æ”¹è¿›ç‡é«˜è¾¾69.2%ã€‚æ­¤å¤–ï¼ŒPass@kæ›²çº¿çš„åˆ†æè¡¨æ˜ï¼ŒRL-PLUSæœ‰æ•ˆåœ°è§£å†³äº†èƒ½åŠ›è¾¹ç•Œå´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRè™½ç„¶æå‡äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨èƒ½åŠ›è¾¹ç•Œé—®é¢˜ã€‚</li>
<li>RL-PLUSæ˜¯ä¸€ç§æ–°å‹æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³RLVRçš„é—®é¢˜ï¼Œæå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RL-PLUSåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šé‡é‡è¦æ€§é‡‡æ ·å’ŒåŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ã€‚</li>
<li>RL-PLUSåœ¨å¤šä¸ªæ•°å­¦æ¨ç†å’Œéå¸¸è§„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>RL-PLUSå…·æœ‰ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ï¼Œé€‚ç”¨äºå¤šç§æ¨¡å‹å®¶æ—ã€‚</li>
<li>RL-PLUSå®ç°äº†å¹³å‡ç›¸å¯¹æ”¹è¿›ç‡é«˜è¾¾69.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b705c46901573087be60afde27aa6a70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1580606ef9879d9d85299f3f2a6a3f76.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Med-R-3-Enhancing-Medical-Retrieval-Augmented-Reasoning-of-LLMs-via-Progressive-Reinforcement-Learning"><a href="#Med-R-3-Enhancing-Medical-Retrieval-Augmented-Reasoning-of-LLMs-via-Progressive-Reinforcement-Learning" class="headerlink" title="Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning"></a>Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning</h2><p><strong>Authors:Keer Lu, Zheng Liang, Youquan Li, Jiejun Tan, Da Pan, Shusen Zhang, Guosheng Dong, Huang Leng, Bin Cui, Wentao Zhang</strong></p>
<p>In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce <strong>Med-R$^3$</strong>, a <strong>Med</strong>ical <strong>R</strong>etrieval-augmented <strong>R</strong>easoning framework driven by progressive <strong>R</strong>einforcement learning. In this framework, we first develop the modelâ€™s ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the modelâ€™s retrieval and reasoning coordination. Extensive experiments indicate that <strong>Med-R$^3$</strong> could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53%. </p>
<blockquote>
<p>åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåœ°æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¹¶åˆ©ç”¨å…¶è¿›è¡Œä¸¥è°¨çš„é€»è¾‘æ¨ç†å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰ã€‚å°½ç®¡å­˜åœ¨æ½œåœ¨çš„å¯èƒ½æ€§ï¼Œä½†ç›®å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å•ç‹¬å¢å¼ºæ¨¡å‹çš„æ£€ç´¢æˆ–æ¨ç†èƒ½åŠ›ä¸Šï¼Œå¾ˆå°‘å…³æ³¨å®ƒä»¬çš„è”åˆä¼˜åŒ–ï¼Œå¯¼è‡´è¿™ä¸¤ä¸ªè¿‡ç¨‹ä¹‹é—´çš„åè°ƒæœ‰é™ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹è®°å¿†ç°æœ‰çš„é—®é¢˜è§£å†³è·¯å¾„ï¼Œä»è€Œåœ¨é¢å¯¹æ–°çš„é—®é¢˜æƒ…å¢ƒæ—¶é™åˆ¶å…¶æ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶å·²ç»æ¢ç´¢äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ æ”¹è¿›ä¸€èˆ¬é¢†åŸŸçš„æ£€ç´¢å¢å¼ºæ¨ç†ï¼Œä½†ä»–ä»¬çš„å¥–åŠ±å‡½æ•°è®¾è®¡å¹¶æ²¡æœ‰å……åˆ†æ•æ‰åˆ°åŒ»ç–—é¢†åŸŸçš„ç‰¹å®šéœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>Med-R$^3$<strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„</strong>åŒ»ç–—æ£€ç´¢å¢å¼ºæ¨ç†</strong>æ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œä»¥æ›´å¥½åœ°é€‚åº”çŸ¥è¯†åº“å’Œå¤–éƒ¨ä¿¡æ¯åˆ©ç”¨çš„ç‰¹æ€§ï¼Œè´¯ç©¿æ•´ä¸ªæ¨ç†è¿‡ç¨‹ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ£€ç´¢å’Œæ¨ç†åè°ƒè¿›è¡Œè”åˆä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ**Med-R$^3$**å¯ä»¥è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒLLaMA3.1-8B-Instructç»“åˆMed-R$^3$åœ¨å¯æ¯”è¾ƒçš„å‚æ•°è§„æ¨¡ä¸‹è¶…è¶Šé—­æºçš„GPT-4o-mini 3.93%ï¼Œè€ŒQwen2.5-14Bç»“åˆMed-R$^3$åˆ™æ˜¾ç¤ºå‡ºæ›´å¤§çš„æå‡å¹…åº¦ï¼Œè¾¾åˆ°13.53%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23541v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåŒ»ç–—é¢†åŸŸä¸­çš„æ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶Med-RÂ³ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨è”åˆä¼˜åŒ–æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œé¦–å…ˆåŸ¹å…»æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„èƒ½åŠ›ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šè‡ªé€‚åº”ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œä»¥æ›´å¥½åœ°é€‚åº”çŸ¥è¯†åº“å’Œå¤–éƒ¨ä¿¡æ¯çš„ä½¿ç”¨ç‰¹ç‚¹ã€‚æœ€ç»ˆå®ç°äº†æ¨¡å‹æ£€ç´¢å’Œæ¨ç†çš„è”åˆä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒMed-RÂ³å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸é—­æºGPT-4o-miniç›¸æ¯”ï¼ŒLLaMA3.1-8B-Instruct + Med-RÂ³çš„æ€§èƒ½æé«˜äº†3.93%ï¼Œè€ŒQwen2.5-14Bä¸Med-RÂ³çš„ç»“åˆåˆ™æ˜¾ç¤ºå‡ºæ›´å¤§çš„å¢ç›Šï¼Œæé«˜äº†13.53%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-RÂ³æ¡†æ¶è§£å†³äº†åŒ»ç–—é¢†åŸŸä¸­æ£€ç´¢å’Œæ¨ç†çš„è”åˆä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡é€æ­¥å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Med-RÂ³é¦–å…ˆåŸ¹å…»æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„èƒ½åŠ›ï¼Œç„¶åä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œä»¥é€‚åº”çŸ¥è¯†åº“å’Œå¤–éƒ¨ä¿¡æ¯çš„ä½¿ç”¨ç‰¹ç‚¹ã€‚</li>
<li>Med-RÂ³å®ç°äº†æ¨¡å‹æ£€ç´¢å’Œæ¨ç†çš„è”åˆä¼˜åŒ–ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¸é—­æºGPT-4o-miniç›¸æ¯”ï¼ŒLLaMA3.1-8B-Instruct + Med-RÂ³çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚</li>
<li>Qwen2.5-14Bä¸Med-RÂ³çš„ç»“åˆæ˜¾ç¤ºå‡ºæ›´å¤§çš„æ€§èƒ½æå‡ã€‚</li>
<li>Med-RÂ³æ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸçš„çŸ¥è¯†ç®¡ç†å’Œé—®é¢˜è§£ç­”æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-169a15c561a7cd5ee612d8d41fdbe4e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c765c9c2918ef3eefd8a50e3ac67f34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80513fc864b19ef77c55b70982db027a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af52adda2f08eff4c33b4d359415cee1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Sufficiency-to-Reflection-Reinforcement-Guided-Thinking-Quality-in-Retrieval-Augmented-Reasoning-for-LLMs"><a href="#From-Sufficiency-to-Reflection-Reinforcement-Guided-Thinking-Quality-in-Retrieval-Augmented-Reasoning-for-LLMs" class="headerlink" title="From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in   Retrieval-Augmented Reasoning for LLMs"></a>From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in   Retrieval-Augmented Reasoning for LLMs</h2><p><strong>Authors:Jie He, Victor GutiÃ©rrez-Basulto, Jeff Z. Pan</strong></p>
<p>Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/probe2/TIRESRAG-R1">https://github.com/probe2/TIRESRAG-R1</a>. </p>
<blockquote>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä»…ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆçš„å¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è´¨é‡ã€‚æœ¬æ–‡åˆ†æäº†ç°æœ‰çš„RAGæ¨ç†æ¨¡å‹ï¼Œå¹¶è¯†åˆ«å‡ºä¸‰ç§ä¸»è¦çš„å¤±è´¥æ¨¡å¼ï¼šï¼ˆ1ï¼‰ä¿¡æ¯ä¸è¶³ï¼Œå³æ¨¡å‹æœªèƒ½æ£€ç´¢åˆ°è¶³å¤Ÿçš„æ”¯æŒï¼›ï¼ˆ2ï¼‰æ¨ç†é”™è¯¯ï¼Œå°½ç®¡ä¿¡æ¯å……è¶³ï¼Œä½†é€»è¾‘æˆ–å†…å®¹å±‚é¢ä»ç„¶å‡ºç°ç¼ºé™·ï¼›ï¼ˆ3ï¼‰ç­”æ¡ˆä¸æ¨ç†ä¸ä¸€è‡´ï¼Œå³æœ‰æ•ˆçš„æ¨ç†é“¾å¯¼è‡´ä¸åŒ¹é…çš„æœ€ç»ˆç­”æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†TIRESRAG-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ€è€ƒ-æ£€ç´¢-åæ€è¿‡ç¨‹å’Œå¤šç»´å¥–åŠ±ç³»ç»Ÿçš„æ–°å‹æ¡†æ¶ï¼Œä»¥æé«˜æ¨ç†å’Œç¨³å®šæ€§ã€‚TIRESRAG-R1å¼•å…¥ï¼šï¼ˆ1ï¼‰å……è¶³æ€§å¥–åŠ±ï¼Œä»¥é¼“åŠ±å½»åº•æ£€ç´¢ï¼›ï¼ˆ2ï¼‰æ¨ç†è´¨é‡å¥–åŠ±ï¼Œä»¥è¯„ä¼°æ¨ç†é“¾çš„åˆç†æ€§å’Œå‡†ç¡®æ€§ï¼›ï¼ˆ3ï¼‰åæ€å¥–åŠ±ï¼Œä»¥æ£€æµ‹å’Œä¿®æ­£é”™è¯¯ã€‚å®ƒè¿˜é‡‡ç”¨éš¾åº¦æ„ŸçŸ¥é‡æƒç­–ç•¥å’ŒåŸ¹è®­æ ·æœ¬è¿‡æ»¤ï¼Œä»¥æé«˜åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨å››ä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTIRESRAG-R1ä¼˜äºå…ˆå‰çš„RAGæ–¹æ³•ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å•è·³ä»»åŠ¡ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/probe2/TIRESRAG-R1">https://github.com/probe2/TIRESRAG-R1</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22716v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä»…ä¾èµ–æœ€ç»ˆç­”æ¡ˆå¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è´¨é‡ã€‚æœ¬æ–‡åˆ†æäº†ç°æœ‰çš„RAGæ¨ç†æ¨¡å‹ï¼Œå¹¶æå‡ºäº†TIRESRAG-R1æ¡†æ¶ï¼Œé‡‡ç”¨æ€è€ƒ-æ£€ç´¢-åæ€è¿‡ç¨‹å’Œå¤šç»´å¥–åŠ±ç³»ç»Ÿæ¥æ”¹å–„æ¨ç†å’Œç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥å……åˆ†æ€§å¥–åŠ±ã€æ¨ç†è´¨é‡å¥–åŠ±å’Œåæ€å¥–åŠ±ï¼Œå¹¶é‡‡ç”¨éš¾åº¦æ„ŸçŸ¥é‡åŠ æƒç­–ç•¥å’Œè®­ç»ƒæ ·æœ¬è¿‡æ»¤æ¥æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒTIRESRAG-R1åœ¨å››ä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„RAGæ–¹æ³•ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å•è·³ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RAGæ–¹æ³•å¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¤šæ•°ä»…ä¾èµ–æœ€ç»ˆç­”æ¡ˆå¥–åŠ±ï¼Œå¿½è§†ä¸­é—´æ¨ç†è´¨é‡ã€‚</li>
<li>ç°æœ‰RAGæ¨ç†æ¨¡å‹å­˜åœ¨ä¸‰å¤§å¤±è´¥æ¨¡å¼ï¼šä¿¡æ¯ä¸è¶³ã€é€»è¾‘é”™è¯¯æˆ–å†…å®¹å±‚é¢ç¼ºé™·ã€ç­”æ¡ˆä¸æ¨ç†ä¸ä¸€è‡´ã€‚</li>
<li>TIRESRAG-R1æ¡†æ¶é€šè¿‡æ€è€ƒ-æ£€ç´¢-åæ€è¿‡ç¨‹å’Œå¤šç»´å¥–åŠ±ç³»ç»Ÿæ”¹å–„æ¨ç†å’Œç¨³å®šæ€§ã€‚</li>
<li>TIRESRAG-R1å¼•å…¥å……åˆ†æ€§å¥–åŠ±ã€æ¨ç†è´¨é‡å¥–åŠ±å’Œåæ€å¥–åŠ±ã€‚</li>
<li>TIRESRAG-R1é‡‡ç”¨éš¾åº¦æ„ŸçŸ¥é‡åŠ æƒç­–ç•¥å’Œè®­ç»ƒæ ·æœ¬è¿‡æ»¤æ¥æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTIRESRAG-R1åœ¨å¤šä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½æ¨å¹¿è‡³å•è·³ä»»åŠ¡ã€‚</li>
<li>TIRESRAG-R1çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/probe2/TIRESRAG-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/probe2/TIRESRAG-R1æ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-303868e352d481a09add708e4ebdda0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-503c68755568ecbeb0936865a73c475b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24122a811f0760edf3df7a2c06810fcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9f425e044a6ab34db12f1295f538ac4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards"><a href="#Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards" class="headerlink" title="Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards"></a>Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards</h2><p><strong>Authors:Aybora Koksal, A. Aydin Alatan</strong></p>
<p>Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervisionâ€“relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the â€œ1-shot RLVRâ€ paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarksâ€“including classification, visual question answering, and groundingâ€“show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æˆä¸ºå¯èƒ½ï¼Œç„¶è€Œï¼Œåœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸï¼Œç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æ˜‚è´µï¼Œè¿™äº›æ¨¡å‹ä»ä¸å®ç”¨ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºäº†å«æ˜Ÿå›¾åƒé¢†åŸŸçš„å°‘æ ·æœ¬å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€å­—å¹•ç›‘ç£â€”â€”ä»…ä¾èµ–è½»é‡çº§ã€åŸºäºè§„åˆ™çš„äºŒå…ƒå¥–åŠ±æˆ–åŸºäºIoUçš„å¥–åŠ±ã€‚æˆ‘ä»¬å°†è¯­è¨€æ¨¡å‹çš„â€œ1æ¬¡å°„å‡»RLVRâ€èŒƒå¼é€‚åº”åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä»…ä¸€ä¸ªç²¾é€‰æ ·æœ¬è¿›è¡Œç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼Œä»¥å®ç°å¯¹å«æ˜Ÿæ¨ç†ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºå¯¹é½ã€‚åœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒâ€”â€”åŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œæ¥åœ°â€”â€”è¡¨æ˜ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªä¾‹å­ä¹Ÿèƒ½ç»™åŸºç¡€æ¨¡å‹å¸¦æ¥å®è´¨æ€§çš„æ”¹è¿›ã€‚æ‰©å±•åˆ°128ä¸ªä¾‹å­å¯ä¸åœ¨æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚è™½ç„¶æç«¯çš„ä¸€æ¬¡æ€§è®¾ç½®å¯èƒ½ä¼šå¯¼è‡´ç‰¹å®šçš„ä»»åŠ¡è¿‡æ‹Ÿåˆï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦æœ‰é‡å¤§å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æˆæœ¬æ•ˆç›Šå’Œæ•°æ®æ•ˆç‡çš„é¢†åŸŸä¸“ä¸šè§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹å¼€å‘ï¼Œä¸ºæ•°æ®ç¨€ç¼ºé¢†åŸŸæä¾›äº†å®ç”¨æ–¹æ¡ˆï¼šä»ç´§å‡‘çš„VLMå¼€å§‹ï¼Œç²¾é€‰å°‘æ•°å¯éªŒè¯å¥–åŠ±çš„æ¡ˆä¾‹ï¼Œå¹¶é€šè¿‡RLVRè¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21745v2">PDF</a> ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10   pages, 3 figures, 6 tables. Our model, training code and dataset will be at   <a target="_blank" rel="noopener" href="https://github.com/aybora/FewShotReasoning">https://github.com/aybora/FewShotReasoning</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥èµ‹äºˆäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸä»ä¸é€‚ç”¨ï¼Œå› ä¸ºæ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æ˜‚è´µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªæ— éœ€å­—å¹•ç›‘ç£çš„ã€åŸºäºå°‘æ•°æ¡ˆä¾‹å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„å«æ˜Ÿå›¾åƒæ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸€ç§â€œ1æ¬¡ç‚¹å‡»RLVRâ€æ¨¡å¼ï¼Œå€ŸåŠ©å°‘é‡ç²¾é€‰æ¡ˆä¾‹è¿›è¡Œç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼Œä½¿æ¨¡å‹è¾“å‡ºä¸å«æ˜Ÿæ¨ç†ä»»åŠ¡å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å•ä¸€æ¡ˆä¾‹ä¸‹ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºç¡€æ¨¡å‹ä¹Ÿæœ‰æ˜¾è‘—æå‡ï¼Œå½“æ‰©å±•åˆ°128ä¸ªæ¡ˆä¾‹æ—¶ï¼Œå…¶æ€§èƒ½ç”šè‡³å¯ä¸åœ¨æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾ã€‚å°½ç®¡ä¸€æ¬¡æ€§çš„è®¾ç½®å¯èƒ½ä¼šå¼•å‘ç‰¹å®šä»»åŠ¡çš„è¿‡åº¦æ‹Ÿåˆï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸­å‡å±•ç°å‡ºç¨³å¥çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦æœ‰é‡å¤§å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿé¢†åŸŸçš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ ‡æ³¨ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºå°‘æ•°æ¡ˆä¾‹å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„å«æ˜Ÿå›¾åƒæ¡†æ¶ï¼Œæ— éœ€å­—å¹•ç›‘ç£ã€‚</li>
<li>é€šè¿‡ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼Œä»…ä½¿ç”¨å°‘é‡ç²¾é€‰æ¡ˆä¾‹å³å¯å®ç°æ¨¡å‹è¾“å‡ºä¸å«æ˜Ÿæ¨ç†ä»»åŠ¡çš„åŒ¹é…ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œå®šä½ä»»åŠ¡ã€‚</li>
<li>åœ¨å•ä¸€æ¡ˆä¾‹ä¸‹ç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œæ‰©å±•åˆ°æ›´å¤šæ¡ˆä¾‹æ—¶æ€§èƒ½å¯ä¸å¤§é‡æ ‡æ³¨æ ·æœ¬è®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾ã€‚</li>
<li>å°½ç®¡å­˜åœ¨ä»»åŠ¡ç‰¹å®šè¿‡åº¦æ‹Ÿåˆçš„é£é™©ï¼Œä½†æ–¹æ³•å±•ç°å‡ºç¨³å¥çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-732b3786f4aa504a527762387ae04233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-523cadb83435b7fe50117ac9758c77eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da7cf5bd8e6c3b9062f2351e76d97a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2521a48ab6d958e0c2842dd6e3b642c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learning-Only-with-Images-Visual-Reinforcement-Learning-with-Reasoning-Rendering-and-Visual-Feedback"><a href="#Learning-Only-with-Images-Visual-Reinforcement-Learning-with-Reasoning-Rendering-and-Visual-Feedback" class="headerlink" title="Learning Only with Images: Visual Reinforcement Learning with Reasoning,   Rendering, and Visual Feedback"></a>Learning Only with Images: Visual Reinforcement Learning with Reasoning,   Rendering, and Visual Feedback</h2><p><strong>Authors:Yang Chen, Yufan Shen, Wenxuan Huang, Sheng Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Jiajun Bu, Botian Shi, Yu Qiao</strong></p>
<p>Multimodal Large Language Models (MLLMs) exhibit impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework, <code>Reasoning-Rendering-Visual-Feedback&#39;&#39; (RRVF), that enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the </code>Asymmetry of Verificationâ€™â€™ principle, i.e., verifying the rendered output against the source image is substantially easier than performing deep visual reasoning to generate a faithful, structured representation such as code. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL), thereby reducing reliance on image-text supervision. RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform complex reasoning, including self-correction through multi-turn interactions. This process is optimized end-to-end using the GRPO algorithm. Extensive evaluations are conducted on image-to-code generation across two diverse domains: data charts and web interfaces. The RRVF-trained model not only outperforms existing similarly sized open-source MLLMs and supervised fine-tuning baselines but also exhibits superior generalization. Notably, the model outperforms the more advanced MLLM used to generate visual feedback during training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/L-O-I/RRVF">https://github.com/L-O-I/RRVF</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚éšåå…³äºæé«˜å…¶è§†è§‰æ¨ç†èƒ½åŠ›çš„è°ƒæŸ¥æ˜¾è‘—æ‰©å¤§äº†å…¶æ€§èƒ½èŒƒå›´ã€‚ç„¶è€Œï¼Œåœ¨æ¨åŠ¨MLLMså®ç°æ·±åº¦è§†è§‰æ¨ç†æ–¹é¢ï¼Œå®ƒä»¬å¯¹ç²¾é€‰çš„å›¾åƒæ–‡æœ¬ç›‘ç£çš„ä¸¥é‡ä¾èµ–æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œReasoning-Rendering-Visual-Feedbackâ€ï¼ˆRRVFï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿MLLMsèƒ½å¤Ÿä»åŸå§‹å›¾åƒä¸­å­¦ä¹ å¤æ‚çš„è§†è§‰æ¨ç†ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨â€œéªŒè¯ä¸å¯¹ç§°æ€§â€åŸç†ä¹‹ä¸Šï¼Œå³éªŒè¯æ¸²æŸ“è¾“å‡ºä¸æºå›¾åƒç›¸æ¯”ï¼Œåœ¨ç”Ÿæˆå¿ å®ã€ç»“æ„åŒ–è¡¨ç¤ºï¼ˆå¦‚ä»£ç ï¼‰æ—¶è¿›è¡Œæ·±åº¦è§†è§‰æ¨ç†è¦å®¹æ˜“å¾—å¤šã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§ç›¸å¯¹è½»æ¾ä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¼˜åŒ–æä¾›äº†ç†æƒ³çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå‡å°‘äº†å›¾åƒæ–‡æœ¬ç›‘ç£çš„ä¾èµ–ã€‚RRVFå®ç°äº†æ¶µç›–æ¨ç†ã€æ¸²æŸ“å’Œè§†è§‰åé¦ˆç»„ä»¶çš„é—­ç¯è¿­ä»£è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ‰§è¡ŒåŒ…æ‹¬é€šè¿‡å¤šè½®äº¤äº’è¿›è¡Œè‡ªæˆ‘æ ¡æ­£çš„å¤æ‚æ¨ç†ã€‚è¯¥è¿‡ç¨‹ä½¿ç”¨GRPOç®—æ³•è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚åœ¨æ•°æ®å›¾è¡¨å’ŒWebç•Œé¢ä¸¤ä¸ªä¸åŒé¢†åŸŸçš„å›¾åƒåˆ°ä»£ç ç”Ÿæˆæ–¹é¢è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚RRVFè®­ç»ƒçš„æ¨¡å‹ä¸ä»…ä¼˜äºç°æœ‰ç±»ä¼¼è§„æ¨¡çš„å¼€æºMLLMså’Œç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œè€Œä¸”è¡¨ç°å‡ºå“è¶Šçš„æ€»ä½“åŒ–æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹è¿˜è¶…è¶Šäº†è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºç”Ÿæˆè§†è§‰åé¦ˆçš„æ›´å…ˆè¿›çš„MLLMã€‚ä»£ç å¯ç”¨<a target="_blank" rel="noopener" href="https://github.com/L-O-I/RRVF%E3%80%82">https://github.com/L-O-I/RRVFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20766v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œReasoning-Rendering-Visual-Feedbackâ€ï¼ˆRRVFï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»…ä»åŸå§‹å›¾åƒä¸­å­¦ä¹ å¤æ‚çš„è§†è§‰æ¨ç†æˆä¸ºå¯èƒ½ã€‚è¯¥æ¡†æ¶åŸºäºâ€œéªŒè¯ä¸å¯¹ç§°æ€§â€åŸç†ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–ï¼Œå®ç°ä»å›¾åƒåˆ°ä»£ç ç”Ÿæˆçš„å¤æ‚æ¨ç†ä»»åŠ¡ã€‚RRVFæ¡†æ¶å®æ–½äº†ä¸€ä¸ªæ¶µç›–æ¨ç†ã€æ¸²æŸ“å’Œè§†è§‰åé¦ˆç»„ä»¶çš„é—­ç¯è¿­ä»£è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¤šè½®äº¤äº’è¿›è¡Œè‡ªæˆ‘æ ¡æ­£ã€‚åœ¨æ•°æ®å›¾è¡¨å’Œç½‘é¡µç•Œé¢ä¸¤ä¸ªä¸åŒé¢†åŸŸè¿›è¡Œçš„å›¾åƒåˆ°ä»£ç ç”Ÿæˆè¯„ä¼°è¡¨æ˜ï¼ŒRRVFè®­ç»ƒçš„æ¨¡å‹ä¸ä»…ä¼˜äºåŒæ ·è§„æ¨¡çš„å…¶ä»–å¼€æºMLLMså’Œç›‘ç£ç²¾ç»†è°ƒæ•´åŸºå‡†æ¨¡å‹ï¼Œè€Œä¸”è¡¨ç°å‡ºæ›´ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨æ·±åº¦è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨ä¾èµ–å›¾åƒæ–‡æœ¬ç›‘ç£çš„ç“¶é¢ˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶â€œReasoning-Rendering-Visual-Feedbackâ€ï¼ˆRRVFï¼‰ï¼Œä½¿MLLMsèƒ½å¤Ÿä»åŸå§‹å›¾åƒä¸­å­¦ä¹ å¤æ‚è§†è§‰æ¨ç†ã€‚</li>
<li>RRVFæ¡†æ¶åŸºäºâ€œéªŒè¯ä¸å¯¹ç§°æ€§â€åŸç†ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›ä¼˜åŒ–å¥–åŠ±ä¿¡å·ã€‚</li>
<li>RRVFå®æ–½äº†ä¸€ä¸ªæ¶µç›–æ¨ç†ã€æ¸²æŸ“å’Œè§†è§‰åé¦ˆçš„é—­ç¯è¿­ä»£è¿‡ç¨‹ï¼Œå®ç°è‡ªæˆ‘æ ¡æ­£å’Œå¤æ‚æ¨ç†ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å›¾åƒåˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶‰åŠæ•°æ®å›¾è¡¨å’Œç½‘é¡µç•Œé¢ä¸¤ä¸ªé¢†åŸŸã€‚</li>
<li>RRVFè®­ç»ƒçš„æ¨¡å‹ä¼˜äºå…¶ä»–ç±»ä¼¼è§„æ¨¡çš„MLLMså’Œç›‘ç£ç²¾ç»†è°ƒæ•´åŸºå‡†æ¨¡å‹ï¼Œå¹¶è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aaf0da0b0184bab4f67a4148e958d2dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ece80e7a7db6ef1ea5c9717366a63ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-982f9f98dce2dbb87ae69ac573f3c124.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c9aaa6c0fd9df21e5255630128f6a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b91f8c0a7ee8484a8aa22492d8881b6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b15956f4afc66801371ce2b5ee6ff822.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CLARIFID-Improving-Radiology-Report-Generation-by-Reinforcing-Clinically-Accurate-Impressions-and-Enforcing-Detailed-Findings"><a href="#CLARIFID-Improving-Radiology-Report-Generation-by-Reinforcing-Clinically-Accurate-Impressions-and-Enforcing-Detailed-Findings" class="headerlink" title="CLARIFID: Improving Radiology Report Generation by Reinforcing   Clinically Accurate Impressions and Enforcing Detailed Findings"></a>CLARIFID: Improving Radiology Report Generation by Reinforcing   Clinically Accurate Impressions and Enforcing Detailed Findings</h2><p><strong>Authors:Kyeongkyu Lee, Seonghwan Yoon, Hongki Lim</strong></p>
<p>Automatic generation of radiology reports has the potential to alleviate radiologistsâ€™ significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) enforces reasoning-aware decoding that completes â€œFindingsâ€ before synthesizing the â€œImpressionâ€, and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a reasoning-aware next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive Findings section before synthesizing the Impression and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on both standard NLG metrics and clinically aware scores. </p>
<blockquote>
<p>è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šå…·æœ‰ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿå·¨å¤§å·¥ä½œé‡çš„æ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨æä¾›ä¸´åºŠå¯é ç»“è®ºæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¾§é‡äºç”Ÿæˆæµç•…çš„æ–‡å­—ï¼Œè€Œæ²¡æœ‰æœ‰æ•ˆåœ°ç¡®ä¿æŠ¥å‘Šçš„äº‹å®æ­£ç¡®æ€§ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºå•è§†å›¾å›¾åƒï¼Œè¿™é™åˆ¶äº†è¯Šæ–­çš„å…¨é¢æ€§ã€‚æˆ‘ä»¬æå‡ºäº†CLARIFIDè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡é•œåƒä¸“å®¶çš„ä¸¤æ­¥å·¥ä½œæµç¨‹æ¥ç›´æ¥ä¼˜åŒ–è¯Šæ–­çš„æ­£ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒCLARIFIDï¼ˆ1ï¼‰é€šè¿‡åˆ†æ®µé¢„è®­ç»ƒå­¦ä¹ ä»æ£€æŸ¥ç»“æœåˆ°å°è±¡çš„é€»è¾‘æµç¨‹ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­å°è±¡éƒ¨åˆ†çš„CheXbert F1åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œï¼ˆ3ï¼‰å¼ºåˆ¶æ‰§è¡Œæ¨ç†æ„ŸçŸ¥è§£ç ï¼Œåœ¨å®Œæˆâ€œæ£€æŸ¥ç»“æœâ€ååˆæˆâ€œå°è±¡â€ï¼Œï¼ˆ4ï¼‰é€šè¿‡åŸºäºè§†è§‰å˜æ¢å™¨çš„å¤šè§†å›¾ç¼–ç å™¨èåˆå¤šä¸ªèƒ¸éƒ¨Xå…‰ç‰‡è§†å›¾ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸€ç§æ¨ç†æ„ŸçŸ¥çš„ä¸‹ä¸€ä¸ªæ ‡è®°å¼ºåˆ¶ç­–ç•¥ï¼Œç„¶åè¿›è¡ŒæŠ¥å‘Šçº§åˆ«çš„é‡æ–°æ’åºï¼Œç¡®ä¿æ¨¡å‹é¦–å…ˆç”Ÿæˆå…¨é¢çš„æ£€æŸ¥ç»“æœéƒ¨åˆ†ï¼Œç„¶åå†åˆæˆå°è±¡ï¼Œä»è€Œä¿æŒè¿è´¯çš„ä¸´åºŠæ¨ç†ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†å“è¶Šçš„ä¸´åºŠæ•ˆæœï¼Œåœ¨æ ‡å‡†è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å’Œä¸´åºŠæ„è¯†è¯„åˆ†ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17234v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCLARIFIDçš„æ–°æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆè¯Šæ–­æ­£ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡ä»¿ä¸“å®¶ä¸¤æ­¥å·¥ä½œæµç¨‹ã€å­¦ä¹ é€»è¾‘æµç¨‹ã€ç²¾ç»†è°ƒæ•´è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ã€å®æ–½æ¨ç†æ„ŸçŸ¥è§£ç å’Œèåˆå¤šè§†è§’çš„èƒ¸éƒ¨Xå…‰ç‰‡ï¼Œæé«˜äº†æŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œè¯Šæ–­çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLARIFIDåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„ä¸´åºŠæ•ˆæœæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLARIFIDæ¡†æ¶æ—¨åœ¨è§£å†³æ”¾å°„å­¦æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆä¸­è¯Šæ–­æ­£ç¡®æ€§çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ¨¡ä»¿ä¸“å®¶å·¥ä½œæµç¨‹æ¥å­¦ä¹ é€»è¾‘æµç¨‹ï¼Œä»è€Œæé«˜æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>CLARIFIDé‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç²¾ç»†è°ƒæ•´æ¨¡å‹ï¼Œä½¿ç”¨CheXbert F1åˆ†æ•°ä½œä¸ºå¥–åŠ±ã€‚</li>
<li>æ¨ç†æ„ŸçŸ¥è§£ç ç¡®ä¿æŠ¥å‘Šä¸­çš„â€œå‘ç°â€éƒ¨åˆ†å…ˆäºâ€œå°è±¡â€éƒ¨åˆ†å®Œæˆï¼Œä¿æŒè¿è´¯çš„ä¸´åºŠæ¨ç†ã€‚</li>
<li>CLARIFIDé€šè¿‡èåˆå¤šè§†è§’çš„èƒ¸éƒ¨Xå…‰ç‰‡æ¥æé«˜æŠ¥å‘Šçš„å…¨é¢æ€§ã€‚</li>
<li>åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCLARIFIDåœ¨ä¸´åºŠæ•ˆæœå’Œæ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a8caa07dcc30956b8fb294930cf3754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c57b969e4e3149082361d1f08ad8a3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae8905313d4e51644f0a2c46d342181.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-684ca478433efdebec89d42b64dba222.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning"><a href="#Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning" class="headerlink" title="Perception-Aware Policy Optimization for Multimodal Reasoning"></a>Perception-Aware Policy Optimization for Multimodal Reasoning</h2><p><strong>Authors:Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, a novel policy gradient algorithm that encourages the model to learn to perceive while learning to reason. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term, which can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely on additional data curation, reward models, or stronger teacher models. To further enhance the training stability of PAPO, we introduce the Double Entropy Loss, which effectively regularizes the new KL objective without compromising performance. Despite its simplicity, PAPO yields significant overall improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%-19.1%, on tasks with high vision dependency. We also observe a substantial reduction of 30.5% in perception errors, indicating improved perceptual capabilities with PAPO. Overall, our work introduces a deeper integration of perception-aware supervision into core learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Code and data will be made publicly available for research purposes. Project page: <a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">https://mikewangwzhl.github.io/PAPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²è¢«è¯æ˜æ˜¯èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›çš„ä¸€ç§é«˜æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡ä¸ä¼˜åŒ–ä»é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯¯å·®ä¸»è¦æ¥æºäºå¯¹è§†è§‰è¾“å…¥çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†PAPOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­å­¦ä¼šç†è§£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»¥KLæ•£åº¦é¡¹çš„å½¢å¼å¼•å…¥éšæ„ŸçŸ¥æŸå¤±ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°æ’å…¥åˆ°ä¸»æµRLVRç®—æ³•ä¸­ï¼Œå¦‚GRPOå’ŒDAPOã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPAPOä¸ä¾èµ–é¢å¤–çš„æ•°æ®æ•´ç†ã€å¥–åŠ±æ¨¡å‹æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜PAPOçš„è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡ç†µæŸå¤±ï¼Œå®ƒæœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†æ–°çš„KLç›®æ ‡ï¼ŒåŒæ—¶ä¸æŸå®³æ€§èƒ½ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼ŒPAPOåœ¨å¤šæ ·åŒ–çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†4.4%~17.5%çš„æ˜¾è‘—æ€»ä½“æ”¹è¿›ã€‚åœ¨é«˜åº¦ä¾èµ–è§†è§‰çš„ä»»åŠ¡ä¸­ï¼Œæ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œè¾¾åˆ°8.0%~19.1%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯ç‡é™ä½äº†30.5%ï¼Œè¿™è¡¨æ˜PAPOæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°é›†æˆåˆ°æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰åŒ–æ¨ç†çš„æ–°RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚ä¸ºäº†ç ”ç©¶ç›®çš„ï¼Œä»£ç å’Œæ•°æ®å°†å…¬å¼€æä¾›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO%E3%80%82">https://mikewangwzhl.github.io/PAPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06448v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RLVRç­–ç•¥å·²è¯æ˜èƒ½å¤Ÿæœ‰æ•ˆèµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸»è¦è¯¯å·®ã€‚ä¸ºè§£å†³æ­¤ç“¶é¢ˆï¼Œæå‡ºæ–°å‹ç­–ç•¥PAPOï¼Œç»“åˆç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ¨ç†ä¸­å­¦ä¹ æ„ŸçŸ¥ï¼Œå¼•å…¥éšæ€§æ„ŸçŸ¥æŸå¤±ï¼ˆImplicit Perception Lossï¼‰ä»¥KLæ•£åº¦å½¢å¼ï¼Œå¯æ— ç¼æ’å…¥ä¸»æµRLVRç®—æ³•ä¸­ã€‚PAPOæ— éœ€é¢å¤–æ•°æ®æ•´ç†ã€å¥–åŠ±æ¨¡å‹æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶å¼•å…¥åŒé‡ç†µæŸå¤±ä»¥å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPAPOå¸¦æ¥4.4%~17.5%çš„æ•´ä½“æ”¹è¿›ï¼Œé«˜è§†è§‰ä¾èµ–æ€§ä»»åŠ¡æ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼ˆè¾¾8.0%~19.1%ï¼‰ï¼Œæ„ŸçŸ¥é”™è¯¯ç‡é™ä½30.5%ã€‚æ•´ä½“è€Œè¨€ï¼ŒPAPOæ¨åŠ¨æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥èå…¥æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ï¼Œä¸ºé¼“åŠ±è§†è§‰æ¨ç†çš„æ–°å‹RLæ¡†æ¶å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRç­–ç•¥åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨è§†è§‰æ„ŸçŸ¥è¯¯å·®é—®é¢˜ã€‚</li>
<li>PAPOç­–ç•¥æ˜¯ä¸€ç§æ–°å‹çš„æ”¿ç­–æ¢¯åº¦ç®—æ³•ï¼Œç”¨äºè§£å†³RLVRåœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>PAPOå¼•å…¥éšæ€§æ„ŸçŸ¥æŸå¤±ï¼ˆImplicit Perception Lossï¼‰ï¼Œä»¥KLæ•£åº¦å½¢å¼æ— ç¼é›†æˆåˆ°ä¸»æµRLVRç®—æ³•ä¸­ã€‚</li>
<li>PAPOæ”¹è¿›äº†å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œæ€»ä½“æ”¹è¿›èŒƒå›´è¾¾åˆ°4.4%~17.5%ï¼Œé«˜è§†è§‰ä¾èµ–æ€§ä»»åŠ¡çš„æ”¹è¿›æ›´ä¸ºæ˜¾è‘—ã€‚</li>
<li>PAPOé€šè¿‡å¼•å…¥åŒé‡ç†µæŸå¤±å¢å¼ºäº†è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>PAPOé™ä½äº†æ„ŸçŸ¥é”™è¯¯ç‡30.5%ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b63126f4f96f245c92532a7e38611a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee7ac5965f5f8f956f5eec34f9d4025f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62977bb69d3cc3862504b2693aaecfbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-859439980e5c19b7805a632f72e60c9d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers"><a href="#A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers" class="headerlink" title="A Comparative Study of Specialized LLMs as Dense Retrievers"></a>A Comparative Study of Specialized LLMs as Dense Retrievers</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo</strong></p>
<p>While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code&#x2F;math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºå¯†é›†æ£€ç´¢å™¨ï¼Œå…¶ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMçš„ä»»åŠ¡ç‰¹å®šé€‚åº”å¦‚ä½•å½±å“å…¶æ£€ç´¢èƒ½åŠ›ï¼Œè¿™æ˜¯æœç€å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªQwen2.5 7B LLMä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€ã€æŒ‡ä»¤è°ƒä¼˜ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šåŒ–ã€é•¿æ¨ç†å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè·¨è¶Šé›¶å°„å‡»æ£€ç´¢è®¾ç½®å’Œæœ‰ç›‘ç£è®¾ç½®ã€‚å¯¹äºé›¶å°„å‡»æ£€ç´¢è®¾ç½®ï¼Œæˆ‘ä»¬è€ƒè™‘æ¥è‡ªBEIRåŸºå‡†æµ‹è¯•çš„æ–‡æœ¬æ£€ç´¢å’Œæ¥è‡ªCoIRåŸºå‡†æµ‹è¯•çš„ä»£ç æ£€ç´¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°æœ‰ç›‘ç£æ€§èƒ½ï¼Œæ‰€æœ‰LLMéƒ½åœ¨MS MARCOæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å‘ç°æ•°å­¦ä¸“ä¸šåŒ–ä¸é•¿æ¨ç†èƒ½åŠ›ä¼šåœ¨ä¸‰ç§è®¾ç½®ä¸­é€ æˆæŒç»­çš„é€€åŒ–ï¼Œè¡¨æ˜æ•°å­¦æ¨ç†ä¸è¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šåŒ–çš„LLMåœ¨é›¶å°„å‡»æ€§èƒ½ä¸Šè¡¨ç°å‡ºå“è¶Šçš„è¡¨ç°ï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œåœ¨æœ‰ç›‘ç£ç¯å¢ƒä¸­ä¸åŸºç¡€LLMä¿æŒç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡æ˜¯å……æ»¡å¸Œæœ›çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03958v2">PDF</a> Accepted by CCIR25 and published by Springer LNCS or LNAI</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯†é›†æ£€ç´¢ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶é¢†åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMsçš„ä»»åŠ¡ç‰¹å®šé€‚åº”å¯¹å…¶æ£€ç´¢èƒ½åŠ›çš„å½±å“ï¼Œè¿™æ˜¯æœç€å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚é€šè¿‡å¯¹å…«ç§Qwen2.5<br>7B LLMsè¿›è¡Œå®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æŒ‡ä»¤è°ƒä¼˜ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šåŒ–ã€é€»è¾‘æ¨ç†å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨éå³å¸­æ£€ç´¢å’Œå—æ§ç›‘ç£ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›åœ¨ä¸‰ç§ç¯å¢ƒä¸­å‡å­˜åœ¨æŒç»­é€€åŒ–ç°è±¡ï¼Œè¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ç”¨LLMsåœ¨é›¶å°„å‡»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œå¹¶åœ¨å—æ§ç¯å¢ƒä¸­ä¿æŒäº†ä¸åŸºç¡€æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯†é›†æ£€ç´¢ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶é¢†åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>LLMsçš„ä»»åŠ¡ç‰¹å®šé€‚åº”å¯¹å…¶æ£€ç´¢èƒ½åŠ›æœ‰ç³»ç»Ÿæ€§å½±å“ï¼Œè¿™æ˜¯å¼€å‘ç»Ÿä¸€æ£€ç´¢å™¨çš„é‡è¦æ­¥éª¤ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹ã€‚</li>
<li>æ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›åœ¨LLMsçš„æ£€ç´¢è¡¨ç°ä¸­å­˜åœ¨å†²çªï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ç”¨LLMsåœ¨é›¶å°„å‡»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³è¶…è¶ŠBM25åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>LLMsåœ¨å—æ§ç›‘ç£ç¯å¢ƒä¸‹è¡¨ç°ç¨³å®šï¼Œä¸åŸºç¡€æ¨¡å‹æ€§èƒ½ç›¸å½“ã€‚</li>
<li>ç ”ç©¶å‘ç°è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-013a99ff3c6a8ba2f36ed89ac4ac46b2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Effective agent performance relies on the ability to compose tools and agents into effective workflows. However, progress in Large Language Model (LLM) planning and reasoning is limited by the scarcity of scalable, reliable evaluation data. This study addresses this limitation by identifying a suitable workflow domain for LLM application. I introduce NL2Flow, a fully automated system for parametrically generating planning problems, which are expressed in natural language, a structured intermediate representation, and formal PDDL, and rigorously evaluating the quality of generated plans. NL2Flow generates a dataset of 2296 low-difficulty problems in automated workflow generation and evaluates multiple open-sourced, instruct-tuned LLMs without task-specific optimization or architectural modifications. Results reveal that the highest performing model achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible plans. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. To investigate the potential of LLMs as natural language-to-JSON translators for workflow definition, and to facilitate integration with downstream symbolic computation tools and a symbolic planner, I evaluated the LLMâ€™s translation performance on natural language workflow descriptions. I observed that translating natural language into a JSON representation of a workflow problem yielded a lower success rate than generating a plan directly, suggesting that unnecessary decomposition of the reasoning task may degrade performance and highlighting the benefit of models capable of reasoning directly from natural language to action. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>æœ‰æ•ˆçš„æ™ºèƒ½ä½“æ€§èƒ½ä¾èµ–äºå°†å·¥å…·å’Œæ™ºèƒ½ä½“ç»„åˆæˆæœ‰æ•ˆå·¥ä½œæµç¨‹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢ç¼ºä¹å¯æ‰©å±•çš„ã€å¯é çš„è¯„ä»·æ•°æ®ï¼ŒLLMçš„è¿›å±•å—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶é€šè¿‡ç¡®å®šLLMåº”ç”¨çš„åˆé€‚å·¥ä½œæµç¨‹åŸŸæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚æˆ‘ä»‹ç»äº†NL2Flowï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç³»ç»Ÿï¼Œå¯ä»¥å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä»¥è‡ªç„¶è¯­è¨€è¡¨ç¤ºï¼Œå…·æœ‰ç»“æ„åŒ–çš„ä¸­é—´è¡¨ç¤ºå½¢å¼å’Œæ­£å¼çš„PDDLï¼Œå¹¶å¯¹ç”Ÿæˆè®¡åˆ’çš„è´¨é‡è¿›è¡Œä¸¥è°¨è¯„ä¼°ã€‚NL2Flowç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«2296ä¸ªä½éš¾åº¦é—®é¢˜çš„æ•°æ®é›†ï¼Œåœ¨è‡ªåŠ¨å·¥ä½œæµç¨‹ç”Ÿæˆä¸­è¯„ä¼°äº†å¤šä¸ªå¼€æºã€ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–æˆ–æ¶æ„ä¿®æ”¹ã€‚ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³æ¨¡å‹çš„ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æˆåŠŸç‡ä¸º86%ï¼Œç”Ÿæˆæœ€ä¼˜è®¡åˆ’æˆåŠŸç‡ä¸º69%ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æœ‰å¯è¡Œè®¡åˆ’çš„é—®é¢˜ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹å¾å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚ä¸ºäº†ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªç„¶è¯­è¨€åˆ°JSONç¿»è¯‘å™¨çš„æ½œåŠ›ï¼Œç”¨äºå·¥ä½œæµç¨‹å®šä¹‰ï¼Œå¹¶ä¾¿äºä¸ä¸‹æ¸¸ç¬¦å·è®¡ç®—å·¥å…·å’Œç¬¦å·è§„åˆ’å™¨é›†æˆï¼Œæˆ‘è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½åœ¨è‡ªç„¶è¯­è¨€å·¥ä½œæµæè¿°ä¸Šã€‚æˆ‘è§‚å¯Ÿåˆ°ï¼Œå°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆå·¥ä½œæµé—®é¢˜çš„JSONè¡¨ç¤ºå½¢å¼çš„æˆåŠŸç‡ä½äºç›´æ¥ç”Ÿæˆè®¡åˆ’çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜ä¸å¿…è¦çš„åˆ†è§£æ¨ç†ä»»åŠ¡å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹çš„å¥½å¤„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›åœ¨å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜æ—¶å¾—åˆ°æå‡ï¼Œäº†è§£è¿™äº›ç³»ç»Ÿå†…ç“¶é¢ˆå’Œé”™è¯¯æºçš„è½¬å˜å°†è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v3">PDF</a> 26 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå·¥ä½œæµç¨‹è®¡åˆ’æ–¹é¢çš„æ€§èƒ½ã€‚ç”±äºç¼ºå°‘å¯ä¼¸ç¼©ã€å¯é çš„è¯„ä¼°æ•°æ®ï¼ŒLLMåœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„è¿›å±•å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†NL2Flowç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€ã€ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºå½¢å¼å’Œæ­£å¼PDDLå½¢å¼è¡¨è¾¾ï¼ŒåŒæ—¶ä¸¥æ ¼è¯„ä¼°ç”Ÿæˆè®¡åˆ’çš„è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ€§èƒ½æœ€ä½³æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢è¾¾åˆ°äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢è¾¾åˆ°äº†69%ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†LLMä½œä¸ºè‡ªç„¶è¯­è¨€åˆ°JSONç¿»è¯‘å™¨çš„æ½œåŠ›ï¼Œç”¨äºå·¥ä½œæµç¨‹å®šä¹‰ï¼Œå¹¶ä¾¿äºä¸ä¸‹æ¸¸ç¬¦å·è®¡ç®—å·¥å…·å’Œç¬¦å·è§„åˆ’å™¨é›†æˆã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå°†è‡ªç„¶è¯­è¨€ç¿»è¯‘ä¸ºå·¥ä½œæµç¨‹é—®é¢˜çš„JSONè¡¨ç¤ºå½¢å¼ï¼Œå…¶æˆåŠŸç‡ä½äºç›´æ¥ç”Ÿæˆè®¡åˆ’ï¼Œè¿™è¡¨æ˜å°†æ¨ç†ä»»åŠ¡è¿‡åº¦åˆ†è§£å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒéœ€è¦èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹ã€‚éšç€LLMæ¨ç†åœ¨è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ä¸Šçš„æ‰©å±•ï¼Œç†è§£è¿™äº›ç³»ç»Ÿå†…éƒ¨ç“¶é¢ˆå’Œé”™è¯¯æ¥æºçš„è½¬å˜å°†è‡³å…³é‡è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿæˆå·¥ä½œæµç¨‹è®¡åˆ’æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ã€‚</li>
<li>NL2Flowç³»ç»Ÿå¯è‡ªåŠ¨ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œå¹¶ä¸¥æ ¼è¯„ä¼°ç”Ÿæˆè®¡åˆ’çš„è´¨é‡ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢è¾¾åˆ°86%çš„æˆåŠŸç‡ï¼Œç”Ÿæˆæœ€ä¼˜è®¡åˆ’ä¸º69%ã€‚</li>
<li>å›å½’åˆ†ææ­ç¤ºäº†é—®é¢˜ç‰¹æ€§ã€æ¨¡å‹å’Œæç¤ºè®¾è®¡å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“ã€‚</li>
<li>LLMä½œä¸ºè‡ªç„¶è¯­è¨€åˆ°JSONç¿»è¯‘å™¨çš„æ½œåŠ›ï¼Œä¾¿äºä¸ç¬¦å·è®¡ç®—å·¥å…·å’Œè§„åˆ’å™¨é›†æˆã€‚</li>
<li>ç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹æ€§èƒ½æ›´ä½³ï¼Œè¿‡åº¦åˆ†è§£æ¨ç†ä»»åŠ¡å¯èƒ½é™ä½æ€§èƒ½ã€‚</li>
<li>éšç€LLMæ¨ç†å¤„ç†æ›´å¤æ‚é—®é¢˜ï¼Œç†è§£ç³»ç»Ÿå†…éƒ¨è½¬å˜çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db1a26ecfa40678fa6c892a859b25ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8fbb06b9226d44d5933b6f55c6e886.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-437e6002cf95e423276f26901f987d9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ae47048764fa0ab3f30f98365be80fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AI4Research-A-Survey-of-Artificial-Intelligence-for-Scientific-Research"><a href="#AI4Research-A-Survey-of-Artificial-Intelligence-for-Scientific-Research" class="headerlink" title="AI4Research: A Survey of Artificial Intelligence for Scientific Research"></a>AI4Research: A Survey of Artificial Intelligence for Scientific Research</h2><p><strong>Authors:Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che</strong></p>
<p>Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAI-o1å’ŒDeepSeek-R1ï¼‰æ–¹é¢ï¼Œå·²åœ¨é€»è¾‘ã€æ¨ç†å’Œå®éªŒç¼–ç ç­‰å¤æ‚é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚å—åˆ°è¿™äº›è¿›æ­¥çš„æ¨åŠ¨ï¼Œè®¸å¤šç ”ç©¶å·²æ¢ç´¢å°†äººå·¥æ™ºèƒ½åº”ç”¨äºåˆ›æ–°è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦ç ”ç©¶èƒŒæ™¯ä¸‹ã€‚è¿™äº›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ç›®æ ‡ä¸»è¦æ˜¯å¼€å‘èƒ½å¤Ÿè‡ªä¸»å®Œæˆå¹¿æ³›ç§‘å­¦ç ”ç©¶è¿‡ç¨‹çš„ç³»ç»Ÿã€‚å°½ç®¡å–å¾—äº†è¿™äº›é‡è¦è¿›å±•ï¼Œä½†å…³äºäººå·¥æ™ºèƒ½ç”¨äºç ”ç©¶ï¼ˆAI4Researchï¼‰çš„å…¨é¢è°ƒæŸ¥ä»ç„¶ç¼ºå¤±ï¼Œè¿™é˜»ç¢äº†æˆ‘ä»¬å¯¹è¯¥é¢†åŸŸçš„ç†è§£å’Œè¿›ä¸€æ­¥å‘å±•ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œå¹¶ä¸ºAI4Researchæä¾›äº†ç»Ÿä¸€çš„è§†è§’ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬å·¥ä½œçš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰ç³»ç»Ÿåˆ†ç±»å­¦ï¼šæˆ‘ä»¬é¦–å…ˆå¼•å…¥ç³»ç»Ÿåˆ†ç±»å­¦ï¼Œå¯¹AI4Researchä¸­çš„äº”ä¸ªä¸»æµä»»åŠ¡è¿›è¡Œåˆ†ç±»ã€‚ï¼ˆ2ï¼‰æ–°å‰æ²¿ï¼šç„¶åï¼Œæˆ‘ä»¬ç¡®å®šäº†å…³é”®çš„ç ”ç©¶ç©ºç™½ï¼Œå¹¶å¼ºè°ƒäº†æœ‰å‰æ™¯çš„æœªæ¥æ–¹å‘ï¼Œä¾§é‡äºè‡ªåŠ¨åŒ–å®éªŒçš„ä¸¥è°¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠç¤¾ä¼šå½±å“ã€‚ï¼ˆ3ï¼‰ä¸°å¯Œçš„åº”ç”¨å’Œèµ„æºï¼šæœ€åï¼Œæˆ‘ä»¬æ•´ç†äº†å¤§é‡çš„èµ„æºï¼ŒåŒ…æ‹¬ç›¸å…³çš„å¤šå­¦ç§‘åº”ç”¨ã€æ•°æ®é›†åˆå’Œå·¥å…·ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½ä¸ºç ”ç©¶ç•Œå¿«é€Ÿæä¾›è¿™äº›èµ„æºï¼Œå¹¶åˆºæ¿€AI4Researchä¸­çš„åˆ›æ–°çªç ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01903v2">PDF</a> Preprint, Paper list is available at   <a target="_blank" rel="noopener" href="https://github.com/LightChen233/Awesome-AI4Research">https://github.com/LightChen233/Awesome-AI4Research</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸäººå·¥æ™ºèƒ½åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢çš„è¿›å±•ï¼Œå¦‚OpenAI-o1å’ŒDeepSeek-R1ï¼Œåœ¨é€»è¾‘å’Œå®éªŒç¼–ç ç­‰å¤æ‚é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œäººä»¬å¼€å§‹æ¢ç´¢äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä¸»è¦å¯¹AIåœ¨ç§‘ç ”é¢†åŸŸçš„åº”ç”¨è¿›è¡Œå…¨é¢è°ƒæŸ¥ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§’ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šæå‡ºç³»ç»Ÿåˆ†ç±»æ³•ï¼Œåˆ†ç±»äº†äº”å¤§ä¸»æµä»»åŠ¡ï¼›ç¡®å®šå…³é”®ç ”ç©¶ç©ºç™½å¹¶çªå‡ºæœªæ¥æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼›ç¼–è¯‘å¤šå­¦ç§‘åº”ç”¨ã€è¯­æ–™åº“å’Œå·¥å…·ç­‰èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢çš„è¿›å±•ä¸ºç§‘ç ”é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ½œåŠ›ã€‚</li>
<li>ç›®å‰å¯¹äºäººå·¥æ™ºèƒ½åœ¨ç§‘ç ”é¢†åŸŸåº”ç”¨çš„å…¨é¢è°ƒæŸ¥ä»æœ‰æ‰€æ¬ ç¼ºã€‚</li>
<li>ç³»ç»Ÿåˆ†ç±»æ³•è¢«ç”¨æ¥åˆ†ç±»äº”å¤§ä¸»æµä»»åŠ¡åœ¨äººå·¥æ™ºèƒ½ç”¨äºç§‘ç ”ä¸­ã€‚</li>
<li>ç¡®å®šå­˜åœ¨çš„å…³é”®ç ”ç©¶ç©ºç™½ä»¥åŠæœªæ¥æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œå°¤å…¶æ˜¯è‡ªåŠ¨åŒ–å®éªŒçš„ä¸¥è°¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠç¤¾ä¼šå½±å“æ–¹é¢ã€‚</li>
<li>ç¼–è¯‘äº†ä¸°å¯Œçš„èµ„æºï¼ŒåŒ…æ‹¬å¤šå­¦ç§‘åº”ç”¨ã€è¯­æ–™åº“å’Œå·¥å…·ç­‰ã€‚</li>
<li>æ­¤å·¥ä½œæ—¨åœ¨ä¸ºç§‘ç ”ç¤¾åŒºæä¾›å¿«é€Ÿè®¿é—®èµ„æºçš„é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-918e20f7b193abe03147dd836ed0471f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bc737bcf9a55d7042b5adbdeb27867.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Multi-Preference-Lambda-weighted-Listwise-DPO-for-Small-Scale-Model-Alignment"><a href="#Multi-Preference-Lambda-weighted-Listwise-DPO-for-Small-Scale-Model-Alignment" class="headerlink" title="Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model   Alignment"></a>Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model   Alignment</h2><p><strong>Authors:Yuhui Sun, Xiyao Wang, Zixi Li, Zhenlong Yuan, Jinman Zhao</strong></p>
<p>Large language models (LLMs) demonstrate strong generalization across a wide range of language tasks, but often generate outputs that misalign with human preferences. Reinforcement Learning from Human Feedback (RLHF) addresses this by optimizing models toward human preferences using a learned reward function and reinforcement learning, yielding improved alignment but suffering from high computational cost and instability. Direct Preference Optimization (DPO) simplifies the process by treating alignment as a classification task over binary preference pairs, reducing training overhead while achieving competitive performance. However, it assumes fixed, single-dimensional preferences and only supports pairwise supervision.   To address these limitations, we propose Multi-Preference Lambda-weighted Listwise DPO, which allows the model to learn from more detailed human feedback and flexibly balance multiple goals such as helpfulness, honesty, and fluency. Our method models full-ranked preference distributions rather than binary comparisons, enabling more informative learning signals. The lambda vector controls the relative importance of different alignment goals, allowing the model to generalize across diverse human objectives. During inference, lambda can be adjusted without retraining, providing controllable alignment behavior for downstream use. We also introduce a learned scheduler that dynamically samples performant lambda configurations to improve robustness.   Notably, our method requires only 20GB of GPU memory for training, making it suitable for compute-constrained settings such as academic labs, educational tools, or on-device assistants. Experiments on 1B-2B scale models show that our method consistently outperforms standard DPO on alignment benchmarks while enabling efficient, controllable, and fine-grained adaptation suitable for real-world deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å…¶ç”Ÿæˆçš„è¾“å‡ºå¾€å¾€ä¸äººç±»åå¥½ä¸ç¬¦ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡ä½¿ç”¨å­¦ä¹ çš„å¥–åŠ±å‡½æ•°å’Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹ä»¥ç¬¦åˆäººç±»åå¥½ï¼Œä»è€Œæé«˜å¯¹é½ç¨‹åº¦ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œä¸ç¨³å®šçš„é—®é¢˜ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®€åŒ–äº†æµç¨‹ï¼Œå°†å¯¹é½è§†ä¸ºäºŒå…ƒåå¥½å¯¹ä¸Šçš„åˆ†ç±»ä»»åŠ¡ï¼Œé™ä½äº†è®­ç»ƒå¼€é”€ï¼ŒåŒæ—¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒå‡å®šå›ºå®šçš„å•ç»´åå¥½ï¼Œä»…æ”¯æŒæˆå¯¹ç›‘ç£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19780v5">PDF</a> 12 pages, 12 figures, appendix included. To appear in Proceedings of   AAAI 2026. Code:   <a target="_blank" rel="noopener" href="https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO">https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</a></p>
<p><strong>æ‘˜è¦</strong><br>å¤§è¯­è¨€æ¨¡å‹å…·å¤‡å¹¿æ³›çš„è·¨è¯­è¨€ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¾€å¾€äº§ç”Ÿçš„è¾“å‡ºä¸äººç±»åå¥½ä¸ç¬¦ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡ä¼˜åŒ–æ¨¡å‹å‘äººç±»åå¥½é æ‹¢ï¼Œä½¿ç”¨å­¦ä¹ å¥–åŠ±å‡½æ•°å’Œå¼ºåŒ–å­¦ä¹ æ”¹è¿›å¯¹é½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”ä¸ç¨³å®šã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®€åŒ–äº†æµç¨‹ï¼Œå°†å¯¹é½è§†ä¸ºäºŒå…ƒåå¥½å¯¹çš„åˆ†ç±»ä»»åŠ¡ï¼Œå‡å°‘äº†è®­ç»ƒå¼€é”€ä¸”è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒå‡å®šå›ºå®šçš„å•ä¸€åå¥½å¹¶ä¸”ä»…æ”¯æŒæˆå¯¹ç›‘ç£ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šåå¥½Î»åŠ æƒåˆ—è¡¨DPOï¼Œå…è®¸æ¨¡å‹ä»æ›´è¯¦ç»†çš„äººç±»åé¦ˆä¸­å­¦ä¹ ï¼Œå¹¶çµæ´»å¹³è¡¡å¤šä¸ªç›®æ ‡å¦‚æœ‰ç”¨æ€§ã€è¯šå®æ€§å’Œæµç•…æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿå®Œæ•´çš„åå¥½åˆ†å¸ƒè€Œä¸æ˜¯äºŒå…ƒæ¯”è¾ƒï¼Œä»è€Œè·å¾—æ›´å¤šä¿¡æ¯çš„å­¦ä¹ ä¿¡å·ã€‚Î»å‘é‡æ§åˆ¶ä¸åŒå¯¹é½ç›®æ ‡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§äººç±»ç›®æ ‡ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒÎ»å¯ä»¥æ— éœ€é‡æ–°è®­ç»ƒè€Œè°ƒæ•´ï¼Œä¸ºä¸‹æ¸¸ä½¿ç”¨æä¾›å¯æ§çš„å¯¹é½è¡Œä¸ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå­¦ä¹ è°ƒåº¦å™¨ï¼ŒåŠ¨æ€é‡‡æ ·é«˜æ€§èƒ½çš„Î»é…ç½®ä»¥æé«˜ç¨³å¥æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…éœ€è¦20GBçš„GPUå†…å­˜è¿›è¡Œè®­ç»ƒï¼Œé€‚åˆè®¡ç®—å—é™çš„ç¯å¢ƒå¦‚å®éªŒå®¤ã€æ•™è‚²å·¥å…·æˆ–è®¾å¤‡åŠ©æ‰‹ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†å¯¹é½æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæ ‡å‡†DPOï¼ŒåŒæ—¶å®ç°äº†é«˜æ•ˆã€å¯æ§å’Œç²¾ç»†çš„é€‚åº”ï¼Œé€‚åˆéƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¸­ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¾“å‡ºå¸¸ä¸äººç±»åå¥½ä¸ç¬¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡ä¼˜åŒ–æ¨¡å‹ä»¥ç¬¦åˆäººç±»åå¥½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”ä¸ç¨³å®šã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®€åŒ–äº†æµç¨‹ï¼Œå°†å¯¹é½è§†ä¸ºäºŒå…ƒåå¥½å¯¹çš„åˆ†ç±»ä»»åŠ¡ï¼Œé™ä½è®­ç»ƒå¼€é”€å¹¶æé«˜æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¤šåå¥½Î»åŠ æƒåˆ—è¡¨DPOï¼Œè§£å†³äº†DPOçš„å±€é™æ€§ï¼Œå…è®¸æ¨¡å‹ä»æ›´è¯¦ç»†çš„äººç±»åé¦ˆä¸­å­¦ä¹ å¹¶çµæ´»å¹³è¡¡å¤šä¸ªç›®æ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•æ¨¡æ‹Ÿå®Œæ•´çš„åå¥½åˆ†å¸ƒè€ŒéäºŒå…ƒæ¯”è¾ƒï¼Œæä¾›æ›´ä¸ºä¿¡æ¯ä¸°å¯Œçš„å­¦ä¹ ä¿¡å·ã€‚</li>
<li>é€šè¿‡Î»å‘é‡æ§åˆ¶ä¸åŒå¯¹é½ç›®æ ‡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯è°ƒæ•´Î»å€¼ï¼Œä¸ºä¸‹æ¸¸åº”ç”¨æä¾›å¯æ§çš„å¯¹é½è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ad2ead2567d8e8884e4e66f4dfa523e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95882d2190268495d90c0eadcaaa93fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0b2abbf4c4cab080f17bf192bb1292f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eeca1d245b267f588210a3e5b4c7601d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96aa02ed69ea518eb15329a6ad623dbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9e3004eda7f78f8f1880d9c89a50f1c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EVINET-Towards-Open-World-Graph-Learning-via-Evidential-Reasoning-Network"><a href="#EVINET-Towards-Open-World-Graph-Learning-via-Evidential-Reasoning-Network" class="headerlink" title="EVINET: Towards Open-World Graph Learning via Evidential Reasoning   Network"></a>EVINET: Towards Open-World Graph Learning via Evidential Reasoning   Network</h2><p><strong>Authors:Weijie Guan, Haohui Wang, Jian Kang, Lihui Liu, Dawei Zhou</strong></p>
<p>Graph learning has been crucial to many real-world tasks, but they are often studied with a closed-world assumption, with all possible labels of data known a priori. To enable effective graph learning in an open and noisy environment, it is critical to inform the model users when the model makes a wrong prediction to in-distribution data of a known class, i.e., misclassification detection or when the model encounters out-of-distribution from novel classes, i.e., out-of-distribution detection. This paper introduces Evidential Reasoning Network (EVINET), a framework that addresses these two challenges by integrating Beta embedding within a subjective logic framework. EVINET includes two key modules: Dissonance Reasoning for misclassification detection and Vacuity Reasoning for out-of-distribution detection. Extensive experiments demonstrate that EVINET outperforms state-of-the-art methods across multiple metrics in the tasks of in-distribution classification, misclassification detection, and out-of-distribution detection. EVINET demonstrates the necessity of uncertainty estimation and logical reasoning for misclassification detection and out-of-distribution detection and paves the way for open-world graph learning. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/SSSKJ/EviNET">https://github.com/SSSKJ/EviNET</a>. </p>
<blockquote>
<p>å›¾å­¦ä¹ åœ¨å¤šä¸ªç°å®ä»»åŠ¡ä¸­å‡èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½†é€šå¸¸åœ¨å°é—­ä¸–ç•Œå‡è®¾ä¸‹è¿›è¡Œç ”ç©¶çš„ï¼Œäº‹å…ˆå·²çŸ¥æ‰€æœ‰æ•°æ®å¯èƒ½çš„æ‰€æœ‰æ ‡ç­¾ã€‚ä¸ºäº†åœ¨å¼€æ”¾ä¸”å……æ»¡å™ªéŸ³çš„ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆçš„å›¾å­¦ä¹ ï¼Œæ¨¡å‹ç”¨æˆ·å¯¹æ¨¡å‹åœ¨å·²çŸ¥ç±»åˆ«å†…éƒ¨æ•°æ®çš„é¢„æµ‹å‡ºé”™è¿›è¡Œåé¦ˆæ˜¯éå¸¸å…³é”®çš„ï¼Œæ¯”å¦‚é”™è¯¯åˆ†ç±»æ£€æµ‹æˆ–å½“æ¨¡å‹é‡åˆ°æœªçŸ¥ç±»åˆ«çš„æ•°æ®ï¼Œå³æœªçŸ¥åˆ†å¸ƒæ£€æµ‹ã€‚æœ¬æ–‡ä»‹ç»äº†Evidential Reasoning Networkï¼ˆEVINETï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•´åˆBetaåµŒå…¥ä¸»è§‚é€»è¾‘æ¡†æ¶æ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚EVINETåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç”¨äºé”™è¯¯åˆ†ç±»æ£€æµ‹çš„Dissonance Reasoningå’Œç”¨äºæœªçŸ¥åˆ†å¸ƒæ£€æµ‹çš„çœŸç©ºåŸç†ï¼ˆVacuity Reasoningï¼‰ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œåœ¨åˆ†å¸ƒå†…åˆ†ç±»ã€é”™è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥åˆ†å¸ƒæ£€æµ‹çš„ä»»åŠ¡ä¸­ï¼ŒEVINETåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚EVINETè¯æ˜äº†ä¸ç¡®å®šä¼°è®¡å’Œé€»è¾‘æ¨ç†å¯¹äºé”™è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥åˆ†å¸ƒæ£€æµ‹çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://github.com/SSSKJ/EviNET">https://github.com/SSSKJ/EviNET</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07288v3">PDF</a> KDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Evidential Reasoning Networkï¼ˆEVINETï¼‰æ¡†æ¶ï¼Œè§£å†³äº†åœ¨å¼€æ”¾å’Œå™ªå£°ç¯å¢ƒä¸‹çš„å›¾å­¦ä¹ æ‰€é¢ä¸´çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šè¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆBetaåµŒå…¥å’Œä¸»è§‚é€»è¾‘æ¡†æ¶ï¼Œå®ç°äº†ä¸å’Œè°æ¨ç†å’ŒçœŸç©ºæ¨ç†ä¸¤ä¸ªå…³é”®æ¨¡å—ã€‚å®éªŒè¯æ˜ï¼ŒEVINETåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶å¼ºè°ƒäº†ä¸ç¡®å®šæ€§å’Œé€»è¾‘æ¨ç†å¯¹äºè¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹çš„é‡è¦æ€§ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Evidential Reasoning Network (EVINET) æ˜¯ä¸€ä¸ªè§£å†³å¼€æ”¾å’Œå™ªå£°ç¯å¢ƒä¸‹å›¾å­¦ä¹ æŒ‘æˆ˜çš„æ–°æ¡†æ¶ã€‚</li>
<li>EVINET æ•´åˆäº†BetaåµŒå…¥å’Œä¸»è§‚é€»è¾‘æ¡†æ¶ï¼Œä»¥è¿›è¡Œä¸å’Œè°æ¨ç†å’ŒçœŸç©ºæ¨ç†ã€‚</li>
<li>ä¸å’Œè°æ¨ç†ç”¨äºè¯¯åˆ†ç±»æ£€æµ‹ï¼Œè€ŒçœŸç©ºæ¨ç†ç”¨äºæœªçŸ¥ç±»åˆ«æ£€æµ‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒEVINETåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>EVINET å¼ºè°ƒäº†ä¸ç¡®å®šæ€§å’Œé€»è¾‘æ¨ç†åœ¨è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>EVINET ä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb647ac1824623e02338798013f0c20a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9461f2eb58f89f7010702eb3d6e4893.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b9cd8d0d29ce3dba3358a6746355669.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-42d76b246d84322f1c5364930f29a881.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-11  CRAFT Your Dataset Task-Specific Synthetic Dataset Generation Through   Corpus Retrieval and Augmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6eca8c36554ad573a9576ae93ae01c08.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Population-Based Screening and Primary Diagnosis in a Global,   Multiethnic Population (Study Protocol)
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
