<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-06  TEMPLETemporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-1d84b630c6ef5e08e7192221537574d7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-06-æ›´æ–°"><a href="#2025-04-06-æ›´æ–°" class="headerlink" title="2025-04-06 æ›´æ–°"></a>2025-04-06 æ›´æ–°</h1><h2 id="TEMPLE-Temporal-Preference-Learning-of-Video-LLMs-via-Difficulty-Scheduling-and-Pre-SFT-Alignment"><a href="#TEMPLE-Temporal-Preference-Learning-of-Video-LLMs-via-Difficulty-Scheduling-and-Pre-SFT-Alignment" class="headerlink" title="TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment"></a>TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment</h2><p><strong>Authors:Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun</strong></p>
<p>Video Large Language Models (Video LLMs) have achieved significant success by leveraging a two-stage paradigm: pretraining on large-scale video-text data for vision-language alignment, followed by supervised fine-tuning (SFT) for task-specific capabilities. However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and reliance on the next-token prediction paradigm during training. To address these limitations, we propose TEMPLE (TEMporal Preference Learning), a systematic framework that enhances Video LLMsâ€™ temporal reasoning capabilities through Direct Preference Optimization (DPO). To facilitate this, we introduce an automated preference data generation pipeline that systematically constructs preference pairs by selecting videos that are rich in temporal information, designing video-specific perturbation strategies, and finally evaluating model responses on clean and perturbed video inputs. Our temporal alignment features two key innovations: curriculum learning which that progressively increases perturbation difficulty to improve model robustness and adaptability; and â€œPre-SFT Alignmentâ€™â€™, applying preference optimization before instruction tuning to prioritize fine-grained temporal comprehension. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. We further analyze the transferability of DPO data across architectures and the role of difficulty scheduling in optimization. Our findings highlight our TEMPLE as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE">https://github.com/lscpku/TEMPLE</a>. </p>
<blockquote>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰é€šè¿‡åˆ©ç”¨ä¸¤é˜¶æ®µèŒƒå¼å–å¾—äº†å·¨å¤§æˆåŠŸï¼šé¦–å…ˆåœ¨å¤§å‹è§†é¢‘æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œç„¶åè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥è·å–ç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºæ•°æ®ä¸­çš„æ—¶é—´å¯¹åº”å…³ç³»è¾ƒå¼±ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼ï¼Œå› æ­¤åœ¨æ—¶é—´æ¨ç†æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TEMPLEï¼ˆTEMPoral Preference Learningï¼Œæ—¶ç©ºåå¥½å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºè§†é¢‘LLMæ—¶é—´æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿæ¡†æ¶ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åå¥½æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡é€‰æ‹©å¯Œå«æ—¶é—´ä¿¡æ¯çš„è§†é¢‘ã€è®¾è®¡é’ˆå¯¹è§†é¢‘çš„ç‰¹æ®Šæ‰°åŠ¨ç­–ç•¥ï¼Œä»¥åŠè¯„ä¼°æ¨¡å‹å¯¹å¹²å‡€å’Œæ‰°åŠ¨è§†é¢‘è¾“å…¥çš„å“åº”æ¥ç³»ç»Ÿåœ°æ„å»ºåå¥½å¯¹ã€‚æˆ‘ä»¬çš„æ—¶é—´å¯¹é½å…·æœ‰ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè¯¾ç¨‹å­¦ä¹ ï¼Œé€šè¿‡é€æ¸å¢åŠ æ‰°åŠ¨éš¾åº¦æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼›ä»¥åŠâ€œPre-SFTå¯¹é½â€ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´ä¹‹å‰åº”ç”¨åå¥½ä¼˜åŒ–ï¼Œä»¥ä¼˜å…ˆè¿›è¡Œç²¾ç»†çš„æ—¶é—´ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨ç›¸å¯¹å°‘é‡çš„è‡ªæˆ‘ç”Ÿæˆçš„DPOæ•°æ®æ—¶ï¼Œèƒ½å¤ŸæŒç»­æé«˜è§†é¢‘LLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†DPOæ•°æ®åœ¨ä¸åŒæ¶æ„ä¹‹é—´çš„å¯è¿ç§»æ€§ä»¥åŠéš¾åº¦è°ƒåº¦åœ¨ä¼˜åŒ–ä¸­çš„è§’è‰²ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†æˆ‘ä»¬çš„TEMPLEä½œä¸ºåŸºäºSFTçš„æ–¹æ³•çš„å¯æ‰©å±•å’Œé«˜æ•ˆçš„è¡¥å……ï¼Œä¸ºå¼€å‘å¯é çš„è§†é¢‘LLMé“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lscpku/TEMPLEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16929v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰çš„ç°æœ‰æŒ‘æˆ˜åŠå…¶æ”¹è¿›æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨æ—¶åºæ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†TEMPLEæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºè§†é¢‘LLMçš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è‡ªåŠ¨åŒ–åå¥½æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡é€‰æ‹©å¯Œå«æ—¶åºä¿¡æ¯çš„è§†é¢‘ã€è®¾è®¡è§†é¢‘ç‰¹å®šæ‰°åŠ¨ç­–ç•¥ï¼Œä»¥åŠè¯„ä¼°æ¨¡å‹å¯¹å¹²å‡€å’Œæ‰°åŠ¨è§†é¢‘è¾“å…¥çš„å“åº”æ¥è¿›è¡Œä¼˜åŒ–ã€‚å…¶åˆ›æ–°ç‚¹åŒ…æ‹¬è¯¾ç¨‹å­¦ä¹ å’Œâ€œPre-SFTå¯¹é½â€ï¼Œé€æ­¥å¢åŠ æ‰°åŠ¨éš¾åº¦ä»¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ï¼Œå¹¶åœ¨æŒ‡ä»¤å¾®è°ƒä¹‹å‰åº”ç”¨åå¥½ä¼˜åŒ–ï¼Œä»¥ä¼˜å…ˆè¿›è¡Œç²¾ç»†çš„æ—¶åºç†è§£ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†è§†é¢‘LLMçš„æ€§èƒ½ï¼Œä¸”ä½¿ç”¨è‡ªç”Ÿæˆçš„DPOæ•°æ®çš„æ•ˆç‡è¾ƒé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video LLMsé‡‡ç”¨ä¸¤é˜¶æ®µèŒƒå¼ï¼šåœ¨å¤§å‹è§†é¢‘æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œç„¶åé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è·å¾—ç‰¹å®šä»»åŠ¡èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ—¶åºæ¨ç†çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ•°æ®ä¸­çš„æ—¶é—´å¯¹åº”å…³ç³»è¾ƒå¼±ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–äºä¸‹ä¸€ä¸ªä»¤ç‰Œçš„é¢„æµ‹èŒƒå¼ã€‚</li>
<li>TEMPLEæ¡†æ¶é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºè§†é¢‘LLMçš„æ—¶åºæ¨ç†èƒ½åŠ›ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TEMPLEå¼•å…¥äº†è‡ªåŠ¨åŒ–åå¥½æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡é€‰æ‹©å¯Œå«æ—¶åºä¿¡æ¯çš„è§†é¢‘ã€è®¾è®¡ç‰¹å®šçš„è§†é¢‘æ‰°åŠ¨ç­–ç•¥æ¥æ„å»ºåå¥½å¯¹ã€‚</li>
<li>TEMPLEåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šè¯¾ç¨‹å­¦ä¹ ï¼Œé€æ­¥å¢åŠ æ‰°åŠ¨éš¾åº¦ä»¥æé«˜æ¨¡å‹ç¨³å¥æ€§å’Œé€‚åº”æ€§ï¼›ä»¥åŠPre-SFTå¯¹é½ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒä¹‹å‰åº”ç”¨åå¥½ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTEMPLEæ–¹æ³•èƒ½æé«˜è§†é¢‘LLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œä¸”ä½¿ç”¨è‡ªç”Ÿæˆçš„DPOæ•°æ®çš„æ•ˆç‡è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5510be75ebe851a3144f1b3a10d7e22c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e7d12dbbb5194fa383c16648b21567a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be6a348d7181b26b10b8be3bcacdd034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f5aa05b29fc3af80cf08594d4f77c34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abd74dc5d9651e2ca31dfde7f587f11b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Safety-Evaluation-and-Enhancement-of-DeepSeek-Models-in-Chinese-Contexts"><a href="#Safety-Evaluation-and-Enhancement-of-DeepSeek-Models-in-Chinese-Contexts" class="headerlink" title="Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts"></a>Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts</h2><p><strong>Authors:Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Limin Han, Jiaojiao Zhao, Beibei Huang, Zhenhong Long, Junting Guo, Meijuan An, Rongjia Du, Ning Wang, Kai Wang, Shiguo Lian</strong></p>
<p>DeepSeek-R1, renowned for its exceptional reasoning capabilities and open-source strategy, is significantly influencing the global artificial intelligence landscape. However, it exhibits notable safety shortcomings. Recent research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 achieves a 100% attack success rate when processing harmful prompts. Furthermore, multiple security firms and research institutions have identified critical security vulnerabilities within the model. Although China Unicom has uncovered safety vulnerabilities of R1 in Chinese contexts, the safety capabilities of the remaining distilled models in the R1 series have not yet been comprehensively evaluated. To address this gap, this study utilizes the comprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth safety evaluation of the DeepSeek-R1 series distilled models. The objective is to assess the safety capabilities of these models in Chinese contexts both before and after distillation, and to further elucidate the adverse effects of distillation on model safety. Building on these findings, we implement targeted safety enhancements for six distilled models. Evaluation results indicate that the enhanced models achieve significant improvements in safety while maintaining reasoning capabilities without notable degradation. We open-source the safety-enhanced models at <a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main">https://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main</a> to serve as a valuable resource for future research and optimization of DeepSeek models. </p>
<blockquote>
<p>DeepSeek-R1ä»¥å…¶å‡ºè‰²çš„æ¨ç†èƒ½åŠ›å’Œå¼€æºç­–ç•¥è€Œé—»åï¼Œæ­£åœ¨å…¨çƒäººå·¥æ™ºèƒ½é¢†åŸŸäº§ç”Ÿé‡å¤§å½±å“ã€‚ç„¶è€Œï¼Œå®ƒå­˜åœ¨æ˜æ˜¾çš„å®‰å…¨ç¼ºé™·ã€‚æœ€è¿‘ï¼Œæ€ç§‘å­å…¬å¸Robust Intelligenceä¸å®¾å¤•æ³•å°¼äºšå¤§å­¦åˆä½œè¿›è¡Œçš„ç ”ç©¶æ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨å¤„ç†æœ‰å®³æç¤ºæ—¶è¾¾åˆ°äº†100%çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œå¤šå®¶å®‰å…¨å…¬å¸å’Œç ”ç©¶æœºæ„å·²å‘ç°è¯¥æ¨¡å‹ä¸­å­˜åœ¨å…³é”®çš„å®‰å…¨æ¼æ´ã€‚è™½ç„¶ä¸­å›½è”é€šå·²åœ¨ä¸­å›½èƒŒæ™¯ä¸‹å‘ç°äº†R1çš„å®‰å…¨æ¼æ´ï¼Œä½†R1ç³»åˆ—ä¸­å‰©ä½™è’¸é¦æ¨¡å‹çš„å®‰å…¨èƒ½åŠ›å°šæœªè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨å…¨é¢çš„ä¸­æ–‡å®‰å…¨åŸºå‡†CHiSafetyBenchå¯¹DeepSeek-R1ç³»åˆ—è’¸é¦æ¨¡å‹è¿›è¡Œæ·±å…¥çš„å®‰å…¨è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¸­å›½èƒŒæ™¯ä¸‹çš„è’¸é¦å‰åçš„å®‰å…¨èƒ½åŠ›ï¼Œå¹¶è¿›ä¸€æ­¥é˜æ˜è’¸é¦å¯¹æ¨¡å‹å®‰å…¨çš„è´Ÿé¢å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¯¹å…­ç§è’¸é¦æ¨¡å‹è¿›è¡Œäº†æœ‰é’ˆå¯¹æ€§çš„å®‰å…¨å¢å¼ºã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå¢å¼ºå‹æ¨¡å‹åœ¨å®‰å…¨æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†èƒ½åŠ›ï¼Œæ²¡æœ‰æ˜æ˜¾é€€åŒ–ã€‚æˆ‘ä»¬å°†å¼€æºå¢å¼ºå‹æ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main">https://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main</a>ï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶å’Œä¼˜åŒ–DeepSeekæ¨¡å‹æ—¶ä½œä¸ºæœ‰ä»·å€¼çš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16529v1">PDF</a> 21 pages,13 figures</p>
<p><strong>Summary</strong><br>     æ·±åº¦æœç´¢R1ä»¥å…¶å‡ºè‰²çš„æ¨ç†èƒ½åŠ›å’Œå¼€æºç­–ç•¥è€Œé—»åï¼Œæ­£åœ¨å…¨çƒäººå·¥æ™ºèƒ½é¢†åŸŸäº§ç”Ÿé‡å¤§å½±å“ã€‚ç„¶è€Œï¼Œå®ƒå­˜åœ¨æ˜¾è‘—çš„å®‰å…¨ç¼ºé™·ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦æœç´¢R1åœ¨å¤„ç†æœ‰å®³æç¤ºæ—¶è¾¾åˆ°äº†100%çš„æ”»å‡»æˆåŠŸç‡ã€‚å°½ç®¡ä¸­å›½è”é€šå¸¸å·²å‘ç°å…¶åœ¨ä¸­æ–‡ç¯å¢ƒä¸­çš„å®‰å…¨æ¼æ´ï¼Œä½†å¯¹R1ç³»åˆ—å…¶ä½™è’¸é¦æ¨¡å‹çš„å®‰å…¨èƒ½åŠ›å°šæœªè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚æœ¬ç ”ç©¶ä½¿ç”¨å…¨é¢çš„ä¸­æ–‡å®‰å…¨åŸºå‡†CHiSafetyBenchå¯¹æ·±åº¦æœç´¢R1ç³»åˆ—è’¸é¦æ¨¡å‹è¿›è¡Œæ·±å…¥çš„å®‰å…¨è¯„ä¼°ã€‚ç›®æ ‡æ˜¯è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨è’¸é¦å‰åçš„ä¸­æ–‡ç¯å¢ƒä¸‹çš„å®‰å…¨èƒ½åŠ›ï¼Œå¹¶è¿›ä¸€æ­¥é˜æ˜è’¸é¦å¯¹æ¨¡å‹å®‰å…¨çš„è´Ÿé¢å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¯¹å…­ç§è’¸é¦æ¨¡å‹è¿›è¡Œäº†æœ‰é’ˆå¯¹æ€§çš„å®‰å…¨å¢å¼ºã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå¢å¼ºå‹æ¨¡å‹åœ¨å®‰å…¨æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒæ¨ç†èƒ½åŠ›æ— æ˜æ˜¾ä¸‹é™ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main">https://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main</a>å…¬å¼€äº†å®‰å…¨å¢å¼ºçš„æ¨¡å‹ï¼Œä¸ºDeepSeekæ¨¡å‹çš„æœªæ¥ç ”ç©¶å’Œä¼˜åŒ–æä¾›å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦æœç´¢R1åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰æ˜¾è‘—å½±å“ï¼Œä½†å­˜åœ¨ä¸¥é‡çš„å®‰å…¨æ¼æ´ã€‚</li>
<li>æœ€æ–°ç ”ç©¶å‘ç°æ·±åº¦æœç´¢R1åœ¨å¤„ç†æœ‰å®³æç¤ºæ—¶æ”»å‡»æˆåŠŸç‡ä¸º100%ã€‚</li>
<li>ä¸­å›½è”é€šå¸¸å·²å‘ç°æ·±åº¦æœç´¢R1åœ¨ä¸­æ–‡ç¯å¢ƒä¸­çš„å®‰å…¨æ¼æ´ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨CHiSafetyBenchå¯¹æ·±åº¦æœç´¢R1ç³»åˆ—è’¸é¦æ¨¡å‹è¿›è¡Œå®‰å…¨è¯„ä¼°ã€‚</li>
<li>è¯„ä¼°ç›®çš„æ˜¯äº†è§£è¿™äº›æ¨¡å‹åœ¨è’¸é¦å‰åçš„ä¸­æ–‡ç¯å¢ƒä¸‹çš„å®‰å…¨èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°è’¸é¦è¿‡ç¨‹å¯¹æ¨¡å‹å®‰å…¨æœ‰è´Ÿé¢å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a308878ece4ecf2e2b1e625bb4f29f2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a2cbd30edd55cce4c36800712843cc2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cce29d6d6e0e026807f1c783c664fa6d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Think-or-Not-Think-A-Study-of-Explicit-Thinking-inRule-Based-Visual-Reinforcement-Fine-Tuning"><a href="#Think-or-Not-Think-A-Study-of-Explicit-Thinking-inRule-Based-Visual-Reinforcement-Fine-Tuning" class="headerlink" title="Think or Not Think: A Study of Explicit Thinking inRule-Based Visual   Reinforcement Fine-Tuning"></a>Think or Not Think: A Study of Explicit Thinking inRule-Based Visual   Reinforcement Fine-Tuning</h2><p><strong>Authors:Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Kaipeng Zhang</strong></p>
<p>This paper investigates rule-based reinforcement learning (RL) fine-tuning for visual classification using multi-modal large language models (MLLMs) and the role of the thinking process. We begin by exploring \textit{CLS-RL}, a method that leverages verifiable signals as rewards to encourage MLLMs to â€˜thinkâ€™ before classifying. Our experiments across \textbf{eleven} datasets demonstrate that CLS-RL achieves significant improvements over supervised fine-tuning (SFT) in both base-to-new generalization and few-shot learning scenarios. Notably, we observe a â€˜free-lunchâ€™ phenomenon where fine-tuning on one dataset unexpectedly enhances performance on others, suggesting that RL effectively teaches fundamental classification skills. However, we question whether the explicit thinking, a critical aspect of rule-based RL, is always beneficial or indispensable. Challenging the conventional assumption that complex reasoning enhances performance, we introduce \textit{No-Thinking-RL}, a novel approach that minimizes the modelâ€™s thinking during fine-tuning by utilizing an equality accuracy reward. Our experiments reveal that No-Thinking-RL achieves superior in-domain performance and generalization capabilities compared to CLS-RL, while requiring significantly less fine-tuning time. This underscores that, contrary to prevailing assumptions, reducing the thinking process can lead to more efficient and effective MLLM fine-tuning for some visual tasks. Furthermore, No-Thinking-RL demonstrates enhanced performance on other visual benchmarks, such as a 6.4% improvement on CVBench. We hope our findings provides insights into the impact of thinking in RL-based fine-tuning. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œè§†è§‰åˆ†ç±»çš„å¾®è°ƒæŠ€æœ¯ï¼Œä»¥åŠæ€è€ƒè¿‡ç¨‹çš„ä½œç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ¢è®¨äº†CLS-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥é¼“åŠ±MLLMåœ¨åˆ†ç±»ä¹‹å‰è¿›è¡Œâ€œæ€è€ƒâ€ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åä¸€ä¸ªæ•°æ®é›†ä¸Šï¼ŒCLS-RLåœ¨åŸºç¡€åˆ°æ–°çš„æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€ç§â€œå…è´¹åˆé¤â€ç°è±¡ï¼Œå³åœ¨æŸä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä¼šæ„å¤–åœ°æé«˜å…¶ä»–æ•°æ®é›†çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆåœ°ä¼ æˆäº†åŸºæœ¬çš„åˆ†ç±»æŠ€èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä¸­æ˜ç¡®çš„æ€è€ƒæ˜¯å¦æ€»æ˜¯æœ‰ç›Šæˆ–ä¸å¯æˆ–ç¼ºæŒæ€€ç–‘æ€åº¦ã€‚æˆ‘ä»¬æŒ‘æˆ˜äº†ä¼ ç»Ÿå‡è®¾â€”â€”å¤æ‚æ¨ç†èƒ½æé«˜æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹æ³•No-Thinking-RLã€‚å®ƒé€šè¿‡é‡‡ç”¨å¹³ç­‰ç²¾åº¦å¥–åŠ±æ¥æœ€å°åŒ–æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼ŒNo-Thinking-RLä¸CLS-RLç›¸æ¯”ï¼Œåœ¨é¢†åŸŸå†…éƒ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¸”éœ€è¦æ›´çŸ­çš„å¾®è°ƒæ—¶é—´ã€‚è¿™å¼ºè°ƒäº†ä¸€ä¸ªäº‹å®ï¼šä¸æ™®éå‡è®¾ç›¸åï¼Œå‡å°‘æ€è€ƒè¿‡ç¨‹å¯ä»¥é’ˆå¯¹æŸäº›è§†è§‰ä»»åŠ¡å®ç°æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„MLLMå¾®è°ƒã€‚æ­¤å¤–ï¼ŒNo-Thinking-RLåœ¨å…¶ä»–è§†è§‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¦‚åœ¨CVBenchä¸Šçš„æ”¹è¿›è¾¾åˆ°6.4%ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å‘ç°èƒ½ä¸ºåŸºäºRLçš„å¾®è°ƒä¸­çš„æ€è€ƒå½±å“æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16188v2">PDF</a> Preprint, work in progress. Add results on CVBench</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­çš„å¾®è°ƒæ–¹æ³•ï¼Œå¹¶ç ”ç©¶äº†æ€è€ƒè¿‡ç¨‹çš„ä½œç”¨ã€‚æ–‡ç« ä»‹ç»äº†CLS-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥é¼“åŠ±MLLMsåœ¨åˆ†ç±»ä¹‹å‰è¿›è¡Œâ€œæ€è€ƒâ€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLS-RLåœ¨åŸºç¡€åˆ°æ–°çš„æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è§‚å¯Ÿåˆ°ä¸€ç§â€œå…è´¹åˆé¤â€ç°è±¡ï¼Œå³åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä¼šæ„å¤–åœ°æé«˜åœ¨å…¶ä»–æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¡¨æ˜RLå®é™…ä¸Šæ•™ä¼šäº†æ¨¡å‹åŸºæœ¬çš„åˆ†ç±»æŠ€èƒ½ã€‚ç„¶è€Œï¼Œæ–‡ç« ä¹Ÿè´¨ç–‘æ˜ç¡®çš„æ€è€ƒè¿‡ç¨‹æ˜¯å¦æ€»æ˜¯æœ‰ç›Šæˆ–å¿…ä¸å¯å°‘ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— æ€è€ƒå¼ºåŒ–å­¦ä¹ ï¼ˆNo-Thinking-RLï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¹³ç­‰ç²¾åº¦å¥–åŠ±æ¥æœ€å°åŒ–æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNo-Thinking-RLåœ¨åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºCLS-RLï¼Œä¸”å¾®è°ƒæ—¶é—´å¤§å¤§å‡å°‘ã€‚è¿™è¡¨æ˜åœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸­ï¼Œå‡å°‘æ€è€ƒè¿‡ç¨‹å¯èƒ½å¯¼è‡´æ›´æœ‰æ•ˆç‡ä¸”æ•ˆæœæ›´å¥½çš„MLLMå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>CLS-RLæ–¹æ³•åˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥é¼“åŠ±æ¨¡å‹åœ¨åˆ†ç±»å‰â€œæ€è€ƒâ€ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è§‚å¯Ÿåˆ°ä¸€ç§â€œå…è´¹åˆé¤â€ç°è±¡ï¼Œå³åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å¾®è°ƒèƒ½æé«˜åœ¨å…¶ä»–æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¡¨æ˜å¼ºåŒ–å­¦ä¹ æ•™ä¼šäº†æ¨¡å‹åŸºæœ¬çš„åˆ†ç±»æŠ€èƒ½ã€‚</li>
<li>è®ºæ–‡è´¨ç–‘æ˜ç¡®çš„æ€è€ƒè¿‡ç¨‹æ˜¯å¦æ€»æ˜¯æœ‰ç›Šæˆ–å¿…ä¸å¯å°‘ï¼Œå¹¶å¼•å…¥äº†No-Thinking-RLæ–¹æ³•ã€‚</li>
<li>No-Thinking-RLé€šè¿‡æœ€å°åŒ–æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼Œå®ç°äº†ä¼˜äºCLS-RLçš„åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†å¾®è°ƒæ—¶é—´ã€‚</li>
<li>å‡å°‘æ€è€ƒè¿‡ç¨‹åœ¨æŸäº›è§†è§‰ä»»åŠ¡ä¸­å¯èƒ½å¯¼è‡´æ›´æœ‰æ•ˆç‡ä¸”æ•ˆæœæ›´å¥½çš„MLLMå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-866251bec4922f62650474d8c5e7a524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c60867265dfc7ac113cc743e22f5ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da83c51ce5b007bc6b87eb9c77b44bec.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning"><a href="#Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning" class="headerlink" title="Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning"></a>Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning</h2><p><strong>Authors: NVIDIA,  :, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, Zhe Zhang</strong></p>
<p>Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1">https://github.com/nvidia-cosmos/cosmos-reason1</a>. </p>
<blockquote>
<p>ç‰©ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿéœ€è¦åœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥ã€ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„åŠ¨ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Cosmos-Reason1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸€ç³»åˆ—æ·±å…¥çš„æ€è€ƒè¿‡ç¨‹ç†è§£ç‰©ç†ä¸–ç•Œï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼åšå‡ºé€‚å½“çš„å®ä½“å†³ç­–ï¼ˆä¾‹å¦‚ï¼Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ç‰©ç†äººå·¥æ™ºèƒ½æ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†ã€‚ä¸ºäº†è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œæˆ‘ä»¬ä½¿ç”¨å±‚æ¬¡æœ¬ä½“è®ºæ¥æ•æ‰å…³äºç©ºé—´ã€æ—¶é—´å’Œç‰©ç†å­¦çš„åŸºæœ¬çŸ¥è¯†ã€‚å¯¹äºå®ä½“æ¨ç†ï¼Œæˆ‘ä»¬ä¾èµ–äºä¸€ä¸ªäºŒç»´æœ¬ä½“è®ºï¼Œè¯¥æœ¬ä½“è®ºå¯ä»¥æ¦‚æ‹¬ä¸åŒçš„ç‰©ç†å®ä½“ã€‚åŸºäºè¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå³Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚æˆ‘ä»¬æ•´ç†æ•°æ®å¹¶åœ¨å››ä¸ªé˜¶æ®µè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼šè§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç‰©ç†äººå·¥æ™ºèƒ½SFTå’Œç‰©ç†äººå·¥æ™ºèƒ½å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºåè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æ ¹æ®æˆ‘ä»¬çš„æœ¬ä½“è®ºå»ºç«‹äº†ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç‰©ç†äººå·¥æ™ºèƒ½SFTå’Œå¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸ºäº†ä¿ƒè¿›ç‰©ç†äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæˆ‘ä»¬å°†åœ¨NVIDIA Open Model Licenseä¸‹æä¾›æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ°å€æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1%E3%80%82">https://github.com/nvidia-cosmos/cosmos-reason1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15558v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Cosmos-Reason1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£ç‰©ç†ä¸–ç•Œå¹¶ç”Ÿæˆç›¸åº”çš„å…·ä½“å†³ç­–ã€‚å®ƒèšç„¦äºç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†ï¼Œä½¿ç”¨åˆ†å±‚æœ¬ä½“è®ºæ¥è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œå¹¶ä¾èµ–äºŒç»´æœ¬ä½“è®ºè¿›è¡Œå®ä½“æ¨ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡å››ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒå’Œå¼€å‘ï¼ŒåŒ…æ‹¬è§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒã€ç‰©ç†AIç›‘ç£å’Œç‰©ç†AIå¼ºåŒ–å­¦ä¹ ã€‚è¯„ä¼°å’Œå®éªŒç»“æœè¡¨æ˜ï¼Œç‰©ç†AIç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cosmos-Reason1æ¨¡å‹èƒ½ç†è§£ç‰©ç†ä¸–ç•Œå¹¶ç”Ÿæˆå…·ä½“å†³ç­–ã€‚</li>
<li>æ¨¡å‹èšç„¦äºç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†ã€‚</li>
<li>ä½¿ç”¨åˆ†å±‚æœ¬ä½“è®ºå’ŒäºŒç»´æœ¬ä½“è®ºè¿›è¡ŒçŸ¥è¯†è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹ç»è¿‡å››ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒå’Œå¼€å‘ï¼ŒåŒ…æ‹¬è§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒã€ç‰©ç†AIç›‘ç£å’Œç‰©ç†AIå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>è¯„ä¼°å’Œå®éªŒæ˜¾ç¤ºï¼Œç‰©ç†AIç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ¨¡å‹å¯åº”ç”¨äºå¤šç§åœºæ™¯ï¼Œå¦‚æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5180c62fc77f538fc50442309d0ced5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856bc8687ff4a4cacb12a7ffe717b60f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ec9f52c985fc47aa47580339277604c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-731456c5726227d06f6f44bf8457ddbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99659305b782ad2bbf56ee38fd36b36c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have advanced reasoning in natural scenes, but their role in medical imaging remains underexplored. Medical reasoning tasks demand robust image analysis and well-justified answers, posing challenges due to the complexity of medical images. Transparency and trustworthiness are essential for clinical adoption and regulatory compliance. We introduce Med-R1, a framework exploring reinforcement learning (RL) to enhance VLMsâ€™ generalizability and trustworthiness in medical reasoning. Leveraging the DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which often overfits and lacks generalization, RL fosters robust and diverse reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT, MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography (OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B, Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B, which has 36 times more parameters. Testing across five question types-modality recognition, anatomy identification, disease diagnosis, lesion grading, and biological attribute analysis Med-R1 demonstrates superior generalization, exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type generalization. These findings show that RL improves medical reasoning and enables parameter-efficient models to outperform significantly larger ones. By demonstrating strong cross-domain and cross-task performance, Med-R1 points toward a new direction for developing practical and generalizable medical VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›å·²ç»å¾—åˆ°æå‡ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚åŒ»å­¦æ¨ç†ä»»åŠ¡éœ€è¦å¯é çš„å›¾åƒåˆ†æå’Œæœ‰å……åˆ†ä¾æ®çš„ç­”æ¡ˆï¼Œç”±äºåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ï¼Œè¿™æ„æˆäº†æŒ‘æˆ˜ã€‚é€æ˜åº¦å’Œå¯ä¿¡åº¦å¯¹äºä¸´åºŠé‡‡ç”¨å’Œæ³•è§„åˆè§„è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-R1æ¡†æ¶ï¼Œæ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å¢å¼ºVLMsåœ¨åŒ»å­¦æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚åˆ©ç”¨DeepSeekç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼æ¨ç†è·¯å¾„ã€‚ä¸ç»å¸¸è¿‡åº¦æ‹Ÿåˆä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼ŒRLä¿ƒè¿›ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚Med-R1åœ¨å…«ç§åŒ»å­¦å½±åƒæ¨¡æ€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€è¶…å£°æ³¢ã€çš®è‚¤é•œæ£€æŸ¥ã€çœ¼åº•æ‘„å½±ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€æ˜¾å¾®é•œå’ŒXå°„çº¿æˆåƒã€‚ç›¸æ¯”å…¶åŸºå‡†æ¨¡å‹Qwen2-VL-2Bï¼ŒMed-R1å®ç°äº†29.94%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶è¶…è¶Šäº†å‚æ•°æ›´å¤šçš„Qwen2-VL-72Bã€‚åœ¨äº”ç§é—®é¢˜ç±»å‹ï¼ˆæ¨¡æ€è¯†åˆ«ã€è§£å‰–ç»“æ„è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†çº§å’Œç”Ÿç‰©å±æ€§åˆ†æï¼‰çš„æµ‹è¯•ä¸Šï¼ŒMed-R1å±•ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸å¯¹äºQwen2-VL-2Bæé«˜32.06%ï¼Œå¹¶åœ¨é—®é¢˜ç±»å‹æ³›åŒ–ä¸Šè¶…è¶Šäº†Qwen2-VL-72Bã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½æå‡åŒ»å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä½¿å¾—å‚æ•°æ•ˆç‡æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—è¶…è¶Šæ›´å¤§çš„æ¨¡å‹ã€‚é€šè¿‡å±•ç¤ºå¼ºå¤§çš„è·¨åŸŸå’Œè·¨ä»»åŠ¡æ€§èƒ½ï¼ŒMed-R1ä¸ºå¼€å‘å®ç”¨å’Œé€šç”¨çš„åŒ»å­¦VLMsæŒ‡æ˜äº†æ–°æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨åŒ»ç–—å›¾åƒé¢†åŸŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—å½±åƒä¸­çš„åº”ç”¨ä»å¾…å‘æ˜ã€‚ç”±äºå¤æ‚çš„åŒ»å­¦å½±åƒæ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸€ç§é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å¢å¼ºå…¶æ³›åŒ–èƒ½åŠ›å’Œä¿¡ä»»åº¦çš„æ¨¡å‹æ¡†æ¶Med-R1åº”è¿è€Œç”Ÿã€‚åˆ©ç”¨DeepSeekç­–ç•¥å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰æ¥æŒ‡å¯¼æ¨ç†è·¯å¾„ã€‚ç›¸è¾ƒäºä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRLæé«˜äº†æ¨ç†çš„ç¨³å¥æ€§å’Œå¤šæ ·æ€§ã€‚åœ¨å…«ç§åŒ»å­¦å½±åƒæ¨¡æ€çš„æµ‹è¯•ä¸‹ï¼ŒMed-R1ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹Qwen2-VL-2Bæé«˜äº†29.94%çš„å‡†ç¡®æ€§ï¼Œä¸”åœ¨æ³›åŒ–æ€§èƒ½æ–¹é¢æ˜¾è‘—è¶…è¶Šåè€…å’Œæ›´å¤§çš„æ¨¡å‹Qwen2-VL-72Bã€‚è¿™æ˜¾ç¤ºäº†RLåœ¨åŒ»ç–—æ¨ç†ä¸­çš„ä¼˜åŠ¿ï¼Œå¹¶å¼€å¯äº†å¼€å‘å®ç”¨ä¸”é€šç”¨åŒ–çš„åŒ»ç–—VLMsçš„æ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨åŒ»ç–—å½±åƒä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>åŒ»ç–—æ¨ç†ä»»åŠ¡éœ€è¦å¼ºå¤§çš„å›¾åƒåˆ†æå’Œåˆç†ç­”æ¡ˆçš„èƒ½åŠ›ã€‚</li>
<li>Med-R1æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜VLMsçš„æ³›åŒ–èƒ½åŠ›å’Œä¿¡ä»»åº¦ã€‚</li>
<li>é‡‡ç”¨DeepSeekç­–ç•¥å’ŒGRPOæ–¹æ³•å¼•å¯¼æ¨ç†è·¯å¾„ã€‚</li>
<li>ä¸ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼Œå¼ºåŒ–å­¦ä¹ æé«˜äº†æ¨ç†çš„ç¨³å¥æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>Med-R1æ¨¡å‹åœ¨å„ç§åŒ»å­¦å½±åƒæ¨¡æ€å’Œå¤šç§ç±»å‹çš„é—®é¢˜æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œç›¸æ¯”åŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-493098047c4888775e2434f309a91f1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1ed8d422ae58efcff4e27383b15f5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b399f0b2843dcc1dd51f473a8f67034d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93fca9a2ac353f03516b58dc89ef0d52.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Light-R1-Curriculum-SFT-DPO-and-RL-for-Long-COT-from-Scratch-and-Beyond"><a href="#Light-R1-Curriculum-SFT-DPO-and-RL-for-Long-COT-from-Scratch-and-Beyond" class="headerlink" title="Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and   Beyond"></a>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and   Beyond</h2><p><strong>Authors:Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang</strong></p>
<p>This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.   Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 &amp; 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.   Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at <a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1">https://github.com/Qihoo360/Light-R1</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Light-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºå¥—ä»¶ï¼Œé‡‡ç”¨å¯é‡å¤å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•è®­ç»ƒé•¿æ¨ç†æ¨¡å‹ã€‚è€ƒè™‘åˆ°DeepSeek-R1ç³»åˆ—ä¸­ä½¿ç”¨çš„æ•°æ®çš„ä¸“æœ‰æ€§è´¨ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§çº¯ç²¹åˆ©ç”¨å…¬å¼€æ•°æ®å’Œæ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„è¯¾ç¨‹è®­ç»ƒé€æ­¥å¢åŠ æ•°æ®éš¾åº¦ï¼Œå¹¶ç»“åˆå¤šé˜¶æ®µåè®­ç»ƒã€‚æˆ‘ä»¬çš„Light-R1-32Bæ¨¡å‹ï¼Œä»¥Qwen2.5-32B-Instructè¿›è¡Œè®­ç»ƒï¼Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢ä¼˜äºDeepSeek-R1-Distill-Qwen-32Bã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä¸åŒè®­ç»ƒé˜¶æ®µæ‹¥æœ‰ä¸åŒä¸”å¤šæ ·çš„æ•°æ®é›†æ—¶ï¼Œè¿™ç§è¯¾ç¨‹æ–¹æ³•å˜å¾—æ›´åŠ æœ‰æ•ˆï¼šç”¨æˆ‘ä»¬çš„è¯¾ç¨‹æ•°æ®é›†ä¸­çš„3000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¾‹å­å¯¹DeepSeek-R1-Distilledæ¨¡å‹ï¼ˆDeepSeekå›¢é˜Ÿåœ¨ä¸“æœ‰æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼‰è¿›è¡Œå¾®è°ƒï¼Œäº§ç”Ÿäº†æœ€å…ˆè¿›çš„7Bå’Œ14Bæ¨¡å‹ï¼Œè€Œ32Bæ¨¡å‹Light-R1-32B-DSçš„è¡¨ç°ä¸QwQ-32Bå’ŒDeepSeek-R1ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å·¥ä½œæ‰©å±•åˆ°é•¿æ¨ç†æ¨¡å‹ä¸Šåº”ç”¨GRPOã€‚æˆ‘ä»¬æœ€ç»ˆçš„Light-R1-14B-DSåœ¨æ•°å­¦çš„14Bæ¨¡å‹ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼ŒAIME24å’ŒAIME25çš„åˆ†æ•°åˆ†åˆ«ä¸º74.0å’Œ60.2ï¼Œè¶…è¿‡äº†è®¸å¤š32Bæ¨¡å‹å’ŒDeepSeek-R1-Distill-Llama-70Bã€‚å°½ç®¡ä»¥æ•°å­¦ä¸ºé‡ç‚¹è¿›è¡Œè®­ç»ƒï¼Œä½†Light-R1-14B-DSè¡¨ç°å‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚Light-R1æ ‡å¿—ç€è®©å¤æ‚çš„æ¨ç†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­æ›´åŠ å¯è®¿é—®å’Œå¯å®ç°çš„ä¸€ä¸ªé‡å¤§è¿›æ­¥ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€è®­ç»ƒæ•°æ®å’Œä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Qihoo360/Light-R1ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10460v3">PDF</a> v3: minor modifications; v2: better writing &amp; format for later   submission; all release at <a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1">https://github.com/Qihoo360/Light-R1</a></p>
<p><strong>Summary</strong></p>
<p>Light-R1æ˜¯ä¸€å¥—å¼€æºçš„ç”¨äºè®­ç»ƒé•¿æ¨ç†æ¨¡å‹çš„å¥—ä»¶ï¼Œå®ƒé‡‡ç”¨å¯å¤åˆ¶å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è¯¾ç¨‹è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å…¬å¼€æ•°æ®å’Œæ¨¡å‹ï¼Œé€æ­¥å¢åŠ æ•°æ®éš¾åº¦å¹¶ç»“åˆå¤šé˜¶æ®µåè®­ç»ƒã€‚Light-R1ç³»åˆ—æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯Light-R1-32Bæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°è¯•å°†GRPOåº”ç”¨äºé•¿æ¨ç†æ¨¡å‹ï¼Œæœ€ç»ˆLight-R1-14B-DSæ¨¡å‹åœ¨æ•°å­¦ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Light-R1æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒé•¿æ¨ç†æ¨¡å‹çš„å¼€æºå¥—ä»¶ï¼Œé‡‡ç”¨å¯å¤åˆ¶å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è¯¾ç¨‹è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å…¬å¼€æ•°æ®å’Œæ¨¡å‹ã€‚</li>
<li>Light-R1ç³»åˆ—æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Light-R1-32Bæ¨¡å‹åœ¨ç‰¹å®šè®­ç»ƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>GRPOè¢«æˆåŠŸåº”ç”¨äºé•¿æ¨ç†æ¨¡å‹ï¼ŒLight-R1-14B-DSæ¨¡å‹å®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>Light-R1-14B-DSæ¨¡å‹åœ¨æ•°å­¦ä¸Šå…·æœ‰å¼ºåŠ²è¡¨ç°ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Light-R1çš„ç ”ç©¶ä½¿å¾—é«˜çº§æ¨ç†æ¨¡å‹æ›´æ˜“äºè®¿é—®å¹¶åœ¨å®é™…åº”ç”¨ç¨‹åºä¸­å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f7b62a538b1534eecf397ddad360e52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79905522623dadf408e4a770abc1d279.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd8ac68a17a2bb9b13bd9613957599eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1df2a45f2ea69557e33b58c003c6cc55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d84b630c6ef5e08e7192221537574d7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VisRL-Intention-Driven-Visual-Perception-via-Reinforced-Reasoning"><a href="#VisRL-Intention-Driven-Visual-Perception-via-Reinforced-Reasoning" class="headerlink" title="VisRL: Intention-Driven Visual Perception via Reinforced Reasoning"></a>VisRL: Intention-Driven Visual Perception via Reinforced Reasoning</h2><p><strong>Authors:Zhangquan Chen, Xufang Luo, Dongsheng Li</strong></p>
<p>Visual understanding is inherently intention-driven - humans selectively focus on different regions of a scene based on their goals. Recent advances in large multimodal models (LMMs) enable flexible expression of such intentions through natural language, allowing queries to guide visual reasoning processes. Frameworks like Visual Chain-of-Thought have demonstrated the benefit of incorporating explicit reasoning steps, where the model predicts a focus region before answering a query. However, existing approaches rely heavily on supervised training with annotated intermediate bounding boxes, which severely limits scalability due to the combinatorial explosion of intention-region pairs. To overcome this limitation, we propose VisRL, the first framework that applies reinforcement learning (RL) to the problem of intention-driven visual perception. VisRL optimizes the entire visual reasoning process using only reward signals. By treating intermediate focus selection as an internal decision optimized through trial-and-error, our method eliminates the need for costly region annotations while aligning more closely with how humans learn to perceive the world. Extensive experiments across multiple benchmarks show that VisRL consistently outperforms strong baselines, demonstrating both its effectiveness and its strong generalization across different LMMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhangquanchen/VisRL">https://github.com/zhangquanchen/VisRL</a>. </p>
<blockquote>
<p>è§†è§‰ç†è§£æœ¬è´¨ä¸Šæ˜¯ç›®æ ‡é©±åŠ¨çš„â€”â€”äººç±»ä¼šæ ¹æ®ä»–ä»¬çš„ç›®æ ‡é€‰æ‹©æ€§åœ°å…³æ³¨åœºæ™¯çš„ä¸åŒåŒºåŸŸã€‚æœ€è¿‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›æ­¥å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€çµæ´»åœ°è¡¨è¾¾è¿™æ ·çš„æ„å›¾ï¼Œå…è®¸æŸ¥è¯¢å¼•å¯¼è§†è§‰æ¨ç†è¿‡ç¨‹ã€‚åƒâ€œè§†è§‰æ€ç»´é“¾â€è¿™æ ·çš„æ¡†æ¶å·²ç»è¯æ˜äº†åŠ å…¥æ˜ç¡®æ¨ç†æ­¥éª¤çš„å¥½å¤„ï¼Œå…¶ä¸­æ¨¡å‹åœ¨å›ç­”é—®é¢˜ä¹‹å‰ä¼šé¢„æµ‹ä¸€ä¸ªå…³æ³¨åŒºåŸŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºç”¨æ ‡æ³¨çš„ä¸­é—´è¾¹ç•Œæ¡†è¿›è¡Œçš„æœ‰ç›‘ç£è®­ç»ƒï¼Œè¿™ç”±äºæ„å›¾-åŒºåŸŸå¯¹çš„ç»„åˆçˆ†ç‚¸è€Œä¸¥é‡é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VisRLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºç›®æ ‡é©±åŠ¨è§†è§‰æ„ŸçŸ¥é—®é¢˜çš„æ¡†æ¶ã€‚VisRLä»…ä½¿ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ•´ä¸ªè§†è§‰æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å°†ä¸­é—´ç„¦ç‚¹é€‰æ‹©è§†ä¸ºé€šè¿‡è¯•é”™ä¼˜åŒ–çš„å†…éƒ¨å†³ç­–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ˜‚è´µçš„åŒºåŸŸæ ‡æ³¨çš„éœ€æ±‚ï¼ŒåŒæ—¶æ›´è´´è¿‘äººç±»å­¦ä¹ æ„ŸçŸ¥ä¸–ç•Œçš„æ–¹å¼ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVisRLå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº <a target="_blank" rel="noopener" href="https://github.com/zhangquanchen/VisRL%E3%80%82">https://github.com/zhangquanchen/VisRLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07523v2">PDF</a> 18pages,11 figures</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰ç†è§£æœ¬è´¨ä¸Šæ˜¯æ„å›¾é©±åŠ¨çš„â€”â€”äººç±»ä¼šæ ¹æ®ç›®æ ‡æœ‰é€‰æ‹©åœ°å…³æ³¨åœºæ™¯çš„ä¸åŒåŒºåŸŸã€‚æœ€è¿‘çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›æ­¥èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€çµæ´»è¡¨è¾¾æ„å›¾ï¼Œä½¿å¾—æŸ¥è¯¢èƒ½å¤Ÿå¼•å¯¼è§†è§‰æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¸¦æœ‰ä¸­é—´è¾¹ç•Œæ¡†æ³¨è§£çš„ç›‘ç£è®­ç»ƒï¼Œè¿™é™åˆ¶äº†æ‰©å±•æ€§ï¼Œå› ä¸ºæ„å›¾åŒºåŸŸå¯¹çš„ç»„åˆå‘ˆç°çˆ†ç‚¸æ€§å¢é•¿ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºVisRLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºæ„å›¾é©±åŠ¨è§†è§‰æ„ŸçŸ¥é—®é¢˜çš„æ¡†æ¶ã€‚VisRLä»…ä½¿ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ•´ä¸ªè§†è§‰æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å°†ä¸­é—´ç„¦ç‚¹é€‰æ‹©è§†ä¸ºé€šè¿‡è¯•é”™ä¼˜åŒ–çš„å†…éƒ¨å†³ç­–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ˜‚è´µçš„åŒºåŸŸæ³¨è§£çš„éœ€æ±‚ï¼ŒåŒæ—¶æ›´è´´è¿‘äººç±»æ„ŸçŸ¥ä¸–ç•Œçš„å­¦ä¹ æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»çš„è§†è§‰ç†è§£æ˜¯æ„å›¾é©±åŠ¨çš„ï¼Œä¼šåŸºäºç›®æ ‡é€‰æ‹©å…³æ³¨åœºæ™¯çš„ä¸åŒåŒºåŸŸã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰èƒ½å¤Ÿçµæ´»è¡¨è¾¾æ„å›¾ï¼Œå¼•å¯¼è§†è§‰æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ç›‘ç£è®­ç»ƒå’Œä¸­é—´è¾¹ç•Œæ¡†æ³¨è§£ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚</li>
<li>VisRLæ¡†æ¶é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºæ„å›¾é©±åŠ¨è§†è§‰æ„ŸçŸ¥é—®é¢˜ã€‚</li>
<li>VisRLä¼˜åŒ–æ•´ä¸ªè§†è§‰æ¨ç†è¿‡ç¨‹ï¼Œä»…ä½¿ç”¨å¥–åŠ±ä¿¡å·ã€‚</li>
<li>VisRLé€šè¿‡è¯•é”™ä¼˜åŒ–ä¸­é—´ç„¦ç‚¹é€‰æ‹©ï¼Œæ— éœ€æ˜‚è´µçš„åŒºåŸŸæ³¨è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d428624ff675e2f0049127e6d74aea16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-002a9a866abcf7112e5a6556a41de6e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cafd9c3c54bd0815aebcda9565a48239.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d60e6bc7ef13c03ca9354fadaa57459.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DeepRetrieval-Hacking-Real-Search-Engines-and-Retrievers-with-Large-Language-Models-via-Reinforcement-Learning"><a href="#DeepRetrieval-Hacking-Real-Search-Engines-and-Retrievers-with-Large-Language-Models-via-Reinforcement-Learning" class="headerlink" title="DeepRetrieval: Hacking Real Search Engines and Retrievers with Large   Language Models via Reinforcement Learning"></a>DeepRetrieval: Hacking Real Search Engines and Retrievers with Large   Language Models via Reinforcement Learning</h2><p><strong>Authors:Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, Jiawei Han</strong></p>
<p>Information retrieval systems are crucial for enabling effective access to large document collections. Recent approaches have leveraged Large Language Models (LLMs) to enhance retrieval performance through query augmentation, but often rely on expensive supervised learning or distillation techniques that require significant computational resources and hand-labeled data. We introduce DeepRetrieval, a reinforcement learning (RL) approach that trains LLMs for query generation through trial and error without supervised data (reference query). Using retrieval metrics as rewards, our system generates queries that maximize retrieval performance. DeepRetrieval outperforms leading methods on literature search with 65.07% (vs. previous SOTA 24.68%) recall for publication search and 63.18% (vs. previous SOTA 32.11%) recall for trial search using real-world search engines. DeepRetrieval also dominates in evidence-seeking retrieval, classic information retrieval and SQL database search. With only 3B parameters, it outperforms industry-leading models like GPT-4o and Claude-3.5-Sonnet on 11&#x2F;13 datasets. These results demonstrate that our RL approach offers a more efficient and effective paradigm for information retrieval. Our data and code are available at: <a target="_blank" rel="noopener" href="https://github.com/pat-jj/DeepRetrieval">https://github.com/pat-jj/DeepRetrieval</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿå¯¹äºæœ‰æ•ˆè®¿é—®å¤§å‹æ–‡æ¡£é›†åˆè‡³å…³é‡è¦ã€‚è¿‘æœŸçš„æ–¹æ³•é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æŸ¥è¯¢æ‰©å……å¢å¼ºæ£€ç´¢æ€§èƒ½ï¼Œä½†é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„ç›‘ç£å­¦ä¹ æˆ–è’¸é¦æŠ€æœ¯ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ‰‹å·¥æ ‡æ³¨æ•°æ®ã€‚æˆ‘ä»¬å¼•å…¥äº†DeepRetrievalï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œç”¨äºé€šè¿‡è¯•é”™è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢ç”Ÿæˆï¼Œæ— éœ€ç›‘ç£æ•°æ®ï¼ˆå‚è€ƒæŸ¥è¯¢ï¼‰ã€‚ä½¿ç”¨æ£€ç´¢æŒ‡æ ‡ä½œä¸ºå¥–åŠ±ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿç”Ÿæˆäº†æœ€å¤§åŒ–æ£€ç´¢æ€§èƒ½çš„æŸ¥è¯¢ã€‚DeepRetrievalåœ¨æ–‡çŒ®æœç´¢æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨å‡ºç‰ˆç‰©æœç´¢çš„å¬å›ç‡ä¸º65.07%ï¼ˆä¸ä¹‹å‰çš„æœ€ä½³æ°´å¹³24.68%ç›¸æ¯”ï¼‰ï¼Œåœ¨è¯•éªŒæœç´¢çš„å¬å›ç‡ä¸º63.18%ï¼ˆä¸ä¹‹å‰çš„æœ€ä½³æ°´å¹³32.11%ç›¸æ¯”ï¼‰ï¼Œä½¿ç”¨ç°å®ä¸–ç•Œæœç´¢å¼•æ“ã€‚DeepRetrievalåœ¨å¾ªè¯æ£€ç´¢ã€ç»å…¸ä¿¡æ¯æ£€ç´¢å’ŒSQLæ•°æ®åº“æœç´¢ä¸­ä¹Ÿå æ®ä¸»å¯¼åœ°ä½ã€‚ä»…éœ€3Bå‚æ•°ï¼Œå®ƒåœ¨13ä¸ªæ•°æ®é›†ä¸­çš„11ä¸ªä¸Šè¶…è¶Šäº†GPT-4oå’ŒClaude-3.5-Sonnetç­‰ä¸šç•Œé¢†å…ˆæ¨¡å‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸ºä¿¡æ¯æ£€ç´¢æä¾›äº†æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„èŒƒå¼ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/pat-jj/DeepRetrieval">https://github.com/pat-jj/DeepRetrieval</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00223v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿâ€”â€”DeepRetrievalã€‚å®ƒé€šè¿‡è¯•é”™è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”ŸæˆæŸ¥è¯¢ï¼Œæ— éœ€ç›‘ç£æ•°æ®ï¼Œä½¿ç”¨æ£€ç´¢æŒ‡æ ‡ä½œä¸ºå¥–åŠ±ï¼Œæœ€å¤§åŒ–æ£€ç´¢æ€§èƒ½ã€‚DeepRetrievalåœ¨æ–‡çŒ®æœç´¢æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒçœŸå®æœç´¢å¼•æ“çš„å¬å›ç‡è¾¾åˆ°äº†æƒŠäººçš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨è¯æ®æ£€ç´¢ã€ç»å…¸ä¿¡æ¯æ£€ç´¢å’ŒSQLæ•°æ®åº“æœç´¢æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ›´æœ‰æ•ˆç‡ä¸”æ›´æœ‰æ•ˆæœã€‚ç›¸å…³æ•°æ®å’Œä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepRetrievalä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆæŸ¥è¯¢ï¼Œæ— éœ€ç›‘ç£æ•°æ®ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡è¯•é”™æ–¹å¼ï¼Œä»¥æ£€ç´¢æŒ‡æ ‡ä½œä¸ºå¥–åŠ±ï¼Œæœ€å¤§åŒ–æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>DeepRetrievalåœ¨æ–‡çŒ®æœç´¢æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒçœŸå®æœç´¢å¼•æ“çš„å¬å›ç‡è¾¾åˆ°äº†å¾ˆé«˜çš„æ°´å¹³ã€‚</li>
<li>å®ƒåœ¨è¯æ®æ£€ç´¢ã€ç»å…¸ä¿¡æ¯æ£€ç´¢å’ŒSQLæ•°æ®åº“æœç´¢æ–¹é¢ä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>ä¸ç°æœ‰çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒDeepRetrievalæ›´åŠ é«˜æ•ˆä¸”æœ‰æ•ˆã€‚</li>
<li>è¯¥ç³»ç»Ÿå·²ç»è¶…è¶Šäº†ç°æœ‰çš„é¡¶å°–æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒClaude-3.5-Sonnetï¼Œåœ¨å¤šæ•°æ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-087588fff6dd43310a53832a6ce85d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f08386bb9b1d93b4fa5f70360a6965d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3516f5a2dd21545233758da6f3b4b7ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bd79e7fb9a3e9d92a196885ab254950.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ChatReID-Open-ended-Interactive-Person-Retrieval-via-Hierarchical-Progressive-Tuning-for-Vision-Language-Models"><a href="#ChatReID-Open-ended-Interactive-Person-Retrieval-via-Hierarchical-Progressive-Tuning-for-Vision-Language-Models" class="headerlink" title="ChatReID: Open-ended Interactive Person Retrieval via Hierarchical   Progressive Tuning for Vision Language Models"></a>ChatReID: Open-ended Interactive Person Retrieval via Hierarchical   Progressive Tuning for Vision Language Models</h2><p><strong>Authors:Ke Niu, Haiyang Yu, Mengyang Zhao, Teng Fu, Siyang Yi, Wei Lu, Bin Li, Xuelin Qian, Xiangyang Xue</strong></p>
<p>Person re-identification (Re-ID) is a crucial task in computer vision, aiming to recognize individuals across non-overlapping camera views. While recent advanced vision-language models (VLMs) excel in logical reasoning and multi-task generalization, their applications in Re-ID tasks remain limited. They either struggle to perform accurate matching based on identity-relevant features or assist image-dominated branches as auxiliary semantics. In this paper, we propose a novel framework ChatReID, that shifts the focus towards a text-side-dominated retrieval paradigm, enabling flexible and interactive re-identification. To integrate the reasoning abilities of language models into Re-ID pipelines, We first present a large-scale instruction dataset, which contains more than 8 million prompts to promote the model fine-tuning. Next. we introduce a hierarchical progressive tuning strategy, which endows Re-ID ability through three stages of tuning, i.e., from person attribute understanding to fine-grained image retrieval and to multi-modal task reasoning. Extensive experiments across ten popular benchmarks demonstrate that ChatReID outperforms existing methods, achieving state-of-the-art performance in all Re-ID tasks. More experiments demonstrate that ChatReID not only has the ability to recognize fine-grained details but also to integrate them into a coherent reasoning process. </p>
<blockquote>
<p>è¡Œäººå†è¯†åˆ«ï¼ˆRe-IDï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«ä¸åŒéé‡å æ‘„åƒå¤´è§†è§’ä¸‹çš„ä¸ªä½“ã€‚å°½ç®¡æœ€è¿‘çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é€»è¾‘æ¨ç†å’Œå¤šä»»åŠ¡æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨Re-IDä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚è¿™äº›æ¨¡å‹è¦ä¹ˆåœ¨åŸºäºèº«ä»½ç›¸å…³ç‰¹å¾çš„å‡†ç¡®åŒ¹é…æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè¦ä¹ˆä½œä¸ºè¾…åŠ©è¯­ä¹‰è¾…åŠ©å›¾åƒä¸»å¯¼çš„åˆ†æ”¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ChatReIDï¼Œå®ƒè½¬å‘ä»¥æ–‡æœ¬ä¸ºä¸»çš„æ£€ç´¢èŒƒå¼ï¼Œå®ç°çµæ´»å’Œäº¤äº’å¼çš„å†è¯†åˆ«ã€‚ä¸ºäº†å°†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ•´åˆåˆ°Re-IDæµç¨‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæä¾›äº†ä¸€ä¸ªå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡800ä¸‡ä¸ªæç¤ºæ¥ä¿ƒè¿›æ¨¡å‹çš„å¾®è°ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ†å±‚æ¸è¿›çš„è°ƒè¯•ç­–ç•¥ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªé˜¶æ®µï¼ˆå³ä»ç†è§£äººç‰©å±æ€§åˆ°ç²¾ç»†å›¾åƒæ£€ç´¢å†åˆ°å¤šæ¨¡æ€ä»»åŠ¡æ¨ç†ï¼‰èµ‹äºˆRe-IDèƒ½åŠ›ã€‚åœ¨åä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒChatReIDä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ‰€æœ‰Re-IDä»»åŠ¡ä¸­å‡è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚æ›´å¤šçš„å®éªŒè¡¨æ˜ï¼ŒChatReIDä¸ä»…å…·æœ‰è¯†åˆ«ç»†å¾®ç»†èŠ‚çš„èƒ½åŠ›ï¼Œè¿˜èƒ½å°†å®ƒä»¬æ•´åˆåˆ°ä¸€ä¸ªè¿è´¯çš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19958v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºChatReIDçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å°†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›èå…¥è¡Œäººå†è¯†åˆ«ï¼ˆRe-IDï¼‰ä»»åŠ¡ä¸­ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†å’Œåˆ†å±‚æ¸è¿›è°ƒå‚ç­–ç•¥ï¼ŒChatReIDå®ç°äº†ä»ç†è§£äººç‰©å±æ€§åˆ°ç²¾ç»†å›¾åƒæ£€ç´¢å’Œå¤šæ¨¡æ€ä»»åŠ¡æ¨ç†çš„ä¸‰ä¸ªé˜¶æ®µè°ƒå‚ã€‚å®éªŒè¯æ˜ï¼ŒChatReIDåœ¨å¤šä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„çŠ¶æ€ï¼Œå±•ç°å‡ºç²¾ç»†è¯†åˆ«ä¸æ•´åˆæ¨ç†çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatReIDæ¡†æ¶å°†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›èå…¥è¡Œäººå†è¯†åˆ«ä»»åŠ¡ä¸­ã€‚</li>
<li>æ„å»ºå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡800ä¸‡æç¤ºï¼Œä»¥ä¿ƒè¿›æ¨¡å‹å¾®è°ƒã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚æ¸è¿›è°ƒå‚ç­–ç•¥ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è°ƒå‚å®ç°è¡Œäººå†è¯†åˆ«ã€‚</li>
<li>ChatReIDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ChatReIDèƒ½å¤Ÿè¯†åˆ«ç²¾ç»†ç»†èŠ‚ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°è¿è´¯çš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚</li>
<li>ChatReIDå®ç°äº†çµæ´»çš„äº¤äº’å¼è¡Œäººå†è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9282c75a979a0d53aafa79ae9220a0ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b4d0f66d8ab5ef0735be00086b29143.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89ea51c9924d6ce4f0bca5eb017282da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9905472017d8aad549de5109e8dd65f0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LR-2-Bench-Evaluating-Long-chain-Reflective-Reasoning-Capabilities-of-Large-Language-Models-via-Constraint-Satisfaction-Problems"><a href="#LR-2-Bench-Evaluating-Long-chain-Reflective-Reasoning-Capabilities-of-Large-Language-Models-via-Constraint-Satisfaction-Problems" class="headerlink" title="LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of   Large Language Models via Constraint Satisfaction Problems"></a>LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of   Large Language Models via Constraint Satisfaction Problems</h2><p><strong>Authors:Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang</strong></p>
<p>Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/UltraRonin/LR2Bench">https://huggingface.co/spaces/UltraRonin/LR2Bench</a> </p>
<blockquote>
<p>æœ€è¿‘o1ç±»æ¨¡å‹çš„è¿›å±•å¤§å¤§æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿé€šè¿‡å‡è®¾ã€å›æº¯å’Œè‡ªæˆ‘å®Œå–„ç­‰åæ€èƒ½åŠ›æ¥å¤„ç†æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é€‚å½“çš„åŸºå‡†æµ‹è¯•ï¼Œæœ‰æ•ˆè¯„ä¼°è¿™æ ·çš„åæ€èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†LR$^2$Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMçš„é•¿é“¾åæ€æ¨ç†èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚LR$^2$BenchåŒ…å«6ä¸ªçº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰çš„850ä¸ªæ ·æœ¬ï¼Œåœ¨è¿™äº›é—®é¢˜ä¸­ï¼Œåæ€æ¨ç†å¯¹äºå¾—å‡ºæ»¡è¶³æ‰€æœ‰ç»™å®šçº¦æŸçš„è§£å†³æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚æ¯ç§ä»»åŠ¡éƒ½ä¸“æ³¨äºä¸åŒçš„çº¦æŸæ¨¡å¼ï¼Œå¦‚çŸ¥è¯†å‹ã€é€»è¾‘å‹å’Œç©ºé—´å‹çº¦æŸï¼Œä»è€Œå…¨é¢è¯„ä¼°å„ç§é—®é¢˜è§£å†³åœºæ™¯ã€‚æˆ‘ä»¬å¯¹ä¼ ç»Ÿæ¨¡å‹å’Œo1ç±»æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨ç†ä¸“ç”¨æ¨¡å‹ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1-previewï¼Œåœ¨LR$^2$Benchçš„ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°æŒ£æ‰ï¼Œå¹³å‡ç²¾ç¡®åŒ¹é…ç‡ä»…ä¸º20.0%å’Œ23.6%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å½“å‰LLMçš„åæ€æ¨ç†èƒ½åŠ›ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•çš„æ’è¡Œæ¦œå¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/UltraRonin/LR2Bench">https://huggingface.co/spaces/UltraRonin/LR2Bench</a>ä¸ŠæŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17848v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°è¿›å±•çš„o1ç±»æ¨¡å‹æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶å…·å¤‡åº”å¯¹å¤æ‚ä»»åŠ¡çš„åæ€èƒ½åŠ›ï¼Œå¦‚å‡è®¾ã€å›æº¯å’Œè‡ªæˆ‘å®Œå–„ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é€‚å½“çš„åŸºå‡†æµ‹è¯•ï¼Œæœ‰æ•ˆè¯„ä¼°è¿™äº›åæ€èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LR$^2$BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„é•¿æœŸåæ€æ¨ç†èƒ½åŠ›ã€‚LR$^2$BenchåŒ…å«æ¶‰åŠå…­ç§çº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰çš„850ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­åœ¨è§£å†³æ‰€æœ‰ç»™å®šçº¦æŸçš„è§£å†³æ–¹æ¡ˆä¸­åæ€æ¨ç†è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å…¨é¢è¯„ä¼°äº†ä¼ ç»Ÿæ¨¡å‹å’Œo1ç±»æ¨¡å‹ï¼Œå‘ç°æœ€å…ˆè¿›çš„æ¨ç†ç‰¹å®šæ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI o1-previewåœ¨LR$^2$Benchçš„ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ï¼Œå¹³å‡ç²¾ç¡®åŒ¹é…å¾—åˆ†ä»…ä¸º20.0%å’Œ23.6%ã€‚è¿™çªæ˜¾äº†å½“å‰LLMåœ¨åæ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æå‡ç©ºé—´ã€‚LR$^2$Benchæ’è¡Œæ¦œå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/UltraRonin/LR2Bench%E3%80%82">https://huggingface.co/spaces/UltraRonin/LR2Benchã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>o1ç±»æ¨¡å‹å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå…·å¤‡å‡è®¾ã€å›æº¯å’Œè‡ªæˆ‘å®Œå–„ç­‰åæ€èƒ½åŠ›ã€‚</li>
<li>ç¼ºä¹é€‚å½“çš„åŸºå‡†æµ‹è¯•æ˜¯è¯„ä¼°LLMåæ€èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥LR$^2$BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„é•¿æœŸåæ€æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«æ¶‰åŠå…­ç§çº¦æŸæ»¡è¶³é—®é¢˜çš„850ä¸ªæ ·æœ¬ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨ç†ç‰¹å®šæ¨¡å‹åœ¨LR$^2$Benchä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹³å‡ç²¾ç¡®åŒ¹é…å¾—åˆ†è¾ƒä½ã€‚</li>
<li>å½“å‰LLMåœ¨åæ€æ¨ç†èƒ½åŠ›æ–¹é¢è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1440b0fe4641cb51941e897608cf8d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0eca2b2e36660956ce9c374ea9a7529c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b4e86e3ee08abc21b40428426183285.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2e7ed860584517d8e1101a4bd6cb18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d75f2ac31f210d1de6f07e06343ec3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Class-Dependent-Perturbation-Effects-in-Evaluating-Time-Series-Attributions"><a href="#Class-Dependent-Perturbation-Effects-in-Evaluating-Time-Series-Attributions" class="headerlink" title="Class-Dependent Perturbation Effects in Evaluating Time Series   Attributions"></a>Class-Dependent Perturbation Effects in Evaluating Time Series   Attributions</h2><p><strong>Authors:Gregor Baer, Isel Grau, Chao Zhang, Pieter Van Gorp</strong></p>
<p>As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contribute the most to a modelâ€™s prediction, with their evaluation typically relying on perturbation-based metrics. Through systematic empirical analysis across multiple datasets, model architectures, and perturbation strategies, we reveal previously overlooked class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions, offering particular value for class-imbalanced datasets. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common. </p>
<blockquote>
<p>éšç€æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åº”ç”¨ä¸­çš„æ™®åŠï¼Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•å¯¹äºç†è§£å…¶é¢„æµ‹ç»“æœè‡³å…³é‡è¦ã€‚åœ¨XAIä¸­ï¼Œç‰¹å¾å½’å› æ–¹æ³•æ—¨åœ¨è¯†åˆ«å“ªäº›è¾“å…¥ç‰¹å¾å¯¹æ¨¡å‹çš„é¢„æµ‹è´¡çŒ®æœ€å¤§ï¼Œå…¶è¯„ä¼°é€šå¸¸ä¾èµ–äºåŸºäºå¾®æ‰°çš„åº¦é‡æŒ‡æ ‡ã€‚é€šè¿‡å¯¹å¤šä¸ªæ•°æ®é›†ã€æ¨¡å‹æ¶æ„å’Œå¾®æ‰°ç­–ç•¥çš„ç³»ç»Ÿå®è¯åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›æŒ‡æ ‡ä¸­ä»¥å‰è¢«å¿½è§†çš„ç±»ç›¸å…³æ•ˆåº”ï¼šå®ƒä»¬åœ¨å„ç±»ä¹‹é—´çš„æœ‰æ•ˆæ€§å„ä¸ç›¸åŒï¼Œå¯¹æŸäº›ç±»æœ‰æ•ˆï¼Œä½†å¯¹å…¶ä»–ç±»ä¸å¤ªæ•æ„Ÿã€‚å°¤å…¶æˆ‘ä»¬å‘ç°æœ€æœ‰æ•ˆçš„å¾®æ‰°ç­–ç•¥å¾€å¾€æ˜¾ç¤ºå‡ºæœ€æ˜æ˜¾çš„ç±»é—´å·®å¼‚ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™äº›å½±å“æºäºåˆ†ç±»å™¨çš„å­¦ä¹ åè§ï¼Œè¿™è¡¨æ˜åŸºäºå¾®æ‰°çš„è¯„ä¼°å¯èƒ½åæ˜ äº†ç‰¹å®šæ¨¡å‹çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯å†…åœ¨çš„å½’å› è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¸¦æœ‰ç±»åˆ«æ„ŸçŸ¥æƒ©ç½šé¡¹çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥å¸®åŠ©è¯„ä¼°å’Œè€ƒè™‘è¿™äº›å½±å“åœ¨ç‰¹å¾å½’å› è¯„ä¼°ä¸­çš„ä½œç”¨ï¼Œå¯¹äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†ç‰¹åˆ«æœ‰ä»·å€¼ã€‚å°½ç®¡æˆ‘ä»¬çš„åˆ†æä¾§é‡äºæ—¶é—´åºåˆ—åˆ†ç±»ï¼Œä½†è¿™äº›ç±»åˆ«ç›¸å…³çš„æ•ˆåº”å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç»“æ„åŒ–æ•°æ®é¢†åŸŸï¼Œå…¶ä¸­åŸºäºå¾®æ‰°çš„è¯„ä¼°å¾ˆå¸¸è§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17022v2">PDF</a> Accepted at The World Conference on eXplainable Artificial   Intelligence (XAI-2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åº”ç”¨ä¸­çš„æ™®åŠï¼Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•å¯¹äºç†è§£æ¨¡å‹é¢„æµ‹è‡³å…³é‡è¦ã€‚ç‰¹å¾å½’å› æ–¹æ³•æ—¨åœ¨è¯†åˆ«å¯¹æ¨¡å‹é¢„æµ‹è´¡çŒ®æœ€å¤§çš„è¾“å…¥ç‰¹å¾ï¼Œå…¶è¯„ä¼°é€šå¸¸ä¾èµ–äºåŸºäºæ‰°åŠ¨çš„æŒ‡æ ‡ã€‚é€šè¿‡å¯¹å¤šä¸ªæ•°æ®é›†ã€æ¨¡å‹æ¶æ„å’Œæ‰°åŠ¨ç­–ç•¥çš„ç³»ç»Ÿå®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¹‹å‰è¢«å¿½è§†çš„ç±»ç›¸å…³æ•ˆåº”ï¼šè¿™äº›æŒ‡æ ‡åœ¨ä¸åŒç±»åˆ«ä¸Šçš„æœ‰æ•ˆæ€§å­˜åœ¨å·®å¼‚ï¼Œå¯¹æŸäº›ç±»åˆ«æ•ˆæœå¥½ï¼Œå¯¹å¦ä¸€äº›ç±»åˆ«åˆ™ä¸å¤ªæ•æ„Ÿã€‚æå‡ºä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå¸¦æœ‰ç±»æ„ŸçŸ¥æƒ©ç½šé¡¹ï¼Œä»¥å¸®åŠ©è¯„ä¼°å’Œè€ƒè™‘è¿™äº›æ•ˆåº”åœ¨è¯„ä¼°ç‰¹å¾å½’å› æ—¶çš„ä»·å€¼ï¼Œå¯¹ç±»ä¸å¹³è¡¡æ•°æ®é›†ç‰¹åˆ«æœ‰ä»·å€¼ã€‚è™½ç„¶åˆ†æé‡ç‚¹åœ¨æ—¶åºåˆ†ç±»ä¸Šï¼Œä½†è¿™äº›ç±»ç›¸å…³æ•ˆåº”å¯èƒ½ä¹Ÿé€‚ç”¨äºå…¶ä»–ç»“æ„åŒ–æ•°æ®é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨æ—¶é—´åºåˆ—åº”ç”¨ä¸­ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹ç†è§£ä¾èµ–äºå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•ã€‚</li>
<li>ç‰¹å¾å½’å› æ–¹æ³•æ—¨åœ¨è¯†åˆ«å½±å“æ¨¡å‹é¢„æµ‹çš„å…³é”®è¾“å…¥ç‰¹å¾ã€‚</li>
<li>åŸºäºæ‰°åŠ¨çš„è¯„ä¼°æŒ‡æ ‡åœ¨ç±»é—´å­˜åœ¨æœ‰æ•ˆæ€§å·®å¼‚ï¼Œå¯¹æŸäº›ç±»åˆ«çš„é¢„æµ‹æ•ˆæœå¥½ï¼Œå¯¹å¦ä¸€äº›åˆ™ä¸æ•æ„Ÿã€‚</li>
<li>æœ€æœ‰æ•ˆçš„æ‰°åŠ¨ç­–ç•¥å¾€å¾€åœ¨ç±»é—´å·®å¼‚æœ€å¤§ã€‚</li>
<li>è¿™äº›ç±»é—´å·®å¼‚æºäºåˆ†ç±»å™¨çš„å­¦ä¹ åè§ï¼Œæ‰°åŠ¨è¯„ä¼°å¯èƒ½åæ˜ ç‰¹å®šæ¨¡å‹è¡Œä¸ºè€Œéå†…åœ¨å½’å› è´¨é‡ã€‚</li>
<li>æå‡ºä¸€ä¸ªå¸¦æœ‰ç±»æ„ŸçŸ¥æƒ©ç½šé¡¹çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å’Œè€ƒè™‘ç±»é—´æ•ˆåº”åœ¨ç‰¹å¾å½’å› ä¸­çš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59072010efe4ab1892cc43e27345235c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c497c3867bdf351b9028783830846a4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81db10a07cd24efcdcf975b22cfbc8e2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-06/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-06/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7e6e8eb67bffed7f4a3fa4775183fbef.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-06  Tomography of Quantum States from Structured Measurements via   quantum-aware transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1babe486df58dece80aec746b4fd24d5.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  OmniTalker Real-Time Text-Driven Talking Head Generation with   In-Context Audio-Visual Style Replication
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
