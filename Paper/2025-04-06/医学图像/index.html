<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-06  MedConv Convolutions Beat Transformers on Long-Tailed Bone Density   Prediction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-26613c19e31caa9022529215f236cdb2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-06-æ›´æ–°"><a href="#2025-04-06-æ›´æ–°" class="headerlink" title="2025-04-06 æ›´æ–°"></a>2025-04-06 æ›´æ–°</h1><h2 id="MedConv-Convolutions-Beat-Transformers-on-Long-Tailed-Bone-Density-Prediction"><a href="#MedConv-Convolutions-Beat-Transformers-on-Long-Tailed-Bone-Density-Prediction" class="headerlink" title="MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density   Prediction"></a>MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density   Prediction</h2><p><strong>Authors:Xuyin Qi, Zeyu Zhang, Huazhan Zheng, Mingxi Chen, Numan Kutaiba, Ruth Lim, Cherie Chiang, Zi En Tham, Xuan Ren, Wenxin Zhang, Lei Zhang, Hao Zhang, Wenbing Lv, Guangzhen Yao, Renda Han, Kangsheng Wang, Mingyuan Li, Hongtao Mao, Yu Li, Zhibin Liao, Yang Zhao, Minh-Son To</strong></p>
<p>Bone density prediction via CT scans to estimate T-scores is crucial, providing a more precise assessment of bone health compared to traditional methods like X-ray bone density tests, which lack spatial resolution and the ability to detect localized changes. However, CT-based prediction faces two major challenges: the high computational complexity of transformer-based architectures, which limits their deployment in portable and clinical settings, and the imbalanced, long-tailed distribution of real-world hospital data that skews predictions. To address these issues, we introduce MedConv, a convolutional model for bone density prediction that outperforms transformer models with lower computational demands. We also adapt Bal-CE loss and post-hoc logit adjustment to improve class balance. Extensive experiments on our AustinSpine dataset shows that our approach achieves up to 21% improvement in accuracy and 20% in ROC AUC over previous state-of-the-art methods. </p>
<blockquote>
<p>é€šè¿‡CTæ‰«æé¢„æµ‹éª¨å¯†åº¦ä»¥ä¼°ç®—Tå€¼éå¸¸é‡è¦ã€‚ä¸ä¼ ç»Ÿçš„Xå°„çº¿éª¨å¯†åº¦æµ‹è¯•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†æ›´ç²¾ç¡®çš„éª¨éª¼å¥åº·è¯„ä¼°ï¼Œå› ä¸ºä¼ ç»Ÿçš„Xå°„çº¿æµ‹è¯•ç¼ºä¹ç©ºé—´åˆ†è¾¨ç‡å’Œæ£€æµ‹å±€éƒ¨å˜åŒ–çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºCTçš„é¢„æµ‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯åŸºäºå˜å‹å™¨çš„æ¶æ„å…·æœ‰æé«˜çš„è®¡ç®—å¤æ‚æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ä¾¿æºå’Œä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ï¼›äºŒæ˜¯ç°å®ä¸–ç•ŒåŒ»é™¢æ•°æ®å­˜åœ¨ä¸å¹³è¡¡çš„é•¿å°¾åˆ†å¸ƒï¼Œå¯¼è‡´é¢„æµ‹ç»“æœå‡ºç°åå·®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedConvï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºéª¨å¯†åº¦é¢„æµ‹çš„å·ç§¯æ¨¡å‹ï¼Œå®ƒä»¥æ›´ä½çš„è®¡ç®—éœ€æ±‚è¶…è¶Šäº†å˜å‹å™¨æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜é€‚åº”äº†Bal-CEæŸå¤±å’Œäº‹åé€»è¾‘è°ƒæ•´æ¥æ”¹å–„ç±»åˆ«å¹³è¡¡ã€‚åœ¨æˆ‘ä»¬AustinSpineæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§å’ŒROC AUCæ–¹é¢åˆ†åˆ«æé«˜äº†é«˜è¾¾21%å’Œ20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00631v2">PDF</a> Accepted to IJCNN 2025</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨CTæ‰«æè¿›è¡Œéª¨å¯†åº¦é¢„æµ‹ä»¥ä¼°ç®—Tå€¼ï¼Œå¯¹äºè¯„ä¼°éª¨å¥åº·è‡³å…³é‡è¦ï¼Œç›¸è¾ƒäºç¼ºä¹ç©ºé—´åˆ†è¾¨ç‡å’Œæ£€æµ‹å±€éƒ¨å˜åŒ–èƒ½åŠ›çš„ä¼ ç»ŸXå°„çº¿éª¨å¯†åº¦æµ‹è¯•æ–¹æ³•æ›´ä¸ºç²¾ç¡®ã€‚ç„¶è€Œï¼ŒCTé¢„æµ‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŸºäºå˜å‹å™¨æ¶æ„çš„é«˜è®¡ç®—å¤æ‚åº¦å’Œç°å®ä¸–ç•ŒåŒ»é™¢æ•°æ®ä¸å¹³è¡¡ã€é•¿å°¾åˆ†å¸ƒå¯¼è‡´çš„é¢„æµ‹åå·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºMedConvæ¨¡å‹ï¼Œå³ç”¨äºéª¨å¯†åº¦é¢„æµ‹çš„å·ç§¯æ¨¡å‹ï¼Œåœ¨é™ä½è®¡ç®—éœ€æ±‚çš„åŒæ—¶è¶…è¶Šå˜å‹å™¨æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é‡‡ç”¨Bal-CEæŸå¤±å’Œäº‹åé€»è¾‘è°ƒæ•´æ”¹å–„ç±»åˆ«å¹³è¡¡é—®é¢˜ã€‚åœ¨AustinSpineæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾ƒä»¥å¾€å…ˆè¿›æ–¹æ³•å‡†ç¡®åº¦å’ŒROC AUCå€¼åˆ†åˆ«æé«˜äº†é«˜è¾¾21%å’Œ20%ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CTæ‰«æç”¨äºéª¨å¯†åº¦é¢„æµ‹æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ç²¾ç¡®ã€‚</li>
<li>CTé¢„æµ‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šé«˜è®¡ç®—å¤æ‚åº¦å’Œæ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>MedConvæ¨¡å‹æ˜¯ä¸€ç§ç”¨äºéª¨å¯†åº¦é¢„æµ‹çš„å·ç§¯æ¨¡å‹ï¼Œæ€§èƒ½ä¼˜äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ä¸”è®¡ç®—éœ€æ±‚è¾ƒä½ã€‚</li>
<li>é‡‡ç”¨Bal-CEæŸå¤±å’Œäº‹åé€»è¾‘è°ƒæ•´æ”¹å–„ç±»åˆ«å¹³è¡¡é—®é¢˜ã€‚</li>
<li>åœ¨AustinSpineæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•è¾ƒä»¥å¾€æŠ€æœ¯æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f41e69932ac4d891e6322b548ceeeada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84faf4398dda036a0685c5b7798595a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcb606b04a8c7c949afadc0480909cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-104e7fe486f79f82b61cf94ac52f83fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0e75e6dc9c8447026065b4d7a0b2815.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18caf63b2943163ad363fc3391318e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc6f0e0fa207de6af3f2ea0a071571e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a45c8edb535098b9c7cf890031463f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74e3210a5c5e30b46101a66f85bc0d3c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CAD-Confidence-Aware-Adaptive-Displacement-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#CAD-Confidence-Aware-Adaptive-Displacement-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical   Image Segmentation"></a>CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical   Image Segmentation</h2><p><strong>Authors:Wenbo Xiao, Zhihao Xu, Guiping Liang, Yangjun Deng, Yi Xiao</strong></p>
<p>Semi-supervised medical image segmentation aims to leverage minimal expert annotations, yet remains confronted by challenges in maintaining high-quality consistency learning. Excessive perturbations can degrade alignment and hinder precise decision boundaries, especially in regions with uncertain predictions. In this paper, we introduce Confidence-Aware Adaptive Displacement (CAD), a framework that selectively identifies and replaces the largest low-confidence regions with high-confidence patches. By dynamically adjusting both the maximum allowable replacement size and the confidence threshold throughout training, CAD progressively refines the segmentation quality without overwhelming the learning process. Experimental results on public medical datasets demonstrate that CAD effectively enhances segmentation quality, establishing new state-of-the-art accuracy in this field. The source code will be released after the paper is published. </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨åˆ©ç”¨æœ€å°‘çš„ä¸“å®¶æ³¨é‡Šï¼Œä½†é¢ä¸´ç€å¦‚ä½•ä¿æŒé«˜è´¨é‡ä¸€è‡´æ€§å­¦ä¹ çš„æŒ‘æˆ˜ã€‚è¿‡åº¦çš„æ‰°åŠ¨å¯èƒ½ä¼šç ´åå¯¹é½å¹¶é˜»ç¢ç²¾ç¡®çš„å†³ç­–è¾¹ç•Œï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹ä¸ç¡®å®šçš„åŒºåŸŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¿¡å¿ƒæ„ŸçŸ¥è‡ªé€‚åº”ä½ç§»ï¼ˆCADï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°è¯†åˆ«å’Œç”¨é«˜ä¿¡å¿ƒè¡¥ä¸æ›¿æ¢æœ€å¤§ä½ä¿¡å¿ƒåŒºåŸŸã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æœ€å¤§å…è®¸æ›¿æ¢å¤§å°å’Œç½®ä¿¡é˜ˆå€¼ï¼ŒCADèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥æ”¹è¿›åˆ†å‰²è´¨é‡ï¼Œè€Œä¸ä¼šä½¿å­¦ä¹ è¿‡ç¨‹è¿‡äºå¤æ‚ã€‚åœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCADèƒ½æœ‰æ•ˆæé«˜åˆ†å‰²è´¨é‡ï¼Œåœ¨è¯¥é¢†åŸŸå»ºç«‹äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„ç²¾åº¦ã€‚è®ºæ–‡å‘è¡¨åå°†å…¬å¸ƒæºä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00536v2">PDF</a> 9 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŠç›‘ç£å­¦ä¹ æ—¨åœ¨åˆ©ç”¨å°‘é‡çš„ä¸“å®¶æ ‡æ³¨æ•°æ®ï¼Œä½†é¢ä¸´ç€å¦‚ä½•ä¿æŒé«˜è´¨é‡ä¸€è‡´æ€§å­¦ä¹ çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºConfidence-Aware Adaptive Displacementï¼ˆCADï¼‰çš„æ¡†æ¶ï¼Œå®ƒèƒ½è‡ªåŠ¨è¯†åˆ«å¹¶æ›¿æ¢æœ€å¤§ä½ç½®ä¿¡åº¦åŒºåŸŸçš„é«˜ç½®ä¿¡åº¦è¡¥ä¸ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æœ€å¤§å…è®¸æ›¿æ¢å¤§å°å’Œç½®ä¿¡é˜ˆå€¼ï¼ŒCADèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥æé«˜åˆ†å‰²è´¨é‡ï¼Œè€Œä¸ä¼šä½¿å­¦ä¹ è¿‡ç¨‹è¿‡äºå¤æ‚ã€‚åœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCADèƒ½æœ‰æ•ˆæé«˜åˆ†å‰²è´¨é‡ï¼Œè¾¾åˆ°è¯¥é¢†åŸŸçš„æ–°æ°´å¹³ã€‚æºä»£ç å°†åœ¨è®ºæ–‡å‘è¡¨åå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ä¿æŒé«˜è´¨é‡ä¸€è‡´æ€§å­¦ä¹ çš„æŒ‘æˆ˜ã€‚</li>
<li>CADæ¡†æ¶é€šè¿‡è¯†åˆ«å¹¶æ›¿æ¢ä½ç½®ä¿¡åº¦åŒºåŸŸï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†å‰²è´¨é‡ã€‚</li>
<li>CADèƒ½åŠ¨æ€è°ƒæ•´æœ€å¤§å…è®¸æ›¿æ¢å¤§å°å’Œç½®ä¿¡é˜ˆå€¼ï¼Œå®ç°ç²¾ç»†çš„åˆ†å‰²è´¨é‡æ”¹è¿›ã€‚</li>
<li>CADåœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œè¾¾åˆ°è¯¥é¢†åŸŸçš„æ–°æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†è‡ªé€‚åº”å’Œçµæ´»æ€§åœ¨è§£å†³ä¸ç¡®å®šæ€§é—®é¢˜ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥CADæ¡†æ¶ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ç§è§£å†³åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æŒ‘æˆ˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12d3f21d621463bda073bed44374b022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16d3a740a54342129db0bb5fb3d6dc25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a53918e8c7d51f99e6b8db6b987852b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a1765d4a15e12f81f1ae3e9051ee39c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BIOMEDICA-An-Open-Biomedical-Image-Caption-Archive-Dataset-and-Vision-Language-Models-Derived-from-Scientific-Literature"><a href="#BIOMEDICA-An-Open-Biomedical-Image-Caption-Archive-Dataset-and-Vision-Language-Models-Derived-from-Scientific-Literature" class="headerlink" title="BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and   Vision-Language Models Derived from Scientific Literature"></a>BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and   Vision-Language Models Derived from Scientific Literature</h2><p><strong>Authors:Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, Serena Yeung-Levy</strong></p>
<p>The development of vision-language models (VLMs) is driven by large-scale and diverse multimodal datasets. However, progress toward generalist biomedical VLMs is limited by the lack of annotated, publicly accessible datasets across biology and medicine. Existing efforts are restricted to narrow domains, missing the full diversity of biomedical knowledge encoded in scientific literature. To address this gap, we introduce BIOMEDICA, a scalable, open-source framework to extract, annotate, and serialize the entirety of the PubMed Central Open Access subset into an easy-to-use, publicly accessible dataset. Our framework produces a comprehensive archive with over 24 million unique image-text pairs from over 6 million articles. Metadata and expert-guided annotations are also provided. We demonstrate the utility and accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style models continuously pre-trained on the BIOMEDICA dataset via streaming, eliminating the need to download 27 TB of data locally. On average, our models achieve state-of-the-art performance across 40 tasks - spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology - excelling in zero-shot classification with a 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively), and stronger image-text retrieval, all while using 10x less compute. To foster reproducibility and collaboration, we release our codebase and dataset for the broader research community. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•æ˜¯ç”±å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†é©±åŠ¨çš„ã€‚ç„¶è€Œï¼Œé€šç”¨ç”Ÿç‰©åŒ»å­¦VLMsçš„è¿›å±•å—é™äºç”Ÿç‰©å­¦å’ŒåŒ»å­¦é¢†åŸŸç¼ºä¹å…¬å¼€è®¿é—®çš„æ³¨é‡Šæ•°æ®é›†ã€‚ç°æœ‰çš„åŠªåŠ›ä»…é™äºç‹­çª„çš„é¢†åŸŸï¼Œé”™è¿‡äº†ç§‘å­¦æ–‡çŒ®ä¸­ç¼–ç çš„å®Œæ•´ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†BIOMEDICAï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æ”¾æºä»£ç æ¡†æ¶ï¼Œç”¨äºæå–ã€æ³¨é‡Šå’Œåºåˆ—åŒ–PubMed Central Open Accesså­é›†çš„å®Œæ•´å†…å®¹ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ˜“äºä½¿ç”¨ã€å¯å…¬å¼€è®¿é—®çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¡†æ¶äº§ç”Ÿäº†åŒ…å«è¶…è¿‡24ä¸‡ä¸ªå”¯ä¸€å›¾åƒæ–‡æœ¬å¯¹çš„ç»¼åˆæ¡£æ¡ˆï¼Œæ¶µç›–è¶…è¿‡6ä¸‡ç¯‡æ–‡ç« ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…ƒæ•°æ®å’Œä¸“ä¸šæŒ‡å¯¼çš„æ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡å‘å¸ƒBMCA-CLIPæ¥è¯æ˜æˆ‘ä»¬èµ„æºçš„å®ç”¨æ€§å’Œå¯è®¿é—®æ€§ï¼Œè¿™æ˜¯ä¸€å¥—CLIPé£æ ¼çš„æ¨¡å‹ï¼Œåœ¨BIOMEDICAæ•°æ®é›†ä¸Šè¿ç»­è¿›è¡Œé¢„è®­ç»ƒæµå¤„ç†ï¼Œæ— éœ€ä¸‹è½½æœ¬åœ°é«˜è¾¾27TBçš„æ•°æ®ã€‚å¹³å‡è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¶µç›–ç—…ç†å­¦ã€æ”¾å°„å­¦ã€çœ¼ç§‘ã€çš®è‚¤ç§‘ã€å¤–ç§‘ã€åˆ†å­ç”Ÿç‰©å­¦ã€å¯„ç”Ÿè™«å­¦å’Œç»†èƒç”Ÿç‰©å­¦ç­‰é¢†åŸŸçš„å››åå¤šé¡¹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡æ”¹è¿›ç‡ä¸º6.56%ï¼ˆçš®è‚¤ç§‘å’Œçœ¼ç§‘é«˜è¾¾29.8%å’Œ17.5%ï¼‰ï¼Œå¹¶ä¸”åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢æ–¹é¢æ›´å¼ºï¼ŒåŒæ—¶ä½¿ç”¨è®¡ç®—æœºçš„è®¡ç®—èƒ½åŠ›å‡å°‘åå€ã€‚ä¸ºäº†ä¿ƒè¿›å¯å¤åˆ¶æ€§å’Œåˆä½œï¼Œæˆ‘ä»¬å‘æ›´å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07171v3">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ä¸ªåä¸ºBIOMEDICAçš„å¼€æºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä¸­è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ã€‚é€šè¿‡æå–ã€æ ‡æ³¨å’Œåºåˆ—åŒ–PubMed Central Open Accesså­é›†çš„å…¨éƒ¨å†…å®¹ï¼Œè¯¥æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡2.4äº¿ä¸ªç‹¬ç‰¹çš„å›¾åƒæ–‡æœ¬å¯¹çš„ç»¼åˆæ¡£æ¡ˆåº“ã€‚åŒæ—¶ï¼Œé€šè¿‡è¿ç»­è®­ç»ƒåŸºäºBIOMEDICAæ•°æ®é›†çš„CLIPé£æ ¼æ¨¡å‹BMCA-CLIPï¼Œè¯æ˜äº†å…¶èµ„æºçš„å®ç”¨æ€§å’Œå¯åŠæ€§ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç—…ç†å­¦ã€æ”¾å°„å­¦ã€çœ¼ç§‘ç­‰ã€‚ä¸ºäº†ä¿ƒè¿›å†ç°æ€§å’Œåˆä½œï¼Œç ”ç©¶å›¢é˜Ÿå°†å…¬å¼€æ•°æ®é›†å’Œä»£ç åº“ä¾›æ›´å¹¿æ³›çš„ç ”ç©¶ç¾¤ä½“ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BIOMEDICAæ¡†æ¶è§£å†³äº†ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>BIOMEDICAæ¡†æ¶æå–å¹¶æ ‡æ³¨äº†PubMed Central Open Accesså­é›†çš„å…¨éƒ¨å†…å®¹ï¼Œå»ºç«‹äº†åŒ…å«è¶…è¿‡2.4äº¿ä¸ªå›¾åƒæ–‡æœ¬å¯¹çš„ç»¼åˆæ¡£æ¡ˆåº“ã€‚</li>
<li>BMCA-CLIPæ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªåœ¨BIOMEDICAæ•°æ®é›†ä¸Šè¿ç»­è®­ç»ƒçš„CLIPé£æ ¼æ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½å¤§é‡æ•°æ®å³å¯ä½¿ç”¨ã€‚</li>
<li>BMCA-CLIPæ¨¡å‹åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†6.56%ï¼Œåœ¨çš®è‚¤ç§‘å’Œçœ¼ç§‘é¢†åŸŸåˆ†åˆ«æé«˜äº†é«˜è¾¾29.8%å’Œ17.5%ã€‚</li>
<li>BMCA-CLIPæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒæ–‡æœ¬æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>BIOMEDICAæ¡†æ¶å’ŒBMCA-CLIPæ¨¡å‹çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œä¾›æ›´å¹¿æ³›çš„ç ”ç©¶ç¾¤ä½“ä½¿ç”¨ã€‚</li>
<li>BIOMEDICAæ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒæ›´å¤šçš„ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ•°æ®é›†æ•´åˆå’Œæ ‡æ³¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00afc5065334b6a79d406c238c5c5dea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd58bea75fcd892f034ddaf551be5dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db83b4501bc3ff79af30cdaabd7e61a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4d7e437bd88b6653b7e1d086aed47d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04f704bf34093b820e3672a0a7126146.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HCMA-UNet-A-Hybrid-CNN-Mamba-UNet-with-Axial-Self-Attention-for-Efficient-Breast-Cancer-Segmentation"><a href="#HCMA-UNet-A-Hybrid-CNN-Mamba-UNet-with-Axial-Self-Attention-for-Efficient-Breast-Cancer-Segmentation" class="headerlink" title="HCMA-UNet: A Hybrid CNN-Mamba UNet with Axial Self-Attention for   Efficient Breast Cancer Segmentation"></a>HCMA-UNet: A Hybrid CNN-Mamba UNet with Axial Self-Attention for   Efficient Breast Cancer Segmentation</h2><p><strong>Authors:Haoxuan Li, Wei song, Peiwu Qin, Xi Yuan, Zhenglin Chen</strong></p>
<p>Breast cancer lesion segmentation in DCE-MRI remains challenging due to heterogeneous tumor morphology and indistinct boundaries. To address these challenges, this study proposes a novel hybrid segmentation network, HCMA-UNet, for lesion segmentation of breast cancer. Our network consists of a lightweight CNN backbone and a Multi-view Axial Self-Attention Mamba (MISM) module. The MISM module integrates Visual State Space Block (VSSB) and Axial Self-Attention (ASA) mechanism, effectively reducing parameters through Asymmetric Split Channel (ASC) strategy to achieve efficient tri-directional feature extraction. Our lightweight model achieves superior performance with 2.87M parameters and 126.44 GFLOPs. A Feature-guided Region-aware loss function (FRLoss) is proposed to enhance segmentation accuracy. Extensive experiments on one private and two public DCE-MRI breast cancer datasets demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. FRLoss also exhibits good cross-architecture generalization capabilities. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/Haoxuanli-Thu/HCMA-UNet">https://github.com/Haoxuanli-Thu/HCMA-UNet</a>. </p>
<blockquote>
<p>ä¹³è…ºç™Œç—…ç¶åœ¨DCE-MRIä¸­çš„åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè‚¿ç˜¤å½¢æ€å¤šæ ·ä¸”è¾¹ç•Œä¸æ¸…ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆåˆ†å‰²ç½‘ç»œHCMA-UNetï¼Œç”¨äºä¹³è…ºç™Œç—…ç¶çš„åˆ†å‰²ã€‚æˆ‘ä»¬çš„ç½‘ç»œç”±è½»é‡çº§CNNéª¨å¹²ç½‘å’Œå¤šè§†å›¾è½´å‘è‡ªæ³¨æ„åŠ›Mambaï¼ˆMISMï¼‰æ¨¡å—ç»„æˆã€‚MISMæ¨¡å—ç»“åˆäº†è§†è§‰çŠ¶æ€ç©ºé—´å—ï¼ˆVSSBï¼‰å’Œè½´å‘è‡ªæ³¨æ„åŠ›ï¼ˆASAï¼‰æœºåˆ¶ï¼Œé€šè¿‡ä¸å¯¹ç§°åˆ†è£‚é€šé“ï¼ˆASCï¼‰ç­–ç•¥æœ‰æ•ˆåœ°å‡å°‘äº†å‚æ•°ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸‰å‘ç‰¹å¾æå–ã€‚æˆ‘ä»¬çš„è½»é‡çº§æ¨¡å‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå…·æœ‰287ä¸‡å‚æ•°å’Œæ¯ç§’æµ®ç‚¹è¿ç®—é‡è¾¾æ¯ç§’æµ®ç‚¹è¿ç®—é‡è¾¾æ¯ç§’æµ®ç‚¹è¿ç®—é‡è¾¾æ¯ç§’æµ®ç‚¹è¿ç®—é‡è¾¾æ¯ç§’æµ®ç‚¹è¿ç®—é‡è®¡ç®—æ¬¡æ•°è¾¾åˆ°ä¸€ç™¾ä¸‡æµ®ç‚¹æ“ä½œæ¯å¦™çº§åˆ«æ€§èƒ½ä¼˜è‰¯å…¶æ‹¥æœ‰é¢†å…ˆçš„è®¡ç®—æ•ˆç‡ä¸ºæœ€é«˜è¾¾åˆ°äº¿çº§è¿ç®—æ¬¡ä»¥ä¸Šï¼›å¹¶æå‡ºäº†ä¸€ç§ç‰¹å¾å¼•å¯¼åŒºåŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼ˆFRLossï¼‰æ¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸€å¥—ç§æœ‰æ•°æ®é›†å’Œä¸¤ä¸ªå…¬å¼€DCE-MRIä¹³è…ºç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚FRLossè¿˜è¡¨ç°å‡ºè‰¯å¥½çš„è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Haoxuanli-Thu/HCMA-UNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Haoxuanli-Thu/HCMA-UNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00751v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä¹³è…ºç™ŒDCE-MRIå½±åƒçš„ç—…ç¶åˆ†å‰²ä»é¢ä¸´æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ··åˆåˆ†å‰²ç½‘ç»œHCMA-UNetã€‚è¯¥ç½‘ç»œåŒ…æ‹¬è½»é‡çº§CNNéª¨å¹²å’Œå¤šè§†è§’è½´å‘è‡ªæ³¨æ„åŠ›Mambaï¼ˆMISMï¼‰æ¨¡å—ã€‚MISMæ¨¡å—ç»“åˆè§†è§‰çŠ¶æ€ç©ºé—´å—ï¼ˆVSSBï¼‰å’Œè½´å‘è‡ªæ³¨æ„åŠ›ï¼ˆASAï¼‰æœºåˆ¶ï¼Œé€šè¿‡ä¸å¯¹ç§°åˆ†è£‚é€šé“ï¼ˆASCï¼‰ç­–ç•¥å®ç°é«˜æ•ˆçš„ä¸‰å‘ç‰¹å¾æå–ã€‚è¯¥è½»é‡çº§æ¨¡å‹å‚æ•°ä¸º2.87Mï¼Œè®¡ç®—å¤æ‚åº¦ä¸º126.44 GFLOPsï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç‰¹å¾å¼•å¯¼åŒºåŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼ˆFRLossï¼‰ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨ç§äººåŠä¸¤ä¸ªå…¬å…±DCE-MRIä¹³è…ºç™Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚FRLossè¿˜å…·æœ‰è‰¯å¥½çš„è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¹³è…ºç™ŒDCE-MRIå½±åƒçš„ç—…ç¶åˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè‚¿ç˜¤å½¢æ€å¼‚è´¨æ€§å’Œè¾¹ç•Œä¸æ¸…æ™°ã€‚</li>
<li>HCMA-UNetæ˜¯ä¸€ä¸ªæ–°å‹çš„æ··åˆåˆ†å‰²ç½‘ç»œï¼ŒåŒ…æ‹¬è½»é‡çº§CNNéª¨å¹²å’ŒMISMæ¨¡å—ã€‚</li>
<li>MISMæ¨¡å—ç»“åˆäº†VSSBå’ŒASAæœºåˆ¶ï¼Œé€šè¿‡ASCç­–ç•¥å®ç°é«˜æ•ˆä¸‰å‘ç‰¹å¾æå–ã€‚</li>
<li>HCMA-UNetæ¨¡å‹å…·æœ‰ä¼˜è¶Šæ€§èƒ½ï¼Œå‚æ•°å°‘ï¼ˆ2.87Mï¼‰ä¸”è®¡ç®—æ•ˆç‡é«˜ï¼ˆ126.44 GFLOPsï¼‰ã€‚</li>
<li>æå‡ºäº†ç‰¹å¾å¼•å¯¼åŒºåŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼ˆFRLossï¼‰ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜HCMA-UNetè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼Œä¸”FRLosså…·æœ‰è‰¯å¥½çš„è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea2ef53bf1f9aa92e9167e27f96103fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2389eafa2e4a8ab9860831410983b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4039f683b55bfa5b94f6026636db5008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a68d2d5841c4eb16eda3f67e25561ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d889c845e6bed6b14d8ca4ce3c7a96e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e4f3885f7669a7c9fb52935fcbe292.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26613c19e31caa9022529215f236cdb2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Head-and-Neck-Tumor-Segmentation-of-MRI-from-Pre-and-Mid-radiotherapy-with-Pre-training-Data-Augmentation-and-Dual-Flow-UNet"><a href="#Head-and-Neck-Tumor-Segmentation-of-MRI-from-Pre-and-Mid-radiotherapy-with-Pre-training-Data-Augmentation-and-Dual-Flow-UNet" class="headerlink" title="Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy   with Pre-training, Data Augmentation and Dual Flow UNet"></a>Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy   with Pre-training, Data Augmentation and Dual Flow UNet</h2><p><strong>Authors:Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang</strong></p>
<p>Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/WltyBY/HNTS-MRG2024_train_code">https://github.com/WltyBY/HNTS-MRG2024_train_code</a>. </p>
<blockquote>
<p>å¤´éƒ¨å’Œé¢ˆéƒ¨è‚¿ç˜¤ä»¥åŠè½¬ç§»æ€§æ·‹å·´ç»“å¯¹æ²»ç–—è®¡åˆ’å’Œé¢„ååˆ†æè‡³å…³é‡è¦ã€‚è¿™äº›ç»“æ„çš„ç²¾ç¡®åˆ†å‰²å’Œå®šé‡åˆ†æéœ€è¦åƒç´ çº§æ³¨é‡Šï¼Œå› æ­¤è‡ªåŠ¨åˆ†å‰²æŠ€æœ¯åœ¨å¤´éƒ¨å’Œé¢ˆéƒ¨ç™Œç—‡çš„è¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šç§ç­–ç•¥å¯¹æ”¾ç–—å‰ï¼ˆpre-RTï¼‰å’Œæ”¾ç–—ä¸­ï¼ˆmid-RTï¼‰å›¾åƒåˆ†å‰²çš„å½±å“ã€‚å¯¹äºæ”¾ç–—å‰å›¾åƒçš„åˆ†å‰²ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ï¼š1ï¼‰å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼›2ï¼‰ä½¿ç”¨é¢„è®­ç»ƒæƒé‡å’ŒMixUpæ•°æ®å¢å¼ºæŠ€æœ¯å¢å¼ºåŒä¸€æ–¹æ³•ã€‚å¯¹äºæ”¾ç–—ä¸­å›¾åƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è®¡ç®—å‹å¥½çš„æ–°å‹ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„å…·æœ‰é’ˆå¯¹æ”¾ç–—ä¸­å›¾åƒå’Œå·²æ³¨å†Œæ”¾ç–—å‰å›¾åƒçš„å•ç‹¬ç¼–ç å™¨ï¼Œå¹¶å¸¦æœ‰å…¶æ ‡ç­¾ã€‚æ”¾ç–—ä¸­ç¼–ç å™¨åˆ†æ”¯åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ­¥æ•´åˆæ¥è‡ªæ”¾ç–—å‰å›¾åƒå’Œæ ‡ç­¾çš„ä¿¡æ¯ã€‚æˆ‘ä»¬ä»æ¯ä¸€æŠ˜ä¸­é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å…¶é¢„æµ‹ç»“æœåˆ›å»ºé›†æˆå¹³å‡å€¼ä»¥è¿›è¡Œæ¨æ–­ã€‚åœ¨æœ€ç»ˆæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨HiLabçš„èšåˆDiceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸Šè¾¾åˆ°äº†æ”¾ç–—å‰åˆ†å‰²æ€§èƒ½ä¸º82.38%ï¼Œæ”¾ç–—ä¸­åˆ†å‰²æ€§èƒ½ä¸º72.53%ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/WltyBY/HNTS-MRG2024_train_code%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/WltyBY/HNTS-MRG2024_train_codeè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14846v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤´é¢ˆéƒ¨è‚¿ç˜¤å’Œè½¬ç§»æ·‹å·´ç»“çš„åˆ†å‰²æŠ€æœ¯ï¼Œå¯¹é¢„æ”¾ç–—ï¼ˆpre-RTï¼‰å’Œä¸­æœŸæ”¾ç–—ï¼ˆmid-RTï¼‰å›¾åƒçš„åˆ†å‰²ç­–ç•¥è¿›è¡Œäº†è°ƒæŸ¥ã€‚é‡‡ç”¨å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•å’Œå¢å¼ºé¢„è®­ç»ƒæƒé‡åŠMixUpæ•°æ®å¢å¼ºæŠ€æœ¯çš„ç»„åˆå¯¹é¢„æ”¾ç–—å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚å¯¹äºä¸­æœŸæ”¾ç–—å›¾åƒï¼Œæå‡ºäº†ä¸€ç§è®¡ç®—å‹å¥½çš„ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„å…·æœ‰é’ˆå¯¹ä¸­æœŸæ”¾ç–—å›¾åƒå’Œå·²æ³¨å†Œé¢„æ”¾ç–—å›¾åƒçš„å•ç‹¬ç¼–ç å™¨ï¼Œå¹¶åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ­¥æ•´åˆä¿¡æ¯ã€‚æœ€ç»ˆæµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨HiLabä¸Šçš„åˆ†å‰²æ€§èƒ½è¾¾åˆ°é¢„æ”¾ç–—çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º82.38%ï¼Œä¸­æœŸæ”¾ç–—çš„ä¸º72.53%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤´é¢ˆéƒ¨è‚¿ç˜¤å’Œè½¬ç§»æ·‹å·´ç»“çš„ç²¾ç¡®åˆ†å‰²å¯¹æ²»ç–—è®¡åˆ’å’Œé¢„ååˆ†æè‡³å…³é‡è¦ï¼Œéœ€è¦åƒç´ çº§åˆ«çš„æ³¨é‡Šï¼Œå› æ­¤è‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯å¯¹äºå¤´é¢ˆéƒ¨ç™Œç—‡çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶ä¸­è°ƒæŸ¥äº†é¢„æ”¾ç–—å’Œä¸­æœŸæ”¾ç–—å›¾åƒçš„åˆ†å‰²ç­–ç•¥ã€‚</li>
<li>å¯¹äºé¢„æ”¾ç–—å›¾åƒçš„åˆ†å‰²ï¼Œé‡‡ç”¨äº†å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•å’Œå¢å¼ºé¢„è®­ç»ƒæƒé‡åŠMixUpæ•°æ®å¢å¼ºæŠ€æœ¯çš„ç»„åˆã€‚</li>
<li>å¯¹äºä¸­æœŸæ”¾ç–—å›¾åƒï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è®¡ç®—å‹å¥½ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥æ•´åˆé¢„æ”¾ç–—å›¾åƒå’Œæ ‡ç­¾çš„ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹åœ¨åˆ†å‰²å¤´é¢ˆéƒ¨è‚¿ç˜¤å’Œè½¬ç§»æ·‹å·´ç»“æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œé¢„æ”¾ç–—å›¾åƒçš„åˆ†å‰²æ€§èƒ½è¾¾åˆ°82.38%ï¼Œä¸­æœŸæ”¾ç–—å›¾åƒçš„åˆ†å‰²æ€§èƒ½ä¸º72.53%ã€‚</li>
<li>æ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a78a8d289764a9e1e95f6fd5f10217c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d20352c3aa04f048cb44632cf46b006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b192492412e5674598b5b0a9eaa49db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e33f744d54a26efc5c92774c85ce8f6f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Medical-GAT-Cancer-Document-Classification-Leveraging-Graph-Based-Residual-Network-for-Scenarios-with-Limited-Data"><a href="#Medical-GAT-Cancer-Document-Classification-Leveraging-Graph-Based-Residual-Network-for-Scenarios-with-Limited-Data" class="headerlink" title="Medical-GAT: Cancer Document Classification Leveraging Graph-Based   Residual Network for Scenarios with Limited Data"></a>Medical-GAT: Cancer Document Classification Leveraging Graph-Based   Residual Network for Scenarios with Limited Data</h2><p><strong>Authors:Elias Hossain, Tasfia Nuzhat, Shamsul Masum, Shahram Rahimi, Noorbakhsh Amiri Golilarz</strong></p>
<p>Accurate classification of cancer-related medical abstracts is crucial for healthcare management and research. However, obtaining large, labeled datasets in the medical domain is challenging due to privacy concerns and the complexity of clinical data. This scarcity of annotated data impedes the development of effective machine learning models for cancer document classification. To address this challenge, we present a curated dataset of 1,874 biomedical abstracts, categorized into thyroid cancer, colon cancer, lung cancer, and generic topics. Our research focuses on leveraging this dataset to improve classification performance, particularly in data-scarce scenarios. We introduce a Residual Graph Attention Network (R-GAT) with multiple graph attention layers that capture the semantic information and structural relationships within cancer-related documents. Our R-GAT model is compared with various techniques, including transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT), RoBERTa, and domain-specific models like BioBERT and Bio+ClinicalBERT. We also evaluated deep learning models (CNNs, LSTMs) and traditional machine learning models (Logistic Regression, SVM). Additionally, we explore ensemble approaches that combine deep learning models to enhance classification. Various feature extraction methods are assessed, including Term Frequency-Inverse Document Frequency (TF-IDF) with unigrams and bigrams, Word2Vec, and tokenizers from BERT and RoBERTa. The R-GAT model outperforms other techniques, achieving precision, recall, and F1 scores of 0.99, 0.97, and 0.98 for thyroid cancer; 0.96, 0.94, and 0.95 for colon cancer; 0.96, 0.99, and 0.97 for lung cancer; and 0.95, 0.96, and 0.95 for generic topics. </p>
<blockquote>
<p>ç™Œç—‡ç›¸å…³åŒ»å­¦æ‘˜è¦çš„ç²¾ç¡®åˆ†ç±»å¯¹åŒ»ç–—ç®¡ç†å’Œç ”ç©¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºéšç§é—®é¢˜å’Œä¸´åºŠæ•°æ®çš„å¤æ‚æ€§ï¼Œåœ¨åŒ»å­¦é¢†åŸŸè·å¾—å¤§é‡æœ‰æ ‡ç­¾çš„æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™ç§æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§é˜»ç¢äº†ç™Œç—‡æ–‡æ¡£åˆ†ç±»çš„æœ‰æ•ˆæœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å‘ˆç°äº†ä¸€ä¸ªåŒ…å«1874ç¯‡ç”Ÿç‰©åŒ»å­¦æ‘˜è¦çš„æ•°æ®é›†ï¼Œè¿™äº›æ‘˜è¦è¢«åˆ†ç±»ä¸ºç”²çŠ¶è…ºç™Œã€ç»“è‚ ç™Œã€è‚ºç™Œå’Œé€šç”¨ä¸»é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡ç‚¹æ˜¯åˆ©ç”¨æ­¤æ•°æ®é›†æ¥æ”¹å–„åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¸¦æœ‰å¤šä¸ªå›¾æ³¨æ„åŠ›å±‚çš„æ®‹å·®å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆR-GATï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰ç™Œç—‡ç›¸å…³æ–‡æ¡£ä¸­çš„è¯­ä¹‰ä¿¡æ¯å’Œç»“æ„å…³ç³»ã€‚æˆ‘ä»¬å°†R-GATæ¨¡å‹ä¸å„ç§æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œå¦‚æ¥è‡ªå˜å‹å™¨çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼ˆBERTï¼‰ã€RoBERTaï¼Œä»¥åŠç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œå¦‚BioBERTå’ŒBio+ClinicalBERTã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå·ç§¯ç¥ç»ç½‘ç»œã€é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼‰å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ç»“åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥å¢å¼ºåˆ†ç±»çš„é›†æˆæ–¹æ³•ã€‚è¿˜è¯„ä¼°äº†å„ç§ç‰¹å¾æå–æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸€å…ƒè¯å’ŒäºŒå…ƒè¯çš„è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰ã€Word2Vecä»¥åŠæ¥è‡ªBERTå’ŒRoBERTaçš„æ ‡è®°å™¨ã€‚R-GATæ¨¡å‹çš„æ€§èƒ½ä¼˜äºå…¶ä»–æŠ€æœ¯ï¼Œåœ¨ç”²çŠ¶è…ºç™Œæ–¹é¢è¾¾åˆ°ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º0.99ã€0.97å’Œ0.98ï¼›ç»“è‚ ç™Œåˆ†åˆ«ä¸º0.96ã€0.94å’Œ0.95ï¼›è‚ºç™Œåˆ†åˆ«ä¸º0.96ã€0.99å’Œ0.97ï¼›é€šç”¨ä¸»é¢˜åˆ†åˆ«ä¸º0.95ã€0.96å’Œ0.95ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15198v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç™Œç—‡ç›¸å…³åŒ»å­¦æ‘˜è¦çš„åˆ†ç±»å¯¹äºåŒ»ç–—ç®¡ç†å’Œç ”ç©¶çš„é‡è¦æ€§ã€‚é’ˆå¯¹åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ•°æ®é›†çš„ç¼ºä¹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºResidual Graph Attention Networkï¼ˆR-GATï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç™Œç—‡ç›¸å…³æ–‡æ¡£ä¸­çš„è¯­ä¹‰ä¿¡æ¯å’Œç»“æ„å…³ç³»ï¼Œä¸å…¶ä»–æŠ€æœ¯ç›¸æ¯”è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç™Œç—‡ç›¸å…³åŒ»å­¦æ‘˜è¦çš„åˆ†ç±»å¯¹åŒ»ç–—ç®¡ç†å’Œç ”ç©¶è‡³å…³é‡è¦ã€‚</li>
<li>åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ•°æ®é›†çš„ç¼ºä¹æ˜¯å¼€å‘æœ‰æ•ˆæœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†Residual Graph Attention Networkï¼ˆR-GATï¼‰æ¨¡å‹ï¼Œç”¨äºæ”¹å–„åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</li>
<li>R-GATæ¨¡å‹é€šè¿‡æ•æ‰ç™Œç—‡ç›¸å…³æ–‡æ¡£ä¸­çš„è¯­ä¹‰ä¿¡æ¯å’Œç»“æ„å…³ç³»ï¼Œå®ç°äº†é«˜æ€§èƒ½åˆ†ç±»ã€‚</li>
<li>R-GATæ¨¡å‹ä¸å…¶ä»–æŠ€æœ¯ï¼ˆåŒ…æ‹¬BERTã€RoBERTaã€BioBERTã€Bio+ClinicalBERTç­‰ï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1c29e7c3ae327ca3cc01dee3041e12a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c8342d0ce23bf7ad2701d3429fd02f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b59d9b5241925b55f63dad67305b5f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="First-image-guided-treatment-of-a-mouse-tumor-with-radioactive-ion-beams"><a href="#First-image-guided-treatment-of-a-mouse-tumor-with-radioactive-ion-beams" class="headerlink" title="First image-guided treatment of a mouse tumor with radioactive ion beams"></a>First image-guided treatment of a mouse tumor with radioactive ion beams</h2><p><strong>Authors:Daria Boscolo, Giulio Lovatti, Olga Sokol, Tamara Vitacchio, Francesco Evangelista, Emma Haettner, Walter Tinganelli, Christian Graeff, Uli Weber, Christoph Schuy, Munetaka Nitta, Martina Moglioni, Daria Kostyleva, Sivaji Purushothaman, Peter G. Thirolf, Jonathan Bortfeldt, Christoph Scheidenberger, Katia Parodi, Marco Durante</strong></p>
<p>Radioactive ion beams (RIB) are a key focus of current research in nuclear physics. Already long ago it was proposed that they could have applications in cancer therapy. In fact, while charged particle therapy is potentially the most effective radiotherapy technique available, it is highly susceptible to uncertainties in the beam range. RIB are well-suited for image-guided particle therapy, as isotopes that undergo \b{eta}+-decay can be precisely visualized using positron emission tomography (PET), enabling accurate real-time monitoring of the beam range. We successfully treated a mouse osteosarcoma using a radioactive 11C-ion beam. The tumor was located in the neck, in close proximity to the spinal cord, increasing the risk of radiation-induced myelopathy from even slight variations in the beam range caused by anatomical changes or incorrect calibration of the planning CT. We managed to completely control the tumor with the highest dose while minimizing toxicity. Low-grade neurological side effects were correlated to the positron activity measured in the spine. The biological washout of the activity from the tumor volume was dependent on the dose, indicating a potential component of vascular damage at high doses. This experiment marks the first instance of tumor treatment using RIB and paves the way for future clinical applications. </p>
<blockquote>
<p>æ”¾å°„æ€§ç¦»å­æŸï¼ˆRIBï¼‰æ˜¯å½“å‰æ ¸ç‰©ç†ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ã€‚å…¶å®å¾ˆæ—©ä»¥å‰å°±æœ‰äººæå‡ºå®ƒä»¬å¯èƒ½åœ¨ç™Œç—‡æ²»ç–—ä¸­æœ‰åº”ç”¨ã€‚å®é™…ä¸Šï¼Œè™½ç„¶å¸¦ç”µç²’å­ç–—æ³•å¯èƒ½æ˜¯ç›®å‰å¯ç”¨çš„æœ€æœ‰æ•ˆçš„æ”¾å°„æ²»ç–—æŠ€æœ¯ï¼Œä½†å®ƒå¯¹å…‰æŸèŒƒå›´çš„ä¸ç¡®å®šæ€§éå¸¸æ•æ„Ÿã€‚æ”¾å°„æ€§ç¦»å­æŸéå¸¸é€‚åˆäºå›¾åƒå¼•å¯¼ç²’å­ç–—æ³•ï¼Œå› ä¸ºå¯ä»¥é€šè¿‡æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ç²¾ç¡®å¯è§†åŒ–å‘ç”Ÿæ­£ç”µå­è¡°å˜çš„åŒä½ç´ ï¼Œä»è€Œå®ç°å¯¹å…‰æŸèŒƒå›´çš„å®æ—¶å‡†ç¡®ç›‘æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨æ”¾å°„æ€§ç¢³ç¦»å­æŸæˆåŠŸæ²»ç–—äº†ä¸€åªæ‚£æœ‰éª¨è‚‰ç˜¤çš„å°é¼ ã€‚è‚¿ç˜¤ä½äºé¢ˆéƒ¨ï¼Œé è¿‘è„Šé«“ï¼Œå³ä½¿ç”±è§£å‰–ç»“æ„å˜åŒ–æˆ–è®¡åˆ’CTçš„æ ¡å‡†ä¸æ­£ç¡®å¼•èµ·çš„å…‰æŸèŒƒå›´è½»å¾®å˜åŒ–ï¼Œä¹Ÿä¼šå¢åŠ è¾å°„è¯±å¯¼è„Šé«“ç—…çš„é£é™©ã€‚æˆ‘ä»¬è®¾æ³•ä»¥æœ€é«˜å‰‚é‡å®Œå…¨æ§åˆ¶è‚¿ç˜¤ï¼ŒåŒæ—¶å°½é‡å‡å°‘æ¯’æ€§ã€‚ä½çº§åˆ«çš„ç¥ç»å‰¯ä½œç”¨ä¸è„ŠæŸ±ä¸­æµ‹é‡çš„æ­£ç”µå­æ´»æ€§æœ‰å…³ã€‚è‚¿ç˜¤ä½“ç§¯å†…çš„æ´»æ€§ç”Ÿç‰©æ¸…é™¤ä¾èµ–äºå‰‚é‡ï¼Œè¿™è¡¨æ˜åœ¨é«˜å‰‚é‡æ—¶å¯èƒ½å­˜åœ¨è¡€ç®¡æŸä¼¤æˆåˆ†ã€‚è¿™ä¸ªå®éªŒæ ‡å¿—ç€ä½¿ç”¨RIBè¿›è¡Œè‚¿ç˜¤æ²»ç–—çš„é¦–ä¾‹ï¼Œä¸ºæœªæ¥çš„ä¸´åºŠåº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14898v4">PDF</a> 56 pages, 13 figures, supplements. Video supplements available on   request</p>
<p><strong>Summary</strong></p>
<p>æ”¾å°„æ€§ç¦»å­æŸï¼ˆRIBï¼‰æ˜¯å½“å‰æ ¸ç‰©ç†ç ”ç©¶çš„é‡è¦æ–¹å‘ä¹‹ä¸€ï¼Œå…·æœ‰åº”ç”¨äºç™Œç—‡æ²»ç–—çš„æ½œåŠ›ã€‚å°½ç®¡ç¦»å­æŸç–—æ³•åœ¨æ”¾ç–—é¢†åŸŸå¯èƒ½æœ€æœ‰æ•ˆï¼Œä½†å®ƒé«˜åº¦ä¾èµ–æŸæµçš„å‡†ç¡®åº¦ã€‚ç„¶è€Œï¼Œé€šè¿‡åˆ©ç”¨æ”¾å°„æ€§åŒä½ç´ å‘ç”Ÿetaè¡°å˜çš„ç‰¹æ€§ï¼Œå€ŸåŠ©æ­£ç”µå­å‘å°„æ–­å±‚æ‰«ææŠ€æœ¯ï¼ˆPETï¼‰è¿›è¡Œå¯è§†åŒ–ï¼ŒRIBåœ¨å›¾åƒå¼•å¯¼ä¸‹æ²»ç–—è‚¿ç˜¤æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚ç ”ç©¶å›¢é˜ŸæˆåŠŸä½¿ç”¨æ”¾å°„æ€§ç¢³ç¦»å­æŸæ²»ç–—äº†ä¸€åªæ‚£æœ‰é¢ˆéƒ¨éª¨è‚¿ç˜¤çš„é¼ æ¨¡å‹ï¼Œåœ¨æ²»ç–—è¿‡ç¨‹ä¸­ç²¾å‡†æ§åˆ¶å‰‚é‡ä»¥é™ä½æ¯’æ€§å¹¶é˜²æ­¢å› æŸæµè¯¯å·®å¼•èµ·çš„è¾å°„æŸä¼¤è„Šé«“é£é™©ã€‚å®éªŒé¦–æ¬¡å±•ç¤ºäº†RIBåœ¨è‚¿ç˜¤æ²»ç–—ä¸­çš„åº”ç”¨ï¼Œä¸ºæœªæ¥çš„ä¸´åºŠåº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„æ€§ç¦»å­æŸï¼ˆRIBï¼‰æ˜¯æ ¸ç‰©ç†é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚</li>
<li>RIBåœ¨ç™Œç—‡æ²»ç–—ä¸­æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>RIBé€‚ç”¨äºå›¾åƒå¼•å¯¼ä¸‹çš„ç²’å­ç–—æ³•ã€‚</li>
<li>RIBé€šè¿‡PETå¯è§†åŒ–åŒä½ç´ è¡°å˜æœ‰åŠ©äºç²¾å‡†ç›‘æµ‹æŸæµèŒƒå›´ã€‚</li>
<li>æˆåŠŸä½¿ç”¨æ”¾å°„æ€§ç¢³ç¦»å­æŸæ²»ç–—é¼ é¢ˆéƒ¨éª¨è‚¿ç˜¤æ¡ˆä¾‹ã€‚</li>
<li>æ²»ç–—è¿‡ç¨‹ä¸­éœ€ç²¾å‡†æ§åˆ¶å‰‚é‡ä»¥é™ä½æ¯’æ€§å¹¶é¿å…è¾å°„æŸä¼¤è„Šé«“é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-85fa2ef8126bcdf1f0f01f71de9ac2df.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Evaluating-Pre-trained-Convolutional-Neural-Networks-and-Foundation-Models-as-Feature-Extractors-for-Content-based-Medical-Image-Retrieval"><a href="#Evaluating-Pre-trained-Convolutional-Neural-Networks-and-Foundation-Models-as-Feature-Extractors-for-Content-based-Medical-Image-Retrieval" class="headerlink" title="Evaluating Pre-trained Convolutional Neural Networks and Foundation   Models as Feature Extractors for Content-based Medical Image Retrieval"></a>Evaluating Pre-trained Convolutional Neural Networks and Foundation   Models as Feature Extractors for Content-based Medical Image Retrieval</h2><p><strong>Authors:Amirreza Mahbod, Nematollah Saeidi, Sepideh Hatamikia, Ramona Woitek</strong></p>
<p>Medical image retrieval refers to the task of finding similar images for given query images in a database, with applications such as diagnosis support. While traditional medical image retrieval relied on clinical metadata, content-based medical image retrieval (CBMIR) depends on image features, which can be extracted automatically or semi-automatically. Many approaches have been proposed for CBMIR, and among them, using pre-trained convolutional neural networks (CNNs) is a widely utilized approach. However, considering the recent advances in the development of foundation models for various computer vision tasks, their application for CBMIR can also be investigated.   In this study, we used several pre-trained feature extractors from well-known pre-trained CNNs and pre-trained foundation models and investigated the CBMIR performance on eight types of two-dimensional (2D) and three-dimensional (3D) medical images. Furthermore, we investigated the effect of image size on the CBMIR performance.   Our results show that, overall, for the 2D datasets, foundation models deliver superior performance by a large margin compared to CNNs, with the general-purpose self-supervised model for computational pathology (UNI) providing the best overall performance across all datasets and image sizes. For 3D datasets, CNNs and foundation models deliver more competitive performance, with contrastive learning from captions for histopathology model (CONCH) achieving the best overall performance. Moreover, our findings confirm that while using larger image sizes (especially for 2D datasets) yields slightly better performance, competitive CBMIR performance can still be achieved even with smaller image sizes. Our codes to reproduce the results are available at: <a target="_blank" rel="noopener" href="https://github.com/masih4/MedImageRetrieval">https://github.com/masih4/MedImageRetrieval</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ£€ç´¢æ˜¯æŒ‡åœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°ä¸ç»™å®šæŸ¥è¯¢å›¾åƒç›¸ä¼¼çš„å›¾åƒçš„ä»»åŠ¡ï¼Œå…¶åº”ç”¨åœºæ™¯å¦‚è¯Šæ–­æ”¯æŒã€‚ä¼ ç»Ÿçš„åŒ»å­¦å›¾åƒæ£€ç´¢ä¾èµ–äºä¸´åºŠå…ƒæ•°æ®ï¼Œè€ŒåŸºäºå†…å®¹çš„åŒ»å­¦å›¾åƒæ£€ç´¢ï¼ˆCBMIRï¼‰åˆ™ä¾èµ–äºå›¾åƒç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥è‡ªåŠ¨æˆ–åŠè‡ªåŠ¨æå–ã€‚å·²ç»æå‡ºäº†è®¸å¤šCBMIRçš„æ–¹æ³•ï¼Œå…¶ä¸­ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°æœ€è¿‘é’ˆå¯¹å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹å¼€å‘æ–¹é¢çš„è¿›å±•ï¼Œä¹Ÿå¯ä»¥ç ”ç©¶å®ƒä»¬å¯¹CBMIRçš„åº”ç”¨ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªè‘—åé¢„è®­ç»ƒCNNå’Œé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„å‡ ä¸ªé¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬åœ¨äºŒç»´ï¼ˆ2Dï¼‰å’Œä¸‰ç»´ï¼ˆ3Dï¼‰åŒ»å­¦å›¾åƒä¸Šçš„CBMIRæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å›¾åƒå¤§å°å¯¹CBMIRæ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ€»ä½“è€Œè¨€ï¼Œå¯¹äºäºŒç»´æ•°æ®é›†ï¼ŒåŸºç¡€æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¡¨ç°å‡ºä¼˜äºCNNçš„æ€§èƒ½ï¼Œå…¶ä¸­ç”¨äºè®¡ç®—ç—…ç†å­¦çš„é€šç”¨è‡ªç›‘ç£æ¨¡å‹ï¼ˆUNIï¼‰åœ¨æ‰€æœ‰æ•°æ®é›†å’Œå›¾åƒå¤§å°ä¸Šè¡¨ç°å‡ºæœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚å¯¹äºä¸‰ç»´æ•°æ®é›†ï¼ŒCNNå’ŒåŸºç¡€æ¨¡å‹è¡¨ç°å‡ºæ›´å…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå…¶ä¸­åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç»„ç»‡ç—…ç†å­¦æ¨¡å‹ï¼ˆCONCHï¼‰å–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä½¿ç”¨è¾ƒå¤§çš„å›¾åƒå¤§å°ï¼ˆå°¤å…¶æ˜¯äºŒç»´æ•°æ®é›†ï¼‰ç•¥å¾®æé«˜äº†æ€§èƒ½ï¼Œä½†å³ä½¿ä½¿ç”¨è¾ƒå°çš„å›¾åƒå¤§å°ä¹Ÿå¯ä»¥å®ç°å…·æœ‰ç«äº‰åŠ›çš„CBMIRæ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/masih4/MedImageRetrieval%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/masih4/MedImageRetrievalä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09430v2">PDF</a> 37 pages</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„CNNå’Œé¢„è®­ç»ƒçš„Foundationæ¨¡å‹è¿›è¡ŒåŸºäºå†…å®¹çš„åŒ»å­¦å›¾åƒæ£€ç´¢ï¼ˆCBMIRï¼‰ï¼Œå¹¶åœ¨äºŒç»´å’Œä¸‰ç»´åŒ»å­¦å›¾åƒä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å‘ç°å¯¹äºäºŒç»´æ•°æ®é›†ï¼ŒFoundationæ¨¡å‹ç›¸è¾ƒäºCNNå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶ä¸­è®¡ç®—ç—…ç†å­¦é€šç”¨è‡ªç›‘ç£æ¨¡å‹ï¼ˆUNIï¼‰è¡¨ç°æœ€ä½³ã€‚å¯¹äºä¸‰ç»´æ•°æ®é›†ï¼ŒCNNå’ŒFoundationæ¨¡å‹è¡¨ç°æ›´ç«äº‰ï¼ŒåŸºäºç—…ç†ç»„ç»‡å­¦æ¨¡å‹ï¼ˆCONCHï¼‰å¯¹æ¯”å­¦ä¹ å®ç°æœ€ä½³æ€§èƒ½ã€‚å¢å¤§å›¾åƒå°ºå¯¸èƒ½æé«˜æ€§èƒ½ï¼Œä½†å³ä½¿ä½¿ç”¨è¾ƒå°çš„å›¾åƒå°ºå¯¸ä¹Ÿèƒ½å®ç°ç«äº‰æ€§çš„CBMIRæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºå†…å®¹çš„åŒ»å­¦å›¾åƒæ£€ç´¢ï¼ˆCBMIRï¼‰ä¾èµ–äºå›¾åƒç‰¹å¾ï¼Œå¯è‡ªåŠ¨æˆ–åŠè‡ªåŠ¨æå–ã€‚</li>
<li>é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å¹¿æ³›åº”ç”¨äºCBMIRã€‚</li>
<li>åœ¨äºŒç»´æ•°æ®é›†ä¸Šï¼ŒFoundationæ¨¡å‹ç›¸è¾ƒäºCNNå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯è®¡ç®—ç—…ç†å­¦é€šç”¨è‡ªç›‘ç£æ¨¡å‹ï¼ˆUNIï¼‰ã€‚</li>
<li>å¯¹äºä¸‰ç»´æ•°æ®é›†ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒFoundationæ¨¡å‹è¡¨ç°æ›´å…·ç«äº‰åŠ›ï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆCONCHï¼‰è¡¨ç°æœ€ä½³ã€‚</li>
<li>å¢å¤§å›¾åƒå°ºå¯¸å¯ä»¥æé«˜CBMIRçš„æ€§èƒ½ï¼Œä½†ä½¿ç”¨è¾ƒå°çš„å›¾åƒå°ºå¯¸ä¹Ÿèƒ½å®ç°ç«äº‰æ€§ç»“æœã€‚</li>
<li>æœ¬ç ”ç©¶çš„ç»“æœè¡¨æ˜Foundationæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒæ£€ç´¢ä¸­çš„æ½œåŠ›ä¸åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47c25ee8b493ef178fc2916a245a775d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f78c79326bffb54273e71eac0d97edf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a05d7dcd2fa72311d9dc393962a033ab.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DetailCLIP-Detail-Oriented-CLIP-for-Fine-Grained-Tasks"><a href="#DetailCLIP-Detail-Oriented-CLIP-for-Fine-Grained-Tasks" class="headerlink" title="DetailCLIP: Detail-Oriented CLIP for Fine-Grained Tasks"></a>DetailCLIP: Detail-Oriented CLIP for Fine-Grained Tasks</h2><p><strong>Authors:Amin Karimi Monsefi, Kishore Prakash Sailaja, Ali Alilooee, Ser-Nam Lim, Rajiv Ramnath</strong></p>
<p>In this paper, we introduce DetailCLIP: A Detail-Oriented CLIP to address the limitations of contrastive learning-based vision-language models, particularly CLIP, in handling detail-oriented and fine-grained tasks like segmentation. While CLIP and its variants excel in the global alignment of image and text representations, they often struggle to capture the fine-grained details necessary for precise segmentation. To overcome these challenges, we propose a novel framework that employs patch-level comparison of self-distillation and pixel-level reconstruction losses, enhanced with an attention-based token removal mechanism. This approach selectively retains semantically relevant tokens, enabling the model to focus on the imageâ€™s critical regions aligned with the specific functions of our model, including textual information processing, patch comparison, and image reconstruction, ensuring that the model learns high-level semantics and detailed visual features. Our experiments demonstrate that DetailCLIP surpasses existing CLIP-based and traditional self-supervised learning (SSL) models in segmentation accuracy and exhibits superior generalization across diverse datasets. DetailCLIP represents a significant advancement in vision-language modeling, offering a robust solution for tasks that demand high-level semantic understanding and detailed feature extraction. <a target="_blank" rel="noopener" href="https://github.com/KishoreP1/DetailCLIP">https://github.com/KishoreP1/DetailCLIP</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DetailCLIPï¼šä¸€ç§é¢å‘ç»†èŠ‚çš„CLIPï¼Œæ—¨åœ¨è§£å†³åŸºäºå¯¹æ¯”å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯CLIPåœ¨å¤„ç†é¢å‘ç»†èŠ‚å’Œç²¾ç»†ç²’åº¦ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰æ—¶çš„å±€é™æ€§ã€‚è™½ç„¶CLIPåŠå…¶å˜ä½“åœ¨å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºçš„å…¨å±€å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥æ•æ‰ç²¾ç¡®åˆ†å‰²æ‰€éœ€çš„ç²¾ç»†ç»†èŠ‚ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé‡‡ç”¨è‡ªè’¸é¦çš„è¡¥ä¸çº§æ¯”è¾ƒå’Œåƒç´ çº§é‡å»ºæŸå¤±ï¼Œè¾…ä»¥åŸºäºæ³¨æ„åŠ›çš„ä»¤ç‰Œç§»é™¤æœºåˆ¶ã€‚æ­¤æ–¹æ³•æœ‰é€‰æ‹©åœ°ä¿ç•™è¯­ä¹‰ç›¸å…³ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ä¸æ¨¡å‹ç‰¹å®šåŠŸèƒ½ç›¸å¯¹åº”çš„å›¾å½¢çš„å…³é”®åŒºåŸŸï¼ŒåŒ…æ‹¬æ–‡æœ¬ä¿¡æ¯å¤„ç†ã€è¡¥ä¸æ¯”è¾ƒå’Œå›¾åƒé‡å»ºï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ é«˜çº§è¯­ä¹‰å’Œè¯¦ç»†çš„è§†è§‰ç‰¹å¾ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDetailCLIPåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰çš„CLIPå’Œä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚DetailCLIPåœ¨è§†è§‰è¯­è¨€å»ºæ¨¡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä¸ºéœ€è¦é«˜çº§è¯­ä¹‰ç†è§£å’Œè¯¦ç»†ç‰¹å¾æå–çš„ä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚è¯¦æƒ…è¯·è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/KishoreP1/DetailCLIP%E3%80%82">https://github.com/KishoreP1/DetailCLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06809v2">PDF</a> Accepted in SSI-FM Workshop of ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†DetailCLIPï¼šä¸€ç§é¢å‘ç»†èŠ‚çš„CLIPï¼Œæ—¨åœ¨è§£å†³åŸºäºå¯¹æ¯”å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¤„ç†ç»†èŠ‚å¯¼å‘å’Œç²¾ç»†ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰æ—¶çš„å±€é™æ€§ã€‚DetailCLIPé‡‡ç”¨è¡¥ä¸çº§åˆ«çš„è‡ªæˆ‘è’¸é¦å’Œåƒç´ çº§åˆ«çš„é‡å»ºæŸå¤±çš„å¯¹æ¯”ï¼Œç»“åˆåŸºäºæ³¨æ„åŠ›çš„ä»¤ç‰Œç§»é™¤æœºåˆ¶ï¼Œä»¥é€‰æ‹©æ€§ä¿ç•™è¯­ä¹‰ç›¸å…³çš„ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ä¸ç‰¹å®šåŠŸèƒ½å¯¹é½çš„å›¾åƒå…³é”®åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼ŒDetailCLIPåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰çš„CLIPå’Œä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DetailCLIPæ—¨åœ¨è§£å†³CLIPåœ¨å¤„ç†ç»†èŠ‚å¯¼å‘å’Œç²¾ç»†ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰æ—¶çš„å±€é™æ€§ã€‚</li>
<li>DetailCLIPé€šè¿‡è¡¥ä¸çº§åˆ«çš„è‡ªæˆ‘è’¸é¦å’Œåƒç´ çº§åˆ«çš„é‡å»ºæŸå¤±çš„å¯¹æ¯”æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºæ³¨æ„åŠ›çš„ä»¤ç‰Œç§»é™¤æœºåˆ¶ï¼Œä»¥ä¿ç•™è¯­ä¹‰ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚</li>
<li>DetailCLIPä½¿æ¨¡å‹å…³æ³¨ä¸ç‰¹å®šä»»åŠ¡å¯¹é½çš„å›¾åƒå…³é”®åŒºåŸŸã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDetailCLIPåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰çš„CLIPå’Œä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>DetailCLIPåœ¨å„ç§æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d070b99c8f737f4b43496969cd29e5de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11f56c9d9f6e12694127bcfada8a0959.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-language-guided-unsupervised-adaptation-improve-medical-image-classification-using-unpaired-images-and-texts"><a href="#Can-language-guided-unsupervised-adaptation-improve-medical-image-classification-using-unpaired-images-and-texts" class="headerlink" title="Can language-guided unsupervised adaptation improve medical image   classification using unpaired images and texts?"></a>Can language-guided unsupervised adaptation improve medical image   classification using unpaired images and texts?</h2><p><strong>Authors:Umaima Rahman, Raza Imam, Mohammad Yaqub, Boulbaba Ben Amor, Dwarikanath Mahapatra</strong></p>
<p>In medical image classification, supervised learning is challenging due to the scarcity of labeled medical images. To address this, we leverage the visual-textual alignment within Vision-Language Models (VLMs) to enable unsupervised learning of a medical image classifier. In this work, we propose \underline{Med}ical \underline{Un}supervised \underline{A}daptation (\texttt{MedUnA}) of VLMs, where the LLM-generated descriptions for each class are encoded into text embeddings and matched with class labels via a cross-modal adapter. This adapter attaches to a visual encoder of \texttt{MedCLIP} and aligns the visual embeddings through unsupervised learning, driven by a contrastive entropy-based loss and prompt tuning. Thereby, improving performance in scenarios where textual information is more abundant than labeled images, particularly in the healthcare domain. Unlike traditional VLMs, \texttt{MedUnA} uses \textbf{unpaired images and text} for learning representations and enhances the potential of VLMs beyond traditional constraints. We evaluate the performance on three chest X-ray datasets and two multi-class datasets (diabetic retinopathy and skin lesions), showing significant accuracy gains over the zero-shot baseline. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/rumaima/meduna">https://github.com/rumaima/meduna</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ï¼Œç”±äºç¼ºå°‘æ ‡è®°çš„åŒ»å­¦å›¾åƒï¼Œç›‘ç£å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„è§†è§‰æ–‡æœ¬å¯¹é½åŠŸèƒ½ï¼Œå®ç°åŒ»å­¦å›¾åƒåˆ†ç±»å™¨çš„æ— ç›‘ç£å­¦ä¹ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒ»å­¦æ— ç›‘ç£é€‚é…ï¼ˆMedUnAï¼‰çš„VLMsæ–¹æ³•ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆçš„æè¿°è¢«ç¼–ç ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€é€‚é…å™¨ä¸ç±»åˆ«æ ‡ç­¾è¿›è¡ŒåŒ¹é…ã€‚è¯¥é€‚é…å™¨é™„åŠ åˆ°MedCLIPçš„è§†è§‰ç¼–ç å™¨ä¸Šï¼Œé€šè¿‡å¯¹æ— ç›‘ç£å­¦ä¹ äº§ç”Ÿçš„è§†è§‰åµŒå…¥è¿›è¡Œå¯¹é½ï¼Œç”±åŸºäºå¯¹æ¯”ç†µçš„æŸå¤±å’Œæç¤ºè°ƒæ•´é©±åŠ¨ã€‚å› æ­¤ï¼Œåœ¨æ–‡æœ¬ä¿¡æ¯æ¯”æ ‡è®°å›¾åƒä¸°å¯Œçš„æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸï¼Œæé«˜äº†æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„VLMsä¸åŒï¼ŒMedUnAä½¿ç”¨éé…å¯¹å›¾åƒå’Œæ–‡æœ¬æ¥å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶è¶…è¶Šäº†ä¼ ç»Ÿçº¦æŸçš„VLMsçš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†å’Œä¸¤ä¸ªå¤šç±»æ•°æ®é›†ï¼ˆç³–å°¿ç—…è§†ç½‘è†œç—…å˜å’Œçš®è‚¤ç—…å˜ï¼‰ä¸Šè¯„ä¼°äº†æ€§èƒ½ï¼Œä¸é›¶æ ·æœ¬åŸºçº¿ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æé«˜ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rumaima/meduna%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rumaima/medunaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02729v2">PDF</a> Conference paper at International Symposium on Biomedical Imaging   (ISBI) 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç›‘ç£å­¦ä¹ å› ç¼ºä¹æ ‡æ³¨å›¾åƒè€Œé¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†è§‰æ–‡æœ¬å¯¹é½ç‰¹æ€§ï¼Œå®ç°åŒ»ç–—å›¾åƒåˆ†ç±»å™¨çš„æ— ç›‘ç£å­¦ä¹ ã€‚æœ¬ç ”ç©¶æå‡ºåŒ»ç–—æ— ç›‘ç£é€‚åº”ï¼ˆMedUnAï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å„ç±»æè¿°ï¼Œç¼–ç æˆæ–‡æœ¬åµŒå…¥ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€é€‚é…å™¨ä¸ç±»æ ‡ç­¾åŒ¹é…ã€‚é€‚é…å™¨è¿æ¥MedCLIPçš„è§†è§‰ç¼–ç å™¨ï¼Œé€šè¿‡å¯¹æ— ç›‘ç£å­¦ä¹ é©±åŠ¨çš„å¯¹é½è§†è§‰åµŒå…¥ï¼Œå‡å°‘å¯¹æ¯”ç†µæŸå¤±å¹¶è°ƒæ•´æç¤ºï¼Œæé«˜åœ¨æ–‡æœ¬ä¿¡æ¯ä¸°å¯Œäºæ ‡æ³¨å›¾åƒçš„åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚ä¸åŒäºä¼ ç»ŸVLMsï¼ŒMedUnAåˆ©ç”¨éé…å¯¹å›¾åƒå’Œæ–‡æœ¬å­¦ä¹ è¡¨å¾ï¼Œçªç ´äº†ä¼ ç»Ÿçº¦æŸï¼Œæé«˜äº†VLMsçš„æ½œåŠ›ã€‚åœ¨ä¸‰ä¸ªèƒ¸éƒ¨Xå…‰æ•°æ®é›†å’Œä¸¤ä¸ªå¤šç±»æ•°æ®é›†ï¼ˆç³–å°¿ç—…è§†ç½‘è†œç—…å˜å’Œçš®è‚¤ç—…å˜ï¼‰ä¸Šè¯„ä¼°æ€§èƒ½ï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬åŸºçº¿æœ‰æ˜¾è‘—ç²¾åº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»é¢ä¸´ç›‘ç£å­¦ä¹ æŒ‘æˆ˜ï¼Œå› ç¼ºä¹æ ‡æ³¨å›¾åƒã€‚</li>
<li>æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†è§‰æ–‡æœ¬å¯¹é½ç‰¹æ€§ä»¥å®ç°æ— ç›‘ç£å­¦ä¹ ã€‚</li>
<li>å¼•å…¥åŒ»ç–—æ— ç›‘ç£é€‚åº”ï¼ˆMedUnAï¼‰æ–¹æ³•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€é€‚é…å™¨ã€‚</li>
<li>MedUnAé€šè¿‡ç¼–ç ç±»æè¿°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼Œå¹¶ä¸ç±»æ ‡ç­¾åŒ¹é…ã€‚</li>
<li>MedUnAé€‚é…å™¨è¿æ¥è‡³MedCLIPçš„è§†è§‰ç¼–ç å™¨ä»¥å®ç°è§†è§‰åµŒå…¥å¯¹é½ã€‚</li>
<li>æ— ç›‘ç£å­¦ä¹ é€šè¿‡å‡å°‘å¯¹æ¯”ç†µæŸå¤±å’Œè°ƒæ•´æç¤ºæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>MedUnAåœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¿¡æ¯ä¸°å¯Œçš„åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-73737be3acd911118ec9d376907ef5da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50318993b5bc361646f35c05a59bcee4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-545e3673199812d918cb1efbb1fdfbd4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1cab6540d78d4b7817187dc0f07e9c4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SMAFormer-Synergistic-Multi-Attention-Transformer-for-Medical-Image-Segmentation"><a href="#SMAFormer-Synergistic-Multi-Attention-Transformer-for-Medical-Image-Segmentation" class="headerlink" title="SMAFormer: Synergistic Multi-Attention Transformer for Medical Image   Segmentation"></a>SMAFormer: Synergistic Multi-Attention Transformer for Medical Image   Segmentation</h2><p><strong>Authors:Fuchen Zheng, Xuhang Chen, Weihuang Liu, Haolun Li, Yingtie Lei, Jiahui He, Chi-Man Pun, Shounjun Zhou</strong></p>
<p>In medical image segmentation, specialized computer vision techniques, notably transformers grounded in attention mechanisms and residual networks employing skip connections, have been instrumental in advancing performance. Nonetheless, previous models often falter when segmenting small, irregularly shaped tumors. To this end, we introduce SMAFormer, an efficient, Transformer-based architecture that fuses multiple attention mechanisms for enhanced segmentation of small tumors and organs. SMAFormer can capture both local and global features for medical image segmentation. The architecture comprises two pivotal components. First, a Synergistic Multi-Attention (SMA) Transformer block is proposed, which has the benefits of Pixel Attention, Channel Attention, and Spatial Attention for feature enrichment. Second, addressing the challenge of information loss incurred during attention mechanism transitions and feature fusion, we design a Feature Fusion Modulator. This module bolsters the integration between the channel and spatial attention by mitigating reshaping-induced information attrition. To evaluate our method, we conduct extensive experiments on various medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, achieving state-of-the-art results. Code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/CXH-Research/SMAFormer">https://github.com/CXH-Research/SMAFormer</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œç‰¹å®šçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨ï¼ˆtransformerï¼‰å’Œé‡‡ç”¨è·³è·ƒè¿æ¥çš„æ®‹å·®ç½‘ç»œï¼ˆresidual networksï¼‰ï¼Œåœ¨æå‡æ€§èƒ½ä¸Šå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ¨¡å‹åœ¨åˆ†å‰²å°å‹ã€å½¢çŠ¶ä¸è§„åˆ™çš„è‚¿ç˜¤æ—¶ç»å¸¸ä¼šé‡åˆ°å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SMAFormerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åŸºäºå˜å‹å™¨çš„æ¶æ„ï¼Œèåˆäº†å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæ”¹è¿›å°å‹è‚¿ç˜¤å’Œå™¨å®˜çš„åˆ†å‰²ã€‚SMAFormerå¯ä»¥æ•è·åŒ»å­¦å›¾åƒåˆ†å‰²çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚è¯¥æ¶æ„åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ååŒå¤šæ³¨æ„åŠ›ï¼ˆSMAï¼‰å˜å‹å™¨å—ï¼Œè¯¥å—å…·æœ‰åƒç´ æ³¨æ„åŠ›ã€é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›çš„ä¼˜ç‚¹ï¼Œä»¥å®ç°ç‰¹å¾ä¸°å¯Œã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³æ³¨æ„åŠ›æœºåˆ¶è½¬æ¢å’Œç‰¹å¾èåˆè¿‡ç¨‹ä¸­ä¿¡æ¯æŸå¤±çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹å¾èåˆè°ƒåˆ¶å™¨ã€‚è¯¥æ¨¡å—é€šè¿‡å‡è½»é‡å¡‘å¼•èµ·çš„ä¿¡æ¯è¡°å‡ï¼ŒåŠ å¼ºäº†é€šé“å’Œç©ºé—´æ³¨æ„åŠ›ä¹‹é—´çš„é›†æˆã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬å¤šå™¨å®˜ã€è‚è„è‚¿ç˜¤å’Œè†€èƒ±è‚¿ç˜¤åˆ†å‰²ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/CXH-Research/SMAFormer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CXH-Research/SMAFormerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00346v3">PDF</a> Accepted by IEEE BIBM 2024</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®ç½‘ç»œçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯¹äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½æå‡èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºå°å‹ã€å½¢çŠ¶ä¸è§„åˆ™çš„è‚¿ç˜¤çš„åˆ†å‰²ï¼Œå…ˆå‰æ¨¡å‹å¸¸å¸¸å­˜åœ¨ç¼ºé™·ã€‚æˆ‘ä»¬æå‡ºäº†SMAFormerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åŸºäºTransformerçš„æ¶æ„ï¼Œèåˆäº†å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæ”¹è¿›å°å‹è‚¿ç˜¤å’Œå™¨å®˜çš„åˆ†å‰²ã€‚SMAFormerèƒ½å¤Ÿæ•æ‰åŒ»å­¦å›¾åƒåˆ†å‰²çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚è¯¥æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€æ˜¯ååŒå¤šæ³¨æ„åŠ›ï¼ˆSMAï¼‰Transformerå—ï¼Œå…·æœ‰åƒç´ æ³¨æ„åŠ›ã€é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›çš„ä¼˜ç‚¹ï¼Œç”¨äºç‰¹å¾å¢å¼ºï¼›äºŒæ˜¯è§£å†³æ³¨æ„åŠ›æœºåˆ¶è½¬æ¢å’Œç‰¹å¾èåˆè¿‡ç¨‹ä¸­ä¿¡æ¯æŸå¤±çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹å¾èåˆè°ƒåˆ¶å™¨ã€‚è¯¥æ¨¡å—é€šè¿‡å‡è½»é‡å¡‘å¼•èµ·çš„ä¿¡æ¯è¡°å‡ï¼ŒåŠ å¼ºäº†é€šé“å’Œç©ºé—´æ³¨æ„åŠ›ä¹‹é—´çš„é›†æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šé¡¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬å¤šå™¨å®˜ã€è‚è„è‚¿ç˜¤å’Œè†€èƒ±è‚¿ç˜¤åˆ†å‰²ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®ç½‘ç»œçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯å±•ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åˆ†å‰²å°å‹ã€å½¢çŠ¶ä¸è§„åˆ™çš„è‚¿ç˜¤æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>æå‡ºSMAFormeræ¶æ„ï¼Œèåˆå¤šç§æ³¨æ„åŠ›æœºåˆ¶ä»¥æ”¹è¿›å°å‹è‚¿ç˜¤å’Œå™¨å®˜çš„åˆ†å‰²ã€‚</li>
<li>SMAFormeråŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šååŒå¤šæ³¨æ„åŠ›ï¼ˆSMAï¼‰Transformerå—å’Œç‰¹å¾èåˆè°ƒåˆ¶å™¨ã€‚</li>
<li>SMATransformerå—èƒ½æ•æ‰åŒ»å­¦å›¾åƒçš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>ç‰¹å¾èåˆè°ƒåˆ¶å™¨è§£å†³äº†æ³¨æ„åŠ›æœºåˆ¶è½¬æ¢å’Œç‰¹å¾èåˆä¸­çš„ä¿¡æ¯æŸå¤±é—®é¢˜ã€‚</li>
<li>åœ¨å¤šå™¨å®˜ã€è‚è„è‚¿ç˜¤å’Œè†€èƒ±è‚¿ç˜¤åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå–å¾—æœ€æ–°æˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b3a3383326a512251c6b00fed09b3ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f24c3e2127a3318cd3a77d49288da59d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-665b56326451ef026c2753f3eabe9bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de89789ce56b5bcd333a9e53ee1a43ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6283d32b2744229b38adfb7888a46841.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-721458567a776343bb13c5f5ee26e647.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbd35af1843c621d6d41f7f25552ea06.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AI-in-radiological-imaging-of-soft-tissue-and-bone-tumours-a-systematic-review-evaluating-against-CLAIM-and-FUTURE-AI-guidelines"><a href="#AI-in-radiological-imaging-of-soft-tissue-and-bone-tumours-a-systematic-review-evaluating-against-CLAIM-and-FUTURE-AI-guidelines" class="headerlink" title="AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines"></a>AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines</h2><p><strong>Authors:Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. GruÃ¼hagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans</strong></p>
<p>Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review provides an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods. The review covered literature from several bibliographic databases, including papers published before 17&#x2F;07&#x2F;2024. Original research in peer-reviewed journals focused on radiology-based AI for diagnosing or prognosing primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers for eligibility. Eligible papers were assessed against guidelines by one of three independent reviewers. The search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, explainability), evaluation (e.g. evaluating and addressing biases, evaluating AI against best practices), and data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods. </p>
<blockquote>
<p>è½¯ç»„ç»‡ä¸éª¨è‚¿ç˜¤ï¼ˆSTBTï¼‰æ˜¯ç½•è§ä¸”è¯Šæ–­å›°éš¾çš„ç—…å˜ï¼Œå…¶ä¸´åºŠè¡Œä¸ºå’Œæ²»ç–—æ–¹å¼å„å¼‚ã€‚è¿™ç¯‡ç³»ç»Ÿæ€§ç»¼è¿°æ¦‚è¿°äº†ä½¿ç”¨æ”¾å°„å½±åƒå­¦è¿›è¡Œè¯Šæ–­ä¸é¢„åé¢„æµ‹çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–¹æ³•ï¼Œå¼ºè°ƒäº†è¿™äº›è‚¿ç˜¤åœ¨ä¸´åºŠç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶è¯„ä¼°äº†ç ”ç©¶æ˜¯å¦ç¬¦åˆåŒ»å­¦å½±åƒäººå·¥æ™ºèƒ½æ ¸æŸ¥è¡¨ï¼ˆCLAIMï¼‰ä»¥åŠå¯ä¿¡ä¸”å¯éƒ¨ç½²çš„äººå·¥æ™ºèƒ½çš„FUTURE-AIå›½é™…å…±è¯†æŒ‡å—ï¼Œä»¥ä¿ƒè¿›AIæ–¹æ³•çš„ä¸´åºŠç¿»è¯‘ã€‚ç»¼è¿°æ¶µç›–äº†æ¥è‡ªå¤šä¸ªæ–‡çŒ®æ•°æ®åº“çš„æ–‡çŒ®ï¼ŒåŒ…æ‹¬åœ¨2024å¹´7æœˆ17æ—¥ä¹‹å‰å‘è¡¨çš„æ–‡ç« ã€‚çº³å…¥çš„æ˜¯ç»è¿‡åŒè¡Œè¯„å®¡çš„åŸåˆ›ç ”ç©¶ï¼Œè¿™äº›ç ”ç©¶ä¾§é‡äºåŸºäºæ”¾å°„å­¦çš„AIç”¨äºåŸå‘æ€§STBTçš„è¯Šæ–­æˆ–é¢„åé¢„æµ‹ã€‚æ’é™¤æ ‡å‡†æ˜¯åŠ¨ç‰©ã€å°¸ä½“æˆ–å®éªŒå®¤ç ”ç©¶ï¼Œä»¥åŠéè‹±æ–‡æ–‡ç« ã€‚æ‘˜è¦ç”±ä¸‰åç‹¬ç«‹è¯„å®¡å‘˜ä¸­çš„ä¸¤åè¿›è¡Œèµ„æ ¼ç­›é€‰ã€‚ç¬¦åˆèµ„æ ¼çš„è®ºæ–‡ç”±ä¸‰åç‹¬ç«‹è¯„å®¡å‘˜ä¸­çš„ä¸€åæ ¹æ®æŒ‡å—è¿›è¡Œè¯„ä¼°ã€‚æœç´¢å…±ç¡®å®šäº†15ï¼Œ015ç¯‡æ‘˜è¦ï¼Œå…¶ä¸­325ç¯‡æ–‡ç« è¢«çº³å…¥è¯„ä¼°ã€‚å¤§å¤šæ•°ç ”ç©¶åœ¨CLAIMä¸Šçš„è¡¨ç°ä¸­ç­‰ï¼Œå¹³å‡å¾—åˆ†ä¸º28.9Â±7.5ï¼ˆæ»¡åˆ†53åˆ†ï¼‰ï¼Œä½†åœ¨FUTURE-AIä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œå¹³å‡å¾—åˆ†ä¸º5.1Â±2.1ï¼ˆæ»¡åˆ†30åˆ†ï¼‰ã€‚é’ˆå¯¹STBTçš„æˆåƒAIå·¥å…·ä»å¤„äºæ¦‚å¿µéªŒè¯é˜¶æ®µï¼Œè¡¨æ˜è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœªæ¥äººå·¥æ™ºèƒ½å¼€å‘è€…çš„åŠªåŠ›åº”é›†ä¸­åœ¨è®¾è®¡ï¼ˆä¾‹å¦‚ç¡®å®šæœªæ»¡è¶³çš„ä¸´åºŠéœ€æ±‚ã€é¢„æœŸçš„ä¸´åºŠç¯å¢ƒä»¥åŠAIå¦‚ä½•èå…¥ä¸´åºŠå·¥ä½œæµç¨‹ï¼‰ã€å¼€å‘ï¼ˆä¾‹å¦‚åœ¨ä¹‹å‰çš„å·¥ä½œåŸºç¡€ä¸Šè¿›è¡Œå»ºè®¾ã€è§£é‡Šæ€§ï¼‰ã€è¯„ä¼°ï¼ˆä¾‹å¦‚è¯„ä¼°å’Œè§£å†³åè§ã€æŒ‰ç…§æœ€ä½³å®è·µè¯„ä¼°AIï¼‰ï¼Œä»¥åŠæ•°æ®å¯é‡å¤æ€§å’Œå¯ç”¨æ€§ï¼ˆå…¬å¼€æä¾›æœ‰è®°å½•çš„ä»£ç å’Œæ•°æ®ï¼‰ã€‚éµå¾ªè¿™äº›å»ºè®®å¯èƒ½ä¼šæ”¹å–„AIæ–¹æ³•çš„ä¸´åºŠç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12491v2">PDF</a> 25 pages, 6 figures, 8 supplementary figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†ä½¿ç”¨æ”¾å°„å½±åƒå­¦è¿›è¡Œè½¯ç»„ç»‡ä¸éª¨è‚¿ç˜¤ï¼ˆSTBTï¼‰è¯Šæ–­ä¸é¢„åçš„AIæ–¹æ³•ã€‚æ–‡ç« æ¦‚è¿°äº†AIåœ¨STBTè¯Šæ–­ä¸é¢„åæ–¹é¢çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†ä¸´åºŠç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶è¯„ä¼°äº†ç ”ç©¶æ˜¯å¦ç¬¦åˆåŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½ï¼ˆCLAIMï¼‰æ¸…å•å’Œæœªæ¥äººå·¥æ™ºèƒ½ï¼ˆFUTURE-AIï¼‰å›½é™…å…±è¯†æŒ‡å—ã€‚æ–‡ç« æ¶µç›–äº†å¤šä¸ªæ–‡çŒ®æ•°æ®åº“çš„æ–‡çŒ®ï¼ŒåŒ…æ‹¬è‡³XXXXå¹´XXæœˆXXæ—¥å‰å‘è¡¨çš„è®ºæ–‡ã€‚ç ”ç©¶ä¸­ï¼Œç»è¿‡ç­›é€‰ï¼Œæœ€ç»ˆæœ‰XXç¯‡æ–‡ç« ç¬¦åˆæ¡ä»¶ã€‚å¤§éƒ¨åˆ†ç ”ç©¶åœ¨CLAIMä¸Šçš„è¡¨ç°æ˜¯ä¸­ç­‰çš„ï¼Œå¹³å‡å¾—åˆ†ä¸ºXXÂ±Xåˆ†ï¼ˆæ»¡åˆ†XXåˆ†ï¼‰ï¼Œä½†åœ¨FUTURE-AIä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œå¹³å‡å¾—åˆ†ä¸ºXÂ±Xåˆ†ï¼ˆæ»¡åˆ†XXåˆ†ï¼‰ã€‚å¯¹äºSTBTçš„æˆåƒäººå·¥æ™ºèƒ½å·¥å…·ä»å¤„äºæ¦‚å¿µéªŒè¯é˜¶æ®µï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœªæ¥äººå·¥æ™ºèƒ½å¼€å‘è€…åº”åœ¨è®¾è®¡ã€å¼€å‘ã€è¯„ä¼°å’Œæ•°æ®å¯é‡å¤æ€§ç­‰æ–¹é¢åŠ å¤§æŠ•å…¥ï¼Œä»¥ä¿ƒè¿›äººå·¥æ™ºèƒ½åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚æ€»ä¹‹ï¼Œé€šè¿‡éµå¾ªè¿™äº›å»ºè®®ï¼Œäººå·¥æ™ºèƒ½æ–¹æ³•çš„ä¸´åºŠè½¬åŒ–å°†å¾—åˆ°æ”¹è¿›ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AIåœ¨STBTè¯Šæ–­å’Œæ²»ç–—ä¸­çš„åº”ç”¨å¾—åˆ°äº†ç³»ç»Ÿç»¼è¿°ï¼Œå¼ºè°ƒäº†å…¶åœ¨ä¸´åºŠç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶ç¬¦åˆåŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½ï¼ˆCLAIMï¼‰æ¸…å•å’Œæœªæ¥äººå·¥æ™ºèƒ½ï¼ˆFUTURE-AIï¼‰å›½é™…å…±è¯†æŒ‡å—çš„è¯„ä»·æ ‡å‡†ã€‚</li>
<li>å¤§éƒ¨åˆ†ç ”ç©¶åœ¨CLAIMä¸Šçš„è¡¨ç°ä¸­ç­‰ï¼Œä½†åœ¨FUTURE-AIä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œæ˜¾ç¤ºå‡ºAIå·¥å…·åœ¨STBTé¢†åŸŸçš„æ”¹è¿›ç©ºé—´ã€‚</li>
<li>äººå·¥æ™ºèƒ½å·¥å…·ç›®å‰ä»å¤„äºæ¦‚å¿µéªŒè¯é˜¶æ®µï¼Œéœ€è¦è¿›ä¸€æ­¥æé«˜å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„é€‚ç”¨æ€§ã€‚</li>
<li>AIå¼€å‘è€…éœ€è¦åœ¨è®¾è®¡ã€å¼€å‘ã€è¯„ä¼°å’Œæ•°æ®å¯é‡å¤æ€§æ–¹é¢åŠ å¤§æŠ•å…¥ï¼Œä»¥ä¿ƒè¿›äººå·¥æ™ºèƒ½çš„ä¸´åºŠè½¬åŒ–ã€‚</li>
<li>å®šä¹‰æœªæ»¡è¶³çš„ä¸´åºŠéœ€æ±‚ã€é¢„æœŸçš„ä¸´åºŠç¯å¢ƒä»¥åŠAIå¦‚ä½•èå…¥ä¸´åºŠå·¥ä½œæµç¨‹å¯¹äºAIçš„è®¾è®¡è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6d1d90463beb6ebf6b91fc61e8d7f8d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing"><a href="#Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing" class="headerlink" title="Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing"></a>Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing</h2><p><strong>Authors:Alexandre Matov</strong></p>
<p>Methods: We have developed a software suite (DataSet Tracker) for real-time analysis designed to run on computers, smartphones, and smart glasses hardware and suitable for resource-constrained, on-the-fly computing in microscopes without internet connectivity; a demo is available for viewing at datasetanalysis.com. Our objective is to present the community with an integrated, easy to use by all, tool for resolving the complex dynamics of the cytoskeletal meshworks, intracytoplasmic membranous networks, and vesicle trafficking. Our software is optimized for resource-constrained computing and can be installed even on microscopes without internet connectivity.   Results: Our computational platform can provide high-content analyses and functional secondary screening of novel compounds that are in the process of approval, or at a pre-clinical stage of development, and putative combination therapies based on FDA-approved drugs. Importantly, dissecting the mechanisms of drug action with quantitative detail will allow the design of drugs that impede relapse and optimal dose regimens with minimal harmful side effects by carefully exploiting disease-specific aberrations.   Conclusions: DataSet Tracker, the real-time optical flow feature tracking software presented in this contribution, can serve as the base module of an integrated platform of existing and future algorithms for real-time cellular analysis. The computational assay we propose could successfully be applied to evaluate treatment strategies for any human organ. It is our goal to have this integrated tool approved for use in the clinical practice. </p>
<blockquote>
<p><strong>æ–¹æ³•</strong>ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å®æ—¶åˆ†æè½¯ä»¶å¥—ä»¶ï¼ˆDataSet Trackerï¼‰ï¼Œå¯åœ¨è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œç­‰ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œé€‚ç”¨äºæ— ç½‘ç»œè¿æ¥æ˜¾å¾®é•œä¸‹çš„èµ„æºå—é™ã€å³æ—¶è®¡ç®—ã€‚å¯ä»¥åœ¨datasetanalysis.comä¸ŠæŸ¥çœ‹æ¼”ç¤ºç‰ˆã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‘ç ”ç©¶ç¾¤ä½“æä¾›ä¸€ä¸ªé›†æˆå·¥å…·ï¼Œè§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€èƒè´¨å†…è†œç½‘ç»œå’Œå›Šæ³¡è½¬è¿çš„å¤æ‚åŠ¨æ€é—®é¢˜ï¼Œè¯¥å·¥å…·æ˜“äºæ‰€æœ‰äººä½¿ç”¨ã€‚æˆ‘ä»¬çš„è½¯ä»¶é’ˆå¯¹èµ„æºå—é™è®¡ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç”šè‡³å¯ä»¥åœ¨æ²¡æœ‰äº’è”ç½‘è¿æ¥çš„æ˜¾å¾®é•œä¸Šå®‰è£…ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>ç»“æœ</strong>ï¼šæˆ‘ä»¬çš„è®¡ç®—å¹³å°å¯ä»¥æä¾›é«˜å†…å®¹åˆ†æå’Œæ–°å‹åŒ–åˆç‰©çš„åŠŸèƒ½æ€§äºŒæ¬¡ç­›é€‰ï¼Œè¿™äº›åŒ–åˆç‰©å¯èƒ½å¤„äºæ‰¹å‡†è¿‡ç¨‹ä¸­æˆ–å¤„äºå¼€å‘é¢„ä¸´åºŠé˜¶æ®µï¼Œä»¥åŠåŸºäºFDAæ‰¹å‡†è¯ç‰©çš„æ½œåœ¨è”åˆç–—æ³•ã€‚é‡è¦çš„æ˜¯ï¼Œä»¥å®šé‡è¯¦ç»†çš„æ–¹å¼åˆ†æè¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œå°†æœ‰åŠ©äºè®¾è®¡èƒ½å¤Ÿé˜»ç¢å¤å‘çš„è¯ç‰©ï¼Œå¹¶å€ŸåŠ©å¯¹ç‰¹å®šç–¾ç—…å¼‚å¸¸çš„ç²¾ç»†åˆ©ç”¨ï¼Œåˆ¶å®šå…·æœ‰æœ€å°æœ‰å®³å‰¯ä½œç”¨çš„æœ€ä½³ç»™è¯æ–¹æ¡ˆã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15735v8">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®é›†è¿½è¸ªå™¨æ˜¯ä¸€æ¬¾ä¸ºå®æ—¶åˆ†æè€Œå¼€å‘çš„è½¯ä»¶å¥—ä»¶ï¼Œé€‚ç”¨äºè®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œç­‰ç¡¬ä»¶èµ„æºå—é™çš„æ˜¾å¾®é•œç¯å¢ƒã€‚è¯¥è½¯ä»¶æ—¨åœ¨è§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€ç»†èƒå†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“çš„å¤æ‚åŠ¨æ€é—®é¢˜ï¼Œå¹¶å¯ä½œä¸ºç°æœ‰å’Œæœªæ¥ç®—æ³•çš„é›†æˆå¹³å°ï¼Œç”¨äºå®æ—¶ç»†èƒåˆ†æã€‚è¯¥è®¡ç®—å¹³å°å¯ä»¥å¯¹å¤„äºæ‰¹å‡†è¿‡ç¨‹ä¸­æˆ–å¤„äºå¼€å‘é¢„ä¸´åºŠé˜¶æ®µçš„æ–°å‹åŒ–åˆç‰©è¿›è¡Œé«˜å†…å®¹åˆ†æå’ŒåŠŸèƒ½äºŒæ¬¡ç­›é€‰ï¼Œå¹¶é€šè¿‡å®šé‡è¯¦ç»†ç ”ç©¶è¯ç‰©ä½œç”¨æœºåˆ¶æ¥è®¾è®¡èƒ½å¤Ÿé˜»æ­¢å¤å‘çš„è¯ç‰©ï¼Œä»¥åŠåˆ¶å®šå…·æœ‰æœ€å°‘æœ‰å®³å‰¯ä½œç”¨çš„æœ€ä½³å‰‚é‡æ–¹æ¡ˆã€‚æ•°æ®é›†è¿½è¸ªå™¨æœ€ç»ˆç›®æ ‡æ˜¯è·å¾—æ‰¹å‡†ç”¨äºä¸´åºŠå®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è½¯ä»¶å¥—ä»¶DataSet Trackerç”¨äºå®æ—¶åˆ†æï¼Œå¯åœ¨è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œç­‰ç¡¬ä»¶ä¸Šè¿è¡Œã€‚</li>
<li>è¯¥è½¯ä»¶é€‚ç”¨äºèµ„æºå—é™çš„æ˜¾å¾®é•œç¯å¢ƒï¼Œæ— éœ€äº’è”ç½‘è¿æ¥ã€‚</li>
<li>DataSet Trackeræ—¨åœ¨è§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€ç»†èƒå†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“çš„å¤æ‚åŠ¨æ€é—®é¢˜ã€‚</li>
<li>è¯¥è®¡ç®—å¹³å°å¯ä»¥è¿›è¡Œé«˜å†…å®¹åˆ†æå’ŒåŠŸèƒ½äºŒæ¬¡ç­›é€‰ï¼ŒåŒ…æ‹¬å¯¹æ–°å‹åŒ–åˆç‰©çš„åˆ†æã€‚</li>
<li>é€šè¿‡å®šé‡è¯¦ç»†ç ”ç©¶è¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œå¯è®¾è®¡é˜»æ­¢å¤å‘çš„è¯ç‰©å¹¶åˆ¶å®šæœ€ä½³å‰‚é‡æ–¹æ¡ˆã€‚</li>
<li>DataSet Trackerå¯ä»¥ä½œä¸ºé›†æˆç°æœ‰å’Œæœªæ¥ç®—æ³•çš„å®æ—¶ç»†èƒåˆ†æçš„åŸºç¡€æ¨¡å—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94f1386177611dc8371b3965f53e31eb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CancerLLM-A-Large-Language-Model-in-Cancer-Domain"><a href="#CancerLLM-A-Large-Language-Model-in-Cancer-Domain" class="headerlink" title="CancerLLM: A Large Language Model in Cancer Domain"></a>CancerLLM: A Large Language Model in Cancer Domain</h2><p><strong>Authors:Mingchen Li, Jiatan Huang, Jeremy Yeung, Anne Blaes, Steven Johnson, Hongfang Liu, Hua Xu, Rui Zhang</strong></p>
<p>Medical Large Language Models (LLMs) have demonstrated impressive performance on a wide variety of medical NLP tasks; however, there still lacks a LLM specifically designed for phenotyping identification and diagnosis in cancer domain. Moreover, these LLMs typically have several billions of parameters, making them computationally expensive for healthcare systems. Thus, in this study, we propose CancerLLM, a model with 7 billion parameters and a Mistral-style architecture, pre-trained on nearly 2.7M clinical notes and over 515K pathology reports covering 17 cancer types, followed by fine-tuning on two cancer-relevant tasks, including cancer phenotypes extraction and cancer diagnosis generation. Our evaluation demonstrated that the CancerLLM achieves state-of-the-art results with F1 score of 91.78% on phenotyping extraction and 86.81% on disganois generation. It outperformed existing LLMs, with an average F1 score improvement of 9.23%. Additionally, the CancerLLM demonstrated its efficiency on time and GPU usage, and robustness comparing with other LLMs. We demonstrated that CancerLLM can potentially provide an effective and robust solution to advance clinical research and practice in cancer domain </p>
<blockquote>
<p>åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åŒ»ç–—NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼›ç„¶è€Œï¼Œé’ˆå¯¹ç™Œç—‡é¢†åŸŸçš„è¡¨å‹è¯†åˆ«å’Œè¯Šæ–­è®¾è®¡çš„LLMä»ç„¶ç¼ºä¹ã€‚æ­¤å¤–ï¼Œè¿™äº›LLMé€šå¸¸æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œä½¿å¾—å®ƒä»¬åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†CancerLLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰7äº¿ä¸ªå‚æ•°å’ŒMistralé£æ ¼æ¶æ„çš„æ¨¡å‹ï¼Œå®ƒåœ¨è¿‘270ä¸‡ä¸ªä¸´åºŠç¬”è®°å’Œè¶…è¿‡51.5ä¸‡ä¸ªæ¶µç›–17ç§ç™Œç—‡ç±»å‹çš„ç—…ç†æŠ¥å‘Šä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œéšåå¯¹ç™Œç—‡ç›¸å…³çš„ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼ŒåŒ…æ‹¬ç™Œç—‡è¡¨å‹æå–å’Œç™Œç—‡è¯Šæ–­ç”Ÿæˆã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒCancerLLMåœ¨è¡¨å‹æå–æ–¹é¢è¾¾åˆ°äº†91.78%çš„F1åˆ†æ•°ï¼Œåœ¨è¯Šæ–­ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†86.81%çš„å‡†ç¡®ç‡ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚å®ƒä¼˜äºç°æœ‰çš„LLMï¼Œå¹³å‡F1åˆ†æ•°æé«˜äº†9.23%ã€‚æ­¤å¤–ï¼ŒCancerLLMåœ¨æ—¶é—´å’ŒGPUä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºäº†å…¶æ•ˆç‡ï¼Œå¹¶ä¸”ä¸å…¶ä»–LLMç›¸æ¯”å…·æœ‰ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒCancerLLMæœ‰æœ›ä¸ºç™Œç—‡é¢†åŸŸçš„ä¸´åºŠç ”ç©¶å’Œå®è·µæä¾›æœ‰æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10459v3">PDF</a> new version, add the RAG version of cancerLLM</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç™Œç—‡é¢†åŸŸä¸“é—¨è®¾è®¡çš„Medical Large Language Modelï¼ˆLLMï¼‰â€”â€”CancerLLMã€‚è¯¥æ¨¡å‹å…·æœ‰7äº¿å‚æ•°å’ŒMistralé£æ ¼æ¶æ„ï¼Œåœ¨çº¦270ä¸‡ä»½ä¸´åºŠç¬”è®°å’Œè¶…è¿‡51.5ä¸‡ä»½ç—…ç†æŠ¥å‘Šä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–17ç§ç™Œç—‡ç±»å‹ã€‚é€šè¿‡å¾®è°ƒä¸¤ä¸ªä¸ç™Œç—‡ç›¸å…³çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç™Œç—‡è¡¨å‹æå–å’Œç™Œç—‡è¯Šæ–­ç”Ÿæˆï¼ŒCancerLLMå®ç°äº†å…ˆè¿›çš„ç»“æœï¼Œåœ¨è¡¨å‹æå–æ–¹é¢è¾¾åˆ°äº†91.78%çš„F1åˆ†æ•°ï¼Œåœ¨è¯Šæ–­ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†86.81%ã€‚ç›¸è¾ƒäºç°æœ‰LLMï¼ŒCancerLLMå¹³å‡F1åˆ†æ•°æé«˜äº†9.23%ï¼Œå¹¶åœ¨æ—¶é—´æ•ˆç‡å’ŒGPUä½¿ç”¨æ–¹é¢å±•ç°äº†ä¼˜åŠ¿ï¼Œå…·æœ‰é²æ£’æ€§ã€‚è¯¥æ¨¡å‹ä¸ºç™Œç—‡é¢†åŸŸçš„ä¸´åºŠç ”ç©¶å’Œå®è·µæä¾›äº†æœ‰æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CancerLLMæ˜¯ä¸“é—¨ä¸ºç™Œç—‡é¢†åŸŸè®¾è®¡çš„åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰7äº¿å‚æ•°å’ŒMistralé£æ ¼æ¶æ„ã€‚</li>
<li>CancerLLMç»è¿‡åœ¨å¤§é‡ä¸´åºŠç¬”è®°å’Œç—…ç†æŠ¥å‘Šä¸Šçš„é¢„è®­ç»ƒï¼Œæ¶µç›–17ç§ç™Œç—‡ç±»å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒï¼ŒCancerLLMåœ¨ç™Œç—‡è¡¨å‹æå–å’Œè¯Šæ–­ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å…ˆè¿›ç»“æœã€‚</li>
<li>CancerLLMç›¸è¾ƒäºç°æœ‰LLMï¼ŒF1åˆ†æ•°å¹³å‡æé«˜äº†9.23%ã€‚</li>
<li>CancerLLMåœ¨æ—¶é—´å’ŒGPUä½¿ç”¨æ•ˆç‡æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3d5c46fb9c8fe40a88a43e903d67e98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfbfd241114394d7e140ea8422616d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc0b0485bd8369070d1f057917b30d6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cfbbe5e4bd956df02d5e594dd67f67e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a73293013ed06512ebbe50d98ca8336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1e20978fae82f3d29f90a1b50c1ea21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10d0eb1c7a7e7a285ad6e519ff9f5a07.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CARL-A-Framework-for-Equivariant-Image-Registration"><a href="#CARL-A-Framework-for-Equivariant-Image-Registration" class="headerlink" title="CARL: A Framework for Equivariant Image Registration"></a>CARL: A Framework for Equivariant Image Registration</h2><p><strong>Authors:Hastings Greer, Lin Tian, Francois-Xavier Vialard, Roland Kwitt, Raul San Jose Estepar, Marc Niethammer</strong></p>
<p>Image registration estimates spatial correspondences between a pair of images. These estimates are typically obtained via numerical optimization or regression by a deep network. A desirable property of such estimators is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images. Formally, the estimator should be equivariant to a desired class of image transformations. In this work, we present careful analyses of the desired equivariance properties in the context of multi-step deep registration networks. Based on these analyses we 1) introduce the notions of $[U,U]$ equivariance (network equivariance to the same deformations of the input images) and $[W,U]$ equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall $[W,U]$ equivariance if the first step has $[W,U]$ equivariance and all others have $[U,U]$ equivariance; we 3) show that common displacement-predicting networks only exhibit $[U,U]$ equivariance to translations instead of the more powerful $[W,U]$ equivariance; and we 4) show how to achieve multi-step $[W,U]$ equivariance via a coordinate-attention mechanism combined with displacement-predicting refinement layers (CARL). Overall, our approach obtains excellent practical registration performance on several 3D medical image registration tasks and outperforms existing unsupervised approaches for the challenging problem of abdomen registration. </p>
<blockquote>
<p>å›¾åƒé…å‡†ä¼°è®¡ä¸€å¯¹å›¾åƒä¹‹é—´çš„ç©ºé—´å¯¹åº”å…³ç³»ã€‚è¿™äº›ä¼°è®¡é€šå¸¸é€šè¿‡æ•°å€¼ä¼˜åŒ–æˆ–æ·±åº¦ç½‘ç»œçš„å›å½’æ¥è·å¾—ã€‚æ­¤ç±»ä¼°è®¡å™¨çš„ä¸€ä¸ªç†æƒ³ç‰¹æ€§æ˜¯ï¼Œåœ¨è¾“å…¥å›¾åƒå˜å½¢çš„æƒ…å†µä¸‹ï¼Œä¸€å¯¹å›¾åƒçš„å¯¹åº”ä¼°è®¡ï¼ˆä¾‹å¦‚ï¼ŒçœŸå®å¯¹åº”ï¼‰ä»ç„¶ä¿æŒä¸å˜ã€‚åœ¨å½¢å¼ä¸Šï¼Œä¼°è®¡å™¨åº”å¯¹æ‰€éœ€çš„å›¾åƒè½¬æ¢ç±»åˆ«å…·æœ‰ç­‰å˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¤šæ­¥æ·±åº¦é…å‡†ç½‘ç»œçš„æ‰€éœ€ç­‰å˜æ€§å±æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬1ï¼‰ä»‹ç»äº†$[U,U]$ç­‰å˜æ€§ï¼ˆç½‘ç»œå¯¹è¾“å…¥å›¾åƒçš„ç›¸åŒå˜å½¢çš„ç­‰å˜æ€§ï¼‰å’Œ$[W,U]$ç­‰å˜æ€§ï¼ˆè¾“å…¥å›¾åƒå¯ä»¥ç»å†ä¸åŒçš„å˜å½¢ï¼‰çš„æ¦‚å¿µï¼›æˆ‘ä»¬2ï¼‰è¡¨æ˜ï¼Œåœ¨åˆé€‚çš„å¤šæ­¥é…å‡†è®¾ç½®ä¸­ï¼Œå¦‚æœç¬¬ä¸€æ­¥å…·æœ‰$[W,U]$ç­‰å˜æ€§ï¼Œè€Œå…¶ä»–æ‰€æœ‰æ­¥éª¤éƒ½å…·æœ‰$[U,U]$ç­‰å˜æ€§ï¼Œé‚£ä¹ˆå¯¹äºæ•´ä½“è€Œè¨€ï¼Œ$[W,U]$ç­‰å˜æ€§å°±è¶³å¤Ÿäº†ï¼›æˆ‘ä»¬3ï¼‰è¡¨æ˜ï¼Œå¸¸è§çš„ä½ç§»é¢„æµ‹ç½‘ç»œä»…å¯¹å¹³ç§»å…·æœ‰$[U,U]$ç­‰å˜æ€§ï¼Œè€Œä¸æ˜¯æ›´å¼ºå¤§çš„$[W,U]$ç­‰å˜æ€§ï¼›æˆ‘ä»¬4ï¼‰å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç»“åˆåæ ‡æ³¨æ„æœºåˆ¶ä¸ä½ç§»é¢„æµ‹ç»†åŒ–å±‚ï¼ˆCARLï¼‰æ¥å®ç°å¤šæ­¥$[W,U]$ç­‰å˜æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ª3DåŒ»å­¦å›¾åƒé…å‡†ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„å®é™…é…å‡†æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è…¹éƒ¨é…å‡†é—®é¢˜ä¸Šä¼˜äºç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16738v3">PDF</a> CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å›¾åƒé…å‡†ä¼°è®¡ä¸€å¯¹å›¾åƒä¹‹é—´çš„ç©ºé—´å¯¹åº”å…³ç³»ã€‚è¿™äº›ä¼°è®¡é€šå¸¸é€šè¿‡æ•°å€¼ä¼˜åŒ–æˆ–æ·±åº¦ç½‘ç»œçš„å›å½’è·å¾—ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™ç§ä¼°è®¡å™¨åº”èƒ½åœ¨è¾“å…¥å›¾åƒå˜å½¢çš„æƒ…å†µä¸‹ä¿æŒä¸€å¯¹å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»ä¸å˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¼°è®¡å™¨åº”å¯¹æœŸæœ›çš„å›¾åƒå˜æ¢ç±»å…·æœ‰ç­‰å˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¤šæ­¥æ·±åº¦é…å‡†ç½‘ç»œçš„æœŸæœ›ç­‰å˜æ€§å±æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬ï¼š1ï¼‰å¼•å…¥äº†$[U,U]$ç­‰å˜æ€§ï¼ˆç½‘ç»œå¯¹è¾“å…¥å›¾åƒçš„ç›¸åŒå˜å½¢çš„ç­‰å˜æ€§ï¼‰å’Œ$[W,U]$ç­‰å˜æ€§ï¼ˆè¾“å…¥å›¾åƒå¯ä»¥ç»å†ä¸åŒçš„å˜å½¢ï¼‰çš„æ¦‚å¿µï¼›2ï¼‰è¡¨æ˜åœ¨åˆé€‚çš„å¤šæ­¥é…å‡†è®¾ç½®ä¸­ï¼Œå¦‚æœç¬¬ä¸€æ­¥å…·æœ‰$[W,U]$ç­‰å˜æ€§ï¼Œå…¶ä½™æ­¥éª¤å…·æœ‰$[U,U]$ç­‰å˜æ€§ï¼Œé‚£ä¹ˆæ•´ä½“çš„$[W,U]$ç­‰å˜æ€§å°±è¶³å¤Ÿäº†ï¼›3ï¼‰è¡¨æ˜å¸¸è§çš„ä½ç§»é¢„æµ‹ç½‘ç»œä»…å¯¹å¹³ç§»å…·æœ‰$[U,U]$ç­‰å˜æ€§ï¼Œè€Œä¸æ˜¯æ›´å¼ºå¤§çš„$[W,U]$ç­‰å˜æ€§ï¼›4ï¼‰å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç»“åˆåæ ‡æ³¨æ„æœºåˆ¶ä¸ä½ç§»é¢„æµ‹ç»†åŒ–å±‚ï¼ˆCARLï¼‰æ¥å®ç°å¤šæ­¥$[W,U]$ç­‰å˜æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä¸‰ç»´åŒ»å­¦å›¾åƒé…å‡†ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„å®é™…é…å‡†æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è…¹éƒ¨é…å‡†é—®é¢˜ä¸Šä¼˜äºç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>å›¾åƒé…å‡†ä¼°è®¡å›¾åƒé—´çš„ç©ºé—´å¯¹åº”å…³ç³»ï¼Œé€šå¸¸é€šè¿‡æ•°å€¼ä¼˜åŒ–æˆ–æ·±åº¦ç½‘ç»œå®ç°ã€‚</li>
<li>ç†æƒ³æƒ…å†µä¸‹ï¼Œé…å‡†ä¼°è®¡å™¨åœ¨è¾“å…¥å›¾åƒå˜å½¢çš„æƒ…å†µä¸‹åº”ä¿æŒå¯¹åº”å…³ç³»ä¸å˜ã€‚è¿™è¦æ±‚ä¼°è®¡å™¨å¯¹ç‰¹å®šçš„å›¾åƒå˜æ¢å…·æœ‰ç­‰å˜æ€§ã€‚</li>
<li>å¼•å…¥$[U,U]$ç­‰å˜æ€§å’Œ$[W,U]$ç­‰å˜æ€§çš„æ¦‚å¿µã€‚</li>
<li>åœ¨å¤šæ­¥é…å‡†ä¸­ï¼Œè‹¥ç¬¬ä¸€æ­¥å…·å¤‡$[W,U]$ç­‰å˜æ€§ä¸”å…¶ä½™æ­¥éª¤å…·å¤‡$[U,U]$ç­‰å˜æ€§ï¼Œåˆ™è¶³ä»¥å®ç°æ•´ä½“çš„$[W,U]$ç­‰å˜æ€§ã€‚</li>
<li>å¸¸è§çš„ä½ç§»é¢„æµ‹ç½‘ç»œä»…å¯¹å¹³ç§»å…·æœ‰$[U,U]$ç­‰å˜æ€§ï¼Œç¼ºä¹æ›´é«˜çº§çš„$[W,U]$ç­‰å˜æ€§ã€‚</li>
<li>æå‡ºç»“åˆåæ ‡æ³¨æ„æœºåˆ¶ä¸ä½ç§»é¢„æµ‹ç»†åŒ–å±‚ï¼ˆCARLï¼‰çš„æ–¹æ³•ï¼Œå®ç°å¤šæ­¥$[W,U]$ç­‰å˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7cdcb1d7cb5c19901efcd5761b6cd71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16f4b33c2be461c4d0b0b1c9c2f6fffb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Image-segmentation-of-treated-and-untreated-tumor-spheroids-by-Fully-Convolutional-Networks"><a href="#Image-segmentation-of-treated-and-untreated-tumor-spheroids-by-Fully-Convolutional-Networks" class="headerlink" title="Image segmentation of treated and untreated tumor spheroids by Fully   Convolutional Networks"></a>Image segmentation of treated and untreated tumor spheroids by Fully   Convolutional Networks</h2><p><strong>Authors:Matthias Streller, SoÅˆa MichlÃ­kovÃ¡, Willy Ciecior, Katharina LÃ¶nnecke, Leoni A. Kunz-Schughart, Steffen Lange, Anja Voss-BÃ¶hme</strong></p>
<p>Multicellular tumor spheroids (MCTS) are advanced cell culture systems for assessing the impact of combinatorial radio(chemo)therapy. They exhibit therapeutically relevant in-vivo-like characteristics from 3D cell-cell and cell-matrix interactions to radial pathophysiological gradients related to proliferative activity and nutrient&#x2F;oxygen supply, altering cellular radioresponse. State-of-the-art assays quantify long-term curative endpoints based on collected brightfield image time series from large treated spheroid populations per irradiation dose and treatment arm. Here, spheroid control probabilities are documented analogous to in-vivo tumor control probabilities based on Kaplan-Meier curves. This analyses require laborious spheroid segmentation of up to 100.000 images per treatment arm to extract relevant structural information from the images, e.g., diameter, area, volume and circularity. While several image analysis algorithms are available for spheroid segmentation, they all focus on compact MCTS with clearly distinguishable outer rim throughout growth. However, treated MCTS may partly be detached and destroyed and are usually obscured by dead cell debris. We successfully train two Fully Convolutional Networks, UNet and HRNet, and optimize their hyperparameters to develop an automatic segmentation for both untreated and treated MCTS. We systematically validate the automatic segmentation on larger, independent data sets of spheroids derived from two human head-and-neck cancer cell lines. We find an excellent overlap between manual and automatic segmentation for most images, quantified by Jaccard indices at around 90%. For images with smaller overlap of the segmentations, we demonstrate that this error is comparable to the variations across segmentations from different biological experts, suggesting that these images represent biologically unclear or ambiguous cases. </p>
<blockquote>
<p>å¤šç»†èƒè‚¿ç˜¤çƒä½“ï¼ˆMCTSï¼‰æ˜¯è¯„ä¼°ç»„åˆæ”¾ç–—ï¼ˆåŒ–å­¦ç–—æ³•ï¼‰å½±å“çš„å…ˆè¿›ç»†èƒåŸ¹å…»ç³»ç»Ÿã€‚å®ƒä»¬å±•ç°å‡ºä¸ä½“å†…ç¯å¢ƒç›¸å…³çš„æ²»ç–—ç‰¹æ€§ï¼Œä»ä¸‰ç»´ç»†èƒä¸ç»†èƒå¤–åŸºè´¨é—´çš„ç›¸äº’ä½œç”¨åˆ°ä¸å¢æ®–æ´»æ€§ã€è¥å…»ç‰©&#x2F;æ°§æ°”ä¾›åº”ç›¸å…³çš„å¾„å‘ç—…ç†ç”Ÿç†æ¢¯åº¦å˜åŒ–ï¼Œè¿™äº›å› ç´ éƒ½å½±å“ç»†èƒçš„æ”¾å°„ååº”ã€‚æœ€æ–°çš„å®éªŒæ–¹æ³•åŸºäºä»æ¯ä¸ªè¾å°„å‰‚é‡å’Œæ²»ç–—ç»„æ”¶é›†çš„æ˜è§†é‡å›¾åƒæ—¶é—´åºåˆ—ï¼Œå¯¹é•¿æœŸæ²»ç–—æ•ˆæœç»ˆç‚¹è¿›è¡Œé‡åŒ–è¯„ä¼°å¤§å‹å¤„ç†çƒä½“äººç¾¤ã€‚åœ¨æ­¤ï¼Œçƒä½“æ§åˆ¶æ¦‚ç‡çš„è®°å½•æ–¹å¼ä¸ä½“å†…è‚¿ç˜¤æ§åˆ¶æ¦‚ç‡ç±»ä¼¼ï¼ŒåŸºäºKaplan-Meieræ›²çº¿ã€‚è¿™äº›åˆ†æéœ€è¦ç¹ççš„çƒä½“åˆ†å‰²å·¥ä½œï¼Œæ¯ä¸ªæ²»ç–—ç»„å¯èƒ½éœ€è¦å¤„ç†é«˜è¾¾10ä¸‡å¼ å›¾åƒï¼Œä»¥ä»å›¾åƒä¸­æå–ç›¸å…³çš„ç»“æ„ä¿¡æ¯ï¼Œä¾‹å¦‚ç›´å¾„ã€é¢ç§¯ã€ä½“ç§¯å’Œåœ†åº¦ã€‚è™½ç„¶æœ‰å‡ ä¸ªå›¾åƒåˆ†æç®—æ³•å¯ç”¨äºçƒä½“åˆ†å‰²ï¼Œä½†å®ƒä»¬éƒ½ä¾§é‡äºç´§å‡‘çš„MCTSï¼Œåœ¨å…¶ç”Ÿé•¿è¿‡ç¨‹ä¸­å¤–å›´è½®å»“æ¸…æ™°å¯è¾¨ã€‚ç„¶è€Œï¼Œç»è¿‡å¤„ç†çš„MCTSå¯èƒ½ä¼šéƒ¨åˆ†è„±è½å’Œç ´åï¼Œé€šå¸¸ä¼šè¢«æ­»ç»†èƒç¢ç‰‡é®è”½ã€‚æˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†ä¸¤ä¸ªå…¨å·ç§¯ç½‘ç»œï¼Œå³UNetå’ŒHRNetï¼Œå¹¶ä¼˜åŒ–äº†å…¶è¶…å‚æ•°ï¼Œä»¥å®ç°å¯¹æœªå¤„ç†å’Œç»è¿‡å¤„ç†çš„MCTSçš„è‡ªåŠ¨åˆ†å‰²ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªä¸¤ä¸ªäººå¤´é¢ˆéƒ¨ç™Œç»†èƒç³»çš„æ›´å¤§ç‹¬ç«‹æ•°æ®é›†çš„çƒä½“è¿›è¡Œäº†è‡ªåŠ¨åˆ†å‰²çš„ç³»ç»ŸéªŒè¯ã€‚æˆ‘ä»¬å‘ç°å¤§å¤šæ•°å›¾åƒçš„è‡ªåŠ¨åˆ†å‰²ä¸æ‰‹åŠ¨åˆ†å‰²ä¹‹é—´æœ‰ç€æå¥½çš„é‡å ç¨‹åº¦ï¼ŒJaccardæŒ‡æ•°çº¦ä¸º90%ã€‚å¯¹äºåˆ†å‰²é‡å è¾ƒå°‘çš„å›¾åƒï¼Œæˆ‘ä»¬è¯æ˜è¿™ç§é”™è¯¯ä¸ä¸åŒç”Ÿç‰©å­¦ä¸“å®¶ä¹‹é—´çš„åˆ†å‰²å·®å¼‚ç›¸å½“ï¼Œè¿™è¡¨æ˜è¿™äº›å›¾åƒä»£è¡¨äº†ç”Ÿç‰©å­¦ä¸Šä¸æ˜ç¡®æˆ–æ¨¡ç³Šçš„æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.01105v3">PDF</a> 30 pages, 23 figures</p>
<p><strong>æ‘˜è¦</strong><br>     å¤šç»†èƒè‚¿ç˜¤çƒçŠ¶ä½“ï¼ˆMCTSï¼‰æ˜¯è¯„ä¼°ç»„åˆæ”¾ç–—ï¼ˆåŒ–ç–—ï¼‰å½±å“çš„é«˜çº§ç»†èƒåŸ¹å…»ç³»ç»Ÿã€‚å®ƒä»¬å±•ç°å‡ºä»ä¸‰ç»´ç»†èƒé—´ä¸ç»†èƒåŸºè´¨ç›¸äº’ä½œç”¨åˆ°ä¸å¢æ®–æ´»æ€§ã€è¥å…»&#x2F;æ°§æ°”ä¾›åº”ç›¸å…³çš„å¾„å‘ç—…ç†ç”Ÿç†æ¢¯åº¦çš„æ²»ç–—ç›¸å…³ä½“å†…ç±»ä¼¼ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§æ”¹å˜äº†ç»†èƒçš„æ”¾å°„ååº”ã€‚åŸºäºä»å¤§å‹å¤„ç†çƒçŠ¶ä½“ç¾¤ä½“æ”¶é›†çš„æ˜è§†é‡å›¾åƒæ—¶é—´åºåˆ—ï¼Œå½“å‰æœ€å…ˆè¿›çš„å®éªŒé•¿æœŸç–—æ•ˆè¯„ä¼°ç«¯ç‚¹è®°å½•ç±»ä¼¼äºä½“å†…è‚¿ç˜¤æ§åˆ¶æ¦‚ç‡çš„çƒçŠ¶ä½“æ§åˆ¶æ¦‚ç‡ã€‚ä½†è¿™ç§åˆ†æéœ€è¦ç¹ççš„çƒçŠ¶ä½“åˆ†å‰²ï¼Œæœ€å¤šå¯è¾¾æ¯ä¸ªæ²»ç–—æ‰‹è‡‚çš„10ä¸‡å¼ å›¾åƒï¼Œä»¥ä»å›¾åƒä¸­æå–ç›¸å…³çš„ç»“æ„ä¿¡æ¯ï¼Œä¾‹å¦‚ç›´å¾„ã€é¢ç§¯ã€ä½“ç§¯å’Œåœ†åº¦ã€‚è™½ç„¶æœ‰å¤šç§å›¾åƒåˆ†æç®—æ³•å¯ç”¨äºçƒçŠ¶ä½“åˆ†å‰²ï¼Œä½†å®ƒä»¬ä¸»è¦é’ˆå¯¹ç´§å‡‘çš„MCTSï¼Œç”Ÿé•¿è¿‡ç¨‹ä¸­å…·æœ‰æ¸…æ™°å¯è¾¨çš„å¤–ç¼˜ã€‚ç„¶è€Œï¼Œç»è¿‡å¤„ç†çš„MCTSå¯èƒ½ä¼šéƒ¨åˆ†è„±è½å’Œç ´åï¼Œé€šå¸¸ä¼šè¢«æ­»ç»†èƒç¢ç‰‡é®è”½ã€‚æˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†ä¸¤ä¸ªå…¨å·ç§¯ç½‘ç»œUNetå’ŒHRNetå¹¶ä¼˜åŒ–äº†å…¶è¶…å‚æ•°ä»¥å®ç°å¯¹æœªå¤„ç†å’Œç»è¿‡å¤„ç†çš„MCTSçš„è‡ªåŠ¨åˆ†å‰²ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªä¸¤ç§äººå¤´é¢ˆéƒ¨ç™Œç»†èƒç³»çš„ç‹¬ç«‹æ•°æ®é›†ä¸Šçš„çƒçŠ¶ä½“è¿›è¡Œäº†ç³»ç»ŸéªŒè¯ã€‚æˆ‘ä»¬å‘ç°å¤§å¤šæ•°å›¾åƒçš„è‡ªåŠ¨åˆ†å‰²ä¸æ‰‹åŠ¨åˆ†å‰²ä¹‹é—´æœ‰å¾ˆå¥½çš„é‡å ï¼Œé›…å¡å°”æŒ‡æ•°çº¦ä¸º90%ã€‚å¯¹äºåˆ†å‰²é‡å è¾ƒå°çš„å›¾åƒï¼Œæˆ‘ä»¬è¯æ˜è¿™ç§è¯¯å·®ä¸ä¸åŒç”Ÿç‰©å­¦ä¸“å®¶ä¹‹é—´çš„åˆ†å‰²å˜åŒ–ç›¸å½“ï¼Œè¿™è¡¨æ˜è¿™äº›å›¾åƒä»£è¡¨äº†ç”Ÿç‰©å­¦ä¸Šä¸æ¸…æ¥šæˆ–æ¨¡ç³Šçš„æƒ…å†µã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šç»†èƒè‚¿ç˜¤çƒçŠ¶ä½“ï¼ˆMCTSï¼‰æ˜¯è¯„ä¼°ç»„åˆæ”¾ç–—å’ŒåŒ–ç–—æ•ˆæœçš„é«˜çº§ç»†èƒåŸ¹å…»ç³»ç»Ÿï¼Œå…·æœ‰ä½“å†…ç±»ä¼¼çš„ç‰¹æ€§ã€‚</li>
<li>å½“å‰åˆ†æéœ€è¦ç¹ççš„çƒçŠ¶ä½“åˆ†å‰²ä»¥æå–ç›¸å…³ç»“æ„ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰çš„å›¾åƒåˆ†æç®—æ³•ä¸»è¦é’ˆå¯¹å…·æœ‰æ¸…æ™°å¤–ç¼˜çš„ç´§å‡‘MCTSã€‚</li>
<li>ç»è¿‡æ²»ç–—çš„MCTSå¯èƒ½éƒ¨åˆ†è„±è½å’Œç ´åï¼Œå¹¶è¢«æ­»ç»†èƒç¢ç‰‡é®è”½ã€‚</li>
<li>è®­ç»ƒäº†UNetå’ŒHRNetä¸¤ä¸ªå…¨å·ç§¯ç½‘ç»œè¿›è¡Œè‡ªåŠ¨åˆ†å‰²ã€‚</li>
<li>åœ¨ç‹¬ç«‹æ•°æ®é›†ä¸ŠéªŒè¯äº†è‡ªåŠ¨åˆ†å‰²çš„æ•ˆæœï¼Œå‘ç°å¤§å¤šæ•°å›¾åƒçš„è‡ªåŠ¨åˆ†å‰²ä¸æ‰‹åŠ¨åˆ†å‰²ä¹‹é—´æœ‰å¾ˆå¥½çš„é‡å ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.01105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9f36f6157eb2085bbd9e9b5ced1fd2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec47c5e4fae1b6133839b6aeb6e8d6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc1168d0d254624ea12680d1f2bc8251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c322d4f242a1c80139c2c4af27ed2dd.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain"><a href="#Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain" class="headerlink" title="Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain"></a>Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain</h2><p><strong>Authors:Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath</strong></p>
<p>Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. The method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNetâ€™s superior performance in both inference time and accuracy. </p>
<blockquote>
<p>æ ‡å‡†ç°ä»£æœºå™¨å­¦ä¹ æˆåƒæ–¹æ³•åœ¨é¢å¯¹åŒ»ç–—åº”ç”¨æ—¶é‡åˆ°äº†æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ•°æ®é›†æ„å»ºçš„æˆæœ¬é«˜æ˜‚ï¼Œå› æ­¤å¯ç”¨çš„æ ‡è®°è®­ç»ƒæ•°æ®æœ‰é™ã€‚æ­¤å¤–ï¼Œåœ¨éƒ¨ç½²åï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç”¨äºå¤„ç†æ¯å¤©çš„å¤§é‡æ•°æ®ï¼Œç»™åŒ»ç–—æœºæ„å¸¦æ¥äº†é«˜æ˜‚çš„ç»´æŠ¤æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºLoGoNetï¼Œå¹¶é…å¤‡äº†ä¸€ç§é‡èº«å®šåˆ¶çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•æ¥ç¼“è§£è¿™äº›æŒ‘æˆ˜ã€‚LoGoNetåœ¨ä¸€ä¸ªUå½¢æ¶æ„å†…é›†æˆäº†ä¸€ç§æ–°å‹ç‰¹å¾æå–å™¨ï¼Œåˆ©ç”¨å¤§å†…æ ¸æ³¨æ„åŠ›ï¼ˆLKAï¼‰å’ŒåŒç¼–ç ç­–ç•¥æ¥å·§å¦™åœ°æ•æ‰é•¿çŸ­èŒƒå›´çš„ç‰¹å¾ä¾èµ–å…³ç³»ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œåè€…ä¾èµ–äºå¢åŠ ç½‘ç»œå®¹é‡æ¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚æ¨¡å‹ä¸­ç»“åˆçš„æ–°æŠ€æœ¯å¯¹äºåŒ»å­¦å›¾åƒåˆ†å‰²ç‰¹åˆ«æœ‰ç›Šï¼Œè€ƒè™‘åˆ°å­¦ä¹ å¤æ‚ä¸”é€šå¸¸ä¸è§„åˆ™çš„å™¨å®˜å½¢çŠ¶ï¼ˆå¦‚è„¾è„ï¼‰çš„å›°éš¾æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹3Då›¾åƒçš„æ–°å‹SSLæ–¹æ³•ï¼Œä»¥å¼¥è¡¥å¤§å‹æ ‡è®°æ•°æ®é›†çš„ç¼ºä¹ã€‚è¯¥æ–¹æ³•ç»“åˆå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å†…çš„å±è”½å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä¸Vision Transformerï¼ˆViTï¼‰å’ŒCNNæ¨¡å‹å…¼å®¹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ï¼ˆå³BTCVå’ŒMSDï¼‰çš„å¤šä¸ªä»»åŠ¡ä¸­è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸å…«ç§æœ€æ–°æ¨¡å‹çš„åŸºå‡†æ¯”è¾ƒçªæ˜¾äº†LoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06190v3">PDF</a> Accepted to KDD 2024</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„LoGoNetï¼Œç»“åˆè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£å†³åŒ»ç–—å›¾åƒåˆ†æä¸­æ•°æ®é›†æ„å»ºæˆæœ¬é«˜ã€è®­ç»ƒæ•°æ®æœ‰é™åŠéƒ¨ç½²åç»´æŠ¤æˆæœ¬é«˜ç­‰é—®é¢˜ã€‚LoGoNeté‡‡ç”¨Uå‹æ¶æ„ç»“åˆå¤§å‹å†…æ ¸æ³¨æ„åŠ›æœºåˆ¶ä¸åŒç¼–ç ç­–ç•¥ï¼Œèƒ½çµæ´»æ•æ‰é•¿çŸ­è·ç¦»ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œç‰¹åˆ«é€‚åˆåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚åŒæ—¶ï¼Œé’ˆå¯¹ä¸‰ç»´å›¾åƒæå‡ºæ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¼¥è¡¥ç¼ºä¹å¤§é‡æ ‡è®°æ•°æ®é›†çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•ç»“åˆæ©ç å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œåœ¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å†…å…¼å®¹ViTå’ŒCNNæ¨¡å‹ã€‚åœ¨ä¸¤é¡¹æ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LoGoNetæ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºè§£å†³åŒ»ç–—å›¾åƒåˆ†æä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥åº”å¯¹æ•°æ®é›†æ„å»ºæˆæœ¬é«˜å’Œè®­ç»ƒæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>LoGoNeté‡‡ç”¨Uå‹æ¶æ„ã€å¤§å‹å†…æ ¸æ³¨æ„åŠ›æœºåˆ¶å’ŒåŒç¼–ç ç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆæ•æ‰ç‰¹å¾ä¾èµ–å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€‚ç”¨äºä¸‰ç»´å›¾åƒï¼Œä»¥å¼¥è¡¥ç¼ºä¹æ ‡è®°æ•°æ®é›†çš„ä¸è¶³ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ©ç å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶åœ¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å†…å®æ–½ã€‚</li>
<li>LoGoNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦ä¹ å¤æ‚å’Œä¸è§„åˆ™å™¨å®˜å½¢çŠ¶æ–¹é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a71eb24b531acc715049643b77a17e12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-633d9ccad9baba3c7cf2f763e4d1b953.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea006b54cbff3ce97de70c07152c8b38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6e3efb149ba8b1340f34bcf77e17796.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819a5dbc858d19aa0001260dd1ad4b34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69a0363843da6aa4f73c13d70aafd3cf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CLIP-in-Medical-Imaging-A-Survey"><a href="#CLIP-in-Medical-Imaging-A-Survey" class="headerlink" title="CLIP in Medical Imaging: A Survey"></a>CLIP in Medical Imaging: A Survey</h2><p><strong>Authors:Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, Dinggang Shen</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP), a simple yet effective pre-training paradigm, successfully introduces text supervision to vision models. It has shown promising results across various tasks due to its generalizability and interpretability. The use of CLIP has recently gained increasing interest in the medical imaging domain, serving as a pre-training paradigm for image-text alignment, or a critical component in diverse clinical tasks. With the aim of facilitating a deeper understanding of this promising direction, this survey offers an in-depth exploration of the CLIP within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications. In this paper, we (1) first start with a brief introduction to the fundamentals of CLIP methodology; (2) then investigate the adaptation of CLIP pre-training in the medical imaging domain, focusing on how to optimize CLIP given characteristics of medical images and reports; (3) further explore practical utilization of CLIP pre-trained models in various tasks, including classification, dense prediction, and cross-modal tasks; and (4) finally discuss existing limitations of CLIP in the context of medical imaging, and propose forward-looking directions to address the demands of medical imaging domain. Studies featuring technical and practical value are both investigated. We expect this survey will provide researchers with a holistic understanding of the CLIP paradigm and its potential implications. The project page of this survey can also be found on <a target="_blank" rel="noopener" href="https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging">https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging</a>. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„é¢„è®­ç»ƒèŒƒå¼ï¼ŒæˆåŠŸåœ°å°†æ–‡æœ¬ç›‘ç£å¼•å…¥è§†è§‰æ¨¡å‹ã€‚ç”±äºå…¶é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå®ƒåœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœã€‚CLIPåœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„ä½¿ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œæ—¢ä½œä¸ºå›¾åƒæ–‡æœ¬å¯¹é½çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œä¹Ÿæ˜¯å¤šç§ä¸´åºŠä»»åŠ¡çš„å…³é”®ç»„ä»¶ã€‚ä¸ºäº†åŠ æ·±å¯¹è¿™ä¸€æœ‰å‰é€”æ–¹å‘çš„æ·±å…¥ç†è§£ï¼Œæœ¬è°ƒæŸ¥å¯¹åŒ»å­¦æˆåƒé¢†åŸŸçš„CLIPè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œæ¶‰åŠç²¾ç»†çš„CLIPé¢„è®­ç»ƒå’ŒCLIPé©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ï¼ˆ1ï¼‰é¦–å…ˆä»CLIPæ–¹æ³•çš„åŸºæœ¬åŸç†å¼€å§‹ç®€è¦ä»‹ç»ï¼›ï¼ˆ2ï¼‰ç„¶åç ”ç©¶CLIPé¢„è®­ç»ƒåœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•æ ¹æ®åŒ»å­¦å›¾åƒå’ŒæŠ¥å‘Šçš„ç‰¹æ€§ä¼˜åŒ–CLIPï¼›ï¼ˆ3ï¼‰è¿›ä¸€æ­¥æ¢ç´¢CLIPé¢„è®­ç»ƒæ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€å¯†é›†é¢„æµ‹å’Œè·¨æ¨¡æ€ä»»åŠ¡ï¼›ï¼ˆ4ï¼‰æœ€åè®¨è®ºCLIPåœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„ç°æœ‰å±€é™æ€§ï¼Œå¹¶æå‡ºå‰ç»æ€§çš„æ–¹å‘ä»¥æ»¡è¶³åŒ»å­¦æˆåƒé¢†åŸŸçš„éœ€æ±‚ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†å…·æœ‰æŠ€æœ¯å’Œå®ç”¨ä»·å€¼çš„é¡¹ç›®ã€‚æˆ‘ä»¬å¸Œæœ›æ­¤æ¬¡è°ƒæŸ¥èƒ½ä¸ºç ”ç©¶äººå‘˜æä¾›å¯¹CLIPèŒƒå¼åŠå…¶æ½œåœ¨å½±å“çš„å…¨é¢äº†è§£ã€‚æœ¬è°ƒæŸ¥çš„é¡¹ç›®é¡µé¢ä¹Ÿå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imagingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.07353v6">PDF</a> Project page available at   <a target="_blank" rel="noopener" href="https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging">https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†Contrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ã€‚æ–‡ç« é¦–å…ˆç®€è¦ä»‹ç»äº†CLIPæ–¹æ³•çš„åŸºæœ¬åŸç†ï¼Œç„¶åæ¢è®¨äº†CLIPåœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„é€‚åº”æ€§é¢„è®­ç»ƒï¼Œå¹¶æ·±å…¥æ¢ç´¢äº†CLIPåœ¨åˆ†ç±»ã€å¯†é›†é¢„æµ‹å’Œè·¨æ¨¡æ€ä»»åŠ¡ç­‰å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚æ–‡ç« è¿˜è®¨è®ºäº†CLIPåœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„ç°æœ‰å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•ï¼ŒæˆåŠŸåœ°å°†æ–‡æœ¬ç›‘ç£å¼•å…¥åˆ°è§†è§‰æ¨¡å‹ä¸­ã€‚</li>
<li>CLIPåœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼ŒCLIPè¢«ç”¨ä½œå›¾åƒæ–‡æœ¬å¯¹é½çš„é¢„è®­ç»ƒæˆ–ä¸´åºŠä»»åŠ¡çš„å…³é”®ç»„ä»¶ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†CLIPåœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„ç²¾ç»†åŒ–é¢„è®­ç»ƒã€‚</li>
<li>CLIPåœ¨åˆ†ç±»ã€å¯†é›†é¢„æµ‹å’Œè·¨æ¨¡æ€ä»»åŠ¡ç­‰åŒ»å­¦æˆåƒä»»åŠ¡ä¸­æœ‰å®é™…åº”ç”¨ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†CLIPåœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.07353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89a682024c6d99ff207bfcec73de19dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98482236d3d69fefff593fd16bac2e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1baf3c22530479de81e6e3d9d36395a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b3ca716209384f5b3491a79811cc617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb22f829c18b1b1e754733c60b254fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a25b46b27eca09c00e0d70a3fdc924b6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DG-TTA-Out-of-domain-Medical-Image-Segmentation-through-Augmentation-and-Descriptor-driven-Domain-Generalization-and-Test-Time-Adaptation"><a href="#DG-TTA-Out-of-domain-Medical-Image-Segmentation-through-Augmentation-and-Descriptor-driven-Domain-Generalization-and-Test-Time-Adaptation" class="headerlink" title="DG-TTA: Out-of-domain Medical Image Segmentation through Augmentation   and Descriptor-driven Domain Generalization and Test-Time Adaptation"></a>DG-TTA: Out-of-domain Medical Image Segmentation through Augmentation   and Descriptor-driven Domain Generalization and Test-Time Adaptation</h2><p><strong>Authors:Christian Weihsbach, Christian N. Kruse, Alexander Bigalke, Mattias P. Heinrich</strong></p>
<p>Purpose: Applying pre-trained medical deep learning segmentation models on out-of-domain images often yields predictions of insufficient quality. In this study, we propose to use a powerful generalizing descriptor along with augmentation to enable domain-generalized pre-training and test-time adaptation, achieving high-quality segmentation in unseen domains.   Materials and Methods: In this retrospective study five different publicly available datasets (2012 to 2022) including 3D CT and MRI images are used to evaluate segmentation performance in out-of-domain scenarios. The settings include abdominal, spine, and cardiac imaging. The data is randomly split into training and test samples. Domain-generalized pre-training on source data is used to obtain the best initial performance in the target domain. We introduce the combination of the generalizing SSC descriptor and GIN intensity augmentation for optimal generalization. Segmentation results are subsequently optimized at test time, where we propose to adapt the pre-trained models for every unseen scan with a consistency scheme using the same augmentation-descriptor combination. The segmentation is evaluated using Dice similarity and Hausdorff distance and the significance of improvements is tested with the Wilcoxon signed-rank test.   Results: The proposed generalized pre-training and subsequent test-time adaptation improves model performance significantly in CT to MRI cross-domain prediction for abdominal (+46.2% and +28.2% Dice), spine (+72.9%), and cardiac (+14.2% and +55.7% Dice) scenarios (p&lt;0.001).   Conclusion: Our method enables optimal, independent usage of medical image source and target data and bridges domain gaps successfully with a compact and efficient methodology. Open-source code available at: <a target="_blank" rel="noopener" href="https://github.com/multimodallearning/DG-TTA">https://github.com/multimodallearning/DG-TTA</a> </p>
<blockquote>
<p>ç›®çš„ï¼šå°†åœ¨ç‰¹å®šé¢†åŸŸé¢„è®­ç»ƒçš„åŒ»å­¦æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹åº”ç”¨äºéé¢†åŸŸå›¾åƒé€šå¸¸ä¼šäº§ç”Ÿè´¨é‡ä¸è¶³çš„é¢„æµ‹ç»“æœã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¼ºå¤§çš„é€šç”¨æè¿°ç¬¦å’Œå¢å¼ºæŠ€æœ¯ï¼Œä»¥å®ç°é¢†åŸŸé€šç”¨é¢„è®­ç»ƒå’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼Œä»è€Œåœ¨æœªè§é¢†åŸŸå®ç°é«˜è´¨é‡åˆ†å‰²ã€‚</p>
</blockquote>
<p>ææ–™ä¸æ–¹æ³•ï¼šæœ¬ç ”ç©¶ä¸ºä¸€é¡¹å›é¡¾æ€§ç ”ç©¶ï¼Œä½¿ç”¨äº†äº”ä¸ªä¸åŒå…¬å…±æ•°æ®é›†ï¼ˆ2012å¹´è‡³2022å¹´ï¼‰ï¼ŒåŒ…æ‹¬3D CTå’ŒMRIå›¾åƒï¼Œä»¥è¯„ä¼°éé¢†åŸŸåœºæ™¯ä¸­åˆ†å‰²æ€§èƒ½ã€‚è®¾ç½®åŒ…æ‹¬è…¹éƒ¨ã€è„Šæ¤å’Œå¿ƒè„æˆåƒã€‚æ•°æ®è¢«éšæœºåˆ†å‰²ä¸ºè®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬ã€‚åœ¨æºæ•°æ®ä¸Šè¿›è¡Œé¢†åŸŸé€šç”¨é¢„è®­ç»ƒï¼Œä»¥è·å¾—ç›®æ ‡é¢†åŸŸçš„æœ€ä½³åˆå§‹æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†é€šç”¨SSCæè¿°ç¬¦å’ŒGINå¼ºåº¦å¢å¼ºçš„ç»„åˆï¼Œä»¥å®ç°æœ€ä½³é€šç”¨æ€§ã€‚éšååœ¨æµ‹è¯•æ—¶é—´ä¼˜åŒ–åˆ†å‰²ç»“æœï¼Œæˆ‘ä»¬æå‡ºç”¨ä¸€è‡´æ€§æ–¹æ¡ˆï¼Œåˆ©ç”¨ç›¸åŒçš„å¢å¼º-æè¿°ç¬¦ç»„åˆï¼Œå¯¹æ¯ä¸€æ¬¡æœªè§æ‰«æè¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„é€‚åº”ã€‚ä½¿ç”¨Diceç›¸ä¼¼åº¦å’ŒHausdorffè·ç¦»å¯¹åˆ†å‰²ç»“æœè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä½¿ç”¨Wilcoxonç¬¦å·ç§©æ£€éªŒæ¥æµ‹è¯•æ”¹è¿›çš„é‡è¦æ€§ã€‚</p>
<p>ç»“æœï¼šæ‰€æå‡ºçš„é€šç”¨é¢„è®­ç»ƒå’Œéšåçš„æµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†CTåˆ°MRIè·¨é¢†åŸŸé¢„æµ‹ä¸­çš„æ¨¡å‹æ€§èƒ½ï¼Œåœ¨è…¹éƒ¨ï¼ˆ+46.2%å’Œ+28.2% Diceï¼‰ã€è„Šæ¤ï¼ˆ+72.9%ï¼‰å’Œå¿ƒè„ï¼ˆ+14.2%å’Œ+55.7% Diceï¼‰åœºæ™¯ä¸­åº”ç”¨æ•ˆæœæ˜¾è‘—ï¼ˆp&lt;0.001ï¼‰ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.06275v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä½¿ç”¨é€šç”¨æè¿°ç¬¦ä¸æ•°æ®å¢å¼ºæ–¹æ³•å®ç°è·¨åŸŸåŒ»å­¦å½±åƒåˆ†å‰²çš„é¢„è®­ç»ƒä¸æµ‹è¯•æ—¶é—´è‡ªé€‚åº”æ–¹æ³•ã€‚è¯¥ç ”ç©¶åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œå›é¡¾æ€§å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨CTè‡³MRIè·¨åŸŸé¢„æµ‹ä¸­æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è…¹éƒ¨ã€è„Šæ¤å’Œå¿ƒè„å½±åƒåœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä½¿ç”¨é€šç”¨æè¿°ç¬¦ä¸æ•°æ®å¢å¼ºè¿›è¡ŒåŸŸæ³›åŒ–é¢„è®­ç»ƒï¼Œä»¥æé«˜åœ¨æœªè§åŸŸä¸­çš„åˆ†å‰²è´¨é‡ã€‚</li>
<li>å®éªŒé‡‡ç”¨äº”ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œæ¶µç›–3D CTå’ŒMRIå›¾åƒï¼Œç”¨äºè¯„ä¼°è·¨åŸŸåœºæ™¯ä¸­çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>é€šè¿‡éšæœºåˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¿›è¡Œæºæ•°æ®çš„åŸŸæ³›åŒ–é¢„è®­ç»ƒï¼Œä»¥è·å¾—ç›®æ ‡åŸŸä¸­çš„æœ€ä½³åˆå§‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥ç»“åˆSSCé€šç”¨æè¿°ç¬¦å’ŒGINå¼ºåº¦å¢å¼ºçš„æ–¹æ³•ï¼Œå®ç°æœ€ä½³æ³›åŒ–ã€‚</li>
<li>åœ¨æµ‹è¯•æ—¶ä¼˜åŒ–åˆ†å‰²ç»“æœï¼Œé€šè¿‡ä¸€è‡´æ€§æ–¹æ¡ˆé€‚åº”æ¯ä¸ªæœªè§æ‰«æï¼ŒåŒæ—¶ä½¿ç”¨ç›¸åŒçš„å¢å¼º-æè¿°ç¬¦ç»„åˆã€‚</li>
<li>ä½¿ç”¨Diceç›¸ä¼¼åº¦å’ŒHausdorffè·ç¦»è¯„ä¼°åˆ†å‰²æ•ˆæœï¼Œå¹¶é€šè¿‡Wilcoxonç¬¦å·ç§©æ£€éªŒè¯„ä¼°æ”¹è¿›æ˜¾è‘—æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.06275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fda9822f5cf208e856d525e9ec6153e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a972da4f826bcd8365c557f481997a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-600f7dd5485247639fbb1e7063210471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f276c29069fec26a060896c437e8de4d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Efficient-Model-Based-Deep-Learning-via-Network-Pruning-and-Fine-Tuning"><a href="#Efficient-Model-Based-Deep-Learning-via-Network-Pruning-and-Fine-Tuning" class="headerlink" title="Efficient Model-Based Deep Learning via Network Pruning and Fine-Tuning"></a>Efficient Model-Based Deep Learning via Network Pruning and Fine-Tuning</h2><p><strong>Authors:Chicago Y. Park, Weijie Gan, Zihao Zou, Yuyang Hu, Zhixin Sun, Ulugbek S. Kamilov</strong></p>
<p>Model-based deep learning (MBDL) is a powerful methodology for designing deep models to solve imaging inverse problems. MBDL networks can be seen as iterative algorithms that estimate the desired image using a physical measurement model and a learned image prior specified using a convolutional neural net (CNNs). The iterative nature of MBDL networks increases the test-time computational complexity, which limits their applicability in certain large-scale applications. Here we make two contributions to address this issue: First, we show how structured pruning can be adopted to reduce the number of parameters in MBDL networks. Second, we present three methods to fine-tune the pruned MBDL networks to mitigate potential performance loss. Each fine-tuning strategy has a unique benefit that depends on the presence of a pre-trained model and a high-quality ground truth. We show that our pruning and fine-tuning approach can accelerate image reconstruction using popular deep equilibrium learning (DEQ) and deep unfolding (DU) methods by 50% and 32%, respectively, with nearly no performance loss. This work thus offers a step forward for solving inverse problems by showing the potential of pruning to improve the scalability of MBDL. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wustl-cig/MBDL_Pruning">https://github.com/wustl-cig/MBDL_Pruning</a> . </p>
<blockquote>
<p>åŸºäºæ¨¡å‹çš„æ·±åº¦å­¦ä¹ ï¼ˆMBDLï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„æ–¹æ³•è®ºï¼Œç”¨äºè®¾è®¡æ·±åº¦æ¨¡å‹ä»¥è§£å†³æˆåƒåé—®é¢˜ã€‚MBDLç½‘ç»œå¯ä»¥çœ‹ä½œæ˜¯ä½¿ç”¨ç‰©ç†æµ‹é‡æ¨¡å‹å’Œé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æŒ‡å®šçš„å­¦ä¹ å›¾åƒå…ˆéªŒæ¥ä¼°è®¡æ‰€éœ€å›¾åƒçš„è¿­ä»£ç®—æ³•ã€‚MBDLç½‘ç»œçš„è¿­ä»£æ€§è´¨å¢åŠ äº†æµ‹è¯•æ—¶çš„è®¡ç®—å¤æ‚æ€§ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æŸäº›å¤§è§„æ¨¡åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªé—®é¢˜åšå‡ºäº†ä¸¤ä¸ªè´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é‡‡ç”¨ç»“æ„åŒ–ä¿®å‰ªæ¥å‡å°‘MBDLç½‘ç»œä¸­çš„å‚æ•°æ•°é‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§æ–¹æ³•æ¥å¾®è°ƒä¿®å‰ªè¿‡çš„MBDLç½‘ç»œï¼Œä»¥å‡è½»æ½œåœ¨çš„æ€§èƒ½æŸå¤±ã€‚æ¯ç§å¾®è°ƒç­–ç•¥éƒ½å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œè¿™å–å†³äºæ˜¯å¦å­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹å’Œé«˜è´¨é‡çš„çœŸå®æ ‡ç­¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„ä¿®å‰ªå’Œå¾®è°ƒæ–¹æ³•å¯ä»¥é€šè¿‡è¿‘ä¹ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æµè¡Œçš„æ·±åº¦å‡è¡¡å­¦ä¹ ï¼ˆDEQï¼‰å’Œæ·±åº¦å±•å¼€ï¼ˆDUï¼‰æ–¹æ³•åˆ†åˆ«å°†å›¾åƒé‡å»ºé€Ÿåº¦æé«˜50%å’Œ32%ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œé€šè¿‡å±•ç¤ºä¿®å‰ªåœ¨æé«˜MBDLå¯æ‰©å±•æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè§£å†³åé—®é¢˜æä¾›äº†å‰è¿›çš„ä¸€æ­¥ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/wustl-cig/MBDL_Pruning%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wustl-cig/MBDL_Pruningè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02003v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹é©±åŠ¨çš„æ·±åº¦å­¦ä¹ ï¼ˆMBDLï¼‰æ˜¯è§£å†³æˆåƒåé—®é¢˜çš„å¼ºå¤§æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºä¸¤é¡¹è´¡çŒ®ä»¥å‡è½»MBDLç½‘ç»œçš„è®¡ç®—å¤æ‚æ€§ï¼Œé¦–å…ˆæ˜¯é€šè¿‡ç»“æ„ä¿®å‰ªå‡å°‘å…¶å‚æ•°æ•°é‡ï¼Œç„¶åæä¾›ä¸‰ç§æ–¹æ³•æ¥å¾®è°ƒä¿®å‰ªåçš„MBDLç½‘ç»œä»¥å‡å°‘æ½œåœ¨çš„æ€§èƒ½æŸå¤±ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¯¹æµè¡Œæ·±åº¦å¹³è¡¡å­¦ä¹ ï¼ˆDEQï¼‰å’Œæ·±åº¦å±•å¼€ï¼ˆDUï¼‰æ–¹æ³•è¿›è¡Œå›¾åƒé‡å»ºæ—¶ï¼Œåˆ†åˆ«æé«˜çº¦50%å’Œ32%çš„è®¡ç®—é€Ÿåº¦ï¼ŒåŒæ—¶å‡ ä¹ä¸æŸå¤±æ€§èƒ½ã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†ä¿®å‰ªåœ¨æå‡MBDLå¯æ‰©å±•æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹é©±åŠ¨çš„æ·±åº¦å­¦ä¹ ï¼ˆMBDLï¼‰æ˜¯è§£å†³æˆåƒåé—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç»“æ„ä¿®å‰ªå¯ä»¥å‡å°‘MBDLç½‘ç»œçš„å‚æ•°æ•°é‡ã€‚</li>
<li>é€šè¿‡ä¸‰ç§å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥å‡è½»ä¿®å‰ªåMBDLç½‘ç»œçš„æ€§èƒ½æŸå¤±ã€‚</li>
<li>ç»“æ„ä¿®å‰ªå’Œå¾®è°ƒæ–¹æ³•å¯ä»¥åŠ é€Ÿå›¾åƒé‡å»ºè¿‡ç¨‹ã€‚</li>
<li>å¯¹æ·±åº¦å¹³è¡¡å­¦ä¹ ï¼ˆDEQï¼‰å’Œæ·±åº¦å±•å¼€ï¼ˆDUï¼‰æ–¹æ³•çš„åŠ é€Ÿæ•ˆæœåˆ†åˆ«è¾¾åˆ°äº†çº¦50%å’Œ32%ã€‚</li>
<li>ä¿®å‰ªæ–¹æ³•æé«˜äº†MBDLçš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.02003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2215eed7ba336700932360355540d80f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e5986c9b6bff1e52cec0b796fc401d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4bcaddfb29caf258add452ce29742cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6085e805a364f90643e5589d3f08cf1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98f62346219f91bc347f45c0a2f06597.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a5f36b513b1243aa2b646edc2fb65a1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-00d616db923791adfaaebfa4006122cc.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  MME-Unify A Comprehensive Benchmark for Unified Multimodal   Understanding and Generation Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7e6e8eb67bffed7f4a3fa4775183fbef.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-06  Tomography of Quantum States from Structured Measurements via   quantum-aware transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
