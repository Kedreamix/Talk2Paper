<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-19  A Survey on Bridging EEG Signals and Generative AI From Image and Text   to Beyond">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-372830aee3ac139b61ac05c1ccd7eebc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-19-更新"><a href="#2025-02-19-更新" class="headerlink" title="2025-02-19 更新"></a>2025-02-19 更新</h1><h2 id="A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond"><a href="#A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond" class="headerlink" title="A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond"></a>A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond</h2><p><strong>Authors:Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury</strong></p>
<p>Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction. </p>
<blockquote>
<p>脑机接口（BCIs）与生成式人工智能（GenAI）的融合为脑信号解码开辟了新的前沿领域，能够实现辅助通信、神经表征学习和多模式融合。特别是利用脑电图（EEG）的脑机接口为非侵入性地转换神经活动为有意义的输出提供了一种手段。深度学习领域的最新进展，包括生成对抗网络（GANs）和基于变压器的自然语言大型模型（LLMs），已经极大地提高了基于EEG的图像、文本和语音生成。本文综述了基于EEG的多模式生成的最新研究状况，重点关注（i）通过GANs、变分自动编码器（VAEs）和扩散模型实现的EEG到图像生成，（ii）利用基于变压器的语言模型和对比学习方法实现的EEG到文本生成。此外，我们还讨论了新兴的EEG到语音合成领域，这是一个不断发展的多模式前沿领域。本文强调了关键数据集、用例、挑战以及支撑生成方法的基础EEG特征编码方法。通过对基于EEG的生成式人工智能的结构性概述，本综述旨在为研究人员和实践者提供洞察力，以推动神经解码的发展，提高辅助技术的效能，并拓展脑机交互的边界。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12048v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文探讨了脑机接口（BCIs）与生成式人工智能（GenAI）的融合，在脑信号解码方面开创了新的前沿领域。论文重点介绍了利用脑电图（EEG）的脑机接口，通过深度学习技术生成图像、文本和语音的应用。论文回顾了EEG基于多模态生成的研究现状，讨论了关键数据集、用例、挑战和EEG特征编码方法。旨在为研究人员和实践者提供洞察，以推动神经解码的发展，提高辅助技术的性能，并拓展脑机交互的边界。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脑机接口（BCIs）与生成式人工智能（GenAI）的融合为脑信号解码提供了新的机会。</li>
<li>利用脑电图（EEG）的脑机接口可实现非侵入式神经活动翻译。</li>
<li>深度学习技术，包括生成对抗网络（GANs）、基于Transformer的大型语言模型（LLMs）等，已显著改善EEG生成的图像、文本和语音的质量。</li>
<li>EEG基于多模态生成的研究已成为热点，包括EEG转图像和EEG转文本生成。</li>
<li>EEG转语音合成是一个新兴的多模态前沿领域。</li>
<li>论文回顾了关键数据集、用例、挑战和EEG特征编码方法，以提供对EEG基于生成式人工智能的深入理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f89a92f9f6583f89738d30ff8feb081e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f4c288ef1da1c6353c7dc833140956e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93096ddd3246c583190a570d31c2b141.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UNITE-FND-Reframing-Multimodal-Fake-News-Detection-through-Unimodal-Scene-Translation"><a href="#UNITE-FND-Reframing-Multimodal-Fake-News-Detection-through-Unimodal-Scene-Translation" class="headerlink" title="UNITE-FND: Reframing Multimodal Fake News Detection through Unimodal   Scene Translation"></a>UNITE-FND: Reframing Multimodal Fake News Detection through Unimodal   Scene Translation</h2><p><strong>Authors:Arka Mukherjee, Shreya Ghosh</strong></p>
<p>Multimodal fake news detection typically demands complex architectures and substantial computational resources, posing deployment challenges in real-world settings. We introduce UNITE-FND, a novel framework that reframes multimodal fake news detection as a unimodal text classification task. We propose six specialized prompting strategies with Gemini 1.5 Pro, converting visual content into structured textual descriptions, and enabling efficient text-only models to preserve critical visual information. To benchmark our approach, we introduce Uni-Fakeddit-55k, a curated dataset family of 55,000 samples each, each processed through our multimodal-to-unimodal translation framework. Experimental results demonstrate that UNITE-FND achieves 92.52% accuracy in binary classification, surpassing prior multimodal models while reducing computational costs by over 10x (TinyBERT variant: 14.5M parameters vs. 250M+ in SOTA models). Additionally, we propose a comprehensive suite of five novel metrics to evaluate image-to-text conversion quality, ensuring optimal information preservation. Our results demonstrate that structured text-based representations can replace direct multimodal processing with minimal loss of accuracy, making UNITE-FND a practical and scalable alternative for resource-constrained environments. </p>
<blockquote>
<p>多模态虚假新闻检测通常需要复杂的架构和大量的计算资源，这在现实世界的部署中构成了挑战。我们引入了UNITE-FND，这是一个将多模态虚假新闻检测重新定位为单模态文本分类任务的新型框架。我们提出了六种专业的提示策略与双子座1.5专业版相结合，将视觉内容转换为结构化文本描述，使文本模型能够高效保存关键视觉信息。为了对我们的方法进行基准测试，我们引入了Uni-Fakeddit-55k数据集家族，包含经过我们多模态到单模态翻译框架处理的每个样本为55000个样本的数据集。实验结果表明，UNITE-FND在二分类中的准确度达到了92.52%，超越了先前的多模态模型，并将计算成本降低了超过十倍（TinyBERT变体：参数为14.5M与SOTA模型中的超过2.5亿相比）。此外，我们还提出了一个综合的五个新指标来评估图像到文本的转换质量，确保信息的最佳保存。我们的结果表明，基于结构化文本的表示可以替代直接的多模态处理而不会产生显著的精度损失，这使得UNITE-FND成为资源受限环境的实用且可扩展的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11132v1">PDF</a> 28 pages, 16 figures</p>
<p><strong>Summary</strong><br>多媒体假新闻检测通常要求复杂的架构和大量的计算资源，这对真实环境中的部署提出了挑战。我们推出了UNITE-FND，这是一种将多媒体假新闻检测重新定位为单模态文本分类任务的新框架。通过六种专门的提示策略和Gemini 1.5 Pro，我们将视觉内容转换为结构化文本描述，使文本模型能够保留关键视觉信息。为了评估我们的方法，我们引入了Uni-Fakeddit-55k数据集，包含55000个样本，每个样本都通过我们的多媒体到单模态翻译框架进行处理。实验结果表明，UNITE-FND在二元分类中达到了92.52%的准确率，超越了先前的多媒体模型，同时降低了超过10倍的计算成本。此外，我们还提出了一个综合的五种新型指标来评估图像到文本的转换质量，确保最优的信息保留。我们的研究结果表明，基于结构化文本的表示可以替代直接的多模态处理，且不会造成显著的精度损失，这使得UNITE-FND成为资源受限环境中的实用和可扩展的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNITE-FND框架将多媒体假新闻检测重新定位为单模态文本分类任务，简化了检测过程。</li>
<li>提出了六种专门的提示策略，将视觉内容转换为结构化文本描述，便于文本模型处理。</li>
<li>引入了Uni-Fakeddit-55k数据集，用于评估多媒体到单模态翻译框架的性能。</li>
<li>UNITE-FND在二元分类中实现了高准确率（92.52%），同时显著降低了计算成本。</li>
<li>与现有模型相比，UNITE-FND的计算成本降低了10倍以上，具有更好的实用性。</li>
<li>提出了五种新型指标来评估图像到文本的转换质量，确保信息保留的完整性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11132">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5a30635c6daf97fe539ba4d5cbcb0ef0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06b935d63a3c636b7037a1d3b9c4c0b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6e313ab280dd354081613e7573ba42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eae8a0d97960a929b3a1993c94a3bd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec6e879fb2d845b8d3f6f8ec3e668ecd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Manual2Skill-Learning-to-Read-Manuals-and-Acquire-Robotic-Skills-for-Furniture-Assembly-Using-Vision-Language-Models"><a href="#Manual2Skill-Learning-to-Read-Manuals-and-Acquire-Robotic-Skills-for-Furniture-Assembly-Using-Vision-Language-Models" class="headerlink" title="Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for   Furniture Assembly Using Vision-Language Models"></a>Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for   Furniture Assembly Using Vision-Language Models</h2><p><strong>Authors:Chenrui Tie, Shengxiang Sun, Jinxuan Zhu, Yiwei Liu, Jingxiang Guo, Yue Hu, Haonan Chen, Junting Chen, Ruihai Wu, Lin Shao</strong></p>
<p>Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities. </p>
<blockquote>
<p>人类拥有一种非凡的能力，可以通过解读抽象说明书来理解和执行复杂的操作任务。然而，对于机器人来说，这一能力仍然是一个巨大的挑战，因为它们无法解读抽象指令并将其转化为可执行的行动。在本文中，我们提出了Manual2Skill这一新型框架，使机器人能够在高级手册指令的引导下完成复杂的装配任务。我们的方法利用视觉语言模型（VLM）从教学图像中提取结构信息，然后使用这些信息来构建层次化的装配图。这些图代表了部件、子组件和它们之间的关系。为了促进任务执行，姿态估计模型预测了每个装配步骤中组件的6D相对姿态。同时，运动规划模块为现实世界的机器人实现生成可操作序列。我们通过成功组装几个现实世界的宜家家具产品来证明Manual2Skill的有效性。这一应用凸显了其在长周期操作任务中的效率和精确度，显著提高了机器人从说明书学习的实用性。这项工作标志着机器人系统在理解和执行复杂操作任务方面的能力向前迈进了一步，以一种类似于人类能力的方式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10090v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>机器人执行复杂操作任务的能力一直是其面临的一大挑战，特别是它们无法解读抽象指令并将其转化为可执行动作。本文提出了Manual2Skill框架，使机器人能够依据高级手册指令执行复杂的装配任务。该框架通过视觉语言模型从教学图像中提取结构化信息，构建层次化的装配图，展示部件、子组件及其关系。同时，姿态估计模型预测每个装配步骤中组件的6D相对姿态，运动规划模块则生成可用于实际机器人操作的行动序列。通过成功装配宜家家具等实例，展示了Manual2Skill在管理和执行长期操作任务中的高效性和精确度，显著提高了机器人从说明书学习的实用性。本研究标志着机器人在理解和执行复杂操作任务方面取得了进展，与人类能力相媲美。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人执行复杂操作任务是一大挑战，无法直接解读抽象指令。</li>
<li>Manual2Skill框架使机器人能够依据高级手册指令执行复杂的装配任务。</li>
<li>Manual2Skill通过视觉语言模型从教学图像中提取结构化信息。</li>
<li>姿态估计模型和运动规划模块共同促进任务执行。</li>
<li>通过成功装配宜家家具，展示了Manual2Skill的高效性和精确度。</li>
<li>Manual2Skill增强了机器人从说明书学习的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10090">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-32bb919ca0c84f630af7e46375a69be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568de30dc59955d2c7faca5e690bf850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8ef9b0092c1424eff18e041c8167288.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bidirectional-Diffusion-Bridge-Models"><a href="#Bidirectional-Diffusion-Bridge-Models" class="headerlink" title="Bidirectional Diffusion Bridge Models"></a>Bidirectional Diffusion Bridge Models</h2><p><strong>Authors:Duc Kieu, Kien Do, Toan Nguyen, Dang Nguyen, Thin Nguyen</strong></p>
<p>Diffusion bridges have shown potential in paired image-to-image (I2I) translation tasks. However, existing methods are limited by their unidirectional nature, requiring separate models for forward and reverse translations. This not only doubles the computational cost but also restricts their practicality. In this work, we introduce the Bidirectional Diffusion Bridge Model (BDBM), a scalable approach that facilitates bidirectional translation between two coupled distributions using a single network. BDBM leverages the Chapman-Kolmogorov Equation for bridges, enabling it to model data distribution shifts across timesteps in both forward and backward directions by exploiting the interchangeability of the initial and target timesteps within this framework. Notably, when the marginal distribution given endpoints is Gaussian, BDBM’s transition kernels in both directions possess analytical forms, allowing for efficient learning with a single network. We demonstrate the connection between BDBM and existing bridge methods, such as Doob’s h-transform and variational approaches, and highlight its advantages. Extensive experiments on high-resolution I2I translation tasks demonstrate that BDBM not only enables bidirectional translation with minimal additional cost but also outperforms state-of-the-art bridge models. Our source code is available at [<a target="_blank" rel="noopener" href="https://github.com/kvmduc/BDBM||https://github.com/kvmduc/BDBM]">https://github.com/kvmduc/BDBM||https://github.com/kvmduc/BDBM]</a>. </p>
<blockquote>
<p>扩散桥在配对图像到图像（I2I）翻译任务中显示出潜力。然而，现有方法受到其单向性质的限制，需要单独的模型进行正向和反向翻译。这不仅使计算成本翻倍，而且限制了它们的实用性。在这项工作中，我们引入了双向扩散桥模型（BDBM），这是一种可扩展的方法，使用一个网络实现两个耦合分布之间的双向翻译。BDBM利用Chapman-Kolmogorov方程构建桥梁，能够在框架内初始和目标时间点互换，以在两个方向上建模数据分布变化的时序迁移。值得注意的是，当给定端点的边缘分布为高斯分布时，BDBM在双向的转换内核具有分析形式，允许使用单个网络进行有效学习。我们展示了BDBM与现有桥梁方法（如Doob的h变换和变分方法）之间的联系，并强调了其优势。在高分辨率I2I翻译任务上的大量实验表明，BDBM不仅实现了双向翻译且几乎没有额外的成本，而且还优于最新的桥梁模型。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/kvmduc/BDBM%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/kvmduc/BDBM访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09655v1">PDF</a> Source code: <a target="_blank" rel="noopener" href="https://github.com/kvmduc/BDBM">https://github.com/kvmduc/BDBM</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了双向扩散桥梁模型（BDBM），该模型可实现两个耦合分布之间的双向翻译，且仅使用一个网络。它利用Chapman-Kolmogorov方程构建桥梁，能在前后方向上模拟数据分布的时间步移，并通过此框架内初始和目标时间步的互换性来实现。当给定端点的边际分布为高斯分布时，BDBM在双向的过渡核具有解析形式，使得单网络的学习更为高效。实验证明，BDBM在图像到图像（I2I）翻译任务上表现优异，不仅实现了双向翻译且成本增加极小，还优于现有桥梁模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BDBM模型可以实现两个耦合分布之间的双向翻译，并仅使用一个网络。</li>
<li>BDBM利用Chapman-Kolmogorov方程构建桥梁，实现前后方向上的数据分布时间步移模拟。</li>
<li>当边际分布为高斯分布时，BDBM的过渡核具有解析形式，提高学习效率。</li>
<li>BDBM与现有的桥梁方法如Doob的h-transform和变分方法有联系。</li>
<li>BDBM在图像到图像（I2I）翻译任务上表现优异。</li>
<li>BDBM实现了双向翻译且成本增加极小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9961d944db66d1dde88130f36c292809.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c919ff41ade74edc347d6c88f2b4511.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60f76a03d060dc8b45494eee2bdafd57.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Brainwaves-to-Brain-Scans-A-Robust-Neural-Network-for-EEG-to-fMRI-Synthesis"><a href="#From-Brainwaves-to-Brain-Scans-A-Robust-Neural-Network-for-EEG-to-fMRI-Synthesis" class="headerlink" title="From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI   Synthesis"></a>From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI   Synthesis</h2><p><strong>Authors:Kristofer Grover Roos, Atsushi Fukuda, Quan Huu Cap</strong></p>
<p>While functional magnetic resonance imaging (fMRI) offers rich spatial resolution, it is limited by high operational costs and significant infrastructural demands. In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial resolution necessary for precise neural localization. To bridge these gaps, we introduce E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data. E2fNet is specifically designed to capture and translate meaningful features from EEG across electrode channels into accurate fMRI representations. Extensive evaluations across three datasets demonstrate that E2fNet consistently outperforms existing methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM). Our findings suggest that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities. The code is available at <a target="_blank" rel="noopener" href="https://github.com/kgr20/E2fNet">https://github.com/kgr20/E2fNet</a>. </p>
<blockquote>
<p>功能磁共振成像（fMRI）虽然提供了丰富的空间分辨率，但由于操作成本高和对基础设施的巨大需求而受到限制。相比之下，脑电图（EEG）在捕捉电活动方面提供了毫秒级的精确度，但缺乏精确神经定位所需的空间分辨率。为了弥补这些差距，我们引入了E2fNet，这是一个简单有效的深度学习模型，可以从低成本的EEG数据中合成fMRI图像。E2fNet专门设计用于捕获和翻译EEG跨电极通道的有意义特征，转化为准确的fMRI表示。在三个数据集上的广泛评估表明，E2fNet始终优于现有方法，在结构相似性指数度量（SSIM）方面达到最新结果。我们的研究结果表明，E2fNet是一个有前景的、经济实惠的解决方案，可增强神经成像能力。代码可在<a target="_blank" rel="noopener" href="https://github.com/kgr20/E2fNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kgr20/E2fNet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08025v2">PDF</a> </p>
<p><strong>Summary</strong>:<br>EEG与fMRI融合研究取得突破，通过深度学习模型E2fNet实现从低成本EEG数据合成fMRI图像。E2fNet设计精巧，能有效捕捉EEG数据中的有意义特征并转化为准确的fMRI表示。评估显示其表现超越现有方法，具有成为增强神经影像学能力的潜力且经济实惠。代码已公开分享。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>E2fNet是一个深度学习模型，旨在解决低成本EEG数据与富空间分辨率的fMRI图像之间的差距。</li>
<li>E2fNet可以从EEG数据中捕捉有意义特征并将其转化为fMRI图像表示。</li>
<li>通过在三组数据集上的评估，E2fNet在结构相似性指数度量（SSIM）方面达到了业界最佳表现。</li>
<li>E2fNet表现超越现有方法，有望为神经影像学提供更经济高效的解决方案。</li>
<li>该模型能够实现精准的神经定位并丰富空间分辨率。</li>
<li>模型可以通过公开的GitHub仓库获得并应用于进一步研究和实际应用中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-372830aee3ac139b61ac05c1ccd7eebc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abae403a3766f0d98aee36c1ffdab3bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e3b563e432effe929fd3a540eb0804.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8afe8889eccff1819fb72ce7210c3b9d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Looking-Backward-Streaming-Video-to-Video-Translation-with-Feature-Banks"><a href="#Looking-Backward-Streaming-Video-to-Video-Translation-with-Feature-Banks" class="headerlink" title="Looking Backward: Streaming Video-to-Video Translation with Feature   Banks"></a>Looking Backward: Streaming Video-to-Video Translation with Feature   Banks</h2><p><strong>Authors:Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, Diana Marculescu</strong></p>
<p>This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames. At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. This is realized by maintaining a feature bank, which archives information from past frames. For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output. The feature bank is continually updated by merging stored and new features, making it compact but informative. StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative metrics and user studies confirm StreamV2V’s exceptional ability to maintain temporal consistency. </p>
<blockquote>
<p>本文介绍了StreamV2V，这是一种扩散模型，通过用户提示实现实时流式视频到视频（V2V）的翻译。不同于之前使用批次处理有限帧的V2V方法，我们选择以流式方式处理帧，以支持无限帧。StreamV2V的核心在于一个后顾原则，将现在与过去联系起来。这是通过维护一个特征银行来实现的，该银行存档过去帧的信息。对于传入帧，StreamV2V将自注意力扩展到包括银行密钥和值，并直接将相似的过去特征融合到输出中。特征银行通过合并存储的新特征来不断更新，使其紧凑而富有信息。StreamV2V以其适应性和效率而脱颖而出，无需微调即可无缝集成到图像扩散模型中。它可以在一个A100 GPU上以每秒20帧的速度运行，比FlowVid、CoDeF、Rerender和TokenFlow分别快15倍、46倍、108倍和158倍。定量指标和用户研究证实了StreamV2V在保持时间一致性方面的出色能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15757v3">PDF</a> ICLR 2025. Project page:   <a target="_blank" rel="noopener" href="https://jeff-liangf.github.io/projects/streamv2v">https://jeff-liangf.github.io/projects/streamv2v</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了StreamV2V模型，这是一个基于扩散的视频转视频（V2V）实时翻译模型，通过用户提示实现。与传统的批处理方式的V2V方法不同，StreamV2V采用流式处理方式支持无限帧数的处理。其核心原理是利用向后观察原则将当前与过去相联系，通过维护特征库存储过去帧的信息。对于新进入的帧，StreamV2V扩展了自注意力机制，包括存储的关键字和值，并将相似的过去特征直接融合到输出中。该特征库通过合并存储的新特征不断更新，使其紧凑且信息丰富。StreamV2V具有适应性和效率，可无缝集成图像扩散模型而无需微调。它在A100 GPU上的运行速度为每秒20帧，相较于其他模型有显著的速度优势。其卓越的临时一致性能力得到了定量指标和用户研究的证实。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamV2V是一个实时视频转视频翻译模型，支持无限帧处理。</li>
<li>采用流式处理方法，维护一个特征库存储过去帧的信息。</li>
<li>利用向后观察原则将当前与过去相联系。</li>
<li>扩展自注意力机制以包括存储的关键字和值，融合相似的过去特征到输出中。</li>
<li>特征库通过合并新旧特征不断更新，保持紧凑且信息丰富。</li>
<li>StreamV2V适应性强、效率高，可无缝集成图像扩散模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15757">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-74172f33a91d57da9ba00b4784af4ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb70aff9cfe7ece29358e3a822bbcee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f7bdc98dad3a30228cd9b9d53523721.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94c69663e1654d7b35818cb622d34b22.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PipeOptim-Ensuring-Effective-1F1B-Schedule-with-Optimizer-Dependent-Weight-Prediction"><a href="#PipeOptim-Ensuring-Effective-1F1B-Schedule-with-Optimizer-Dependent-Weight-Prediction" class="headerlink" title="PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent   Weight Prediction"></a>PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent   Weight Prediction</h2><p><strong>Authors:Lei Guan, Dongsheng Li, Yongle Chen, Jiye Liang, Wenjian Wang, Xicheng Lu</strong></p>
<p>Asynchronous pipeline model parallelism with a “1F1B” (one forward, one backward) schedule generates little bubble overhead and always provides quite a high throughput. However, the “1F1B” schedule inevitably leads to weight inconsistency and weight staleness issues due to the cross-training of different mini-batches across GPUs. To simultaneously address these two problems, in this paper, we propose an optimizer-dependent weight prediction strategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight of our proposal is that we employ a weight prediction strategy in the forward pass to ensure that each mini-batch uses consistent and staleness-free weights to compute the forward pass. To be concrete, we first construct the weight prediction scheme based on the update rule of the used optimizer when training the deep neural network models. Then throughout the “1F1B” pipelined training, each mini-batch is mandated to execute weight prediction ahead of the forward pass, subsequently employing the predicted weights to perform the forward pass. As a result, PipeOptim 1) inherits the advantage of the “1F1B” schedule and generates pretty high throughput, and 2) can ensure effective parameter learning regardless of the type of the used optimizer. To verify the effectiveness of our proposal, we conducted extensive experimental evaluations using eight different deep-learning models spanning three machine-learning tasks including image classification, sentiment analysis, and machine translation. The experiment results demonstrate that PipeOptim outperforms the popular pipelined approaches including GPipe, PipeDream, PipeDream-2BW, and SpecTrain. The code of PipeOptim can be accessible at <a target="_blank" rel="noopener" href="https://github.com/guanleics/PipeOptim">https://github.com/guanleics/PipeOptim</a>. </p>
<blockquote>
<p>采用异步流水线模型并行性的”1F1B”（一前一后）时间表，会产生较少的冒泡开销，并且始终提供较高的吞吐量。然而，“1F1B”时间表不可避免地会导致权重不一致和权重陈旧问题，这是由于不同小批量数据在GPU之间的交叉训练造成的。为了解决这两个问题，本文提出了一种依赖于优化器的权重预测策略（也称为PipeOptim），用于异步流水线训练。我们提案的关键见解是，我们在前向传播过程中采用权重预测策略，以确保每个小批量数据使用一致且无陈旧的权重进行计算。具体来说，我们首先基于所用优化器的更新规则构建权重预测方案，用于训练深度神经网络模型。然后在“1F1B”流水线训练中，每个小批量数据必须在前向传播之前执行权重预测，随后使用预测的权重执行前向传播。因此，PipeOptim 1）继承了“1F1B”时间表的优点，并产生了较高的吞吐量，2）无论使用哪种优化器，都可以确保有效的参数学习。为了验证我们提案的有效性，我们使用涵盖三项机器学习任务（包括图像分类、情感分析和机器翻译）的八个不同深度学习模型进行了广泛的实验评估。实验结果表明，PipeOptim优于流行的流水线方法，包括GPipe、PipeDream、PipeDream-2BW和SpecTrain。PipeOptim的代码可在[<a target="_blank" rel="noopener" href="https://github.com/guanleics/PipeOptim%E8%AE%BF%E9%97%AE%E3%80%82]">https://github.com/guanleics/PipeOptim访问。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00839v3">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对异步流水线训练的优化器依赖权重预测策略（PipeOptim）。该策略在“1F1B”（一前一后）调度的基础上，通过在正向传播过程中采用权重预测策略，确保每个小批次使用一致且无陈旧权重的计算。PipeOptim不仅继承了“1F1B”调度的高吞吐量优势，还能确保各种优化器的有效参数学习。实验结果表明，PipeOptim在图像分类、情感分析和机器翻译等任务上的表现优于其他流行的流水线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异步流水线模型并行性采用“1F1B”调度在高吞吐量方面表现出色，但存在权重不一致和陈旧问题。</li>
<li>PipeOptim通过优化器依赖的权重预测策略解决上述问题，确保每个小批次使用一致且无陈旧的权重进行计算。</li>
<li>PipeOptim继承了“1F1B”调度的优势，并确保了各种优化器的有效参数学习。</li>
<li>实验结果表明，PipeOptim在多种深度学习模型和任务上优于其他流水线方法。</li>
<li>PipeOptim的代码已公开，可访问<a target="_blank" rel="noopener" href="https://github.com/guanleics/PipeOptim%E3%80%82">https://github.com/guanleics/PipeOptim。</a></li>
<li>该策略适用于图像分类、情感分析和机器翻译等任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00839">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ded84ee5f8998fd40b4340be5ca8a3ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8303ecfceb7ee7a887f659bf4377c893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a9646197f2b1e1e5f41f0f68fed1338.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc1799c45604b7a5fb67f4986a0ea240.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e416b4b2051d47a82f3c40b72e1ca60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87ef70c5e7554ecff9adeddd7f57e030.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5a6a7497e10f5b7af110af042f7a72e9.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-02-19  SVBench A Benchmark with Temporal Multi-Turn Dialogues for Streaming   Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-60fda58901d2659265d175113e9252b7.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-19  SQL-o1 A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
