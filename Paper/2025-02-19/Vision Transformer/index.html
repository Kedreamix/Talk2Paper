<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual   Metrics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f00d4fe6d309d36ceb57e859c10e7028.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="Adversarially-Robust-CLIP-Models-Can-Induce-Better-Robust-Perceptual-Metrics"><a href="#Adversarially-Robust-CLIP-Models-Can-Induce-Better-Robust-Perceptual-Metrics" class="headerlink" title="Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual   Metrics"></a>Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual   Metrics</h2><p><strong>Authors:Francesco Croce, Christian Schlarmann, Naman Deep Singh, Matthias Hein</strong></p>
<p>Measuring perceptual similarity is a key tool in computer vision. In recent years perceptual metrics based on features extracted from neural networks with large and diverse training sets, e.g. CLIP, have become popular. At the same time, the metrics extracted from features of neural networks are not adversarially robust. In this paper we show that adversarially robust CLIP models, called R-CLIP$_\textrm{F}$, obtained by unsupervised adversarial fine-tuning induce a better and adversarially robust perceptual metric that outperforms existing metrics in a zero-shot setting, and further matches the performance of state-of-the-art metrics while being robust after fine-tuning. Moreover, our perceptual metric achieves strong performance on related tasks such as robust image-to-image retrieval, which becomes especially relevant when applied to â€œNot Safe for Workâ€ (NSFW) content detection and dataset filtering. While standard perceptual metrics can be easily attacked by a small perturbation completely degrading NSFW detection, our robust perceptual metric maintains high accuracy under an attack while having similar performance for unperturbed images. Finally, perceptual metrics induced by robust CLIP models have higher interpretability: feature inversion can show which images are considered similar, while text inversion can find what images are associated to a given prompt. This also allows us to visualize the very rich visual concepts learned by a CLIP model, including memorized persons, paintings and complex queries. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œè¡¡é‡æ„ŸçŸ¥ç›¸ä¼¼æ€§æ˜¯ä¸€é¡¹å…³é”®å·¥å…·ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºä»å¤§å‹å’Œå¤šæ ·åŒ–è®­ç»ƒé›†ä¸­æå–çš„ç‰¹å¾çš„ç¥ç»ç½‘ç»œæ„ŸçŸ¥åº¦é‡æ–¹æ³•ï¼Œä¾‹å¦‚CLIPï¼Œå·²ç»å˜å¾—éå¸¸æµè¡Œã€‚ç„¶è€ŒåŒæ—¶ï¼Œä»ç¥ç»ç½‘ç»œç‰¹å¾ä¸­æå–çš„æŒ‡æ ‡å¹¶ä¸å…·å¤‡å¯¹æŠ—ç¨³å¥æ€§ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡å¯¹æŠ—æ€§æ— ç›‘ç£å¾®è°ƒè·å¾—çš„å¯¹æŠ—æ€§ç¨³å¥CLIPæ¨¡å‹ï¼ˆç§°ä¸ºR-CLIP<sub>F</sub>ï¼‰èƒ½å¤Ÿè¯±å¯¼å‡ºæ›´å¥½çš„å¯¹æŠ—æ€§ç¨³å¥æ„ŸçŸ¥åº¦é‡æ–¹æ³•ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè¯¥åº¦é‡æ–¹æ³•ä¼˜äºç°æœ‰åº¦é‡æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¾®è°ƒååŒ¹é…äº†æœ€å…ˆè¿›çš„åº¦é‡æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥åº¦é‡åœ¨ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå¦‚ç¨³å¥çš„å›¾åƒæ£€ç´¢ï¼Œè¿™åœ¨åº”ç”¨äºâ€œä¸é€‚åˆå·¥ä½œâ€ï¼ˆNSFWï¼‰å†…å®¹æ£€æµ‹å’Œæ•°æ®é›†è¿‡æ»¤æ—¶å°¤å…¶é‡è¦ã€‚è™½ç„¶æ ‡å‡†æ„ŸçŸ¥åº¦é‡å¾ˆå®¹æ˜“å—åˆ°å¾®å°æ‰°åŠ¨çš„å½±å“è€Œå®Œå…¨ç ´åNSFWæ£€æµ‹ï¼Œä½†æˆ‘ä»¬çš„ç¨³å¥æ„ŸçŸ¥åº¦é‡åœ¨å—åˆ°æ”»å‡»æ—¶ä»èƒ½ç»´æŒé«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¯¹æœªå—æ‰°åŠ¨å›¾åƒçš„æ€§èƒ½ç›¸ä¼¼ã€‚æœ€åï¼Œç”±ç¨³å¥CLIPæ¨¡å‹è¯±å¯¼çš„æ„ŸçŸ¥åº¦é‡å…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§ï¼šç‰¹å¾åè½¬å¯ä»¥æ˜¾ç¤ºå“ªäº›å›¾åƒè¢«è§†ä¸ºç›¸ä¼¼ï¼Œè€Œæ–‡æœ¬åè½¬å¯ä»¥æ‰¾åˆ°ä¸ç»™å®šæç¤ºç›¸å…³è”çš„å›¾åƒã€‚è¿™ä¹Ÿè®©æˆ‘ä»¬èƒ½å¤Ÿå¯è§†åŒ–CLIPæ¨¡å‹å­¦ä¹ çš„ä¸°å¯Œè§†è§‰æ¦‚å¿µï¼ŒåŒ…æ‹¬è®°å¿†çš„äººç‰©ã€ç”»ä½œå’Œå¤æ‚æŸ¥è¯¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11725v1">PDF</a> This work has been accepted for publication in the IEEE Conference on   Secure and Trustworthy Machine Learning (SaTML). The final version will be   available on IEEE Xplore</p>
<p><strong>Summary</strong><br>åŸºäºç¥ç»ç½‘ç»œçš„ç‰¹å¾æå–æŠ€æœ¯æˆä¸ºäº†è®¡ç®—æœºè§†è§‰ä¸­æµ‹é‡æ„ŸçŸ¥ç›¸ä¼¼æ€§çš„å…³é”®å·¥å…·ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¯¹æŠ—é²æ£’CLIPæ¨¡å‹ï¼ˆç§°ä¸ºR-CLIP$_F$ï¼‰è¿›è¡Œæ— ç›‘ç£å¯¹æŠ—å¾®è°ƒï¼Œå¯ä»¥å¾—åˆ°æ›´å¥½çš„å¯¹æŠ—é²æ£’æ„ŸçŸ¥åº¦é‡ï¼Œå®ƒåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰åº¦é‡æ–¹æ³•ï¼Œå¹¶åœ¨å¾®è°ƒåä¾ç„¶ä¿æŒé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ„ŸçŸ¥åº¦é‡åœ¨ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚é²æ£’å›¾åƒåˆ°å›¾åƒæ£€ç´¢ï¼‰ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨â€œéå®‰å…¨å·¥ä½œç¯å¢ƒâ€ï¼ˆNSFWï¼‰å†…å®¹æ£€æµ‹å’Œç­›é€‰åº”ç”¨ä¸­å°¤ä¸ºé‡è¦ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ„ŸçŸ¥åº¦é‡å®¹æ˜“è¢«æ”»å‡»çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„é²æ£’æ„ŸçŸ¥åº¦é‡åœ¨è¢«æ”»å‡»æ—¶ä»èƒ½ç»´æŒé«˜ç²¾ç¡®åº¦ï¼ŒåŒæ—¶åœ¨ä¸å—å¹²æ‰°çš„å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ã€‚æœ€åï¼Œé²æ£’CLIPæ¨¡å‹è¯±å¯¼çš„æ„ŸçŸ¥åº¦é‡å…·å¤‡æ›´é«˜çš„å¯è§£é‡Šæ€§ï¼Œé€šè¿‡ç‰¹å¾åè½¬å¯ä»¥å±•ç¤ºå“ªäº›å›¾åƒè¢«è§†ä¸ºç›¸ä¼¼ï¼Œæ–‡æœ¬åè½¬åˆ™èƒ½æ‰¾åˆ°ä¸ç»™å®šæç¤ºç›¸å…³çš„å›¾åƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯è§†åŒ–CLIPæ¨¡å‹å­¦åˆ°çš„ä¸°å¯Œè§†è§‰æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—é²æ£’CLIPæ¨¡å‹ï¼ˆR-CLIP$_F$ï¼‰é€šè¿‡æ— ç›‘ç£å¯¹æŠ—å¾®è°ƒæä¾›æ›´ä½³çš„æ„ŸçŸ¥åº¦é‡ã€‚</li>
<li>è¯¥æ„ŸçŸ¥åº¦é‡åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰ä¼˜å¼‚è¡¨ç°ï¼Œå¹¶èƒ½åœ¨å¾®è°ƒåä¿æŒé²æ£’æ€§ã€‚</li>
<li>é²æ£’æ„ŸçŸ¥åº¦é‡åœ¨ç›¸å…³ä»»åŠ¡å¦‚å›¾åƒæ£€ç´¢ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨NSFWå†…å®¹æ£€æµ‹å’Œç­›é€‰æ–¹é¢æ›´å…·å®é™…æ„ä¹‰ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿæ„ŸçŸ¥åº¦é‡ï¼Œé²æ£’æ„ŸçŸ¥åº¦é‡åœ¨é­å—æ”»å‡»æ—¶ä»èƒ½ç»´æŒé«˜ç²¾ç¡®åº¦ã€‚</li>
<li>é²æ£’CLIPæ¨¡å‹è¯±å¯¼çš„æ„ŸçŸ¥åº¦é‡å…·å¤‡é«˜å¯è§£é‡Šæ€§ï¼Œèƒ½é€šè¿‡ç‰¹å¾åè½¬å’Œæ–‡æœ¬åè½¬å±•ç¤ºæ¨¡å‹çš„è§†è§‰æ¦‚å¿µå­¦ä¹ ã€‚</li>
<li>è¯¥æŠ€æœ¯èƒ½å¯è§†åŒ–å¤æ‚æŸ¥è¯¢å’Œè®°å¿†ä¸­çš„å›¾åƒæ¦‚å¿µï¼Œå¦‚äººç‰©ã€ç”»ä½œç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6aa69f02259427a23bf07eac908d2c41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55cac78535f863c79a83fd51bddcb05b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd6f6dfbdd92acf476c0f4321b5f25c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2139d8825a0b7dc5dd6effe2e8be66c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automatic-Quality-Assessment-of-First-Trimester-Crown-Rump-Length-Ultrasound-Images"><a href="#Automatic-Quality-Assessment-of-First-Trimester-Crown-Rump-Length-Ultrasound-Images" class="headerlink" title="Automatic Quality Assessment of First Trimester Crown-Rump-Length   Ultrasound Images"></a>Automatic Quality Assessment of First Trimester Crown-Rump-Length   Ultrasound Images</h2><p><strong>Authors:Sevim Cengiz, Ibraheem Hamdi, Mohammad Yaqub</strong></p>
<p>Fetal gestational age (GA) is vital clinical information that is estimated during pregnancy in order to assess fetal growth. This is usually performed by measuring the crown-rump-length (CRL) on an ultrasound image in the Dating scan which is then correlated with fetal age and growth trajectory. A major issue when performing the CRL measurement is ensuring that the image is acquired at the correct view, otherwise it could be misleading. Although clinical guidelines specify the criteria for the correct CRL view, sonographers may not regularly adhere to such rules. In this paper, we propose a new deep learning-based solution that is able to verify the adherence of a CRL image to clinical guidelines in order to assess image quality and facilitate accurate estimation of GA. We first segment out important fetal structures then use the localized structures to perform a clinically-guided mapping that verifies the adherence of criteria. The segmentation method combines the benefits of Convolutional Neural Network (CNN) and the Vision Transformer (ViT) to segment fetal structures in ultrasound images and localize important fetal landmarks. For segmentation purposes, we compare our proposed work with UNet and show that our CNN&#x2F;ViT-based method outperforms an optimized version of UNet. Furthermore, we compare the output of the mapping with classification CNNs when assessing the clinical criteria and the overall acceptability of CRL images. We show that the proposed mapping is not only explainable but also more accurate than the best performing classification CNNs. </p>
<blockquote>
<p>èƒå„¿èƒé¾„ï¼ˆGAï¼‰æ˜¯å­•æœŸè¯„ä¼°èƒå„¿ç”Ÿé•¿çš„é‡è¦ä¸´åºŠä¿¡æ¯ã€‚è¿™é€šå¸¸é€šè¿‡åœ¨è¶…å£°æ£€æŸ¥æ—¶æµ‹é‡å¤´è‡€é•¿ï¼ˆCRLï¼‰æ¥å®Œæˆï¼Œç„¶åå°†æµ‹é‡çš„ç»“æœä¸èƒå„¿çš„å¹´é¾„å’Œç”Ÿé•¿è½¨è¿¹è¿›è¡Œå…³è”ã€‚åœ¨è¿›è¡ŒCRLæµ‹é‡æ—¶é¢ä¸´çš„ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ç¡®ä¿è·å–çš„å›¾åƒæ˜¯ç¬¦åˆæ ‡å‡†çš„è§†è§’ï¼Œå¦åˆ™å¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼ã€‚è™½ç„¶ä¸´åºŠæŒ‡å—è§„å®šäº†æ­£ç¡®çš„CRLè§†è§’çš„è¯„ä¼°æ ‡å‡†ï¼Œä½†è¶…å£°åŒ»ç”Ÿå¯èƒ½å¹¶ä¸æ€»æ˜¯éµå¾ªè¿™äº›è§„å®šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤ŸéªŒè¯CRLå›¾åƒæ˜¯å¦ç¬¦åˆä¸´åºŠæŒ‡å—çš„è§„å®šï¼Œä»è€Œè¯„ä¼°å›¾åƒè´¨é‡å¹¶æœ‰åŠ©äºå‡†ç¡®ä¼°ç®—GAã€‚æˆ‘ä»¬é¦–å…ˆå¯¹é‡è¦çš„èƒå„¿ç»“æ„è¿›è¡Œåˆ†å‰²ï¼Œç„¶åä½¿ç”¨å®šä½çš„ç»“æ„è¿›è¡Œä¸´åºŠæŒ‡å¯¼æ˜ å°„ï¼Œä»¥éªŒè¯æ˜¯å¦æ»¡è¶³æ ‡å‡†ã€‚åˆ†å‰²æ–¹æ³•ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„ä¼˜ç‚¹ï¼Œç”¨äºåˆ†å‰²è¶…å£°å›¾åƒä¸­çš„èƒå„¿ç»“æ„å¹¶å®šä½é‡è¦çš„èƒå„¿åœ°æ ‡ã€‚ä¸ºäº†åˆ†å‰²çš„ç›®çš„ï¼Œæˆ‘ä»¬å°†æ‰€æå‡ºçš„å·¥ä½œä¸UNetè¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„CNN&#x2F;ViTæ–¹æ³•ä¼˜äºä¼˜åŒ–åçš„UNetç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œåœ¨è¯„ä¼°ä¸´åºŠæ ‡å‡†å’ŒCRLå›¾åƒçš„æ•´ä½“å¯æ¥å—æ€§æ—¶ï¼Œæˆ‘ä»¬å°†æ˜ å°„çš„è¾“å‡ºä¸åˆ†ç±»CNNè¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„æ˜ å°„ä¸ä»…å…·æœ‰å¯è§£é‡Šæ€§ï¼Œè€Œä¸”æ¯”è¡¨ç°æœ€ä½³çš„åˆ†ç±»CNNæ›´å‡†ç¡®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10908v1">PDF</a> 9 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºéªŒè¯èƒå„¿å† è‡€é•¿ï¼ˆCRLï¼‰å›¾åƒçš„é‡‡é›†æ˜¯å¦ç¬¦åˆä¸´åºŠæŒ‡å—è§„å®šï¼Œä»¥æé«˜å›¾åƒè´¨é‡å¹¶å‡†ç¡®ä¼°è®¡èƒé¾„ï¼ˆGAï¼‰ã€‚è¯¥ç ”ç©¶ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œè¯†åˆ«é‡è¦èƒå„¿ç»“æ„å¹¶å®šä½å…³é”®èƒå„¿åœ°æ ‡ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ–¹æ³•åœ¨éªŒè¯CRLå›¾åƒæ˜¯å¦ç¬¦åˆä¸´åºŠæŒ‡å—æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•èƒ½æé«˜ä¼°è®¡GAçš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒé¾„ï¼ˆGAï¼‰çš„ä¼°è®¡æ˜¯å­•æœŸè¯„ä¼°èƒå„¿ç”Ÿé•¿çš„é‡è¦ä¸´åºŠä¿¡æ¯ã€‚<br>2.CRLæµ‹é‡åœ¨è¯„ä¼°èƒé¾„ä¸­éå¸¸å…³é”®ï¼Œä½†å›¾åƒè·å–è§’åº¦çš„æ­£ç¡®æ€§å¯¹æµ‹é‡ç»“æœçš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>è™½ç„¶æœ‰ä¸´åºŠæŒ‡å—è§„å®šCRLè§†å›¾çš„æ ‡å‡†ï¼Œä½†è¶…å£°åŒ»å¸ˆå¯èƒ½å¹¶ä¸æ€»æ˜¯éµå¾ªè¿™äº›è§„åˆ™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆæ¥éªŒè¯CRLå›¾åƒæ˜¯å¦ç¬¦åˆä¸´åºŠæŒ‡å—ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†CNNå’ŒViTè¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œä»¥è¯†åˆ«å’Œå®šä½é‡è¦çš„èƒå„¿ç»“æ„å’Œåœ°æ ‡ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éªŒè¯CRLå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ee54057a92a6002d15f60bd8615e3be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4ecb275882595dea81d0e42a6c5750d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fa4cb899ecc9672764347ac515fb161.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-CNN-Approach-to-Automated-Detection-and-Classification-of-Brain-Tumors"><a href="#A-CNN-Approach-to-Automated-Detection-and-Classification-of-Brain-Tumors" class="headerlink" title="A CNN Approach to Automated Detection and Classification of Brain Tumors"></a>A CNN Approach to Automated Detection and Classification of Brain Tumors</h2><p><strong>Authors:Md. Zahid Hasan, Abdullah Tamim, D. M. Asadujjaman, Md. Mahfujur Rahman, Md. Abu Ahnaf Mollick, Nosin Anjum Dristi,  Abdullah-Al-Noman</strong></p>
<p>Brain tumors require an assessment to ensure timely diagnosis and effective patient treatment. Morphological factors such as size, location, texture, and variable appearance com- plicate tumor inspection. Medical imaging presents challenges, including noise and incomplete images. This research article presents a methodology for processing Magnetic Resonance Imag- ing (MRI) data, encompassing techniques for image classification and denoising. The effective use of MRI images allows medical professionals to detect brain disorders, including tumors. This research aims to categorize healthy brain tissue and brain tumors by analyzing the provided MRI data. Unlike alternative methods like Computed Tomography (CT), MRI technology offers a more detailed representation of internal anatomical components, mak- ing it a suitable option for studying data related to brain tumors. The MRI picture is first subjected to a denoising technique utilizing an Anisotropic diffusion filter. The dataset utilized for the models creation is a publicly accessible and validated Brain Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE was employed for data augmentation and dataset balancing. Convolutional Neural Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for the classification procedure. EfficientNet attained an accuracy of 98%, the highest recorded. </p>
<blockquote>
<p>å¯¹è„‘è‚¿ç˜¤è¿›è¡Œè¯„ä¼°æ˜¯ç¡®ä¿åŠæ—¶è¯Šæ–­å’Œæ²»ç–—çš„å…³é”®ã€‚è‚¿ç˜¤çš„å½¢æ€å› ç´ ï¼Œå¦‚å¤§å°ã€ä½ç½®ã€çº¹ç†å’Œå¤–è§‚å˜åŒ–ï¼Œä½¿è‚¿ç˜¤æ£€æµ‹å˜å¾—å¤æ‚ã€‚åŒ»å­¦æˆåƒé¢ä¸´å™ªå£°å’Œå›¾åƒä¸å®Œæ•´ç­‰æŒ‘æˆ˜ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§å¤„ç†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œå»å™ªæŠ€æœ¯ã€‚æœ‰æ•ˆåœ°ä½¿ç”¨MRIå›¾åƒå¯ä»¥è®©åŒ»ç–—ä¸“ä¸šäººå£«æ£€æµ‹åŒ…æ‹¬è‚¿ç˜¤åœ¨å†…çš„è„‘éƒ¨ç–¾ç—…ã€‚è¿™é¡¹ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ†ææä¾›çš„MRIæ•°æ®æ¥åŒºåˆ†å¥åº·çš„å¤§è„‘ç»„ç»‡å’Œè„‘è‚¿ç˜¤ã€‚ä¸è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰æ›¿ä»£æ–¹æ³•ä¸åŒï¼ŒMRIæŠ€æœ¯æä¾›æ›´è¯¦ç»†çš„å†…éƒ¨è§£å‰–ç»“æ„è¡¨ç¤ºï¼Œä½¿å…¶æˆä¸ºç ”ç©¶è„‘è‚¿ç˜¤ç›¸å…³æ•°æ®çš„ç†æƒ³é€‰æ‹©ã€‚MRIå›¾åƒé¦–å…ˆé‡‡ç”¨å„å‘å¼‚æ€§æ‰©æ•£æ»¤æ³¢å™¨è¿›è¡Œå»å™ªå¤„ç†ã€‚ç”¨äºæ¨¡å‹åˆ›å»ºçš„æ•°æ®é›†æ˜¯å…¬å¼€å¯è®¿é—®ä¸”ç»è¿‡éªŒè¯çš„Brain Tumour Classificationï¼ˆMRIï¼‰æ•°æ®åº“ï¼ŒåŒ…å«3264ä¸ªè„‘éƒ¨MRIæ‰«æã€‚é‡‡ç”¨SMOTEè¿›è¡Œæ•°æ®å¢å¼ºå’Œæ•°æ®é›†å¹³è¡¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¦‚ResNet152V2ã€VGGã€ViTå’ŒEfficientNetè¢«ç”¨äºåˆ†ç±»è¿‡ç¨‹ã€‚EfficientNetè¾¾åˆ°äº†98%çš„å‡†ç¡®ç‡ï¼Œä¸ºç›®å‰è®°å½•çš„æœ€é«˜å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09731v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤„ç†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å›¾åƒåˆ†ç±»å’Œå»å™ªæŠ€æœ¯æ¥æ£€æµ‹è„‘éƒ¨è‚¿ç˜¤ã€‚ç ”ç©¶ä½¿ç”¨MRIå›¾åƒæ¥åŒºåˆ†å¥åº·è„‘ç»„ç»‡å’Œè„‘éƒ¨è‚¿ç˜¤ï¼Œå¹¶ä½¿ç”¨å…¬å¼€éªŒè¯çš„Brain Tumour Classificationï¼ˆMRIï¼‰æ•°æ®åº“æ„å»ºæ¨¡å‹ã€‚é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œåˆ†ç±»ï¼Œå…¶ä¸­EfficientNetçš„å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°98%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨äºä½¿ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®å¤„ç†æ–¹æ³•ï¼Œä»¥æ£€æµ‹å¹¶åˆ†ç±»è„‘éƒ¨è‚¿ç˜¤ã€‚</li>
<li>åŒ»å­¦æˆåƒåœ¨è‚¿ç˜¤æ£€æµ‹ä¸­é¢ä¸´å™ªå£°å’Œå›¾åƒä¸å®Œæ•´ç­‰æŒ‘æˆ˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å»å™ªæŠ€æœ¯ï¼Œé‡‡ç”¨Anisotropicæ‰©æ•£æ»¤æ³¢å™¨å¤„ç†MRIå›¾åƒã€‚</li>
<li>ä½¿ç”¨å…¬å¼€ä¸”ç»è¿‡éªŒè¯çš„Brain Tumour Classificationï¼ˆMRIï¼‰æ•°æ®åº“è¿›è¡Œæ¨¡å‹æ„å»ºã€‚</li>
<li>æ•°æ®å¢å¼ºå’Œå¹³è¡¡æ•°æ®é›†é‡‡ç”¨äº†SMOTEæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¤šç§å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬ResNet152V2ã€VGGã€ViTå’ŒEfficientNetã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d2c4dede4bfe5db66741d6e47dcfb44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acb180e5dd64d2570c6cd4ed88f9996e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-309dc569c0044af9cc01efb31dd22d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19b4c51a7858da0fef6ac0b944a8fb6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8221213d9c418709bc9cd17c636908e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-294095a16124d8e9c36326bcb62a13a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f225e251f469c56283b593d8945edef.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Experts-Made-Personalized-Federated-Prompt-Learning-for-Vision-Language-Models"><a href="#Mixture-of-Experts-Made-Personalized-Federated-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="Mixture of Experts Made Personalized: Federated Prompt Learning for   Vision-Language Models"></a>Mixture of Experts Made Personalized: Federated Prompt Learning for   Vision-Language Models</h2><p><strong>Authors:Jun Luo, Chen Chen, Shandong Wu</strong></p>
<p>Federated prompt learning benefits federated learning with CLIP-like Vision-Language Modelâ€™s (VLMâ€™s) robust representation learning ability through prompt learning. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating clients are generally only allowed to download a single globally aggregated model from the server. While justifiable for training full-sized models under federated settings, in this work, we argue that this paradigm is ill-suited for lightweight prompts. By facilitating the clients to download multiple pre-aggregated prompts as fixed non-local experts, we propose Personalized Federated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning process through the lens of Mixture of Experts (MoE). pFedMoAP implements a local attention-based gating network that learns to generate enhanced text features for better alignment with local image data, benefiting from both local and downloaded non-local adaptive prompt experts. Extensive experiments on 9 datasets under various federated settings demonstrate the efficacy of the proposed pFedMoAP algorithm. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ljaiverson/pFedMoAP">https://github.com/ljaiverson/pFedMoAP</a>. </p>
<blockquote>
<p>è”é‚¦æç¤ºå­¦ä¹ é€šè¿‡æç¤ºå­¦ä¹ èƒ½åŠ›æå‡äº†è”é‚¦å­¦ä¹ ä¸CLIPç±»ä¼¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç¨³å¥è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„è”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸å—é™äºä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰èŒƒå¼ï¼Œåœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œå‚ä¸å®¢æˆ·ç«¯é€šå¸¸åªå…è®¸ä»æœåŠ¡å™¨ä¸‹è½½ä¸€ä¸ªå…¨å±€èšåˆæ¨¡å‹ã€‚åœ¨è”é‚¦è®¾ç½®ä¸‹è®­ç»ƒå…¨å°ºå¯¸æ¨¡å‹æ—¶è¿™ç§åšæ³•æ˜¯åˆç†çš„ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§èŒƒå¼ä¸é€‚ç”¨äºè½»é‡çº§æç¤ºã€‚é€šè¿‡ä¿ƒè¿›å®¢æˆ·ç«¯ä¸‹è½½å¤šä¸ªé¢„å…ˆèšåˆçš„æç¤ºä½œä¸ºå›ºå®šçš„éå±€éƒ¨ä¸“å®¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–è”é‚¦æ··åˆè‡ªé€‚åº”æç¤ºï¼ˆpFedMoAPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è§†è§’ä¸ªæ€§åŒ–æç¤ºå­¦ä¹ è¿‡ç¨‹çš„æ–°å‹è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚pFedMoAPå®ç°äº†ä¸€ä¸ªåŸºäºæœ¬åœ°æ³¨æ„åŠ›çš„é—¨æ§ç½‘ç»œï¼Œå­¦ä¹ ç”Ÿæˆå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°ä¸æœ¬åœ°å›¾åƒæ•°æ®å¯¹é½ï¼Œå—ç›Šäºæœ¬åœ°å’Œä¸‹è½½çš„éå±€éƒ¨è‡ªé€‚åº”æç¤ºä¸“å®¶ã€‚åœ¨å¤šç§è”é‚¦è®¾ç½®ä¸‹çš„9ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†pFedMoAPç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ljaiverson">https://github.com/ljaiverson</a> è®¿é—®åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10114v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>åŸºäºCLIPç±»ä¼¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¼ºå¤§è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼Œè”é‚¦æç¤ºå­¦ä¹ å¯¹è”é‚¦å­¦ä¹ äº§ç”Ÿäº†ç§¯æå½±å“ã€‚ä¼ ç»Ÿçš„è”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸å±€é™äºä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ æ¨¡å¼ï¼Œåªå…è®¸å®¢æˆ·ç«¯ä»æœåŠ¡å™¨ä¸‹è½½å•ä¸€çš„å…¨å±€èšåˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œæœ¬æ–‡è®¤ä¸ºè¿™ç§æ¨¡å¼ä¸é€‚åˆè½»é‡çº§æç¤ºã€‚é€šè¿‡å…è®¸å®¢æˆ·ç«¯ä¸‹è½½å¤šä¸ªé¢„å…ˆèšåˆçš„æç¤ºä½œä¸ºå›ºå®šçš„éæœ¬åœ°ä¸“å®¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–è”é‚¦æ··åˆè‡ªé€‚åº”æç¤ºï¼ˆpFedMoAPï¼‰è¿™ä¸€æ–°å‹è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„è§†è§’å®ç°äº†ä¸ªæ€§åŒ–çš„æç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚pFedMoAPé‡‡ç”¨åŸºäºæœ¬åœ°æ³¨æ„åŠ›æœºåˆ¶çš„é—¨æ§ç½‘ç»œï¼Œå­¦ä¹ ç”Ÿæˆå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°ä¸æœ¬åœ°å›¾åƒæ•°æ®å¯¹é½ï¼Œå—ç›Šäºæœ¬åœ°å’Œä¸‹è½½çš„éæœ¬åœ°è‡ªé€‚åº”æç¤ºä¸“å®¶ã€‚å®éªŒåœ¨ä¹ä¸ªæ•°æ®é›†å’Œå„ç§è”é‚¦è®¾ç½®ä¸‹éªŒè¯äº†pFedMoAPç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/ljaiverson/pFedMoAP%E3%80%82">https://github.com/ljaiverson/pFedMoAPã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è”é‚¦æç¤ºå­¦ä¹ é€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è”é‚¦å­¦ä¹ ä¸­å¼•å…¥äº†ä¸€ç§æ–°çš„å­¦ä¹ æ¨¡å¼ã€‚</li>
<li>ä¼ ç»Ÿè”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–äºå•ä¸€å…¨å±€èšåˆæ¨¡å‹ï¼Œä½†è¿™ç§æ–¹å¼åœ¨è½»é‡çº§æç¤ºåœºæ™¯ä¸‹å¯èƒ½ä¸å¤Ÿçµæ´»ã€‚</li>
<li>pFedMoAPæ¡†æ¶å…è®¸å®¢æˆ·ç«¯ä¸‹è½½å¤šä¸ªé¢„å…ˆèšåˆçš„æç¤ºä½œä¸ºéæœ¬åœ°ä¸“å®¶ï¼Œå®ç°äº†ä¸ªæ€§åŒ–çš„æç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>pFedMoAPé‡‡ç”¨åŸºäºæœ¬åœ°æ³¨æ„åŠ›æœºåˆ¶çš„é—¨æ§ç½‘ç»œï¼Œä»¥ç”Ÿæˆä¸æœ¬åœ°å›¾åƒæ•°æ®å¯¹é½çš„å¢å¼ºæ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†æœ¬åœ°å’Œä¸‹è½½çš„éæœ¬åœ°è‡ªé€‚åº”æç¤ºä¸“å®¶çš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒçš„è”é‚¦è®¾ç½®ä¸‹è¿›è¡Œçš„å®éªŒéªŒè¯äº†pFedMoAPç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d579f330f1812b9b135483e192e62d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fa4948f80bbbc65df64185622faeb88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-106d3744acf009344de64512364352e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ExPLoRA-Parameter-Efficient-Extended-Pre-Training-to-Adapt-Vision-Transformers-under-Domain-Shifts"><a href="#ExPLoRA-Parameter-Efficient-Extended-Pre-Training-to-Adapt-Vision-Transformers-under-Domain-Shifts" class="headerlink" title="ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts"></a>ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts</h2><p><strong>Authors:Samar Khanna, Medhanie Irgau, David B. Lobell, Stefano Ermon</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation (LoRA) can effectively adapt large pre-trained foundation models to downstream tasks using only a small fraction (0.1%-10%) of the original trainable weights. An under-explored question of PEFT is in extending the pre-training phase without supervised labels; that is, can we adapt a pre-trained foundation model to a new domain via efficient self-supervised pre-training on this new domain? In this work, we introduce ExPLoRA, a highly effective technique to improve transfer learning of pre-trained vision transformers (ViTs) under domain shifts. Initializing a ViT with pre-trained weights on large, natural-image datasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised pre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks and tuning all other layers with LoRA. We then fine-tune the resulting model only with LoRA on this new domain for supervised learning. Our experiments demonstrate state-of-the-art results on satellite imagery, even outperforming fully pre-training and fine-tuning ViTs. Using the DinoV2 training objective, we demonstrate up to 8% improvement in linear probing top-1 accuracy on downstream tasks while using &lt;10% of the number of parameters that are used in prior fully-tuned state-of-the art approaches. Our ablation studies confirm the efficacy of our approach over other baselines, including PEFT and unfreezing more ViT blocks. Code is available on the project website: <a target="_blank" rel="noopener" href="https://samar-khanna.github.io/ExPLoRA/">https://samar-khanna.github.io/ExPLoRA/</a> </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä½¿å¤§å‹é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»…ä½¿ç”¨åŸå§‹å¯è®­ç»ƒæƒé‡çš„æå°éƒ¨åˆ†ï¼ˆ0.1%-10%ï¼‰ã€‚å…³äºPEFTçš„ä¸€ä¸ªå°šæœªæ¢ç´¢çš„é—®é¢˜æ˜¯æ‰©å±•é¢„è®­ç»ƒé˜¶æ®µè€Œæ— éœ€ç›‘ç£æ ‡ç­¾ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æ˜¯å¦å¯ä»¥é€šè¿‡æ–°é¢†åŸŸä¸Šçš„é«˜æ•ˆè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå°†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹é€‚åº”åˆ°æ–°é¢†åŸŸï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ExPLoRAï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆæé«˜é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨é¢†åŸŸè½¬ç§»ä¸­çš„è¿ç§»å­¦ä¹ èƒ½åŠ›çš„é«˜æ•ˆæŠ€æœ¯ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†ï¼ˆå¦‚DinoV2æˆ–MAEï¼‰ä¸Šåˆå§‹åŒ–çš„é¢„è®­ç»ƒæƒé‡æ¥å¯åŠ¨ViTï¼ŒExPLoRAç»§ç»­åœ¨æ–°é¢†åŸŸä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œè§£å†»1-2ä¸ªé¢„è®­ç»ƒçš„ViTå—å¹¶ç”¨LoRAè°ƒæ•´æ‰€æœ‰å…¶ä»–å±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨LoRAåœ¨æ­¤æ–°é¢†åŸŸä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å«æ˜Ÿå›¾åƒä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œç”šè‡³è¶…è¿‡äº†å®Œå…¨é¢„è®­ç»ƒå’Œå¾®è°ƒViTsã€‚ä½¿ç”¨DinoV2è®­ç»ƒç›®æ ‡ï¼Œæˆ‘ä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„çº¿æ€§æ¢æµ‹top-1å‡†ç¡®ç‡ä¸Šæé«˜äº†é«˜è¾¾8%ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°å°‘äºå…ˆå‰å®Œå…¨è°ƒæ•´è¿‡çš„æœ€æ–°æ–¹æ³•æ‰€ä½¿ç”¨çš„å‚æ•°çš„10%ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬PEFTå’Œè§£å†»æ›´å¤šViTå—çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://samar-khanna.github.io/ExPLoRA/">https://samar-khanna.github.io/ExPLoRA/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10973v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼ˆPEFTï¼‰å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œåœ¨æ–°é¢†åŸŸä¸Šå¯¹é¢„è®­ç»ƒçš„è§†è§‰Transformerï¼ˆViTï¼‰è¿›è¡Œè½¬ç§»å­¦ä¹ ã€‚æ–‡ç« æå‡ºäº†ExPLoRAæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç»§ç»­æ— ç›‘ç£é¢„è®­ç»ƒç›®æ ‡çš„åŒæ—¶ï¼Œåªè§£å†»1-2ä¸ªé¢„è®­ç»ƒçš„ViTå—ï¼Œå¹¶ä½¿ç”¨LoRAè°ƒæ•´æ‰€æœ‰å…¶ä»–å±‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å«æ˜Ÿå›¾åƒç­‰é¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œç”šè‡³è¶…è¿‡äº†å®Œå…¨é¢„è®­ç»ƒå’Œå¾®è°ƒViTçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ExPLoRAæ–¹æ³•åˆ©ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼ˆPEFTï¼‰å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œé¢„è®­ç»ƒè§†è§‰Transformerçš„è½¬ç§»å­¦ä¹ ã€‚</li>
<li>ExPLoRAæ–¹æ³•å¯ä»¥åœ¨æ–°é¢†åŸŸä¸Šç»§ç»­æ— ç›‘ç£é¢„è®­ç»ƒï¼ŒåŒæ—¶åªè§£å†»éƒ¨åˆ†é¢„è®­ç»ƒçš„ViTå—ã€‚</li>
<li>ä½¿ç”¨LoRAå¯¹ViTçš„æ‰€æœ‰å…¶ä»–å±‚è¿›è¡Œè°ƒæ•´ï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒExPLoRAæ–¹æ³•åœ¨å«æ˜Ÿå›¾åƒç­‰é¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>ä¸å®Œå…¨é¢„è®­ç»ƒå’Œå¾®è°ƒViTç›¸æ¯”ï¼ŒExPLoRAæ–¹æ³•åœ¨ä½¿ç”¨æ›´å°‘å‚æ•°çš„æƒ…å†µä¸‹ï¼Œä»ç„¶è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« æä¾›çš„ç½‘ç«™é“¾æ¥åŒ…å«ç›¸å…³ä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-703d4ddec993f3a7328ef42d7a643333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d689fd363c30dbf1809d7a2f9e14be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51cb3d62c4d50fb27ddce9ade3c7904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e0174dde54ba2acdd75a50fc9406a5c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="REP-Resource-Efficient-Prompting-for-Rehearsal-Free-Continual-Learning"><a href="#REP-Resource-Efficient-Prompting-for-Rehearsal-Free-Continual-Learning" class="headerlink" title="REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning"></a>REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning</h2><p><strong>Authors:Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon</strong></p>
<p>Recent rehearsal-free methods, guided by prompts, excel in vision-related continual learning (CL) with drifting data but lack resource efficiency, making real-world deployment challenging. In this paper, we introduce Resource-Efficient Prompting (REP), which improves the computational and memory efficiency of prompt-based rehearsal-free methods while minimizing accuracy trade-offs. Our approach employs swift prompt selection to refine input data using a carefully provisioned model and introduces adaptive token merging (AToM) and layer dropping (ALD) for efficient prompt updates. AToM and ALD selectively skip data and model layers while preserving task-specific features during new-task learning. Extensive experiments on multiple image classification datasets demonstrates REPâ€™s superior resource efficiency over state-of-the-art ViT- and CNN-based methods. </p>
<blockquote>
<p>æœ€è¿‘çš„æ— å¤è¿°æ–¹æ³•ï¼Œåœ¨æç¤ºçš„å¼•å¯¼ä¸‹ï¼Œåœ¨æ•°æ®æ¼‚ç§»çš„æƒ…å¢ƒä¸‹æ“…é•¿è§†è§‰ç›¸å…³çš„æŒç»­å­¦ä¹ ï¼ˆCLï¼‰ï¼Œä½†èµ„æºåˆ©ç”¨æ•ˆç‡ä¸è¶³ï¼Œç»™å®é™…éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†èµ„æºé«˜æ•ˆæç¤ºï¼ˆREPï¼‰ï¼Œå®ƒæé«˜äº†åŸºäºæç¤ºçš„æ— å¤è¿°æ–¹æ³•çš„è®¡ç®—å’Œå†…å­˜æ•ˆç‡ï¼ŒåŒæ—¶æœ€å°åŒ–äº†å‡†ç¡®åº¦çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¿«é€Ÿæç¤ºé€‰æ‹©ï¼Œä½¿ç”¨ç²¾å¿ƒæä¾›çš„æ¨¡å‹å¯¹è¾“å…¥æ•°æ®è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œå¹¶å¼•å…¥äº†è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼ˆAToMï¼‰å’Œå±‚ä¸¢å¼ƒï¼ˆALDï¼‰æ¥è¿›è¡Œé«˜æ•ˆçš„æç¤ºæ›´æ–°ã€‚AToMå’ŒALDæœ‰é€‰æ‹©åœ°è·³è¿‡æ•°æ®å’Œæ¨¡å‹å±‚ï¼ŒåŒæ—¶åœ¨æ–°ä»»åŠ¡å­¦ä¹ ä¸­ä¿ç•™ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾ã€‚åœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒREPåœ¨èµ„æºæ•ˆç‡æ–¹é¢ä¼˜äºæœ€æ–°çš„ViTå’ŒCNNæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04772v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§èµ„æºé«˜æ•ˆçš„æç¤ºæ–¹æ³•ï¼ˆREPï¼‰ï¼Œç”¨äºæ”¹è¿›åŸºäºæç¤ºçš„æ— æ’ç»ƒæ–¹æ³•åœ¨è®¡ç®—å’Œå†…å­˜æ•ˆç‡æ–¹é¢çš„ä¸è¶³ï¼ŒåŒæ—¶å°½é‡å‡å°‘å‡†ç¡®åº¦çš„æŸå¤±ã€‚REPé‡‡ç”¨å¿«é€Ÿæç¤ºé€‰æ‹©æ¥ä¼˜åŒ–è¾“å…¥æ•°æ®ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼ˆAToMï¼‰å’Œå±‚ä¸¢å¼ƒï¼ˆALDï¼‰æ¥è¿›è¡Œé«˜æ•ˆçš„æç¤ºæ›´æ–°ã€‚åœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒREPåœ¨èµ„æºæ•ˆç‡ä¸Šä¼˜äºåŸºäºViTå’ŒCNNçš„æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”èµ„æºé«˜æ•ˆçš„æç¤ºæ–¹æ³•ï¼ˆREPï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›åŸºäºæç¤ºçš„æ— æ’ç»ƒæ–¹æ³•åœ¨è§†è§‰ç›¸å…³çš„æŒç»­å­¦ä¹ ä¸­çš„è®¡ç®—ä¸å†…å­˜æ•ˆç‡ã€‚</li>
<li>REPé€šè¿‡ä½¿ç”¨å¿«é€Ÿæç¤ºé€‰æ‹©æ¥ä¼˜åŒ–è¾“å…¥æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>REPå¼•å…¥äº†è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼ˆAToMï¼‰å’Œå±‚ä¸¢å¼ƒï¼ˆALDï¼‰æŠ€æœ¯ï¼Œä»¥è¿›è¡Œé«˜æ•ˆçš„æç¤ºæ›´æ–°ã€‚</li>
<li>AToMå’ŒALDèƒ½å¤Ÿé€‰æ‹©æ€§åœ°è·³è¿‡æ•°æ®å’Œæ¨¡å‹å±‚ï¼ŒåŒæ—¶ä¿ç•™ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾ï¼Œæœ‰åŠ©äºåœ¨æ–°ä»»åŠ¡å­¦ä¹ ä¸­æé«˜æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒREPåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„èµ„æºæ•ˆç‡ä¼˜äºåŸºäºViTå’ŒCNNçš„å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>REPåœ¨åº”å¯¹æ•°æ®æ¼‚ç§»é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½¿å¾—å…¶åœ¨è§†è§‰ç›¸å…³çš„æŒç»­å­¦ä¹ ä¸­å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ada5b2ed1a86b7acd5cd4721dd4fb5dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ec3f8f01b4204e5027bd0dc2dbeb5dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57bccdcb4dd989ec25236734d4d7ce67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98bdcba8c502c5b145aee5c73645cbcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f7668528c4b536a8d0d39702bf5168c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70da2953625fb18bf69f4d8c96371d1b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synergy-and-Diversity-in-CLIP-Enhancing-Performance-Through-Adaptive-Backbone-Ensembling"><a href="#Synergy-and-Diversity-in-CLIP-Enhancing-Performance-Through-Adaptive-Backbone-Ensembling" class="headerlink" title="Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive   Backbone Ensembling"></a>Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive   Backbone Ensembling</h2><p><strong>Authors:Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Damien Teney, Hamed Damirchi, Edison Marrese-Taylor, Anton van den Hengel</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning. Various architectures, from vision transformers (ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks. This paper explores the differences across various CLIP-trained vision backbones. Despite using the same data and training objective, we find that these architectures have notably different representations, different classification performance across datasets, and different robustness properties to certain types of image perturbations. Our findings indicate a remarkable possible synergy across backbones by leveraging their respective strengths. In principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example.Using this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones. The approach uses as few as one labeled example per class to tune the adaptive combination of backbones. On a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä½œä¸ºå›¾åƒè¡¨ç¤ºå­¦ä¹ çš„ä¸€ç§çªå‡ºæ–¹æ³•è€Œå¤‡å—å…³æ³¨ã€‚ä»è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åˆ°å·ç§¯ç½‘ç»œï¼ˆResNetsï¼‰ï¼Œå„ç§æ¶æ„éƒ½ç»è¿‡CLIPè®­ç»ƒï¼Œä½œä¸ºè§£å†³å„ç§è§†è§‰ä»»åŠ¡çš„ä¸€èˆ¬è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æ¢è®¨äº†ä¸åŒCLIPè®­ç»ƒè§†è§‰éª¨æ¶ä¹‹é—´çš„å·®å¼‚ã€‚å°½ç®¡ä½¿ç”¨äº†ç›¸åŒçš„æ•°æ®å’Œè®­ç»ƒç›®æ ‡ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™äº›æ¶æ„çš„è¡¨ç¤ºæ³•ã€ä¸åŒæ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½ä»¥åŠé’ˆå¯¹æŸäº›ç±»å‹çš„å›¾åƒæ‰°åŠ¨çš„ç¨³å¥æ€§å±æ€§éƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨å„è‡ªçš„ä¼˜ç‚¹ï¼Œå¯ä»¥åœ¨éª¨æ¶ä¹‹é—´å®ç°æ˜¾è‘—çš„ååŒä½œç”¨ã€‚åŸåˆ™ä¸Šï¼Œé€šè¿‡é’ˆå¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬é€‰æ‹©æœ€ä½³éª¨æ¶çš„æ˜æ™ºé€‰æ‹©ï¼Œåˆ†ç±»ç²¾åº¦å¯ä»¥æé«˜40%ä»¥ä¸Šã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„è‡ªé€‚åº”é›†æˆå¤šç§éª¨æ¶çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¯ä¸ªç±»åˆ«çš„ä¸€ä¸ªæ ‡è®°ç¤ºä¾‹æ¥è°ƒæ•´éª¨æ¶çš„è‡ªé€‚åº”ç»„åˆã€‚åœ¨å¤§é‡æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†ç›¸å¯¹äºæœ€ä½³å•ä¸ªéª¨æ¶é«˜è¾¾39.1%çš„å‡†ç¡®åº¦æå‡ï¼Œè¿œè¿œè¶…è¿‡äº†ä¼ ç»Ÿé›†æˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17139v2">PDF</a> ICLR 2025. arXiv admin note: text overlap with arXiv:2312.14400</p>
<p><strong>Summary</strong></p>
<p>CLIPæ–¹æ³•åœ¨å›¾åƒè¡¨ç¤ºå­¦ä¹ é¢†åŸŸè¡¨ç°çªå‡ºï¼Œæœ¬æ–‡æ¢è®¨äº†ä¸åŒCLIPè®­ç»ƒè§†è§‰éª¨æ¶ï¼ˆå¦‚è§†è§‰å˜å‹å™¨å’Œå·ç§¯ç½‘ç»œï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚å°½ç®¡ä½¿ç”¨ç›¸åŒçš„æ•°æ®å’Œè®­ç»ƒç›®æ ‡ï¼Œä½†è¿™äº›æ¶æ„çš„è¡¨ç¤ºã€æ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½å’Œå›¾åƒæ‰°åŠ¨çš„ç¨³å¥æ€§å´å¤§ä¸ç›¸åŒã€‚é€šè¿‡èåˆå„ç§éª¨æ¶çš„ä¼˜åŠ¿ï¼Œåˆ†ç±»ç²¾åº¦å¯æé«˜40%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„è‡ªé€‚åº”é›†æˆå¤šç§éª¨æ¶çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªç±»åˆ«æ ‡ç­¾çš„ä¾‹å­å³å¯è°ƒæ•´éª¨æ¶çš„è‡ªé€‚åº”ç»„åˆï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ç›¸å¯¹äºæœ€ä½³å•ä¸€éª¨æ¶é«˜è¾¾39.1%çš„å‡†ç¡®åº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ˜¯ä¸€ç§ç”¨äºå›¾åƒè¡¨ç¤ºå­¦ä¹ çš„ä¸»æµæ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ä¸åŒCLIPè®­ç»ƒçš„è§†è§‰éª¨æ¶ï¼ˆå¦‚ViTså’ŒResNetsï¼‰å…·æœ‰æ˜¾è‘—ä¸åŒçš„è¡¨ç¤ºã€åˆ†ç±»æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å„ç§éª¨æ¶çš„ç‰¹æ€§ï¼Œå‘ç°å®ƒä»¬ä¹‹é—´çš„ååŒä½œç”¨æ½œåŠ›å·¨å¤§ã€‚</li>
<li>é€‰æ‹©é€‚å½“çš„éª¨æ¶å¯ä»¥æ˜¾è‘—æé«˜åˆ†ç±»ç²¾åº¦ï¼Œæœ€é«˜å¯æé«˜40%ä»¥ä¸Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„è‡ªé€‚åº”é›†æˆå¤šç§éª¨æ¶çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡ç­¾æ ·æœ¬å³å¯è°ƒæ•´éª¨æ¶ç»„åˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ç›¸å¯¹äºå•ä¸€æœ€ä½³éª¨æ¶é«˜è¾¾39.1%çš„å‡†ç¡®åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa9b80e39a08bb5da39f11e70bbb7ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3017c367551679a220b6b3e8188567f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4725a21536046d0db7ca9e762addcf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c104c002d4df13ef274399f8659e5210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f00d4fe6d309d36ceb57e859c10e7028.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Random-Set-Neural-Networks-RS-NN"><a href="#Random-Set-Neural-Networks-RS-NN" class="headerlink" title="Random-Set Neural Networks (RS-NN)"></a>Random-Set Neural Networks (RS-NN)</h2><p><strong>Authors:Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Keivan Shariatmadar, Fabio Cuzzolin</strong></p>
<p>Machine learning is increasingly deployed in safety-critical domains where erroneous predictions may lead to potentially catastrophic consequences, highlighting the need for learning systems to be aware of how confident they are in their own predictions: in other words, â€˜to know when they do not knowâ€™. In this paper, we propose a novel Random-Set Neural Network (RS-NN) approach to classification which predicts belief functions (rather than classical probability vectors) over the class list using the mathematics of random sets, i.e., distributions over the collection of sets of classes. RS-NN encodes the â€˜epistemicâ€™ uncertainty induced by training sets that are insufficiently representative or limited in size via the size of the convex set of probability vectors associated with a predicted belief function. Our approach outperforms state-of-the-art Bayesian and Ensemble methods in terms of accuracy, uncertainty estimation and out-of-distribution (OoD) detection on multiple benchmarks (CIFAR-10 vs SVHN&#x2F;Intel-Image, MNIST vs FMNIST&#x2F;KMNIST, ImageNet vs ImageNet-O). RS-NN also scales up effectively to large-scale architectures (e.g. WideResNet-28-10, VGG16, Inception V3, EfficientNetB2 and ViT-Base-16), exhibits remarkable robustness to adversarial attacks and can provide statistical guarantees in a conformal learning setting. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åœ¨å®‰å…¨æ€§è‡³å…³é‡è¦çš„é¢†åŸŸå¾—åˆ°äº†è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ï¼Œé”™è¯¯çš„é¢„æµ‹å¯èƒ½å¯¼è‡´ç¾éš¾æ€§çš„åæœï¼Œè¿™å‡¸æ˜¾äº†å­¦ä¹ ç³»ç»Ÿéœ€è¦äº†è§£è‡ªå·±å¯¹é¢„æµ‹çš„ä¿¡å¿ƒç¨‹åº¦çš„é‡è¦æ€§ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯â€œè¦çŸ¥é“è‡ªå·±ä¸çŸ¥é“â€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„éšæœºé›†ç¥ç»ç½‘ç»œï¼ˆRS-NNï¼‰åˆ†ç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨éšæœºé›†çš„æ•°å­¦åŸç†é¢„æµ‹ç±»åˆ—è¡¨ä¸Šçš„ä¿¡å¿µå‡½æ•°ï¼ˆè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ¦‚ç‡å‘é‡ï¼‰ã€‚RS-NNé€šè¿‡é¢„æµ‹çš„ä¿¡ä»°å‡½æ•°å…³è”çš„å‡¸æ¦‚ç‡å‘é‡é›†åˆçš„å¤§å°ï¼Œç¼–ç ç”±è®­ç»ƒé›†å¼•èµ·çš„â€œè®¤è¯†è®ºâ€ä¸ç¡®å®šæ€§ï¼Œè¿™ç§ä¸ç¡®å®šæ€§å¯èƒ½æ˜¯ç”±äºè®­ç»ƒé›†ä»£è¡¨æ€§ä¸è¶³æˆ–è§„æ¨¡æœ‰é™é€ æˆçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆå¦‚CIFAR-10ä¸SVHN&#x2F;Intel-Imageã€MNISTä¸FMNIST&#x2F;KMNISTã€ImageNetä¸ImageNet-Oï¼‰ä¸Šçš„å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œç¦»ç¾¤æ£€æµ‹æ–¹é¢å‡ä¼˜äºå…ˆè¿›çš„è´å¶æ–¯å’Œé›†æˆæ–¹æ³•ã€‚RS-NNè¿˜èƒ½æœ‰æ•ˆåœ°æ‰©å±•åˆ°å¤§è§„æ¨¡æ¶æ„ï¼ˆå¦‚WideResNet-28-10ã€VGG16ã€Inception V3ã€EfficientNetB2å’ŒViT-Base-16ï¼‰ï¼Œå¯¹å¯¹æŠ—æ€§æ”»å‡»è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨ç¬¦åˆå­¦ä¹ çš„ç¯å¢ƒä¸­æä¾›ç»Ÿè®¡ä¿éšœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05772v5">PDF</a> Published as a conference paper at the Thirteenth International   Conference on Learning Representations (ICLR 2025)</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRS-NNçš„éšæœºé›†ç¥ç»ç½‘ç»œåˆ†ç±»æ–¹æ³•ï¼Œå®ƒé€šè¿‡é¢„æµ‹éšæœºé›†ä¸Šçš„ä¿¡å¿µå‡½æ•°è€Œéä¼ ç»Ÿçš„æ¦‚ç‡å‘é‡æ¥è¡¨è¾¾æ¨¡å‹çš„é¢„æµ‹ç½®ä¿¡åº¦ã€‚RS-NNé€šè¿‡ç¼–ç è®­ç»ƒé›†å¼•èµ·çš„çŸ¥è¯†ä¸ç¡®å®šæ€§ï¼Œå³ç”±äºæ•°æ®é›†ä»£è¡¨æ€§ä¸è¶³æˆ–è§„æ¨¡æœ‰é™å¯¼è‡´çš„å‡¸æ¦‚ç‡å‘é‡é›†åˆçš„å¤§å°ï¼Œä»¥åº”å¯¹æœºå™¨å­¦ä¹ ä¸­æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRS-NNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œåˆ†å¸ƒå¤–æ£€æµ‹æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è´å¶æ–¯å’Œé›†æˆæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¤§å‹æ¶æ„ä¸­ä¹Ÿèƒ½æœ‰æ•ˆæ‰©å±•ã€‚æ­¤å¤–ï¼ŒRS-NNè¿˜è¡¨ç°å‡ºå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨åˆè§„å­¦ä¹ ç¯å¢ƒä¸­æä¾›ç»Ÿè®¡ä¿è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RS-NNä½¿ç”¨éšæœºé›†ç¥ç»ç½‘ç»œé¢„æµ‹ä¿¡å¿µå‡½æ•°æ¥åæ˜ æ¨¡å‹é¢„æµ‹çš„ç½®ä¿¡åº¦ã€‚</li>
<li>RS-NNé€šè¿‡ç¼–ç çŸ¥è¯†ä¸ç¡®å®šæ€§æ¥åº”å¯¹æ•°æ®é›†çš„ä¸ç¡®å®šæ€§å’Œå¤æ‚æ€§é—®é¢˜ã€‚çŸ¥è¯†ä¸ç¡®å®šæ€§æ˜¯ç”±äºæ•°æ®é›†ä»£è¡¨æ€§ä¸è¶³æˆ–è§„æ¨¡æœ‰é™æ‰€å¯¼è‡´çš„ã€‚</li>
<li>RS-NNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå‡ºè‰²çš„å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œåˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>RS-NNåœ¨å¤§å‹æ¶æ„ä¸­ä¹Ÿèƒ½æœ‰æ•ˆæ‰©å±•ï¼Œå¦‚WideResNet-28-10ã€VGG16ç­‰ã€‚</li>
<li>RS-NNå±•ç°å‡ºå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™å¯¹äºåœ¨å®‰å…¨å…³é”®é¢†åŸŸéƒ¨ç½²æœºå™¨å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>RS-NNå¯ä»¥åœ¨åˆè§„å­¦ä¹ ç¯å¢ƒä¸­æä¾›ç»Ÿè®¡ä¿è¯ï¼Œç¡®ä¿æ¨¡å‹çš„é¢„æµ‹ç¬¦åˆä¸€å®šçš„å¯é æ€§æ ‡å‡†ã€‚è¿™å¯¹äºæœºå™¨å­¦ä¹ æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.05772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59435f6877efeaefd89935c27693f437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2fa83146a7ab9ee961d944241317d38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fed6e6e4f62c8c05b4af93f4c86a49fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f3b0f8e6003fabd55609f1a9c2cdd7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f9659b84c779ad2a6d2c14255f343c6f.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  FrGNet A fourier-guided weakly-supervised framework for nuclear   instance segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5a6a7497e10f5b7af110af042f7a72e9.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  SVBench A Benchmark with Temporal Multi-Turn Dialogues for Streaming   Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
