<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  HumanGif Single-View Human Diffusion with Generative Prior">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-58d7d2b737a18235e195879eac4e998d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="HumanGif-Single-View-Human-Diffusion-with-Generative-Prior"><a href="#HumanGif-Single-View-Human-Diffusion-with-Generative-Prior" class="headerlink" title="HumanGif: Single-View Human Diffusion with Generative Prior"></a>HumanGif: Single-View Human Diffusion with Generative Prior</h2><p><strong>Authors:Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji</strong></p>
<p>While previous single-view-based 3D human reconstruction methods made significant progress in novel view synthesis, it remains a challenge to synthesize both view-consistent and pose-consistent results for animatable human avatars from a single image input. Motivated by the success of 2D character animation, we propose <strong>HumanGif</strong>, a single-view human diffusion model with generative prior. Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models. To ensure fine-grained and consistent novel view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn spatially aligned features from the input image, implicitly capturing the relative camera and human pose transformation. Furthermore, we introduce an image-level loss during optimization to bridge the gap between latent and image spaces in diffusion models. Extensive experiments on RenderPeople and DNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual performance, with better generalizability for novel view and pose synthesis. </p>
<blockquote>
<p>è™½ç„¶ä¹‹å‰çš„åŸºäºå•è§†è§’çš„3Däººä½“é‡å»ºæ–¹æ³•åœ¨æ–°å‹è§†è§’åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨ä»å•ä¸€å›¾åƒè¾“å…¥åˆæˆä¸è§†è§’å’Œå§¿æ€éƒ½ä¸€è‡´çš„å¯åŠ¨ç”»äººä½“è§’è‰²çš„æŒ‘æˆ˜ã€‚å—åˆ°äºŒç»´è§’è‰²åŠ¨ç”»æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰ç”Ÿæˆå…ˆéªŒçš„å•è§†è§’äººä½“æ‰©æ•£æ¨¡å‹â€”â€”HumanGifã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åŸºäºå•è§†è§’çš„3Däººä½“æ–°å‹è§†è§’å’Œå§¿æ€åˆæˆè¡¨è¿°ä¸ºå•è§†è§’æ¡ä»¶ä¸‹çš„äººä½“æ‰©æ•£è¿‡ç¨‹ï¼Œåˆ©ç”¨åŸºç¡€æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒã€‚ä¸ºç¡®ä¿ç²¾ç»†ä¸”ä¸€è‡´çš„å…¨æ–°è§†è§’å’Œå§¿æ€åˆæˆï¼Œæˆ‘ä»¬åœ¨HumanGifä¸­å¼•å…¥äº†ä¸€ä¸ªHuman NeRFæ¨¡å—ï¼Œç”¨äºä»è¾“å…¥å›¾åƒä¸­å­¦ä¹ ç©ºé—´å¯¹é½ç‰¹å¾ï¼Œä»è€Œéšå¼æ•è·ç›¸å¯¹ç›¸æœºå’Œäººä½“å§¿æ€å˜æ¢ã€‚æ­¤å¤–ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æˆ‘ä»¬å¼•å…¥äº†å›¾åƒçº§æŸå¤±ï¼Œä»¥ç¼©å°æ‰©æ•£æ¨¡å‹ä¸­çš„æ½œåœ¨ç©ºé—´å’Œå›¾åƒç©ºé—´ä¹‹é—´çš„å·®è·ã€‚åœ¨RenderPeopleå’ŒDNA-Renderingæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHumanGifåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”åœ¨æ–°å‹è§†è§’å’Œå§¿æ€åˆæˆæ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12080v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://skhu101.github.io/HumanGif/">https://skhu101.github.io/HumanGif/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå•è§†å›¾çš„äººç±»æ‰©æ•£æ¨¡å‹HumanGifï¼Œç”¨äºç”Ÿæˆå…·æœ‰è§†å›¾ä¸€è‡´æ€§å’Œå§¿æ€ä¸€è‡´æ€§çš„åŠ¨ç”»äººç‰©æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆ2Dè§’è‰²åŠ¨ç”»çš„æˆåŠŸç»éªŒï¼Œåˆ©ç”¨åŸºç¡€æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒä¿¡æ¯ï¼Œå°†å•è§†å›¾ä¸‹çš„3Däººç±»æ–°è§†å›¾å’Œå§¿æ€åˆæˆå®šä¹‰ä¸ºå—å•è§†å›¾æ¡ä»¶çº¦æŸçš„äººç±»æ‰©æ•£è¿‡ç¨‹ã€‚ä¸ºç²¾ç»†ä¸”ä¸€è‡´åœ°åˆæˆæ–°è§†å›¾å’Œå§¿æ€ï¼ŒHumanGifå¼•å…¥äº†Human NeRFæ¨¡å—æ¥ä»è¾“å…¥å›¾åƒä¸­å­¦ä¹ ç©ºé—´å¯¹é½ç‰¹å¾ï¼Œå¹¶éšå«åœ°æ•æ‰ç›¸å¯¹ç›¸æœºå’Œäººç±»å§¿æ€å˜æ¢ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥äº†å›¾åƒçº§æŸå¤±ï¼Œä»¥ç¼©å°æ‰©æ•£æ¨¡å‹ä¸­æ½œç©ºé—´å’Œå›¾åƒç©ºé—´ä¹‹é—´çš„å·®è·ã€‚åœ¨RenderPeopleå’ŒDNA-Renderingæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHumanGifåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šè¾¾åˆ°æœ€ä½³ï¼Œå¯¹äºæ–°è§†å›¾å’Œå§¿æ€çš„åˆæˆå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå•è§†å›¾çš„äººç±»æ‰©æ•£æ¨¡å‹HumanGifï¼Œç”¨äºç”ŸæˆåŠ¨ç”»äººç‰©æ¨¡å‹ã€‚</li>
<li>ç»“åˆ2Dè§’è‰²åŠ¨ç”»çš„æˆåŠŸç»éªŒï¼Œåˆ©ç”¨åŸºç¡€æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒä¿¡æ¯ã€‚</li>
<li>å°†å•è§†å›¾ä¸‹çš„3Däººç±»æ–°è§†å›¾å’Œå§¿æ€åˆæˆå®šä¹‰ä¸ºå—å•è§†å›¾æ¡ä»¶çº¦æŸçš„äººç±»æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†Human NeRFæ¨¡å—ï¼Œä»è¾“å…¥å›¾åƒä¸­å­¦ä¹ ç©ºé—´å¯¹é½ç‰¹å¾ï¼Œå¹¶éšå«åœ°æ•æ‰ç›¸å¯¹ç›¸æœºå’Œå§¿æ€å˜æ¢ã€‚</li>
<li>åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥äº†å›¾åƒçº§æŸå¤±ï¼Œä»¥ç¼©å°æ½œç©ºé—´å’Œå›¾åƒç©ºé—´ä¹‹é—´çš„å·®è·ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHumanGifåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dee8703535e926284b69f92573825892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07339634d79e43e39b0bc02701d2c91b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5df629509bbd1bbd660de60c27d759fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond"><a href="#A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond" class="headerlink" title="A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond"></a>A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond</h2><p><strong>Authors:Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury</strong></p>
<p>Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¼€è¾Ÿäº†æ–°çš„é¢†åŸŸï¼Œå®ç°äº†è¾…åŠ©é€šä¿¡ã€ç¥ç»è¡¨å¾å­¦ä¹ å’Œå¤šæ¨¡æ€èåˆã€‚ç‰¹åˆ«æ˜¯åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰çš„è„‘æœºæ¥å£ï¼Œæä¾›äº†ä¸€ç§å°†ç¥ç»æ´»åŠ¨è½¬åŒ–ä¸ºæœ‰æ„ä¹‰è¾“å‡ºçš„éä¾µå…¥æ€§æ‰‹æ®µã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’ŒåŸºäºå˜å‹å™¨çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæå¤§åœ°æ”¹è¿›äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚æœ¬æ–‡å¯¹åŸºäºEEGçš„å¤šæ¨¡æ€ç”Ÿæˆçš„æœ€æ–°ç ”ç©¶è¿›è¡Œäº†æ–‡çŒ®ç»¼è¿°ï¼Œé‡ç‚¹å…³æ³¨ï¼ˆiï¼‰é€šè¿‡GANsã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œæ‰©æ•£æ¨¡å‹å®ç°EEGåˆ°å›¾åƒç”Ÿæˆï¼Œä»¥åŠï¼ˆiiï¼‰åˆ©ç”¨åŸºäºTransformerçš„è‡ªç„¶è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•å®ç°EEGåˆ°æ–‡æœ¬çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ–°å…´çš„åŸºäºEEGçš„è¯­éŸ³åˆæˆé¢†åŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­å‘å±•çš„å¤šæ¨¡æ€å‰æ²¿é¢†åŸŸã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†å…³é”®æ•°æ®é›†ã€åº”ç”¨åœºæ™¯ã€æŒ‘æˆ˜ä»¥åŠæ”¯æŒç”Ÿæˆæ–¹æ³•çš„EEGç‰¹å¾ç¼–ç æ–¹æ³•ã€‚é€šè¿‡å¯¹åŸºäºEEGçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è¿›è¡Œå…¨é¢æ¦‚è¿°ï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æ´å¯ŸåŠ›ï¼Œä»¥æ¨åŠ¨ç¥ç»è§£ç çš„å‘å±•ï¼Œæé«˜è¾…åŠ©æŠ€æœ¯æ°´å¹³ï¼Œå¹¶æ‹“å±•è„‘æœºäº¤äº’çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12048v1">PDF</a> </p>
<p><strong>Summary</strong><br>    è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¼€åˆ›äº†æ–°çºªå…ƒï¼ŒåŠ©åŠ›è¾…åŠ©æ²Ÿé€šã€ç¥ç»è¡¨å¾å­¦ä¹ ä¸å¤šåª’ä½“èåˆã€‚å€ŸåŠ©è„‘ç”µå›¾ï¼ˆEEGï¼‰çš„BCIå®ç°äº†ç¥ç»æ´»åŠ¨è‡³å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³çš„ç”Ÿæˆã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºEEGçš„å¤šæ¨¡æ€ç”Ÿæˆæœ€æ–°è¿›å±•ï¼Œå…³æ³¨EEGè½¬å›¾åƒå’ŒEEGè½¬æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶æ¢è®¨äº†æ–°å…´çš„EEGè¯­éŸ³åˆæˆé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BCIsä¸GenAIèåˆæ¨åŠ¨äº†è„‘ä¿¡å·è§£ç çš„å‘å±•ã€‚</li>
<li>EEGä¸ºBCIæä¾›äº†éä¾µå…¥å¼çš„ç¥ç»æ´»åŠ¨ç¿»è¯‘æ–¹å¼ã€‚</li>
<li>æ·±åº¦å­¦ä¹ è¿›æ­¥ä¿ƒè¿›äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚</li>
<li>GANsã€VAEså’ŒDiffusion Modelsç”¨äºEEGè½¬å›¾åƒç”Ÿæˆã€‚</li>
<li>TransformeråŸºç¡€çš„è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•ç”¨äºEEGè½¬æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>EEGè¯­éŸ³åˆæˆæ˜¯æ–°å…´çš„å¤šæ¨¡æ€é¢†åŸŸã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†å…³é”®æ•°æ®é›†ã€ç”¨ä¾‹ã€æŒ‘æˆ˜å’ŒEEGç‰¹å¾ç¼–ç æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f89a92f9f6583f89738d30ff8feb081e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f4c288ef1da1c6353c7dc833140956e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93096ddd3246c583190a570d31c2b141.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Image-Inversion-A-Survey-from-GANs-to-Diffusion-and-Beyond"><a href="#Image-Inversion-A-Survey-from-GANs-to-Diffusion-and-Beyond" class="headerlink" title="Image Inversion: A Survey from GANs to Diffusion and Beyond"></a>Image Inversion: A Survey from GANs to Diffusion and Beyond</h2><p><strong>Authors:Yinan Chen, Jiangning Zhang, Yali Bi, Xiaobin Hu, Teng Hu, Zhucun Xue, Ran Yi, Yong Liu, Ying Tai</strong></p>
<p>Image inversion is a fundamental task in generative models, aiming to map images back to their latent representations to enable downstream applications such as editing, restoration, and style transfer. This paper provides a comprehensive review of the latest advancements in image inversion techniques, focusing on two main paradigms: Generative Adversarial Network (GAN) inversion and diffusion model inversion. We categorize these techniques based on their optimization methods. For GAN inversion, we systematically classify existing methods into encoder-based approaches, latent optimization approaches, and hybrid approaches, analyzing their theoretical foundations, technical innovations, and practical trade-offs. For diffusion model inversion, we explore training-free strategies, fine-tuning methods, and the design of additional trainable modules, highlighting their unique advantages and limitations. Additionally, we discuss several popular downstream applications and emerging applications beyond image tasks, identifying current challenges and future research directions. By synthesizing the latest developments, this paper aims to provide researchers and practitioners with a valuable reference resource, promoting further advancements in the field of image inversion. We keep track of the latest works at <a target="_blank" rel="noopener" href="https://github.com/RyanChenYN/ImageInversion">https://github.com/RyanChenYN/ImageInversion</a> </p>
<blockquote>
<p>å›¾åƒåæ¼”æ˜¯ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œæ—¨åœ¨å°†å›¾åƒæ˜ å°„å›å…¶æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚ç¼–è¾‘ã€æ¢å¤å’Œé£æ ¼è½¬æ¢ã€‚æœ¬æ–‡å¯¹å›¾åƒåæ¼”æŠ€æœ¯çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œé‡ç‚¹ä»‹ç»äº†ä¸¤ç§ä¸»è¦èŒƒå¼ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åæ¼”å’Œæ‰©æ•£æ¨¡å‹åæ¼”ã€‚æˆ‘ä»¬æ ¹æ®ä¼˜åŒ–æ–¹æ³•å¯¹è¿™äº›æŠ€æœ¯è¿›è¡Œåˆ†ç±»ã€‚å¯¹äºGANåæ¼”ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å°†å…¶åˆ†ä¸ºåŸºäºç¼–ç å™¨çš„æ–¹æ³•ã€æ½œåœ¨ä¼˜åŒ–æ–¹æ³•å’Œæ··åˆæ–¹æ³•ï¼Œå¹¶å¯¹å…¶ç†è®ºåŸºç¡€ã€æŠ€æœ¯åˆ›æ–°å’Œå®é™…åº”ç”¨è¿›è¡Œäº†æ·±å…¥çš„åˆ†æå’Œè®¨è®ºã€‚å¯¹äºæ‰©æ•£æ¨¡å‹åæ¼”ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ— è®­ç»ƒç­–ç•¥ã€å¾®è°ƒæ–¹æ³•å’Œé™„åŠ è®­ç»ƒæ¨¡å—çš„è®¾è®¡ï¼Œçªå‡ºäº†å®ƒä»¬çš„ç‹¬ç‰¹ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å‡ ä¸ªæµè¡Œçš„ä¸‹æ¸¸åº”ç”¨å’Œè¶…è¶Šå›¾åƒä»»åŠ¡çš„æ–°å…´åº”ç”¨ï¼Œç¡®å®šäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚é€šè¿‡ç»¼åˆæœ€æ–°å‘å±•ï¼Œæœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æœ‰ä»·å€¼çš„å‚è€ƒèµ„æºï¼Œæ¨åŠ¨å›¾åƒåæ¼”é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚æœ‰å…³æœ€æ–°ç ”ç©¶æˆæœçš„æœ€æ–°ä¿¡æ¯è¯·å‚è§ï¼š[<a target="_blank" rel="noopener" href="https://github.com/RyanChenYN/ImageInversion]%EF%BC%88%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E5%8F%AF%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%EF%BC%89%E3%80%82">https://github.com/RyanChenYN/ImageInversion]ï¼ˆç‚¹å‡»é“¾æ¥å¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11974v1">PDF</a> 10 pages, 2 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç»¼è¿°äº†å›¾åƒåè½¬ä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åè½¬å’Œæ‰©æ•£æ¨¡å‹åè½¬çš„ä¸¤ç§ä¸»è¦èŒƒå¼ã€‚æ–‡ç« å¯¹è¿™ä¸¤ç§æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ†ç±»ï¼Œå¹¶åˆ†æäº†å®ƒä»¬çš„ç†è®ºæ¡†æ¶ã€æŠ€æœ¯åˆ›æ–°å’Œå®è·µä¸­çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†å›¾åƒåè½¬åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„çƒ­é—¨é¢†åŸŸä»¥åŠè¶…å‡ºç°æœ‰å›¾åƒä»»åŠ¡çš„æ–°å…´åº”ç”¨ï¼ŒæŒ‡å‡ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåè½¬æ˜¯ç”Ÿæˆæ¨¡å‹ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œæ—¨åœ¨å°†å›¾åƒæ˜ å°„å›å…¶æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚ç¼–è¾‘ã€æ¢å¤å’Œé£æ ¼è½¬æ¢ã€‚</li>
<li>GANåè½¬å’Œæ‰©æ•£æ¨¡å‹åè½¬æ˜¯å›¾åƒåè½¬ä¸­çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œæœ¬æ–‡è¿›è¡Œäº†è¯¦ç»†ç»¼è¿°ã€‚</li>
<li>GANåè½¬æ–¹æ³•åŒ…æ‹¬åŸºäºç¼–ç å™¨çš„æ–¹æ³•ã€æ½œåœ¨ä¼˜åŒ–æ–¹æ³•å’Œæ··åˆæ–¹æ³•ï¼Œå„æœ‰å…¶ç†è®ºã€æŠ€æœ¯å’Œå®è·µä¸Šçš„ç‰¹ç‚¹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åè½¬ç­–ç•¥æ¶‰åŠæ— è®­ç»ƒç­–ç•¥ã€å¾®è°ƒæ–¹æ³•å’Œå¯è®­ç»ƒæ¨¡å—çš„è®¾è®¡ï¼Œå…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>å›¾åƒåè½¬åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„çƒ­é—¨é¢†åŸŸåŒ…æ‹¬ç¼–è¾‘ã€æ¢å¤å’Œé£æ ¼è½¬æ¢ç­‰ï¼ŒåŒæ—¶ä¹Ÿæœ‰è¶…å‡ºç°æœ‰å›¾åƒä»»åŠ¡çš„æ–°å…´åº”ç”¨ã€‚</li>
<li>å½“å‰å›¾åƒåè½¬é¢†åŸŸä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„ç¨³å®šæ€§ã€æ•ˆç‡ã€ä»¥åŠè·¨æ¨¡æ€çš„åº”ç”¨ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-271393c492104679e791e3d684d05f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9926c2711d0b232467f7d966f8eb6222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed974cd6ae8e9926dce1129e9378ae39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6858f773040d9f1574c1fd6a516f859c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MVTokenFlow-High-quality-4D-Content-Generation-using-Multiview-Token-Flow"><a href="#MVTokenFlow-High-quality-4D-Content-Generation-using-Multiview-Token-Flow" class="headerlink" title="MVTokenFlow: High-quality 4D Content Generation using Multiview Token   Flow"></a>MVTokenFlow: High-quality 4D Content Generation using Multiview Token   Flow</h2><p><strong>Authors:Hanzhuo Huang, Yuan Liu, Ge Zheng, Jiepeng Wang, Zhiyang Dou, Sibei Yang</strong></p>
<p>In this paper, we present MVTokenFlow for high-quality 4D content creation from monocular videos. Recent advancements in generative models such as video diffusion models and multiview diffusion models enable us to create videos or 3D models. However, extending these generative models for dynamic 4D content creation is still a challenging task that requires the generated content to be consistent spatially and temporally. To address this challenge, MVTokenFlow utilizes the multiview diffusion model to generate multiview images on different timesteps, which attains spatial consistency across different viewpoints and allows us to reconstruct a reasonable coarse 4D field. Then, MVTokenFlow further regenerates all the multiview images using the rendered 2D flows as guidance. The 2D flows effectively associate pixels from different timesteps and improve the temporal consistency by reusing tokens in the regeneration process. Finally, the regenerated images are spatiotemporally consistent and utilized to refine the coarse 4D field to get a high-quality 4D field. Experiments demonstrate the effectiveness of our design and show significantly improved quality than baseline methods. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MVTokenFlowï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­åˆ›å»ºé«˜è´¨é‡4Då†…å®¹ã€‚æœ€è¿‘çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼‰çš„è¿›æ­¥ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ›å»ºè§†é¢‘æˆ–3Dæ¨¡å‹ã€‚ç„¶è€Œï¼Œå°†è¿™äº›ç”Ÿæˆæ¨¡å‹æ‰©å±•åˆ°åŠ¨æ€4Då†…å®¹åˆ›å»ºä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç¡®ä¿ç”Ÿæˆçš„å†…å®¹åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šä¿æŒä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒMVTokenFlowåˆ©ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åœ¨ä¸åŒçš„æ—¶é—´æ­¥é•¿ä¸Šç”Ÿæˆå¤šè§†å›¾å›¾åƒï¼Œè¿™å®ç°äº†ä¸åŒè§†ç‚¹ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œå¹¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡å»ºåˆç†çš„ç²—ç•¥4Dåœºã€‚ç„¶åï¼ŒMVTokenFlowè¿›ä¸€æ­¥ä»¥æ¸²æŸ“çš„2Dæµä¸ºå¼•å¯¼ï¼Œé‡æ–°ç”Ÿæˆæ‰€æœ‰å¤šè§†å›¾å›¾åƒã€‚2Dæµæœ‰æ•ˆåœ°å…³è”äº†æ¥è‡ªä¸åŒæ—¶é—´æ­¥é•¿çš„åƒç´ ï¼Œå¹¶åœ¨å†ç”Ÿè¿‡ç¨‹ä¸­é‡ç”¨ä»¤ç‰Œï¼Œæé«˜äº†æ—¶é—´ä¸€è‡´æ€§ã€‚æœ€åï¼Œå†ç”Ÿå›¾åƒå…·æœ‰æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶ç”¨äºç»†åŒ–ç²—ç•¥çš„4Dåœºï¼Œä»¥è·å¾—é«˜è´¨é‡çš„4Dåœºã€‚å®éªŒè¯æ˜äº†æˆ‘ä»¬è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºæ¯”åŸºçº¿æ–¹æ³•æ˜¾è‘—æé«˜çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11697v1">PDF</a> ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://soolab.github.io/MVTokenFlow">https://soolab.github.io/MVTokenFlow</a></p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡æå‡ºäº†MVTokenFlowï¼Œä¸€ç§ç”¨äºé«˜è´¨é‡å››ç»´å†…å®¹åˆ›å»ºçš„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åŸºäºå•ç›®è§†é¢‘ï¼Œåˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¦‚è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå…·æœ‰æ—¶ç©ºä¸€è‡´æ€§çš„å¤šè§†è§’å›¾åƒï¼Œè¿›è€Œæ„å»ºåˆç†çš„ç²—ç•¥å››ç»´åœºã€‚é€šè¿‡åˆ©ç”¨æ¸²æŸ“çš„äºŒç»´æµä½œä¸ºæŒ‡å¯¼ï¼ŒMVTokenFlowèƒ½å¤Ÿæ”¹å–„æ—¶é—´ä¸€è‡´æ€§å¹¶å†ç”Ÿæ‰€æœ‰å¤šè§†è§’å›¾åƒã€‚æœ€ç»ˆï¼Œä½¿ç”¨æ—¶ç©ºä¸€è‡´çš„å†ç”Ÿå›¾åƒå¯¹ç²—ç•¥å››ç»´åœºè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œä»¥è·å¾—é«˜è´¨é‡çš„å››ç»´åœºã€‚å®éªŒè¯æ˜äº†è¯¥è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVTokenFlowæ˜¯ä¸€ç§ç”¨äºé«˜è´¨é‡å››ç»´å†…å®¹åˆ›å»ºçš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­ç”Ÿæˆå¤šè§†è§’å›¾åƒã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¦‚è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€æ„å»ºæŠ€æœ¯æ¡†æ¶ã€‚</li>
<li>é€šè¿‡æ„å»ºå¤šè§†è§’å›¾åƒåœ¨ä¸åŒæ—¶é—´ç‚¹çš„ç”Ÿæˆï¼Œå®ç°ç©ºé—´ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨æ¸²æŸ“çš„äºŒç»´æµä½œä¸ºæŒ‡å¯¼ï¼Œæ”¹å–„æ—¶é—´ä¸€è‡´æ€§å¹¶å†ç”Ÿå¤šè§†è§’å›¾åƒã€‚</li>
<li>é€šè¿‡æ—¶ç©ºä¸€è‡´çš„å†ç”Ÿå›¾åƒå¯¹ç²—ç•¥å››ç»´åœºè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>å®éªŒè¯æ˜äº†MVTokenFlowè®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†å››ç»´å†…å®¹åˆ›å»ºçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-217ae737fdde4bc5adc7526815970c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50dcb0b0a3030b67ae7f4d98311e146e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ad60d61b36961a2f7b5cec6dcea86eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-to-Sample-Effective-and-Diverse-Prompts-for-Text-to-Image-Generation"><a href="#Learning-to-Sample-Effective-and-Diverse-Prompts-for-Text-to-Image-Generation" class="headerlink" title="Learning to Sample Effective and Diverse Prompts for Text-to-Image   Generation"></a>Learning to Sample Effective and Diverse Prompts for Text-to-Image   Generation</h2><p><strong>Authors:Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, Ling Pan</strong></p>
<p>Recent advances in text-to-image diffusion models have achieved impressive image generation capabilities. However, it remains challenging to control the generation process with desired properties (e.g., aesthetic quality, user intention), which can be expressed as black-box reward functions. In this paper, we focus on prompt adaptation, which refines the original prompt into model-preferred prompts to generate desired images. While prior work uses reinforcement learning (RL) to optimize prompts, we observe that applying RL often results in generating similar postfixes and deterministic behaviors. To this end, we introduce \textbf{P}rompt \textbf{A}daptation with \textbf{G}FlowNets (\textbf{PAG}), a novel approach that frames prompt adaptation as a probabilistic inference problem. Our key insight is that leveraging Generative Flow Networks (GFlowNets) allows us to shift from reward maximization to sampling from an unnormalized density function, enabling both high-quality and diverse prompt generation. However, we identify that a naive application of GFlowNets suffers from mode collapse and uncovers a previously overlooked phenomenon: the progressive loss of neural plasticity in the model, which is compounded by inefficient credit assignment in sequential prompt generation. To address this critical challenge, we develop a systematic approach in PAG with flow reactivation, reward-prioritized sampling, and reward decomposition for prompt adaptation. Extensive experiments validate that PAG successfully learns to sample effective and diverse prompts for text-to-image generation. We also show that PAG exhibits strong robustness across various reward functions and transferability to different text-to-image models. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•å·²ç»å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœŸæœ›çš„å±æ€§ï¼ˆå¦‚ç¾å­¦è´¨é‡ã€ç”¨æˆ·æ„å›¾ï¼‰æ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™äº›å±æ€§å¯ä»¥è¡¨ç¤ºä¸ºé»‘ç›’å¥–åŠ±å‡½æ•°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæç¤ºé€‚åº”ï¼Œå®ƒå°†åŸå§‹æç¤ºç»†åŒ–ä¸ºæ¨¡å‹åå¥½çš„æç¤ºä»¥ç”Ÿæˆæ‰€éœ€çš„å›¾åƒã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–æç¤ºï¼Œä½†æˆ‘ä»¬å‘ç°åº”ç”¨RLé€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆç›¸ä¼¼çš„åç¼€å’Œç¡®å®šæ€§è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰GFlowNetsçš„æç¤ºé€‚åº”ï¼ˆPAGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†æç¤ºé€‚åº”æ¡†æ¶åŒ–ä¸ºæ¦‚ç‡æ¨ç†é—®é¢˜çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ©ç”¨ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰å…è®¸æˆ‘ä»¬ä»å¥–åŠ±æœ€å¤§åŒ–è½¬å‘ä»æœªå½’ä¸€åŒ–çš„å¯†åº¦å‡½æ•°ä¸­è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œå®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æç¤ºç”Ÿæˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°GFlowNetsçš„æœ´ç´ åº”ç”¨å­˜åœ¨æ¨¡å¼å´©æºƒçš„é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†ä¸€ä¸ªä»¥å‰è¢«å¿½è§†çš„ç°è±¡ï¼šæ¨¡å‹ä¸­ç¥ç»å¯å¡‘æ€§çš„é€æ­¥ä¸§å¤±ï¼Œè¿™æ˜¯ç”±é¡ºåºæç¤ºç”Ÿæˆä¸­çš„ä½æ•ˆä¿¡ç”¨åˆ†é…æ‰€åŠ å‰§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨PAGä¸­å¼€å‘äº†ä¸€ç§ç³»ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬æµå¤æ´»ã€å¥–åŠ±ä¼˜å…ˆé‡‡æ ·å’Œå¥–åŠ±åˆ†è§£æ¥è¿›è¡Œæç¤ºé€‚åº”ã€‚å¤§é‡å®éªŒéªŒè¯ï¼ŒPAGæˆåŠŸå­¦ä¼šäº†å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿›è¡Œæœ‰æ•ˆä¸”å¤šæ ·åŒ–çš„æç¤ºé‡‡æ ·ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒPAGåœ¨å„ç§å¥–åŠ±å‡½æ•°ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸”å¯è½¬ç§»åˆ°ä¸åŒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11477v1">PDF</a> 18 pages, 14 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä»¥è·å–å…·æœ‰ç‰¹å®šå±æ€§ï¼ˆå¦‚ç¾å­¦è´¨é‡ã€ç”¨æˆ·æ„å›¾ï¼‰çš„å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡å…³æ³¨æç¤ºé€‚åº”ï¼ˆPrompt Adaptationï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†åŸå§‹æç¤ºç»†åŒ–ä¸ºæ¨¡å‹åå¥½çš„æç¤ºï¼Œä»¥ç”Ÿæˆæ‰€éœ€çš„å›¾åƒã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•PAGï¼ˆåˆ©ç”¨GFlowNetsçš„æç¤ºé€‚åº”ï¼‰ï¼Œå°†æç¤ºé€‚åº”è§†ä¸ºæ¦‚ç‡æ¨ç†é—®é¢˜ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ©ç”¨GFlowNetsä»éæ ‡å‡†åŒ–å¯†åº¦å‡½æ•°ä¸­é‡‡æ ·ï¼Œä»¥å®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æç¤ºç”Ÿæˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å•çº¯åº”ç”¨GFlowNetsä¼šå‡ºç°æ¨¡å¼å´©æºƒçš„é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†ä¸€ä¸ªè¢«å¿½è§†çš„ç°è±¡ï¼šæ¨¡å‹ä¸­ç¥ç»å¯å¡‘æ€§çš„é€æ­¥ä¸§å¤±ï¼Œè¿™åœ¨åºåˆ—æç¤ºç”Ÿæˆä¸­çš„ä¿¡ç”¨åˆ†é…æ•ˆç‡ä½ä¸‹æ—¶æ›´ä¸ºä¸¥é‡ã€‚ä¸ºè§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨PAGä¸­é‡‡ç”¨äº†æµå†æ¿€æ´»ã€å¥–åŠ±ä¼˜å…ˆé‡‡æ ·å’Œå¥–åŠ±åˆ†è§£çš„æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒPAGæˆåŠŸå­¦ä¹ äº†å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æœ‰æ•ˆå’Œå¤šæ ·åŒ–çš„æç¤ºè¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒPAGåœ¨ä¸åŒçš„å¥–åŠ±å‡½æ•°ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå¹¶å¯åœ¨ä¸åŒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­è¿›è¡Œè½¬ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä»¥è·å–å…·æœ‰ç‰¹å®šå±æ€§çš„å›¾åƒä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æç¤ºé€‚åº”æŠ€æœ¯é€šè¿‡ç»†åŒ–åŸå§‹æç¤ºä¸ºæ¨¡å‹åå¥½çš„æç¤ºï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>å¼•å…¥æ–°çš„æ–¹æ³•PAGï¼Œå°†æç¤ºé€‚åº”è§†ä¸ºæ¦‚ç‡æ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨GFlowNetsè¿›è¡Œéæ ‡å‡†åŒ–å¯†åº¦å‡½æ•°é‡‡æ ·ï¼Œå®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æç¤ºç”Ÿæˆã€‚</li>
<li>å‘ç°å•çº¯åº”ç”¨GFlowNetsä¼šå¯¼è‡´æ¨¡å¼å´©æºƒï¼Œå¹¶æ­ç¤ºæ¨¡å‹ä¸­ç¥ç»å¯å¡‘æ€§çš„ä¸§å¤±å’Œåºåˆ—æç¤ºç”Ÿæˆä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼ŒPAGé‡‡ç”¨äº†æµå†æ¿€æ´»ã€å¥–åŠ±ä¼˜å…ˆé‡‡æ ·å’Œå¥–åŠ±åˆ†è§£çš„æ–¹æ³•ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒPAGèƒ½æœ‰æ•ˆå­¦ä¹ å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„æç¤ºè¿›è¡Œé‡‡æ ·ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å’Œåœ¨ä¸åŒæ¨¡å‹ä¸­çš„å¯è½¬ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-988a9aebb6be4b5f28b8e7e2e64022e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83d030748f481b22645a61238f19a72f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2bddbb43f6fb54b478522bf2358be7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb74f93867c1ec02d7f891b802eeccc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101bba53463a41acc85b9cd32882c520.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PDA-Generalizable-Detection-of-AI-Generated-Images-via-Post-hoc-Distribution-Alignment"><a href="#PDA-Generalizable-Detection-of-AI-Generated-Images-via-Post-hoc-Distribution-Alignment" class="headerlink" title="PDA: Generalizable Detection of AI-Generated Images via Post-hoc   Distribution Alignment"></a>PDA: Generalizable Detection of AI-Generated Images via Post-hoc   Distribution Alignment</h2><p><strong>Authors:Li Wang, Wenyu Chen, Zheng Li, Shanqing Guo</strong></p>
<p>The rapid advancement of generative models has led to the proliferation of highly realistic AI-generated images, posing significant challenges for detection methods to generalize across diverse and evolving generative techniques. Existing approaches often fail to adapt to unknown models without costly retraining, limiting their practicability. To fill this gap, we propose Post-hoc Distribution Alignment (PDA), a novel approach for the generalizable detection for AI-generated images. The key idea is to use the known generative model to regenerate undifferentiated test images. This process aligns the distributions of the re-generated real images with the known fake images, enabling effective distinction from unknown fake images. PDA employs a two-step detection framework: 1) evaluating whether a test image aligns with the known fake distribution based on deep k-nearest neighbor (KNN) distance, and 2) re-generating test images using known generative models to create pseudo-fake images for further classification. This alignment strategy allows PDA to effectively detect fake images without relying on unseen data or requiring retraining. Extensive experiments demonstrate the superiority of PDA, achieving 96.73% average accuracy across six state-of-the-art generative models, including GANs, diffusion models, and text-to-image models, and improving by 16.07% over the best baseline. Through t-SNE visualizations and KNN distance analysis, we provide insights into PDAâ€™s effectiveness in separating real and fake images. Our work provides a flexible and effective solution for real-world fake image detection, advancing the generalization ability of detection systems. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œé«˜åº¦é€¼çœŸçš„AIç”Ÿæˆå›¾åƒå¤§é‡æ¶Œç°ï¼Œè¿™ä¸ºæ£€æµ‹æ–¹æ³•çš„é€šç”¨æ€§å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç”ŸæˆæŠ€æœ¯å¤šç§å¤šæ ·ä¸”ä¸æ–­æ¼”å˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•é€‚åº”æœªçŸ¥æ¨¡å‹ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é‡æ–°è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†äº‹ååˆ†å¸ƒå¯¹é½ï¼ˆPDAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºAIç”Ÿæˆå›¾åƒçš„å¯æ³›åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨å·²çŸ¥çš„ç”Ÿæˆæ¨¡å‹é‡æ–°ç”Ÿæˆæœªåˆ†åŒ–çš„æµ‹è¯•å›¾åƒã€‚è¿™ä¸ªè¿‡ç¨‹å°†é‡æ–°ç”Ÿæˆçš„çœŸå®å›¾åƒåˆ†å¸ƒä¸å·²çŸ¥çš„è™šå‡å›¾åƒå¯¹é½ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†æœªçŸ¥çš„è™šå‡å›¾åƒã€‚PDAé‡‡ç”¨ä¸¤æ­¥æ£€æµ‹æ¡†æ¶ï¼š1ï¼‰åŸºäºæ·±åº¦kæœ€è¿‘é‚»ï¼ˆKNNï¼‰è·ç¦»è¯„ä¼°æµ‹è¯•å›¾åƒæ˜¯å¦ä¸å·²çŸ¥è™šå‡åˆ†å¸ƒå¯¹é½ï¼›2ï¼‰ä½¿ç”¨å·²çŸ¥ç”Ÿæˆæ¨¡å‹é‡æ–°ç”Ÿæˆæµ‹è¯•å›¾åƒä»¥åˆ›å»ºä¼ªå‡å›¾åƒè¿›è¡Œè¿›ä¸€æ­¥åˆ†ç±»ã€‚è¿™ç§å¯¹é½ç­–ç•¥å…è®¸PDAæœ‰æ•ˆåœ°æ£€æµ‹è™šå‡å›¾åƒï¼Œè€Œæ— éœ€ä¾èµ–æœªè§è¿‡çš„æ•°æ®æˆ–è¿›è¡Œé‡æ–°è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPDAåœ¨å…­ç§æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬GANsã€æ‰©æ•£æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¹¶æ¯”æœ€ä½³åŸºçº¿æé«˜äº†16.07%ã€‚é€šè¿‡t-SNEå¯è§†åŒ–å’ŒKNNè·ç¦»åˆ†æï¼Œæˆ‘ä»¬æ·±å…¥äº†è§£äº†PDAåœ¨åˆ†ç¦»çœŸå®å’Œè™šå‡å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ç§çµæ´»æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºçœŸå®ä¸–ç•Œçš„è™šå‡å›¾åƒæ£€æµ‹ï¼Œæé«˜äº†æ£€æµ‹ç³»ç»Ÿçš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10803v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç”Ÿæˆæ¨¡å‹å¿«é€Ÿå‘å±•å¸¦æ¥çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åéªŒåˆ†å¸ƒå¯¹é½ï¼ˆPDAï¼‰çš„é€šç”¨æ£€æµ‹æ–¹æ³•ã€‚å®ƒé€šè¿‡å·²çŸ¥ç”Ÿæˆæ¨¡å‹é‡æ–°ç”Ÿæˆæµ‹è¯•å›¾åƒæ¥å¯¹é½çœŸå®ä¸ç”Ÿæˆå›¾åƒçš„åˆ†å¸ƒï¼Œä»è€Œå®ç°æœªçŸ¥ç”Ÿæˆæ¨¡å‹ä¸‹ç”Ÿæˆå›¾åƒçš„æœ‰æ•ˆé‰´åˆ«ã€‚è¯¥ç­–ç•¥é€šè¿‡è¯„ä¼°æµ‹è¯•å›¾åƒä¸å·²çŸ¥å‡å›¾åƒåˆ†å¸ƒçš„åŒ¹é…ç¨‹åº¦ï¼Œå¹¶åˆ©ç”¨å·²çŸ¥ç”Ÿæˆæ¨¡å‹é‡æ–°ç”Ÿæˆæµ‹è¯•å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œæ— éœ€æœªè§æ•°æ®æˆ–é‡æ–°è®­ç»ƒå³å¯å®ç°æ£€æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒPDAåœ¨å…­ç§å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ä¸Šå–å¾—äº†å¹³å‡å‡†ç¡®ç‡é«˜è¾¾96.73%çš„æ•ˆæœï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æé«˜äº†16.07%ã€‚å®ƒä¸ºçœŸå®ä¸–ç•Œçš„å‡å›¾åƒæ£€æµ‹æä¾›äº†çµæ´»æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”æœªçŸ¥æ¨¡å‹ã€‚</li>
<li>PDAæ–¹æ³•é€šè¿‡å·²çŸ¥ç”Ÿæˆæ¨¡å‹é‡æ–°ç”Ÿæˆæµ‹è¯•å›¾åƒï¼Œå®ç°çœŸå®ä¸ç”Ÿæˆå›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚</li>
<li>PDAåˆ©ç”¨æ·±åº¦kè¿‘é‚»ï¼ˆKNNï¼‰è·ç¦»è¯„ä¼°æµ‹è¯•å›¾åƒä¸å·²çŸ¥å‡åˆ†å¸ƒçš„åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>PDAåœ¨å¤šç§ç”Ÿæˆæ¨¡å‹ä¸Šå®ç°äº†é«˜æ•ˆçš„å‡å›¾åƒæ£€æµ‹ï¼Œå¹³å‡å‡†ç¡®ç‡é«˜è¾¾96.73%ã€‚</li>
<li>PDAç›¸è¾ƒäºæœ€ä½³åŸºçº¿æé«˜äº†16.07%ï¼Œæ˜¾ç¤ºå‡ºå…¶æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>PDAæ–¹æ³•é€šè¿‡t-SNEå¯è§†åŒ–å’ŒKNNè·ç¦»åˆ†æè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3737ef045efe264f163e1c30582de0bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-891f28810e48a5ff5c2be30c65d51722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4b4feaaa73b155414a9e6c5d5d33b21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55a2075a40345d6c0597999b2fe058c6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffusing-DeBias-a-Recipe-for-Turning-a-Bug-into-a-Feature"><a href="#Diffusing-DeBias-a-Recipe-for-Turning-a-Bug-into-a-Feature" class="headerlink" title="Diffusing DeBias: a Recipe for Turning a Bug into a Feature"></a>Diffusing DeBias: a Recipe for Turning a Bug into a Feature</h2><p><strong>Authors:Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino</strong></p>
<p>Deep learning model effectiveness in classification tasks is often challenged by the quality and quantity of training data which, whenever containing strong spurious correlations between specific attributes and target labels, can result in unrecoverable biases in model predictions. Tackling these biases is crucial in improving model generalization and trust, especially in real-world scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting as a plug-in for common methods in model debiasing while exploiting the inherent bias-learning tendency of diffusion models. Our approach leverages conditional diffusion models to generate synthetic bias-aligned images, used to train a bias amplifier model, to be further employed as an auxiliary method in different unsupervised debiasing approaches. Our proposed method, which also tackles the common issue of training set memorization typical of this type of tech- niques, beats current state-of-the-art in multiple benchmark datasets by significant margins, demonstrating its potential as a versatile and effective tool for tackling dataset bias in deep learning applications. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å¸¸å¸¸å—åˆ°è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡çš„æŒ‘æˆ˜ã€‚è®­ç»ƒæ•°æ®ä¸­ç‰¹å®šå±æ€§ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„è™šå‡å…³è”æ—¶ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹é¢„æµ‹ä¸­å‡ºç°æ— æ³•æŒ½å›çš„åè§ã€‚è§£å†³è¿™äº›åè§å¯¹äºæé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œä¿¡ä»»åº¦è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®åœºæ™¯ä¸­åº”ç”¨å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡æå‡ºäº†Diffusing DeBiasï¼ˆDDBï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå¯ä½œä¸ºé€šç”¨æ¨¡å‹å»åæ–¹æ³•çš„æ’ä»¶ï¼ŒåŒæ—¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å›ºæœ‰çš„åå­¦ä¹ å€¾å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆåç½®å¯¹é½å›¾åƒï¼Œç”¨äºè®­ç»ƒåç½®æ”¾å¤§å™¨æ¨¡å‹ï¼Œè¿›ä¸€æ­¥ç”¨äºä¸åŒçš„æ— ç›‘ç£å»åæ–¹æ³•ä¸­ä½œä¸ºè¾…åŠ©æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¿˜è§£å†³äº†æ­¤ç±»æŠ€æœ¯å…¸å‹çš„è®­ç»ƒé›†è®°å¿†åŒ–é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¤§å¹…åº¦åœ°è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­è§£å†³æ•°æ®é›†åè§çš„æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09564v2">PDF</a> 29 Pages, 12 Figures</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ç»å¸¸å—åˆ°è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ•°é‡çš„æŒ‘æˆ˜ã€‚å½“è®­ç»ƒæ•°æ®åŒ…å«ç‰¹å®šå±æ€§ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„å¼ºçƒˆè™šå‡å…³è”æ—¶ï¼Œä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹ä¸­å‡ºç°ä¸å¯æŒ½å›çš„åè§ã€‚æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDiffusing DeBiasï¼ˆDDBï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰åè§å­¦ä¹ å€¾å‘ï¼Œä½œä¸ºé€šç”¨æ–¹æ³•çš„ä¸€ä¸ªæ’ä»¶æ¥è§£å†³åè§é—®é¢˜ã€‚DDBé€šè¿‡ç”Ÿæˆåˆæˆåè§å¯¹é½å›¾åƒæ¥è®­ç»ƒåè§æ”¾å¤§å™¨æ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥ç”¨äºä¸åŒçš„æ— ç›‘ç£å»åè§æ–¹æ³•ã€‚è¯¥æ–¹æ³•è¿˜è§£å†³äº†æ­¤ç±»æŠ€æœ¯ä¸­å¸¸è§çš„è®­ç»ƒé›†è®°å¿†é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ï¼Œè¡¨æ˜å…¶åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­è§£å†³æ•°æ®é›†æ½œåŠ›çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­æ˜“å—è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡çš„å½±å“ã€‚</li>
<li>å½“è®­ç»ƒæ•°æ®å­˜åœ¨è™šå‡å…³è”æ—¶ï¼Œä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹å‡ºç°åè§ã€‚</li>
<li>Diffusing DeBiasï¼ˆDDBï¼‰æ˜¯ä¸€ç§æ–°çš„å»åè§æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å€¾å‘æ€§ã€‚</li>
<li>DDBé€šè¿‡ç”Ÿæˆåˆæˆåè§å¯¹é½å›¾åƒæ¥è®­ç»ƒåè§æ”¾å¤§å™¨æ¨¡å‹ã€‚</li>
<li>DDBå¯ä½œä¸ºæ’ä»¶ç”¨äºä¸åŒçš„æ— ç›‘ç£å»åè§æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­çš„è®­ç»ƒé›†è®°å¿†é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42dab58265724c78d09fb0245fd25bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de77b375a1010c091b5bf595eaaed59.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ConsistentDreamer-View-Consistent-Meshes-Through-Balanced-Multi-View-Gaussian-Optimization"><a href="#ConsistentDreamer-View-Consistent-Meshes-Through-Balanced-Multi-View-Gaussian-Optimization" class="headerlink" title="ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View   Gaussian Optimization"></a>ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View   Gaussian Optimization</h2><p><strong>Authors:Onat Åahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu</strong></p>
<p>Recent advances in diffusion models have significantly improved 3D generation, enabling the use of assets generated from an image for embodied AI simulations. However, the one-to-many nature of the image-to-3D problem limits their use due to inconsistent content and quality across views. Previous models optimize a 3D model by sampling views from a view-conditioned diffusion prior, but diffusion models cannot guarantee view consistency. Instead, we present ConsistentDreamer, where we first generate a set of fixed multi-view prior images and sample random views between them with another diffusion model through a score distillation sampling (SDS) loss. Thereby, we limit the discrepancies between the views guided by the SDS loss and ensure a consistent rough shape. In each iteration, we also use our generated multi-view prior images for fine-detail reconstruction. To balance between the rough shape and the fine-detail optimizations, we introduce dynamic task-dependent weights based on homoscedastic uncertainty, updated automatically in each iteration. Additionally, we employ opacity, depth distortion, and normal alignment losses to refine the surface for mesh extraction. Our method ensures better view consistency and visual quality compared to the state-of-the-art. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•æå¤§åœ°æ”¹è¿›äº†3Dç”Ÿæˆï¼Œä½¿å¾—å¯ä»¥ä½¿ç”¨ä»å›¾åƒç”Ÿæˆçš„èµ„äº§è¿›è¡Œå®ä½“AIæ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œå›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šæ€§è´¨ç”±äºä»ä¸åŒè§†è§’çš„ä¸ä¸€è‡´å†…å®¹å’Œè´¨é‡è€Œé™åˆ¶äº†å…¶ä½¿ç”¨ã€‚ä»¥å‰çš„æ¨¡å‹ä¼šé€šè¿‡ä»è§†è§’è°ƒèŠ‚çš„æ‰©æ•£å…ˆéªŒä¸­é‡‡æ ·è§†è§’æ¥ä¼˜åŒ–3Dæ¨¡å‹ï¼Œä½†æ‰©æ•£æ¨¡å‹æ— æ³•ä¿è¯è§†è§’çš„ä¸€è‡´æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ConsistentDreamerï¼Œæˆ‘ä»¬é¦–å…ˆç”Ÿæˆä¸€ç»„å›ºå®šçš„å¤šè§†è§’å…ˆéªŒå›¾åƒï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ä½¿ç”¨å¦ä¸€ä¸ªæ‰©æ•£æ¨¡å‹è¿›è¡Œéšæœºè§†è§’é‡‡æ ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡SDSæŸå¤±æŒ‡å¯¼çš„è§†å›¾é™åˆ¶äº†å·®å¼‚ï¼Œå¹¶ç¡®ä¿äº†ä¸€è‡´çš„ç²—ç•¥å½¢çŠ¶ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ç”Ÿæˆçš„å¤šè§†è§’å…ˆéªŒå›¾åƒè¿›è¡Œç»†èŠ‚é‡å»ºã€‚ä¸ºäº†å¹³è¡¡ç²—ç•¥å½¢çŠ¶å’Œç»†èŠ‚ä¼˜åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¼‚æ–¹å·®ä¸ç¡®å®šæ€§çš„åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­è‡ªåŠ¨æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±æ¥å®Œå–„è¡¨é¢ç½‘æ ¼æå–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯ç¡®ä¿äº†æ›´å¥½çš„è§†è§’ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09278v2">PDF</a> Manuscript accepted by Pattern Recognition Letters</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ˜¾è‘—æé«˜3Dç”Ÿæˆèƒ½åŠ›ï¼Œå¯å®ç°ä»å›¾åƒç”Ÿæˆèµ„äº§ç”¨äºå®ä½“AIæ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œå›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šé—®é¢˜å¯¼è‡´å…¶å†…å®¹åœ¨ä¸åŒè§†è§’ä¸Šä¸ä¸€è‡´ï¼Œå½±å“ä½¿ç”¨ã€‚ä¹‹å‰çš„æ–¹æ³•é€šè¿‡ä»è§†è§’è°ƒèŠ‚çš„æ‰©æ•£å…ˆéªŒä¸­é‡‡æ ·è§†è§’ä¼˜åŒ–3Dæ¨¡å‹ï¼Œä½†æ‰©æ•£æ¨¡å‹æ— æ³•ä¿è¯è§†è§’ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºConsistentDreamerï¼Œé¦–å…ˆç”Ÿæˆä¸€ç»„å›ºå®šçš„å¤šè§†è§’å…ˆéªŒå›¾åƒï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´é€šè¿‡å¦ä¸€ä¸ªæ‰©æ•£æ¨¡å‹è¿›è¡Œéšæœºè§†è§’é‡‡æ ·ï¼Œé€šè¿‡å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±æ¥é™åˆ¶è§†è§’ä¹‹é—´çš„å·®å¼‚ï¼Œç¡®ä¿ä¸€è‡´çš„ç²—ç•¥å½¢çŠ¶ã€‚æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ç”Ÿæˆçš„å¤šè§†è§’å…ˆéªŒå›¾åƒè¿›è¡Œç»†èŠ‚é‡å»ºã€‚ä¸ºäº†å¹³è¡¡ç²—ç•¥å½¢çŠ¶å’Œç»†èŠ‚ä¼˜åŒ–çš„å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŒæ–¹å·®ä¸ç¡®å®šæ€§çš„åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­è‡ªåŠ¨æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±æ¥ä¼˜åŒ–è¡¨é¢ä»¥è¿›è¡Œç½‘æ ¼æå–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œåœ¨è§†è§’ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ä¸Šè¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†ä»å›¾åƒç”Ÿæˆèµ„äº§åœ¨AIæ¨¡æ‹Ÿä¸­çš„åº”ç”¨ã€‚</li>
<li>å›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šé—®é¢˜å¯¼è‡´å†…å®¹åœ¨ä¸åŒè§†è§’ä¸Šçš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ConsistentDreameré€šè¿‡ç”Ÿæˆå›ºå®šå¤šè§†è§’å…ˆéªŒå›¾åƒå’Œå¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±æ¥è§£å†³è§†è§’ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>å¤šè§†è§’å…ˆéªŒå›¾åƒç”¨äºç»†èŠ‚é‡å»ºï¼Œå¹¶åœ¨è¿­ä»£ä¸­è‡ªåŠ¨æ›´æ–°æƒé‡ä»¥å¹³è¡¡å½¢çŠ¶å’Œç»†èŠ‚ä¼˜åŒ–ã€‚</li>
<li>åˆ©ç”¨åŒæ–¹å·®ä¸ç¡®å®šæ€§çš„åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡æ¥å¹³è¡¡ç²—ç•¥å½¢çŠ¶å’Œç²¾ç»†ç»†èŠ‚çš„ä¼˜åŒ–ã€‚</li>
<li>é‡‡ç”¨é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±æ¥ä¼˜åŒ–è¡¨é¢ä»¥è¿›è¡Œé«˜è´¨é‡çš„ç½‘æ ¼æå–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45653dac6a6a0fb9eae544e11189347a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed5f0ba33ce7a2574c9d7095cb94ecfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa28380074acef91e864378a95dc9e31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9ffd86e52ab7c589009a617ae9d403c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8223589b1165304b30a4456ba1818b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e16f9ef0fdaf6910e75ea37fdcd56a31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4450c67b82bb527b2e06e92c46281222.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Diffusion-Models-for-Anomaly-Detection"><a href="#A-Survey-on-Diffusion-Models-for-Anomaly-Detection" class="headerlink" title="A Survey on Diffusion Models for Anomaly Detection"></a>A Survey on Diffusion Models for Anomaly Detection</h2><p><strong>Authors:Jing Liu, Zhenchao Ma, Zepu Wang, Chenxuanyin Zou, Jiayang Ren, Zehua Wang, Liang Song, Bo Hu, Yang Liu, Victor C. M. Leung</strong></p>
<p>Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we review recent advances in DMAD research. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at <a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">https://github.com/fdjingliu/DMAD</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºä¸€ç±»å¼ºå¤§çš„ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹å·²ç»å´­éœ²å¤´è§’ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ï¼Œæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼Œå¦‚ç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œåˆ¶é€ ç­‰ã€‚è¿™ä¸¤ä¸ªé¢†åŸŸçš„äº¤é›†ï¼Œè¢«ç§°ä¸ºç”¨äºå¼‚å¸¸æ£€æµ‹çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMADï¼‰ï¼Œä¸ºè¯†åˆ«æ—¥ç›Šå¤æ‚å’Œé«˜ç»´æ•°æ®ä¸­çš„åå·®æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†DMADç ”ç©¶çš„æœ€æ–°è¿›å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»ADå’ŒDMçš„åŸºæœ¬æ¦‚å¿µï¼Œç„¶åå¯¹ç»å…¸çš„DMæ¶æ„è¿›è¡Œå…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬DDPMsã€DDIMSå’ŒScore SDEsã€‚æˆ‘ä»¬å°†ç°æœ‰çš„DMADæ–¹æ³•è¿›ä¸€æ­¥åˆ†ä¸ºåŸºäºé‡å»ºçš„ã€åŸºäºå¯†åº¦çš„å’Œæ··åˆæ–¹æ³•ï¼Œå¹¶å¯¹å…¶æ–¹æ³•åˆ›æ–°è¿›è¡Œè¯¦ç»†çš„ç ”ç©¶ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸åŒæ•°æ®æ¨¡æ€çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†å…³é”®çš„æŒ‘æˆ˜å’Œæ–°å…´çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€å¢å¼ºç¨³å¥æ€§ã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆã€‚DMADç ”ç©¶è®ºæ–‡å’Œèµ„æºé›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fdjingliu/DMADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11430v4">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ–°å…´å¼ºå¤§ç±»åˆ«ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¹¿æ³›åº”ç”¨äºç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œåˆ¶é€ ç­‰é¢†åŸŸã€‚æœ¬æ–‡ç»¼è¿°äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œä»‹ç»äº†å¼‚å¸¸æ£€æµ‹å’Œæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µï¼Œåˆ†æäº†ç»å…¸çš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œå¦‚DDPMsã€DDIIMså’ŒScore SDEsï¼Œå¹¶å°†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•åˆ†ä¸ºé‡å»ºå‹ã€å¯†åº¦å‹å’Œæ··åˆå‹æ–¹æ³•ï¼Œè¯¦ç»†æ¢è®¨äº†å®ƒä»¬çš„æ–¹æ³•åˆ›æ–°ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸åŒæ•°æ®æ¨¡æ€çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚æ–‡ç« æœ€åè®¨è®ºäº†å…³é”®æŒ‘æˆ˜å’Œæ–°å…´ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆã€‚ç›¸å…³èµ„æºå¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œé€‚ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºå¼‚å¸¸æ£€æµ‹ï¼ˆDMADï¼‰æ˜¯è¯†åˆ«é«˜ç»´æ•°æ®ä¸­åå·®çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç»å…¸æ‰©æ•£æ¨¡å‹æ¶æ„åŒ…æ‹¬DDPMsã€DDIIMså’ŒScore SDEsã€‚</li>
<li>ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¯åˆ†ä¸ºé‡å»ºå‹ã€å¯†åº¦å‹å’Œæ··åˆå‹æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯å¤„ç†å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®ç­‰å¤šç§ç±»å‹çš„æ•°æ®ã€‚</li>
<li>é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§å’Œé²æ£’æ€§å¢å¼ºç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-283a909c43a207b7bf832f36d4df87b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5757f4748c16ca0b92aef664646e184.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78ffb4067f89f8f2fbe695cb0bfaaa79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91a59d760f27fd4e8799dcc76376000a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5133c2fff8778a66219d0ec65d750a5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mask-Approximation-Net-A-Novel-Diffusion-Model-Approach-for-Remote-Sensing-Change-Captioning"><a href="#Mask-Approximation-Net-A-Novel-Diffusion-Model-Approach-for-Remote-Sensing-Change-Captioning" class="headerlink" title="Mask Approximation Net: A Novel Diffusion Model Approach for Remote   Sensing Change Captioning"></a>Mask Approximation Net: A Novel Diffusion Model Approach for Remote   Sensing Change Captioning</h2><p><strong>Authors:Dongwei Sun, Jing Yao, Changsheng Zhou, Xiangyong Cao, Pedram Ghamisi</strong></p>
<p>Remote sensing image change description represents an innovative multimodal task within the realm of remote sensing processing. This task not only facilitates the detection of alterations in surface conditions, but also provides comprehensive descriptions of these changes, thereby improving human interpretability and interactivity.Generally, existing deep-learning-based methods predominantly utilized a three-stage framework that successively perform feature extraction, feature fusion, and localization from bitemporal images before text generation. However, this reliance often leads to an excessive focus on the design of specific network architectures and restricts the feature distributions to the dataset at hand, which in turn results in limited generalizability and robustness during application.To address these limitations, this paper proposes a novel approach for remote sensing image change detection and description that incorporates diffusion models, aiming to transition the emphasis of modeling paradigms from conventional feature learning to data distribution learning. The proposed method primarily includes a simple multi-scale change detection module, whose output features are subsequently refined by an well-designed diffusion model. Furthermore, we introduce a frequency-guided complex filter module to boost the model performance by managing high-frequency noise throughout the diffusion process. We validate the effectiveness of our proposed method across several datasets for remote sensing change detection and description, showcasing its superior performance compared to existing techniques. The code will be available at \href{<a target="_blank" rel="noopener" href="https://github.com/sundongwei%7D%7BMaskApproxNet%7D">https://github.com/sundongwei}{MaskApproxNet}</a> after a possible publication. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ˜¯é¥æ„Ÿå¤„ç†é¢†åŸŸçš„ä¸€é¡¹åˆ›æ–°çš„å¤šæ¨¡å¼ä»»åŠ¡ã€‚è¿™ä¸€ä»»åŠ¡ä¸ä»…æœ‰åŠ©äºæ£€æµ‹åœ°è¡¨æ¡ä»¶çš„å˜åŒ–ï¼Œè€Œä¸”æä¾›äº†å¯¹è¿™äº›å˜åŒ–çš„å…¨é¢æè¿°ï¼Œä»è€Œæé«˜äº†äººç±»å¯è§£é‡Šæ€§å’Œäº¤äº’æ€§ã€‚ç›®å‰ï¼Œç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ä¸»è¦é‡‡ç”¨äº†ä¸‰é˜¶æ®µçš„æ¡†æ¶ï¼Œä¾æ¬¡ä»åŒæ—¶åºå›¾åƒä¸­è¿›è¡Œç‰¹å¾æå–ã€ç‰¹å¾èåˆå’Œå®šä½ï¼Œç„¶åè¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–å¾€å¾€å¯¼è‡´è¿‡äºä¸“æ³¨äºç‰¹å®šç½‘ç»œæ¶æ„çš„è®¾è®¡ï¼Œå¹¶é™åˆ¶ç‰¹å¾åˆ†å¸ƒåˆ°å½“å‰æ•°æ®é›†ï¼Œä»è€Œåœ¨åº”ç”¨è¿‡ç¨‹ä¸­å¯¼è‡´æœ‰é™çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹è¿›è¡Œé¥æ„Ÿå›¾åƒå˜åŒ–æ£€æµ‹ä¸æè¿°çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å°†å»ºæ¨¡èŒƒå¼çš„é‡ç‚¹ä»ä¼ ç»Ÿç‰¹å¾å­¦ä¹ è½¬å‘æ•°æ®åˆ†å¸ƒå­¦ä¹ ã€‚è¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸€ä¸ªç®€å•çš„å¤šå°ºåº¦å˜åŒ–æ£€æµ‹æ¨¡å—ï¼Œå…¶è¾“å‡ºç‰¹å¾éšåè¢«ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ‰©æ•£æ¨¡å‹æ‰€ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢‘ç‡å¼•å¯¼å¤æ‚æ»¤æ³¢å™¨æ¨¡å—ï¼Œé€šè¿‡ç®¡ç†æ‰©æ•£è¿‡ç¨‹ä¸­çš„é«˜é¢‘å™ªå£°æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¥æ„Ÿå˜åŒ–æ£€æµ‹å’Œæè¿°çš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯çš„ä¼˜è¶Šæ€§èƒ½ã€‚ä»£ç å°†åœ¨å¯èƒ½å‘è¡¨åäºMaskApproxNetï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/sundongwei%EF%BC%89%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/sundongweiï¼‰ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19179v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ˜¯é¥æ„Ÿå¤„ç†ä¸­çš„ä¸€ç§åˆ›æ–°çš„å¤šæ¨¡å¼ä»»åŠ¡ã€‚æ­¤ä»»åŠ¡ä¸ä»…ä¿ƒè¿›äº†åœ°è¡¨æ¡ä»¶å˜åŒ–çš„æ£€æµ‹ï¼Œè€Œä¸”æä¾›äº†è¿™äº›å˜åŒ–çš„å…¨é¢æè¿°ï¼Œæé«˜äº†äººç±»å¯è§£é‡Šæ€§å’Œäº¤äº’æ€§ã€‚ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œä¾æ¬¡ä»åŒæ—¶æ€å›¾åƒè¿›è¡Œç‰¹å¾æå–ã€ç‰¹å¾èåˆå’Œå®šä½ï¼Œç„¶åè¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•è¿‡äºä¾èµ–ç‰¹å®šç½‘ç»œæ¶æ„çš„è®¾è®¡ï¼Œé™åˆ¶äº†ç‰¹å¾åˆ†å¸ƒå¯¹æ•°æ®é›†çš„ä¾èµ–ï¼Œå¯¼è‡´åœ¨åº”ç”¨ä¸­çš„é€šç”¨æ€§å’Œé²æ£’æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ—¨åœ¨ä»ä¼ ç»Ÿçš„ç‰¹å¾å­¦ä¹ è½¬å‘æ•°æ®åˆ†å¸ƒå­¦ä¹ ã€‚æ–°æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç®€å•å¤šå°ºåº¦å˜åŒ–æ£€æµ‹æ¨¡å—å’Œç²¾å¿ƒè®¾è®¡æ‰©æ•£æ¨¡å‹æ¥ä¼˜åŒ–ç‰¹å¾è¾“å‡ºã€‚æ­¤å¤–ï¼Œå¼•å…¥é¢‘ç‡å¼•å¯¼å¤æ‚æ»¤æ³¢å™¨æ¨¡å—ï¼Œé€šè¿‡ç®¡ç†æ‰©æ•£è¿‡ç¨‹ä¸­çš„é«˜é¢‘å™ªå£°æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡é¥æ„Ÿå˜åŒ–æ£€æµ‹å’Œæè¿°çš„å‡ ä¸ªæ•°æ®é›†éªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ç›¸å…³ä»£ç å°†åœ¨å¯èƒ½å‘å¸ƒåäºMaskApproxNetç½‘ç«™ä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ˜¯å¤šæ¨¡å¼é¥æ„Ÿå¤„ç†ä»»åŠ¡ä¹‹ä¸€ï¼Œèƒ½æé«˜äººç±»è§£é‡Šæ€§å’Œäº¤äº’æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–ç‰¹å®šç½‘ç»œæ¶æ„è®¾è®¡ï¼Œé™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>æ–°æ–¹æ³•ç»“åˆæ‰©æ•£æ¨¡å‹ï¼Œä»æ•°æ®åˆ†å¸ƒè§’åº¦è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ–°æ–¹æ³•åŒ…æ‹¬ç®€å•å¤šå°ºåº¦å˜åŒ–æ£€æµ‹æ¨¡å—å’Œé¢‘ç‡å¼•å¯¼å¤æ‚æ»¤æ³¢å™¨æ¨¡å—æ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>ç›¸å…³ä»£ç å°†åœ¨å‘å¸ƒåå…¬å¼€äºMaskApproxNetç½‘ç«™ä¸Šä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b7479149d0661c3d5bc42089b6806de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b451069ab63bc5403c067ebced9c8d19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-141262c770d47ac3f9038036aa5b46f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-343730b34b59f7f4b7ed59629ebfec51.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adapting-Image-to-Video-Diffusion-Models-for-Large-Motion-Frame-Interpolation"><a href="#Adapting-Image-to-Video-Diffusion-Models-for-Large-Motion-Frame-Interpolation" class="headerlink" title="Adapting Image-to-Video Diffusion Models for Large-Motion Frame   Interpolation"></a>Adapting Image-to-Video Diffusion Models for Large-Motion Frame   Interpolation</h2><p><strong>Authors:Luoxu Jin, Hiroshi Watanabe</strong></p>
<p>With the development of video generation models has advanced significantly in recent years, we adopt large-scale image-to-video diffusion models for video frame interpolation. We present a conditional encoder designed to adapt an image-to-video model for large-motion frame interpolation. To enhance performance, we integrate a dual-branch feature extractor and propose a cross-frame attention mechanism that effectively captures both spatial and temporal information, enabling accurate interpolations of intermediate frames. Our approach demonstrates superior performance on the Fr&#39;echet Video Distance (FVD) metric when evaluated against other state-of-the-art approaches, particularly in handling large motion scenarios, highlighting advancements in generative-based methodologies. </p>
<blockquote>
<p>éšç€è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥æ˜¾è‘—å‘å±•ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¸§æ’å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡ä»¶ç¼–ç å™¨ï¼Œæ—¨åœ¨é€‚åº”å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå¤§è¿åŠ¨å¸§æ’å€¼ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬æ•´åˆäº†åŒåˆ†æ”¯ç‰¹å¾æå–å™¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§è·¨å¸§æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œä»è€Œå®ç°ä¸­é—´å¸§çš„å‡†ç¡®æ’å€¼ã€‚ä¸å…¶ä»–æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FrÃ©chetè§†é¢‘è·ç¦»ï¼ˆFVDï¼‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è¿åŠ¨åœºæ™¯æ—¶ï¼Œçªæ˜¾äº†åŸºäºç”Ÿæˆçš„æ–¹æ³•çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17042v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤§è§„æ¨¡çš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¸§æ’å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡ä»¶ç¼–ç å™¨ï¼Œæ—¨åœ¨é€‚åº”å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå¤§è¿åŠ¨å¸§æ’å€¼ã€‚ä¸ºæé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬æ•´åˆäº†åŒåˆ†æ”¯ç‰¹å¾æå–å™¨ï¼Œå¹¶æå‡ºäº†è·¨å¸§æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆæ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œå®ç°ä¸­é—´å¸§çš„å‡†ç¡®æ’å€¼ã€‚ä¸å…¶ä»–æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FrÃ©chetè§†é¢‘è·ç¦»ï¼ˆFVDï¼‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è¿åŠ¨åœºæ™¯æ—¶ï¼Œå‡¸æ˜¾äº†åŸºäºç”Ÿæˆæ–¹æ³•çš„æŠ€æœ¯è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥æœ‰æ˜¾è‘—å‘å±•ã€‚</li>
<li>é‡‡ç”¨å¤§è§„æ¨¡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¸§æ’å€¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ¡ä»¶ç¼–ç å™¨ï¼Œé€‚åº”å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå¤§è¿åŠ¨å¸§æ’å€¼ã€‚</li>
<li>æ•´åˆåŒåˆ†æ”¯ç‰¹å¾æå–å™¨æé«˜æ€§èƒ½ã€‚</li>
<li>è·¨å¸§æ³¨æ„åŠ›æœºåˆ¶èƒ½æœ‰æ•ˆæ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ã€‚</li>
<li>æ–¹æ³•åœ¨FrÃ©chetè§†é¢‘è·ç¦»ï¼ˆFVDï¼‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3da3acc94bf45155bc9773e798428455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53d3ba007edc40616191d5ef2bf0a65d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14dfedcc41c3cabdaf76f3c0dab85ea8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4701af9c4a0021500deaa15d8d74ed81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034c912176f49c15554212834b358014.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8644cdae30ba93af75f9f8eb24c60d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc0a9391b5e51da0f52cfe5cf866d04c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="How-to-Backdoor-Consistency-Models"><a href="#How-to-Backdoor-Consistency-Models" class="headerlink" title="How to Backdoor Consistency Models?"></a>How to Backdoor Consistency Models?</h2><p><strong>Authors:Chengen Wang, Murat Kantarcioglu</strong></p>
<p>Consistency models are a new class of models that generate images by directly mapping noise to data, allowing for one-step generation and significantly accelerating the sampling process. However, their robustness against adversarial attacks has not yet been thoroughly investigated. In this work, we conduct the first study on the vulnerability of consistency models to backdoor attacks. While previous research has explored backdoor attacks on diffusion models, those studies have primarily focused on conventional diffusion models, employing a customized backdoor training process and objective, whereas consistency models have distinct training processes and objectives. Our proposed framework demonstrates the vulnerability of consistency models to backdoor attacks. During image generation, poisoned consistency models produce images with a Fr&#39;echet Inception Distance (FID) comparable to that of a clean model when sampling from Gaussian noise. However, once the trigger is activated, they generate backdoor target images. We explore various trigger and target configurations to evaluate the vulnerability of consistency models, including the use of random noise as a trigger. This novel trigger is visually inconspicuous, more challenging to detect, and aligns well with the sampling process of consistency models. Across all configurations, our framework successfully compromises the consistency models while maintaining high utility and specificity. We also examine the stealthiness of our proposed attack, which is attributed to the unique properties of consistency models and the elusive nature of the Gaussian noise trigger. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/chengenw/backdoorCM%7D%7Bhttps://github.com/chengenw/backdoorCM%7D">https://github.com/chengenw/backdoorCM}{https://github.com/chengenw/backdoorCM}</a>. </p>
<blockquote>
<p>ä¸€è‡´æ€§æ¨¡å‹æ˜¯ä¸€ç±»æ–°çš„æ¨¡å‹ï¼Œå®ƒä»¬é€šè¿‡ç›´æ¥å°†å™ªå£°æ˜ å°„åˆ°æ•°æ®æ¥ç”Ÿæˆå›¾åƒï¼Œå®ç°äº†ä¸€æ­¥ç”Ÿæˆï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿäº†é‡‡æ ·è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹æŠ—å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§å°šæœªå¾—åˆ°å½»åº•ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸€è‡´æ€§æ¨¡å‹å¯¹åé—¨æ”»å‡»çš„è„†å¼±æ€§è¿›è¡Œäº†é¦–æ¬¡ç ”ç©¶ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹ä¸Šçš„åé—¨æ”»å‡»ï¼Œä½†è¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ä¸Šï¼Œé‡‡ç”¨å®šåˆ¶çš„åé—¨è®­ç»ƒè¿‡ç¨‹å’Œç›®æ ‡ï¼Œè€Œä¸€è‡´æ€§æ¨¡å‹å…·æœ‰ä¸åŒçš„è®­ç»ƒè¿‡ç¨‹å’Œç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶è¯æ˜äº†ä¸€è‡´æ€§æ¨¡å‹å¯¹åé—¨æ”»å‡»çš„è„†å¼±æ€§ã€‚åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä¸­æ¯’çš„ä¸€è‡´æ€§æ¨¡å‹åœ¨ä»é«˜æ–¯å™ªå£°é‡‡æ ·æ—¶äº§ç”Ÿçš„å›¾åƒä¸å¹²å‡€æ¨¡å‹çš„FrÃ©chet Inception Distance (FID)ç›¸å½“ã€‚ç„¶è€Œï¼Œä¸€æ—¦è§¦å‘æ¡ä»¶è¢«æ¿€æ´»ï¼Œå®ƒä»¬å°±ä¼šç”Ÿæˆåé—¨ç›®æ ‡å›¾åƒã€‚æˆ‘ä»¬æ¢ç´¢äº†å„ç§è§¦å‘å™¨å’Œç›®æ ‡é…ç½®æ¥è¯„ä¼°ä¸€è‡´æ€§æ¨¡å‹çš„è„†å¼±æ€§ï¼ŒåŒ…æ‹¬ä½¿ç”¨éšæœºå™ªå£°ä½œä¸ºè§¦å‘å™¨ã€‚è¿™ç§æ–°å‹è§¦å‘å™¨åœ¨è§†è§‰ä¸Šå¹¶ä¸æ˜¾çœ¼ï¼Œæ›´éš¾ä»¥æ£€æµ‹ï¼Œå¹¶ä¸”ä¸ä¸€è‡´æ€§æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹éå¸¸å¥‘åˆã€‚åœ¨æ‰€æœ‰é…ç½®ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸæ”»å‡»äº†ä¸€è‡´æ€§æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å®ç”¨æ€§å’Œç‰¹å¼‚æ€§ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥äº†æ‰€æå‡ºçš„æ”»å‡»çš„éšè”½æ€§ï¼Œè¿™å½’åŠŸäºä¸€è‡´æ€§æ¨¡å‹çš„ç‹¬ç‰¹å±æ€§å’Œé«˜æ–¯å™ªå£°è§¦å‘å™¨çš„éšè”½æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chengenw/backdoorCM">https://github.com/chengenw/backdoorCM</a>å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19785v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä¸€è‡´æ€§æ¨¡å‹æ˜¯ä¸€ç±»æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬é€šè¿‡ç›´æ¥å°†å™ªå£°æ˜ å°„åˆ°æ•°æ®æ¥ç”Ÿæˆå›¾åƒï¼Œå®ç°äº†ä¸€æ­¥ç”Ÿæˆï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿäº†é‡‡æ ·è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹æŠ—æ¶æ„æ”»å‡»çš„ç¨³å®šæ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†ä¸€è‡´æ€§æ¨¡å‹å¯¹åé—¨æ”»å‡»çš„è„†å¼±æ€§ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„åé—¨æ”»å‡»ï¼Œä½†è¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é‡‡ç”¨å®šåˆ¶åé—¨è®­ç»ƒè¿‡ç¨‹å’Œç›®æ ‡çš„ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸Šï¼Œè€Œä¸€è‡´æ€§æ¨¡å‹å…·æœ‰ä¸åŒçš„è®­ç»ƒè¿‡ç¨‹å’Œç›®æ ‡ã€‚æœ¬ç ”ç©¶æå‡ºçš„æ¡†æ¶è¯æ˜äº†ä¸€è‡´æ€§æ¨¡å‹å¯¹åé—¨æ”»å‡»çš„è„†å¼±æ€§ã€‚åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä¸­æ¯’çš„ä¸€è‡´æ€§æ¨¡å‹äº§ç”Ÿçš„å›¾åƒçš„FrÂ´echet Inception Distance (FID)ä¸å¹²å‡€æ¨¡å‹çš„FIDç›¸å½“ï¼Œè¿™æ˜¯ä»é«˜æ–¯å™ªå£°ä¸­é‡‡æ ·å¾—åˆ°çš„ã€‚ç„¶è€Œï¼Œä¸€æ—¦è§¦å‘æ¡ä»¶è¢«æ¿€æ´»ï¼Œå®ƒä»¬å°±ä¼šç”Ÿæˆåé—¨ç›®æ ‡å›¾åƒã€‚æˆ‘ä»¬æ¢ç´¢äº†å„ç§è§¦å‘å™¨å’Œç›®æ ‡é…ç½®ï¼Œä»¥è¯„ä¼°ä¸€è‡´æ€§æ¨¡å‹çš„è„†å¼±æ€§ï¼ŒåŒ…æ‹¬ä½¿ç”¨éšæœºå™ªå£°ä½œä¸ºè§¦å‘å™¨ã€‚è¿™ç§æ–°å‹è§¦å‘å™¨åœ¨è§†è§‰ä¸Šå¹¶ä¸æ˜¾çœ¼ï¼Œæ›´éš¾æ£€æµ‹ï¼Œä¸”ä¸ä¸€è‡´æ€§æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹éå¸¸å¥‘åˆã€‚åœ¨æ‰€æœ‰é…ç½®ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸåœ°æ”»å‡»äº†ä¸€è‡´æ€§æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å®ç”¨æ€§å’Œç‰¹å¼‚æ€§ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥äº†æˆ‘ä»¬æå‡ºçš„æ”»å‡»çš„éšè”½æ€§ï¼Œè¿™å½’åŠŸäºä¸€è‡´æ€§æ¨¡å‹çš„ç‹¬ç‰¹å±æ€§å’Œé«˜æ–¯å™ªå£°è§¦å‘å™¨çš„éš¾ä»¥æ‰æ‘¸ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chengenw/backdoorCM">https://github.com/chengenw/backdoorCM</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¸€è‡´æ€§æ¨¡å‹æ˜¯ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç›´æ¥æ˜ å°„å™ªå£°åˆ°æ•°æ®æ¥ç”Ÿæˆå›¾åƒï¼ŒåŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>ç›®å‰å¯¹äºä¸€è‡´æ€§æ¨¡å‹å¯¹æŠ—æ¶æ„æ”»å‡»çš„ç¨³å®šæ€§ç ”ç©¶å°šä¸å……åˆ†ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç©¶äº†ä¸€è‡´æ€§æ¨¡å‹å¯¹åé—¨æ”»å‡»çš„è„†å¼±æ€§ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶æˆåŠŸå±•ç¤ºäº†ä¸€è‡´æ€§æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„åé—¨æ”»å‡»è„†å¼±æ€§ã€‚</li>
<li>ä¸­æ¯’çš„ä¸€è‡´æ€§æ¨¡å‹åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å›¾åƒä¸å¹²å‡€æ¨¡å‹çš„å›¾åƒç›¸ä¼¼ï¼Œä½†åœ¨è§¦å‘æ¡ä»¶æ¿€æ´»åç”Ÿæˆç›®æ ‡åé—¨å›¾åƒã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å„ç§è§¦å‘å™¨å’Œç›®æ ‡é…ç½®æ¥è¯„ä¼°ä¸€è‡´æ€§æ¨¡å‹çš„è„†å¼±æ€§ï¼ŒåŒ…æ‹¬ä½¿ç”¨è§†è§‰ä¸Šä¸æ˜¾çœ¼çš„éšæœºå™ªå£°ä½œä¸ºè§¦å‘å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52b7027ca39561446a97edde35cc2a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49885af4493d624f0f14208a76273084.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-043fcff2b209a7083b3a707cd7e27b89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90f6c3baaf03db1781655a049ef756cb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Efficient-Domain-Augmentation-for-Autonomous-Driving-Testing-Using-Diffusion-Models"><a href="#Efficient-Domain-Augmentation-for-Autonomous-Driving-Testing-Using-Diffusion-Models" class="headerlink" title="Efficient Domain Augmentation for Autonomous Driving Testing Using   Diffusion Models"></a>Efficient Domain Augmentation for Autonomous Driving Testing Using   Diffusion Models</h2><p><strong>Authors:Luciano Baresi, Davide Yi Xian Hu, Andrea Stocco, Paolo Tonella</strong></p>
<p>Simulation-based testing is widely used to assess the reliability of Autonomous Driving Systems (ADS), but its effectiveness is limited by the operational design domain (ODD) conditions available in such simulators. To address this limitation, in this work, we explore the integration of generative artificial intelligence techniques with physics-based simulators to enhance ADS system-level testing. Our study evaluates the effectiveness and computational overhead of three generative strategies based on diffusion models, namely instruction-editing, inpainting, and inpainting with refinement. Specifically, we assess these techniquesâ€™ capabilities to produce augmented simulator-generated images of driving scenarios representing new ODDs. We employ a novel automated detector for invalid inputs based on semantic segmentation to ensure semantic preservation and realism of the neural generated images. We then perform system-level testing to evaluate the ADSâ€™s generalization ability to newly synthesized ODDs. Our findings show that diffusion models help increase the ODD coverage for system-level testing of ADS. Our automated semantic validator achieved a percentage of false positives as low as 3%, retaining the correctness and quality of the generated images for testing. Our approach successfully identified new ADS system failures before real-world testing. </p>
<blockquote>
<p>åŸºäºæ¨¡æ‹Ÿå™¨çš„æµ‹è¯•æ˜¯è¯„ä¼°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰å¯é æ€§çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—é™äºæ¨¡æ‹Ÿå™¨ä¸­å¯ç”¨çš„æ“ä½œè®¾è®¡åŸŸï¼ˆODDï¼‰æ¡ä»¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨ç›¸ç»“åˆï¼Œä»¥æé«˜ADSç³»ç»Ÿçº§æµ‹è¯•çš„æ•ˆæœã€‚æˆ‘ä»¬çš„ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œè®¡ç®—å¼€é”€ï¼Œåˆ†åˆ«æ˜¯æŒ‡ä»¤ç¼–è¾‘ã€å›¾åƒè¡¥å…¨å’Œç²¾ç»†å›¾åƒè¡¥å…¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™äº›æŠ€æœ¯åœ¨ç”Ÿæˆä»£è¡¨æ–°ODDçš„é©¾é©¶åœºæ™¯æ¨¡æ‹Ÿå™¨å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºè¯­ä¹‰åˆ†å‰²çš„æ–°å‹è‡ªåŠ¨åŒ–æ£€æµ‹å™¨ï¼Œä»¥ç¡®ä¿ç¥ç»ç”Ÿæˆå›¾åƒè¯­ä¹‰çš„ä¿ç•™å’ŒçœŸå®æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œç³»ç»Ÿçº§æµ‹è¯•ï¼Œä»¥è¯„ä¼°ADSå¯¹æ–°åˆæˆODDçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹æœ‰åŠ©äºå¢åŠ ç³»ç»Ÿçº§æµ‹è¯•ä¸­å¯¹ADSçš„ODDè¦†ç›–ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨è¯­ä¹‰éªŒè¯å™¨å®ç°äº†é«˜è¾¾3%çš„è¯¯æŠ¥ç‡ï¼Œä¿æŒäº†æµ‹è¯•å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ­£ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°åœ¨çœŸå®ä¸–ç•Œæµ‹è¯•ä¹‹å‰è¯†åˆ«å‡ºäº†æ–°çš„ADSç³»ç»Ÿæ•…éšœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13661v3">PDF</a> Accepted for publication at the 47th International Conference on   Software Engineering (ICSE 2025). This research was partially supported by   project EMELIOT, funded by MUR under the PRIN 2020 program (n. 2020W3A5FY),   by the Bavarian Ministry of Economic Affairs, Regional Development and   Energy, by the TUM Global Incentive Fund, and by the EU Project Sec4AI4Sec   (n. 101120393)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”ŸæˆæŠ€æœ¯åœ¨è‡ªä¸»é©¾é©¶ç³»ç»Ÿä»¿çœŸæµ‹è¯•ä¸­çš„åº”ç”¨æ‘˜è¦ï¼šä¸ºè§£å†³ä»¿çœŸæµ‹è¯•åœ¨è¯„ä¼°è‡ªä¸»é©¾é©¶ç³»ç»Ÿå¯é æ€§æ—¶å—é™äºæ“ä½œè®¾è®¡åŸŸæ¡ä»¶çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸åŸºäºç‰©ç†çš„ä»¿çœŸå™¨çš„èåˆï¼Œä»¥æå‡è‡ªä¸»é©¾é©¶ç³»ç»Ÿçº§åˆ«çš„æµ‹è¯•æ°´å¹³ã€‚ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥ï¼ŒåŒ…æ‹¬æŒ‡ä»¤ç¼–è¾‘ã€è¡¥å…¨åŠå¸¦ç²¾ä¿®çš„è¡¥å…¨æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆä»£è¡¨æ–°æ“ä½œè®¾è®¡åŸŸé©¾é©¶åœºæ™¯å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§åŠè®¡ç®—å¼€é”€ã€‚é‡‡ç”¨æ–°å‹è‡ªåŠ¨åŒ–æ— æ•ˆè¾“å…¥æ£€æµ‹å™¨ç¡®ä¿ç¥ç»ç”Ÿæˆå›¾åƒè¯­ä¹‰çš„å®Œæ•´æ€§å’ŒçœŸå®æ€§ã€‚ç³»ç»Ÿçº§æµ‹è¯•æ˜¾ç¤ºï¼Œæ‰©æ•£æ¨¡å‹æœ‰åŠ©äºæé«˜è‡ªä¸»é©¾é©¶ç³»ç»Ÿå¯¹æ–°åˆæˆæ“ä½œè®¾è®¡åŸŸçš„è¦†ç›–èƒ½åŠ›ï¼Œè‡ªåŠ¨è¯­ä¹‰éªŒè¯å™¨çš„è¯¯æŠ¥ç‡ä½äº3%ï¼Œç¡®ä¿äº†æµ‹è¯•å›¾åƒçš„è´¨é‡å’Œæ­£ç¡®æ€§ã€‚æœ¬ç ”ç©¶æ–¹æ³•å¯åœ¨çœŸå®ä¸–ç•Œæµ‹è¯•å‰æˆåŠŸè¯†åˆ«å‡ºæ–°çš„è‡ªä¸»é©¾é©¶ç³»ç»Ÿæ•…éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»¿çœŸæµ‹è¯•åœ¨è¯„ä¼°è‡ªä¸»é©¾é©¶ç³»ç»Ÿå¯é æ€§æ—¶å—åˆ°æ“ä½œè®¾è®¡åŸŸçš„é™åˆ¶ã€‚</li>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸ç‰©ç†ä»¿çœŸå™¨çš„ç»“åˆèƒ½æå‡è‡ªä¸»é©¾é©¶ç³»ç»Ÿçº§åˆ«çš„æµ‹è¯•ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥åœ¨ç”Ÿæˆæ–°æ“ä½œè®¾è®¡åŸŸé©¾é©¶åœºæ™¯å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é‡‡ç”¨è‡ªåŠ¨åŒ–æ— æ•ˆè¾“å…¥æ£€æµ‹å™¨ç¡®ä¿ç¥ç»ç”Ÿæˆå›¾åƒè¯­ä¹‰çš„å®Œæ•´æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æé«˜äº†è‡ªä¸»é©¾é©¶ç³»ç»Ÿå¯¹æ–°åˆæˆæ“ä½œè®¾è®¡åŸŸçš„è¦†ç›–èƒ½åŠ›ã€‚</li>
<li>è‡ªåŠ¨è¯­ä¹‰éªŒè¯å™¨çš„è¯¯æŠ¥ç‡ä½äº3%ï¼Œç¡®ä¿æµ‹è¯•å›¾åƒçš„è´¨é‡å’Œæ­£ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œæµ‹è¯•å‰è¯†åˆ«å‡ºæ–°çš„è‡ªä¸»é©¾é©¶ç³»ç»Ÿæ•…éšœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-deb9e6baf43786a5816f55518bada801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1a1137d6c9f219c9dab81f9e57d9b46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0da5b0902f1db3ddbc0915c3b898f6ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-115c8ba24542a9f9ed4af45af09d383f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38a1b302037ef9009c03d3b5bf3bbced.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DiffQRCoder-Diffusion-based-Aesthetic-QR-Code-Generation-with-Scanning-Robustness-Guided-Iterative-Refinement"><a href="#DiffQRCoder-Diffusion-based-Aesthetic-QR-Code-Generation-with-Scanning-Robustness-Guided-Iterative-Refinement" class="headerlink" title="DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning   Robustness Guided Iterative Refinement"></a>DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning   Robustness Guided Iterative Refinement</h2><p><strong>Authors:Jia-Wei Liao, Winston Wang, Tzu-Sian Wang, Li-Xuan Peng, Ju-Hsuan Weng, Cheng-Fu Chou, Jun-Cheng Chen</strong></p>
<p>With the success of Diffusion Models for image generation, the technologies also have revolutionized the aesthetic Quick Response (QR) code generation. Despite significant improvements in visual attractiveness for the beautified codes, their scannabilities are usually sacrificed and thus hinder their practical uses in real-world scenarios. To address this issue, we propose a novel training-free Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both scannable and visually pleasing QR codes. The proposed approach introduces Scanning-Robust Perceptual Guidance (SRPG), a new diffusion guidance for Diffusion Models to guarantee the generated aesthetic codes to obey the ground-truth QR codes while maintaining their attractiveness during the denoising process. Additionally, we present another post-processing technique, Scanning Robust Manifold Projected Gradient Descent (SR-MPGD), to further enhance their scanning robustness through iterative latent space optimization. With extensive experiments, the results demonstrate that our approach not only outperforms other compared methods in Scanning Success Rate (SSR) with better or comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves the SSR of the ControlNet-only approach from 60% to 99%. The subjective evaluation indicates that our approach achieves promising visual attractiveness to users as well. Finally, even with different scanning angles and the most rigorous error tolerance settings, our approach robustly achieves over 95% SSR, demonstrating its capability for real-world applications. Our project page is available at <a target="_blank" rel="noopener" href="https://jwliao1209.github.io/DiffQRCoder">https://jwliao1209.github.io/DiffQRCoder</a>. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„æˆåŠŸï¼Œè¯¥æŠ€æœ¯ä¹Ÿæ·±åˆ»å½±å“äº†ç¾å­¦å¿«é€Ÿå“åº”ï¼ˆQRï¼‰ç ç”Ÿæˆã€‚å°½ç®¡åœ¨ç¾åŒ–ä»£ç çš„è§†è§‰å¸å¼•åŠ›æ–¹é¢å–å¾—äº†é‡å¤§æ”¹è¿›ï¼Œä½†å®ƒä»¬çš„å¯æ‰«ææ€§é€šå¸¸ä¼šè¢«ç‰ºç‰²ï¼Œä»è€Œé˜»ç¢äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åŸºäºæ‰©æ•£çš„QRç ç”Ÿæˆå™¨ï¼ˆDiffQRCoderï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢å¯æ‰«æåˆå…·æœ‰è§†è§‰å¸å¼•åŠ›çš„QRç ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ‰«æé²æ£’æ„ŸçŸ¥æŒ‡å¯¼ï¼ˆSRPGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æŒ‡å¯¼æ–¹æ³•ï¼Œå¯ä¿è¯ç”Ÿæˆçš„ç¾å­¦ä»£ç éµå¾ªçœŸå®QRä»£ç ï¼ŒåŒæ—¶åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿æŒå…¶å¸å¼•åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å¦ä¸€ç§åå¤„ç†æŠ€æœ¯ï¼Œæ‰«æé²æ£’æµå½¢æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆSR-MPGDï¼‰ï¼Œé€šè¿‡è¿­ä»£æ½œåœ¨ç©ºé—´ä¼˜åŒ–è¿›ä¸€æ­¥æé«˜å…¶æ‰«æç¨³å¥æ€§ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨æ‰«ææˆåŠŸç‡ï¼ˆSSRï¼‰ä¸Šä¼˜äºå…¶ä»–å¯¹æ¯”æ–¹æ³•ï¼Œä¸”CLIPç¾å­¦è¯„åˆ†ï¼ˆCLIP-aes.ï¼‰æ›´å¥½æˆ–ç›¸å½“ï¼›åŒæ—¶ï¼Œä¸ä»…ä½¿ç”¨ControlNetçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬å°†SSRä»60%æé«˜åˆ°99%ã€‚ä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰å¸å¼•åŠ›æ–¹é¢å¯¹ç”¨æˆ·å…·æœ‰å¾ˆå¤§æ½œåŠ›ã€‚æœ€åï¼Œå³ä½¿åœ¨ä¸åŒçš„æ‰«æè§’åº¦å’Œæœ€ä¸¥æ ¼çš„é”™è¯¯å®¹å¿è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»ç„¶ç¨³å¥åœ°å®ç°äº†è¶…è¿‡95%çš„SSRï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://jwliao1209.github.io/DiffQRCoder">https://jwliao1209.github.io/DiffQRCoder</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06355v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æˆåŠŸä¹Ÿæ¨åŠ¨äº†ç¾åŒ–äºŒç»´ç ç”Ÿæˆçš„æŠ€æœ¯é©å‘½ã€‚ä¸ºè§£å†³ç¾è§‚äºŒç»´ç æ‰«ææ€§èƒ½å·®çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ‰©æ•£å¼äºŒç»´ç ç”Ÿæˆå™¨ï¼ˆDiffQRCoderï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢ç¾è§‚åˆå®ç”¨çš„äºŒç»´ç ã€‚é€šè¿‡å¼•å…¥æ‰«æç¨³å¥æ„ŸçŸ¥æŒ‡å¯¼ï¼ˆSRPGï¼‰å’Œæ‰«æç¨³å¥æµå½¢æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆSR-MPGDï¼‰ï¼Œç¡®ä¿ç”Ÿæˆçš„äºŒç»´ç åœ¨ä¿æŒç¾è§‚çš„åŒæ—¶å…·å¤‡è‰¯å¥½çš„æ‰«ææ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨å…¶ä»–æ–¹æ³•çš„æ‰«ææˆåŠŸç‡ï¼ˆSSRï¼‰ä¸Šå…·æœ‰æ›´é«˜çš„è¡¨ç°æˆ–ä¸ä¹‹æŒå¹³çš„CLIPç¾å­¦å¾—åˆ†ï¼ˆCLIP-aes.ï¼‰ï¼Œè¿˜èƒ½æ˜¾è‘—æé«˜ä»…ä½¿ç”¨ControlNetæ–¹æ³•çš„SSRå€¼ä»60%æå‡è‡³99%ã€‚ä¸»è§‚è¯„ä»·æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹ç”¨æˆ·å…·æœ‰å¸å¼•åŠ›ã€‚åœ¨ä¸åŒæ‰«æè§’åº¦å’Œä¸¥æ ¼çš„å®¹é”™è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¾ç„¶ç¨³å¥åœ°å®ç°äº†è¶…è¿‡95%çš„SSRï¼Œè¡¨æ˜å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„èƒ½åŠ›ã€‚æ›´å¤šè¯¦æƒ…å¯è§é¡¹ç›®ç½‘é¡µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²åº”ç”¨äºäºŒç»´ç ç”Ÿæˆï¼Œå®ç°äº†ç¾è§‚ä¸å®ç”¨æ€§çš„ç»“åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— éœ€è®­ç»ƒçš„æ‰©æ•£å¼äºŒç»´ç ç”Ÿæˆå™¨ï¼ˆDiffQRCoderï¼‰ã€‚</li>
<li>å¼•å…¥æ‰«æç¨³å¥æ„ŸçŸ¥æŒ‡å¯¼ï¼ˆSRPGï¼‰ä¿è¯ç”Ÿæˆçš„äºŒç»´ç åœ¨ç¾è§‚åŒæ—¶ä¿æŒè‰¯å¥½æ‰«ææ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æ‰«æç¨³å¥æµå½¢æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆSR-MPGDï¼‰è¿›ä¸€æ­¥æé«˜äºŒç»´ç çš„æ‰«æç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ‰«ææˆåŠŸç‡ï¼ˆSSRï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜ä»…ä½¿ç”¨ControlNetæ–¹æ³•çš„SSRå€¼ã€‚</li>
<li>ä¸»è§‚è¯„ä»·æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¯¹ç”¨æˆ·å…·æœ‰å¸å¼•åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-466d632975b4fe1285693cbea4e04d36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c016d21860d92edafec229a1393977d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5fe7ebe1be740308e8b4020a3ef2560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55eee44b6391947aa0908a7a71635571.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-167b05d6f5237f32dd4a8546bcdf3031.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CatVTON-Concatenation-Is-All-You-Need-for-Virtual-Try-On-with-Diffusion-Models"><a href="#CatVTON-Concatenation-Is-All-You-Need-for-Virtual-Try-On-with-Diffusion-Models" class="headerlink" title="CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion   Models"></a>CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion   Models</h2><p><strong>Authors:Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, Dongmei Jiang, Xiaodan Liang</strong></p>
<p>Virtual try-on methods based on diffusion models achieve realistic effects but often require additional encoding modules, a large number of training parameters, and complex preprocessing, which increases the burden on training and inference. In this work, we re-evaluate the necessity of additional modules and analyze how to improve training efficiency and reduce redundant steps in the inference process. Based on these insights, we propose CatVTON, a simple and efficient virtual try-on diffusion model that transfers in-shop or worn garments of arbitrary categories to target individuals by concatenating them along spatial dimensions as inputs of the diffusion model. The efficiency of CatVTON is reflected in three aspects: (1) Lightweight network. CatVTON consists only of a VAE and a simplified denoising UNet, removing redundant image and text encoders as well as cross-attentions, and includes just 899.06M parameters. (2) Parameter-efficient training. Through experimental analysis, we identify self-attention modules as crucial for adapting pre-trained diffusion models to the virtual try-on task, enabling high-quality results with only 49.57M training parameters. (3) Simplified inference. CatVTON eliminates unnecessary preprocessing, such as pose estimation, human parsing, and captioning, requiring only a person image and garment reference to guide the virtual try-on process, reducing over 49% memory usage compared to other diffusion-based methods. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results compared to baseline methods and demonstrates strong generalization performance in in-the-wild scenarios, despite being trained solely on public datasets with 73K samples. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•è™½ç„¶å¯ä»¥å®ç°é€¼çœŸçš„æ•ˆæœï¼Œä½†é€šå¸¸éœ€è¦é¢å¤–çš„ç¼–ç æ¨¡å—ã€å¤§é‡çš„è®­ç»ƒå‚æ•°å’Œå¤æ‚çš„é¢„å¤„ç†ï¼Œè¿™å¢åŠ äº†è®­ç»ƒå’Œæ¨ç†çš„è´Ÿæ‹…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†é¢å¤–æ¨¡å—çš„å¿…è¦æ€§ï¼Œå¹¶åˆ†æäº†å¦‚ä½•æé«˜è®­ç»ƒæ•ˆç‡å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™æ­¥éª¤ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†CatVTONï¼Œè¿™æ˜¯ä¸€ç§ç®€å•é«˜æ•ˆçš„è™šæ‹Ÿè¯•ç©¿æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡æ²¿ç©ºé—´ç»´åº¦æ‹¼æ¥åº—å†…æˆ–ç©¿ç€çš„ä»»æ„ç±»åˆ«çš„æœè£…ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„è¾“å…¥ï¼Œå°†æœè£…è½¬ç§»åˆ°ç›®æ ‡ä¸ªä½“ä¸Šã€‚CatVTONçš„æ•ˆç‡ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼š</p>
</blockquote>
<p>(1) è½»é‡åŒ–ç½‘ç»œï¼šCatVTONä»…ç”±VAEå’Œç®€åŒ–çš„å»å™ªUNetç»„æˆï¼Œå»é™¤äº†å†—ä½™çš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ä»¥åŠè·¨æ³¨æ„åŠ›æ¨¡å—ï¼Œä»…åŒ…å«899.06Må‚æ•°ã€‚</p>
<p>(2) å‚æ•°é«˜æ•ˆè®­ç»ƒï¼šé€šè¿‡å®éªŒåˆ†æï¼Œæˆ‘ä»¬è®¤å®šè‡ªæ³¨æ„åŠ›æ¨¡å—å¯¹äºé€‚åº”è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä»…ä½¿ç”¨49.57Mè®­ç»ƒå‚æ•°å³å¯å®ç°é«˜è´¨é‡ç»“æœã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15886v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong><br>    åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯æ—¨åœ¨å®ç°é€¼çœŸçš„æ•ˆæœï¼Œä½†åœ¨æ„å»ºé¢å¤–ç¼–ç æ¨¡å—æ—¶å¾€å¾€æ¶‰åŠå¤§é‡çš„è®­ç»ƒå‚æ•°å’Œå¤æ‚çš„é¢„å¤„ç†æ­¥éª¤ï¼Œä»è€Œå¢åŠ äº†è®­ç»ƒå’Œæ¨ç†çš„è´Ÿæ‹…ã€‚æœ¬ç ”ç©¶é‡æ–°è¯„ä¼°äº†é¢å¤–æ¨¡å—çš„å¿…è¦æ€§ï¼Œå¹¶åˆ†æäº†å¦‚ä½•ä¼˜åŒ–è®­ç»ƒæ•ˆç‡å¹¶ç®€åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™æ­¥éª¤ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CatVTONï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•é«˜æ•ˆçš„è™šæ‹Ÿè¯•ç©¿æ‰©æ•£æ¨¡å‹ï¼Œå®ƒé€šè¿‡æ²¿ç©ºé—´ç»´åº¦æ‹¼æ¥åº—å†…æˆ–å·²ç©¿ä¸Šçš„ä»»æ„ç±»åˆ«çš„æœè£…åˆ°ç›®æ ‡ä¸ªä½“ä¸Šï¼Œå®ç°äº†é«˜æ•ˆçš„è™šæ‹Ÿè¯•ç©¿æ•ˆæœã€‚CatVTONçš„ä¼˜åŠ¿ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šè½»é‡çº§ç½‘ç»œã€å‚æ•°é«˜æ•ˆè®­ç»ƒå’Œç®€åŒ–æ¨ç†ã€‚å®ƒé€šè¿‡å»é™¤å†—ä½™çš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ä»¥åŠè·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†ç½‘ç»œçš„è½»é‡åŒ–ï¼›å¹¶é€šè¿‡å®éªŒåˆ†æç¡®å®šäº†è‡ªæ³¨æ„åŠ›æ¨¡å—å¯¹äºé€‚åº”è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡çš„é‡è¦æ€§ï¼Œå®ç°äº†é«˜è´¨é‡çš„ç»“æœï¼›æœ€åï¼Œå®ƒç®€åŒ–äº†ä¸å¿…è¦çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå¦‚å§¿æ€ä¼°è®¡ã€äººä½“è§£æå’Œå­—å¹•ç”Ÿæˆç­‰ï¼Œåªéœ€äººç‰©å›¾åƒå’Œæœè£…å‚è€ƒå³å¯å¼•å¯¼è™šæ‹Ÿè¯•ç©¿è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜äº†CatVTONç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”åœ¨å…¬å¼€æ•°æ®é›†ä¸Šå…·æœ‰å¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯å¯å®ç°é€¼çœŸæ•ˆæœï¼Œä½†éœ€é¢å¤–ç¼–ç æ¨¡å—å’Œå¤æ‚é¢„å¤„ç†ã€‚</li>
<li>CatVTONæ¨¡å‹æå‡ºç®€åŒ–è™šæ‹Ÿè¯•ç©¿è¿‡ç¨‹çš„æ–¹æ³•ï¼Œé€šè¿‡æ²¿ç©ºé—´ç»´åº¦æ‹¼æ¥æœè£…åˆ°ç›®æ ‡ä¸ªä½“ä¸Šå®ç°é«˜æ•ˆè¯•ç©¿ã€‚</li>
<li>CatVTONå…·æœ‰è½»é‡çº§ç½‘ç»œè®¾è®¡ï¼Œé€šè¿‡å»é™¤å†—ä½™æ¨¡å—é™ä½ç½‘ç»œå¤æ‚æ€§ã€‚</li>
<li>å‚æ•°é«˜æ•ˆè®­ç»ƒæ˜¯å…³é”®ï¼Œè‡ªæ³¨æ„åŠ›æ¨¡å—å¯¹é€‚åº”è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>CatVTONç®€åŒ–äº†ä¸å¿…è¦çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå‡å°‘äº†å†…å­˜ä½¿ç”¨å¹¶æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>CatVTONåœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-631491cbdc42de66d9784e404fb4d928.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e23cd3c73d5a060e09e0eb78a52dfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1ba8644f780f38490009901811993d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-350eda8a77364ca259a0315957f88c57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e786bd51ebe64d5f4c6944d4030ee58.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Looking-Backward-Streaming-Video-to-Video-Translation-with-Feature-Banks"><a href="#Looking-Backward-Streaming-Video-to-Video-Translation-with-Feature-Banks" class="headerlink" title="Looking Backward: Streaming Video-to-Video Translation with Feature   Banks"></a>Looking Backward: Streaming Video-to-Video Translation with Feature   Banks</h2><p><strong>Authors:Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, Diana Marculescu</strong></p>
<p>This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames. At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. This is realized by maintaining a feature bank, which archives information from past frames. For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output. The feature bank is continually updated by merging stored and new features, making it compact but informative. StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative metrics and user studies confirm StreamV2Vâ€™s exceptional ability to maintain temporal consistency. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†StreamV2Vï¼Œè¿™æ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå€ŸåŠ©ç”¨æˆ·æç¤ºå®ç°äº†å®æ—¶æµå¼è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰çš„ç¿»è¯‘ã€‚ä¸åŒäºä¹‹å‰ä½¿ç”¨æ‰¹æ¬¡å¤„ç†æœ‰é™å¸§çš„V2Væ–¹æ³•ï¼Œæˆ‘ä»¬é€‰æ‹©ä»¥æµå¼æ–¹å¼å¤„ç†å¸§ï¼Œä»¥æ”¯æŒæ— é™å¸§ã€‚StreamV2Vçš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªåè§†åŸåˆ™ï¼Œå°†ç°åœ¨ä¸è¿‡å»è”ç³»èµ·æ¥ã€‚è¿™æ˜¯é€šè¿‡ç»´æŠ¤ä¸€ä¸ªç‰¹å¾é“¶è¡Œæ¥å®ç°çš„ï¼Œè¯¥é“¶è¡Œå­˜æ¡£è¿‡å»å¸§çš„ä¿¡æ¯ã€‚å¯¹äºä¼ å…¥çš„å¸§ï¼ŒStreamV2Vå°†è‡ªæ³¨æ„åŠ›æ‰©å±•åˆ°åŒ…æ‹¬é“¶è¡Œå¯†é’¥å’Œå€¼ï¼Œå¹¶ç›´æ¥å°†ç›¸ä¼¼çš„è¿‡å»ç‰¹å¾èåˆåˆ°è¾“å‡ºä¸­ã€‚ç‰¹å¾é“¶è¡Œé€šè¿‡åˆå¹¶å­˜å‚¨çš„æ–°ç‰¹å¾æ¥ä¸æ–­æ›´æ–°ï¼Œä½¿å…¶æ—¢ç´§å‡‘åˆå¯Œæœ‰ä¿¡æ¯ã€‚StreamV2Vä»¥å…¶é€‚åº”æ€§å’Œæ•ˆç‡è€Œè„±é¢–è€Œå‡ºï¼Œæ— éœ€å¾®è°ƒå³å¯æ— ç¼é›†æˆåˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ã€‚å®ƒå¯ä»¥åœ¨ä¸€ä¸ªA100 GPUä¸Šä»¥æ¯ç§’20å¸§çš„é€Ÿåº¦è¿è¡Œï¼Œæ¯”FlowVidã€CoDeFã€Rerenderå’ŒTokenFlowåˆ†åˆ«å¿«15å€ã€46å€ã€108å€å’Œ158å€ã€‚å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶è¯å®äº†StreamV2Våœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„å‡ºè‰²èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15757v3">PDF</a> ICLR 2025. Project page:   <a target="_blank" rel="noopener" href="https://jeff-liangf.github.io/projects/streamv2v">https://jeff-liangf.github.io/projects/streamv2v</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†StreamV2Vï¼Œä¸€ç§æ”¯æŒç”¨æˆ·æç¤ºçš„å®æ—¶æµå¼è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰ç¿»è¯‘æ¨¡å‹ã€‚å®ƒé€šè¿‡é‡‡ç”¨æµå¼å¤„ç†æ¡†æ¶ï¼Œå®ç°äº†å¯¹æ— é™å¸§çš„æ”¯æŒï¼Œå¹¶é‡‡ç”¨äº†åè§†åŸåˆ™ï¼Œå°†å½“å‰å¸§ä¸è¿‡å»å¸§ç›¸å…³è”ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯ç‰¹å¾é“¶è¡Œï¼Œå¯ä»¥å­˜å‚¨è¿‡å»å¸§çš„ä¿¡æ¯ï¼Œå¹¶ç”¨äºå½“å‰å¸§çš„è‡ªæ³¨æ„åŠ›è®¡ç®—å’Œç‰¹å¾èåˆã€‚StreamV2Vå…·æœ‰é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œå¯æ— ç¼é›†æˆå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒã€‚å®ƒåœ¨æ€§èƒ½ä¸Šè¿œè¶…å…¶ä»–æ–¹æ³•ï¼Œå¯åœ¨å•ä¸ªA100 GPUä¸Šä»¥æ¯ç§’è¿è¡Œè¾¾åˆ°é«˜è¾¾äºŒåå¸§çš„é€Ÿåº¦è¿è¡Œã€‚æ­¤å¤–ï¼Œé€šè¿‡å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶è¯å®å…¶ä¿æŒè‰¯å¥½çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamV2Væ˜¯ä¸€ä¸ªå®æ—¶æµå¼è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘æ¨¡å‹ã€‚</li>
<li>å®ƒä½¿ç”¨æµå¼å¤„ç†æ¡†æ¶ä»¥æ”¯æŒæ— é™å¸§å¤„ç†ã€‚</li>
<li>StreamV2Vé‡‡ç”¨åè§†åŸåˆ™ï¼Œå°†å½“å‰å¸§ä¸è¿‡å»å¸§ç›¸å…³è”ã€‚</li>
<li>ç‰¹å¾é“¶è¡Œæ˜¯æ¨¡å‹çš„æ ¸å¿ƒæœºåˆ¶ï¼Œç”¨äºå­˜å‚¨å’Œæ›´æ–°è¿‡å»å¸§çš„ä¿¡æ¯ã€‚</li>
<li>StreamV2Vå…·æœ‰é«˜æ•ˆæ€§èƒ½ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªA100 GPUä¸Šä»¥æ¯ç§’è¿è¡Œé«˜è¾¾äºŒåå¸§çš„é€Ÿåº¦è¿è¡Œã€‚</li>
<li>å®ƒå¯æ— ç¼é›†æˆå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74172f33a91d57da9ba00b4784af4ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb70aff9cfe7ece29358e3a822bbcee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f7bdc98dad3a30228cd9b9d53523721.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94c69663e1654d7b35818cb622d34b22.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Learning-to-Discretize-Denoising-Diffusion-ODEs"><a href="#Learning-to-Discretize-Denoising-Diffusion-ODEs" class="headerlink" title="Learning to Discretize Denoising Diffusion ODEs"></a>Learning to Discretize Denoising Diffusion ODEs</h2><p><strong>Authors:Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert</strong></p>
<p>Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/vinhsuhi/LD3">https://github.com/vinhsuhi/LD3</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒåˆæˆå’Œ3Dç‚¹äº‘ç”Ÿæˆç­‰å„ä¸ªé¢†åŸŸéƒ½æœ‰ç€ä¸ä¿—çš„è¡¨ç°ã€‚ä»é¢„è®­ç»ƒçš„DPMsä¸­è¿›è¡Œé‡‡æ ·ï¼Œéœ€è¦é€šè¿‡å¤šæ¬¡ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰å°†é«˜æ–¯å™ªå£°æ ·æœ¬è½¬åŒ–ä¸ºå›¾åƒï¼Œç›¸è¾ƒäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ç­‰å•æ­¥ç”Ÿæˆæ¨¡å‹ï¼Œå…¶è®¡ç®—æˆæœ¬æ›´é«˜ã€‚å› æ­¤ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å‡å°‘NFEsçš„æ•°é‡è‡³å…³é‡è¦ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LD3ï¼Œä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ é‡‡æ ·çš„æœ€ä¼˜æ—¶é—´ç¦»æ•£åŒ–ã€‚LD3å¯ä»¥ä¸å„ç§é‡‡æ ·å™¨ç›¸ç»“åˆï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒèµ„æºå¯†é›†å‹çš„ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ï¼ŒæŒç»­æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬åˆ†æå’Œå®è¯åœ°è¯æ˜ï¼ŒLD3èƒ½æé«˜é‡‡æ ·æ•ˆç‡ï¼Œä¸”è®¡ç®—å¼€é”€è¾ƒå°ã€‚æˆ‘ä»¬åœ¨7ä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¶µç›–äº†åƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´DPMsçš„æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„é‡‡æ ·ã€‚æˆ‘ä»¬åœ¨æ— æ¡ä»¶CIFAR10å’ŒAFHQv2ä¸Šå®ç°äº†FIDä¸º2.38ï¼ˆ10æ¬¡NFEï¼‰å’Œ2.27ï¼ˆ10æ¬¡NFEï¼‰ï¼Œè®­ç»ƒæ—¶é—´åªéœ€5-10åˆ†é’Ÿã€‚LD3ä¸ºä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vinhsuhi/LD3%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vinhsuhi/LD3æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15506v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    DPMï¼ˆæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼‰æ˜¯ä¸€ç§è¡¨ç°å¼ºåŠ²çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆæˆå’Œä¸‰ç»´ç‚¹äº‘ç”Ÿæˆç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºå…¶é‡‡æ ·è¿‡ç¨‹æ¶‰åŠå¤šæ¬¡ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºLD3è¿™ä¸€è½»é‡çº§æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ æœ€ä½³é‡‡æ ·æ—¶é—´ç¦»æ•£åŒ–ç­–ç•¥ã€‚LD3èƒ½ä¸å¤šç§é‡‡æ ·å™¨ç»“åˆä½¿ç”¨ï¼Œåœ¨ä¸é‡æ–°è®­ç»ƒèµ„æºå¯†é›†å‹ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹æé«˜ç”Ÿæˆè´¨é‡ã€‚ç»åˆ†æä¸å®è¯éªŒè¯ï¼ŒLD3èƒ½æé«˜é‡‡æ ·æ•ˆç‡ä¸”è®¡ç®—å¼€é”€è¾ƒå°ã€‚åœ¨7ä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— æ¡ä»¶CIFAR10å’ŒAFHQv2ä¸Šçš„FIDè¾¾åˆ°2.38ï¼ˆ10æ¬¡NFEï¼‰å’Œ2.27ï¼ˆ10æ¬¡NFEï¼‰ï¼Œè®­ç»ƒæ—¶é—´ä»…éœ€5-10åˆ†é’Ÿã€‚LD3ä¸ºä»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·æä¾›äº†ä¸€ç§é«˜æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DPMsåœ¨å›¾åƒåˆæˆå’Œä¸‰ç»´ç‚¹äº‘ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>é‡‡æ ·ä»é¢„è®­ç»ƒçš„DPMsæ¶‰åŠå¤šæ¬¡NFEï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>LD3æ¡†æ¶æ—¨åœ¨å­¦ä¹ æœ€ä½³é‡‡æ ·æ—¶é—´ç¦»æ•£åŒ–ç­–ç•¥ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>LD3å¯ä¸å„ç§é‡‡æ ·å™¨ç»“åˆä½¿ç”¨ï¼Œæ— éœ€é‡æ–°è®­ç»ƒèµ„æºå¯†é›†å‹ç¥ç»ç½‘ç»œã€‚</li>
<li>LD3é€šè¿‡å‡å°‘NFEæ¬¡æ•°æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>åœ¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLD3åœ¨FIDæŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>LD3æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§ï¼Œè®­ç»ƒæ—¶é—´çŸ­ï¼Œä»£ç å·²å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d59632664ec92e2073b2acd11ee4de8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbb76384d5a545835a9c63f2c34d228a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-637f5670d77fbad7e66095ace0255e09.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Zero-Reference-Lighting-Estimation-Diffusion-Model-for-Low-Light-Image-Enhancement"><a href="#Zero-Reference-Lighting-Estimation-Diffusion-Model-for-Low-Light-Image-Enhancement" class="headerlink" title="Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image   Enhancement"></a>Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image   Enhancement</h2><p><strong>Authors:Jinhong He, Minglong Xue, Aoxiang Ning, Chengyun Song</strong></p>
<p>Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„ä½å…‰å›¾åƒå¢å¼ºæ–¹æ³•ä¸¥é‡ä¾èµ–äºé…å¯¹è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´å…¶å¹¿æ³›åº”ç”¨å—é™ã€‚åŒæ—¶ï¼Œç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ç¼ºä¹å¯¹æœªçŸ¥é™è§£çš„æœ‰æ•ˆæ¡¥æ¢ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºä½å…‰å›¾åƒå¢å¼ºçš„æ–°å‹æ— å‚è€ƒå…‰ç…§ä¼°è®¡æ‰©æ•£æ¨¡å‹ï¼Œåä¸ºZero-LEDã€‚å®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ”¶æ•›èƒ½åŠ›ï¼Œå¼¥åˆäº†ä½å…‰é¢†åŸŸå’ŒçœŸå®æ­£å¸¸å…‰é¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œå¹¶é€šè¿‡æ— å‚è€ƒå­¦ä¹ æˆåŠŸå‡è½»äº†å¯¹é…å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡åˆå§‹ä¼˜åŒ–ç½‘ç»œå¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶é€šè¿‡å¤šä¸ªç›®æ ‡å‡½æ•°åœ¨æ‰©æ•£æ¨¡å‹å’Œåˆå§‹ä¼˜åŒ–ç½‘ç»œä¹‹é—´å®ç°åŒå‘çº¦æŸã€‚éšåï¼Œä¼˜åŒ–ç°å®ä¸–ç•Œåœºæ™¯çš„é™è§£å› ç´ ï¼Œä»¥è¿­ä»£çš„æ–¹å¼å®ç°æœ‰æ•ˆçš„å…‰ç…§å¢å¼ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§åŸºäºé¢‘åŸŸå’Œè¯­ä¹‰å¼•å¯¼çš„å¤–è§‚é‡å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—é¼“åŠ±æ¢å¤å›¾åƒçš„ç‰¹åœ¨ç»†ç²’åº¦å±‚æ¬¡å¯¹é½ï¼Œå¹¶æ»¡è¶³ä¸»è§‚é¢„æœŸã€‚æœ€åï¼Œå¤§é‡å®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”å…·æœ‰æ›´æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€æºä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02879v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— å‚è€ƒä½å…‰å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œåä¸ºZero-LEDã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ”¶æ•›èƒ½åŠ›ï¼Œæ¡¥æ¥ä½å…‰åŸŸå’Œæ­£å¸¸å…‰åŸŸï¼Œé€šè¿‡æ— å‚è€ƒå­¦ä¹ å‡è½»å¯¹é…å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡è®¾è®¡åˆå§‹ä¼˜åŒ–ç½‘ç»œå¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹å’Œåˆå§‹ä¼˜åŒ–ç½‘ç»œä¹‹é—´é€šè¿‡å¤šä¸ªç›®æ ‡å‡½æ•°å®ç°åŒå‘çº¦æŸã€‚è¿­ä»£ä¼˜åŒ–çœŸå®åœºæ™¯ä¸­çš„é€€åŒ–å› ç´ ï¼Œå®ç°æœ‰æ•ˆçš„å…‰ç…§å¢å¼ºã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†åŸºäºé¢‘åŸŸå’Œè¯­ä¹‰å¼•å¯¼çš„å¤–è§‚é‡å»ºæ¨¡å—ï¼Œä½¿æ¢å¤çš„ç‰¹å¾åœ¨ç²¾ç»†å±‚é¢ä¸Šå¯¹é½ï¼Œæ»¡è¶³ä¸»è§‚æœŸæœ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ä½å…‰å›¾åƒå¢å¼ºä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†ä»å­˜åœ¨å¯¹é…å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ— ç›‘ç£æ–¹æ³•åœ¨æœªçŸ¥é€€åŒ–æ–¹é¢çš„æ¡¥æ¢ä½œç”¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†åä¸ºZero-LEDçš„æ–°æ‰©æ•£æ¨¡å‹ï¼Œä»¥å‡è½»å¯¹é…å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œå¹¶é€šè¿‡ç¨³å®šæ”¶æ•›èƒ½åŠ›æ¡¥æ¥ä¸åŒå…‰ç…§åŸŸã€‚</li>
<li>è®¾è®¡äº†åˆå§‹ä¼˜åŒ–ç½‘ç»œè¿›è¡Œå›¾åƒé¢„å¤„ç†ï¼Œå¹¶é€šè¿‡å¤šä¸ªç›®æ ‡å‡½æ•°å®ç°æ‰©æ•£æ¨¡å‹å’Œåˆå§‹ä¼˜åŒ–ç½‘ç»œä¹‹é—´çš„åŒå‘çº¦æŸã€‚</li>
<li>é€šè¿‡è¿­ä»£ä¼˜åŒ–çœŸå®åœºæ™¯ä¸­çš„é€€åŒ–å› ç´ ï¼Œå®ç°äº†æœ‰æ•ˆçš„å…‰ç…§å¢å¼ºã€‚</li>
<li>å¼•å…¥äº†åŸºäºé¢‘åŸŸå’Œè¯­ä¹‰å¼•å¯¼çš„å¤–è§‚é‡å»ºæ¨¡å—ï¼Œæé«˜äº†æ¢å¤å›¾åƒçš„ç‰¹å¾å¯¹é½ç²¾åº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.02879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7a3f544f7d3b07617dc5672b10c6ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c37e788561843e460248ae48d3ad40a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9b5416df99f3c9bf78a001a3966ca21.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Diffusion-EXR-Controllable-Review-Generation-for-Explainable-Recommendation-via-Diffusion-Models"><a href="#Diffusion-EXR-Controllable-Review-Generation-for-Explainable-Recommendation-via-Diffusion-Models" class="headerlink" title="Diffusion-EXR: Controllable Review Generation for Explainable   Recommendation via Diffusion Models"></a>Diffusion-EXR: Controllable Review Generation for Explainable   Recommendation via Diffusion Models</h2><p><strong>Authors:Ling Li, Shaohua Li, Winda Marantika, Alex C. Kot, Huijing Zhan</strong></p>
<p>Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in image and audio generation tasks. However, there exist few attempts to employ DDPM in the text generation, especially review generation under recommendation systems. Fueled by the predicted reviews explainability that justifies recommendations could assist users better understand the recommended items and increase the transparency of recommendation system, we propose a Diffusion Model-based Review Generation towards EXplainable Recommendation named Diffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by incrementally introducing varied levels of Gaussian noise to the sequence of word embeddings and learns to reconstruct the original word representations in the reverse process. The nature of DDPM enables our lightweight Transformer backbone to perform excellently in the recommendation review generation task. Extensive experimental results have demonstrated that Diffusion-EXR can achieve state-of-the-art review generation for recommendation on two publicly available benchmark datasets. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰åœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„ç«äº‰åŠ›ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰å°è¯•å°†DDPMåº”ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œå°¤å…¶æ˜¯åœ¨æ¨èç³»ç»Ÿçš„è¯„è®ºç”Ÿæˆæ–¹é¢ã€‚æˆ‘ä»¬ä»¥å¯è§£é‡Šçš„æ¨èé¢„æµ‹ä¸ºåŸºç¡€ï¼Œæ¨åŠ¨ç”¨æˆ·æ›´å¥½åœ°ç†è§£æ¨èé¡¹ç›®å¹¶å¢åŠ æ¨èç³»ç»Ÿçš„é€æ˜åº¦ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢å‘å¯è§£é‡Šæ¨èçš„è¯„è®ºç”Ÿæˆæ–¹æ³•ï¼Œåä¸ºDiffusion-EXRã€‚Diffusion-EXRé€šè¿‡é€æ­¥å‘è¯„è®ºåµŒå…¥åºåˆ—å¼•å…¥ä¸åŒçº§åˆ«çš„é«˜æ–¯å™ªå£°æ¥ç ´åè¯„è®ºåµŒå…¥åºåˆ—ï¼Œå¹¶åœ¨åå‘è¿‡ç¨‹ä¸­å­¦ä¹ é‡å»ºåŸå§‹å•è¯è¡¨ç¤ºã€‚DDPMçš„ç‰¹æ€§ä½¿æˆ‘ä»¬çš„è½»é‡çº§Transformeréª¨å¹²åœ¨æ¨èè¯„è®ºç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusion-EXRåœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è¯„è®ºç”Ÿæˆæ¨èæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.15490v4">PDF</a> We request to withdraw our paper from the archive due to significant   errors identified in the analysis and conclusions. Upon further review, we   realized that these errors undermine the validity of our findings. We plan to   conduct additional research to correct these issues and resubmit a revised   version in the future</p>
<p><strong>Summary</strong><br>     åŸºäºDDPMçš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ç”¨äºæ–‡æœ¬ç”Ÿæˆã€ç‰¹åˆ«æ˜¯æ¨èç³»ç»Ÿä¸‹çš„è¯„è®ºç”Ÿæˆçš„å°è¯•ä»è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiffusion-EXRçš„æ‰©æ•£æ¨¡å‹è¯„è®ºç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é¢„æµ‹çš„è¯„è®ºè§£é‡Šæ€§æ¥è§£é‡Šæ¨èå¹¶å¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£æ¨èå•†å“ï¼Œå¢åŠ æ¨èç³»ç»Ÿçš„é€æ˜åº¦ã€‚å®ƒé€šè¿‡é€æ­¥åœ¨åºåˆ—çš„è¯åµŒå…¥ä¸­åŠ å…¥ä¸åŒç¨‹åº¦çš„é«˜æ–¯å™ªå£°æ¥ç ´åè¯„è®ºåºåˆ—ï¼Œå¹¶åœ¨é€†å‘è¿‡ç¨‹ä¸­å­¦ä¹ é‡å»ºåŸå§‹è¯è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusion-EXRåœ¨ä¸¤ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„æ¨èè¯„è®ºç”Ÿæˆä»»åŠ¡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDPMåœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆä¸­æ€§èƒ½ä¼˜è¶Šï¼Œä½†ç”¨äºæ–‡æœ¬ç”Ÿæˆç‰¹åˆ«æ˜¯åœ¨æ¨èç³»ç»Ÿçš„è¯„è®ºç”Ÿæˆæ–¹é¢åº”ç”¨è¾ƒå°‘ã€‚</li>
<li>Diffusion-EXRæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¯„è®ºç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨èç³»ç»Ÿçš„é€æ˜åº¦ã€‚</li>
<li>Diffusion-EXRé€šè¿‡é€æ­¥åŠ å…¥é«˜æ–¯å™ªå£°ç ´åè¯„è®ºåºåˆ—ï¼Œå¹¶åœ¨é€†å‘è¿‡ç¨‹ä¸­é‡å»ºåŸå§‹è¯è¡¨ç¤ºã€‚</li>
<li>Diffusion-EXRåˆ©ç”¨è½»é‡çº§Transformerä½œä¸ºéª¨å¹²ç½‘ï¼Œä½¿å…¶åœ¨æ¨èè¯„è®ºç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>Diffusion-EXRåœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è¯„è®ºç”Ÿæˆæ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹çš„è¯„è®ºè§£é‡Šæ€§æ¥è§£é‡Šæ¨èï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£æ¨èå•†å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.15490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3824497521dc0d80028fdc6db97c82ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68cad8b70b5ae49628cbb50a60cafd9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58d7d2b737a18235e195879eac4e998d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b192aa42121f8d5a60b9ed822647bca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c04b371e31f9912c0932d386f051f2e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-618c4ef4a4d647ceb3647625c385adcd.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-04e49eae87daaae7c03cbe2a05d4be89.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Robotic CBCT Meets Robotic Ultrasound
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-271393c492104679e791e3d684d05f93.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  HumanGif Single-View Human Diffusion with Generative Prior
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
