<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Idiosyncrasies in Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-83954c98f1edebc8d74ecef1c4eefbee.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="Idiosyncrasies-in-Large-Language-Models"><a href="#Idiosyncrasies-in-Large-Language-Models" class="headerlink" title="Idiosyncrasies in Large Language Models"></a>Idiosyncrasies in Large Language Models</h2><p><strong>Authors:Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu</strong></p>
<p>In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) â€“ unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each modelâ€™s idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at <a target="_blank" rel="noopener" href="https://github.com/locuslab/llm-idiosyncrasies">https://github.com/locuslab/llm-idiosyncrasies</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºå¹¶ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‹¬ç‰¹æ€§â€”â€”è¾“å‡ºä¸­çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ç”¨äºåŒºåˆ†æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªç®€å•çš„åˆ†ç±»ä»»åŠ¡ï¼šç»™å®šç‰¹å®šçš„æ–‡æœ¬è¾“å‡ºï¼Œç›®æ ‡æ˜¯é¢„æµ‹ç”Ÿæˆè¯¥æ–‡æœ¬çš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒç»„LLMçš„è¿™ä¸ªåˆæˆä»»åŠ¡ï¼Œå‘ç°ä»…é€šè¿‡å¾®è°ƒç°æœ‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹å³å¯åœ¨LLMç”Ÿæˆçš„æ–‡æœ¬ä¸Šè·å¾—å‡ºè‰²çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ¶‰åŠChatGPTã€Claudeã€Grokã€Geminiå’ŒDeepSeekçš„äº”åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¿ç•™çš„éªŒè¯æ•°æ®ä¸Šè¾¾åˆ°äº†97.1%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥è°ƒæŸ¥è¡¨æ˜ï¼Œè¿™äº›ç‹¬ç‰¹æ€§æ ¹æ¤äºå•è¯çº§åˆ«çš„åˆ†å¸ƒã€‚è¿™äº›æ¨¡å¼åœ¨æ–‡æœ¬è¢«é‡å†™ã€ç¿»è¯‘æˆ–æ‘˜è¦å¤–éƒ¨LLMæ—¶ä»ç„¶å­˜åœ¨ï¼Œè¿™è¡¨æ˜å®ƒä»¬ä¹Ÿç¼–ç åœ¨è¯­ä¹‰å†…å®¹ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨LLMä½œä¸ºæ³•å®˜æ¥ç”Ÿæˆå…³äºæ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹æ€§çš„è¯¦ç»†ã€å¼€æ”¾å¼æè¿°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬çš„å‘ç°å¯¹è®­ç»ƒåˆæˆæ•°æ®å’Œæ¨æ–­æ¨¡å‹ç›¸ä¼¼æ€§çš„æ›´å¹¿æ³›å½±å“ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/locuslab/llm-idiosyncrasies">https://github.com/locuslab/llm-idiosyncrasies</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12150v1">PDF</a> Website at   <a target="_blank" rel="noopener" href="https://eric-mingjie.github.io/llm-idiosyncrasies/index.html">https://eric-mingjie.github.io/llm-idiosyncrasies/index.html</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ­ç¤ºå¹¶ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰¹æ€§â€”â€”æ¨¡å‹è¾“å‡ºä¸­çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ç”¨äºåŒºåˆ†ä¸åŒçš„æ¨¡å‹ã€‚æ–‡ç« é€šè¿‡ä¸€ä¸ªç®€å•çš„åˆ†ç±»ä»»åŠ¡æ¥å±•ç¤ºè¿™äº›ç‰¹æ€§ï¼Œå³æ ¹æ®æ–‡æœ¬è¾“å‡ºé¢„æµ‹ç”Ÿæˆæ–‡æœ¬çš„LLMæ¥æºã€‚é€šè¿‡åœ¨ä¸åŒLLMç»„ä¸Šè¯„ä¼°è¿™ä¸€åˆæˆä»»åŠ¡å‘ç°ï¼Œä»…å¯¹ç°æœ‰æ–‡æœ¬åµŒå…¥æ¨¡å‹è¿›è¡Œå¾®è°ƒå³å¯è·å¾—å‡ºè‰²çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚åœ¨æ¶‰åŠChatGPTã€Claudeã€Grokã€Geminiå’ŒDeepSeekçš„äº”åˆ†ç±»é—®é¢˜ä¸Šï¼Œæˆ‘ä»¬åœ¨éªŒè¯æ•°æ®ä¸Šè¾¾åˆ°äº†97.1%çš„å‡†ç¡®ç‡ã€‚è¿›ä¸€æ­¥çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›ç‰¹æ€§æ ¹æ¤äºå•è¯çº§åˆ«çš„åˆ†å¸ƒæ¨¡å¼ï¼Œå¹¶ä¸”å³ä½¿æ–‡æœ¬è¢«é‡å†™ã€ç¿»è¯‘æˆ–æ‘˜è¦ï¼Œè¿™äº›æ¨¡å¼ä¾ç„¶å­˜åœ¨ï¼Œè¿™è¡¨æ˜å®ƒä»¬ä¹Ÿå­˜åœ¨äºè¯­ä¹‰å†…å®¹ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨LLMä½œä¸ºæ³•å®˜æ¥ç”Ÿæˆå…³äºæ¯ä¸ªæ¨¡å‹ç‰¹æ€§çš„è¯¦ç»†å¼€æ”¾å¼æè¿°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬çš„å‘ç°å¯¹åˆæˆæ•°æ®è®­ç»ƒå’Œæ¨æ–­æ¨¡å‹ç›¸ä¼¼æ€§çš„æ›´å¹¿æ³›å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å¯ç”¨äºåŒºåˆ†ä¸åŒçš„LLMã€‚</li>
<li>é€šè¿‡ç®€å•çš„åˆ†ç±»ä»»åŠ¡è¯„ä¼°äº†LLMçš„ç‰¹æ€§ï¼Œå³é¢„æµ‹æ–‡æœ¬è¾“å‡ºå¯¹åº”çš„LLMæ¥æºã€‚</li>
<li>é€šè¿‡å¯¹æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„å¾®è°ƒï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡çš„åˆ†ç±»ã€‚</li>
<li>åœ¨æ¶‰åŠå¤šä¸ªLLMçš„äº”åˆ†ç±»é—®é¢˜ä¸Šï¼ŒéªŒè¯æ•°æ®å‡†ç¡®ç‡è¾¾åˆ°äº†97.1%ã€‚</li>
<li>LLMçš„ç‰¹æ€§æºäºå•è¯çº§åˆ«çš„åˆ†å¸ƒæ¨¡å¼ï¼Œä¸”è¿™äº›æ¨¡å¼åœ¨æ–‡æœ¬é‡å†™ã€ç¿»è¯‘æˆ–æ‘˜è¦ä¸­ä¾ç„¶å­˜åœ¨ã€‚</li>
<li>è¿™äº›ç‰¹æ€§ä¸ä»…å­˜åœ¨äºLLMçš„è¾“å‡ºä¸­ï¼Œä¹Ÿä¸å…¶è¯­ä¹‰å†…å®¹æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27841a3faac7a31b8a8f71eac44e72ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8a537ff0d0bf273efdc7e40171e8292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbd48ff3e657fe4f152e954bdfefd12f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45cc64a7f379856988fc573b83a1508f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad93ed6cb7e0d44a44448cde3cd03952.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdad4af27af54fc6ab049cd2018a976.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ee27949d696b74e2bdf518169392cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a170ee67084fafab0858fb5070d63c12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798926cce0edafbb966caf28870f88ec.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HermesFlow-Seamlessly-Closing-the-Gap-in-Multimodal-Understanding-and-Generation"><a href="#HermesFlow-Seamlessly-Closing-the-Gap-in-Multimodal-Understanding-and-Generation" class="headerlink" title="HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and   Generation"></a>HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and   Generation</h2><p><strong>Authors:Ling Yang, Xinchen Zhang, Ye Tian, Chenming Shang, Minghao Xu, Wentao Zhang, Bin Cui</strong></p>
<p>The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/HermesFlow">https://github.com/Gen-Verse/HermesFlow</a> </p>
<blockquote>
<p>è‡ªåŠ¨å›å½’èŒƒå¼çš„å·¨å¤§æˆåŠŸä½¿å¾—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åƒShow-oã€Transfusionå’ŒEmu3ç­‰å¼ºå¤§æ¨¡å‹åœ¨ç»Ÿä¸€å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æˆ‘ä»¬é¦–æ¬¡å‘ç°äº†ä¸€ä¸ªæ™®éç°è±¡ï¼šMLLMsçš„ç†è§£èƒ½åŠ›é€šå¸¸å¼ºäºå…¶ç”Ÿæˆèƒ½åŠ›ï¼Œä¸¤è€…ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†HermesFlowï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œé€šç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ— ç¼æ¡¥æ¥MLLMsä¸­ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»¥åŒæºæ•°æ®ä½œä¸ºè¾“å…¥ï¼Œæ•´ç†å‡ºç†è§£å’Œç”Ÿæˆæ–¹é¢çš„åŒæºåå¥½æ•°æ®ã€‚é€šè¿‡Pair-DPOå’Œè‡ªæˆ‘åšå¼ˆè¿­ä»£ä¼˜åŒ–ï¼ŒHermesFlowæœ‰æ•ˆåœ°ä½¿ç”¨åŒæºåå¥½æ•°æ®å¯¹é½å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚å¤§é‡å®éªŒè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•è¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—çš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼©å°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„å·®è·æ–¹é¢ã€‚è¿™äº›å‘ç°çªæ˜¾äº†HermesFlowä½œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ä¸€èˆ¬å¯¹é½æ¡†æ¶çš„æ½œåŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/HermesFlow">https://github.com/Gen-Verse/HermesFlow</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12148v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/HermesFlow">https://github.com/Gen-Verse/HermesFlow</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è‡ªå›å½’èŒƒå¼å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼ŒShow-oã€Transfusionå’ŒEmu3ç­‰å¼ºå¤§æ¨¡å‹åœ¨ç»Ÿä¸€å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç ”ç©¶å‘ç°MLLMsçš„ç†è§£èƒ½åŠ›é€šå¸¸å¼ºäºç”Ÿæˆèƒ½åŠ›ï¼Œä¸¤è€…ä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®è·ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†HermesFlowæ¡†æ¶ï¼Œæ—¨åœ¨æ— ç¼æ¡¥æ¥MLLMsä¸­çš„ç†è§£ä¸ç”Ÿæˆå·®è·ã€‚é€šè¿‡åŒæºæ•°æ®è¾“å…¥ã€åŒæºåå¥½æ•°æ®æ•´åˆä»¥åŠPair-DPOå’Œè‡ªæˆ‘åšå¼ˆè¿­ä»£ä¼˜åŒ–ï¼ŒHermesFlowæœ‰æ•ˆå¯¹é½å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼©å°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆå·®è·æ–¹é¢ã€‚è¿™å‡¸æ˜¾äº†HermesFlowä½œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„é€šç”¨å¯¹é½æ¡†æ¶çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡ªå›å½’èŒƒå¼çš„æˆåŠŸæ¨åŠ¨äº†æ¨¡å‹å¦‚Show-oã€Transfusionå’ŒEmu3çš„è¿›å±•ã€‚</li>
<li>MLLMsçš„ç†è§£èƒ½åŠ›é€šå¸¸å¼ºäºç”Ÿæˆèƒ½åŠ›ï¼Œå­˜åœ¨èƒ½åŠ›å·®è·ã€‚</li>
<li>HermesFlowæ¡†æ¶æ—¨åœ¨æ¡¥æ¥MLLMsä¸­çš„ç†è§£ä¸ç”Ÿæˆå·®è·ã€‚</li>
<li>HermesFlowåˆ©ç”¨åŒæºæ•°æ®è¾“å…¥å’ŒåŒæºåå¥½æ•°æ®æ•´åˆã€‚</li>
<li>é€šè¿‡Pair-DPOå’Œè‡ªæˆ‘åšå¼ˆè¿­ä»£ä¼˜åŒ–ï¼ŒHermesFlowæœ‰æ•ˆå¯¹é½å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚</li>
<li>å®éªŒè¡¨æ˜HermesFlowåœ¨ç¼©å°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆå·®è·æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7864ae33e0d06bc6fa5e5b12bf343c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eb14f9828444a44229d93de95df2fea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0904dce5b700484231cc2301bbc6392b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a1ae534ea9c635ac6fff223c5ec7c9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e53eccf439969bf9833aaf5f95a69f6f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SoftCoT-Soft-Chain-of-Thought-for-Efficient-Reasoning-with-LLMs"><a href="#SoftCoT-Soft-Chain-of-Thought-for-Efficient-Reasoning-with-LLMs" class="headerlink" title="SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs"></a>SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs</h2><p><strong>Authors:Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao</strong></p>
<p>Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the underlying LLM. Specifically, we employ a lightweight assistant model to generate instance-specific soft thought tokens speculatively as the initial chain of thoughts, which are then mapped into the LLMâ€™s representation space via a projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†é€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾§é‡äºç¡¬ä»¤ç‰Œè§£ç ï¼Œè¿™é™åˆ¶äº†æ¨ç†åœ¨ç¦»æ•£è¯æ±‡ç©ºé—´å†…ï¼Œå¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„ã€‚è™½ç„¶æœ€è¿‘çš„åŠªåŠ›æ¢ç´¢è¿ç»­ç©ºé—´æ¨ç†ï¼Œä½†å®ƒä»¬ç»å¸¸é­å—ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å·²ç»è¡¨ç°è‰¯å¥½çš„é›¶æ ·æœ¬è®¾ç½®çš„æœ€æ–°LLMä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸éœ€è¦ä¿®æ”¹åŸºç¡€LLMçš„è¿ç»­ç©ºé—´æ¨ç†æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è½»é‡çº§åŠ©ç†æ¨¡å‹æ¥ç”Ÿæˆå®ä¾‹ç‰¹å®šçš„è½¯æ€ç»´ä»¤ç‰Œä½œä¸ºåˆå§‹çš„æ€ç»´é“¾ï¼Œç„¶åé€šè¿‡æŠ•å½±æ¨¡å—å°†å…¶æ˜ å°„åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›‘ç£ã€å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œæé«˜äº†LLMçš„æ¨ç†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12134v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥å®ç°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šé›†ä¸­åœ¨ç¡¬ä»¤ç‰Œè§£ç ä¸Šï¼Œè¿™é™åˆ¶äº†æ¨ç†åœ¨ç¦»æ•£è¯æ±‡ç©ºé—´å†…çš„è¿›è¡Œï¼Œå¹¶ä¸æ€»æ˜¯æœ€ä¼˜ã€‚å°½ç®¡æœ€è¿‘æœ‰åŠªåŠ›æ¢ç´¢è¿ç»­ç©ºé—´æ¨ç†ï¼Œä½†å®ƒä»¬ç»å¸¸é­å—ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸éœ€è¦ä¿®æ”¹åŸºç¡€LLMçš„è¿ç»­ç©ºé—´æ¨ç†æ–°æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é‡‡ç”¨è½»é‡çº§åŠ©ç†æ¨¡å‹æ¥ç”Ÿæˆå®ä¾‹ç‰¹å®šçš„è½¯æ€ç»´ä»¤ç‰Œä½œä¸ºåˆæ­¥æ€ç»´é“¾ï¼Œç„¶åé€šè¿‡æŠ•å½±æ¨¡å—æ˜ å°„åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›‘ç£ã€å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæé«˜äº†LLMçš„æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoTæ¨ç†ä½¿LLMèƒ½å¤Ÿè§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç¡¬ä»¤ç‰Œè§£ç ä¸Šï¼Œè¿™å¯èƒ½é™åˆ¶æ¨ç†çš„ä¼˜åŒ–ã€‚</li>
<li>è¿ç»­ç©ºé—´æ¨ç†æ˜¯æœ€è¿‘çš„ç ”ç©¶æ–¹å‘ï¼Œä½†å­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¿ç»­ç©ºé—´æ¨ç†æ–¹æ³•ï¼Œä¸éœ€è¦ä¿®æ”¹åŸºç¡€LLMã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§åŠ©ç†æ¨¡å‹ç”Ÿæˆå®ä¾‹ç‰¹å®šçš„è½¯æ€ç»´ä»¤ç‰Œä½œä¸ºåˆæ­¥æ€ç»´é“¾ã€‚</li>
<li>é€šè¿‡æŠ•å½±æ¨¡å—å°†è½¯æ€ç»´ä»¤ç‰Œæ˜ å°„åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf1dca08f4ce5b29644670e96917f2e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c9d684e991250980b4b114e928e523.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scaling-Autonomous-Agents-via-Automatic-Reward-Modeling-And-Planning"><a href="#Scaling-Autonomous-Agents-via-Automatic-Reward-Modeling-And-Planning" class="headerlink" title="Scaling Autonomous Agents via Automatic Reward Modeling And Planning"></a>Scaling Autonomous Agents via Automatic Reward Modeling And Planning</h2><p><strong>Authors:Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agentsâ€™ limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agentsâ€™ decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMä»ç„¶åœ¨å¤„ç†éœ€è¦å¤šæ­¥éª¤å†³ç­–å’Œç¯å¢ƒåé¦ˆçš„é—®é¢˜ä¸Šé‡åˆ°æŒ‘æˆ˜ï¼Œå¦‚åœ¨åœ¨çº¿è´­ç‰©ã€ç§‘å­¦æ¨ç†å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰é¢†åŸŸã€‚ä¸çº¯æ–‡æœ¬æ•°æ®ä¸åŒï¼Œæ”¶é›†å¤§è§„æ¨¡å†³ç­–æ•°æ®æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè®¸å¤šå¼ºå¤§çš„LLMåªèƒ½é€šè¿‡APIè®¿é—®ï¼Œè¿™å¢åŠ äº†å¯¹ä»£ç†ä»»åŠ¡çš„å¾®è°ƒæˆæœ¬å¹¶å¸¦æ¥å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³LLMä»£ç†äººçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯ä»¥è‡ªåŠ¨ä»ç¯å¢ƒä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹çš„æ¡†æ¶ï¼Œæ— éœ€äººå·¥æ³¨é‡Šã€‚è¯¥æ¨¡å‹å¯ç”¨äºè¯„ä¼°LLMä»£ç†çš„è¡ŒåŠ¨è½¨è¿¹ï¼Œå¹¶ä¸ºä»»åŠ¡è§„åˆ’æä¾›å¯å‘å¼ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨ä¸€ä¸ªåŸºäºLLMçš„ä»£ç†åœ¨ç¯å¢ƒä¸­éšæœºå¯¼èˆªï¼Œç”Ÿæˆå„ç§è¡ŒåŠ¨è½¨è¿¹ã€‚ç„¶ååˆ©ç”¨å¦ä¸€ä¸ªLLMä¸ºæ¯æ¡è½¨è¿¹åˆ†é…ä»»åŠ¡æ„å›¾ï¼Œå¹¶åˆæˆä¸€ä¸ªä¸æ­£ç¡®å“åº”ç›¸å¯¹åº”çš„è´Ÿé¢å“åº”ã€‚è¿™äº›ä¸‰å…ƒç»„ï¼ˆä»»åŠ¡æ„å›¾ã€æ­£å‘å“åº”å’Œè´Ÿé¢å“åº”ï¼‰éšåè¢«ç”¨ä½œè®­ç»ƒæ•°æ®ï¼Œä»¥ä¼˜åŒ–èƒ½å¤Ÿè¯„ä¼°è¡ŒåŠ¨è½¨è¿¹çš„å¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸åŒä»£ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨å¢å¼ºLLMä»£ç†çš„å†³ç­–èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚é€šè¿‡è‡ªåŠ¨å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬å…‹æœäº†æ•°æ®ç¨€ç¼ºå’ŒAPIé™åˆ¶çš„æŒ‘æˆ˜ï¼Œæœ‰å¯èƒ½å½»åº•æ”¹å˜LLMåœ¨å¤æ‚å’Œäº¤äº’å¼ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚è¿™é¡¹ç ”ç©¶ä¸ºèƒ½å¤Ÿå¤„ç†éœ€è¦å¤šæ­¥éª¤å†³ç­–çš„å„ç§ç°å®ä¸–ç•Œé—®é¢˜çš„æ›´é«˜çº§AIä»£ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12130v1">PDF</a> ICLR2025, Project page: <a target="_blank" rel="noopener" href="https://armap-agent.github.io/">https://armap-agent.github.io</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥éª¤å†³ç­–ã€ç¯å¢ƒåé¦ˆçš„æƒ…å¢ƒï¼Œå¦‚åœ¨çº¿è´­ç‰©ã€ç§‘å­¦æ¨ç†å’Œæ•°å­¦é—®é¢˜è§£å†³ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹LLMä»£ç†çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨ä»ç¯å¢ƒä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹çš„æ¡†æ¶ï¼Œæ— éœ€äººå·¥æ³¨é‡Šå³å¯è¯„ä¼°LLMä»£ç†çš„è¡ŒåŠ¨è½¨è¿¹ï¼Œä¸ºä»»åŠ¡è§„åˆ’æä¾›å¯å‘å¼ä¿¡æ¯ã€‚è¯¥æ¡†æ¶é€šè¿‡LLMä»£ç†åœ¨ç¯å¢ƒä¸­éšæœºå¯¼èˆªç”Ÿæˆå¤šæ ·çš„è¡ŒåŠ¨è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨å¦ä¸€LLMä¸ºæ¯æ¡è½¨è¿¹åˆ†é…ä»»åŠ¡æ„å›¾å’Œåˆæˆè´Ÿé¢å“åº”åŠæ­£ç¡®å“åº”ï¼Œå½¢æˆè®­ç»ƒæ•°æ®æ¥ä¼˜åŒ–èƒ½å¤Ÿè¯„åˆ†è¡ŒåŠ¨è½¨è¿¹çš„å¥–åŠ±æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚æ€»ä¹‹ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†LLMä»£ç†çš„å†³ç­–èƒ½åŠ›ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å¥–åŠ±æ¨¡å‹çš„å­¦ä¹ å…‹æœäº†æ•°æ®ç¨€ç¼ºå’ŒAPIé™åˆ¶çš„æŒ‘æˆ˜ï¼Œæœ‰æœ›ä¸ºå¤æ‚ã€äº¤äº’å¼ç¯å¢ƒä¸­çš„LLMåº”ç”¨å¸¦æ¥é©å‘½æ€§å˜é©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥éª¤å†³ç­–å’Œç¯å¢ƒåé¦ˆçš„é—®é¢˜ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶èƒ½è‡ªåŠ¨ä»ç¯å¢ƒä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€äººå·¥æ³¨é‡Šå³å¯è¯„ä¼°LLMä»£ç†çš„è¡ŒåŠ¨è½¨è¿¹ã€‚</li>
<li>æ¡†æ¶é€šè¿‡LLMä»£ç†åœ¨ç¯å¢ƒä¸­éšæœºå¯¼èˆªç”Ÿæˆå¤šæ ·çš„è¡ŒåŠ¨è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨LLMåˆæˆè®­ç»ƒæ•°æ®æ¥ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶çš„æœ‰æ•ˆæ€§åœ¨å¤šä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ°è¯æ˜ã€‚</li>
<li>è¯¥æ¡†æ¶å…‹æœäº†æ•°æ®ç¨€ç¼ºå’ŒAPIé™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰æœ›ä¸ºå¤æ‚ã€äº¤äº’å¼ç¯å¢ƒä¸­çš„LLMåº”ç”¨å¸¦æ¥å˜é©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ecaa8bbacf0a474378f688366096a0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-187d99963871431099fb488d2e5eb90a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c00f9c8a8917ae580b851c7e98f2d6ee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SWE-Lancer-Can-Frontier-LLMs-Earn-1-Million-from-Real-World-Freelance-Software-Engineering"><a href="#SWE-Lancer-Can-Frontier-LLMs-Earn-1-Million-from-Real-World-Freelance-Software-Engineering" class="headerlink" title="SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance   Software Engineering?"></a>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance   Software Engineering?</h2><p><strong>Authors:Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke</strong></p>
<p>We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at $1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasksâ€“ranging from $50 bug fixes to $32,000 feature implementationsâ€“and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (<a target="_blank" rel="noopener" href="https://github.com/openai/SWELancer-Benchmark">https://github.com/openai/SWELancer-Benchmark</a>). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SWE-Lancerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¥è‡ªUpworkçš„è‡ªç”±èŒä¸šè½¯ä»¶å·¥ç¨‹é¡¹ç›®åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡1400é¡¹ä»»åŠ¡ï¼Œæ€»ä»·å€¼ç™¾ä¸‡ç¾å…ƒçº§åˆ«çš„çœŸå®ä¸–ç•Œæ”¯ä»˜é‡‘é¢ã€‚SWE-Lancerä¸ä»…åŒ…æ‹¬ä»ä»·å€¼50ç¾å…ƒçš„bugä¿®å¤åˆ°ä»·å€¼é«˜è¾¾3ä¸‡2åƒç¾å…ƒçš„åŠŸèƒ½å¼€å‘ç­‰ç‹¬ç«‹å·¥ç¨‹ä»»åŠ¡ï¼Œè¿˜åŒ…æ‹¬ç®¡ç†ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹éœ€è¦åœ¨æŠ€æœ¯å®æ–½æ–¹æ¡ˆä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚ç‹¬ç«‹ä»»åŠ¡é€šè¿‡ç»éªŒä¸°å¯Œçš„è½¯ä»¶å·¥ç¨‹å¸ˆè¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•çš„ä¸‰é‡éªŒè¯è¿›è¡Œè¯„åˆ†ï¼Œè€Œç®¡ç†å†³ç­–åˆ™æ ¹æ®æœ€åˆé›‡ä½£çš„å·¥ç¨‹ç»ç†çš„é€‰æ‹©è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶å‘ç°å‰æ²¿æ¨¡å‹ä»ç„¶æ— æ³•è§£å†³å¤§å¤šæ•°ä»»åŠ¡ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†ä¸€ä¸ªç»Ÿä¸€çš„Dockeré•œåƒå’Œä¸€ä¸ªå…¬å…±è¯„ä¼°ç‰ˆæœ¬SWE-Lancer Diamondï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/openai/SWELancer-Benchmark%EF%BC%89%E3%80%82%E9%80%9A%E8%BF%87%E6%8A%8A%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%98%A0%E5%B0%84%E5%88%B0%E8%B4%A7%E5%B8%81%E4%BB%B7%E5%80%BC%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9BSWE-Lancer%E8%83%BD%E5%A4%9F%E4%BF%83%E8%BF%9B%E5%AF%B9AI%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E7%BB%8F%E6%B5%8E%E5%BD%B1%E5%93%8D%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/openai/SWELancer-Benchmarkï¼‰ã€‚é€šè¿‡æŠŠæ¨¡å‹æ€§èƒ½æ˜ å°„åˆ°è´§å¸ä»·å€¼ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›SWE-Lancerèƒ½å¤Ÿä¿ƒè¿›å¯¹AIæ¨¡å‹å¼€å‘ç»æµå½±å“çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12115v1">PDF</a> 9 pages, 24 pages appendix</p>
<p><strong>Summary</strong><br>    SWE-Lanceræ˜¯æ¥è‡ªäºUpworkçš„çœŸå®ä¸–ç•Œè½¯ä»¶å¼€å‘ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è¶…è¿‡1400é¡¹ä»»åŠ¡ï¼Œæ€»ä»·å€¼è¾¾ç™¾ä¸‡ç¾å…ƒã€‚å®ƒåŒ…æ‹¬ç‹¬ç«‹å·¥ç¨‹ä»»åŠ¡å’Œç®¡ç†ä»»åŠ¡ï¼Œæ¨¡å‹éœ€åœ¨æŠ€æœ¯å®æ–½ææ¡ˆä¸­è¿›è¡Œé€‰æ‹©ã€‚ç‹¬ç«‹ä»»åŠ¡é€šè¿‡ç»éªŒä¸°å¯Œçš„è½¯ä»¶å·¥ç¨‹å¸ˆè¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•çš„ä¸‰é‡éªŒè¯è¿›è¡Œåˆ†çº§ï¼Œç®¡ç†å†³ç­–åˆ™ä¸æœ€åˆé›‡ä½£çš„å·¥ç¨‹ç»ç†çš„é€‰æ‹©ç›¸å¯¹æ¯”è¿›è¡Œè¯„ä¼°ã€‚ä½œè€…è¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ï¼Œå‘ç°å‰æ²¿æ¨¡å‹ä»æ— æ³•è§£å†³å¤§å¤šæ•°ä»»åŠ¡ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼Œä½œè€…å¼€æºäº†ç»Ÿä¸€çš„Dockeré•œåƒå’Œå…¬å…±è¯„ä¼°ç‰ˆæœ¬SWE-Lancer Diamondï¼Œé€šè¿‡å°†æ¨¡å‹æ€§èƒ½æ˜ å°„ä¸ºè´§å¸ä»·å€¼ï¼Œä¸ºäººå·¥æ™ºèƒ½æ¨¡å‹å¼€å‘çš„ç»æµç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SWE-Lanceræ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡1400é¡¹çœŸå®ä¸–ç•Œè½¯ä»¶å¼€å‘ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ç‹¬ç«‹å·¥ç¨‹å’Œç®¡ç†ä»»åŠ¡ã€‚</li>
<li>ç‹¬ç«‹ä»»åŠ¡é€šè¿‡ç«¯åˆ°ç«¯æµ‹è¯•çš„ä¸‰é‡éªŒè¯è¿›è¡Œåˆ†çº§ï¼Œç®¡ç†å†³ç­–åˆ™ä¸å·¥ç¨‹ç»ç†çš„é€‰æ‹©ç›¸å¯¹æ¯”è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å‰æ²¿æ¨¡å‹ä»æ— æ³•è§£å†³å¤§å¤šæ•°ä»»åŠ¡ï¼Œè¯´æ˜åœ¨å¤æ‚çš„è½¯ä»¶å¼€å‘ä»»åŠ¡ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SWE-Lancerå¼€æ”¾äº†ä¸€ä¸ªå…¬å…±è¯„ä¼°ç‰ˆæœ¬SWE-Lancer Diamondï¼Œå¹¶æä¾›ç»Ÿä¸€çš„Dockeré•œåƒï¼Œæ–¹ä¾¿æœªæ¥ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å°†æ¨¡å‹æ€§èƒ½æ˜ å°„ä¸ºè´§å¸ä»·å€¼ï¼ŒSWE-Lanceræœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„ç»æµå½±å“ã€‚</li>
<li>è¿™ä¸ªåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºAIæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œè½¯ä»¶å¼€å‘ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67dbd64322c0a23c1ee2a592a477a72e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c230e24eb686574b45694f6e59d3e5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e76468e170bb92fcf6c8dc772c0486d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ac1d5c8a818a22832c5ffaa60eb780.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-994b671f56f6e4b50b52f4e652e0767b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-MEM-Agentic-Memory-for-LLM-Agents"><a href="#A-MEM-Agentic-Memory-for-LLM-Agents" class="headerlink" title="A-MEM: Agentic Memory for LLM Agents"></a>A-MEM: Agentic Memory for LLM Agents</h2><p><strong>Authors:Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang</strong></p>
<p>While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systemsâ€™ fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/WujiangXu/AgenticMemory">https://github.com/WujiangXu/AgenticMemory</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·å®Œæˆå¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œä½†å®ƒä»¬éœ€è¦è®°å¿†ç³»ç»Ÿæ¥åˆ©ç”¨å†å²ç»éªŒã€‚å½“å‰çš„è®°å¿†ç³»ç»Ÿè™½ç„¶å®ç°äº†åŸºæœ¬çš„å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½ï¼Œä½†ç¼ºä¹é«˜çº§çš„è®°å¿†ç»„ç»‡åŠŸèƒ½ï¼Œå°½ç®¡æœ€è¿‘æœ‰å°è¯•èå…¥å›¾å½¢æ•°æ®åº“ã€‚æ­¤å¤–ï¼Œè¿™äº›ç³»ç»Ÿçš„å›ºå®šæ“ä½œå’Œç»“æ„é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è®°å¿†ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä»¥åŠ¨æ€çš„æ–¹å¼ç»„ç»‡è®°å¿†ã€‚</p>
</blockquote>
<p>éµå¾ªZettelkastenæ–¹æ³•çš„åŸºæœ¬åŸåˆ™ï¼Œæˆ‘ä»¬è®¾è®¡çš„è®°å¿†ç³»ç»Ÿé€šè¿‡åŠ¨æ€ç´¢å¼•å’Œé“¾æ¥åˆ›å»ºç›¸äº’å…³è”çš„çŸ¥è¯†ç½‘ç»œã€‚æ¯å½“æ·»åŠ æ–°è®°å¿†æ—¶ï¼Œæˆ‘ä»¬ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å«å¤šä¸ªç»“æ„åŒ–å±æ€§çš„ç»¼åˆç¬”è®°ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡æè¿°ã€å…³é”®è¯å’Œæ ‡ç­¾ã€‚ç„¶åï¼Œç³»ç»Ÿåˆ†æå†å²è®°å¿†ä»¥è¯†åˆ«ç›¸å…³è¿æ¥ï¼Œåœ¨å­˜åœ¨æœ‰æ„ä¹‰çš„ç›¸ä¼¼æ€§æ—¶å»ºç«‹é“¾æ¥ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªè¿‡ç¨‹ä½¿è®°å¿†å¾—ä»¥è¿›åŒ–â€”â€”éšç€æ–°è®°å¿†çš„èå…¥ï¼Œå®ƒä»¬å¯ä»¥è§¦å‘å¯¹ç°æœ‰å†å²è®°å¿†çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºå’Œå±æ€§çš„æ›´æ–°ï¼Œä½¿è®°å¿†ç½‘ç»œèƒ½å¤ŸæŒç»­ä¸æ–­åœ°ä¼˜åŒ–å…¶ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†Zettelkastençš„ç»“æ„åŒ–ç»„ç»‡åŸåˆ™ä¸ä»£ç†é©±åŠ¨çš„å†³ç­–çµæ´»æ€§ï¼Œä»è€Œå®ç°æ›´é€‚åº”ä¸Šä¸‹æ–‡å˜åŒ–çš„è®°å¿†ç®¡ç†ã€‚åœ¨å…­ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å®è¯å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€æ–°æŠ€æœ¯åŸºå‡†ç›¸æ¯”ï¼Œæœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WujiangXu/AgenticMemory%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WujiangXu/AgenticMemoryæ‰¾åˆ°ã€‚</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12110v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦è®°å¿†ç³»ç»Ÿæ¥åˆ©ç”¨å†å²ç»éªŒä»¥å®Œæˆå¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ã€‚å½“å‰è®°å¿†ç³»ç»Ÿè™½èƒ½è¿›è¡ŒåŸºæœ¬å­˜å‚¨å’Œæ£€ç´¢ï¼Œä½†åœ¨ç»„ç»‡è®°å¿†æ–¹é¢ç¼ºä¹é«˜çº§åŠŸèƒ½ï¼Œä¸”å›ºå®šæ“ä½œå’Œç»“æ„çš„é™åˆ¶å½±å“äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è¯­è¨€æ¨¡å‹è®°å¿†ç³»ç»Ÿï¼Œé‡‡ç”¨åŠ¨æ€ç´¢å¼•å’Œé“¾æ¥çš„æ–¹å¼åˆ›å»ºçŸ¥è¯†ç½‘ç»œã€‚æ–°è®°å¿†è¢«æ·»åŠ æ—¶ï¼Œç³»ç»Ÿç”ŸæˆåŒ…å«ä¸Šä¸‹æ–‡æè¿°ã€å…³é”®è¯å’Œæ ‡ç­¾çš„ç»¼åˆæ€§ç¬”è®°ï¼Œå¹¶åˆ†æå†å²è®°å¿†ä»¥è¯†åˆ«å…³è”ï¼Œå»ºç«‹æœ‰æ„ä¹‰çš„é“¾æ¥ã€‚æ–°è®°å¿†çš„èå…¥å¯è§¦å‘ç°æœ‰å†å²è®°å¿†çš„æ›´æ–°ï¼Œä½¿è®°å¿†ç½‘ç»œä¸æ–­å®Œå–„ç†è§£ã€‚ç»“åˆZettelkastençš„ç»“æ„åŒ–ç»„ç»‡åŸåˆ™å’Œä»£ç†é©±åŠ¨çš„å†³ç­–çµæ´»æ€§ï¼Œå®ç°æ›´é€‚åº”ä¸Šä¸‹æ–‡å˜åŒ–çš„è®°å¿†ç®¡ç†ã€‚åœ¨å…­ç§åŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMéœ€è¦å¤§é‡è®°å¿†ç³»ç»Ÿæ¥åˆ©ç”¨å†å²ç»éªŒå®Œæˆå¤æ‚ä»»åŠ¡ã€‚</li>
<li>å½“å‰è®°å¿†ç³»ç»Ÿç¼ºä¹é«˜çº§ç»„ç»‡åŠŸèƒ½ï¼Œä¸èƒ½é€‚åº”å¤šæ ·åŒ–ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è®°å¿†ç³»ç»Ÿï¼Œé‡‡ç”¨åŠ¨æ€ç»„ç»‡æ–¹å¼åˆ›å»ºçŸ¥è¯†ç½‘ç»œã€‚</li>
<li>æ–°è®°å¿†ç”Ÿæˆæ—¶ï¼Œç³»ç»Ÿç”Ÿæˆç»¼åˆæ€§ç¬”è®°å¹¶è¯†åˆ«å†å²è®°å¿†ä¸­çš„å…³è”ã€‚</li>
<li>æ–°è®°å¿†çš„èå…¥èƒ½å¤Ÿè§¦å‘å†å²è®°å¿†çš„æ›´æ–°ï¼Œå¢å¼ºè®°å¿†ç½‘ç»œçš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆZettelkastençš„ç»“æ„åŒ–ç»„ç»‡å’Œä»£ç†å†³ç­–çš„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d71fd6c61220dd82bdda13f18ad606d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08eda8390fbb3564ae9e94aca52f571a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60b6463f4633e582f7c06f5bbfc60070.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="APB-Accelerating-Distributed-Long-Context-Inference-by-Passing-Compressed-Context-Blocks-across-GPUs"><a href="#APB-Accelerating-Distributed-Long-Context-Inference-by-Passing-Compressed-Context-Blocks-across-GPUs" class="headerlink" title="APB: Accelerating Distributed Long-Context Inference by Passing   Compressed Context Blocks across GPUs"></a>APB: Accelerating Distributed Long-Context Inference by Passing   Compressed Context Blocks across GPUs</h2><p><strong>Authors:Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Hao Zhou, Jie Zhou, Zhiyuan Liu, Maosong Sun</strong></p>
<p>While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in <a target="_blank" rel="noopener" href="https://github.com/thunlp/APB">https://github.com/thunlp/APB</a>. </p>
<blockquote>
<p>è™½ç„¶é•¿ä¸Šä¸‹æ–‡æ¨ç†å¯¹äºæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨çš„è¿›æ­¥è‡³å…³é‡è¦ï¼Œä½†å…¶é¢„å¡«å……é€Ÿåº¦ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç“¶é¢ˆã€‚å½“å‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åºåˆ—å¹¶è¡Œç­–ç•¥å’Œé€šè¿‡è¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶å‡å°‘è®¡ç®—ï¼Œä»ç„¶æœªèƒ½å®ç°æœ€ä½³çš„æ¨ç†æ•ˆç‡ã€‚è¿™é˜»ç¢äº†æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—è¾“å…¥å¹¶åŠæ—¶å¤„ç†é•¿ä¸Šä¸‹æ–‡æŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†APBï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤šä¸»æœºè¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶åŒæ—¶æé«˜è®¡ç®—æ•ˆç‡å’Œå¹¶è¡Œæ€§æ¥æé«˜é¢„å¡«å……é€Ÿåº¦ã€‚APBåœ¨åºåˆ—å¹¶è¡Œæ¡†æ¶å†…å¼•å…¥äº†ä¸€ç§å…³é”®é”®å€¼å¯¹çš„é€šä¿¡æœºåˆ¶ï¼Œä»¥æ›´å¿«çš„æ¨ç†é€Ÿåº¦ç»´æŒä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆå®šåˆ¶çš„FlashAttnå†…æ ¸å’Œä¼˜åŒ–çš„åˆ†å¸ƒç­–ç•¥æ¥å®ç°APBï¼Œæ”¯æŒä¸åŒçš„æ¨¡å‹å’Œå¹¶è¡Œé…ç½®ã€‚ä¸FlashAttnã€RingAttnå’ŒStarAttnç›¸æ¯”ï¼ŒAPBåˆ†åˆ«å®ç°äº†æœ€é«˜è¾¾9.2å€ã€4.2å€å’Œ1.6å€çš„åŠ é€Ÿï¼Œä¸”æ— ä»»ä½•ä»»åŠ¡æ€§èƒ½ä¸‹é™çš„å¯è§‚å¯Ÿç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/thunlp/APB%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86APB%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%92%8C%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/thunlp/APBä¸Šæä¾›äº†APBçš„å®ç°å’Œå®éªŒä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12085v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong><br>    ä¸ºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ä¸­é•¿æ–‡æœ¬æ¨ç†çš„é¢„å¡«å……é€Ÿåº¦ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†APBé«˜æ•ˆé•¿æ–‡æœ¬æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šä¸»æœºè¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å¹¶å¢å¼ºå¹¶è¡Œæ€§ï¼Œæé«˜äº†é¢„å¡«å……é€Ÿåº¦ã€‚é€šè¿‡å¼•å…¥åºåˆ—å¹¶è¡Œæ¡†æ¶å†…çš„å…³é”®é”®å€¼å¯¹é€šä¿¡æœºåˆ¶ï¼Œå®ç°åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚APBé€šè¿‡å®šåˆ¶çš„FlashAttnå†…æ ¸å’Œä¼˜åŒ–åˆ†å¸ƒç­–ç•¥å®ç°ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œå¹¶è¡Œé…ç½®ã€‚ä¸FlashAttnã€RingAttnå’ŒStarAttnç›¸æ¯”ï¼ŒAPBåˆ†åˆ«å®ç°äº†æœ€é«˜è¾¾9.2å€ã€4.2å€å’Œ1.6å€çš„åŠ é€Ÿï¼Œä¸”ä»»åŠ¡æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>é•¿æ–‡æœ¬æ¨ç†çš„é¢„å¡«å……é€Ÿåº¦æ˜¯LLMåº”ç”¨ä¸­çš„ç“¶é¢ˆã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚åºåˆ—å¹¶è¡Œç­–ç•¥å’Œè¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶ä»æœªèƒ½å®ç°æœ€ä¼˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>APBæ¡†æ¶åˆ©ç”¨å¤šä¸»æœºè¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶æé«˜é¢„å¡«å……é€Ÿåº¦ã€‚</li>
<li>APBé€šè¿‡å¼•å…¥åºåˆ—å¹¶è¡Œæ¡†æ¶å†…çš„å…³é”®é”®å€¼å¯¹é€šä¿¡æœºåˆ¶ï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>APBé€šè¿‡å®šåˆ¶åŒ–çš„FlashAttnå†…æ ¸å’Œä¼˜åŒ–åˆ†å¸ƒç­–ç•¥å®ç°ã€‚</li>
<li>APBä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚</li>
<li>APBçš„å®ç°å’Œå®éªŒä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fd4a6a9b23701a2bdd0bb97e73dc90f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4377c7aa75f0b7f78855081d3013a4cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3480795ca043303c3102796cc06bafc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0ef1339a70df0a9583c8c943b83d85b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="On-the-robustness-of-ChatGPT-in-teaching-Korean-Mathematics"><a href="#On-the-robustness-of-ChatGPT-in-teaching-Korean-Mathematics" class="headerlink" title="On the robustness of ChatGPT in teaching Korean Mathematics"></a>On the robustness of ChatGPT in teaching Korean Mathematics</h2><p><strong>Authors:Phuong-Nam Nguyen, Quang Nguyen-The, An Vu-Minh, Diep-Anh Nguyen, Xuan-Lam Pham</strong></p>
<p>ChatGPT, an Artificial Intelligence model, has the potential to revolutionize education. However, its effectiveness in solving non-English questions remains uncertain. This study evaluates ChatGPTâ€™s robustness using 586 Korean mathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering 391 out of 586 questions. We also assess its ability to rate mathematics questions based on eleven criteria and perform a topic analysis. Our findings show that ChatGPTâ€™s ratings align with educational theory and test-taker perspectives. While ChatGPT performs well in question classification, it struggles with non-English contexts, highlighting areas for improvement. Future research should address linguistic biases and enhance accuracy across diverse languages. Domain-specific optimizations and multilingual training could improve ChatGPTâ€™s role in personalized education. </p>
<blockquote>
<p>ChatGPTä½œä¸ºä¸€ç§äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå…·æœ‰é¢ è¦†æ•™è‚²çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶åœ¨è§£å†³éè‹±è¯­é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§å°šä¸ç¡®å®šã€‚æœ¬ç ”ç©¶ä½¿ç”¨586é“éŸ©å›½æ•°å­¦é¢˜ç›®å¯¹ChatGPTçš„ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚ChatGPTå‡†ç¡®ç‡ä¸º66.72%ï¼Œå›ç­”äº†å…¶ä¸­çš„391ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¯¹å…¶æ ¹æ®åä¸€ä¸ªæ ‡å‡†å¯¹æ•°å­¦é¢˜ç›®è¿›è¡Œè¯„åˆ†çš„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¿›è¡Œäº†ä¸»é¢˜åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒChatGPTçš„è¯„åˆ†ä¸æ•™è‚²ç†è®ºåŠæµ‹è¯•è€…çš„è§‚ç‚¹ä¸€è‡´ã€‚è™½ç„¶ChatGPTåœ¨é—®é¢˜åˆ†ç±»æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éè‹±è¯­ç¯å¢ƒä¸­å´æ„Ÿåˆ°å›°éš¾ï¼Œè¿™å‡¸æ˜¾äº†éœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚æœªæ¥çš„ç ”ç©¶åº”è§£å†³è¯­è¨€åè§é—®é¢˜ï¼Œæé«˜åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸­çš„å‡†ç¡®æ€§ã€‚é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¼˜åŒ–å’Œå¤šè¯­è¨€è®­ç»ƒå¯ä»¥æé«˜ChatGPTåœ¨ä¸ªæ€§åŒ–æ•™è‚²ä¸­çš„è§’è‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11915v1">PDF</a> 21 pages, 12 figures, includes statistical analysis of ChatGPTâ€™s   robustness in solving and rating multilingual mathematics questions. Focus on   Korean CSAT Mathematics. Evaluates AI accuracy, rating effectiveness, and   topic analysis</p>
<p><strong>Summary</strong></p>
<p>ChatGPTåœ¨è§£å†³éè‹±è¯­é—®é¢˜æ–¹é¢çš„æ•ˆæœå°šå¾…éªŒè¯ã€‚æœ¬ç ”ç©¶é€šè¿‡586é“éŸ©å›½æ•°å­¦é¢˜ç›®å¯¹ChatGPTçš„ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚ChatGPTå‡†ç¡®ç‡ä¸º66.72%ï¼Œå³æ­£ç¡®å›ç­”äº†391ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜è¯„ä¼°äº†å…¶æ ¹æ®11é¡¹æ ‡å‡†å¯¹æ•°å­¦é¢˜ç›®è¿›è¡Œè¯„åˆ†çš„èƒ½åŠ›å¹¶è¿›è¡Œäº†ä¸»é¢˜åˆ†æã€‚ç ”ç©¶å‘ç°ChatGPTçš„è¯„åˆ†ä¸æ•™è‚²ç†è®ºåŠè€ƒç”Ÿè§‚ç‚¹ç›¸ç¬¦ã€‚ChatGPTåœ¨é—®é¢˜åˆ†ç±»æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éè‹±è¯­ç¯å¢ƒä¸‹è¡¨ç°æ¬ ä½³ï¼Œæœªæ¥ç ”ç©¶åº”è§£å†³è¯­è¨€åè§é—®é¢˜å¹¶æå‡å…¶åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPTåœ¨è§£å†³éŸ©å›½æ•°å­¦é¢˜ç›®æ—¶è¡¨ç°å‡ºä¸€å®šçš„å‡†ç¡®æ€§ï¼Œæ­£ç¡®ç‡ä¸º66.72%ã€‚</li>
<li>ChatGPTèƒ½å¤Ÿæ ¹æ®æ•™è‚²ç†è®ºå’Œè€ƒç”Ÿè§‚ç‚¹å¯¹æ•°å­¦é¢˜ç›®è¿›è¡Œè¯„åˆ†ã€‚</li>
<li>ChatGPTåœ¨éè‹±è¯­ç¯å¢ƒä¸‹çš„è¡¨ç°å°šå¾…æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³éè‹±è¯­æ•°å­¦é¢˜ç›®æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ChatGPTåœ¨é—®é¢˜åˆ†ç±»æ–¹é¢å±•ç°å‡ºäº†è‰¯å¥½çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°ChatGPTåœ¨æŸäº›æ–¹é¢å­˜åœ¨è¯­è¨€åè§ï¼Œæœªæ¥ç ”ç©¶éœ€è¦è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é€šè¿‡åŸŸç‰¹å®šä¼˜åŒ–å’Œå¤šè¯­è¨€è®­ç»ƒï¼ŒChatGPTåœ¨ä¸ªæ€§åŒ–æ•™è‚²ä¸­çš„è§’è‰²æœ‰æœ›å¾—åˆ°æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49ea010f86c708063f7167af712eeb5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83954c98f1edebc8d74ecef1c4eefbee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0a9a77eaaaf40eb7a2cc399a8dd62e2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Building-A-Proof-Oriented-Programmer-That-Is-64-Better-Than-GPT-4o-Under-Data-Scarsity"><a href="#Building-A-Proof-Oriented-Programmer-That-Is-64-Better-Than-GPT-4o-Under-Data-Scarsity" class="headerlink" title="Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o   Under Data Scarsity"></a>Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o   Under Data Scarsity</h2><p><strong>Authors:Dylan Zhang, Justin Wang, Tianran Sun</strong></p>
<p>Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4oâ€™s performance by 54% by repairing its outputs over GPT-4oâ€™s self-repair. </p>
<blockquote>
<p>ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨é¢å‘è¯æ˜ç¼–ç¨‹æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®ç¨€ç¼ºå¯¼è‡´çš„ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰ç¼ºä¹è¶³å¤Ÿçš„é¢å‘è¯æ˜ç¼–ç¨‹è¯­è¨€çš„è¯­æ–™åº“ï¼Œå¦‚F*ï¼Œä»¥åŠï¼ˆ2ï¼‰ç¼ºä¹å¤§è§„æ¨¡çš„é¡¹ç›®çº§é¢å‘è¯æ˜çš„ç¼–ç¨‹å®ç°ï¼Œè¿™äº›å®ç°å¯ä»¥æ•™æˆæ¨¡å‹åœ¨è¿›è¡Œé¢å‘è¯æ˜ç¼–ç¨‹æ—¶çš„å¤æ‚æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬é¦–æ¬¡ä»‹ç»äº†é¡¹ç›®çº§é¢å‘è¯æ˜çš„ç¼–ç¨‹çš„åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå’Œä¿®å¤ä¸¤ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆæˆåŸºæœ¬çš„é¢å‘è¯æ˜çš„ç¼–ç¨‹é—®é¢˜æ¥è§£å†³è¯¥è¯­è¨€çš„ç†Ÿç»ƒç¨‹åº¦é—®é¢˜ï¼›é€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®æ¥æ¿€å‘æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ç°æœ‰å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚è¿™ç§æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆæˆå’Œä¿®å¤å‡½æ•°çº§å’Œå­˜å‚¨åº“çº§çš„ä»£ç è¯æ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç»è¿‡å¾®è°ƒæ‹¥æœ‰14Bå‚æ•°çš„æ¨¡å‹PoPilotï¼Œåœ¨é¢å‘é¡¹ç›®çº§åˆ«çš„è¯æ˜ç¼–ç¨‹ä¸Šèƒ½å¤Ÿè¶…è¶ŠGPT-4oæ¨¡å‹çš„è¡¨ç°ï¼Œç›¸å¯¹æ€§èƒ½æå‡é«˜è¾¾64%ï¼Œå¹¶ä¸”åœ¨ä¿®å¤è¾“å‡ºæ–¹é¢ç›¸è¾ƒäºGPT-4oçš„è‡ªæˆ‘ä¿®å¤æ€§èƒ½æå‡äº†54%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11901v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¯æ˜å¯¼å‘ç¼–ç¨‹æ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¡¨ç°ä¸ºç¼ºä¹è¶³å¤Ÿçš„é’ˆå¯¹è¯æ˜å¯¼å‘ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚F*ï¼‰çš„è¯­æ–™åº“ä»¥åŠç¼ºä¹å¤§è§„æ¨¡çš„é¡¹ç›®çº§è¯æ˜å¯¼å‘å®ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå’Œä¿®å¤è¯æ˜å¯¼å‘ç¼–ç¨‹çš„é¡¹ç›®çº§ä»£ç ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆåŸºæœ¬çš„è¯æ˜å¯¼å‘ç¼–ç¨‹é—®é¢˜æ¥è®­ç»ƒæ¨¡å‹åœ¨è¯¥è¯­è¨€ä¸­çš„ç†Ÿç»ƒç¨‹åº¦ï¼ŒåŒæ—¶å¼•å…¥å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®æ¥æ¿€å‘æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ç°æœ‰å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ç²¾ç»†è®­ç»ƒçš„PoPilotæ¨¡å‹ï¼ˆå‚æ•°ä¸º14Bï¼‰åœ¨é¢å‘é¡¹ç›®çš„è¯æ˜å¯¼å‘ç¼–ç¨‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†GPT-4oæ¨¡å‹ï¼Œç›¸å¯¹æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°64%ï¼Œå¹¶ä¸”åœ¨ä¿®å¤è¾“å‡ºæ–¹é¢æ¯”GPT-4oçš„è‡ªæˆ‘ä¿®å¤åŠŸèƒ½æé«˜äº†54%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¯æ˜å¯¼å‘ç¼–ç¨‹æ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>ç¼ºä¹è¶³å¤Ÿçš„é’ˆå¯¹è¯æ˜å¯¼å‘ç¼–ç¨‹è¯­è¨€çš„è¯­æ–™åº“ä»¥åŠç¼ºä¹å¤§è§„æ¨¡çš„é¡¹ç›®çº§è¯æ˜å¯¼å‘å®ç°æ˜¯æ•°æ®ç¨€ç¼ºçš„ä¸¤ä¸ªä¸»è¦è¡¨ç°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒåŒ…æ‹¬åˆæˆè¯æ˜å¯¼å‘ç¼–ç¨‹é—®é¢˜çš„è®­ç»ƒã€å¼•å…¥å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®å’Œåœ¨ç°æœ‰å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜PoPilotæ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†GPT-4oæ¨¡å‹åœ¨é¢å‘é¡¹ç›®çš„è¯æ˜å¯¼å‘ç¼–ç¨‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç›¸å¯¹æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°64%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e21c5ef15637e7f74dbc4b86c61c81e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52f4675ab6ef39859ce7d5e688c490c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b78df742bd6ae5fe7ed02b6ea410e776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e652f69b300fcbd4907eb2b2bfa8144d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b62697f744abee0385136f1efaeaf57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GLTW-Joint-Improved-Graph-Transformer-and-LLM-via-Three-Word-Language-for-Knowledge-Graph-Completion"><a href="#GLTW-Joint-Improved-Graph-Transformer-and-LLM-via-Three-Word-Language-for-Knowledge-Graph-Completion" class="headerlink" title="GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion"></a>GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion</h2><p><strong>Authors:Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun</strong></p>
<p>Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency.Importantly, we combine iGT with an LLM that takes KG language prompts as input.Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†çŸ¥è¯†å›¾è°±ä¸­çš„é‡è¦ç»“æ„ä¿¡æ¯æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºGLTWï¼Œè¯¥æ–¹æ³•å¯¹çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå¹¶ä¸LLMåˆå¹¶ï¼Œä»¥æé«˜KGCçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„å›¾è½¬æ¢å™¨ï¼ˆiGTï¼‰ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°ç¼–ç å…·æœ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯çš„å­å›¾ï¼Œå¹¶ç»§æ‰¿è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œä½¿ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†iGTä¸ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œè¯¥æ¨¡å‹ä»¥çŸ¥è¯†å›¾è°±è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬åœ¨å„ç§çŸ¥è¯†å›¾è°±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒGLTWå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†çŸ¥è¯†å›¾è°±çš„é‡è¦ç»“æ„ä¿¡æ¯æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GLTWï¼Œè¯¥æ–¹æ³•ç¼–ç çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯å¹¶å°†å…¶ä¸LLMåˆå¹¶ï¼Œä»¥æé«˜KGCæ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†æ”¹è¿›çš„å›¾è½¬æ¢å™¨ï¼ˆiGTï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°ç¼–ç å…·æœ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯çš„å­å›¾ï¼Œå¹¶ç»§æ‰¿è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œä»¥çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†iGTä¸æ¥å—çŸ¥è¯†å›¾è°±è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥çš„LLMç›¸ç»“åˆã€‚åœ¨å¤šä¸ªçŸ¥è¯†å›¾è°±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGLTWä¸æœ€æ–°åŸºçº¿ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ˜¯çŸ¥è¯†å›¾è°±ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ã€‚</li>
<li>æ•´åˆçŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GLTWï¼Œé€šè¿‡ç¼–ç çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯å¹¶ä¸LLMåˆå¹¶ï¼Œä»¥æé«˜KGCæ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ”¹è¿›çš„å›¾è½¬æ¢å™¨ï¼ˆiGTï¼‰ï¼Œæœ‰æ•ˆç¼–ç å­å›¾çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯ï¼Œå¹¶ç»§æ‰¿è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ã€‚</li>
<li>å¼€å‘åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>iGTä¸æ¥å—çŸ¥è¯†å›¾è°±è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥çš„LLMç›¸ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-36237af926d19d4865cf8d00f1640aad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eed81ef8baacc8d3e7de73d46fcdeea7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6ea70906a6fb25c839bcbba5eb55d68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6effeaeebf7e65c62181eb46bc72370c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Do-we-Really-Need-Visual-Instructions-Towards-Visual-Instruction-Free-Fine-tuning-for-Large-Vision-Language-Models"><a href="#Do-we-Really-Need-Visual-Instructions-Towards-Visual-Instruction-Free-Fine-tuning-for-Large-Vision-Language-Models" class="headerlink" title="Do we Really Need Visual Instructions? Towards Visual Instruction-Free   Fine-tuning for Large Vision-Language Models"></a>Do we Really Need Visual Instructions? Towards Visual Instruction-Free   Fine-tuning for Large Vision-Language Models</h2><p><strong>Authors:Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen</strong></p>
<p>Visual instruction tuning has become the predominant technology in eliciting the multimodal task-solving capabilities of large vision-language models (LVLMs). Despite the success, as visual instructions require images as the input, it would leave the gap in inheriting the task-solving capabilities from the backbone LLMs, and make it costly to collect a large-scale dataset. To address it, we propose ViFT, a visual instruction-free fine-tuning framework for LVLMs. In ViFT, we only require the text-only instructions and image caption data during training, to separately learn the task-solving and visual perception abilities. During inference, we extract and combine the representations of the text and image inputs, for fusing the two abilities to fulfill multimodal tasks. Experimental results demonstrate that ViFT can achieve state-of-the-art performance on several visual reasoning and visual instruction following benchmarks, with rather less training data. Our code and data will be publicly released. </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´å·²æˆä¸ºæ¿€å‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡å¼ä»»åŠ¡è§£å†³èƒ½åŠ›çš„ä¸»å¯¼æŠ€æœ¯ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†ç”±äºè§†è§‰æŒ‡ä»¤éœ€è¦å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¿™å°†ç•™ä¸‹ä»ä¸»å¹²LLMsç»§æ‰¿ä»»åŠ¡è§£å†³èƒ½åŠ›çš„ç©ºç™½ï¼Œå¹¶ä¸”æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†çš„æˆæœ¬å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ViFTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºLVLMsçš„è§†è§‰æŒ‡ä»¤å…è´¹å¾®è°ƒæ¡†æ¶ã€‚åœ¨ViFTä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çº¯æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒæ ‡é¢˜æ•°æ®ï¼Œä»¥åˆ†åˆ«å­¦ä¹ ä»»åŠ¡è§£å†³å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨æ¨ç†æœŸé—´ï¼Œæˆ‘ä»¬æå–å¹¶ç»“åˆæ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„ä»£è¡¨ï¼Œèåˆè¿™ä¸¤ç§èƒ½åŠ›æ¥å®Œæˆå¤šæ¨¡å¼ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViFTå¯ä»¥åœ¨å‡ ä¸ªè§†è§‰æ¨ç†å’Œè§†è§‰æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”æ‰€éœ€çš„è®­ç»ƒæ•°æ®è¾ƒå°‘ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11427v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è§†è®¯æŒ‡ä»¤è°ƒæ•´æ˜¯å¯å‘å¤šä»»åŠ¡è§£å†³èƒ½åŠ›çš„ä¸»è¦æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºè§†è®¯æŒ‡ä»¤éœ€è¦å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¯¼è‡´æ— æ³•ç»§æ‰¿ä¸»å¹²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»»åŠ¡è§£å†³èƒ½åŠ›ï¼Œå¹¶ä¸”æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†çš„æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºViFTï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è§†è§‰æŒ‡ä»¤çš„LVLMsç²¾ç»†è°ƒæ•´æ¡†æ¶ã€‚åœ¨ViFTä¸­ï¼Œæˆ‘ä»¬ä»…éœ€åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çº¯æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒæè¿°æ•°æ®ï¼Œä»¥åˆ†åˆ«å­¦ä¹ ä»»åŠ¡è§£å†³èƒ½åŠ›å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå–å¹¶ç»“åˆæ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„è¡¨ç¤ºå½¢å¼ï¼Œèåˆä¸¤ç§èƒ½åŠ›æ¥å®Œæˆå¤šæ¨¡å¼ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViFTåœ¨å¤šä¸ªè§†è§‰æ¨ç†å’Œè§†è§‰æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šèƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æŒ‡ä»¤è°ƒæ•´æ˜¯å¯å‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¤šä»»åŠ¡è§£å†³èƒ½åŠ›çš„ä¸»è¦æŠ€æœ¯ï¼Œä½†éœ€è¦å›¾åƒä½œä¸ºè¾“å…¥ã€‚</li>
<li>æ— éœ€è§†è§‰æŒ‡ä»¤çš„ViFTæ¡†æ¶æ—¨åœ¨è§£å†³è¯¥é—®é¢˜ï¼Œå…è®¸ä½¿ç”¨çº¯æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒæè¿°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ViFTåˆ†åˆ«å­¦ä¹ ä»»åŠ¡è§£å†³èƒ½åŠ›å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼ŒViFTèåˆæ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„è¡¨ç¤ºå½¢å¼ï¼Œä»¥å®Œæˆå¤šæ¨¡å¼ä»»åŠ¡ã€‚</li>
<li>å®éªŒè¯æ˜ViFTåœ¨è§†è§‰æ¨ç†å’Œè§†è§‰æŒ‡ä»¤éµå¾ªæ–¹é¢è¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>å…¬å¼€å‘å¸ƒä»£ç å’Œæ•°æ®ä»¥ä¾¿ä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a2a08745f17d206bf74e43c27ad962e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c1eca6d2b8e9dbadc2a2e284c840164.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3017044795dff18ae070198e932dc43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-840f73de2d7564aacd8ca0683019fa9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b83e5ccd6eacd37501c21e193f051d1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="InsBank-Evolving-Instruction-Subset-for-Ongoing-Alignment"><a href="#InsBank-Evolving-Instruction-Subset-for-Ongoing-Alignment" class="headerlink" title="InsBank: Evolving Instruction Subset for Ongoing Alignment"></a>InsBank: Evolving Instruction Subset for Ongoing Alignment</h2><p><strong>Authors:Jiayi Shi, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Huan Ren, Yao Hu, Kan Li</strong></p>
<p>Large language models (LLMs) typically undergo instruction tuning to enhance alignment. Recent studies emphasize that quality and diversity of instruction data are more crucial than quantity, highlighting the need to select diverse, high-quality subsets to reduce training costs. However, how to evolve these selected subsets alongside the development of new instruction data remains insufficiently explored. To achieve LLMsâ€™ ongoing alignment, we introduce Instruction Bank (InsBank), a continuously updated repository that integrates the latest valuable instruction data. We further propose Progressive Instruction Bank Evolution (PIBE), a novel framework designed to evolve InsBank effectively and efficiently over time. PIBE employs a gradual data selection strategy to maintain long-term efficiency, leveraging a representation-based diversity score to capture relationships between data points and retain historical information for comprehensive diversity evaluation. This also allows for flexible combination of diversity and quality scores during data selection and ranking. Extensive experiments demonstrate that PIBE significantly outperforms baselines in InsBank evolution and is able to extract budget-specific subsets, demonstrating its effectiveness and adaptability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡æŒ‡ä»¤å¾®è°ƒæ¥å¢å¼ºå¯¹é½æ•ˆæœã€‚æœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒï¼ŒæŒ‡ä»¤æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦ï¼Œè¿™å‡¸æ˜¾äº†é€‰æ‹©å¤šæ ·ã€é«˜è´¨é‡å­é›†çš„éœ€è¦ï¼Œä»¥é™ä½è®­ç»ƒæˆæœ¬ã€‚ç„¶è€Œï¼Œå¦‚ä½•éšç€æ–°æŒ‡ä»¤æ•°æ®çš„å‘å±•è€Œå‘å±•è¿™äº›é€‰å®šå­é›†ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚ä¸ºäº†å®ç°LLMçš„æŒç»­å¯¹é½ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŒ‡ä»¤åº“ï¼ˆInsBankï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­æ›´æ–°çš„å­˜å‚¨åº“ï¼Œé›†æˆäº†æœ€æ–°çš„å®è´µæŒ‡ä»¤æ•°æ®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ¸è¿›å¼æŒ‡ä»¤åº“è¿›åŒ–ï¼ˆPIBEï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°InsBankçš„æœ‰æ•ˆå’Œé«˜æ•ˆé•¿æœŸè¿›åŒ–ã€‚PIBEé‡‡ç”¨æ¸è¿›çš„æ•°æ®é€‰æ‹©ç­–ç•¥æ¥ç»´æŒé•¿æœŸæ•ˆç‡ï¼Œåˆ©ç”¨åŸºäºè¡¨ç¤ºçš„å¤šæ ·æ€§åˆ†æ•°æ¥æ•æ‰æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»å¹¶ä¿ç•™å†å²ä¿¡æ¯è¿›è¡Œå…¨é¢çš„å¤šæ ·æ€§è¯„ä¼°ã€‚è¿™ä¹Ÿå…è®¸åœ¨æ•°æ®é€‰æ‹©å’Œæ’åè¿‡ç¨‹ä¸­çµæ´»ç»“åˆå¤šæ ·æ€§å’Œè´¨é‡åˆ†æ•°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPIBEåœ¨InsBankè¿›åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¹¶ä¸”èƒ½å¤Ÿæå–ç‰¹å®šé¢„ç®—çš„å­é›†ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11419v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æŒ‡ä»¤è°ƒæ•´å¢å¼ºå¯¹é½æ•ˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæŒ‡ä»¤æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦ï¼Œéœ€è¦é€‰æ‹©å¤šæ ·ä¸”é«˜è´¨é‡çš„å­é›†æ¥é™ä½è®­ç»ƒæˆæœ¬ã€‚ä¸ºé…åˆæ–°æŒ‡ä»¤æ•°æ®çš„å‘å±•ï¼Œæœ¬æ–‡æå‡ºäº†æŒç»­æ›´æ–°çš„æŒ‡ä»¤åº“Instruction Bankï¼ˆInsBankï¼‰ï¼Œä»¥åŠæ–°å‹çš„æŒ‡ä»¤åº“è¿›åŒ–æ¡†æ¶Progressive Instruction Bank Evolutionï¼ˆPIBEï¼‰ã€‚PIBEé‡‡ç”¨æ¸è¿›çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä»¥é•¿æœŸä¿æŒæ•ˆç‡ï¼Œå¹¶åˆ©ç”¨åŸºäºè¡¨ç¤ºçš„å¤šæ ·æ€§å¾—åˆ†æ¥æ•æ‰æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œä¿ç•™å†å²ä¿¡æ¯è¿›è¡Œå…¨é¢çš„å¤šæ ·æ€§è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒPIBEåœ¨æŒ‡ä»¤åº“è¿›åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿæå–é¢„ç®—ç‰¹å®šçš„å­é›†ï¼Œå±•ç°å‡ºå…¶æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æŒ‡ä»¤è°ƒæ•´å¢å¼ºå¯¹é½æ•ˆæœã€‚</li>
<li>æŒ‡ä»¤æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦ã€‚</li>
<li>éœ€è¦é€‰æ‹©å¤šæ ·ä¸”é«˜è´¨é‡çš„å­é›†æ¥é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>æå‡ºäº†Instruction Bankï¼ˆInsBankï¼‰ä½œä¸ºæŒç»­æ›´æ–°çš„æŒ‡ä»¤åº“ã€‚</li>
<li>ä»‹ç»äº†Progressive Instruction Bank Evolutionï¼ˆPIBEï¼‰æ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆå’Œé«˜æ•ˆåœ°è¿›åŒ–InsBankã€‚</li>
<li>PIBEé‡‡ç”¨æ¸è¿›çš„æ•°æ®é€‰æ‹©ç­–ç•¥å’ŒåŸºäºè¡¨ç¤ºçš„å¤šæ ·æ€§å¾—åˆ†æ¥æ•æ‰æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c94bf7325b0535e43c07136201f31cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac7b06209c007b5c46134e204bc6a86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ecf66641f810518eb2aef770d4856da.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis"><a href="#Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis" class="headerlink" title="Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis"></a>Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis</h2><p><strong>Authors:Shiguo Lian, Kaikai Zhao, Xuejiao Lei, Ning Wang, Zhenhong Long, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Zhaoxiang Liu</strong></p>
<p>DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and DeepSeek-R1-Distill-Llama series on A-Eval, an application-driven benchmark. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications. </p>
<blockquote>
<p>DeepSeek-R1ä»¥å…¶ä½è®­ç»ƒæˆæœ¬å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›è€Œè‘—ç§°ï¼Œå·²åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç„¶è€Œï¼Œä»ç°å®ä¸–ç•Œåº”ç”¨çš„è§’åº¦è¿›è¡Œçš„è¯¦ç»†è¯„ä¼°ä»ç„¶ç¼ºä¹ï¼Œè¿™ä½¿å¾—ç”¨æˆ·éš¾ä»¥ä¸ºå…¶ç‰¹å®šéœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹DeepSeek-V3ã€DeepSeek-R1ã€DeepSeek-R1-Distill-Qwenç³»åˆ—å’ŒDeepSeek-R1-Distill-Llamaç³»åˆ—åœ¨A-Evalè¿™ä¸€åº”ç”¨é©±åŠ¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸å…¶è’¸é¦å¯¹åº”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ†æäº†æ¨ç†å¢å¼ºå¦‚ä½•å½±å“å„ç§å®é™…ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ¨ç†å¢å¼ºæ¨¡å‹é€šå¸¸å¼ºå¤§ï¼Œä½†å¹¶ä¸åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°æœ€ä½³ï¼Œä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½æå‡å·®å¼‚å¾ˆå¤§ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¸®åŠ©ç”¨æˆ·è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œæˆ‘ä»¬é€šè¿‡æ€§èƒ½åˆ†çº§å’Œç›´è§‚çš„çº¿å½¢å›¾æ¥é‡åŒ–DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚å…·ä½“ç¤ºä¾‹æä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ï¼Œæœ‰åŠ©äºç”¨æˆ·é€‰æ‹©å’Œéƒ¨ç½²æœ€ç»æµå®æƒ çš„DeepSeekæ¨¡å‹ï¼Œç¡®ä¿åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®ç°æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11164v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹åœ¨å„ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä½†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ€§èƒ½è¯„ä»·ä»ç¼ºä¹è¯¦ç»†çš„ç ”ç©¶ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹DeepSeekç³»åˆ—æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¯¹æ¯”äº†åŸå§‹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸è’¸é¦æ¨¡å‹åœ¨å¤šç§å®ç”¨ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨ç†å¢å¼ºæ¨¡å‹æ•´ä½“å¼ºå¤§ï¼Œä½†å¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½è¡¨ç°æœ€ä½³ï¼Œæ€§èƒ½å’Œæ”¶ç›Šåœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­å·®å¼‚æ˜¾è‘—ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æ€§èƒ½ç­‰çº§åˆ†ç±»å’Œç›´è§‚å›¾è¡¨é‡åŒ–DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œä»¥å¸®åŠ©ç”¨æˆ·é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ€§èƒ½è¯„ä»·å°šå¾…æ·±å…¥ç ”ç©¶ã€‚</li>
<li>å¯¹DeepSeekç³»åˆ—æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œæ¨ç†å¢å¼ºæ¨¡å‹æ•´ä½“å¼ºå¤§ã€‚</li>
<li>ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œæ¨ç†å¢å¼ºæ¨¡å‹çš„æ€§èƒ½å¢ç›Šå­˜åœ¨å·®å¼‚ã€‚</li>
<li>åŸå§‹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸è’¸é¦æ¨¡å‹åœ¨å®ç”¨ä»»åŠ¡ä¸­çš„è¡¨ç°æœ‰å¯¹æ¯”ä»·å€¼ã€‚</li>
<li>é€šè¿‡æ€§èƒ½ç­‰çº§åˆ†ç±»å’Œç›´è§‚å›¾è¡¨ï¼Œå¯é‡åŒ–DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚</li>
<li>ç”¨æˆ·åœ¨é€‰æ‹©å’Œéƒ¨ç½²æ¨¡å‹æ—¶ï¼Œåº”å…³æ³¨æˆæœ¬æ•ˆç›Šã€æ€§èƒ½åŠèµ„æºæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39bae6e7892891c86e539b6f738fa555.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fc38eeb109ac20ae10993ffb52fc7f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aeeb3ff4d183f580b4fd6954ffee360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-206613ac008042ce6c8f7a97a5477d3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-339cf8c0046c9e8ec7be7b168ffa40ff.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SyncSpeech-Low-Latency-and-Efficient-Dual-Stream-Text-to-Speech-based-on-Temporal-Masked-Transformer"><a href="#SyncSpeech-Low-Latency-and-Efficient-Dual-Stream-Text-to-Speech-based-on-Temporal-Masked-Transformer" class="headerlink" title="SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based   on Temporal Masked Transformer"></a>SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based   on Temporal Masked Transformer</h2><p><strong>Authors:Zhengyan Sheng, Zhihao Du, Shiliang Zhang, Zhijie Yan, Yexin Yang, Zhenhua Ling</strong></p>
<p>This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at <a target="_blank" rel="noopener" href="https://syncspeech.github.io/%7D%7Bhttps://SyncSpeech.github.io/">https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæµæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹SyncSpeechï¼Œå®ƒèƒ½å¤Ÿä»ä¸Šæ¸¸æ¨¡å‹æ¥æ”¶æµå¼æ–‡æœ¬è¾“å…¥ï¼ŒåŒæ—¶ç”Ÿæˆæµå¼è¯­éŸ³ï¼Œä¾¿äºä¸å¤§è¯­è¨€æ¨¡å‹æ— ç¼äº¤äº’ã€‚SyncSpeechå…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼šä½å»¶è¿Ÿï¼Œå› ä¸ºåœ¨æ¥æ”¶åˆ°ç¬¬äºŒä¸ªæ–‡æœ¬æ ‡è®°æ—¶å°±å¼€å§‹ç”Ÿæˆæµå¼è¯­éŸ³ï¼›é«˜æ•ˆç‡ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨ä¸€æ­¥ä¸­è§£ç æ¯ä¸ªåˆ°è¾¾æ–‡æœ¬æ ‡è®°å¯¹åº”çš„æ‰€æœ‰è¯­éŸ³æ ‡è®°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ—¶é—´æ©ç è½¬æ¢å™¨ä½œä¸ºSyncSpeechçš„éª¨å¹²ï¼Œç»“åˆæ ‡è®°çº§æŒç»­æ—¶é—´é¢„æµ‹æ¥é¢„æµ‹è¯­éŸ³æ ‡è®°å’Œä¸‹ä¸€ä¸ªæ­¥éª¤çš„æŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬åœ¨è‹±è¯­å’Œæ™®é€šè¯æ•°æ®é›†ä¸Šè¯„ä¼°äº†SyncSpeechã€‚ä¸æœ€è¿‘çš„åŒæµTTSæ¨¡å‹ç›¸æ¯”ï¼ŒSyncSpeechæ˜¾è‘—å‡å°‘äº†è¯­éŸ³æ ‡è®°çš„ç¬¬ä¸€åŒ…å»¶è¿Ÿå¹¶åŠ é€Ÿäº†å®æ—¶å› å­ã€‚æ­¤å¤–ï¼Œåœ¨ç›¸åŒçš„æ•°æ®è§„æ¨¡ä¸‹ï¼ŒSyncSpeechåœ¨è¯­éŸ³è´¨é‡å’Œç¨³å¥æ€§æ–¹é¢è¾¾åˆ°äº†ä¸ä¼ ç»ŸåŸºäºè‡ªå›å½’çš„TTSæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://syncspeech.github.ioä¸Šæ‰¾åˆ°./">https://SyncSpeech.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11094v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæµæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹SyncSpeechï¼Œèƒ½å¤Ÿæ¥æ”¶æ¥è‡ªä¸Šæ¸¸æ¨¡å‹çš„æµå¼æ–‡æœ¬è¾“å…¥ï¼ŒåŒæ—¶ç”Ÿæˆæµå¼è¯­éŸ³ï¼Œä¾¿äºä¸å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼äº¤äº’ã€‚SyncSpeechå…·æœ‰ä½å»¶è¿Ÿå’Œé«˜æ•ˆç‡çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨æ¥æ”¶åˆ°ç¬¬äºŒä¸ªæ–‡æœ¬æ ‡è®°æ—¶å¼€å§‹ç”Ÿæˆæµå¼è¯­éŸ³ï¼Œå¹¶åœ¨ä¸€æ­¥ä¸­è§£ç æ¯ä¸ªåˆ°è¾¾æ–‡æœ¬æ ‡è®°çš„æ‰€æœ‰è¯­éŸ³æ ‡è®°ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥æ—¶é—´æ©ç è½¬æ¢å™¨ä¸ºSyncSpeechçš„éª¨å¹²ï¼Œç»“åˆæ ‡è®°çº§æ—¶é•¿é¢„æµ‹ï¼Œé¢„æµ‹è¯­éŸ³æ ‡è®°å’Œä¸‹ä¸€æ­¥çš„æ—¶é•¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬åœ¨è‹±è¯­å’Œæ™®é€šè¯æ•°æ®é›†ä¸Šè¯„ä¼°äº†SyncSpeechçš„è¡¨ç°ã€‚ä¸æœ€è¿‘çš„åŒæµTTSæ¨¡å‹ç›¸æ¯”ï¼ŒSyncSpeechæ˜¾è‘—é™ä½äº†è¯­éŸ³æ ‡è®°çš„ç¬¬ä¸€åŒ…å»¶è¿Ÿå¹¶åŠ é€Ÿäº†å®æ—¶å› å­ã€‚å³ä½¿åœ¨ç›¸åŒçš„æ•°æ®è§„æ¨¡ä¸‹ï¼ŒSyncSpeechåœ¨è¯­éŸ³è´¨é‡å’Œç¨³å¥æ€§æ–¹é¢éƒ½å®ç°äº†ä¸ä¼ ç»ŸåŸºäºè‡ªå›å½’çš„TTSæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncSpeechæ˜¯ä¸€ç§åŒæµæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°æµå¼æ–‡æœ¬è¾“å…¥å’Œè¯­éŸ³ç”Ÿæˆçš„æ— ç¼äº¤äº’ã€‚</li>
<li>SyncSpeechå…·æœ‰ä½å»¶è¿Ÿå’Œé«˜æ•ˆç‡çš„ä¼˜ç‚¹ï¼Œå¯å¿«é€Ÿç”Ÿæˆè¯­éŸ³ã€‚</li>
<li>SyncSpeeché‡‡ç”¨æ—¶é—´æ©ç è½¬æ¢å™¨ä½œä¸ºéª¨å¹²ï¼Œç»“åˆæ ‡è®°çº§æ—¶é•¿é¢„æµ‹æ¥ç”Ÿæˆè¯­éŸ³ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ç”¨äºæé«˜è®­ç»ƒæ•ˆç‡å’Œè¯­éŸ³ç”Ÿæˆè´¨é‡ã€‚</li>
<li>SyncSpeechåœ¨è‹±è¯­å’Œæ™®é€šè¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°è‰¯å¥½ã€‚</li>
<li>ä¸å…¶ä»–åŒæµTTSæ¨¡å‹ç›¸æ¯”ï¼ŒSyncSpeeché™ä½äº†è¯­éŸ³æ ‡è®°çš„ç¬¬ä¸€åŒ…å»¶è¿Ÿå¹¶åŠ é€Ÿäº†å®æ—¶æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f51fc94d07b54ba23454ff241f4df6c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a3b955a6a813e99e756fe05814b96f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f6c264c51f4b98b4ac9c82f738ddbed.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-Similarity-A-Gradient-based-Graph-Method-for-Instruction-Tuning-Data-Selection"><a href="#Beyond-Similarity-A-Gradient-based-Graph-Method-for-Instruction-Tuning-Data-Selection" class="headerlink" title="Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning   Data Selection"></a>Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning   Data Selection</h2><p><strong>Authors:Yang Zhao, Li Du, Xiao Ding, Yangou Ouyang, Hepeng Wang, Kai Xiong, Jinglong Gao, Zhouhao Sun, Dongliang Xu, Yang Qing, Dongchen Li, Bing Qin, Ting Liu</strong></p>
<p>Large language models (LLMs) have shown great potential across various industries due to their remarkable ability to generalize through instruction tuning. However, the limited availability of domain-specific data significantly hampers their performance on specialized tasks. While existing methods primarily focus on selecting training data from general datasets that are similar to the target domain, they often fail to consider the joint distribution of instructions, resulting in inefficient learning and suboptimal knowledge transfer. To address these challenges, we introduce G2IS (Gradient-based Graph Instruction Selection), a novel method that constructs a mixed gradient-based instruction graph to capture the joint distribution and interdependencies between instructions. By accounting for the relationships between instructions, G2IS improves domain adaptation efficiency. Additionally, we propose a gradient walk algorithm to refine the data selection process, enhancing both training effectiveness and efficiency. Our experiments demonstrate that G2IS outperforms traditional methods across various domain adaptation tasks, yielding significant performance gains, particularly in complex, data-scarce scenarios. These results underscore the potential of G2IS in advancing the development of large, domain-specific models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶é€šè¿‡æŒ‡ä»¤è°ƒæ•´å®ç°æ³›åŒ–çš„æ˜¾è‘—èƒ½åŠ›è€Œåœ¨å„è¡Œå„ä¸šå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç‰¹å®šé¢†åŸŸæ•°æ®çš„æœ‰é™å¯ç”¨æ€§æ˜¾è‘—é˜»ç¢äº†å®ƒä»¬åœ¨ä¸“ä¸šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»ä¸ç›®æ ‡åŸŸç›¸ä¼¼çš„é€šç”¨æ•°æ®é›†ä¸­é€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†æŒ‡ä»¤çš„è”åˆåˆ†å¸ƒï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹å’ŒçŸ¥è¯†è½¬ç§»ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†G2ISï¼ˆåŸºäºæ¢¯åº¦çš„å›¾æŒ‡ä»¤é€‰æ‹©ï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªåŸºäºæ··åˆæ¢¯åº¦çš„æŒ‡ä»¤å›¾æ¥æ•æ‰æŒ‡ä»¤ä¹‹é—´çš„è”åˆåˆ†å¸ƒå’Œç›¸äº’ä¾èµ–æ€§ã€‚é€šè¿‡è€ƒè™‘æŒ‡ä»¤ä¹‹é—´çš„å…³ç³»ï¼ŒG2ISæé«˜äº†åŸŸé€‚åº”æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¢¯åº¦æ­¥è¡Œç®—æ³•æ¥æ”¹è¿›æ•°æ®é€‰æ‹©è¿‡ç¨‹ï¼Œæé«˜è®­ç»ƒå’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒG2ISåœ¨å„ç§åŸŸé€‚åº”ä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œäº§ç”Ÿäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ã€æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚è¿™äº›ç»“æœçªå‡ºäº†G2ISåœ¨æ¨åŠ¨å¤§å‹ç‰¹å®šé¢†åŸŸæ¨¡å‹å¼€å‘æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11062v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰é€šè¿‡æŒ‡ä»¤è°ƒæ•´è¿›è¡Œæ³›åŒ–çš„èƒ½åŠ›ï¼Œå› æ­¤åœ¨å„è¡Œä¸šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œé¢†åŸŸç‰¹å®šæ•°æ®çš„æœ‰é™å¯ç”¨æ€§å¯¹æ¨¡å‹åœ¨ä¸“é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°æ„æˆäº†æ˜¾è‘—éšœç¢ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»ç±»ä¼¼ç›®æ ‡åŸŸçš„é€šç”¨æ•°æ®é›†ä¸­é€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œå´å¿½è§†äº†æŒ‡ä»¤çš„è”åˆåˆ†å¸ƒï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹å’ŒçŸ¥è¯†è½¬ç§»ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†G2ISï¼ˆåŸºäºæ¢¯åº¦çš„å›¾æŒ‡ä»¤é€‰æ‹©ï¼‰æ–¹æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„æ··åˆæŒ‡ä»¤å›¾æ¥æ•æ‰æŒ‡ä»¤ä¹‹é—´çš„è”åˆåˆ†å¸ƒå’Œäº’ä¾èµ–æ€§ã€‚é€šè¿‡è€ƒè™‘æŒ‡ä»¤ä¹‹é—´çš„å…³ç³»ï¼ŒG2ISæé«˜äº†é¢†åŸŸé€‚åº”æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ¢¯åº¦æ­¥è¡Œç®—æ³•æ¥ä¼˜åŒ–æ•°æ®é€‰æ‹©è¿‡ç¨‹ï¼Œæé«˜äº†è®­ç»ƒå’Œæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒG2ISåœ¨å¤šç§é¢†åŸŸé€‚åº”ä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ã€æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹æ€§èƒ½æå‡æ˜¾è‘—ã€‚è¿™çªæ˜¾äº†G2ISåœ¨å¼€å‘å¤§å‹é¢†åŸŸç‰¹å®šæ¨¡å‹ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·æœ‰é€šè¿‡æŒ‡ä»¤è°ƒæ•´è¿›è¡Œæ³›åŒ–çš„èƒ½åŠ›ï¼Œä½†åœ¨é¢†åŸŸç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™äºæ•°æ®å¯ç”¨æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»é€šç”¨æ•°æ®é›†ä¸­é€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œå¿½ç•¥äº†æŒ‡ä»¤çš„è”åˆåˆ†å¸ƒã€‚</li>
<li>G2ISæ–¹æ³•é€šè¿‡æ„å»ºæ··åˆæ¢¯åº¦æŒ‡ä»¤å›¾æ¥æ•æ‰æŒ‡ä»¤ä¹‹é—´çš„è”åˆåˆ†å¸ƒå’Œäº’ä¾èµ–æ€§ï¼Œæé«˜é¢†åŸŸé€‚åº”æ•ˆç‡ã€‚</li>
<li>G2ISé‡‡ç”¨æ¢¯åº¦æ­¥è¡Œç®—æ³•ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œæé«˜è®­ç»ƒå’Œæ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒG2ISåœ¨å¤šç§é¢†åŸŸé€‚åº”ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„å¤æ‚åœºæ™¯ä¸‹ã€‚</li>
<li>G2ISå…·æœ‰å¼€å‘å¤§å‹é¢†åŸŸç‰¹å®šæ¨¡å‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4ea665e9e118e3beb610ae2aaf6cfc2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35ba3f496beb741d7bbba912494eda73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-585e377f7c2cb8a1f04af2a9872ed1ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a2ace35111f0c9deec72ed34a170039.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HybriDNA-A-Hybrid-Transformer-Mamba2-Long-Range-DNA-Language-Model"><a href="#HybriDNA-A-Hybrid-Transformer-Mamba2-Long-Range-DNA-Language-Model" class="headerlink" title="HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model"></a>HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model</h2><p><strong>Authors:Mingqian Ma, Guoqing Liu, Chuan Cao, Pan Deng, Tri Dao, Albert Gu, Peiran Jin, Zhao Yang, Yingce Xia, Renqian Luo, Pipi Hu, Zun Wang, Yuan-Jyue Chen, Haiguang Liu, Tao Qin</strong></p>
<p>Advances in natural language processing and large language models have sparked growing interest in modeling DNA, often referred to as the â€œlanguage of lifeâ€. However, DNA modeling poses unique challenges. First, it requires the ability to process ultra-long DNA sequences while preserving single-nucleotide resolution, as individual nucleotides play a critical role in DNA function. Second, success in this domain requires excelling at both generative and understanding tasks: generative tasks hold potential for therapeutic and industrial applications, while understanding tasks provide crucial insights into biological mechanisms and diseases. To address these challenges, we propose HybriDNA, a decoder-only DNA language model that incorporates a hybrid Transformer-Mamba2 architecture, seamlessly integrating the strengths of attention mechanisms with selective state-space models. This hybrid design enables HybriDNA to efficiently process DNA sequences up to 131kb in length with single-nucleotide resolution. HybriDNA achieves state-of-the-art performance across 33 DNA understanding datasets curated from the BEND, GUE, and LRB benchmarks, and demonstrates exceptional capability in generating synthetic cis-regulatory elements (CREs) with desired properties. Furthermore, we show that HybriDNA adheres to expected scaling laws, with performance improving consistently as the model scales from 300M to 3B and 7B parameters. These findings underscore HybriDNAâ€™s versatility and its potential to advance DNA research and applications, paving the way for innovations in understanding and engineering the â€œlanguage of lifeâ€. </p>
<blockquote>
<p>éšç€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¯¹DNAçš„å»ºæ¨¡ï¼Œå¸¸è¢«ç§°ä¸ºâ€œç”Ÿå‘½çš„è¯­è¨€â€ï¼Œå¼•å‘äº†è¶Šæ¥è¶Šå¤šçš„å…´è¶£ã€‚ç„¶è€Œï¼ŒDNAå»ºæ¨¡å­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒéœ€è¦åœ¨ä¿æŒå•æ ¸è‹·é…¸åˆ†è¾¨ç‡çš„åŒæ—¶å¤„ç†è¶…é•¿çš„DNAåºåˆ—ï¼Œå› ä¸ºå•ä¸ªæ ¸è‹·é…¸åœ¨DNAåŠŸèƒ½ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚å…¶æ¬¡ï¼Œåœ¨è¿™ä¸ªé¢†åŸŸå–å¾—æˆåŠŸéœ€è¦åœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºè‰²ï¼šç”Ÿæˆä»»åŠ¡åœ¨æ²»ç–—å’Œå·¥ä¸šåº”ç”¨æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œè€Œç†è§£ä»»åŠ¡åˆ™æä¾›äº†å¯¹ç”Ÿç‰©æœºåˆ¶å’Œç–¾ç—…çš„æ·±åˆ»è§è§£ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HybriDNAï¼Œè¿™æ˜¯ä¸€ä¸ªä»…è§£ç çš„DNAè¯­è¨€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†æ··åˆTransformer-Mamba2æ¶æ„ï¼Œæ— ç¼é›†æˆäº†æ³¨æ„åŠ›æœºåˆ¶çš„é€‰æ‹©çŠ¶æ€ç©ºé—´æ¨¡å‹çš„ä¼˜ç‚¹ã€‚è¿™ç§æ··åˆè®¾è®¡ä½¿HybriDNAèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é•¿è¾¾131kbçš„DNAåºåˆ—ï¼Œå¹¶ä¿ç•™å•æ ¸è‹·é…¸åˆ†è¾¨ç‡ã€‚HybriDNAåœ¨æ¥è‡ªBENDã€GUEå’ŒLRBåŸºå‡†æµ‹è¯•çš„33ä¸ªDNAç†è§£æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºå‡ºç”Ÿæˆå…·æœ‰æ‰€éœ€ç‰¹æ€§çš„åˆæˆé¡ºå¼è°ƒæ§å…ƒä»¶ï¼ˆCREsï¼‰çš„å“è¶Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜HybriDNAéµå¾ªé¢„æœŸçš„æ¯”ä¾‹å®šå¾‹ï¼Œéšç€æ¨¡å‹ä»3äº¿å‚æ•°æ‰©å±•åˆ°7äº¿å‚æ•°ï¼Œå…¶æ€§èƒ½ä¸€ç›´åœ¨ä¸æ–­æé«˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†HybriDNAçš„é€šç”¨æ€§ä»¥åŠå…¶åœ¨æ¨åŠ¨DNAç ”ç©¶ä¸åº”ç”¨æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºç†è§£å’Œå·¥ç¨‹åŒ–â€œç”Ÿå‘½çš„è¯­è¨€â€é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10807v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¿€å‘äº†äººä»¬å¯¹DNAå»ºæ¨¡çš„å…´è¶£ï¼ŒDNAè¢«ç§°ä¸ºâ€œç”Ÿå‘½çš„è¯­è¨€â€ã€‚ç„¶è€Œï¼ŒDNAå»ºæ¨¡å­˜åœ¨æŒ‘æˆ˜ã€‚HybriDNAæ¨¡å‹ç»“åˆTransformerå’ŒMamba2æ¶æ„ï¼Œå®ç°äº†è¶…é•¿DNAåºåˆ—çš„å•æ ¸è‹·é…¸åˆ†è¾¨ç‡å¤„ç†ï¼ŒåŒæ—¶å®Œæˆç”Ÿæˆå’Œç†è§£ä»»åŠ¡ã€‚æ¨¡å‹æ€§èƒ½åœ¨å¤šä¸ªDNAç†è§£æ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œèƒ½ç”Ÿæˆå…·æœ‰æ‰€éœ€å±æ€§çš„åˆæˆé¡ºå¼è°ƒæ§å…ƒä»¶ï¼ˆCREsï¼‰ã€‚éšç€æ¨¡å‹è§„æ¨¡ä»3äº¿åˆ°3åƒäº¿å’Œ7åƒäº¿å‚æ•°çš„æ‰©å¤§ï¼Œæ€§èƒ½ä¸æ–­æé«˜ã€‚è¿™æ˜¾ç¤ºäº†HybriDNAçš„é€šç”¨æ€§å’Œæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DNAå»ºæ¨¡è¢«ç§°ä¸ºâ€œç”Ÿå‘½çš„è¯­è¨€â€ï¼Œå—åˆ°è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå‘å±•çš„å¯å‘ã€‚</li>
<li>DNAå»ºæ¨¡é¢ä¸´å¤„ç†è¶…é•¿åºåˆ—å¹¶ä¿æŒå•æ ¸è‹·é…¸åˆ†è¾¨ç‡çš„æŒ‘æˆ˜ã€‚</li>
<li>HybriDNAç»“åˆäº†Transformerå’ŒMamba2æ¶æ„ï¼Œå®ç°äº†é«˜æ•ˆDNAåºåˆ—å¤„ç†ã€‚</li>
<li>HybriDNAèƒ½åœ¨å¤šä¸ªDNAç†è§£æ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>HybriDNAèƒ½ç”Ÿæˆå…·æœ‰æ‰€éœ€å±æ€§çš„åˆæˆé¡ºå¼è°ƒæ§å…ƒä»¶ï¼ˆCREsï¼‰ã€‚</li>
<li>éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼ŒHybriDNAçš„æ€§èƒ½æŒç»­æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-637136d9fb1124bf1c5eee51497f78e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd940001ac6c6153642eb392af8693b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="E2LVLM-Evidence-Enhanced-Large-Vision-Language-Model-for-Multimodal-Out-of-Context-Misinformation-Detection"><a href="#E2LVLM-Evidence-Enhanced-Large-Vision-Language-Model-for-Multimodal-Out-of-Context-Misinformation-Detection" class="headerlink" title="E2LVLM:Evidence-Enhanced Large Vision-Language Model for Multimodal   Out-of-Context Misinformation Detection"></a>E2LVLM:Evidence-Enhanced Large Vision-Language Model for Multimodal   Out-of-Context Misinformation Detection</h2><p><strong>Authors:Junjie Wu, Yumeng Fu, Nan Yu, Guohong Fu</strong></p>
<p>Recent studies in Large Vision-Language Models (LVLMs) have demonstrated impressive advancements in multimodal Out-of-Context (OOC) misinformation detection, discerning whether an authentic image is wrongly used in a claim. Despite their success, the textual evidence of authentic images retrieved from the inverse search is directly transmitted to LVLMs, leading to inaccurate or false information in the decision-making phase. To this end, we present E2LVLM, a novel evidence-enhanced large vision-language model by adapting textual evidence in two levels. First, motivated by the fact that textual evidence provided by external tools struggles to align with LVLMs inputs, we devise a reranking and rewriting strategy for generating coherent and contextually attuned content, thereby driving the aligned and effective behavior of LVLMs pertinent to authentic images. Second, to address the scarcity of news domain datasets with both judgment and explanation, we generate a novel OOC multimodal instruction-following dataset by prompting LVLMs with informative content to acquire plausible explanations. Further, we develop a multimodal instruction-tuning strategy with convincing explanations for beyond detection. This scheme contributes to E2LVLM for multimodal OOC misinformation detection and explanation. A multitude of experiments demonstrate that E2LVLM achieves superior performance than state-of-the-art methods, and also provides compelling rationales for judgments. </p>
<blockquote>
<p>åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°ç ”ç©¶ä¸­ï¼Œå¤šæ¨¡æ€è„±ç¦»ä¸Šä¸‹æ–‡ï¼ˆOOCï¼‰çš„è™šå‡ä¿¡æ¯æ£€æµ‹å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œèƒ½å¤Ÿè¾¨åˆ«çœŸå®å›¾åƒæ˜¯å¦è¢«é”™è¯¯åœ°ç”¨äºæŸä¸ªå£°æ˜ä¸­ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†æˆåŠŸï¼Œé€šè¿‡åå‘æœç´¢æ£€ç´¢åˆ°çš„çœŸå®å›¾åƒçš„æ–‡å­—è¯æ®ä¼šç›´æ¥ä¼ é€’ç»™LVLMsï¼Œå¯¼è‡´å†³ç­–é˜¶æ®µå‡ºç°ä¸å‡†ç¡®æˆ–é”™è¯¯çš„ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†E2LVLMï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¢å¼ºå‹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå±‚æ¬¡é€‚åº”æ–‡æœ¬è¯æ®ã€‚é¦–å…ˆï¼Œå—åˆ°å¤–éƒ¨å·¥å…·æä¾›çš„æ–‡æœ¬è¯æ®éš¾ä»¥ä¸LVLMsè¾“å…¥å¯¹é½çš„äº‹å®çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é‡æ–°æ’åºå’Œé‡å†™ç­–ç•¥ï¼Œä»¥ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡åè°ƒçš„å†…å®¹ï¼Œä»è€Œé©±åŠ¨ä¸çœŸå®å›¾åƒç›¸å…³çš„LVLMsçš„å¯¹é½å’Œæœ‰æ•ˆè¡Œä¸ºã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³åŒæ—¶åŒ…å«åˆ¤æ–­å’Œè§£é‡Šçš„æ–°é—»é¢†åŸŸæ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡æç¤ºLVLMsä»¥åŒ…å«ä¿¡æ¯çš„å†…å®¹æ¥ç”Ÿæˆä¸€ä¸ªæ–°çš„OOCå¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªæ•°æ®é›†ï¼Œä»¥è·å¾—åˆç†çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å…·æœ‰è¯´æœåŠ›çš„è§£é‡Šçš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œä¸ä»…é™äºæ£€æµ‹ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºE2LVLMå¤šæ¨¡æ€OOCè™šå‡ä¿¡æ¯æ£€æµ‹å’Œè§£é‡Šåšå‡ºäº†è´¡çŒ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒE2LVLMè¾ƒç°æœ‰æ–¹æ³•å–å¾—äº†ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸ºåˆ¤æ–­æä¾›äº†ä»¤äººä¿¡æœçš„ä¾æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10455v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„ç ”ç©¶åœ¨è„±ç¦»ä¸Šä¸‹æ–‡çš„å¤šåª’ä½“ä¿¡æ¯æ£€æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåˆ¤æ–­å›¾åƒæ˜¯å¦è¢«é”™è¯¯ä½¿ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç›´æ¥å°†é€šè¿‡åå‘æœç´¢è·å–çš„å›¾åƒæ–‡æœ¬è¯æ®ä¼ é€’ç»™LVLMï¼Œå¯¼è‡´å†³ç­–é˜¶æ®µå¯èƒ½å‡ºç°ä¸å‡†ç¡®æˆ–é”™è¯¯ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è¯æ®å¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹E2LVLMï¼Œé€šè¿‡ä¸¤ä¸ªå±‚çº§é€‚åº”æ–‡æœ¬è¯æ®ã€‚é¦–å…ˆï¼Œé’ˆå¯¹å¤–éƒ¨å·¥å…·æä¾›çš„æ–‡æœ¬è¯æ®ä¸LVLMè¾“å…¥å¯¹é½å›°éš¾çš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§é‡æ–°æ’åºå’Œé‡å†™ç­–ç•¥ï¼Œç”Ÿæˆè¿è´¯ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„å†…å®¹ï¼Œä»è€Œé©±åŠ¨LVLMå¯¹çœŸå®å›¾åƒçš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§è¡Œä¸ºã€‚å…¶æ¬¡ï¼Œä¸ºè§£å†³æ–°é—»é¢†åŸŸæ•°æ®é›†åˆ¤æ–­ä¸è§£é‡Šå…¼å¤‡çš„ç¨€ç¼ºé—®é¢˜ï¼Œé€šè¿‡å¼•å¯¼LVLMsè·å–åˆç†è§£é‡Šï¼Œç”Ÿæˆæ–°å‹è„±ç¦»ä¸Šä¸‹æ–‡çš„å¤šåª’ä½“æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§å…·æœ‰è¯´æœåŠ›çš„è§£é‡Šçš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒE2LVLMç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸ºåˆ¤æ–­æä¾›äº†æœ‰åŠ›ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨è„±ç¦»ä¸Šä¸‹æ–‡çš„å¤šåª’ä½“ä¿¡æ¯æ£€æµ‹æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç›´æ¥å°†å›¾åƒæ–‡æœ¬è¯æ®ä¼ é€’ç»™LVLMï¼Œå¯èƒ½å¯¼è‡´å†³ç­–é˜¶æ®µçš„ä¸å‡†ç¡®æˆ–é”™è¯¯ä¿¡æ¯ã€‚</li>
<li>E2LVLMé€šè¿‡ä¸¤ä¸ªå±‚çº§é€‚åº”æ–‡æœ¬è¯æ®ï¼Œè§£å†³æ–‡æœ¬è¯æ®ä¸LVLMè¾“å…¥å¯¹é½å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>E2LVLMé‡‡ç”¨é‡æ–°æ’åºå’Œé‡å†™ç­–ç•¥ï¼Œç”Ÿæˆè¿è´¯ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„å†…å®¹ï¼Œä»¥é©±åŠ¨LVLMçš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§è¡Œä¸ºã€‚</li>
<li>é’ˆå¯¹æ–°é—»é¢†åŸŸæ•°æ®é›†åˆ¤æ–­ä¸è§£é‡Šå…¼å¤‡çš„ç¨€ç¼ºé—®é¢˜ï¼ŒE2LVLMç”Ÿæˆæ–°å‹è„±ç¦»ä¸Šä¸‹æ–‡çš„å¤šåª’ä½“æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ã€‚</li>
<li>E2LVLMå¼€å‘äº†ä¸€ç§å…·æœ‰è¯´æœåŠ›çš„è§£é‡Šçš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒE2LVLMç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7b79055b0a40a6b8b99f943bdecc08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a25c2c5ee11ee7c345e66f9f1fcd324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-649fbe76fb9305c9f9cc4af6225d0f4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91f34f724ae4a06d82125b3216a06769.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Improving-Acoustic-Side-Channel-Attacks-on-Keyboards-Using-Transformers-and-Large-Language-Models"><a href="#Improving-Acoustic-Side-Channel-Attacks-on-Keyboards-Using-Transformers-and-Large-Language-Models" class="headerlink" title="Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers   and Large Language Models"></a>Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers   and Large Language Models</h2><p><strong>Authors:Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai</strong></p>
<p>The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNetâ€™s performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios. </p>
<blockquote>
<p>æ—¥å¸¸è®¾å¤‡ä¸­éº¦å…‹é£çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œå¯¹åœ¨çº¿æœåŠ¡çš„ä¾èµ–ç¨‹åº¦ä¹Ÿè¶Šæ¥è¶Šé«˜ï¼Œè¿™åŠ å¤§äº†é’ˆå¯¹é”®ç›˜çš„å£°ä¾§ä¿¡é“æ”»å‡»ï¼ˆASCAsï¼‰çš„é£é™©ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å˜å‹å™¨ï¼ˆVTsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æé«˜æ­¤ç±»æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨å…ˆå‰ç ”ç©¶çš„åŸºç¡€ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ï¼ŒCoAtNetæ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„CoAtNetåœ¨é€šè¿‡æ™ºèƒ½æ‰‹æœºï¼ˆPhoneï¼‰è®°å½•çš„é”®å‡»ä¸Šå®ç°äº†5.0%çš„æ”¹è¿›ï¼Œåœ¨é€šè¿‡Zoomè®°å½•çš„é”®å‡»ä¸Šå®ç°äº†5.9%çš„æ”¹è¿›ï¼Œè¶…è¿‡äº†å…ˆå‰çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å˜å‹å™¨æ¶æ„å’Œè¯­è¨€æ¨¡å‹ï¼Œæœ€ä½³VTæ¨¡å‹ä¸CoAtNetçš„æ€§èƒ½ç›¸åŒ¹é…ã€‚ä¸€ä¸ªå…³é”®çš„è¿›æ­¥æ˜¯å¼•å…¥äº†ç”¨äºç°å®ä¸–ç•Œçš„å™ªå£°ç¼“è§£æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨LLMsè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œæ£€æµ‹å’Œçº æ­£å˜ˆæ‚ç¯å¢ƒä¸­çš„é”™è¯¯é”®å‡»ï¼Œæé«˜äº†ASCAçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹å¯äº§ç”Ÿä¸å…·æœ‰67å€ä»¥ä¸Šå‚æ•°çš„å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚VTså’ŒLLMsçš„é›†æˆæé«˜äº†ASCAç¼“è§£çš„å®é™…é€‚ç”¨æ€§ï¼Œæ ‡å¿—ç€è¿™äº›æŠ€æœ¯åœ¨åº”å¯¹ç°å®ä¸–ç•Œçš„ASCAså’Œé”™è¯¯çº æ­£æ–¹é¢çš„é¦–æ¬¡åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09782v2">PDF</a> We will reflect comments from the reviewers and re-submit</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ—¥ç›Šæ™®åŠçš„éº¦å…‹é£åœ¨æ—¥å¸¸è®¾å¤‡ä¸­çš„é£é™©ä»¥åŠåœ¨çº¿æœåŠ¡å¢é•¿çš„èƒŒæ™¯ä¸‹ï¼Œé’ˆå¯¹é”®ç›˜çš„å£°å­¦ä¾§ä¿¡é“æ”»å‡»ï¼ˆASCAsï¼‰é—®é¢˜ã€‚ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„è§†è§‰å˜å‹å™¨ï¼ˆVTsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æŠ€æœ¯æ¥æå‡æ­¤ç±»æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚CoAtNetæ¨¡å‹å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½ï¼Œä¸å…ˆå‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œåœ¨æ™ºèƒ½æ‰‹æœºå’ŒZoomè®°å½•çš„å…³é”®å­—è¾“å…¥æ–¹é¢åˆ†åˆ«æé«˜äº†5.0%å’Œ5.9%ã€‚ç ”ç©¶è¿˜è¯„ä¼°äº†å˜å‹å™¨æ¶æ„å’Œè¯­è¨€æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç”¨äºçœŸå®åœºæ™¯çš„å™ªå£°ç¼“è§£æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨LLMsè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œå¯ä»¥åœ¨å˜ˆæ‚ç¯å¢ƒä¸­æ£€æµ‹å’Œçº æ­£é”™è¯¯çš„é”®å‡»ï¼Œæé«˜ASCAæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰çš„å¾®è°ƒè½»é‡çº§è¯­è¨€æ¨¡å‹å¯å®ç°ä¸å…·æœ‰æ›´å¤šå‚æ•°çš„é‡é‡çº§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚VTså’ŒLLMsçš„ç»“åˆæé«˜äº†ASCAç¼“è§£çš„å®é™…é€‚ç”¨æ€§ï¼Œæ ‡å¿—ç€è¿™äº›æŠ€æœ¯é¦–æ¬¡è¢«ç”¨äºè§£å†³çœŸå®åœºæ™¯ä¸­çš„ASCAå’Œé”™è¯¯çº æ­£é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éº¦å…‹é£åœ¨æ—¥å¸¸è®¾å¤‡ä¸­çš„æ™®åŠå’Œåœ¨çº¿æœåŠ¡çš„å¢é•¿å¢åŠ äº†å£°å­¦ä¾§ä¿¡é“æ”»å‡»ï¼ˆASCAsï¼‰çš„é£é™©ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„è§†è§‰å˜å‹å™¨ï¼ˆVTsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¢å¼ºASCAsçš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚</li>
<li>CoAtNetæ¨¡å‹å®ç°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼Œç›¸æ¯”å…ˆå‰ç ”ç©¶ï¼Œåœ¨å…³é”®è¾“å…¥è®°å½•æ–¹é¢æœ‰æ‰€æ”¹å–„ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é’ˆå¯¹çœŸå®åœºæ™¯çš„å™ªå£°ç¼“è§£æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨LLMsè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œèƒ½åœ¨å˜ˆæ‚ç¯å¢ƒä¸­æ£€æµ‹å’Œçº æ­£é”™è¯¯çš„é”®å‡»ã€‚</li>
<li>ä½¿ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰çš„è½»é‡çº§è¯­è¨€æ¨¡å‹å¯å®ç°ä¸é‡é‡çº§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5265c9c8a7219913d1e01a89746d432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7e0789cfcf7a3534a02cd33bdb6d7fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6270588840f86de4b3007016d05efed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95841e816f35d0005d7d3dcf6fc7b494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-121a024fbd0291d120525b176d1e92a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d777c2c1c996729ab6ad726912d93ab9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df9744b47b68b12857ca7c62aa417381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e43243b366021a4492deab8d780a2037.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebde3e029be267ee99597e4de0aab12b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evaluating-GPTâ€™s-Capability-in-Identifying-Stages-of-Cognitive-Impairment-from-Electronic-Health-Data"><a href="#Evaluating-GPTâ€™s-Capability-in-Identifying-Stages-of-Cognitive-Impairment-from-Electronic-Health-Data" class="headerlink" title="Evaluating GPTâ€™s Capability in Identifying Stages of Cognitive   Impairment from Electronic Health Data"></a>Evaluating GPTâ€™s Capability in Identifying Stages of Cognitive   Impairment from Electronic Health Data</h2><p><strong>Authors:Yu Leng, Yingnan He, Colin Magdamo, Ana-Maria Vranceanu, Christine S. Ritchie, Shibani S. Mukerji, Lidia M. V. R. Moura, John R. Dickson, Deborah Blacker, Sudeshna Das</strong></p>
<p>Identifying cognitive impairment within electronic health records (EHRs) is crucial not only for timely diagnoses but also for facilitating research. Information about cognitive impairment often exists within unstructured clinician notes in EHRs, but manual chart reviews are both time-consuming and error-prone. To address this issue, our study evaluates an automated approach using zero-shot GPT-4o to determine stage of cognitive impairment in two different tasks. First, we evaluated the ability of GPT-4o to determine the global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who visited the memory clinic at Massachusetts General Hospital (MGH), and achieved a weighted kappa score of 0.83. Second, we assessed GPT-4oâ€™s ability to differentiate between normal cognition, mild cognitive impairment (MCI), and dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o attained a weighted kappa score of 0.91 in comparison to specialist chart reviews and 0.96 on cases that the clinical adjudicators rated with high confidence. Our findings demonstrate GPT-4oâ€™s potential as a scalable chart review tool for creating research datasets and assisting diagnosis in clinical settings in the future. </p>
<blockquote>
<p>è¯†åˆ«ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­çš„è®¤çŸ¥éšœç¢ä¸ä»…å¯¹äºåŠæ—¶è¯Šæ–­è‡³å…³é‡è¦ï¼Œè€Œä¸”å¯¹äºä¿ƒè¿›ç ”ç©¶ä¹Ÿè‡³å…³é‡è¦ã€‚å…³äºè®¤çŸ¥éšœç¢çš„ä¿¡æ¯é€šå¸¸å­˜åœ¨äºEHRsä¸­çš„éç»“æ„åŒ–åŒ»ç”Ÿç¬”è®°ä¸­ï¼Œä½†æ‰‹åŠ¨å›¾è¡¨å®¡æŸ¥æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¯„ä¼°äº†ä¸€ç§ä½¿ç”¨é›¶æ ·æœ¬GPT-4oçš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä»¥ç¡®å®šä¸¤ä¸ªä¸åŒä»»åŠ¡ä¸­çš„è®¤çŸ¥éšœç¢é˜¶æ®µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯„ä¼°äº†GPT-4oç¡®å®šæ¥è‡ªé©¬è¨è¯¸å¡å·æ€»åŒ»é™¢ï¼ˆMGHï¼‰è®°å¿†è¯Šæ‰€çš„769åæ‚£è€…çš„å…¨å±€ä¸´åºŠç—´å‘†è¯„åˆ†ï¼ˆCDRï¼‰çš„èƒ½åŠ›ï¼ŒåŠ æƒkappaå¾—åˆ†ä¸º0.83ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¯„ä¼°äº†GPT-4oåŒºåˆ†æ¥è‡ª860ååŒ»ç–—ä¿é™©æ‚£è€…ä¸‰å¹´æ‰€æœ‰è®°å½•çš„å¸¸è§„è®¤çŸ¥ã€è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰å’Œç—´å‘†çš„èƒ½åŠ›ã€‚GPT-4oä¸ä¸“ä¸šå›¾è¡¨å®¡æŸ¥çš„åŠ æƒkappaå¾—åˆ†ä¸º0.91ï¼Œè€Œåœ¨ä¸´åºŠè£å®šè€…é«˜åº¦è¯„ä»·çš„æ¡ˆä¾‹ä¸­è¾¾åˆ°äº†0.96ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGPT-4oå…·æœ‰ä½œä¸ºå¯ä¼¸ç¼©çš„å›¾è¡¨å®¡æŸ¥å·¥å…·åœ¨æœªæ¥åˆ›å»ºç ”ç©¶æ•°æ®é›†å’ŒååŠ©ä¸´åºŠç¯å¢ƒä¸­è¯Šæ–­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09715v1">PDF</a> Findings paper presented at Machine Learning for Health (ML4H)   symposium 2024, December 15-16, 2024, Vancouver, Canada, 7 pages</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶åˆ©ç”¨GPT-4oè‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªä»»åŠ¡è¯„ä¼°å…¶åœ¨ç”µå­å¥åº·è®°å½•ä¸­è¯†åˆ«è®¤çŸ¥éšœç¢é˜¶æ®µçš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œåœ¨è®°å¿†è¯Šæ‰€æ‚£è€…æ•°æ®é›†ä¸­ï¼ŒGPT-4oèƒ½å¤Ÿç¡®å®šä¸´åºŠç—´å‘†è¯„åˆ†ï¼ŒåŠ æƒkappaå¾—åˆ†ä¸º0.83ã€‚å…¶æ¬¡ï¼Œåœ¨åŒ»ä¿æ‚£è€…æ•°æ®é›†ä¸­ï¼ŒGPT-4oèƒ½åŒºåˆ†æ­£å¸¸è®¤çŸ¥ã€è½»åº¦è®¤çŸ¥éšœç¢å’Œç—´å‘†ï¼ŒåŠ æƒkappaå¾—åˆ†ä¸º0.91ã€‚ç ”ç©¶ç»“æœè¡¨æ˜GPT-4oå…·æœ‰ä½œä¸ºå¯è§„æ¨¡åŒ–å›¾è¡¨å®¡æŸ¥å·¥å…·åœ¨æœªæ¥åˆ›å»ºç ”ç©¶æ•°æ®é›†å’Œè¾…åŠ©ä¸´åºŠè¯Šç–—çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GPT-4oè¢«ç”¨äºè‡ªåŠ¨åŒ–è¯†åˆ«ç”µå­å¥åº·è®°å½•ä¸­çš„è®¤çŸ¥éšœç¢é˜¶æ®µã€‚</li>
<li>åœ¨è®°å¿†è¯Šæ‰€æ‚£è€…æ•°æ®é›†ä¸­ï¼ŒGPT-4oçš„ä¸´åºŠç—´å‘†è¯„åˆ†åŠ æƒkappaå¾—åˆ†ä¸º0.83ã€‚</li>
<li>GPT-4oåœ¨åŒ»ä¿æ‚£è€…æ•°æ®é›†ä¸­èƒ½å¤ŸåŒºåˆ†æ­£å¸¸è®¤çŸ¥ã€è½»åº¦è®¤çŸ¥éšœç¢å’Œç—´å‘†ï¼ŒåŠ æƒkappaå¾—åˆ†ä¸º0.91ã€‚</li>
<li>GPT-4oçš„è¡¨ç°åœ¨é«˜ç½®ä¿¡åº¦ç—…ä¾‹ä¸­å°¤å…¶å‡ºè‰²ï¼Œä¸ä¸“å®¶è¯„å®¡çš„åŠ æƒkappaå¾—åˆ†è¾¾åˆ°0.96ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½å¤Ÿæé«˜è¯Šæ–­æ—¶æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œå¯¹äºç ”ç©¶å’Œä¸´åºŠå®è·µå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>GPT-4oçš„æ½œåŠ›åœ¨äºå¯ä½œä¸ºå¯è§„æ¨¡åŒ–å›¾è¡¨å®¡æŸ¥å·¥å…·ï¼Œä¾¿äºåˆ›å»ºç ”ç©¶æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1abf60d620dc552082b680be646e4b5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-877f3736e182d3a9778b8d3f314cfda2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baa1a896f318a2f7ebbdd719c790ff4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57474e646b57d29898ca5a3339245bbb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Optimizing-GPT-for-Video-Understanding-Zero-Shot-Performance-and-Prompt-Engineering"><a href="#Optimizing-GPT-for-Video-Understanding-Zero-Shot-Performance-and-Prompt-Engineering" class="headerlink" title="Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt   Engineering"></a>Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt   Engineering</h2><p><strong>Authors:Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu</strong></p>
<p>In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPTâ€™s performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPTâ€™s performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry. </p>
<blockquote>
<p>åœ¨æœ¬æ¬¡ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹GPTä¸ºåŸºç¡€æ¨¡å‹çš„æ¢ç´¢ä¸ä¼˜åŒ–ï¼Œé’ˆå¯¹è§†é¢‘è´¨é‡çš„ä¸ƒå¤§å…³é”®ç±»åˆ«ï¼Œå®ç°é›¶æ ·æœ¬åˆ†ç±»çš„è§†é¢‘å†…å®¹åˆ†ç±»è¡Œä¸šæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›GPTæ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æç¤ºä¼˜åŒ–å’Œæ”¿ç­–å®Œå–„ï¼Œè¯æ˜ç®€åŒ–å¤æ‚æ”¿ç­–å¯ä»¥æ˜¾è‘—é™ä½å‡é˜´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåˆ†è§£èšåˆçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå®ƒä¼˜äºä¼ ç»Ÿçš„å•ä¸€æç¤ºæ–¹æ³•ã€‚è¿™äº›å®éªŒæ˜¯åœ¨çœŸå®çš„è¡Œä¸šé—®é¢˜ä¸Šè¿›è¡Œçš„ï¼Œè¡¨æ˜æœ‰ç­–ç•¥çš„æç¤ºè®¾è®¡å¯ä»¥åœ¨ä¸å¢åŠ å¾®è°ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜GPTçš„æ€§èƒ½ï¼Œä¸ºæ”¹å–„å·¥ä¸šå„é¢†åŸŸè§†é¢‘åˆ†ç±»ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09573v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é€šè¿‡æ¢ç´¢å’Œä¼˜åŒ–GPTæ¨¡å‹ä¸ºåŸºç¡€çš„æ–¹æ³•ï¼Œè§£å†³è§†é¢‘å†…å®¹åˆ†ç±»è¡Œä¸šä¸­çš„æŒ‘æˆ˜ï¼Œå®ç°äº†é›¶æ ·æœ¬åˆ†ç±»åœ¨ä¸ƒä¸ªå…³é”®è§†é¢‘è´¨é‡ç±»åˆ«ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶é€šè¿‡æ”¹è¿›GPTæ€§èƒ½ï¼Œæå‡ºä¼˜åŒ–æç¤ºå’Œæ”¿ç­–ç²¾ç‚¼çš„æ–°æ–¹æ³•ï¼Œç®€åŒ–å¤æ‚æ”¿ç­–ä»¥é™ä½è¯¯æŠ¥ç‡ã€‚åŒæ—¶å¼•å…¥åŸºäºåˆ†è§£èšåˆæŠ€æœ¯çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œè¶…è¶Šä¼ ç»Ÿå•ä¸€æç¤ºæ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œæ— éœ€é¢å¤–å¾®è°ƒï¼Œç²¾å¿ƒè®¾è®¡æç¤ºå¯ä»¥æ˜¾è‘—æé«˜GPTæ€§èƒ½ï¼Œä¸ºå·¥ä¸šç•Œå„é¢†åŸŸæ”¹è¿›è§†é¢‘åˆ†ç±»ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆå¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹è§†é¢‘å†…å®¹åˆ†ç±»è¡Œä¸šçš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨GPTæ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®ç°é›¶æ ·æœ¬åˆ†ç±»åœ¨ä¸ƒä¸ªå…³é”®è§†é¢‘è´¨é‡ç±»åˆ«ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºä¼˜åŒ–æç¤ºå’Œæ”¿ç­–ç²¾ç‚¼çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜GPTçš„æ€§èƒ½ã€‚</li>
<li>ç®€åŒ–å¤æ‚æ”¿ç­–ä»¥é™ä½è¯¯æŠ¥ç‡ã€‚</li>
<li>å¼•å…¥åŸºäºåˆ†è§£èšåˆæŠ€æœ¯çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ç²¾å¿ƒè®¾è®¡æç¤ºå¯ä»¥æ˜¾è‘—æé«˜GPTæ€§èƒ½ï¼Œæ— éœ€é¢å¤–å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de42fe670e9baf23168d191b22a7cc9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3baab7f73a65aa24ca22cb86b3fae73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c0c8806d0233825c0c2a63ac4417a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a079a809870630bc3d105b5c7838e28.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-187d99963871431099fb488d2e5eb90a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  HARBOR Exploring Persona Dynamics in Multi-Agent Competition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-17/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-21f4b35a5a54a1097fe7a3669dedb28f.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-17  OmniHuman-1 Rethinking the Scaling-Up of One-Stage Conditioned Human   Animation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
