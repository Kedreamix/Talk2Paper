<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  A Survey on Bridging EEG Signals and Generative AI From Image and Text   to Beyond">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6188e4b07f6d8e3271f65954c044ec74.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond"><a href="#A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond" class="headerlink" title="A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond"></a>A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond</h2><p><strong>Authors:Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury</strong></p>
<p>Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¼€è¾Ÿäº†æ–°çš„é¢†åŸŸï¼Œå®ç°äº†è¾…åŠ©é€šä¿¡ã€ç¥ç»è¡¨å¾å­¦ä¹ å’Œå¤šæ¨¡å¼èåˆã€‚ç‰¹åˆ«æ˜¯åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰çš„è„‘æœºæ¥å£ï¼Œä¸ºå°†ç¥ç»æ´»åŠ¨è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„è¾“å‡ºæä¾›äº†éä¾µå…¥æ€§çš„æ‰‹æ®µã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’ŒåŸºäºå˜å‹å™¨çš„è‡ªç„¶è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæå¤§åœ°æé«˜äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºEEGçš„å¤šæ¨¡å¼ç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨ï¼šï¼ˆiï¼‰é€šè¿‡GANsã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œæ‰©æ•£æ¨¡å‹å®ç°EEGåˆ°å›¾åƒç”Ÿæˆï¼›ï¼ˆiiï¼‰åˆ©ç”¨åŸºäºTransformerçš„è‡ªç„¶è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•å®ç°EEGåˆ°æ–‡æœ¬ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ–°å…´çš„EEGè¯­éŸ³åˆæˆé¢†åŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­å‘å±•çš„å¤šæ¨¡å¼å‰æ²¿é¢†åŸŸã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†å…³é”®æ•°æ®é›†ã€åº”ç”¨åœºæ™¯ã€æŒ‘æˆ˜ä»¥åŠæ”¯æ’‘ç”Ÿæˆæ–¹æ³•çš„åŸºæœ¬EEGç‰¹å¾ç¼–ç æ–¹æ³•ã€‚é€šè¿‡å¯¹åŸºäºEEGçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ç»“æ„æ€§æ¦‚è¿°ï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æ´å¯ŸåŠ›ï¼Œä»¥æ¨åŠ¨ç¥ç»è§£ç çš„å‘å±•ï¼Œæé«˜è¾…åŠ©æŠ€æœ¯çš„æ•ˆèƒ½ï¼Œå¹¶æ‹“å±•è„‘æœºäº¤äº’çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12048v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>BCIsä¸GenAIçš„æ•´åˆä¸ºè„‘ä¿¡å·è§£ç å¼€å¯äº†æ–°çºªå…ƒï¼ŒåŠ©åŠ›è¾…åŠ©æ²Ÿé€šã€ç¥ç»è¡¨å¾å­¦ä¹ ä¸å¤šåª’ä½“èåˆã€‚å€ŸåŠ©è„‘ç”µå›¾ï¼ˆEEGï¼‰çš„BCIä¸ºéä¾µå…¥å¼ç¥ç»æ´»åŠ¨è½¬è¯‘æä¾›é€”å¾„ã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›å±•ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¤§å¹…æ”¹å–„äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬åŠè¯­éŸ³ç”Ÿæˆã€‚æœ¬æ–‡æ—¨åœ¨ç»¼è¿°EEGåŸºå¤šå…ƒç”Ÿæˆç ”ç©¶çš„æœ€æ–°è¿›å±•ï¼Œèšç„¦äºEEGè½¬å›¾åƒç”Ÿæˆä¸EEGè½¬æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶æ¢è®¨æ–°å…´çš„EEGè¯­éŸ³åˆæˆé¢†åŸŸã€‚é€šè¿‡æ¦‚è¿°EEGåŸºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä¸å®è·µè€…æä¾›ç¥ç»è§£ç ã€è¾…åŠ©æŠ€æœ¯ä¸è„‘æœºäº¤äº’å‰æ²¿çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BCIä¸GenAIèåˆä¸ºè„‘ä¿¡å·è§£ç å¸¦æ¥æ–°çªç ´ï¼Œä¿ƒè¿›è¾…åŠ©æ²Ÿé€šã€ç¥ç»è¡¨å¾å­¦ä¹ ä¸å¤šåª’ä½“èåˆã€‚</li>
<li>EEGä¸ºéä¾µå…¥å¼ç¥ç»æ´»åŠ¨ç¿»è¯‘æä¾›äº†é‡è¦æ‰‹æ®µã€‚</li>
<li>æ·±åº¦å­¦ä¹ è¿›æ­¥å¦‚GANså’ŒLLMsæ˜¾è‘—æå‡äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æ–‡ç« èšç„¦äºEEGè½¬å›¾åƒç”Ÿæˆå’ŒEEGè½¬æ–‡æœ¬ç”Ÿæˆçš„æœ€æ–°ç ”ç©¶ã€‚</li>
<li>EEGè¯­éŸ³åˆæˆæ˜¯ä¸€ä¸ªæ–°å…´ä¸”ä¸æ–­å‘å±•çš„å¤šåª’ä½“èåˆå‰æ²¿é¢†åŸŸã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†EEGåŸºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å…³é”®æ•°æ®é›†ã€ç”¨ä¾‹ã€æŒ‘æˆ˜å’ŒEEGç‰¹å¾ç¼–ç æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f89a92f9f6583f89738d30ff8feb081e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f4c288ef1da1c6353c7dc833140956e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93096ddd3246c583190a570d31c2b141.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NaturalL2S-End-to-End-High-quality-Multispeaker-Lip-to-Speech-Synthesis-with-Differential-Digital-Signal-Processing"><a href="#NaturalL2S-End-to-End-High-quality-Multispeaker-Lip-to-Speech-Synthesis-with-Differential-Digital-Signal-Processing" class="headerlink" title="NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis   with Differential Digital Signal Processing"></a>NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis   with Differential Digital Signal Processing</h2><p><strong>Authors:Yifan Liang, Fangkun Liu, Andong Li, Xiaodong Li, Chengshi Zheng</strong></p>
<p>Recent advancements in visual speech recognition (VSR) have promoted progress in lip-to-speech synthesis, where pre-trained VSR models enhance the intelligibility of synthesized speech by providing valuable semantic information. The success achieved by cascade frameworks, which combine pseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the transcribed text, highlights the benefits of leveraging VSR models. However, these methods typically rely on mel-spectrograms as an intermediate representation, which may introduce a key bottleneck: the domain gap between synthetic mel-spectrograms, generated from inherently error-prone lip-to-speech mappings, and real mel-spectrograms used to train vocoders. This mismatch inevitably degrades synthesis quality. To bridge this gap, we propose Natural Lip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic inductive biases with differentiable speech generation components. Specifically, we introduce a fundamental frequency (F0) predictor to capture prosodic variations in synthesized speech. The predicted F0 then drives a Differentiable Digital Signal Processing (DDSP) synthesizer to generate a coarse signal which serves as prior information for subsequent speech synthesis. Additionally, instead of relying on a reference speaker embedding as an auxiliary input, our approach achieves satisfactory performance on speaker similarity without explicitly modelling speaker characteristics. Both objective and subjective evaluation results demonstrate that NaturalL2S can effectively enhance the quality of the synthesized speech when compared to state-of-the-art methods. Our demonstration page is accessible at <a target="_blank" rel="noopener" href="https://yifan-liang.github.io/NaturalL2S/">https://yifan-liang.github.io/NaturalL2S/</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰çš„è¿›å±•ä¿ƒè¿›äº†å”‡è¯­åˆæˆæŠ€æœ¯çš„æå‡ã€‚é¢„è®­ç»ƒçš„VSRæ¨¡å‹é€šè¿‡æä¾›æœ‰ä»·å€¼çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜äº†åˆæˆè¯­éŸ³çš„å¯æ‡‚åº¦ã€‚çº§è”æ¡†æ¶ç»“åˆäº†ä¼ªVSRå’Œä¼ªæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æˆ–éšå«åœ°ä½¿ç”¨è½¬å½•æ–‡æœ¬ï¼Œå–å¾—äº†æˆåŠŸï¼Œçªæ˜¾äº†åˆ©ç”¨VSRæ¨¡å‹çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºæ¢…å°”é¢‘è°±å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œå¯èƒ½ä¼šå¼•å…¥ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šç”±æœ¬è´¨ä¸Šå®¹æ˜“å‡ºé”™çš„å”‡è¯­æ˜ å°„ç”Ÿæˆçš„åˆæˆæ¢…å°”é¢‘è°±å›¾ä¸ç”¨äºè®­ç»ƒvocoderçš„çœŸå®æ¢…å°”é¢‘è°±å›¾ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ã€‚è¿™ç§ä¸åŒ¹é…ä¼šä¸å¯é¿å…åœ°é™ä½åˆæˆè´¨é‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªç„¶å”‡è¯­ï¼ˆNaturalL2Sï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œèåˆäº†å£°å­¦å½’çº³åè§å’Œå¯å¾®åˆ†çš„è¯­éŸ³ç”Ÿæˆç»„ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºé¢‘ï¼ˆF0ï¼‰é¢„æµ‹å™¨æ¥æ•æ‰åˆæˆè¯­éŸ³ä¸­çš„éŸµå¾‹å˜åŒ–ã€‚é¢„æµ‹çš„F0ç„¶åé©±åŠ¨ä¸€ä¸ªå¯å¾®åˆ†çš„æ•°å­—ä¿¡å·å¤„ç†ï¼ˆDDSPï¼‰åˆæˆå™¨ç”Ÿæˆä¸€ä¸ªç²—ç•¥çš„ä¿¡å·ï¼Œä½œä¸ºåç»­è¯­éŸ³åˆæˆçš„å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¾èµ–å‚è€ƒè¯´è¯äººåµŒå…¥ä½œä¸ºè¾…åŠ©è¾“å…¥ï¼Œåœ¨ä¸éœ€è¦æ˜¾å¼å»ºæ¨¡è¯´è¯äººç‰¹å¾çš„æƒ…å†µä¸‹å®ç°äº†ä»¤äººæ»¡æ„çš„è¯´è¯äººç›¸ä¼¼æ€§æ€§èƒ½ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ç»“æœå‡è¡¨æ˜ï¼Œä¸è‡ªç„¶å”‡è¯­ç›¸æ¯”ï¼Œæœ€æ–°æ–¹æ³•çš„åˆæˆè¯­éŸ³è´¨é‡å¾—åˆ°äº†æœ‰æ•ˆæå‡ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://yifan-liang.github.io/NaturalL2S/">https://yifan-liang.github.io/NaturalL2S/</a> è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12002v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­éŸ³è¯†åˆ«çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†å”‡è¯­åˆæˆæŠ€æœ¯çš„æå‡ã€‚å€ŸåŠ©é¢„è®­ç»ƒçš„è§†è§‰è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œåˆæˆçš„è¯­éŸ³å¯å€ŸåŠ©è¯­ä¹‰ä¿¡æ¯å¢å¼ºå…¶æ¸…æ™°åº¦ã€‚å°½ç®¡ä¸²è”æ¡†æ¶é€šè¿‡ç»“åˆä¼ªå”‡è¯­ä¸ä¼ªæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰æŠ€æœ¯æˆ–é—´æ¥åˆ©ç”¨è½¬å½•æ–‡æœ¬å–å¾—äº†æˆåŠŸï¼Œä½†è¿™äº›æ–¹æ³•ä¸»è¦ä¾èµ–äºæ¢…å°”é¢‘è°±å›¾ä½œä¸ºä¸­é—´è¡¨è¾¾å½¢å¼ï¼Œè¿™å¯èƒ½äº§ç”ŸåŸŸå·®è·é—®é¢˜ï¼šåŸºäºå®¹æ˜“å‡ºé”™å”‡è¯­åˆ°è¯­éŸ³æ˜ å°„ç”Ÿæˆçš„åˆæˆæ¢…å°”é¢‘è°±å›¾ä¸ç”¨äºè®­ç»ƒç¼–ç å™¨çš„çœŸå®æ¢…å°”é¢‘è°±å›¾ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ç°è±¡ã€‚ä¸ºç¼©çŸ­æ­¤å·®è·ï¼Œæå‡ºäº†è‡ªç„¶å”‡è¯­åˆ°è¯­éŸ³è½¬æ¢ï¼ˆNaturalL2Sï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å£°å­¦å½’çº³åè§ä¸å¯å¾®åˆ†çš„è¯­éŸ³ç”Ÿæˆç»„ä»¶ç›¸ç»“åˆã€‚å®éªŒè¯æ˜ï¼Œä¸è‡ªç„¶L2Sæ¡†æ¶ç”Ÿæˆçš„è¯­éŸ³ç›¸æ¯”ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œå…¶åœ¨åˆæˆè¯­éŸ³è´¨é‡ä¸Šæœ‰æ‰€æå‡ã€‚å…¶æ¼”ç¤ºé¡µé¢å¯è®¿é—®ç½‘å€ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://yifan-liang.github.io/NaturalL2S/]">https://yifan-liang.github.io/NaturalL2S/]</a> ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰çš„è¿›å±•æ¨åŠ¨äº†å”‡è¯­åˆæˆæŠ€æœ¯çš„è¿›æ­¥ã€‚</li>
<li>é¢„è®­ç»ƒçš„VSRæ¨¡å‹æä¾›è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºäº†åˆæˆè¯­éŸ³çš„æ¸…æ™°åº¦ã€‚</li>
<li>ä¸²è”æ¡†æ¶ç»“åˆäº†ä¼ªå”‡è¯­ä¸ä¼ªæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ï¼Œè¡¨ç°å‡ºåˆ©ç”¨VSRæ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>å­˜åœ¨åŸŸå·®è·é—®é¢˜ï¼šåˆæˆæ¢…å°”é¢‘è°±å›¾ä¸çœŸå®æ¢…å°”é¢‘è°±å›¾ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ç°è±¡ã€‚</li>
<li>è‡ªç„¶å”‡è¯­åˆ°è¯­éŸ³è½¬æ¢ï¼ˆNaturalL2Sï¼‰æ¡†æ¶ç»“åˆäº†å£°å­¦å½’çº³åè§å’Œå¯å¾®åˆ†çš„è¯­éŸ³ç”Ÿæˆç»„ä»¶ã€‚</li>
<li>NaturalL2Sæ¡†æ¶é€šè¿‡å¼•å…¥åŸºé¢‘ï¼ˆF0ï¼‰é¢„æµ‹å™¨æ•æ‰åˆæˆè¯­éŸ³ä¸­çš„éŸµå¾‹å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef4af6aed3b27e13211cbc7d5fd7c925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682ff72abd4b3b066ac68d265c9cbffa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fa22329f43cdd447a0da3372825fea2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Step-Audio-Unified-Understanding-and-Generation-in-Intelligent-Speech-Interaction"><a href="#Step-Audio-Unified-Understanding-and-Generation-in-Intelligent-Speech-Interaction" class="headerlink" title="Step-Audio: Unified Understanding and Generation in Intelligent Speech   Interaction"></a>Step-Audio: Unified Understanding and Generation in Intelligent Speech   Interaction</h2><p><strong>Authors:Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuting Yan, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu</strong></p>
<p>Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio">https://github.com/stepfun-ai/Step-Audio</a>. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³äº¤äº’ä½œä¸ºäººæœºäº¤äº’çš„åŸºæœ¬æ¥å£ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„å¼€æºæ¨¡å‹é¢ä¸´ç€è¯­éŸ³æ•°æ®é‡‡é›†æˆæœ¬é«˜ã€åŠ¨æ€æ§åˆ¶å¼±ã€æ™ºèƒ½æœ‰é™ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†Step-Audioï¼Œè¿™æ˜¯é¦–ä¸ªæŠ•å…¥ç”Ÿäº§çš„å¼€æºè§£å†³æ–¹æ¡ˆã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š1ï¼‰ä¸€ä¸ªæ‹¥æœ‰130Bå‚æ•°çš„ç»Ÿä¸€è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°äº†ç»Ÿä¸€çš„ç†è§£å’Œç”Ÿæˆï¼Œå…¶ä¸­Step-Audio-Chatç‰ˆæœ¬å·²å¼€æºï¼›2ï¼‰ä¸€ä¸ªç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»æµå®æƒ çš„è¯­éŸ³å…‹éš†æ¡†æ¶ï¼Œå¹¶é€šè¿‡è’¸é¦æŠ€æœ¯äº§ç”Ÿäº†å¼€æºçš„è½»é‡çº§Step-Audio-TTS-3Bæ¨¡å‹ï¼›3ï¼‰ä¸€ä¸ªæŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®ç°ä¸åŒæ–¹è¨€ã€æƒ…æ„Ÿã€æ­Œå”±å’ŒRAPçš„åŠ¨æ€è°ƒæ•´ï¼›4ï¼‰ä¸€ä¸ªå¢å¼ºçš„è®¤çŸ¥æ¶æ„ï¼Œé€šè¿‡å·¥å…·è°ƒç”¨å’Œè§’è‰²æ‰®æ¼”èƒ½åŠ›æ¥æœ‰æ•ˆç®¡ç†å¤æ‚ä»»åŠ¡ã€‚åŸºäºæˆ‘ä»¬æ–°çš„StepEval-Audio-360è¯„ä¼°åŸºå‡†ï¼ŒStep-Audioåœ¨äººå·¥è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢ã€‚åœ¨LLaMA Questionç­‰å¼€æºåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æ€§èƒ½æé«˜äº†9.3%ï¼Œè¿™å±•ç¤ºäº†æˆ‘ä»¬å¯¹æ¨åŠ¨å¼€æºå¤šæ¨¡æ€è¯­è¨€æŠ€æœ¯å‘å±•çš„æ‰¿è¯ºã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/stepfun-ai/Step-Audioè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹åä¸ºStep-Audioçš„å¼€æºå¤šæ¨¡æ€è¯­éŸ³æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†å½“å‰è¯­éŸ³äº¤äº’é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é«˜æˆæœ¬çš„æ•°æ®æ”¶é›†ã€åŠ¨æ€æ§åˆ¶ä¸è¶³å’Œæ™ºèƒ½é™åˆ¶ã€‚å…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬ç»Ÿä¸€çš„è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ã€ç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“ã€æŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿä»¥åŠå¢å¼ºè®¤çŸ¥æ¶æ„ã€‚Step-Audioåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå±•ç°äº†å…¶åœ¨æ¨è¿›å¼€æºå¤šæ¨¡æ€è¯­è¨€æŠ€æœ¯å‘å±•æ–¹é¢çš„æ‰¿è¯ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Step-Audioæ˜¯é¦–ä¸ªç”Ÿäº§å°±ç»ªçš„å¼€æºå¤šæ¨¡æ€è¯­éŸ³è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†è¯­éŸ³äº¤äº’çš„å¤šä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æä¾›äº†ç»Ÿä¸€çš„è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°äº†ç†è§£å’Œç”Ÿæˆçš„åŒé‡è§†è§’ï¼Œå¹¶å¼€æºäº†Step-Audio-Chatç‰ˆæœ¬ã€‚</li>
<li>é€šè¿‡è’¸é¦è¿‡ç¨‹ï¼Œå¼€æºäº†è½»é‡çº§çš„Step-Audio-TTS-3Bæ¨¡å‹ã€‚</li>
<li>å¼•å…¥äº†æŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿï¼Œå¯å®ç°è·¨æ–¹è¨€ã€æƒ…æ„Ÿã€æ­Œå”±å’ŒRAPçš„åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>å¢å¼ºè®¤çŸ¥æ¶æ„ä½¿Step-Audioèƒ½å¤Ÿç®¡ç†å¤æ‚ä»»åŠ¡ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œè§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚</li>
<li>Step-Audioåœ¨å¤šä¸ªå¼€æºåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€ä½³æ€§èƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07b2b4440ed6a01006e31ed673f3b1e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31d6d360dc1ec5145451ae0ac4c21b48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e801713a5fa5f6ee5ecec11004556e3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Knowing-Your-Target-Target-Aware-Transformer-Makes-Better-Spatio-Temporal-Video-Grounding"><a href="#Knowing-Your-Target-Target-Aware-Transformer-Makes-Better-Spatio-Temporal-Video-Grounding" class="headerlink" title="Knowing Your Target: Target-Aware Transformer Makes Better   Spatio-Temporal Video Grounding"></a>Knowing Your Target: Target-Aware Transformer Makes Better   Spatio-Temporal Video Grounding</h2><p><strong>Authors:Xin Gu, Yaojie Shen, Chenxi Luo, Tiejian Luo, Yan Huang, Yuewei Lin, Heng Fan, Libo Zhang</strong></p>
<p>Transformer has attracted increasing interest in STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (\e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy. </p>
<blockquote>
<p>Transformerå› å…¶åœ¨ç«¯åˆ°ç«¯çš„ç®¡é“å’Œä»¤äººæŒ¯å¥‹çš„ç»“æœè€Œåœ¨STVGï¼ˆç©ºé—´å’Œæ—¶é—´è§†é¢‘å­—å¹•ç”Ÿæˆï¼‰ä¸­å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…´è¶£ã€‚ç°æœ‰çš„åŸºäºTransformerçš„STVGæ–¹æ³•é€šå¸¸åˆ©ç”¨ä¸€ç»„å¯¹è±¡æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢æœ€åˆä½¿ç”¨é›¶å€¼è¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡ä¸å¤šæ¨¡æ€ç‰¹å¾çš„è¿­ä»£äº¤äº’é€æ¸å­¦ä¹ ç›®æ ‡ä½ç½®ä¿¡æ¯ï¼Œç”¨äºç©ºé—´å’Œæ—¶é—´å®šä½ã€‚å°½ç®¡ç®€å•ï¼Œä½†è¿™äº›é›¶å¯¹è±¡æŸ¥è¯¢ç”±äºç¼ºä¹ç›®æ ‡ç‰¹å®šçº¿ç´¢ï¼Œåœ¨å¤æ‚åœºæ™¯ï¼ˆä¾‹å¦‚æœ‰å¹²æ‰°ç‰©æˆ–é®æŒ¡ç‰©ï¼‰ä¸­ï¼Œå¾ˆéš¾ä»ä¸å¤šæ¨¡æ€ç‰¹å¾çš„äº¤äº’ä¸­å­¦ä¹ å‡ºæœ‰åŒºåˆ†åŠ›çš„ç›®æ ‡ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç”¨äºSTVGçš„ç›®æ ‡æ„ŸçŸ¥Transformerï¼ˆTA-STVGï¼‰ï¼Œå®ƒæ—¨åœ¨é€šè¿‡æ¢ç´¢ç»™å®šè§†é¢‘æ–‡æœ¬å¯¹ä¸­çš„ç›®æ ‡ç‰¹å®šçº¿ç´¢æ¥è‡ªé€‚åº”åœ°ç”Ÿæˆå¯¹è±¡æŸ¥è¯¢ï¼Œä»¥æé«˜STVGçš„æ€§èƒ½ã€‚å…³é”®åœ¨äºä¸¤ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¨¡å—ï¼ŒåŒ…æ‹¬æ–‡æœ¬å¼•å¯¼çš„æ—¶é—´é‡‡æ ·ï¼ˆTTSï¼‰å’Œå±æ€§æ„ŸçŸ¥çš„ç©ºé—´æ¿€æ´»ï¼ˆASAï¼‰ï¼Œå®ƒä»¬ååŒå·¥ä½œã€‚å‰è€…ä¸“æ³¨äºåˆ©ç”¨æ•´ä½“æ–‡æœ¬ä¿¡æ¯ä»è§†é¢‘ä¸­é€‰å–ç›®æ ‡ç›¸å…³çš„æ—¶åºçº¿ç´¢ï¼Œè€Œåè€…åˆ™æ—¨åœ¨è¿›ä¸€æ­¥æŒ–æ˜ä¹‹å‰ç›®æ ‡æ„ŸçŸ¥æ—¶åºçº¿ç´¢çš„å¯¹è±¡çš„ç²¾ç»†è§†è§‰å±æ€§ä¿¡æ¯ï¼Œè¿™åº”ç”¨äºå¯¹è±¡æŸ¥è¯¢çš„åˆå§‹åŒ–ã€‚ä¸ç°æœ‰ä½¿ç”¨é›¶åˆå§‹åŒ–æŸ¥è¯¢çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„TA-STVGä¸­çš„å¯¹è±¡æŸ¥è¯¢ç›´æ¥ä»ç»™å®šçš„è§†é¢‘æ–‡æœ¬å¯¹ä¸­ç”Ÿæˆï¼Œè‡ªç„¶æºå¸¦ç›®æ ‡ç‰¹å®šçº¿ç´¢ï¼Œä½¿å…¶è‡ªé€‚åº”å¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°ä¸å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œäº¤äº’ï¼Œä»è€Œå­¦ä¹ æ›´å¤šæœ‰åŒºåˆ†åŠ›çš„ä¿¡æ¯ä»¥æé«˜STVGã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTA-STVGè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11168v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeråœ¨STVGï¼ˆç©ºé—´å’Œæ—¶é—´è§†é¢‘å®šä½ï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é›¶åˆå§‹åŒ–çš„å¯¹è±¡æŸ¥è¯¢ï¼Œé€šè¿‡ä¸å¤šæ¨¡æ€ç‰¹å¾çš„è¿­ä»£äº¤äº’æ¥å­¦ä¹ ç›®æ ‡ä½ç½®ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­ï¼ˆå¦‚æœ‰å¹²æ‰°ç‰©æˆ–é®æŒ¡ï¼‰éš¾ä»¥å­¦ä¹ åŒºåˆ†æ€§ç›®æ ‡ä¿¡æ¯ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç›®æ ‡æ„ŸçŸ¥Transformerï¼ˆTA-STVGï¼‰ï¼Œå…¶é€šè¿‡æ¢ç´¢è§†é¢‘æ–‡æœ¬å¯¹ä¸­çš„ç›®æ ‡ç‰¹å®šçº¿ç´¢æ¥è‡ªé€‚åº”ç”Ÿæˆå¯¹è±¡æŸ¥è¯¢ï¼Œä»¥æé«˜STVGæ€§èƒ½ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªç®€å•æœ‰æ•ˆçš„æ¨¡å—ï¼šæ–‡æœ¬å¼•å¯¼çš„ä¸´æ—¶é‡‡æ ·ï¼ˆTTSï¼‰å’Œå±æ€§æ„ŸçŸ¥çš„ç©ºé—´æ¿€æ´»ï¼ˆASAï¼‰ã€‚TTSä¸“æ³¨äºåˆ©ç”¨æ•´ä½“æ–‡æœ¬ä¿¡æ¯ä»è§†é¢‘ä¸­é€‰å–ç›®æ ‡ç›¸å…³çš„ä¸´æ—¶çº¿ç´¢ï¼Œè€ŒASAåˆ™è¿›ä¸€æ­¥åˆ©ç”¨ä¹‹å‰çš„ç›®æ ‡æ„ŸçŸ¥ä¸´æ—¶çº¿ç´¢ä¸­çš„ç²¾ç»†ç²’åº¦è§†è§‰å±æ€§ä¿¡æ¯ï¼Œç”¨äºå¯¹è±¡æŸ¥è¯¢åˆå§‹åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTA-STVGç›´æ¥ä»ç»™å®šçš„è§†é¢‘æ–‡æœ¬å¯¹ä¸­ç”Ÿæˆå¯¹è±¡æŸ¥è¯¢ï¼Œæºå¸¦ç›®æ ‡ç‰¹å®šçº¿ç´¢ï¼Œä½¿å…¶èƒ½æ›´å¥½åœ°ä¸å¤šæ¨¡æ€ç‰¹å¾äº¤äº’ï¼Œå­¦ä¹ æ›´å¤šåŒºåˆ†æ€§ä¿¡æ¯ï¼Œæé«˜STVGæ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTA-STVGè¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeråœ¨STVGé¢†åŸŸæ­£å¸å¼•è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå› å…¶ç«¯åˆ°ç«¯çš„ç®¡é“å’Œå‰æ™¯è‰¯å¥½çš„ç»“æœã€‚</li>
<li>ç°æœ‰Transformer-based STVGæ–¹æ³•é€šå¸¸ä½¿ç”¨é›¶åˆå§‹åŒ–çš„å¯¹è±¡æŸ¥è¯¢ï¼Œè¿™åœ¨å¤æ‚åœºæ™¯ä¸­éš¾ä»¥å­¦ä¹ åŒºåˆ†æ€§ç›®æ ‡ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥çš„æ–°å‹Target-Aware Transformerï¼ˆTA-STVGï¼‰é€šè¿‡æ¢ç´¢è§†é¢‘æ–‡æœ¬å¯¹ä¸­çš„ç›®æ ‡ç‰¹å®šçº¿ç´¢æ¥æ”¹è¿›STVGã€‚</li>
<li>TA-STVGåŒ…å«ä¸¤ä¸ªæœ‰æ•ˆæ¨¡å—ï¼šæ–‡æœ¬å¼•å¯¼çš„ä¸´æ—¶é‡‡æ ·ï¼ˆTTSï¼‰å’Œå±æ€§æ„ŸçŸ¥çš„ç©ºé—´æ¿€æ´»ï¼ˆASAï¼‰ã€‚</li>
<li>TTSæ¨¡å—åˆ©ç”¨æ•´ä½“æ–‡æœ¬ä¿¡æ¯ä»è§†é¢‘ä¸­é€‰å–ç›®æ ‡ç›¸å…³çš„ä¸´æ—¶çº¿ç´¢ã€‚</li>
<li>ASAæ¨¡å—åˆ©ç”¨ä¹‹å‰çš„ç›®æ ‡æ„ŸçŸ¥ä¸´æ—¶çº¿ç´¢ä¸­çš„ç²¾ç»†ç²’åº¦è§†è§‰å±æ€§ä¿¡æ¯ï¼Œç”¨äºå¯¹è±¡æŸ¥è¯¢åˆå§‹åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab54b509c268f7be5a37828037be37de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d235ba5177b4e07295bd3545db45016d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbec2c51778d5ad8ca61daa60cdc823e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5208c6caddea04e8cad5dacba086304c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cac494c1734d6bb42ee0e285732b027.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FELLE-Autoregressive-Speech-Synthesis-with-Token-Wise-Coarse-to-Fine-Flow-Matching"><a href="#FELLE-Autoregressive-Speech-Synthesis-with-Token-Wise-Coarse-to-Fine-Flow-Matching" class="headerlink" title="FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine   Flow Matching"></a>FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine   Flow Matching</h2><p><strong>Authors:Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin</strong></p>
<p>To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language modelâ€™s output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in <a target="_blank" rel="noopener" href="https://aka.ms/felle">https://aka.ms/felle</a>. </p>
<blockquote>
<p>ä¸ºæ¨åŠ¨è¿ç»­å€¼ä»¤ç‰Œå»ºæ¨¡å’Œæ—¶é—´è¿è´¯æ€§æ‰§è¡Œï¼Œæˆ‘ä»¬æå‡ºäº†FELLEï¼Œè¿™æ˜¯ä¸€ç§å°†è¯­è¨€å»ºæ¨¡ä¸ä»¤ç‰Œçº§æµåŒ¹é…ç›¸ç»“åˆçš„è‡ªå›å½’æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨å’ŒæµåŒ¹é…çš„ç”Ÿæˆæ•ˆç‡ï¼ŒFELLEæœ‰æ•ˆåœ°é¢„æµ‹äº†è¿ç»­å€¼ä»¤ç‰Œï¼ˆæ¢…å°”é¢‘è°±å›¾ï¼‰ã€‚å¯¹äºæ¯ä¸ªè¿ç»­å€¼ä»¤ç‰Œï¼ŒFELLEé€šè¿‡ç»“åˆå‰ä¸€æ­¥çš„ä¿¡æ¯ï¼Œå¯¹æµåŒ¹é…ä¸­çš„ä¸€èˆ¬å…ˆéªŒåˆ†å¸ƒè¿›è¡Œä¿®æ”¹ï¼Œæé«˜äº†è¿è´¯æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºåˆæˆè´¨é‡ï¼ŒFELLEå¼•å…¥äº†ä¸€ç§ç”±ç²—åˆ°ç»†çš„æµåŒ¹é…æœºåˆ¶ï¼Œä»¥å±‚æ¬¡åŒ–çš„æ–¹å¼ç”Ÿæˆè¿ç»­å€¼ä»¤ç‰Œï¼Œä»¥è¯­è¨€æ¨¡å‹çš„è¾“å‡ºä¸ºæ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è‡ªå›å½’æ¢…å°”é¢‘è°±å»ºæ¨¡ä¸­èå…¥æµåŒ¹é…æŠ€æœ¯å…·æœ‰æ½œåŠ›ï¼Œèƒ½æ˜¾è‘—æé«˜TTSç”Ÿæˆè´¨é‡ï¼Œè¯¦æƒ…å¯è§<a target="_blank" rel="noopener" href="https://aka.ms/felle%E3%80%82">https://aka.ms/felleã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11128v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºFELLEçš„è‡ªå›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è¯­è¨€å»ºæ¨¡å’Œä»¤ç‰Œçº§æµåŒ¹é…æŠ€æœ¯ï¼Œç”¨äºæ¨è¿›è¿ç»­å€¼ä»¤ç‰Œå»ºæ¨¡å’Œæ—¶é—´è¿è´¯æ€§æ‰§è¡Œã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨å’ŒæµåŒ¹é…çš„ç”Ÿæˆæ•ˆèƒ½ï¼ŒFELLEèƒ½å¤Ÿæœ‰æ•ˆåœ°é¢„æµ‹è¿ç»­å€¼ä»¤ç‰Œï¼ˆæ¢…å°”é¢‘è°±å›¾ï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå‰ä¸€æ­¥çš„ä¿¡æ¯ï¼Œæ”¹è¿›äº†æµåŒ¹é…ä¸­çš„ä¸€èˆ¬å…ˆéªŒåˆ†å¸ƒï¼Œæé«˜äº†è¿è´¯æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºåˆæˆè´¨é‡ï¼ŒFELLEå¼•å…¥äº†ä»ç²—åˆ°ç»†çš„æµåŒ¹é…æœºåˆ¶ï¼Œä»¥å±‚æ¬¡åŒ–æ–¹å¼ç”Ÿæˆè¿ç»­å€¼ä»¤ç‰Œï¼Œå–å†³äºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è‡ªå›å½’æ¢…å°”é¢‘è°±å›¾å»ºæ¨¡ä¸­èå…¥æµåŒ¹é…æŠ€æœ¯å…·æœ‰æ½œåŠ›ï¼Œèƒ½æ˜¾è‘—æé«˜TTSç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FELLEæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œç»“åˆäº†è¯­è¨€å»ºæ¨¡å’Œä»¤ç‰Œçº§æµåŒ¹é…ã€‚</li>
<li>å®ƒåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨åŠæµåŒ¹é…çš„ç”Ÿæˆæ•ˆèƒ½è¿›è¡Œæœ‰æ•ˆé¢„æµ‹è¿ç»­å€¼ä»¤ç‰Œï¼ˆæ¢…å°”é¢‘è°±å›¾ï¼‰ã€‚</li>
<li>FELLEé€šè¿‡ç»“åˆå‰ä¸€æ­¥ä¿¡æ¯æ”¹è¿›äº†ä¸€èˆ¬å…ˆéªŒåˆ†å¸ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¿è´¯æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†ä»ç²—åˆ°ç»†çš„æµåŒ¹é…æœºåˆ¶ï¼Œä»¥å±‚æ¬¡åŒ–æ–¹å¼ç”Ÿæˆè¿ç»­å€¼ä»¤ç‰Œï¼Œè¿›ä¸€æ­¥æå‡åˆæˆè´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†åœ¨è‡ªå›å½’æ¢…å°”é¢‘è°±å›¾å»ºæ¨¡ä¸­èå…¥æµåŒ¹é…æŠ€æœ¯çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒTTSç”Ÿæˆè´¨é‡å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1e4ea4136bef5e30889124b9307f1d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-831b11602e189f432e3542f573a7598a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SyncSpeech-Low-Latency-and-Efficient-Dual-Stream-Text-to-Speech-based-on-Temporal-Masked-Transformer"><a href="#SyncSpeech-Low-Latency-and-Efficient-Dual-Stream-Text-to-Speech-based-on-Temporal-Masked-Transformer" class="headerlink" title="SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based   on Temporal Masked Transformer"></a>SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based   on Temporal Masked Transformer</h2><p><strong>Authors:Zhengyan Sheng, Zhihao Du, Shiliang Zhang, Zhijie Yan, Yexin Yang, Zhenhua Ling</strong></p>
<p>This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at <a target="_blank" rel="noopener" href="https://syncspeech.github.io/%7D%7Bhttps://SyncSpeech.github.io/">https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæµæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹SyncSpeechï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¥æ”¶æ¥è‡ªä¸Šæ¸¸æ¨¡å‹çš„æµå¼æ–‡æœ¬è¾“å…¥ï¼ŒåŒæ—¶ç”Ÿæˆæµå¼è¯­éŸ³ï¼Œä¾¿äºä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ— ç¼äº¤äº’ã€‚SyncSpeechå…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼šä½å»¶è¿Ÿï¼Œå› ä¸ºåœ¨æ¥æ”¶åˆ°ç¬¬äºŒä¸ªæ–‡æœ¬æ ‡è®°æ—¶å°±å¼€å§‹ç”Ÿæˆæµå¼è¯­éŸ³ï¼›é«˜æ•ˆç‡ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨ä¸€æ­¥ä¸­è§£ç æ¯ä¸ªåˆ°è¾¾çš„æ–‡æœ¬æ ‡è®°å¯¹åº”çš„æ‰€æœ‰è¯­éŸ³æ ‡è®°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´æ©æ¨¡è½¬æ¢å™¨ä½œä¸ºSyncSpeechçš„éª¨å¹²ï¼Œç»“åˆæ ‡è®°çº§æŒç»­æ—¶é—´é¢„æµ‹æ¥é¢„æµ‹è¯­éŸ³æ ‡è®°å’Œä¸‹ä¸€ä¸ªæ­¥éª¤çš„æŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬åœ¨è‹±è¯­å’Œæ™®é€šè¯æ•°æ®é›†ä¸Šè¯„ä¼°äº†SyncSpeechã€‚ä¸æœ€è¿‘çš„åŒæµTTSæ¨¡å‹ç›¸æ¯”ï¼ŒSyncSpeechæ˜¾è‘—å‡å°‘äº†è¯­éŸ³æ ‡è®°çš„ç¬¬ä¸€åŒ…å»¶è¿Ÿå¹¶åŠ é€Ÿäº†å®æ—¶å› å­ã€‚è€Œä¸”ï¼Œåœ¨ç›¸åŒçš„æ•°æ®è§„æ¨¡ä¸‹ï¼ŒSyncSpeechåœ¨è¯­éŸ³è´¨é‡å’Œç¨³å¥æ€§æ–¹é¢è¾¾åˆ°äº†ä¸ä¼ ç»ŸåŸºäºè‡ªåŠ¨å›å½’çš„TTSæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://syncspeech.github.ioä¸Šæ‰¾åˆ°./">https://SyncSpeech.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11094v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŒæµæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹SyncSpeechï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¥æ”¶æ¥è‡ªä¸Šæ¸¸æ¨¡å‹çš„æµå¼æ–‡æœ¬è¾“å…¥å¹¶åŒæ—¶ç”Ÿæˆæµå¼è¯­éŸ³ï¼Œä¾¿äºä¸å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼äº¤äº’ã€‚SyncSpeechå…·æœ‰ä½å»¶è¿Ÿå’Œé«˜æ•ˆç‡çš„ä¼˜ç‚¹ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨ä¸´æ—¶æ©ç è½¬æ¢å™¨ä½œä¸ºéª¨æ¶å¹¶ç»“åˆæ ‡è®°çº§æŒç»­æ—¶é—´é¢„æµ‹æ¥å®ç°ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè¯­éŸ³çš„è´¨é‡ã€‚åœ¨è‹±æ–‡å’Œæ™®é€šè¯æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSyncSpeechæ˜¾è‘—å‡å°‘äº†è¯­éŸ³æ ‡è®°çš„ç¬¬ä¸€åŒ…å»¶è¿Ÿå¹¶æé«˜äº†å®æ—¶å› å­ã€‚ä¸ä¼ ç»ŸåŸºäºè‡ªå›å½’çš„TTSæ¨¡å‹ç›¸æ¯”ï¼ŒSyncSpeechåœ¨è¯­éŸ³è´¨é‡å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncSpeechæ˜¯ä¸€ç§åŒæµTTSæ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°æµå¼æ–‡æœ¬è¾“å…¥å’Œè¯­éŸ³ç”Ÿæˆçš„æ— ç¼äº¤äº’ã€‚</li>
<li>SyncSpeechå…·æœ‰ä½å»¶è¿Ÿå’Œé«˜æ•ˆç‡çš„ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿåœ¨æ¥æ”¶åˆ°ç¬¬äºŒä¸ªæ–‡æœ¬æ ‡è®°æ—¶å¼€å§‹ç”Ÿæˆæµå¼è¯­éŸ³ã€‚</li>
<li>SyncSpeeché€šè¿‡ä½¿ç”¨ä¸´æ—¶æ©ç è½¬æ¢å™¨ä½œä¸ºéª¨æ¶å¹¶ç»“åˆæ ‡è®°çº§æŒç»­æ—¶é—´é¢„æµ‹æ¥å®ç°å…¶é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œè¯­éŸ³ç”Ÿæˆè´¨é‡ã€‚</li>
<li>SyncSpeechåœ¨è‹±æ–‡å’Œæ™®é€šè¯æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†è¯­éŸ³æ ‡è®°çš„ç¬¬ä¸€åŒ…å»¶è¿Ÿã€‚</li>
<li>SyncSpeechçš„å®æ—¶å› å­å¾—åˆ°äº†æé«˜ï¼Œä¸ä¼ ç»ŸåŸºäºè‡ªå›å½’çš„TTSæ¨¡å‹ç›¸æ¯”ï¼Œå…¶åœ¨è¯­éŸ³è´¨é‡å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f51fc94d07b54ba23454ff241f4df6c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a3b955a6a813e99e756fe05814b96f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f6c264c51f4b98b4ac9c82f738ddbed.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Less-is-More-for-Synthetic-Speech-Detection-in-the-Wild"><a href="#Less-is-More-for-Synthetic-Speech-Detection-in-the-Wild" class="headerlink" title="Less is More for Synthetic Speech Detection in the Wild"></a>Less is More for Synthetic Speech Detection in the Wild</h2><p><strong>Authors:Ashi Garg, Zexin Cai, Henry Li Xinyuan, Leibny Paola GarcÃ­a-Perera, Kevin Duh, Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews</strong></p>
<p>Driven by advances in self-supervised learning for speech, state-of-the-art synthetic speech detectors have achieved low error rates on popular benchmarks such as ASVspoof. However, prior benchmarks do not address the wide range of real-world variability in speech. Are reported error rates realistic in real-world conditions? To assess detector failure modes and robustness under controlled distribution shifts, we introduce ShiftySpeech, a benchmark with more than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12 vocoders, and 3 languages. We found that all distribution shifts degraded model performance, and contrary to prior findings, training on more vocoders, speakers, or with data augmentation did not guarantee better generalization. In fact, we found that training on less diverse data resulted in better generalization, and that a detector fit using samples from a single carefully selected vocoder and a small number of speakers, without data augmentations, achieved state-of-the-art results on the challenging In-the-Wild benchmark. </p>
<blockquote>
<p>å¾—ç›Šäºè¯­éŸ³è‡ªç›‘ç£å­¦ä¹ çš„è¿›æ­¥ï¼Œæœ€å…ˆè¿›çš„åˆæˆè¯­éŸ³æ£€æµ‹å™¨åœ¨è¯¸å¦‚ASVspoofç­‰æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¯¯å·®ç‡éå¸¸ä½ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªè§£å†³çœŸå®ä¸–ç•Œä¸­è¯­éŸ³çš„å¹¿æ³›å˜åŒ–èŒƒå›´é—®é¢˜ã€‚åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹ï¼ŒæŠ¥å‘Šçš„è¯¯å·®ç‡æ˜¯å¦ç°å®ï¼Ÿä¸ºäº†è¯„ä¼°æ£€æµ‹å™¨åœ¨å—æ§åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ•…éšœæ¨¡å¼å’Œç¨³å¥æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ShiftySpeechåŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡3000å°æ—¶çš„åˆæˆè¯­éŸ³ï¼Œæ¶µç›–7ä¸ªé¢†åŸŸã€6ä¸ªæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿå’Œ3ç§è¯­è¨€ã€‚æˆ‘ä»¬å‘ç°æ‰€æœ‰çš„åˆ†å¸ƒå˜åŒ–éƒ½ä¼šé™ä½æ¨¡å‹æ€§èƒ½ï¼Œä¸å‰äººçš„ç ”ç©¶ç»“æœç›¸åï¼Œä½¿ç”¨æ›´å¤šç¼–ç å™¨ã€æ›´å¤šè¯´è¯è€…æˆ–æ•°æ®å¢å¼ºè¿›è¡Œè®­ç»ƒå¹¶ä¸èƒ½ä¿è¯æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨è¾ƒå°‘å¤šæ ·åŒ–æ•°æ®ä¸Šè®­ç»ƒä¼šå¯¼è‡´æ›´å¥½çš„æ³›åŒ–æ•ˆæœï¼Œä½¿ç”¨æ¥è‡ªå•ä¸ªç²¾å¿ƒæŒ‘é€‰çš„ç¼–ç å™¨å’Œå°æ•°é‡è¯´è¯è€…çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒçš„æ£€æµ‹å™¨ï¼Œåœ¨ä¸è¿›è¡Œæ•°æ®å¢å¼ºçš„æƒ…å†µä¸‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„In-the-WildåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05674v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è‡ªç›‘ç£å­¦ä¹ åœ¨è¯­éŸ³é¢†åŸŸçš„è¿›æ­¥ï¼Œæœ€å…ˆè¿›çš„è¯­éŸ³åˆæˆæ£€æµ‹å™¨åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå¦‚ASVspoofï¼‰ä¸Šçš„é”™è¯¯ç‡å¾ˆä½ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½æ¶µç›–ç°å®ä¸–ç•Œä¸­è¯­éŸ³çš„å¹¿æ³›å˜åŒ–ã€‚é€šè¿‡å¼•å…¥ShiftySpeechåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ£€æµ‹å™¨åœ¨å—æ§åˆ†å¸ƒå˜åŒ–ä¸‹çš„å¤±è´¥æ¨¡å¼å’Œç¨³å¥æ€§ã€‚å‘ç°æ‰€æœ‰åˆ†å¸ƒå˜åŒ–éƒ½ä¼šé™ä½æ¨¡å‹æ€§èƒ½ï¼Œä¸å…ˆå‰çš„ç ”ç©¶ç»“æœç›¸åï¼Œè®­ç»ƒæ›´å¤šçš„vocoderã€æ¼”è®²è€…æˆ–é‡‡ç”¨æ•°æ®å¢å¼ºå¹¶ä¸ä¸€å®šèƒ½æ›´å¥½åœ°æ¨å¹¿ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å‘ç°è®­ç»ƒåœ¨è¾ƒå°‘å¤šæ ·åŒ–çš„æ•°æ®ä¸Šèƒ½æ›´å¥½åœ°æ¨å¹¿ï¼Œä½¿ç”¨å•ä¸€ç²¾å¿ƒé€‰æ‹©çš„vocoderå’Œå°æ•°é‡æ¼”è®²è€…çš„æ£€æµ‹å™¨æ ·æœ¬ï¼Œåœ¨ä¸è¿›è¡Œæ•°æ®å¢å¼ºçš„æƒ…å†µä¸‹ï¼Œèƒ½åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„In-the-WildåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ æ¨åŠ¨äº†è¯­éŸ³åˆæˆæ£€æµ‹å™¨çš„å…ˆè¿›æ€§èƒ½ï¼Œåœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„é”™è¯¯ç‡è¾ƒä½ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½æ¶µç›–ç°å®ä¸–ç•Œä¸­è¯­éŸ³çš„å¹¿æ³›å˜åŒ–ã€‚</li>
<li>å¼•å…¥ShiftySpeechåŸºå‡†æµ‹è¯•ä»¥è¯„ä¼°æ£€æµ‹å™¨åœ¨åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>æ‰€æœ‰åˆ†å¸ƒå˜åŒ–éƒ½ä¼šå½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒæ›´å¤šçš„vocoderã€æ¼”è®²è€…æˆ–é‡‡ç”¨æ•°æ®å¢å¼ºå¹¶ä¸ä¸€å®šèƒ½æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨è¾ƒå°‘å¤šæ ·åŒ–çš„æ•°æ®ä¸Šè®­ç»ƒæ£€æµ‹å™¨èƒ½æ›´å¥½åœ°æ¨å¹¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-afde1104c8eead48cd642c35fef8e0f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62deff9d42703da59aa2483c104e556d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f359ed81f85f57df547921d955c7c9aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed0b56c7fcf2fd42d9e9f92fd5b718a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed074330ef96712fce2bec8e081250b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Compressed-â€˜CMB-liteâ€™-Likelihoods-Using-Automatic-Differentiation"><a href="#Compressed-â€˜CMB-liteâ€™-Likelihoods-Using-Automatic-Differentiation" class="headerlink" title="Compressed â€˜CMB-liteâ€™ Likelihoods Using Automatic Differentiation"></a>Compressed â€˜CMB-liteâ€™ Likelihoods Using Automatic Differentiation</h2><p><strong>Authors:L. Balkenhol</strong></p>
<p>The compression of multi-frequency cosmic microwave background (CMB) power spectrum measurements into a series of foreground-marginalised CMB-only band powers allows for the construction of faster and more easily interpretable â€˜liteâ€™ likelihoods. However, obtaining the compressed data vector is computationally expensive and yields a covariance matrix with sampling noise. In this work, we present an implementation of the CMB-lite framework relying on automatic differentiation. The technique presented reduces the computational cost of the lite likelihood construction to one minimisation and one Hessian evaluation, which run on a personal computer in about a minute. We demonstrate the efficiency and accuracy of this procedure by applying it to the differentiable SPT-3G 2018 TT&#x2F;TE&#x2F;EE likelihood from the candl library. We find good agreement between the marginalised posteriors of cosmological parameters yielded by the resulting lite likelihood and the reference multi-frequency version for all cosmological models tested; the best-fit values shift by $&lt;0.1,\sigma$, where $\sigma$ is the width of the multi-frequency posterior, and the inferred parameter error bars match to within $&lt;10%$. We publicly release the SPT-3G 2018 TT&#x2F;TE&#x2F;EE lite likelihood and a python notebook showing its construction at <a target="_blank" rel="noopener" href="https://github.com/Lbalkenhol/candl">https://github.com/Lbalkenhol/candl</a> . </p>
<blockquote>
<p>å°†å¤šé¢‘å®‡å®™å¾®æ³¢èƒŒæ™¯ï¼ˆCMBï¼‰åŠŸç‡è°±æµ‹é‡å€¼å‹ç¼©æˆä¸€ç³»åˆ—ä»…å«CMBçš„å‰ç½®è¾¹ç¼˜åŒ–é¢‘æ®µåŠŸç‡å€¼ï¼Œå¯ä»¥æ„å»ºæ›´å¿«ã€æ›´æ˜“è§£è¯»çš„â€œç²¾ç®€ç‰ˆâ€æ¦‚ç‡æ¨¡å‹ã€‚ç„¶è€Œï¼Œè·å–å‹ç¼©æ•°æ®å‘é‡è®¡ç®—é‡å¤§ä¸”äº§ç”Ÿçš„åæ–¹å·®çŸ©é˜µå­˜åœ¨é‡‡æ ·å™ªå£°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¾èµ–è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯çš„CMB-liteæ¡†æ¶å®ç°æ–¹æ³•ã€‚æ‰€æå‡ºçš„æŠ€æœ¯å°†ç²¾ç®€ç‰ˆæ¦‚ç‡æ¨¡å‹çš„æ„å»ºçš„è®¡ç®—æˆæœ¬é™ä½è‡³ä»…éœ€ä¸€æ¬¡æœ€å°åŒ–å’Œä¸€æ¬¡Hessianè¯„ä¼°ï¼Œå¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šå¤§çº¦ä¸€åˆ†é’Ÿå†…è¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶åº”ç”¨äºå¯å¾®åˆ†çš„SPT-3G 2018 TT&#x2F;TE&#x2F;EEæ¦‚ç‡æ¨¡å‹ï¼ˆæ¥è‡ªcandlåº“ï¼‰æ¥å±•ç¤ºè¯¥ç¨‹åºçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæµ‹è¯•çš„æ‰€æœ‰å®‡å®™å­¦æ¨¡å‹ï¼Œç”±æ‰€å¾—ç²¾ç®€ç‰ˆæ¦‚ç‡æ¨¡å‹äº§ç”Ÿçš„è¾¹ç¼˜åŒ–å®‡å®™å­¦å‚æ•°çš„åéªŒåˆ†å¸ƒä¸å‚è€ƒå¤šé¢‘ç‰ˆæœ¬çš„ç»“æœéå¸¸ä¸€è‡´ï¼›æœ€ä½³æ‹Ÿåˆå€¼ç›¸å¯¹äºå¤šé¢‘åéªŒçš„å®½åº¦ç§»åŠ¨å°äº$ 0.1\sigma $ï¼Œæ¨æ–­å‡ºçš„å‚æ•°è¯¯å·®èŒƒå›´åŒ¹é…åº¦åœ¨å°äº10%ä»¥å†…ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lbalkenhol/candl">https://github.com/Lbalkenhol/candl</a>ä¸Šå…¬å¼€å‘å¸ƒäº†SPT-3G 2018 TT&#x2F;TE&#x2F;EEç²¾ç®€ç‰ˆæ¦‚ç‡æ¨¡å‹å’Œå±•ç¤ºå…¶æ„å»ºçš„Pythonç¬”è®°æœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00826v2">PDF</a> 8 pages, 4 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯å®ç°å®‡å®™å¾®æ³¢èƒŒæ™¯è¾å°„ï¼ˆCMBï¼‰åŠŸç‡è°±æµ‹é‡æ•°æ®çš„å‹ç¼©æ–¹æ³•ï¼Œæ„å»ºå‡ºå¿«é€Ÿä¸”æ˜“äºè§£é‡Šçš„â€œè½»é‡çº§â€ä¼¼ç„¶å‡½æ•°ã€‚è¯¥æ–¹æ³•é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä»…éœ€è¦ä¸€æ¬¡æœ€å°åŒ–å’Œä¸€æ¬¡Hessianè¯„ä¼°ï¼Œå¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šå¤§çº¦ä¸€åˆ†é’Ÿå†…è¿è¡Œã€‚ä½œè€…é€šè¿‡åº”ç”¨è¯¥æ–¹æ³•äºå¯å¾®åˆ†çš„SPT-3G 2018 TT&#x2F;TE&#x2F;EEä¼¼ç„¶å‡½æ•°åº“ï¼ŒéªŒè¯äº†å…¶æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è½»é‡çº§ä¼¼ç„¶å‡½æ•°å¾—å‡ºçš„å®‡å®™å­¦å‚æ•°è¾¹ç¼˜åˆ†å¸ƒä¸å¤šé¢‘ç‰ˆæœ¬çš„ç»“æœåœ¨æ‰€æœ‰æµ‹è¯•çš„å®‡å®™å­¦æ¨¡å‹ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­ä»‹ç»äº†ä¸€ç§åŸºäºè‡ªåŠ¨å¾®åˆ†æŠ€æœ¯çš„CMB-liteæ¡†æ¶ï¼Œç”¨äºå‹ç¼©å¤šé¢‘å®‡å®™å¾®æ³¢èƒŒæ™¯ï¼ˆCMBï¼‰åŠŸç‡è°±æµ‹é‡æ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•æ„å»ºå‡ºå¿«é€Ÿä¸”æ˜“äºè§£é‡Šçš„â€œè½»é‡çº§â€ä¼¼ç„¶å‡½æ•°ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…éœ€è¦ä¸€æ¬¡æœ€å°åŒ–å’Œä¸€æ¬¡Hessianè¯„ä¼°ï¼Œå¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šå¿«é€Ÿè¿è¡Œã€‚</li>
<li>ä½œè€…é€šè¿‡åº”ç”¨è¯¥æ–¹æ³•äºSPT-3G 2018 TT&#x2F;TE&#x2F;EEä¼¼ç„¶å‡½æ•°åº“è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜äº†å…¶æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è½»é‡çº§ä¼¼ç„¶å‡½æ•°å¾—å‡ºçš„å®‡å®™å­¦å‚æ•°è¾¹ç¼˜åˆ†å¸ƒä¸å¤šé¢‘ç‰ˆæœ¬çš„ç»“æœä¸€è‡´æ€§å¥½ã€‚</li>
<li>æ–‡ä¸­å…¬å¼€å‘å¸ƒäº†SPT-3G 2018 TT&#x2F;TE&#x2F;EEè½»é‡çº§ä¼¼ç„¶å‡½æ•°å’Œä¸€ä¸ªå±•ç¤ºå…¶æ„å»ºçš„Pythonç¬”è®°æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºåŠ å¿«å®‡å®™å­¦å‚æ•°æ¨æ–­çš„é€Ÿåº¦ï¼Œå¹¶ä½¿å¾—ç»“æœæ›´å®¹æ˜“è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77ffdfb04ceb1d38660e9f15848b0c25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ad202aaee066d74db652ac01605b631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00cde7bc808d788602d6f8acd33878d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6188e4b07f6d8e3271f65954c044ec74.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiTTo-TTS-Diffusion-Transformers-for-Scalable-Text-to-Speech-without-Domain-Specific-Factors"><a href="#DiTTo-TTS-Diffusion-Transformers-for-Scalable-Text-to-Speech-without-Domain-Specific-Factors" class="headerlink" title="DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without   Domain-Specific Factors"></a>DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without   Domain-Specific Factors</h2><p><strong>Authors:Keon Lee, Dong Won Kim, Jaehyeon Kim, Seungjun Chung, Jaewoong Cho</strong></p>
<p>Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in text-to-speech (TTS) limits scalability and access from other fields. While recent studies show potential in removing these domain-specific factors, performance remains suboptimal. In this work, we introduce DiTTo-TTS, a Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based TTS can achieve state-of-the-art performance without domain-specific factors. Through rigorous analysis and empirical exploration, we find that (1) DiT with minimal modifications outperforms U-Net, (2) variable-length modeling with a speech length predictor significantly improves results over fixed-length approaches, and (3) conditions like semantic alignment in speech latent representations are key to further enhancement. By scaling our training data to 82K hours and the model size to 790M parameters, we achieve superior or comparable zero-shot performance to state-of-the-art TTS models in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific factors. Speech samples are available at <a target="_blank" rel="noopener" href="https://ditto-tts.github.io/">https://ditto-tts.github.io</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å¤šç§æ¨¡æ€çš„å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ä¸­å¯¹éŸ³ç´ å’ŒæŒç»­æ—¶é—´çš„ä¾èµ–é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œåœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜å»é™¤è¿™äº›é¢†åŸŸç‰¹å®šå› ç´ å…·æœ‰æ½œåŠ›ï¼Œä½†æ€§èƒ½ä»ç„¶ä¸å°½äººæ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„TTSæ¨¡å‹DiTTo-TTSï¼Œæ—¨åœ¨ç ”ç©¶åŸºäºLDMçš„TTSæ˜¯å¦èƒ½åœ¨æ²¡æœ‰é¢†åŸŸç‰¹å®šå› ç´ çš„æƒ…å†µä¸‹å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡ä¸¥æ ¼çš„åˆ†æå’Œå®è¯æ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°ï¼ˆ1ï¼‰ç¨ä½œä¿®æ”¹çš„DiTä¼˜äºU-Netï¼Œï¼ˆ2ï¼‰ä½¿ç”¨è¯­éŸ³é•¿åº¦é¢„æµ‹å˜é‡çš„å»ºæ¨¡æ–¹æ³•æ˜¾è‘—ä¼˜äºå›ºå®šé•¿åº¦æ–¹æ³•çš„ç»“æœï¼Œï¼ˆ3ï¼‰è¯­éŸ³æ½œåœ¨è¡¨ç¤ºä¸­çš„è¯­ä¹‰å¯¹é½ç­‰æ¡ä»¶æ˜¯è¿›ä¸€æ­¥æ”¹è¿›çš„å…³é”®ã€‚é€šè¿‡å°†è®­ç»ƒæ•°æ®è§„æ¨¡æ‰©å¤§åˆ°8.2ä¸‡å°æ—¶ï¼Œæ¨¡å‹è§„æ¨¡æ‰©å¤§åˆ°7.9äº¿å‚æ•°ï¼Œæˆ‘ä»¬åœ¨è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢å®ç°äº†æˆ–ä¼˜äºæœ€å…ˆè¿›TTSæ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ²¡æœ‰ä¾èµ–é¢†åŸŸç‰¹å®šå› ç´ ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://ditto-tts.github.ioæ‰¾åˆ°./">https://ditto-tts.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11427v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†åŸºäºæ‰©æ•£Transformerï¼ˆDiTï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹DiTTo-TTSã€‚è¯¥ç ”ç©¶å‘ç°åœ¨å¤§è§„æ¨¡è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°çš„æ”¯æŒä¸‹ï¼ŒDiTTo-TTSèƒ½å¤Ÿå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼ŒåŒæ—¶æ— éœ€ä¾èµ–ç‰¹å®šé¢†åŸŸçš„å› ç´ å¦‚éŸ³ç´ å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡åˆ†æå’Œå®éªŒå‘ç°ï¼Œå¯å˜é•¿åº¦å»ºæ¨¡å’Œè¯­éŸ³é•¿åº¦é¢„æµ‹å™¨èƒ½æé«˜æ€§èƒ½ï¼Œè¯­ä¹‰å¯¹é½ç­‰æ¡ä»¶ä¹Ÿæ˜¯è¿›ä¸€æ­¥æé«˜çš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiTTo-TTSæ˜¯åŸºäºæ‰©æ•£Transformerï¼ˆDiTï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è·¨æ¨¡æ€å†…å®¹ç”Ÿæˆä¸­çš„å±€é™æ€§ã€‚</li>
<li>DiTåœ¨è½»å¾®ä¿®æ”¹åè¡¨ç°å‡ºäº†è¶…è¶ŠU-Netçš„æ€§èƒ½ã€‚</li>
<li>å¯å˜é•¿åº¦å»ºæ¨¡ä¸è¯­éŸ³é•¿åº¦é¢„æµ‹å™¨çš„ç»“åˆæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œç›¸è¾ƒäºå›ºå®šé•¿åº¦çš„æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>è¯­ä¹‰å¯¹é½ç­‰æ¡ä»¶åœ¨è¿›ä¸€æ­¥æé«˜è¯­éŸ³è´¨é‡æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®è‡³82Kå°æ—¶å’Œæ¨¡å‹å‚æ•°è‡³790Mï¼ŒDiTTo-TTSåœ¨è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„TTSæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹çš„å…³é”®ä¼˜åŠ¿åœ¨äºå…¶é›¶æ ·æœ¬æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œå¹¶ä¸”ä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„å› ç´ å¦‚éŸ³ç´ å’ŒæŒç»­æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-786c8c43a8d343a376a5be2a1f4b5c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cedefde23348a4ff1be248a1e676c82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-695adb8aee01bb6374f8d7096ad484d1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2543f27f0f2637a9e9b71ad8b1afab40.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Dialogue-based Explanations for Logical Reasoning using Structured   Argumentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-04e49eae87daaae7c03cbe2a05d4be89.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Robotic CBCT Meets Robotic Ultrasound
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11370.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
