<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Robotic CBCT Meets Robotic Ultrasound">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-04e49eae87daaae7c03cbe2a05d4be89.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="Robotic-CBCT-Meets-Robotic-Ultrasound"><a href="#Robotic-CBCT-Meets-Robotic-Ultrasound" class="headerlink" title="Robotic CBCT Meets Robotic Ultrasound"></a>Robotic CBCT Meets Robotic Ultrasound</h2><p><strong>Authors:Feng Li, Yuan Bi, Dianye Huang, Zhongliang Jiang, Nassir Navab</strong></p>
<p>The multi-modality imaging system offers optimal fused images for safe and precise interventions in modern clinical practices, such as computed tomography - ultrasound (CT-US) guidance for needle insertion. However, the limited dexterity and mobility of current imaging devices hinder their integration into standardized workflows and the advancement toward fully autonomous intervention systems. In this paper, we present a novel clinical setup where robotic cone beam computed tomography (CBCT) and robotic US are pre-calibrated and dynamically co-registered, enabling new clinical applications. This setup allows registration-free rigid registration, facilitating multi-modal guided procedures in the absence of tissue deformation. First, a one-time pre-calibration is performed between the systems. To ensure a safe insertion path by highlighting critical vasculature on the 3D CBCT, SAM2 segments vessels from B-mode images, using the Doppler signal as an autonomously generated prompt. Based on the registration, the Doppler image or segmented vessel masks are then mapped onto the CBCT, creating an optimally fused image with comprehensive detail. To validate the system, we used a specially designed phantom, featuring lesions covered by ribs and multiple vessels with simulated moving flow. The mapping error between US and CBCT resulted in an average deviation of 1.72+-0.62 mm. A user study demonstrated the effectiveness of CBCT-US fusion for needle insertion guidance, showing significant improvements in time efficiency, accuracy, and success rate. Needle intervention performance improved by approximately 50% compared to the conventional US-guided workflow. We present the first robotic dual-modality imaging system designed to guide clinical applications. The results show significant performance improvements compared to traditional manual interventions. </p>
<blockquote>
<p>å¤šæ¨¡æ€æˆåƒç³»ç»Ÿä¸ºç°ä»£ä¸´åºŠå®è·µä¸­çš„å®‰å…¨å’Œç²¾ç¡®å¹²é¢„æä¾›äº†æœ€ä½³çš„èåˆå›¾åƒï¼Œä¾‹å¦‚è®¡ç®—æœºæ–­å±‚æ‰«æ-è¶…å£°ï¼ˆCT-USï¼‰å¼•å¯¼çš„é’ˆæ’å…¥ã€‚ç„¶è€Œï¼Œå½“å‰æˆåƒè®¾å¤‡çš„æœ‰é™çµæ´»æ€§å’ŒæœºåŠ¨æ€§é˜»ç¢äº†å…¶èå…¥æ ‡å‡†åŒ–å·¥ä½œæµç¨‹ï¼Œå¹¶é˜»ç¢äº†å‘å®Œå…¨è‡ªä¸»å¹²é¢„ç³»ç»Ÿçš„è¿ˆè¿›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æ–°å‹ä¸´åºŠè®¾ç½®ï¼Œå…¶ä¸­æœºå™¨äººé”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰å’Œæœºå™¨äººè¶…å£°é¢„å…ˆæ ¡å‡†å¹¶åŠ¨æ€å…±æ³¨å†Œï¼Œä»è€Œå¯ç”¨äº†æ–°çš„ä¸´åºŠåº”ç”¨ã€‚è¯¥è®¾ç½®å¯å®ç°å…æ³¨å†Œåˆšæ€§æ³¨å†Œï¼Œåœ¨ä¸å­˜åœ¨ç»„ç»‡å˜å½¢çš„æƒ…å†µä¸‹ï¼Œä¿ƒè¿›å¤šæ¨¡æ€å¼•å¯¼ç¨‹åºã€‚é¦–å…ˆï¼Œåœ¨ç³»ç»Ÿä¹‹é—´è¿›è¡Œä¸€æ¬¡æ€§é¢„æ ¡å‡†ã€‚ä¸ºäº†ç¡®ä¿é€šè¿‡çªå‡ºæ˜¾ç¤º3D CBCTä¸Šçš„å…³é”®è¡€ç®¡æ¥ç¡®ä¿å®‰å…¨çš„æ’å…¥è·¯å¾„ï¼ŒSAM2ä»Bæ¨¡å¼å›¾åƒä¸­åˆ†å‰²è¡€ç®¡ï¼Œå¹¶ä½¿ç”¨å¤šæ™®å‹’ä¿¡å·ä½œä¸ºè‡ªä¸»ç”Ÿæˆçš„æç¤ºã€‚åŸºäºæ³¨å†Œï¼Œå°†å¤šæ™®å‹’å›¾åƒæˆ–åˆ†å‰²çš„è¡€ç®¡æ©æ¨¡æ˜ å°„åˆ°CBCTä¸Šï¼Œåˆ›å»ºå…·æœ‰å…¨é¢ç»†èŠ‚çš„ä¼˜è´¨èåˆå›¾åƒã€‚ä¸ºäº†éªŒè¯ç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„å¹»å½±è¿›è¡Œäº†æµ‹è¯•ï¼Œè¯¥å¹»å½±å…·æœ‰è¢«è‚‹éª¨è¦†ç›–çš„ç—…å˜å’Œæ¨¡æ‹ŸæµåŠ¨çš„å¤šæ¡è¡€ç®¡ã€‚è¶…å£°å’ŒCBCTä¹‹é—´çš„æ˜ å°„è¯¯å·®å¯¼è‡´å¹³å‡åå·®ä¸º1.72+-0.62æ¯«ç±³ã€‚ä¸€é¡¹ç”¨æˆ·ç ”ç©¶è¯æ˜äº†CBCT-USèåˆåœ¨é’ˆæ’å…¥æŒ‡å¯¼ä¸­çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ—¶æ•ˆã€å‡†ç¡®æ€§å’ŒæˆåŠŸç‡æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹å–„ã€‚ä¸ä¼ ç»Ÿçš„è¶…å£°å¼•å¯¼å·¥ä½œæµç¨‹ç›¸æ¯”ï¼Œé’ˆåˆºå¹²é¢„æ€§èƒ½æé«˜äº†çº¦50%ã€‚æˆ‘ä»¬å±•ç¤ºäº†é¦–ä¸ªç”¨äºæŒ‡å¯¼ä¸´åºŠåº”ç”¨çš„æœºå™¨äººåŒæ¨¡æ€æˆåƒç³»ç»Ÿã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„äººå·¥å¹²é¢„ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šæœ‰äº†æ˜¾ç€çš„æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12019v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€æˆåƒç³»ç»Ÿï¼Œèåˆäº†æœºå™¨äººåŒ–è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰å’Œè¶…å£°æ³¢æŠ€æœ¯ã€‚ç³»ç»Ÿèƒ½å¤Ÿå®ç°éåˆšæ€§çš„å®æ—¶èåˆæ³¨å†Œï¼Œæä¾›äº†ä¸‰ç»´ç¯å¢ƒä¸‹å¤šæ¨¡æ€æŒ‡å¯¼çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéª¨éª¼ä¸è½¯ç»„ç»‡å¹¶å­˜çš„ç—…ç¶å¯¼èˆªä¸ç²¾ç¡®çš„ç©¿åˆºå¼•å¯¼æœ‰æ˜¾è‘—çš„æ•ˆèƒ½æ”¹è¿›ã€‚å…¶ä¸­ç”¨åˆ°çš„æ ¸å¿ƒåŒ…æ‹¬ç³»ç»Ÿçš„é¢„è£…æ ¡é˜Ÿè®¾ç½®å’Œå¤šæ¨¡æ€å½±åƒçš„èåˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿæé«˜æ“ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ€»ç»“ä¸ºï¼šæ–°ä¸€ä»£æœºå™¨äººå¤šæ¨¡æ€æˆåƒç³»ç»Ÿæ˜¾è‘—æé«˜è¯Šç–—ç²¾å‡†åº¦å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„ä¸»è¦è§è§£ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€æˆåƒç³»ç»Ÿåœ¨ç°ä»£åŒ»ç–—å®è·µä¸­è¡¨ç°å‡ºåœ¨å®‰å…¨å’Œç²¾ç¡®å¹²é¢„æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¦‚CT-USå¼•å¯¼ä¸‹çš„é’ˆæ’æœ¯ã€‚ç„¶è€Œï¼Œå½“å‰æˆåƒè®¾å¤‡çš„çµæ´»æ€§å’Œç§»åŠ¨æ€§é™åˆ¶é˜»ç¢äº†å…¶åœ¨æ ‡å‡†åŒ–å·¥ä½œæµç¨‹å’Œå…¨è‡ªåŠ¨å¹²é¢„ç³»ç»Ÿä¸­çš„æ•´åˆã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹ä¸´åºŠè®¾ç½®ï¼Œç»“åˆäº†æœºå™¨äººCBCTå’Œæœºå™¨äººè¶…å£°æŠ€æœ¯ï¼Œä¸¤è€…é¢„å…ˆæ ¡å‡†å¹¶åŠ¨æ€æ³¨å†Œï¼Œä¸ºæ–°çš„ä¸´åºŠåº”ç”¨æä¾›äº†å¯èƒ½ã€‚è¿™ç§è®¾ç½®å…è®¸æ— æ³¨å†Œåˆšæ€§æ³¨å†Œï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€å¼•å¯¼ç¨‹åºåœ¨ä¸å­˜åœ¨ç»„ç»‡å˜å½¢çš„æƒ…å†µä¸‹è¿›è¡Œã€‚</li>
<li>é€šè¿‡ä¸€æ¬¡é¢„å…ˆæ ¡å‡†åœ¨ä¸¤ä¸ªç³»ç»Ÿé—´è¿›è¡Œæ³¨å†Œï¼Œå¹¶åˆ©ç”¨SAM2ä»Bæ¨¡å¼å›¾åƒä¸­åˆ†å‰²è¡€ç®¡ï¼Œåˆ©ç”¨å¤šæ™®å‹’ä¿¡å·ä½œä¸ºè‡ªä¸»ç”Ÿæˆçš„æç¤ºæ¥ç¡®ä¿å®‰å…¨æ’å…¥è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ae06661ec80314e97cfb7762d066f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64e14a95d59f041936afd132365ecbdd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Out-of-Distribution-Detection-in-Medical-Imaging-with-Normalizing-Flows"><a href="#Enhancing-Out-of-Distribution-Detection-in-Medical-Imaging-with-Normalizing-Flows" class="headerlink" title="Enhancing Out-of-Distribution Detection in Medical Imaging with   Normalizing Flows"></a>Enhancing Out-of-Distribution Detection in Medical Imaging with   Normalizing Flows</h2><p><strong>Authors:Dariush Lotfi, Mohammad-Ali Nikouei Mahani, Mohamad Koohi-Moghadam, Kyongtae Ty Bae</strong></p>
<p>Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging to ensure reliability and safety by identifying inputs outside a modelâ€™s training distribution. Existing methods often require retraining or modifications to pre-trained models, which is impractical for clinical applications. This study introduces a post-hoc normalizing flow-based approach that seamlessly integrates with pre-trained models. By leveraging normalizing flows, it estimates the likelihood of feature vectors extracted from pre-trained models, capturing semantically meaningful representations without relying on pixel-level statistics. The method was evaluated using the MedMNIST benchmark and a newly curated MedOOD dataset simulating clinically relevant distributional shifts. Performance was measured using standard OOD detection metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses comparing it against ten baseline methods. On MedMNIST, the proposed model achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD, it achieved an AUROC of 84.61%, demonstrating superior performance against other methods. Its post-hoc nature ensures compatibility with existing clinical workflows, addressing the limitations of previous approaches. The model and code to build OOD datasets are available at <a target="_blank" rel="noopener" href="https://github.com/dlotfi/MedOODFlow">https://github.com/dlotfi/MedOODFlow</a>. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„åŒ»å­¦å½±åƒä¸­ï¼Œç¦»ç¾¤å€¼æ£€æµ‹ï¼ˆOut-of-distributionï¼Œç®€ç§°OODï¼‰æ˜¯ç¡®ä¿å¯é æ€§å’Œå®‰å…¨æ€§çš„å…³é”®ã€‚å®ƒèƒ½å¤Ÿé€šè¿‡è¯†åˆ«è¶…å‡ºæ¨¡å‹è®­ç»ƒåˆ†å¸ƒèŒƒå›´çš„è¾“å…¥æ¥è¿›è¡Œé¢„æµ‹ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹ï¼Œè¿™åœ¨ä¸´åºŠåº”ç”¨ä¸­æ˜¯ä¸å¯å–çš„ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§äº‹åå½’ä¸€åŒ–æµæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ã€‚é€šè¿‡åˆ©ç”¨å½’ä¸€åŒ–æµï¼Œè¯¥æ–¹æ³•ä¼°è®¡ä»é¢„è®­ç»ƒæ¨¡å‹æå–çš„ç‰¹å¾å‘é‡çš„å¯èƒ½æ€§ï¼Œå¹¶æ•æ‰è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œè€Œæ— éœ€ä¾èµ–åƒç´ çº§ç»Ÿè®¡ä¿¡æ¯ã€‚è¯¥ç ”ç©¶ä½¿ç”¨MedMNISTåŸºå‡†æµ‹è¯•å’Œæ¨¡æ‹Ÿä¸´åºŠç›¸å…³åˆ†å¸ƒå˜åŒ–çš„å…¨æ–°MedOODæ•°æ®é›†å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚æ€§èƒ½é€šè¿‡æ ‡å‡†çš„ç¦»ç¾¤æ£€æµ‹æŒ‡æ ‡ï¼ˆå¦‚AUROCã€FPR@95ã€AUPR_INã€AUPR_OUTï¼‰æ¥è¡¡é‡ï¼Œå¹¶é€šè¿‡ä¸åç§åŸºå‡†æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ¥è¿›è¡Œç»Ÿè®¡åˆ†æã€‚åœ¨MedMNISTä¸Šï¼Œæ‰€æå‡ºæ¨¡å‹çš„AUROCè¾¾åˆ°äº†93.80%ï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨MedOODä¸Šï¼Œå…¶AUROCè¾¾åˆ°äº†84.61%ï¼Œè¯æ˜äº†ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å…¶äº‹åç‰¹æ€§ç¡®ä¿äº†ä¸ç°æœ‰ä¸´åºŠå·¥ä½œæµç¨‹çš„å…¼å®¹æ€§ï¼Œè§£å†³äº†ä¹‹å‰æ–¹æ³•çš„å±€é™æ€§ã€‚æ¨¡å‹å’Œæ„å»ºç¦»ç¾¤æ£€æµ‹æ•°æ®é›†çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dlotfi/MedOODFlow%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dlotfi/MedOODFlowæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11638v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºäº‹åå½’ä¸€åŒ–æµçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œç”¨äºåŒ»ç–—å›¾åƒä¸­çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å½’ä¸€åŒ–æµä¼°ç®—é¢„è®­ç»ƒæ¨¡å‹æå–çš„ç‰¹å¾å‘é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ•æ‰è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œæ— éœ€ä¾èµ–åƒç´ çº§ç»Ÿè®¡ã€‚åœ¨MedMNISTåŸºå‡†æµ‹è¯•å’Œæ¨¡æ‹Ÿä¸´åºŠç›¸å…³åˆ†å¸ƒåç§»çš„MedOODæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ ‡å‡†OODæ£€æµ‹æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºåç§åŸºçº¿æ–¹æ³•ã€‚å…¶äº‹åæ€§è´¨ç¡®ä¿äº†ä¸ç°æœ‰ä¸´åºŠå·¥ä½œæµç¨‹çš„å…¼å®¹æ€§ï¼Œè§£å†³äº†ä»¥å‰æ–¹æ³•çš„å±€é™æ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºäºå½’ä¸€åŒ–æµçš„æ–¹æ³•ç”¨äºåŒ»ç–—å›¾åƒä¸­çš„OODæ£€æµ‹ã€‚</li>
<li>æ–¹æ³•æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ— éœ€é‡è®­æˆ–ä¿®æ”¹æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨å½’ä¸€åŒ–æµä¼°ç®—é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾å‘é‡æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>åœ¨MedMNISTå’ŒMedOODæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•çš„æ€§èƒ½ä½¿ç”¨æ ‡å‡†OODæ£€æµ‹æŒ‡æ ‡è¿›è¡Œè¡¡é‡ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰äº‹åæ€§è´¨ï¼Œä¸ç°æœ‰ä¸´åºŠå·¥ä½œæµç¨‹å…¼å®¹ã€‚</li>
<li>æ¨¡å‹å’Œæ„å»ºOODæ•°æ®é›†çš„ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec89ced5d916eac9a730a39d0a03caac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0996f96f25fa10d7336bfe64e58e4d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad4fa15af8aa8982c18f088441d04ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdf0e3ddda3ee1c2587f8249376135b5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Variable-frame-CNNLSTM-for-Breast-Nodule-Classification-using-Ultrasound-Videos"><a href="#Variable-frame-CNNLSTM-for-Breast-Nodule-Classification-using-Ultrasound-Videos" class="headerlink" title="Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound   Videos"></a>Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound   Videos</h2><p><strong>Authors:Xiangxiang Cui, Zhongyu Li, Xiayue Fan, Peng Huang, Ying Wang, Meng Yang, Shi Chang, Jihua Zhu</strong></p>
<p>The intersection of medical imaging and artificial intelligence has become an important research direction in intelligent medical treatment, particularly in the analysis of medical images using deep learning for clinical diagnosis. Despite the advances, existing keyframe classification methods lack extraction of time series features, while ultrasonic video classification based on three-dimensional convolution requires uniform frame numbers across patients, resulting in poor feature extraction efficiency and model classification performance. This study proposes a novel video classification method based on CNN and LSTM, introducing NLPâ€™s long and short sentence processing scheme into video classification for the first time. The method reduces CNN-extracted image features to 1x512 dimension, followed by sorting and compressing feature vectors for LSTM training. Specifically, feature vectors are sorted by patient video frame numbers and populated with padding value 0 to form variable batches, with invalid padding values compressed before LSTM training to conserve computing resources. Experimental results demonstrate that our variable-frame CNNLSTM method outperforms other approaches across all metrics, showing improvements of 3-6% in F1 score and 1.5% in specificity compared to keyframe methods. The variable-frame CNNLSTM also achieves better accuracy and precision than equal-frame CNNLSTM. These findings validate the effectiveness of our approach in classifying variable-frame ultrasound videos and suggest potential applications in other medical imaging modalities. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒä¸äººå·¥æ™ºèƒ½çš„äº¤æ±‡ç‚¹å·²æˆä¸ºæ™ºèƒ½åŒ»ç–—æ²»ç–—çš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ å¯¹åŒ»å­¦å½±åƒè¿›è¡Œä¸´åºŠåˆ†ææ–¹é¢ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œç°æœ‰çš„å…³é”®å¸§åˆ†ç±»æ–¹æ³•ç¼ºä¹æ—¶é—´åºåˆ—ç‰¹å¾çš„æå–èƒ½åŠ›ã€‚åŸºäºä¸‰ç»´å·ç§¯çš„è¶…å£°è§†é¢‘åˆ†ç±»åˆ™éœ€è¦è·¨æ‚£è€…ç»Ÿä¸€å¸§æ•°é‡ï¼Œå¯¼è‡´ç‰¹å¾æå–æ•ˆç‡å’Œæ¨¡å‹åˆ†ç±»æ€§èƒ½è¾ƒå·®ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºCNNå’ŒLSTMçš„æ–°å‹è§†é¢‘åˆ†ç±»æ–¹æ³•ï¼Œé¦–æ¬¡å°†NLPçš„é•¿çŸ­å¥å¤„ç†æ–¹æ¡ˆå¼•å…¥è§†é¢‘åˆ†ç±»ä¸­ã€‚è¯¥æ–¹æ³•å°†CNNæå–çš„å›¾åƒç‰¹å¾é™ç»´è‡³1x512ç»´ï¼Œç„¶åå¯¹ç‰¹å¾å‘é‡è¿›è¡Œæ’åºå’Œå‹ç¼©ï¼Œä»¥ä¾›LSTMè®­ç»ƒä½¿ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæ ¹æ®æ‚£è€…çš„è§†é¢‘å¸§æ•°é‡å¯¹ç‰¹å¾å‘é‡è¿›è¡Œæ’åºï¼Œå¹¶ç”¨å¡«å……å€¼0æ¥å½¢æˆå¯å˜æ‰¹æ¬¡ï¼Œåœ¨LSTMè®­ç»ƒä¹‹å‰å‹ç¼©æ— æ•ˆçš„å¡«å……å€¼ä»¥èŠ‚çœè®¡ç®—èµ„æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¯å˜å¸§CNNLSTMæ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸å…³é”®å¸§æ–¹æ³•ç›¸æ¯”ï¼ŒF1å¾—åˆ†æé«˜äº†3-6%ï¼Œç‰¹å¼‚æ€§æé«˜äº†1.5%ã€‚æ­¤å¤–ï¼Œå¯å˜å¸§CNNLSTMçš„å‡†ç¡®æ€§å’Œç²¾åº¦ä¹Ÿé«˜äºç­‰å¸§CNNLSTMã€‚è¿™äº›å‘ç°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†ç±»å¯å˜å¸§è¶…å£°è§†é¢‘ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æš—ç¤ºå…¶åœ¨å…¶ä»–åŒ»å­¦å½±åƒæ¨¡å¼ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11481v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒä¸äººå·¥æ™ºèƒ½çš„äº¤èå·²æˆä¸ºæ™ºèƒ½åŒ»ç–—æ²»ç–—çš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ å¯¹åŒ»å­¦å›¾åƒè¿›è¡Œä¸´åºŠè¯Šæ–­åˆ†ææ–¹é¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºCNNå’ŒLSTMçš„æ–°å‹è§†é¢‘åˆ†ç±»æ–¹æ³•ï¼Œé¦–æ¬¡å¼•å…¥NLPçš„é•¿çŸ­å¥å¤„ç†æ–¹æ¡ˆç”¨äºè§†é¢‘åˆ†ç±»ã€‚è¯¥æ–¹æ³•é™ä½äº†CNNæå–çš„å›¾åƒç‰¹å¾ç»´åº¦ï¼Œå¯¹ç‰¹å¾å‘é‡è¿›è¡Œæ’åºå’Œå‹ç¼©ï¼Œç”¨äºLSTMè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸å…³é”®å¸§æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å¯å˜å¸§CNNLSTMæ–¹æ³•åœ¨F1å¾—åˆ†ã€ç‰¹å¼‚æ€§å’Œå‡†ç¡®æ€§ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸äººå·¥æ™ºèƒ½çš„èåˆåœ¨æ™ºèƒ½åŒ»ç–—é¢†åŸŸæˆä¸ºé‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>ç°æœ‰å…³é”®å¸§åˆ†ç±»æ–¹æ³•å­˜åœ¨æ—¶é—´åºåˆ—è¡¨å¾æå–ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åŸºäºä¸‰ç»´å·ç§¯çš„è¶…å£°è§†é¢‘åˆ†ç±»è¦æ±‚æ‚£è€…é—´å¸§æ•°ç›®ä¸€è‡´ï¼Œå½±å“ç‰¹å¾æå–æ•ˆç‡å’Œæ¨¡å‹åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºCNNå’ŒLSTMçš„è§†é¢‘åˆ†ç±»æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥NLPçš„é•¿çŸ­å¥å¤„ç†æ–¹æ¡ˆï¼Œé™ä½CNNå›¾åƒç‰¹å¾ç»´åº¦è‡³1x512ã€‚</li>
<li>é€šè¿‡æ’åºå’Œå‹ç¼©ç‰¹å¾å‘é‡ï¼Œé‡‡ç”¨å¯å˜æ‰¹æ¬¡è®­ç»ƒLSTMã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–°æ–¹æ³•åœ¨F1å¾—åˆ†ã€ç‰¹å¼‚æ€§å’Œå‡†ç¡®æ€§ç­‰æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a20e1c8d9603ac1776e0fada0d769d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb27aef2d0adddc7a6a23a62f0bf69e6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-Labelled-Data-Knowledge-A-Cooperative-Rectification-Learning-Network-for-Semi-supervised-3D-Medical-Image-Segmentation"><a href="#Leveraging-Labelled-Data-Knowledge-A-Cooperative-Rectification-Learning-Network-for-Semi-supervised-3D-Medical-Image-Segmentation" class="headerlink" title="Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning   Network for Semi-supervised 3D Medical Image Segmentation"></a>Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning   Network for Semi-supervised 3D Medical Image Segmentation</h2><p><strong>Authors:Yanyan Wang, Kechen Song, Yuyuan Liu, Shuai Ma, Yunhui Yan, Gustavo Carneiro</strong></p>
<p>Semi-supervised 3D medical image segmentation aims to achieve accurate segmentation using few labelled data and numerous unlabelled data. The main challenge in the design of semi-supervised learning methods consists in the effective use of the unlabelled data for training. A promising solution consists of ensuring consistent predictions across different views of the data, where the efficacy of this strategy depends on the accuracy of the pseudo-labels generated by the model for this consistency learning strategy. In this paper, we introduce a new methodology to produce high-quality pseudo-labels for a consistency learning strategy to address semi-supervised 3D medical image segmentation. The methodology has three important contributions. The first contribution is the Cooperative Rectification Learning Network (CRLN) that learns multiple prototypes per class to be used as external knowledge priors to adaptively rectify pseudo-labels at the voxel level. The second contribution consists of the Dynamic Interaction Module (DIM) to facilitate pairwise and cross-class interactions between prototypes and multi-resolution image features, enabling the production of accurate voxel-level clues for pseudo-label rectification. The third contribution is the Cooperative Positive Supervision (CPS), which optimises uncertain representations to align with unassertive representations of their class distributions, improving the modelâ€™s accuracy in classifying uncertain regions. Extensive experiments on three public 3D medical segmentation datasets demonstrate the effectiveness and superiority of our semi-supervised learning method. </p>
<blockquote>
<p>åŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨ä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®å’Œå¤§é‡æœªæ ‡è®°æ•°æ®å®ç°å‡†ç¡®åˆ†å‰²ã€‚åŠç›‘ç£å­¦ä¹ æ–¹æ³•è®¾è®¡çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¡®ä¿æ•°æ®ä¸åŒè§†å›¾ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§ï¼Œè¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§å–å†³äºæ¨¡å‹ä¸ºæ­¤ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ç”Ÿæˆä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ä¸ºåŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚è¯¥æ–¹æ³•æœ‰ä¸‰ä¸ªé‡è¦è´¡çŒ®ã€‚ç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯åˆä½œæ ¡æ­£å­¦ä¹ ç½‘ç»œï¼ˆCRLNï¼‰ï¼Œå®ƒå­¦ä¹ æ¯ç±»çš„å¤šä¸ªåŸå‹ï¼Œä½œä¸ºå¤–éƒ¨çŸ¥è¯†å…ˆéªŒï¼Œä»¥è‡ªé€‚åº”åœ°çº æ­£ä¼ªæ ‡ç­¾çš„åƒç´ çº§åˆ«ã€‚ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯åŠ¨æ€äº¤äº’æ¨¡å—ï¼ˆDIMï¼‰ï¼Œå®ƒä¿ƒè¿›äº†åŸå‹å’Œå¤šåˆ†è¾¨ç‡å›¾åƒç‰¹å¾ä¹‹é—´çš„é…å¯¹å’Œè·¨ç±»åˆ«äº¤äº’ï¼Œä¸ºä¼ªæ ‡ç­¾æ ¡æ­£äº§ç”Ÿå‡†ç¡®çš„åƒç´ çº§çº¿ç´¢ã€‚ç¬¬ä¸‰ä¸ªè´¡çŒ®æ˜¯åˆä½œæ­£é¢ç›‘ç£ï¼ˆCPSï¼‰ï¼Œå®ƒä¼˜åŒ–ä¸ç¡®å®šçš„è¡¨ç¤ºï¼Œä½¿å…¶ä¸ç±»åˆ†å¸ƒçš„ä¸ç¡®å®šè¡¨ç¤ºå¯¹é½ï¼Œæé«˜æ¨¡å‹å¯¹ä¸ç¡®å®šåŒºåŸŸçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å…±3DåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11456v1">PDF</a> Medical Image Analysis</p>
<p><strong>Summary</strong><br>åŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨åˆ©ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨æ•°æ®å®ç°å‡†ç¡®åˆ†å‰²ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¡®ä¿æ•°æ®ä¸åŒè§†å›¾ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§ï¼Œè¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§å–å†³äºæ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ä¸ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œä»¥è§£å†³åŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•æœ‰ä¸‰ä¸ªé‡è¦è´¡çŒ®ï¼šåˆä½œä¿®æ­£å­¦ä¹ ç½‘ç»œï¼ˆCRLNï¼‰ã€åŠ¨æ€äº¤äº’æ¨¡å—ï¼ˆDIMï¼‰å’Œåˆä½œæ­£å‘ç›‘ç£ï¼ˆCPSï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®å’Œå¤§é‡çš„æœªæ ‡æ³¨æ•°æ®å®ç°å‡†ç¡®åˆ†å‰²ã€‚</li>
<li>è®¾è®¡çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é¢„æµ‹ä¸åŒè§†å›¾ä¹‹é—´çš„ä¸€è‡´æ€§å¯¹äºåŠç›‘ç£å­¦ä¹ æ–¹æ³•è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šåˆä½œä¿®æ­£å­¦ä¹ ç½‘ç»œï¼ˆCRLNï¼‰ã€åŠ¨æ€äº¤äº’æ¨¡å—ï¼ˆDIMï¼‰å’Œåˆä½œæ­£å‘ç›‘ç£ï¼ˆCPSï¼‰ã€‚</li>
<li>CRLNå­¦ä¹ æ¯ç±»çš„å¤šä¸ªåŸå‹ä½œä¸ºå¤–éƒ¨çŸ¥è¯†å…ˆéªŒï¼Œä»¥è‡ªé€‚åº”åœ°çº æ­£ä¼ªæ ‡ç­¾ã€‚</li>
<li>DIMä¿ƒè¿›åŸå‹å’Œå¤šåˆ†è¾¨ç‡å›¾åƒç‰¹å¾ä¹‹é—´çš„é…å¯¹å’Œè·¨ç±»äº¤äº’ï¼Œä¸ºä¼ªæ ‡ç­¾çº æ­£æä¾›å‡†ç¡®çš„ä½“ç´ çº§çº¿ç´¢ã€‚</li>
<li>CPSä¼˜åŒ–ä¸ç¡®å®šè¡¨ç¤ºï¼Œä½¿å…¶ä¸ç±»åˆ†å¸ƒçš„ä¸ç¡®å®šè¡¨ç¤ºå¯¹é½ï¼Œæé«˜æ¨¡å‹å¯¹ä¸ç¡®å®šåŒºåŸŸçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-309ee22817a299c58db76348854ae966.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cee15a4110f3e674c6a91724a894cce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74f1470d480e9d0322a85806b667945e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94b7913ec201c9b0ac76579d9c911ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4cd493f3da04ffcdea1d6ed18961525.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Medical-Image-Registration-Meets-Vision-Foundation-Model-Prototype-Learning-and-Contour-Awareness"><a href="#Medical-Image-Registration-Meets-Vision-Foundation-Model-Prototype-Learning-and-Contour-Awareness" class="headerlink" title="Medical Image Registration Meets Vision Foundation Model: Prototype   Learning and Contour Awareness"></a>Medical Image Registration Meets Vision Foundation Model: Prototype   Learning and Contour Awareness</h2><p><strong>Authors:Hao Xu, Tengfei Xue, Jianan Fan, Dongnan Liu, Yuqian Chen, Fan Zhang, Carl-Fredrik Westin, Ron Kikinis, Lauren J. Oâ€™Donnell, Weidong Cai</strong></p>
<p>Medical image registration is a fundamental task in medical image analysis, aiming to establish spatial correspondences between paired images. However, existing unsupervised deformable registration methods rely solely on intensity-based similarity metrics, lacking explicit anatomical knowledge, which limits their accuracy and robustness. Vision foundation models, such as the Segment Anything Model (SAM), can generate high-quality segmentation masks that provide explicit anatomical structure knowledge, addressing the limitations of traditional methods that depend only on intensity similarity. Based on this, we propose a novel SAM-assisted registration framework incorporating prototype learning and contour awareness. The framework includes: (1) Explicit anatomical information injection, where SAM-generated segmentation masks are used as auxiliary inputs throughout training and testing to ensure the consistency of anatomical information; (2) Prototype learning, which leverages segmentation masks to extract prototype features and aligns prototypes to optimize semantic correspondences between images; and (3) Contour-aware loss, a contour-aware loss is designed that leverages the edges of segmentation masks to improve the modelâ€™s performance in fine-grained deformation fields. Extensive experiments demonstrate that the proposed framework significantly outperforms existing methods across multiple datasets, particularly in challenging scenarios with complex anatomical structures and ambiguous boundaries. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration">https://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒé…å‡†æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œæ—¨åœ¨å»ºç«‹é…å¯¹å›¾åƒä¹‹é—´çš„ç©ºé—´å¯¹åº”å…³ç³»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— ç›‘ç£å¯å˜å½¢é…å‡†æ–¹æ³•ä»…ä¾èµ–äºåŸºäºå¼ºåº¦çš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œç¼ºä¹æ˜ç¡®çš„è§£å‰–çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼‰å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åˆ†å‰²æ©æ¨¡ï¼Œæä¾›æ˜ç¡®çš„è§£å‰–ç»“æ„çŸ¥è¯†ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–å¼ºåº¦ç›¸ä¼¼æ€§çš„å±€é™æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆåŸå‹å­¦ä¹ å’Œè½®å»“æ„ŸçŸ¥çš„SAMè¾…åŠ©é…å‡†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æ˜¾å¼è§£å‰–ä¿¡æ¯æ³¨å…¥ï¼šåœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨SAMç”Ÿæˆçš„åˆ†å‰²æ©æ¨¡ä½œä¸ºè¾…åŠ©è¾“å…¥ï¼Œä»¥ç¡®ä¿è§£å‰–ä¿¡æ¯çš„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰åŸå‹å­¦ä¹ ï¼šåˆ©ç”¨åˆ†å‰²æ©æ¨¡æå–åŸå‹ç‰¹å¾ï¼Œå¹¶å¯¹é½åŸå‹ä»¥ä¼˜åŒ–å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼›ï¼ˆ3ï¼‰è½®å»“æ„ŸçŸ¥æŸå¤±ï¼šè®¾è®¡äº†ä¸€ç§åˆ©ç”¨åˆ†å‰²æ©æ¨¡è¾¹ç¼˜çš„è½®å»“æ„ŸçŸ¥æŸå¤±ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç²¾ç»†é¢—ç²’å˜å½¢åœºä¸­çš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤æ‚è§£å‰–ç»“æ„å’Œæ¨¡ç³Šè¾¹ç•Œçš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registrationæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11440v1">PDF</a> Accepted by Information Processing in Medical Imaging (IPMI) 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé…å‡†æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åŸºæœ¬ä»»åŠ¡ï¼Œæ—¨åœ¨å»ºç«‹é…å¯¹å›¾åƒä¹‹é—´çš„ç©ºé—´å¯¹åº”å…³ç³»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— ç›‘ç£å¯å˜å½¢é…å‡†æ–¹æ³•ä»…ä¾èµ–äºå¼ºåº¦ç›¸ä¼¼åº¦åº¦é‡ï¼Œç¼ºä¹æ˜ç¡®çš„è§£å‰–çŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç»“åˆä½¿ç”¨åˆ†æ®µæ¨¡å‹SAMæå‡ºäº†ä¸€ç§æ–°å‹çš„é…å‡†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«åŸå‹å­¦ä¹ å’Œè½®å»“æ„ŸçŸ¥æŠ€æœ¯ã€‚é€šè¿‡æ³¨å…¥æ˜ç¡®çš„è§£å‰–å­¦ä¿¡æ¯ã€åŸå‹ç‰¹å¾å¯¹é½å’Œè½®å»“æ„ŸçŸ¥æŸå¤±è®¾è®¡ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ç²¾ç»†å˜å½¢åœºä¸­çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒé…å‡†æ—¨åœ¨å»ºç«‹é…å¯¹å›¾åƒé—´çš„ç©ºé—´å¯¹åº”å…³ç³»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¼ºåº¦ç›¸ä¼¼åº¦åº¦é‡ï¼Œç¼ºä¹æ˜ç¡®çš„è§£å‰–çŸ¥è¯†ã€‚</li>
<li>SAMæ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åˆ†æ®µæ©æ¨¡ï¼Œæä¾›æ˜ç¡®çš„è§£å‰–ç»“æ„çŸ¥è¯†ã€‚</li>
<li>æå‡ºçš„æ–°å‹é…å‡†æ¡†æ¶ç»“åˆäº†SAMæ¨¡å‹ã€åŸå‹å­¦ä¹ å’Œè½®å»“æ„ŸçŸ¥æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶é€šè¿‡æ³¨å…¥è§£å‰–å­¦ä¿¡æ¯ã€åŸå‹ç‰¹å¾å¯¹é½æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>è½®å»“æ„ŸçŸ¥æŸå¤±è®¾è®¡è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹åœ¨ç²¾ç»†å˜å½¢åœºä¸­çš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98d5abcc48366a2470fc903a1e8f1ded.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebae1abe84e27630bc2442a5c7d8815b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60eaaecde9622a8915606fae564fa01a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="WRT-SAM-Foundation-Model-Driven-Segmentation-for-Generalized-Weld-Radiographic-Testing"><a href="#WRT-SAM-Foundation-Model-Driven-Segmentation-for-Generalized-Weld-Radiographic-Testing" class="headerlink" title="WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld   Radiographic Testing"></a>WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld   Radiographic Testing</h2><p><strong>Authors:Yunyi Zhou, Kun Shi, Gang Hao</strong></p>
<p>Radiographic testing is a fundamental non-destructive evaluation technique for identifying weld defects and assessing quality in industrial applications due to its high-resolution imaging capabilities. Over the past decade, deep learning techniques have significantly advanced weld defect identification in radiographic images. However, conventional approaches, which rely on training small-scale, task-specific models on single-scenario datasets, exhibit poor cross-scenario generalization. Recently, the Segment Anything Model (SAM), a pre-trained visual foundation model trained on large-scale datasets, has demonstrated exceptional zero-shot generalization capabilities. Fine-tuning SAM with limited domain-specific data has yielded promising results in fields such as medical image segmentation and anomaly detection. To the best of our knowledge, this work is the first to introduce SAM-based segmentation for general weld radiographic testing images. We propose WRT-SAM, a novel weld radiographic defect segmentation model that leverages SAM through an adapter-based integration with a specialized prompt generator architecture. To improve adaptability to grayscale weld radiographic images, we introduce a frequency prompt generator module, which enhances the modelâ€™s sensitivity to frequency-domain information. Furthermore, to address the multi-scale nature of weld defects, we incorporate a multi-scale prompt generator module, enabling the model to effectively extract and encode defect information across varying scales. Extensive experimental evaluations demonstrate that WRT-SAM achieves a recall of 78.87%, a precision of 84.04%, and an AUC of 0.9746, setting a new state-of-the-art (SOTA) benchmark. Moreover, the model exhibits superior zero-shot generalization performance, highlighting its potential for practical deployment in diverse radiographic testing scenarios. </p>
<blockquote>
<p>æ”¾å°„æ£€æµ‹æ˜¯ä¸€ç§é‡è¦çš„æ— æŸè¯„ä¼°æŠ€æœ¯ï¼Œå› å…¶é«˜åˆ†è¾¨ç‡æˆåƒèƒ½åŠ›ï¼Œåœ¨å·¥ä¸šåº”ç”¨ä¸­èƒ½å¤Ÿè¯†åˆ«ç„Šæ¥ç¼ºé™·å¹¶è¯„ä¼°è´¨é‡ã€‚è¿‡å»åå¹´ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å·²æ˜¾è‘—æ¨è¿›äº†æ”¾å°„å›¾åƒä¸­çš„ç„Šæ¥ç¼ºé™·è¯†åˆ«ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåœ¨å•ä¸€åœºæ™¯æ•°æ®é›†ä¸Šè®­ç»ƒå°è§„æ¨¡ã€ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ï¼Œå…¶è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ€è¿‘ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰è¿™ä¸€åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå·²å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä½¿ç”¨æœ‰é™çš„é¢†åŸŸç‰¹å®šæ•°æ®å¯¹SAMè¿›è¡Œå¾®è°ƒï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œå¼‚å¸¸æ£€æµ‹ç­‰é¢†åŸŸå·²å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡å¼•å…¥äº†åŸºäºSAMçš„åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºé€šç”¨ç„Šæ¥æ”¾å°„æ£€æµ‹å›¾åƒã€‚æˆ‘ä»¬æå‡ºäº†WRT-SAMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç„Šæ¥æ”¾å°„ç¼ºé™·åˆ†å‰²æ¨¡å‹ï¼Œå®ƒé€šè¿‡åŸºäºé€‚é…å™¨çš„é›†æˆä¸ä¸“é—¨çš„æç¤ºç”Ÿæˆæ¶æ„ï¼Œåˆ©ç”¨SAMçš„ä¼˜åŠ¿ã€‚ä¸ºäº†æ”¹å–„å¯¹ç°åº¦ç„Šæ¥æ”¾å°„å›¾åƒçš„å¯é€‚åº”æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢‘ç‡æç¤ºç”Ÿæˆå™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—æé«˜äº†æ¨¡å‹å¯¹é¢‘åŸŸä¿¡æ¯çš„æ•æ„Ÿæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†åº”å¯¹ç„Šæ¥ç¼ºé™·çš„å¤šå°ºåº¦ç‰¹æ€§ï¼Œæˆ‘ä»¬èå…¥äº†å¤šå°ºåº¦æç¤ºç”Ÿæˆå™¨æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒå°ºåº¦ä¸Šæœ‰æ•ˆåœ°æå–å’Œç¼–ç ç¼ºé™·ä¿¡æ¯ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒWRT-SAMè¾¾åˆ°äº†78.87%çš„å¬å›ç‡ã€84.04%çš„ç²¾ç¡®åº¦å’Œ0.9746çš„AUCï¼Œåˆ›ä¸‹äº†æ–°çš„SOTAåŸºå‡†ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨å„ç§æ”¾å°„æ£€æµ‹åœºæ™¯ä¸­çš„å®é™…éƒ¨ç½²æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11338v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é‡‡ç”¨æ·±åº¦å­¦ä¹ çš„ç„Šç¼ç¼ºé™·æ£€æµ‹æŠ€æœ¯åœ¨è¿‡å»åå¹´ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›å·®çš„ç¼ºé™·ã€‚ä¸ºæ”¹å–„æ­¤çŠ¶å†µï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†é¢„è®­ç»ƒçš„é€šç”¨è§†è§‰æ¨¡å‹SAMï¼ˆSegment Anything Modelï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºWRT-SAMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆSAMå’Œç‰¹å®šçš„é€‚é…å™¨æ¶æ„è¿›è¡Œç„Šç¼æ”¾å°„æ€§ç¼ºé™·æ£€æµ‹ã€‚ä¸ºæé«˜å¯¹ç°åº¦ç„Šç¼æ”¾å°„æ€§å›¾åƒçš„é€‚åº”æ€§ï¼Œå¼•å…¥äº†é¢‘ç‡æç¤ºç”Ÿæˆå™¨æ¨¡å—å’Œå¤šå°ºåº¦æç¤ºç”Ÿæˆå™¨æ¨¡å—ï¼Œè¿™äº›æ”¹è¿›æ¨¡å—æœ‰æ•ˆæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºWRT-SAMè¾¾åˆ°æ–°çš„æœ€é«˜æ€§èƒ½æ°´å¹³ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. æ”¾å°„æ€§æ£€æµ‹åœ¨å·¥ä¸šåº”ç”¨ä¸­æ˜¯é‰´å®šç„Šç¼ç¼ºé™·å’Œè¯„ä¼°è´¨é‡çš„é‡è¦æŠ€æœ¯ã€‚<br>     2. ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è·¨åœºæ™¯æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™ã€‚<br>     3. Segment Anything Modelï¼ˆSAMï¼‰å±•ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚<br>     4. WRT-SAMæ¨¡å‹ç»“åˆSAMå’Œç‰¹å®šçš„é€‚é…å™¨æ¶æ„ç”¨äºç„Šç¼æ”¾å°„æ€§ç¼ºé™·æ£€æµ‹ã€‚<br>     5. å¼•å…¥é¢‘ç‡æç¤ºç”Ÿæˆå™¨æ¨¡å—æé«˜æ¨¡å‹å¯¹é¢‘ç‡ä¿¡æ¯çš„æ•æ„Ÿæ€§ã€‚<br>     6. å¤šå°ºåº¦æç¤ºç”Ÿæˆå™¨æ¨¡å—è§£å†³äº†ç„Šç¼ç¼ºé™·çš„å¤šå°ºåº¦é—®é¢˜ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1441472dd98b4bc778b09aa07832e19e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86927f4decfd4ee7f273015458eb91a2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RemInD-Remembering-Anatomical-Variations-for-Interpretable-Domain-Adaptive-Medical-Image-Segmentation"><a href="#RemInD-Remembering-Anatomical-Variations-for-Interpretable-Domain-Adaptive-Medical-Image-Segmentation" class="headerlink" title="RemInD: Remembering Anatomical Variations for Interpretable Domain   Adaptive Medical Image Segmentation"></a>RemInD: Remembering Anatomical Variations for Interpretable Domain   Adaptive Medical Image Segmentation</h2><p><strong>Authors:Xin Wang, Yin Guo, Kaiyu Zhang, Niranjan Balu, Mahmud Mossa-Basha, Linda Shapiro, Chun Yuan</strong></p>
<p>This work presents a novel Bayesian framework for unsupervised domain adaptation (UDA) in medical image segmentation. While prior works have explored this clinically significant task using various strategies of domain alignment, they often lack an explicit and explainable mechanism to ensure that target image features capture meaningful structural information. Besides, these methods are prone to the curse of dimensionality, inevitably leading to challenges in interpretability and computational efficiency. To address these limitations, we propose RemInD, a framework inspired by human adaptation. RemInD learns a domain-agnostic latent manifold, characterized by several anchors, to memorize anatomical variations. By mapping images onto this manifold as weighted anchor averages, our approach ensures realistic and reliable predictions. This design mirrors how humans develop representative components to understand images and then retrieve component combinations from memory to guide segmentation. Notably, model prediction is determined by two explainable factors: a low-dimensional anchor weight vector, and a spatial deformation. This design facilitates computationally efficient and geometry-adherent adaptation by aligning weight vectors between domains on a probability simplex. Experiments on two public datasets, encompassing cardiac and abdominal imaging, demonstrate the superiority of RemInD, which achieves state-of-the-art performance using a single alignment approach, outperforming existing methods that often rely on multiple complex alignment strategies. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰çš„æ–°å‹è´å¶æ–¯æ¡†æ¶ã€‚è™½ç„¶å…ˆå‰çš„ç ”ç©¶å·²ç»ä½¿ç”¨å„ç§åŸŸå¯¹é½ç­–ç•¥æ¢ç´¢äº†è¿™é¡¹å…·æœ‰ä¸´åºŠæ„ä¹‰çš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹æ˜ç¡®å’Œå¯è§£é‡Šçš„æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç›®æ ‡å›¾åƒç‰¹å¾æ•è·æœ‰æ„ä¹‰çš„ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å®¹æ˜“å—åˆ°ç»´åº¦è¯…å’’çš„å½±å“ï¼Œä¸å¯é¿å…åœ°å¯¼è‡´è§£é‡Šæ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RemInDï¼Œä¸€ä¸ªå—äººç±»é€‚åº”å¯å‘çš„æ¡†æ¶ã€‚RemInDå­¦ä¹ ä¸€ä¸ªä¸åŸŸæ— å…³çš„æ½œåœ¨æµå½¢ï¼Œè¯¥æµå½¢ä»¥å¤šä¸ªé”šç‚¹ä¸ºç‰¹å¾ï¼Œä»¥è®°å¿†è§£å‰–å˜å¼‚ã€‚é€šè¿‡å°†å›¾åƒæ˜ å°„åˆ°è¿™ä¸ªæµå½¢ä½œä¸ºåŠ æƒé”šç‚¹å¹³å‡å€¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†é¢„æµ‹çš„ç°å®æ€§å’Œå¯é æ€§ã€‚è¿™ç§è®¾è®¡åæ˜ äº†äººç±»å¦‚ä½•å‘å±•ä»£è¡¨æ€§ç»„ä»¶æ¥ç†è§£å›¾åƒï¼Œç„¶åä»å†…å­˜ä¸­æ£€ç´¢ç»„ä»¶ç»„åˆæ¥æŒ‡å¯¼åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡å‹é¢„æµ‹ç”±ä¸¤ä¸ªå¯è§£é‡Šçš„å› ç´ å†³å®šï¼šä½ç»´é”šç‚¹æƒé‡å‘é‡å’Œç©ºé—´å˜å½¢ã€‚è¿™ç§è®¾è®¡é€šè¿‡å¯¹æ¦‚ç‡å•çº¯å½¢ä¸Šçš„åŸŸé—´æƒé‡å‘é‡è¿›è¡Œå¯¹é½ï¼Œå®ç°äº†è®¡ç®—é«˜æ•ˆå’Œå‡ ä½•é€‚åº”çš„é€‚åº”ã€‚åœ¨åŒ…æ‹¬å¿ƒè„å’Œè…¹éƒ¨æˆåƒçš„ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRemInDå…·æœ‰ä¼˜è¶Šæ€§ï¼Œä½¿ç”¨å•ä¸€çš„å¯¹é½æ–¹æ³•å³å¯å®ç°æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†é‚£äº›é€šå¸¸ä¾èµ–äºå¤šä¸ªå¤æ‚å¯¹é½ç­–ç•¥çš„ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10887v1">PDF</a> Accepted by IPMI 2025 (Information Processing in Medical Imaging)</p>
<p><strong>Summary</strong></p>
<p>è¯¥å·¥ä½œæå‡ºä¸€ç§æ–°å‹çš„è´å¶æ–¯æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰ã€‚é’ˆå¯¹ä»¥å¾€æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„è§£é‡Šæ€§æœºåˆ¶ä»¥åŠé¢ä¸´ç»´åº¦è¯…å’’çš„é—®é¢˜ï¼Œæå‡ºRemInDæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äººç±»é€‚åº”æ€§å¯å‘å­¦ä¹ åŸŸæ— å…³çš„æ½œåœ¨æµå½¢å¹¶ä»¥å…¶ä½œä¸ºé”šç‚¹è®°å¿†è§£å‰–ç»“æ„å˜å¼‚ï¼Œé€šè¿‡å°†å›¾åƒæ˜ å°„åˆ°æ­¤æµå½¢ä¸Šä½œä¸ºåŠ æƒé”šç‚¹å¹³å‡å€¼æ¥ç¡®ä¿é¢„æµ‹çš„çœŸå®æ€§å’Œå¯é æ€§ã€‚æ¨¡å‹é¢„æµ‹ç”±ä¸¤ä¸ªå¯è§£é‡Šå› ç´ å†³å®šï¼šä½ç»´é”šç‚¹æƒé‡å‘é‡å’Œç©ºé—´å˜å½¢ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºè®¡ç®—æ•ˆç‡é«˜ä¸”ç¬¦åˆå‡ ä½•é€‚åº”æ€§çš„åŸŸé—´æƒé‡å‘é‡å¯¹é½ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºRemInDè¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼Œä½¿ç”¨å•ä¸€å¯¹é½æ–¹æ³•ä¼˜äºå…¶ä»–ä¾èµ–å¤æ‚å¤šé‡å¯¹é½ç­–ç•¥çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è´å¶æ–¯æ¡†æ¶ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„è§£é‡Šæ€§æœºåˆ¶ï¼Œéš¾ä»¥ç¡®ä¿ç›®æ ‡å›¾åƒç‰¹å¾æ•æ‰æœ‰æ„ä¹‰çš„ç»“æ„ä¿¡æ¯ã€‚</li>
<li>RemInDæ¡†æ¶é€šè¿‡äººç±»é€‚åº”æ€§å¯å‘å­¦ä¹ åŸŸæ— å…³çš„æ½œåœ¨æµå½¢å¹¶è®°å¿†è§£å‰–ç»“æ„å˜å¼‚ã€‚</li>
<li>RemInDæ¡†æ¶é€šè¿‡å°†å›¾åƒæ˜ å°„åˆ°æµå½¢ä¸Šçš„åŠ æƒé”šç‚¹å¹³å‡å€¼æ¥ç¡®ä¿é¢„æµ‹çš„çœŸå®æ€§å’Œå¯é æ€§ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹ç”±ä¸¤ä¸ªå¯è§£é‡Šå› ç´ å†³å®šï¼šä½ç»´é”šç‚¹æƒé‡å‘é‡å’Œç©ºé—´å˜å½¢ã€‚</li>
<li>è¯¥è®¾è®¡åœ¨è®¡ç®—æ•ˆç‡å’Œå‡ ä½•é€‚åº”æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå®ç°äº†åŸŸé—´æƒé‡å‘é‡çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c3d7b65934d94dd13c1df256924fd97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0563b82f0c7f1e457fd5c06632984298.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Wound-Tissue-Segmentation-A-Comprehensive-Evaluation-using-A-Novel-Dataset"><a href="#Deep-Learning-for-Wound-Tissue-Segmentation-A-Comprehensive-Evaluation-using-A-Novel-Dataset" class="headerlink" title="Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation   using A Novel Dataset"></a>Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation   using A Novel Dataset</h2><p><strong>Authors:Muhammad Ashad Kabir, Nidita Roy, Md. Ekramul Hossain, Jill Featherston, Sayed Ahmed</strong></p>
<p>Deep learning (DL) techniques have emerged as promising solutions for medical wound tissue segmentation. However, a notable limitation in this field is the lack of publicly available labelled datasets and a standardised performance evaluation of state-of-the-art DL models on such datasets. This study addresses this gap by comprehensively evaluating various DL models for wound tissue segmentation using a novel dataset. We have curated a dataset comprising 147 wound images exhibiting six tissue types: slough, granulation, maceration, necrosis, bone, and tendon. The dataset was meticulously labelled for semantic segmentation employing supervised machine learning techniques. Three distinct labelling formats were developed â€“ full image, patch, and superpixel. Our investigation encompassed a wide array of DL segmentation and classification methodologies, ranging from conventional approaches like UNet, to generative adversarial networks such as cGAN, and modified techniques like FPN+VGG16. Also, we explored DL-based classification methods (e.g., ResNet50) and machine learning-based classification leveraging DL features (e.g., AlexNet+RF). In total, 82 wound tissue segmentation models were derived across the three labelling formats. Our analysis yielded several notable findings, including identifying optimal DL models for each labelling format based on weighted average Dice or F1 scores. Notably, FPN+VGG16 emerged as the top-performing DL model for wound tissue segmentation, achieving a dice score of 82.25%. This study provides a valuable benchmark for evaluating wound image segmentation and classification models, offering insights to inform future research and clinical practice in wound care. The labelled dataset created in this study is available at <a target="_blank" rel="noopener" href="https://github.com/akabircs/WoundTissue">https://github.com/akabircs/WoundTissue</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æŠ€æœ¯åœ¨åŒ»å­¦ä¼¤å£ç»„ç»‡åˆ†å‰²æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„ä¸€ä¸ªæ˜¾è‘—é™åˆ¶æ˜¯ç¼ºä¹å…¬å¼€æ ‡è®°çš„æ•°æ®é›†å’Œåœ¨è¿™äº›æ•°æ®é›†ä¸Šè¯„ä¼°æœ€å…ˆè¿›DLæ¨¡å‹çš„æ ‡å‡†åŒ–æ€§èƒ½ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»¼åˆè¯„ä¼°ç”¨äºä¼¤å£ç»„ç»‡åˆ†å‰²çš„å¤šç§DLæ¨¡å‹æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ªæ–°å‹æ•°æ®é›†ã€‚æˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«147å¼ ä¼¤å£å›¾åƒçš„æ•°æ®é›†ï¼Œè¿™äº›å›¾åƒå±•ç¤ºäº†å…­ç§ç»„ç»‡ç±»å‹ï¼šè…çƒ‚ã€è‚‰èŠ½ã€è½¯åŒ–ã€åæ­»ã€éª¨éª¼å’Œè‚Œè…±ã€‚è¯¥æ•°æ®é›†ç»è¿‡ä»”ç»†æ ‡æ³¨ï¼Œä»¥ç”¨äºè¯­ä¹‰åˆ†å‰²ï¼Œé‡‡ç”¨ç›‘ç£æœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸‰ç§ä¸åŒçš„æ ‡æ³¨æ ¼å¼ï¼ŒåŒ…æ‹¬å…¨å›¾ã€è¡¥ä¸å’Œè¶…åƒç´ ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†å¹¿æ³›çš„DLåˆ†å‰²å’Œåˆ†ç±»æ–¹æ³•ï¼Œä»ä¼ ç»Ÿçš„UNetæ–¹æ³•åˆ°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆå¦‚cGANï¼‰å’Œä¿®æ”¹åçš„æŠ€æœ¯ï¼ˆå¦‚FPN+VGG16ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†åŸºäºDLçš„åˆ†ç±»æ–¹æ³•ï¼ˆä¾‹å¦‚ResNet50ï¼‰å’Œåˆ©ç”¨DLç‰¹å¾è¿›è¡Œæœºå™¨å­¦ä¹ çš„åˆ†ç±»æ–¹æ³•ï¼ˆä¾‹å¦‚AlexNet+RFï¼‰ã€‚æ€»å…±åœ¨ä¸‰ç§æ ‡æ³¨æ ¼å¼ä¸‹è¡ç”Ÿå‡º82ä¸ªä¼¤å£ç»„ç»‡åˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æå¾—å‡ºäº†å‡ ä¸ªé‡è¦å‘ç°ï¼ŒåŒ…æ‹¬æ ¹æ®åŠ æƒå¹³å‡Diceæˆ–F1åˆ†æ•°ç¡®å®šæ¯ç§æ ‡æ³¨æ ¼å¼çš„æœ€ä½³DLæ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFPN+VGG16åœ¨ä¼¤å£ç»„ç»‡åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºæœ€ä½³çš„DLæ¨¡å‹æ€§èƒ½ï¼Œè¾¾åˆ°äº†82.25%çš„Diceç³»æ•°ã€‚æœ¬ç ”ç©¶ä¸ºè¯„ä¼°ä¼¤å£å›¾åƒåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹æä¾›äº†ä¸€ä¸ªå®è´µçš„åŸºå‡†ï¼Œä¸ºæœªæ¥çš„ä¼¤å£æŠ¤ç†ç ”ç©¶å’Œä¸´åºŠå®è·µæä¾›äº†å¯ç¤ºã€‚æœ¬ç ”ç©¶åˆ›å»ºçš„æ ‡è®°æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/akabircs/WoundTissue%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/akabircs/WoundTissueä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10652v1">PDF</a> 35 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯å¯¹ä¼¤å£ç»„ç»‡è¿›è¡Œåˆ†å‰²ï¼Œé’ˆå¯¹ç›®å‰å…¬å¼€æ ‡æ³¨æ•°æ®é›†ç¼ºä¹å’Œæ ‡å‡†æ€§èƒ½è¯„ä¼°çš„é—®é¢˜ï¼Œå¯¹å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«å…­ç§ç»„ç»‡ç±»å‹çš„æ–°æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸‰ç§ä¸åŒçš„æ ‡æ³¨æ ¼å¼å¯¹ä¼¤å£å›¾åƒè¿›è¡Œç²¾ç»†æ ‡æ³¨ã€‚ç ”ç©¶æ¶µç›–äº†å¤šç§æ·±åº¦å­¦ä¹ åˆ†å‰²å’Œåˆ†ç±»æ–¹æ³•ï¼Œå¹¶å‘ç°FPN+VGG16æ¨¡å‹åœ¨ä¼¤å£ç»„ç»‡åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ï¼ŒåŠ æƒå¹³å‡Diceå¾—åˆ†ä¸º82.25%ã€‚æ­¤ç ”ç©¶ä¸ºè¯„ä¼°ä¼¤å£å›¾åƒåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹æä¾›äº†å®è´µçš„åŸºå‡†ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œä¸´åºŠå®è·µæä¾›äº†é‡è¦å‚è€ƒã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/akabircs/WoundTissue">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é’ˆå¯¹åŒ»ç–—ä¼¤å£ç»„ç»‡åˆ†å‰²é—®é¢˜ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªåŒ…å«å…­ç§ç»„ç»‡ç±»å‹çš„æ–°æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œç²¾ç»†æ ‡æ³¨ã€‚</li>
<li>é‡‡ç”¨ä¸‰ç§ä¸åŒçš„æ ‡æ³¨æ ¼å¼å¯¹ä¼¤å£å›¾åƒè¿›è¡Œå¤„ç†ï¼Œä»¥è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>ç ”ç©¶äº†å¤šç§æ·±åº¦å­¦ä¹ åˆ†å‰²å’Œåˆ†ç±»æ–¹æ³•ï¼ŒåŒ…æ‹¬UNetã€cGANã€FPN+VGG16ç­‰ã€‚</li>
<li>å‘ç°FPN+VGG16æ¨¡å‹åœ¨ä¼¤å£ç»„ç»‡åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªè¯„ä¼°ä¼¤å£å›¾åƒåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹çš„åŸºå‡†ï¼Œæœ‰åŠ©äºæŒ‡å¯¼æœªæ¥çš„ç ”ç©¶å’Œä¸´åºŠå®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c75eb132d9be4fc84aed2ed35fdc8d0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ed366e1d734b3eb7504fabdfa78a4b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="QMaxViT-Unet-A-Query-Based-MaxViT-Unet-with-Edge-Enhancement-for-Scribble-Supervised-Segmentation-of-Medical-Images"><a href="#QMaxViT-Unet-A-Query-Based-MaxViT-Unet-with-Edge-Enhancement-for-Scribble-Supervised-Segmentation-of-Medical-Images" class="headerlink" title="QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for   Scribble-Supervised Segmentation of Medical Images"></a>QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for   Scribble-Supervised Segmentation of Medical Images</h2><p><strong>Authors:Thien B. Nguyen-Tat, Hoang-An Vo, Phuoc-Sang Dang</strong></p>
<p>The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the modelâ€™s ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1% DSC and 1.316mm HD95 on ACDC, 88.4% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4% DSC and 4.996mm HD95 on SUN-SEG, and 69.4% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/anpc849/QMaxViT-Unet">https://github.com/anpc849/QMaxViT-Unet</a> </p>
<blockquote>
<p>å°†å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²äºåŒ»å­¦å›¾åƒåˆ†å‰²ç»å¸¸å—åˆ°éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„é™åˆ¶ã€‚å¼±ç›‘ç£å­¦ä¹ å…è®¸ä¸é‚£ä¹ˆç²¾ç¡®çš„æ ‡ç­¾ï¼Œå·²æˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†QMaxViT-Unet+æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¶‚é¸¦ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºU-Netæ¶æ„æ„å»ºï¼Œå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨è¢«æ›¿æ¢ä¸ºå¤šè½´è§†è§‰è½¬æ¢å™¨ï¼ˆMaxViTï¼‰å—ã€‚è¿™äº›æ¨¡å—å¢å¼ºäº†æ¨¡å‹é«˜æ•ˆå­¦ä¹ å±€éƒ¨å’Œå…¨å±€ç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºæŸ¥è¯¢çš„è½¬æ¢å™¨è§£ç å™¨æ¥ä¼˜åŒ–ç‰¹å¾å’Œä¸€ä¸ªè¾¹ç¼˜å¢å¼ºæ¨¡å—æ¥è¡¥å¿æ¶‚é¸¦æ ‡ç­¾ä¸­æœ‰é™çš„è¾¹ç•Œä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„QMaxViT-Unet+ï¼Œè¿™äº›æ•°æ®é›†ä¸“æ³¨äºå¿ƒè„ç»“æ„ã€ç»“è‚ ç›´è‚ æ¯è‚‰å’Œä¹³è…ºç™Œï¼šACDCã€MS-CMRSegã€SUN-SEGå’ŒBUSIã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’ŒHausdorffè·ç¦»çš„ç¬¬95ä¸ªç™¾åˆ†ç‚¹ï¼ˆHD95ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQMaxViT-Unet+åœ¨ACDCä¸Šå®ç°äº†89.1%çš„DSCå’Œ1.316mmçš„HD95ï¼Œåœ¨MS-CMRSegä¸Šå®ç°äº†88.4%çš„DSCå’Œ2.226mmçš„HD95ï¼Œåœ¨SUN-SEGä¸Šå®ç°äº†71.4%çš„DSCå’Œ4.996mmçš„HD95ï¼Œä»¥åŠåœ¨BUSIä¸Šå®ç°äº†69.4%çš„DSCå’Œ50.122mmçš„HD95ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¸å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚è¿™ä½¿å¾—å®ƒéå¸¸é€‚åˆåŒ»å­¦å›¾åƒåˆ†æï¼Œå› ä¸ºé«˜è´¨é‡çš„æ³¨é‡Šå¾€å¾€å¾ˆç¨€ç¼ºï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„åŠªåŠ›å’Œè´¹ç”¨ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/anpc849/QMaxViT-Unet">https://github.com/anpc849/QMaxViT-Unet</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼±ç›‘ç£å­¦ä¹ çš„æ¶‚é¸¦ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶QMaxViT-Unet+ï¼Œä½¿ç”¨Multi-Axis Vision Transformerï¼ˆMaxViTï¼‰å—æ›¿æ¢U-Netæ¶æ„çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œæé«˜æ¨¡å‹å­¦ä¹ å±€éƒ¨å’Œå…¨å±€ç‰¹å¾çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé›†æˆåŸºäºæŸ¥è¯¢çš„Transformerè§£ç å™¨å’Œè¾¹ç¼˜å¢å¼ºæ¨¡å—ä»¥ä¼˜åŒ–ç‰¹å¾å¹¶è¡¥å¿æ¶‚é¸¦æ ‡ç­¾çš„è¾¹ç•Œä¿¡æ¯ä¸è¶³ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQMaxViT-Unet+åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ä¸å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QMaxViT-Unet+æ¡†æ¶åˆ©ç”¨å¼±ç›‘ç£å­¦ä¹ åº”å¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŸºäºU-Netæ¶æ„ï¼Œä½¿ç”¨Multi-Axis Vision Transformerï¼ˆMaxViTï¼‰å—å¢å¼ºç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>é›†æˆåŸºäºæŸ¥è¯¢çš„Transformerè§£ç å™¨å’Œè¾¹ç¼˜å¢å¼ºæ¨¡å—ä»¥ä¼˜åŒ–åˆ†å‰²ç»“æœã€‚</li>
<li>åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†QMaxViT-Unet+çš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
<li>QMaxViT-Unet+ä¸å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œå°¤å…¶é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µã€‚</li>
<li>QMaxViT-Unet+ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-809cc01849e9267d2cbd99d7f9494cf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-987aeea54b8448d48d46fa690f094ed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b42d7479a318537315be2d11027dc58.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Solver-Aided-Hierarchical-Language-for-LLM-Driven-CAD-Design"><a href="#A-Solver-Aided-Hierarchical-Language-for-LLM-Driven-CAD-Design" class="headerlink" title="A Solver-Aided Hierarchical Language for LLM-Driven CAD Design"></a>A Solver-Aided Hierarchical Language for LLM-Driven CAD Design</h2><p><strong>Authors:Benjamin T. Jones, Felix HÃ¤hnlein, Zihan Zhang, Maaz Ahmad, Vladimir Kim, Adriana Schulz</strong></p>
<p>Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning to generate complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å„ç§ç»“æ„å’Œéç»“æ„ç”Ÿæˆä»»åŠ¡æ–¹é¢å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ä¸­çš„è¿‡ç¨‹å‡ ä½•ç”Ÿæˆæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›å›°éš¾äº§ç”Ÿäºæ— æ³•è¿›è¡Œç©ºé—´æ¨ç†å’Œå¿…é¡»é€šè¿‡å¤æ‚ã€é•¿æœŸè§„åˆ’æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆå¤æ‚å‡ ä½•çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§è¾…åŠ©æ±‚è§£å™¨ã€å±‚æ¬¡åŒ–çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰AIDLï¼Œå®ç°äº†ç”¨LLMè¿›è¡Œç”Ÿæˆå¼CADè®¾è®¡ã€‚AIDLå°†ç©ºé—´æ¨ç†è¦æ±‚è½¬ç§»åˆ°å‡ ä½•çº¦æŸæ±‚è§£å™¨ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒAIDLç”šè‡³è¡¨ç°å‡ºæ¯”ä½¿ç”¨è®­ç»ƒæ•°æ®ï¼ˆOpenSCADï¼‰çš„è¯­è¨€æ›´å¥½çš„æ€§èƒ½ï¼Œä¸ä»…åœ¨ç”Ÿæˆæ›´æ¥è¿‘æç¤ºçš„è§†è§‰ç»“æœæ–¹é¢ï¼Œè€Œä¸”åœ¨ç”Ÿæˆæ›´å®¹æ˜“è¿›è¡ŒåæœŸå¤„ç†å’Œæ¨ç†çš„å¯¹è±¡æ–¹é¢ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09819v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤šç§ç»“æ„åŒ–å’Œéç»“æ„åŒ–ç”Ÿæˆä»»åŠ¡æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ä¸­çš„ç¨‹åºåŒ–å‡ ä½•ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºAIDLçš„æ±‚è§£å™¨è¾…åŠ©ã€åˆ†å±‚é¢†åŸŸç‰¹å®šè¯­è¨€ï¼Œå®ƒå°†ç©ºé—´æ¨ç†è¦æ±‚è½¬ç§»åˆ°å‡ ä½•çº¦æŸæ±‚è§£å™¨ä¸Šï¼Œä»è€Œå®ç°äº†LLMsçš„ç”Ÿæˆå¼CADè®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒAIDLç”šè‡³åœ¨ç”Ÿæˆè§†è§‰ç»“æœå’Œåˆ›å»ºå¯åå¤„ç†å’Œæ¨ç†çš„å¯¹è±¡æ–¹é¢ï¼Œä¼˜äºå…·æœ‰è®­ç»ƒæ•°æ®çš„è¯­è¨€ï¼ˆå¦‚OpenSCADï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤šç§ç”Ÿæˆä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„ç¨‹åºåŒ–å‡ ä½•ç”Ÿæˆæ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´å›°éš¾ã€‚</li>
<li>å›°éš¾æºäºç©ºé—´æ¨ç†èƒ½åŠ›çš„ä¸è¶³ä»¥åŠéœ€è¦æŒ‡å¯¼æ¨¡å‹è¿›è¡Œå¤æ‚ã€é•¿æœŸè§„åˆ’ä»¥ç”Ÿæˆå¤æ‚å‡ ä½•çš„éœ€æ±‚ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†AIDLè¿™ç§æ±‚è§£å™¨è¾…åŠ©ã€åˆ†å±‚é¢†åŸŸç‰¹å®šè¯­è¨€ã€‚</li>
<li>AIDLå°†ç©ºé—´æ¨ç†è¦æ±‚è½¬ç§»åˆ°å‡ ä½•çº¦æŸæ±‚è§£å™¨ä¸Šï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´è½»æ¾åœ°å®ŒæˆCADè®¾è®¡ã€‚</li>
<li>åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒAIDLåœ¨ç”Ÿæˆè§†è§‰ç»“æœæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œæ›´æ¥è¿‘æç¤ºï¼Œå¹¶ä¸”åˆ›å»ºçš„ç‰©ä½“æ›´æ˜“äºåå¤„ç†å’Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8030b9633966c5c6b2eda96cd616bf35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-057aa338a72d8272db79e627cc3a8424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7881b1ad50eca7887d6178230822ee85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49a4a08b7dbe41cff6c32961d1f2e0d7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Vision-based-Geo-Localization-of-Future-Mars-Rotorcraft-in-Challenging-Illumination-Conditions"><a href="#Vision-based-Geo-Localization-of-Future-Mars-Rotorcraft-in-Challenging-Illumination-Conditions" class="headerlink" title="Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging   Illumination Conditions"></a>Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging   Illumination Conditions</h2><p><strong>Authors:Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis</strong></p>
<p>Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASAâ€™s Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day. </p>
<blockquote>
<p>åˆ©ç”¨ç©ºä¸­èµ„æºè¿›è¡Œè¡Œæ˜Ÿæ¢ç´¢å…·æœ‰åœ¨ç«æ˜Ÿä¸Šè·å¾—å‰æ‰€æœªæœ‰çš„ç§‘å­¦å‘ç°çš„æ½œåŠ›ã€‚è™½ç„¶ç¾å›½å®‡èˆªå±€çš„ç«æ˜Ÿç›´å‡æœºâ€œæœºæ™ºå·â€è¯æ˜äº†åœ¨ç«æ˜Ÿå¤§æ°”ä¸­é£è¡Œæ˜¯å¯èƒ½çš„ï¼Œä½†æœªæ¥çš„ç«æ˜Ÿæ—‹ç¿¼é£è¡Œå™¨å°†éœ€è¦å…ˆè¿›çš„å¯¼èˆªèƒ½åŠ›ä»¥è¿›è¡Œè¿œç¨‹é£è¡Œã€‚å…¶ä¸­ä¸€é¡¹å…³é”®èƒ½åŠ›æ˜¯åœ°å›¾å®šä½ï¼ˆMbLï¼‰ï¼Œå®ƒåœ¨é£è¡Œè¿‡ç¨‹ä¸­å°†æœºè½½å›¾åƒæ³¨å†Œåˆ°å‚è€ƒåœ°å›¾ä¸Šï¼Œä»¥å‡å°‘è§†è§‰é‡Œç¨‹è®¡çš„ç´¯ç§¯æ¼‚ç§»ã€‚ç„¶è€Œï¼Œæ—‹ç¿¼é£è¡Œå™¨è§‚æµ‹ä¸å‚è€ƒåœ°å›¾ä¹‹é—´çš„æ˜¾è‘—ç…§æ˜å·®å¼‚å¯¹ä¼ ç»Ÿçš„MbLç³»ç»Ÿæ„æˆäº†æŒ‘æˆ˜ï¼Œé™åˆ¶äº†è½¦è¾†çš„ä½œä¸šçª—å£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ–°çš„MbLç³»ç»Ÿï¼Œå¹¶æå‡ºäº†Geo-LoFTRï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå‡ ä½•è¾…åŠ©çš„æ·±åº¦å­¦ä¹ å›¾åƒæ³¨å†Œæ¨¡å‹ã€‚å®ƒåœ¨å¤§ç…§æ˜å·®å¼‚ä¸‹çš„é²æ£’æ€§è¶…è¿‡äº†å…ˆå‰æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿå¾—åˆ°äº†ä¸€ä¸ªè‡ªå®šä¹‰ä»¿çœŸæ¡†æ¶çš„æ”¯æŒï¼Œè¯¥æ¡†æ¶ä½¿ç”¨çœŸå®çš„è½¨é“åœ°å›¾ç”Ÿæˆå¤§é‡çœŸå®çš„ç«æ˜Ÿåœ°å½¢å›¾åƒã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œåœ¨è¾ƒå¤§çš„å…‰ç…§å’Œå°ºåº¦å˜åŒ–ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿåœ¨å®šä½ç²¾åº¦æ–¹é¢ä¼˜äºå…ˆå‰çš„MbLå·¥ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿçš„ç«æ˜Ÿæ—¥å‘¨æœŸå†…éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09795v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æ¢è®¨ä½¿ç”¨æ— äººæœºæŠ€æœ¯å®ç°ç«æ˜Ÿè¡¨é¢é«˜ç²¾åº¦åœ°å›¾å¯¼èˆªå®šä½çš„æ–¹æ³•ï¼Œæ¶‰åŠäº†æ–°é¢–çš„åŸºäºæ·±åº¦å­¦ä¹ å›¾åƒæ³¨å†Œçš„åœ°ç†å®šä½ç³»ç»ŸGeo-LoFTRåŠå…¶å…³é”®æŠ€æœ¯åº”ç”¨å’ŒéªŒè¯æƒ…å†µã€‚è¯¥æ–¹æ³•å¯ä»¥æå¤§åœ°æ‰©å¤§æ¢æµ‹çª—å£ã€‚é‡ç‚¹å…³æ³¨è¯¥æ–¹æ³•å…·å¤‡æŠµæŠ—å…‰ç…§å˜åŒ–çš„èƒ½åŠ›ï¼Œä»¥åŠå…¶åœ¨æ¨¡æ‹Ÿç«æ˜Ÿç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡é‡‡ç”¨çœŸå®ç«æ˜Ÿè½¨é“åœ°å›¾ï¼Œç ”ç©¶æ„å»ºäº†è‡ªå®šä¹‰ä»¿çœŸæ¡†æ¶ç”¨äºæ¨¡æ‹Ÿæµ‹è¯•ç¯å¢ƒï¼ŒæˆåŠŸéªŒè¯æ‰€æå‡ºç³»ç»Ÿèƒ½å¤Ÿåœ¨å„ç§å…‰ç…§å’Œå°ºåº¦å˜åŒ–æ¡ä»¶ä¸‹å®ç°ç²¾å‡†å®šä½ã€‚è¯¥æˆæœæœ‰æœ›ä¸ºæœªæ¥çš„ç«æ˜Ÿæ¢ç´¢ä»»åŠ¡æä¾›å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç«æ˜Ÿæ¢ç´¢æ­£å€ŸåŠ©æ— äººæœºæŠ€æœ¯å¯»æ±‚å‰æ‰€æœªæœ‰çš„ç§‘å­¦å‘ç°æ½œåŠ›ã€‚</li>
<li>Map-based Localizationï¼ˆMbLï¼‰ç³»ç»Ÿæ˜¯æœªæ¥ç«æ˜Ÿæ— äººæœºè¿œç¨‹é£è¡Œæ‰€éœ€çš„å…³é”®å¯¼èˆªèƒ½åŠ›ä¹‹ä¸€ã€‚</li>
<li>å…‰ç…§å·®å¼‚ç»™MbLç³»ç»Ÿå¸¦æ¥æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¢æµ‹å™¨çš„æ“ä½œçª—å£ã€‚</li>
<li>æå‡ºçš„Geo-LoFTRç³»ç»Ÿæ˜¯ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ å’Œå‡ ä½•ä¿¡æ¯çš„å›¾åƒæ³¨å†Œæ–¹æ³•ï¼Œèƒ½æ›´ç¨³å¥åœ°åº”å¯¹å…‰ç…§å˜åŒ–ã€‚</li>
<li>è‡ªå®šä¹‰ä»¿çœŸæ¡†æ¶åˆ©ç”¨çœŸå®ç«æ˜Ÿè½¨é“åœ°å›¾ç”Ÿæˆé€¼çœŸçš„ç«æ˜Ÿåœ°å½¢å›¾åƒï¼Œç”¨äºç³»ç»Ÿæµ‹è¯•ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒGeo-LoFTRç³»ç»Ÿåœ¨å…‰ç…§å’Œå°ºåº¦å˜åŒ–æ¡ä»¶ä¸‹å®šä½ç²¾åº¦ä¼˜äºå…ˆå‰MbLæ–¹æ³•ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿç«æ˜Ÿç¯å¢ƒä¸‹çš„éªŒè¯è¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae652b3539445c52034d551318410e69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69d7d400c0c1c39a1b1de2129da4deda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6064f4da7bfab57eeaa38da39d13f87b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ee3aaa42465d2842ea57b6193b5284e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9411beedf0c78d095a4839abe2a0d83b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a27cdfd1ff592b47512d4d1a99768914.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-CNN-Approach-to-Automated-Detection-and-Classification-of-Brain-Tumors"><a href="#A-CNN-Approach-to-Automated-Detection-and-Classification-of-Brain-Tumors" class="headerlink" title="A CNN Approach to Automated Detection and Classification of Brain Tumors"></a>A CNN Approach to Automated Detection and Classification of Brain Tumors</h2><p><strong>Authors:Md. Zahid Hasan, Abdullah Tamim, D. M. Asadujjaman, Md. Mahfujur Rahman, Md. Abu Ahnaf Mollick, Nosin Anjum Dristi,  Abdullah-Al-Noman</strong></p>
<p>Brain tumors require an assessment to ensure timely diagnosis and effective patient treatment. Morphological factors such as size, location, texture, and variable appearance com- plicate tumor inspection. Medical imaging presents challenges, including noise and incomplete images. This research article presents a methodology for processing Magnetic Resonance Imag- ing (MRI) data, encompassing techniques for image classification and denoising. The effective use of MRI images allows medical professionals to detect brain disorders, including tumors. This research aims to categorize healthy brain tissue and brain tumors by analyzing the provided MRI data. Unlike alternative methods like Computed Tomography (CT), MRI technology offers a more detailed representation of internal anatomical components, mak- ing it a suitable option for studying data related to brain tumors. The MRI picture is first subjected to a denoising technique utilizing an Anisotropic diffusion filter. The dataset utilized for the models creation is a publicly accessible and validated Brain Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE was employed for data augmentation and dataset balancing. Convolutional Neural Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for the classification procedure. EfficientNet attained an accuracy of 98%, the highest recorded. </p>
<blockquote>
<p>å¯¹è„‘è‚¿ç˜¤è¿›è¡Œè¯„ä¼°æ˜¯ç¡®ä¿åŠæ—¶è¯Šæ–­å’Œæ²»ç–—çš„å…³é”®ã€‚è‚¿ç˜¤çš„å½¢æ€å­¦å› ç´ ï¼Œå¦‚å¤§å°ã€ä½ç½®ã€è´¨åœ°å’Œå¤–è§‚å˜åŒ–ï¼Œä½¿å¾—è‚¿ç˜¤æ£€æŸ¥å˜å¾—å¤æ‚ã€‚åŒ»å­¦æˆåƒä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å™ªå£°å’Œä¸å®Œæ•´çš„å›¾åƒã€‚è¿™ç¯‡ç ”ç©¶è®ºæ–‡æå‡ºäº†ä¸€ç§å¤„ç†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œå»å™ªæŠ€æœ¯ã€‚æœ‰æ•ˆåœ°ä½¿ç”¨MRIå›¾åƒå¯ä»¥ä½¿åŒ»ç–—ä¸“ä¸šäººå‘˜èƒ½å¤Ÿæ£€æµ‹åŒ…æ‹¬è‚¿ç˜¤åœ¨å†…çš„è„‘éƒ¨ç–¾ç—…ã€‚æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯é€šè¿‡åˆ†ææä¾›çš„MRIæ•°æ®æ¥åŒºåˆ†å¥åº·è„‘ç»„ç»‡å’Œè„‘è‚¿ç˜¤ã€‚ä¸è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰æ›¿ä»£æ–¹æ³•ä¸åŒï¼ŒMRIæŠ€æœ¯æä¾›æ›´è¯¦ç»†çš„å†…éƒ¨è§£å‰–ç»“æ„æˆåˆ†è¡¨ç¤ºï¼Œä½¿å…¶æˆä¸ºç ”ç©¶è„‘è‚¿ç˜¤ç›¸å…³æ•°æ®çš„ç†æƒ³é€‰æ‹©ã€‚MRIå›¾åƒé¦–å…ˆç»è¿‡ä½¿ç”¨å„å‘å¼‚æ€§æ‰©æ•£æ»¤æ³¢å™¨è¿›è¡Œå»å™ªå¤„ç†ã€‚ç”¨äºæ¨¡å‹åˆ›å»ºçš„æ•°æ®é›†æ˜¯å…¬å¼€å¯è®¿é—®ä¸”ç»è¿‡éªŒè¯çš„Brain Tumour Classificationï¼ˆMRIï¼‰æ•°æ®åº“ï¼ŒåŒ…å«3264ä¸ªè„‘éƒ¨MRIæ‰«æã€‚é‡‡ç”¨SMOTEè¿›è¡Œæ•°æ®å¢å¼ºå’Œæ•°æ®é›†å¹³è¡¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¦‚ResNet152V2ã€VGGã€ViTå’ŒEfficientNetè¢«ç”¨äºåˆ†ç±»è¿‡ç¨‹ã€‚EfficientNetçš„å‡†ç¡®ç‡è¾¾åˆ°äº†98%ï¼Œä¸ºç›®å‰æœ€é«˜è®°å½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09731v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤„ç†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œå»å™ªæŠ€æœ¯ã€‚ç ”ç©¶æ—¨åœ¨é€šè¿‡MRIæ•°æ®åˆ†ææ¥åŒºåˆ†å¥åº·è„‘ç»„ç»‡å’Œè„‘è‚¿ç˜¤ã€‚ä½¿ç”¨MRIå›¾åƒå¯ä»¥æ›´å‡†ç¡®åœ°æ£€æµ‹è„‘éƒ¨ç–¾ç—…ï¼ŒåŒ…æ‹¬è‚¿ç˜¤ã€‚ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å…ˆè¿›çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¦‚EfficientNetï¼Œå‡†ç¡®ç‡è¾¾åˆ°98%ï¼Œä¸ºåˆ†ç±»è¿‡ç¨‹æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½¢æ€å› ç´ å¦‚å¤§å°ã€ä½ç½®ã€çº¹ç†å’Œå¤–è§‚å˜åŒ–ä½¿å¾—è„‘è‚¿ç˜¤çš„è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>åŒ»å­¦æˆåƒå­˜åœ¨å™ªå£°å’Œå›¾åƒä¸å®Œæ•´ç­‰é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤„ç†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œå»å™ªæŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨MRIå›¾åƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æ£€æµ‹è„‘éƒ¨ç–¾ç—…ï¼Œå¦‚è‚¿ç˜¤ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨SMOTEè¿›è¡Œæ•°æ®å¢å¼ºå’Œå¹³è¡¡æ•°æ®é›†ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†å¤šç§å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼Œå…¶ä¸­EfficientNetè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º98%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d2c4dede4bfe5db66741d6e47dcfb44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acb180e5dd64d2570c6cd4ed88f9996e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-309dc569c0044af9cc01efb31dd22d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b4c51a7858da0fef6ac0b944a8fb6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8221213d9c418709bc9cd17c636908e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-294095a16124d8e9c36326bcb62a13a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f225e251f469c56283b593d8945edef.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Leveraging-Machine-Learning-and-Deep-Learning-Techniques-for-Improved-Pathological-Staging-of-Prostate-Cancer"><a href="#Leveraging-Machine-Learning-and-Deep-Learning-Techniques-for-Improved-Pathological-Staging-of-Prostate-Cancer" class="headerlink" title="Leveraging Machine Learning and Deep Learning Techniques for Improved   Pathological Staging of Prostate Cancer"></a>Leveraging Machine Learning and Deep Learning Techniques for Improved   Pathological Staging of Prostate Cancer</h2><p><strong>Authors:Raziehsadat Ghalamkarian, Marziehsadat Ghalamkarian, MortezaAli Ahmadi, Sayed Mohammad Ahmadi, Abolfazl Diyanat</strong></p>
<p>Prostate cancer (Pca) continues to be a leading cause of cancer-related mortality in men, and the limitations in precision of traditional diagnostic methods such as the Digital Rectal Exam (DRE), Prostate-Specific Antigen (PSA) testing, and biopsies underscore the critical importance of accurate staging detection in enhancing treatment outcomes and improving patient prognosis. This study leverages machine learning and deep learning approaches, along with feature selection and extraction methods, to enhance PCa pathological staging predictions using RNA sequencing data from The Cancer Genome Atlas (TCGA). Gene expression profiles from 486 tumors were analyzed using advanced algorithms, including Random Forest (RF), Logistic Regression (LR), Extreme Gradient Boosting (XGB), and Support Vector Machine (SVM). The performance of the study is measured with respect to the F1-score, as well as precision and recall, all of which are calculated as weighted averages. The results reveal that the highest test F1-score, approximately 83%, was achieved by the Random Forest algorithm, followed by Logistic Regression at 80%, while both Extreme Gradient Boosting (XGB) and Support Vector Machine (SVM) scored around 79%. Furthermore, deep learning models with data augmentation achieved an accuracy of 71. 23%, while PCA-based dimensionality reduction reached an accuracy of 69.86%. This research highlights the potential of AI-driven approaches in clinical oncology, paving the way for more reliable diagnostic tools that can ultimately improve patient outcomes. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œï¼ˆPcaï¼‰ä»ç„¶æ˜¯ç”·æ€§ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å¦‚ç›´è‚ æŒ‡æ£€ï¼ˆDREï¼‰ã€å‰åˆ—è…ºç‰¹å¼‚æ€§æŠ—åŸï¼ˆPSAï¼‰æ£€æµ‹å’Œæ´»æ£€åœ¨è¯Šæ–­ç²¾ç¡®åº¦ä¸Šçš„å±€é™æ€§å‡¸æ˜¾äº†ç²¾ç¡®åˆ†æœŸæ£€æµ‹åœ¨æ”¹å–„æ‚£è€…æ²»ç–—ç»“æœå’Œé¢„åæ–¹é¢çš„é‡è¦ä»·å€¼ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç»“åˆç‰¹å¾é€‰æ‹©å’Œæå–æ–¹æ³•ï¼Œå€ŸåŠ©ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰çš„RNAæµ‹åºæ•°æ®ï¼Œæé«˜å‰åˆ—è…ºç™Œç—…ç†åˆ†æœŸé¢„æµ‹æ°´å¹³ã€‚ç ”ç©¶å¯¹æ¥è‡ª486ä¸ªè‚¿ç˜¤çš„åŸºå› è¡¨è¾¾è°±è¿›è¡Œäº†åˆ†æï¼Œä½¿ç”¨äº†åŒ…æ‹¬éšæœºæ£®æ—ï¼ˆRFï¼‰ã€é€»è¾‘å›å½’ï¼ˆLRï¼‰ã€æç«¯æ¢¯åº¦å¢å¼ºï¼ˆXGBï¼‰å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ç­‰é«˜çº§ç®—æ³•ã€‚æœ¬ç ”ç©¶çš„æ€§èƒ½æ˜¯é€šè¿‡F1åˆ†æ•°ã€ç²¾ç¡®åº¦ä»¥åŠå¬å›ç‡æ¥è¡¡é‡çš„ï¼Œè¿™äº›æŒ‡æ ‡å‡è¢«è®¡ç®—ä¸ºåŠ æƒå¹³å‡å€¼ã€‚ç»“æœæ˜¾ç¤ºï¼Œéšæœºæ£®æ—ç®—æ³•è·å¾—äº†æœ€é«˜çš„æµ‹è¯•F1åˆ†æ•°ï¼Œçº¦ä¸º83%ï¼Œé€»è¾‘å›å½’å¾—åˆ†80%ï¼Œè€Œæç«¯æ¢¯åº¦å¢å¼ºï¼ˆXGBï¼‰å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰å¾—åˆ†çº¦ä¸º79%ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ•°æ®å¢å¼ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†71.23%ï¼Œè€ŒåŸºäºä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰çš„é™ç»´æŠ€æœ¯å‡†ç¡®ç‡ä¸º6 6 8%ã€‚è¿™é¡¹ç ”ç©¶çªæ˜¾äº†äººå·¥æ™ºèƒ½åœ¨ä¸´åºŠè‚¿ç˜¤å­¦ä¸­çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å¯é çš„è¯Šæ–­å·¥å…·å¥ å®šäº†åŸºç¡€ï¼Œæœ‰æœ›æœ€ç»ˆæ”¹å–„æ‚£è€…æ²»ç–—æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å‰åˆ—è…ºç™Œï¼ˆPcaï¼‰çš„ç—…ç†åˆ†æœŸé¢„æµ‹ï¼Œé‡‡ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆç‰¹å¾é€‰æ‹©å’Œæå–æŠ€æœ¯ï¼Œåˆ†ææ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰çš„RNAæµ‹åºæ•°æ®ã€‚ç ”ç©¶ä½¿ç”¨å¤šç§ç®—æ³•ï¼ŒåŒ…æ‹¬éšæœºæ£®æ—ï¼ˆRFï¼‰ã€é€»è¾‘å›å½’ï¼ˆLRï¼‰ã€æç«¯æ¢¯åº¦æå‡ï¼ˆXGBï¼‰å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œéšæœºæ£®æ—ç®—æ³•è·å¾—æœ€é«˜çš„æµ‹è¯•F1åˆ†æ•°ï¼Œçº¦ä¸º83%ï¼Œé€»è¾‘å›å½’å¾—åˆ†ä¸º80%ï¼Œè€ŒXGBå’ŒSVMå¾—åˆ†çº¦ä¸º79%ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œæ•°æ®å¢å¼ºæŠ€æœ¯è¾¾åˆ°71.23%çš„å‡†ç¡®ç‡ï¼Œè€ŒåŸºäºPCAçš„é™ç»´æŠ€æœ¯è¾¾åˆ°69.86%çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶çªæ˜¾äº†äººå·¥æ™ºèƒ½åœ¨ä¸´åºŠè‚¿ç˜¤å­¦ä¸­çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å¯é çš„è¯Šæ–­å·¥å…·é“ºå¹³äº†é“è·¯ï¼Œæœ€ç»ˆå¯èƒ½æ”¹å–„æ‚£è€…é¢„åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰åˆ—è…ºç™Œä»æ˜¯ç”·æ€§ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œéœ€è¦æ›´ç²¾ç¡®çš„åˆ†æœŸæ£€æµ‹æ¥æé«˜æ²»ç–—æ•ˆæœå’Œæ‚£è€…é¢„åã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•æ¥åˆ†æç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰çš„RNAæµ‹åºæ•°æ®ï¼Œä»¥æé«˜å‰åˆ—è…ºç™Œç—…ç†åˆ†æœŸé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¤šç§ç®—æ³•ï¼ŒåŒ…æ‹¬éšæœºæ£®æ—ã€é€»è¾‘å›å½’ã€æç«¯æ¢¯åº¦æå‡å’Œæ”¯æŒå‘é‡æœºã€‚</li>
<li>éšæœºæ£®æ—ç®—æ³•è·å¾—æœ€é«˜çš„æµ‹è¯•F1åˆ†æ•°ï¼Œçº¦ä¸º83%ï¼Œæ˜¾ç¤ºå‡ºåœ¨å‰åˆ—è…ºç™Œåˆ†æœŸé¢„æµ‹ä¸­çš„æ½œåŠ›ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸æ•°æ®å¢å¼ºæŠ€æœ¯ç›¸ç»“åˆï¼Œæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åŸºäºPCAçš„é™ç»´æŠ€æœ¯åœ¨å‰åˆ—è…ºç™Œåˆ†æœŸé¢„æµ‹ä¸­ä¹Ÿè¡¨ç°å‡ºä¸€å®šçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3897f3fc83ecb79b4ad7b1638ac44cb0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f6a040b34582011140a8b5c2ca00ef5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Quantitative-evaluation-of-unsupervised-clustering-algorithms-for-dynamic-total-body-PET-image-analysis"><a href="#Quantitative-evaluation-of-unsupervised-clustering-algorithms-for-dynamic-total-body-PET-image-analysis" class="headerlink" title="Quantitative evaluation of unsupervised clustering algorithms for   dynamic total-body PET image analysis"></a>Quantitative evaluation of unsupervised clustering algorithms for   dynamic total-body PET image analysis</h2><p><strong>Authors:Oona Rainio, Maria K. Jaakkola, Riku KlÃ©n</strong></p>
<p>Background. Recently, dynamic total-body positron emission tomography (PET) imaging has become possible due to new scanner devices. While clustering algorithms have been proposed for PET analysis already earlier, there is still little research systematically evaluating these algorithms for processing of dynamic total-body PET images. Materials and methods. Here, we compare the performance of 15 unsupervised clustering methods, including K-means either by itself or after principal component analysis (PCA) or independent component analysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM), agglomerative clustering, spectral clustering, and several newer clustering algorithms, for classifying time activity curves (TACs) in dynamic PET images. We use dynamic total-body $^{15}$O-water PET images collected from 30 patients with suspected or confirmed coronary artery disease. To evaluate the clustering algorithms in a quantitative way, we use them to classify 5000 TACs from each image based on whether the curve is taken from brain, right heart ventricle, right kidney, lower right lung lobe, or urinary bladder. Results. According to our results, the best methods are GMM, FCM, and ICA combined with mini batch K-means, which classified the TACs with a median accuracies of 89%, 83%, and 81%, respectively, in a processing time of half a second or less on average for each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show promise for dynamic total-body PET analysis. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæœ€è¿‘ï¼Œç”±äºæ–°çš„æ‰«æè®¾å¤‡ï¼ŒåŠ¨æ€å…¨èº«æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒå·²ç»æˆä¸ºå¯èƒ½ã€‚å°½ç®¡èšç±»ç®—æ³•åœ¨PETåˆ†ææ–¹é¢å·²ç»æå‡ºäº†ä¸€æ®µæ—¶é—´ï¼Œä½†å¯¹äºå¤„ç†åŠ¨æ€å…¨èº«PETå›¾åƒï¼Œç³»ç»Ÿè¯„ä¼°è¿™äº›ç®—æ³•çš„å­¦æœ¯ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚ææ–™ä¸æ–¹æ³•ï¼šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹15ç§æ— ç›‘ç£èšç±»ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¿™äº›ç®—æ³•åŒ…æ‹¬å•ç‹¬ä½¿ç”¨Kå‡å€¼ç®—æ³•æˆ–ç»“åˆä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æˆ–ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ã€é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ã€æ¨¡ç³Šcå‡å€¼ï¼ˆFCMï¼‰ã€å‡èšèšç±»ã€è°±èšç±»ä»¥åŠå‡ ç§æ–°çš„èšç±»ç®—æ³•ï¼Œç”¨äºå¯¹åŠ¨æ€PETå›¾åƒä¸­çš„æ—¶é—´æ´»åŠ¨æ›²çº¿ï¼ˆTACsï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨ä»ç–‘ä¼¼æˆ–ç¡®è¯Šå† çŠ¶åŠ¨è„‰ç–¾ç—…çš„30ä¾‹æ‚£è€…èº«ä¸Šæ”¶é›†çš„$^{15}$Oæ°´åŠ¨æ€å…¨èº«PETå›¾åƒã€‚ä¸ºäº†å®šé‡è¯„ä¼°èšç±»ç®—æ³•çš„æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›ç®—æ³•æ ¹æ®æ›²çº¿æ˜¯å¦æ¥è‡ªå¤§è„‘ã€å³å¿ƒå®¤ã€å³è‚¾ã€å³ä¸‹è‚ºå¶æˆ–è†€èƒ±æ¥åˆ†ç±»æ¯ä¸ªå›¾åƒçš„5000æ¡TACsã€‚ç»“æœï¼šæ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼Œæœ€ä½³æ–¹æ³•æ˜¯GMMã€FCMä»¥åŠç»“åˆå°æ‰¹é‡Kå‡å€¼ç®—æ³•çš„ICAï¼Œå®ƒä»¬åœ¨å¤„ç†åŠ¨æ€å…¨èº«PETå›¾åƒæ—¶ï¼Œå°†TACsåˆ†ç±»çš„å‡†ç¡®åº¦ä¸­ä½æ•°åˆ†åˆ«ä¸º89%ã€83%å’Œ81%ï¼Œæ¯å¼ å›¾åƒçš„å¹³å‡å¤„ç†æ—¶é—´ä¸åˆ°åŠç§’ã€‚ç»“è®ºï¼šå¯¹äºåŠ¨æ€å…¨èº«PETåˆ†æï¼ŒGMMã€FCMä»¥åŠç»“åˆå°æ‰¹é‡Kå‡å€¼ç®—æ³•çš„ICAæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07511v2">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŠ¨æ€å…¨èº«æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒæŠ€æœ¯çš„æœ€æ–°å‘å±•ä½¿å¾—èšç±»ç®—æ³•åœ¨å¤„ç†è¿™ç§æˆåƒæŠ€æœ¯æ—¶æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡å¯¹æ¯”äº†15ç§æ— ç›‘ç£èšç±»ç®—æ³•åœ¨å¤„ç†åŠ¨æ€å…¨èº«PETå›¾åƒä¸­çš„è¡¨ç°ï¼Œå‘ç°é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ã€æ¨¡ç³ŠCå‡å€¼ï¼ˆFCMï¼‰å’Œç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ç»“åˆå°æ‰¹é‡K-å‡å€¼æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œåˆ†ç±»å‡†ç¡®åº¦ä¸­ä½æ•°åˆ†åˆ«ä¸º89%ã€83%å’Œ81%ï¼Œä¸”å¤„ç†æ—¶é—´å¹³å‡æ¯å¼ å›¾åƒåœ¨åŠç§’ä»¥å†…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠ¨æ€å…¨èº«PETæˆåƒæŠ€æœ¯å› æ–°æ‰«æè®¾å¤‡è€Œè¿‘æœŸæˆä¸ºå¯èƒ½ã€‚</li>
<li>èšç±»ç®—æ³•åœ¨PETåˆ†æä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†é’ˆå¯¹åŠ¨æ€å…¨èº«PETå›¾åƒçš„å¤„ç†ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>å¯¹æ¯”äº†15ç§æ— ç›‘ç£èšç±»ç®—æ³•åœ¨å¤„ç†åŠ¨æ€PETå›¾åƒä¸­çš„è¡¨ç°ã€‚</li>
<li>GMMã€FCMå’ŒICAç»“åˆmini batch K-meansè¡¨ç°å‡ºæœ€ä½³æ•ˆæœï¼Œåˆ†ç±»å‡†ç¡®åº¦è¾ƒé«˜ä¸”å¤„ç†é€Ÿåº¦å¿«ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f128c1d8c3da5f22fb7a98669e87a15d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ded55bf92e4c70805b68e008f8613c46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bee5810932bea92e7f584d04694dd6b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="QTSeg-A-Query-Token-Based-Dual-Mix-Attention-Framework-with-Multi-Level-Feature-Distribution-for-Medical-Image-Segmentation"><a href="#QTSeg-A-Query-Token-Based-Dual-Mix-Attention-Framework-with-Multi-Level-Feature-Distribution-for-Medical-Image-Segmentation" class="headerlink" title="QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level   Feature Distribution for Medical Image Segmentation"></a>QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level   Feature Distribution for Medical Image Segmentation</h2><p><strong>Authors:Phuong-Nam Tran, Nhat Truong Pham, Duc Ngoc Minh Dang, Eui-Nam Huh, Choong Seon Hong</strong></p>
<p>Medical image segmentation plays a crucial role in assisting healthcare professionals with accurate diagnoses and enabling automated diagnostic processes. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies, while transformer-based architectures, despite their effectiveness, come with increased computational complexity. Recent efforts have focused on combining CNNs and transformers to balance performance and efficiency, but existing approaches still face challenges in achieving high segmentation accuracy while maintaining low computational costs. Furthermore, many methods underutilize the CNN encoderâ€™s capability to capture local spatial information, concentrating primarily on mitigating long-range dependency issues. To address these limitations, we propose QTSeg, a novel architecture for medical image segmentation that effectively integrates local and global information. QTSeg features a dual-mix attention decoder designed to enhance segmentation performance through: (1) a cross-attention mechanism for improved feature alignment, (2) a spatial attention module to capture long-range dependencies, and (3) a channel attention block to learn inter-channel relationships. Additionally, we introduce a multi-level feature distribution module, which adaptively balances feature propagation between the encoder and decoder, further boosting performance. Extensive experiments on five publicly available datasets covering diverse segmentation tasks, including lesion, polyp, breast cancer, cell, and retinal vessel segmentation, demonstrate that QTSeg outperforms state-of-the-art methods across multiple evaluation metrics while maintaining lower computational costs. Our implementation can be found at: <a target="_blank" rel="noopener" href="https://github.com/tpnam0901/QTSeg">https://github.com/tpnam0901/QTSeg</a> (v1.0.0) </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å¸®åŠ©åŒ»ç–—ä¸“ä¸šäººå£«è¿›è¡Œå‡†ç¡®è¯Šæ–­å’Œå®ç°è‡ªåŠ¨åŒ–è¯Šæ–­è¿‡ç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œè€ŒåŸºäºtransformerçš„æ¶æ„è™½ç„¶æœ‰æ•ˆï¼Œä½†è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚è¿‘æœŸçš„ç ”ç©¶åŠªåŠ›é›†ä¸­äºç»“åˆCNNå’Œtransformerä»¥å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å®ç°é«˜åˆ†å‰²å‡†ç¡®åº¦çš„åŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨CNNç¼–ç å™¨æ•æ‰å±€éƒ¨ç©ºé—´ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¸»è¦é›†ä¸­åœ¨ç¼“è§£é•¿è·ç¦»ä¾èµ–é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†QTSegï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚QTSegé‡‡ç”¨åŒæ··åˆæ³¨æ„åŠ›è§£ç å™¨è®¾è®¡ï¼Œæ—¨åœ¨é€šè¿‡ä»¥ä¸‹æ–¹å¼æé«˜åˆ†å‰²æ€§èƒ½ï¼šï¼ˆ1ï¼‰æ”¹è¿›ç‰¹å¾å¯¹é½çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œï¼ˆ2ï¼‰ç”¨äºæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»çš„ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥åŠï¼ˆ3ï¼‰ç”¨äºå­¦ä¹ è·¨é€šé“å…³ç³»çš„é€šé“æ³¨æ„åŠ›å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šçº§ç‰¹å¾åˆ†å¸ƒæ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°å¹³è¡¡äº†ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„ç‰¹å¾ä¼ æ’­ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚åœ¨äº”ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒæ¶µç›–äº†åŒ…æ‹¬ç—…å˜ã€æ¯è‚‰ã€ä¹³è…ºç™Œã€ç»†èƒä»¥åŠè§†ç½‘è†œè¡€ç®¡åˆ†å‰²ç­‰å¤šç§åˆ†å‰²ä»»åŠ¡ï¼Œè¯æ˜äº†QTSegåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚æˆ‘ä»¬çš„å®ç°å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/tpnam0901/QTSeg%EF%BC%88v1.0.0%EF%BC%89">https://github.com/tpnam0901/QTSegï¼ˆv1.0.0ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17241v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒåŸºäºå˜å‹å™¨çš„æ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„QTSegï¼Œå®ƒç»“åˆäº†CNNå’Œå˜å‹å™¨çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åŒæ··åˆæ³¨æ„åŠ›è§£ç å™¨æœ‰æ•ˆæ•´åˆå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚åœ¨äº”ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQTSegåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>åŸºäºå˜å‹å™¨çš„æ¶æ„è™½ç„¶æœ‰æ•ˆï¼Œä½†è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚</li>
<li>QTSegæ˜¯ä¸€ä¸ªæ–°å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼Œç»“åˆäº†CNNå’Œå˜å‹å™¨çš„ä¼˜ç‚¹ã€‚</li>
<li>QTSegé€šè¿‡åŒæ··åˆæ³¨æ„åŠ›è§£ç å™¨æé«˜åˆ†å‰²æ€§èƒ½ï¼ŒåŒ…æ‹¬äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€ç©ºé—´æ³¨æ„åŠ›æ¨¡å—å’Œé€šé“æ³¨æ„åŠ›å—ã€‚</li>
<li>QTSegå¼•å…¥å¤šå±‚æ¬¡ç‰¹å¾åˆ†å¸ƒæ¨¡å—ï¼Œè‡ªé€‚åº”å¹³è¡¡ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„ç‰¹å¾ä¼ æ’­ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac1b0274ccf049c898a08216424f7eab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984b158bbeb9366e2816ebec7b645f36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f2f6d6596061ed624a4abe2b4d867f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-384b240bb10e5b0fe54f0c0227fb4851.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Libra-Leveraging-Temporal-Images-for-Biomedical-Radiology-Analysis"><a href="#Libra-Leveraging-Temporal-Images-for-Biomedical-Radiology-Analysis" class="headerlink" title="Libra: Leveraging Temporal Images for Biomedical Radiology Analysis"></a>Libra: Leveraging Temporal Images for Biomedical Radiology Analysis</h2><p><strong>Authors:Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</strong></p>
<p>Radiology report generation (RRG) requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. While multimodal large language models (MLLMs) align with pre-trained vision encoders to enhance visual-language understanding, most existing methods rely on single-image analysis or rule-based heuristics to process multiple images, failing to fully leverage temporal information in multi-modal medical datasets. In this paper, we introduce Libra, a temporal-aware MLLM tailored for chest X-ray report generation. Libra combines a radiology-specific image encoder with a novel Temporal Alignment Connector (TAC), designed to accurately capture and integrate temporal differences between paired current and prior images. Extensive experiments on the MIMIC-CXR dataset demonstrate that Libra establishes a new state-of-the-art benchmark among similarly scaled MLLMs, setting new standards in both clinical relevance and lexical accuracy. </p>
<blockquote>
<p>æ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰éœ€è¦å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†æã€æœ‰æ•ˆçš„æ—¶åºæ¨ç†å’Œå‡†ç¡®çš„æ–‡æœ¬ç”Ÿæˆã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œæé«˜äº†è§†è§‰è¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå•å›¾åƒåˆ†ææˆ–åŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³•æ¥å¤„ç†å¤šå›¾åƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†ä¸­çš„æ—¶åºä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Libraï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘èƒ¸éƒ¨Xå°„çº¿æŠ¥å‘Šç”Ÿæˆè®¾è®¡çš„å…·æœ‰æ—¶åºæ„ŸçŸ¥èƒ½åŠ›çš„MLLMã€‚Libraç»“åˆäº†æ”¾å°„å­¦ä¸“ç”¨å›¾åƒç¼–ç å™¨å’Œæ–°å‹æ—¶åºå¯¹é½è¿æ¥å™¨ï¼ˆTACï¼‰ï¼Œæ—¨åœ¨å‡†ç¡®æ•æ‰å’Œæ•´åˆå½“å‰å›¾åƒä¸å…ˆå‰é…å¯¹å›¾åƒä¹‹é—´çš„æ—¶åºå·®å¼‚ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLibraåœ¨åŒç±»è§„æ¨¡çš„MLLMä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„åŸºå‡†ï¼Œåœ¨ä¸´åºŠç›¸å…³æ€§å’Œè¯æ±‡å‡†ç¡®æ€§æ–¹é¢è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19378v2">PDF</a> 30 pages, 5 figures, Adding Appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆçš„ä¸´æ—¶æ„ŸçŸ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Libraã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ”¾å°„å­¦ç‰¹å®šçš„å›¾åƒç¼–ç å™¨ä¸æ–°å‹çš„æ—¶é—´å¯¹é½è¿æ¥å™¨ï¼ˆTACï¼‰ï¼Œæ—¨åœ¨å‡†ç¡®æ•æ‰å’Œæ•´åˆå½“å‰å›¾åƒä¸å…ˆå‰å›¾åƒä¹‹é—´çš„æ—¶é—´å·®å¼‚ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLibraåœ¨åŒç±»è§„æ¨¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å»ºç«‹äº†æ–°çš„æœ€ä½³åŸºå‡†ï¼Œä¸ºä¸´åºŠç›¸å…³æ€§å’Œè¯æ±‡å‡†ç¡®æ€§è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Libraæ˜¯ä¸€ä¸ªé’ˆå¯¹èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆçš„ä¸´æ—¶æ„ŸçŸ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†æ”¾å°„å­¦ç‰¹å®šçš„å›¾åƒç¼–ç å™¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ—¶é—´å¯¹é½è¿æ¥å™¨ï¼ˆTACï¼‰ï¼Œç”¨äºæ•æ‰å’Œæ•´åˆæ—¶é—´å·®å¼‚ã€‚</li>
<li>æ¨¡å‹åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚</li>
<li>Libraåœ¨åŒç±»è§„æ¨¡çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>Libraè®¾å®šäº†ä¸´åºŠç›¸å…³æ€§å’Œè¯æ±‡å‡†ç¡®æ€§çš„æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-447f961f79103ee696809890f0d1d0c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e708ee7ce9188f4e0e1a3b628a0b81e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a301e7bb52efe154c5e2da2b1eea83.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Intensity-Spatial-Dual-Masked-Autoencoder-for-Multi-Scale-Feature-Learning-in-Chest-CT-Segmentation"><a href="#Intensity-Spatial-Dual-Masked-Autoencoder-for-Multi-Scale-Feature-Learning-in-Chest-CT-Segmentation" class="headerlink" title="Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature   Learning in Chest CT Segmentation"></a>Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature   Learning in Chest CT Segmentation</h2><p><strong>Authors:Yuexing Ding, Jun Wang, Hongbing Lyu</strong></p>
<p>In the field of medical image segmentation, challenges such as indistinct lesion features, ambiguous boundaries,and multi-scale characteristics have long revailed. This paper proposes an improved method named Intensity-Spatial Dual Masked AutoEncoder (ISD-MAE). Based on the tissue-contrast semi-masked autoencoder, a Masked AutoEncoder (MAE) branch is introduced to perform intensity masking and spatial masking operations on chest CT images for multi-scale feature learning and segmentation tasks. The model utilizes a dual-branch structure and contrastive learning to enhance the ability to learn tissue features and boundary details. Experiments are conducted on multiple 2D and 3D datasets. The results show that ISD-MAE significantly outperforms other methods in 2D pneumonia and mediastinal tumor segmentation tasks. For example, the Dice score reaches 90.10% on the COVID19 LESION dataset, and the performance is relatively stable. However, there is still room for improvement on 3D datasets. In response to this, improvement directions are proposed, including optimizing the loss function, using enhanced 3D convolution blocks, and processing datasets from multiple perspectives.Our code is available at:<a target="_blank" rel="noopener" href="https://github.com/prowontheus/ISD-MAE">https://github.com/prowontheus/ISD-MAE</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œé•¿æœŸä»¥æ¥ä¸€ç›´å­˜åœ¨ç€ç—…ç¶ç‰¹å¾ä¸æ˜æ˜¾ã€è¾¹ç•Œæ¨¡ç³Šå’Œå¤šå°ºåº¦ç‰¹å¾ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ–¹æ³•ï¼Œåä¸ºå¼ºåº¦-ç©ºé—´åŒé‡æ©è†œè‡ªç¼–ç å™¨ï¼ˆISD-MAEï¼‰ã€‚è¯¥æ–¹æ³•åŸºäºç»„ç»‡å¯¹æ¯”åŠæ©è†œè‡ªç¼–ç å™¨ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ©è†œè‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åˆ†æ”¯ï¼Œå¯¹èƒ¸éƒ¨CTå›¾åƒè¿›è¡Œå¼ºåº¦æ©è†œå’Œç©ºé—´æ©è†œæ“ä½œï¼Œä»¥è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾å­¦ä¹ å’Œåˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŒåˆ†æ”¯ç»“æ„å’Œå¯¹æ¯”å­¦ä¹ ï¼Œå¢å¼ºäº†å­¦ä¹ ç»„ç»‡ç‰¹å¾å’Œè¾¹ç•Œç»†èŠ‚çš„èƒ½åŠ›ã€‚å®éªŒæ˜¯åœ¨å¤šä¸ª2Då’Œ3Dæ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨2Dè‚ºç‚å’Œçºµéš”è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒISD-MAEæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨COVID19 LESIONæ•°æ®é›†ä¸Šï¼ŒDiceå¾—åˆ†è¾¾åˆ°90.10%ï¼Œæ€§èƒ½ç›¸å¯¹ç¨³å®šã€‚ä½†åœ¨3Dæ•°æ®é›†ä¸Šä»æœ‰æå‡ç©ºé—´ã€‚å¯¹æ­¤ï¼Œæå‡ºäº†æ”¹è¿›æ–¹å‘ï¼ŒåŒ…æ‹¬ä¼˜åŒ–æŸå¤±å‡½æ•°ã€ä½¿ç”¨å¢å¼ºçš„3Då·ç§¯å—ä»¥åŠä»å¤šä¸ªè§’åº¦å¤„ç†æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/prowontheus/ISD-MAE%E3%80%82">https://github.com/prowontheus/ISD-MAEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13198v2">PDF</a> During further verification, we found that due to operational errors,   a small number of images in the dataset used for training appeared in the   validation set, which led to inaccurate main conclusions. We are correcting   these problems and plan to withdraw this paper.</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ”¹è¿›çš„æ–¹æ³•ï¼Œåä¸ºå¼ºåº¦ç©ºé—´åŒæ©è†œè‡ªç¼–ç å™¨ï¼ˆISD-MAEï¼‰ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚ç—…ç¶ç‰¹å¾ä¸æ¸…æ™°ã€è¾¹ç•Œæ¨¡ç³Šå’Œå¤šå°ºåº¦ç‰¹å¾ç­‰ã€‚è¯¥æ–¹æ³•åŸºäºç»„ç»‡å¯¹æ¯”åŠæ©è†œè‡ªç¼–ç å™¨ï¼Œå¼•å…¥æ©è†œè‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åˆ†æ”¯ï¼Œå¯¹èƒ¸éƒ¨CTå›¾åƒè¿›è¡Œå¼ºåº¦æ©è†œå’Œç©ºé—´æ©è†œæ“ä½œï¼Œä»¥å®ç°å¤šå°ºåº¦ç‰¹å¾å­¦ä¹ å’Œåˆ†å‰²ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒISD-MAEåœ¨äºŒç»´è‚ºç‚å’Œçºµéš”è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚åœ¨COVID19 LESIONæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†è¾¾åˆ°90.10%ï¼Œä¸”æ€§èƒ½ç›¸å¯¹ç¨³å®šã€‚ä½†å¯¹äºä¸‰ç»´æ•°æ®é›†ï¼Œä»æœ‰æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ISD-MAEæ–¹æ³•è¢«æå‡ºä»¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç—…ç¶ç‰¹å¾ä¸æ¸…æ™°ã€è¾¹ç•Œæ¨¡ç³Šå’Œå¤šå°ºåº¦ç‰¹å¾é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åŸºäºç»„ç»‡å¯¹æ¯”åŠæ©è†œè‡ªç¼–ç å™¨ï¼Œå¼•å…¥MAEåˆ†æ”¯è¿›è¡Œå¼ºåº¦æ©è†œå’Œç©ºé—´æ©è†œæ“ä½œã€‚</li>
<li>ISD-MAEç”¨äºå¤„ç†èƒ¸éƒ¨CTå›¾åƒï¼Œå¹¶è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾å­¦ä¹ å’Œåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒISD-MAEåœ¨äºŒç»´è‚ºç‚å’Œçºµéš”è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨COVID19 LESIONæ•°æ®é›†ä¸Šï¼ŒISD-MAEçš„Diceå¾—åˆ†è¾¾åˆ°90.10%ï¼Œæ€§èƒ½ç¨³å®šã€‚</li>
<li>å¯¹äºä¸‰ç»´æ•°æ®é›†ï¼Œä»å­˜åœ¨æ”¹è¿›ç©ºé—´ï¼Œæè®®çš„ä¼˜åŒ–æ–¹å‘åŒ…æ‹¬ä¼˜åŒ–æŸå¤±å‡½æ•°ã€ä½¿ç”¨å¢å¼ºçš„ä¸‰ç»´å·ç§¯å—ä»¥åŠä»å¤šä¸ªè§’åº¦å¤„ç†æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5afdcbecb0278322e0b56f0b71b8478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce455f235cc3c4507e5abf87e85df41c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0d2f51652c242bdf96178b1b00feca4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caf84c8d8037c14500c89d24b7f579f2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="One-Leaf-Reveals-the-Season-Occlusion-Based-Contrastive-Learning-with-Semantic-Aware-Views-for-Efficient-Visual-Representation"><a href="#One-Leaf-Reveals-the-Season-Occlusion-Based-Contrastive-Learning-with-Semantic-Aware-Views-for-Efficient-Visual-Representation" class="headerlink" title="One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with   Semantic-Aware Views for Efficient Visual Representation"></a>One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with   Semantic-Aware Views for Efficient Visual Representation</h2><p><strong>Authors:Xiaoyu Yang, Lijian Xu, Hongsheng Li, Shaoting Zhang</strong></p>
<p>This paper proposes a scalable and straightforward pre-training paradigm for efficient visual conceptual representation called occluded image contrastive learning (OCL). Our OCL approach is simple: we randomly mask patches to generate different views within an image and contrast them among a mini-batch of images. The core idea behind OCL consists of two designs. First, masked tokens have the potential to significantly diminish the conceptual redundancy inherent in images, and create distinct views with substantial fine-grained differences on the semantic concept level instead of the instance level. Second, contrastive learning is adept at extracting high-level semantic conceptual features during the pre-training, circumventing the high-frequency interference and additional costs associated with image reconstruction. Importantly, OCL learns highly semantic conceptual representations efficiently without relying on hand-crafted data augmentations or additional auxiliary modules. Empirically, OCL demonstrates high scalability with Vision Transformers, as the ViT-L&#x2F;16 can complete pre-training in 133 hours using only 4 A100 GPUs, achieving 85.8% accuracy in downstream fine-tuning tasks. Code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/OLRS/">https://anonymous.4open.science/r/OLRS/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•ä¸”ç›´è§‚çš„é«˜æ•ˆè§†è§‰æ¦‚å¿µè¡¨ç¤ºé¢„è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºé®æŒ¡å›¾åƒå¯¹æ¯”å­¦ä¹ ï¼ˆOCLï¼‰ã€‚æˆ‘ä»¬çš„OCLæ–¹æ³•å¾ˆç®€å•ï¼šæˆ‘ä»¬éšæœºé®æŒ¡å›¾åƒä¸­çš„å°å—åŒºåŸŸä»¥ç”Ÿæˆä¸åŒçš„è§†å›¾ï¼Œå¹¶åœ¨ä¸€æ‰¹å›¾åƒä¹‹é—´è¿›è¡Œå¯¹æ¯”ã€‚OCLèƒŒåçš„æ ¸å¿ƒç†å¿µåŒ…æ‹¬ä¸¤ä¸ªè®¾è®¡ã€‚é¦–å…ˆï¼Œé®æŒ¡çš„æ ‡è®°å…·æœ‰é™ä½å›¾åƒå›ºæœ‰æ¦‚å¿µå†—ä½™çš„æ½œåŠ›ï¼Œå¹¶èƒ½å¤Ÿåˆ›å»ºå…·æœ‰æ˜æ˜¾ç»†å¾®å·®å¼‚çš„è§†å›¾ï¼Œè¿™äº›å·®å¼‚ä½“ç°åœ¨è¯­ä¹‰æ¦‚å¿µå±‚é¢è€Œéå®ä¾‹å±‚é¢ã€‚å…¶æ¬¡ï¼Œå¯¹æ¯”å­¦ä¹ åœ¨é¢„è®­ç»ƒæœŸé—´æ“…é•¿æå–é«˜çº§è¯­ä¹‰æ¦‚å¿µç‰¹å¾ï¼Œé¿å…äº†ä¸å›¾åƒé‡å»ºç›¸å…³çš„é«˜é¢‘å¹²æ‰°å’Œé¢å¤–æˆæœ¬ã€‚é‡è¦çš„æ˜¯ï¼ŒOCLèƒ½å¤Ÿé«˜æ•ˆåœ°å­¦ä¹ é«˜åº¦è¯­ä¹‰åŒ–çš„æ¦‚å¿µè¡¨ç¤ºï¼Œæ— éœ€ä¾èµ–æ‰‹å·¥åˆ¶ä½œçš„æ•°æ®å¢å¼ºæˆ–é¢å¤–çš„è¾…åŠ©æ¨¡å—ã€‚ç»éªŒä¸Šï¼ŒOCLåœ¨è§†è§‰è½¬æ¢å™¨ä¸Šå…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ï¼Œä¾‹å¦‚ViT-L&#x2F;16å¯ä»¥åœ¨ä»…ä½¿ç”¨4ä¸ªA100 GPUçš„æƒ…å†µä¸‹åœ¨133å°æ—¶å†…å®Œæˆé¢„è®­ç»ƒï¼Œå¹¶åœ¨ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡ä¸­å®ç°85.8%çš„å‡†ç¡®ç‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/OLRS/%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/OLRS/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09858v2">PDF</a> 16 pages</p>
<p><strong>Summary</strong><br>åŸºäºè®ºæ–‡æå‡ºä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§†è§‰æ¦‚å¿µè¡¨ç¤ºé¢„è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºé®æŒ¡å›¾åƒå¯¹æ¯”å­¦ä¹ ï¼ˆOCLï¼‰ã€‚å…¶ä¸»è¦é€šè¿‡éšæœºé®æŒ¡å›¾åƒåŒºåŸŸæ¥ç”Ÿæˆä¸åŒçš„è§†å›¾ï¼Œå¹¶åœ¨ä¸€æ‰¹å›¾åƒä¸­è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚å…¶æ ¸å¿ƒæ€æƒ³åœ¨äºé€šè¿‡é®æŒ¡æ ‡è®°å‡å°‘å›¾åƒä¸­çš„æ¦‚å¿µå†—ä½™ï¼Œç”Ÿæˆåœ¨è¯­ä¹‰å±‚é¢ä¸Šæœ‰æ˜¾è‘—å·®å¼‚çš„ä¸åŒè§†å›¾ã€‚å¯¹æ¯”å­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆæå–é«˜çº§è¯­ä¹‰æ¦‚å¿µç‰¹å¾è¿›è¡Œé¢„è®­ç»ƒï¼Œé¿å…å›¾åƒé‡å»ºå¸¦æ¥çš„é«˜é¢‘å¹²æ‰°å’Œé¢å¤–æˆæœ¬ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒOCLæ— éœ€ä¾èµ–æ‰‹å·¥åˆ¶ä½œçš„æ•°æ®å¢å¼ºæˆ–é¢å¤–è¾…åŠ©æ¨¡å—ï¼Œå¯é«˜æ•ˆå­¦ä¹ é«˜åº¦è¯­ä¹‰åŒ–çš„æ¦‚å¿µè¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒOCLåœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformersï¼‰ä¸Šå…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ï¼ŒViT-L&#x2F;16å¯ä»¥åœ¨ä»…ä½¿ç”¨4ä¸ªA100 GPUçš„æƒ…å†µä¸‹å®Œæˆé¢„è®­ç»ƒï¼Œå¹¶åœ¨ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡ä¸­å®ç°85.8%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OCLï¼ˆé®æŒ¡å›¾åƒå¯¹æ¯”å­¦ä¹ ï¼‰æ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œç”¨äºé«˜æ•ˆè§†è§‰æ¦‚å¿µè¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>OCLé€šè¿‡éšæœºé®æŒ¡å›¾åƒåŒºåŸŸç”Ÿæˆä¸åŒè§†å›¾è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>æ ¸å¿ƒæ€æƒ³åœ¨äºåˆ©ç”¨é®æŒ¡æ ‡è®°å‡å°‘å›¾åƒæ¦‚å¿µå†—ä½™ï¼Œç”Ÿæˆè¯­ä¹‰å±‚é¢ä¸Šçš„ä¸åŒè§†å›¾ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ èƒ½å¤Ÿé«˜æ•ˆæå–é«˜çº§è¯­ä¹‰æ¦‚å¿µç‰¹å¾ã€‚</li>
<li>OCLæ— éœ€ä¾èµ–æ‰‹å·¥æ•°æ®å¢å¼ºæˆ–é¢å¤–è¾…åŠ©æ¨¡å—ã€‚</li>
<li>OCLåœ¨è§†è§‰è½¬æ¢å™¨ä¸Šè¡¨ç°å‡ºé«˜å¯æ‰©å±•æ€§ï¼Œä½¿ç”¨è¾ƒå°‘çš„è®¡ç®—èµ„æºå³å¯å®ç°é«˜æ•ˆé¢„è®­ç»ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbad1da0f657b464bfbec343083799b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-713e7b9d65460b684fdd6342e8888b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f951ab084d221dda8480747eccc40fff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-581cd4bed37528c32920ae4358142118.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4c42c72fb70ff5d3391a03c02e2ccde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b51a5f8cf159b2d763b77851159aaec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04e49eae87daaae7c03cbe2a05d4be89.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FlexCAD-Unified-and-Versatile-Controllable-CAD-Generation-with-Fine-tuned-Large-Language-Models"><a href="#FlexCAD-Unified-and-Versatile-Controllable-CAD-Generation-with-Fine-tuned-Large-Language-Models" class="headerlink" title="FlexCAD: Unified and Versatile Controllable CAD Generation with   Fine-tuned Large Language Models"></a>FlexCAD: Unified and Versatile Controllable CAD Generation with   Fine-tuned Large Language Models</h2><p><strong>Authors:Zhanwei Zhang, Shizhao Sun, Wenxiao Wang, Deng Cai, Jiang Bian</strong></p>
<p>Recently, there is a growing interest in creating computer-aided design (CAD) models based on user intent, known as controllable CAD generation. Existing work offers limited controllability and needs separate models for different types of control, reducing efficiency and practicality. To achieve controllable generation across all CAD construction hierarchies, such as sketch-extrusion, extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by fine-tuning large language models (LLMs). First, to enhance comprehension by LLMs, we represent a CAD model as a structured text by abstracting each hierarchy as a sequence of text tokens. Second, to address various controllable generation tasks in a unified model, we introduce a hierarchy-aware masking strategy. Specifically, during training, we mask a hierarchy-aware field in the CAD text with a mask token. This field, composed of a sequence of tokens, can be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to predict this masked field. During inference, the user intent is converted into a CAD text with a mask token replacing the part the user wants to modify, which is then fed into FlexCAD to generate new CAD models. Comprehensive experiments on public dataset demonstrate the effectiveness of FlexCAD in both generation quality and controllability. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/FlexCAD">https://github.com/microsoft/FlexCAD</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºç”¨æˆ·æ„å›¾åˆ›å»ºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„å…´è¶£æ—¥ç›Šæµ“åšï¼Œè¿™è¢«ç§°ä¸ºå¯æ§CADç”Ÿæˆã€‚ç°æœ‰å·¥ä½œæä¾›çš„å¯æ§æ€§æœ‰é™ï¼Œå¹¶ä¸”éœ€è¦é’ˆå¯¹ä¸åŒç±»å‹çš„æ§åˆ¶ä½¿ç”¨å•ç‹¬çš„æ¨¡å‹ï¼Œè¿™é™ä½äº†æ•ˆç‡å’Œå®ç”¨æ€§ã€‚ä¸ºäº†å®ç°è·¨æ‰€æœ‰CADæ„å»ºå±‚æ¬¡ï¼ˆå¦‚è‰å›¾æŒ¤å‹ã€æŒ¤å‹ã€è‰å›¾ã€é¢ã€å¾ªç¯å’Œæ›²çº¿ï¼‰çš„å¯æ§ç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºäº†FlexCADï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»Ÿä¸€æ¨¡å‹ã€‚é¦–å…ˆï¼Œä¸ºäº†å¢å¼ºLLMçš„ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†CADæ¨¡å‹è¡¨ç¤ºä¸ºç»“æ„åŒ–çš„æ–‡æœ¬ï¼Œé€šè¿‡å°†æ¯ä¸ªå±‚æ¬¡æŠ½è±¡ä¸ºæ–‡æœ¬æ ‡è®°çš„åºåˆ—ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³ç»Ÿä¸€æ¨¡å‹ä¸­çš„å„ç§å¯æ§ç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å±‚æ¬¡æ„ŸçŸ¥æ©ç ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†CADæ–‡æœ¬ä¸­çš„å±‚æ¬¡æ„ŸçŸ¥å­—æ®µç”¨æ©ç æ ‡è®°è¿›è¡Œæ©ç›–ã€‚è¿™ä¸ªç”±ä¸€ç³»åˆ—æ ‡è®°ç»„æˆçš„å­—æ®µå¯ä»¥çµæ´»åœ°è®¾ç½®ï¼Œä»¥è¡¨ç¤ºå„ç§å±‚æ¬¡ã€‚ç„¶åï¼Œæˆ‘ä»¬è¦æ±‚LLMé¢„æµ‹è¿™ä¸ªè¢«æ©ç›–çš„å­—æ®µã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·æ„å›¾è¢«è½¬æ¢ä¸ºå¸¦æœ‰æ©ç æ ‡è®°çš„CADæ–‡æœ¬ï¼Œä»£æ›¿ç”¨æˆ·æƒ³è¦ä¿®æ”¹çš„éƒ¨åˆ†ï¼Œç„¶åè¾“å…¥FlexCADä»¥ç”Ÿæˆæ–°çš„CADæ¨¡å‹ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFlexCADåœ¨ç”Ÿæˆè´¨é‡å’Œå¯æ§æ€§æ–¹é¢éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/FlexCAD%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/microsoft/FlexCADä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05823v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·æ„å›¾çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹ç”Ÿæˆæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç°æœ‰å·¥ä½œå¯æ§æ€§æœ‰é™ï¼Œä¸”éœ€è¦é’ˆå¯¹ä¸åŒæ§åˆ¶ç±»å‹å»ºç«‹å•ç‹¬æ¨¡å‹ï¼Œé™ä½äº†æ•ˆç‡ä¸å®ç”¨æ€§ã€‚ä¸ºå®ç°åœ¨è‰å›¾æŒ¤å‹ã€æŒ¤å‹ã€è‰å›¾ã€é¢ã€ç¯è·¯å’Œæ›²çº¿ç­‰æ‰€æœ‰CADæ„å»ºå±‚æ¬¡ä¸Šçš„å¯æ§ç”Ÿæˆï¼Œæå‡ºFlexCADï¼Œé€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ç»Ÿä¸€æ¨¡å‹ã€‚FlexCADé€šè¿‡æ–‡æœ¬ç»“æ„åŒ–è¡¨ç¤ºCADæ¨¡å‹ä»¥å¢å¼ºLLMçš„ç†è§£èƒ½åŠ›ï¼Œå¼•å…¥å±‚æ¬¡æ„ŸçŸ¥æ©ç ç­–ç•¥ä»¥è§£å†³ç»Ÿä¸€æ¨¡å‹ä¸­çš„å¤šç§å¯æ§ç”Ÿæˆä»»åŠ¡ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†FlexCADåœ¨ç”Ÿæˆè´¨é‡å’Œå¯æ§æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹ç”Ÿæˆå·¥ä½œå¯æ§æ€§æœ‰é™ï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
<li>FlexCADæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¨¡å‹ï¼Œç”¨äºå®ç°CADæ¨¡å‹çš„å¯æ§ç”Ÿæˆã€‚</li>
<li>FlexCADé€šè¿‡æ–‡æœ¬ç»“æ„åŒ–è¡¨ç¤ºå¢å¼ºå¯¹CADæ¨¡å‹çš„ç†è§£ã€‚</li>
<li>FlexCADå¼•å…¥å±‚æ¬¡æ„ŸçŸ¥æ©ç ç­–ç•¥ï¼Œä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­è§£å†³å¤šç§å¯æ§ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ç”¨æˆ·æ„å›¾é€šè¿‡è½¬æ¢ä¸ºå¸¦æœ‰æ©ç ä»¤ç‰Œçš„CADæ–‡æœ¬ï¼Œç”¨äºç”Ÿæˆæ–°çš„CADæ¨¡å‹ã€‚</li>
<li>ç»¼åˆå®éªŒè¯æ˜äº†FlexCADåœ¨ç”Ÿæˆè´¨é‡å’Œå¯æ§æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-921bebbf15f5103b975c14f5a5adc20f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d8cc6a0b7fb53e2983416319c84ad54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55281e0986724ed555998884f2386101.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Agent-Skill-Acquisition-for-Large-Language-Models-via-CycleQD"><a href="#Agent-Skill-Acquisition-for-Large-Language-Models-via-CycleQD" class="headerlink" title="Agent Skill Acquisition for Large Language Models via CycleQD"></a>Agent Skill Acquisition for Large Language Models via CycleQD</h2><p><strong>Authors:So Kuroki, Taishi Nakamura, Takuya Akiba, Yujin Tang</strong></p>
<p>Training large language models to acquire specific skills remains a challenging endeavor. Conventional training approaches often struggle with data distribution imbalances and inadequacies in objective functions that do not align well with task-specific performance. To address these challenges, we introduce CycleQD, a novel approach that leverages the Quality Diversity framework through a cyclic adaptation of the algorithm, along with a model merging based crossover and an SVD-based mutation. In CycleQD, each taskâ€™s performance metric is alternated as the quality measure while the others serve as the behavioral characteristics. This cyclic focus on individual tasks allows for concentrated effort on one task at a time, eliminating the need for data ratio tuning and simplifying the design of the objective function. Empirical results from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT based models not only enables them to surpass traditional fine-tuning methods in coding, operating systems, and database tasks, but also achieves performance on par with GPT-3.5-TURBO, which potentially contains much more parameters, across these domains. Crucially, this enhanced performance is achieved while retaining robust language capabilities, as evidenced by its performance on widely adopted language benchmark tasks. We highlight the key design choices in CycleQD, detailing how these contribute to its effectiveness. Furthermore, our method is general and can be applied to image segmentation models, highlighting its applicability across different domains. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥è·å–ç‰¹å®šæŠ€èƒ½ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å·¥ä½œã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•å¸¸å¸¸é¢ä¸´æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡å’Œå®¢è§‚å‡½æ•°ä¸ç‰¹å®šä»»åŠ¡æ€§èƒ½ä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CycleQDè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è´¨é‡å¤šæ ·æ€§æ¡†æ¶é€šè¿‡ç®—æ³•çš„å¾ªç¯é€‚åº”ï¼Œä»¥åŠåŸºäºæ¨¡å‹åˆå¹¶çš„äº¤å‰å’ŒåŸºäºSVDçš„çªå˜ã€‚åœ¨CycleQDä¸­ï¼Œæ¯ä¸ªä»»åŠ¡çš„æ€§èƒ½æŒ‡æ ‡è¢«äº¤æ›¿ä½œä¸ºè´¨é‡åº¦é‡ï¼Œè€Œå…¶ä»–æŒ‡æ ‡åˆ™ä½œä¸ºè¡Œä¸ºç‰¹å¾ã€‚è¿™ç§å¯¹å•ä¸ªä»»åŠ¡çš„å¾ªç¯å…³æ³¨å…è®¸æ¯æ¬¡é›†ä¸­ç²¾åŠ›å®Œæˆä¸€ä¸ªä»»åŠ¡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ•°æ®æ¯”ç‡è°ƒæ•´çš„éœ€æ±‚ï¼Œå¹¶ç®€åŒ–äº†ç›®æ ‡å‡½æ•°çš„è®¾è®¡ã€‚æ¥è‡ªAgentBenchçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œå°†CycleQDåº”ç”¨äºLLAMA3-8B-INSTRUCTåŸºç¡€æ¨¡å‹ï¼Œä¸ä»…èƒ½ä½¿å®ƒä»¬åœ¨ç¼–ç ã€æ“ä½œç³»ç»Ÿå’Œæ•°æ®åº“ä»»åŠ¡ä¸Šè¶…è¶Šä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œè€Œä¸”åœ¨è¿™å‡ ä¸ªé¢†åŸŸå®ç°äº†ä¸GPT-3.5-TURBOç›¸å½“çš„æ€§èƒ½ï¼Œå°½ç®¡GPT-3.5-TURBOå¯èƒ½åŒ…å«æ›´å¤šçš„å‚æ•°ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ç§å¢å¼ºçš„æ€§èƒ½æ˜¯åœ¨ä¿ç•™ç¨³å¥çš„è¯­è¨€èƒ½åŠ›çš„åŒæ—¶å®ç°çš„ï¼Œè¿™åœ¨å…¶å¹¿æ³›é‡‡ç”¨çš„è¯­è¨€åŸºå‡†ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†è¯æ˜ã€‚æˆ‘ä»¬å¼ºè°ƒäº†CycleQDä¸­çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œè¯¦ç»†è¯´æ˜äº†è¿™äº›æ˜¯å¦‚ä½•ä¿ƒè¿›å…¶æœ‰æ•ˆæ€§çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºå›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œçªå‡ºäº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14735v4">PDF</a> To appear at the 13th International Conference on Learning   Representations (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>CycleQDæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹è®­ç»ƒæ–¹æ³•ï¼Œå®ƒé€šè¿‡è´¨é‡å¤šæ ·æ€§æ¡†æ¶çš„å¾ªç¯é€‚åº”ç®—æ³•ï¼Œæ¨¡å‹åˆå¹¶äº¤å‰å’ŒSVDåŸºå› çªå˜æ¥è§£å†³ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å¯é›†ä¸­å…³æ³¨äºå•ä¸ªä»»åŠ¡ï¼Œç®€åŒ–ç›®æ ‡å‡½æ•°è®¾è®¡ï¼Œå¹¶å‡å°‘æ•°æ®æ¯”ç‡è°ƒæ•´çš„éœ€æ±‚ã€‚åœ¨AgentBenchä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CycleQDè®­ç»ƒçš„LLAMA3-8B-INSTRUCTæ¨¡å‹åœ¨ç¼–ç ã€æ“ä½œç³»ç»Ÿå’Œæ•°æ®åº“ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼Œç”šè‡³ä¸GPT-3.5-TURBOç›¸å½“ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†æ¨¡å‹çš„ç¨³å¥è¯­è¨€åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CycleQDæ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼ŒåŸºäºè´¨é‡å¤šæ ·æ€§æ¡†æ¶ã€‚</li>
<li>CycleQDé€šè¿‡å¾ªç¯é€‚åº”ç®—æ³•ã€æ¨¡å‹åˆå¹¶äº¤å‰å’ŒSVDåŸºå› çªå˜æ¥ä¼˜åŒ–è®­ç»ƒã€‚</li>
<li>CycleQDå¯é›†ä¸­å…³æ³¨äºå•ä¸ªä»»åŠ¡ï¼Œç®€åŒ–ç›®æ ‡å‡½æ•°è®¾è®¡ã€‚</li>
<li>ä½¿ç”¨CycleQDè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>CycleQDè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸GPT-3.5-TURBOç›¸å½“ã€‚</li>
<li>CycleQDä¿ç•™äº†æ¨¡å‹çš„ç¨³å¥è¯­è¨€åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a6eeabd3695229800faad0716f41b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4da8ead3391d46521dd5d4f4dd251ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db210a04bf3de59b821d362ea3c78899.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6188e4b07f6d8e3271f65954c044ec74.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  A Survey on Bridging EEG Signals and Generative AI From Image and Text   to Beyond
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-58d7d2b737a18235e195879eac4e998d.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  HumanGif Single-View Human Diffusion with Generative Prior
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16668k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
