<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  A Survey on Bridging EEG Signals and Generative AI From Image and Text   to Beyond">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond"><a href="#A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond" class="headerlink" title="A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond"></a>A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond</h2><p><strong>Authors:Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury</strong></p>
<p>Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIsï¼‰ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¼€è¾Ÿäº†æ–°çš„é¢†åŸŸï¼Œä¸ºå®ç°è¾…åŠ©é€šä¿¡ã€ç¥ç»è¡¨å¾å­¦ä¹ å’Œå¤šæ¨¡å¼èåˆæä¾›äº†å¯èƒ½ã€‚è„‘æœºæ¥å£ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰çš„è„‘æœºæ¥å£ï¼Œæä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„æ–¹æ³•ï¼Œå°†ç¥ç»æ´»åŠ¨è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„è¾“å‡ºã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’ŒåŸºäºå˜å‹å™¨çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå·²ç»æ˜¾è‘—æé«˜äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºEEGçš„å¤šæ¨¡æ€ç”Ÿæˆçš„æœ€æ–°ç ”ç©¶çŠ¶å†µï¼Œé‡ç‚¹å…³æ³¨ï¼ˆiï¼‰é€šè¿‡GANsã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œæ‰©æ•£æ¨¡å‹å®ç°çš„EEGåˆ°å›¾åƒç”Ÿæˆï¼Œï¼ˆiiï¼‰åˆ©ç”¨åŸºäºTransformerçš„è‡ªç„¶è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•å®ç°çš„EEGåˆ°æ–‡æœ¬ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ–°å…´çš„EEGåˆ°è¯­éŸ³åˆæˆé¢†åŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­å‘å±•çš„å¤šæ¨¡æ€å‰æ²¿é¢†åŸŸã€‚æˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†å…³é”®æ•°æ®é›†ã€ç”¨ä¾‹ã€æŒ‘æˆ˜ä»¥åŠæ”¯æ’‘ç”Ÿæˆæ–¹æ³•çš„EEGç‰¹å¾ç¼–ç æ–¹æ³•ã€‚é€šè¿‡å¯¹åŸºäºEEGçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è¿›è¡Œç»“æ„åŒ–æ¦‚è¿°ï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›æ´å¯Ÿï¼Œä»¥ä¿ƒè¿›ç¥ç»è§£ç çš„å‘å±•ï¼Œæå‡è¾…åŠ©æŠ€æœ¯çš„æ€§èƒ½ï¼Œå¹¶æ‰©å±•è„‘æœºäº¤äº’çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12048v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>BCIsä¸GenAIçš„æ•´åˆä¸ºè„‘ä¿¡å·è§£ç æ‰“å¼€äº†æ–°é¢†åŸŸï¼Œä¿ƒè¿›è¾…åŠ©äº¤æµã€ç¥ç»ç½‘ç»œè¡¨ç¤ºå­¦ä¹ å’Œå¤šæ¨¡å¼æ•´åˆã€‚é€šè¿‡è„‘ç”µå›¾ï¼ˆEEGï¼‰å®ç°çš„BCIsä¸ºéä¾µå…¥æ€§åœ°è½¬åŒ–ç¥ç»æ´»åŠ¨ä¸ºæœ‰æ„ä¹‰è¾“å‡ºæä¾›äº†æ‰‹æ®µã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæå¤§åœ°æ”¹è¿›äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºEEGçš„å¤šæ¨¡å¼ç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬EEGè½¬å›¾åƒç”Ÿæˆå’ŒEEGè½¬æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶è®¨è®ºäº†æ–°å…´çš„EEGè½¬è¯­éŸ³åˆæˆé¢†åŸŸã€‚é€šè¿‡æä¾›å¯¹åŸºäºEEGçš„ç”Ÿæˆå¼AIçš„ç»“æ„æ€§æ¦‚è¿°ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æ´å¯Ÿï¼Œä»¥æ¨åŠ¨ç¥ç»è§£ç çš„å‘å±•ã€å¢å¼ºè¾…åŠ©æŠ€æœ¯ï¼Œå¹¶æ‰©å±•è„‘æœºäº¤äº’çš„è¾¹ç•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BCIä¸GenAIçš„æ•´åˆæ¨åŠ¨äº†è„‘ä¿¡å·è§£ç çš„æ–°å‘å±•ï¼ŒåŠ©åŠ›è¾…åŠ©äº¤æµã€ç¥ç»ç½‘ç»œè¡¨ç¤ºå­¦ä¹ å’Œå¤šæ¨¡å¼æ•´åˆã€‚</li>
<li>EEG-based BCIsä¸ºéä¾µå…¥æ€§åœ°è½¬åŒ–ç¥ç»æ´»åŠ¨æä¾›äº†æ‰‹æ®µã€‚</li>
<li>æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ï¼Œå¦‚GANså’ŒLLMsï¼Œå¤§å¹…æå‡äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åŸºäºEEGçš„å¤šæ¨¡å¼ç”Ÿæˆæœ€æ–°è¿›å±•åŒ…æ‹¬EEGè½¬å›¾åƒç”Ÿæˆå’ŒEEGè½¬æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>æ–°å…´çš„EEGè½¬è¯­éŸ³åˆæˆé¢†åŸŸæˆä¸ºå¤šæ¨¡æ€äº¤äº’çš„æ–°å‰æ²¿ã€‚</li>
<li>æ–‡ç« ç»¼è¿°äº†å…³é”®æ•°æ®é›†ã€åº”ç”¨åœºæ™¯ã€æŒ‘æˆ˜å’ŒEEGç‰¹å¾ç¼–ç æ–¹æ³•ï¼Œè¿™äº›éƒ½æ˜¯ç”Ÿæˆå¼æ–¹æ³•çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f89a92f9f6583f89738d30ff8feb081e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f4c288ef1da1c6353c7dc833140956e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93096ddd3246c583190a570d31c2b141.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NaturalL2S-End-to-End-High-quality-Multispeaker-Lip-to-Speech-Synthesis-with-Differential-Digital-Signal-Processing"><a href="#NaturalL2S-End-to-End-High-quality-Multispeaker-Lip-to-Speech-Synthesis-with-Differential-Digital-Signal-Processing" class="headerlink" title="NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis   with Differential Digital Signal Processing"></a>NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis   with Differential Digital Signal Processing</h2><p><strong>Authors:Yifan Liang, Fangkun Liu, Andong Li, Xiaodong Li, Chengshi Zheng</strong></p>
<p>Recent advancements in visual speech recognition (VSR) have promoted progress in lip-to-speech synthesis, where pre-trained VSR models enhance the intelligibility of synthesized speech by providing valuable semantic information. The success achieved by cascade frameworks, which combine pseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the transcribed text, highlights the benefits of leveraging VSR models. However, these methods typically rely on mel-spectrograms as an intermediate representation, which may introduce a key bottleneck: the domain gap between synthetic mel-spectrograms, generated from inherently error-prone lip-to-speech mappings, and real mel-spectrograms used to train vocoders. This mismatch inevitably degrades synthesis quality. To bridge this gap, we propose Natural Lip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic inductive biases with differentiable speech generation components. Specifically, we introduce a fundamental frequency (F0) predictor to capture prosodic variations in synthesized speech. The predicted F0 then drives a Differentiable Digital Signal Processing (DDSP) synthesizer to generate a coarse signal which serves as prior information for subsequent speech synthesis. Additionally, instead of relying on a reference speaker embedding as an auxiliary input, our approach achieves satisfactory performance on speaker similarity without explicitly modelling speaker characteristics. Both objective and subjective evaluation results demonstrate that NaturalL2S can effectively enhance the quality of the synthesized speech when compared to state-of-the-art methods. Our demonstration page is accessible at <a target="_blank" rel="noopener" href="https://yifan-liang.github.io/NaturalL2S/">https://yifan-liang.github.io/NaturalL2S/</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰çš„è¿›å±•æ¨åŠ¨äº†å”‡éŸ³åˆæˆæŠ€æœ¯çš„è¿›æ­¥ã€‚é¢„è®­ç»ƒçš„VSRæ¨¡å‹é€šè¿‡æä¾›æœ‰ä»·å€¼çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜äº†åˆæˆè¯­éŸ³çš„å¯è¯†åˆ«æ€§ã€‚çº§è”æ¡†æ¶ç»“åˆä¼ªVSRå’Œä¼ªæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æˆ–éšå¼åˆ©ç”¨è½¬å½•æ–‡æœ¬çš„æˆåŠŸï¼Œçªæ˜¾äº†åˆ©ç”¨VSRæ¨¡å‹çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºæ¢…å°”é¢‘è°±å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šç”±æœ¬è´¨ä¸Šæ˜¯æ˜“å‡ºé”™çš„å”‡éŸ³æ˜ å°„ç”Ÿæˆçš„åˆæˆæ¢…å°”é¢‘è°±å›¾ä¸ç”¨äºè®­ç»ƒvocoderçš„çœŸå®æ¢…å°”é¢‘è°±å›¾ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚è¿™ç§ä¸åŒ¹é…ä¸å¯é¿å…åœ°ä¼šé™ä½åˆæˆè´¨é‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†Natural Lip-to-Speechï¼ˆNaturalL2Sï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å£°å­¦å½’çº³åè§ä¸å¯å¾®åˆ†çš„è¯­éŸ³ç”Ÿæˆç»„ä»¶ç›¸ç»“åˆç«¯åˆ°ç«¯æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºé¢‘ï¼ˆF0ï¼‰é¢„æµ‹å™¨æ¥æ•æ‰åˆæˆè¯­éŸ³ä¸­çš„éŸµå¾‹å˜åŒ–ã€‚é¢„æµ‹çš„F0ç„¶åé©±åŠ¨ä¸€ä¸ªå¯å¾®åˆ†çš„æ•°å­—ä¿¡å·å¤„ç†ï¼ˆDDSPï¼‰åˆæˆå™¨ç”Ÿæˆä¸€ä¸ªç²—ç•¥çš„ä¿¡å·ï¼Œè¯¥ä¿¡å·ä½œä¸ºåç»­è¯­éŸ³åˆæˆçš„å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ²¡æœ‰ä¾é å‚è€ƒè¯´è¯äººåµŒå…¥ä½œä¸ºè¾…åŠ©è¾“å…¥æ¥å®ç°ä»¤äººæ»¡æ„çš„è¯´è¯äººç›¸ä¼¼æ€§è¡¨ç°ï¼Œæ— éœ€æ˜¾å¼å»ºæ¨¡è¯´è¯äººçš„ç‰¹ç‚¹ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸è‡ªç„¶ç•Œçš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒNaturalL2Så¯ä»¥æœ‰æ•ˆæé«˜åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢å¯è®¿é—®äºï¼š[<a target="_blank" rel="noopener" href="https://yifan-liang.github.io/NaturalL2S/]">https://yifan-liang.github.io/NaturalL2S/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12002v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†å”‡åˆ°è¯­éŸ³åˆæˆçš„è¿›æ­¥ã€‚é¢„è®­ç»ƒçš„è§†è§‰è¯­éŸ³è¯†åˆ«æ¨¡å‹ä¸ºåˆæˆè¯­éŸ³æä¾›äº†å®è´µçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºäº†è¯­éŸ³çš„å¯è¯†åˆ«æ€§ã€‚çº§è”æ¡†æ¶çš„æˆåŠŸï¼Œç»“åˆä¼ªè§†è§‰è¯­éŸ³è¯†åˆ«ä¸ä¼ªæ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯æˆ–éšå«åœ°ä½¿ç”¨è½¬å½•æ–‡æœ¬ï¼Œçªæ˜¾äº†åˆ©ç”¨è§†è§‰è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºæ¢…å°”é¢‘è°±å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œå¯èƒ½ä¼šå¼•å…¥ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šç”±å”‡åˆ°è¯­éŸ³æ˜ å°„æœ¬èº«å­˜åœ¨çš„é”™è¯¯ç”Ÿæˆçš„åˆæˆæ¢…å°”é¢‘è°±å›¾ä¸ç”¨äºè®­ç»ƒç¼–ç å™¨çš„çœŸå®æ¢…å°”é¢‘è°±å›¾ä¹‹é—´çš„åŸŸå·®è·ã€‚è¿™ç§ä¸åŒ¹é…ä¸å¯é¿å…åœ°é™ä½äº†åˆæˆè´¨é‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ç«¯åˆ°ç«¯çš„è‡ªç„¶å”‡åˆ°è¯­éŸ³æ¡†æ¶ï¼ˆNaturalL2Sï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å£°å­¦è¯±å¯¼åè§ä¸å¯å¾®åˆ†çš„è¯­éŸ³ç”Ÿæˆç»„ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºé¢‘é¢„æµ‹å™¨æ¥æ•æ‰åˆæˆè¯­éŸ³ä¸­çš„éŸµå¾‹å˜åŒ–ã€‚é¢„æµ‹çš„åŸºé¢‘é©±åŠ¨å¯å¾®åˆ†çš„æ•°å­—ä¿¡å·å¤„ç†åˆæˆå™¨ç”Ÿæˆç²—ç•¥ä¿¡å·ï¼Œä½œä¸ºåç»­è¯­éŸ³åˆæˆçš„å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä¾èµ–å‚è€ƒè¯´è¯äººåµŒå…¥ä½œä¸ºè¾…åŠ©è¾“å…¥å°±èƒ½å®ç°ä»¤äººæ»¡æ„çš„è¯´è¯äººç›¸ä¼¼æ€§å»ºæ¨¡ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ç»“æœå‡è¡¨æ˜ï¼Œä¸è‡ªç„¶å”‡åˆ°è¯­éŸ³æ¡†æ¶ç›¸æ¯”ï¼Œæœ€æ–°æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://yifan-liang.github.io/NaturalL2S/">é“¾æ¥</a>è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰çš„è¿›å±•æ¨åŠ¨äº†å”‡åˆ°è¯­éŸ³åˆæˆçš„è¿›æ­¥ã€‚</li>
<li>é¢„è®­ç»ƒçš„VSRæ¨¡å‹å¢å¼ºäº†åˆæˆè¯­éŸ³çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>çº§è”æ¡†æ¶ç»“åˆä¼ªVSRä¸ä¼ªæ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯æ˜¾ç¤ºäº†VSRæ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>ä¾èµ–æ¢…å°”é¢‘è°±å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼å¯èƒ½ä¼šå¼•å…¥åŸŸå·®è·é—®é¢˜ã€‚</li>
<li>åˆæˆæ¢…å°”é¢‘è°±å›¾ä¸çœŸå®æ¢…å°”é¢‘è°±å›¾ä¹‹é—´çš„åŸŸå·®è·é™ä½äº†åˆæˆè´¨é‡ã€‚</li>
<li>æå‡ºçš„NaturalL2Sæ¡†æ¶ç»“åˆäº†å£°å­¦è¯±å¯¼åè§ä¸å¯å¾®åˆ†çš„è¯­éŸ³ç”Ÿæˆç»„ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef4af6aed3b27e13211cbc7d5fd7c925.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-682ff72abd4b3b066ac68d265c9cbffa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fa22329f43cdd447a0da3372825fea2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Step-Audio-Unified-Understanding-and-Generation-in-Intelligent-Speech-Interaction"><a href="#Step-Audio-Unified-Understanding-and-Generation-in-Intelligent-Speech-Interaction" class="headerlink" title="Step-Audio: Unified Understanding and Generation in Intelligent Speech   Interaction"></a>Step-Audio: Unified Understanding and Generation in Intelligent Speech   Interaction</h2><p><strong>Authors:Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuting Yan, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu</strong></p>
<p>Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio">https://github.com/stepfun-ai/Step-Audio</a>. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³äº¤äº’ä½œä¸ºäººæœºåä½œçš„åŸºæœ¬æ¥å£ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¼€æºæ¨¡å‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è¯­éŸ³æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€åŠ¨æ€æ§åˆ¶èƒ½åŠ›ä¸è¶³ä»¥åŠæ™ºèƒ½æœ‰é™ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†Step-Audioï¼Œè¿™æ˜¯ä¸€ä¸ªå‡†å¤‡æŠ•å…¥ç”Ÿäº§çš„ç¬¬ä¸€æ¬¾å¼€æºè§£å†³æ–¹æ¡ˆã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š1ï¼‰ä¸€ä¸ªæ‹¥æœ‰130Bå‚æ•°çš„ç»Ÿä¸€è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°äº†ç»Ÿä¸€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå…¶ä¸­Step-Audio-Chatç‰ˆæœ¬å·²å¼€æºï¼›2ï¼‰ä¸€ä¸ªç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»æµå®æƒ çš„è¯­éŸ³å…‹éš†æ¡†æ¶ï¼Œå¹¶é€šè¿‡è’¸é¦æŠ€æœ¯äº§ç”Ÿäº†å¼€æºçš„è½»é‡çº§Step-Audio-TTS-3Bæ¨¡å‹ï¼›3ï¼‰ä¸€ä¸ªæŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®ç°ä¸åŒæ–¹è¨€ã€æƒ…æ„Ÿã€æ­Œå”±å’ŒRAPçš„åŠ¨æ€è°ƒæ•´ï¼›4ï¼‰ä¸€ä¸ªå¢å¼ºå‹è®¤çŸ¥æ¶æ„ï¼Œé€šè¿‡å¢åŠ å·¥å…·è°ƒç”¨å’Œè§’è‰²æ‰®æ¼”èƒ½åŠ›ï¼Œæœ‰æ•ˆç®¡ç†å¤æ‚ä»»åŠ¡ã€‚åŸºäºæˆ‘ä»¬æ–°çš„StepEval-Audio-360è¯„ä¼°åŸºå‡†ï¼ŒStep-Audioåœ¨äººä½“è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢ã€‚åœ¨LLaMA Questionç­‰å¼€æºåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æ€§èƒ½æé«˜äº†9.3%ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ¨åŠ¨å¼€æºå¤šæ¨¡æ€è¯­è¨€æŠ€æœ¯å‘å±•çš„æ‰¿è¯ºã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/stepfun-ai/Step-Audioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹åä¸ºStep-Audioçš„å¼€æ”¾æºä»£ç è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå®æ—¶è¯­éŸ³äº¤äº’ï¼Œè§£å†³äº†å½“å‰é¢ä¸´çš„æ•°æ®æ”¶é›†æˆæœ¬é«˜ã€åŠ¨æ€æ§åˆ¶å¼±å’Œæ™ºèƒ½æœ‰é™ç­‰æŒ‘æˆ˜ã€‚å®ƒåŒ…å«ä¸€ç³»åˆ—å…³é”®è´¡çŒ®ï¼Œå¦‚ç»Ÿä¸€è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ã€ç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“ã€æŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿå’Œå¢å¼ºçš„è®¤çŸ¥æ¶æ„ç­‰ã€‚åŸºäºStepEval-Audio-360è¯„ä¼°åŸºå‡†ï¼ŒStep-Audioåœ¨äººæœºè¯„ä¼°ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½æå‡9.3%ï¼Œæ˜¾ç¤ºå‡ºå¯¹æ¨åŠ¨å¼€æºå¤šæ¨¡æ€è¯­è¨€æŠ€æœ¯å‘å±•çš„æ‰¿è¯ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Step-Audioæ˜¯ä¸€ç§è§£å†³å®æ—¶è¯­éŸ³äº¤äº’æŒ‘æˆ˜çš„ç”Ÿäº§å°±ç»ªå¼€æºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®ƒåŒ…å«å¤šæ¨¡æ€ç»Ÿä¸€è¯­éŸ³æ–‡æœ¬æ¨¡å‹ï¼Œå®ç°äº†ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€ã€‚</li>
<li>é€šè¿‡è’¸é¦æŠ€æœ¯ï¼ŒStep-Audioèƒ½å¤Ÿç”Ÿäº§è½»é‡çº§çš„è¯­éŸ³åˆæˆæ¨¡å‹ã€‚</li>
<li>æä¾›äº†æŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿï¼Œå¯å®ç°è·¨æ–¹è¨€ã€æƒ…æ„Ÿã€æ­Œå”±å’ŒRAPçš„åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>å¢å¼ºçš„è®¤çŸ¥æ¶æ„ä½¿Step-Audioèƒ½å¤Ÿç®¡ç†å¤æ‚ä»»åŠ¡ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œè§’è‰²æ‰®æ¼”ã€‚</li>
<li>åŸºäºStepEval-Audio-360è¯„ä¼°åŸºå‡†ï¼ŒStep-Audioåœ¨äººæœºè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-07b2b4440ed6a01006e31ed673f3b1e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31d6d360dc1ec5145451ae0ac4c21b48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e801713a5fa5f6ee5ecec11004556e3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LMFCA-Net-A-Lightweight-Model-for-Multi-Channel-Speech-Enhancement-with-Efficient-Narrow-Band-and-Cross-Band-Attention"><a href="#LMFCA-Net-A-Lightweight-Model-for-Multi-Channel-Speech-Enhancement-with-Efficient-Narrow-Band-and-Cross-Band-Attention" class="headerlink" title="LMFCA-Net: A Lightweight Model for Multi-Channel Speech Enhancement with   Efficient Narrow-Band and Cross-Band Attention"></a>LMFCA-Net: A Lightweight Model for Multi-Channel Speech Enhancement with   Efficient Narrow-Band and Cross-Band Attention</h2><p><strong>Authors:Yaokai Zhang, Hanchen Pei, Wanqi Wang, Gongping Huang</strong></p>
<p>Deep learning based end-to-end multi-channel speech enhancement methods have achieved impressive performance by leveraging sub-band, cross-band, and spatial information. However, these methods often demand substantial computational resources, limiting their practicality on terminal devices. This paper presents a lightweight multi-channel speech enhancement network with decoupled fully connected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis decoupled fully-connected attention (T-FCA) and frequency-axis decoupled fully-connected attention (F-FCA) mechanisms to effectively capture long-range narrow-band and cross-band information without recurrent units. Experimental results show that LMFCA-Net performs comparably to state-of-the-art methods while significantly reducing computational complexity and latency, making it a promising solution for practical applications. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯å¤šé€šé“è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å­å¸¦ã€è·¨å¸¦å’Œç©ºé—´ä¿¡æ¯å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œåœ¨ç»ˆç«¯è®¾å¤‡ä¸Šå®ç”¨æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å¤šé€šé“è¯­éŸ³å¢å¼ºç½‘ç»œï¼Œå…·æœ‰è§£è€¦çš„å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆLMFCA-Netï¼‰ã€‚æå‡ºçš„LMFCA-Netå¼•å…¥äº†æ—¶é—´è½´è§£è€¦çš„å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆT-FCAï¼‰å’Œé¢‘ç‡è½´è§£è€¦çš„å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆF-FCAï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰é•¿è·ç¦»çª„å¸¦å’Œè·¨å¸¦ä¿¡æ¯ï¼Œè€Œæ— éœ€å¾ªç¯å•å…ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLMFCA-Netä¸æœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ç›¸æ¯”è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚æ€§å’Œå»¶è¿Ÿï¼Œä½¿å…¶æˆä¸ºå®é™…åº”ç”¨ä¸­å¾ˆæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11462v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è½»é‡çº§å¤šé€šé“è¯­éŸ³å¢å¼ºç½‘ç»œï¼Œç§°ä¸ºLMFCA-Netã€‚è¯¥ç½‘ç»œå¼•å…¥äº†æ—¶é—´è½´è§£è€¦å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆT-FCAï¼‰å’Œé¢‘ç‡è½´è§£è€¦å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆF-FCAï¼‰æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆæ•æ‰é•¿è·ç¦»çª„å¸¦å’Œè·¨å¸¦ä¿¡æ¯ï¼Œä¸”æ— éœ€é€’å½’å•å…ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLMFCA-Netä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”è¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—å¤æ‚æ€§å’Œå»¶è¿Ÿï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMFCA-Netæ˜¯ä¸€ç§è½»é‡çº§çš„æ·±åº¦å­¦ä¹ å¤šé€šé“è¯­éŸ³å¢å¼ºç½‘ç»œã€‚</li>
<li>è¯¥ç½‘ç»œå¼•å…¥äº†æ—¶é—´è½´è§£è€¦å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆT-FCAï¼‰å’Œé¢‘ç‡è½´è§£è€¦å…¨è¿æ¥æ³¨æ„åŠ›ï¼ˆF-FCAï¼‰æœºåˆ¶ã€‚</li>
<li>LMFCA-Netèƒ½æœ‰æ•ˆæ•æ‰é•¿è·ç¦»çª„å¸¦å’Œè·¨å¸¦ä¿¡æ¯ï¼Œè€Œæ— éœ€ä½¿ç”¨é€’å½’å•å…ƒã€‚</li>
<li>å®éªŒè¡¨æ˜LMFCA-Netçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ã€‚</li>
<li>LMFCA-Netæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚æ€§å’Œå»¶è¿Ÿã€‚</li>
<li>LMFCA-Netä¸ºå®é™…åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46f52d40c7ecdc8b29f2eaf8ed050beb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08f5021201ce4ac80df7a00e8ad721e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ace143816df154477ad8706048968e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a3756fd5481f8b0ab8e6f83772c525.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f37d85ba2f8ce626e1ded6590f6a1e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef36a82b5d2d0cce715babddb12e612.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DuplexMamba-Enhancing-Real-time-Speech-Conversations-with-Duplex-and-Streaming-Capabilities"><a href="#DuplexMamba-Enhancing-Real-time-Speech-Conversations-with-Duplex-and-Streaming-Capabilities" class="headerlink" title="DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and   Streaming Capabilities"></a>DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and   Streaming Capabilities</h2><p><strong>Authors:Xiangyu Lu, Wang Xu, Haoyu Wang, Hongyun Zhou, Haiyan Zhao, Conghui Zhu, Tiejun Zhao, Muyun Yang</strong></p>
<p>Real-time speech conversation is essential for natural and efficient human-machine interactions, requiring duplex and streaming capabilities. Traditional Transformer-based conversational chatbots operate in a turn-based manner and exhibit quadratic computational complexity that grows as the input size increases. In this paper, we propose DuplexMamba, a Mamba-based end-to-end multimodal duplex model for speech-to-text conversation. DuplexMamba enables simultaneous input processing and output generation, dynamically adjusting to support real-time streaming. Specifically, we develop a Mamba-based speech encoder and adapt it with a Mamba-based language model. Furthermore, we introduce a novel duplex decoding strategy that enables DuplexMamba to process input and generate output simultaneously. Experimental results demonstrate that DuplexMamba successfully implements duplex and streaming capabilities while achieving performance comparable to several recently developed Transformer-based models in automatic speech recognition (ASR) tasks and voice assistant benchmark evaluations. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³å¯¹è¯å¯¹äºè‡ªç„¶é«˜æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œéœ€è¦åŒå‘å’Œæµå¼å¤„ç†èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„åŸºäºTransformerçš„èŠå¤©æœºå™¨äººä»¥è½®æ¬¡ä¸ºåŸºç¡€è¿›è¡Œæ“ä½œï¼Œè¡¨ç°å‡ºéšç€è¾“å…¥å¤§å°å¢åŠ è€Œäº§ç”Ÿçš„äºŒæ¬¡è®¡ç®—å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMambaçš„ç«¯åˆ°ç«¯å¤šæ¨¡å¼åŒå‘å¯¹è¯æ¨¡å‹DuplexMambaï¼Œç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬çš„å¯¹è¯ã€‚DuplexMambaèƒ½å¤ŸåŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆè¾“å‡ºï¼ŒåŠ¨æ€è°ƒæ•´ä»¥æ”¯æŒå®æ—¶æµå¼å¤„ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºMambaçš„è¯­éŸ³ç¼–ç å™¨ï¼Œå¹¶å°†å…¶ä¸åŸºäºMambaçš„è¯­è¨€æ¨¡å‹ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŒå‘è§£ç ç­–ç•¥ï¼Œä½¿DuplexMambaèƒ½å¤ŸåŒæ—¶å¤„ç†è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDuplexMambaæˆåŠŸå®ç°äº†åŒå‘å’Œæµå¼å¤„ç†åŠŸèƒ½ï¼ŒåŒæ—¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡å’Œè¯­éŸ³åŠ©æ‰‹åŸºå‡†è¯„ä¼°ä¸­å®ç°äº†ä¸æœ€è¿‘å¼€å‘çš„å‡ ä¸ªåŸºäºTransformerçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11123v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMambaçš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€åŒå‘æ¨¡å‹DuplexMambaï¼Œç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬çš„å¯¹è¯ã€‚è¯¥æ¨¡å‹å¯å®ç°å®æ—¶æµå¼å¤„ç†ï¼ŒåŒæ—¶æ”¯æŒè¾“å…¥å¤„ç†å’Œè¾“å‡ºç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºMambaçš„è¯­éŸ³ç¼–ç å™¨ï¼Œå¹¶é€‚é…äº†åŸºäºMambaçš„è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŒå‘è§£ç ç­–ç•¥ï¼Œä½¿DuplexMambaèƒ½å¤ŸåŒæ—¶å¤„ç†è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDuplexMambaæˆåŠŸå®ç°äº†åŒå‘å’Œæµå¼å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡å’Œè¯­éŸ³åŠ©æ‰‹åŸºå‡†è¯„ä¼°ä¸­å–å¾—äº†ä¸æœ€è¿‘å¼€å‘çš„å‡ ç§åŸºäºTransformerçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>1.DuplexMambaæ˜¯åŸºäºMambaçš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€åŒå‘æ¨¡å‹ï¼Œç”¨äºå®ç°è¯­éŸ³åˆ°æ–‡æœ¬çš„å®æ—¶å¯¹è¯ã€‚<br>2.DuplexMambaæ”¯æŒå®æ—¶æµå¼å¤„ç†ï¼Œå®ç°è¾“å…¥å¤„ç†å’Œè¾“å‡ºç”Ÿæˆçš„åŒæ­¥è¿›è¡Œã€‚<br>3.é€šè¿‡å¼€å‘åŸºäºMambaçš„è¯­éŸ³ç¼–ç å™¨å’Œé€‚é…åŸºäºMambaçš„è¯­è¨€æ¨¡å‹ï¼ŒDuplexMambaå¯æœ‰æ•ˆå¤„ç†è¯­éŸ³æ•°æ®ã€‚<br>4.å¼•å…¥æ–°é¢–çš„åŒå‘è§£ç ç­–ç•¥ï¼Œä½¿DuplexMambaèƒ½å¤ŸåŒæ—¶å¤„ç†è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºå“åº”ã€‚<br>5.å®éªŒè¯å®DuplexMambaåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚<br>6.DuplexMambaåœ¨è¯­éŸ³åŠ©æ‰‹åŸºå‡†è¯„ä¼°ä¸­çš„è¡¨ç°ä¸æœ€æ–°Transformeræ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07fe69e32227e0e63ce7a585ee7b0959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c44731a4556564987beb4da70e205175.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58b857ce467d28a8f4e477fa921445c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d8a0cad8a94418de9bf05dc5300863f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-172d4750c8a8276785dc495f07b1cc5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86fd669d2684ec09cb09f4decfd9262f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SpeechT-RAG-Reliable-Depression-Detection-in-LLMs-with-Retrieval-Augmented-Generation-Using-Speech-Timing-Information"><a href="#SpeechT-RAG-Reliable-Depression-Detection-in-LLMs-with-Retrieval-Augmented-Generation-Using-Speech-Timing-Information" class="headerlink" title="SpeechT-RAG: Reliable Depression Detection in LLMs with   Retrieval-Augmented Generation Using Speech Timing Information"></a>SpeechT-RAG: Reliable Depression Detection in LLMs with   Retrieval-Augmented Generation Using Speech Timing Information</h2><p><strong>Authors:Xiangyu Zhang, Hexin Liu, Qiquan Zhang, Beena Ahmed, Julien Epps</strong></p>
<p>Large Language Models (LLMs) have been increasingly adopted for health-related tasks, yet their performance in depression detection remains limited when relying solely on text input. While Retrieval-Augmented Generation (RAG) typically enhances LLM capabilities, our experiments indicate that traditional text-based RAG systems struggle to significantly improve depression detection accuracy. This challenge stems partly from the rich depression-relevant information encoded in acoustic speech patterns information that current text-only approaches fail to capture effectively. To address this limitation, we conduct a systematic analysis of temporal speech patterns, comparing healthy individuals with those experiencing depression. Based on our findings, we introduce Speech Timing-based Retrieval-Augmented Generation, SpeechT-RAG, a novel system that leverages speech timing features for both accurate depression detection and reliable confidence estimation. This integrated approach not only outperforms traditional text-based RAG systems in detection accuracy but also enhances uncertainty quantification through a confidence scoring mechanism that naturally extends from the same temporal features. Our unified framework achieves comparable results to fine-tuned LLMs without additional training while simultaneously addressing the fundamental requirements for both accuracy and trustworthiness in mental health assessment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†åœ¨ä»…ä¾èµ–æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šå¸¸å¯ä»¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„RAGç³»ç»Ÿåœ¨æé«˜æŠ‘éƒç—‡æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚è¿™ä¸€æŒ‘æˆ˜éƒ¨åˆ†æºäºç¼–ç åœ¨è¯­éŸ³æ¨¡å¼ä¿¡æ¯ä¸­çš„ä¸°å¯ŒæŠ‘éƒç—‡ç›¸å…³ä¿¡æ¯ï¼Œè€Œå½“å‰ä»…ä½¿ç”¨æ–‡æœ¬çš„æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°æ•è·è¿™äº›ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ—¶é—´è¯­éŸ³æ¨¡å¼è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œæ¯”è¾ƒäº†å¥åº·ä¸ªä½“å’ŒæŠ‘éƒç—‡æ‚£è€…çš„è¯­éŸ³æ¨¡å¼ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè¯­éŸ³æ—¶åºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆSpeechT-RAGï¼‰è¿™ä¸€æ–°å‹ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨è¯­éŸ³æ—¶åºç‰¹å¾è¿›è¡Œå‡†ç¡®çš„æŠ‘éƒç—‡æ£€æµ‹å’Œå¯é çš„å¯ä¿¡åº¦è¯„ä¼°ã€‚è¿™ç§ç»¼åˆæ–¹æ³•ä¸ä»…ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„RAGç³»ç»Ÿåœ¨æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢ï¼Œè€Œä¸”è¿˜é€šè¿‡ä¿¡å¿ƒè¯„åˆ†æœºåˆ¶å¢å¼ºäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè¯¥æœºåˆ¶è‡ªç„¶åœ°æ¥æºäºç›¸åŒçš„æ—¶åºç‰¹å¾ã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°äº†ä¸å¾®è°ƒLLMç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶æ»¡è¶³äº†å¿ƒç†å¥åº·è¯„ä¼°ä¸­å¯¹å‡†ç¡®æ€§å’Œå¯é æ€§çš„åŸºæœ¬è¦æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10950v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†åœ¨ä»…ä¾èµ–æ–‡æœ¬è¾“å…¥è¿›è¡ŒæŠ‘éƒç—‡æ£€æµ‹æ—¶ï¼Œå…¶æ€§èƒ½è¡¨ç°å—é™ã€‚å°½ç®¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯é€šå¸¸èƒ½æå‡LLMçš„èƒ½åŠ›ï¼Œä½†å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿæ–‡æœ¬ä¸ºåŸºç¡€çš„RAGç³»ç»Ÿåœ¨æé«˜æŠ‘éƒç—‡æ£€æµ‹å‡†ç¡®åº¦æ–¹é¢æ•ˆæœæœ‰é™ã€‚è¿™éƒ¨åˆ†æŒ‘æˆ˜æºäºè¯­éŸ³ä¸­çš„æ—¶åºæ¨¡å¼æ‰€è•´å«ä¸°å¯Œçš„æŠ‘éƒç—‡ç›¸å…³ä¿¡æ¯ï¼Œè€Œå½“å‰ä»…ä¾èµ–æ–‡æœ¬çš„æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰è¿™äº›ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸€ç³»åˆ—é’ˆå¯¹å¥åº·ä¸ªä½“å’ŒæŠ‘éƒç—‡æ‚£è€…ä¹‹é—´è¯­éŸ³æ—¶åºæ¨¡å¼çš„ç³»ç»Ÿåˆ†æï¼Œå¹¶æ®æ­¤æå‡ºäº†åŸºäºè¯­éŸ³æ—¶åºç‰¹å¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿâ€”â€”SpeechT-RAGã€‚è¯¥ç³»ç»Ÿä¸ä»…æé«˜äº†æŠ‘éƒç—‡æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œè¿˜é€šè¿‡ä¿¡å¿ƒè¯„åˆ†æœºåˆ¶å¢å¼ºäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè¯¥æœºåˆ¶è‡ªç„¶åœ°æ‰©å±•è‡ªç›¸åŒçš„æ—¶åºç‰¹å¾ã€‚åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒSpeechT-RAGå–å¾—äº†ä¸å¾®è°ƒLLMç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶æ»¡è¶³äº†ç²¾ç¥å¥åº·è¯„ä¼°å¯¹å‡†ç¡®æ€§å’Œå¯é æ€§çš„åŸºæœ¬è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„æ€§èƒ½å—é™äºä»…ä½¿ç”¨æ–‡æœ¬è¾“å…¥ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åœ¨æå‡LLMåœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸Šçš„è¡¨ç°æ–¹é¢æ•ˆæœæœ‰é™ã€‚</li>
<li>è¯­éŸ³ä¸­çš„æ—¶åºæ¨¡å¼åŒ…å«ä¸°å¯Œçš„æŠ‘éƒç—‡ç›¸å…³ä¿¡æ¯ï¼Œè€Œä¼ ç»Ÿæ–‡æœ¬æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰ã€‚</li>
<li>é€šè¿‡å¯¹å¥åº·ä¸ªä½“å’ŒæŠ‘éƒç—‡æ‚£è€…ä¹‹é—´çš„è¯­éŸ³æ—¶åºæ¨¡å¼è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œæå‡ºäº†SpeechT-RAGç³»ç»Ÿã€‚</li>
<li>SpeechT-RAGç³»ç»Ÿåˆ©ç”¨è¯­éŸ³æ—¶åºç‰¹å¾è¿›è¡ŒæŠ‘éƒç—‡æ£€æµ‹å’Œä¿¡å¿ƒè¯„ä¼°ï¼Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>SpeechT-RAGé€šè¿‡ä¿¡å¿ƒè¯„åˆ†æœºåˆ¶å¢å¼ºäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè¯¥æœºåˆ¶è‡ªç„¶åœ°æ‰©å±•è‡ªè¯­éŸ³æ—¶åºç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdef7e78ccda7459ab2e45c7553d6a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da61bdc072194d245fa80d802aa1143f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-384cf19146b0d6638180d99d96fe0d62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2a5d40d3ac6f138ef604dfea04f172d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ef5de101ade385181e6808b85f10577.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="NeuroAMP-A-Novel-End-to-end-General-Purpose-Deep-Neural-Amplifier-for-Personalized-Hearing-Aids"><a href="#NeuroAMP-A-Novel-End-to-end-General-Purpose-Deep-Neural-Amplifier-for-Personalized-Hearing-Aids" class="headerlink" title="NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for   Personalized Hearing Aids"></a>NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for   Personalized Hearing Aids</h2><p><strong>Authors:Shafique Ahmed, Ryandhimas E. Zezario, Hui-Guan Yuan, Amir Hussain, Hsin-Min Wang, Wei-Ho Chung, Yu Tsao</strong></p>
<p>The prevalence of hearing aids is increasing. However, optimizing the amplification processes of hearing aids remains challenging due to the complexity of integrating multiple modular components in traditional methods. To address this challenge, we present NeuroAMP, a novel deep neural network designed for end-to-end, personalized amplification in hearing aids. NeuroAMP leverages both spectral features and the listenerâ€™s audiogram as inputs, and we investigate four architectures: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer. We also introduce Denoising NeuroAMP, an extension that integrates noise reduction along with amplification capabilities for improved performance in real-world scenarios. To enhance generalization, a comprehensive data augmentation strategy was employed during training on diverse speech (TIMIT and TMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the Hearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index (HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the Transformer architecture within NeuroAMP achieves the best performance, with SRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI) on the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy maintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ). Furthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC approach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a 10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results highlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable improvements in personalized hearing aid amplification. </p>
<blockquote>
<p>åŠ©å¬å™¨çš„æ™®åŠç‡æ­£åœ¨ä¸æ–­æé«˜ã€‚ç„¶è€Œï¼Œç”±äºä¼ ç»Ÿæ–¹æ³•ä¸­æ•´åˆå¤šä¸ªæ¨¡å—åŒ–ç»„ä»¶çš„å¤æ‚æ€§ï¼Œä¼˜åŒ–åŠ©å¬å™¨çš„æ”¾å¤§è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeuroAMPï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºåŠ©å¬å™¨ç«¯åˆ°ç«¯ä¸ªæ€§åŒ–æ”¾å¤§è€Œè®¾è®¡çš„æ–°å‹æ·±åº¦ç¥ç»ç½‘ç»œã€‚NeuroAMPåˆ©ç”¨é¢‘è°±ç‰¹å¾å’Œå¬è€…çš„å¬åŠ›å›¾ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å››ç§æ¶æ„ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ã€å·ç§¯å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰å’ŒTransformerã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Denoising NeuroAMPï¼Œå®ƒæ˜¯NeuroAMPçš„æ‰©å±•ï¼Œé›†æˆäº†é™å™ªåŠŸèƒ½ï¼Œä»¥æé«˜ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨å¤šæ ·çš„è¯­éŸ³ï¼ˆTIMITå’ŒTMHINTï¼‰å’ŒéŸ³ä¹ï¼ˆCadenza Challenge MUSICï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œé‡‡ç”¨äº†ä¸€ç§å…¨é¢çš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚ä½¿ç”¨åŠ©å¬è¯­éŸ³æ„ŸçŸ¥æŒ‡æ•°ï¼ˆHASPIï¼‰ã€åŠ©å¬è¯­éŸ³è´¨é‡æŒ‡æ•°ï¼ˆHASQIï¼‰å’ŒåŠ©å¬éŸ³é¢‘è´¨é‡æŒ‡æ•°ï¼ˆHAAQIï¼‰è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒNeuroAMPä¸­çš„Transformeræ¶æ„è¡¨ç°æœ€ä½³ï¼Œåœ¨TIMITä¸Šçš„HASQIå’ŒHASPIå¾—åˆ†åˆ†åˆ«ä¸º0.9927å’Œ0.9905ï¼Œåœ¨Cadenza Challenge MUSICæ•°æ®é›†ä¸Šçš„HAAQIå¾—åˆ†ä¸º0.9738ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®å¢å¼ºç­–ç•¥åœ¨æœªè§è¿‡çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚VCTKã€MUSDB18-HQï¼‰ä¸Šä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDenoising NeuroAMPåœ¨VoiceBank+DEMANDæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„NAL-R+WDRCæ–¹æ³•å’Œä¸¤é˜¶æ®µåŸºçº¿æ–¹æ³•ï¼ŒHASPIå’ŒHASQIå¾—åˆ†å‡æé«˜äº†10%ã€‚è¿™äº›ç»“æœçªå‡ºäº†NeuroAMPå’ŒDenoising NeuroAMPåœ¨å®ç°ä¸ªæ€§åŒ–åŠ©å¬å™¨æ”¾å¤§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10822v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¬åŠ›è¾…åŠ©è®¾å¤‡çš„éœ€æ±‚å¢é•¿ï¼Œä½†ç”±äºé›†æˆå¤šä¸ªæ¨¡å—åŒ–ç»„ä»¶çš„å¤æ‚æ€§ï¼Œä¼˜åŒ–å¬åŠ›è¾…åŠ©è®¾å¤‡çš„æ”¾å¤§è¿‡ç¨‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºNeuroAMPçš„æ–°å‹æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç”¨äºå¬åŠ›è¾…åŠ©è®¾å¤‡çš„ç«¯åˆ°ç«¯ä¸ªæ€§åŒ–æ”¾å¤§ã€‚NeuroAMPåˆ©ç”¨é¢‘è°±ç‰¹å¾å’Œå¬è€…çš„å¬åŠ›å­¦æ£€æµ‹ä½œä¸ºè¾“å…¥ï¼Œç ”ç©¶äº†å››ç§æ¶æ„ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†é›†æˆé™å™ªåŠŸèƒ½çš„Denoising NeuroAMPï¼Œä»¥æé«˜ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒNeuroAMPä¸­çš„Transformeræ¶æ„è¡¨ç°æœ€ä½³ï¼ŒDenoising NeuroAMPåœ¨æœªè§æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¾—åˆ°ç»´æŒå¹¶è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚è¿™çªæ˜¾äº†NeuroAMPå’ŒDenoising NeuroAMPåœ¨ä¸ªæ€§åŒ–å¬åŠ›è¾…åŠ©è®¾å¤‡æ”¾å¤§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¬åŠ›è¾…åŠ©è®¾å¤‡éœ€æ±‚å¢é•¿ï¼Œä¼˜åŒ–æ”¾å¤§è¿‡ç¨‹æˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>NeuroAMPæ˜¯ä¸€ç§æ–°å‹æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç”¨äºå¬åŠ›è¾…åŠ©è®¾å¤‡çš„ç«¯åˆ°ç«¯ä¸ªæ€§åŒ–æ”¾å¤§ã€‚</li>
<li>NeuroAMPåˆ©ç”¨é¢‘è°±ç‰¹å¾å’Œå¬è€…çš„å¬åŠ›å­¦æ£€æµ‹ä½œä¸ºè¾“å…¥ï¼Œç ”ç©¶äº†CNNã€LSTMã€CRNNå’ŒTransformerå››ç§æ¶æ„ã€‚</li>
<li>Denoising NeuroAMPé›†æˆäº†é™å™ªåŠŸèƒ½ï¼Œæé«˜äº†ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜Transformeræ¶æ„åœ¨NeuroAMPä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>Denoising NeuroAMPåœ¨æœªè§æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¾—åˆ°ç»´æŒï¼Œå¹¶è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3aa6aebcba01b461dfe51f9955626d62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819867d65cd68297999061b641025940.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c17c47e61626217d5ff7f28cd1b81fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a7e331e7ab36bbda37d8a3908f4f1b1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VarGes-Improving-Variation-in-Co-Speech-3D-Gesture-Generation-via-StyleCLIPS"><a href="#VarGes-Improving-Variation-in-Co-Speech-3D-Gesture-Generation-via-StyleCLIPS" class="headerlink" title="VarGes: Improving Variation in Co-Speech 3D Gesture Generation via   StyleCLIPS"></a>VarGes: Improving Variation in Co-Speech 3D Gesture Generation via   StyleCLIPS</h2><p><strong>Authors:Ming Meng, Ke Mu, Yonggui Zhu, Zhe Zhu, Haoyu Sun, Heyang Yan, Zhaoxin Fan</strong></p>
<p>Generating expressive and diverse human gestures from audio is crucial in fields like human-computer interaction, virtual reality, and animation. Though existing methods have achieved remarkable performance, they often exhibit limitations due to constrained dataset diversity and the restricted amount of information derived from audio inputs. To address these challenges, we present VarGes, a novel variation-driven framework designed to enhance co-speech gesture generation by integrating visual stylistic cues while maintaining naturalness. Our approach begins with the Variation-Enhanced Feature Extraction (VEFE) module, which seamlessly incorporates \textcolor{blue}{style-reference} video data into a 3D human pose estimation network to extract StyleCLIPS, thereby enriching the input with stylistic information. Subsequently, we employ the Variation-Compensation Style Encoder (VCSE), a transformer-style encoder equipped with an additive attention mechanism pooling layer, to robustly encode diverse StyleCLIPS representations and effectively manage stylistic variations. Finally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio features with StyleCLIPS encodings via cross-attention, injecting this fused data into a cross-conditional autoregressive model to modulate 3D human gesture generation based on audio input and stylistic clues. The efficacy of our approach is validated on benchmark datasets, where it outperforms existing methods in terms of gesture diversity and naturalness. The code and video results will be made publicly available upon acceptance:<a target="_blank" rel="noopener" href="https://github.com/mookerr/VarGES/">https://github.com/mookerr/VarGES/</a> . </p>
<blockquote>
<p>åœ¨äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®å’ŒåŠ¨ç”»ç­‰é¢†åŸŸä¸­ï¼Œä»éŸ³é¢‘ç”Ÿæˆè¡¨è¾¾ä¸°å¯Œã€å¤šæ ·åŒ–çš„æ‰‹åŠ¿è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†ç”±äºæ•°æ®é›†å¤šæ ·æ€§çš„é™åˆ¶ä»¥åŠä»éŸ³é¢‘è¾“å…¥ä¸­è·å–çš„æœ‰é™ä¿¡æ¯ï¼Œå®ƒä»¬å¾€å¾€å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VarGesï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä»¥å˜åŒ–é©±åŠ¨ä¸ºæ ¸å¿ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆè§†è§‰é£æ ¼çº¿ç´¢æ¥å¢å¼ºä¸è¯­éŸ³åŒæ­¥çš„æ‰‹åŠ¿ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å§‹äºå¢å¼ºç‰¹å¾æå–æ¨¡å—ï¼ˆVEFEï¼‰ï¼Œè¯¥æ¨¡å—æ— ç¼åœ°ç»“åˆäº†é£æ ¼å‚è€ƒè§†é¢‘æ•°æ®åˆ°ä¸€ä¸ª3Däººä½“å§¿æ€ä¼°è®¡ç½‘ç»œä¸­ï¼Œä»è€Œæå–StyleCLIPSï¼Œä½¿è¾“å…¥ä¿¡æ¯ä¸°å¯Œå¹¶å¸¦æœ‰é£æ ¼ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é…å¤‡é™„åŠ æ³¨æ„åŠ›æœºåˆ¶æ± å±‚çš„å˜ä½“è¡¥å¿é£æ ¼ç¼–ç å™¨ï¼ˆVCSEï¼‰ï¼Œä»¥ç¨³å¥åœ°ç¼–ç å¤šæ ·åŒ–çš„StyleCLIPSè¡¨ç¤ºå¹¶æœ‰æ•ˆåœ°ç®¡ç†é£æ ¼å˜åŒ–ã€‚æœ€åï¼Œä»¥å˜åŒ–é©±åŠ¨çš„æ‰‹åŠ¿é¢„æµ‹å™¨ï¼ˆVDGPï¼‰æ¨¡å—é€šè¿‡è·¨æ³¨æ„åŠ›èåˆMFCCéŸ³é¢‘ç‰¹å¾ä¸StyleCLIPSç¼–ç ï¼Œå°†æ­¤èåˆæ•°æ®æ³¨å…¥è·¨æ¡ä»¶è‡ªå›å½’æ¨¡å‹ï¼Œä»¥æ ¹æ®éŸ³é¢‘è¾“å…¥å’Œé£æ ¼çº¿ç´¢è°ƒèŠ‚3Dæ‰‹åŠ¿ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å·²åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œåœ¨æ‰‹åŠ¿å¤šæ ·æ€§å’Œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œè§†é¢‘ç»“æœå°†åœ¨æ¥å—åå…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/mookerr/VarGES/%E3%80%82">https://github.com/mookerr/VarGES/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10729v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®å’ŒåŠ¨ç”»ç­‰é¢†åŸŸä¸­ï¼Œç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ä¸°å¯Œå¤šå˜çš„äººç±»åŠ¨ä½œè¡¨æƒ…éå¸¸é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•å°½ç®¡å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ•ˆæœï¼Œä½†ç”±äºæ•°æ®é›†å¤šæ ·æ€§å—é™ä»¥åŠä»éŸ³é¢‘è¾“å…¥ä¸­è·å¾—çš„ä¿¡æ¯æœ‰é™ï¼Œä»å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VarGesè¿™ä¸€åˆ›æ–°æ€§çš„ä»¥å˜åŒ–é©±åŠ¨çš„æ–¹æ³•æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè§†è§‰é£æ ¼çº¿ç´¢æ¥å¢å¼ºè¯­éŸ³åŠ¨ä½œè¡¨æƒ…çš„ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æ€§ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šVariation-Enhanced Feature Extractionæ¨¡å—ç”¨äºèå…¥é£æ ¼å‚è€ƒè§†é¢‘æ•°æ®ï¼Œå¹¶æå–StyleCLIPSä¿¡æ¯ä¸°å¯Œè¾“å…¥å†…å®¹ï¼›Variation-Compensation Style Encoderæ¨¡å—è´Ÿè´£ä½¿ç”¨å¸¦åŠ æ³•æ³¨æ„åŠ›æœºåˆ¶poolingå±‚çš„transformeré£æ ¼çš„ç¼–ç å™¨è¿›è¡Œç¨³å¥ç¼–ç ï¼›æœ€åï¼ŒVariation-Driven Gesture Predictoræ¨¡å—é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶èåˆMFCCéŸ³é¢‘ç‰¹å¾ä¸StyleCLIPSç¼–ç ä¿¡æ¯ï¼Œæ³¨å…¥ä¸€ä¸ªè·¨æ¡ä»¶è‡ªå›å½’æ¨¡å‹ä¸­ä»¥æ ¹æ®éŸ³é¢‘è¾“å…¥å’Œé£æ ¼çº¿ç´¢è°ƒæ•´ä¸‰ç»´äººç±»åŠ¨ä½œè¡¨æƒ…çš„ç”Ÿæˆã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†åŠ¨ä½œè¡¨æƒ…çš„å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚ç›¸å…³ä»£ç å’Œè§†é¢‘ç»“æœå°†åœ¨æ¥å—åå…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/mookerr/VarGES/%E3%80%82">https://github.com/mookerr/VarGES/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„äººç±»åŠ¨ä½œè¡¨æƒ…åœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ•°æ®é›†å¤šæ ·æ€§å’Œä¿¡æ¯æå–çš„å±€é™æ€§ã€‚</li>
<li>VarGesæ¡†æ¶æ—¨åœ¨å¢å¼ºè¯­éŸ³åŠ¨ä½œè¡¨æƒ…çš„ç”Ÿæˆï¼Œç»“åˆè§†è§‰é£æ ¼çº¿ç´¢ä¿æŒè‡ªç„¶æ€§ã€‚</li>
<li>åŒ…å«ä¸‰å¤§æ ¸å¿ƒæ¨¡å—ï¼šå˜åŒ–å¢å¼ºç‰¹å¾æå–ã€è¡¥å¿é£æ ¼ç¼–ç å™¨å’Œé©±åŠ¨åŠ¨ä½œè¡¨æƒ…é¢„æµ‹å™¨ã€‚</li>
<li>StyleCLIPSä¿¡æ¯èåˆä¸°å¯Œäº†è¾“å…¥å†…å®¹ï¼Œæé«˜äº†åŠ¨ä½œè¡¨æƒ…çš„å¤šæ ·æ€§ã€‚</li>
<li>é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶èåˆéŸ³é¢‘ä¸è§†è§‰ä¿¡æ¯ï¼Œå¢å¼ºäº†åŠ¨ä½œè¡¨æƒ…çš„è‡ªç„¶æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a495e62f6ac026a43b4099cdcf87851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf476a26c2b0c5662384ed2344b1f47e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09315c1371a85150405d1392b09b6745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49955f5c218441c72beda1f64c75d3b3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Age-Related-Robustness-in-Children-Speaker-Verification"><a href="#Enhancing-Age-Related-Robustness-in-Children-Speaker-Verification" class="headerlink" title="Enhancing Age-Related Robustness in Children Speaker Verification"></a>Enhancing Age-Related Robustness in Children Speaker Verification</h2><p><strong>Authors:Vishwas M. Shetty, Jiusi Zheng, Steven M. Lulich, Abeer Alwan</strong></p>
<p>One of the main challenges in childrenâ€™s speaker verification (C-SV) is the significant change in childrenâ€™s voices as they grow. In this paper, we propose two approaches to improve age-related robustness in C-SV. We first introduce a Feature Transform Adapter (FTA) module that integrates local patterns into higher-level global representations, reducing overfitting to specific local features and improving the inter-year SV performance of the system. We then employ Synthetic Audio Augmentation (SAA) to increase data diversity and size, thereby improving robustness against age-related changes. Since the lack of longitudinal speech datasets makes it difficult to measure age-related robustness of C-SV systems, we introduce a longitudinal dataset to assess inter-year verification robustness of C-SV systems. By integrating both of our proposed methods, the average equal error rate was reduced by 19.4%, 13.0%, and 6.1% in the one-year, two-year, and three-year gap inter-year evaluation sets, respectively, compared to the baseline. </p>
<blockquote>
<p>åœ¨å„¿ç«¥è¯´è¯äººéªŒè¯ï¼ˆC-SVï¼‰ä¸­ï¼Œä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å„¿ç«¥å£°éŸ³éšå¹´é¾„å¢é•¿è€Œå‘ç”Ÿçš„æ˜¾è‘—å˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥æé«˜C-SVä¸­ä¸å¹´é¾„ç›¸å…³çš„ç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‰¹å¾è½¬æ¢é€‚é…å™¨ï¼ˆFTAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†å±€éƒ¨æ¨¡å¼é›†æˆåˆ°é«˜çº§å…¨å±€è¡¨ç¤ºä¸­ï¼Œå‡å°‘äº†å¯¹ç‰¹å®šå±€éƒ¨ç‰¹å¾çš„è¿‡åº¦æ‹Ÿåˆï¼Œæé«˜äº†ç³»ç»Ÿçš„è·¨å¹´åº¦SVæ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åˆæˆéŸ³é¢‘å¢å¼ºï¼ˆSAAï¼‰æ¥å¢åŠ æ•°æ®çš„å¤šæ ·æ€§å’Œè§„æ¨¡ï¼Œä»è€Œæé«˜å¯¹ä¸å¹´é¾„ç›¸å…³çš„å˜åŒ–çš„ç¨³å¥æ€§ã€‚ç”±äºç¼ºä¹çºµå‘è¯­éŸ³æ•°æ®é›†ï¼Œä½¿å¾—éš¾ä»¥è¡¡é‡C-SVç³»ç»Ÿçš„ä¸å¹´é¾„ç›¸å…³çš„ç¨³å¥æ€§ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªçºµå‘æ•°æ®é›†æ¥è¯„ä¼°C-SVç³»ç»Ÿçš„è·¨å¹´åº¦éªŒè¯ç¨³å¥æ€§ã€‚é€šè¿‡æ•´åˆæˆ‘ä»¬æå‡ºçš„ä¸¤ç§æ–¹æ³•ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œåœ¨ä¸€å¹´ã€ä¸¤å¹´å’Œä¸‰å¹´é—´éš”çš„è·¨å¹´åº¦è¯„ä¼°é›†ä¸­ï¼Œå¹³å‡ç­‰é”™è¯¯ç‡åˆ†åˆ«é™ä½äº†19.4%ã€13.0%å’Œ6.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10511v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥æé«˜å„¿ç«¥è¯´è¯è€…éªŒè¯ï¼ˆC-SVï¼‰çš„å¹´é¾„ç›¸å…³ç¨³å¥æ€§ã€‚é¦–å…ˆå¼•å…¥ç‰¹å¾è½¬æ¢é€‚é…å™¨ï¼ˆFTAï¼‰æ¨¡å—ï¼Œå°†å±€éƒ¨æ¨¡å¼é›†æˆåˆ°é«˜çº§å…¨å±€è¡¨ç¤ºä¸­ï¼Œä»è€Œæé«˜ç³»ç»Ÿè·¨å¹´é¾„æ®µçš„ç¨³å¥æ€§ã€‚ç„¶åé‡‡ç”¨åˆæˆéŸ³é¢‘å¢å¼ºï¼ˆSAAï¼‰æ¥å¢åŠ æ•°æ®çš„å¤šæ ·æ€§å’Œè§„æ¨¡ï¼Œè¿›ä¸€æ­¥æé«˜å¯¹å¹´é¾„ç›¸å…³å˜åŒ–çš„ç¨³å¥æ€§ã€‚ç”±äºç¼ºä¹çºµå‘è¯­éŸ³æ•°æ®é›†æ¥è¡¡é‡C-SVç³»ç»Ÿçš„å¹´é¾„ç›¸å…³ç¨³å¥æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªçºµå‘æ•°æ®é›†æ¥è¯„ä¼°C-SVç³»ç»Ÿçš„è·¨å¹´åº¦éªŒè¯ç¨³å¥æ€§ã€‚é€šè¿‡æ•´åˆè¿™ä¸¤ç§æ–¹æ³•ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œåœ¨ä¸€å¹´ã€ä¸¤å¹´å’Œä¸‰å¹´é—´éš”çš„è·¨å¹´åº¦è¯„ä¼°é›†ä¸­ï¼Œå¹³å‡ç­‰è¯¯ç‡åˆ†åˆ«é™ä½äº†19.4%ã€13.0%å’Œ6.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å„¿ç«¥è¯´è¯è€…éªŒè¯ï¼ˆC-SVï¼‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å„¿ç«¥å£°éŸ³éšå¹´é¾„çš„é‡å¤§å˜åŒ–ã€‚</li>
<li>ç‰¹å¾è½¬æ¢é€‚é…å™¨ï¼ˆFTAï¼‰æ¨¡å—è¢«å¼•å…¥ï¼Œä»¥æ•´åˆå±€éƒ¨æ¨¡å¼åˆ°é«˜çº§å…¨å±€è¡¨ç¤ºï¼Œæé«˜ç³»ç»Ÿè·¨å¹´é¾„æ®µçš„ç¨³å¥æ€§ã€‚</li>
<li>åˆæˆéŸ³é¢‘å¢å¼ºï¼ˆSAAï¼‰è¢«ç”¨æ¥å¢åŠ æ•°æ®çš„å¤šæ ·æ€§å’Œè§„æ¨¡ï¼Œè¿›ä¸€æ­¥æé«˜å¯¹å¹´é¾„ç›¸å…³å˜åŒ–çš„ç¨³å¥æ€§ã€‚</li>
<li>ç¼ºä¹çºµå‘è¯­éŸ³æ•°æ®é›†æ¥è¡¡é‡C-SVç³»ç»Ÿçš„å¹´é¾„ç›¸å…³ç¨³å¥æ€§ï¼Œå› æ­¤å¼•å…¥äº†ä¸€ä¸ªçºµå‘æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡æ•´åˆFTAå’ŒSAAä¸¤ç§æ–¹æ³•ï¼Œåœ¨è·¨å¹´åº¦è¯„ä¼°ä¸­ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡ç­‰è¯¯ç‡æ˜¾è‘—é™ä½ã€‚</li>
<li>FTAå’ŒSAAçš„æ•´åˆæœ‰åŠ©äºæ”¹å–„ç³»ç»Ÿåœ¨å„¿ç«¥æˆé•¿è¿‡ç¨‹ä¸­çš„å£°éŸ³å˜åŒ–æ–¹é¢çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1c65c81af4bb4f00c6d1bd4448894be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe430fc7aeb52c1f1c0be211b49e1b3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16e5be3146979475959c6159ce9ca4f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78b90b062a5b4b5196f4a9e5b8552cf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53afeb1fdc04550cb52d0a3965217882.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OWLS-Scaling-Laws-for-Multilingual-Speech-Recognition-and-Translation-Models"><a href="#OWLS-Scaling-Laws-for-Multilingual-Speech-Recognition-and-Translation-Models" class="headerlink" title="OWLS: Scaling Laws for Multilingual Speech Recognition and Translation   Models"></a>OWLS: Scaling Laws for Multilingual Speech Recognition and Translation   Models</h2><p><strong>Authors:William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Huck Yang, Shinji Watanabe</strong></p>
<p>Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages&#x2F;dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on <a target="_blank" rel="noopener" href="https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d">https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d</a> for future studies. </p>
<blockquote>
<p>ç¥ç»ç¼©æ”¾å®šå¾‹ä¸ºè®¾è®¡ç¨³å¥çš„åºåˆ—å¤„ç†æ¶æ„æä¾›äº†å®è´µçš„è§è§£ã€‚è™½ç„¶è¿™äº›å®šå¾‹åœ¨å…¶ä»–æ¨¡æ€ä¸Šå·²ç»è¢«å¹¿æ³›è¡¨å¾ï¼Œä½†å®ƒä»¬åœ¨è¯­éŸ³ä¸­çš„è¡Œä¸ºä»ç„¶ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OWLSï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾è®¿é—®çš„ã€å¯é‡å¤ä½¿ç”¨çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ¨¡å‹å¥—ä»¶ï¼Œæ¶µç›–0.25Båˆ°18Bå‚æ•°ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œ18Bç‰ˆæœ¬æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è¯­éŸ³æ¨¡å‹ã€‚OWLSåˆ©ç”¨è·¨è¶Š150ç§è¯­è¨€çš„36ä¸‡å°æ—¶å…¬å¼€è¯­éŸ³æ•°æ®ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°ç ”ç©¶æ•°æ®ã€æ¨¡å‹å’Œè®¡ç®—ç¼©æ”¾å¦‚ä½•åˆ†åˆ«å½±å“å¤šè¯­è¨€è¯­éŸ³ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨OWLSæ¥æ¨å¯¼ç¥ç»ç¼©æ”¾å®šå¾‹ï¼Œå±•ç¤ºåœ¨ç¼©æ”¾æ—¶å¦‚ä½•å¯é åœ°é¢„æµ‹æœ€ç»ˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°ä¹‹ä¸€æ˜¯ï¼Œç¼©æ”¾å¢å¼ºäº†å¯¹ä½èµ„æºè¯­è¨€&#x2F;æ–¹è¨€çš„æ€§èƒ½ï¼Œæœ‰åŠ©äºå‡è½»åè§ï¼Œæé«˜è¯­éŸ³æŠ€æœ¯çš„å¯åŠæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨OWLSä¸ºæ–°çš„ç ”ç©¶æ–¹å‘æä¾›åŠ¨åŠ›ï¼Œé€šè¿‡å‘ç°å¤§è§„æ¨¡è¯­éŸ³æ¨¡å‹çš„æ–°å…´èƒ½åŠ›ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å°†åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/collections/espnet/owls-scaling%E8%B7%BD%E9%9C%BE%E5%AE%9A%E5%BE%8B%E4%BB%A3%E8%AF%AD%E8%AF%86%E5%88%AB%E5%92%8C%E8%BD%AC%E8%AF%91-%E6%A8%A1%E5%9D%BF-%E7%AC%A6%E5%BC%BA%-e5b4bf5adfeeaefd">https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8dä¸Šå‘å¸ƒï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10373v1">PDF</a> 23 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OWLSï¼Œä¸€å¥—æ¶µç›–å¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ¨¡å‹çš„å¼€æºå¥—ä»¶ï¼Œæ¨¡å‹å‚æ•°ä»æ•°åäº¿åˆ°ä¸Šç™¾äº¿ä¸ç­‰ï¼Œå…¶ä¸­æœ€å¤§çš„æ¨¡å‹å‚æ•°ä¸º18Bã€‚OWLSåˆ©ç”¨è·¨150ç§è¯­è¨€çš„å…¬å¼€è¯­éŸ³æ•°æ®ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®ã€æ¨¡å‹å’Œè®¡ç®—è§„æ¨¡åœ¨å¤šè¯­ç§è¯­éŸ³ä»»åŠ¡ä¸­çš„å½±å“ï¼Œå¹¶å¾—å‡ºäº†ç¥ç»è§„æ¨¡å®šå¾‹ã€‚ç ”ç©¶å‘ç°ï¼Œè§„æ¨¡æ‰©å¤§èƒ½æé«˜ä½èµ„æºè¯­è¨€å’Œæ–¹è¨€çš„æ€§èƒ½ï¼Œæœ‰åŠ©äºå‡å°‘åè§å¹¶æé«˜è¯­éŸ³æŠ€æœ¯çš„æ™®åŠæ€§ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d%E4%B8%8A%EF%BC%8C%E4%BB%A5%E4%BE%BF%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8dä¸Šï¼Œä»¥ä¾¿æœªæ¥ç ”ç©¶ä½¿ç”¨ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»OWLSä½œä¸ºæ¶µç›–å¤šè¯­ç§è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ¨¡å‹çš„å¼€æºå¥—ä»¶ã€‚</li>
<li>OWLSæ¨¡å‹å‚æ•°èŒƒå›´å¹¿æ³›ï¼Œä»æ•°åäº¿åˆ°ä¸Šç™¾äº¿ä¸ç­‰ï¼Œå…¶ä¸­æœ€å¤§çš„æ¨¡å‹å‚æ•°ä¸º18Bã€‚</li>
<li>åˆ©ç”¨å…¬å¼€è¯­éŸ³æ•°æ®ï¼Œç³»ç»Ÿåœ°ç ”ç©¶æ•°æ®ã€æ¨¡å‹å’Œè®¡ç®—è§„æ¨¡åœ¨å¤šè¯­ç§è¯­éŸ³ä»»åŠ¡ä¸­çš„ä½œç”¨ã€‚</li>
<li>é€šè¿‡OWLSå‘ç°ç¥ç»è§„æ¨¡å®šå¾‹ï¼Œæ­ç¤ºäº†å¦‚ä½•é¢„æµ‹æœ€ç»ˆçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è§„æ¨¡æ‰©å¤§èƒ½æé«˜ä½èµ„æºè¯­è¨€å’Œæ–¹è¨€çš„æ€§èƒ½ï¼Œæœ‰åŠ©äºå‡å°‘åè§å’Œæé«˜è¯­éŸ³æŠ€æœ¯çš„æ™®åŠæ€§ã€‚</li>
<li>é€šè¿‡OWLSçš„ç ”ç©¶å‘ç°ï¼Œå¤§è§„æ¨¡è¯­éŸ³æ¨¡å‹èƒ½å¤Ÿå±•ç°å‡ºæ–°å…´çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-525eb58740474d67a4556b5038dd8124.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e574500c2d0e9d44b6b4e5a005b861eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c00330c16c06026af32b21ed06b277b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b05c892a24d599b55a68b860e629d16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fd177ff21188852175ee09028efbfa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2fe05ef6cfee5f3204681c990adab9d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MTLM-an-Innovative-Language-Model-Training-Paradigm-for-ASR"><a href="#MTLM-an-Innovative-Language-Model-Training-Paradigm-for-ASR" class="headerlink" title="MTLM: an Innovative Language Model Training Paradigm for ASR"></a>MTLM: an Innovative Language Model Training Paradigm for ASR</h2><p><strong>Authors:Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai</strong></p>
<p>Pre-training Transformer-based language models (LMs) on a large amount of text has proven crucial for improving automatic speech recognition (ASR) performance. Generally, traditional LMs are unidirectional and unable to access the context on the right. This paper proposes a method for training LMs that enable traditional unidirectional LMs to fully utilize left and right contexts. Compared with the unidirectional LMs, our LM facilitates ASR to transcribe hypotheses more consistently and in a more semantically unambiguous way, as it incorporates richer contextual representations. Finally, our experimental results on the LibriSpeech corpus demonstrate that our model outperforms traditional unidirectional LMs, whether n-best rescoring or shallow fusion is used as the decoding algorithm. </p>
<blockquote>
<p>åŸºäºå¤§é‡æ–‡æœ¬å¯¹Transformerè¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒå·²ç»è¢«è¯æ˜å¯¹æ”¹å–„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½è‡³å…³é‡è¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹æ˜¯å•æ–¹å‘çš„ï¼Œæ— æ³•è®¿é—®åˆ°å³ä¾§çš„å†…å®¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿ä¼ ç»Ÿçš„å•å‘è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿçš„å•å‘è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹å¯ä»¥ä¿ƒä½¿è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ›´åŠ ä¸€è‡´ä¸”è¯­ä¹‰æ›´åŠ æ˜ç¡®åœ°è½¬å½•å‡è®¾ï¼Œå› ä¸ºå®ƒèå…¥äº†æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æœ€åï¼Œåœ¨LibriSpeechè¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ— è®ºæ˜¯ä½¿ç”¨n-besté‡æ‰“åˆ†è¿˜æ˜¯æµ…èåˆä½œä¸ºè§£ç ç®—æ³•ï¼Œéƒ½ä¼˜äºä¼ ç»Ÿçš„å•å‘è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10058v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒTransformeråŸºç¡€çš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å¤§é‡æ–‡æœ¬ä¸Šå¯¹äºæé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½è‡³å…³é‡è¦ã€‚ä¼ ç»ŸLMsé€šå¸¸æ˜¯å•å‘çš„ï¼Œæ— æ³•è·å–å³ä¾§ä¸Šä¸‹æ–‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§è®­ç»ƒLMsçš„æ–¹æ³•ï¼Œä½¿ä¼ ç»Ÿå•å‘LMsèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡ã€‚ä¸ä¼ ç»Ÿå•å‘LMsç›¸æ¯”ï¼Œæˆ‘ä»¬çš„LMä¿ƒè¿›ASRæ›´ä¸€è‡´ä¸”è¯­ä¹‰æ›´æ˜ç¡®åœ°è½¬å½•å‡è®¾ï¼Œå› ä¸ºå®ƒåŒ…å«äº†æ›´ä¸°å¯Œçš„å†…å®¹è¡¨ç¤ºã€‚åœ¨LibriSpeechè¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºä½¿ç”¨n-besté‡è¯„åˆ†è¿˜æ˜¯æµ…èåˆä½œä¸ºè§£ç ç®—æ³•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹éƒ½ä¼˜äºä¼ ç»Ÿå•å‘LMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒTransformeråŸºç¡€çš„è¯­è¨€æ¨¡å‹å¯¹æ”¹å–„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»ŸLMsé€šå¸¸æ˜¯å•å‘çš„ï¼Œæ— æ³•è·å–å³ä¾§ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•ï¼Œä½¿ä¼ ç»ŸLMsèƒ½å¤Ÿåˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡ï¼Œä»è€Œä¸°å¯Œå…¶å†…å®¹è¡¨ç¤ºã€‚</li>
<li>ä¸ä¼ ç»Ÿå•å‘LMsç›¸æ¯”ï¼Œè¯¥LMä¿ƒè¿›ASRæ›´ä¸€è‡´ä¸”è¯­ä¹‰æ›´æ˜ç¡®åœ°è½¬å½•å‡è®¾ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨LibriSpeechè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºä¼ ç»Ÿå•å‘LMsã€‚</li>
<li>è¯¥è®ºæ–‡ç ”ç©¶çš„æ¨¡å‹æ— è®ºä½¿ç”¨n-besté‡è¯„åˆ†è¿˜æ˜¯æµ…èåˆä½œä¸ºè§£ç ç®—æ³•éƒ½æœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cdc00ae573940adbc036e430e5fb4bf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21b62d3d9bef6ec71f33ef571d0d616f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0940e1aa4e5b0073d0122e7155cfc4fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a73d6b97cac9f2333a6047ecacc7ed62.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Microphone-Array-Geometry-Independent-Multi-Talker-Distant-ASR-NTT-System-for-the-DASR-Task-of-the-CHiME-8-Challenge"><a href="#Microphone-Array-Geometry-Independent-Multi-Talker-Distant-ASR-NTT-System-for-the-DASR-Task-of-the-CHiME-8-Challenge" class="headerlink" title="Microphone Array Geometry Independent Multi-Talker Distant ASR: NTT   System for the DASR Task of the CHiME-8 Challenge"></a>Microphone Array Geometry Independent Multi-Talker Distant ASR: NTT   System for the DASR Task of the CHiME-8 Challenge</h2><p><strong>Authors:Naoyuki Kamo, Naohiro Tawara, Atsushi Ando, Takatomo Kano, Hiroshi Sato, Rintaro Ikeshita, Takafumi Moriya, Shota Horiguch, Kohei Matsuura, Atsunori Ogawa, Alexis Plaquet, Takanori Ashihara, Tsubasa Ochiai, Masato Mimura, Marc Delcroix, Tomohiro Nakatani, Taichi Asami, Shoko Araki</strong></p>
<p>In this paper, we introduce a multi-talker distant automatic speech recognition (DASR) system we designed for the DASR task 1 of the CHiME-8 challenge. Our system performs speaker counting, diarization, and ASR. It handles various recording conditions, from diner parties to professional meetings and from two to eight speakers. We perform diarization first, followed by speech enhancement, and then ASR as the challenge baseline. However, we introduced several key refinements. First, we derived a powerful speaker diarization relying on end-to-end speaker diarization with vector clustering (EEND-VC), multi-channel speaker counting using enhanced embeddings from EEND-VC, and target-speaker voice activity detection (TS-VAD). For speech enhancement, we introduced a novel microphone selection rule to better select the most relevant microphones among the distributed microphones and investigated improvements to beamforming. Finally, for ASR, we developed several models exploiting Whisper and WavLM speech foundation models. We present the results we submitted to the challenge and updated results we obtained afterward. Our strongest system achieves a 63% relative macro tcpWER improvement over the baseline and outperforms the challenge best results on the NOTSOFAR-1 meeting evaluation data among geometry-independent systems. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€é¡¹é’ˆå¯¹CHiME-8æŒ‘æˆ˜ä»»åŠ¡ä¸­çš„è¿œè·ç¦»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰ä»»åŠ¡è®¾è®¡çš„å¤šè¯´è¯è€…è¿œè·ç¦»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥è¿›è¡Œè¯´è¯äººæ•°ç»Ÿè®¡ã€è¯­éŸ³æ—¥è®°è¯†åˆ«ä»¥åŠè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚è¯¥ç³»ç»Ÿå¯ä»¥å¤„ç†å„ç§å½•éŸ³ç¯å¢ƒï¼Œæ— è®ºæ˜¯å®¶åº­èšä¼šè¿˜æ˜¯ä¸“ä¸šä¼šè®®ï¼Œä»ä¸¤äººåˆ°å…«äººçš„åœºæ™¯ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œè¯­éŸ³æ—¥è®°è¯†åˆ«ï¼Œç„¶åè¿›è¡Œè¯­éŸ³å¢å¼ºï¼Œæœ€åè¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä½œä¸ºæŒ‘æˆ˜åŸºçº¿ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ é¡¹å…³é”®æ”¹è¿›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¾é ä¸€ç§å¼ºå¤§çš„è¯­éŸ³æ—¥è®°è¯†åˆ«æ–¹æ³•å®ç°ç«¯åˆ°ç«¯çš„è¯­éŸ³æ—¥è®°è¯†åˆ«ï¼Œé‡‡ç”¨å‘é‡èšç±»ï¼ˆEEND-VCï¼‰ã€é€šè¿‡EEND-VCå¢å¼ºçš„åµŒå…¥å®ç°å¤šé€šé“è¯´è¯äººæ•°ç»Ÿè®¡ä»¥åŠç›®æ ‡è¯´è¯è€…è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰ã€‚å¯¹äºè¯­éŸ³å¢å¼ºéƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„éº¦å…‹é£é€‰æ‹©è§„åˆ™ï¼Œä»¥æ›´å¥½åœ°é€‰æ‹©åˆ†å¸ƒéº¦å…‹é£ä¸­æœ€ç›¸å…³çš„éº¦å…‹é£ï¼Œå¹¶ç ”ç©¶æ”¹è¿›æ³¢æŸå½¢æˆæŠ€æœ¯ã€‚æœ€åï¼Œå¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€äº›åˆ©ç”¨Whisperå’ŒWavLMè¯­éŸ³åŸºç¡€æ¨¡å‹çš„æ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†æäº¤ç»™æŒ‘æˆ˜çš„ç»“æœä»¥åŠä¹‹åè·å¾—çš„æ›´æ–°ç»“æœã€‚æˆ‘ä»¬æœ€å¼ºå¤§çš„ç³»ç»Ÿåœ¨å®tcpWERä¸Šçš„ç›¸å¯¹æ”¹è¿›ç‡æé«˜äº†63%ï¼Œå¹¶ä¼˜äºåœ¨å‡ ä½•æ— å…³ç³»ç»Ÿä¸­é’ˆå¯¹NOTSOFAR-1ä¼šè®®è¯„ä¼°æ•°æ®çš„æŒ‘æˆ˜æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09859v1">PDF</a> 55 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹CHiME-8æŒ‘æˆ˜DASRä»»åŠ¡1çš„å¤šè¯´è¯äººè¿œè·ç¦»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ‰§è¡Œè¯´è¯äººè®¡æ•°ã€è¯­éŸ³è¯†åˆ«åŠè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚å®ƒèƒ½åº”å¯¹ä»é¤æ¡Œèšä¼šåˆ°ä¸“ä¸šä¼šè®®ä»¥åŠä»ä¸¤ä½åˆ°å…«ä½è¯´è¯äººçš„å„ç§å½•éŸ³ç¯å¢ƒã€‚é¦–å…ˆè¿›è¡Œè¯´è¯äººè¯†åˆ«ï¼Œç„¶åè¿›è¡Œè¯­éŸ³å¢å¼ºï¼Œæœ€åè¿›è¡ŒASRä½œä¸ºåŸºçº¿æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œè®ºæ–‡å¼•å…¥äº†å¤šé¡¹å…³é”®æŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬é‡‡ç”¨åŸºäºç«¯å¯¹ç«¯è¯´è¯äººè¯†åˆ«çš„å‘é‡èšç±»ï¼ˆEEND-VCï¼‰è¿›è¡Œå¼ºå¤§çš„è¯´è¯äººè¯†åˆ«ã€ä½¿ç”¨EEND-VCå¢å¼ºåµŒå…¥çš„å¤šé€šé“è¯´è¯äººè®¡æ•°ä»¥åŠç›®æ ‡è¯´è¯äººçš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰ã€‚åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„éº¦å…‹é£é€‰æ‹©è§„åˆ™ï¼Œä»¥æ›´å¥½åœ°é€‰æ‹©åˆ†å¸ƒå¼éº¦å…‹é£ä¸­æœ€ç›¸å…³çš„éº¦å…‹é£ï¼Œå¹¶å¯¹æ³¢æŸå½¢æˆè¿›è¡Œäº†æ”¹è¿›ã€‚åœ¨ASRæ–¹é¢ï¼Œè®ºæ–‡å¼€å‘äº†åˆ©ç”¨Whisperå’ŒWavLMè¯­éŸ³åŸºç¡€æ¨¡å‹çš„å¤šä¸ªæ¨¡å‹ã€‚è®ºæ–‡å±•ç¤ºäº†æäº¤æŒ‘æˆ˜çš„ç»“æœä»¥åŠåç»­æ›´æ–°çš„ç»“æœã€‚å…¶æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸å¯¹äºåŸºçº¿å®ç°äº†63%çš„ç›¸å¯¹å®è§‚tcpWERæ”¹è¿›ï¼Œå¹¶åœ¨å‡ ä½•æ— å…³ç³»ç»Ÿä¸­è¶…è¶Šäº†æŒ‘æˆ˜çš„æœ€ä½³ç»“æœåœ¨NOTSOFAR-1ä¼šè®®è¯„ä¼°æ•°æ®ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹CHiME-8æŒ‘æˆ˜çš„å¤šè¯´è¯äººè¿œè·ç¦»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿèƒ½åº”å¯¹ä¸åŒåœºæ™¯å’Œè¯´è¯äººæ•°é‡çš„å˜åŒ–ã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨äº†åŸºäºç«¯å¯¹ç«¯è¯´è¯äººè¯†åˆ«çš„å‘é‡èšç±»çš„æŠ€æœ¯æ”¹è¿›ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„éº¦å…‹é£é€‰æ‹©è§„åˆ™ç”¨äºè¯­éŸ³å¢å¼ºã€‚</li>
<li>åœ¨ASRæ–¹é¢ï¼Œåˆ©ç”¨äº†Whisperå’ŒWavLMè¯­éŸ³åŸºç¡€æ¨¡å‹å¼€å‘çš„å¤šä¸ªæ¨¡å‹ã€‚</li>
<li>æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸å¯¹äºåŸºçº¿æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6592e76ace735a7633e276040145e269.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2db7bfc9955cc8e0297a9d7420219c8a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Online-Social-Support-Detection-in-Spanish-Social-Media-Texts"><a href="#Online-Social-Support-Detection-in-Spanish-Social-Media-Texts" class="headerlink" title="Online Social Support Detection in Spanish Social Media Texts"></a>Online Social Support Detection in Spanish Social Media Texts</h2><p><strong>Authors:Moein Shahiki Tash, Luis Ramos, Zahra Ahani, Raul Monroy, Olga kolesnikova, Hiram Calvo, Grigori Sidorov</strong></p>
<p>The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“çš„å…´èµ·å·²ç»æ”¹å˜äº†äººä»¬çš„äº¤æµæ–¹å¼ï¼Œä½¿å¾—ä¸ªäººèƒ½å¤Ÿåˆ†äº«ä»–ä»¬çš„ç»å†ã€å¯»æ±‚æ”¯æŒï¼Œå¹¶å‚ä¸å„ç§è®¨è®ºã€‚è™½ç„¶å¤§é‡ç ”ç©¶èšç„¦äºè¯†åˆ«æœ‰å®³å†…å®¹ï¼Œå¦‚ä»‡æ¨è¨€è®ºï¼Œä½†å¯¹ç§¯æå’Œæ”¯æŒæ€§äº’åŠ¨çš„è®¤è¯†å’Œä¿ƒè¿›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ£€æµ‹è¥¿ç­ç‰™è¯­æ–‡æœ¬ä¸­åœ¨çº¿ç¤¾ä¼šæ”¯æŒçš„åˆ›æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºè¿™é¡¹ä»»åŠ¡åˆ›å»ºçš„ç¬¬ä¸€ä¸ªæ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…å«3189æ¡è¢«åˆ†ç±»ä¸ºæ”¯æŒæ€§æˆ–éæ”¯æŒæ€§çš„YouTubeè¯„è®ºã€‚ä¸ºäº†è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨GPT-4oç”Ÿæˆäº†åŒä¹‰æ›¿æ¢çš„è¯„è®ºï¼Œåˆ›å»ºäº†å¹³è¡¡æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€æ·±åº¦å­¦ä¹ æ¶æ„å’ŒåŸºäºè½¬æ¢æ¨¡å‹çš„GPT-4oè¯„ä¼°ç¤¾ä¼šæ”¯æŒåˆ†ç±»çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤è¿‡ç¨‹åªåœ¨éå¹³è¡¡æ•°æ®é›†ä¸Šè¿›è¡Œã€‚éšåï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªè½¬æ¢æ¨¡å‹æ¯”è¾ƒå¹³è¡¡å’Œéå¹³è¡¡æ•°æ®é›†çš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä»»åŠ¡2ï¼ˆä¸ªäººå’Œå›¢ä½“ï¼‰ã€ä»»åŠ¡3ï¼ˆå›½å®¶ã€å…¶ä»–ã€LGBTQã€é»‘äººç¤¾åŒºã€å¥³æ€§ã€å®—æ•™ï¼‰ä¸­ï¼Œå¹³è¡¡æ•°æ®é›†çš„ç»“æœæœ‰æ‰€æ”¹å–„ï¼Œè€ŒGPT-4oåœ¨ä»»åŠ¡1ï¼ˆç¤¾ä¼šæ”¯æŒå’Œéæ”¯æŒï¼‰ä¸­çš„è¡¨ç°æœ€ä½³ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¿ƒè¿›æ”¯æŒæ€§åœ¨çº¿ç¯å¢ƒçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„è‡ªåŠ¨åŒ–ç¤¾ä¼šæ”¯æŒæ£€æµ‹ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09640v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç¤¾äº¤åª’ä½“çš„å‡ºç°æ”¹å˜äº†äººä»¬çš„äº¤æµæ–¹å¼ï¼Œä½¿å¾—ä¸ªäººèƒ½å¤Ÿåˆ†äº«ç»å†ã€å¯»æ±‚æ”¯æŒå¹¶å‚ä¸å„ç§è®¨è®ºã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ£€æµ‹åœ¨çº¿ç¤¾äº¤æ”¯æŒçš„åˆ›æ–°æ–¹æ³•ï¼Œå¹¶ä¸“é—¨åˆ›å»ºäº†é¦–ä¸ªç”¨äºæ­¤ä»»åŠ¡çš„æ³¨é‡Šæ•°æ®é›†ã€‚ç ”ç©¶åŒ…æ‹¬åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†å’Œè¯„ä¼°ä¸åŒç±»å‹çš„æ¨¡å‹å¯¹åœ¨çº¿ç¤¾äº¤æ”¯æŒåˆ†ç±»çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¹³è¡¡æ•°æ®é›†èƒ½å¤Ÿæé«˜æŸäº›ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸”GPT-4oåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚è¿™ä¸ºæœªæ¥çš„åœ¨çº¿æ”¯æŒæ£€æµ‹ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“æ”¹å˜äº†äººä»¬çš„äº¤æµæ–¹å¼ï¼Œä¿ƒè¿›ä¸ªäººåˆ†äº«ç»å†ã€å¯»æ±‚æ”¯æŒå’Œå‚ä¸è®¨è®ºã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨æ£€æµ‹åœ¨çº¿ç¤¾äº¤æ”¯æŒï¼Œå¹¶åˆ›å»ºäº†é¦–ä¸ªä¸“é—¨ç”¨äºæ­¤ä»»åŠ¡çš„æ³¨é‡Šæ•°æ®é›†ã€‚</li>
<li>ç ”ç©¶ä¸­é‡‡ç”¨äº†å¤šç§æŠ€æœ¯æ¥åˆ›å»ºå¹³è¡¡æ•°æ®é›†ï¼Œä»¥åº”å¯¹æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸åŒç±»å‹çš„æ¨¡å‹åœ¨åœ¨çº¿ç¤¾äº¤æ”¯æŒåˆ†ç±»æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>GPT-4oåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°æœ€ä½³ã€‚</li>
<li>å¹³è¡¡æ•°æ®é›†åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58c15895de79600a19c5b618d8fe4403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ed989439df451c71c2e494598348b0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b590ec3cd7600b4ad14cf957243089c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-897d13dd5b7daa4606faa1753a46b2f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8f646669fd0c9c3f71d420f8082d51f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63e32215592679958da62e2e3fb9af28.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation"><a href="#DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation" class="headerlink" title="DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation"></a>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation</h2><p><strong>Authors:Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang</strong></p>
<p>Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness. </p>
<blockquote>
<p>è¿‘æœŸæœ‰å‡ é¡¹ç ”ç©¶å°è¯•ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ï¼Œæ— ç¦»æ•£è¯­éŸ³æ ‡è®°åœ°è‡ªåŠ¨ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€é¢ä¸´è®¡ç®—è´Ÿè½½è¿‡å¤§æˆ–ç»“æœä¸ç†æƒ³ç­‰æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£å˜æ¢è‡ªå›å½’å»ºæ¨¡ï¼ˆDiTARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å˜å‹å™¨çš„åŸºäºè¡¥ä¸çš„è‡ªå›å½’æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­æ ‡è®°çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARé‡‡ç”¨ä¸€ç§åˆ†è€Œæ²»ä¹‹çš„è¡¥ä¸ç”Ÿæˆç­–ç•¥ï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹å¤„ç†èšåˆçš„è¡¥ä¸åµŒå…¥ï¼Œç„¶åæ‰©æ•£å˜å‹å™¨æ ¹æ®è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æè®®å°†æ¸©åº¦å®šä¹‰ä¸ºåå‘æ‰©æ•£ODEè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹ï¼Œä»¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚åœ¨å¹¿æ³›çš„è§„æ¨¡åˆ†æä¸­ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜DiTARå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­ï¼ŒDiTARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03930v2">PDF</a> 16 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiffusion Transformer Autoregressive Modelingï¼ˆDiTARï¼‰çš„æ–¹æ³•ï¼Œç»“åˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å˜å‹å™¨ï¼Œä»¥æ–‘å—ä¸ºåŸºç¡€è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œæ–‘å—ç”Ÿæˆï¼Œå¹¶é€šè¿‡è°ƒæ•´åå‘æ‰©æ•£ODEä¸­çš„æ¸©åº¦æ¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œä»è€Œå®ç°æ›´å¥½çš„æ¨ç†ã€‚DiTARåœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“åˆæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>æå‡ºDiffusion Transformer Autoregressive Modelingï¼ˆDiTARï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆå¢å¼ºè‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­æ ‡è®°çš„æ•ˆåŠ›å¹¶é™ä½è®¡ç®—éœ€æ±‚ã€‚</li>
<li>é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œæ–‘å—ç”Ÿæˆï¼Œè¯­è¨€æ¨¡å‹å¤„ç†èšåˆæ–‘å—åµŒå…¥ï¼Œæ‰©æ•£å˜å‹å™¨æ ¹æ®è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªæ–‘å—ã€‚</li>
<li>é€šè¿‡è°ƒæ•´åå‘æ‰©æ•£ODEä¸­çš„æ¸©åº¦æ¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œå®ç°æ›´å¥½çš„æ¨ç†ã€‚</li>
<li>DiTARå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ï¼Œåœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨é²æ£’æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4836e7f99351dd1aa225b0a35ea7264.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa1a0942a07751f603ae646b5cd3489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3223eeaa8af78853b6729b093b239b3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-End-to-End-Continuous-Spanish-Lipreading-in-Different-Data-Conditions"><a href="#Evaluation-of-End-to-End-Continuous-Spanish-Lipreading-in-Different-Data-Conditions" class="headerlink" title="Evaluation of End-to-End Continuous Spanish Lipreading in Different Data   Conditions"></a>Evaluation of End-to-End Continuous Spanish Lipreading in Different Data   Conditions</h2><p><strong>Authors:David Gimeno-GÃ³mez, Carlos-D. MartÃ­nez-Hinarejos</strong></p>
<p>Visual speech recognition remains an open research problem where different challenges must be considered by dispensing with the auditory sense, such as visual ambiguities, the inter-personal variability among speakers, and the complex modeling of silence. Nonetheless, recent remarkable results have been achieved in the field thanks to the availability of large-scale databases and the use of powerful attention mechanisms. Besides, multiple languages apart from English are nowadays a focus of interest. This paper presents noticeable advances in automatic continuous lipreading for Spanish. First, an end-to-end system based on the hybrid CTC&#x2F;Attention architecture is presented. Experiments are conducted on two corpora of disparate nature, reaching state-of-the-art results that significantly improve the best performance obtained to date for both databases. In addition, a thorough ablation study is carried out, where it is studied how the different components that form the architecture influence the quality of speech recognition. Then, a rigorous error analysis is carried out to investigate the different factors that could affect the learning of the automatic system. Finally, a new Spanish lipreading benchmark is consolidated. Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading">https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading</a>. </p>
<blockquote>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œå¿…é¡»è€ƒè™‘èˆå¼ƒå¬è§‰æ‰€å¸¦æ¥çš„ä¸åŒæŒ‘æˆ˜ï¼Œä¾‹å¦‚è§†è§‰æ¨¡ç³Šã€è¯´è¯è€…ä¹‹é—´çš„ä¸ªä½“å·®å¼‚ä»¥åŠæ²‰é»˜çš„å¤æ‚å»ºæ¨¡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®åº“çš„å¯ç”¨æ€§å’Œå¼ºå¤§æ³¨æ„åŠ›æœºåˆ¶çš„ä½¿ç”¨ï¼Œè¯¥é¢†åŸŸå·²ç»å–å¾—äº†ä»¤äººç©ç›®çš„æœ€æ–°æˆæœã€‚æ­¤å¤–ï¼Œå¦‚ä»Šé™¤è‹±è¯­å¤–çš„å¤šç§è¯­è¨€ä¹Ÿæˆä¸ºå…³æ³¨çš„é‡ç‚¹ã€‚æœ¬æ–‡ä»‹ç»äº†è¥¿ç­ç‰™è¯­çš„è‡ªåŠ¨è¿ç»­å”‡è¯»æ–¹é¢çš„æ˜¾è‘—è¿›å±•ã€‚é¦–å…ˆï¼Œä»‹ç»äº†ä¸€ç§åŸºäºæ··åˆCTC&#x2F;æ³¨æ„åŠ›æ¶æ„çš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚å®éªŒåœ¨ä¸¤ä¸ªæœ¬è´¨ä¸åŒçš„è¯­æ–™åº“ä¸Šè¿›è¡Œï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—æ”¹è¿›äº†è¿„ä»Šä¸ºæ­¢åœ¨è¿™ä¸¤ä¸ªæ•°æ®åº“ä¸Šè·å¾—çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œç ”ç©¶äº†æ„æˆæ¶æ„çš„ä¸åŒç»„ä»¶å¦‚ä½•å½±å“è¯­éŸ³è¯†åˆ«è´¨é‡ã€‚ç„¶åï¼Œè¿›è¡Œäº†ä¸¥æ ¼çš„è¯¯å·®åˆ†æï¼Œä»¥è°ƒæŸ¥å¯èƒ½å½±å“è‡ªåŠ¨ç³»ç»Ÿå­¦ä¹ çš„ä¸åŒå› ç´ ã€‚æœ€åï¼Œå·©å›ºäº†ä¸€ä¸ªæ–°çš„è¥¿ç­ç‰™è¯­å”‡è¯»åŸºå‡†æµ‹è¯•ã€‚ä»£ç å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/david-gimeno/evaluating-end2end-spanish-lipreadingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00464v2">PDF</a> Accepted in the â€œLanguage Resources and Evaluationâ€ journal, Springer   Nature</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œå…¶æŒ‘æˆ˜åœ¨äºæ‘’å¼ƒå¬è§‰ï¼Œé¢ä¸´è§†è§‰æ¨¡ç³Šã€ä¸åŒä¸ªä½“ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ²‰é»˜çš„å¤æ‚å»ºæ¨¡ç­‰é—®é¢˜ã€‚è¿‘æœŸç”±äºå¤§è§„æ¨¡æ•°æ®åº“å’Œå¼ºå¤§æ³¨æ„åŠ›æœºåˆ¶çš„å¯ç”¨æ€§ï¼Œè¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡ä»‹ç»äº†è¥¿ç­ç‰™è¯­çš„è‡ªåŠ¨è¿ç»­å”‡è¯»æ–¹é¢çš„æ˜¾è‘—è¿›å±•ã€‚é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ··åˆCTC&#x2F;æ³¨æ„åŠ›æ¶æ„çš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚å®éªŒåœ¨ä¸¤ä¸ªæ€§è´¨ä¸åŒçš„è¯­æ–™åº“ä¸Šè¿›è¡Œï¼Œè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼Œæ˜¾è‘—æé«˜äº†è¿™ä¸¤ä¸ªæ•°æ®åº“çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†æ·±å…¥çš„åˆ†æç ”ç©¶ï¼Œç ”ç©¶äº†æ„æˆæ¶æ„çš„ä¸åŒéƒ¨åˆ†å¯¹è¯­éŸ³è¯†åˆ«è´¨é‡çš„å½±å“ã€‚ç„¶åè¿›è¡Œäº†ä¸¥æ ¼çš„è¯¯å·®åˆ†æï¼Œä»¥è°ƒæŸ¥å¯èƒ½å½±å“è‡ªåŠ¨ç³»ç»Ÿå­¦ä¹ çš„ä¸åŒå› ç´ ã€‚æœ€åå·©å›ºäº†ä¸€ä¸ªæ–°çš„è¥¿ç­ç‰™è¯­å”‡è¯»åŸºå‡†æµ‹è¯•ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading">https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading</a> è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­éŸ³è¯†åˆ«æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œé¢ä¸´è§†è§‰æ¨¡ç³Šã€ä¸ªä½“å·®å¼‚å’Œæ²‰é»˜å»ºæ¨¡ç­‰æŒ‘æˆ˜ã€‚</li>
<li>è¿‘æœŸç”±äºå¤§è§„æ¨¡æ•°æ®åº“å’Œæ³¨æ„åŠ›æœºåˆ¶çš„å¯ç”¨æ€§ï¼Œè§†è§‰è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ··åˆCTC&#x2F;æ³¨æ„åŠ›æ¶æ„çš„ç«¯åˆ°ç«¯ç³»ç»Ÿåœ¨è‡ªåŠ¨è¿ç»­å”‡è¯»æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>å®éªŒåœ¨ä¸¤ä¸ªä¸åŒæ€§è´¨çš„è¯­æ–™åº“ä¸Šè¿›è¡Œï¼Œè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼Œå¹¶æ˜¾è‘—æé«˜äº†ä¹‹å‰æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¿›è¡Œäº†æ·±å…¥åˆ†æç ”ç©¶ï¼Œæ¢è®¨äº†æ¶æ„çš„ä¸åŒéƒ¨åˆ†å¯¹è¯­éŸ³è¯†åˆ«è´¨é‡çš„å½±å“ã€‚</li>
<li>é€šè¿‡ä¸¥æ ¼çš„è¯¯å·®åˆ†æï¼Œç ”ç©¶äº†å½±å“è‡ªåŠ¨ç³»ç»Ÿå­¦ä¹ çš„ä¸åŒå› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61c3a739328bc2f12cff7efb2045b256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ecbc292a9daa7604094727517cde82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e690439ca5bbd3fcb5fb04a41d20a693.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition"><a href="#CR-CTC-Consistency-regularization-on-CTC-for-improved-speech-recognition" class="headerlink" title="CR-CTC: Consistency regularization on CTC for improved speech   recognition"></a>CR-CTC: Consistency regularization on CTC for improved speech   recognition</h2><p><strong>Authors:Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey</strong></p>
<p>Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC. It significantly improves the CTC performance, achieving state-of-the-art results comparable to those attained by transducer or systems combining CTC and attention-based encoder-decoder (CTC&#x2F;AED). We release our code at <a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall">https://github.com/k2-fsa/icefall</a>. </p>
<blockquote>
<p>è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–¹æ³•ï¼Œä»¥å…¶ç®€å•æ€§å’Œè®¡ç®—æ•ˆç‡è€Œé—»åã€‚ç„¶è€Œï¼Œå®ƒåœ¨è¯†åˆ«æ€§èƒ½ä¸Šå¸¸å¸¸æœ‰æ‰€ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€è‡´æ€§æ­£åˆ™åŒ–CTCï¼ˆCR-CTCï¼‰ï¼Œå®ƒå¼ºåˆ¶è¾“å…¥è¯­éŸ³æ¢…å°”é¢‘è°±å›¾çš„ä¸åŒå¢å¼ºè§†å›¾æ‰€è·å¾—çš„ä¸¤ä¸ªCTCåˆ†å¸ƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬ä»ä»¥ä¸‹ä¸‰ä¸ªè§’åº¦å¯¹å…¶æ ¸å¿ƒè¡Œä¸ºè¿›è¡Œäº†æ·±å…¥æ´å¯Ÿï¼š1ï¼‰å®ƒåœ¨å¤„ç†ä¸åŒå¢å¼ºè§†å›¾çš„éšæœºå­æ¨¡å‹å¯¹ä¹‹é—´è¿›è¡Œè‡ªæˆ‘è’¸é¦ï¼›2ï¼‰å®ƒé€šè¿‡æ—¶é—´æ©ç åŒºåŸŸå†…çš„ä½ç½®è¿›è¡Œæ©ç é¢„æµ‹æ¥å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯å½“æˆ‘ä»¬å¢åŠ æ—¶é—´æ©ç çš„æ•°é‡æ—¶ï¼›3ï¼‰å®ƒæŠ‘åˆ¶äº†æå°–çš„CTCåˆ†å¸ƒï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨LibriSpeechã€Aishell-1å’ŒGigaSpeechæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„CR-CTCçš„æœ‰æ•ˆæ€§ã€‚å®ƒæ˜¾è‘—æé«˜äº†CTCçš„æ€§èƒ½ï¼Œå®ç°äº†ä¸è½¬æ¢å™¨æˆ–ç»“åˆCTCå’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ï¼ˆCTC&#x2F;AEDï¼‰çš„ç³»ç»Ÿæ‰€è¾¾åˆ°çš„æœ€æ–°æŠ€æœ¯æˆæœç›¸å½“çš„ç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/k2-fsa/icefallä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05101v4">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºConsistency-Regularized CTCï¼ˆCR-CTCï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„Connectionist Temporal Classificationï¼ˆCTCï¼‰æ–¹æ³•çš„è¯†åˆ«æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¸åŒå¢å¼ºè§†å›¾ä¸‹è·å–çš„ä¸¤ä¸ªCTCåˆ†å¸ƒä¹‹é—´å¼ºåˆ¶æ‰§è¡Œä¸€è‡´æ€§ï¼Œå®ç°äº†è‡ªæˆ‘è’¸é¦å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶æŠ‘åˆ¶äº†è¿‡äºå°–é”çš„CTCåˆ†å¸ƒã€‚å®éªŒè¡¨æ˜ï¼ŒCR-CTCèƒ½æ˜¾è‘—æé«˜CTCæ€§èƒ½ï¼Œè¾¾åˆ°ä¸è½¬æ¢å™¨æˆ–CTCä¸åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ï¼ˆCTC&#x2F;AEDï¼‰ç›¸ç»“åˆçš„ç³»ç»Ÿç›¸å½“çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Consistency-Regularized CTC (CR-CTC) æ˜¯å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­å¹¿æ³›ä½¿ç”¨çš„Connectionist Temporal Classificationï¼ˆCTCï¼‰æ–¹æ³•çš„æ”¹è¿›ã€‚</li>
<li>CR-CTC é€šè¿‡åœ¨ä¸åŒå¢å¼ºè§†å›¾ä¸‹è·å–çš„ä¸¤ä¸ª CTC åˆ†å¸ƒä¹‹é—´å¼ºåˆ¶æ‰§è¡Œä¸€è‡´æ€§ï¼Œå®ç°äº†æ€§èƒ½æå‡ã€‚</li>
<li>CR-CTC å®ç°äº†è‡ªæˆ‘è’¸é¦å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡éšæœºå­æ¨¡å‹å¤„ç†ä¸åŒå¢å¼ºè§†å›¾è¿›è¡Œè‡ªæˆ‘è’¸é¦ï¼Œå¹¶å­¦ä¹ æ—¶é—´é®æŒ¡åŒºåŸŸå†…çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚</li>
<li>CR-CTC æŠ‘åˆ¶äº†è¿‡äºå°–é”çš„ CTC åˆ†å¸ƒï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCR-CTC åœ¨ LibriSpeechã€Aishell-1 å’Œ GigaSpeech æ•°æ®é›†ä¸Šå‡èƒ½æœ‰æ•ˆæé«˜ CTC æ€§èƒ½ã€‚</li>
<li>CR-CTC è¾¾åˆ°ä¸å½“å‰æœ€ä½³æ°´å¹³ç›¸å½“çš„ç»“æœï¼Œå¦‚è½¬æ¢å™¨æˆ–ç»“åˆ CTC å’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ï¼ˆCTC&#x2F;AEDï¼‰çš„ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c782291bfbfd42bc25039f95d61bbdf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4f5940d0dbf0e4aa9dcc6cdafffa4f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1ec9bc930d4c1997854dfb61b3d0882.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MoWE-Audio-Multitask-AudioLLMs-with-Mixture-of-Weak-Encoders"><a href="#MoWE-Audio-Multitask-AudioLLMs-with-Mixture-of-Weak-Encoders" class="headerlink" title="MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"></a>MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders</h2><p><strong>Authors:Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou, Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F. Chen, Ai Ti Aw</strong></p>
<p>The rapid advancements in large language models (LLMs) have significantly enhanced natural language processing capabilities, facilitating the development of AudioLLMs that process and understand speech and audio inputs alongside text. Existing AudioLLMs typically combine a pre-trained audio encoder with a pre-trained LLM, which are subsequently finetuned on specific audio tasks. However, the pre-trained audio encoder has constrained capacity to capture features for new tasks and datasets. To address this, we propose to incorporate mixtures of &#96;weakâ€™ encoders (MoWE) into the AudioLLM framework. MoWE supplements a base encoder with a pool of relatively light weight encoders, selectively activated based on the audio input to enhance feature extraction without significantly increasing model size. Our empirical results demonstrate that MoWE effectively improves multi-task performance, broadening the applicability of AudioLLMs to more diverse audio tasks. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›å¾—åˆ°äº†æå¤§çš„æå‡ï¼Œæ¨åŠ¨äº†å¤„ç†å’Œç†è§£è¯­éŸ³å’ŒéŸ³é¢‘è¾“å…¥çš„AudioLLMçš„å‘å±•ã€‚ç°æœ‰çš„AudioLLMé€šå¸¸å°†é¢„è®­ç»ƒçš„éŸ³é¢‘ç¼–ç å™¨ä¸é¢„è®­ç»ƒçš„LLMç›¸ç»“åˆï¼Œéšååœ¨ç‰¹å®šçš„éŸ³é¢‘ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒçš„éŸ³é¢‘ç¼–ç å™¨åœ¨æ•è·æ–°ä»»åŠ¡å’Œæ•°æ®é›†çš„ç‰¹å¾æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåœ¨AudioLLMæ¡†æ¶ä¸­èå…¥â€œå¼±ç¼–ç å™¨æ··åˆç‰©â€ï¼ˆMoWEï¼‰ã€‚MoWEä»¥åŸºæœ¬ç¼–ç å™¨ä¸ºåŸºç¡€ï¼Œè¾…ä»¥ä¸€ç»„ç›¸å¯¹è½»é‡çº§çš„ç¼–ç å™¨æ± ï¼Œæ ¹æ®éŸ³é¢‘è¾“å…¥è¿›è¡Œé€‰æ‹©æ¿€æ´»ï¼Œä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ä¼šæ˜¾è‘—å¢åŠ æ¨¡å‹å¤§å°ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒMoWEæœ‰æ•ˆæé«˜å¤šä»»åŠ¡æ€§èƒ½ï¼Œæ‹“å®½äº†AudioLLMåœ¨æ›´å¤šä¸åŒéŸ³é¢‘ä»»åŠ¡ä¸­çš„åº”ç”¨èŒƒå›´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06635v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ˜¾è‘—æå‡äº†è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œä¿ƒè¿›äº†èƒ½å¤Ÿå¤„ç†å’Œç†è§£è¯­éŸ³å’ŒéŸ³é¢‘è¾“å…¥çš„AudioLLMçš„å‘å±•ã€‚é’ˆå¯¹ç°æœ‰AudioLLMä¸­é¢„è®­ç»ƒéŸ³é¢‘ç¼–ç å™¨å¯¹æ–°ä»»åŠ¡å’Œæ•°æ®é›†ç‰¹å¾æ•æ‰èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†æ··åˆå¼±ç¼–ç å™¨ï¼ˆMoWEï¼‰æ–¹æ³•ã€‚MoWEä»¥åŸºæœ¬ç¼–ç å™¨ä¸ºåŸºç¡€ï¼Œè¾…ä»¥ä¸€ç»„ç›¸å¯¹è½»é‡çº§çš„ç¼–ç å™¨æ± ï¼Œæ ¹æ®éŸ³é¢‘è¾“å…¥é€‰æ‹©æ€§æ¿€æ´»ï¼Œä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æ˜¾è‘—å¢åŠ æ¨¡å‹å¤§å°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoWEæœ‰æ•ˆæé«˜å¤šä»»åŠ¡æ€§èƒ½ï¼Œæ‹“å®½AudioLLMåœ¨æ›´å¤šä¸åŒéŸ³é¢‘ä»»åŠ¡ä¸­çš„åº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>AudioLLMèƒ½å¤Ÿå¤„ç†å’Œç†è§£è¯­éŸ³å’ŒéŸ³é¢‘è¾“å…¥ã€‚</li>
<li>é¢„è®­ç»ƒéŸ³é¢‘ç¼–ç å™¨å¯¹æ–°ä»»åŠ¡å’Œæ•°æ®é›†ç‰¹å¾æ•æ‰èƒ½åŠ›æœ‰é™ã€‚</li>
<li>MoWEæ–¹æ³•é€šè¿‡æ··åˆå¼±ç¼–ç å™¨æ± æé«˜AudioLLMæ€§èƒ½ã€‚</li>
<li>MoWEæ–¹æ³•æ ¹æ®éŸ³é¢‘è¾“å…¥é€‰æ‹©æ€§æ¿€æ´»ç¼–ç å™¨ã€‚</li>
<li>MoWEæœ‰æ•ˆå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æ˜¾è‘—å¢åŠ æ¨¡å‹å¤§å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a5cfab28ede6d6643e571a9ce3d744ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79997d6dbad1c891d4af87916254ce75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f726a108bfc3e1066489bdb799969b08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3def82f5939cf93b0445b8c4f9a96545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac616a31420155adcc3083267927e556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f85bccb557d79382a4278a8c905479a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Wideband-Relative-Transfer-Function-RTF-Estimation-Exploiting-Frequency-Correlations"><a href="#Wideband-Relative-Transfer-Function-RTF-Estimation-Exploiting-Frequency-Correlations" class="headerlink" title="Wideband Relative Transfer Function (RTF) Estimation Exploiting   Frequency Correlations"></a>Wideband Relative Transfer Function (RTF) Estimation Exploiting   Frequency Correlations</h2><p><strong>Authors:Giovanni Bologni, Richard C. Hendriks, Richard Heusdens</strong></p>
<p>This article focuses on estimating relative transfer functions (RTFs) for beamforming applications. Traditional methods often assume that spectra are uncorrelated, an assumption that is often violated in practical scenarios due to factors such as time-domain windowing or the non-stationary nature of signals, as observed in speech. To overcome these limitations, we propose an RTF estimation technique that leverages spectral and spatial correlations through subspace analysis. Additionally, we derive Cram&#39;erâ€“Rao bounds (CRBs) for the RTF estimation task, providing theoretical insights into the achievable estimation accuracy. These bounds reveal that channel estimation can be performed more accurately if the noise or the target signal exhibits spectral correlations. Experiments with both real and synthetic data show that our technique outperforms the narrowband maximum-likelihood estimator, known as covariance whitening (CW), when the target exhibits spectral correlations. Although the proposed algorithm generally achieves accuracy close to the theoretical bound, there is potential for further improvement, especially in scenarios with highly spectrally correlated noise. While channel estimation has various applications, we demonstrate the method using a minimum variance distortionless (MVDR) beamformer for multichannel speech enhancement. A free Python implementation is also provided. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹ç ”ç©¶æ³¢æŸå½¢æˆåº”ç”¨ä¸­ç›¸å¯¹ä¼ é€’å‡½æ•°ï¼ˆRTFï¼‰çš„ä¼°è®¡ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å‡è®¾å…‰è°±ä¸ç›¸å…³ï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­ï¼Œç”±äºæ—¶åŸŸçª—å£åŒ–æˆ–è¯­éŸ³ä¸­çš„éå¹³ç¨³ä¿¡å·ç­‰å› ç´ ï¼Œè¿™ä¸€å‡è®¾ç»å¸¸è¢«è¿åã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å­ç©ºé—´åˆ†æè¿›è¡ŒRTFä¼°è®¡çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨å…‰è°±å’Œç©ºé—´ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å¯¼å‡ºäº†RTFä¼°è®¡ä»»åŠ¡çš„CramÃ©r-Raoç•Œï¼ˆCRBsï¼‰ï¼Œä¸ºä¼°è®¡ç²¾åº¦çš„å¯å®ç°æ€§æä¾›äº†ç†è®ºè§è§£ã€‚è¿™äº›ç•Œé™è¡¨æ˜ï¼Œå¦‚æœå™ªå£°æˆ–ç›®æ ‡ä¿¡å·è¡¨ç°å‡ºå…‰è°±ç›¸å…³æ€§ï¼Œåˆ™å¯ä»¥è¿›è¡Œæ›´å‡†ç¡®çš„ä¿¡é“ä¼°è®¡ã€‚ä½¿ç”¨çœŸå®å’Œåˆæˆæ•°æ®çš„å®éªŒè¡¨æ˜ï¼Œå½“ç›®æ ‡è¡¨ç°å‡ºå…‰è°±ç›¸å…³æ€§æ—¶ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯ä¼˜äºçª„å¸¦æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨ï¼Œç§°ä¸ºåæ–¹å·®ç™½åŒ–ï¼ˆCWï¼‰ã€‚è™½ç„¶æ‰€æç®—æ³•ä¸€èˆ¬èƒ½è¾¾åˆ°æ¥è¿‘ç†è®ºç•Œé™çš„ç²¾åº¦ï¼Œä½†åœ¨å…‰è°±ç›¸å…³æ€§é«˜çš„å™ªå£°åœºæ™¯ä¸­ä»æœ‰è¿›ä¸€æ­¥æ”¹è¿›çš„æ½œåŠ›ã€‚è™½ç„¶ä¿¡é“ä¼°è®¡æœ‰å¤šç§åº”ç”¨ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨æœ€å°æ–¹å·®æ— å¤±çœŸï¼ˆMVDRï¼‰æ³¢æŸå½¢æˆå™¨è¿›è¡Œå¤šé€šé“è¯­éŸ³å¢å¼ºæ¥å±•ç¤ºè¯¥æ–¹æ³•ã€‚è¿˜æä¾›äº†å…è´¹çš„Pythonå®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.14152v2">PDF</a> Accepted version</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨è°±å’Œç©ºåŸŸç›¸å…³æ€§é€šè¿‡å­ç©ºé—´åˆ†æä¼°è®¡ç›¸å¯¹ä¼ é€’å‡½æ•°ï¼ˆRTFsï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ³¢æŸå½¢æˆåº”ç”¨ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å‡è®¾è°±ä¸ç›¸å…³åœ¨å®é™…åœºæ™¯ä¸­ç»å¸¸å¤±æ•ˆçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°æŠ€æœ¯å¹¶æ¨å¯¼å‡ºRTFä¼°è®¡ä»»åŠ¡çš„CramÃ©r-Raoç•Œï¼ˆCRBsï¼‰ï¼Œä¸ºä¼°è®¡ç²¾åº¦æä¾›ç†è®ºè§è§£ã€‚å®éªŒè¡¨æ˜ï¼Œå½“ç›®æ ‡è¡¨ç°å‡ºè°±ç›¸å…³æ€§æ—¶ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯ä¼˜äºçª„å¸¦æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨åæ–¹å·®ç™½åŒ–ï¼ˆCWï¼‰ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºå¤šé€šé“è¯­éŸ³å¢å¼ºçš„æœ€å°æ–¹å·®æ— å¤±çœŸï¼ˆMVDRï¼‰æ³¢æŸå½¢æˆå™¨è¿›è¡Œæ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« èšç„¦äºæ³¢æŸå½¢æˆä¸­çš„ç›¸å¯¹ä¼ é€’å‡½æ•°ï¼ˆRTFsï¼‰ä¼°è®¡é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å‡è®¾è°±ä¸ç›¸å…³ï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­è¿™ä¸€å‡è®¾ç»å¸¸ä¸æˆç«‹ã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨è°±å’Œç©ºåŸŸç›¸å…³æ€§é€šè¿‡å­ç©ºé—´åˆ†æçš„RTFä¼°è®¡æŠ€æœ¯ã€‚</li>
<li>æ¨å¯¼äº†CramÃ©r-Raoç•Œï¼ˆCRBsï¼‰ä»¥äº†è§£å¯è¾¾åˆ°çš„ä¼°è®¡ç²¾åº¦ã€‚</li>
<li>å½“ç›®æ ‡ä¿¡å·è¡¨ç°å‡ºè°±ç›¸å…³æ€§æ—¶ï¼Œæ–°æŠ€æœ¯ä¼˜äºä¼ ç»Ÿçš„åæ–¹å·®ç™½åŒ–ï¼ˆCWï¼‰æ–¹æ³•ã€‚</li>
<li>æŠ€æœ¯åº”ç”¨äºå¤šé€šé“è¯­éŸ³å¢å¼ºçš„æœ€å°æ–¹å·®æ— å¤±çœŸï¼ˆMVDRï¼‰æ³¢æŸå½¢æˆå™¨è¿›è¡Œæ¼”ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.14152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5be8bbf833fc6d0e151bb6c0fc427771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aca0074870f55d36eb97435dbcd1bb48.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DiTTo-TTS-Diffusion-Transformers-for-Scalable-Text-to-Speech-without-Domain-Specific-Factors"><a href="#DiTTo-TTS-Diffusion-Transformers-for-Scalable-Text-to-Speech-without-Domain-Specific-Factors" class="headerlink" title="DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without   Domain-Specific Factors"></a>DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without   Domain-Specific Factors</h2><p><strong>Authors:Keon Lee, Dong Won Kim, Jaehyeon Kim, Seungjun Chung, Jaewoong Cho</strong></p>
<p>Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in text-to-speech (TTS) limits scalability and access from other fields. While recent studies show potential in removing these domain-specific factors, performance remains suboptimal. In this work, we introduce DiTTo-TTS, a Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based TTS can achieve state-of-the-art performance without domain-specific factors. Through rigorous analysis and empirical exploration, we find that (1) DiT with minimal modifications outperforms U-Net, (2) variable-length modeling with a speech length predictor significantly improves results over fixed-length approaches, and (3) conditions like semantic alignment in speech latent representations are key to further enhancement. By scaling our training data to 82K hours and the model size to 790M parameters, we achieve superior or comparable zero-shot performance to state-of-the-art TTS models in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific factors. Speech samples are available at <a target="_blank" rel="noopener" href="https://ditto-tts.github.io/">https://ditto-tts.github.io</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ä¸­å¯¹éŸ³ç´ å’ŒæŒç»­æ—¶é—´çš„ä¾èµ–é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå…¶ä»–é¢†åŸŸçš„è®¿é—®ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶æ˜¾ç¤ºå»é™¤è¿™äº›é¢†åŸŸç‰¹å®šå› ç´ å…·æœ‰æ½œåŠ›ï¼Œä½†æ€§èƒ½ä»ç„¶ä¸å°½äººæ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„TTSæ¨¡å‹DiTTo-TTSï¼Œæ—¨åœ¨ç ”ç©¶åŸºäºLDMçš„TTSæ˜¯å¦èƒ½åœ¨ä¸ä¾èµ–é¢†åŸŸç‰¹å®šå› ç´ çš„æƒ…å†µä¸‹è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡ä¸¥æ ¼çš„åˆ†æå’Œå®è¯æ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°ï¼ˆ1ï¼‰DiTåœ¨æœ€å°ä¿®æ”¹çš„æƒ…å†µä¸‹ä¼˜äºU-Netï¼Œï¼ˆ2ï¼‰ä½¿ç”¨è¯­éŸ³é•¿åº¦é¢„æµ‹çš„å¯å˜é•¿åº¦å»ºæ¨¡æ˜¾è‘—æ”¹å–„äº†ç»“æœï¼Œä¼˜äºå›ºå®šé•¿åº¦çš„æ–¹æ³•ï¼Œï¼ˆ3ï¼‰è¯­éŸ³æ½œåœ¨è¡¨ç¤ºä¸­çš„è¯­ä¹‰å¯¹é½ç­‰æ¡ä»¶æ˜¯è¿›ä¸€æ­¥æ”¹è¿›çš„å…³é”®ã€‚é€šè¿‡å°†è®­ç»ƒæ•°æ®æ‰©å±•åˆ°8.2ä¸‡å°æ—¶å’Œæ¨¡å‹å¤§å°æ‰©å±•åˆ°7.9äº¿å‚æ•°ï¼Œæˆ‘ä»¬åœ¨è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„TTSæ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä¸ä¾èµ–äºé¢†åŸŸç‰¹å®šå› ç´ ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://ditto-tts.github.ioä¸Šæ‰¾åˆ°./">https://ditto-tts.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11427v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºDiffusion Transformerï¼ˆDiTï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹DiTTo-TTSï¼Œè¯¥æ¨¡å‹åœ¨å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„åŸºç¡€ä¸Šï¼Œå®ç°äº†æ— éœ€ç‰¹å®šé¢†åŸŸå› ç´ å³å¯è¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰TTSæ¨¡å‹æ€§èƒ½çš„æ•ˆæœã€‚é€šè¿‡æ”¹è¿›æ¨¡å‹å’Œæ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ï¼Œè¯¥æ¨¡å‹åœ¨è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiTTo-TTSæ¨¡å‹åŸºäºDiffusion Transformerï¼ˆDiTï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä¸­çš„é¢†åŸŸç‰¹å®šå› ç´ ä¾èµ–é—®é¢˜ã€‚</li>
<li>DiTæ¨¡å‹åœ¨æœ€å°ä¿®æ”¹çš„æƒ…å†µä¸‹ä¼˜äºU-Netæ¨¡å‹ï¼ŒéªŒè¯äº†Diffusion Transformeråœ¨TTSä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯å˜é•¿åº¦å»ºæ¨¡å’Œè¯­éŸ³é•¿åº¦é¢„æµ‹å™¨ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œä¼˜äºå›ºå®šé•¿åº¦æ–¹æ³•ã€‚</li>
<li>è¯­ä¹‰å¯¹é½ç­‰æ¡ä»¶åœ¨è¯­éŸ³æ½œåœ¨è¡¨ç¤ºä¸­å¯¹äºè¿›ä¸€æ­¥æé«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®è‡³82Kå°æ—¶å’Œæ¨¡å‹è§„æ¨¡è‡³790Må‚æ•°ï¼ŒDiTTo-TTSå®ç°äº†é›¶æ ·æœ¬æ€§èƒ½ï¼Œåœ¨è‡ªç„¶åº¦ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ä¸æœ€ä½³TTSæ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>è¯¥æ¨¡å‹çš„æ€§èƒ½æå‡æœªä¾èµ–ç‰¹å®šçš„é¢†åŸŸå› ç´ ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-786c8c43a8d343a376a5be2a1f4b5c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cedefde23348a4ff1be248a1e676c82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-695adb8aee01bb6374f8d7096ad484d1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DP-DyLoRA-Fine-Tuning-Transformer-Based-Models-On-Device-under-Differentially-Private-Federated-Learning-using-Dynamic-Low-Rank-Adaptation"><a href="#DP-DyLoRA-Fine-Tuning-Transformer-Based-Models-On-Device-under-Differentially-Private-Federated-Learning-using-Dynamic-Low-Rank-Adaptation" class="headerlink" title="DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under   Differentially Private Federated Learning using Dynamic Low-Rank Adaptation"></a>DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under   Differentially Private Federated Learning using Dynamic Low-Rank Adaptation</h2><p><strong>Authors:Jie Xu, Karthikeyan Saravanan, Rogier van Dalen, Haaris Mehmood, David Tuckey, Mete Ozay</strong></p>
<p>Federated learning (FL) allows clients to collaboratively train a global model without sharing their local data with a server. However, clientsâ€™ contributions to the server can still leak sensitive information. Differential privacy (DP) addresses such leakage by providing formal privacy guarantees, with mechanisms that add randomness to the clientsâ€™ contributions. The randomness makes it infeasible to train large transformer-based models, common in modern federated learning systems. In this work, we empirically evaluate the practicality of fine-tuning large scale on-device transformer-based models with differential privacy in a federated learning system. We conduct comprehensive experiments on various system properties for tasks spanning a multitude of domains: speech recognition, computer vision (CV) and natural language understanding (NLU). Our results show that full fine-tuning under differentially private federated learning (DP-FL) generally leads to huge performance degradation which can be alleviated by reducing the dimensionality of contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks of existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA) consistently outperforms other methods. An even more promising approach, DyLoRA, which makes the low rank variable, when naively combined with FL would straightforwardly break differential privacy. We therefore propose an adaptation method that can be combined with differential privacy and call it DP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word error rate (WER) increase due to DP to less than 2% and 7% respectively with 1 million clients and a stringent privacy budget of $\epsilon&#x3D;2$. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å…è®¸å®¢æˆ·ç«¯åœ¨æ— éœ€å°†æœ¬åœ°æ•°æ®åˆ†äº«ç»™æœåŠ¡å™¨çš„æƒ…å†µä¸‹å…±åŒè®­ç»ƒå…¨çƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œå®¢æˆ·ç«¯å¯¹æœåŠ¡å™¨çš„è´¡çŒ®ä»ç„¶å¯èƒ½æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚å·®åˆ†éšç§ï¼ˆDPï¼‰é€šè¿‡æä¾›æ­£å¼çš„éšç§ä¿è¯æ¥è§£å†³æ­¤ç±»æ³„éœ²é—®é¢˜ï¼Œå…¶æœºåˆ¶ä¼šä¸ºå®¢æˆ·ç«¯çš„è´¡çŒ®å¢åŠ éšæœºæ€§ã€‚è¿™ç§éšæœºæ€§ä½¿å¾—åœ¨è”é‚¦å­¦ä¹ ç³»ç»Ÿä¸­è®­ç»ƒåŸºäºå¤§å‹å˜æ¢æ¨¡å‹çš„æ–¹æ¡ˆä¸å¯è¡Œï¼Œè€Œè¿™ç§æ¨¡å‹åœ¨ç°ä»£è”é‚¦å­¦ä¹ ç³»ç»Ÿä¸­å¾ˆå¸¸è§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å®è¯è§’åº¦è¯„ä¼°äº†åœ¨è”é‚¦å­¦ä¹ ç³»ç»Ÿä¸­å¯¹åŸºäºå¤§å‹å˜æ¢æ¨¡å‹çš„è®¾å¤‡è¿›è¡Œå¾®è°ƒä¸å·®åˆ†éšç§çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤šä¸ªé¢†åŸŸçš„ä»»åŠ¡ä¸Šå¯¹å„ç§ç³»ç»Ÿå±æ€§è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å’Œè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å·®åˆ†ç§æœ‰è”é‚¦å­¦ä¹ ï¼ˆDP-FLï¼‰ä¸‹è¿›è¡Œå…¨é¢å¾®è°ƒé€šå¸¸ä¼šå¯¼è‡´å·¨å¤§çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™å¯ä»¥é€šè¿‡é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å‡å°‘è´¡çŒ®çš„ç»´åº¦æ¥ç¼“è§£ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„DP-PEFTæ–¹æ³•çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒDP-Low-Rank Adaptationï¼ˆDP-LoRAï¼‰å§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ä¸€ç§æ›´æœ‰å‰é€”çš„æ–¹æ³•æ˜¯å°†ä½ç­‰çº§å˜é‡DyLoRAåŒ–ï¼Œä½†ç®€å•åœ°ä¸FLç»“åˆå°†ç›´æ¥ç ´åå·®åˆ†éšç§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯ä»¥ä¸å·®åˆ†éšç§ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç§°ä¹‹ä¸ºDP-DyLoRAã€‚æœ€åï¼Œåœ¨æ‹¥æœ‰1ç™¾ä¸‡å®¢æˆ·ç«¯å’Œä¸¥æ ¼çš„éšç§é¢„ç®—Îµ&#x3D;2çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ç”±äºDPå¯¼è‡´çš„å‡†ç¡®æ€§ä¸‹é™å’Œå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šå‡åˆ†åˆ«é™ä½åˆ°ä¸åˆ°2%å’Œ7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.06368v4">PDF</a> 16 pages, 10 figures, 5 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨è”é‚¦å­¦ä¹ ç³»ç»Ÿä¸­ï¼Œå¯¹å¤§è§„æ¨¡è®¾å¤‡ä¸ŠåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒä¸å·®åˆ†éšç§ç»“åˆçš„å®é™…åº”ç”¨ã€‚é€šè¿‡å®éªŒï¼Œå‘ç°å·®åˆ†ç§æœ‰è”é‚¦å­¦ä¹ ï¼ˆDP-FLï¼‰ä¸‹çš„å…¨é‡å¾®è°ƒä¼šå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå¯é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰é™ä½è´¡çŒ®çš„ç»´åº¦æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚æ–‡ç« è¯„ä¼°äº†ç°æœ‰çš„DP-PEFTæ–¹æ³•ï¼Œå‘ç°DP-Low-Rank Adaptationï¼ˆDP-LoRAï¼‰è¡¨ç°æœ€ä½³ã€‚ç„¶è€Œï¼Œå½“ä¸è”é‚¦å­¦ä¹ ç»“åˆæ—¶ï¼Œæ›´å…ˆè¿›çš„DyLoRAæ–¹æ³•ä¼šç ´åå·®åˆ†éšç§ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯ä»¥ä¸å·®åˆ†éšç§ç»“åˆä½¿ç”¨çš„é€‚åº”æ–¹æ³•ï¼Œç§°ä¸ºDP-DyLoRAã€‚æœ€ç»ˆï¼Œåœ¨ä¸¥æ ¼çš„éšç§é¢„ç®—ä¸‹ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•èƒ½å°†ç”±äºå·®åˆ†éšç§å¯¼è‡´çš„ç²¾åº¦ä¸‹é™å’Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰å¢åŠ å‡å°‘åˆ°ä¸åˆ°2%å’Œ7%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ å…è®¸å®¢æˆ·ç«¯åœ¨æ²¡æœ‰å°†æ•°æ®åˆ†äº«ç»™æœåŠ¡å™¨çš„æƒ…å†µä¸‹å…±åŒè®­ç»ƒå…¨å±€æ¨¡å‹ï¼Œä½†å®¢æˆ·ç«¯å¯¹æœåŠ¡å™¨çš„è´¡çŒ®ä»ç„¶å¯èƒ½æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>å·®åˆ†éšç§é€šè¿‡ä¸ºå®¢æˆ·ç«¯çš„è´¡çŒ®å¢åŠ éšæœºæ€§æ¥è§£å†³æ­¤ç±»æ³„éœ²é—®é¢˜ï¼Œä½†è¿™å¯¹äºç°ä»£è”é‚¦å­¦ä¹ ç³»ç»Ÿä¸­å¸¸è§çš„åŸºäºå¤§å‹è½¬æ¢æ¨¡å‹çš„è®­ç»ƒæ¥è¯´å¹¶ä¸å®ç”¨ã€‚</li>
<li>å…¨é‡å¾®è°ƒåœ¨å·®åˆ†ç§æœ‰è”é‚¦å­¦ä¹ ï¼ˆDP-FLï¼‰ä¸­ä¼šå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå¯é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰é™ä½è´¡çŒ®çš„ç»´åº¦æ¥ç¼“è§£ã€‚</li>
<li>DP-Low-Rank Adaptationï¼ˆDP-LoRAï¼‰åœ¨ç°æœ‰çš„DP-PEFTæ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ›´å…ˆè¿›çš„DyLoRAæ–¹æ³•è‹¥ç›´æ¥ç»“åˆè”é‚¦å­¦ä¹ ä¼šç ´åå·®åˆ†éšç§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DP-DyLoRAï¼Œæ—¨åœ¨ç»“åˆå·®åˆ†éšç§ï¼Œå‡å°‘ç”±äºå·®åˆ†éšç§å¯¼è‡´çš„æ€§èƒ½æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.06368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02b124820ae35e0a04bbe7641a8bdc8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7942df17f1929358b40f2197af4e94fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58aafe448f1b218e1be4f4a4d0c54d94.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d277754a1f10b01e9a2c5624094a71b7.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  THESAURUS Contrastive Graph Clustering by Swapping Fused   Gromov-Wasserstein Couplings
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d657651226aaccedaa9dd3a03afefce3.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound   Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
