<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  SQL-o1 A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-60fda58901d2659265d175113e9252b7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-19-æ›´æ–°"><a href="#2025-02-19-æ›´æ–°" class="headerlink" title="2025-02-19 æ›´æ–°"></a>2025-02-19 æ›´æ–°</h1><h2 id="SQL-o1-A-Self-Reward-Heuristic-Dynamic-Search-Method-for-Text-to-SQL"><a href="#SQL-o1-A-Self-Reward-Heuristic-Dynamic-Search-Method-for-Text-to-SQL" class="headerlink" title="SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL"></a>SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL</h2><p><strong>Authors:Shuai Lyu, Haoran Luo, Zhonghong Ou, Yifan Zhu, Xiaoran Shang, Yang Qin, Meina Song</strong></p>
<p>The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:<a target="_blank" rel="noopener" href="https://github.com/ShuaiLyu0110/SQL-o1">https://github.com/ShuaiLyu0110/SQL-o1</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLï¼ˆText2SQLï¼‰ä»»åŠ¡æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ï¼Œè¯¥é¢†åŸŸå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€æœ‰é™çš„ç”Ÿæˆç©ºé—´å’ŒSQLç”Ÿæˆä¸­çš„è¿è´¯æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQL-o1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘å¥–åŠ±çš„å¯å‘å¼æœç´¢æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜LLMåœ¨SQLæŸ¥è¯¢ç”Ÿæˆä¸­çš„æ¨ç†èƒ½åŠ›ã€‚SQL-o1ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œå¯å‘å¼è¿‡ç¨‹çº§æœç´¢ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ¨¡å¼æ„ŸçŸ¥æ•°æ®é›†ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°äº†è§£æ•°æ®åº“æ¨¡å¼ã€‚åœ¨Birdå’ŒSpideræ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒSQL-o1åœ¨å¤æ‚çš„Birdæ•°æ®é›†ä¸Šæ‰§è¡Œå‡†ç¡®æ€§æé«˜äº†10.8%ï¼Œç”šè‡³è¶…è¶Šäº†GPT-4çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSQL-o1åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ShuaiLyu0110/SQL-o1%E3%80%82">https://github.com/ShuaiLyu0110/SQL-o1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11741v1">PDF</a> 10 pages,4 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬è½¬æ¢ä»»åŠ¡æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚è™½ç„¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨åœ¨è¿™æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†æ¨¡å‹å¯æ‰©å±•æ€§ã€æœ‰é™çš„ç”Ÿæˆç©ºé—´å’ŒSQLç”Ÿæˆè¿è´¯æ€§é—®é¢˜ç­‰æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQL-o1ï¼Œä¸€ç§åŸºäºè‡ªæˆ‘å¥–åŠ±çš„å¯å‘å¼æœç´¢æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜LLMåœ¨SQLæŸ¥è¯¢ç”Ÿæˆä¸­çš„æ¨ç†èƒ½åŠ›ã€‚SQL-o1ç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œå¯å‘å¼è¿‡ç¨‹çº§æœç´¢ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªSchema-Awareæ•°æ®é›†ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°äº†è§£æ•°æ®åº“æ¨¡å¼ã€‚åœ¨Birdå’ŒSpideræ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSQL-o1æé«˜äº†åœ¨å¤æ‚æ•°æ®é›†ä¸Šçš„æ‰§è¡Œç²¾åº¦ï¼Œå¹¶åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Text-to-SQLä»»åŠ¡æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚</li>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´æ¨¡å‹å¯æ‰©å±•æ€§ã€ç”Ÿæˆç©ºé—´æœ‰é™å’ŒSQLç”Ÿæˆè¿è´¯æ€§é—®é¢˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>SQL-o1æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘å¥–åŠ±çš„å¯å‘å¼æœç´¢æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜LLMåœ¨SQLæŸ¥è¯¢ç”Ÿæˆä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SQL-o1ç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œè¿‡ç¨‹çº§æœç´¢ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªSchema-Awareæ•°æ®é›†ã€‚</li>
<li>åœ¨Birdå’ŒSpideræ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSQL-o1æé«˜äº†æ‰§è¡Œç²¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ•°æ®é›†ä¸Šã€‚</li>
<li>SQL-o1åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b76bda56b08d5ae2c558a203c0c9ac3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c40caf6c1427e37bdbd08d1c0fcf1a01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d19f4223f07e8a1736f2a278efe2e5c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars"><a href="#RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars" class="headerlink" title="RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars"></a>RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars</h2><p><strong>Authors:Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari</strong></p>
<p>Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignmentâ€“factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at <a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE">https://github.com/AnonymousCode-ComputerScience/RIDE</a>. </p>
<blockquote>
<p>å¯¹é½è°ƒæ•´å¯¹äºç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥é“å¾·å’Œæœ‰ç›Šçš„æ–¹å¼è¡Œä¸ºè‡³å…³é‡è¦ã€‚å½“å‰çš„å¯¹é½æ–¹æ³•éœ€è¦é«˜è´¨é‡æ ‡æ³¨å’Œå¤§é‡çš„è®­ç»ƒèµ„æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„å…è°ƒæ•´æ–¹æ³•ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å¢å¼ºLLMçš„å¯¹é½èƒ½åŠ›ã€‚é€šè¿‡å¯¹é«˜è´¨é‡ICLæ¼”ç¤ºå†…å®¹çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°é£æ ¼æ˜¯å½±å“LLMå¯¹é½èƒ½åŠ›çš„å…³é”®å› ç´ ï¼Œå¹¶åŸºäºæ­¤é£æ ¼æ¡†æ¶æ˜ç¡®åœ°å¯¹ICLç¤ºä¾‹è¿›è¡Œäº†é‡æ–°è®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é‡æ–°è®¾è®¡çš„æ¼”ç¤ºå†…å®¹ç›¸ç»“åˆï¼Œå®ç°äº†LLMå¯¹é½çš„ä¸¤ä¸ªæ–¹é¢ä¹‹é—´çš„å¹³è¡¡â€”â€”äº‹å®æ€§å’Œå®‰å…¨æ€§ã€‚æˆ‘ä»¬å°†é‡æ–°è®¾è®¡çš„ç¤ºä¾‹æ‰“åŒ…ä¸ºæç¤ºï¼Œä»¥è§¦å‘å°‘é‡å­¦ä¹ ï¼Œä»è€Œæé«˜LLMçš„å¯¹é½èƒ½åŠ›ã€‚ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ˆæœ€é«˜å¹³å‡å¾—åˆ†ä¸º5.00ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Alpacaä»»åŠ¡ä¸Šæœ€é«˜å¾—åˆ†å¢åŠ äº†0.10ï¼ˆä»4.50åˆ°4.60ï¼‰ï¼Œåœ¨Just-evalåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†0.22ï¼ˆä»4.34åˆ°4.56ï¼‰ï¼Œåœ¨MT-Benchæ•°æ®é›†ä¸Šçš„æœ€é«˜æ”¹è¿›ä¸º0.32ï¼ˆä»3.53åˆ°3.85ï¼‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE">https://github.com/AnonymousCode-ComputerScience/RIDE</a>ä¸Šå‘å¸ƒäº†ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11681v1">PDF</a> 37 pages, 1 figure, 20 tables; The paper is under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬ã€æ— éœ€è°ƒæ•´çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡é«˜è´¨é‡ICLæ¼”ç¤ºåˆ†æï¼Œå‘ç°é£æ ¼æ˜¯å½±å“LLMå¯¹é½èƒ½åŠ›çš„é‡è¦å› ç´ ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šé‡æ–°è®¾è®¡äº†ICLèŒƒä¾‹ã€‚é€šè¿‡ç»“åˆè¿™äº›é‡æ–°è®¾è®¡çš„èŒƒä¾‹ï¼Œå®ç°äº†LLMå¯¹é½çš„ä¸¤ä¸ªæ–¹é¢â€”â€”äº‹å®æ€§å’Œå®‰å…¨æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•æé«˜äº†LLMçš„å¯¹é½èƒ½åŠ›ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä½æˆæœ¬ã€æ— éœ€è°ƒæ•´çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼¦ç†å’Œå‹å¥½è¡Œä¸ºå¯¹é½ã€‚</li>
<li>è®ºæ–‡å‘ç°é£æ ¼æ˜¯å½±å“LLMå¯¹é½èƒ½åŠ›çš„å…³é”®å› ç´ ï¼Œå¹¶åŸºäºæ­¤é‡æ–°è®¾è®¡äº†ICLèŒƒä¾‹ã€‚</li>
<li>è®ºæ–‡é€šè¿‡ç»“åˆé‡æ–°è®¾è®¡çš„èŒƒä¾‹ï¼Œå¹³è¡¡äº†LLMå¯¹é½çš„ä¸¤ä¸ªæ–¹é¢ï¼šäº‹å®æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è§¦å‘å°‘æ ·æœ¬å­¦ä¹ ï¼Œæé«˜äº†LLMçš„å¯¹é½èƒ½åŠ›ã€‚</li>
<li>ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè®ºæ–‡ä¸­çš„æ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>è®ºæ–‡å…¬å¼€äº†ç›¸å…³ä»£ç å’Œæ•°æ®ï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-068ecf4ab281af134d5798a02c8e8d6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb429d76cda68361b2b39742087f867.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8772cbaa15cc59a66a5d8d3345febd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1b1faa528f8d163e6e1f10391ca85b3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VisPath-Automated-Visualization-Code-Synthesis-via-Multi-Path-Reasoning-and-Feedback-Driven-Optimization"><a href="#VisPath-Automated-Visualization-Code-Synthesis-via-Multi-Path-Reasoning-and-Feedback-Driven-Optimization" class="headerlink" title="VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning   and Feedback-Driven Optimization"></a>VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning   and Feedback-Driven Optimization</h2><p><strong>Authors:Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, Seunghyun Lee</strong></p>
<p>Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation, which systematically enhances code quality through structured reasoning and refinement. VisPath is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, VisPath generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17%, offering a more reliable solution for AI-driven visualization code generation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´æ€§è¿›å±•å¢å¼ºäº†å…¶åœ¨è‡ªåŠ¨åŒ–å¯è§†åŒ–ä»£ç ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚å°½ç®¡å°‘æ ·æœ¬æç¤ºå’ŒæŸ¥è¯¢æ‰©å±•æŠ€æœ¯æ˜¾è‘—æé«˜äº†æ•°æ®å¯è§†åŒ–æ€§èƒ½ï¼Œä½†ä»ç„¶æ— æ³•å…‹æœè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„æ¨¡ç³Šæ€§å’Œå¤æ‚æ€§ï¼Œç»™äººå·¥å¹²é¢„å¸¦æ¥äº†å›ºæœ‰è´Ÿæ‹…ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶â€”â€”VisPathï¼šå¯è§†åŒ–ä»£ç ç”Ÿæˆçš„å¤šè·¯å¾„æ¨ç†ä¸åé¦ˆé©±åŠ¨ä¼˜åŒ–æ¡†æ¶ã€‚VisPathé€šè¿‡ç»“æ„åŒ–æ¨ç†å’Œç»†åŒ–ç³»ç»Ÿåœ°æé«˜ä»£ç è´¨é‡ã€‚VisPathæ˜¯ä¸€ä¸ªå¤šé˜¶æ®µæ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡æ¥å¤„ç†æœªæŒ‡å®šçš„æŸ¥è¯¢ã€‚ä¸ºäº†ç”Ÿæˆç¨³å¥çš„æœ€ç»ˆå¯è§†åŒ–ä»£ç ï¼Œå®ƒé¦–å…ˆåˆ©ç”¨åˆå§‹æŸ¥è¯¢é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºç”Ÿæˆå¤šæ ·åŒ–çš„é‡æ„æŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢ä»£è¡¨ä¸€ä¸ªç‹¬ç‰¹çš„æ¨ç†è·¯å¾„ã€‚ç»è¿‡ç»†åŒ–çš„æŸ¥è¯¢ç”¨äºç”Ÿæˆå€™é€‰å¯è§†åŒ–è„šæœ¬ï¼Œç„¶åæ‰§è¡Œç”Ÿæˆå¤šä¸ªå›¾åƒã€‚å…¨é¢è¯„ä¼°è¾“å‡ºçš„æ­£ç¡®æ€§å’Œè´¨é‡ï¼ŒVisPathä¸ºæ¯å¼ å›¾åƒç”Ÿæˆåé¦ˆï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°èšåˆæ¨¡å—ä¸­ä»¥ç”Ÿæˆæœ€ä½³ç»“æœã€‚åœ¨åŒ…æ‹¬MatPlotBenchå’ŒQwen-Agentä»£ç è§£é‡Šå™¨åŸºå‡†æµ‹è¯•åœ¨å†…çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVisPathæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†17%ï¼Œä¸ºAIé©±åŠ¨çš„å¯è§†åŒ–ä»£ç ç”Ÿæˆæä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11140v1">PDF</a> 14 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–å¯è§†åŒ–ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚è™½ç„¶å°‘æ ·æœ¬æç¤ºå’ŒæŸ¥è¯¢æ‰©å±•æŠ€æœ¯æ˜¾è‘—æé«˜äº†æ•°æ®å¯è§†åŒ–æ€§èƒ½ï¼Œä½†ä»æœªèƒ½å…‹æœè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„æ¨¡ç³Šæ€§å’Œå¤æ‚æ€§ï¼Œä»éœ€è¦äººå·¥å¹²é¢„ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisPathæ¡†æ¶ï¼Œå³ä¸€ä¸ªç”¨äºå¯è§†åŒ–ä»£ç ç”Ÿæˆçš„å¤šè·¯å¾„æ¨ç†å’Œåé¦ˆé©±åŠ¨çš„ä¼˜åŒ–æ¡†æ¶ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–æ¨ç†å’Œç»†åŒ–ç³»ç»Ÿåœ°æé«˜ä»£ç è´¨é‡ã€‚VisPathæ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†æœªæŒ‡å®šæŸ¥è¯¢çš„å¤šé˜¶æ®µæ¡†æ¶ã€‚å®ƒé¦–å…ˆåˆ©ç”¨åˆå§‹æŸ¥è¯¢ç”Ÿæˆå¤šç§ç»è¿‡æ”¹é©çš„æŸ¥è¯¢ï¼Œç„¶åé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºä»£è¡¨ä¸åŒçš„æ¨ç†è·¯å¾„ã€‚è¿™äº›ç²¾ç»†æŸ¥è¯¢ç”¨äºç”Ÿæˆå€™é€‰å¯è§†åŒ–è„šæœ¬ï¼Œè¿›è€Œæ‰§è¡Œç”Ÿæˆå¤šä¸ªå›¾åƒã€‚å…¨é¢è¯„ä¼°è¾“å‡ºç»“æœçš„æ­£ç¡®æ€§å’Œè´¨é‡åï¼ŒVisPathä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆåé¦ˆï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°èšåˆæ¨¡å—ä¸­ä»¥ç”Ÿæˆæœ€ä½³ç»“æœã€‚åœ¨MatPlotBenchå’ŒQwen-Agentä»£ç è§£é‡Šå™¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVisPathæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹³å‡æé«˜äº†17%ï¼Œä¸ºAIé©±åŠ¨çš„å¯è§†åŒ–ä»£ç ç”Ÿæˆæä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Language Models (LLMs) å·²åœ¨è‡ªåŠ¨åŒ–å¯è§†åŒ–ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—é‡å¤§çªç ´ã€‚</li>
<li>è™½ç„¶å°‘æ ·æœ¬æç¤ºå’ŒæŸ¥è¯¢æ‰©å±•æŠ€æœ¯æœ‰æ‰€åŠ©ç›Šï¼Œä½†ä»å­˜åœ¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„æ¨¡ç³Šæ€§å’Œå¤æ‚æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†VisPathæ¡†æ¶ï¼Œä¸€ä¸ªåŒ…å«å¤šé˜¶æ®µã€å¤šè·¯å¾„æ¨ç†å’Œåé¦ˆé©±åŠ¨çš„ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥å¤„ç†æœªæŒ‡å®šçš„æŸ¥è¯¢å¹¶æå‡ä»£ç è´¨é‡ã€‚</li>
<li>VisPathé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºç”Ÿæˆå¤šç§æŸ¥è¯¢ï¼Œä»£è¡¨ä¸åŒçš„æ¨ç†è·¯å¾„ã€‚</li>
<li>VisPathèƒ½å¤Ÿç”Ÿæˆå€™é€‰å¯è§†åŒ–è„šæœ¬ï¼Œå¹¶é€šè¿‡æ‰§è¡Œç”Ÿæˆå¤šä¸ªå›¾åƒã€‚</li>
<li>é€šè¿‡å…¨é¢è¯„ä¼°è¾“å‡ºç»“æœçš„æ­£ç¡®æ€§å’Œè´¨é‡ï¼ŒVisPathä¸ºå›¾åƒç”Ÿæˆåé¦ˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVisPathåœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹³å‡æé«˜äº†17%çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1ee4a9a0550f570e8b510a3e28a43519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abb2f06aa96b897e77e7ca5409928bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60fda58901d2659265d175113e9252b7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data"><a href="#Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data" class="headerlink" title="Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?"></a>Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?</h2><p><strong>Authors:Yutong Yin, Zhaoran Wang</strong></p>
<p>Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B &#x3D; f(A) ) from one source and ( C &#x3D; g(B) ) from another, they can deduce ( C&#x3D;g(B)&#x3D;g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, â€œFTCTâ€ (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿé€šè¿‡æ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„çŸ¥è¯†å±•ç°å‡ºæƒŠäººçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰äººä»æŸä¸ªæ¥æºå­¦ä¹ åˆ°å…¬å¼ï¼ˆB&#x3D;fï¼ˆAï¼‰ï¼‰ï¼Œå¹¶ä»å¦ä¸€ä¸ªæ¥æºå­¦ä¹ åˆ°å…¬å¼ï¼ˆC&#x3D;gï¼ˆBï¼‰ï¼‰ï¼Œå³ä½¿æ²¡æœ‰åŒæ—¶é‡åˆ°è¿™ä¸‰ä¸ªå…ƒç´ ï¼ˆABCï¼‰ï¼Œä»–ä»¬ä¹Ÿèƒ½å¤Ÿæ¨æ–­å‡ºï¼ˆC&#x3D;gï¼ˆfï¼ˆAï¼‰ï¼‰ï¼‰ï¼Œè¿™å±•ç°äº†äººç±»æ™ºåŠ›çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€é¡¹åˆæˆå­¦ä¹ ä»»åŠ¡â€œFTCTâ€ï¼ˆè®­ç»ƒæ—¶ç‰‡æ®µåŒ–ï¼Œæµ‹è¯•æ—¶é“¾æ¥ï¼‰ï¼Œä»¥éªŒè¯Transformeråœ¨å¤åˆ¶è¿™é¡¹æŠ€èƒ½æ–¹é¢çš„æ½œåŠ›å¹¶è§£é‡Šå…¶å†…åœ¨æœºåˆ¶ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ•°æ®ç”±æ¥è‡ªæ•´ä½“å› æœå›¾çš„ç‹¬ç«‹çŸ¥è¯†ç‰‡æ®µç»„æˆã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼ŒTransformerå¿…é¡»é€šè¿‡æ•´åˆè¿™äº›ç‰‡æ®µæ¥æ¨æ–­å®Œæ•´çš„å› æœå›¾è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œé€šè¿‡æ­ç¤ºæ­£ç¡®çš„ç‰‡æ®µç»„åˆï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨è¿™æ ·çš„ç»„åˆï¼Œå°‘æ•°é“¾å¼æ€ç»´æç¤ºä¹Ÿèƒ½ä½¿Transformeråœ¨FTCTä¸Šæ‰§è¡Œç»„åˆæ¨ç†ã€‚æ­¤å¤–ï¼Œç»„åˆæ¨ç†èƒ½åŠ›çš„å‡ºç°ä¸æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒæµ‹è¯•æ•°æ®ç›¸ä¼¼æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬ä»ç†è®ºå’Œå®è¯ä¸¤ä¸ªæ–¹é¢æå‡ºï¼ŒTransformerä»è®­ç»ƒä¸­å­¦ä¹ åˆ°ä¸€ä¸ªæ½œåœ¨çš„å¯æ¨å¹¿ç¨‹åºï¼Œä»è€Œåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­å®ç°æœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15857v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äººç±»å±•ç°å‡ºçš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡æ•´åˆä¸åŒæ¥æºçš„çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†éªŒè¯Transformeræ˜¯å¦å…·æœ‰è¿™ç§èƒ½åŠ›ï¼Œæœ¬æ–‡å¼•å…¥äº†â€œFTCTâ€ï¼ˆè®­ç»ƒæ—¶ç¢ç‰‡åŒ–ï¼Œæµ‹è¯•æ—¶è¿é”ï¼‰åˆæˆå­¦ä¹ ä»»åŠ¡ï¼Œå¹¶æ­ç¤ºäº†å…¶å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°‘æ ·æœ¬çš„â€œæ€ç»´é“¾â€æç¤ºèƒ½å¤Ÿä½¿å¾—Transformeråœ¨FTCTä»»åŠ¡ä¸Šæ‰§è¡Œç»„åˆæ¨ç†ï¼Œé€šè¿‡æ­ç¤ºçŸ¥è¯†ç‰‡æ®µçš„æ­£ç¡®ç»„åˆæ¥æ¨å¯¼å‡ºå®Œæ•´çš„å› æœå›¾è½¨è¿¹ã€‚æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒæµ‹è¯•æ•°æ®ç›¸ä¼¼æ€§å¯¹ç»„åˆæ¨ç†èƒ½åŠ›çš„æ¶Œç°æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»èƒ½æ•´åˆä¸åŒæ¥æºçš„çŸ¥è¯†è¿›è¡Œç»„åˆæ¨ç†ã€‚</li>
<li>å¼•å…¥äº†â€œFTCTâ€åˆæˆå­¦ä¹ ä»»åŠ¡æ¥éªŒè¯Transformerçš„ç»„åˆæ¨ç†æ½œåŠ›ã€‚</li>
<li>å°‘æ ·æœ¬çš„â€œæ€ç»´é“¾â€æç¤ºèƒ½è®©Transformeråœ¨FTCTä»»åŠ¡ä¸Šæ‰§è¡Œç»„åˆæ¨ç†ã€‚</li>
<li>Transformerèƒ½é€šè¿‡å­¦ä¹ åº•å±‚çš„ä¸€èˆ¬ç¨‹åºæ¥å®ç°æœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</li>
<li>æ¨¡å‹å¤æ‚åº¦å¯¹Transformerçš„ç»„åˆæ¨ç†èƒ½åŠ›æœ‰é‡è¦å½±å“ã€‚</li>
<li>è®­ç»ƒæµ‹è¯•æ•°æ®çš„ç›¸ä¼¼æ€§å¯¹æ¨¡å‹åœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dac936aae519b3ee1d70d5ba4c39ed5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df1a1e1d6c7939762f9f833466e43c67.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Option-ID-Based-Elimination-For-Multiple-Choice-Questions"><a href="#Option-ID-Based-Elimination-For-Multiple-Choice-Questions" class="headerlink" title="Option-ID Based Elimination For Multiple Choice Questions"></a>Option-ID Based Elimination For Multiple Choice Questions</h2><p><strong>Authors:Zhenhao Zhu, Bulou Liu, Qingyao Ai, Yiqun Liu</strong></p>
<p>Multiple choice questions (MCQs) are a popular and important task for evaluating large language models (LLMs). Based on common strategies people use when answering MCQs, the process of elimination (PoE) has been proposed as an effective problem-solving method. Existing methods to the PoE generally fall into two categories: one involves having the LLM directly select the incorrect options, while the other involves scoring the options. However, both methods incur high computational costs and often perform worse than methods that directly answer the MCQs with the option IDs. To address this issue, this paper proposes a PoE based on option ID. Specifically, our method eliminates option by selecting the option ID with the lowest probability. We conduct experiments with 10 different LLMs in zero-shot settings on 7 publicly available datasets. The experimental results demonstrate that our method significantly improves the LLMâ€™s performance. Further analysis reveals that the sequential elimination strategy can effectively enhance the LLMâ€™s reasoning ability. Additionally, we find that sequential elimination is also applicable to few-shot settings and can be combined with debias methods to further improve LLMâ€™s performance. </p>
<blockquote>
<p>å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æµè¡Œä¸”é‡è¦çš„ä»»åŠ¡ã€‚åŸºäºäººä»¬å›ç­”é€‰æ‹©é¢˜æ—¶å¸¸ç”¨çš„ç­–ç•¥ï¼Œæå‡ºäº†æ’é™¤æ³•ï¼ˆPoEï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è§£å†³é—®é¢˜çš„æ–¹æ³•ã€‚ç°æœ‰è§£å†³æ’é™¤æ³•çš„æ–¹æ³•å¤§è‡´åˆ†ä¸ºä¸¤ç§ï¼šä¸€ç§æ˜¯è®©LLMç›´æ¥é€‰æ‹©é”™è¯¯çš„é€‰é¡¹ï¼Œå¦ä¸€ç§æ˜¯ç»™é€‰é¡¹æ‰“åˆ†ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä¸”å¾€å¾€ä¸å¦‚ç›´æ¥é€šè¿‡é€‰é¡¹IDå›ç­”é—®é¢˜çš„æ–¹æ³•è¡¨ç°å¾—æ›´å¥½ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€‰é¡¹IDçš„æ’é™¤æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡é€‰æ‹©æ¦‚ç‡æœ€ä½çš„é€‰é¡¹IDæ¥æ’é™¤é€‰é¡¹ã€‚æˆ‘ä»¬åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹ä½¿ç”¨10ä¸ªä¸åŒçš„LLMåœ¨7ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMçš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œé¡ºåºæ’é™¤ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é¡ºåºæ’é™¤ä¹Ÿé€‚ç”¨äºå°æ ·æœ¬ç¯å¢ƒï¼Œå¹¶ä¸”å¯ä»¥ä¸å»åæ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜LLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15175v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šé€‰é¢˜çš„ç‰¹ç‚¹å’Œç”¨æˆ·ç­”é¢˜æ—¶çš„ç­–ç•¥ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé€‰é¡¹IDçš„è¿‡ç¨‹æ¶ˆé™¤æ³•ï¼ˆPoEï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é€‰æ‹©æ¦‚ç‡æœ€ä½çš„é€‰é¡¹IDè¿›è¡Œæ¶ˆé™¤ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¹Ÿé€‚ç”¨äºå°æ ·æœ¬åœºæ™¯ï¼Œå¹¶èƒ½ä¸å»åæ–¹æ³•ç»“åˆè¿›ä¸€æ­¥æå‡LLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹æ¶ˆé™¤æ³•ï¼ˆPoEï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³å¤šé¡¹é€‰æ‹©é¢˜çš„æ–¹æ³•ï¼ŒåŸºäºé€‰é¡¹IDè¿›è¡Œé€‰æ‹©ï¼Œå¯æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰PoEæ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šç›´æ¥é€‰æ‹©é”™è¯¯é€‰é¡¹å’Œè¯„åˆ†é€‰é¡¹ï¼Œä½†éƒ½å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>æ–°æå‡ºçš„PoEæ–¹æ³•é€šè¿‡é€‰æ‹©æ¦‚ç‡æœ€ä½çš„é€‰é¡¹IDè¿›è¡Œæ¶ˆé™¤ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹éƒ½æœ‰æ•ˆï¼Œä¸”èƒ½ä¸å…¶ä»–æ–¹æ³•ï¼ˆå¦‚å»åæ–¹æ³•ï¼‰ç»“åˆè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>è¿™ç§æ–¹æ³•èƒ½å¤Ÿå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¤šé¡¹é€‰æ‹©é¢˜æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä»»åŠ¡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0469fb45e945dc02923fd26a649ed284.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed3fd2c1a8e8cd6f50c7faa5e3173ef1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a8e2fafe70bc0b6aa0f98100371d333.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ac52ad87136c3b48c33cf4300672027.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay"><a href="#Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay" class="headerlink" title="Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay"></a>Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay</h2><p><strong>Authors:Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, Xiangyang Ji</strong></p>
<p>The foundation model enables general-purpose problem-solving and enjoys desirable rapid adaptation due to its adopted cross-task generalization paradigms, e.g., pretraining, meta-training, and finetuning. Recent advances in these paradigms show the crucial role of challenging tasksâ€™ prioritized sampling in enhancing adaptation robustness. However, ranking task difficulties exhausts massive task queries to evaluate, thus computation and annotation intensive, which is typically unaffordable in practice. This work underscores the criticality of both adaptation robustness and learning efficiency, especially in scenarios where tasks are risky or costly to evaluate, e.g., policy evaluations in Markov decision processes (MDPs) or inference with large models. To this end, we present Model Predictive Task Sampling (MPTS) to establish connections between the task space and adaptation risk landscape to form a theoretical guideline in robust active task sampling. MPTS characterizes the task episodic information with a generative model and directly predicts task-specific adaptation risk values from posterior inference. The developed risk learner can amortize expensive evaluation and provably approximately rank task difficulties in the pursuit of task robust adaptation. MPTS can be seamlessly integrated into zero-shot, few-shot, and many-shot learning paradigms. Extensive experimental results are conducted to exhibit the superiority of the proposed framework, remarkably increasing task adaptation robustness and retaining learning efficiency in contrast to existing state-of-the-art (SOTA) methods. The code is available at the project site <a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS">https://github.com/thu-rllab/MPTS</a>. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹èƒ½å¤Ÿå®ç°é€šç”¨é—®é¢˜è§£å†³ï¼Œå¹¶ç”±äºå…¶é‡‡ç”¨çš„è·¨ä»»åŠ¡æ³›åŒ–èŒƒå¼ï¼ˆå¦‚é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒï¼‰è€Œäº«æœ‰ä»¤äººæ„‰æ‚¦çš„å¿«é€Ÿé€‚åº”ã€‚è¿™äº›èŒƒå¼çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œä¼˜å…ˆé‡‡æ ·å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡åœ¨æé«˜é€‚åº”ç¨³å¥æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œå¯¹ä»»åŠ¡éš¾åº¦çš„æ’åºéœ€è¦å¤§é‡çš„ä»»åŠ¡æŸ¥è¯¢æ¥è¯„ä¼°ï¼Œå› æ­¤è®¡ç®—å’Œæ ‡æ³¨æˆæœ¬å¯†é›†ï¼Œåœ¨å®è·µä¸­é€šå¸¸æ˜¯æ— æ³•æ‰¿å—çš„ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡è¯„ä¼°é£é™©å¤§æˆ–æˆæœ¬é«˜çš„åœºæ™¯ä¸­ï¼Œå¦‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„ç­–ç•¥è¯„ä¼°æˆ–ä½¿ç”¨å¤§å‹æ¨¡å‹çš„æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œä»¥å»ºç«‹ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ä¹‹é—´çš„è”ç³»ï¼Œå½¢æˆç¨³å¥ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·çš„ç†è®ºæŒ‡å—ã€‚MPTSä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¯¹ä»»åŠ¡ç‰‡æ®µä¿¡æ¯è¿›è¡Œè¡¨å¾ï¼Œå¹¶é€šè¿‡åéªŒæ¨æ–­ç›´æ¥é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©å€¼ã€‚æ‰€å¼€å‘çš„é£é™©å­¦ä¹ è€…å¯ä»¥æ‘Šé”€æ˜‚è´µçš„è¯„ä¼°æˆæœ¬ï¼Œå¹¶è¯æ˜å¯ä»¥è¿‘ä¼¼åœ°å¯¹ä»»åŠ¡éš¾åº¦è¿›è¡Œæ’åï¼Œä»¥å®ç°ç¨³å¥çš„ä»»åŠ¡é€‚åº”ã€‚MPTSå¯ä»¥æ— ç¼åœ°èå…¥é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ä¸­ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒæ¥å±•ç¤ºæ‰€æå‡ºæ¡†æ¶çš„ä¼˜è¶Šæ€§ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å¢åŠ ä»»åŠ¡é€‚åº”ç¨³å¥æ€§çš„åŒæ—¶ä¿æŒäº†å­¦ä¹ æ•ˆç‡ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-rllab/MPTSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11039v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒç­‰è·¨ä»»åŠ¡æ³›åŒ–èŒƒå¼ä½¿å¾—åŸºç¡€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé€šç”¨é—®é¢˜æ±‚è§£ï¼Œå¹¶å…·å¤‡ç†æƒ³çš„å¿«é€Ÿé€‚åº”èƒ½åŠ›ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼ŒæŒ‘æˆ˜æ€§ä»»åŠ¡çš„ä¼˜å…ˆé‡‡æ ·åœ¨å¢å¼ºé€‚åº”ç¨³å¥æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œä»»åŠ¡éš¾åº¦æ’åºéœ€è¦å¤§é‡ä»»åŠ¡æŸ¥è¯¢è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—ä¸æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œå®è·µä¸­é€šå¸¸éš¾ä»¥å®ç°ã€‚æœ¬ç ”ç©¶å¼ºè°ƒé€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡è¯„ä¼°é£é™©å¤§æˆ–æˆæœ¬é«˜çš„åœºæ™¯ä¸­ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œé€šè¿‡å»ºç«‹ä»»åŠ¡ç©ºé—´ä¸é€‚åº”é£é™©æ™¯è§‚ä¹‹é—´çš„è”ç³»ï¼Œå½¢æˆç¨³å¥æ´»åŠ¨ä»»åŠ¡é‡‡æ ·çš„ç†è®ºæŒ‡å—ã€‚MPTSåˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆ»ç”»ä»»åŠ¡ç‰‡æ®µä¿¡æ¯ï¼Œé€šè¿‡åå¤©æ¨ç†ç›´æ¥é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©å€¼ã€‚æ‰€å¼€å‘çš„é£é™©å­¦ä¹ è€…å¯ä»¥æ‘Šé”€æ˜‚è´µçš„è¯„ä¼°æˆæœ¬ï¼Œå¹¶è¯æ˜å¯ä»¥è¿‘ä¼¼åœ°å¯¹ä»»åŠ¡éš¾åº¦è¿›è¡Œæ’åºï¼Œä»¥å®ç°ç¨³å¥çš„ä»»åŠ¡é€‚åº”ã€‚MPTSå¯ä»¥æ— ç¼é›†æˆåˆ°é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨ä»»åŠ¡é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹é€šè¿‡æ³›åŒ–èŒƒå¼å¦‚é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒï¼Œå®ç°é€šç”¨é—®é¢˜æ±‚è§£å’Œå¿«é€Ÿé€‚åº”ã€‚</li>
<li>æŒ‘æˆ˜æ€§ä»»åŠ¡çš„ä¼˜å…ˆé‡‡æ ·å¯¹äºå¢å¼ºé€‚åº”ç¨³å¥æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ä»»åŠ¡éš¾åº¦æ’åºéœ€è¦å¤§é‡ä»»åŠ¡æŸ¥è¯¢è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—ä¸æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡åœ¨é£é™©æ€§æˆ–æˆæœ¬é«˜çš„ä»»åŠ¡è¯„ä¼°ä¸­å°¤ä¸ºé‡è¦ã€‚</li>
<li>æå‡ºæ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ä»¥å»ºç«‹ä»»åŠ¡ç©ºé—´ä¸é€‚åº”é£é™©ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>MPTSåˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆ»ç”»ä»»åŠ¡ç‰‡æ®µä¿¡æ¯ï¼Œé€šè¿‡åå¤©æ¨ç†é¢„æµ‹ä»»åŠ¡é€‚åº”é£é™©å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9c0b2c8cf2707c6afde8eb6aaf5b0e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24139fcc86a2134517cbf5de30c17419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4d6c54a15203254ee1caf0da3e467c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c26a0190b18b8a161ce4e375f396331.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BoostStep-Boosting-mathematical-capability-of-Large-Language-Models-via-improved-single-step-reasoning"><a href="#BoostStep-Boosting-mathematical-capability-of-Large-Language-Models-via-improved-single-step-reasoning" class="headerlink" title="BoostStep: Boosting mathematical capability of Large Language Models via   improved single-step reasoning"></a>BoostStep: Boosting mathematical capability of Large Language Models via   improved single-step reasoning</h2><p><strong>Authors:Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang</strong></p>
<p>Large language models (LLMs) have demonstrated impressive ability in solving complex mathematical problems with multi-step reasoning and can be further enhanced with well-designed in-context learning (ICL) examples. However, this potential is often constrained by two major challenges in ICL: granularity mismatch and irrelevant information. We observe that while LLMs excel at decomposing mathematical problems, they often struggle with reasoning errors in fine-grained steps. Moreover, ICL examples retrieved at the question level may omit critical steps or even mislead the model with irrelevant details. To address this issue, we propose BoostStep, a method that enhances reasoning accuracy through step-aligned ICL, a novel mechanism that carefully aligns retrieved reference steps with the corresponding reasoning steps. Additionally, BoostStep incorporates an effective â€œfirst-tryâ€ strategy to deliver exemplars highly relevant to the current state of reasoning. BoostStep is a flexible and powerful method that integrates seamlessly with chain-of-thought (CoT) and tree search algorithms, refining both candidate selection and decision-making. Empirical results show that BoostStep improves GPT-4oâ€™s CoT performance by 4.6% across mathematical benchmarks, significantly surpassing traditional few-shot learningâ€™s 1.2%. Moreover, it can achieve an additional 7.5% gain combined with tree search. Surprisingly, it enhances state-of-the-art LLMs to solve challenging math problems using simpler examples. It improves DeepSeek-R1-671Bâ€™s performance on AIME by 2.2%, leveraging simple examples only from the MATH dataset. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„å¤æ‚æ•°å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç²¾å¿ƒè®¾è®¡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹æ¥è¿›ä¸€æ­¥å¢å¼ºã€‚ç„¶è€Œï¼Œè¿™ä¸€æ½œåŠ›ç»å¸¸å—åˆ°ICLä¸­ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜çš„é™åˆ¶ï¼šç²’åº¦ä¸åŒ¹é…å’Œæ— å…³ä¿¡æ¯ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶LLMåœ¨åˆ†è§£æ•°å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨è¿›è¡Œç²¾ç»†æ­¥éª¤çš„æ¨ç†æ—¶ç»å¸¸ä¼šå‡ºç°é”™è¯¯ã€‚æ­¤å¤–ï¼Œåœ¨é—®é¢˜å±‚é¢æ£€ç´¢çš„ICLç¤ºä¾‹å¯èƒ½ä¼šé—æ¼å…³é”®æ­¥éª¤ï¼Œæˆ–è€…ç”šè‡³ç”¨ä¸ç›¸å…³çš„ç»†èŠ‚è¯¯å¯¼æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BoostStepæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ­¥éª¤å¯¹é½çš„ICLå¢å¼ºæ¨ç†å‡†ç¡®æ€§ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æœºåˆ¶ï¼Œå¯ä»¥ä»”ç»†å¯¹é½æ£€ç´¢åˆ°çš„å‚è€ƒæ­¥éª¤ä¸ç›¸åº”çš„æ¨ç†æ­¥éª¤ã€‚æ­¤å¤–ï¼ŒBoostStepè¿˜èå…¥äº†ä¸€ç§æœ‰æ•ˆçš„â€œé¦–æ¬¡å°è¯•â€ç­–ç•¥ï¼Œä»¥æä¾›ä¸å½“å‰æ¨ç†çŠ¶æ€é«˜åº¦ç›¸å…³çš„èŒƒä¾‹ã€‚BoostStepæ˜¯ä¸€ç§çµæ´»ä¸”å¼ºå¤§çš„æ–¹æ³•ï¼Œå¯ä»¥ä¸æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ ‘æœç´¢ç®—æ³•æ— ç¼é›†æˆï¼Œæ”¹è¿›å€™é€‰é€‰æ‹©å’Œå†³ç­–åˆ¶å®šã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒBoostStepæé«˜äº†GPT-4oåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„æ€ç»´é“¾æ€§èƒ½4.6%ï¼Œæ˜¾è‘—è¶…è¿‡äº†ä¼ ç»Ÿå°‘æ ·æœ¬å­¦ä¹ çš„1.2%ã€‚è€Œä¸”ï¼Œå½“å®ƒä¸æ ‘æœç´¢ç®—æ³•ç»“åˆæ—¶ï¼Œå¯ä»¥å®ç°é¢å¤–çš„7.5%çš„å¢ç›Šã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡æ›´ç®€å•çš„ç¤ºä¾‹å¢å¼ºæœ€å…ˆè¿›çš„LLMè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ã€‚å®ƒåˆ©ç”¨MATHæ•°æ®é›†ä¸­çš„ç®€å•ç¤ºä¾‹ï¼Œæé«˜äº†DeepSeek-R1-671Båœ¨AIMEä¸Šçš„æ€§èƒ½2.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03226v3">PDF</a> Codes and Data are available at   <a target="_blank" rel="noopener" href="https://github.com/beichenzbc/BoostStep">https://github.com/beichenzbc/BoostStep</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„å¤æ‚æ•°å­¦é—®é¢˜æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç²¾å¿ƒè®¾è®¡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç¤ºä¾‹æ¥è¿›ä¸€æ­¥å¢å¼ºã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸¤ä¸ªä¸»è¦çš„æŒ‘æˆ˜ï¼šç²’åº¦ä¸åŒ¹é…å’Œæ— å…³ä¿¡æ¯ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°LLMsæ“…é•¿åˆ†è§£æ•°å­¦é—®é¢˜ï¼Œä½†åœ¨ç²¾ç»†æ­¥éª¤ä¸­çš„æ¨ç†é”™è¯¯æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œåœ¨é—®é¢˜çº§åˆ«æ£€ç´¢çš„ICLç¤ºä¾‹å¯èƒ½ä¼šé—æ¼å…³é”®æ­¥éª¤æˆ–ç”šè‡³é€šè¿‡ä¸ç›¸å…³çš„ç»†èŠ‚è¯¯å¯¼æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BoostStepæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ­¥éª¤å¯¹é½çš„ICLå’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚BoostStepä¸æ€ç»´é“¾å’Œæ ‘æœç´¢ç®—æ³•æ— ç¼é›†æˆï¼Œæ”¹è¿›äº†å€™é€‰é€‰æ‹©å’Œå†³ç­–åˆ¶å®šã€‚BoostStepæ”¹è¿›äº†GPT-4oåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„æ€ç»´é“¾æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿçš„å°‘æ ·æœ¬å­¦ä¹ ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®ƒè¿˜å¯ä»¥ä¸æ ‘æœç´¢ç›¸ç»“åˆå®ç°é¢å¤–å¢ç›Šï¼Œå¹¶ä½¿ç”¨æ›´ç®€å•çš„ç¤ºä¾‹æé«˜æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³æŒ‘æˆ˜æ€§æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥å¾ˆå¥½åœ°è§£å†³å¤æ‚æ•°å­¦é—®é¢˜ï¼Œä½†éœ€è¦å¢å¼ºå…¶åœ¨ç²¾ç»†æ­¥éª¤ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯å¢å¼ºLLMsè§£å†³æ•°å­¦é—®é¢˜èƒ½åŠ›çš„ä¸€ç§æ–¹æ³•ï¼Œä½†å­˜åœ¨ç²’åº¦ä¸åŒ¹é…å’Œæ— å…³ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>BoostStepæ–¹æ³•é€šè¿‡æ­¥éª¤å¯¹é½çš„ICLå’Œâ€œé¦–æ¬¡å°è¯•â€ç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>BoostStepä¸æ€ç»´é“¾å’Œæ ‘æœç´¢ç®—æ³•ç›¸ç»“åˆï¼Œæ”¹è¿›äº†å€™é€‰é€‰æ‹©å’Œå†³ç­–åˆ¶å®šã€‚</li>
<li>BoostStepæé«˜äº†GPT-4oåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿå°‘æ ·æœ¬å­¦ä¹ ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>ç»“åˆæ ‘æœç´¢ï¼ŒBoostStepå¯ä»¥å®ç°é¢å¤–çš„æ€§èƒ½å¢ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b56df2416f68dd938177772e7622910a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56fe49c423ad409984226c77c446721c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cb0a9d1c255592bc38c9b2f1ce8594d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Graph-based-Retrieval-Augmented-Generation-for-Dynamic-Few-shot-Text-Classification"><a href="#Graph-based-Retrieval-Augmented-Generation-for-Dynamic-Few-shot-Text-Classification" class="headerlink" title="Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text   Classification"></a>Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text   Classification</h2><p><strong>Authors:Yubo Wang, Haoyang Li, Fei Teng, Lei Chen</strong></p>
<p>Text classification is a fundamental task in data mining, pivotal to various applications such as tabular understanding and recommendation. Although neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and new target labels frequently appear based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding ability. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to classify texts. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. Rather than treating each input independently, GORAG constructs and maintains a weighted graph by extracting side information across all target texts. In this graph, text keywords and labels are represented as nodes, with edges indicating the correlations between them. To model these correlations, GORAG employs an edge weighting mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and precise contextual information. </p>
<blockquote>
<p>æ–‡æœ¬åˆ†ç±»æ˜¯æ•°æ®æŒ–æ˜ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå¯¹äºè¡¨æ ¼ç†è§£å’Œæ¨èç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡åŸºäºç¥ç»ç½‘ç»œæ¨¡å‹çš„CNNå’ŒBERTç­‰åœ¨æ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„æ•ˆåŠ›ä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ã€‚è¿™ç§ä¾èµ–ä½¿å¾—è¿™äº›æ¨¡å‹åœ¨åŠ¨æ€å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä¸­çš„æ•ˆæœè¾ƒå·®ï¼Œè¿™é‡Œçš„æ ‡è®°æ•°æ®ç¨€ç¼ºï¼Œå¹¶ä¸”æ ¹æ®åº”ç”¨éœ€æ±‚é¢‘ç¹å‡ºç°æ–°çš„ç›®æ ‡æ ‡ç­¾ã€‚æœ€è¿‘ï¼Œç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒå’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚å½“å‰çš„æ–¹æ³•ä¸ºLLMæä¾›æ–‡æœ¬è¾“å…¥ã€å€™é€‰æ ‡ç­¾å’Œé¢å¤–çš„ä¾§é¢ä¿¡æ¯ï¼ˆä¾‹å¦‚æè¿°ï¼‰æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ•ˆåŠ›å—åˆ°è¾“å…¥å¤§å°å¢åŠ å’Œé€šè¿‡ä¾§é¢ä¿¡æ¯å¤„ç†å¼•å…¥çš„å™ªå£°çš„é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›¾çš„åœ¨çº¿æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œå³GORAGï¼Œç”¨äºåŠ¨æ€å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»ã€‚GORAGä¸æ˜¯ç‹¬ç«‹å¤„ç†æ¯ä¸ªè¾“å…¥ï¼Œè€Œæ˜¯é€šè¿‡å»ºç«‹å’Œç»´æŠ¤ä¸€ä¸ªåŠ æƒå›¾æ¥æå–æ‰€æœ‰ç›®æ ‡æ–‡æœ¬çš„ä¾§é¢ä¿¡æ¯ã€‚åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œæ–‡æœ¬å…³é”®è¯å’Œæ ‡ç­¾è¢«è¡¨ç¤ºä¸ºèŠ‚ç‚¹ï¼Œè¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„å…³è”ã€‚ä¸ºäº†å»ºæ¨¡è¿™äº›å…³è”ï¼ŒGORAGé‡‡ç”¨è¾¹åŠ æƒæœºåˆ¶æ¥ä¼˜å…ˆå¤„ç†æå–ä¿¡æ¯çš„é‡è¦æ€§å’Œå¯é æ€§ï¼Œå¹¶åŠ¨æ€ä½¿ç”¨é’ˆå¯¹æ¯ä¸ªæ–‡æœ¬è¾“å…¥å®šåˆ¶çš„æœ€å°æˆæœ¬ç”Ÿæˆæ ‘æ¥æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGORAGé€šè¿‡æä¾›æ›´å…¨é¢å’Œç²¾ç¡®çš„ä¸Šæ–‡ä¿¡æ¯ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02844v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ†ç±»æ˜¯æ•°æ®æŒ–æ˜ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå¯¹è¡¨æ ¼ç†è§£å’Œæ¨èç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡ç¥ç»ç½‘ç»œæ¨¡å‹å¦‚CNNå’ŒBERTåœ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºå¤§é‡æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œè¿™åœ¨åŠ¨æ€å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä¸­æ•ˆæœä¸ä½³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åœ¨é¢„è®­ç»ƒå’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢çš„èƒ½åŠ›è€Œå±•ç°å‡ºæ½œåŠ›ã€‚é’ˆå¯¹LLMsåœ¨å¤„ç†æ–‡æœ¬è¾“å…¥ã€å€™é€‰æ ‡ç­¾å’Œé¢å¤–ä¾§é¢ä¿¡æ¯æ—¶çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›¾çš„åœ¨çº¿æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶GORAGã€‚å®ƒé€šè¿‡æ„å»ºå’Œç»´æŠ¤ä¸€ä¸ªåŠ æƒå›¾æ¥æå–æ‰€æœ‰ç›®æ ‡æ–‡æœ¬çš„ä¾§é¢ä¿¡æ¯ï¼Œå…¶ä¸­æ–‡æœ¬å…³é”®è¯å’Œæ ‡ç­¾è¢«è¡¨ç¤ºä¸ºèŠ‚ç‚¹ï¼Œè¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚GORAGé‡‡ç”¨è¾¹åŠ æƒæœºåˆ¶æ¥ä¼˜å…ˆå¤„ç†æå–ä¿¡æ¯çš„é‡è¦æ€§å’Œå¯é æ€§ï¼Œå¹¶ä¸ºæ¯ä¸ªæ–‡æœ¬è¾“å…¥é‡èº«å®šåˆ¶æœ€å°æˆæœ¬ç”Ÿæˆæ ‘è¿›è¡ŒåŠ¨æ€æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒGORAGåœ¨æä¾›æ›´å…¨é¢å’Œç²¾ç¡®ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ†ç±»æ˜¯æ•°æ®æŒ–æ˜çš„æ ¸å¿ƒä»»åŠ¡ï¼Œå¯¹å¤šç§åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸¥é‡ä¾èµ–äºå¤§é‡æ ‡è®°æ•°æ®ï¼Œè¿™åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ï¼Œå› ä¸ºå®ƒä»¬å…·å¤‡å¹¿æ³›çš„é¢„è®­ç»ƒå’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</li>
<li>å½“å‰LLMæ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬è¾“å…¥ã€å€™é€‰æ ‡ç­¾å’Œä¾§é¢ä¿¡æ¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>GORAGæ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒåŸºäºå›¾æ„å»ºå¹¶ç»´æŠ¤ä¸€ä¸ªåŠ æƒå›¾æ¥æå–å’Œè¡¨ç¤ºæ–‡æœ¬å…³é”®è¯å’Œæ ‡ç­¾çš„ç›¸å…³æ€§ã€‚</li>
<li>GORAGé€šè¿‡è¾¹åŠ æƒæœºåˆ¶ä¼˜å…ˆå¤„ç†é‡è¦å’Œå¯é çš„ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æœ€å°æˆæœ¬ç”Ÿæˆæ ‘è¿›è¡ŒåŠ¨æ€ä¸Šä¸‹æ–‡æ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fa594aa2e5867d4377841bfb349f97e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68785c6c675ccab2c0d002bbba9e3ebd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a92d48c5af68cb75b33ba71a832b089d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22215fd30e021e3053b4bd68ec3ee790.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00a1699a7297716cc963699dc961a77d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-Input-Attributions-Interpret-the-Inductive-Reasoning-Process-Elicited-in-In-Context-Learning"><a href="#Can-Input-Attributions-Interpret-the-Inductive-Reasoning-Process-Elicited-in-In-Context-Learning" class="headerlink" title="Can Input Attributions Interpret the Inductive Reasoning Process   Elicited in In-Context Learning?"></a>Can Input Attributions Interpret the Inductive Reasoning Process   Elicited in In-Context Learning?</h2><p><strong>Authors:Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki</strong></p>
<p>Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying&#x2F;solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests in linguistics; here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates the task demonstrated. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæ¨¡å‹çš„å†…éƒ¨è¿‡ç¨‹è§£é‡Šé•¿ä¹…ä»¥æ¥ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ—¶ä»£ä»ç„¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¾‹å¦‚ï¼ŒICLå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå³è§£é‡Šå“ªäº›å°‘æ•°ä¾‹å­å¯¹è¯†åˆ«æˆ–è§£å†³ä»»åŠ¡äº§ç”Ÿäº†è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è®¾è®¡äº†åŸºäºå½’çº³æ¨ç†çš„åˆæˆè¯Šæ–­ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å—åˆ°è¯­è¨€å­¦ä¸­æ³›åŒ–æµ‹è¯•çš„å¯å‘ï¼›åœ¨è¿™é‡Œï¼Œå¤§å¤šæ•°ä¸Šä¸‹æ–‡ä¾‹å­å¯¹äºå…¶æ½œåœ¨è§„åˆ™æ˜¯æ¨¡ç³Šçš„ï¼Œåªæœ‰ä¸€ä¸ªå…³é”®ä¾‹å­æ˜ç¡®äº†ä»»åŠ¡æ¼”ç¤ºã€‚é—®é¢˜æ˜¯ä¼ ç»Ÿçš„è¾“å…¥å½’å› ï¼ˆIAï¼‰æ–¹æ³•æ˜¯å¦èƒ½å¤Ÿè¿½è¸ªè¿™æ ·çš„æ¨ç†è¿‡ç¨‹ï¼Œå³åœ¨ICLä¸­è¯†åˆ«å…³é”®ä¾‹å­ã€‚æˆ‘ä»¬çš„å®éªŒæä¾›äº†ä¸€äº›å®é™…å‘ç°ï¼›ä¾‹å¦‚ï¼ŒæŸç§ç®€å•çš„IAæ–¹æ³•æ•ˆæœæœ€ä½³ï¼Œå¹¶ä¸”æ¨¡å‹è¶Šå¤§ï¼ŒåŸºäºæ¢¯åº¦çš„IAæ–¹æ³•è§£é‡ŠICLçš„éš¾åº¦é€šå¸¸è¶Šå¤§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15628v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡è®¾è®¡äº†ä¸€ç³»åˆ—åŸºäºå½’çº³æ¨ç†çš„åˆæˆè¯Šæ–­ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­å¢ƒå­¦ä¹ ä¸­çš„è§£é‡Šéš¾é¢˜ã€‚åœ¨å¤šæ•°æƒ…å†µä¸‹ï¼Œä¸Šä¸‹æ–‡å®ä¾‹åœ¨éšå«è§„åˆ™ä¸Šå­˜åœ¨æ­§ä¹‰ï¼Œä¸€ä¸ªå…³é”®å®ä¾‹å¯ä»¥æ¶ˆé™¤ä»»åŠ¡ä¸­çš„æ­§ä¹‰ã€‚è®ºæ–‡æ¢è®¨äº†ä¼ ç»Ÿè¾“å…¥å½’å› æ–¹æ³•æ˜¯å¦èƒ½è¿½è¸ªè¿™ç§æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å‘ç°ä¸€ç§ç®€å•çš„è¾“å…¥å½’å› æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œæ¨¡å‹è¶Šå¤§ï¼ŒåŸºäºæ¢¯åº¦çš„è¾“å…¥å½’å› æ–¹æ³•è¶Šéš¾è§£è¯»è¯­å¢ƒå­¦ä¹ çš„æƒ…å†µã€‚è®ºæ–‡é‡ç‚¹åœ¨äºè¯­å¢ƒå­¦ä¹ å†…éƒ¨æœºåˆ¶çš„è§£æå’Œç†è§£ã€‚è¯¥å·¥ä½œå°†æ·±åº¦åˆ†æä¸ç®€å•çš„å®éªŒè¯æ®ç›¸ç»“åˆï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹å†…éƒ¨çš„å¤æ‚æ€§å’Œéš¾ä»¥æ‰æ‘¸æ€§ã€‚æ€»ç»“èµ·æ¥ï¼Œè¿™æ˜¯ä¸€é¡¹é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­å¢ƒå­¦ä¹ çš„æ·±å…¥è§£æç ”ç©¶ã€‚é€šè¿‡ä¸€ç³»åˆ—åˆæˆè¯Šæ–­ä»»åŠ¡æ­ç¤ºäº†æ¨¡å‹åœ¨é¢å¯¹è¯­å¢ƒå­¦ä¹ çš„å¤æ‚æ€§æ–¹é¢çš„è¡Œä¸ºå’Œé™åˆ¶ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡ä¸€ä¸ªç®€å•çš„è¾“å…¥å½’å› æ–¹æ³•å±•ç°äº†ä¸€ä¸ªå…³é”®å®ä¾‹åœ¨è§£å†³æ­§ä¹‰ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°äº†æ¨¡å‹è§„æ¨¡ä¸è§£é‡Šéš¾åº¦ä¹‹é—´çš„å…³ç³»ã€‚å°½ç®¡å­˜åœ¨æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶ä¸ºæœªæ¥çš„è§£é‡Šå·¥ä½œæä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œç ”ç©¶æ–¹å‘ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>è®¾è®¡åˆæˆè¯Šæ–­ä»»åŠ¡ä»¥ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­å¢ƒå­¦ä¹ ä¸­çš„è§£é‡Šé—®é¢˜ã€‚</li>
<li>ä¸Šä¸‹æ–‡å®ä¾‹åœ¨éšå«è§„åˆ™ä¸Šé€šå¸¸å­˜åœ¨æ­§ä¹‰ï¼Œä¸€ä¸ªå…³é”®å®ä¾‹èƒ½å¤Ÿæ¶ˆé™¤è¿™ç§æ­§ä¹‰ã€‚</li>
<li>ç®€å•è¾“å…¥å½’å› æ–¹æ³•åœ¨è¿½è¸ªæ¨ç†è¿‡ç¨‹ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦çš„è¾“å…¥å½’å› æ–¹æ³•æ¥è§£é‡Šè¯­å¢ƒå­¦ä¹ è¶Šå›°éš¾ã€‚</li>
<li>å…³é”®å®ä¾‹åœ¨è§£å†³ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨å¾—åˆ°äº†å®è¯ç ”ç©¶çš„æ”¯æŒã€‚</li>
<li>è®¾è®¡äº†ç”¨äºæ¢ç©¶æ¨¡å‹åœ¨è¯­å¢ƒå­¦ä¹ è¿‡ç¨‹ä¸­çš„å¤æ‚æ€§çš„æ–¹æ³•ï¼Œè¿™å¯¹äºè§£é‡Šæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b95ae8f143669516287d70afe466aa5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28de4e1dc6a9f27d74eb6d76286ba1cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d756c54cd9a3efe2b1c97aa6049ff5b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Step-Guided-Reasoning-Improving-Mathematical-Reasoning-using-Guidance-Generation-and-Step-Reasoning"><a href="#Step-Guided-Reasoning-Improving-Mathematical-Reasoning-using-Guidance-Generation-and-Step-Reasoning" class="headerlink" title="Step Guided Reasoning: Improving Mathematical Reasoning using Guidance   Generation and Step Reasoning"></a>Step Guided Reasoning: Improving Mathematical Reasoning using Guidance   Generation and Step Reasoning</h2><p><strong>Authors:Lang Cao, Chao Peng, Renhong Chen, Wu Ning, Yingtian Zou, Yitong Li</strong></p>
<p>Mathematical reasoning has been challenging for large language models (LLMs). However, the introduction of step-by-step Chain-of-Thought (CoT) inference has significantly advanced the mathematical capabilities of LLMs. Despite this progress, current approaches either necessitate extensive inference datasets for training or depend on few-shot methods that frequently compromise computational accuracy. To address these bottlenecks in mathematical reasoning, we propose a novel method called Step Guidied Reasoning, which is more stable and generalizable than few-shot methods and does not involve further fine-tuning of the model. In this approach, LLMs reflect on small reasoning steps, similar to how humans deliberate and focus attention on what to do next. By incorporating this reflective process into the inference stage, LLMs can effectively guide their reasoning from one step to the next. Through extensive experiments, we demonstrate the significant effect of Step Guidied Reasoning in augmenting mathematical performance in state-of-the-art language models. Qwen2-72B-Instruct outperforms its math-specific counterpart, Qwen2.5-72B-Math-Instruct, on MMLU- STEM with a score of 90.9%, compared to 87.3%. The average scores of Qwen2-7B-Instruct and Qwen2-72B-Instruct increase from 27.1% to 36.3% and from 36.5% to 47.4% on the mathematics domain, respectively. </p>
<blockquote>
<p>åœ¨æ•°å­¦æ¨ç†æ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸€ç›´é¢ä¸´æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œé€šè¿‡å¼•å…¥é€æ­¥çš„â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æ¨ç†ï¼ŒLLMçš„æ•°å­¦èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å°½ç®¡æœ‰äº†è¿™äº›è¿›å±•ï¼Œå½“å‰çš„æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡æ¨ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¦ä¹ˆä¾èµ–äºå°‘æ ·æœ¬æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ç»å¸¸ç‰ºç‰²è®¡ç®—å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³æ•°å­¦æ¨ç†ä¸­çš„è¿™äº›ç“¶é¢ˆé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºâ€œæ­¥éª¤å¼•å¯¼æ¨ç†â€ï¼Œè¯¥æ–¹æ³•æ¯”å°‘æ ·æœ¬æ–¹æ³•æ›´ç¨³å®šã€æ›´å…·æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”ä¸æ¶‰åŠæ¨¡å‹çš„è¿›ä¸€æ­¥å¾®è°ƒã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼ŒLLMä¼šåæ€å¾®å°çš„æ¨ç†æ­¥éª¤ï¼Œç±»ä¼¼äºäººç±»æ€è€ƒå¹¶é›†ä¸­æ³¨æ„åŠ›äºä¸‹ä¸€æ­¥è¦åšä»€ä¹ˆã€‚é€šè¿‡å°†è¿™ç§åæ€è¿‡ç¨‹çº³å…¥æ¨ç†é˜¶æ®µï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼å…¶æ¨ç†é€æ­¥è¿›è¡Œã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†â€œæ­¥éª¤å¼•å¯¼æ¨ç†â€åœ¨å¢å¼ºå…ˆè¿›è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—å½±å“ã€‚Qwen2-72B-Instructåœ¨MMLU-STEMä¸Šçš„è¡¨ç°ä¼˜äºå…¶æ•°å­¦ä¸“ç”¨å¯¹åº”æ¨¡å‹Qwen2.5-72B-Math-Instructï¼Œå¾—åˆ†ä¸º90.9%ï¼Œè€ŒQwen2.5-72B-Math-Instructçš„å¾—åˆ†ä¸º87.3%ã€‚Qwen2-7B-Instructå’ŒQwen2-72B-Instructçš„å¹³å‡åˆ†æ•°åœ¨æ•°å­¦é¢†åŸŸåˆ†åˆ«ä»27.1%æé«˜åˆ°36.3%å’Œä»36.5%æé«˜åˆ°47.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19817v2">PDF</a> 8 pages, 4 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¼•å…¥åˆ†æ­¥æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ¨ç†æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨éœ€è¦å¤§é‡æ¨ç†æ•°æ®é›†æˆ–ç‰ºç‰²è®¡ç®—ç²¾åº¦çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ­¥éª¤å¼•å¯¼æ¨ç†ï¼ˆStep Guidied Reasoningï¼‰ï¼Œå®ƒæ¯”å°‘æ ·æœ¬æ–¹æ³•æ›´ç¨³å®šã€æ›´å…·æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€å¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥çš„å¾®è°ƒã€‚è¯¥æ–¹æ³•ä½¿LLMsèƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ€è€ƒæ¨ç†çš„æ¯ä¸€æ­¥ï¼Œä»è€Œæœ‰æ•ˆåœ°å¼•å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼•å…¥åˆ†æ­¥æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨éœ€è¦å¤§é‡æ¨ç†æ•°æ®é›†æˆ–ç‰ºç‰²è®¡ç®—ç²¾åº¦çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ­¥éª¤å¼•å¯¼æ¨ç†ï¼ˆStep Guidied Reasoningï¼‰ï¼Œå®ƒæ›´ç¨³å®šã€æ›´å…·æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€è¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>æ­¥éª¤å¼•å¯¼æ¨ç†ä½¿LLMsèƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ€è€ƒæ¨ç†çš„æ¯ä¸€æ­¥ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¼•å¯¼æ¨ç†æ­¥éª¤ï¼Œæœ‰æ•ˆåœ°æé«˜äº†è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>Qwen2-72B-Instructæ¨¡å‹åœ¨æ•°å­¦æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶æ•°å­¦ä¸“ç”¨å¯¹åº”æ¨¡å‹Qwen2.5-72B-Math-Instructï¼Œåœ¨MMLU-STEMä¸Šçš„å¾—åˆ†ä¸º90.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2a6bef516f92e8fdd6b267a6bcb7c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2cd585338cc8bcdf990da58adc38ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758c95d74527737aaf4ebb802444abde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00db42fbc360ed97b5ad5e05e167ab39.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StateAct-State-Tracking-and-Reasoning-for-Acting-and-Planning-with-Large-Language-Models"><a href="#StateAct-State-Tracking-and-Reasoning-for-Acting-and-Planning-with-Large-Language-Models" class="headerlink" title="StateAct: State Tracking and Reasoning for Acting and Planning with   Large Language Models"></a>StateAct: State Tracking and Reasoning for Acting and Planning with   Large Language Models</h2><p><strong>Authors:Nikolai Rozanov, Marek Rei</strong></p>
<p>Planning and acting to solve <code>real&#39; tasks using large language models (LLMs) in interactive environments has become a new frontier for AI methods. While recent advances allowed LLMs to interact with online tools, solve robotics tasks and many more, long range reasoning tasks remain a problem for LLMs. Existing methods to address this issue are very resource intensive and require additional data or human crafted rules, instead, we propose a simple method based on few-shot in-context learning alone to enhance </code>chain-of-thoughtâ€™ with state-tracking for planning and acting with LLMs. We show that our method establishes the new state-of-the-art on Alfworld for in-context learning methods (+14% over the previous best few-shot in-context learning method) and performs on par with methods that use additional training data and additional tools such as code-execution. We also demonstrate that our enhanced <code>chain-of-states&#39; allows the agent to both solve longer horizon problems and to be more efficient in number of steps required to solve a task. We show that our method works across a variety of LLMs for both API-based and open source ones. Finally, we also conduct ablation studies and show that </code>chain-of-thoughtsâ€™ helps state-tracking accuracy, while a json-structure harms overall performance. We open-source our code and annotations at <a target="_blank" rel="noopener" href="https://github.com/ai-nikolai/StateAct">https://github.com/ai-nikolai/StateAct</a>. </p>
<blockquote>
<p>åœ¨äº¤äº’å¼ç¯å¢ƒä¸­åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³â€œçœŸå®â€ä»»åŠ¡çš„è®¾è®¡å’Œè¡ŒåŠ¨å·²æˆä¸ºäººå·¥æ™ºèƒ½æ–¹æ³•çš„æ–°å‰æ²¿ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•ä½¿å¾—LLMèƒ½å¤Ÿåœ¨çº¿å·¥å…·äº¤äº’ã€å®Œæˆæœºå™¨äººä»»åŠ¡ç­‰ç­‰ï¼Œä½†é•¿è¿œæ¨ç†ä»»åŠ¡ä»ç„¶æ˜¯LLMçš„ä¸€ä¸ªéš¾é¢˜ã€‚ç°æœ‰è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•éå¸¸è€—è´¹èµ„æºï¼Œéœ€è¦é¢å¤–æ•°æ®æˆ–äººä¸ºåˆ¶å®šçš„è§„åˆ™ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„ç®€å•æ–¹æ³•ï¼Œé€šè¿‡çŠ¶æ€è·Ÿè¸ªå¢å¼ºLLMçš„è®¡åˆ’ä¸è¡ŒåŠ¨èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Alfworldä¸Šå»ºç«‹äº†æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•çš„æœ€æ–°çŠ¶æ€ï¼ˆè¶…è¿‡ä¹‹å‰çš„æœ€ä½³å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•æé«˜14%ï¼‰ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ•°æ®çš„ä½¿ç”¨ä¸Šä¸é™„åŠ è®­ç»ƒæ•°æ®å’Œå·¥å…·å¦‚ä»£ç æ‰§è¡Œçš„æ–¹æ³•ä¸ç›¸ä¸Šä¸‹ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œæˆ‘ä»¬å¢å¼ºçš„â€œçŠ¶æ€é“¾â€å…è®¸ä»£ç†è§£å†³é•¿æœŸé—®é¢˜ï¼Œå¹¶ä¸”åœ¨è§£å†³ä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤ä¸Šæ›´åŠ é«˜æ•ˆã€‚æˆ‘ä»¬å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå„ç§LLMï¼Œæ— è®ºæ˜¯åŸºäºAPIçš„è¿˜æ˜¯å¼€æºçš„ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†æ¶ˆèç ”ç©¶å¹¶è¯æ˜ï¼Œâ€œæ€ç»´é“¾â€æœ‰åŠ©äºæé«˜çŠ¶æ€è·Ÿè¸ªçš„å‡†ç¡®æ€§ï¼Œè€Œjsonç»“æ„åˆ™å¯èƒ½æŸå®³æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai-nikolai/StateAct%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%B3%A8%E9%87%8A%E3%80%82">https://github.com/ai-nikolai/StateActä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç å’Œæ³¨é‡Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02810v2">PDF</a> 9 pages, 5 pages appendix, 7 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’å¼ç¯å¢ƒâ€œé“¾å¼æ€ç»´â€çŠ¶æ€è·Ÿè¸ªæ–¹æ³•åœ¨è§£å†³é•¿æœŸæ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºä¼˜è¶Šæ€§ã€‚é€šè¿‡å°‘é‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å­¦ä¹ ï¼Œæå‡äº†åœ¨Alfworldä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ•ˆç‡æ›´é«˜ä¸”èƒ½è§£å†³æ›´é•¿è¿œçš„ä»»åŠ¡ã€‚æ­¤æ–¹æ³•é€‚ç”¨äºå¤šç§LLMï¼ŒåŒ…æ‹¬APIå’Œå¼€æºç‰ˆæœ¬ã€‚åŒæ—¶ï¼Œå…¬å¼€äº†ç›¸å…³ä»£ç å’Œæ³¨è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³é•¿æœŸæ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå°‘é‡ä¸Šä¸‹æ–‡ä¿¡æ¯å­¦ä¹ çš„ç®€å•æ–¹æ³•ï¼Œå¢å¼ºâ€œé“¾å¼æ€ç»´â€ä¸çŠ¶æ€è·Ÿè¸ªã€‚</li>
<li>åœ¨Alfworldä»»åŠ¡ä¸Šå®ç°æœ€æ–°è¡¨ç°ï¼Œä¼˜äºä¹‹å‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä¸ä½¿ç”¨é¢å¤–è®­ç»ƒæ•°æ®å’Œå·¥å…·çš„æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚</li>
<li>å¢å¼ºäº†è§£å†³é•¿æœŸé—®é¢˜å’Œä»»åŠ¡æ•ˆç‡çš„èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•é€‚ç”¨äºå¤šç§LLMï¼ŒåŒ…æ‹¬APIå’Œå¼€æºç‰ˆæœ¬ã€‚</li>
<li>å…¬å¼€äº†ç›¸å…³ä»£ç å’Œæ³¨è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-862e1fc1eeb16d01d6b0f3b40cdb87da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d42edae8e3636f3814c78ca499c4751.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-534c26fbe33f7d645620e90576e3ce28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3edf2e9b3164cef1eed429d27c874f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad048005308e3ae861ee254c2fe38b52.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AER-LLM-Ambiguity-aware-Emotion-Recognition-Leveraging-Large-Language-Models"><a href="#AER-LLM-Ambiguity-aware-Emotion-Recognition-Leveraging-Large-Language-Models" class="headerlink" title="AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language   Models"></a>AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language   Models</h2><p><strong>Authors:Xin Hong, Yuan Gong, Vidhyasaharan Sethu, Ting Dang</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have demonstrated great success in many Natural Language Processing (NLP) tasks. In addition to their cognitive intelligence, exploring their capabilities in emotional intelligence is also crucial, as it enables more natural and empathetic conversational AI. Recent studies have shown LLMsâ€™ capability in recognizing emotions, but they often focus on single emotion labels and overlook the complex and ambiguous nature of human emotions. This study is the first to address this gap by exploring the potential of LLMs in recognizing ambiguous emotions, leveraging their strong generalization capabilities and in-context learning. We design zero-shot and few-shot prompting and incorporate past dialogue as context information for ambiguous emotion recognition. Experiments conducted using three datasets indicate significant potential for LLMs in recognizing ambiguous emotions, and highlight the substantial benefits of including context information. Furthermore, our findings indicate that LLMs demonstrate a high degree of effectiveness in recognizing less ambiguous emotions and exhibit potential for identifying more ambiguous emotions, paralleling human perceptual capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è®¸å¤šä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚é™¤äº†è®¤çŸ¥æ™ºèƒ½ï¼Œæ¢ç´¢å…¶åœ¨æƒ…ç»ªæ™ºèƒ½æ–¹é¢çš„èƒ½åŠ›ä¹Ÿè‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™èƒ½ä½¿å¯¹è¯äººå·¥æ™ºèƒ½æ›´åŠ è‡ªç„¶å’Œå¯Œæœ‰åŒæƒ…å¿ƒã€‚è™½ç„¶æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜LLMæœ‰èƒ½åŠ›è¯†åˆ«æƒ…ç»ªï¼Œä½†å®ƒä»¬é€šå¸¸ä¸“æ³¨äºå•ä¸€çš„æƒ…ç»ªæ ‡ç­¾ï¼Œè€Œå¿½ç•¥äººç±»æƒ…ç»ªçš„å¤æ‚å’Œæ¨¡ç³Šæ€§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡é€šè¿‡æ¢ç´¢LLMåœ¨è¯†åˆ«æ¨¡ç³Šæƒ…ç»ªæ–¹é¢çš„æ½œåŠ›æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œåˆ©ç”¨å®ƒä»¬å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œå¹¶å°†è¿‡å»çš„å¯¹è¯ä½œä¸ºæ¨¡ç³Šæƒ…ç»ªè¯†åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜LLMåœ¨è¯†åˆ«æ¨¡ç³Šæƒ…ç»ªæ–¹é¢å­˜åœ¨å·¨å¤§æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å·¨å¤§å¥½å¤„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜å‘ç°ï¼ŒLLMåœ¨è¯†åˆ«è¾ƒä¸æ¨¡ç³Šçš„æƒ…ç»ªæ–¹é¢è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç°å‡ºè¯†åˆ«æ›´æ¨¡ç³Šæƒ…ç»ªçš„æ½œåŠ›ï¼Œä¸äººç±»æ„ŸçŸ¥èƒ½åŠ›ç›¸åª²ç¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18339v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æƒ…æ„Ÿæ™ºèƒ½æ–¹é¢çš„æ¢ç´¢æ„ˆå‘é‡è¦ï¼Œè¿‘æœŸç ”ç©¶å¼€å§‹å…³æ³¨å…¶åœ¨è¯†åˆ«æ¨¡ç³Šæƒ…ç»ªæ–¹é¢çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶åˆ©ç”¨LLMçš„å¼ºæ³›åŒ–èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œé€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œå¹¶ç»“åˆè¿‡å»çš„å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ¨¡ç³Šæƒ…æ„Ÿè¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜LLMåœ¨è¯†åˆ«æ¨¡ç³Šæƒ…æ„Ÿæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä¸”ä¸Šä¸‹æ–‡ä¿¡æ¯æä¸ºé‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æƒ…æ„Ÿæ™ºèƒ½æ–¹é¢çš„æ¢ç´¢æ„ˆå‘é‡è¦ã€‚</li>
<li>LLMå…·å¤‡è¯†åˆ«æ¨¡ç³Šæƒ…ç»ªçš„èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨LLMçš„å¼ºæ³›åŒ–èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è¿›è¡Œæ¨¡ç³Šæƒ…æ„Ÿè¯†åˆ«ã€‚</li>
<li>å®éªŒä½¿ç”¨ä¸‰ç§æ•°æ®é›†è¿›è¡Œï¼Œç»“æœæ˜¾è‘—ã€‚</li>
<li>ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹äºæ¨¡ç³Šæƒ…æ„Ÿè¯†åˆ«è‡³å…³é‡è¦ã€‚</li>
<li>LLMåœ¨è¯†åˆ«æ¨¡ç³Šæƒ…æ„Ÿæ–¹é¢è¡¨ç°å‡ºé«˜æ•ˆæœï¼Œä¸äººç±»æ„ŸçŸ¥èƒ½åŠ›ç›¸å¹³è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de02c7679b8c7917b6a3e09db747c7cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d27153e15386678ca8ada42e4dced58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dde846dd6fb556ebd959be664690ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cba5e70e0487fb793ab54f82f1f86e35.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Local-Prompt-Extensible-Local-Prompts-for-Few-Shot-Out-of-Distribution-Detection"><a href="#Local-Prompt-Extensible-Local-Prompts-for-Few-Shot-Out-of-Distribution-Detection" class="headerlink" title="Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution   Detection"></a>Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution   Detection</h2><p><strong>Authors:Fanhu Zeng, Zhen Cheng, Fei Zhu, Hongxin Wei, Xu-Yao Zhang</strong></p>
<p>Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods. Code is released at <a target="_blank" rel="noopener" href="https://github.com/AuroraZengfh/Local-Prompt">https://github.com/AuroraZengfh/Local-Prompt</a>. </p>
<blockquote>
<p>å¼‚å¸¸åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æ—¨åœ¨åŒºåˆ†å·²çŸ¥ç±»åˆ«ä¸­çš„å¼‚å¸¸å€¼ï¼Œåœ¨å®é™…åœºæ™¯ä¸­å·²ç»å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å‡ºç°å¢åŠ äº†é€šè¿‡å°‘é‡æ ·æœ¬è°ƒæ•´å¢å¼ºVLMçš„OODæ£€æµ‹çš„å…´è¶£ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–å…¨å±€æç¤ºä¸Šï¼Œå¿½ç•¥äº†ä½¿ç”¨å±€éƒ¨ä¿¡æ¯æ¥è¯†åˆ«å¼‚å¸¸å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å†»ç»“å…¨å±€æç¤ºå¹¶å¼•å…¥Local-Promptï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä»ç²—åˆ°ç»†çš„è°ƒæ•´èŒƒå¼ï¼Œé€šè¿‡å±€éƒ¨æç¤ºæ¥å¼ºè°ƒåŒºåŸŸå¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼šå…¨å±€æç¤ºå¼•å¯¼è´Ÿå¢å¼ºå’Œå±€éƒ¨æç¤ºå¢å¼ºåŒºåŸŸæ­£åˆ™åŒ–ã€‚å‰è€…åˆ©ç”¨å†»ç»“çš„ç²—ç³™å…¨å±€æç¤ºä½œä¸ºå¼•å¯¼çº¿ç´¢æ¥èå…¥è´Ÿå¢å¼ºï¼Œä»è€Œåˆ©ç”¨å±€éƒ¨å¼‚å¸¸å€¼çŸ¥è¯†ã€‚åè€…åˆ™é‡‡ç”¨å¯è®­ç»ƒå±€éƒ¨æç¤ºå’ŒåŒºåŸŸæ­£åˆ™åŒ–æ¥æœ‰æ•ˆåœ°æ•è·å±€éƒ¨ä¿¡æ¯ï¼Œæœ‰åŠ©äºè¯†åˆ«å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸åŒºåŸŸç›¸å…³çš„æŒ‡æ ‡æ¥å¢å¼ºOODæ£€æµ‹çš„ä¸°å¯Œæ€§ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ–¹æ³•åªä¸“æ³¨äºå¢å¼ºå±€éƒ¨æç¤ºï¼Œå› æ­¤å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— ç¼é›†æˆå·²è®­ç»ƒçš„å…¨å±€æç¤ºä»¥æé«˜æ€§èƒ½ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ImageNet-1kæ•°æ®é›†ä¸Šè¿›è¡Œ4æ¬¡æ ·æœ¬è°ƒæ•´æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¹³å‡FPR95é™ä½äº†5.17%ï¼Œè¶…è¿‡äº†æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä¹‹å‰æ–¹æ³•çš„16æ¬¡æ ·æœ¬è°ƒæ•´ç»“æœã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/AuroraZengfh/Local-Prompt%E3%80%82">https://github.com/AuroraZengfh/Local-Promptã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04796v2">PDF</a> Accepted by The Thirteenth International Conference on Learning   Representations (ICLR 2025). Code is available at   <a target="_blank" rel="noopener" href="https://github.com/AuroraZengfh/Local-Prompt">https://github.com/AuroraZengfh/Local-Prompt</a></p>
<p><strong>Summary</strong><br>åŸºäºå±€éƒ¨æç¤ºå’Œå…¨å±€æç¤ºæŒ‡å¯¼çš„è´Ÿå¢å¼ºä¸åŒºåŸŸæ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ç²—åˆ°ç»†çš„å¾®è°ƒèŒƒå¼ï¼Œæ—¨åœ¨å¼ºåŒ–å±€éƒ¨ä¿¡æ¯åœ¨å¼‚å¸¸å€¼æ£€æµ‹ä¸­çš„ä½œç”¨ï¼Œæé«˜è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬ä¸‹çš„OODæ£€æµ‹æ€§èƒ½ã€‚æ­¤æ–¹æ³•å¯æœ‰æ•ˆé™ä½åœ¨ImageNet-1kæ•°æ®é›†ä¸Šçš„å¹³å‡FPR95å€¼ï¼Œæå‡æ•ˆæœæ˜æ˜¾ã€‚å…·ä½“æ–¹æ³•å®ç°äº†åŒºåŸŸç›¸å…³çš„åº¦é‡æ¥æå‡OODæ£€æµ‹çš„ä¸°å¯Œæ€§ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OODæ£€æµ‹æ—¨åœ¨åŒºåˆ†å¼‚å¸¸å€¼ä¸å·²çŸ¥ç±»åˆ«ï¼Œåœ¨å®é™…åœºæ™¯ä¸­å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å…¨å±€æç¤ºçš„ä¼˜åŒ–ï¼Œå¿½ç•¥äº†å±€éƒ¨ä¿¡æ¯åœ¨å¼‚å¸¸å€¼æ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºå±€éƒ¨æç¤ºçš„æ–¹æ³•ï¼Œåˆ©ç”¨å±€éƒ¨ä¿¡æ¯å’ŒåŒºåŸŸæ­£åˆ™åŒ–æ¥æé«˜å¼‚å¸¸å€¼çš„æ£€æµ‹èƒ½åŠ›ã€‚æ­¤æ–¹æ³•ç”±å…¨å±€æç¤ºå¼•å¯¼è´Ÿå¢å¼ºå’Œå±€éƒ¨æç¤ºå¢å¼ºåŒºåŸŸæ­£åˆ™åŒ–ä¸¤éƒ¨åˆ†ç»„æˆã€‚</li>
<li>åˆ©ç”¨å…¨çƒç²—æç¤ºä½œä¸ºæŒ‡å¯¼çº¿ç´¢è¿›è¡Œè´Ÿå¢å¼ºï¼Œç»“åˆå±€éƒ¨å¼‚å¸¸å€¼çŸ¥è¯†æé«˜æ€§èƒ½ã€‚è®­ç»ƒå±€éƒ¨æç¤ºå¹¶ä½¿ç”¨åŒºåŸŸæ­£åˆ™åŒ–æœ‰æ•ˆæ•æ‰å±€éƒ¨ä¿¡æ¯ã€‚æå‡ºåŒºåŸŸç›¸å…³åº¦é‡ä»¥å¼ºåŒ–OODæ£€æµ‹çš„ä¸°å¯Œæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38b97b166a25aa965f0e7a2c846ff38d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a15893f7a5e61b2edb089eea06494cf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4885b7003df8a7f81c378237998ee9ce.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="NarrativeBridge-Enhancing-Video-Captioning-with-Causal-Temporal-Narrative"><a href="#NarrativeBridge-Enhancing-Video-Captioning-with-Causal-Temporal-Narrative" class="headerlink" title="NarrativeBridge: Enhancing Video Captioning with Causal-Temporal   Narrative"></a>NarrativeBridge: Enhancing Video Captioning with Causal-Temporal   Narrative</h2><p><strong>Authors:Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain, Adrian Hilton, Armin Mustafa</strong></p>
<p>Existing video captioning benchmarks and models lack causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts modelsâ€™ ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content. To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions; and (2) a Cause-Effect Network (CEN) with separate encoders for capturing cause and effect dynamics, enabling effective learning and generation of captions with causal-temporal narrative. Extensive experiments demonstrate that CEN significantly outperforms state-of-the-art models in articulating the causal and temporal aspects of video content: 17.88 and 17.44 CIDEr on the MSVD-CTN and MSRVTT-CTN datasets, respectively. Cross-dataset evaluations further showcase CENâ€™s strong generalization capabilities. The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning. For project details, visit <a target="_blank" rel="noopener" href="https://narrativebridge.github.io/">https://narrativebridge.github.io/</a>. </p>
<blockquote>
<p>ç°æœ‰è§†é¢‘å­—å¹•åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ç¼ºä¹å› æœæ—¶é—´å™äº‹ï¼Œå³ä¸€ç³»åˆ—é€šè¿‡å› æœå…³ç³»è”ç³»çš„äº‹ä»¶ï¼Œéšç€æ—¶é—´çš„æ¨ç§»è€Œå±•å¼€ï¼Œå¹¶ç”±è§’è‰²æˆ–ä»£ç†é©±åŠ¨ã€‚è¿™ç§å™äº‹ç¼ºå¤±é™åˆ¶äº†æ¨¡å‹ç”Ÿæˆèƒ½å¤Ÿæ•æ‰è§†é¢‘å†…å®¹ä¸­å›ºæœ‰çš„å› æœå’Œæ—¶é—´åŠ¨æ€æ–‡æœ¬æè¿°çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†NarrativeBridgeæ–¹æ³•ï¼Œå®ƒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°‘é‡æç¤ºç”Ÿæˆçš„æ–°å‹å› æœæ—¶é—´å™äº‹ï¼ˆCTNï¼‰å­—å¹•åŸºå‡†æµ‹è¯•ï¼Œåœ¨è§†é¢‘æè¿°ä¸­æ˜¾å¼ç¼–ç å› æœå…³ç³»å’Œæ—¶é—´å…³ç³»ï¼›ï¼ˆ2ï¼‰å› æœæ•ˆåº”ç½‘ç»œï¼ˆCENï¼‰ï¼Œå…·æœ‰æ•æ‰å› æœæ•ˆåº”åŠ¨åŠ›å­¦çš„ç‹¬ç«‹ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆå­¦ä¹ å’Œç”Ÿæˆå…·æœ‰å› æœæ—¶é—´å™äº‹çš„å­—å¹•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCENåœ¨é˜è¿°è§†é¢‘å†…å®¹çš„å› æœå’Œæ—¶é—´æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°æ¨¡å‹ï¼šåœ¨MSVD-CTNå’ŒMSRVTT-CTNæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†CIDErå¾—åˆ†17.88å’Œ17.44ã€‚è·¨æ•°æ®é›†è¯„ä¼°è¿›ä¸€æ­¥å±•ç¤ºäº†CENçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æ‰€æå‡ºçš„æ¡†æ¶ç†è§£å’Œç”Ÿæˆäº†å¸¦æœ‰è§†é¢‘ä¸­å¤æ‚å› æœæ—¶é—´å™äº‹ç»“æ„çš„å¾®å¦™æ–‡æœ¬æè¿°ï¼Œè§£å†³äº†è§†é¢‘å­—å¹•ä¸­çš„å…³é”®å±€é™æ€§ã€‚æœ‰å…³é¡¹ç›®è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://narrativebridge.github.io/">https://narrativebridge.github.io/ã€‚</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06499v3">PDF</a> International Conference on Learning Representations (ICLR) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå½“å‰è§†é¢‘æè¿°åŸºå‡†æ¨¡å‹å’Œå™äº‹æ¨¡å‹çš„ä¸è¶³ï¼Œå®ƒä»¬ç¼ºä¹å› æœæ—¶é—´å™äº‹ï¼Œæ— æ³•æ•æ‰è§†é¢‘ä¸­çš„å› æœå’Œæ—¶é—´åŠ¨æ€ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæå‡ºäº†NarrativeBridgeæ–¹æ³•ï¼ŒåŒ…æ‹¬æ–°çš„å› æœæ—¶é—´å™äº‹ï¼ˆCTNï¼‰åŸºå‡†æ•°æ®é›†å’Œå› æœæ•ˆåº”ç½‘ç»œï¼ˆCENï¼‰ã€‚CENæ¨¡å‹èƒ½æœ‰æ•ˆå­¦ä¹ å’Œç”Ÿæˆå…·æœ‰å› æœæ—¶é—´å™äº‹èƒ½åŠ›çš„è§†é¢‘æè¿°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†é¢‘æè¿°æ¨¡å‹ç¼ºä¹å› æœæ—¶é—´å™äº‹ï¼Œæ— æ³•æ•æ‰è§†é¢‘ä¸­çš„å› æœå’Œæ—¶é—´åŠ¨æ€ã€‚</li>
<li>æå‡ºäº†NarrativeBridgeæ–¹æ³•ï¼ŒåŒ…æ‹¬æ–°çš„å› æœæ—¶é—´å™äº‹ï¼ˆCTNï¼‰åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>CTNæ•°æ®é›†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°‘é‡æç¤ºç”Ÿæˆæè¿°ï¼Œæ˜ç¡®ç¼–ç è§†é¢‘ä¸­çš„å› æœå’Œæ—¶é—´å…³ç³»ã€‚</li>
<li>æå‡ºäº†å› æœæ•ˆåº”ç½‘ç»œï¼ˆCENï¼‰æ¨¡å‹ï¼ŒåŒ…æ‹¬ç”¨äºæ•æ‰å› æœæ•ˆåº”åŠ¨æ€çš„å•ç‹¬ç¼–ç å™¨ã€‚</li>
<li>CENæ¨¡å‹èƒ½æœ‰æ•ˆå­¦ä¹ å’Œç”Ÿæˆå…·æœ‰å› æœæ—¶é—´å™äº‹èƒ½åŠ›çš„è§†é¢‘æè¿°ã€‚</li>
<li>CENæ¨¡å‹åœ¨MSVD-CTNå’ŒMSRVTT-CTNæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåˆ†åˆ«è¾¾åˆ°äº†17.88å’Œ17.44çš„CIDErå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f37def15658e07f8f36c550e13ab8ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a29d3f54a1a01a3b28088bd205d7ad5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6494a9e7c3e3b92f0289f730b839d4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d15438a253708ca5334a1cbaf2630c2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c941d31803447ae459cf0f88348f9cfa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Point-of-View-of-a-Sentiment-Towards-Clinician-Bias-Detection-in-Psychiatric-Notes"><a href="#The-Point-of-View-of-a-Sentiment-Towards-Clinician-Bias-Detection-in-Psychiatric-Notes" class="headerlink" title="The Point of View of a Sentiment: Towards Clinician Bias Detection in   Psychiatric Notes"></a>The Point of View of a Sentiment: Towards Clinician Bias Detection in   Psychiatric Notes</h2><p><strong>Authors:Alissa A. Valentine, Lauren A. Lepow, Lili Chan, Alexander W. Charney, Isotta Landi</strong></p>
<p>Negative patient descriptions and stigmatizing language can contribute to generating healthcare disparities in two ways: (1) read by patients, they can harm their trust and engagement with the medical center; (2) read by physicians, they may negatively influence their perspective of a future patient. In psychiatry, the patient-clinician therapeutic alliance is a major determinant of clinical outcomes. Therefore, language usage in psychiatric clinical notes may not only create healthcare disparities, but also perpetuate them. Recent advances in NLP systems have facilitated the efforts to detect discriminatory language in healthcare. However, such attempts have only focused on the perspectives of the medical center and its physicians. Considering both physicians and non-physiciansâ€™ point of view is a more translatable approach to identifying potentially harmful language in clinical notes. By leveraging pre-trained and large language models (PLMs and LLMs), this work aims to characterize potentially harmful language usage in psychiatric notes by identifying the sentiment expressed in sentences describing patients based on the readerâ€™s point of view. Extracting 39 sentences from the Mount Sinai Health System containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa, GatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and few-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to classify the sentiment of the sentences according to the physician or non-physician point of view. Results showed that GPT-3.5 aligned best to physician point of view and Mistral aligned best to non-physician point of view. These results underline the importance of recognizing the readerâ€™s point of view, not only for improving the note writing process, but also for the quantification, identification, and reduction of bias in computational systems for downstream analyses. </p>
<blockquote>
<p>è´Ÿé¢æ‚£è€…æè¿°å’Œæ±¡ååŒ–è¯­è¨€ä¼šäº§ç”Ÿä¸¤ç§æ–¹å¼æ¥é€ æˆåŒ»ç–—ä¿å¥å·®å¼‚ï¼šï¼ˆ1ï¼‰å½“æ‚£è€…é˜…è¯»æ—¶ï¼Œå®ƒä»¬ä¼šæŸå®³ä»–ä»¬å¯¹åŒ»ç–—ä¸­å¿ƒçš„ä¿¡ä»»å’Œå‚ä¸ç¨‹åº¦ï¼›ï¼ˆ2ï¼‰å½“åŒ»ç”Ÿé˜…è¯»æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šå¯¹åŒ»ç”Ÿå¯¹æœªæ¥æ‚£è€…çš„çœ‹æ³•äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨ç²¾ç¥ç—…å­¦ä¸­ï¼Œæ‚£è€…ä¸ä¸´åºŠåŒ»ç”Ÿä¹‹é—´çš„æ²»ç–—è”ç›Ÿæ˜¯ä¸´åºŠç»“æœçš„ä¸»è¦å†³å®šå› ç´ ã€‚å› æ­¤ï¼Œç²¾ç¥ç—…å­¦ä¸´åºŠç¬”è®°ä¸­çš„è¯­è¨€ä½¿ç”¨ä¸ä»…å¯èƒ½åˆ›é€ åŒ»ç–—ä¿å¥å·®å¼‚ï¼Œè€Œä¸”å¯èƒ½ä½¿è¿™äº›å·®å¼‚æŒç»­å­˜åœ¨ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç³»ç»Ÿçš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†æ£€æµ‹åŒ»ç–—ä¿å¥ä¸­æ­§è§†æ€§è¯­è¨€çš„åŠªåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›å°è¯•ä»…å…³æ³¨åŒ»ç–—ä¸­å¿ƒåŠå…¶åŒ»ç”Ÿçš„è§‚ç‚¹ã€‚è€ƒè™‘åˆ°åŒ»ç”Ÿå’ŒéåŒ»ç”Ÿçš„è§‚ç‚¹æ˜¯ä¸€ç§æ›´å¯ç¿»è¯‘çš„æ–¹æ³•æ¥è¯†åˆ«ä¸´åºŠç¬”è®°ä¸­å¯èƒ½æœ‰å®³çš„è¯­è¨€ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPLMså’ŒLLMsï¼‰ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨é€šè¿‡è¯†åˆ«æè¿°æ‚£è€…çš„å¥å­ä¸­æ‰€è¡¨è¾¾çš„æƒ…æ„Ÿæ¥åˆ»ç”»ç²¾ç¥ç—…å­¦ç¬”è®°ä¸­å¯èƒ½æœ‰å®³çš„è¯­è¨€ç”¨æ³•ï¼Œè¿™åŸºäºè¯»è€…çš„è§‚ç‚¹ã€‚æˆ‘ä»¬ä»Mount Sinai Health Systemä¸­æå–äº†åŒ…å«ç²¾ç¥ç—…å­¦è¯æ±‡çš„39ä¸ªå¥å­ï¼Œå¯¹ä¸‰ä¸ªPLMï¼ˆRoBERTaã€GatorTronå’ŒGatorTron + Task Adaptationï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶ä¸ºä¸‰ä¸ªLLMï¼ˆGPT-3.5ã€Llama-3.1å’ŒMistralï¼‰å®æ–½äº†é›¶é•œå¤´å’Œå°‘é‡é•œå¤´ICLæ–¹æ³•æ¥æ ¹æ®åŒ»ç”Ÿæˆ–éåŒ»ç”Ÿçš„è§‚ç‚¹å¯¹å¥å­è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-3.5æœ€ç¬¦åˆåŒ»ç”Ÿçš„è§‚ç‚¹ï¼Œè€ŒMistralæœ€ç¬¦åˆéåŒ»ç”Ÿçš„è§‚ç‚¹ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨å†™ä½œè¿‡ç¨‹ä¸­è®¤è¯†åˆ°è¯»è€…è§‚ç‚¹çš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒåœ¨ä¸‹æ¸¸åˆ†æä¸­è¯†åˆ«ã€é‡åŒ–å¹¶å‡å°‘è®¡ç®—ç³»ç»Ÿä¸­çš„åè§çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20582v2">PDF</a> Oral presentation at NAACL 2024 Queer in AI Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è´Ÿé¢æ‚£è€…æè¿°å’Œæ ‡ç­¾åŒ–è¯­è¨€åœ¨åŒ»ç–—å¥åº·é¢†åŸŸå¸¦æ¥çš„åŒé‡è´Ÿé¢å½±å“ã€‚é¦–å…ˆï¼Œå¯¹æ‚£è€…è€Œè¨€ï¼Œè¿™ç§è¯­è¨€å¯èƒ½æŸå®³ä»–ä»¬å¯¹åŒ»ç–—ä¸­å¿ƒçš„ä¿¡ä»»ä¸å‚ä¸åº¦ï¼›å…¶æ¬¡ï¼Œå¯¹äºåŒ»ç”Ÿï¼Œè¿™ç§è¯­è¨€å¯èƒ½å¯¹ä»–ä»¬çš„æœªæ¥æ‚£è€…äº§ç”Ÿè´Ÿé¢çœ‹æ³•ã€‚åœ¨ç²¾ç¥ç—…å­¦ä¸­ï¼ŒåŒ»æ‚£é—´çš„æ²»ç–—è”ç›Ÿæ˜¯ä¸´åºŠç»“æœçš„é‡è¦å†³å®šå› ç´ ã€‚å› æ­¤ï¼Œç²¾ç¥ç—…ä¸´åºŠç¬”è®°ä¸­çš„è¯­è¨€ä½¿ç”¨ä¸ä»…å¯èƒ½é€ æˆåŒ»ç–—å·®å¼‚ï¼Œè¿˜å¯èƒ½ä½¿å…¶æŒç»­å­˜åœ¨ã€‚æœ€è¿‘è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„è¿›æ­¥æœ‰åŠ©äºæ£€æµ‹åŒ»ç–—é¢†åŸŸä¸­çš„æ­§è§†æ€§è¯­è¨€ã€‚ç„¶è€Œï¼Œè¿™äº›å°è¯•ä»…å…³æ³¨åŒ»ç–—ä¸­å¿ƒå’ŒåŒ»ç”Ÿçš„è§’åº¦ã€‚è€ƒè™‘åˆ°åŒ»ç”Ÿå’ŒéåŒ»ç”Ÿçš„è§‚ç‚¹æ˜¯æ›´å¯è¡Œçš„åŠæ³•æ¥è¯†åˆ«ä¸´åºŠç¬”è®°ä¸­æ½œåœ¨çš„æœ‰å®³è¯­è¨€ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºè¯»è€…è§’åº¦è¯†åˆ«æè¿°æ‚£è€…çš„å¥å­ä¸­çš„æƒ…æ„Ÿï¼Œä»è€Œåˆ»ç”»ç²¾ç¥ç—…ç¬”è®°ä¸­æ½œåœ¨çš„æœ‰å®³è¯­è¨€ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è´Ÿé¢æ‚£è€…æè¿°å’Œæ ‡ç­¾åŒ–è¯­è¨€å¯èƒ½å¯¼è‡´åŒ»ç–—å¥åº·é¢†åŸŸçš„å·®å¼‚å¯¹å¾…ã€‚</li>
<li>è¿™ç§è¯­è¨€å¯¹æ‚£è€…çš„ä¿¡ä»»ä¸å‚ä¸åº¦ã€åŒ»ç”Ÿçš„æœªæ¥æ‚£è€…è§‚äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>åœ¨ç²¾ç¥ç—…å­¦é¢†åŸŸï¼ŒåŒ»æ‚£é—´çš„æ²»ç–—è”ç›Ÿå¯¹ä¸´åºŠç»“æœè‡³å…³é‡è¦ã€‚</li>
<li>æœ‰å®³çš„è¯­è¨€ä½¿ç”¨ä¸ä»…å¯èƒ½åˆ¶é€ åŒ»ç–—å·®å¼‚ï¼Œè¿˜å¯èƒ½ä½¿å…¶æŒç»­å­˜åœ¨ã€‚</li>
<li>NLPç³»ç»Ÿçš„è¿›æ­¥æœ‰åŠ©äºæ£€æµ‹åŒ»ç–—æ­§è§†æ€§è¯­è¨€ï¼Œä½†ç°æœ‰å°è¯•ä»…å…³æ³¨åŒ»ç–—ä¸­å¿ƒå’ŒåŒ»ç”Ÿè§†è§’ã€‚</li>
<li>è€ƒè™‘åŒ»ç”Ÿå’ŒéåŒ»ç”Ÿçš„è§‚ç‚¹æ˜¯è¯†åˆ«ä¸´åºŠç¬”è®°ä¸­æ½œåœ¨æœ‰å®³è¯­è¨€çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥æ ¹æ®è¯»è€…è§’åº¦è¯†åˆ«æè¿°æ‚£è€…çš„å¥å­ä¸­çš„æƒ…æ„Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2adce6640ea693d23b32c0dd70a1ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776276df5e02f5d4afb9fe1aa8c3503f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-167a1e21f4909bf1b2ea0ba1fb47f8af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1119621148cab1d52e752fdc7fd5b8d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Contrastive-Reasoners"><a href="#Large-Language-Models-are-Contrastive-Reasoners" class="headerlink" title="Large Language Models are Contrastive Reasoners"></a>Large Language Models are Contrastive Reasoners</h2><p><strong>Authors:Liang Yao</strong></p>
<p>Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding â€œLetâ€™s give a correct and a wrong answer.â€ before LLMs provide answers. Experiments on various large language models show that zero-shot contrastive prompting improves the performance of standard zero-shot prompting on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yao8839836/cp">https://github.com/yao8839836/cp</a> </p>
<blockquote>
<p>æç¤ºæ–¹æ³•åœ¨å¢å¼ºé¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬æ¢è®¨äº†å¯¹æ¯”æç¤ºï¼ˆCPï¼‰å¦‚ä½•æ˜¾è‘—å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œå¤æ‚æ¨ç†çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ç®€å•åœ°åœ¨LLMæä¾›ç­”æ¡ˆä¹‹å‰æ·»åŠ â€œè®©æˆ‘ä»¬ç»™å‡ºä¸€ä¸ªæ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆã€‚â€æ¥è¯æ˜LLMæ˜¯å‡ºè‰²çš„å¯¹æ¯”æ¨ç†å™¨ã€‚åœ¨å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé›¶å‡»å¯¹æ¯”æç¤ºæé«˜äº†æ ‡å‡†é›¶å‡»æç¤ºåœ¨å„ç§ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ— éœ€ä»»ä½•æ‰‹å·¥åˆ¶ä½œçš„å°‘é‡ç¤ºä¾‹ï¼Œä¾‹å¦‚ä½¿ç”¨æœ€å…ˆè¿›çš„GPT-4æ¨¡å‹ï¼Œå°†GSM8Kçš„å‡†ç¡®ç‡ä»35.9%æé«˜åˆ°88.8%ï¼ŒAQUA-RATçš„å‡†ç¡®ç‡ä»4 .%æé«˜åˆ°åŸæ¥çš„è¿‘ä¸¤å€ç™¾åˆ†ä¹‹å››åä¸€ä¸‰åå…­åˆ†ä¹‹å…­åäºŒç‚¹äºŒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨æœ€å…³é”®çš„ç®—æœ¯å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†é›¶å‡»CoTå’Œå°‘å‡»CoTï¼Œè¿˜å¯ä»¥æ— ç¼åœ°èå…¥ç°æœ‰çš„æç¤ºæ–¹æ³•ï¼Œåœ¨ä¸æœ€æ–°æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œç»“æœæœ‰æ‰€æå‡æˆ–æŒå¹³ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨xxxç½‘ç«™ä¸Šæ‰¾åˆ°ï¼ˆå ä½ç¬¦ç½‘ç«™åœ°å€ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08211v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¯¹æ¯”æç¤ºæ–¹æ³•æ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ·»åŠ â€œè®©æˆ‘ä»¬ç»™å‡ºä¸€ä¸ªæ­£ç¡®å’Œä¸€ä¸ªé”™è¯¯çš„ç­”æ¡ˆâ€æ¥ä¿ƒä½¿LLMå›ç­”é—®é¢˜ï¼Œå®éªŒè¡¨æ˜LLMå…·å¤‡å‡ºè‰²çš„å¯¹æ¯”æ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šç§ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ä»»åŠ¡ä¸Šï¼Œé›¶å¯¹æ¯”æç¤ºæ–¹æ³•æé«˜äº†æ ‡å‡†é›¶å¯¹æ¯”æç¤ºçš„æ€§èƒ½ï¼Œå¦‚å°†GSM8Kçš„å‡†ç¡®ç‡ä»35.9%æé«˜åˆ°88.8%ï¼ŒAQUA-RATçš„å‡†ç¡®ç‡ä»41.3%æé«˜åˆ°62.2%ã€‚è¯¥æ–¹æ³•ä¸ä»…è¶…è¶Šäº†é›¶å¯¹æ¯”æ€è€ƒå’Œå°‘å¯¹æ¯”æ€è€ƒçš„ç®—æœ¯å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ï¼Œè¿˜èƒ½æ— ç¼èå…¥ç°æœ‰æç¤ºæ–¹æ³•ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œç»“æœæœ‰æ‰€æé«˜æˆ–ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”æç¤ºæ–¹æ³•èƒ½æé«˜é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç®€å•æ·»åŠ å¯¹æ¯”æç¤ºè¯­å¥ï¼ŒLLMå¯ä»¥åŒºåˆ†æ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆã€‚</li>
<li>é›¶å¯¹æ¯”æç¤ºæ–¹æ³•åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¦‚ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ã€‚</li>
<li>å¯¹æ¯”æç¤ºæ–¹æ³•èƒ½æé«˜ç‰¹å®šä»»åŠ¡çš„å‡†ç¡®ç‡ï¼Œå¦‚GSM8Kå’ŒAQUA-RATã€‚</li>
<li>å¯¹æ¯”æç¤ºæ–¹æ³•è¶…è¶Šé›¶å¯¹æ¯”æ€è€ƒå’Œå°‘å¯¹æ¯”æ€è€ƒåœ¨å¤šæ•°ç®—æœ¯å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å¯¹æ¯”æç¤ºæ–¹æ³•èƒ½æ— ç¼èå…¥ç°æœ‰æç¤ºæ–¹æ³•ï¼Œæé«˜æˆ–ä¿æŒä¸æœ€æ–°æ–¹æ³•çš„ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-539f660d66a0f84996c7308bb2dfb703.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291536d1ca4ee987b2ced5ca84a0e90c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e16d6f9f406e91f120b2046a49e6bee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-246af51643f6b8d4dcd0b7d7f0c59565.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PrototypeFormer-Learning-to-Explore-Prototype-Relationships-for-Few-shot-Image-Classification"><a href="#PrototypeFormer-Learning-to-Explore-Prototype-Relationships-for-Few-shot-Image-Classification" class="headerlink" title="PrototypeFormer: Learning to Explore Prototype Relationships for   Few-shot Image Classification"></a>PrototypeFormer: Learning to Explore Prototype Relationships for   Few-shot Image Classification</h2><p><strong>Authors:Meijuan Su, Feihong He, Fanzhang Li</strong></p>
<p>Few-shot image classification has received considerable attention for overcoming the challenge of limited classification performance with limited samples in novel classes. Most existing works employ sophisticated learning strategies and feature learning modules to alleviate this challenge. In this paper, we propose a novel method called PrototypeFormer, exploring the relationships among category prototypes in the few-shot scenario. Specifically, we utilize a transformer architecture to build a prototype extraction module, aiming to extract class representations that are more discriminative for few-shot classification. Besides, during the model training process, we propose a contrastive learning-based optimization approach to optimize prototype features in few-shot learning scenarios. Despite its simplicity, our method performs remarkably well, with no bells and whistles. We have experimented with our approach on several popular few-shot image classification benchmark datasets, which shows that our method outperforms all current state-of-the-art methods. In particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with accuracy of 0.57% and 6.84%, respectively. The code will be released later. </p>
<blockquote>
<p>å°‘æ ·æœ¬å›¾åƒåˆ†ç±»å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œæ—¨åœ¨å…‹æœåœ¨æ–°å‹ç±»åˆ«ä¸­å› æ ·æœ¬æ•°é‡æœ‰é™è€Œå¯¼è‡´çš„åˆ†ç±»æ€§èƒ½æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç°æœ‰ä½œå“é‡‡ç”¨å¤æ‚çš„å­¦ä¹ ç­–ç•¥å’Œç‰¹å¾å­¦ä¹ æ¨¡å—æ¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPrototypeFormerçš„æ–°æ–¹æ³•ï¼Œæ¢ç´¢å°‘æ ·æœ¬åœºæ™¯ä¸­çš„ç±»åˆ«åŸå‹ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å˜å‹å™¨æ¶æ„æ„å»ºåŸå‹æå–æ¨¡å—ï¼Œæ—¨åœ¨æå–å¯¹å°‘æ ·æœ¬åˆ†ç±»æ›´å…·åŒºåˆ†åŠ›çš„ç±»åˆ«è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„åŸå‹ç‰¹å¾ã€‚å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•å¾ˆç®€å•ï¼Œä½†è¡¨ç°éå¸¸å‡ºè‰²ã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªæµè¡Œçš„å°‘æ ·æœ¬å›¾åƒåˆ†ç±»åŸºå‡†æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæ‰€æœ‰å½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨miniImageNetçš„5è·¯5æ¬¡å°„å‡»å’Œ5è·¯å•æ¬¡å°„å‡»ä»»åŠ¡ä¸Šåˆ†åˆ«è¾¾åˆ°äº†97.07%å’Œ90.88%çš„å‡†ç¡®ç‡ï¼Œåˆ†åˆ«æ¯”ç°æœ‰æœ€é«˜å‡†ç¡®ç‡é«˜å‡º0.57%å’Œ6.84%ã€‚ä»£ç å°†åœ¨ç¨åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03517v2">PDF</a> Submitted to Neurocomputing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPrototypeFormerçš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³å°‘æ ·æœ¬å›¾åƒåˆ†ç±»é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜å‹å™¨æ¶æ„æ„å»ºåŸå‹æå–æ¨¡å—ï¼Œæå–æ›´å…·åŒºåˆ†åº¦çš„ç±»åˆ«è¡¨ç¤ºï¼Œå¹¶æå‡ºåŸºäºå¯¹æ¯”å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„åŸå‹ç‰¹å¾ã€‚åœ¨å¤šä¸ªæµè¡Œçš„å°‘æ ·æœ¬å›¾åƒåˆ†ç±»åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰æ‰€æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨miniImageNetçš„5ç±»5æ ·æœ¬å’Œ5ç±»1æ ·æœ¬ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾97.07%å’Œ90.88%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PrototypeFormeræ˜¯ä¸€ç§ç”¨äºè§£å†³å°‘æ ·æœ¬å›¾åƒåˆ†ç±»é—®é¢˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å˜å‹å™¨æ¶æ„æ„å»ºåŸå‹æå–æ¨¡å—ï¼Œä»¥æå–æ›´å…·åŒºåˆ†åº¦çš„ç±»åˆ«è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„åŸå‹ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPrototypeFormeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>åœ¨miniImageNetçš„5ç±»5æ ·æœ¬ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾97.07%çš„å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨miniImageNetçš„5ç±»1æ ·æœ¬ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾90.88%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.03517">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-497339de60906bcd1ea1e75f3c7633dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b0b7afd2669e5c8a6e0908b03fcef3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4804874e23070177eaf17c3922ddfe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25c724e0806c22d97100b07893fc31bd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-372830aee3ac139b61ac05c1ccd7eebc.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  A Survey on Bridging EEG Signals and Generative AI From Image and Text   to Beyond
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-187d99963871431099fb488d2e5eb90a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  HARBOR Exploring Persona Dynamics in Multi-Agent Competition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13597.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
