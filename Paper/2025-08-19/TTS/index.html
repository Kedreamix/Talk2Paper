<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-08-19  MoE-TTS Enhancing Out-of-Domain Text Understanding for   Description-based TTS via Mixture-of-Experts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-522dbbaa71aab70b1b338d37f90cc03f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-19-更新"><a href="#2025-08-19-更新" class="headerlink" title="2025-08-19 更新"></a>2025-08-19 更新</h1><h2 id="MoE-TTS-Enhancing-Out-of-Domain-Text-Understanding-for-Description-based-TTS-via-Mixture-of-Experts"><a href="#MoE-TTS-Enhancing-Out-of-Domain-Text-Understanding-for-Description-based-TTS-via-Mixture-of-Experts" class="headerlink" title="MoE-TTS: Enhancing Out-of-Domain Text Understanding for   Description-based TTS via Mixture-of-Experts"></a>MoE-TTS: Enhancing Out-of-Domain Text Understanding for   Description-based TTS via Mixture-of-Experts</h2><p><strong>Authors:Heyang Xue, Xuchen Song, Yu Tang, Jianyu Chen, Yanru Chen, Yang Li, Yahui Zhou</strong></p>
<p>Description-based text-to-speech (TTS) models exhibit strong performance on in-domain text descriptions, i.e., those encountered during training. However, in real-world applications, the diverse range of user-generated descriptions inevitably introduces numerous out-of-domain inputs that challenge the text understanding capabilities of these systems. To address this issue, we propose MoE-TTS, a description-based TTS model designed to enhance the understanding of out-of-domain text descriptions. MoE-TTS employs a modality-based mixture-of-experts (MoE) approach to augment a pre-trained textual large language model (LLM) with a set of specialized weights adapted to the speech modality while maintaining the original LLM frozen during training. This approach allows MoE-TTS to effectively leverage the pre-trained knowledge and text understanding abilities of textual LLMs. Our experimental results indicate that: first, even the most advanced closed-source commercial products can be challenged by carefully designed out-of-domain description test sets; second, MoE-TTS achieves superior performance in generating speech that more accurately reflects the descriptions. We encourage readers to listen to the demos at <a target="_blank" rel="noopener" href="https://welkinyang.github.io/MoE-TTS/">https://welkinyang.github.io/MoE-TTS/</a>. </p>
<blockquote>
<p>基于描述的文本转语音（TTS）模型在领域内的文本描述上表现出强大的性能，即那些在训练过程中遇到的描述。然而，在现实世界的应用中，用户生成的各种描述不可避免地引入了众多的离域输入，这些输入对这些系统的文本理解能力提出了挑战。为了解决这一问题，我们提出了MoE-TTS，这是一种基于描述的TTS模型，旨在增强对离域文本描述的理解。MoE-TTS采用基于模态的混合专家（MoE）方法，对预训练的文本大型语言模型（LLM）进行增强，通过一系列适应于语音模态的专用权重，同时保持训练过程中的原始LLM冻结。这种方法允许MoE-TTS有效利用预训练的知识和文本理解能力。我们的实验结果表明：首先，甚至最先进的闭源商业产品也可能会受到精心设计的离域描述测试集的挑战；其次，MoE-TTS在生成更能准确反映描述的语音方面表现出卓越的性能。我们鼓励读者在<a target="_blank" rel="noopener" href="https://welkinyang.github.io/MoE-TTS/%E4%B8%8A%E5%90%AC%E5%8F%96%E6%BC%94%E7%A4%BA%E3%80%82">https://welkinyang.github.io/MoE-TTS/上听取演示。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11326v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于描述的文本转语音（TTS）模型在训练过程中的内部文本描述上表现出强大的性能。然而，在真实世界应用中，用户生成描述的多样性不可避免地引入了众多超出领域的输入，这对这些系统的文本理解能力提出了挑战。为解决此问题，我们提出了MoE-TTS模型，该模型采用基于模态的混合专家（MoE）方法，增强了对超出领域文本描述的理解能力。该模型通过增强预训练的文本大型语言模型（LLM）并维持训练期间的原始LLM冻结来实现这一点。我们的实验结果表明：首先，最先进的闭源商业产品也可能受到精心设计的不在领域内的描述测试集的挑战；其次，MoE-TTS在生成更能准确反映描述的语音方面表现出卓越性能。我们鼓励读者在<a target="_blank" rel="noopener" href="https://welkinyang.github.io/MoE-TTS/%E4%B8%8A%E8%AF%95%E5%90%AC%E6%BC%94%E7%A4%BA%E5%86%85%E5%AE%B9%E3%80%82">https://welkinyang.github.io/MoE-TTS/上试听演示内容。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于描述的TTS模型在内部文本描述上表现良好，但在真实应用中面临用户生成描述的多样性挑战。</li>
<li>MoE-TTS模型采用基于模态的混合专家（MoE）方法增强对超出领域文本描述的理解能力。</li>
<li>MoE-TTS通过在预训练的文本大型语言模型（LLM）的基础上添加特定于语音的权重来解决该问题。</li>
<li>最先进的闭源商业TTS产品在特定测试集上可能表现不佳。</li>
<li>MoE-TTS能够生成更能准确反映描述的语音。</li>
<li>MoE-TTS鼓励听者试听演示内容以更好地了解模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-11a6a07e48eb0536f119de61b296657a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1eb4b44744cf7ffbe5cd133333b671a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4024de9d9b7592ff28171a6c3aaf62c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec36faba079f0bc70e4e19d0914f8731.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EmoSSLSphere-Multilingual-Emotional-Speech-Synthesis-with-Spherical-Vectors-and-Discrete-Speech-Tokens"><a href="#EmoSSLSphere-Multilingual-Emotional-Speech-Synthesis-with-Spherical-Vectors-and-Discrete-Speech-Tokens" class="headerlink" title="EmoSSLSphere: Multilingual Emotional Speech Synthesis with Spherical   Vectors and Discrete Speech Tokens"></a>EmoSSLSphere: Multilingual Emotional Speech Synthesis with Spherical   Vectors and Discrete Speech Tokens</h2><p><strong>Authors:Joonyong Park, Kenichi Nakamura</strong></p>
<p>This paper introduces EmoSSLSphere, a novel framework for multilingual emotional text-to-speech (TTS) synthesis that combines spherical emotion vectors with discrete token features derived from self-supervised learning (SSL). By encoding emotions in a continuous spherical coordinate space and leveraging SSL-based representations for semantic and acoustic modeling, EmoSSLSphere enables fine-grained emotional control, effective cross-lingual emotion transfer, and robust preservation of speaker identity. We evaluate EmoSSLSphere on English and Japanese corpora, demonstrating significant improvements in speech intelligibility, spectral fidelity, prosodic consistency, and overall synthesis quality. Subjective evaluations further confirm that our method outperforms baseline models in terms of naturalness and emotional expressiveness, underscoring its potential as a scalable solution for multilingual emotional TTS. </p>
<blockquote>
<p>本文介绍了EmoSSLSphere，这是一个用于多语种情感文本转语音（TTS）合成的新型框架，它将球形情感向量与自我监督学习（SSL）衍生的离散令牌特征相结合。通过将情感编码在连续的球形坐标空间中，并利用基于SSL的语义和声学建模表示，EmoSSLSphere能够实现精细的情感控制、有效的跨语言情感转移和稳健的说话人身份保留。我们在英语和日语语料库上对EmoSSLSphere进行了评估，证明了其在语音清晰度、频谱保真度、语调一致性和整体合成质量方面的显著提高。主观评价进一步证实，我们的方法在自然度和情感表现力方面优于基准模型，突显了其作为可扩展的多语言情感TTS解决方案的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11273v1">PDF</a> In Proceedings of the 13th ISCA Speech Synthesis Workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了EmoSSLSphere，一个结合球面情感向量和自监督学习（SSL）的离散令牌特征的多语言情感文本到语音（TTS）合成的新框架。通过情感在连续球坐标空间中的编码以及基于SSL的语义和声学建模，EmoSSLSphere能够实现精细的情感控制、有效的跨语言情感转移和稳健的说话人身份保留。对英文和日文语料库的评价显示，EmoSSLSphere在语音清晰度、频谱保真度、语调一致性和整体合成质量方面都有显著提高。主观评价进一步证明了我们的方法在自然度和情感表现力方面优于基准模型，突显了其在多语言情感TTS领域的可扩展解决方案潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmoSSLSphere是一个用于多语言情感文本到语音（TTS）合成的新框架。</li>
<li>它结合了球面情感向量和自监督学习（SSL）的离散令牌特征。</li>
<li>EmoSSLSphere能在连续球坐标空间中编码情感，实现精细的情感控制。</li>
<li>框架支持有效的跨语言情感转移和稳健的说话人身份保留。</li>
<li>在英文和日文语料库上的评价显示，EmoSSLSphere在语音清晰度、频谱保真度等方面有显著提高。</li>
<li>主观评价证明了EmoSSLSphere在自然度和情感表现力方面优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ca6babc6546e3d65fa0da9f77715c3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a32ecc1b063904e39b0b3dd29f84576b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff3950fa7396c45d56efb38a8d285278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9091b902ddff9d1db8ae5a6eb8ef323d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a06b7a408cfd6940be044e791366dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fe315435cb43b3e7efd759af0178f96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5280f0a8888e28437c0ed2427786ec8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dc59e7c3140a0ae695c425c1d2f72e7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Analysis-of-Domain-Shift-across-ASR-Architectures-via-TTS-Enabled-Separation-of-Target-Domain-and-Acoustic-Conditions"><a href="#Analysis-of-Domain-Shift-across-ASR-Architectures-via-TTS-Enabled-Separation-of-Target-Domain-and-Acoustic-Conditions" class="headerlink" title="Analysis of Domain Shift across ASR Architectures via TTS-Enabled   Separation of Target Domain and Acoustic Conditions"></a>Analysis of Domain Shift across ASR Architectures via TTS-Enabled   Separation of Target Domain and Acoustic Conditions</h2><p><strong>Authors:Tina Raissi, Nick Rossenbach, Ralf Schlüter</strong></p>
<p>We analyze automatic speech recognition (ASR) modeling choices under domain mismatch, comparing classic modular and novel sequence-to-sequence (seq2seq) architectures. Across the different ASR architectures, we examine a spectrum of modeling choices, including label units, context length, and topology. To isolate language domain effects from acoustic variation, we synthesize target domain audio using a text-to-speech system trained on LibriSpeech. We incorporate target domain n-gram and neural language models for domain adaptation without retraining the acoustic model. To our knowledge, this is the first controlled comparison of optimized ASR systems across state-of-the-art architectures under domain shift, offering insights into their generalization. The results show that, under domain shift, rather than the decoder architecture choice or the distinction between classic modular and novel seq2seq models, it is specific modeling choices that influence performance. </p>
<blockquote>
<p>我们分析了在领域不匹配情况下自动语音识别（ASR）的建模选择，对比了经典的模块化架构和新颖的顺序到序列（seq2seq）架构。在不同的ASR架构中，我们研究了包括标签单位、上下文长度和拓扑结构等一系列建模选择。为了将语言领域的影响与声学变化区分开来，我们使用在LibriSpeech上训练的文本到语音系统合成目标领域的音频。我们融入目标领域的n元模型和神经网络语言模型，以进行领域适应，无需重新训练声学模型。据我们所知，这是领域变化背景下优化ASR系统在各主流架构间的首次受控对比研究，为我们提供了有关其泛化性能的洞察力。结果表明，在领域变化下，影响性能的不是解码器架构的选择或是经典模块化与新式seq2seq模型之间的区别，而是特定的建模选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09868v1">PDF</a> Accepted for presentation at IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了自动语音识别（ASR）建模在不同架构下如何处理领域不匹配的问题。该研究比较了经典的模块化和新型的序列到序列（seq2seq）架构，同时研究了标签单元、上下文长度和拓扑结构等一系列建模选择。研究通过合成目标领域音频，以减少语音变化的语言领域影响，并融入了目标领域的n-gram和神经网络语言模型进行领域适应，而无需重新训练声学模型。这是首次针对前沿架构在领域转移方面的优化ASR系统进行控制比较，为它们的泛化能力提供了深入见解。研究结果表明，在领域转移情况下，影响性能的是特定的建模选择，而非解码器架构的选择或经典模块化与新型seq2seq模型之间的差异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文分析了自动语音识别（ASR）建模在领域不匹配问题下的不同架构表现。</li>
<li>研究比较了经典的模块化架构和新型的序列到序列（seq2seq）架构。</li>
<li>研究重点考虑了标签单元、上下文长度和拓扑结构等建模选择。</li>
<li>通过合成目标领域音频，减少了语音变化的语言领域影响。</li>
<li>研究融入了目标领域的n-gram和神经网络语言模型进行领域适应。</li>
<li>这是首次针对前沿架构在领域转移方面的优化ASR系统进行的控制比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09868">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-92b180df22d75c43a3fad1a1951ed243.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aaf10b1d550413bd2f6076009580e698.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c4501bf22a1b756c38b3ca8708ac70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58352439b96a9846fd2054a8df47b6a2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ProMode-A-Speech-Prosody-Model-Conditioned-on-Acoustic-and-Textual-Inputs"><a href="#ProMode-A-Speech-Prosody-Model-Conditioned-on-Acoustic-and-Textual-Inputs" class="headerlink" title="ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual   Inputs"></a>ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual   Inputs</h2><p><strong>Authors:Eray Eren, Qingju Liu, Hyeongwoo Kim, Pablo Garrido, Abeer Alwan</strong></p>
<p>Prosody conveys rich emotional and semantic information of the speech signal as well as individual idiosyncrasies. We propose a stand-alone model that maps text-to-prosodic features such as F0 and energy and can be used in downstream tasks such as TTS. The ProMode encoder takes as input acoustic features and time-aligned textual content, both are partially masked, and obtains a fixed-length latent prosodic embedding. The decoder predicts acoustics in the masked region using both the encoded prosody input and unmasked textual content. Trained on the GigaSpeech dataset, we compare our method with state-of-the-art style encoders. For F0 and energy predictions, we show consistent improvements for our model at different levels of granularity. We also integrate these predicted prosodic features into a TTS system and conduct perceptual tests, which show higher prosody preference compared to the baselines, demonstrating the model’s potential in tasks where prosody modeling is important. </p>
<blockquote>
<p>韵律传达了语音信号的丰富情感和语义信息，以及个人的特殊习惯。我们提出了一个独立的模型，该模型可以将文本映射到诸如F0和能量等韵律特征上，并可用于如文本到语音转换（TTS）等下游任务。ProMode编码器以声学特征和时间对齐的文本内容为输入，两者都被部分掩盖，并获得固定长度的潜在韵律嵌入。解码器使用编码的韵律输入和未屏蔽的文本内容来预测屏蔽区域的声学特征。该模型在GigaSpeech数据集上进行训练，我们将其与最新风格编码器进行比较。对于F0和能量预测，我们的模型在不同的粒度层次上都表现出了一致的优势。我们还将这些预测的韵律特征集成到TTS系统中，并进行感知测试，结果显示我们的模型在韵律偏好上高于基线，这证明了该模型在韵律建模任务中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09389v1">PDF</a> Interspeech 2025; demo page at   <a target="_blank" rel="noopener" href="https://promode8272.github.io/promode/index.html">https://promode8272.github.io/promode/index.html</a></p>
<p><strong>摘要</strong></p>
<p>文本中的语音信号包含丰富的情感和语义信息，以及个人的特殊习惯。我们提出了一种独立的模型，该模型可以将文本映射到音高和能量等语音特征上，并可用于如TTS等下游任务。ProMode编码器以声学特征和时间对齐的文本内容为输入，两者均被部分遮挡，并生成固定长度的潜在语音嵌入。解码器使用编码后的语音输入和未遮挡的文本内容来预测遮挡区域的声学特征。在GigaSpeech数据集上训练后，我们将该方法与最新风格编码器进行比较。对于音高和能量预测，我们的模型在不同粒度上均表现出一致的优势。此外，我们将这些预测的语音特征集成到TTS系统中，并进行感知测试，结果显示我们的模型在重视语音建模的任务中具有潜力。</p>
<p><strong>要点</strong></p>
<ol>
<li>语音传达了丰富的情感和语义信息，以及个人的独特之处。</li>
<li>提出了一种将文本转换为语音特征的独立模型，包括音高和能量等。</li>
<li>ProMode编码器处理部分遮挡的声学特征和文本内容，生成固定长度的潜在语音嵌入。</li>
<li>解码器结合编码后的语音输入和未遮挡的文本内容来预测声学特征。</li>
<li>在GigaSpeech数据集上训练的模型在音高和能量预测方面表现出优越性能。</li>
<li>预测的语音特征被集成到TTS系统中，并进行感知测试，显示出较高的语音偏好。</li>
<li>模型在需要重视语音建模的任务中具有潜在应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09389">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b18191830fb97fb1e6349bd2eaf4507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49ab74d78d81119327830744c69d2681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b97c2219640ed69b03a6251b71c474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-116f6189257d962f29e08d3734436d51.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Fake-Mamba-Real-Time-Speech-Deepfake-Detection-Using-Bidirectional-Mamba-as-Self-Attention’s-Alternative"><a href="#Fake-Mamba-Real-Time-Speech-Deepfake-Detection-Using-Bidirectional-Mamba-as-Self-Attention’s-Alternative" class="headerlink" title="Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional   Mamba as Self-Attention’s Alternative"></a>Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional   Mamba as Self-Attention’s Alternative</h2><p><strong>Authors:Xi Xuan, Zimo Zhu, Wenxin Zhang, Yi-Cheng Lin, Tomi Kinnunen</strong></p>
<p>Advances in speech synthesis intensify security threats, motivating real-time deepfake detection research. We investigate whether bidirectional Mamba can serve as a competitive alternative to Self-Attention in detecting synthetic speech. Our solution, Fake-Mamba, integrates an XLSR front-end with bidirectional Mamba to capture both local and global artifacts. Our core innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and PN-BiMamba. Leveraging XLSR’s rich linguistic representations, PN-BiMamba can effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and 5.85% EER, respectively, representing substantial relative gains over SOTA models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time inference across utterance lengths, demonstrating strong generalization and practical viability. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xuanxixi/Fake-Mamba">https://github.com/xuanxixi/Fake-Mamba</a>. </p>
<blockquote>
<p>随着语音合成技术的进步，加剧了安全威胁，推动了实时深度伪造检测研究。我们研究了双向Mamba是否能成为检测合成语音中自注意力机制的竞争替代方案。我们的解决方案“Fake-Mamba”结合了XLSR前端与双向Mamba，可以捕捉局部和全局伪迹。我们的核心创新在于推出了三种高效编码器：TransBiMamba、ConBiMamba和PN-BiMamba。利用XLSR丰富的语言表示，PN-BiMamba可以有效地捕捉合成语音的微妙线索。在ASVspoof 21 LA、21 DF和In-The-Wild基准测试中，Fake-Mamba的EER分别为0.97%、1.74%和5.85%，相较于最新模型XLSR-Conformer和XLSR-Mamba取得了实质性的相对增益。该框架能够维持跨句子长度的实时推理，显示出强大的泛化能力和实用性。代码可在<a target="_blank" rel="noopener" href="https://github.com/xuanxixi/Fake-Mamba%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xuanxixi/Fake-Mamba找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09294v1">PDF</a> Accepted at IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了语音合成技术的进展所带来的安全威胁，并强调了实时检测深度伪造语音的重要性。研究团队提出了一种名为Fake-Mamba的新方法，该方法结合了XLSR前端技术与双向Mamba，旨在捕捉语音中的局部和全局特征。其核心创新点在于引入了三种高效的编码器：TransBiMamba、ConBiMamba和PN-BiMamba。在ASVspoof 21 LA、21 DF和In-The-Wild基准测试中，Fake-Mamba取得了显著的成绩，相对于现有最佳模型XLSR-Conformer和XLSR-Mamba有明显的相对增益。该框架实现了跨语句长度的实时推理，展现出强大的通用性和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音合成技术的进步加剧了安全威胁，推动了实时深度伪造语音检测研究的重要性。</li>
<li>Fake-Mamba方法结合了XLSR前端技术与双向Mamba，用以捕捉语音中的局部和全局特征。</li>
<li>Fake-Mamba的核心创新在于引入了TransBiMamba、ConBiMamba和PN-BiMamba三种高效编码器。</li>
<li>PN-BiMamba能有效捕捉合成语音的细微特征，利用XLSR丰富的语言表征。</li>
<li>Fake-Mamba在ASVspoof 21 LA、21 DF和In-The-Wild基准测试中表现优异，相对现有模型有显著改进。</li>
<li>Fake-Mamba框架实现了跨语句长度的实时推理，展现出良好的通用性和实用性。</li>
<li>研究的代码已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09294">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-683cc4b593ffc919cd5a1d46b4aa7ece.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2547ee12a4dfe562a6fab1891f601d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522dbbaa71aab70b1b338d37f90cc03f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07cad8646684bb2b2dacb35eaa2aeccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d155da1c4f520a5d2fab354ece2606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85e92b22b85d509a33e02b517420f55c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e34879e3dc5e8e110b31d89a1204e8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS"><a href="#Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS" class="headerlink" title="Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in   Dysarthric Speech Cloning using F5-TTS"></a>Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in   Dysarthric Speech Cloning using F5-TTS</h2><p><strong>Authors:M Anuprabha, Krishna Gurugubelli, Anil Kumar Vuppala</strong></p>
<p>Dysarthric speech poses significant challenges in developing assistive technologies, primarily due to the limited availability of data. Recent advances in neural speech synthesis, especially zero-shot voice cloning, facilitate synthetic speech generation for data augmentation; however, they may introduce biases towards dysarthric speech. In this paper, we investigate the effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using TORGO dataset, focusing on intelligibility, speaker similarity, and prosody preservation. We also analyze potential biases using fairness metrics like Disparate Impact and Parity Difference to assess disparities across dysarthric severity levels. Results show that F5-TTS exhibits a strong bias toward speech intelligibility over speaker and prosody preservation in dysarthric speech synthesis. Insights from this study can help integrate fairness-aware dysarthric speech synthesis, fostering the advancement of more inclusive speech technologies. </p>
<blockquote>
<p>发音障碍的语音为开发辅助技术带来了重大挑战，这主要是因为数据有限。神经网络语音合成的最新进展，尤其是零样本语音克隆技术，促进了语音合成的数据增强；然而，它们可能对发音障碍的语音引入偏见。在本文中，我们使用TORGO数据集研究使用当前先进的F5-TTS进行发音障碍语音克隆的有效性，重点关注清晰度、说话人相似性和韵律保持。我们还使用公平性的指标，如不公平影响和均等差异等评估不同发音障碍严重程度的差距来评估潜在的偏见。结果表明，在发音障碍语音合成中，F5-TTS更偏向于语音清晰度而非说话人和韵律的保留。这项研究的见解有助于实现公平性意识的发音障碍语音合成整合，从而促进更具包容性的语音技术的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05102v3">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>摘要</strong></p>
<p>本研究探讨了先进的F5-TTS在利用TORGO数据集进行语言克隆技术在助残语音合成方面的有效性，重点研究了其清晰度、说话人相似性和语调保持能力。同时，本研究还利用公正性指标（如Disparate Impact和Parity Difference）分析了可能存在的偏见问题，以评估不同语言障碍程度之间的差异。研究结果表明，F5-TTS在合成语言障碍语音时，更偏向于清晰度而非说话人和语调的保持。这项研究为融合公正意识的语言障碍语音合成提供了重要见解，推动了更具包容性的语音技术的进展。尽管对于解决这一难题的最新神经语音合成方法展现了很大潜力，但由于该领域的复杂性和需求的多变性，语言障碍人士的辅助技术仍然面临巨大的挑战。未来的研究需要更深入地探索如何在保障语音合成质量的同时，减少偏见和误差，以满足不同语言障碍程度的需求。同时，还需要收集更多真实且多样化的数据，以提高技术的适用性。因此，建立一个公平、高效的语音辅助系统是一个重要的研究方向。同时我们还需要继续深入研究和改进现有技术以提高语音合成的质量减少偏见和误差更好地满足语言障碍者的需求推动人工智能技术在无障碍交流领域的广泛应用。我们相信通过不断的研究和创新我们可以为语言障碍者提供更好的支持和帮助让他们能够更轻松地与他人交流并享受高质量的生活体验。我们相信人工智能的无障碍交流潜力将为未来的社会带来深远影响。我们相信人工智能的无障碍交流潜力将为未来的社会带来深远影响并促进人类社会的持续进步和发展。</p>
<p><strong>关键见解</strong></p>
<p>一、神经语音合成技术在解决语言障碍数据稀缺问题上展现出巨大潜力，尤其在零样本语音克隆方面尤为突出。这对于增强语言障碍者的交流能力具有关键作用。<br>二、先进F5-TTS技术在克隆语言障碍语音方面效果显著，但在清晰度、说话人相似性和语调保持三者之间存在权衡关系。本研究揭示了其更偏向于清晰度的倾向性。这对于理解其性能和优化该领域技术具有指导意义。<br>三、本研究使用公正性指标分析了潜在的偏见问题，表明评估不同语言障碍程度之间的差异对开发更具包容性的语音技术至关重要。这为未来研究提供了重要方向，即如何在保障语音合成质量的同时减少偏见和误差。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7d381dde504306abd19801f5512adc7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11cb965df036a539d539002d06657872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d352577f01a896095cd9abf96721ba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f31d6b4e4c9415f9ea9912a565e405e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19be485cd9d2cd3196c29abf4624f25b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bbaf8b862f267b0ecb30dd70a751e25d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-595a3f2489e08dd204c7dc0d37291d91.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Marco-Voice-Technical-Report"><a href="#Marco-Voice-Technical-Report" class="headerlink" title="Marco-Voice Technical Report"></a>Marco-Voice Technical Report</h2><p><strong>Authors:Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Marco-Voice">https://github.com/AIDC-AI/Marco-Voice</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS">https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS</a> respectively. </p>
<blockquote>
<p>本文介绍了一个多功能语音合成系统，该系统在一个统一框架内集成了语音克隆和情感控制语音合成。本工作的目标是解决长期以来在实现高度表达、可控和自然语音生成方面所面临的挑战，忠实地在各种语言和情感上下文中保留说话者身份。我们的方法引入了一种有效的说话人情感分离机制，采用批量对比学习，实现对说话人身份和情感风格的独立操作，以及用于平稳情感控制的旋转情感嵌入集成方法。为了支持全面的训练和评估，我们构建了CSEMOTIONS数据集，这是一个高质量的情感语音数据集，包含来自六位专业说话人的10小时普通话语音，跨越七个情感类别。大量实验表明，我们的系统Marco-Voice在客观和主观指标上均取得了显著改进。进行了全面的评估和分析，结果表明MarcoVoice在语音清晰度和情感丰富度方面表现出竞争力，代表了神经语音合成领域的重大进展。我们的代码和数据集分别在<a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Marco-Voice%E5%92%8Chttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONS%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/AIDC-AI/Marco-Voice和https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS上公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02038v4">PDF</a> Technical Report. Our code and dataset are publicly available at   <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Marco-Voice">https://github.com/AIDC-AI/Marco-Voice</a> and   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS">https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS</a> respectively</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个多功能语音合成系统，该系统在统一框架内集成了语音克隆和情感控制语音合成。旨在解决长期存在的挑战，实现高度表达、可控和自然的语音生成，并忠实保留说话者身份在不同的语言和情感背景中。通过引入有效的说话人情感分离机制和旋转情感嵌入集成方法，实现对说话人身份和情感风格的独立操作和平滑的情感控制。为支持全面的训练和评估，构建了高质量的情感语音数据集CSEMOTIONS。实验表明，Marco-Voice系统在客观和主观指标上取得了显著改进，并在语音清晰度和情感丰富度方面表现出竞争力，代表了神经语音合成领域的重大进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文介绍了一个集成语音克隆和情感控制语音合成的多功能语音合成系统。</li>
<li>系统的目标是实现高度表达、可控和自然的语音生成，并忠实保留说话者身份。</li>
<li>通过引入有效的说话人情感分离机制和旋转情感嵌入集成方法，实现了对说话人身份和情感风格的独立操作。</li>
<li>为支持全面的训练和评估，构建了高质量的情感语音数据集CSEMOTIONS。</li>
<li>Marco-Voice系统在客观和主观指标上均取得了显著改进。</li>
<li>综合评估结果显示，MarcoVoice在语音清晰度和情感丰富度方面表现出竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f8ef6708c0466935694ee50d46cad3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65fa35ed79a2045e61868e07682c1d5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EmoVoice-LLM-based-Emotional-Text-To-Speech-Model-with-Freestyle-Text-Prompting"><a href="#EmoVoice-LLM-based-Emotional-Text-To-Speech-Model-with-Freestyle-Text-Prompting" class="headerlink" title="EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text   Prompting"></a>EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text   Prompting</h2><p><strong>Authors:Guanrou Yang, Chen Yang, Qian Chen, Ziyang Ma, Wenxi Chen, Wen Wang, Tianrui Wang, Yifan Yang, Zhikang Niu, Wenrui Liu, Fan Yu, Zhihao Du, Zhifu Gao, ShiLiang Zhang, Xie Chen</strong></p>
<p>Human speech goes beyond the mere transfer of information; it is a profound exchange of emotions and a connection between individuals. While Text-to-Speech (TTS) models have made huge progress, they still face challenges in controlling the emotional expression in the generated speech. In this work, we propose EmoVoice, a novel emotion-controllable TTS model that exploits large language models (LLMs) to enable fine-grained freestyle natural language emotion control, and a phoneme boost variant design that makes the model output phoneme tokens and audio tokens in parallel to enhance content consistency, inspired by chain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring expressive speech and fine-grained emotion labels with natural language descriptions. EmoVoice achieves state-of-the-art performance on the English EmoVoice-DB test set using only synthetic training data, and on the Chinese Secap test set using our in-house data. We further investigate the reliability of existing emotion evaluation metrics and their alignment with human perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and Gemini to assess emotional speech. Dataset, code, checkpoints, and demo samples are available at <a target="_blank" rel="noopener" href="https://github.com/yanghaha0908/EmoVoice">https://github.com/yanghaha0908/EmoVoice</a>. </p>
<blockquote>
<p>人类语言不仅仅是信息的传递，更是一种情感上的深刻交流和个人之间的连接。尽管文本转语音（TTS）模型已经取得了巨大的进步，但在控制生成语音的情感表达方面仍然面临挑战。在这项工作中，我们提出了EmoVoice，这是一种新型的情感可控TTS模型，它利用大型语言模型（LLM）实现精细的自由式自然语言情感控制，并设计了一种音素增强变体，使模型能够并行输出音素标记和音频标记，以增强内容的一致性，该设计灵感来源于思维链（CoT）和模态链（CoM）技术。此外，我们还介绍了EmoVoice-DB，这是一个高质量的40小时英语情感数据集，包含表达性语音和具有自然语言描述的精细情感标签。EmoVoice仅使用合成训练数据即可在英文EmoVoice-DB测试集上实现最新性能，并在使用我们内部数据的中文Secap测试集上表现出色。我们进一步研究了现有情感评估指标的可靠性及其与人类感知偏好的一致性，并探讨了使用最先进的多媒体LLM GPT-4o-audio和Gemini来评估情感语音。数据集、代码、检查点和演示样本均可在<a target="_blank" rel="noopener" href="https://github.com/yanghaha0908/EmoVoice%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yanghaha0908/EmoVoice上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12867v4">PDF</a> Accepted at ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为EmoVoice的新型情感可控文本转语音（TTS）模型。该模型利用大型语言模型（LLM）实现精细的自由式自然语言情感控制，并提出一种音素增强变体设计，以在生成语音时增强内容一致性。此外，还引入了高质量的情感数据集EmoVoice-DB，并发现现有情感评估指标的可靠性及其与人类感知偏好的一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmoVoice是一个新型的情感可控TTS模型，可借助大型语言模型进行精细的情感控制。</li>
<li>EmoVoice利用音素增强变体设计，通过生成音素令牌和音频令牌来增强内容一致性。</li>
<li>引入高质量的情感数据集EmoVoice-DB，用于训练和评估TTS模型的性能。</li>
<li>EmoVoice在英文和中文数据集上均达到最新技术水平。</li>
<li>现有情感评估指标的可靠性及其与人类感知偏好的一致性有待进一步探讨。</li>
<li>使用SOTA多模态大型语言模型GPT-4o-audio和Gemini来评估情感语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a14efc1c01ba8c0b01c03137cbc42ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19424b059b00990b3a5e4b40353fd6f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f8b8f824e7680815b8618381c5c21b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c7959d78c8a7935a8a941acc9717758.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-002790b9d15bcadeba967796018f6558.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc4196e3281ed544b5120bc8970520ad.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9e0a2043427c83e6f6f25324a69a1027.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-08-19  Seeing, Listening, Remembering, and Reasoning A Multimodal Agent with   Long-Term Memory
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-409025a5b77b6e0cd179f75508e39a11.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-19  DashCam Video A complementary low-cost data stream for on-demand   forest-infrastructure system monitoring
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
